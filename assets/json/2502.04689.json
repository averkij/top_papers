{
    "paper_title": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning",
    "authors": [
        "Yuwei Yin",
        "Giuseppe Carenini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (\"think step by step\"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR."
        },
        {
            "title": "Start",
            "content": "ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning Yuwei Yin University of British Columbia yuweiyin@cs.ubc.ca Giuseppe Carenini University of British Columbia carenini@cs.ubc.ca 5 2 0 2 7 ] . [ 1 9 8 6 4 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiplechoice question-answering (QA) tasks. Zeroshot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (think step by step). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Zhao et al., 2023; Min et al., 2023; Minaee et al., 2024) have been transformative technique in Natural Language Processing (NLP) owing to their excellent text generation and conversation abilities (Hurst et al., 2024; Anthropic, 2024; Team et al., 2024a). Challenging benchmarks for language model evaluation have significantly driven LLM advancements (Chang et al., 2024), with most designed as multiple-choice question-answering (MCQA) tasks (Robinson and Wingate, 2023) requiring answer selection from given options (Clark et al., 2018; Liu et al., 2020; Hendrycks et al., 2021). Recent LLM benchmarks demand extensive commonsense, world knowledge, 1Source code: https://github.com/YuweiYin/ARR 1 Figure 1: ARR motivation. To answer question, we often need to analyze the questions intent, retrieve relevant information, and reason step by step. and complex reasoning (Srivastava et al., 2023; Suzgun et al., 2023; Wang et al., 2024b), posing significant challenges for LLMs. Optimizing LLM performance in QA tasks is increasingly crucial for their continued development. Recent advancements have introduced various methods to enhance LLM reasoning abilities (Qiao et al., 2023; Sun et al., 2023), with Chain-ofThought (CoT) prompting proving effective across various tasks (Li et al., 2024). Key variants include few-shot CoT (Wei et al., 2022), which provides rationale-based exemplars for in-context learning (Brown et al., 2020; Dong et al., 2024), and zero-shot CoT (Kojima et al., 2022), which employs general instructions such as Lets think step by step. Due to its simplicity and effectiveness, zero-shot CoT has gained extensive adoption. By appending this trigger sentence to the original QA prompt, LLMs generate step-by-step reasoning to improve question-answering performance. Despite its widespread use, zero-shot CoT prompting provides only generic reasoning guidance. As illustrated in Figure 1, answering complex questions typically involves three key steps: (1) analyzing the questions intent (Adams, 1986; Mele, 1989; Mele and Moser, 1994) to obtain thorough context understanding, clear problem-solving target, and purposeful planning guide, (2) retrieving relevant information from context, external sources, or memory for supportive reference (Jones and Steinhardt, 2022; Shi et al., 2023), and (3) systematically applying inductive and deductive reasoning (Clark, 1969; Johnson-Laird, 1999; Heit, 2000; Hayes and Heit, 2018). Therefore, we hypothesize that an effective prompt should direct LLMs to complete these steps. To verify this hypothesis, we propose refined zero-shot prompting method, ARR, which explicitly incorporates these three elements: Analyzing, Retrieving, and Reasoning. Specifically, ARR employs the answer trigger sentence: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. This structured approach is expected to enhance the performance across diverse QA tasks and various models, akin to the improvements observed with zero-shot CoT upon its first introduction (Kojima et al., 2022). To evaluate the effectiveness of the proposed ARR method, we test the performance (accuracy) of open-weights LLMs (Dubey et al., 2024) on 10 multiple-choice QA datasets, covering reading comprehension (Clark et al., 2019; Liu et al., 2020), commonsense reasoning (Talmor et al., 2019; Sap et al., 2019), world knowledge (Welbl et al., 2017; Mihaylov et al., 2018; Clark et al., 2018), and multitask understanding (Suzgun et al., 2023; Hendrycks et al., 2021; Wang et al., 2024b). Compared to the Baseline method without specific trigger sentence and the zero-shot CoT method with general prompts, ARR consistently improves QA performance across all datasets, demonstrating its effectiveness and superiority. Additionally, ablation studies show that each individual component of ARRAnalyzing, Retrieving, and Reasoning outperforms both the Baseline and CoT methods, confirming their positive contributions. Notably, the Analyzing-only setting yields the largest performance gain on average, highlighting the critical role of intent analysis in question answering. Beyond quantitative results, we provide qualitative case studies to reveal problems in the Baseline and CoT methods such as intent misunderstanding, context misuse, and faulty reasoning. Furthermore, we conduct extensive experiments across various settings to assess the generalizability of the proposed ARR method. ARR consistently outperforms alternatives across different model sizes, LLM series (architectures), generation temperatures, and few-shot scenarios. These comprehensive experiments and analyses further solidify its effectiveness, robustness, and adaptability. The key contributions of this work are as follows: 1. This paper proposes ARR, an intuitive, general, and effective zero-shot prompting method to improve LLM performance in various question-answering tasks. 2. Comprehensive experiments across diverse QA tasks demonstrate that ARR consistently outperforms the Baseline and CoT methods. Ablation and case studies further validate the positive contributions of each component. 3. Additional extensive experiments on various settings solidify the effectiveness and generalizability of ARR across different model sizes, LLM series, and generation configurations."
        },
        {
            "title": "2.1 LLM Prompting",
            "content": "Recent large language models (LLMs) (Dubey et al., 2024; Lambert et al., 2024; Liu et al., 2024) are pre-trained on large-scale text corpora curated from the Internet (Soldaini et al., 2024; Penedo et al., 2024; Weber et al., 2024). Their advanced text understanding and generation capabilities (Hurst et al., 2024; Anthropic, 2024; Team et al., 2024a) have significantly revolutionized the field of natural language processing (NLP). Consequently, the NLP paradigm is shifting toward framework comprising pre-training, posttraining, and prompting (Liu et al., 2023), with post-training focusing on aligning models with human preferences (Ouyang et al., 2022; Bai et al., 2022; Rafailov et al., 2023) rather than fine-tuning for specific downstream tasks (Devlin et al., 2019). After the training stages, LLMs can generate satisfactory responses to natural language instructions and questions, highlighting the growing importance of prompt design (White et al., 2023; Giray, 2023; Sahoo et al., 2024). In this work, we propose an intuitive, general, and effective prompting method to enhance LLM performance in question-answering."
        },
        {
            "title": "2.2 LLM Reasoning",
            "content": "Recent LLM research increasingly emphasizes reasoning abilities (Qiao et al., 2023; Sun et al., 2023). Chain-of-Thought (CoT) is prompting strategy that enhances problem-solving by guiding LLMs to generate intermediate reasoning steps. 2 Figure 2: Question answering with LLMs. We first obtain rationale ri by reasoning generation and then select the optimal option via evaluating the language modeling losses of different context-option combinations. Main variants include zero-shot CoT (Kojima et al., 2022) that uses general instructions such as Lets think step by step and few-shot CoT (Wei et al., 2022) that provides exemplars with rationales to leverage in-context learning (Brown et al., 2020; Dong et al., 2024). Building on CoT, various reasoning techniques have emerged (Zhou et al., 2023a,b; Wang et al., 2023a; Yasunaga et al., 2024; Wang and Zhou, 2024). Some studies explore optimal reasoning paths through selfconsistency (Wang et al., 2023c; Chen et al., 2023) or tree-like searches (Yao et al., 2023), while others investigate self-refinement (Madaan et al., 2023), self-correction (Huang et al., 2024; Tyen et al., 2024; Chen et al., 2024), self-verification (Cobbe et al., 2021; Li et al., 2023b; Lightman et al., 2024), and self-evolution (Guan et al., 2025; Lee et al., 2025) mechanisms. Beyond prompting and generation-based approaches, post-training methods (Chu et al., 2025), particularly those leveraging reinforcement learning (RL) (Sutton and Barto, 2018), have been developed to enhance reasoning capabilities (Shao et al., 2024; Wang et al., 2024a; Setlur et al., 2024; Xu et al., 2025). As reasoning-eliciting prompting approach, ARR effectively complements existing research by guiding LLMs through three essential steps: intent analysis, information retrieval, and step-by-step reasoning."
        },
        {
            "title": "2.3 Retrieval-Augmented Generation",
            "content": "Retrieval-Augmented Generation (RAG) enhances output quality by retrieving relevant information from pre-processed knowledge sources (Gao et al., 2023). The retrieving component of our ARR method is inspired by the traditional external RAG approach (Lewis et al., 2020), which retrieves relevant information from the explicit context or outer sources, and realizes instead form of internal RAG, which utilizes language models as implicit knowledge bases (Petroni et al., 2019; Jiang et al., 2020) and extracts references from memory (training data) (Carlini et al., 2021; Shi et al., 2024). This retrieval mechanism is essential for enhancing LLM performance in question answering, as irrelevant information can significantly degrade accuracy (Jones and Steinhardt, 2022; Shi et al., 2023; Yoran et al., 2024)."
        },
        {
            "title": "3 Question Answering with LLMs",
            "content": "This section presents formally defined multiplechoice question-answering workflow using large language models. Our pipeline combines ideas from the two-step prompting introduced by Kojima et al. (2022) and the multiple-choice selection method proposed by Robinson and Wingate (2023)."
        },
        {
            "title": "3.1 Question Answering Data",
            "content": "In this work, we consider multiple-choice questionanswering (MCQA) tasks with one correct answer, where the model is asked to answer the question by selecting an option from list of choices. Formally, let = {X , Y} be MCQA dataset, where = {X1, X2, . . . , Xn} is the input information, = {y1, y2, . . . , yn} is the corresponding correctchoice label (yi R), and is the number of instances in D. In closed-book QA tasks, Xi = {qi, oi}, where qi is the i-th question, and oi = {oj j=1 is the option list with choices. In open-book QA tasks, Xi = {pi, qi, oi}, where pi is the i-th passage provided by the task. Then, we obtain the input prompt xi for LLMs as follows: }m xi = (cid:40) P(pi, qi, oi), Open-book QA P(qi, oi), Closed-book QA (1) where P() denotes the prompt function which concatenates the string objects in Xi using line breaks as the delimiter ( =n). Thus, P(pi, qi, oi) is: pi qi o1 o2 . . . om ϕ The answer trigger sentence ϕ is the only difference between the proposed ARR method and 3 baseline methods in each experiment. Figure 2 presents each ϕ used in Baseline, zero-shot CoT, and our ARR methods. For simplicity, = {x1, x2, . . . , xn} is used in the rest of the paper for both openand closed-book QA."
        },
        {
            "title": "3.2 Multiple-Choice Question Answering",
            "content": "Stage 1: Reasoning Generation. Let xi be the tokenized representation of text xi. The decoderonly Transformer-based (Vaswani et al., 2017; Radford et al., 2018) LLM takes xi as input and generate new token after each timestep. The model freely generates the text response given by ri = M(xi), (2) where ri may contain the analysis, reasoning, and answer. Then, we combine the original text input xi, the generated response ri, and each choice oj in the option list oi as follows: = P(xi, ri, oj zj ). (3) Stage 2: Option Selection. Let zj = [tj;1 , tj;2 , ] RL be the tokenized zj . . . , tj;L , where is the number of effective tokens that are not used for word masking or sequence padding. To select an option, we feed the model and obtain the cross-entropy loss (Shannon, 1948, 1951; Jurafsky and Martin, 2025) of each zj Lj = (cid:88) log Pr(tj;k as follows: tj;<k ; Θ), (4) where Θ is the parameters of M, tj;k token, and tj;<k before tj;k {oj }m loss Lj is selected, i.e., is the k-th denotes all the previous tokens . Hence, for each option oj in oi = j=1, we have corresponding cross-entropy . Then, the option with the lowest loss value ˆyi = arg min j{1,2,...,m} {Lj }m j=1. Thus, the overall accuracy is calculated by α = 1 (cid:88) i= I(yi = ˆyi), (5) (6) where α [0, 1] and the indicator function I() returns 1 if yi = ˆyi or 0 otherwise."
        },
        {
            "title": "4 Experimental Setup",
            "content": "This section introduces the experimental setup, including datasets, models, and evaluation settings.2 2Please refer to Appendix for more experiment details."
        },
        {
            "title": "Split",
            "content": "# Item # Token # Class BoolQ LogiQA CSQA SIQA SciQ OBQA ARC BBH MMLU MMLU-Pro"
        },
        {
            "title": "Valid\nTest\nValid\nValid\nTest\nTest\nTest\nTest\nTest\nTest",
            "content": "3,270 651 1221 1,954 1,000 500 3,548 5,281 13,842 12,032 145 192 43 51 132 55 59 112 108 186 2 4 5 3 4 4 4 218 4 10 Table 1: QA dataset statistics. # Class is the number of options m, # Item is the total number of data items for evaluation, and # Token is the average number of tokens per instance (zero-shot prompt), tokenized by the LLaMA (Dubey et al., 2024) tokenizer."
        },
        {
            "title": "4.1 Datasets",
            "content": "As mentioned in 3.1, we consider 10 multiplechoice QA tasks with questions qi and options oi. Reading comprehension tasks (Chen, 2018) explicitly provide passages pi to base on. The model is asked to answer the question by choosing one from the option list. We consider wide range of QA benchmarks to evaluate the capabilities of in different aspects, including reading comprehension, commonsense reasoning, world knowledge, and multitask understanding. The dataset statistics are presented in Table 1."
        },
        {
            "title": "4.1.1 Reading Comprehension\nBoolQ. BoolQ (Clark et al., 2019) is a question\nanswering dataset for yes/no questions. It evaluates\nthe performance of M on reading comprehension.",
            "content": "LogiQA. LogiQA (Liu et al., 2020) is reading comprehension dataset that requires to have logical reasoning for question-answering."
        },
        {
            "title": "4.1.2 Commonsense Reasoning\nCSQA. Commonsense QA (Talmor et al., 2019)\nexamines M on commonsense question-answering\nproblems constructed using information from Con-\nceptNet (Speer et al., 2017).",
            "content": "SIQA. SocialIQA (Sap et al., 2019) is largescale QA benchmark for commonsense reasoning about social situations, which probes emotional and social intelligence in everyday situations."
        },
        {
            "title": "Multitask Understanding",
            "content": "BoolQ LogiQA CSQA SIQA SciQ OBQA ARC BBH MMLU MMLU-Pro"
        },
        {
            "title": "Baseline\nCoT\nARR",
            "content": "84.16 84.65 86.33 35.79 38.10 39.02 72.97 73.71 74.94 69.55 68.12 70.98 85.90 93.70 94.40 72.20 78.20 80. 82.59 84.31 84.84 52.19 58.40 59.01 60.67 62.08 63.51 38.75 40.10 42.72 Avg. 65.48 68.14 69. Table 2: Main experiments. The zero-shot performance (Accuracy %) of the LLaMA3-8B-Chat model on various multiple-choice QA datasets using different answer trigger sentences ϕ. (1) Baseline: ϕ is Answer:; (2) CoT (Kojima et al., 2022): ϕ is Answer: Lets think step by step.; (3) ARR: our method that elicits intent analyzing, information retrieving, and step-by-step reasoning. OBQA. OpenBookQA (Mihaylov et al., 2018) asks to answer the question based on the given elementary level science facts and broad commonsense knowledge. ARC. AI2 Reasoning Challenge (Clark et al., 2018) contains grade-school science questions and is divided into Challenge and an Easy set."
        },
        {
            "title": "4.1.4 Multitask Understanding",
            "content": "BBH. BIG-Bench Hard (Suzgun et al., 2023) is suite challenging tasks filtered from BIGBench (Srivastava et al., 2023). Solving these problems often requires multi-step reasoning. In this work, 4 (out of 27) subtasks in BBH are discarded as they are not multiple-choice QA tasks. MMLU. MMLU (Hendrycks et al., 2021) comprehensively measures the multitask accuracy of on 57 tasks including elementary mathematics, history, computer science, and more. MMLU-Pro. MMLU-Pro (Wang et al., 2024b) extends the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options."
        },
        {
            "title": "4.2 Models",
            "content": "Our experiments adopt open-weights, decoderonly, and Transformer-based (Vaswani et al., 2017) LLMs. We mainly employ LLaMA3-8BChat (Dubey et al., 2024), an instruction-following LLM with 8 billion model parameters, and use the model implementation and checkpoints provided by Hugging Face Transformers (Wolf et al., 2020). In generalizability experiments, we also explore LLaMA3-Chat models of different sizes in 6.1 and 7B-Chat models of different LLM series in 6.2, i.e., Qwen2.5 (Yang et al., 2024), Gemma (Team et al., 2024b,c), and Mistral (Jiang et al., 2023)."
        },
        {
            "title": "4.3 Evaluation",
            "content": "To evaluate the QA performance of LLMs, we apply two-step process including reasoning generation and option selection, as mentioned in 3.2. First, we let the model freely generate text responses that may include their analysis, reasoning, and answer choice. Then, we concatenate the input and output in the first stage with each choice from the given option list, pass each concatenation to the model, and select the option with the lowest cross-entropy loss. The loss corresponds to the perplexity of language models: lower loss means lower perplexity and higher confidence. Length normalization is not applied because the options are mostly in the A/B/C/D, Yes/No, or True/False format. As the datasets in our experiments are all multiple-choice QA tasks, we adopt accuracy as the evaluation metric, which is calculated by Eq. 6."
        },
        {
            "title": "5 Main Experiments",
            "content": "In this section, we present the main experimental results, which verify our hypothesis by demonstrating the effectiveness of the proposed ARR method."
        },
        {
            "title": "5.1 QA Performance",
            "content": "The main experiments test the zero-shot QA performance of LLaMA3-8B-Chat (Dubey et al., 2024) on various multiple-choice QA datasets. We compare three methods (i.e., Baseline, zero-shot CoT (Kojima et al., 2022), and ARR), and the only difference between them is the answer trigger sentence ϕ shown in Figure 2. The results in Table 2 demonstrate that our ARR method boosts the Baseline method by large margin, with an improvement of +4.1% on average. Moreover, ARR consistently outperforms zero-shot CoT prompting across all QA datasets, highlighting its universal superiority in various task types including reading comprehension, commonsense reasoning, world knowledge, and multitask understanding. 5 Answer Trigger Sentence ϕ ➀ Answer: Lets analyze the intent of the question , find relevant information , and answer the question with step-by-step reasoning . ➁ ➂ ➃ ➄ Answer: Lets analyze the intent of the question , and answer the question. Answer: Lets find relevant information , and answer the question. Answer: Lets answer the question with step-by-step reasoning . Answer: Table 3: Ablation study prompts. The answer trigger sentences ϕ used in different ARR ablation study settings."
        },
        {
            "title": "Multitask Understanding",
            "content": "A BoolQ LogiQA CSQA SIQA SciQ OBQA ARC BBH MMLU MMLU-Pro ➀ 86.33 ➁ ➂ ➃ 86.09 85.35 85.87 ➄ 84. 39.02 38.40 37.79 38.86 35.79 74.94 70.98 94. 80.00 84.84 59.01 63.51 75.76 75.59 74.53 70.78 68.01 68. 94.30 92.80 94.50 86.80 81.20 82.60 85.83 85.33 85.03 57.08 58.27 58.96 63.66 63.73 61.77 72. 69.55 85.90 72.20 82.59 52.19 60. 42.72 42.54 43.08 41.11 38.75 Avg. 69.58 70.12 69.12 69. 65.48 Table 4: Ablation study results. The accuracy scores (%) of the LLaMA3-8B-Chat model on various multiplechoice QA datasets using different answer trigger sentences ϕ ( Analyzing , Retrieving , and Reasoning )."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "To better understand the performance gains shown in Table 2, we conduct an ablation study to explore the efficacy of each component of the ARR method, i.e., analyzing, retrieving, and reasoning. Specifically, we test the models QA performance using the five different answer trigger sentences ϕ in Table 3. Table 4 reports the accuracy scores of LLaMA3-8B-Chat under different ablation cases, where ➀ is the full version of ARR and ➄ is equivalent to the Baseline method in Table 2. In ➁, ➂, and ➃, ϕ only contains one single component, i.e., analyzing, retrieving, and reasoning, respectively. We observe that all the single-component ARR settings (➁, ➂, and ➃) outperform the Baseline method (➄) by large margin, which verifies that each ARR component contributes positively. In addition, ➃ outperforms the CoT method in Table 2, which means the Reasoning prompting of our ARR method (i.e., answer the question with stepby-step reasoning) is better than zero-shot CoT prompting (i.e., think step by step) for QA tasks. Furthermore, the complete ARR method (➀) has higher accuracy score than the Retrieving-only (➂) and Reasoning-only (➃) methods, meaning the intent Analyzing benefits the other two parts. Notably, the Analyzing-only setting (➁) brings the greatest improvement gain, suggesting the significance of analyzing the intent of the question. ➁ even outperforms the full ARR version (➀) on average, mainly because it performs excellently on the OBQA dataset (86.80%), which means understanding the questions intent in this task is vital. Excluding OBQA, the full ARR method achieves an average accuracy of 68.41%, surpassing ➁ (68.27%). Hence, we dive into concrete case study to examine the impact of intent analysis on QA solving."
        },
        {
            "title": "5.3 Case Study",
            "content": "Table 5 shows case in the OBQA dataset (Mihaylov et al., 2018) drawn from the main experiments3, where our ARR method correctly answers the question but the Baseline and CoT methods fail. We can observe that the baseline methods reasoning is incorrect: After stating that the summer solstice is in June, it wrongly concludes that the summer solstice is four months before July. The CoT method misunderstands the question, resulting in counting four months before June instead of after June. In contrast, our ARR method identifies the questions intent clearly, leading to correct reasoning path and final answer."
        },
        {
            "title": "6 Generalizability",
            "content": "The main experiments in 5 have validated the effectiveness of our ARR method quantitatively and qualitatively. To verify the generalizability of ARR, we conduct additional extensive experiments under different configurations on three challenging, reasoning-intense, and multitask benchmarks introduced in 4.1.4: BBH, MMLU, and MMLU-Pro. 3Please refer to Appendix for more cases. 6 Task Input xi Fact: the summer solstice is on June 21st in the northern hemisphere. Question: The summer solstice in the northern hemisphere is four months before (A) May (B) July (C) April (D) October Baseline ϕi Answer: Reasoning ri (B) July. Explanation: The summer solstice is the longest day of the year and marks the beginning of summer. It is four months before the beginning of winter. The beginning of winter is in December, so the summer solstice is in June. Therefore, the summer solstice is four months before July. CoT ϕi Answer: Lets think step by step. Reasoning ri The summer solstice is on June 21st. If we count four months before that, we get March 21st. March is the third month of the year, so the answer is (C). April is the fourth month, May is the fifth month, and October is the tenth month. None of these months is four months before June. July is the seventh month, which is five months after June, so it is not the correct answer either. The correct answer is (C). ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks for the month that is four months after the summer solstice. The summer solstice is on June 21st. Four months after June 21st is October 21st. Therefore, the correct answer is (D) October. Table 5: Case study. An instance from the OpenBookQA dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail because of intent misunderstanding or faulty reasoning. Size Method BBH MMLU MMLU-Pro Avg. Series Method BBH MMLU MMLU-Pro Avg. 1B 3B 8B"
        },
        {
            "title": "Baseline\nCoT\nARR",
            "content": "35.88 36.30 39.02 45.65 46.89 51.97 52.19 58.40 59.01 43.27 41.10 42.70 48.26 46.80 52.82 60.67 62.08 63. 21.62 22.74 23.49 30.88 30.03 33.39 38.75 40.10 42.72 33.59 33.38 35.07 41.60 41.24 46.06 50.54 53.53 55."
        },
        {
            "title": "Baseline\nCoT\nARR",
            "content": "39.21 36.66 40.50 40.09 44.39 45.31 46.27 53.42 53.55 48.36 44.91 50.34 45.46 47.17 50.73 55.61 61.16 61. 32.35 29.26 39.10 23.45 26.20 26.98 30.68 34.73 35.21 39.97 36.94 43.31 36.33 39.25 41.01 44.19 49.77 50. Table 6: Model size experiments. The zero-shot performance (Accuracy %) of LLaMA3-Chat models of different sizes on multiple-choice QA datasets. Table 7: LLM series experiments. The zero-shot performance (Accuracy %) of 7B-Chat models of different LLM series on multiple-choice QA datasets. performs the Baseline method on MMLU, likely due to the weaker instruction-following ability in smaller models. Still, our ARR method achieves overall performance improvements over the Baseline in the 1B setting. In addition, Figure 3 illustrates the trend of QA performance changes as the model becomes larger. The results conform to the scaling laws of language models (Kaplan et al., 2020), demonstrating the potential of the proposed ARR method when applied to larger models."
        },
        {
            "title": "6.2 LLM Series",
            "content": "To verify the effectiveness of our ARR method on models other than LLaMA3 (Dubey et al., 2024), we conduct experiments on 7B-Chat LLMs of different series: Qwen2.5 (Yang et al., 2024), Gemma (Team et al., 2024b,c), and Mistral (Jiang et al., 2023). The results in Table 7 exhibit consistent superiority of the proposed ARR method over the Baseline and CoT methods. This is similar to the findings in the main experiments  (Table 2)  , solidifying the efficacy and generalizability of ARR. Figure 3: Model size experiments. The trend of QA performance changes as the model becomes larger."
        },
        {
            "title": "6.1 Model Sizes",
            "content": "We evaluate the LLaMA3-Chat models of different sizes, i.e., 1B, 3B, and 8B (default) parameters, on multiple-choice QA tasks. As the accuracy scores (%) shown in Table 6, our ARR method brings solid performance gains over the Baseline method and consistently outperforms zeroshot CoT. For the 1B model, ARR slightly under7 Temp. Method BBH MMLU MMLU-Pro Avg. Shot Method BBH MMLU MMLU-Pro Avg. 0.0 0. 1.0 1."
        },
        {
            "title": "Baseline\nCoT\nARR",
            "content": "52.19 58.40 59.01 50.19 56.58 58.87 46.33 51.46 52.90 40.84 42.53 42.65 60.67 62.08 63.51 59.35 60.82 62. 54.80 55.57 56.58 45.03 44.85 45.16 38.75 40.10 42.72 36.88 37.82 42.64 33.10 33.00 36.73 26.85 25.61 27. 50.54 53.53 55.08 48.81 51.74 54.79 44.74 46.68 48.74 37.57 37.66 38.42 0"
        },
        {
            "title": "Baseline\nCoT\nARR",
            "content": "52.19 58.40 59.01 35.68 47.39 47.22 34.39 42.84 40.19 34.11 39.92 40.68 60.67 62.08 63.51 44.80 48.36 49. 42.08 48.21 49.68 41.14 47.48 49.19 38.75 40.10 42.72 28.62 31.07 34.33 25.92 26.69 37.04 25.76 26.12 36. 50.54 53.53 55.08 36.37 42.27 43.61 34.13 39.25 42.30 33.67 37.84 42.16 Table 8: Generation temperature experiments. The zero-shot performance (Accuracy %) of the LLaMA38B-Chat model on multiple-choice QA datasets using different generation temperatures (default: 0.0). Table 9: Few-shot experiments. The few-shot performance (Accuracy %) of the LLaMA3-8B-Chat model on multiple-choice QA datasets using 1, 3, and 5 fewshow examples with rationales."
        },
        {
            "title": "6.3 Generation Temperatures",
            "content": "For reproducibility, we set the generation temperature to 0 by default, as this setting makes the generation process deterministic. However, higher temperature brings more diverse output, which may lead to different QA accuracy. To study the effect of this key factor, we report the QA accuracy (%) of the LLaMA3-8B-Chat model using different temperatures during the reasoning generation stage: 0.0 (default), 0.5, 1.0, and 1.5. As shown in Table 8, our ARR method surpasses the Baseline and CoT methods with different temperatures, demonstrating strong robustness of ARR. In addition, we observe that the model generally performs better when the temperature is lower."
        },
        {
            "title": "6.4 Few-shot Generation",
            "content": "Few-shot Examples with Rationales. For each subtask in QA dataset, we randomly pick 10 examples from the training or validation set if they exists. If subtask only has the test set, 10 test examples are held out for few-shot usage, slightly reducing the number of items for evaluation. For each raw example, we construct the CoT and ARR rationales using GPT-4o (Hurst et al., 2024). Specifically, the input prompts provided to GPT-4o match those used in the evaluation experiments under CoT/ARR settings. The models output is extracted as CoT/ARR rationales. In few-shot examples, these rationales, along with correct answers, are appended to the answer trigger sentence ϕ. For the Baseline setting, few-shot examples include correct answers for in-context learning (ICL) (Brown et al., 2020; Dong et al., 2024) but exclude rationales. Few-shot Results. Table 9 presents the accuracy scores (%) of the LLaMA3-Chat model on multiplechoice QA tasks. Using different numbers of fewshot examples (1, 3, and 5), our few-shot ARR method outperforms the ICL Baseline and few-shot CoT (Wei et al., 2022) methods on average. Comparison across the three few-shot settings reveals that additional examples do not necessarily enhance performance. Moreover, QA performance is lower in the few-shot experiments than in the zero-shot setting, likely because the randomly selected examples mislead the reasoning process (Zhao et al., 2021; Lu et al., 2022; Peng et al., 2024). While demonstration selection methods could mitigate this issue (Gao et al., 2021; Rubin et al., 2022; Li et al., 2023a; Wang et al., 2023b), their exploration is beyond the scope of this study."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce ARR, an intuitive, simple, and general prompting method that effectively enhances the question-answering performance of LLMs by integrating three key steps: analyzing the questions intent, retrieving relevant information, and reasoning step by step. Extensive experiments across diverse QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms the CoT prompting method. Ablation and case studies further validate the positive contributions of each component, with intent analysis proving particularly crucial. In addition, evaluations across various model sizes, LLM series, and generation configurations confirm the effectiveness, robustness, and generalizability of the proposed ARR method."
        },
        {
            "title": "References",
            "content": "We did not explore variations or paraphrases of the proposed ARR prompts, opting instead for the straightforward expressions presented in this paper. While certain phrasings may further enhance performance, the core idea remains the same. In addition, resource constraints limited our focus to open-weights LLMs with no more than 8B parameters. However, the results from model size experiments ( 6.1) align with the scaling laws for language models (Kaplan et al., 2020), demonstrating the potential and generalizability of our ARR method when applied to larger models. Lastly, we observe that some generated rationales in the Reasoning Generation stage are repetitive and redundant. dynamic stopping strategy or post-processing to filter out redundancies can reduce the computational cost and potentially further boost the final QA accuracy."
        },
        {
            "title": "Acknowledgments",
            "content": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC). This research was supported in part by the computational resources and services provided by Advanced Research Computing at the University of British Columbia and the Digital Research Alliance of Canada (alliancecan.ca). We would also like to thank UBC NLP Group members for their constructive feedback. Douglas Adams. 1979. The Hitchhikers Guide to the Galaxy. Pan Books. Frederick Adams. 1986. Intention and intentional action: The simple view. Mind & Language, 1(4):281 301. Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume 33, pages 18771901, Virtual Event. NeurIPS. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 26332650. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2024. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3). Danqi Chen. 2018. Neural Reading Comprehension and Beyond. Stanford University. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations. 9 Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, Minneapolis, Minnesota. Association for Computational Linguistics. Herbert Clark. 1969. Linguistic processes in deductive reasoning. Psychological review, 76(4):387. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. survey on in-context learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11071128, Miami, Florida, USA. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 38163830, Online. Association for Computational Linguistics. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997. Louie Giray. 2023. Prompt engineering with chatgpt: guide for academic writers. Annals of Biomedical Engineering, 51(12):26292633. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Brett Hayes and Evan Heit. 2018. Inductive reasoning 2.0. Wiley Interdisciplinary Reviews: Cognitive Science, 9(3):e1459. Evan Heit. 2000. Properties of inductive reasoning. Psychonomic bulletin & review, 7:569592. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language In The models cannot self-correct reasoning yet. Twelfth International Conference on Learning Representations. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438. Philip Johnson-Laird. 1999. Deductive reasoning. Annual review of psychology, 50(1):109135. Erik Jones and Jacob Steinhardt. 2022. Capturing failures of large language models via human cognitive biases. In Advances in Neural Information Processing Systems, volume 35, pages 1178511799. Curran Associates, Inc. Daniel Jurafsky and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, 10 and Speech Recognition with Language Models, 3rd edition. Prentice Hall PTR. Online manuscript released January 12, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. 2024. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. 2025. Evolving deeper llm thinking. arXiv preprint arXiv:2501.09891. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023a. Unified demonstration retriever for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46444668, Toronto, Canada. Association for Computational Linguistics. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware In Proceedings of the 61st Annual Meetverifier. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333, Toronto, Canada. Association for Computational Linguistics. Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. 2024. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 36223628. International Joint Conferences on Artificial Intelligence Organization. Main track. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pretrain, prompt, and predict: systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):195:1195:35. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80868098, Dublin, Ireland. Association for Computational Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Curran Associates, Inc. Alfred Mele. 1989. Intention, belief, and intentional action. American Philosophical Quarterly, 26(1):19 30. Alfred Mele and Paul Moser. 1994. Intentional action. Nous, 28(1):3968. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models: survey. ACM Computing Surveys, 56(2):140. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: survey. arXiv preprint arXiv:2402.06196. 11 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557. Keqin Peng, Liang Ding, Yancheng Yuan, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. 2024. Revisiting demonstration selection strategies in in-context learning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9090 9101, Bangkok, Thailand. Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowlIn Proceedings of the 2019 Conferedge bases? ence on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 24632473, Hong Kong, China. Association for Computational Linguistics. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53685393, Toronto, Canada. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. OpenAI blog. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems. Joshua Robinson and David Wingate. 2023. Leveraging large language models for multiple choice question answering. In The Eleventh International Conference on Learning Representations. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 26552671, Seattle, United States. Association for Computational Linguistics. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China. Association for Computational Linguistics. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Jonathan Berant, and Aviral Kumar. Agarwal, 2024. Rewarding progress: Scaling automated proarXiv preprint cess verifiers for llm reasoning. arXiv:2410.08146. Claude Elwood Shannon. 1948. mathematical theory of communication. The Bell System Technical Journal, 27(3):379423. Claude Elwood Shannon. 1951. Prediction and entropy of printed english. Bell System Technical Journal, 30(1):5064. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 3121031227. PMLR. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, 12 Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, Bangkok, Thailand. Association for Computational Linguistics. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 44444451. AAAI Press. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. 2023. survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562. Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Bradford Book, Cambridge, MA, USA. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024a. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024b. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024c. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. 2024. LLMs cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, Bangkok, Thailand. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. 2024a. Offline reinforcement learning for llm multi-step reasoning. arXiv preprint arXiv:2412.16145. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language In Proceedings of the 61st Annual Meetmodels. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26092634, Toronto, Canada. Association for Computational Linguistics. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023b. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Xuezhi Wang and Denny Zhou. 2024. Chain-ofthought reasoning without prompting. arXiv preprint arXiv:2402.10200. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024b. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. Maurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, an open dataset for et al. 2024. Redpajama: 13 models robust to irrelevant context. In The Twelfth International Conference on Learning Representations. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1269712706. PMLR. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed H. Chi. 2023a. Least-to-most prompting enables comIn The plex reasoning in large language models. Eleventh International Conference on Learning Representations. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023b. Large language models are human-level In The Eleventh International prompt engineers. Conference on Learning Representations. training large language models. arXiv:2411.12372. arXiv preprint Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas Schmidt. 2023. prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. 2025. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. 2024. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations. Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2024. Making retrieval-augmented language"
        },
        {
            "title": "A Experiment Details",
            "content": "A.1 Dataset Details All QA datasets used in this work are loaded from Hugging Face datasets4. Table 10 lists the URL link of each dataset."
        },
        {
            "title": "URL",
            "content": "Link BoolQ (Clark et al., 2019) Link LogiQA (Liu et al., 2020) Link CSQA (Talmor et al., 2019) Link SIQA (Sap et al., 2019) Link SciQ (Welbl et al., 2017) Link OBQA (Mihaylov et al., 2018) Link ARC (Clark et al., 2018) Link BBH (Suzgun et al., 2023) MMLU (Hendrycks et al., 2021) Link MMLU-Pro (Wang et al., 2024b) Link Table 10: The URL links of adopted QA datasets. A.2 Model Details As mentioned in 4.2, we mainly employ LLaMA3-8B-Chat (Dubey et al., 2024), an instruction-following LLM with 8 billion model parameters, for most experiments. In generalizability experiments ( 6), we also explore LLaMA3Chat models of different sizes in 6.1 and 7BChat models of different LLM series in 6.2, i.e., Qwen2.5 (Yang et al., 2024), Gemma (Team et al., 2024b,c), and Mistral (Jiang et al., 2023). Table 11 lists the URL link of each model and tokenizer provided by Hugging Face Transformers (Wolf et al., 2020)."
        },
        {
            "title": "Size Type URL",
            "content": "LLaMA3 (Dubey et al., 2024) Qwen2.5 (Yang et al., 2024) Gemma (Team et al., 2024b,c) Mistral (Jiang et al., 2023) 8B 3B 1B 7B 7B 7B"
        },
        {
            "title": "Link\nLink\nLink",
            "content": "Table 11: The URL links of models and tokenizers. A.3 LLM Generation Details For each experimental setting, the model needs to perform reasoning generation and option selection sessions on every QA dataset. For each running session, all experiments are conducted on single NVIDIA V100 GPU with 32GB memory except the few-shot experiments in 6.4, which use single NVIDIA A100 GPU with 40GB memory since the input length is much longer considering the few-shot examples with rationales. To avoid out-of-memory issue, all the models are loaded in half-precision (float16) mode, and the generation batch size is 1. The input sequence is not truncated since we do not want to lose the context information or the answer trigger sentence ϕ, but we set the maximum number of newly generated tokens as 512 during reasoning generation. A.4 Reproducibility For the reproducibility of this work, we set the generation temperature as 0 by default and disable token sampling for deterministic generation. In addition, we pre-set the random seed for all random modules at the beginning of each experiment session. By an unofficial tradition6, we set 42 as the random seed and do not tune the value. To validate the reproducibility, we ran the main experiments twice and obtained the same results as shown in Table 2. Our source code is available on GitHub: https://github.com/YuweiYin/ARR A.5 Experimental Cost In the reasoning generation stage, the total computational cost is approximately 6,900 GPU hours on NVIDIA V100 clusters (about 288 days) and 1,300 hours on A100 clusters (about 54 days). We only use V100 clusters for option selection, and the overall running time is approximately 500 hours (about 21 days). In the few-shot experiments, the expense for GPT-4o API calls to generate CoT and ARR rationales for few-shot examples is below US$20."
        },
        {
            "title": "B More Cases",
            "content": "As discussed in 5.3, this section presents additional cases  (Table 1221)  from our main experiments ( 5, Table 2) to provide closer look at how the ARR method enhances reasoning and improves QA accuracy. We present one case for each QA dataset introduced in 4.1. 4Data source: https://huggingface.co/datasets 5Model source: https://huggingface.co/models 6 The answer to the ultimate question of life, the universe, and everything is forty-two. (Adams, 1979) 15 Task Input xi Passage: Feather Contour feathers are not uniformly distributed on the skin of the bird except in some groups such as the penguins, ratites and screamers. In most birds the feathers grow from specific tracts of skin called pterylae; between the pterylae there are regions which are free of feathers called apterylae (or apteria). Filoplumes and down may arise from the apterylae. The arrangement of these feather tracts, pterylosis or pterylography, varies across bird families and has been used in the past as means for determining the evolutionary relationships of bird families. Question: Do penguins have feathers arising from the epidermis? Baseline ϕi Answer: Reasoning ri No, penguins have feathers arising from specific tracts of skin called pterylae. The pterylae are the areas of skin where the feathers grow. The pterylae are not uniformly distributed on the skin of the penguin. The feathers grow from the pterylae, and the areas between the pterylae are free of feathers and are called apterylae. The arrangement of these feather tracts, pterylosis or pterylography, varies across bird families. In penguins, the feathers are uniformly distributed on the skin, but they arise from specific tracts of skin called pterylae. The feathers do not arise from the epidermis. The epidermis is the outermost layer of the skin, and in most birds, the feathers grow from the dermal layer of the skin, not the epidermis. CoT ϕi Answer: Lets think step by step. Reasoning ri Penguins are group of birds that have feathers uniformly distributed on their skin. The passage states that in most birds, feathers grow from specific tracts of skin called pterylae, and between these tracts, there are regions called apterylae that are free of feathers. However, it also mentions that in some groups, such as penguins, ratites, and screamers, feathers are not uniformly distributed on the skin. This suggests that penguins do not have feathers arising from the epidermis. Therefore, the answer is no. Penguins do not have feathers arising from the epidermis. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks about the distribution of feathers on penguins. The passage states that in most birds, feathers grow from specific tracts of skin called pterylae, but penguins are an exception. The passage does not mention the epidermis, but it does mention that in penguins, feathers are uniformly distributed on the skin. This suggests that feathers in penguins may arise from the epidermis, as there are no specific tracts of skin called pterylae. Therefore, the answer is yes. Table 12: Case study. An instance from the Boolq dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Task Input xi Passage: There are five stations set up from west to east along section of the high-speed railway along the river. It is known that Fuyi Station is east of Haoyun Station and West of Huyao Station, and is adjacent to Huyao Station; Jiushang Station is adjacent to Yinling Station. Question: If Haoyun stands to the east of Jiushang Station, you can get (A) Yinling Station is adjacent to Haoyun Station and west of Haoyun Station; (B) Haoyun Station is adjacent to Fuyi Station and west of Fuyi Station; (C) Jiushang Station is adjacent to Haoyun Station and west of Haoyun Station; (D) Yinling Station is adjacent to Fuyi Station and west of Fuyi Station; Baseline ϕi Answer: Reasoning ri (C) Jiushang Station is adjacent to Haoyun Station and west of Haoyun Station; (Explanation) Since Jiushang Station is adjacent to Yinling Station, and Yinling Station is adjacent to Haoyun Station, Jiushang Station is adjacent to Haoyun Station. Since Jiushang Station is east of Yinling Station, and Yinling Station is west of Haoyun Station, Jiushang Station is west of Haoyun Station. Therefore, Jiushang Station is adjacent to Haoyun Station and west of Haoyun Station. CoT ϕi Answer: Lets think step by step."
        },
        {
            "title": "Reasoning ri",
            "content": "If Haoyun stands to the east of Jiushang Station, then Haoyun Station is west of Jiushang Station. Since Jiushang Station is adjacent to Yinling Station, Haoyun Station is adjacent to Yinling Station. Since Fuyi Station is east of Haoyun Station, Fuyi Station is east of Yinling Station. Since Huyao Station is west of Fuyi Station, Huyao Station is west of Yinling Station. Since Jiushang Station is adjacent to Yinling Station, Huyao Station is adjacent to Jiushang Station . . . ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks us to determine the correct relationship between the stations based on the given information. We are told that Fuyi Station is east of Haoyun Station and west of Huyao Station, and is adjacent to Huyao Station. This means that Fuyi Station is between Haoyun Station and Huyao Station. Additionally, we know that Jiushang Station is adjacent to Yinling Station. The question also states that Haoyun stands to the east of Jiushang Station. This implies that Haoyun Station is to the east of Jiushang Station. Option (B) states that Haoyun Station is adjacent to Fuyi Station and west of Fuyi Station. This is consistent with the given information that Fuyi Station is east of Haoyun Station. Therefore, the correct answer is option (B) Table 13: Case study. An instance from the LogiQA dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Analysis: the correct station order can be Jiushang-Yinling-Haoyun-FuyiHuyao or Yinling-Jiushang-Haoyun-Fuyi-Huyao. Task Input xi Question: What do people aim to do at work? (A) complete job (B) learn from each other (C) kill animals (D) wear hats (E) talk to each other Baseline ϕi Answer: Reasoning ri (B) learn from each other. The best answer is B. Step 1: Understand the context of the question: The question is asking about the purpose or goal of people at work. Step 2: Analyze the options given Option (A) \"complete job\" is task-oriented answer, but it doesnt necessarily imply learning or personal growth. Option (C) \"kill animals\" is unrelated to the typical goals of work in most industries. Option (D) \"wear hats\" is superficial aspect of work attire and not goal. Option (E) \"talk to each other\" is common aspect of work but doesnt necessarily imply learning. Step 3: Identify the most appropriate answer: Option (B) \"learn from each other\" is goal that aligns with personal and professional development, which is common aim in many workplaces. It suggests focus on growth, improvement, and collaboration, which are key aspects of many work environments. The final answer is: CoT ϕi Answer: Lets think step by step. Reasoning ri (B) People go to work to earn living, but they also go to learn from each other and to be part of community. Choice (A) is incorrect because people do not go to work to complete job. They go to work to do job. Choice (C) is incorrect because people do not go to work to kill animals. Choice (D) is incorrect because people do not go to work to wear hats. Choice (E) is incorrect because people do not go to work to talk to each other. They go to work to do job. The best answer is B. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. The question asks what people aim to do at work. The relevant information is that people go to work to earn living. The answer is that people aim to complete their jobs at work. Choice (B) is incorrect because people may learn from each other at work, but that is not the main purpose of going to work. Choice (C) is incorrect because people do not go to work to kill animals. Choice (D) is incorrect because people do not go to work to wear hats. Choice (E) is incorrect because people do not go to work to talk to each other. The best answer is A. Reasoning ri Table 14: Case study. An instance from the Commonsense QA dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Task Input xi Context: Alex grew closer to their significant other after they vacationed together. Question: How would Alex feel afterwards? (A) in love (B) happy (C) back from vacation Baseline ϕi Answer: Reasoning ri (B) happy Reasoning Skill: Empathy And Perspective Taking This question requires the test-taker to put themselves in Alexs shoes and understand how they would feel after positive experience like vacationing with their significant other. The correct answer, \"happy\", is natural emotional response to pleasant experience. The other options are not directly related to the emotional state of Alex after the vacation. \"In love\" is more intense and long-term feeling, \"back from vacation\" is factual statement rather than an emotional response, and \"happy\" is the most appropriate answer given the context. CoT ϕi Answer: Lets think step by step. Reasoning ri Alex grew closer to their significant other after they vacationed together. This implies that the vacation strengthened their relationship. Therefore, Alex would likely feel happy after the vacation. Reasoning skill: Empathy And Perspective Taking. This question requires the reader to put themselves in Alexs shoes and understand how they would feel after positive experience. It also requires the reader to consider the context of the situation and how it would impact Alexs emotions. The best answer is (B) ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri Step 1: Identify the context of the question. The context is about Alex growing closer to their significant other after vacation. Step 2: Determine the emotional state of Alex after the vacation. The question asks how Alex would feel afterwards. Step 3: Analyze the relationship between Alex and their significant other. The question implies that the vacation strengthened their bond. Step 4: Determine the emotional state of Alex based on the context. Given that the vacation brought them closer together, it is likely that Alex would feel happy or in love. Step 5: Choose the most appropriate answer. Between \"happy\" and \"in love,\" \"in love\" is more intense emotion that aligns with the context of growing closer to someone. The final answer is: (A) in love. Table 15: Case study. An instance from the SocialIQA dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Analysis: Option is not wrong, but Option is more appropriate as they are in romantic relationship (significant other). 17 Task Input xi Question: Rupture can cause fragments of what to travel via the bloodstream and become lodged in other arteries? (A) red blood cells (B) white blood cells (C) plaque (D) enamel Baseline ϕi Answer: Reasoning ri Rupture of an atherosclerotic plaque can cause fragments of the plaque to travel via the bloodstream and become lodged in other arteries. This is known as an embolus. Choice (B) is incorrect because white blood cells are not typically associated with atherosclerosis. Choice (C) is incorrect because plaque is the substance that ruptures. Choice (D) is incorrect because enamel is the hard, outer layer of the teeth. The best answer is (A). CoT ϕi Answer: Lets think step by step. Reasoning ri The question is asking about the consequences of rupture, which implies physical injury or damage to structure. The correct answer should be related to the physical structure that can be damaged and cause fragments to travel through the bloodstream. Option (A) red blood cells are type of cell that carries oxygen throughout the body via the bloodstream, and they can be damaged or ruptured in the event of physical injury or damage to blood vessel. Option (B) white blood cells are part of the immune system and help fight infections, but they are not physical structure that can rupture and cause fragments to travel through the bloodstream. Option (C) plaque is sticky film of bacteria that can cause dental problems, but its not physical structure that can rupture and cause fragments to travel through the bloodstream. Option (D) enamel is hard, outer layer of teeth, which is not directly related to the bloodstream or arteries. Therefore, the correct answer is (A) red blood cells. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. The question is asking about the consequences of rupture, which is likely referring to rupture of an aneurysm or an atherosclerotic plaque. In the context of aneurysms or atherosclerosis, rupture can lead to the release of fragments into the bloodstream. These fragments can then travel to other parts of the body and become lodged in other arteries, potentially causing blockages or other complications. Considering the options provided, the most relevant choice is (C) plaque, as atherosclerotic plaque is common cause of arterial blockages and can rupture, releasing fragments into the bloodstream. The other options are not directly related to the consequences of rupture in the context of aneurysms or atherosclerosis. Reasoning ri Table 16: Case study. An instance from the SciQ dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Task Input xi Fact: force causes the speed of an object to decrease. Question: The amount of friction and the speed of an object have what kind of relationship? (A) inverse (B) reverse (C) direct (D) equal Baseline ϕi Answer: Reasoning ri (C) Direct Explanation: The more friction an object has, the slower it will move. The less friction an object has, the faster it will move. This is direct relationship. The amount of friction and the speed of an object are directly related. CoT ϕi Answer: Lets think step by step. Reasoning ri Step 1: Understand the relationship between friction and speed The relationship between friction and speed is that as friction increases, the speed of an object decreases. Step 2: Determine the type of relationship Since an increase in friction leads to decrease in speed, this is direct relationship. direct relationship means that as one variable increases, the other variable also increases. Step 3: Choose the correct answer Based on the understanding of the relationship between friction and speed, the correct answer is (C) direct. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks about the relationship between the amount of friction and the speed of an object. The relevant information is that force causes the speed of an object to decrease. The intent of the question is to determine the relationship between the amount of friction and the speed of an object. The correct answer is (A) inverse. The amount of friction and the speed of an object have an inverse relationship. The more friction, the slower the object moves. The less friction, the faster the object moves. Table 17: Case study. An instance from the OpenBookQA dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Analysis: The CoT method provides correct fact about the direct relationship (A direct relationship means that . . . ), but wrongly infers that this is direct relationship from an increase in friction leads to decrease in speed. This is because the wrong statement is made without conditioning on the correct fact that is presented after the statement (due to the autoregressive generation). In contrast, the ARR method performs reasoning based on sufficient context after analyzing the intent and finding relevant information. 18 Task Input xi Question: scientist maps long region in which earthquakes originate and determines this region is transform plate boundary. Which evidence would cause the scientist to reevaluate this determination? (A) Volcanism also characterizes the region. (B) Earthquake centers in the region occur at shallow depths. (C) The region shows extensive faulting of sediments. (D) Equal crust densities are found on opposite sides of the region. Baseline ϕi Answer: Reasoning ri The best answer is C. Transform plate boundaries are characterized by horizontal motion and faulting, but the faulting is not necessarily extensive. Volcanism is characteristic of divergent and convergent plate boundaries, not transform plate boundaries. Shallow earthquake foci are characteristic of transform plate boundaries. Equal crust densities on opposite sides of the region would indicate that the region is transform plate boundary. Therefore, (A), (B), and (D) are incorrect. (C) is correct because extensive faulting is characteristic of transform plate boundaries. CoT ϕi Answer: Lets think step by step. Reasoning ri Transform plate boundaries are characterized by horizontal motion of the Earths crust, which is evidenced by faulting. Volcanism is characteristic of divergent and convergent plate boundaries, not transform plate boundaries. Therefore, (A) is incorrect. Earthquakes at transform plate boundaries can occur at any depth, so (B) is incorrect. (C) is correct because faulting is characteristic of transform plate boundaries. (D) is incorrect because transform plate boundaries are characterized by horizontal motion of the Earths crust, not by equal crust densities. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks for evidence that would cause scientist to reevaluate the determination that region is transform plate boundary. transform plate boundary is region where two plates slide past one another horizontally. The evidence that would cause scientist to reevaluate this determination would be evidence that is characteristic of different type of plate boundary. divergent plate boundary is region where two plates move apart from one another, and convergent plate boundary is region where two plates move toward one another. Volcanism is characteristic of divergent plate boundary, so (A) is correct. Table 18: Case study. An instance from the ARC dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Task Input xi Question: The following paragraphs each describe set of three objects arranged in fixed order. The statements are logically consistent within each paragraph. fruit stand sells three fruits: peaches, mangoes, and apples. The peaches are more expensive than the apples. The mangoes are the cheapest. (A) The peaches are the second-most expensive (B) The mangoes are the second-most expensive (C) The apples are the second-most expensive Baseline ϕi Answer: Reasoning ri The peaches are the second-most expensive. The mangoes are the cheapest. The apples are the least expensive of the three fruits. The peaches are more expensive than the apples. The mangoes are the cheapest. The peaches are the second-most expensive. The mangoes are the cheapest. The apples are the least expensive of the three fruits. The peaches are more expensive than the apples. The mangoes are the cheapest . . . CoT ϕi Answer: Lets think step by step. Reasoning ri The peaches are more expensive than the apples. The mangoes are the cheapest. Therefore, the mangoes are the least expensive of the three fruits. The peaches are more expensive than the apples, so the peaches are the second-most expensive of the three fruits. Choice (B) is incorrect. The mangoes are the cheapest of the three fruits. Choice (C) is incorrect. The apples are less expensive than the peaches. The best answer is A. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks for the second-most expensive fruit. The peaches are more expensive than the apples, and the mangoes are the cheapest. Therefore, the second-most expensive fruit is the apples. Choice is incorrect. The peaches are more expensive than the apples, and the mangoes are the cheapest. Therefore, the peaches are the most expensive fruit, not the second-most expensive. Choice is incorrect. The mangoes are the cheapest fruit. Table 19: Case study. An instance from the BBH dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Analysis: The Baseline method rushes to conclusions and then tries to present an explanation. However, the false statement confuses the whole reasoning process and makes the model caught in an infinite self-doubting loop. 19 Task Input xi Question: Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: teaches y) (A) Tdc (B) Tcd (C) Tcc (D) dTc Baseline ϕi Answer: Reasoning ri (B) The translation of the sentence into predicate logic is Tcd, where Txy means teaches y. The subject of the sentence is David, the object is Chris, and the predicate is teaches. The correct translation is Tcd, not Tdc, because the subject comes before the predicate in predicate logic statement. Choice (C) is incorrect because it reverses the subject and object. Choice (D) is incorrect because it reverses the subject and predicate. CoT ϕi Answer: Lets think step by step. Reasoning ri (B) The predicate logic translation of the sentence is Tcd, where Txy means teaches y. The best answer is B. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Reasoning ri The question asks for the best translation into predicate logic. The relevant information is the statement \"David teaches Chris.\" The statement is in the active voice, so the subject is the teacher, and the object is the student. The predicate logic translation is Tdc, where Txy means teaches y, is Chris, and is David. Choice (B) is incorrect. This is the translation of \"Chris teaches David.\" Choice (C) is incorrect. This is the translation of \"Chris teaches Chris.\" Choice (D) is incorrect. This is the translation of \"David is taught by Chris.\" The best answer is A. Table 20: Case study. An instance from the MMLU dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Analysis: The Baseline and CoT methods jump to the wrong conclusions confidently, while the ARR method understands the question first and analyzes each option correctly. Task Input xi Question: Which type of research methods are designed to elicit responses to predetermined, standardized questions from many respondents? (A) Non-probability. (F) Experimental. (B) Cross-sectional. (H) Observational. (D) Ethnographic. (E) Longitudinal. (J) Quantitative. (G) Probability. (C) Qualitative. (I) Case Study. Baseline ϕi Answer: Reasoning ri The best answer is G. Probability sampling is type of research method that is designed to elicit responses to predetermined, standardized questions from many respondents. Probability sampling is type of sampling method that is used to select sample of participants from larger population. Probability sampling methods include simple random sampling, stratified sampling, and cluster sampling. CoT ϕi Answer: Lets think step by step. Reasoning ri (G) Probability sampling is method of selecting sample from population in which every member of the population has an equal chance of being selected. Probability sampling is used in research to ensure that the sample is representative of the population. ARR ϕi Answer: Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. The question asks about research methods that are designed to elicit responses to predetermined, standardized questions from many respondents. This is clue that the research method is quantitative. Quantitative research methods are designed to elicit responses to predetermined, standardized questions from many respondents. The correct answer is (J). The other choices are incorrect because they are not quantitative research methods. Reasoning ri Table 21: Case study. An instance from the MMLU-Pro dataset, where our ARR method correctly answers the question but the Baseline and CoT methods fail. Analysis: The ARR method grasps the questions intent and implications before answering."
        }
    ],
    "affiliations": [
        "University of British Columbia"
    ]
}