{
    "paper_title": "Stable Flow: Vital Layers for Training-Free Image Editing",
    "authors": [
        "Omri Avrahami",
        "Or Patashnik",
        "Ohad Fried",
        "Egor Nemchinov",
        "Kfir Aberman",
        "Dani Lischinski",
        "Daniel Cohen-Or"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have revolutionized the field of content synthesis and editing. Recent models have replaced the traditional UNet architecture with the Diffusion Transformer (DiT), and employed flow-matching for improved training and sampling. However, they exhibit limited generation diversity. In this work, we leverage this limitation to perform consistent image edits via selective injection of attention features. The main challenge is that, unlike the UNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it unclear in which layers to perform the injection. Therefore, we propose an automatic method to identify \"vital layers\" within DiT, crucial for image formation, and demonstrate how these layers facilitate a range of controlled stable edits, from non-rigid modifications to object addition, using the same mechanism. Next, to enable real-image editing, we introduce an improved image inversion method for flow models. Finally, we evaluate our approach through qualitative and quantitative comparisons, along with a user study, and demonstrate its effectiveness across multiple applications. The project page is available at https://omriavrahami.com/stable-flow"
        },
        {
            "title": "Start",
            "content": "Stable Flow: Vital Layers for Training-Free Image Editing Omri Avrahami1,2 Or Patashnik1,3 Ohad Fried4 Egor Nemchinov1 Kfir Aberman1 Dani Lischinski2 Daniel Cohen-Or1, 1Snap Research 2The Hebrew University of Jerusalem 3Tel Aviv University 4Reichman University 4 2 0 2 1 2 ] . [ 1 0 3 4 4 1 . 1 1 4 2 : r Figure 1. Stable Flow. Our training-free editing method is able to perform various types of image editing operations, including non-rigid editing, object addition, object removal, and global scene editing. These different edits are done using the same mechanism."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Diffusion models have revolutionized the field of content synthesis and editing. Recent models have replaced the traditional UNet architecture with the Diffusion Transformer (DiT), and employed flow-matching for improved training and sampling. However, they exhibit limited generation diversity. In this work, we leverage this limitation to perform consistent image edits via selective injection of attention features. The main challenge is that, unlike the UNetbased models, DiT lacks coarse-to-fine synthesis structure, making it unclear in which layers to perform the injection. Therefore, we propose an automatic method to identify vital layers within DiT, crucial for image formation, and demonstrate how these layers facilitate range of controlled stable edits, from non-rigid modifications to object addition, using the same mechanism. Next, to enable real-image editing, we introduce an improved image inversion method for flow models. Finally, we evaluate our approach through qualitative and quantitative comparisons, along with user study, and demonstrate its effectiveness across multiple applications. Over the recent years, we have witnessed an unprecedented explosion in creative applications of generative models, fueled by diffusion-based models [36, 7880]. Recent models, such as FLUX [46] and SD3 [25], have replaced the traditional UNet architecture [73] with the Diffusion Transformer (DiT) [63], and adopted flow matching [5, 49, 50] as superior alternative for training and sampling. These flow-based models are based on optimal transport conditional probability paths, resulting in faster training and sampling, compared to diffusion models. This is attributed [49] to the fact that they follow straight line trajectories, rather than curved paths. One of the known consequences of this difference, however, is that these models exhibit lower diversity than previous diffusion models [26], as shown in Figure 2(1-2). While reduced diversity is generally considered an undesirable characteristic, in this paper, we suggest leveraging it for the task of training-free image editing, as shown in Figure 2(3) and Figure 1. Specifically, we explore image editing via parallel generation [4, 18, 91], where features from the generative trajectory of the source (reference) image are injected into the trajectory of the edited image. Such an approach has been shown effective in the context of convolutional UNet-based Project page is available at: https://omriavrahami.com/stable-flow This research was performed while Omri was at Snap. 1 ] 6 6 [ S ) 1 ( ] 6 4 [ F ) 2 ( F a ) 3 ( photo of dog and cat... + dog wearing blue hat + cat wearing yellow glasses + casting shadows Figure 2. Leveraging Reduced Diversity. Using the same initial seed with different editing prompts, diffusion models such as (1) SDXL generate diverse results (different identities of the dog and the cat), while (2) FLUX generates more stable (less diverse) set of results out-of-the-box. However, there are still some unintended differences (the dog is standing in the leftmost column and sitting in the others, the color of the cat is changing, and the road is different on the right). Using our approach, (3) Stable Flow, the edits are stable, maintaining consistency of the unrelated content. diffusion models [18], where the roles of the different attention layers are well understood. However, such understanding has not yet emerged for DiT [63]. Specifically, DiT does not exhibit the same fine-coarse-fine structure of the UNet [63], hence it is not clear which layers should be tampered with to achieve the desired editing behavior. To address this gap, we analyze the importance of the different components in the DiT architecture, in order to determine the subset that should be injected while editing. More specifically, we introduce an automatic method for detecting set of vital layers layers that are essential for the image formation by measuring the deviation in image content resulting from bypassing each layer. We show that there is no simple relationship between the vitality of layer and its position in the architecture, i.e., the vital layers are spread across the transformer. close examination of the vital layers suggests that the injection of features into these layers strikes good balance in the multimodal attention between the reference image content and the editing prompt. Consequently, limiting the injection of features to only the vital layers tends to yield stable edit, i.e., an edit that changes only the part(s) specified by the text prompt, while leaving the rest of the image intact, as demonstrated in Figure 2(3). We demonstrate that performing the same feature injection, enables performing variety of image edits, including non-rigid editing, addition of objects, and scene changes, as demonstrated in Figure 1. 2 In order to support editing real images, it is typically necessary to invert them first [53, 79]. We employ an Inverse Euler Ordinary Differential Equation (ODE) solver to invert real images in the FLUX model [46]. However, this method fails to reconstruct the input image in satisfactory manner. To improve reconstruction accuracy, we introduce an out-of-distribution (OOD) nudging technique, in which we apply small scalar perturbation to the clean latent before inverting it. We demonstrate that this makes FLUX less prone to undesired changes in the image during the forward pass. Finally, we compare our method against its baselines qualitatively and quantitatively, and reaffirm the results with user study. We also demonstrate several applications of our method. In summary, our contributions are: (1) we propose an automatic method to detect the set of vital layers in DiT models and demonstrate how to use them for image editing; (2) we are the first method to harness the limited diversity of flow-based models to perform different image editing tasks using the same mechanism; and (3) we present an extension that allows editing real images using the FLUX model. 2. Related Work Text-Driven Image Editing. After the emergence of textto-image models, many works have suggested using them for various applications [7, 12, 20, 37, 75], including textdriven image editing tasks [39, 65]. SDEdit [52] addressed the image-to-image translation task adding noise and denoising with new prompt. Blended Diffusion [8, 9] suggested performing localized image editing [15, 38, 47, 55, 62] incorporating an input mask into the diffusion process in training-free manner, while GLIDE [54], ImagenEditor [89], and SmartBrush [93] offered fine-tuning the model on given input mask. Other works suggested inferring the mask from the input image [21, 88] or via an additional click input [69]. Other works [18, 34, 60, 84] offered to inject information from the input image using parallel generation. While some methods [13, 45, 85] suggested finetuning the model per-image, recent line of work suggested training designated model on large synthetic dataset for instruction-based edits [17, 76, 97]. However, none of the above methods is training-free method that supports nonrigid editing, object adding/replacement and scene editing altogether. Concurrency, Add-it [81] offers solution to the task of object addition using FLUX. Our method, on the other hand, supports other types of image editing (e.g., nonrigid editing, object replacement). In the realm of generative models, inImage Inversion. version [92] is the task of finding code within the latent space of generator [31, 43, 44] that faithfully reconstructs Initial methods were developed for GAN given image. Figure 3. Layer Removal. (Left) Text-to-image DiT models consist of consecutive layers connected through residual connections [33]. Each layer implements multimodal diffusion transformer block [25] that processes combined sequence of text and image embeddings. (Right) For each DiT layer, we performe an ablation by bypassing the layer using its residual connection. Then, we compare the generated result on the ablated model with the complete model using perceptual similarity metric. models [13, 14, 24, 59, 64, 70, 71, 83, 95, 99, 100], and more recently for diffusion-based models [16, 23, 29, 32, 40, 51, 53, 58, 79, 87]. In this work, we suggest inverting real image in flow models using latent nudging, as we found that the standard inverse ODE solver is insufficient. 3. Method Our goal is to edit images based on text prompts while faithfully preserving the unedited regions of the source image. Given an input image and an editing prompt p, we aim to generate modified image ˆx that exhibits the desired changes specified by while maintaining the original content elsewhere. We leverage the limited diversity of the FLUX model and further constrain it to enable such stable image edits through selective injection of attention features of the source image into the process that generates ˆx. Our approach is described in more detail below. In Section 3.1, we evaluate layer importance in the DiT model by analyzing the perceptual impact of layer removal (Figure 3). Next, in Section 3.2, we employ the most influential layers (termed vital layers) for image editing through attention injection. Finally, in Section 3.3, we extend our method to real image editing by inverting images into the latent space using the Euler inverse ODE solver, enhanced by latent nudging. 3.1. Measuring the Importance of DiT Layers Recent text-to-image diffusion models [36, 66, 68, 72, 74] predominantly use CNN-based UNets, which exhibit wellunderstood layer roles. In discriminative tasks, early layers detect simple features like edges, while deeper layers capture higher-level semantic concepts [77, 96]. Similarly, in generative models, early-middle layers determine shape and color, while deeper layers control finer details [43]. This structure has been successfully exploited in text-driven editing [18, 30, 84] through targeted manipulation of UNet decoder layers [73]. In contrast, state-of-the-art text-to-image DiT [63] models (FLUX [46] and SD3 [25]) employ fundamentally different architecture, as shown in Figure 3(left). These models consist of consecutive layers connected through residual connections [33], without convolutions. Each layer implements multimodal diffusion transformer block [25] (MMDiT-Block) that processes combined sequence of text and image embeddings. Unlike in UNets, the roles of the different layers are not yet intuitively clear, making it challenging to determine which layers are best suited for image editing. To quantify layer importance in the FLUX model, we devised systematic evaluation approach. Using ChatGPT [56], we automatically generated set of = 64 diverse text prompts, and draw set of random seeds. Each of these prompts was used to generate reference image, yielding in set Gref. For each DiT layer ℓ L, we performed an ablation by bypassing the layer using its residual connection, as illustrated in Figure 3(right). This process generated set of images Gℓ from the same prompts and seeds. See the supplementary material for more details. To assess the impact of each layer, we measured the perceptual similarity between Gref and Gℓ and using DINOv2 [57] (see Figure 3). The results, plotted in Figure 4, show that removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. We formally define the vitality of layer ℓ as: vitality(ℓ) = 1 1 (cid:88) sS,pP d(Mfull(s, p), M-ℓ(s, p)), (1) where Mfull represents the complete model, M-ℓ denotes the 3 0.95 0.9 0.85 0.8 r i i t r 0 5 10 15 20 25 30 35 40 45 50 55 Layer Index Figure 4. Layer Removal Quantitative Comparison. As explained in Section 3.1, we measured the effect of removing each layer of the model by calculating the perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images (Figure 5). As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. Note that the first vital layers were omitted for clarity (as their perceptual similarity approached zero). model with layer ℓ omitted, and d(, ) is the perceptual similarity metric. The set of vital layers is then defined as: = {ℓ vitality(ℓ) τvit} , (2) where τvit is the vitality threshold. Figure 5 illustrates the qualitative differences between vital and non-vital layers. While bypassing non-vital layers results in minor alterations, removing vital layers leads to significant changes: complete noise generation (G0), global structure and identity changes (G18), and alterations in texture and fine details (G56). 3.2. Image Editing using Vital Layers Given source image generated with known seed and prompt p, we aim to modify and generate an edited image ˆx that exhibits the desired changes, while otherwise preserving the source content. We adapt the self-attention injection mechanism, previously shown effective for image and video editing [18, 91] in UNet-based diffusion models, to the DiT-based FLUX architecture. Since each DiT layer processes sequence of image and text embeddings, we propose generating both and ˆx in parallel while selectively replacing the image embeddings of ˆx with those of x, but only within the vital layers set . Remarkably, as shown in Figure 1, this training-free approach successfully performs diverse editing tasks, including non-rigid deformations, object addition, object replacement, and global modifications, all using the same set of vital layers . G 0 5 8 1 2 5 6 5 Figure 5. Layer Removal Qualitative Comparison. As explained in Section 3.1, we illustrate the qualitative differences between vital and non-vital layers. While bypassing non-vital layers (G5 and G52) results in minor alterations, bypassing vital layers leads to significant changes: complete noise generation (G0), global structure and identity changes (G18), and alterations in texture and fine details (G56). To understand this effectiveness, we analyze the multimodal attention patterns in FLUX. Each visual token simultaneously attends to all visual and text tokens, with attention weights normalized across both modalities. Figure 6 contrasts the attention patterns in vital versus non-vital layers at two key points: yellow point in region that should remain unchanged (requiring copying from the reference image), and red point in an area targeted for editing (requiring generation based on the text prompt). In vital layers (left), points meant to remain unchanged show dominant attention to visual features, while points targeted for editing exhibit stronger attention to relevant text tokens (e.g., avocado). Conversely, non-vital layers (right) show predominantly image-based attention even in regions marked for editing. This suggests that injecting features into vital layers strikes good multimodal attention balance between preserving source content and incorporating text edits. 4 Figure 6. Multi-Modal Attention Distribution. Given an input image of man, we edit it to hold an avocado by injecting the reference image in the vital layers only (left) or in the non-vital layers (right), and visualize the multimodal attention of two points: yellow point in region that should remain unchanged (requiring copying from the reference image), and red point in an area targeted for editing (requiring generation based on the text prompt). As can be seen, in vital layers (left), points meant to remain unchanged show dominant attention to visual features, while points targeted for editing exhibit stronger attention to relevant text tokens (e.g., avocado). Conversely, non-vital layers (right) show predominantly image-based attention even in regions marked for editing. This suggests that injecting features into vital layers strikes good multimodal attention balance between preserving source content and incorporating text-guided modifications. g o / ) ( i n ) ( Input image Reconstruction Raising its hand Figure 7. Latent Nudging. As described in Section 3.3, when inverting real image, (a) simple inverse Euler ODE solver leads to corrupted image reconstructions and unintended modifications during editing. On the other hand, (b) using our latent nudging technique significantly reduces reconstruction errors and better constrains edits to the intended regions. 3.3. Latent Nudging for Real Image Editing Flow models generate samples by matching prior distribution p0 (Gaussian noise) to data distribution p1 (the image manifold). In the space Rd, we define two key components: probability density path pt : [0, 1] Rd R>0, which specifies time-dependent probability density functions ((cid:82) pt(x)dx = 1), and vector field ut [0, 1] Rd Rd. This vector field generates flow ϕ : [0, 1] Rd Rd through the ordinary differential equation (ODE): dt ϕt(x) = ut(ϕt(x)); ϕ0(x) = x. Transforming sample from p0 to sample in p1 is achieved using ODE solvers such as Euler. : 5 To edit real images, we must first invert them into the latent space, transforming samples from p1 to p0. We initially implemented an inverse Euler ODE solver for FLUX by reversing the vector field prediction. Given the forward Euler step: zt1 = zt + (σt+1 σt) ut(zt) (3) where zt represents the latent at timestep t, σt is the optimal transport standard deviation at time t, and ut is the learned vector field, we proposed the inverse step: zt = zt1 + (σt σt+1) ut(zt1) (4) assuming ut(zt) ut(zt1) for small steps. However, as Figure 7(a) demonstrates, this approach proves insufficient for FLUX, resulting in corrupted image reconstructions and unintended modifications during editing. We hypothesize that the assumption u(zt) u(zt1) does not hold, which causes the model to significantly alter the image during the forward process. To address this, we introduce latent nudging: multiplying the initial latent z0 by small scalar λ = 1.15 to slightly offset it from the training distribution. While this modification is visually imperceptible (Figure 7(b)), it significantly reduces reconstruction errors and constrains edits to the intended regions. See the supplementary material for more details 4. Experiments In Section 4.1 we compare our method against its baselines, both qualitatively and quantitatively. Next, in Section 4.2 we conduct user study and report results. Furthermore, in Section 4.3 we present the ablation study results. Finally, in Section 4.4 we demonstrate several applications. Table 1. Quantitative Comparison. We compare our method against the baselines in terms of text similarity (CLIPtxt), image similarity (CLIPimg) and image-text direction similarity (CLIPdir). As can be seen, P2P+NTI [34, 53], Instruct-P2P [17], and MasaCTRL [18] suffer from low similarity to the text prompt. SDEdit [94] and MagicBrush [97] adhere more to the text prompt, but they struggle with image similarity and image-text direction similarity. Our method, on the other hand, achieves better image and image-text direction similarity. Method CLIPtxt () CLIPimg () CLIPdir () SDEdit [94] P2P+NTI [34, 53] Instruct-P2P [17] MagicBrush [97] MasaCTRL [18] Stable Flow (ours) 0.24 0.21 0.22 0.24 0.20 0.23 0.71 0.76 0.87 0.88 0.76 0.92 0.07 0.08 0.07 0.11 0. 0.14 Table 2. Ablation Study. We conduct an ablation study and find that performing attention injection in all the layers or performing an attention extension in all the layers significantly reduces the text similarity. Furthermore, performing an attention injection in the non-vital layers or removing the latent nudging reduces the image similarity. Method CLIPtxt () CLIPimg () CLIPdir () Stable Flow (ours) Injection all layers Injection non-vital layers Extension all layers w/o latent nudging 0.23 0.17 0.25 0.18 0.22 0.92 0.98 0.72 0.98 0.62 0.14 0.00 0.09 0.01 0. Table 3. User Study. We compare our method against the baselines using the standard two-alternative forced-choice format. Users were asked to rate which editing result is better (Ours vs. the baseline) in terms of: (1) target prompt adherence, (2) input image preservation, (3) realism and (4) overall edit quality. We report the win rate of our method compared to each baseline. As shown, our method outperforms the baselines across all categories, achieving win rate higher than the random chance of 50%. Ours vs Prompt Adher. () Image Pres. () Realism () Overall () SDEdit [52] P2P+NTI [34, 53] Instruct-P2P [17] MagicBrush [97] MasaCTRL [18] 69.00% 76.00% 76.33% 61.33% 82.33% 68.00% 71.00% 75.66% 67.33% 80.00% 63.66% 72.66% 68.00% 76.66% 80.33% 70.66% 65.33% 60.33% 74.00% 72.00% 4.1. Qualitative and Quantitative Comparison the most relevant We compare our method against text-driven image editing methods. We re-implement SDEdit [52] using the FLUX.1-dev [46] model, and use the official public implementations of P2P+NTI [34, 53], Instruct-P2P [17], MagicBrush [97] and MasaCTRL [18]. See the supplementary material for more details. In Figure 8 we compare our method against the baseimages. As can be seen, lines qualitatively on real SDEdit [52] has difficulty maintaining object identities and backgrounds. P2P+NTI [34, 53] struggles with preserving object identities and with adding new objects. InstructP2P [17] and MagicBrush [97] face challenges with nonrigid editing. MasaCTRL [18] struggles with preserving object identities and adding new objects. Our method, on the other hand, is adhering to the editing prompt while preserving the identities. To quantify the performance of our method and the baselines, we prepared an evaluation dataset based on COCO [48], that in contrast to previous benchmarks [76, 97], also contains non-rigid editing tasks. We start by filtering the dataset automatically to contain at least one prominent non-rigid body. Next, for each image, we use various image editing tasks (non-rigid editing, object addition, object replacement and scene editing) that take into account the prominent object, resulting in total dataset of 3,200 samples. See the supplementary material for more details. We evaluated the editing results using three metrics: (1) CLIPimg that measures the similarity between the input image and the edited image, (2) CLIPtxt that measure the similarity between the edited image and the target editing prompt, and (3) CLIPdir [28, 61] that measures the similarity between the direction of the prompt change and the direction of the image change. As can be seen in Table 1, P2P+NTI [34, 53], InstructP2P [17], and MasaCTRL [18] suffer from low similarity to the text prompt. SDEdit [94] and MagicBrush [97] adhere more to the text prompt, but they struggle with image similarity and image-text direction similarity. Our method, on the other hand, is able to achieve better image and imagetext direction similarity. 4.2. User Study We conduct an extensive user study using the Amazon Mechanical Turk (AMT) [6] platform, with the automatically generated test examples from Section 4.1. We compare all the baselines against our method using standard twoalternative forced-choice format. Users were given the input image, the edit text prompt, and two editing results (one from our method and one from the baseline). For each comparison, the users were asked to rate which editing result is better in terms of: (1) target prompt adherence, (2) input image preservation, (3) realism and (4) overall edit quality (i.e., when taking all factors into account). As can be seen in Table 3, our method is preferred over the baselines in overall terms as well as the other terms. See the supplementary material for more details and statistical analysis. 4.3. Ablation Study We conduct an ablation study for the following cases: (1) Attention injection in all layers we perform the attention 6 Input SDEdit [52] P2P+NTI [34, 53] Instruct-P2P [17] MagicBrush [97] MasaCTRL [18] Stable Flow (ours) The cat is yelling and raising its paw rabbit toy sitting and wearing pink socks during the late afternoon rubber duck next to purple ball during sunny day dog with small collar lifting its paw while wearing red glasses bottle next to an apple. There is heart painting on the wall. doll with green body wearing hat man with long hair Figure 8. Qualitative Comparison. We compare our method on real images against the baselines. SDEdit [52] faces challenges with preserving object identities and backgrounds (e.g., rabbit and cat examples). P2P+NTI [34, 53] struggles with both preserving object identities (e.g., rabbit and lion dolls examples) and adding new objects (e.g., missing ball in the duck example and missing heart in the bottle example). Instruct-P2P [17] and MagicBrush [97] struggle with performing non-rigid editing (e.g., raising of the paws in dog and cat examples, and the sitting of the rabbit in its example). MasaCTRL [18] has difficulty with preserving object identities (e.g., cat, dog and lion doll examples) and adding new objects (e.g., missing ball in the duck example and missing socks in the rabbit example). Our method, on the other hand, is able to adhere to the editing prompt while preserving the identities. i E . ) 1 ( t . o ) 2 ( i E T ) 3 ( Input Holding hands Wearing glasses Next to an albino porcupine Input Statue of Liberty Taj Mahal Eiffel Tower t e S ) 1 ( . D j ) 2 ( l . ) 3 ( Input Animation Pencil sketch Oil painting Input On the right side of the frame On the left side of the frame On the bottom of the frame Input Man holding sign with the text diffusion in blue color Man holding sign with the uppercase text DIFFUSION Man holding sign with the text Flow Figure 9. Applications. Our method can be used for various applications: (1) Incremental Editing starting from scene of two kids, the user can refine the image iteratively by making the kid hold hands, then wear glasses and finally add porcupine next to them. (2) Consistent Style starting from scene with given style, such as an animation of the Great Pyramid of Giza, the user can generate images of different places with the same style. (3) Text Editing given scene that contains text, our method is able to perform text-related editing such as color change, case change and text replacement. injection that is described in Section 3.2 in all the layers (instead on the vital layers only). (2) Attention injection nonvital layers we performed the attention injection in some non-vital layers (same amount of layers as vital layers). (3) Attention extension instead of performing attention injection as described in Section 3.2, we extended [35, 82] the attention s.t. the generated images can attend to the reference image, as well as themselves. (4) w/o latent nudging we omitted the latent nudging component (Section 3.3). As can be seen in Table 2, we found that (1) performing attention injection in all the layers or performing (3) an attention extension in all the layers, significantly harms the text similarity. In addition, (2) performing an attention extension in the non-vital layers or (4) removing the latent nudging reduces the image similarity. 4.4. Applications As demonstrated in Figure 9, our method can be used for various applications: (1) Incremental Editing starting from given scene, the user can refine the image iteratively in step-by-step manner. (2) Consistent Style starting from scene with given style, the user can generate other images in the same style [27, 35, 41]. (3) Text Editing Input In the forest In the desert On the moon Figure 10. Limitations. Our method suffers from the following limitations: (1) Style Editing given photorealistic image of boy, our method struggles with changing its style to an animation (the identity of the boy also changes), to pencil sketch (changes only to black&white) or to an oil painting (mainly makes the image smoother). (2) Object Dragging given an image of cat, our method is unable to drag it into different locations in the image, but changes the gaze of the cat instead. (3) Background Replacement given an image of rat on the road, our method unable to replace its background entirely (the road leaks). given scene that contains text, our method is able to perform text-related editing such as color change, case change and text replacement. 5. Limitations and Conclusions As demonstrated in Figure 10, our method suffers from the following limitations: (1) Style Editing given an input image in one style (e.g., photorealistic), our method struggles with changing it to different style (e.g., oil painting), as it relies on attention injection (Section 3.2). (2) Object Dragging given an image with an object, our method is unable to drag it [11] into different locations in the image, as text-to-image models often struggle [10] with spa- (3) Background Replacement tial prompt adherence. given an input image, our method struggles with replacing its background entirely with no leakage [22]. In conclusion, we present Stable Flow, training-free method for image editing that enables various image editing tasks using the attention injection of the same vital layers group. We believe that our fully-automated approach of detecting vital layers may be also beneficial for other usecases, such as generative models pruning and distillation. We hope that our layer analysis will inspire more work in the field of generative models and expanding the possibilities for creative expression."
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 44324441, 2019. 3 [2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82968305, 2020. [3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Haim Bermano. Hyperstyle: Stylegan inversion with hypernetworks for real image editing. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1849018500, 2021. 3 [4] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. Cross-image attenIn International tion for zero-shot appearance transfer. Conference on Computer Graphics and Interactive Techniques, 2023. 1 [5] Michael Albergo and Eric Vanden-Eijnden. Building ArXiv, normalizing flows with stochastic interpolants. abs/2209.15571, 2022. 1 [6] Amazon. Amazon mechanical turk. https://www. mturk.com/, 2024. 6, 14 [7] Moab Arar, Andrey Voynov, Amir Hertz, Omri Avrahami, Shlomi Fruchter, Yael Pritch, Daniel Cohen-Or, and Ariel Shamir. Palp: Prompt aligned personalization of text-toimage models. 2024. 2 [8] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1820818218, 2022. 2 [9] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Trans. Graph., 42(4), 2023. 2 [10] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for the controllable image generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1837018380, 2023. In Proceedings of [11] Omri Avrahami, Rinon Gal, Gal Chechik, Ohad Fried, Dani Lischinski, Arash Vahdat, and Weili Nie. Diffuhaul: training-free method for object dragging in images. arXiv preprint arXiv:2406.01594, 2024. 8, 21, 26 [12] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. The chosen one: Consistent characters in textto-image diffusion models. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 2 [13] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. ArXiv, abs/2204.02491, 2022. 2 [14] David Bau, Hendrik Strobelt, William S. Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba. Se9 mantic photo manipulation with generative image prior. ACM Transactions on Graphics (TOG), 38:1 11, 2019. 3 [15] David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, and Antonio Torralba. Paint by word. ArXiv, abs/2103.10951, 2021. [16] Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 88618870, 2023. 3 [17] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2, 6, 7, 13, 14, 15, 16 [18] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. MasaCtrl: tuning-free mutual self-attention control for consistent image syntheIn Proceedings of the IEEE/CVF Intersis and editing. national Conference on Computer Vision (ICCV), pages 2256022570, 2023. 1, 2, 3, 4, 6, 7, 13, 14, 15, 16 [19] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 96309640, 2021. 13, 15, 21, 23 [20] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. ArXiv, abs/2407.08674, 2024. 2 [21] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations, 2022. 2 [22] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multisubject text-to-image generation. ArXiv, abs/2403.16990, 2024. [23] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models. ArXiv, abs/2408.00735, 2024. 3 [24] Tan M. Dinh, A. Tran, Rang Ho Man Nguyen, and BinhSon Hua. Hyperinverter: Improving stylegan inversion via hypernetwork. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11379 11388, 2021. 3 [25] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. ArXiv, abs/2403.03206, 2024. 1, 3, 13, 21, 36 [26] Johannes S. Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Boosting latent diffusion with flow matching. ArXiv, abs/2312.07360, 2023. 1 [27] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Implicit style-content separation using b-lora. Cohen-Or. ArXiv, abs/2403.14572, 2024. [28] Rinon Gal, Or Patashnik, Haggai Maron, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada. ACM Transactions on Graphics (TOG), 41:1 13, 2021. 6, 14 [29] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising. ArXiv, abs/2403.14602, 2024. 3 [30] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 3 [31] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2 [32] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Ding Liu, Qilong Zhangli, Anastasis Stathopoulos, Xiaoxiao He, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, Improving tuningand Dimitris N. Metaxas. Proxedit: 2024 free real image editing with proximal guidance. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 42794289, 2023. 3 [33] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2015. 3 [34] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2022. 2, 6, 7, 13, 14, 15, [35] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 47754785, 2023. 8 [36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising difIn Proc. NeurIPS, 2020. 1, fusion probabilistic models. 3 [37] Eliahu Horwitz, Jonathan Kahana, and Yedid Hoshen. Recovering the pre-fine-tuning weights of generative models. ArXiv, abs/2402.10208, 2024. 2 [38] Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee, and Changsheng Xu. Region-aware diffusion for zero-shot text-driven image editing. ArXiv, abs/2302.11797, 2023. 2 [39] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: survey. ArXiv, abs/2402.17525, 2024. 2 [40] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. arXiv e-prints, pages arXiv2304, 2023. 3 [41] Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping selfattention. ArXiv, abs/2402.12974, 2024. [42] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. ArXiv, abs/1603.08155, 2016. 15 [43] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 44014410, 2019. 2, 3 [44] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of the ing the image quality of stylegan. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81108119, 2020. 2 [45] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 2 [46] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 2, 3, 6, 13, 21 [47] Shanglin Li, Bo-Wen Zeng, Yutang Feng, Sicheng Gao, Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu, Jianzhuang Liu, and Baochang Zhang. Zone: Zero-shot instruction-guided local editing. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 62546263, 2023. [48] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, 2014. 6, 13, 15, 16, 17 [49] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. ArXiv, abs/2210.02747, 2022. 1 [50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. ArXiv, abs/2209.03003, 2022. 1 [51] Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik, Shai Avidan, and Rami Ben-Ari. inversion for text-to-image diffusion models. arXiv preprint arXiv:2312.12540, 2023. 3 Fixed-point [52] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021. 2, 6, 7, 13, 14, 15, 16 [53] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 2, 3, 6, 7, 13, 14, 15, [54] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, 2021. 2 10 [55] Yotam Nitzan, Zongze Wu, Richard Zhang, Eli Shechtman, Daniel Cohen-Or, Taesung Park, and Michael Gharbi. Lazy diffusion transformer for interactive image editing. ArXiv, abs/2404.12382, 2024. 2 [56] OpenAI. ChatGPT. https://chat.openai.com/, 2022. Accessed: 2024-10-1. 3, 13, 21 [57] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. ArXiv, abs/2304.07193, 2023. 3, 13, 15, [58] Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 15866 15875, 2023. 3 [59] Gaurav Parmar, Yijun Li, Jingwan Lu, Richard Zhang, JunYan Zhu, and Krishna Kumar Singh. Spatially-adaptive multilayer selection for gan inversion and editing. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1138911399, 2022. 3 [60] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot imageto-image translation. ACM SIGGRAPH 2023 Conference Proceedings, 2023. 2 [61] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel CohenOr, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 20652074, 2021. 6, 14 [62] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar AverbuchElor, and Daniel Cohen-Or. Localizing object-level shape 2023 variations with text-to-image diffusion models. IEEE/CVF International Conference on Computer Vision (ICCV), pages 2299423004, 2023. 2 [63] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 41724182, 2022. 1, 2, [64] Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1409214101, 2020. 3 [65] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nießner, Bjorn Ommer, Christian Theobalt, Peter Wonka, and Gordon Wetzstein. State of the art on diffusion models for visual computing. ArXiv, abs/2310.07204, 2023. 2 [66] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for highresolution image synthesis. ArXiv, abs/2307.01952, 2023. 2, 3 [67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 13, 15, 21, 23 [68] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey text-conditional arXiv preprint Chu, and Mark Chen. image generation with CLIP latents. arXiv:2204.06125, 2022. 3 Hierarchical [69] Omer Regev, Omri Avrahami, and Dani Lischinski. Click2mask: Local editing with dynamic mask generation. ArXiv, abs/2409.08272, 2024. 2 [70] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: stylegan encoder for image-to-image translation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22872296, 2020. 3 [71] Daniel Roich, Ron Mokady, Amit H. Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on Graphics (TOG), 42:1 13, 2021. 3 [72] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1067410685, 2021. 3 [73] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. ArXiv, abs/1505.04597, 2015. 1, 3 [74] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [75] Mohammad Salama, Jonathan Kahana, Eliahu Horwitz, and Yedid Hoshen. Dataset size recovery from lora weights. ArXiv, abs/2406.19395, 2024. 2 [76] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. ArXiv, abs/2311.10089, 2023. 2, 6 [77] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034, 2013. 3 [78] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International Connonequilibrium thermodynamics. ference on Machine Learning, pages 22562265. PMLR, 2015. 1 11 [91] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 75897599, 2022. 1, [92] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45:31213138, 2021. 2 [93] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2242822437, 2022. 2 [94] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion 2023 IEEE/CVF Conference on Computer Vimodels. sion and Pattern Recognition (CVPR), pages 1838118391, 2022. 6 [95] Zhen Yang, Dinggang Gui, Wen Wang, Hao Chen, Bohan Zhuang, and Chunhua Shen. Object-aware inversion and reassembly for image editing. ArXiv, abs/2310.12149, 2023. 3 [96] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. ArXiv, abs/1311.2901, 2013. 3 [97] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for In Advances in Neural instruction-guided image editing. Information Processing Systems, 2023. 2, 6, 7, 13, 14, 15, [98] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 586595, 2018. 13, 15, 22, 23 [99] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. Indomain gan inversion for real image editing. In European conference on computer vision, pages 592608. Springer, 2020. 3 [100] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Improved stylegan embedding: Where are the good latents? ArXiv, abs/2012.09036, 2020. 3 [79] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. 2, 3 [80] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. 1 [81] Yoad Tewel, Rinon Gal, Dvir Samuel Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models, 2024. 2 [82] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation. ArXiv, abs/2402.03286, 2024. [83] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG), 40:1 14, 2021. 3 [84] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. the image-to-image translation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 2, 3 In Proceedings of [85] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by fine tuning an image generation model on single image. arXiv preprint arXiv:2210.09477, 2022. 2 [86] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. 13, 21 [87] Bram Wallace, Akash Gokul, and Nikhil Vijay Naik. Edict: Exact diffusion inversion via coupled transformations. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2253222541, 2022. [88] Qian Wang, Biao Zhang, Michael Birsak, and Peter Improving automatic masks for Wonka. diffusion-based image editing with user instructions. ArXiv, abs/2305.18047, 2023. 2 Instructedit: [89] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1835918369, 2023. 2 [90] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, 2020. Association for Computational Linguistics. 13 12 Stable Flow: Vital Layers for Training-Free Image Editing Supplementary Material Acknowledgments. We thank Omer Dahary for his valuable help and feedback. This work was supported in part by the Israel Science Foundation (grants 1574/21 and 2203/24). A. Implementation Details In Appendix A.1, we start by providing implementation details for our method. Next, in Appendix A.2, we provide the implementation details for the baselines we compared our method against. Later, in Appendix A.3, we provide the implementation details for the automatic evaluations dataset and metrics. Finally, in Appendix A.4 we provide the full details of the user study we conducted. A.1. Method Implementation Details As described in Section 3.1 of the main paper, we started by collecting dataset of = 64 text prompts using ChatGPT [56]. We instructed it to generate text prompts describing diverse set of objects in different environments, with the focus on one main object. Then, we sampled seeds denoted by and used them to generate corresponding images Gref. Next, for each layer l, we bypass it by taking only the residual connection values. For each bypass, we generate images using the same seed set demoted by Gl. All the images were generated using Euler sampler in 15 steps and guidance scale of 3.5. Next, to evaluate the effect of each layer on the final result, we compared the generated images Gl with their corresponding images Gref using the DINOv2 [57] perceptual similarity metric. We term the layers that effect the generated image the most (i.e., the layers with layers, while the lowest perceptual similarity) as vital the rest of the layers as non-vital layers. We found that the vital layers in the FLUX.1-dev model [46] are [0, 1, 2, 17, 18, 25, 28, 53, 54, 56]. For visualization results, please refer to Appendix B.5. We empirically found that layer 2 can be removed from this set. In addition, the vital layers for the Stable Diffusion 3 (SD3) [25] model vital layers are: [0, 7, 8, 9]. For more details, please refer to Appendix B.6. A.2. Baselines Implementation Details As explained in Section 4.1 of the main paper, we compare our method against the following baselines: SDEdit [52], P2P+NTI [34, 53], Instruct-P2P [17], MagicBrush [97], and MasaCTRL [18]. We reimplement SDEdit using the FLUX.1-dev model [46], and use the official implementation for the rest of the baselines. We adapt the text prompts based on the baseline type: for SDEdit [52], P2P+NTI [34, 53], and MasaCTRL [18], we used the standard text prompt describing the desired edited scene (e.g., photo of man with red hat). For the instruction-based baselines Instruct-P2P [17] and MagicBrush [97] we adapted the style to fit an instructional format (e.g., Make the person wear red hat). We used the following third-party implementations in this project: FLUX.1-dev model [46] HuggingFace Diffusers [86] at https : / / github . com / implementation huggingface/diffusers P2P+NTI [34, 53] official implementation at https:// github.com/google/prompt-to-prompt Instruct-P2P [17] official implementation at https: / / github . com / timothybrooks / instruct - pix2pix MagicBrush [97] official implementation at https:// github.com/OSU-NLP-Group/MagicBrush MasaCTRL [18] official implementation at https:// github.com/TencentARC/MasaCtrl DINOv2 [57] ViT-g/14 implementation by HuggingFace Transformers [90] at https://github.com/ huggingface/transformers. DINOv1 [19] ViT-B/16 implementation by HuggingFace Transformers [90] at https://github.com/ huggingface/transformers. CLIP [67] ViT-L/14 implementation by HuggingFace Transformers [90] implementation at https:// github.com/huggingface/transformers official at https : / / github . com / richzhang / PerceptualSimilarity. implementation LPIPS [98] A.3. Automatic Metrics Implementation Details As explained in Section 4.1 of the main paper, we prepare an evaluation dataset based on the COCO [48] validation dataset. We begin by filtering the dataset automatically to include at least one prominent non-rigid body. More specifically, we filter only images containing humans or animals that at least one of them is prominent enough, but not too small, i.e., the prominent non-rigid body occupies at least 5% of the image but no more than 33%. Next, for each image, we apply various image editing tasks (non-rigid editing, object addition, object replacement, and scene editing) that take into account the prominent object from list of different combinations, resulting in total dataset of 3,200 samples. Examples of images from this dataset can be seen in Figure 13. 13 Figure 11. User Study Instructions. We provide the complete instructions for the user study we conducted using Amazon Mechanical Turk (AMT) [6] to compare our method with each baseline. We evaluate the editing results using three metrics: (1) CLIPimg which measures the similarity between the input image and the edited image by calculating the normalized cosine similarity of their CLIP image embeddings. (2) CLIPtxt which measures the similarity between the edited image and the target editing prompt by calculating the normalized cosine similarity between the CLIP image embedding and the target text CLIP embedding. (3) CLIPdir [28, 61] which measures the similarity between the direction of the prompt change and the direction of the image change. A.4. User Study Details As described in Section 4.2 of the main paper, we conducted an extensive user study using the Amazon Mechanical Turk (AMT) [6] platform, using automatically generated test exFigure 12. User Study Trial. We provide an example of trial task in the user study conducted using Amazon Mechanical Turk (AMT) [6]. Users were asked four questions of two-alternative forced-choice format. Complete instructions are shown in Figure 11. Table 4. User Study Statistical Significance. binomial statistical test of the user study results suggests that our results are statistically significant (p-value < 5%). Ours vs Prompt Adher. p-value SDEdit [52] P2P+NTI [34, 53] Instruct-P2P [17] MagicBrush [97] MasaCTRL [18] < 1e8 < 1e8 < 1e8 < 5e5 < 1e8 Image Pres. Realism Overall p-value p-value p-value < 1e8 < 1e8 < 1e8 < 1e8 < 1e8 < 1e6 < 1e8 < 1e8 < 6e8 < 1e8 < 2e4 < 1e8 < 1e8 < 1e8 < 1e amples, as explained in Appendix A.3. We compared all the baselines with our method using standard two-alternative forced-choice format. The users were given full instructions, as can be seen in Figure 11. Then, for each study trial, as shown in Figure 12, users were presented with an image and an instruction Given the following input image of {CATEGORY} where {CATEGORY} is the COCO category of the prominent object. The users were given two editing results one from our method and one from the baseline, and were asked the following questions: 14 1. Which of the results is better in adhering to the text prompt {PROMPT}?, where {PROMPT} is the editing target prompt. 2. Which of the results is better in preserving the information of the input image? 3. Which of the results looks more realistic? 4. Which of the results is better in overall? We collected five ratings per sample, resulting in 320 ratings per baseline, for total of 1,920 responses. The time allotted per task was one hour, to allow raters to properly evaluate the results without time pressure. binomial statistical test of the user study results, as presented in Table 4, suggests that our results are statistically significant (p-value < 5%). B. Additional Experiments In Appendix B.1, we start by providing additional comparisons and results of our method. Then, in Appendix B.2, we present experiments on using different perceptual metrics. Following that, in Appendix B.3, we test the effect of different sizes for vital layer set. Next, in Appendix B.4, we provide latent nudging experiments. Furthermore, in Appendix B.5 we present full visualization of our layer bypassing method. Finally, in Appendix B.6, we test our method on the Stable Diffusion 3 backbone. B.1. Additional Comparisons and Results In Figure 13 we provide an additional qualitative comparison of our method against the baselines on real images extracted from the COCO [48] dataset, as explained in Section 4.1 in the main paper. As can be seen, SDEdit [52] struggles with preserving the object identities and backgrounds (e.g., the bear and chicken examples). P2P+NTI [34, 53] struggles with preserving object identities (e.g., the bear and person examples) and with adding new objects (e.g., the missing hat in the sheep example and missing ball in the elephant example). Instruct-P2P [17] and MagicBrush [97] struggle with non-rigid editing (e.g., the person raising hand example). MasaCTRL [18] struggles with preserving object identities (e.g., the bear and person examples) and adding new objects (e.g., the sheep and cat examples). Our method, on the other hand, is able to adhere to the editing prompt while preserving the identities. Next, in Figure 14, we provide qualitative comparison of the ablated cases that are explained in, Section 4.3 in the main paper. As can be seen, we found that (1) performing attention injection in all the layers or performing (3) an attention extension in all the layers, encourages the model to directly copy the input image while neglecting the target prompt. In addition, (2) performing an attention extension in the non-vital layers or (4) removing the latent nudging reduces the input image similarity significantly. Finally, in Figures 15 and 16, we present additional image editing results using our method. B.2. Different Perceptual Metrics As explained in Section 3.1 of the main paper, we assess the impact of each layer by measuring the perceptual similarity between Gref and Gℓ using DINOv2 [57]. It raises the question of the importance of the specific perceptual [42] similarity metric when determining the vital layers. To this end, we also experiment with different perceptual metrics: DINOv1 [19], CLIP [67], and LPIPS [98]. In Figures 18, 19 and 20 we plot the perceptual similarity per layer for each of these metrics. The vital layers, ordered by vitality, as defined in Equation 1 of the main paper, for each metric are: DINOv2 [1, 0, 2, 18, 53, 28, 54, 17, 56, 25]. DINOv1 [1, 0, 2, 18, 53, 56, 54, 25, 28, 17]. CLIP [2, 0, 1, 18, 53, 56, 54, 4, 17, 3]. LPIPS [0, 1, 2, 18, 17, 56, 53, 54, 6, 4]. the vital set is equivalent for DIAs can be seen, NOv2 and DINOv1 (even though there is disagreement In addition, all the metrics include the on the order). set of {1, 0, 2, 18, 53, 54, 17, 56} to be included in the vital set, while DINOv1 and DINOv2 suggest also including {28, 25}, CLIP suggests including {3, 4} instead and LPIPS suggests including {6, 4} instead. In Figure 21 we edited images with these slightly different vital layer sets, and found the differences to be negligible in practice. B.3. Number of Vital Layers The somewhat agnostic nature of our method to the specific perceptual metric, as described in Appendix B.2, raises the question of the importance of the entire vital layer set to the editing task. To this end, in Figure 22 we experimented with omitting growing number of vital layers and testing the editing results. As can be seen, when removing 80% of the vital layer set, the changes are negligible. However, when removing more than that, the editing results include unintended changes, such as identity changes (e.g., man and woman examples) and background changes (e.g., cat and blackboard examples). This is consistent with the results from Appendix B.2 that show that the least vital layers for each perceptual metric are less important for the image editing task. B.4. Latent Nudging Experiments As described in Section 3.3 of the main paper, we proposed using latent nudging technique to avoid the bad reconstruction quality of vanilla inverse Euler ODE solver. We suggest multiplying the initial latent z0 by small scalar λ = 1.15 to slightly offset it from the training distribution. As shown in Figure 23, we empirically tested different values for the latent nudging hyperparameter λ. We performed 15 Input SDEdit [52] P2P+NTI [34, 53] Instruct-P2P [17] MagicBrush [97] MasaCTRL [18] Stable Flow (ours) photo of bear with long hair photo of man raising his hand photo of sheep with yellow hat photo of cat wearing purple sunglasses photo of chicken during sunset photo of an elephant next to blue ball Figure 13. Baselines Qualitative Comparison on Automatic Dataset. As explained in Section 4.1 of the main paper, we compare our method against the baselines on real images extracted from the COCO [48] dataset. We find that SDEdit [52] struggles with preserving the object identities and backgrounds (e.g., bear and chicken examples). P2P+NTI [34, 53] struggles with preserving object identities (e.g., bear and person examples) and with adding new objects (e.g., missing hat in the sheep example and missing ball in the elephant example). Instruct-P2P [17] and MagicBrush [97] struggle with non-rigid editing (e.g., person raising hand). MasaCTRL [18] struggles with preserving object identities (e.g., bear and person examples) and adding new objects (e.g., sheep and cat examples). Our method, on the other hand, is able to adhere to the editing prompt while preserving the identities. 16 Input (1) Inj. all layers (2) Inj. non-vital layers (3) Extension all layers (4) w/o latent nudging Stable Flow (ours) photo of bear with long hair photo of man raising his hand photo of sheep with yellow hat photo of cat wearing purple sunglasses photo of chicken during sunset photo of an elephant next to blue ball Figure 14. Ablations Qualitative Comparison on Automatic Dataset. As explained in Section 4.3 of the main paper, we compare our method against several ablation cases on real images extracted from the COCO [48] dataset. As can be seen, we found that (1) performing attention injection in all the layers or performing (3) an attention extension in all the layers encourages the model to directly copy the input image while neglecting the target prompt. In addition, (2) performing an attention extension in the non-vital layers or (4) removing the latent nudging reduces the input image similarity significantly. 17 Input Stable Flow neon sign P = NP neon sign neon sign of avocados Input wooden lion wooden toilet wooden noodles bowl Input hedgehog shark bird Input Jumping Sitting Putting its paw on stone Figure 15. Additional Results. We provide various editing results of our method. These different edits are done using the same vital layer set. 18 Input An albino porcupine horse crow Input Wearing red shirt Wearing purple jeans Wearing glasses Input The text FLUX is written on the bag camel in the background cat inside the bag Input pink car man driving the car In the evening Figure 16. Additional Results. We provide various editing results of our method. These different edits are done using the same vital layer set. 19 Input Minds Think Alike Figure 17. Additional Results. Given an input image that contain text, our method cat edit the text while keeping the background and style. 20 0.98 0.96 0.94 0. i i l p e L 0.95 0. 0.85 0.8 r m a e P 1 D 0 5 10 15 20 25 30 35 40 45 50 55 60 0 10 15 20 25 30 35 40 45 50 55 60 Layer Index Layer Index Figure 18. Layer Removal Quantitative Comparison Using CLIP. As explained in Appendix B.2, we measured the effect of removing each layer of the model by calculating the CLIP [67] perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images. As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. Note that the first vital layers were omitted for clarity (as their perceptual similarity approached zero). Figure 19. Layer Removal Quantitative Comparison Using DINOv1. As explained in Appendix B.2, we measured the effect of removing each layer of the model by calculating the DINOv1 [19] perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images. As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. Note that the first vital layers were omitted for clarity (as their perceptual similarity approached zero). inversion using the inverse Euler ODE solver with high number of 1,000 inversion (and denoising) steps, to reduce the inversion error. However, even when using such high number of inversion/denoising steps, we notice that when not using latent nudging (i.e., λ = 1.0), the reconstruction quality is poor (notice the eyes and the legs of the dog). Next, we found that λ = 1.15 is the smallest value that enables full reconstruction using the inverse Euler solver. Furthermore, nudging values that are too high (e.g., λ = 3.0) result in saturated images. Lastly, we notice that decreasing nudging values (i.e., λ < 1.0) severely damages the reconstruction quality. In addition, we experiment with simpler inversion variant based on latent caching (termed DDPM bucketing in DiffUHaul [11]), in which we saved the series of latents during the inversion process without applying latent nudging. As shown in Figure 24, this approach indeed achieves perfect inversion (second column), but (third column) still struggles with preserving the identities while editing the image (e.g., the rabbit and duck examples) or significantly alters the image (e.g., the cat and man examples). On the other hand, our method (fourth column) with the latent nudging is able to preserve the identities during editing. In practice, we found that using latent caching in addition to latent nudging enables inversion with lower number of steps (50 steps), hence, this is the approach we used. B.5. Layer Bypassing Visualization As explained in Section 3.1 of the main paper, to quantify layer importance in FLUX model, we devised systematic evaluation approach. Using ChatGPT [56], we automatically generated set of = 64 diverse text prompts, and draw set of random seeds. Each of these prompts was used to generate reference image, yielding set Gref. For each DiT layer ℓ L, we performed an ablation by bypassing the layer using its residual connection. This process generated set of images Gℓ from the same prompts and seeds. In Figures 2532, we provide full visualization of the reference set Gref along with the generation sets G0 G56. As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. B.6. Stable Diffusion 3 Results All the experiments in the main paper were based on the FLUX.1-dev [46] model. We also experimented with different DiT text-to-image flow model named Stable Diffusion 3 [25] based on the Diffusers [86] implementation of the medium model. As described in Section 3.1 of the main paper, we mea21 r m a e P ) P - 1 ( 1 1 1 1 1 0 5 10 15 20 25 30 35 40 45 50 55 60 Layer Index Figure 20. Layer Removal Quantitative Comparison Using LPIPS. As explained in Appendix B.2, we measured the effect of removing each layer of the model by calculating the (1 - LPIPS) [98] perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images. As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Importantly, influential layers are distributed across the transformer rather than concentrated in specific regions. Note that the first vital layers were omitted for clarity (as their perceptual similarity approached zero). sured the importance of each of the layers of this model. As shown in Figure 33, we measured the effect of removing each layer from the model by calculating the perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images. As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. Next, in Figure 34 we illustrate the qualitative differences between vital and non-vital layers. While bypassing non-vital layers (G1 and G21) results in modest alterations, bypassing vital layers leads to significant changes: complete noise generation (G0) or severe distortions (G7, G8, and G9). Finally, in Figure 35, we perform various editing operations using the same mechanism of injecting the reference image information into the vital layers of the model, as described in Section 3.2 of the main paper."
        },
        {
            "title": "Input",
            "content": "DINO [19, 57] CLIP [67] LPIPS [98] man holding cup of tea An avocado at the beach blackboard with the text Vital Layers Are All You Need Figure 21. Metrics Qualitative Comparison. As described in Appendix B.2, we also experimented with other perceptual metrics. We found DINOv2 [57] and DINOv1 [19] to produce the same set of vital layer. While CLIP [67] and LPIPS [98] replaced two layers in the vital layers set (though they include most of the vital layer set as in DINO). As can be seen, the differences between these sets are negligible when editing images. The floor is covered with snow"
        },
        {
            "title": "Input",
            "content": "100% 80% 60% 40% 20% man holding cup of tea An avocado at the beach blackboard with the text Vital Layers Are All You Need The floor is covered with snow Figure 22. Number of Vital Layers Comparison. As explained in Appendix B.3, we experimented with choosing different portion of the calculated vital layer set . As can be seen, when removing 80% of the vital layer set, the changes are negligible. However, when removing more than that, the editing results include unintended changes, such as identity changes (e.g., the man and woman examples) and background changes (e.g., the cat and blackboard examples). 24 λ = 1. λ = 1.1 λ = 1.15 λ = 3."
        },
        {
            "title": "Input",
            "content": "λ = 0.9 λ = 0.8 λ = 0.7 λ = 0.6 Figure 23. Latent Nudging Values. As described in Appendix B.4, we empirically tested different values for the latent nudging hyperparameter λ. In our experiments, we performed inversion using the inverse Euler ODE solver with high number of 1,000 inversion (and denoising) steps, to reduce the inversion error. However, even when using such high number of inversion/denoising steps, we notice that when not using latent nudging (i.e., λ = 1.0), the reconstruction quality is poor (notice the eyes and the legs of the dog). Next, we found that λ = 1.15 is the smallest value that enables full reconstruction using the inverse Euler solver. Furthermore, nudging values that are too high (e.g., λ = 3.0) result in saturated images. Lastly, we notice that reducing nudging values (λ < 1.0) severely damages the reconstruction quality."
        },
        {
            "title": "Latent Nudging",
            "content": "A rabbit toy sitting and wearing pink socks during the late afternoon The cat is yelling and raising its paw rubber duck next to purple ball during sunny day Figure 24. Latent Caching. As explained in Appendix B.4, we also tested latent caching approach [11], in which we saved the series of latents during the inversion process without applying latent nudging. As can be seen, this approach indeed achieves perfect inversion (second column), but (third column) still struggles with preserving the identities while editing the image (e.g., the rabbit and duck examples) or significantly alters the image (e.g., the cat and man examples). On the other hand, our method with the latent nudging (fourth column) is able to preserve the identities during editing. man with long hair"
        },
        {
            "title": "Gref",
            "content": "G0 G1 G2 G3 G4 G6 G7 Figure 25. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G0 G2 are vital layers, while G3 G7 are non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G8 G9 G10 G11 G12 G14 G15 Figure 26. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G8 G15 are all non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G16 G17 G18 G19 G20 G22 G23 Figure 27. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G17 and G18 are vital layers, while G16 and G19 G23 are non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G24 G25 G26 G27 G28 G30 G31 Figure 28. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G25 and G28 are vital layers, while G24, G26 G27 and G29 G31 are non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G32 G33 G34 G35 G36 G38 G39 Figure 29. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G31 G39 are non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G40 G41 G42 G43 G44 G46 G47 Figure 30. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G40 G47 are non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G48 G49 G50 G51 G52 G54 G55 Figure 31. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G53 G54 are vital layers, while G48 G52 and G55 are non-vital layers."
        },
        {
            "title": "Gref",
            "content": "G56 Figure 32. Full Layer Bypassing Visualization for Flux. We visualize the individual layer bypassing study we conducted, as described in Appendix B.5. We start by generating set of images Gref using fixed set of seeds and prompts. Then, we bypass each layer ℓ by using its residual connection and generate the set of images Gℓ using the same fixed set of prompts and seeds. In this visualization, G56 is vital layer. 34 G 0 1 7 8 9 1 2 i i S t r 0.8 0.6 0.4 0.2 0 5 10 15 20 Layer Index Figure 33. Layer Removal Quantitative Comparison Stable Diffusion 3. As explained in Appendix B.6, we measured the effect of removing each layer of the model by calculating the perceptual similarity between the generated images with and without this layer. Lower perceptual similarity indicates significant changes in the generated images. As can be seen, removing certain layers significantly affects the generated images, while others have minimal impact. For visual comparison, please refer to Figure 34. Figure 34. Layer Removal Qualitative Comparison Stable Diffusion 3. As explained in Appendix B.6, we illustrate the qualitative differences between vital and non-vital layers. While bypassing non-vital layers (G1 and G21) results in modest alterations, bypassing vital layers leads to significant changes: complete noise generation (G0), or severe distortions (G7, G8 and G9). For quantitative comparison, please refer to Figure 33 35 Input dog statue lemur pig gecko Input Angry Closing his eyes Wearing glasses An old man Input pink car blue car An orange car green car Input Closing its eyes Wearing red hat Wearing green glasses Next to purple stone Figure 35. Stable Diffusion 3 Editing Results. As explained in Appendix B.6, we tested our Stable Flow method on the Stable Diffusion 3 backbone [25]. As can be seen, we are able to perform various editing operations using the same mechanism of injecting the reference image information into the vital layers of the model."
        }
    ],
    "affiliations": [
        "Reichman University",
        "Snap Research",
        "Tel Aviv University",
        "The Hebrew University of Jerusalem"
    ]
}