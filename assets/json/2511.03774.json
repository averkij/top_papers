{
    "paper_title": "Contamination Detection for VLMs using Multi-Modal Semantic Perturbation",
    "authors": [
        "Jaden Park",
        "Mu Cai",
        "Feng Yao",
        "Jingbo Shang",
        "Soochahn Lee",
        "Yong Jae Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Vision-Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 4 7 7 3 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "CONTAMINATION DETECTION FOR VLMS USING MULTI-MODAL SEMANTIC PERTURBATIONS Jaden Park1, Mu Cai1, Feng Yao2, Jingbo Shang2, Soochahn Lee3, Yong Jae Lee1 3Kookmin University 1University of Wisconsin-Madison 2University of California, San Diego"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in VisionLanguage Models (VLMs) have achieved state-of-theart performance on numerous benchmark tasks. However, the use of internetscale, often proprietary, pretraining corpora raises critical concern for both practitioners and users: inflated performance due to test-set leakage. While prior works have proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for contaminated VLMs remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose novel simple yet effective detection method based on multi-modal semantic perturbation, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple realistic contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly. Figure 1: Example of our multi-modal semantic perturbation pipeline applied to RealWorldQA benchmark. Using ControlNet trained with Flux models, new speed limit sign is generated, changing the correct answer from (B) to (C) while preserving the original images overall composition. contaminated model that has memorized the original question is likely to fail on the perturbed version."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in Vision-Language Models (VLMs) have resulted in remarkable performance across wide range of tasks, including visual reasoning (Yue et al., 2024; Liu et al., 2024b; Chen et al., 2024a), real-world understanding (xAI, 2024), and complex mathematical problems (Zhang et al., 2024; Lu et al., 2024b). typical VLM training pipeline involves pretraining vision encoder and language backbone on internet-scale data, followed by fine-tuning stage on high-quality multimodal instruction-tuning datasets. However, as these training corpora are often proprietary with their exact"
        },
        {
            "title": "Requirements",
            "content": "N-gram Accuracy Shared Likelihood Guided Prompting Multi-modal Leakage CircularEval Choice Confusion (Xu et al., 2024) (Oren et al., 2023) (Golchin & Surdeanu, 2024) (Chen et al., 2024a) (Liu et al., 2024b) (Yao et al., 2024) BGR Shuffling Image Masking / Option Shuffling (Lu et al., 2024a) (Song et al., 2025) Multi-modal Semantic Perturbation Ours Reliability"
        },
        {
            "title": "Practicality Consistency",
            "content": "Table 1: Analysis of existing detection methods on VLMs. We label the detection method with if it satisfies all of Requirement 1, 2 or 3 and with otherwise. indicates that the requirement is partially observed but not consistently with varying contamination settings. Most existing detection methods fail to meet the requirements and cannot accurately classify contaminated models. Our method, however, satisfies all requirements. Results for N-gram Accuracy, Shared Likelihood and Guided Prompting are in the Appendix B. composition undisclosed, critical concern has emerged: public benchmark data may have leaked into the training set, leading to inflated and misleading performance metrics. Test-set leakage presents practical and significant challenge. For model users, it becomes difficult to disentangle genuine reasoning and generalization from mere memorization. For developers, exhaustively verifying the absence of test examples in massive pretraining corpora is prohibitively expensive (Bai et al., 2023). Although early work on large models proposed decontamination steps by removing n-gram overlaps (Brown et al., 2020; Abdin et al., 2024a), many recent models do not report such procedures, leaving the extent of contamination largely unexamined. To address this, several methods have been proposed to detect data contamination. One line of work focuses on verbatim memorization, testing whether model can reconstruct exact benchmark questions with high confidence (Xu et al., 2024; Golchin & Surdeanu, 2024; Oren et al., 2023). Another line of work examines generalization failures, measuring if model that succeeds on an original question fails on simpler variant, which is interpreted as an evidence of memorization (Mirzadeh et al., 2024; Yao et al., 2024; Huang et al., 2025). However, these detection methods were primarily designed for Large Language Models (LLMs) and often overlook the unique, multi-modal nature of VLMs. Applying simple text-based perturbations to VLM may not be sufficient, as the model could rely on visual features that remain unchanged. This discrepancy exposes critical gap and raises key question: Is there reliable, practical, and consistent method for detecting contamination in VLMs? In this study, we conduct systematic analysis of the data contamination problem, from which we derive grounded definitions of the core requirements reliability, practicality, and consistency. Guided by these definitions, we systematically contaminate open-source VLMs under varying fine-tuning epochs, data composition, and training strategies (e.g., standard fine-tuning vs. LoRA (Hu et al., 2021)). Our results show that existing detection methods struggle with the complexities of VLMs, frequently failing to meet the core requirements across diverse contamination scenarios (see Table 1). To overcome these limitations, we introduce novel multi-modal semantic perturbation pipeline. Our method generates new test examples by subtly altering the semantics of the image while preserving its overall composition, thereby creating variants of comparable or lower difficulty (Figure 1). The core principle is that contaminated models, which have merely memorized an imagetext pair, will fail to generalize to this perturbed input despite the similar or lower difficulty of the questions. In contrast, clean models with genuine reasoning capabilities should perform correctly or even better. This approach enables robust contamination detection without requiring any ground-truth knowledge of the leaked data."
        },
        {
            "title": "Preprint",
            "content": "Our contributions are threefold: 1. We propose novel and simple yet effective detection framework based on multi-modal semantic perturbations, which effectively identifies contaminated models by testing for generalization failures in the visual domain. 2. We validate our method across multiple contamination settings, proving that it is reliable, practical, and consistent, satisfying all key requirements for robust detection method. 3. We conduct the first systematic study of VLM behavior under diverse contamination and detection strategies, demonstrating that existing methods designed for LLMs are often unreliable for detecting contaminated VLMs."
        },
        {
            "title": "INVESTIGATION SETUP",
            "content": "This section establishes the formal framework for our analysis of data contamination. We begin by defining the degree of contamination, state our core assumption about its relationship with model generalization, and finally, outline three essential requirements for robust detection method. Our analysis is built upon formal definition of contamination at the data-point level. For given data point of dataset and training process consisting of epochs, we define: Definition 1. (Degree of Contamination). The degree of contamination for data point is: degD(x) = (cid:33) 1{x=d} n. (cid:32) (cid:88) dD This quantity reflects the total number of times is seen during training. In our experimental setup, where model is fine-tuned on the entire benchmark dataset for epochs, this simplifies to degD(M) = n. This definition motivates our central assumption, grounded in prior work on model memorization (Zhang et al., 2017; Carlini et al., 2021; Kandpal et al., 2023): Assumption 1. Data points with higher degree of contamination are more likely to be memorized, which increases overfitting risk and impairs generalization. In particular, we posit that as the degree of contamination grows, models will exhibit degraded performance on perturbed or out-of-distribution variants of benchmark items, even when the original questions are answered correctly. The effect need not scale linearly, since fine-tuning can disproportionately distort embedding spaces (Choi et al., 2025). Based on Assumption 1, we argue that any practical and effective detection method must satisfy three fundamental requirements: Requirement 1. (Practicality). The method must operate without the knowledge of the leaked data or the models training corpus, relying only on black-box interactions. Requirement 2. (Reliability). The method must detect contaminated models across heterogeneous fine-tuning strategies (e.g., standard fine-tuning vs. LoRA). Requirement 3. (Consistency). The methods detection signal should be positively correlated with degree of contamination (i.e., = degD(M)). Together, these requirements define principled framework for evaluating contamination detection. method that satisfies all of them can practically flag contaminated models, remain agnostic to training specifics, and provide signal proportional to the extent of contamination."
        },
        {
            "title": "3 PREPARATION OF CONTAMINATED MODELS",
            "content": "Models and Benchmarks. To analyze contamination detection across diverse settings, we pair complementary model families and benchmarks. On the model side, we use LLaVA-v1.5-7B (Liu"
        },
        {
            "title": "Preprint",
            "content": "et al., 2024a) LLaMAadapter design and Qwen2-VL-7B (Wang et al., 2024), which integrates tighter multimodal alignment. Both are widely adopted open-source VLMs with publicly documented training details (e.g., released corpora or decontamination policies, although we do not rely on this for detection), making them suitable for controlled contamination experiments. Spanning these distinct training paradigms lets us probe robustness under heterogeneous contamination footprints. For benchmarks, we use MMStar (Chen et al., 2024a) and RealWorldQA (xAI, 2024), which consist of questions that strictly require visual and textual evidence. MMStar aggregates and filters prior multi-modal benchmarks by removing questions that are leaked or do not have visual dependence, while RealWorldQA enforces visual dependence by design. This prevents linguistic shortcuts and ensures our perturbation-based detector evaluates genuine multi-modal reasoning. Training Strategies. We contaminate models by training directly on evaluation data. Because VLM training typically has two stages(i) large-scale pretraining of the vision encoder and language backbone and (ii) instruction-tuned multimodal fine-tuningleakage can arise at either stage. Ablating pretraining is computationally prohibitive, so we focus on continual fine-tuning, which affords precise control over contamination levels. We compare standard fine-tuning and LoRA (Hu et al., 2021). For standard fine-tuning, we follow three common variants: fine-tune the LLM and adapter as in LLaVA-v1.5-7B; fine-tune only the LLM as in Qwen2-VL-7B; or unfreeze all parameters as in InternVL (Chen et al., 2024b). This diversity tests robustness across parameter-efficient and full fine-tuning regimes. Epochs. We save checkpoints at epochs 13 (i.e. degD(M) {1, 2, 3}), since VLMs are typically fine-tuned for at most three epochs and often just one. Contamination footprints of varying strength enables graded analysis of detection sensitivity (Liu et al., 2024a; Chen et al., 2025). Hyperparameters. We use model defaults, adjusting only the learning rate to ensure inflated performance. For LLaVA-v1.5-7B, we follow the official repo; for Qwen2-VL-7B, we use LLaMAFactory (Zheng et al., 2024) to train the models. (Full settings are in the Appendix A.1.)"
        },
        {
            "title": "4 MULTI-MODAL SEMANTIC PERTURBATION",
            "content": "We propose multi-modal semantic perturbation framework for contamination detection, that generates variants of imagetext pairs with similar or lower difficulty while keeping the original image composition intact. Our method consistently detects contaminated models and satisfies all three requirements, while existing methods fail to yield stable signal of test-set leakage in VLMs. To generate semantically perturbed questions, we combine an LLM with diffusion-based generative model. In our main experiments, we use GPT-4o (OpenAI, 2024) and Flux (Labs, 2024) + ControlNet (Zhang et al., 2023), but later show that our framework is model-agnostic (Section 7). The pipeline of our framework is as follows. First, we randomly change the answer of the original question to different option, preventing contaminated models from getting away with memorized responses. Next, GPT-4o generates dense caption of the image, conditioned both on the original question and the newly chosen answer choice. We find this explicit conditioning essential: it ensures the caption highlights the salient visual features that drive the perturbation process, allowing the generation of questions of similar or lower difficulty (Section 6). Flux ControlNet then uses this caption, together with Canny edge maps (Canny, 1986), to guide the diffusion process. ControlNet preserves the global structure of the image while introducing new elements that minimally alter semantics, yielding an updated image with the randomly sampled different correct answer (Figure 2). We note that, due to limitations in rendering text or complex geometries, especially at low resolution, some generated images do not fully correspond to the new answer. To mitigate this, we filter generated pairs using single criterion: perturbed questions must be answerable unambiguously.1 To detect contamination, i.e., memorization of an image-text pair, we compare model performance on the original vs. perturbed input. If model fails to generalize (i.e., it gets the original input correct but the perturbed one incorrect), we flag contamination. clean model with genuine reasoning capabilities should perform correctly in both settings, given that they have comparable difficulty. 1To further clarify, generation quality itself is not considered in filtering. This ensures that the evaluation set focuses solely on reasoning rather than visual fidelity. While our main results report human-filtered outcomes, we show in Table 7 that this step can be automated. We stress that filtering is necessitated by current generative model limitations, not by our detection principle."
        },
        {
            "title": "Preprint",
            "content": "Critically, our approach enables robust contamination detection without requiring any ground-truth knowledge of the leaked data. Figure 2: Illustration of our multi-modal semantic perturbation pipeline. The original questionimage pair is used to generate dense caption with an LLM, which guides Flux ControlNet to produce perturbed image and new answer, yielding modified but semantically consistent benchmark sample."
        },
        {
            "title": "5 COMPARATIVE EVALUATION OF CONTAMINATION DETECTION",
            "content": "In this section, we present our main results. After perturbation and filtering, 440 imagequestion pairs remain from the original 765 in RealWorldQA, and 478 remain from 1,500 in MMStar (Section 6 shows that these subsets remain representative of the full datasets). Table 2 reports results for MMStar; results for RealWorldQA are in in the Appendix  (Table 11)  . First, we see that clean models perform better than contaminated models, confirming that the perturbed questions are indeed of equal or lower difficulty. In contrast, all contaminated models perform worse, enabling our method to reliably detect via simple check for performance drops. We compare to the following contamination detection methods: Multi-modal Leakage (Chen et al., 2024a). With multi-modal leakage, contamination is flagged by measuring text-only performance on benchmarks that require visual input. After fine-tuning, any boost in text-only performance indicates memorization of leaked questionanswer pairs rather than genuine multi-modal reasoning. Multi-modal leakage requires clean models by design and hence does not satisfy Practicality (Req. 1). And from Table 2 we observe that Reliability (Req. 2) is not satisfied since it fails to detect LLaVA-v1.5-7B trained for 3 epochs with standard fine-tuning. While its gain in performance positively correlates with the degree of contamination in other cases, this trend breaks across benchmarks and training strategies such as for Qwen2-VL-7B trained for 3 epochs with LoRA and LLaVA-v1.5-7B trained for 3 epochs with standard fine-tuning, thus only partially satisfying Consistency (Req. 3). CircularEval (Liu et al., 2024b). Selection bias in multiple-choice questions is mitigated by rotating answer options times and requiring the model to be correct on all rotations. This stricter criterion lowers absolute accuracy compared to standard evaluation. For contamination detection, however, CircularEvals effectiveness is limited. Practicality (Req. 1) fails, as CircularEval lacks clear threshold-independent detection mechanism. Reliability (Req. 2) is undermined by inconsistent contamination signals, as it fails to detect LLaVA-v1.5-7B trained with LoRA for 2 and 3 epochs, and Qwen2-VL-7B trained with standard fine-tuning for 2 epochs. And despite some positive trends, Consistency (Req. 3) breaks across models and training setups, as shown with LLaVA-v1.5-7B trained with LoRA for 2, 3 epochs and Qwen2-VL-7B trained with standard fine-tuning for 2,3 epochs, limiting its diagnostic value. Choice confusion (Yao et al., 2024). Generalization is tested by constructing an easier benchmark variant: false options are replaced with correct answers drawn from unrelated questions. Clean models should leverage this simplification to improve, whereas contaminated modelsbound by memorized original answersmight not. We apply this method to measure the models performance drop () between the original and generalized versions. Clean models gain substantially on generalized benchmarks up to +34.04 on MMStar and +21.30 on RQA confirming their generalization ability. By contrast, contaminated variants show much smaller gains or even losses. This failure to benefit from semantically irrelevant but easier choices reflects classic memorization-based contamination. As detection method, choice confusion meets Practicality (Req. 1) since the method detects models"
        },
        {
            "title": "Preprint",
            "content": "Method Metric LLaVA-v1.5-7B (clean) LoRA (contaminated) LLM+MLP (contaminated) Require clean model? Ours CircularEval Choice Confusion Multi-modal Leakage MMStar MMStar_P Success? MMStar MMStar_C Success? MMStar MMStar_G Success? MMStar_to Success? 37.78 69.29 +31.51 37.78 26.06 -11.72 37.78 71.92 +34. 19.39 Epoch 1 Epoch 2 Epoch 3 Epoch 1 Epoch 2 Epoch 3 (Practicality (Req. 1)) 52.53 44.24 8. 52.53 29.09 -23.44 52.53 53.54 +1.01 26.67 +7. 50.71 37.58 13.13 50.71 45.66 -5.05 50.71 65.66 +14. 29.70 +10.31 54.34 38.18 16.16 54.34 55.56 +1. 54.34 69.09 +14.75 30.51 +11.12 41.82 33.33 8. 41.82 25.86 -15.96 41.82 62.83 +21.01 19.80 +0. 48.89 37.37 11.52 48.89 22.83 -26.06 48.89 66.26 +17. 26.87 +7.48 50.71 36.97 13.74 50.71 22.02 -28. 50.71 62.83 +12.12 8.69 -10.70 No Yes No Yes Method Metric Qwen2-VL-7B (clean) LoRA (contaminated) LLM only (contaminated) Require clean model? Ours CircularEval Choice Confusion Multi-modal Leakage MMStar MMStar_P Success? MMStar MMStar_C Success? MMStar MMStar_G Success? MMStar_to Success? 62.02 78.18 +16.16 62.02 55.96 -6.06 62.02 94.34 +32.32 22.83 Epoch 1 Epoch 2 Epoch 3 Epoch 1 Epoch 2 Epoch 3 (Practicality (Req. 1)) 78.38 71.31 7.07 78.38 54.95 -23.43 78.38 94.34 +15.96 27.07 +4.24 94.14 65.25 28.89 94.14 55.56 -38.58 94.14 93.13 -1.01 28.48 +5.65 95.96 63.64 32.32 95.96 55.76 -40.20 95.96 93.13 -2.83 28.08 +5.25 89.90 60.40 29.50 89.90 82.63 -7.27 89.90 96.77 +6.87 97.98 54.95 43.03 97.98 91.92 -6.06 97.98 96.77 -1.21 98.99 55.96 43.03 98.99 92.73 -6.26 98.99 97.78 -1.21 44.24 +21.41 51.31 +28.48 52.73 +29.90 No Yes No Yes Table 2: Performance of LLaVA-v1.5-7B (top) and Qwen2-VL-7B (bottom) on the MMStar dataset (Corresponding RealWorldQA results are in the Appendix 11). We compare to Multi-modal Leakage Chen et al. (2024a), CircularEval (Liu et al., 2024b), and Choice Confusion Yao et al. (2024). Clean models perform better on our perturbed dataset confirming that the perturbed questions are indeed of equal or lower difficulty. In contrast, all contaminated models perform worse, enabling reliable detection by our method via simple check for performance drops. \"_P\" denotes the semantically perturbed version; \"to\" denotes text-only performance; \"_C\" denotes evaluation using circular options; \"_G\" denotes evaluation using choice confusion; denotes the difference in performance, with positive values indicating gains. \"Success?\" indicates whether the method detected contamination. \"Require clean model?\" indicates whether the method requires access to clean model as baseline. If clean model is required as reference, the method cannot be used to detect the reference models themselves, so the entry is marked as . Full results of multi-modal semantic perturbation is listed in Appendix E.1. that perform worse on the generalized benchmark, but its sensitivity varies across fine-tuning regimes. Choice confusion fails to detect LLaVA-v1.5-7B regardless of its training strategy or the number of epochs, and for LLaVA-v1.5-7B trained with LoRA, the model shows improved performance as the number of training epochs increases. Hence the method fails to satisfy Reliability (Req. 2) while Consistency (Req. 3) is partially observed in other training strategies. Importantly, unlike other approaches, our method requires no dataset-specific thresholds or prior knowledge of leaked data, satisfying Practicality (Req. 1). Moreover, the performance drop scales with contamination degree, satisfying Consistency (Req. 3), and persists across all training regimesstandard fine-tuning, LoRA, and full parameter unfreezing, satisfying Reliability (Req. 2)."
        },
        {
            "title": "6 ANALYSIS OF MULTI-MODAL SEMANTIC PERTURBATION",
            "content": "Filtered images form representative subsample. Since manual filtering removes substantial portion of the original data, natural concern is whether the filtered sets remain representative. To"
        },
        {
            "title": "Preprint",
            "content": "test this, we first compare model performance on the full and filtered datasets and find that the results closely align  (Table 3)  . This confirms that the filtering step does not introduce systematic bias, and the remaining subsets preserve the distributional properties of the original benchmarks."
        },
        {
            "title": "Model",
            "content": "RQA (765 imgs) RQA_filtered (440 imgs) MMStar (1500 imgs) MMStar_filtered (495 imgs) LLaVA-v1.5-7B Qwen2-VL-7B 49.01% 70.33% 52.05% 70.45% 32.87% 59.80% 37.78% 61.62% Table 3: Performance of clean models on RealWorldQA (RQA) and MMStar before and after filtering. Why perturbations yield generalized benchmark. Our core assumption is that by preserving the original question and only altering the answer choice, the question difficulty remains comparable. In addition, we often observe that the perturbation highlights salient visual cues more clearly than the original images  (Fig. 3)  , collectively yielding an alternate benchmark that is similar or easier. We validate this empirically: clean models consistently achieve higher accuracy on perturbed benchmarks  (Table 2)  . Question What does the text on the traffic sign say? A. Student B. Children C. Police (a) Original (b) After multi-modal semantic perturbation Figure 3: Example where the perturbed variant is easier to solve than the original. In the original image, the traffic sign is small and the text barely legible; after perturbation, the sign is enlarged and clearly visible. Providing the question-answer pair in caption generation is critical. As described in Sec. 4, conditioning the caption generation on both the original question and the new answer is critical to creating generalized benchmark. One natural approach is to simply create variant of the original image without changing the answer to the question. To test this, we create dense caption of the original image without providing any context and reconstruct the images. We empirically observe that generative models often fail to extract relevant information from the dense details of the original image, leading to generations that are equivalent in difficulty or inconsistent. Changing the answer is necessary to delineate contaminated models behavior from simply memorizing the answer. Failure modes of multi-modal semantic perturbation. There are cases where multi-modal perturbation fails to reveal contamination. The perturbed image may differ in its visual details that it no longer closely resembles the original. In such cases, contaminated models may answer both the original and perturbed questions correctly as shown in Figure 4, hiding the models contamination. Question Which vehicle is closer to us, the school bus or the black SUV? A. School bus B. Black SUV C. They are at the same distance. (a) Original (b) After multi-modal semantic perturbation Figure 4: Example where contaminated model answers both the original and perturbed questions correctly. This may occur when visual details change significantly that the perturbed image no longer closely resembles the original."
        },
        {
            "title": "Preprint",
            "content": "Limitations of multi-modal semantic perturbation framework. For semantic perturbation to be valid, questions must enforce visual dependence. If question can be answered without visual input, perturbing the image is meaningless, and altering only the answer invalidates the task. As such, we restricted our study to RealWorldQA and MMStar, which are VQA benchmarks with strict visual grounding. This constraint highlights an important boundary: our method is only effective when visual semantics directly determines the correct answer. Both RealWorldQA and MMStar are multiple-choice benchmarks. Although the multiple-choice format simplifies evaluation, our framework is not inherently tied to this setting. Once visual evidence is perturbed, evaluation can be adapted to free-form tasks using string matching, likelihood-based scoring, or LLM-as-a-judge approaches. This principle extends naturally beyond multi-choice VQA. Finally, manual filtering is required only because of current limitations of diffusion models. As generative models improve, we expect manual filtering to become unnecessary, further strengthening the scalability of our approach."
        },
        {
            "title": "7 ABLATION STUDIES",
            "content": "We next validate that the core ideatesting for memorization via mutli-modal semantic perturbationsis robust to design choices. Specifically, we show our method is not tied to synthetic edits, scales to larger models and alternate contamination regimes, and works without GPT-4o or manual filtering. Real-world Counterfactuals: NaturalBench. NaturalBench (Li et al., 2024) provides real counterfactual pairs photos of the same scene under altered conditions serving as natural analogue to our synthetic variants. We fine-tune on one variant and evaluate its paired counterfactual with the same question. Contaminated models drop sharply (up to 45.95%), while clean models remain comparatively stable  (Table 4)  , showing our detector generalizes beyond synthetic perturbations2. Thus, any reliable semantic variation natural, procedural, or synthetic fits our framework. Full results are listed in E.2. Model Scale: LLaVA-13B. To assess scalability, we repeat the RealWorldQA experiment with LLaVA-v1.5-13B. Table 5 shows our detector remains effective at this scale. Given that larger models are more prone to memorization, these findings indicate the approach is suitable for stronger VLMs. Full results are listed in E.2."
        },
        {
            "title": "Method",
            "content": "Epoch Train Set (%) Test Set (%)"
        },
        {
            "title": "Method",
            "content": "Epoch RQA RQA_P"
        },
        {
            "title": "Contamination with RealWorldQA",
            "content": "LLaVA-v1.5-7B (clean) LoRA (contaminated) LLM+MLP (contaminated) 1 2 3 1 2 65.63 81.53 89.79 91.11 79.95 97.05 98.63 65.89 61.37 57.16 57.32 58.79 54.32 53. +0.26 20.16 32.63 33.79 21.16 42.74 45.58 LLaVA-v1.5-13B (clean) LoRA (contaminated) LLM+MLP (contaminated) 1 2 3 1 2 3 51.14 74.32 73.18 77.05 56.59 71.59 75. 57.27 38.18 32.73 34.77 37.50 38.86 37.27 +6.13 36.14 40.45 42.28 19.09 32.73 38. Table 4: Performance of clean and contaminated models on NaturalBench. While clean model shows comparable performance on the test set, contaminated models fail to generalize, demonstrating dramatic performance drop upto -45.58% Table 5: Performance of clean and contaminated LLaVA-v1.5-13B on RealWorldQA after multi-modal semantic perturbation. The result satisfies all three requirements. Test-set leakage during Pretraining. We simulate pretraining leakage by mixing RealWorldQA into the 665K instruction-following pretraining corpus and training LLaVA-v1.5-7B for one epoch  (Table 6)  . Using the same 440 filtered images, our method flags contamination, demonstrating applicability beyond fine-tuning-only settings. Finally, we ablate two key components of our pipeline to show modularity. Automation of filtering Process. We replace manual validation with the o3 model to filter generated imagequestion pairs. The automatic pass removes 471 items and retains 294, of which 253 overlap 2Note that because counterfactuals are not guaranteed to be easier, clean-model deltas need not be positive."
        },
        {
            "title": "Model",
            "content": "RQA RQA_P LLaVA-v1.5-7B (clean) Pretrain (contaminated) 52.05 51.82 56.36 50.00 +4.31 1. Table 6: Performance of clean vs. pretrained-contaminated models on RealWorldQA (440 images). with the manually kept set, indicating high agreement  (Table 7)  . The exact prompt is provided in Appendix A.3 and full results are listed in E.3. Caption generation with Molmo-7B-D. To decouple the pipeline from GPT-4o, we substitute captioning with the lightweight open-source Molmo-7B-D (Deitke et al., 2024) while keeping Flux ControlNet for image generation. After manual filtering, this variant yields 398 valid pairs and preserves the same detection trends, underscoring the flexibility of our approach  (Table 8)  . Full results are listed in E.4."
        },
        {
            "title": "Method",
            "content": "Epoch RQA RQA_P"
        },
        {
            "title": "Method",
            "content": "Epoch RQA RQA_P"
        },
        {
            "title": "Contamination with RealWorldQA",
            "content": "LLaVA-v1.5-7B (clean) LoRA (contaminated) LLM+MLP (contaminated) 1 2 3 1 2 50.68 50.34 66.33 77.21 58.50 60.54 64.97 59.86 44.90 46.60 47.28 54.42 54.42 53. +9.18 5.44 19.73 29.93 4.08 6.12 11.57 LLaVA-v1.5-7B (clean) LoRA (contaminated) LLM+MLP (contaminated) 1 2 3 1 2 3 44.22 44.97 55.78 70.35 56.53 60.80 61. 52.01 35.93 32.66 32.91 39.95 39.45 39.95 +7.79 9.04 23.12 37.44 16.58 21.35 21. Table 7: Performance after filtering with the o3 model, resulting in 294 valid imagequestion pairs. Results satisfy all requirements. Table 8: Performance after generating and filtering images with captions from Molmo-7B-D, resulting in 398 valid imagequestion pairs. Results satisfy all requirements."
        },
        {
            "title": "8 RELATED WORK",
            "content": "Data Contamination in LLMs. Brown et al. (2020) was among the first to highlight the problem of data contamination when training models on internet-scale corpora, proposing an n-gram overlapbased decontamination technique that was later adopted in Abdin et al. (2024a). Building on this, several detection methods exploit verbatim memorization to identify leaked test-set examples (Oren et al., 2023; Golchin & Surdeanu, 2024; Xu et al., 2024). More specifically, Xu et al. (2024); Golchin & Surdeanu (2024) test whether the model can accurately reconstruct masked spans of test questions, while Oren et al. (2023) measure the log-probability of the original ordering of multiple-choice options. Choi et al. (2025) instead examine embedding divergence after fine-tuning, exploiting the observation that embeddings of unseen samples change more substantially than those of memorized ones. However, these methods face key limitations: Oren et al. (2023); Golchin & Surdeanu (2024); Xu et al. (2024) perform poorly on contaminated VLMs, while Choi et al. (2025) requires ground-truth clean model behavior to establish detection threshold, reducing its practicality. Generalized Benchmarks for Detecting Contamination. Yao et al. (2024) address these limitations by testing for generalization rather than memorization. They construct trivial variants of benchmark questions by replacing incorrect multiple-choice options with irrelevant answers. Contaminated models often fail on the easier variants, exposing strong memorization. However, their setup involves training for 36 epochs on single benchmark an unrealistic scenario for modern large-scale training. In similar spirit, GSM-Symbolic and MATH-Perturb (Mirzadeh et al., 2024; Huang et al., 2025) introduce perturbation-based approaches, measuring performance drops on variant questions as contamination signals. Contamination Detection in VLMs. VLMs differ from LLMs due to their multi-modal inputs and multi-stage alignment training, which yield distinct contamination dynamics. Lu et al. (2024a) proposed shuffling BGR color channels to mitigate spurious cues, while Song et al. (2025) introduced image masking and option shuffling to test robustness. Yet these approaches struggle to reliably detect contaminated VLMs, as observed performance drops may arise from confounding factors such as visual artifacts or biased sampling rather than true memorization. In contrast, our framework provides consistent detection across diverse fine-tuning regimes, requires no access to leaked data, and yields performance drops that correlate strongly with the degree of contamination."
        },
        {
            "title": "9 CONCLUSION",
            "content": "Recent advances in Vision-Language Models (VLMs) have raised concerns about inflated benchmark performance due to test-set leakage from large-scale, proprietary training corpora. To overcome the limitations of existing detection methods, we introduce novel approach based on multi-modal semantic perturbation. By deliberately contaminating open-source VLMs and evaluating their generalization behavior, we show that our method consistently identifies contamination where prior approaches either fail or yield inconsistent signals. These results establish multi-modal semantic perturbation as simple and reliable framework for detecting test-set leakage in VLMs. Broader Impacts. Our multi-modal semantic perturbation method aims to uncover and quantify the degree of data contamination in VLMs, promoting cleaner training pipelines and more trustworthy models. By systematically characterizing contamination behaviors, it lays the groundwork for robust model evaluation and contamination detection. While these insights could inform adversaries seeking subtler contamination schemes, we believe this work will foster the development of stronger defenses and decontamination strategies. Reproducibility Statement. We will publicly release our code, models and data. Acknowledgement. This work was supported in part by NSF IIS2404180, Microsoft Accelerate Foundation Models Research Program, and Institute of Information & communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pretraining). The authors would like to thank SeungEun Chung, YoHan Ban, Samuel Low Yu Hang and Junxia Cui for assistance with experiments and Anirudh Sundara Rajan and Zhuoran Yu for helpful discussion."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024a. URL https://arxiv.org/abs/2412.08905. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024b. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https: //arxiv.org/abs/2005.14165. John Canny. computational approach to edge detection. TPAMI, PAMI-8(6):679698, 1986. doi: 10.1109/TPAMI.1986.4767851. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin"
        },
        {
            "title": "Preprint",
            "content": "Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pp. 26332650. USENIX Association, August 2021. ISBN 978-1939133-24-3. URL https://www.usenix.org/conference/usenixsecurity21/ presentation/carlini-extracting. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 2705627087. Curran Associates, Inc., 2024a. URL https://proceedings.neurips.cc/paper_files/paper/2024/ file/2f8ee6a3d766b426d2618e555b5aeb39-Paper-Conference.pdf. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, 2024b. URL https://arxiv.org/abs/2312.14238. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. URL https: //arxiv.org/abs/2412.05271. Hyeong Kyu Choi, Maxim Khanov, Hongxin Wei, and Yixuan Li. How contaminated is your benchmark? quantifying dataset leakage in large language models with kernel divergence, 2025. URL https://arxiv.org/abs/2502.00678. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris CallisonBurch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. URL https://arxiv.org/abs/2409.17146. Gemini Team. Gemini: family of highly capable multimodal models, 2024. Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models, 2024. URL https://arxiv.org/abs/2308.08493. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, Yue Wu, Ming Yin, Shange Tang, Yangsibo Huang, Chi Jin, Xinyun Chen, Chiyuan Zhang, and Mengdi Wang. Math-perturb: Benchmarking llms math reasoning abilities against hard perturbations, 2025. URL https://arxiv.org/abs/2502.06453. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1569615707. PMLR, 2329 Jul 2023. URL https://proceedings.mlr. press/v202/kandpal23a.html."
        },
        {
            "title": "Preprint",
            "content": "Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating visionIn The Thirty-eight Conference on Neural language models on natural adversarial samples. Information Processing Systems Datasets and Benchmarks Track, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024a. URL https://arxiv.org/abs/2310.03744. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Hongyuan Lu, Shujie Miao, and Wai Lam. Clean evaluations on contaminated visual language models, 2024a. URL https://arxiv.org/abs/2410.07030. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024b. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models, 2024. URL https://arxiv.org/abs/2410.05229. OpenAI. GPT-4o technical report. https://openai.com/research/gpt-4o, 2024. Accessed: 2025-05-16. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. Proving test set contamination in black box language models, 2023. URL https://arxiv.org/abs/ 2310.17623. Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, and Benyou Wang. Both text and images leaked! systematic analysis of multimodal llm data contamination, 2025. URL https:// arxiv.org/abs/2411.03823. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409. 12191. xAI. Grok-1.5 vision preview. https://x.ai/blog/grok-1.5v, 2024. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models, 2024. URL https://arxiv.org/abs/2404.18824. Feng Yao, Yufan Zhuang, Zihao Sun, Sunan Xu, Animesh Kumar, and Jingbo Shang. Data contamination can cross language barriers, 2024. URL https://arxiv.org/abs/2406.13236. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx. L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. 2023."
        },
        {
            "title": "Preprint",
            "content": "Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does our multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A EXPERIMENT SETTINGS",
            "content": "In this section, we detail all hyperparameter settings for contaminating models and for multi-modal semantic perturbation of benchmarks. A.1 MODEL TRAINING We contaminate LLaVA-v1.5-7B by following the official repository 3 and Qwen2-VL-7B using LLaMA-Factory. All settings remain identical except for the learning rates, which were tuned over the ranges shown in Table 9. LLaVA-v1.5-7B Qwen2-VL-7B-Instruct"
        },
        {
            "title": "LoRA",
            "content": "Learning rate Adapter LR Effective batch size* LoRA rank 5e6, 1e5, 2e5 5e6 128 2e4, 1e4 2e5 128 8 1e5, 2e5, 5e6 1e6 16 1e4, 1e3 1e6 64 8 Table 9: Hyperparameter settings for LLaVA-v1.5-7B and Qwen2-VL-7B-Instruct under standard fine-tuning and LoRA. Effective batch size accounts for gradient accumulation across 8 GPUs. We perform continual fine-tuning of the model weights on 8 NVIDIA A6000 GPUs. Notice Qwen2VL models are trained with much smaller batch sizes due to heavier GPU VRAM requirements. A.2 MULTI-MODAL SEMANTIC PERTURBATION Multi-modal semantic perturbation is two-stage process. In the first process where we obtain prompts that will be provided to Flux Controlnet model is generated by using GPT4o with api-version=2024-08-01-preview. We set temperature=0.3 and limit max_tokens=800. In the second stage where we create perturbed version of the original image based on the caption generated in the first stage, we utilize Controlnet trained with Flux diffusion model 4. We follow the default hyperparameter settings in the repository, except that we enforce the generated images to have the same resolution as the original images. All images are generated with 25 steps. A.3 SYSTEM PROMPTS TO GPT-4O AND O3 To generate the semantically perturbed question, we utilize GPT-4o OpenAI (2024) and Flux diffusion model Labs (2024) trained on ControlNet Zhang et al. (2023). We generate detailed caption about the image using GPT-4o along with the original question and the answer with the following system prompt: 3The default training script can be found here: https://github.com/haotian-liu/LLaVA. 4Code can be accessed here: https://github.com/XLabs-AI/x-flux"
        },
        {
            "title": "Preprint",
            "content": "Your job is to generate text-to-image prompt that can be used with diffusion model. Based on the question and answer, write detailed caption so that all necessary details are included and the question remains solvable. Additionally, modify the image so that the correct answer changes. For example, if the question asks How many people are in the image?, change the image to have more people. In Section 7, we demonstrated that the manual filtering process can be automated with o3 model which is powerful reasoning model. We utilize the following system prompt to generate model responses that can be used to filter the invalid perturbed images. You will be given question and an image pair, along with the answer. Your job is to critically analyze the image-question pair to verify that the question can be correctly answered. In particular, ensure that one can deduce the correct answer choice and that choice only. If there is any ambiguity, you must reject this question. When finalizing your decision, do NOT take into consideration the quality of the image. As long as the question remains solvable, you should keep it. Provide your answer in the following format: \"Answer: ANSWER\" and answer with KEEP or REJECT. PERPLEXITY-BASED METHODS We evaluate three perplexity-based methods: Shared Likelihood (SL) (Oren et al., 2023), which compares log-likelihoods on original versus shuffled inputs; Guided Prompting (GP) (Golchin & Surdeanu, 2024), which uses an LLM to score model completions against the original; and N-gram Accuracy (NG) (Xu et al., 2024), which masks answer choices and checks reproduction. Each probes whether models memorize test samples through partial or perturbed inputs in the text domain. Results in Table 10 show that SL, GP, and NG fail to meet any of the requirements. All three depend on reference values from clean models, violating Requirement 1. Contaminated models often display counter-intuitive trends, such as improved scores with greater contamination, violating Requirement 2. Finally, none satisfy Requirement 3for instance, SL p-values increase rather than decrease as contamination intensifies, directly contradicting expectation."
        },
        {
            "title": "MMStar",
            "content": "SL (p-val)() GP (%)() NG (%)() SL (p-val)() GP (%)() NG (%)() LLaVA -v1.5-7B"
        },
        {
            "title": "LoRA",
            "content": "LLM +MLP Qwen2 -VL-7B"
        },
        {
            "title": "LLM",
            "content": "1 2 3 1 2 3 1 2 3 1 2 0.926 0.927 0.933 0.935 0.928 0.929 0.932 0.166 0.333 0.166 0.323 0.124 0.070 0. 7.32 8.63 4.44 7.19 4.84 6.67 5.75 38.43 46.27 49.80 47.06 14.64 9.54 11. 25.67 24.94 26.32 25.12 26.09 26.93 26.82 13.10 13.88 14.51 14.54 15.48 16.08 16. 0.201 0.088 0.066 0.072 0.207 0.219 0.238 0.006 0.009 0.004 0.000 0.005 0.004 0. 2.53 0.60 0.80 0.53 0.73 0.47 0.13 13.80 3.07 3.00 1.00 1.13 1.20 1. 14.54 5.63 3.27 2.92 12.69 8.63 3.12 3.08 7.67 8.26 7.96 2.47 3.09 7. Table 10: Perplexity-based baselines (SL = shared_likelihood, GP = guided prompting, NG = n-gram) for RealWorldQA and MMStar. MMBench was left out due to limited compute. To clarify, the models are trained with the dataset they are being evaluated on. There is no clear signal that distinguishes clean and contaminated models."
        },
        {
            "title": "Preprint",
            "content": "C DETECTING CONTAMINATED MODELS WITH MULTI-MODAL SEMANTIC"
        },
        {
            "title": "PERTURBATION ON REALWORLDQA",
            "content": "Table 11 reports results for RealWorldQA. Similar to results on MMStar, we find that (i) clean models consistently outperform contaminated ones, confirming that perturbed questions are not harder; (ii) in contrast, contaminated models show clear performance drops, enabling reliable detection without thresholds or prior knowledge (Requirement 1). The drop scales with contamination level (Requirement 3) and holds across training strategies (Requirement 2)."
        },
        {
            "title": "Metric",
            "content": "LLaVA-v1.5-7B (clean) LoRA (contaminated) LLM+MLP (contaminated) Require clean model?"
        },
        {
            "title": "Choice Confusion",
            "content": "Multi-modal Leakage RQA RQA_P Success? RQA RQA_C Success? RQA RQA_G Success? RQA_to Success? 52.05 56.36 +4.31 52.05 36.59 -15.46 52.05 66.59 +14.54 34.77 Epoch 1 Epoch 2 Epoch 3 Epoch 1 Epoch 2 Epoch 3 (Practicality (Req. 1)) 62.73 51.36 11.37 62.73 17.95 -44.78 62.73 41.36 -21.37 37.27 +2.50 70.00 52.73 17.27 70.00 45.23 -24.77 70.00 59.09 -10.91 47.05 +12.28 79.55 44.77 34.78 79.55 57.50 -22.05 79.55 66.14 -13.41 48.41 +13.64 56.36 52.95 3.41 56.36 33.18 -23.18 56.36 62.95 +6.59 37.73 +2.96 64.55 50.45 14.10 64.55 37.05 -27.50 64.55 62.50 -2.05 40.68 +5.91 70.68 51.82 18.86 70.68 43.40 -27.28 70.68 63.64 -7.04 45.00 +10.23 No Yes No Yes"
        },
        {
            "title": "Metric",
            "content": "Qwen2-VL-7B (clean) LoRA (contaminated) LLM only (contaminated) Require clean model?"
        },
        {
            "title": "Choice Confusion",
            "content": "Multi-modal Leakage RQA RQA_P Success? RQA RQA_C Success? RQA RQA_G Success? RQA_to Success? 70.45 71.36 +0.91 70.45 63.64 -6.81 70.45 91.36 +20.91 36.14 Epoch 1 Epoch 2 Epoch 3 Epoch 1 Epoch 2 Epoch 3 (Practicality (Req. 1)) 79.32 65.00 14.32 79.32 65.00 -14.32 79.32 91.59 +12.27 37.27 +1.13 87.50 64.55 22.95 87.50 65.91 -21.59 87.50 92.05 +4.55 41.14 +5.00 88.86 61.59 27.27 88.86 67.05 -21.81 88.86 92.50 +3.64 42.27 +6.13 74.77 46.82 27.95 74.77 65.23 -9.54 74.77 86.14 +11.37 52.73 +16.59 78.64 50.45 28.19 78.64 66.59 -12.05 78.64 89.77 +11.13 28.41 -7.73 85.23 46.59 38.64 85.23 77.73 -7.50 85.23 95.23 +10.00 62.50 +26.36 No Yes No Yes Table 11: Performance of LLaVA-v1.5-7B (top) and Qwen2-VL-7B (bottom) on the RealWorldQA dataset. We compare to Multi-modal Leakage Chen et al. (2024a), CircularEval (Liu et al., 2024b), and Choice Confusion Yao et al. (2024). Clean models perform better confirming that the perturbed questions are indeed of equal or lower difficulty. In contrast, all contaminated models perform worse, enabling reliable detection by our method via simple check for performance drops. RQA denotes RealWorldQA and \"_P\" denotes the semantically perturbed version; \"to\" denotes text-only performance; \"_C\" denotes evaluation using circular options; \"_G\" denotes evaluation using choice confusion; denotes the difference in performance, with positive values indicating gains. \"Success?\" indicates whether the method detected contamination. \"Require clean model?\" indicates whether the method requires access to clean model as baseline. If clean model is required as reference, the method cannot be used to detect the reference models themselves, so the entry is marked as ."
        },
        {
            "title": "Preprint",
            "content": "D EXTENDED EVALUATION ON ADDITIONAL OPEN-SOURCE AND"
        },
        {
            "title": "PROPRIETARY MODELS",
            "content": "We further apply our pipeline to GPT-4o (OpenAI, 2024), Gemini-2.0-Flash (Gemini Team, 2024), Phi-3.5-V (Abdin et al., 2024b), and InternVL-2.5 (Chen et al., 2025). We do not contaminate these models. Although we cannot guarantee that these models have never encountered our evaluation data, we assume they are uncontaminated. Under this assumption, if our framework is sound, their performance should remain consistent across original and perturbed datasets. Indeed, our results in Table 12 confirm this expectation. Across all models, perturbed performance exceeds original performance, reinforcing that clean models generalize while contaminated ones fail. Notably, Phi3.5-V exhibits the largest gain, while InternVL-2.5-8B remains relatively flat, demonstrating that our metric is robust across architectures. This consistency underscores the effectiveness of multi-modal semantic perturbation as detection framework."
        },
        {
            "title": "Model",
            "content": "RQA RQA_P Gemini-2.0-Flash GPT-4o Phi3.5-Vision InternVL-2.5-8B 68.37 65.68 52.68 64.05 71.59 69.93 68.86 64.77 +3.22 +4.25 +16.18 +0. Table 12: Accuracy on the original vs. perturbed datasets for open-source and proprietary models. indicates the accuracy change."
        },
        {
            "title": "E FULL EXPERIMENT RESULTS",
            "content": "In this section, we provide the full experiment results of multi-modal semantic perturbation across four training strategies that were omitted due to limited space. E.1 FULL RESULTS ON LLAVA-V1.5-7B AND QWEN2-VL-7B"
        },
        {
            "title": "Model",
            "content": "Epoch RQA (%) RQA_P (%)"
        },
        {
            "title": "Model",
            "content": "Epoch RQA (%) RQA_P (%) LLaVA-v1.5-7B 52.05 56.36 +4. Qwen2-VL-7B 70.45 71.36 +0."
        },
        {
            "title": "LLM only",
            "content": "LLM+MLP"
        },
        {
            "title": "ALL",
            "content": "1 2 3 1 2 3 1 2 3 1 2 3 62.73 70.00 79.55 58.41 63.64 70. 56.36 64.55 70.68 56.36 64.77 70.23 51.36 52.73 44.77 51.59 50.68 47.05 52.95 50.45 51.82 54.32 52.73 52. 11.37 17.27 34.78 6.82 12.96 23.86 3.41 14.10 18.86 2.04 12.04 18."
        },
        {
            "title": "LLM only",
            "content": "LLM+MLP"
        },
        {
            "title": "ALL",
            "content": "1 2 3 1 2 3 1 2 3 1 2 3 79.32 87.50 88.86 74.77 78.64 85. 75.23 88.18 93.18 74.77 78.18 87.50 65.00 64.55 61.59 46.82 50.45 46.59 57.95 49.77 47.50 39.09 45.91 40. 14.32 22.95 27.27 27.95 28.19 38.64 17.28 38.41 45.68 35.68 32.27 47.05 Table 13: Performance of clean and contaminated LLaVA-v1.5-7B models on RealWorldQA. _P\" denotes the semantically perturbed version. All three requirements are satisfied. Table 14: Performance of clean and contaminated Qwen2-VL-7B models on RealWorldQA. _P\" denotes the semantically perturbed version. All three requirements are satisfied. Table 13, 14, 15, and 16 list out all results for four varying training strategies for LLaVA-v1.5-7B and Qwen2-VL-7B , including the non-default training strategies for each model which were omitted due to limited space. Note that the results here are computed on 440 manually filtered images. Our approach detects contaminated models regardless of the training strategies and satisfies all three requirements."
        },
        {
            "title": "Model",
            "content": "Epoch RQA (%) RQA_P (%)"
        },
        {
            "title": "Method",
            "content": "Epoch RQA (%) RQA_P (%) LLaVA-v1.5-7B 37.78 69.29 +31. Qwen2-VL-7B 62.02 78.18 +16."
        },
        {
            "title": "LLM only",
            "content": "LLM+MLP"
        },
        {
            "title": "ALL",
            "content": "1 2 3 1 2 3 1 2 3 1 2 3 52.53 50.71 54.34 44.85 48.48 55. 39.39 49.70 53.54 41.82 48.89 50.71 44.24 37.58 38.18 37.98 38.99 38.59 32.12 38.38 38.99 33.33 37.37 36. 8.29 13.13 16.16 6.87 9.49 16.56 7.27 11.32 14.55 8.49 11.52 13."
        },
        {
            "title": "LLM only",
            "content": "LLM+MLP"
        },
        {
            "title": "ALL",
            "content": "1 2 3 1 2 3 1 2 3 1 2 3 77.37 87.88 91.52 80.20 94.95 97. 83.43 94.95 97.98 71.31 93.13 96.77 73.33 68.48 67.47 50.71 52.32 49.09 55.96 52.93 51.11 47.27 43.64 44. 4.04 19.40 24.05 29.49 42.63 48.08 27.47 42.02 46.87 24.04 49.49 52.73 Table 15: Performance of clean and contaminated LLaVA-v1.5-7B models on MMStar. _P\" denotes the semantically perturbed version. All three requirements are satisfied. Table 16: Performance of clean and contaminated Qwen2-VL-7B models on MMStar. _P\" denotes the semantically perturbed version. All three requirements are satisfied. E.2 FULL RESULTS WITH LLAVA-V1.5-13B AND NATURALBENCH Table 17 lists full results for LLaVA-v1.5-13B model trained on RealWorldQA. Note that the results here are computed on 440 manually filtered images. Table 18 lists full results for LLaVA-v1.5-7B traiend on one variant of NaturalBench and tested on the counterfactual version. The clean and consistent detection of contaminated models show that our approach can be applied to models of larger scale and to real-world perturbations."
        },
        {
            "title": "Model",
            "content": "Epoch RQA (%) RQA_P (%) LLaVA-v1.5-13B 51.14 57.27 +6."
        },
        {
            "title": "Model",
            "content": "Epoch Train (%) Test (%) LLaVA-v1.5-7B 65.63 65.89 +0."
        },
        {
            "title": "LLM only",
            "content": "LLM+MLP"
        },
        {
            "title": "ALL",
            "content": "1 2 3 1 2 3 1 2 3 1 2 3 74.32 73.18 77.05 57.05 68.18 68. 56.59 71.59 75.45 57.73 68.41 69.09 38.18 32.73 34.77 44.77 44.32 43.86 37.50 38.86 37.27 44.55 44.32 43. 36.14 40.45 42.28 12.28 23.86 24.78 19.09 32.73 38.18 13.18 24.09 25."
        },
        {
            "title": "LLM only",
            "content": "LLM+MLP"
        },
        {
            "title": "ALL",
            "content": "1 2 3 1 2 3 1 2 3 1 2 3 81.53 89.79 91.11 77.63 88.11 90. 79.95 97.05 98.63 81.84 97.05 98.63 61.37 57.16 57.32 61.95 57.89 57.32 58.79 54.32 53.05 58.42 54.47 52. 20.16 32.63 33.79 15.68 30.21 33.26 21.16 42.74 45.58 23.42 42.58 45.95 Table 17: Performance of LLaVA-v1.5-13B models on the RealWorldQA benchmark and its semantically perturbed version consisting of 440 image-question pairs. All three requirments are satisfied. Table 18: Performance of clean and contaminated LLaVA-v1.5-7B models on NaturalBench. Test set denotes natural counterfactual version of the train set. Clean model maintains similar performance while contaminated models drop in performance for upto -45.95%. E.3 FULL RESULTS WITH O3 FILTERING Table 19 and 20 list out all results for contamination detection results after filtering the perturbed images with o3 model. To clarify, filtering with o3 model results in 294 images. Our approach still detects contaminated models and satisfies all three requirements, proving that our pipeline design is modular and can be automated."
        },
        {
            "title": "Model",
            "content": "Epoch RQA (%) RQA_P (%)"
        },
        {
            "title": "Model",
            "content": "Epoch RQA (%) RQA_P (%)"
        },
        {
            "title": "Contamination with RealWorldQA",
            "content": "LLaVA (clean) LoRA (contaminated) LLM (contaminated) LLM+MLP (contaminated) ALL (contaminated) 1 2 3 1 2 3 1 2 3 1 2 3 50.68 50.34 66.33 77. 59.86 63.95 68.71 58.50 60.54 64.97 59.86 64.63 69.05 59.86 44.90 46.60 47.28 56.80 55.10 50. 54.42 54.42 53.40 56.46 53.40 50.34 +9.18 5.44 19.73 29.93 3.06 8.85 18.71 4.08 6.12 11. 3.40 11.23 18.71 Qwen2-VL (clean) LoRA (contaminated) LLM (contaminated) LLM+MLP (contaminated) ALL (contaminated) 1 2 3 1 2 3 1 2 3 1 2 3 74. 77.21 78.91 79.59 79.25 85.37 90.48 77.89 87.41 88.78 80.27 90.82 92.52 74.83 74.83 74.15 74. 56.12 54.42 55.44 62.24 57.48 58.16 58.50 53.40 52.04 +0.68 2.38 4.76 5.44 23.13 30.95 35. 15.65 29.93 30.62 21.77 37.42 40.48 Table 19: Performance of clean and contaminated LLaVA-v1.5-7B models on RealWorldQA. _P\" denotes the semantically perturbed version. Note that accuracies are measured on 294 filtered images. All three requirements are satisfied. Table 20: Performance of clean and contaminated Qwen2-VL-7B models on RealWorldQA. _P\" denotes the semantically perturbed version. Note that accuracies are measured on 294 filtered images. All three requirements are satisfied. E.4 FULL RESULTS WITH MOLMO-7B-D AS CAPTIONING MODEL Table 21 and 22 list full results for LLaVA-v1.5-7B models evaluated on perturbed images generated from Molmo-7B-D captions. Note that this pipeline yields 398 valid image-question pairs after manual filtering. Both results show clean detection trends, satisfying all three requirements."
        },
        {
            "title": "Model",
            "content": "Config RQA (%) RQA_P (%) (%)"
        },
        {
            "title": "Model",
            "content": "Config RQA (%) RQA_P (%) (%)"
        },
        {
            "title": "Contamination with RealWorldQA",
            "content": "LLaVA (clean) LoRA (contaminated) LLM (contaminated) LLM+MLP (contaminated) ALL (contaminated) 1 2 3 1 2 3 1 2 3 1 2 3 44.22 44.97 55.78 70. 52.76 53.02 53.52 56.53 60.80 61.56 62.81 61.81 62.56 52.01 35.93 32.66 32.91 41.96 43.97 43. 39.95 39.45 39.95 35.43 35.93 35.68 +7.79 9.04 23.12 37.44 10.80 9.05 9.80 16.58 21.35 21. 27.38 25.88 26.88 Qwen2-VL (clean) LoRA (contaminated) LLM (contaminated) LLM+MLP (contaminated) ALL (contaminated) 1 2 3 1 2 3 1 2 3 1 2 3 61. 63.57 67.84 68.84 78.14 84.67 87.19 73.12 83.42 85.18 76.38 88.94 90.20 65.08 59.05 52.26 52. 40.70 32.16 35.43 45.48 34.67 35.93 39.95 32.41 31.41 +3.27 4.52 15.58 16.58 37.44 52.51 51. 27.64 48.75 49.25 36.43 56.53 58.79 Table 21: Performance of clean and contaminated LLaVA-v1.5-7B models on RealWorldQA. _P\" denotes the semantically perturbed version. Note that accuracies are measured on 398 filtered images. All three requirements are satisfied. Table 22: Performance of clean and contaminated Qwen2-VL-7B models on RealWorldQA. _P\" denotes the semantically perturbed version. Note that accuracies are measured on 398 filtered images. All three requirements are satisfied."
        }
    ],
    "affiliations": [
        "Kookmin University",
        "University of California, San Diego",
        "University of Wisconsin-Madison"
    ]
}