{
    "paper_title": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity",
    "authors": [
        "Guangzhi Xiong",
        "Sanchit Sinha",
        "Aidong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE."
        },
        {
            "title": "Start",
            "content": "Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity 6 2 0 2 1 1 ] . [ 1 5 8 5 0 1 . 2 0 6 2 : r a"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "The trade-off between interpretability and accuracy remains core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), novel framework that seamlessly balances interpretability and accuracy. NAEs employ mixture of experts framework, learning multiple specialized networks per feature, while dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the models flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, featurelevel explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE. Proceedings of the 29th International Conference on Artificial Intelligence and Statistics (AISTATS) 2026, Tangier, Morocco. PMLR: Volume 300. Copyright 2026 by the author(s). The tension between interpretability and predictive accuracy is central challenge in modern machine learning. While deep neural networks and other complex models have achieved remarkable success across range of domains, their opaque decision-making processes often hinder adoption in high-stakes settings where transparency and trust are paramount. In contrast, Generalized Additive Models (GAMs) (Hastie, 2017; Agarwal et al., 2021; Lou et al., 2012; Chang et al., 2022) are widely valued for their interpretability, the degree to which an observer can understand the cause of decision (Biran and Cotton, 2017; Miller, 2019), as they decompose predictions into clear, feature-specific contributions (McIntosh, 2025; Ibrahim et al., 2025; Zhang et al., 2024; Bouchiat et al., 2023). However, the strictly additive structure of GAMs can limit their ability to capture complex relationships among features, often resulting in suboptimal predictive performance. common approach to improve the predictive accuracy of GAMs is to incorporate feature interactions (Ruppert, 2004; Lou et al., 2013). However, introducing such interactions often diminishes interpretability, as it becomes more difficult to transparently attribute predictions to individual features, since interpreting single feature may require comprehensive analysis of multiple related feature interactions (Lou et al., 2012; Caruana et al., 2015; Christoph, 2020). Furthermore, GAMs with interactions typically lack explicit mechanisms to control model additivity, the degree to which outputs can be decomposed into additive feature contributions, which is hallmark of standard GAMs. This limitation restricts users ability to flexibly balance accuracy and interpretability to suit specific application requirements. In this work, we propose Neural Additive Experts (NAEs), novel framework designed to address this fundamental trade-off. NAEs extend the classical additive paradigm by associating each feature with set of specialized expert networks. dynamic gating Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity mechanism adaptively integrates information across features, allowing the model to relax the rigid additive constraints when interactions are important. To control the balance between accuracy and interpretability, we introduce targeted regularization term that controls the variance among expert predictions, enabling smooth transition from purely additive model to one that captures nuanced feature interactions while maintaining clarity in feature attributions. Our theoretical analysis demonstrates how NAEs can recover complex data-generating processes that are inaccessible to standard GAMs, and provides insight into the role of regularization in balancing model flexibility and interpretability. Through experiments on synthetic and real-world datasets, we show that NAEs achieve competitive accuracy with state-ofthe-art black-box models, while offering transparent, feature-level explanations. By providing principled mechanism to control the trade-off between interpretability and accuracy, NAEs offer practical solution for applications where both predictive performance and transparency are essential. The contributions of this work are summarized as follows: We introduce Neural Additive Experts (NAEs), novel framework that extends additive models with mixture-of-experts architecture and dynamic gating, enabling flexible integration of feature interactions while preserving interpretability. We propose targeted regularization techniques that allow users to control the variance among expert predictions, providing principled mechanism to balance model flexibility and transparency. We provide theoretical analysis demonstrating how NAEs can recover complex data-generating processes that are inaccessible to standard GAMs, and clarify the role of regularization in managing the trade-off between accuracy and interpretability. We empirically validate NAEs on both synthetic and real-world datasets, showing that our approach achieves competitive predictive performance with state-of-the-art black-box models while maintaining clear, feature-level explanations."
        },
        {
            "title": "2 NEURAL ADDITIVE EXPERTS",
            "content": "with set of expert predictors and employing dynamic, context-aware gating mechanism. This architecture enables the model to capture complex, contextdependent feature effects while preserving the additive structure that supports interpretability. Figure 1 provides an overview of the NAE architecture."
        },
        {
            "title": "Networks",
            "content": "Let xi (i = 1, . . . , n) denote the i-th feature of an input instance. Each feature is first mapped into latent representation via an encoder: Ei : Xi Rd, (1) where Xi is the domain of xi and is the latent dimension. Unlike standard GAMs, which use single predictor per feature, NAEs introduce set of expert networks for each feature. The k-th expert for feature xi produces: oik = gik (cid:0)Ei(xi)(cid:1), (2) where gik : Rd is typically simple function (e.g., linear or shallow neural network) that maps the latent representation to feature effect. 2.2 Dynamic Gating and Expert Aggregation central component of NAEs is the dynamic gating mechanism, which adaptively integrates information across features to determine the relevance of each expert. For each feature xj, we compute score vector to quantify the influence of the xi value on the expert selection for feature xj: φj = µj + (cid:88) i=1 ijEi(xi), (3) where Aij RdK are learnable parameters and µj RK is learnable bias term. This formulation allows the gating to depend on the context provided by all features, relaxing the strict additivity of classical GAMs. To ensure that only the most relevant experts contribute to the final prediction, sparsity-inducing mask Mj RK is applied to φj, where entries corresponding to less significant scores are set to , with remaining values set to be 0. This masking procedure effectively zeroes out the contribution of less important experts after normalization. The relevance weights are then obtained via softmax operation: exp(cid:0)φj[k] + Mj[k](cid:1) l=1 exp(cid:0)φj[l] + Mj[l](cid:1) , = 1, , K, (4) rjk = (cid:80)K Neural Additive Expert (NAE) is framework that extends classical GAMs by associating each feature which ensures that rjk 0 and (cid:80)K k=1 rjk = 1. Subsequently, the output for feature xi is computed as Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Illustration of the Neural Additive Expert (NAE) framework. gating network dynamically assigns Figure 1: relevance scores to multiple expert predictors for each feature, and the aggregated feature contributions are summed to produce the final prediction. This design maintains interpretability while enabling flexible modeling of complex relationships. weighted aggregation of the individual expert predictions: oi = (cid:88) k=1 rikoik, (5) where oik is the output of the k-th expert for feature xi as defined in Equation 2. The overall model prediction is obtained by summing the feature-specific outputs along with an intercept term ω0: ˆy = ω0 + (cid:88) i=1 oi. (6) This structure preserves feature-level interpretability while allowing for adaptive, context-dependent modeling. The computational complexity of NAEs is discussed in Appendix J. tunable mechanism to balance flexibility and interpretability. Standard regularization techniques (e.g., dropout, L2 regularization) are also applied to promote generalization. 2.4 Interpretability and Feature Attribution NAEs maintain interpretability by quantifying the contribution of each feature to the final prediction. For each feature xi, the model aggregates expert outputs using the learned gating weights, ensuring that feature attributions remain bounded and meaningful. To further aid interpretation, we define upper and lower bounds for each features effect: upper(oixi) = max gik lower(oixi) = min gik (cid:0)Ei(xi)(cid:1), (cid:0)Ei(xi)(cid:1). (8) 2.3 Training Objective and Regularization Given dataset {(xt, yt)}N timize: t=1, NAEs are trained to opminimize 1 (cid:88) t=1 λ nN L(cid:0)yt, ˆyt(cid:1)+ (cid:88) (cid:88) (cid:88) (cid:32) t=1 i=1 k=1 ot ik (cid:33) ot il , 1 (cid:88) l=1 (7) where is task-specific loss (e.g., mean squared error or cross-entropy), and λ controls the strength of the expert variation penalty. This regularization encourages consistency among experts for each feature, providing These bounds characterize the range of possible contributions for each feature, and visualizing the distribution of actual feature effects within this range provides insight into both the magnitude and variability of feature influences. While feature attribution is central to NAEs, the model also enables analysis of feature interactions. Since the score vector in Equation (3) is constructed as an additive function over all features, we can isolate the interaction between features xi and xj by focusing on their respective contributions in the gating function: = φ ijEi(xi) and = φ jiEj(xj). (9) Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity By removing the influence of other features, the updated feature outputs with Equation (5) reflect the pairwise feature interactions captured by NAE. Further discussion of feature interactions is provided in Appendix F. and o"
        },
        {
            "title": "3.2 From GA2M to NAE via separable",
            "content": "approximation To show the containment relative to GA2M, we need the following approximation lemma, which follows from the Stone-Weierstrass theorem."
        },
        {
            "title": "NAES",
            "content": "In this section, we characterize the expressivity of NAEs relative to classical additive families and analyze how they control model additivity for featurelevel interpretability. Throughout we assume each feature domain Xi is compact and all functions are continuous on their domains. NAEs use (i) perfeature encoders Ei : Xi Rd, (ii) expert functions gik : Rd R, (iii) softmax gating weights rjk(x) 0 with (cid:80)K k=1 rjk(x) = 1, and (iv) per-feature outputs, oi(x) = (cid:88) k=1 rik(x)gik (cid:0)Ei(xi)(cid:1), ˆy(x) = ω0 + (cid:88) i=1 oi(x), Lemma 1 (Finite separable approximation of pairwise terms). Fix (i, j) and ε > 0. There exist and continuous functions {um : Xi (cid:12) R, vm : Xj R}M (cid:12)fij(xi, xj) (cid:80)M m=1 such that sup(xi,xj ) m=1 um(xi)vm(xj)(cid:12) (cid:12) < ε. We now show that each separable term u(xi)v(xj) is representable by single feature head oi() using two experts and softmax gate that depends on xj only. Lemma 2 (Two-expert product construction). Let : Xi and : Xj be bounded and continuous. Choose > supxj v(xj). Instantiate two experts for gi, feature with outputs gi,+ Ei(xi) = +Cu(xi), Ei(xi) = Cu(xi), and gate them by logits φ(+) (x) = α(x) β(xj), φ() (x) = α(x) + β(xj) with softmax over {+, } only. Then as in Equations (2)-(6). Training uses the loss and expert-variation penalty from Equation (7). For theoretical clarity, we assume dense expert routing architecture in which all experts are available for each feature and gating is performed over the full set. ri,+(x) ri,(x) = tanh (cid:0)β(xj)(cid:1). Setting β(xj) = arctanh(cid:0)v(xj)/C(cid:1) yields (12) oi(x) = ri,+(x)Cu(xi)ri,(x)Cu(xi) = u(xi)v(xj). (13) Model classes. Let GAM denote generalized additive models GAM = (cid:110) (x) = ω0 + (cid:88) i=1 (cid:12) (cid:111) (cid:12) . (cid:12)fi continuous fi(xi) (10) Let GA2M denote generalized additive models with pairwise interactions (GA2M): Moreover, we can enforce the pairwise-only dependence by choosing the gating parameters so that φ() depend on xj but not on other coordinates (i.e., zeroing Aℓi for ℓ = in Equation (3)). Proof sketch. With softmax, if the logits are αβ, the probabilities are σ(2β) and σ(2β), hence their difference is tanh(β). The rest is algebra using boundedness of and the choice of to keep ri, [0, 1]. GA2M = (cid:110) (x) = ω0 + (cid:88) i=1 fi(xi)+ (cid:88) 1i<jn fij(xi, xj) (cid:12) (cid:111) (cid:12) (cid:12)fi, fij continuous . (11) Theorem 2 (GA2M containment up to arbitrary precision). For any GA2M and any ϵ > 0 there exists and an NAE such that supx ˆy(x) (x) < ϵ. In particular, for each feature it suffices to take We write NAE(K) for the set of functions represented by an NAE with experts per feature. 3.1 Exact containment of GAMs Theorem 1 (GAM containment). For any GAM there exists = 1 and parameters of an NAE such that ˆy(x) = (x) for all x. Proof. Choose = 1, set ri1(x) 1, and choose gi1 Ei = fi. Then oi(x) = fi(xi) and ˆy(x) = ω0 + (cid:80) fi(xi) = (x). Ki 1 + 2 (cid:88) Mij, j=i (14) where Mij is the number of separable terms used for fij in Lemma 1. i<j (cid:80)Mij Proof sketch. Write (x) = ω0 + (cid:80) fi(xi) + (cid:80) m=1 uijm(xi)vijm(xj)+r(x), with < ϵ/2 by Lemma 1 on each pair. Realize each fi with dedicated single expert (as in Theorem 1). For each separable uijmvijm assign it to feature head and apply Lemma 2. Summing all heads and the intercept recovers up to the residual r, giving uniform error < ϵ. Guangzhi Xiong, Sanchit Sinha, Aidong Zhang The bound on Ki counts one additive expert plus two per separable term. Thus, GAM NAE(K) (closure in uniform norm, i.e. functions uniformly approximable by NAE) whenever gates are allowed to depend on other features. Hence NAEs are more expressive than vanilla GAM/GA2M but remain feature-decomposed at prediction time."
        },
        {
            "title": "3.3 Controlling additivity via the\nexpert-variation penalty",
            "content": "A key feature of NAEs is the ability to interpolate between strictly additive and highly flexible models by tuning the expert variation penalty. To quantify this, we define an additivity metric: scenarios? (2) How does the expert variation penalty parameter λ influence the trade-off between model flexibility and additivity? Additional simulation studies on robustness to feature sparsity and increasing distributional complexity are provided in Appendix E."
        },
        {
            "title": "4.1 Simulation Setup and Visualization",
            "content": "We generate two synthetic datasets, each with 10,000 samples: Unimodal Distribution (Additive). With x1 (0, 1) and σ = 0.1, the response is sampled as: (cid:16) x1 1 + sin(4πx1), σ2(cid:17) , (16) Additivity = 1 (cid:88) i=1 Var(cid:0)E(oixi)(cid:1) + δ Var(oi) + δ , (15) Multimodal Distribution (Non-Additive). With x2 uniformly sampled from {1, 1}, the response is generated following: where oi is the contribution of feature to the prediction, and δ is small constant for numerical stability. For strictly additive model, this metric equals 1. The training objective of NAEs includes an expert variation penalty weighted by λ (Equation (7)), which encourages consistency among experts for each feature. As λ increases, the model becomes more additive: Theorem 3 (Monotone additivity and additive limit). Let A(λ) be the additivity metric of an NAE trained with penalty λ 0. Then A(λ) is nondecreasing in λ. Moreover, limλ A(λ) = 1, and any limit point of minimizers as λ is GAM, realized by setting rik constants and gik equal across for each feature. Proof sketch. The penalty forces gik(Ei(xi)) to contract toward their within-feature mean, collapsing each head to single expert in the limit; gates then become irrelevant and the model reduces to an additive form, for which the metric equals 1. Monotonicity follows from standard convex-order arguments on the withinfeature variance term. Full proofs are given in the supplement."
        },
        {
            "title": "DATA",
            "content": "In this section, we empirically evaluate NAEs on synthetic datasets specifically constructed to assess their ability to model both additive and non-additive (multimodal) data distributions. We benchmark NAEs against traditional GAMs, implemented as Neural Additive Models (NAMs) (Agarwal et al., 2021). Our experiments address two main questions: (1) Can NAEs accurately recover underlying shape functions in both unimodal (additive) and multimodal (non-additive) (cid:16) (cid:16) x1 1 x1 1 2 sin(4πx1), σ2(cid:17) 2 + sin(4πx1), σ2(cid:17) , , if x2 = 1, if x2 = 1, (17) Figure 2 compares the learned shape functions of NAM and NAE. In the unimodal setting, both models accurately recover the underlying pattern, and NAE maintains interpretability without introducing spurious complexity. In the multimodal setting, NAM produces near-linear fit, failing to capture the interaction, while NAE recovers the multimodal structure, with upper and lower expert bounds reflecting the range of feature contributions. Figure 2: Shape functions learned by NAM and NAE on simulated data. For the multimodal case, NAE captures the oscillatory structure, while NAM fails. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity Table 1: Performance comparison on benchmark datasets. Complex. denotes the explanation complexity, i.e., the number of components required to fully explain model predictions, with representing the number of features. The best-performing models in each category are highlighted. Model Interpretability Complex. Housing MIMIC-II MIMIC-III Income RMSE AUC AUC AUC Credit AUC Year MSE MLP NODE XGBoost EB2M NA2M NB2M NODE-GA2M Linear Spline EBM NAM NBM NODE-GAM NAE No No No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Black-box Models N/A N/A N/A O(n2) O(n2) O(n2) O(n2) O(n) O(n) O(n) O(n) O(n) O(n) O(n) 0.501 0.006 0.523 0.000 0.443 0.000 0.835 0.014 0.843 0.011 0.844 0.012 0.815 0.009 0.828 0.007 0.819 0.004 0.914 0.003 0.919 0.003 0.928 0.003 0.981 0.007 0.981 0.009 0.978 0. 78.48 0.56 76.21 0.12 78.53 0.09 Interaction-explaining Models 0.492 0.000 0.492 0.008 0.4780.002 0.476 0.007 0.848 0.012 0.843 0.012 0.848 0.012 0.846 0.011 0.821 0.004 0.825 0.006 0.819 0.010 0.822 0.007 0.928 0.003 0.912 0.003 0.917 0.003 0.923 0. 0.982 0.006 0.985 0.007 0.978 0.007 0.986 0.010 83.16 0.01 79.80 0.05 79.01 0.03 79.57 0.12 Feature-explaining Models 0.735 0.000 0.568 0.000 0.559 0.000 0.572 0.005 0.564 0.001 0.558 0.003 0.451 0.002 0.796 0.012 0.825 0.011 0.835 0.011 0.834 0.013 0.833 0.013 0.832 0.011 0.847 0.014 0.772 0.009 0.812 0.004 0.809 0.004 0.813 0.003 0.806 0.003 0.814 0.005 0.825 0. 0.900 0.002 0.918 0.003 0.927 0.003 0.910 0.003 0.918 0.003 0.927 0.003 0.927 0.003 0.976 0.012 0.982 0.011 0.974 0.009 0.977 0.015 0.981 0.007 0.981 0.011 0.982 0.009 88.89 0.40 85.96 0.07 85.81 0.11 85.25 0.01 85.10 0.01 85.09 0.01 78.66 0.21 4.2 Effect of Variation Penalty λ The expert variation penalty λ in NAEs controls the balance between flexibility and additivity, as shown in our theoretical analysis (Section 3). To further validate this, we train NAEs on the multimodal simulated dataset with different regularization strengths λ = 0.1, 1, 10 and visualize the learned shape functions for x1. As shown in Figure 3, small λ values allow for high flexibility, which perfectly recovers the underlying distribution and has wide gap between upper and lower bounds. As λ increases, the bounds converge, enforcing additivity of the trained model. The corresponding additivity scores (0.597, 0.709, and 1.000 for λ = 0.1, 1, 10) confirm the theoretical predictions. Figure 3: Effect of the expert variation penalty λ on the learned shape function of x1 in NAEs. As λ increases, the model transitions from flexible to strictly additive."
        },
        {
            "title": "5 EXPERIMENTS ON",
            "content": "REAL-WORLD DATA In this section, we evaluate NAEs on suite of realworld datasets, focusing on both predictive accuracy and interpretability. We describe the datasets and baseline models, analyze predictive performance and feature attributions, and investigate the trade-off between accuracy and interpretability. Additional ablation studies on the gating mechanism and number of experts are provided in Appendices G, H, and I. 5.1 Datasets and Baselines We evaluate NAEs on six widely used datasets spanning regression and classification tasks: Housing (Pace and Barry, 1997), MIMIC-II (Saeed et al., 2011), MIMIC-III (Johnson et al., 2016), Income (Blake, 1998), Credit, and Year. These datasets vary in size, feature types, and the presence of categorical variables, providing comprehensive testbed. Dataset statistics and more details are in Appendix A. We benchmark NAEs against range of baselines: linear and spline models (basic interpretability), Neural Additive Models (NAM) (Agarwal et al., 2021), Neural Basis Models (NBM) (Radenovic et al., 2022), Explainable Boosting Machine (EBM) (Lou et al., 2012; Nori et al., 2019), NODE-GAM (Chang et al., 2022), and their pairwise-interaction variants (e.g., NA2M, NB2M, NODE-GA2M). Black-box models such as Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Figure 4: Comparison of Longitude effect on house price predictions across models. Y-axis shows mean-centered feature contributions. Background bars indicate normalized data density. Blue dots show actual feature effects. Longitude-Latitude interaction learned by NAE is also shown. MLP, NODE (Popov et al., 2020), and XGBoost (Chen and Guestrin, 2016) are also included. Implementation details are in Appendices and C. firms that coastal regions positively influence house prices, while other areas in the same longitude range may have negative effect. 5.2 Predictive Performance and Feature Attribution Table 1 compares NAEs to all baselines. Metrics are reported with arrows indicating the desired direction. The results show that while traditional additive models (e.g., EBM, NAM) provide feature-level explanations, they often underperform compared to models that capture interactions or use black-box architectures. NAEs, however, match or exceed the predictive performance of these more complex models, while maintaining clear, feature-level interpretability. The Complex. column in the table refers to the number of components required for full model inspection. For interaction-level GA2M models, the prediction sums univariate functions and pairwise interaction surfaces, meaning complete inspection requires examining O(n2) components. In contrast, an NAE prediction always decomposes into exactly one scalar contribution per feature. The primary explanation thus consists of only O(n) shape plots, each displaying the features main effect and its associated interaction strength bounds. In high-dimensional settings, this reduction from O(n2) to O(n) explanation complexity offers significant interpretability advantage. Figure 4 demonstrates the interpretability of NAEs by comparing the effect of the Longitude feature on house price predictions across models. EBM, NAM, and NAE all capture similar geographic trends, with higher property values near key longitudes (e.g., -122.5 for San Francisco, -118.5 for Los Angeles). NAE further reveals wide range of possible outcomes between -120 and -119, with most data points near the lower Its Longitude-Latitude interaction plot conbound. 5.3 Controlling the Trade-off Between Accuracy and Interpretability As demonstrated by the theoretical analysis and experiments on the simulated data, NAEs provide tunable trade-off between flexibility and additivity via the expert variation penalty parameter λ. Figure 5 shows shape plots for the Longitude feature with different λ values. As λ increases, the bounds on feature effects tighten, enforcing additivity but potentially reducing flexibility. Figure 5: Shape plots of the Longitude feature learned by NAEs with different λ values. Higher λ enforces additivity and narrows the range of possible feature effects. To quantify this trade-off, we report both the additivity metric from Equation (15) and the RMSE of the trained NAEs. Additionally, we introduce tightness Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity metric to assess how closely the predicted bounds capture the estimated feature effects, reflecting the variability in the models predictions. The tightness metric is defined as follows: Tightness = 1 (cid:88) i= (cid:104) max(oixi) min(oixi) + δ upper(oixi) lower(oixi) + δ (cid:105) . (18) high tightness value indicates that the predicted values closely span the estimated bounds, while low value suggests the bounds are overly loose. Table 2: Quantitative analysis of additivity and bound tightness on the Housing dataset for various λ settings, along with corresponding performance (RMSE). λ = 0 λ = 0.01 λ = 0.1 λ = 1 λ = 10 λ = Additivity 0.522 0.000 0.537 0.000 0.562 0.012 0.639 0.000 0.897 0. 1.000 0.000 Tightness 0.860 0.000 0.895 0.018 0.953 0.011 0.988 0. 0.999 0.000 1.000 0.000 RMSE 0.451 0.003 0.450 0.002 0.451 0. 0.458 0.003 0.515 0.008 0.582 0.008 Table 2 summarizes additivity, tightness, and RMSE on the Housing dataset for different λ values. As λ increases, additivity and tightness improve, but predictive performance (RMSE) may degrade. This confirms that λ provides practical mechanism for balancing accuracy and interpretability in NAEs. In our main experiments, we use λ = 0.1 uniformly across datasets, as it offers good balance between flexibility and additivity. For practical use, we suggest starting with λ = 0.1 and increasing it if greater interpretability is needed, monitoring both validation accuracy and the additivity metric."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 Generalized Additive Models Generalized Additive Models (GAMs) are foundational in interpretable machine learning, modeling the response as sum of univariate functions for clear, feature-level attributions (Hastie, 2017). This additive structure enables visualization and understanding of individual feature effects, making GAMs valuable in transparency-critical domains. Extensions include hierarchical modeling (Pedersen et al., 2019), robust evaluation (Hegselmann et al., 2020; Chang et al., 2021), and higher-order interactions (Enouen and Liu, 2022; Tan et al., 2018; Duong et al., 2024). Recent advances such as Explainable Boosting Machines (EBMs) (Lou et al., 2012), Neural Additive Models (NAMs) (Agarwal et al., 2021), NODE-GAM (Chang et al., 2022), and ProtoNAM (Xiong et al., 2025) improve flexibility and accuracy while preserving interpretability. Further developments, including Neural Basis Models (NBMs) (Radenovic et al., 2022) and Gaussian Process Neural Additive Models (GP-NAMs) (Zhang et al., 2024), enhance parameter efficiency, scalability, and uncertainty quantification (Bouchiat et al., 2023; Thielmann et al., 2024). However, the strictly additive nature of GAMs limits their ability to capture complex, context-dependent feature interactions (Cui et al., 2020; Mueller et al., 2024; Ibrahim et al., 2023). Previous work has attempted to address this limitation by introducing models such as GA2M, where interactions are also captured (Lou et al., 2013). GA2M primarily relies on 2D interaction plots for interpretability, which can become computationally expensive and challenging to visualize as the number of features increases. Also, multiple interaction plots are often required to fully understand the effect of single feature in GA2M, which complicates interpretation. While post-hoc heterogeneity analysis methods were proposed to estimate the single feature effect in GA2M without visualizing all interactions (Goldstein et al., 2015; Gkolemis et al., 2023), their explanations are empirical variability bands based on given samples. Although such bands can be informative, they do not guarantee coverage for unseen samples. In contrast, NAEs provide architectural (model-intrinsic) bounds on feature effects defined by the minimum and maximum outputs of the learned expert functions, ensuring coverage for any context and input, including those not seen during training. Beyond GAMs, there are also post-hoc additive explanation methods like additive SHAP (Lundberg and Lee, 2017), which decompose predictions of complex models into additive contributions. However, these methods provide approximations of the underlying black-box models behavior rather than inherently interpretable architectures, and they can be computationally expensive for large models. In contrast, NAEs build additivity directly into the architecture and training objective. Because the model is defined as sum of per-feature expert contributions, the resulting explanations are exact by construction rather than approximations of more complex underlying function. Furthermore, unlike post-hoc methods that can also be applied to trained models, NAEs allow explicit control over the models during training via the expertvariation penalty λ. This parameter governs the deviation from strict additivity, enabling direct tradeGuangzhi Xiong, Sanchit Sinha, Aidong Zhang off between expressivity and interpretability. Additionally, NAE feature attributions are inherent to the forward pass, avoiding the computational overhead of estimating SHAP values for complex models."
        },
        {
            "title": "6.2 Mixture of Experts",
            "content": "The Mixture of Experts (MoE) paradigm introduces modular approach to modeling, where multiple specialized sub-models (experts) are trained and gating network dynamically selects or combines their outputs (Jacobs et al., 1991). MoE architectures have demonstrated remarkable scalability and adaptability, particularly in large-scale neural networks. Notably, sparse gating mechanisms have enabled efficient training of massive models by activating only subset of experts per input (Shazeer et al., 2017). MoE has been successfully integrated into Transformer architectures (Vaswani et al., 2017), powering advances in large language models (Artetxe et al., 2022; Jiang et al., 2024; Lepikhin et al., 2021) and multimodal systems (Lin et al., 2024; Shen et al., 2023). While MoE models excel at capturing diverse patterns and scaling to large datasets, they typically lack the interpretability of additive models. Specifically, standard MoE architectures employ experts that operate on the full input vector, with gating network combining their outputs into single prediction. While flexible, this approach sacrifices the decomposability required for interpretability. In contrast, NAEs organize experts per feature, ensuring that the final prediction remains sum of feature-wise contributions (Equation (6)) even when the gating network utilizes all features. Similarly, unlike general modular networks which focus on functional specialization without enforcing additive constraints, NAEs enforce modularization specifically at the feature level. This design, combined with our expert-variation penalty, allows NAEs to function as structured, interpretable variant of MoE that retains GAM-like additivity while offering explicit control over interaction strength. 6.3 Integrating Additive Models with Mixture of Experts Several efforts have sought to combine the interpretability of GAMs with the flexibility of MoE architectures, but important limitations remain. The Generalized Additive Models for Location, Scale, and Shape (GAMLSS) framework (Rigby and Stasinopoulos, 2005) extends GAMs to model multiple distributional parameters (e.g., mean and variance) in an additive manner, increasing flexibility but still failing to capture complex feature interactions. FlexMix (Leisch, 2004) introduces mixture model of GAMs with expert weights estimated via ExpectationMaximization, yet these weights are fixed after training and cannot adapt to new inputs, limiting expressiveness. Mixdistreg (Rugamer, 2023; Rugamer et al., 2024) and MNAM (Kim et al., 2024) propose dynamic mixture of additive models with inputdependent weights, but apply the same set of weights uniformly across all features or experts, preventing feature-specific adaptation and lacking explicit mechanisms to balance interpretability and accuracy. In summary, while prior work has made progress toward integrating interpretability and accuracy, existing methods either retain strict additivity or lack finegrained control over the trade-off between accuracy and transparency. This motivates the development of new frameworks, such as NAEs, that provide both interpretability and adaptive modeling feature-level capacity. By enabling context-dependent expert selection and feature-specific adaptation, our model bridges the gap between transparent, interpretable models and highly flexible, accurate architectures, opening new possibilities for trustworthy machine learning in complex domains."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced Neural Additive Experts (NAEs), novel framework that balances interpretability and predictive accuracy by extending additive models with mixture-of-experts architecture and dynamic gating. NAEs enable each feature to be modeled by multiple specialized networks, with gating mechanism that adaptively integrates information across features, thus relaxing strict additive constraints. Our theoretical analysis demonstrates that NAEs can recover complex, context-dependent relationships that are inaccessible to standard GAMs, while targeted regularization provides explicit control over the trade-off between flexibility and transparency. Empirical results on both synthetic and real-world datasets show that NAEs achieve competitive accuracy with state-ofthe-art models, while maintaining clear, feature-level explanations. This work provides principled and practical approach for applications where both interpretability and accuracy are essential. Acknowledgements This work is supported in part by the US National Science Foundation (NSF) and the National Institute of Health (NIH) under grants IIS-2106913, IIS2538206, IIS-2529378, CCF-2217071, CNS-2213700, and R01LM014012-01A1. Any recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NIH or NSF. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity"
        },
        {
            "title": "References",
            "content": "Agarwal, R., Melnick, L., Frosst, N., Zhang, X., Lengerich, B., Caruana, R., and Hinton, G. E. (2021). Neural additive models: Interpretable machine learning with neural nets. Advances in Neural Information Processing Systems, 34:46994711. Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., Anantharaman, G., Li, X., Chen, S., Akin, H., Baines, M., Martin, L., Zhou, X., Koura, P. S., OHoro, B., Wang, J., Zettlemoyer, L., Diab, M., Kozareva, Z., and Stoyanov, V. (2022). Efficient large scale language modeling with mixtures of experts. In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1169911732, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Biran, O. and Cotton, C. (2017). Explanation and justification in machine learning: survey. In IJCAI17 workshop on explainable AI (XAI), volume 8, pages 813. Blake, C. (1998). Uci repository of machine learning databases. http://www. ics. uci. edu/ mlearn/MLRepository. html. Bouchiat, K., Immer, A., Y`eche, H., Ratsch, G., and Fortuin, V. (2023). Improving neural additive models with bayesian principles. Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., and Elhadad, N. (2015). Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 17211730. Chang, C.-H., Caruana, R., and Goldenberg, A. (2022). NODE-GAM: Neural generalized additive model for interpretable deep learning. In International Conference on Learning Representations. Chang, C.-H., Tan, S., Lengerich, B., Goldenberg, A., and Caruana, R. (2021). How interpretable and trustworthy are gams? In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages 95105. Chen, T. and Guestrin, C. (2016). Xgboost: scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785794. Christoph, M. (2020). Interpretable machine learning: guide for making black box models explainable. Cui, Z., Fritz, B. A., King, C. R., Avidan, M. S., and Chen, Y. (2020). factored generalized additive model for clinical decision support in the operating In AMIA Annual Symposium Proceedings, room. volume 2019, page 343. Duong, V., Wu, Q., Zhou, Z., Zhao, H., Luo, C., Zavesky, E., Yao, H., and Shao, H. (2024). Cat: InterIn pretable concept-based taylor additive models. Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 723734. Enouen, J. and Liu, Y. (2022). Sparse interaction additive networks via feature interaction detection and sparse selection. Advances in Neural Information Processing Systems, 35:1390813920. Gkolemis, V., Dalamagas, T., Ntoutsi, E., and Diou, C. (2023). Rhale: Robust and heterogeneity-aware accumulated local effects. In ECAI, pages 859866. Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. journal of Computational and Graphical Statistics, 24(1):4465. Hastie, T. J. (2017). Generalized additive models. In Statistical models in S, pages 249307. Routledge. Hegselmann, S., Volkert, T., Ohlenburg, H., Gottschalk, A., Dugas, M., and Ertmer, C. (2020). An evaluation of the doctor-interpretability of generalized additive models with interactions. In Machine Learning for Healthcare Conference, pages 46 79. PMLR. Ibrahim, S., Afriat, G., Behdin, K., and Mazumder, R. (2023). Grand-slamininterpretable additive modeling with structural constraints. Advances in Neural Information Processing Systems, 36:6115861186. Ibrahim, S., Radchenko, P., Ben-David, E., and Mazumder, R. (2025). Predicting census survey response rates with parsimonious additive models and structured interactions. The Annals of Applied Statistics, 19(1):94120. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures of local experts. Neural computation, 3(1):7987. Jang, E., Gu, S., and Poole, B. (2016). Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2024). Mixtral of experts. arXiv preprint arXiv:2401.04088. Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L., and Mark, R. G. (2016). Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19. Kim, Y. K., Di Martino, J. M., and Sapiro, G. (2024). Generalizing neural additive models via statistical multimodal analysis. Transactions on Machine Learning Research. Leisch, F. (2004). Flexmix: general framework for finite mixture models and latent glass regression in r. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. (2021). {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations. Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Zhang, J., Ning, M., and Yuan, L. (2024). Moellava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947. Loshchilov, I. and Hutter, F. (2018). Decoupled weight decay regularization. In International Conference on Learning Representations. Lou, Y., Caruana, R., and Gehrke, J. (2012). Intelligible models for classification and regression. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150158. Lou, Y., Caruana, R., Gehrke, J., and Hooker, G. (2013). Accurate intelligible models with pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 623631. Lundberg, S. M. and Lee, S.-I. (2017). unified approach to interpreting model predictions. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. McIntosh, C. N. (2025). Generalized additive logit models for clinical prediction. Blood Advances, 9(4):935936. Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267:138. Mueller, A., Siems, J., Nori, H., Salinas, D., Zela, A., Caruana, R., and Hutter, F. (2024). Gamformer: In-context learning for generalized additive models. arXiv preprint arXiv:2410.04560. Nori, H., Jenkins, S., Koch, P., and Caruana, R. Interpretml: unified framework for arXiv preprint (2019). machine learning interpretability. arXiv:1909.09223. Pace, R. K. and Barry, R. (1997). Sparse spatial Statistics & Probability Letters, autoregressions. 33(3):291297. Pedersen, E. J., Miller, D. L., Simpson, G. L., and Ross, N. (2019). Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ, 7:e6876. Popov, S., Morozov, S., and Babenko, A. (2020). Neural oblivious decision ensembles for deep learning on tabular data. In International Conference on Learning Representations. Radenovic, F., Dubey, A., and Mahajan, D. (2022). Neural basis models for interpretability. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural Information Processing Systems, volume 35, pages 8414 8426. Curran Associates, Inc. Rigby, R. A. and Stasinopoulos, D. M. (2005). Generalized additive models for location, scale and shape. Journal of the Royal Statistical Society Series C: Applied Statistics, 54(3):507554. Rugamer, D. (2023). mixdistreg: An package for fitting mixture of experts distributional regression with adaptive first-order methods. arXiv preprint arXiv:2302.02043. Rugamer, D., Pfisterer, F., Bischl, B., and Grun, B. (2024). Mixture of experts distributional regression: implementation using robust estimation with adaptive first-order methods. AStA Advances in Statistical Analysis, 108(2):351373. Ruppert, D. (2004). The elements of statistical learning: data mining, inference, and prediction. Saeed, M., Villarroel, M., Reisner, A. T., Clifford, G., Lehman, L.-W., Moody, G., Heldt, T., Kyaw, T. H., Moody, B., and Mark, R. G. (2011). Multiparameter intelligent monitoring in intensive care ii (mimic-ii): public-access intensive care unit database. Critical care medicine, 39(5):952. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations. Shen, S., Yao, Z., Li, C., Darrell, T., Keutzer, K., and He, Y. (2023). Scaling vision-language models with sparse mixture of experts. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1132911344. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity (c) clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes] (d) description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include: (a) Citations of the creator If your work uses existing assets. [Yes] (b) The license information of the assets, if applicable. [Yes] (c) New assets either in the supplemental mate- (d) Information rial or as URL, if applicable. [Yes] about providers/curators. [Not Applicable] consent from data (e) Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable] 5. If you used crowdsourcing or conducted research with human subjects, check if you include: (a) The full text of instructions given to participants and screenshots. [Not Applicable] (b) Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable] (c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable] Tan, S., Caruana, R., Hooker, G., Koch, P., and Gordo, A. (2018). Learning global additive explanations for neural nets using model distillation. stat, 1050:3. Thielmann, A. F., Kruse, R.-M., Kneib, T., and Safken, B. (2024). Neural additive models for location scale and shape: framework for interpretable neural regression beyond the mean. In International Conference on Artificial Intelligence and Statistics, pages 17831791. PMLR. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, (cid:32)L., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. Xiong, G., Sinha, S., and Zhang, A. (2025). Protonam: Prototypical neural additive models for interpretable deep tabular learning. ACM Trans. Knowl. Discov. Data, 19(9). Zhang, W., Barr, B., and Paisley, J. (2024). Gaussian process neural additive models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(15):1686516872."
        },
        {
            "title": "Checklist",
            "content": "1. For all models and algorithms presented, check if you include: (a) clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes] (b) An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes] (c) (Optional) Anonymized source code, with including specification of all dependencies, external libraries. [Yes] 2. For any theoretical claim, check if you include: (a) Statements of the full set of assumptions of all theoretical results. [Yes] (b) Complete proofs of all theoretical results. [Yes] (c) Clear explanations of any assumptions. [Yes] 3. For all figures and tables that present empirical results, check if you include: (a) The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as URL). [Yes] (b) All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes] Context-Gated Experts for Controllable Model Additivity: Supplementary Materials"
        },
        {
            "title": "A DATASET DESCRIPTION",
            "content": "Housing Dataset The California Housing dataset, cited from Pace and Barry (1997), encompasses regression It comprises 20,640 instances, each task based on median housing prices across Californias census blocks. characterized by eight distinct attributes. MIMIC-II Dataset The MIMIC-II (Multiparameter Intelligent Monitoring in Intensive Care) dataset, as described by Saeed et al. (2011), facilitates binary classification task aimed at predicting mortality in intensive care units (ICUs). It contains 17 attributes, including seven categorical variables. MIMIC-III Dataset MIMIC-III is more extensive and detailed iteration of the MIMIC database referenced from Johnson et al. (2016). In our study, the same setting in NODE-GAM (Chang et al., 2022) is adopted, wherein categorical variables have been transformed into dummy variables for enhanced analysis. Income Dataset Originating from the UCI Machine Learning Repository (Blake, 1998), the Income dataset underpins binary classification task. Its objective is to predict whether an individual earns more than $50,000 annually. Credit Dataset The Credit dataset1 provides samples for binary classification task on transaction fraud detection. It contains 30 anonymized features including 28 coefficients of PCA components. In Credit, 492 out of 284,807 transactions are labeled as frauds. Year Dataset The Year dataset2 contains data for regression task, which uses the audio features to predict the release year of song. It includes 515,345 samples with 90 features. The statistics of each dataset can be found in Table 3. Table 3: Statistics of real-world datasets used in model evaluation. Dataset Task #Instances #Features #Categorical Features #Classes Housing MIMIC-II MIMIC-III Income Credit Year regression classification classification classification classification regression 20,640 24,508 27,348 32,561 284,807 515,345 8 17 57 14 30 90 0 7 0 8 0 0 2 2 2"
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "For comprehensive and equitable evaluation, we employed the officially released codes for baseline models including NODE, NODE-GAM/NODE-GA2M, EBM/EB2M, and NBM/NB2M. We adopted the PyTorch implementation of NAM/NA2M by NBM, which has been benchmarked against their model, for direct comparison 1The Credit dataset can be downloaded at https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud. 2The Year dataset can be downloaded at https://archive.ics.uci.edu/dataset/203/yearpredictionmsd Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity with our proposed models. In developing NAEs, we constructed multi-channel, fully connected network capable of encoding multiple features independently in parallel manner. For fair comparison with existing methods, our experiments employ the processed version of MIMIC-II, MIMICIII, Income, and Credit from Chang et al. (2022), where the datasets are split into five parts for five-fold cross-validation. The Housing dataset and the Year dataset are split into the training, validation, and test sets following setups of previous research (Chang et al., 2022; Radenovic et al., 2022), and each of them is tested with 10 different seeds to examine the variation of predictions. Consistent with previous studies (Chang et al., 2022; Popov et al., 2020; Radenovic et al., 2022), we applied the same quantile transformation to the features. To be consistent with existing neural network-based GAMs (Agarwal et al., 2021; Radenovic et al., 2022), the feature encoders f1, , fn in NAE are implemented with MLPs. The expert predictors g11, , gnK are implemented as one linear layer in our experiments. During training, we decrease the learning rate with cosine annealing following NBM (Radenovic et al., 2022). AdamW optimizer (Loshchilov and Hutter, 2018) is used to optimize the training objective. All experiments were run on NVIDIA A100 GPUs (40 GB / 80 GB)."
        },
        {
            "title": "C HYPERPARAMETERS",
            "content": "Table 4: Hyperparameters for NAE on all datasets. Hyperparameter Housing MIMIC-II MIMIC-III Income #layers Hidden dimension #total experts #activated experts Batch size Max iteration Learning rate Weight decay Dropout Dropout expert Output Penalty Variation Penalty Normalization 4 128 4 4 2048 1000 5.97e-4 5.29e-5 0.1 0.2 1.97e-5 0.1 4 128 4 4 2048 1000 1.97e-4 8.06e-4 0.0 0.4 3.49e-8 0.1 4 128 4 4 1024 500 1.68e-4 2.78e-4 0.1 0.2 4.86e-5 0.1 4 128 4 4 2048 1000 1.11e-4 4.25e-3 0.0 0.5 3.99e-1 0.1 Credit 4 128 4 4 2048 150 4.00e-5 1.88e-3 0.3 0.2 5.07e-4 0. Year 4 128 4 4 512 75 1.17e-4 2.28e-6 0.1 0.6 1.45e-4 0.1 layer norm layer norm layer norm layer norm batch norm layer norm We perform random search over hyperparameters in the following ranges: #layers: the number of layers in each feature encoder, sampled from {3, 4}. Hidden dimension: the number of nodes in each layer of the feature encoder, sampled from {64, 128}. #total experts: the number of total experts for each feature, sampled from {4}. #activated experts: the number of activated experts for each feature, sampled from {4}. Batch size: the number of samples in one batch during training, sampled from {512, 1024, 2048}. Max iteration: the number of iterations for training, sampled from {75, 150, 500, 1000}. Learning rate: the speed of gradient descent, sampled from [1e-6, 1e-1]. Weight decay: the coefficient for the L2 normalization on parameters, sampled from [1e-8, 1e-1]. Dropout: the probability of parameter in feature encoders being replaced as 0 during training, sampled from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. Dropout expert: the probability of an experts output being replaced as 0 during training, sampled from {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}. Output penalty: the coefficient for the L2 normalization on the outputs of features for reducing unimportant ones, sampled from [1e-8, 1e-1]. Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Variation penalty: the weight for the expert variation penalty, sampled from {0.1}. Normalization: normalization methods used in the network, sampled from {batch norm, layer norm}. The best hyperparameters we found for NAE are shown in Tables 4. PROOFS FOR SECTION 3 Throughout, each feature domain Xi is compact and metrizable, hence Xi, Xi Xj are compact Hausdorff. All functions are real-valued and continuous unless noted. We use C(Y ) for the Banach space of continuous real-valued functions on compact space with the sup norm . Lemma 3 (Restatement of Lemma 1). Let fij C(Xi Xj). For any ε > 0 there exist and functions {um C(Xi), vm C(Xj)}M m=1 such that (cid:13) (cid:13)fij (cid:88) m=1 um() vm()(cid:13) (cid:13) < ε. Proof. Consider the set = (cid:110) (cid:88) m=1 um(xi) vm(xj) : N, um C(Xi), vm C(Xj) (cid:111) . is subalgebra of C(Xi Xj): it is closed under pointwise addition and multiplication and contains constants (take 1, 1). It separates points: given (xi, xj) = (x i, choose C(Xi) with u(xi) = u(x j, choose analogously and 1. By StoneWeierstrass, the uniform closure of is C(Xi Xj). Thus every fij can be approximated arbitrarily well by finite sums of products umvm. i) and 1. If xj = j), either xi = j. If xi = or xj = i, Lemma 4 (Restatement of Lemma 2). Let C(Xi) and C(Xj) be bounded. Choose any constant > v. Define two experts (for feature i) and two-class softmax gate with logits gi,+ Ei(xi) = +C u(xi), gi, Ei(xi) = u(xi), ϕ(+) (x) = α(x) β(xj), ϕ() (x) = α(x) + β(xj). Proof. With two-class softmax, if the logits are α β, the probabilities are ri,+(x) = 1 1 + e2β(xj ) , ri,(x) = 1 1 + e2β(xj ) , so ri,+(x) ri,(x) = tanh(β(xj)). Setting β(xj) = arctanh(v(xj)/C) yields ri,+(x) ri,(x) = v(xj)/C. Then oi(x) = ri,+(x) u(xi) + ri,(x) (C u(xi)) = u(xi) v(xj). The dependence on xj only is enforced by letting α be constant and β function of xj alone. Theorem 4 (Restatement of Theorem 1). For any GAM, there exists an NAE with = 1 per feature such that ˆy(x) = (x) for all x. Proof. Set = 1, ri1(x) 1, choose gi1 Ei = fi and ω0 to match the intercept. Then ˆy(x) = (x). Theorem 5 (Restatement of Theorem 2). Let GA2M. For any ϵ > 0, there exists an NAE with context-only gating and finite experts {Ki} such that ˆy < ϵ. Moreover, one may take where Mij is the number of separable terms used in Lemma 1. Ki 1 + 2 (cid:88) j=i Mij, Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity i<j fij(xi, xj). Approximate each fij by (cid:80) Proof. Write (x) = ω0 + (cid:80) uijm(xi)vijm(xj) with error < ϵ/(2(cid:0)n (cid:1)). Realize each fi via single expert with constant gate. Realize each separable product uijm(xi)vijm(xj) using Lemma 2. Summing all contributions gives ˆy within ϵ. Counting experts yields the bound on Ki. fi(xi) + (cid:80) 2 Theorem 6 (Restatement of Theorem 3). Let θλ minimize the penalized empirical objective in Equation (7): LN (θ; λ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) t=1 L(cid:0)yt, ˆyθ(xt)(cid:1) + λ nN (cid:88) (cid:88) (cid:88) (cid:16) t=1 i=1 k=1 oik(xt) 1 oiℓ(xt) (cid:17)2 . (cid:88) ℓ=1 Then the within-feature dispersion Pen(θ) := 1 is nonincreasing in λ, and t,i,k Pen(θλ) 0 as λ . Consequently, any limit point of ˆyθλ is additive; moreover the additivity metric A(θλ) from Equation (15) satisfies A(θλ) 1. nN (cid:0)oik(xt) 1 (cid:80) ℓ oiℓ(xt)(cid:1)2 (cid:80) Proof. (1) Monotonicity in λ. Fix 0 λ1 < λ2 and let θλ1, θλ2 be minimizers for λ1, λ2. By optimality, (cid:88) 1 (cid:124) L(yt, ˆyθλ (xt)) + λ2 Pen(θλ2) 1 (cid:123)(cid:122) LN (θλ2 ;λ2) (cid:125) (cid:124) (cid:88) L(yt, ˆyθλ1 , (xt)) + λ2 Pen(θλ1) (cid:123)(cid:122) LN (θλ1 ;λ2) (cid:125) and LN (θλ1; λ1) LN (θλ2 ; λ1) = 1 (cid:88) L(yt, ˆyθλ2 (xt)) + λ1 Pen(θλ2 ). Subtracting the second inequality from the first yields (λ2 λ1)Pen(θλ2 ) (λ2 λ1)Pen(θλ1 ), so Pen(θλ) is nonincreasing. (2) Pen(θλ) 0. Take zero-penalty competitor θ that implements GAM with = 1 (or > 1 with gik gi so all oik are identical); this is feasible by Theorem 1 and Equations (2) - (6). Then for every λ, LN (θλ; λ) LN (θ; λ) = 1 L(cid:0)yt, ˆyθ(xt)(cid:1) =: C. (cid:88) Because the loss is nonnegative for the tasks considered (e.g., MSE, cross-entropy), we have λ Pen(θλ) C, hence Pen(θλ) C/λ 0. (3) Collapse within each feature head at training points. Fix any feature and training example xt. By definition of Pen and Step (2), 1 (cid:88) k=1 (cid:0)oik(xt) oi(xt)(cid:1)2 0 where oi(xt) = 1 (cid:88) ℓ=1 oiℓ(xt). Thus the vector (cid:0)oi1(xt), . . . , oiK(xt)(cid:1) becomes constant in k: oik(xt) qi(xt) for all k. Since the feature contribution is oi(xt) = (cid:80) oi(xt) (cid:80) rik(xt) qi(xt) = qi(xt). Define hi(xi,t) := qi(xt); then for each (i, t) there exists scalar qi(xt) with rik(xt) oik(xt) (Equation (5)), we get ˆyθλ(xt) = ω0 + (cid:88) i=1 oi(xt) = ω 0 + (cid:88) i=1 hi(xi,t), which is additive on the training set. (4) Additivity in the limit and the metric A(λ). At any exact zero-penalty parameter setting, oik(xt) is identical across for every (i, t), so setting gik gi and any (e.g., constant) softmax weights rik yields GAM realization with the same predictions (Equations (2) - (6)). For the additivity metric in Equation (15), when each oi depends only on xi, we have Var(E[oi xi]) = Var(oi), hence A(θλ) 1 as λ . Guangzhi Xiong, Sanchit Sinha, Aidong Zhang"
        },
        {
            "title": "E MORE SIMULATION STUDIES",
            "content": "E."
        },
        {
            "title": "Impact of Feature Sparsity",
            "content": "To assess robustness to imbalanced feature distributions, we simulate datasets where the proportion of instances with x2 = 1 is set to 50%, 25%, 5%, and 1%. Figure 6 shows shape plots for traditional GAMs and NAEs with increasing numbers of experts (K = 2, 8, 32). Figure 6: Shape plots of GAM and NAEs on simulated data with varying feature sparsity for x2. NAEs recover minority signals even at high sparsity, while GAM overfits the majority class. As sparsity increases, traditional GAMs overfit the majority class and miss minority patterns. NAEs, with more experts, consistently recover the multimodal structure, demonstrating robustness to feature imbalance. E.2 Impact of Distribution Modality We further test NAEs on data with increasing numbers of modes by introducing multiple categorical features: = ε + x1 1 2 + 1 CF CF +1 (cid:88) i= xi sin(4πx1), (19) where ε (0, 0.012), x1 (0, 1), and each xi (i 2) is uniformly sampled from {1, +1}. We vary the number of categorical features (CF = 1, 3, 5, 7). Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity Figure 7: Shape plots of NAEs on simulated data with increasing numbers of categorical features. The model adapts to capture the growing complexity of the underlying function. Figure 7 shows that NAEs flexibly capture the increasing complexity of the shape function as the number of modes grows, highlighting the models scalability and adaptability to complex, multimodal distributions. E.3 Impact of Correlated Features To examine the effect of feature correlation, we simulate data where x2 is correlated with x1. Based on the multimodal setup in Equation (17), we introduce correlation between x1 and x2 by setting x2 as binary variable {1, 1} with conditional probability (x2 = 1x1) = ρx1 + (1 ρ)/2, (20) where ρ {0, 0.5, 0.9} controls the correlation strength. The first feature x1 is drawn uniformly from [0, 1], and the response is generated as = x2 sin(4πx1) + x2 + ε. (21) The comparison results (RMSE, lower is better) between NAE and NAM are shown in Table 5. As shown in the table, NAE maintains low RMSE across all correlation levels, whereas NAM performs significantly worse and only slightly improves with increased correlation. This demonstrates the robustness of NAE to correlated inputs. Table 5: Performance comparison under different correlation levels. Model ρ = 0. ρ = 0.5 ρ = 0.9 NAE 0.1066 NAM 0.7112 0.1068 0.6829 0.1090 0.6088 E. Impact of Generic Interactions To evaluate the ability of NAE to capture generic interactions, we simulate data with complex interaction structure, where x1, x2 nif orm[1, 1] and the response is generated as = 2 sin(πx1) cos(πx2) + 0.5x2 1 + 0.5x2 2 + ε. (22) This target includes multiplicative interaction term that cannot be decomposed additively. With the same evaluation settings as in previous simulations, we test both NAE and NAM on this data. The results show that NAE achieves low RMSE of 0.1306, closely approximating the true function, while NAM fails to capture the interaction and yields much higher RMSE of 1.0114. This illustrates that NAE can effectively learn complex interactions that are not strictly additive, as predicted by Theorem 2. Guangzhi Xiong, Sanchit Sinha, Aidong Zhang COMPARISON BETWEEN NAE AND GA2M As demonstrated in Section 5, NAE provides both individual feature attributions and visualizations of feature interactions. To compare with GA2M, which emphasizes feature interactions, we conducted additional experiments using EB2M. Specifically, we obtained single-feature attributions by summing all interaction terms associated with each feature. For fair comparison, we visualized the actual effect of each feature value, including the interaction effects, and plotted the lower and upper bounds (i.e., the minimum and maximum effect of feature value across all captured interactions). Figure 8: Comparison of NAE and GA2M visualizations for the Longitude feature effect and the LongitudeLatitude interaction on house prices. As shown in Figure 8, while the interaction interpretation by GA2M is similar to NAE, the single-feature attributions from EB2M yield bounds that are too loose to be interpretable, suggesting high variability in predictions, especially for out-of-distribution inputs. Also, this approach to visualizing individual feature attributions is not applicable to NA2M, as it does not provide strict output ranges. Moreover, key advantage of NAE is its ability to control model additivity, allowing smooth transition from complex interaction model to strictly additive one, which is flexibility not available in GA2M. Additionally, NAE is not limited to capturing pairwise interactions. For example, one can modify the relevance estimation in Equation (3) to keep the interaction terms among three features. Figure 9 focuses on the Los Angeles area, demonstrating the effect of AveRooms on the price prediction beyond the Latitude-Longitude interaction. Interestingly, the south-western part of the area is more robust to the AveRooms change compared to other parts, revealing complex three-way interaction that GA2M fails to capture. NAE-D: NAE WITH DIAGONAL ROUTING MATRIX We explored special version of NAE, termed NAE-D, where the scoring matrices compose block-diagonal matrix. In NAE-D, the matrix Aij in Equation (3) becomes zero matrix if = j. This configuration allows NAE-D to function similarly to GAMs, where the relevance estimation of features experts is solely based on its own encoded information. Since the relevance for experts of each feature is solely determined by itself in NAE-D, the estimated relevance scores will be fixed for the same feature value. To encourage the model to learn various distributions with multiple experts, we introduce randomness into the decision-making process by employing the Gumbel-softmax technique (Jang et al., 2016) with the temperature τ = 0.1 to re-sample the experts during training based on their estimated relevance scores in Equation (4): ˆrik = exp (cid:16) log(rik) + gi τ (cid:17)(cid:30) (cid:88) l=1 exp (cid:16) log(ril) + gl τ (cid:17) , where gi, gl Gumbel(0, 1). (23) (24) Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity Figure 9: Comparison of NAE and GA2M visualizations for the AveRooms-Latitude-Longitude interaction on house prices. The performance comparison of NAE-D with other baselines is presented in Table 6. The results demonstrate that, by regularizing the scoring matrix to be diagonal, NAE-D shows performance close to traditional additive models, which have worse accuracies than complex models that capture feature interactions. Table 6: Comparison of NAE-D to other baselines on benchmark datasets. Complex. denotes the explanation complexity, i.e., the number of components required to fully explain model predictions, with representing the number of features. Model Interpretability Complex. Housing MIMIC-II MIMIC-III Income RMSE AUC AUC AUC Credit AUC Year MSE Black-box Models MLP XGBoost EB2M NA2M EBM NAM NAE-D No No Yes Yes Yes Yes Yes N/A N/A 0.501 0.006 0.443 0.000 0.835 0.014 0.844 0.012 0.815 0.009 0.819 0.004 0.914 0.003 0.928 0.003 0.981 0.007 0.978 0.009 78.48 0.56 78.53 0. Interaction-explaining Models O(n2) O(n2) O(n) O(n) O(n) 0.492 0.000 0.492 0.008 0.848 0.012 0.843 0.012 0.821 0.004 0.825 0. 0.928 0.003 0.912 0.003 0.982 0.006 0.985 0.007 83.16 0.01 79.80 0.05 Feature-explaining Models 0.559 0.000 0.572 0.005 0.553 0.001 0.835 0.011 0.834 0.013 0.830 0. 0.809 0.004 0.813 0.003 0.805 0.006 0.927 0.003 0.910 0.003 0.927 0.003 0.974 0.009 0.977 0.015 0.977 0.010 85.81 0.11 85.25 0.01 85.60 0.04 Figure 10 shows the shape plots generated by NAE-D compared to other baseline additive models. As discussed above, NAE-D is strictly additive model where the effect of each feature on the final output is determined Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Figure 10: Visualization of the Longitude feature impact on house prices by NAE-D. by the feature itself. Thus, we plot NAE-D in the same way as we did for EBM and NAM, directly showing the estimated contribution given by each feature value. Moreover, since NAE-D is trained under the general NAE framework with multiple experts learned for each feature, we plot another figure for NAE-D including the estimated upper and lower bounds based on the learned experts. It can be observed from Figure 10 that the patterns captured by NAE-D are similar to the ones by EBM and NAM. Beyond the point estimation provided by traditional additive models, NAE-D offers additional insights with its prediction bounds. In areas with low data density, the plot displays wide gap between the upper and lower bounds, highlighting variability in predictions where less data is available. NAE-E: NAE WITH EVENLY DISTRIBUTED EXPERT ACTIVATION While the activation strategy detailed in Section 2.2 facilitates continuous expert relevance estimation, we propose an adaptation that shifts this estimation to discrete framework. This modification involves adjusting the weights to be evenly distributed across relevant experts, leading to modified model termed NAE-E. To implement this, we use the masking vector outlined in Equation (3) and modify the relevance computation in Equation (4) as follows: rik = exp(φi[k] φ l=1 exp(φi[l] φ (cid:80)K [k] + Mi[k]) [l] + Mi[l]) . (25) φ mirrors the values of φi but does not require gradient computations. The activation strategy implemented by Equation (25) will result in finite set of possible outcomes for each input feature value. Given configuration of experts and activated ones per feature, the predicted output by NAE-E for any given feature will be one of the possible selections from K! C!(KC)! combinations. Table 7 shows the performance (RMSE) of NAE-E on the Housing dataset with different variation penalties, along with their corresponding shape plots of the Longitude feature. All the tested NAE-E models have configuration of = 32 and = 16. In the plots, we illustrate the upper and lower bounds for each feature by selecting the maximum and minimum values from the C!(KC)! possible combinations. K! Compared with NAE  (Table 2)  , the performance of NAE-E is slightly worse due to the limitations imposed by its discrete range. However, even without any variation penalty, the estimated bounds by NAE-E still fit the actual score distribution tightly, accurately reflecting the prediction limits."
        },
        {
            "title": "I PERFORMANCE OF NAE WITH THE SCALING OF EXPERTS",
            "content": "As NAE employs mixture of experts for data modeling, we explore how the number of total experts (K) and the number of activated experts (C) affect the overall model performance. Table 8 presents the performance of both NAE and NAE-E on the Housing dataset under various configurations. The results indicate that NAE reaches optimal performance at = 8 and = 4, and further increases in or do not enhance its performance. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity Table 7: Performance (RMSE) and explanations of NAE-E with different variation penalties. λ = 0 λ = 0.1 λ = λ = 10 λ = 100 0.462 0.002 0.460 0.002 RMSE () 0.483 0.001 Shape Plot 0.551 0.002 0.585 0.003 Such finding is consistent with the role of experts in NAE, which primarily explore the variability in the prediction space to learn the upper and lower bounds, as the dynamic routing mechanism offers continuous range within the bounds for prediction. In contrast, NAE-E benefits from an increased number of and C, showing improved performance as these parameters grow. This difference shows how the continuous expert activation in NAE contrasts with the discrete one in NAE-E, highlighting the flexibility of our NAE framework to adapt to various scenarios through different implementations. Table 8: Results for different numbers of activated experts (C) and all experts (K). The top three scores of each model are highlighted. 4 8 16 NAE 2 4 8 16 0.479 0.003 0.459 0.003 0.451 0.002 0.455 0.004 0.447 0.004 0.449 0.004 0.460 0.004 0.455 0.004 0.456 0.003 0.456 0.005 16 64 128 NAE-E 2 4 8 16 0.486 0.007 0.469 0.007 0.470 0.003 0.587 0.003 0.487 0.005 0.466 0.005 0.459 0.004 0.462 0. 0.486 0.006 0.468 0.004 0.458 0.002 0.451 0.002 0.489 0.005 0.470 0.004 0.458 0.004 0.453 0.002 To examine the interplay between the number of experts and the variation penalty, we conduct grid search on the Housing dataset and report the RMSE scores in Table 9. The results indicate that increasing does not always lead to improved performance, and higher values of require larger values of λ for regularization to prevent overfitting. This suggests that while more experts can capture more complex patterns, they also increase the risk of overfitting, necessitating stronger regularization to maintain generalization performance. Table 9: Effect of λ for different values of K. λ 0.1 1 10 2 4 8 16 0.4805 0.4498 0.4507 0.4662 0.4831 0.4498 0.4470 0. 0.4989 0.4577 0.4460 0.4464 0.5771 0.5068 0.4765 0.4509 Guangzhi Xiong, Sanchit Sinha, Aidong Zhang"
        },
        {
            "title": "J DISCUSSION ON MODEL COMPLEXITY",
            "content": "As NAE is fundamental framework aiming to extend generalized additive models, our design prioritizes the performance and comprehensiveness of the overall framework over the optimization of model efficiency. On the basis of Neural Additive Models (NAMs), the neural-network-based Generalized Additive Models (GAMs), NAE introduces more parameters in the expert encoding and dynamic routing steps. To rigorously analyze the additional cost incurred by NAE compared to NAM, suppose we have features, d-dimensional output for feature encoding, and total experts for each feature. In our experiments, NAE is implemented with = 128 and = 4. For NAE-D discussed in Appendix G, we have = 128 with = 64. The theoretical and empirical additional costs brought by NAE and its variant are presented in Table 10, including the increase in parameter count and the additional memory required (assuming float32 storage). Table 10: Theoretical and empirical additional costs brought by NAE compared to NAM. NAE is implemented with = 128, = 4. NAE-D is implemented with = 128, = 64. denotes the number of input features. 8 17 57 14 30 90 Housing MIMIC-II MIMIC-III Income Credit Year Theoretical NAE NAE-D Parameter Count Memory Usage Parameter Count Memory Usage 37k 157k 1.7M 108k 476k 4.2M 144k 613k 6.5M 420k 1.8M 16.0M 132k 281k 941k 231k 495k 1.5M 516k 1.1M 3.59M 903k 1.89M 5.67M nK[(n + 1)d + 2] nK(2d + 2) The results indicate that the additional cost associated with NAE-D scales linearly with the number of features, whereas the cost for NAE includes squared term due to its more complex routing mechanism. Specifically, NAE utilizes full block matrix to encode feature interactions, which is simplified as block-diagonal matrix in NAE-D. Despite this increased complexity, the additional memory usage remains manageable with current computing resources. Future efforts could be made to sparsify the routing matrix in NAE, which could potentially reduce the additional cost while retaining the model performance. Additionally, we conducted experiments to measure actual training time (in seconds) / memory usage (in MB) across varying numbers of features (n) and experts (K) for NAE. Table 11 summarizes the results with 5000 training samples and 100 training epochs, using the encoder architecture from Table 4 with four layers, each containing 128 neurons. Table 11: Training time / memory usage of NAE with different and values. = 32 = = 128 = 256 = 512 = 4 = 8 = 16 = 32 174.0s / 281.2MB 64.4s / 76.6MB 39.7s / 2.6MB 39.8s / 7.2MB 102.3s / 140.8MB 238.0s / 537.7MB 40.4s / 3.6MB 42.6s / 11.2MB 40.8s / 5.7MB 42.3s / 19.3MB 150.1s / 269.4MB 346.7s / 1050.7MB 42.9s / 9.8MB 89.6s / 35.6MB 111.7s / 135.2MB 277.5s / 526.4MB 559.0s / 2076.8MB 42.8s / 22.3MB 47.6s / 38.4MB 82.9s / 70.7MB The results validate the theoretical analysis in Table 10, showing that NAEs training time and memory usage increase with both the number of features and experts. For comparison, we also measured end-to-end training times and memory usage for NA2M, EB2M, NAM, and EBM, comparing them with NAE when = 4, which is the setting used in our main experiments. As the results in Table 12 show, while NAE preserves the capability to model interactions, it presents computational cost that is lower than NA2M and EB2M, especially when the number of features is large. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity Table 12: Training time and memory usage of NA2M, EB2M, NAM, and EBM for varying numbers of features (n). indicates experiments that exceeded one hour to complete. = 32 = 64 = = 256 = 512 NA2M EB2M NAE (K = 4) NAM EBM 70.9s / 24.2MB 290.7s / 99.0MB 1146.1s / 402.9MB 4.5s / 2.2MB 39.7s / 2.6MB 6.3s / 1.6MB 5.7s / 1.1MB 10.5s / 6.6MB 39.8s / 7.2MB 10.4s / 3.1MB 9.4s / 2.1MB 34.4s / 22.5MB 42.8s / 22.3MB 17.2s / 6.3MB 18.0s / 3.7MB 155.4s / 82.2MB 1004.3s / 313.2MB 174.0s / 281.2MB 64.4s / 76.6MB 66.0s / 26.8MB 33.0s / 12.9MB 95.8s / 5.3MB 37.6s / 4.5MB MORE VISUALIZATION RESULTS OF NAE ON REAL-WORLD DATA Figures 11 - 14 show the complete visualization results of feature explanations by NAE on different real-world datasets. Here we present results for datasets with no more than 50 features, including Housing, MIMIC-II, Income, and Credit. Figure 11: Visualization of feature explanations by NAE on the Housing dataset. Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Figure 12: Visualization of feature explanations by NAE on the MIMIC-II dataset. Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity Figure 13: Visualization of feature explanations by NAE on the Income dataset. Guangzhi Xiong, Sanchit Sinha, Aidong Zhang Figure 14: Visualization of feature explanations by NAE on the Credit dataset."
        }
    ],
    "affiliations": []
}