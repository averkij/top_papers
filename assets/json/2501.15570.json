{
    "paper_title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer",
    "authors": [
        "Lin Yueyu",
        "Li Zhiyuan",
        "Peter Yue",
        "Liu Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at \\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside}, \\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 7 5 5 1 . 1 0 5 2 : r ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer Lin Yueyu & Li Zhiyuan & Peter Yue & Liu Xiao yueyu.lin@me.com lizhiyuan@uniartisan.com ziyinyue07@gmail.com liu.xiao.in@gmail.com Abstract As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models Dong et al. (2024), with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness , we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5s performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at https://github.com/yynil/RWKVInside, https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Architecture 3 From Transformer to RNN 3.1 Stage 1 - Time Mixing module replacing Self-Attention . . . . . . . . . . . . . . . . . . 3.2 Stage 2 - Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Stage 3 - SFT and DPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Evaluation 5 Conclusions 6 Future Work 7 Acknowledgements"
        },
        {
            "title": "1 Introduction",
            "content": "1 2 2 3 4 4 4 5 5 The emergence of Linear RNNs (LRNNs) has grown rapidly, with models like RWKV Peng et al. (2024), DeltaNet Yang et al. (2024b), Mamba-2 Wang et al. (2024), GLA Yang et al. (2023), and others https://github.com/Jellyfish042/Sudoku-RWKV, https://github.com/Jellyfish042/RWKV_Othello https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1 showing strong competitiveness with transformers. However, these models face natural penalties in context learning and long-context retrivel due to subquadratic attention limitations. Recently, RWKV-7 introduced promising architecture, where 0.1B parameter model achieved perfect results in 16k passkey retrieval. With its transition matrix having wider eigenvalues Grazzi et al. (2024), it demonstrates stronger state tracking capabilities than transformers Merrill et al. (2024). While Qwen 2.5 was trained on 18 trillion tokens requiring enormous GPU resources, making pretraining impractical for academic use, we bridge this gap by refining the distillation method. Our approach enables training 7B parameter model on single A100 80G GPU, while 4x8 A100s can handle 32B model."
        },
        {
            "title": "2 Architecture",
            "content": "We know qwen 2.5 dense models is the Transformer-based decoder architecture with group query attention, SwiGLU activation function for non-linear activation, Rotary Positional Embeddings for encoding position information, QKV bias in the attention mechanism and RMSNorm Yang et al. (2024a). We only keep RMSnorm and SwiGLU activiation and replace the rest with rwkv-7 attention.In GQA, queries are grouped while keys and values remain separate: GQA(Qg , K, ) = concat (cid:104) head1, ..., headg (cid:105) where each head is computed as: headi = Attention(QiW , KW , i ) In RWKV-7 attention which is the time mixing: Statet = Statet (cid:16) diag(wt) ˆκT (at (cid:17) ˆκt) + vT kt the state means matrix-valued attention state.where is the in-context learning rate. In the comparing with RWKV-6 Peng et al. (2024): state = diag(w) state + then we just relpace self-attention in every layer with RWKV-7 time mixing module in Figure 1. (1) (2) (3) (4)"
        },
        {
            "title": "3 From Transformer to RNN",
            "content": "Inspired by MambaInLLaMA Wang et al. (2024) and Phi-Mamba, we aimed to produce pure RNN model similar to QRWKV. We tested both layer-wise distillation and one-step distillation methods, finding the latter to be more efficient. Our research revealed that using attention alignment in stage 1 is crucial for maintaining the original models performance. By mimicking self-attention through RWKVs time mixing module, we can preserve attention expressiveness while transforming the state attention to function more like meta-learner Hospedales et al. (2021). https://x.com/BlinkDL_AI/status/1869433254425833487 2 Figure 1. replace self-attention by RWKV-7 time mixing module"
        },
        {
            "title": "3.1 Stage 1 - Time Mixing module replacing Self-Attention",
            "content": "In this stage,we align the hidden state output between the student and teacher attention block Bick et al. (2024), frozen the MLP and remove the group normalization for the attention output and make the gate initialized with 1 .[Figure 4] Context length can dramaticaly increase the trainning time, in this work we train 2048 with one h800 Lspecial = hteacher hstudent 2 (dmodel) 0.5 (5) where dmodel is dimension size of hidden state. We found that initializing state attention from teachers attention is not necessary. The convergence speed and final values of the loss [Figure 5] indicate the target attention ability to capture the teacher models internal attention representations. Bick et al. (2024) due to the fixed state size in rwkv-7 time mixing module, we can see it as compression process Skean et al. (2024) , or it can be seen attention as map between probability measures Castin et al. (2024),"
        },
        {
            "title": "3.2 Stage 2 - Knowledge Distillation",
            "content": "Divergence-based methods minimize diver gence between the probability distributions of the teacher and student models Xu et al. (2024a), We adopt word-level KL-Divergence instead of sequence-level knowledge distillation (SeqKD).In practice, we distill from 32B to 7B model. For the dataset, we balance its distribution Xu et al. (2024b) based on stage 1 training, achieving fast convergence with only 20M tokens. We then experiment with whether or not to freeze the MLP layer and introduce gate-free technique which disables the gate mechanism entirely"
        },
        {
            "title": "3.3 Stage 3 - SFT and DPO",
            "content": "During this stage, we use supervised fine-tuning (SFT) to expand the models context length and Direct Preference Optimization (DPO) to align with user preferences. Our training process involves 20M tokens in stage 1, 40M tokens in stage 2, and 770M tokens in stage 3 for context length extension."
        },
        {
            "title": "4 Evaluation",
            "content": "We conducted ablation experiments between 7B models distilled using different approaches in stage 2. By controlling the presence of Gate, freeze MLP, size of teacher model, we distill model notfreezing MLP(ARWKV-M), model with GATE and not freezing MLP(ARWKV-G-M), model without Gate nor freezing MLP(ARWKV) and model with teacher model as QWEN2.5-32B-Instruct(ARWKVfrom32B). All models are distilled from QWEN2.5-7B-Instruct except ARWKV-from32B. After stage-2 training, all models are tested with several benchmarks to demonstrate the impact of different training factors on the final model performance. Moreover, we found that although we trained the model using bfloat16 (BF16), performing inference with float16 (FP16) significantly improved the performance, This differs from the original RWKV implementation, which required careful tuning of the layer scaling parameters to avoid overflow. Our analysis of Table 1 reveals that knowledge distillation from the 32B parameter model, when performed without gating mechanisms and with frozen MLPs, yields suboptimal results. This performance degradation may be attributed to the limited capacity of the 7B models MLP layers to effectively accommodate and adapt to the more sophisticated attention patterns learned by the 32B models architecture. This observation suggests potential architectural mismatch in the direct transfer of attention mechanisms between models of significantly different scales. 4 Qwen2.5-7B-Instruct ARWKV active MLP w/ gate & active MLP ARWKV-from32B MMLU Squad 71.72 47.89 GPQA(Diamond) 49.0 WinoGrande GSM8K IfEval Arc-c 71.35 82.34 73. 54.86 62.41 40.05 45.5 68.67 39. 52.16 52.22 58.22 40.35 51.1 69. 51.93 48.68 53.52 64.77 38.74 68. 47.99 52.16 52.22 61.78 39.01 68. 43.44 44.12 50.77 Table 1. This is ongoing work, benchmark based on stage-2 , currently we limit the context length to 2048 and use same datasets"
        },
        {
            "title": "5 Conclusions",
            "content": "We demonstrate that attention alignment combined with knowledge distillation can effectively transform transformer attention patterns into RNN-based attention in straightforward manner.In some hybrid architecture ,this attention module serves as memory component Dong et al. (2024), different kind of attention introduce unique inductive bias , bring more expressive power."
        },
        {
            "title": "6 Future Work",
            "content": "For our subsequent phase of investigation, we will implement Stage 3 post-training to replicate the reasoning capabilities demonstrated by deepseek-R1 Guo et al. (2025) models. Furthermore, we propose to generalize this methodology across diverse architectural paradigms, encompassing Mixture-of-Experts (MoE) frameworks, multimodal architectures, hybrid architectures and model compression scenarios. This expansion aims to validate the robustness and transferability of our approach across different computational paradigms."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We would like to express our gratitude to Yuhang He for his endorsement. References A. Bick, K. Y. Li, E. P. Xing, J. Z. Kolter, and A. Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models. arXiv preprint arXiv:2408.10189, 2024. V. Castin, P. Ablin, and G. Peyré. How smooth is attention? In ICML 2024, 2024. https://medium.com/@felixhill/the-agreeable-lesson-9766382c6d83 X. Dong, Y. Fu, S. Diao, W. Byeon, Z. Chen, A. S. Mahabaleshwarkar, S.-Y. Liu, M. Van Keirsbilck, M.-H. Chen, Y. Suhara, et al. Hymba: hybrid-head architecture for small language models. arXiv preprint arXiv:2411.13676, 2024. R. Grazzi, J. Siems, J. K. Franke, A. Zela, F. Hutter, and M. Pontil. Unlocking state-tracking in linear rnns through negative eigenvalues. arXiv preprint arXiv:2411.12537, 2024. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):51495169, 2021. W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. B. Peng, D. Goldstein, Q. Anthony, A. Albalak, E. Alcaide, S. Biderman, E. Cheah, X. Du, T. Ferdinan, H. Hou, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. O. Skean, M. R. Arefin, Y. LeCun, and R. Shwartz-Ziv. Does representation matter? exploring intermediate layers in large language models. arXiv preprint arXiv:2412.09563, 2024. J. Wang, D. Paliotta, A. May, A. M. Rush, and T. Dao. The mamba in the llama: Distilling and accelerating hybrid models. arXiv preprint arXiv:2408.15237, 2024. X. Xu, M. Li, C. Tao, T. Shen, R. Cheng, J. Li, C. Xu, D. Tao, and T. Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024a. Z. Xu, F. Jiang, L. Niu, Y. Deng, R. Poovendran, Y. Choi, and B. Y. Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024b. A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim. Gated linear attention transformers with hardwareefficient training. arXiv preprint arXiv:2312.06635, 2023. S. Yang, B. Wang, Y. Zhang, Y. Shen, and Y. Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024b. 6 Figure 2. RWKV-7 architecture.capability of attention is the key for RNN-based LLMs, which in this case is Time mixing module 7 Figure 3. General Decoder Layer in transformer 8 Figure 4. We replace the standard Attention with an AttentionWrapper that contains both the original selfattention mechanism and TimeMixer. The TimeMixer is trained to minimize the gap between its output and that of the self-attention module. The final output combines the hidden states from the original self-attention with the residual difference between self-attention and TimeMixer outputs. This architecture enables the model to optimize the TimeMixer to progressively reduce the discrepancy between self-attention and TimeMixer outputs. 9 Figure 5. Stage-1 loss, 18 hours with one 8*h800 80G , context length 2048, 4B tokens"
        }
    ],
    "affiliations": []
}