{
    "paper_title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models",
    "authors": [
        "Linhao Zhong",
        "Linyu Wu",
        "Bozhen Fang",
        "Tianjian Feng",
        "Chenchen Jing",
        "Wen Wang",
        "Jiaheng Zhang",
        "Hao Chen",
        "Chunhua Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM."
        },
        {
            "title": "Start",
            "content": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models Linhao Zhong1* Linyu Wu2* Bozhen Fang1 Tianjian Feng1 Chenchen Jing1,3 Wen Wang1 1Zhejiang University Jiaheng Zhang2 Hao Chen1 Chunhua Shen1,3 2National University of Singapore 3Zhejiang University of Technology 6 2 0 J 2 1 ] . [ 1 1 5 3 7 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) offer promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https: //aim-uofa.github.io/EvoTokenDLM."
        },
        {
            "title": "Introduction",
            "content": "Diffusion Language Models (DLMs) (Nie et al., 2025; Zhu et al., 2025; Ye et al., 2025) frame language generation as an iterative refinement process, enabling parallel decoding in contrast to the strictly sequential nature of autoregressive models. By replacing causal token-by-token generation with diffusion-based refinement (Ho et al., 2020; Nichol and Dhariwal, 2021; Song et al., 2020), DLMs offer an alternative generation paradigm that improves decoding parallelism. Most existing DLMs adopt masked diffusion fashion, commonly referred to as masked diffusion language models (MDLMs) (Nie et al., 2025; Zhu et al., 2025; Ye et al., 2025), in which generation *Equal Contribution. Corresponding Author. 1 Figure 1: Inefficient utilization of predictions in masked diffusion language models, where distributions are computed for all positions but only subset are used for decoding. [M1, M2, . . . , Mn] denote the initial mask tokens following prompt , and disti represents the predicted probability distribution for the i-th token in the generation sequence. In this example, the total sequence of 542 tokens consists of 30 prompt tokens and 512 generated tokens, while only two positions are updated per step. is performed by maintaining partially masked sequence and progressively replacing masked positions with discrete token assignments, enabling the simultaneous decoding of multiple tokens. To further improve the practicality of DLMs, recent work has introduced KV-caching mechanisms (Wu et al., 2025b,a; Ma et al., 2025a; Chen et al., 2025) that reuse hidden states across refinement steps to reduce redundant computation. In parallel, blockwise diffusion models (Wang et al., 2025b; Cheng et al., 2025; Bie et al., 2025) apply diffusion-based generation within local token blocks while preserving autoregressive dependencies across blocks, combining global causal coherence with local parallel efficiency. However, most MDLMs rely on hard binary masking with discrete token assignments. Once token is decoded, it is treated as final and excluded from further refinement, resulting in an abrupt tranFigure 2: Comparison between MDLMs and EvoToken-DLM. (a) Standard MDLMs employ only two token states, alternating between <mask> and discrete decoded tokens, leading to abrupt mask-to-token transitions. (b) EvoToken-DLM introduces soft tokens represented by probability distributions and four token states, enabling tokens to evolve progressively through iterative refinement. The top-right panel illustrates quantitative comparison between the two approaches under the same settings based on LLaDA-Instruct-8B. sition from uncertainty to determinism. This irreversibility limits the models ability to revise early decisions and undermines the iterative refinement paradigm of diffusion-based language modeling. In addition, as illustrated in Figure 1, although MDLMs compute token distributions for all positions at each refinement step, only small subset of positions are updated, while the remaining probabilistic information is discarded. In this work, we propose EvoToken-DLM, diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. Instead of predicting discrete tokens in single step, EvoToken-DLM represents each token as probability distribution over the vocabulary and iteratively refines it throughout the diffusion process. As illustrated in Figure 2, token decoding becomes progressive and continuous evolution: [MASK] Soft([MASK] V) Soft(V) [Decode]. (1) This evolution gradually transitions tokens from masked uncertainty to mask-aware soft token distributions, then to fully soft token distributions, and finally to discrete outputs. By allowing token representations to evolve across refinement steps before being finalized, EvoToken-DLM enables smooth and revisable decoding, mitigating premature decisions induced by hard masking. To support this progressive refinement during training, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates along the diffusion trajectory. EvoToken-DLM requires no modification to the underlying model architecture and can be readily adapted from existing MDLMs. Moreover, it is fully compatible with KV-caching and naturally extends to blockwise diffusion settings, demonstrating broad applicability. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently outperforms strong MDLM baselines. Our main contributions are: We propose EvoToken-DLM, diffusionbased language modeling approach that replaces hard binary masks with evolving soft token distributions, enabling staged and revisable decoding process throughout diffusion. 2 We introduce continuous trajectory supervision-based training strategy that aligns model optimization with iterative probabilistic token refinement along the diffusion trajectory, effectively supporting progressive token evolution. We demonstrate that EvoToken-DLM integrates seamlessly with KV-caching and extends naturally to blockwise diffusion architectures. Extensive experiments across diverse model backbones, datasets, and inference configurations show consistent and robust improvements over strong MDLM baselines, highlighting EvoToken-DLM as general and effective enhancement."
        },
        {
            "title": "2 Preliminaries on MDLMs",
            "content": "Masked diffusion language models operate under masked diffusion paradigm. The generation process consists of two main stages: forward corruption process and learned reverse denoising process. 1, . . . , x0 Forward Process. Given an original text sequence 0 = (x0 ) of tokens, the forward process gradually corrupts it into noisy sequence over time schedule [0, ]. This corruption is typically achieved by independently replacing each token with special mask token with probability : (cid:40) q(xt x0 ) = 1 , , if xt if xt = x0 , = <mask> At = , the sequence becomes fully masked. Reverse Process. MDLMs learn parameterized model pθ(X 0 t) to reverse the forward process. The model predicts all masked tokens simultaneously at each inference step, enabling high-speed, parallel generation from the fully masked sequence to the original text 0. In practice, at each decoding step, the model selects subset of masked tokens to finalize based on their predicted confidence rather than decoding all tokens at once. Masked tokens not selected in the current step remain in the mask state and will be decoded in subsequent steps. Furthermore, sequences are partitioned into discrete blocks that are processed in sequential manner, where the model advances to the next block only upon the complete refinement of all masked tokens within the current one."
        },
        {
            "title": "Continuous Relaxation Perspective",
            "content": "Continuous Relaxation. Let = {1, . . . , } denote the vocabulary of size . We define the discrete token space as the set of one-hot vectors = {δ1, . . . , δV } {0, 1}V . Associated with the vocabulary is an embedding matrix RV D. The embedding function maps token index to continuous vector = Ui. We denote the continuous embedding space as the convex hull of the token embeddings: = Conv(U) RD. soft token is any vector that can be expressed as = Up, where 1 lies on the probability simplex. This formulation relaxes the categorical selection into continuous domain. Iterative Refinement in Continuous Domain. Unlike standard MDLMs which predict pθ(X 0 ) iteratively over the discrete vocabulary, our method models the reverse process as an iterative refinement loop in the continuous domain E. Specifically, let denote the masked input sequence and ET be its corresponding embeddings, where each element of ET belongs to E. We introduce auxiliary token states ZT to enable continuous token evolution. The refinement process is governed by transition function Φ, which recursively updates both the continuous embeddings and the token states: (Et1, Zt1) = Φ(Et, Zt). Through successive applications of Φ, the model progressively purifies the noisy input until it reaches the terminal E0, Z0. Finally, E0 is mapped back to the discrete domain to produce the output sequence 0."
        },
        {
            "title": "4 EvoToken-DLM",
            "content": "4.1 Progressive Inference with EvoToken-DLM We formally define the progressive inference procedure of EvoToken-DLM as follows. Given prompt , the objective is to generate response of length . The output is partitioned into = N/B discrete blocks, each of size B. The sequence is constructed by concatenating the prompt with tokens, denoted as = (P, x1, x2, . . . , xN ), where each token xi is characterized by pair (ei, zi), comprising continuous embeddings ei and token state zi. Initially, all target positions are initialized as mask tokens, where zi = [MASK] for all {1, . . . , }, and the corresponding embedding sequence is represented Figure 3: Progressive step-wise token update with blockwise decoding in EvoToken-DLM. as = (eP , e<mask> ). During the evolution process, each token xi transitions through state space consisting of four distinct stages: , . . . , e<mask> 1 [MASK], Soft([MASK] V), Soft(V), [Decode], where is the vocabulary. Token Prediction. At each inference step, we input embeddings into the model to obtain }V predicted distribution {pc c=1 over the vocabulary for each position i. We retain the top-K probabilii }K ties and renormalize them to obtain {ˆpc c=1, along }K with their corresponding tokens {ˆvc c=1 V. Soft embeddings are then computed as: edist = (cid:88) c=1 eˆvc ˆpc , edist+M = α e<mask> + (1 α) edist , (2) where α [0, 1] controls the mixing ratio of the mask embedding. Embedding Assignment by Token State. For token xi, its newly generated embeddings at the current step is assigned based on its current state: ei = , e<mask> edist+M , edist , evi, zi = [MASK] zi = Soft([MASK] V) zi = Soft(V) zi = [Decode] (3) where vi is selected as the token in the vocabulary with the highest confidence among all historical predictions made after xi enters the Soft(V) state. Step-wise Token Update. By default, tokens in the [MASK] state transition to Soft([MASK] V), whereas tokens already in the Soft([MASK] V), Soft(V), or [Decode] states retain their current state. At each step, subset of tokens currently in the [MASK] or Soft([MASK]V) states in the current block is selected to transition to the Soft(V) state. 4 Figure 4: Continuous trajectory supervision by performing τ consecutive refinement steps during training and applying supervision at each step, aligning the training objective with the inference process. Let denote the set of these selected tokens. The complete update rule is formalized as: zi Soft([MASK] V), Soft(V), [Decode], zi {Soft([MASK] V), [MASK]} and xi / xi or zi = Soft(V) zi = [Decode] (4) Blockwise Decoding. Let denote the set of tokens in the current block. Once all tokens in reach the Soft(V) state, they are simultaneously converted to the [Decode] state: zi [Decode], xi B, if all tokens xj are in the Soft(V) state. (5) As illustrated in Figure 3, combining step-wise token update with blockwise decoding, EvoTokenDLM allows each token to gradually refine its representation from [MASK] to final [Decode] through progressive token evolution. The detailed algorithm for progressive inference is provided in Appendix B.1. 4.2 Continuous Trajectory Supervision Unlike conventional masked diffusion frameworks, EvoToken-DLM employs progressive evolution mechanism. In this approach, the current states and embeddings of the tokens are conditioned on the cumulative history of the preceding refinements. This temporal dependency renders standard singlestep denoising objectives inapplicable, necessitating specialized training paradigm that models the trajectory of token evolution. We propose continuous trajectory supervision, training strategy that aligns model optimization with iterative probabilistic token refinement along the diffusion trajectory , as illustrated in Figure 4. This approach ensures consistency from training to inference. Initialization and Masking Strategy. Given sequence comprising prompt and target response, we sample contiguous segment of length from the response as the current training block. To align with the blockwise inference procedure, tokens preceding this block are set to the ground truth, while tokens after this block are replaced with [MASK]. Within the selected block, we randomly mask subset of tokens to initialize the state (0). Trajectory Unrolling. Starting from (0), we simulate τ consecutive refinement steps to sample the trajectory: (i), L(i) = Model(X (i1)), = 1, . . . , τ, (6) where each forward pass produces probability distributions, updated continuous embeddings, and updated token states according to the progressive inference rules described in Section 4.1. Cumulative Trajectory Loss. At each step i, we compute supervised loss L(i) against the groundtruth tokens within the current block. Rather than backpropagating only through the final step, we perform backward pass for every forward step: θL(i), = 1, . . . , τ. (7) By explicitly simulating the progressive refinement during training, continuous trajectory supervision aligns the learning objective with the inference behavior of EvoToken-DLM. The detailed algorithm for continuous trajectory supervision is provided in Appendix B.2. 4.3 Extension to Blockwise Diffusion EvoToken-DLM naturally extends to blockwise diffusion by partitioning the sequence into consecutive blocks. Within each block, tokens undergo full progressive refinement before the generation moves to the next, preserving the global autoregressive structure while enabling intra-block parallelism. For training, we adapt continuous trajectory supervision to this setting. Following existing frameworks (Wang et al., 2025b; Cheng et al., 2025; Bie et al., 2025), we exploit block-level causal dependencies to enable independent, parallel training of blocks. Within each block, the continuous trajectory supervision procedure simulates τ refinement steps for supervision. Figure 5: Ablation study on the presence of intermediate refinement states in EvoToken-DLM."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup We employ LLaDA-Instruct-8B (Nie et al., 2025) as our primary backbone for fine-tuning. To evaluate cross-model consistency, we also apply our method to LLaDA-1.5 (Zhu et al., 2025), Dream-Instruct-7B (Ye et al., 2025) and D2FLLaDA (Wang et al., 2025b), the last of which serves as the base model for our blockwise diffusion experiments. For fine-tuning, we utilize the S1K dataset (Muennighoff et al., 2025) and train the pretrained model for default duration of 10k steps using continuous trajectory supervision. Evaluations are performed across several mathematical and reasoning benchmarks, including Countdown (Pan et al., 2025), GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), and SVAMP (Patel et al., 2021). More details are presented in Appendix C. 5.2 Evaluation Results Main Performance Comparison. Table 1 compares EvoToken-DLM against the original LLaDAInstruct-8B and the FT-baseline across multiple reasoning benchmarks. EvoToken-DLM predominantly surpasses both baselines, exhibiting substantial performance leaps under varying configurations. Specifically, at Gen Len = 1, our method yields average accuracy gains of 17.45% on Countdown, 3.08% on GSM8K, 2.06% on MATH500, and 3.23% on SVAMP compared to the original model. These results underscore the superiority of our soft token evolution framework in enhancing reasoning capabilities and generation quality. Additional results with different block sizes are presented in Appendix E.3, and qualitative comparisons are presented in Appendix E.5. 5 Table 1: Performance comparison on the Countdown, GSM8K, MATH500 and SVAMP datasets across various generation lengths and NFEs based on LLaDA-Instruct-8B. EvoToken-DLM is initialized from LLaDA-Instruct-8B and fine-tuned for 10k steps using continuous trajectory supervision. Comparisons are conducted against both the baseline model and the sft-baseline. Countdown GSM8K MATH500 SVAMP Gen Len Method Baseline 128 256 512 Avg. 128 512 Avg. 128 256 512 Avg. 128 512 Avg. 21.48 23.83 20.70 22.00 70.20 79.30 83.47 77.66 28.80 34.60 39.40 34.27 88.33 84.67 86.00 86.33 1 2 1 4 FT-Base (10k FT) 33.20 21.48 19. 24.74 71.04 82.11 82.56 78.57 26.60 36.20 40.40 34.40 87.67 89.67 89.67 89.00 EvoToken (10k FT) 39.45 74.30 83.47 84.46 80.74 28.40 39.60 41.00 36.33 89.00 89.67 90.00 89.56 39.84 +18.36 +11.72 +22.27 +17.45 +4.10 +4.17 +0.99 +3.08 -0.40 +5.00 +1.60 +2.06 +0.67 +5.00 +4.00 +3.23 42.97 35.55 Baseline 26.17 16.41 16.80 19.79 67.55 77.63 79.83 75.00 26.60 32.20 33.20 30.67 86.00 86.67 84.00 85.56 FT-Base (10k FT) 28. 16.80 16.41 20.44 63.91 78.62 79.00 73.84 22.60 31.20 34.00 29.27 85.00 87.00 89.33 87.11 EvoToken (10k FT) 30.08 31.64 73.54 82.03 81.80 79.12 29.20 36.40 37.40 34.33 89.33 92.33 89.67 90.44 34.77 +8.60 +13.67 +13.28 +11.85 +5.99 +4.40 +1.97 +4.12 +2.60 +4.20 +4.20 +3.66 +3.33 +5.66 +5.67 +4. 30.08 Baseline 17.19 15.62 16.41 16.41 59.14 68.23 66.57 64.65 23.40 26.60 29.60 26.53 81.00 77.33 75.00 77. FT-Base (10k FT) 14.06 13.67 9.77 12.50 49.05 62.17 61.87 57.70 16.20 19.60 23.20 19.67 66.67 75.33 72.00 71.33 EvoToken (10k FT) 23.05 +5.86 16.02 +0.40 12.11 -4.30 17.06 64.82 75.74 72.33 70.96 23.60 31.00 31.20 28.60 78.33 83.33 81.33 81.00 +0.65 +5.68 +7.51 +5.76 +6.31 +0.20 +4.40 +1.60 +2.07 -2.67 +6.00 +6.33 +3.22 Importance of Intermediate States. Figure 5 illustrates the ablation study on the presence of intermediate refinement states. The performance drop observed when removing these states confirms that the gradual transition from mask to soft-token is essential for the model to iteratively refine its predictions. Qualitative Visualization. We provide qualitative visualization of the inference process in Figure 6. By tracing the evolution of selected subsequence, we observe how initial uncertain tokens progressively converge into precise and coherent results. This visualization confirms that EvoToken-DLM effectively implements progressive refinement mechanism, allowing the model to iteratively calibrate its predictions within the diffusion framework. Additional qualitative visualization results of the inference process are presented in Appendix E.4. Compatibility with KV-Caching. To further demonstrate the practical efficiency of EvoTokenDLM, we integrate it with the KV-caching mechanism as proposed in Fast-dLLM (Wu et al., 2025b). This integration is essential to ensure that our adaptive token evolution does not interfere with the accelerated inference pipelines of DLMs. As reported in Table 2, we evaluate the performance of EvoToken-DLM equipped with KV-caching against the baseline on Countdown. The results indicate that our method consistently maintains superior performance with KV-caching across various computational budgets, proving its seamless integration with the KV-caching mechanism. Table 2: Performance comparison on Countdown with KV-caching. EvoToken integrates seamlessly with KVcaching mechanism. Gen Len Gen Len Baseline+Cache EvoToken+Cache 256 1 1/2 1/4 1 1/2 1/4 21.88 23.05 7.42 21.48 19.92 9.77 36.33 (+14.45) 28.52 (+5.47) 18.75 (+11.33) 28.52 (+7.04) 25.00 (+5.08) 12.50 (+2.73) Robustness across Thresholds. We further adopt another parallel generation strategy using the confidence threshold proposed in Fast-dLLM (Wu et al., 2025b) to replace the fixed NFE setting. This allows for more flexible allocation of computational resources during inference. We analyze the sensitivity of our method to different thresholds on the MATH500 dataset, with KV-caching enabled. As illustrated in Figure 7, EvoToken consistently outperforms the binary masking baseline given the same average token budget per step. These results demonstrate the superior adaptability of EvoTokenDLM. Generalization Across Models. We evaluate the transferability of our approach by applying continuous trajectory supervision to the Dream-Instruct-7B pretrained base. As shown in Figure 8, the improvements observed in the primary model consistently generalize to the alternative backbone. This consistency underscores that EvoToken-DLM serves as general enhancement for diffusion language models. Additional results based on LLaDA-1.5 6 Figure 6: An illustrative example of EvoToken-DLM during inference, showing intermediate refinement states for selected subsequence across successive steps. The block size is set to 12, and the refinement process for the first 16 output tokens is visualized. For each position, only the top = 3 most probable tokens are retained. Figure 7: Comparison between EvoToken and binary masking baseline on MATH500 with KV-caching and different confidence thresholds. EvoToken consistently achieves higher accuracy than baseline across various thresholds under the same average tokens per step. are presented in Appendix E.1. Extension to Blockwise Diffusion. To further validate the versatility of EvoToken, we extend our method to the blockwise diffusion framework, specifically using D2F-LLaDA as the base model. As illustrated in Figure 9, EvoToken significantly outperforms the binary masking baseline, proving its robustness and adaptability. Inference Efficiency. Figure 10 illustrates that EvoToken-DLM introduces only negligible latency compared to standard MDLMs, with the marginal overhead stemming primarily from the element7 Figure 8: Comparison between EvoToken and the binary masking baseline based on another pretrained model Dream-Instruct-7B. We apply continuous trajectory supervision and evaluate performance on various datasets. wise addition of token embeddings during the refinement process. Such minimal overhead, alongside substantial improvements, makes EvoTokenDLM highly practical for real-world deployment. Impact of Top-K Filtering. In Figure 11, we analyze the sensitivity of the model to different top-K settings during the refinement process. EvoTokenDLM shows robust performance across wide range of values, consistently outperforming the baseline. An additional ablation study on the mixing ratio α is presented in Appendix E.2. More analyses regarding rapid adaptation from pretrained MDLMs are presented in Appendix D. Figure 9: Comparison between EvoToken and the binary masking baseline based on blockwise diffusion model D2F-LLaDA. We apply continuous trajectory supervision and evaluate performance on SVAMP. Figure 11: Performance comparison between the baseline and EvoToken-DLM with different top-K settings on Countdown. Blockwise Diffusion Language Models. Blockwise MDLMs (Han et al., 2023; Arriola et al., 2025; Zhao et al., 2025; Liu et al., 2025a,b; Cheng et al., 2025; Wang et al., 2025b) hybridize AR global ordering with intra-block diffusion to support KVcaching. To eliminate serial bottlenecks, Wang et al. (2025b) introduces D2F, which enables decoding future blocks from noisy intermediate states. 6.2 Latent Reasoning Reasoning in Continuous Space. To enhance Chain-of-Thought expressivity, recent works transition from discrete tokens to latent spaces. Hao et al. (2024) leverage transformer hidden states, while Xu et al. (2025a,b) and Zhang et al. (2025); Zhuang et al. (2025) utilize projection modules or probability-weighted embeddings. Latent Reasoning with DLMs. To mitigate remasking information loss, recent DLMs integrate continuous semantics. Hersche et al. (2025) propose Soft Masking via dynamic embedding blends, while Zheng et al. (2025) employ dual discretecontinuous diffusion. Additionally, Kang et al. (2025) utilize VAE-based latent spaces to refine reasoning trajectories."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we presented EvoToken-DLM, novel diffusion language modeling approach that replaces rigid binary masks with evolving soft token distributions. This shift enables progressive decoding process, overcoming the limitations of irreversible discrete assignments in traditional MDLMs. By introducing continuous trajectory supervision, we effectively align the training objective with iterative probabilistic refinement. Extensive experiments demonstrate that EvoToken-DLM consistently outperforms strong baselines while remaining fully compatible with KV-caching and blockwise architectures. Figure 10: EvoToken-DLM exhibits competitive inference efficiency, introducing minimal latency penalties relative to standard MDLM architectures."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 MDLMs Masked Diffusion Language Models (MDLMs) adapt the diffusion paradigm (Ho et al., 2020; Podell et al., 2023; Rombach et al., 2022; Song and Ermon, 2019; Song et al., 2020) to discrete text generation. Building on foundational work in noise scheduling and objectives (Austin et al., 2021; Sahoo et al., 2024; Bie et al., 2025; Yang et al., 2025; Nie et al., 2025; Wu et al., 2025c; Wang et al., 2025a), recent large-scale models like LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025) have shown that MDLMs can match autoregressive baselines in complex reasoning. Despite their potential, the iterative denoising process remains computationally expensive. Current research addresses this through two main efficiency frontiers: developing specialized caching mechanisms and architecting blockwise generative processes. KV-Cache Optimization for MDLMs. Standard KV caching is incompatible with bidirectional MDLMs, often necessitating recomputation per step. Fast-dLLM (Wu et al., 2025b,a) mitigates this via block-wise approximate caching, while others (Ma et al., 2025b; Shen et al., 2025; Bao et al., 2025) refine generation coherence. Furthermore, dKV-Cache (Ma et al., 2025a) and dLLMCache (Liu et al., 2025c) utilize selective token updates, and Sparse-dLLM (Song et al., 2025) applies dynamic eviction to reduce long-context memory overhead."
        },
        {
            "title": "Limitations",
            "content": "While our approach enables rapid adaptation from pretrained MDLMs to EvoToken-DLM via lightweight supervised fine-tuning, it faces training challenges when applied to models initialized with autoregressive (AR) priors. The inherent discrepancy between unidirectional AR pretraining and our iterative bidirectional refinement process leads to increased training difficulty and slower convergence for AR-based backbones. We provide detailed comparative analysis in Appendix D."
        },
        {
            "title": "References",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. 2025. Block Interpolating between autoregressive diffusion: arXiv preprint and diffusion language models. arXiv:2503.09573. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993. Wenrui Bao, Zhiben Chen, Dan Xu, and Yuzhang Shang. 2025. Learning to parallel: Accelerating diffusion large language models via learnable parallel decoding. arXiv preprint arXiv:2509.25188. In Proceedings of the 61st Anmodular control. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11575 11596. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Michael Hersche, Samuel Moor-Smith, Thomas HofSoft-masked arXiv preprint mann, and Abbas Rahimi. 2025. diffusion language models. arXiv:2510.17206. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840 6851. Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamäki, Navdeep Jaitly, Yi-An Ma, and Lianhui Qin. 2025. Ladir: Latent diffusion enarXiv preprint hances llms for text reasoning. arXiv:2510.08558. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Xiaoran Liu, Yuerong Song, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Longllada: Unlocking long conQiu. 2025a. text capabilities in diffusion llms. arXiv preprint arXiv:2506.14429. Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, and 1 others. 2025. Llada2. 0: Scaling up diffusion language models to 100b. arXiv preprint arXiv:2512.15745. Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, and 1 others. 2025b. Sequential diffusion language models. arXiv preprint arXiv:2509.24007. Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, and Xinchao Wang. 2025. dparallel: Learnable parallel decoding for dllms. arXiv preprint arXiv:2509.26488. Shuang Cheng, Yihan Bian, Dawei Liu, Linfeng Zhang, Qian Yao, Zhongbo Tian, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, and 1 others. 2025. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. 2023. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. 2025c. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. 2025a. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781. Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, and 1 others. 2025b. dinfer: An efficient inference framework for diffusion language models. arXiv preprint arXiv:2510.08666. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2028620332. 9 Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 81628171. PMLR. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. 2025b. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Large language diffusion models. arXiv preprint arXiv:2502.09992. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025. Tinyzero. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-01-24. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve arXiv preprint 2021. simple math word problems? arXiv:2103.07191. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. 2024. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184. Jucheng Shen, Gaurav Sarkar, Yeonju Ro, Sharath Nittur Sridhar, Zhangyang Wang, Aditya Akella, and Souvik Kundu. 2025. Improving the throughput of diffusion-based large language models via trainingfree confidence-aware calibration. arXiv preprint arXiv:2512.07173. Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32. Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. 2025. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction. arXiv preprint arXiv:2508.02558. Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, and Chunhua Shen. 2025a. Time is feature: Exploiting temporal dynamics in diffusion language models. arXiv preprint arXiv:2508.09138. Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, and Enze Xie. 2025a. Fast-dllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. 2025b. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618. Linyu Wu, Linhao Zhong, Wenjie Qu, Yuexin Li, Yue Liu, Shengfang Zhai, Chunhua Shen, and Jiaheng Zhang. 2025c. Dmark: Order-agnostic watermarking for diffusion large language models. arXiv preprint arXiv:2510.02902. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025a. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134. Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. 2025b. SoftCoT++: Test-time scaling with arXiv preprint soft chain-of-thought reasoning. arXiv:2505.11484. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. 2025. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025. Dream 7b. Zhen Zhang, Xuehai He, Weixiang Yan, Ao Shen, Chenyang Zhao, Shuohang Wang, Yelong Shen, and Xin Eric Wang. 2025. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space. arXiv preprint arXiv:2505.15778. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. 2025. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216. Huangjie Zheng, Shansan Gong, Ruixiang Zhang, Tianrong Chen, Jiatao Gu, Mingyuan Zhou, Navdeep Jaitly, and Yizhe Zhang. 2025. Continuously augmented discrete diffusion model for categorical generative modeling. arXiv preprint arXiv:2510.01329. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and 1 others. 2025. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223. Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, and Jianfeng Gao. 2025. Text generation beyond discrete token sampling. arXiv preprint arXiv:2505.14827."
        },
        {
            "title": "A Appendix Overview",
            "content": "Appendix B: More Methodological Details Appendix B.1: EvoToken Algorithm for Progressive Inference Appendix B.2: EvoToken Algorithm for Continuous Trajectory Supervision Appendix C: More Implementation Details Appendix C.1: Detailed Descriptions of Training Dataset and Evaluation Benchmarks Appendix C.2: Training Configurations for Continuous Trajectory Supervision Appendix C.3: Inference and Evaluation Setup Appendix D: More Analyses Appendix D.1: Paradigm Consistency: Why MDLMs Rapidly Adapt to EvoToken-DLM Appendix D.2: Adaptation Hurdles for AR Backbones: Causal Prior Mismatch Appendix E: More Experimental Results Appendix E.1: Additional Results Based on LLaDA-1.5 Appendix E.2: Ablation Study on the Mixing Ratio α Appendix E.3: Additional Results with Different Block Sizes Appendix E.4: Additional Inference Examples of EvoToken-DLM Appendix E.5: Qualitative Comparisons for EvoToken-DLM and MDLMs"
        },
        {
            "title": "B More Methodological Details",
            "content": "B.1 EvoToken Algorithm for Progressive Inference As presented in Algorithm S1, the core of the inference process lies in the management of the token states and continuous embeddings E. Each token position starts in the [MASK] state. For each block Bb, the model performs multiple forward passes to refine the soft embeddings. In each step, tokens in subset are promoted from mask or soft-mask states to pure soft states. To ensure the stability of the final output, we track the historical highconfidence predictions vi for each position since it entered the soft state. Once all tokens in the current block are in pure soft states, they are transitioned to the [Decode] state, and their hard embeddings are used as context for the next block. This mechanism effectively facilitates progressive refinement process for each token. B.2 EvoToken Algorithm for Continuous Trajectory Supervision Algorithm S2 presents the continuous trajectory supervision procedure. The core design philosophy is to bridge the bias between the static training of traditional MDLMs and the iterative nature of EvoToken inference. We introduce training-time simulation of the inference trajectory by partitioning the target block into Ssof and Smask, and we initialize Ssof positions with their corresponding ground-truth (GT) embeddings eyk . This multistep refinement loop, repeated for τ steps within each training iteration, ensures that the model parameters θ are optimized not just for single-step recovery, but for the evolutionary path. This continuous optimization allows the model to effectively learn the progressive refinement over successive iterations."
        },
        {
            "title": "C More Implementation Details",
            "content": "C.1 Detailed Descriptions of Training Dataset and Evaluation Benchmarks We utilize combination of high-quality instruction-tuning data and diverse mathematical benchmarks. Training Dataset. S1K (Muennighoff et al., 2025): highquality dataset featuring 1,000 diverse and challenging problems, each accompanied by distilled reasoning traces and solutions to faAlgorithm S1 Progressive Inference with EvoToken-DLM Require: Prompt , target length , block size B, mixing ratio α, filtering threshold Ensure: Decoded sequence = (v1, . . . , vN ) 1: Initialize states = (z1, . . . , zN ) where zi [MASK], 2: Initialize embeddings = (eP , e1, . . . , eN ) where ei e<mask>, 3: for block = 1 N/B do 4: {1, . . . , } {1, . . . , } Bb indices of the current block while Bb s.t. zi = [Decode] do c=1 Model(E) c=1 Normalize(TopK({vc }V }K , pc , ˆpc {vc {ˆvc for {1, . . . , } do (cid:80)K edist edist+M c=1 ˆpc eˆvc αe<mask> + (1 α)edist , pc }V c=1, K)) end for Select subset {i Bb zi {[MASK], Soft([MASK] V)}} zi Soft(V), zi Soft([MASK] V), if Bb, zi = Soft(V) then Bb / s.t. zi = [MASK] zi [Decode], Identify vi as the highest-confidence token since zi transitioned to Soft(V), Bb end if for {1, . . . , } do ei e<mask> if zi = [MASK] edist+M edist evi if zi = Soft([MASK] V) if zi = Soft(V) if zi = [Decode] 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end for end while 22: 23: end for 24: return = (v1, . . . , vN ) cilitate complex chain-of-thought reasoning. Evaluation Benchmarks. We evaluate the performance of our model across the following four benchmarks, covering spectrum of arithmetic and logical difficulty. Countdown (Pan et al., 2025): combinatorial arithmetic task that requires models to reach target value using specific set of numbers and basic operators. GSM8K (Cobbe et al., 2021): collection of 8.5K grade-school math problems requiring 28 steps of multi-step arithmetic reasoning. MATH500 (Lightman et al., 2023): subset of 500 challenging high-school competitionlevel problems selected from the MATH dataset. SVAMP (Patel et al., 2021): benchmark of 1K elementary math word problems designed to test model robustness against linguistic variations in narratives. C.2 Training Configurations for Continuous Trajectory Supervision To fine-tune the model under the progressive token evolution mechanism, we employ the following training configurations. The model is trained on the S1K dataset (Muennighoff et al., 2025) for total of 10k steps. LoRA Configuration: The LoRA adapter is applied to the query, key, and value projections. We set the rank = 128, LoRA alpha α = 256, and dropout rate of 0.05, with no bias parameters tuned. Optimization Settings: We use learning 13 Algorithm S2 Continuous Trajectory Supervision for EvoToken-DLM Require: Training dataset D, refinement steps , learning rate η, total iterations Niter Ensure: Optimized parameters θ 1: for iteration = 1 Niter do 2: Sample training pair (X, ) Sample target block Partition into subset Ssof and Smask Initialize states Z(0): zi [Decode], zi Soft(V), zi [MASK], < Ssof Smask {i > B} Initial embeddings E(0): e(0) e{X} e(0) eyi, e(0) e<mask>, for step = 1 τ do Ssof {i < B} Smask {i > B} 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: (i) Modelθ(E(i1)) L(i) CrossEntropy(P (i) Update states Z(i), embeddings E(i) and decode tokens (i) as in Algorithm S1 θ θ ηθL(i) , YB) end for 18: 19: end for 20: return θ rate of 1e 5 with total batch size of 8. Sequence Handling: The maximum sequence length is truncated at 1, 024 tokens. Continuous Simulation: Following our proposed framework, the number of continuous simulation steps τ is set to 4. For blockwise processing, the current block size is fixed at 512. During training, the number of transition tokens is dynamically sampled from the set {1, 2, 4, 8} to enhance the models robustness across varying generation densities. The mixing ratio is stochastically sampled from uniform distribution U(0.5, 1.0). C.3 Inference and Evaluation Setup To ensure fair and reproducible evaluation, we standardize our inference parameters across all datasets. We employ decoding temperature of 0.5 and fix the random seed to 42 to eliminate stochastic variance. For the proposed refinement mechanism, we perform grid search for the hyperparameter α within the candidate set {0.5, 0.6, 0.7, 0.8, 0.9} and report the performance associated with the optimal α for each setting. For evaluations based on Dream-Instruct-7B (Ye et al., 2025), we set the default generation length to 256 with 128 NFEs. For evaluations based on D2FLLaDA (Wang et al., 2025b), we set the default maximum generation length to 512."
        },
        {
            "title": "D More Analyses",
            "content": "D.1 Paradigm Consistency: Why MDLMs Rapidly Adapt to EvoToken-DLM We begin with brief overview of the training process for MDLMs. Formally, we characterize the model distribution via diffusion process consisting of forward and reverse process. The forward process q(X tX 0) gradually corrupts the initial sequence 0 by independently masking tokens with time-dependent probability [0, ], resulting in partially masked sequence where each token is replaced by special mask symbol with probability and remains unchanged with probability 1 . At = , the sequence becomes fully masked. Conversely, the reverse process learns to recover the original data distribution by iteratively predicting the masked tokens in as transitions from to 0. The training objective is defined as follows: 14 2 6 8 rank 61.2% 76.8% 91.5% 94.6% Table S1: Probability of the ground-truth token appearing in the Top-k predictions when substituting an input token in the noisy sequence with random token. The high hit rates demonstrate the models robust capability to infer the correct output based solely on contextual information. L(θ) t,X0,Xt (cid:34) (cid:88) i=1 (cid:104) xt = mask (cid:105) log pθ (cid:16) (cid:35) t(cid:17) , (S1) where denotes the sequence length. Under this training paradigm, the model gradually develops the capability to infer the potential probability distribution of each position based on its surrounding context. Notably, although the supervision is explicitly applied only to the mask tokens, the inherent generalization of the model enables it to support inference at all positions, even those not marked by mask. We further substantiate this observation through empirical analysis, as shown in Table S1. By intentionally replacing token at specific position with random token in noisy sequence, we observe the models ability to predict the ground-truth (GT) token at that location. Our statistical findings indicate that despite the random substitution, the GT tokens remain consistently concentrated among the top-ranked candidates in the models output distribution. This suggests that MDLMs possess robust, context-driven predictive mechanism that transcends the specific masking patterns encountered during training. The inherent capability of MDLMs to continuously predict tokens based on context aligns remarkably well with the core objective of EvoToken, which aims to refine its own tokens in realtime. This alignment facilitates seamless transition from standard MDLMs to the EvoToken-DLM framework. Leveraging this pre-trained inductive bias, the model requires only minimal supervised fine-tuning to adapt to the EvoToken paradigm. Essentially, the foundational training of MDLMs serves as robust prior, enabling the model to internalize the evolutionary refinement process with high training efficiency. 15 D.2 Adaptation Hurdles for AR Backbones: Causal Prior Mismatch In contrast to the seamless adaptation of MDLMs, backbones pre-trained under the autoregressive (AR) paradigm face significant challenges when transitioning to the EvoToken framework. This difficulty stems from fundamental mismatch between the AR prior and the iterative refinement nature of EvoToken. The primary objective of AR training is to predict the next token conditioned solely on preceding context, enforced by unidirectional causal attention mask. This intrinsic constraint prevents the model from performing bi-directional information aggregation, which is essential for iteratively updating and refining existing tokens based on full context. While some recent works attempt to bridge this gap by fine-tuning AR models into diffusionlike or blockwise models (e.g., SDAR (Cheng et al., 2025)), these models often remain heavily tethered to their original causal priors. Consequently, adapting such models to the EvoToken paradigm necessitates substantial training resources to override the deep-seated unidirectional bias. Due to these significant computational overheads, we do not conduct further exploration on these AR-based variants."
        },
        {
            "title": "E More Experimental Results",
            "content": "E.1 Additional Results Based on LLaDA-1.5 To verify the generalizability of EvoToken-DLM across different model versions, we extend our framework to the LLaDA-1.5 pretrained model via continuous trajectory supervision. Figure S2 illustrates the performance gains across multiple benchmarks. The results confirm that the improvements provided by our progressive evolution mechanism are generalizable. EvoToken-DLM consistently enhances reasoning precision compared to the binary masking baseline, even based on another pretrained model. E.2 Ablation Study on the Mixing Ratio α We investigate the sensitivity of EvoToken-DLM to the mixing ratio α in Figure S1. The results demonstrate that our framework maintains remarkably stable performance across the α [0.5, 0.9] range on all four evaluation datasets, highlighting its substantial algorithmic robustness. EvoTokenDLM effectively captures the essential refinement signals even under varying fusion intensities. The absolute gains ranging from +10.55% to +13.67% over the baseline, demonstrating that our approach effectively enhances the models reasoning capabilities across various inference granularities. Robustness to Block Size Variations: EvoToken-DLM demonstrates high resilience to changes in block size, yielding stable and superior results across all tested discretization settings. This suggests that the learned token evolution patterns are agnostic to specific block partitions. Consistency across Tasks: The superiority of our method is consistently maintained across diverse benchmarks, from symbolic reasoning (Countdown) to complex mathematical problem-solving (GSM8K, MATH500), further validating the generalizability of our approach in various downstream scenarios. Overall, these results underscore that EvoTokenDLM is not finely tuned for specific inference setting but rather provides fundamental enhancement to the underlying diffusion generation process. Figure S1: Performance comparison between the baseline and EvoToken-DLM with different α settings on various datasets. fact that performance remains consistent across diverse benchmarks, ranging from simple arithmetic to complex reasoning, underscores the robustness of the mechanism. E.3 Additional Results with Different Block Sizes To further evaluate the robustness and scalability of EvoToken-DLM, we conduct experiments across range of block sizes {8, 16, 32, 64}. As shown in Table S2, our method consistently outperforms both the original LLaDA-Instruct-8B baseline and the FT-baseline across all tested configurations and datasets. The experimental results highlight several key observations: Universal Performance Gains: EvoTokenDLM achieves significant accuracy improvements regardless of the block size. For instance, in the Countdown task, we observe Figure S2: Additional comparison of EvoToken and the binary masking baseline based on another pretrained model LLaDA-1.5. We apply continuous trajectory supervision and evaluate performance on various benchmarks. Table S2: Performance comparison on the Countdown, GSM8K, MATH500 and SVAMP datasets across various block sizes based on LLaDA-Instruct-8B. EvoToken-DLM is initialized from LLaDA-Instruct-8B and fine-tuned for 10k steps using continuous trajectory supervision. Comparisons are conducted against both the baseline model and the sft-baseline. All experiments are conducted with generation length of 256 and an NFE of 128. Block Size Method Countdown GSM8K MATH500 SVAMP 8 16 32 64 74.30% 76.50% 79.00% +4.70% 75.21% 79.08% 80.59% +5.38% 77.63% 78.62% 82.03% +4.40% 76.42% 78.01% 83.40% +6.98% 32.00% 31.60% 34.60% +2.60% 32.60% 32.00% 37.20% +4.60% 32.20% 31.20% 36.40% +4.20% 33.80% 34.60% 38.00% +4.20% 85.00% 87.33% 88.00% +3.00% 84.33% 88.67% 90.00% +5.67% 86.67% 87.00% 92.33% +5.66% 86.67% 87.33% 91.00% +4.33% Baseline FT-Base EvoToken Baseline FT-Base EvoToken Baseline FT-Base EvoToken Baseline FT-Base EvoToken 12.11% 17.97% 22.66% +10.55% 15.23% 16.80% 26.95% +11.72% 16.41% 16.80% 30.08% +13.67% 19.92% 19.14% 31.64% +11.72% 17 E.4 Additional Inference Examples of EvoToken-DLM We provide additional detailed visualizations of the internal refinement process to demonstrate how EvoToken-DLM iteratively refines intermediate states. As shown in Figure S3 and Figure S4, for the prompt involving arithmetic, the model progressively clarifies the soft token representations. The visualization highlights how uncertain embeddings at early simulation steps are refined into sharp, symbolically correct tokens as the step increases. Figure S3: Additional inference example with block size of 12, showing intermediate refinement states for selected subsequence across successive steps. We showcase the refinement states for the first 16 output tokens based on the prompt: \"Kyle bought last years best-selling book for $19.50. This is with 25% discount from the original price. What was the original price of the book?\". 18 Figure S4: Additional inference example with block size of 8, showing intermediate refinement states for selected subsequence across successive steps. We showcase the refinement states for the first 16 output tokens based on the prompt: \"In school there are 569 girls and 236 boys.How many more girls than boys does the school have?\". 19 E.5 Qualitative Comparisons for EvoToken-DLM and MDLMs Example 1 Question: Mattison is 60 inches tall. His two-story house is 7 times as high as he is. The floor to the ceiling on the first floor is thrice as high as he is. How high is the roof from the second floors floor in inches? GT: 240.0 MDLMS: To determine the height of the roof from the second floors floor, we need to follow these steps: 1. Calculate the height height of Mattisons two-story house. 2. Calculate the height of the floor to the ceiling on the first floor. 3. Determine the height of the second floor from the ground. 4. Calculate the height of the roof from the second floors floor. 1. The height of Mattisons two-story house is 7 times his height. Height of the house = 7 60 = 420 inches 2. The height of the floor to the ceiling on the first floor is 3 times his height. Height of floor to ceiling = 3 60 = 180 inches 3. The height of the second floor from the ground is the height of the first floor minus the height of the first floor. Height of second floor = 420 180 = 240 inches 4. The height of the roof from the second floors floor is the height of the second floor minus the height of the first floor. Height of roof = 240 180 = 60 inches <answer> 60 </answer> EVOTOKEN-DLM (OURS): Solution: Let H1 be the height of the first floor. Let H2 be the height of the second floor. Let be the the height of the house. We are given: Mattisons height, 60 inches. The height of the house is 7 times Mattisons height, so = 7 * 60 = 420 inches. The floor to the ceiling on the first floor is 3 times Mattisons height, so H1 = 3 * 60 = 180 inches. The height of the house is the sum of the height of the first floor and the height of the second floor: = H1 + H2 420 = 180 + H2 H2 = 420 - 180 H2 = 240 inches The height of the roof from the second floors floor is the height of the second floor, which is 240 inches. Final Answer: The final answer is 240 </reasoning> <answer> 240 </answer> 20 Example 2 Question: class of 30 students recently took test. If 20 students scored 80, 8 students scored 90, and 2 students scored 100, then what was the class average (the mean) on this test? GT: 84 MDLMS: To find the class average (the mean) on the test, we need to calculate the total score of all students and then divide by the total number of students. 1. Calculate the total score for the students who scored students scored 80: 20 80 = 1600 2. Calculate the total score for the students who scored 90: 8 90 = 720 3. Calculate the total score for the students who scored 100: 2 100 = 100 4. Add these totals together to get the total score: 1600 + 720 + 100 = 2420 5. Divide the total score by the total number of students to find the mean: 2420 30 = 80. </, reasoning> <answer> 80.67 </answer><eot_id> EVOTOKEN-DLM (OURS): Solution: To find the average score, we need the total sum of all scores and then divide by the number of students. 20 students scoring 80 : 20 * 80 = 1600 8 students scoring 90 : 8 * 90 = 2 students scoring 100 : 2 * 100 = 200 Total sum of scores = 1600 + 720 + 200 = 2520 Number of students = 30 Average = Total sum of scores / Number of students = 2520 / 30 = 84 Final Answer: The final answer is 84 </reasoning> <answer> 84 </answer> 21 Example 3 Question: Using only the provided numbers, create an arithmetic expression that evaluates to exactly the provided target number. You may use the operations +, -, *, and / as needed, but each number must be used exactly once. Numbers: [92, 43, 58]; Target: 77. GT: 92 - 58 + 43 MDLMS: To create an arithmetic expression that evaluates to 77 using the numbers 92, 43, and 58, we need to find combination of operations (+, -, *, /) that will result in 77. Lets start by considering the different operations: 1. **Addition and Subtraction**: - 92 + 43 = 135 - 92 43 = 49 - 92 + 58 = 150 - 92 58 = 34 2. **Multiplication**: - 92 43 = 3956 - 92 58 = 5376 - 43 58 = 3. **Division**: - 92 43 2.097 - 92 58 1.587 - 43 58 0.732 We need to find combination of these operations that results in 77. Lets try different combinations: 1. **Multiplication and**: 996 43 = 953 953 58 = 895 895 43 = 852 852 58 = 794 794 43 = 751 751 58 = 693 693 43 = 650 650 58 = 592 592 43 = 549 549 58 = 77 Thus, the expression 592 43 58 = 77 </answer> 592 43 58 </answer> EVOTOKEN-DLM (OURS): We need to find an arithmetic expression using the numbers 92, 43, and 58 that evaluates to 77. We operations allowed are +, -, *, and / Each number must be used exactly once. Lets try different combinations of operations: 1. **Subtraction**: 92 - 43 = 49, 49 - 58 = -9, -9 + 77 = 68 (not target) 2. **Multiplication**: 92 * 43 = 3916, 3916 - 58 = 3858, 3858 - 77 = 3981 (not target) 3. **Division**: 92 / 43 = 2, 2 * 58 = 116, 116 - 77 = 39 (not target) Lets try different approach: the. 92 - 43 = 49, 49 - 58 = -9 (not target) the. 92 / 43 = 2, 92 - 58 = 34 (not target) the. 92 - 58 = 34, 34 + 43 = 77 (target) The correct expression is 92 - 58 + 43. </reasoning> <answer> 92 58 + 43 </answer> 22 Example 4 Question: Faye was placing her pencils into rows with 22 pencils in each row. She had 6 packs of pencils each one having 14 pencils.How many pencils does she have? GT: 84. MDLMS: To determine the total number of pencils Faye has, we need to add the number of pencils in the rows to the number of pencils in the packs. First, we calculate the total number of pencils in the rows by Since there are 22 pencils in each row and Faye has multiple rows, we dont need to exact number of rows for this calculation. Next, we calculate the total number of pencils in the packs by multiplying the number of packs by the number of pencils in each pack. Faye has 6 packs, and each pack contains 14 pencils, so the total number of pencils in the packs is 6 14 = 84. Finally, we add the number of pencils in the rows to the number of pencils in the packs to get the total number of pencils. </reasoning> <answer> 116 </answer> EVOTOKEN-DLM (OURS): We need to determine the total number of pencils Faye has. We know that she has 6 packs with 14 pencils each. number of pencils = number of packs * number of pencils per pack number of pencils = 6 * 14 = 84 Faye is placing her pencils into rows with 22 pencils in each row. We need to find out how many rows she can make with 84 pencils. number of rows = number of pencils / number of pencils per row number of rows = 84 / 22 = 4 However the question asks for the total number of pencils, which is 84. Final Answer: The final answer is 84 </reasoning> <answer> 84 </answer>"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Zhejiang University",
        "Zhejiang University of Technology"
    ]
}