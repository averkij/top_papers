{
    "paper_title": "OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation",
    "authors": [
        "Jitesh Jain",
        "Zhengyuan Yang",
        "Humphrey Shi",
        "Jianfeng Gao",
        "Jianwei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through a vision perspective (objective), i.e., solely natural language supervision is sub-optimal for the MLLM's visual understanding ability. To that end, we propose OLA-VLM, the first approach distilling knowledge into the LLM's hidden representations from a set of target visual representations. Firstly, we formulate the objective during the pretraining stage in MLLMs as a coupled optimization of predictive visual embedding and next text-token prediction. Secondly, we investigate MLLMs trained solely with natural language supervision and identify a positive correlation between the quality of visual representations within these models and their downstream performance. Moreover, upon probing our OLA-VLM, we observe improved representation quality owing to the embedding optimization. Thirdly, we demonstrate that our OLA-VLM outperforms the single and multi-encoder baselines, proving our approach's superiority over explicitly feeding the corresponding features to the LLM. Particularly, OLA-VLM boosts performance by an average margin of up to 2.5% on various benchmarks, with a notable improvement of 8.7% on the Depth task in CV-Bench. Our code is open-sourced at https://github.com/SHI-Labs/OLA-VLM ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 5 8 5 9 0 . 2 1 4 2 : r OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary Embedding Distillation Jitesh Jain1,2*, Zhengyuan Yang2, Humphrey Shi1, Jianfeng Gao2, Jianwei Yang2 1SHI Labs @ Georgia Tech, 2Microsoft Research, Redmond https://github.com/SHI-Labs/OLA-VLM Figure 1. Different Paradigms for Incorporating Visual Information into LLMs. (a, b) Existing approaches [1, 41, 61] feed features from the visual encoder(s) into the LLM and train the model solely with natural language supervision, i.e., next (text) token prediction (NTP) objective to align the embedding space of the vision encoder(s) and the LLM. (c) We propose distilling target visual information into the intermediate representations of the LLM from set of target encoders (Etarget). We adopt predictive embedding [2, 4] optimization approach at selected LLM layers during training to minimize the embedding losses along with the next token prediction (NTP) objective loss function, resulting in vision-centric approach to training the Multimodal Large Language Model. We only use single base vision encoder during inference."
        },
        {
            "title": "Abstract",
            "content": "The standard practice for developing contemporary MLLMs is to feed features from vision encoder(s) into the LLM and train with natural language supervision. In this work, we posit an overlooked opportunity to optimize the intermediate LLM representations through vision perspective (objective), i.e., solely natural language supervision is sub-optimal for the MLLMs visual understanding ability. To that end, we propose OLA-VLM, the first approach distilling knowledge into the LLMs hidden representations from set of target visual representations. Firstly, we formulate the objective during the pretraining stage in MLLMs as coupled optimization of predictive visual embedding and next text-token prediction. Secondly, we investigate MLLMs trained solely with natural language supervision and identify positive correlation between the quality of visual representations within these models and their down- *Work done during JJs internship at MSR. Equal advising. stream performance. Moreover, upon probing our OLAVLM, we observe improved representation quality owing to the embedding optimization. Thirdly, we demonstrate that our OLA-VLM outperforms the single and multi-encoder baselines, proving our approachs superiority over explicitly feeding the corresponding features to the LLM. Particularly, OLA-VLM boosts performance by an average margin of up to 2.5% on various benchmarks, with notable improvement of 8.7% on the Depth task in CV-Bench. 1. Introduction In the last couple of years, open-source Multimodal Large Language Models (MLLMs) [35, 41, 75] development has witnessed rapid growth, owing mainly to the increasing number of powerful LLMs [17, 5860, 62] and large-scale datasets [11, 61, 64, 73]. The established practice to improve MLLMs at visual understanding tasks is training on increasing data with solely natural language supervision, i.e., next text token prediction objective [34, 48, 61]. Regardless of the data quantity, training an MLLM involves two primary stages: (i) Pre-Training (PT), aligning embeddings from vision encoder(s) to those inside the LLM, and (ii) Instruction Fine-Tuning (IFT), training the model on conversation data for downstream instruction following tasks. However, solely using natural language supervision during training does not guarantee the optimal quality of visual representations inside the LLM. To counter this issue, few works propose simply scaling the number of visual (input) encoders [21, 31, 32, 37, 42, 56, 61], or using visual cross-attention [1, 15] inside the LLM, leading to high compute and data costs for convergence during training. In this work, we posit that solely using natural language supervision for scaling MLLMs with more data, visual inputs, or network components often overlooks hidden opportunity to optimize the visual quality of representations inside the LLM directly. To that end, we propose to distill knowledge from set of teacher visual encoders into the LLMs representations during the PT stage through set of embedding losses (Fig. 1c). During inference, we only use single encoder, resulting in better trade-off between visual understanding performance and efficiency as compared to explicitly feeding (multiple) visual inputs to the LLM (Fig. 1a,b). Before experimenting with our embedding losses, we train probe for the representations from each layer inside the LLM (of the MLLM, LLaVA-1.5 [41], in our case) to analyze their quality against the target visual features. We choose features from visual encoders trained for three tasks as targets: image segmentation [29, 30, 33, 52, 76], depth estimation [69, 70] and image generation [50, 53], owing to their well-studied, fundamental nature and the first two being seminal perception abilities [20, 25]. Specifically, we train the probes for each task to predict the corresponding target encoders output. We find that the probing performance gradually improves with more data, indicating that the LLM enhances its visual understanding ability and, consequently, downstream performance, proving the effectiveness of our probing setup, as shown in Fig. 2a. Moreover, we establish that the middle LLM layers are optimal for embedding visual information inside the LLM based on the layer-wise trend for the probes performance. We provide more details about our probing setup in Sec. 3. With the knowledge regarding the nature of the different layers from the probing experiments, we investigate the effect of optimizing the information from the corresponding target encoders in the intermediate LLM representations. Inspired by the recent works in embedding predictive architectures for self-supervised learning [2, 26], we propose OLA-VLM to distill [27] knowledge from the target encoders into the LLMs representation space. Specifically, we optimize an embedding loss between the target feature and the embedding predictor output at the corresponding LLM layer (Fig. 1c). Note that we still use features from Figure 2. Probing reveals relationship between representation quality and performance. (a) We observe that increasing the amount of data and training solely with the next-token prediction objective enhances the visual representation quality within the LLM, resulting in improved performance, underscoring the effectiveness of our probing setup. (b) Our OLA-VLM exhibits superior visual representations and performance compared to LLaVA1.5 [41] under the same settings, demonstrating the effectiveness of minimizing the predictive embedding objective during training. base encoder as inputs to the language model. Furthermore, we incorporate specialized set of tokens, t, enriched with target-specific information into the LLMs input sequence, fostering an implicit visual chain of thought and enhancing the models ability to handle target informationfriendly queries. We hypothesize that optimizing the embeddings during the PT stage results in OLA-VLM outperforming the baselines owing to better initialization of the (MLP) projector for the IFT stage. We validate the hypothesis in Sec. 3 using probes for our OLA-VLM, demonstrating improved performance in the initial LLM layers, indicating more effective projection of vision features. Our experiments in Sec. 5 illustrate the effectiveness of our embedding optimization and special tokens, t, on various benchmarks while outperforming the baselines under the same settings. To summarize, our contributions are as follows: To the best of our knowledge, ours is the first study to analyze the quality of visual representations inside MLLMs. We probe the LLM layers in LLaVA-1.5 [41] based models and present our findings about enhancing MLLMs visual understanding ability with increasing training data and layer index. We present OLA-VLM, the first approach to distill knowledge from target encoders into language model representations through predictive embedding optimization. Furthermore, we integrate tokens enriched with target information into the input sequence, resulting in an implicit visual chain-of-thought technique. We conduct extensive experiments to demonstrate the 2 Figure 3. Probing Visual Representations across LLM layers in Multimodal Large Language Models. (1) As shown in the first row, the multi-encoder baseline has the best probing performance owing to the additional feature inputs. The performance of probes trained on our OLA-VLM falls between the two baselines, demonstrating the effectiveness of our embedding distillation approach in learning an improved projector while only using single encoder during inference. (2) We observe that the probing performance for single encoder models trained solely with natural language supervision improves as the training data of the base MLLM increases, indicating that the LLM improves its visual representations of the world with more training data. In the last row, we observe that our OLA-VLM (base setting) outperforms the LLaVA-1.5 model trained with more data during the PT stage, demonstrating the effectiveness of our approach. improvements of our proposed approach over the corresponding single and multi-encoder baselines on various benchmarks, including 8.7% and 5.6% on the Depth and Distance task on CV-Bench, respectively. 2. Related Works Another line of work explores using either multiple encoder features [37, 55, 61, 74] or multiple visual modalities [12, 31, 42] as inputs to the LLM for improved visual (spatial) reasoning performance. Our approach uses single base vision encoder while embedding information from multiple encoder features into the LLM at different layers. 2.1. MLLMs for Visual Reasoning 2.2. Probing Foundational Models Contemporary MLLMs have three primary components: vision encoder(s), projector, and LLM. One line of work [5, 22, 34, 35, 40, 41, 75] uses single pretrained vision encoder [13, 19, 33, 44, 49, 72] and trains projector, like an MLP [41] or QFormer [14] to align the vision encoder features with the LLM. few recent approaches [18, 57] attempt to develop native MLLMs by directly feeding image patches into the LLM without using any pre-trained encoder. Some works [1, 15, 64, 68] train visual experts or cross-attention modules inside the LLM to disentangle visual and language information inside the LLM, often requiring millions of training data samples. We choose CLIP-ViT-L and CLIP-ConvNeXT-XXL as the base vision encoders for our experiments. OthelloGPT [36] probed the features from GPT-2 [48] trained on sequences from board game, Othello, and found that the probes were able to learn the board state, indicating the ability of sequence models to learn world representations. recent work probes the features from foundational vision encoders [3] for 3D tasks. In our work, we probe the embeddings from the LLM (of an MLLM) layers against target visual features. To the best of our knowledge, we are the first to establish relationship between visual representations quality inside the MLLM and their performance. 2.3. Self-Supervised Learning Distilling information [27] from target encoder into source encoder is well-established technique to improve 3 the source encoders embeddings for downstream task [8, 9, 24]. Recently, I-JEPA [2, 4] proposed an embedding predictive architecture to improve representations inside source encoder by comparing the target encoder features and mapped source encoder features with trained predictor. concurrent work, REPA [71] improved DiTs [47] at image generation by distilling information from DINOv2 [16, 46]. Recently, few works [51, 65] first distill information from target encoders into single model and then leverage the resulting model as the vision encoder in an MLLM. Unlike previous works, we distill information from multiple vision encoders into the LLM embedding space during pre-training using image-text pairs, resulting in more vision-centric MLLM training approach. 3. Visually Probing Language Embeddings In this work, we aim to improve the representations inside the LLM with features from target encoders. However, first, we methodically analyze the quality of such features inside the LLM (of the MLLM) through probing. We choose encoders trained for three visual tasks as targets: image segmentation, depth estimation, and image generation. Our choice is guided by their fundamental and interpretable nature, i.e., visualizing the embedding representations quality through the respective task decoders. We use the encoders from Swin-L OneFormer [29, 43], DINOv2L Depth Anything-v2 [19, 70], and ViT-L unCLIP-SD2.1 [19, 49, 50, 53] as the segmentation (Eseg), depth (Edepth) and generation (Egen) encoders to obtain the target features for probing, respectively. Our probing setup is quite simple. We train singlelayer Perceiver Resampler [28] as the probe head at every layer of the LLM of an MLLM model for each of the three target task features. We use resampler probe head to accommodate the different sequence lengths of the target features and representations inside the LLM. The architecture of the probe head is similar to that of the Emb Predictor shown in Fig. 4. We input set of learnable latent queries for each layers probe head while using tokens from the corresponding layer as keys for cross-attention inside the resampler block. During training, we set the learning rate as 1e3 for the probes and minimize the smoothL1-loss objective with batch size of 256. We train the probes for two epochs on the 118k images from the COCOtrain2017 [38] set with the text query as Describe the image in two lines.. We analyze the following LLaVA-1.5 models with LLaMA-3-8b [58] as the decoder LLM and CLIP-ConvNeXT-XXL [13, 44] as the base encoder: Single-Encoder MLLM with features from Ebase passed through an MLP into the LLM. Multi-Encoder MLLM with features from Ebase, Egen, Edepth, and Eseg concatenated in the feature dimension (in order) and passed through an MLP into the LLM. OLA-VLM with features from Ebase passed through an MLP into the LLM and information from Egen, Edepth, and Eseg distilled into the LLM representations. For evaluating probe, we compute the cosine similarity between the probe outputs and the corresponding target features over the 5k images from the COCO-val2017 [38] set. For example, for probe trained against the depth representations, we calculate the cosine similarity between the depth probe output and features from Edepth to get the probing performance at each layer. Layerwise Trend. Upon evaluating the probes in the single encoder baseline, we observe an interesting trend that the middle (12-24) layer probes show the best representation quality for the depth and seg probing tasks, with an upward trend in quality in the initial layers and downward trend in the deeper layers, as shown in Fig. 3. Surprisingly, the probes trained to predict the features from Egen have fairly high cosine similarity (greater than 0.7) for all layers with some irregularities. We attribute the mentioned phenomenon to the choice of CLIP-based [49] Egen that already has language-aligned features, unlike Edepth and Eseg. Visual Encoding Approach. As shown in the first row of Fig. 3, the probes for multi-encoder MLLM learn better representations than the probes for the single-encoder MLLM. Interestingly, for the former, the depth and seg probe performance remain at the same level till layer 20 and then follow downward trend, indicating the possibility of the deeper layer representations becoming text-rich in deeper layers [39]. We do not observe downward trend for gen probing, owing to the text-aligned target gen features. We also train probes on our OLA-VLM and observe that they fall between the two baselines, serving as good trade-off between efficiency and visual representation accuracy. Training Data. We study the effect of scaling training data on the visual representation quality inside the singleencoder MLLM in the second row of Fig. 3. We analyze four model categories: (i) PT: model after the PT stage, (ii) PT + 50% IFT: model with complete PT and trained on only 50% of the IFT data, (iii) PT + IFT: model with complete PT and IFT, and (iv) PT + VPT + IFT: model with an extra training stage on ALLaVA-Caption [6] data, during which the whole model is trained (see Sec. 5 for more details). We observe that with an increase in the amount of training data for the probed model, the probes show gradual improvement, indicating that the representations of the visual world inside MLLMs and, consequently, performance on downstream tasks improve with just natural language supervision on more data (Fig. 2a)! For experimental completeness, we also report probing for the single-encoder LLaVA-1.5 model trained with additional ALLaVA-Caption [6] data during the PT stage. As illustrated in the third row of Fig. 3, while the inclusion of additional PT data improves LLaVA-1.5s performance on Figure 4. Architecture for OLA-VLM. During Pre-Training (PT), we optimize an embedding loss at specific layers for each target encoder: layers D, S, and for the depth, segmentation, and generation tasks, respectively. We use resampler-based embedding predictor [28], denoted as Pl {task} at each layer l, to output predictions. Each predictor takes in two inputs: set of learnable queries (Qtask) and the token sequence from layer l, with special tokens for other tasks omitted. The final loss is the sum of embedding losses across all selected layers and the next-token prediction objective. During IFT, we train with only the next-token prediction objective while keeping the special tokens frozen so as not to affect their task-specific nature. CV-Bench, our OLA-VLM achieves superior results with lesser PT data, highlighting the effectiveness of our embedding optimization technique. Notably, there is significant improvement in representation quality for the seg probing task with the use of ALLaVA-Caption during PT, which we attribute to the datasets detailed captions fostering enhanced alignment [6]. Please refer to Tab. 7 for results on other benchmarks. 4. Embedding Visual Information into LLM The current trend in developing MLLMs for visual reasoning is gluing vision encoder(s) to an instruction-tuned LLM with projector and training the network to minimize the cross-entropy loss for classification over the LLMs vocabulary for next token prediction (NTP). In our attempt to add vision perspective to training MLLMs, we aim to optimize predictive visual embedding objective for the intermediate LLM layers along with the standard next-token-prediction objective during the PT stage. In other words, we distill auxiliary visual (task) information into the LLMs intermediate representations rather than feeding the visual features into the LLM from the target encoders. We hypothesize that such an approach leads to better projector initialization for the IFT stage. Our hypothesis is validated in Fig. 3, as probes for our OLA-VLM significantly outperform the single encoder baseline, especially in the initial layers. Moreover, we leverage set of learnable special Nseek tokens (t) to capture task-specific information for each additional representation. The are derived from the latent queries input to the corresponding embedding predictor, ensuring the tokens are rich in task-specific information. Therefore, g, d, and denote the tokens containing information for gen, depth, and seg tasks, respectively. During IFT, we train the MLLM using only the next-token prediction objective, without any of the embedding predictors, while keeping g, d, and in the input sequence to provide the model context about the auxiliary information, resulting in an implicit visual chain-of-thought technique. 4.1. Multi-Encoder Feature Distillation Given set of target visual encoders, Edepth, Eseg, and Egen, our goal is to distill the information from their feature outputs into the LLMs representation space at specific layers. We solve this challenge by minimizing an embedding loss 5 between the output of an embedding predictor and the target features at each layer. Since the token sequence inside the LLM has different length as compared to the target features, we use single-layer Perceiver Resampler [28] as the embedding predictor (Pl {task}) that takes as inputs the learnable latent queries (Q{task}) along with the token sequence output from layer of the LLM and outputs prediction, pl. We set the dimension of the Q{task} such that it matches the dimension of the output from the corresponding E{task}. In our experiments, the number of queries in Qdepth, Qseg, and Qgen are 576, 576, and 1, respectively. To amplify the target information inside the MLLM, we use set of special Nseek tokens, for each task. We append to the image tokens in the LLMs input sequence. Specifically, we average pool Qdepth and Qseg into Nseek number of tokens to obtain the and tokens, respectively. Since the number of target tokens for generation features is only one, we average pool to obtain Qgen. During PT, we only train the MLP projector, embedding predictors, and the special tokens t. We extract the tokens corresponding to the system prompt, input image, the corresponding special tokens (t), and the text query from layers output sequence as the input keys to the embedding predictor. For example, Pdepth takes the token sequence {sysimgdtxt} as the key input. We present the overview of our OLA-VLM in Fig. 4. 4.2. Predictive Embedding Optimization At the core of our approach is indirectly optimizing the projector during the PT stage by minimizing an embedding loss for each target representation at specific layers. As shown in Fig. 4, we feed the outputs from the D, S, and layers of the LLM into the corresponding embedding predictor to obtain the predictions for computing an embedding loss (Lemb), which is weighted sum of Smooth-L1Loss [23] and contrastive (InfoNCE) loss [63]. We denote the sets of layers used to compute embedding losses against target features from Edepth, Eseg, and Egen as D, S, and G, respectively. The final embedding loss for each target is sum of losses over all layers from the corresponding layer set. We compute the final loss during PT as the sum of the next-token-prediction objective and embedding losses. Ll sL1(pl, t) = (cid:40) 0.5 (pl t)2, pl 0.5, if pl < 1, otherwise. Ll contrastive = log exp(sim(pl j=1 exp(sim(pl i, ti)/τ ) i, tj)/τ ) (cid:80)B Ll emb = λsL1Ll sL1 + λcontrastiveLl (cid:88) D/S/G emb = Ll emb contrastive lD/S/G LPT = LNTP + λdepthLD emb + λsegLS emb + λgenLG emb (1) We denote pl and as the outputs from the embedding predictor (Pl {task}) at layer and the target task features, respectively, with pl denoting the ith element in batch of embeddings with global batch size of aggregated over all GPUs. We denote τ (initialized to 2.0) as the learnable scaling factor for the contrastive loss. The weights for smoothL1-and contrastive losses are λsL1 = 1 and λcontrastive = 0.3, respectively, at all selected layers. We set the weights for embedding losses, λdepth = λseg = λgen = 0.5. Other Architecture Details. As shown in Fig. 4, we use the DINOv2-L [46] from the Depth Anything v2 model [69, 70] as Edepth, the Swin-L [43] from the OneFormer [29] trained on COCO-train2017 [38] set as the Eseg, and the CLIP-ViTL [19, 49] from the unCLIP-SD-2.1 [50, 53] as the Egen. In the input sequence to the LLM, we append the gen, depth, and seg special tokens in that order to the image tokens. Therefore, given an image-text pair during training, the input token arrangement is {sysimggdstxt}, where sys, img, and txt denote the tokens corresponding to the embedded system prompt, image embeddings, and embedded text query, respectively. The output feature dimensions from Egen, Edepth, and Eseg are (1, 1024), (576, 1024), and (576, 1536), respectively, corresponding to the final layer outputs of each target encoder. 5. Experiments In this section, we first provide comprehensive comparison of our methods performance to that of the base MLLM, LLaVA-1.5 [41] across different base vision encoder and decoder LLM choices in Tab. 1. Next, we provide experimental results with 2.5-stage training strategy, composed of an extra Visual Pre-Training (VPT) stage that involves training on the ALLaVA-Caption-663K [6] dataset to demonstrate the scalability of our approach to more data. Lastly, we methodically study various design factors, including the optimal choice of layers for embedding additional visual information and the number of special tokens (t) through series of ablations. 5.1. Implementation Details Training. During the PT stage, we use the LLaVA558K [40, 41] dataset to train our model for an epoch with lr of 1e3. We only train the (MLP) projector [41], the embedding predictors, and the special tokens: g, d, and s. During the IFT stage, we use the LLaVA-665K [41] dataset and train the projector and LLM for one epoch with an lr of 2e5 while keeping the vision encoder and all the special tokens frozen. When using VPT, we leverage the ALLaVACaption-663K [6] dataset to train the whole model (except the special tokens) for one epoch with an lr of 2e5. During the PT stage, we train our model with next token prediction and the embedding prediction objectives. During the VPT Method Phi3-4k-mini LLaVA-1.5 OLA-VLM (ours) LLaVA-1.5 OLA-VLM (ours) Llama3-8b Encoder CLIP-ViT-L CLIP-ViT-L CLIP-ConvNeXT-XXL CLIP-ConvNeXT-XXL CLIP-ViT-L LLaVA-1.5 CLIP-ViT-L + Edepth + Eseg + Egen LLaVA-1.5 (feat concat.) LLaVA-1.5 (token concat.) CLIP-ViT-L + Edepth + Eseg + Egen OLA-VLM (ours) CLIP-ViT-L LLaVA-1.5 OLA-VLM (ours) CLIP-ConvNeXT-XXL CLIP-ConvNeXT-XXL CV-Bench General Count2D Depth3D Relation2D Distance3D MMStar RWQA OK-VQA Avg 52.4 52.4 51.8 49.4 50.4 45.3 45.9 51.3 54.1 57. 67.2 68.7 70.8 72.5 73.3 75.5 75.7 74.2 62.8 71.5 75.2 76.0 74.0 77. 64.9 70.9 68.9 69.4 69.5 66.8 56.3 56.7 55.3 60.3 48.7 54.3 52.7 54.3 49.8 52. 36.5 36.0 36.4 38.4 38.8 36.1 37.8 39.5 37.4 38.5 57.1 58.0 58.0 58. 57.8 57.5 56.5 57.9 57.5 55.0 56.7 56.4 55.9 56.5 56.9 58.3 59.3 56.6 56.3 59. 57.3 57.7 57.4 58.9 55.1 56.8 56.7 57.6 55.3 57.3 Table 1. Comparisons to Single and Multi-Encoder Baselines. We present results across different base encoders and decoder LLMs. Our OLA-VLM outperforms the single encoder and multi encoder LLaVA-1.5 [41] by up to 2.5% and 0.9% on average across various benchmarks, respectively. The best numbers are set in bold for every base-encoder and decoder LLM combination. (a) depth embedding loss (b) seg embedding loss (c) gen embedding loss Figure 5. Ablating choice of layers for Lemb on CV-Bench. We observe optimal performance by computing the embedding losses for depth, seg, and gen features at the 18th, 18th, and 20th layer, respectively. We do not use any special tokens for this ablation study. Method LLM Count2D Depth3D Relation2D Distance3D Avg LLaVA-1.5 OLA-VLM (ours) Phi3-4k-mini Phi3-4k-mini LLaVA-1.5 Llama3-8b OLA-VLM (ours) Llama3-8b 49.7 53.7 56.3 60.0 70.0 72. 76.8 75.0 72.6 73.1 73.1 70.8 58.7 58.5 50.3 55.2 61.8 63. 63.3 64.6 Table 2. Scalability over more data with VPT. Our OLA-VLM outperforms LLaVA-1.5 on average across different CV-Bench tasks. We use CLIP-ConvNeXT-XXL [13] as the base encoder. Nseek single-encoder 0 4 8 12 16 24 CV-Bench2D CV-Bench3D MMStar RWQA Avg 53. 38.8 61.0 56.0 57.8 56.1 60.3 58.6 58.7 56.6 55.7 62.0 57.9 64.2 60.7 63.6 60. 40.1 38.1 39.5 37.9 37.1 39.3 56.3 56.6 57.9 57.5 54.5 57.4 53.6 53.2 55.1 53.7 52.9 53.1 G {18} {20} {8;14} {8;20} {18;20} {16;18;20} {18} {18} {10;16} {10;18} {18;20} {16;18;20} {20} {20} {12;18} {12;20} {16;20} {16;18;20} CV-Bench2D CV-Bench3D MMStar Avg 52.6 52.4 50.6 54.1 52.0 51.7 58.9 57.6 56.5 58.6 55.8 56.8 37.9 38.8 38.8 39.5 40.8 37. 60.9 60.8 56.4 64.2 59.5 61.3 Table 3. Ablations on Layer sets for embedding losses. Setting D={8, 20}, S={10, 18}, and G={12, 20} performs the best. PT IFT single-encoder CV-Bench2D CV-Bench3D MMStar RWQA Avg 53.4 56.0 38.8 61.0 57. 57.7 58.6 59.1 62.9 64.2 58.3 38.8 39.5 38.3 57.5 57.9 56.2 54.2 55.1 53.0 Table 4. Ablations on training stages for embedding losses. Using the embedding losses only during PT is optimal. and IFT stages, we only use the next-token prediction objective. Unless mentioned otherwise, we report results with models trained with PT and IFT stages. We train all our models, including the single and multiencoder baselines, on 16 AMD 192G-MI300X GPUs with 7 Table 5. Ablations on Nseek. Setting number of special tokens (t) to 8 for each task performs best. The setting with no special tokens (Nseek = 0) also outperforms the single encoder baseline. during IFT CV-Bench2D CV-Bench3D MMStar RWQA Avg 55.1 52.3 frozen learnable 39.5 39.0 57.9 57.3 64.2 56.1 58.6 56.9 Table 6. Ablation on the nature of special tokens during IFT. Keeping frozen during IFT aids in keeping their task-specific nature intact, resulting in better performance. batch size of 256 during PT and 128 during IFT and VPT. We use CLIP-ViT-L [19, 49] and Llama3-8b [58] as the default base vision encoder and decoder LLM, unless mentioned otherwise. We set Nseek to 8 by default. We set D, S, and to {8, 20}, {10, 18}, and {12, 20}, respectively. Evaluation. We primarily evaluate OLA-VLM for visioncentric abilities on CV-Bench (Cambrian Vision centric Benchmark) [61] and report results for all four tasks in CV-Bench: Count (2D), Relation (2D), Depth (3D) and Distance (3D), for fine-grained understanding of the imMethod PT IFT LLaVA-1.5 LLaVA-1.5 LLaVA-665k LLaVA-558K + ALLaVA-Caption-663K LLaVA-665k LLaVA-558K CV-Bench2D CV-Bench3D MMStar OK-VQA Avg 52.4 53.1 56.0 57.5 60.0 56.8 37.4 37.1 56.3 60.8 OLA-VLM LLaVA-558K LLaVA-665k 60.8 62.2 38.5 59. 55.1 Table 7. Using additional data during PT v/s embedding optimization. Our OLA-VLM demonstrates superior performance than the LLaVA-1.5 model trained on with additional ALLaVA-Caption [6] data during the PT stage, underscoring the effectiveness of our approach. 5.3. Ablations Layer Sets for Embedding Losses. The choice of layers in D, S, and for the corresponding embedding losses is crucial design choice with significant effect on the performance. We study the relation between performance and the position of an embedding loss, one layer and loss type at time for Llama-3-8b [58] in Fig. 5. We find that setting D, S, and as {18}, {18}, and {20}, respectively results in most optimal performance individually. We also ablate on the choice of layers sets while using multiple embedding losses at the same time. As shown in Tab. 3, setting D, S, and as {8, 20}, {10, 18}, and {12, 20}, respectively, performs the best overall for the 32-layer Llama3-8b [58]. Training Stage for Embedding Optimization. We observe that adding embedding losses during IFT results in performance drop compared to using them only during the PT stage, as shown in Table 4. We attribute this to the interference of vision-centric supervision with task-aligned natural language supervision during IFT. Additionally, the second row of Table 4 shows that using learnable tokens in the sequence without embedding loss slightly improves over the baseline but remains inferior to our OLA-VLM. Number of Special Tokens. We analyze the effect of the number of special tokens per target feature (t) on the performance in Tab. 5. Setting Nseek as eight results in the best average performance across benchmarks. Nature of Special tokens during IFT. As shown in Tab. 6, keeping the special tokens (t) frozen during the IFT stage performs better as compared to making them learnable. We attribute frozen tokens performing better to the gradients from natural language supervision interfering with the vision-centric information stored in the special tokens during IFT if those are set as learnable parameters. Visualizing Embedding Predictions for Target Tasks. We visualize the visual quality of LLM representations after the PT stage for our OLA-VLM using the decoders from the corresponding target models, as shown in Fig. 6. We observe that the decoder outputs have good object shape and boundary quality, demonstrating the successful representation optimization with our embedding losses. 5.4. Evaluating Probes on Downstream Tasks We evaluate the probes trained against the target features on the corresponding downstream target tasks, i.e., image generation, depth estimation and image segmentation. To Figure 6. Visualizing Embedding Predictor Outputs after the PT stage. The quality of the decoded representations indicates the effectiveness of our embedding optimization. provements brought about by our embedding optimization. For ablations, we report the average accuracy over the 2D (count and relation) and 3D (depth and distance) task categories. We also evaluate our models general visual reasoning capabilities on the MMStar [7], RWQA (RealWorldQA) [66], and OK-VQA [45] benchmarks. 5.2. Main Results As shown in Tab. 1, our OLA-VLM outperforms the single encoder baseline, i.e., LLaVA-1.5 [41] across different base encoder [13, 19, 49] and decoder LLM [58, 59] combinations. Specifically, Llama3-8b [58] based OLA-VLM outperforms LLaVA-1.5 by 5.6% and 4.5% on the Distance and Relation task, respectively for the CLIP-ViT-L base encoder and by 8.7% on the Depth task for the CLIPConvNext-XXL base encoder. Furthermore, we compare our approach to the corresponding two variants of multiencoder baselines: (i) feat concat.: features from all encoders are concatenated along the feature dimension and passed into single MLP; and (ii) token concat.: features from all encoders are first passed through separate MLP and then concatenated along with token dimension after average pooling the sequence outputs from Edepth and Eseg into eight tokens each. As shown in Tab. 1, our OLA-VLM outperforms the feature concat. and token concat. baselines, showing the effectiveness of our approach while only using single encoder during inference. Scalability with VPT. We report results with the 2.5 training (PT+VPT+IFT) stages setup for the LLaVA-1.5 [41] and our OLA-VLM in Tab. 2. our OLA-VLM outperforms baseline model on average across all CV-Bench tasks, demonstrating the scalability of our embedding optimization approach with more training data. Probed Model FID [54] () DA-2K % Acc. [70] () % mIoU [29] () LLaVA-1.5 OLA-VLM (ours) Target Encoder 23.1 22.4 18. 66.4 77.8 97.3 39.3 45.4 64.5 Table 8. Quantitative Evaluation on target probing task. Probes trained for our OLA-VLM perform significantly better as compared to the probes trained on baseline LLaVA-1.5 [41]. obtain the predictions, we feed the outputs from the probes into the decoder from the corresponding target model. Specifically, we report the FID [54] scores on 5k images from COCO-val2017 [38] for gen probes, accuracy on the DA-2K [70] benchmark for depth probes, and mIoU [29] on the COCO-val2017 [38] set for seg probes. We average the scores over all layers for easier comparison. As shown in Tab. 8, probes trained for our OLA-VLM outperform those for the baseline LLaVA-1.5 [41] model across all probing tasks, proving the improved visual representation quality owing to our embedding optimization approach. 6. Future Work With OLA-VLM, we enhanced the quality of representations inside the LLM with guidance from target encoders trained for depth, segmentation, and generation tasks. Looking ahead, incorporating more teacher encoders, such as SigLIP [72], and InternViT [10, 11] offer promising pathway to extend our approach to improving general reasoning abilities. Additionally, applying predictive embedding optimization for low-level information like motion control [4] while training on videos could improve MLLMs spatial and temporal reasoning in the future. 7. Conclusion In this work, we began by probing MLLMs and established relationship between the quality of visual representations within the LLM and performance. Building on these insights, we introduced OLA-VLM, the first approach to distill knowledge from target encoders into the LLM via predictive embedding optimization during the PreTraining stage, complementing the next token prediction objective. We validated that our embedding optimization results stronger vision-language alignment before the IFT stage. Through extensive experiments, we demonstrate OLA-VLMs superiority to the corresponding baselines in terms of both representation quality and performance on vision-centric benchmarks. We hope our work can serve as an inspiration to the community to develop mixed-modality optimization techniques for improving future MLLMs. Acknowledgements. We extend our heartfelt gratitude to Fangrui Zhu, Reuben Tan, Min Shi, Bhavika Devnani, Fiona Ryan, and Chieh-Yun (Astrid) Chen for their valuable feedback and insightful discussions. We also sincerely thank the GCR team at Microsoft for their support in resolving frequent infrastructure challenges, which enabled our experimentation. Lastly, we thank the ML Center @Georgia Tech and Microsoft Research for supporting this work."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. 1, 2, 3 [2] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In ICCV, 2023. 1, 2, 4 [3] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In CVPR, 2024. 3 [4] Adrien Bardes, Quentin Garrido, Jean Ponce, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv, 2024. 1, 4, 9 [5] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer. arXiv, 2024. 3 [6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model, 2024. 4, 5, 6, [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? In NeurIPS, 2024. 8 [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 4 [9] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning, 2020. 4 9 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv, 2023. [11] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 9 [12] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 3 [13] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. 3, 4, 7, 8 [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 3 [15] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv, 2024. 2, 3 [16] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. 4 [17] DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. 1 [18] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. 3 [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3, 4, 6, 7, 8, 13 [20] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep netIn Advances in Neural Information Processing Syswork. tems, pages 23662374, 2014. 2 [21] Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, Ming Zhang, Caishuang Huang, Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. Mousi: Poly-visual-expert vision-language models. arXiv, 2024. 2 [22] Chunjiang Ge, Ziming Wang Sijie Cheng, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Zheng. Convllava: Hierarchical backbones as visual encoder for large multimodal models. arXiv, 2024. [23] Ross Girshick. Fast r-cnn. In ICCV, 2015. 6 [24] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent: new approach to self-supervised learning, 2020. 4 [25] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross GirIn Proceedings of the IEEE Intershick. Mask R-CNN. national Conference on Computer Vision, pages 29612969, 2017. 2 [26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2 [27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. In NeurIPS Deep Learning Workshop, 2014. 2, 3 [28] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Henaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver io: general architecture for structured inputs & outputs. In ICLR, 2022. 4, 5, 6 [29] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. OneFormer: One Transformer to Rule Universal Image Segmentation. In CVPR, 2023. 2, 4, 6, [30] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven Walton, and Humphrey Shi. Semask: Semantically masking transformer backbones for effective semantic segmentation. In ICCVW, 2023. 2 [31] Jitesh Jain, Jianwei Yang, and Humphrey Shi. VCoder: Versatile Vision Encoders for Multimodal Large Language Models. In CVPR, 2024. 2, 3 [32] Oguzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. BRAVE: Broadening the visual encoding of vision-language models. In ECCV, 2024. 2 [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In ICCV, 2023. 2, 3 [34] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 3 [35] Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, and Longyin Wen. CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts. In NeurIPS, 2024. 1, 3 [36] Kenneth Li, Aspen Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring sequence model trained on synthetic task. In ICLR, 2023. [37] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya 10 Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814, 2023. 2, 3 [38] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context. In ECCV, 2014. 4, 6, 9 [39] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. arXiv, 2024. 4 [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 3, [41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 1, 2, 3, 6, 7, 8, 9, 13, 14, 16 [42] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer: vision-language model with multi-task experts. TMLR, 2024. 2, 3 [43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 4, 6 [44] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, 2022. 3, 4 [45] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In CVPR, 2019. 8 [46] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4, 6 [47] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 4 [48] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. arXiv, 2019. 1, 3 [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv, 2021. 3, 4, 6, 7, 8, 13 [50] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv, 2022. 2, 4, 6 [51] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In CVPR, 2024. 4 [52] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 4, 6 [54] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, 2020. Version 0.3.0. 9 [55] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv:2408.15998, 2024. 3 [56] Sirnam Swetha, Jinyu Yang, Tal Neiman, Mamshad Nayeem Rizve, Son Tran, Benjamin Yao, Trishul Chilimbi, and Mubarak Shah. X-former: Unifying contrastive and reconstruction learning for mllms. In ECCV, 2024. 2 [57] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv, 2024. [58] LLaMA-3 Team. The llama 3 herd of models, 2024. 1, 4, 7, 8, 13 [59] Phi3 Team. Phi-3 technical report: highly capable language model locally on your phone. arXiv, 2024. 8 [60] Qwen2 Team. Qwen2 technical report. arXiv, 2024. 1 [61] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv, 2024. 1, 2, 3, 7, 13 [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and finetuned chat models. arXiv, 2023. 1 [63] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv, 2018. [64] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan 11 Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. arXiv, 2023. 1, 3 [65] Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps clip see better. arXiv, 2024. 4 [66] xAI. grok, 2024. 8 [67] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv, 2023. 13 [68] Yifan Xu, Xiaoshan Yang, Yaguang Song, and Changsheng Xu. Libra: Building decoupled vision system on large language models. In ICML, 2024. [69] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 2, 6 [70] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. 2, 4, 6, 9, 14 [71] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv, 2024. 4 [72] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 3, 9 [73] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 1 [74] Xiangyu Zhao, Xiangtai Li, Haodong Duan, Haian Huang, Yining Li, Kai Chen, and Hua Yang. Towards semantic equivalence of tokenization in multimodal llm. arXiv preprint, 2024. 3 [75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv, 2023. 1, [76] Fangrui Zhu, Jianwei Yang, and Huaizu Jiang. Towards flexible visual relationship segmentation. In NeurIPS, 2024. 2 12 key input to emb. predictor CV-Bench2D CV-Bench3D MMStar RWQA Avg img 50.7 sys img 54.5 sys img txt 55.1 38.4 38.8 39.5 56.7 57.4 57. 54.6 63.0 64.2 53.0 58.7 58.6 Table I. Key input to the Embedding Predictor.. Feeding the tokens corresponding to the system prompt, image embeddings, corresponding special tokens, and the text query is optimal. mode LLaVA-1.5 depth seg gen depth + seg depth + gen seg + gen depth + seg + gen CV-Bench2D CV-Bench3D MMStar Avg 51.9 56.0 61.0 38. 58.6 56.2 56.2 58.6 53.6 54.2 58.6 63.5 57.6 65.8 61.8 61.8 60.2 64. 38.8 38.2 39.3 38.6 38.8 39.3 39.5 53.6 50.7 53.8 53.0 51.4 51.2 54. Table II. Embedding Optimization Modes. Using the depth, seg, and gen embedding losses at the same time is optimal. order Count2D Depth3D Relation2D Distance3D Overall LLaVA-1.5 s d s d s d d g d 50.4 49.4 51.6 48.7 46.7 51.3 50.9 73. 68.7 72.8 71.3 71.3 74.2 68.8 64.9 69.2 70.3 65.2 71. 69.4 70.0 48.7 56.2 54.5 52.5 50.8 54.3 50.5 58. 59.9 61.4 58.5 58.9 61.4 59.2 Table III. Order of different special tokens in the input sequence to the LLM. Appending the gen, depth, and seg tokens (in that order) in the LLMs input sequence after the image tokens is the optimal setup."
        },
        {
            "title": "Appendix",
            "content": "In this appendix, we first share additional ablations on the effect of order of different special tokens (g,d,s) in the input sequence to the LLM. We also ablate different key input possibilities to the embedding predictor in Appendix A. We use CLIP-ViT-L [19, 49] and Llama3-8b [58] as the base vision encoder and decoder LLM, respectively, for the ablations, unless mentioned otherwise. Next, we provide qualitative analysis from our probing experiments including the effect of substituting image input with equivalent text input to the MLLM in Appendix B. A. Additional Ablations Input tokens to Embedding Predictor. As shown in Tab. I, we find that including the tokens corresponding to the system prompt in the key input to the embedding predictor is critical for performance. We attribute it to system tokens having high attention scores and effect on the generation [67]. Therefore, distilling target information into the system tokens is crucial for performance. Moreover, including the text query tokens in the key input to the embedding predictors also results in slight performance boost. 13 λdepth λseg LLaVA-1. λgen CV-Bench2D CV-Bench3D MMStar Avg 51.9 61.0 56.0 38.8 0.10 0.25 0.50 0.75 1.00 0.10 0.25 0.50 0.75 1. 0.10 0.25 0.50 0.75 1.00 60.5 56.3 58.6 57.9 55.8 61.3 59.4 64.2 59.4 61.7 38.3 37.1 39.5 37.6 38.1 53.4 50.9 54.1 51.6 51.9 Table IV. Embedding Loss weights during PT. Setting each embedding loss weight to 0.5 is optimal. LsL1 Lcontrastive CV-Bench2D CV-Bench3D MMStar Avg 52.5 54.1 38.3 39.5 62.3 64.2 56.8 58.6 Table V. Ablations on components of embedding losses. Using both smooth-L1-loss and contrastive loss to compute the final embedding loss is optimal. Embedding Optimization Mode. In this ablation study, we evaluate various combinations of embedding losses applied during pretraining (PT). Our results, summarized in Tab. II, reveal that the optimal performance is achieved when all three embedding lossesdepth, seg, and genare used together. Interestingly, we observe that utilizing only depth or gen embedding losses still leads to notable performance improvements, whereas relying solely on seg embedding loss does not yield significant gains. This suggests that different types of target information contribute uniquely to the distillation process. Investigating how the distillation of one type of target information influences the effectiveness of others presents an intriguing direction for future research. Order of Special Tokens. In Tab. III, we ablate the order of different special tokens in the LLMs input sequence. We find that {g s} and {d s} show the best performance on CV-Bench [61]. We choose {g s} as our default order due to its performance being better than the baseline on all sub-tasks in CV-Bench. Embedding Loss weights. In Tab. IV, we ablate on different values of λdepth, λseg, and λgen during for the corresponding embedding losses during the pre-training stage. We find that setting each loss weight to 0.5 is optimal. Effect of Contrastive Embedding Loss. In Tab. V, analyze the impact of the contrastive loss component within the embedding loss. Our findings show that incorporating the contrastive loss significantly enhances performance, highlighting its positive influence on the models effectiveness. We keep the smooth L1 loss as default component to ensure the embedding predictions maintain the same magnitude as the target features, which is crucial for meaningful visualization. Qualitative Comparisons. We provide qualitative comparisons demonstrating the difference between LLaVA1.5 [41] and OLA-VLM for the different tasks in CV-Bench in Fig. I, Fig. III, Fig. IV, and Fig. V. Figure I. Qualitative Examples for the Count task in CV-Bench. Our OLA-VLM can accurately predict the presence of one picture and one tree, unlike LLaVA-1.5 [41]. in Fig. VII and Fig. VIII, respectively. Figure II. Ground-truth outputs from the target models used for Probing MLLMs. B. Visualizing Probe Outputs In this section, we present visualizations for the outputs for the probes trained on the single-encoder LLaVA-1.5 model in Sec. 3 in the main text. We provide the ground-truth visualizations from the teacher models in Fig. II. As shown in Fig. VI, the probe visualizations for the first eight layers of LLaVA-1.5 exhibit blob-like patterns, while the later layers progressively enhance the shape and boundary details of the foreground objects. In contrast, the probe visualizations for our OLA-VLM demonstrate improved object shapes and boundaries starting as early as layer-4, consistent with the findings on representation quality and layer-wise trends discussed in Sec. 3 of the main text. Nevertheless, the visual quality of our probe outputs still lags behind the original Depth-Anything-v2 [70], particularly in accurately representing background objects, as highlighted in Fig. II. Additionally, we present probe visualizations for the segmentation and generative representations 14 Figure III. Qualitative Examples for the Depth task in CV-Bench. Our OLA-VLM can accurately predict that the lamp and keyboard ar closer to the camera in the respective samples. Figure IV. Qualitative Examples for the Relation task in CV-Bench. Our OLA-VLM can accurately predict that the positions of the trees and the bottle in the respective samples. 15 Figure V. Qualitative Examples for the Distance task in CV-Bench. Our OLA-VLM can accurately predict that the distances between the respective pair of objects. Figure VI. Layerwise visualizations for the depth probes. For LLaVA-1.5 [41], the probes generate blob-like outputs up to the eighth layer, with visualizations progressively improving in the middle layers, aligning with the findings presented in Sec. 3 of the main text. Notably, probes trained on OLA-VLM begin producing distinguishable object shapes and boundaries as early as the third layer, attributed to the enhanced projector design and the incorporation of embedding losses. 16 Figure VII. Layerwise visualizations for the seg probes. The LLaVA-1.5 probes often fail to segment the third car in the background for the first sample during the initial layers (layers two to eight), whereas the OLA-VLM probes demonstrate relatively better performance in this scenario. However, for the second sample, both models probes struggle to segment the background regions effectively, highlighting an opportunity for improvement in future work. Figure VIII. Layerwise visualizations for the gen probes. The probe outputs for both the models are of fairly good quality."
        }
    ],
    "affiliations": [
        "Microsoft Research, Redmond",
        "SHI Labs @ Georgia Tech"
    ]
}