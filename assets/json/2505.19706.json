{
    "paper_title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision",
    "authors": [
        "Tej Deep Pala",
        "Panshul Sharma",
        "Amir Zadeh",
        "Chuan Li",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 6 0 7 9 1 . 5 0 5 2 : r Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision Tej Deep Pala1, Panshul Sharma1 Amir Zadeh2, Chuan Li2, Soujanya Poria 1Singapore University of Technology and Design 2Lambda Labs"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 less data. When applied to reward guided greedy search, our model yields prm@8 48.3, +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency 1."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable success on many natural language tasks, including open-ended generation and complex reasoning (Brown et al., 2020; Wei et al., 2022). However, they remain prone to hallucinations and subtle logical errors when generating multi-step solutions, particularly in domains such as mathematical problem solving (Wang et al., 2023; Zheng et al., 2024). Traditional outcome-only verifiers (Outcome Reward Models) can check final answer but fail 1Code: https://github.com/declare-lab/PathFinder-PRM to catch intermediate missteps that lead reasoning astray (Wang et al., 2024b). To address this gap, Process Reward Models (PRMs) have been proposed, which assign individual rewards to each reasoning step. (Uesato et al., 2022; Liu et al., 2025). As such, PRMs can filter out erroneous chains of thought and guide generation toward more reliable reasoning trajectories (Lightman et al., 2023; Zhang et al., 2025). Recent interest in explicitly reasoning-centric LLMs such as DeepSeek-R1 and OpenAIs GPT-o series models underscores the fields growing emphasis on human-like thinking and the ability to flexibly scale test-time compute (Guo et al., 2025; OpenAI, 2025). These models demonstrate extended deliberation and use structured reasoning traces to solve complex problems. In such settings, effective process supervision is crucial: rather than merely verifying final answer, it must guide and correct the reasoning process at every step, ensuring logical coherence and factual accuracy throughout. PRMs are thus an essential component in aligning reasoning LLMs with reliable multi-step reasoning. Despite recent advances, current PRMs still struggle with fine-grained error types. For example, the PRMBench benchmark reveals that many state-of-the-art PRMs fall short of detecting subtler faults such as non-redundancy violations, domain inconsistencies, or deceptive logical steps (Song et al., 2025). Moreover, existing methods typically combine error detection (is this step wrong?) with path optimality (how helpful is this step in reaching the solution?) in single prediction, leaving each signal underutilized (Zhang et al., 2025; Xia et al., 2025). In this work, we argue that error detection and value estimation are complementary but distinct objectives. By decoupling them into two sequential subtasks, first explicitly identifying specific error categories and using those error signals to Figure 1: Comparing the Methodology of Existing PRMs against PathFinder-PRM. compute step-level reward, we can obtain richer supervision and stronger guidance for downstream generation (depicted in Figure 1). To this end, we introduce PathFinder-PRM, an error-aware hierarchical PRM that (1) classifies each step to detect the presence of math or consistency errors, and (2) combines these fine-grained error labels to produce final reward score. We construct new training corpus by augmenting the human-annotated PRM800K (Lightman et al., 2023) and automated RLHFlow Mistral data (Xiong et al., 2024) with our three-dimensional labels, yielding around 400K richly annotated reasoning trajectories. Our experiments on ProcessBench (Zheng et al., 2024), PRMBench (Song et al., 2025), and suite of end-to-end math benchmarks show that PathFinder-PRM not only establishes the new state of the art among PRMs trained on PRM800K-only data but also continues to scale gracefully when incorporating additional automated annotations."
        },
        {
            "title": "Contributions",
            "content": "We propose PathFinder-PRM, the first hierarchical PRM that explicitly types errors into math and consistency categories before estimating step optimality. We curate multi-source dataset of 400K reasoning trajectories with mathematical three-dimensional step-level labels, combining PRM800K and RLHFlow data under our unified schema. We demonstrate that error-aware hierarchical supervision yields substantial gains on ProcessBench and PRMBench, and leads to more accurate and robust end-to-end math problem solving under reward-guided search."
        },
        {
            "title": "2 Related Works",
            "content": "Process Reward Models: Process reward models (PRMs) evaluate the quality of intermediate steps in reasoning processes. PRMs are core component of test-time scaling, enabling small policy models to outperform much larger models on reasoning tasks by reinforcing productive reasoning pathways (Liu et al., 2025). Most recent work on PRMs has varied along two primary axes: (1) the choice of the base model architecture and (2) the methods used to synthesize step-level supervision labels (Wang et al., 2024b; Zhang et al., 2025; Khalifa et al., 2025). Discriminative PRMs are designed to classify the correctness of individual process steps within reasoning trajectories (Uesato et al., 2022). Corresponding training data synthesis approaches include gold human step-level annotations (Lightman et al., 2023), Monte Carlo estimation (Wang et al., 2024b; Luo et al., 2024), and consensus filtering methods (Zhang et al., 2025). Generative PRMs scale verifier compute, utilizing the language modeling head of the PRMs to generate chain of thought (CoT) before producing step-level correctness classifications. Associated training data is typically obtained by filtering LLMas-a-judge reasoning traces against human steplevel labels (Khalifa et al., 2025; She et al., 2025) or relative progress estimation (Zhao et al., 2025). Limitations of Current PRMs: Current PRMs face significant challenges in detecting nuanced error types. Although frontier models excel at identifying obvious mistakes (Zheng et al., 2024), their performance deteriorates markedly when confronted with more subtle error types. The recently introduced PRMBench benchmark demonstrates this limitation, revealing substantial performance drops across fine-grained error categories such as redundancy, circular logic, step inconsistency and domain inconsistency (Song et al., 2025)."
        },
        {
            "title": "3.1 Task: Math Process Reward Modeling",
            "content": "The primary task of Process Reward Model in mathematical problem solving is to evaluate the correctness of each intermediate reasoning step generated by an LLM. Unlike ORMs, which assess only the final answer, PRMs evaluate at the step level, enabling fine-grained supervision and improved interpretability. Formally, given multi-step solution = {s1, s2, . . . , sT } produced by language model for math problem Q, the PRM assigns scalar reward rt to each step st, reflecting its mathematical and logical correctness. These rewards serve two main purposes: 1. Error Detection: By evaluating each step individually, PRMs can identify inaccurate or hallucinated reasoning in the solution process, even when the final answer may coincidentally be correct. 2. Guidance Toward Correct Solutions: The step-level feedback provided by PRMs can be used to steer generation policies, in reinforcement learning or reward-guided generation, towards valid and logically coherent trajectories, thereby improving the overall quality of generated solutions. Process Reward Models enhance the robustness of mathematical reasoning systems by aligning training and inference with process-based correctness, rather than relying only on outcome validation. 3. PathFinder-PRM Existing Process Reward Models tackle both error detection and optimal path guidance jointly. Given mathematical problem and sequence of solution steps generated by policy model, = {s1, s2, . . . , sT }, these models assign each step reward score conditioned on the question and all previous steps: Rt = RM (cid:0)Q, S<t, st (cid:1). where S<t = {s1, s2, . . . , st1} The resulting score implicitly reflects both the presence of errors in st and its contribution towards correct solution. In contrast, our approach takes hierarchical perspective by decomposing the reward assignment into two sequential subtasks: (a) detecting errors in each reasoning step, and (b) using the error information to inform step optimality. Specifically, we first categorize errors into two types: 1. Math Errors: mistakes in arithmetic or algebraic manipulation, incorrect formula application, or invalid implications. 2. Consistency Errors: logical inconsistencies with the question, prior steps, or established constraints. As seen in Figure 1, PathFinder-PRM performs two forward passes per step. In the first pass, it predicts the correctness probabilities for the two error types, Mt and Ct. In the second pass, it estimates the steps reward Rt, explicitly conditioned on the detected error labels: Mt, Ct = PathFinder-PRM(cid:0)Q, S<t, st (cid:1), (cid:1). Rt = PathFinder-PRM(cid:0)Q, S<t, st, Mt, Ct By structuring reward modeling hierarchically, PathFinder-PRM leverages fine-grained error signals to improve reward estimation, while maintaining clear separation between error detection and correct path guidance. We provide further details regarding the inference design in Appendix To investigate this, we propose the following hypothesis: Hypothesis: hierarchical supervision strategyfirst detecting error types, then using them to compute rewardsis more effective than existing methods that compute rewards directly without identifying the presence of errors explicitly."
        },
        {
            "title": "3.3 Creating the PathFinder-PRM Dataset",
            "content": "The PathFinder-PRM dataset includes step-level (1) fine-grained labels across three categories: mathematical reasoning accuracy, (2) consistency with prior steps and mathematical domain, and (3) step correctness. Here, the third category , ccorrectness step correctness identifies whether step is both error-free and optimally contributes to solving the problem. For each process step, we assign three-dimensional categorical score vector ct = , cconsistency (cmath ), where each compot nent c(i) {0, 1} represents binary label for the respective category. We construct this dataset by leveraging two existing datasets: the PRM800k (Lightman et al., 2023) and Mistral-PRM-Data by RLHFlow (Xiong et al., 2024). These datasets contain step-level correctness annotations generated through human evaluation and Monte Carlo estimation, respectively. PRM800K Integration: The original PRM800k dataset contains over 800,000 gold step-level correctness labels lt {1, 0, 1} for each reasoning step st. We transform each correctness label into our categorical score vector as follows: Steps with lt = 1 (correct and optimal) are mapped to ct = (1, 1, 1). Steps with lt = 0 (correct but suboptimal) are mapped to ct = (1, 1, 0). This mapping reflects our interpretation that human labels lt {0, 1} indicate error-free reasoning, with lt = 0 specifically denoting non-optimal process steps. For erroneous steps (lt = 1), the original correctness labels provide insufficient information to determine scores across our three evaluation categories. Therefore, we employ DeepSeek-R1Distill-Qwen-32 to generate binary labels for each category for these steps (DeepSeek-AI, 2025). To maintain dataset quality, we subsequently filter out samples with categorical score vectors that are inconsistent with 1 human annotated labels (i.e., ct = (1, 1, 1)). Mistral-PRM-Data Integration: Since this dataset lacks gold standard step-level correctness labels, we utilize DeepSeek-R1-Distill-Qwen-32B to assign binary categorical labels to small, randomly selected subset of process steps. To ensure data quality, we implement consistency filtering mechanism. This removes score assignments that are logically incompatible with the existing Monte Carlo (MC) estimation labels. Specifically: For steps with MC estimation + labels (indicating positive assessment), we retain only samples with assignments of ct = (1, 1, 1). For steps with MC estimation - labels (indicating negative assessment), we retain samples with categorizations where at least one component equals 0, i.e., ct = (1, 1, 1). In totality, the PathFinder-PRM dataset contains about 400K reasoning trajectory samples with steplevel categorical score vectors ct. Of these 400K trajectories, approximately 345K are sourced from PRM800k and the other 55K reasoning paths are sourced from Mistral-PRM-Data. We train two variants of the model, PathFinder-PRM-7B and PathFinder-PRM-7B-PRM800k trained on the full dataset and just the PRM800K subset respectively."
        },
        {
            "title": "3.4 Training Recipe for PathFinder-PRM",
            "content": "Previous studies demonstrate that models mathematical reasoning ability correlates with its performance as process reward model (Xia et al., 2025). Consequently, we initialize PathFinder-PRM from Qwen2.5-Math-7B-Instruct, which achieves state-of-the-art results on multiple math benchmarks (Yang et al., 2024). Unlike recent PRMs that swap the language modeling head for scalar value head (Zhang et al., 2025; Xia et al., 2025; Tan et al., 2025), we preserve the original LM architecture and extend the tokenizer with two special tokens, <+> and <->, to represent positive and negative step labels. Training Objective Each training example is structured in two parts, mirroring the inference passes: 1. Error Detection Target: Prompt + Math: <+>/<->, Consistency: <+>/<-> 2. Reward Estimation Target: Append the predicted error labels and the token Prompt + Math: [Math label], Consistency: [Consistency Label] + Correctness: <+>/<-> For each sample, we compute the cross-entropy loss only on these label tokens."
        },
        {
            "title": "4.1 Evaluation Benchmarks",
            "content": "For math steps error detection, we use ProcessBench and PRMBench. ProcessBench is benchmark designed to evaluate language models ability to identify errors in mathematical reasoning processes. It comprises 3,400 test cases, primarily Model Simplicity Soundness Sensitivity Overall NR. NCL. Avg. ES. SC. DC. CI. Avg. PS. DR. MS. Avg. LLM-as-judge, Open-source Language Models Qwen-2.5-Math-72B* QwQ-Preview-32B* 55.3 57. 54.9 55.6 55.1 56.4 55.5 67.4 71.6 72.3 58.1 66.2 59.1 66. 61.1 68.2 47.4 57.8 53.8 62.7 100.0 100.0 67.1 73.5 57.4 63. LLM-as-judge, Proprietary Language Models GPT-4o* Gemini-2.0-flash-exp* Gemini-2.0-thinking-exp-1219* 57.0 67.2 68.5 Discriminative Process Reward Models Math-Shepherd-7B* Math-PSA-7B RLHFlow-Mistral-8B* RLHFlow-DeepSeek-8B* Lemma-PRM800k-7B* Skywork-PRM-7B* ReasonEval-7B* Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B* 44.0 47.6 46.1 46.4 49.3 35.7 61.0 48.6 49. PathFinder-PRM-7B-PRM800K 51.5 58.9 w/o Separate Subtask Prediction PathFinder-PRM-7B w/o Separate Error Categories w/o Separate Subtask Prediction 52.1 51.7 57.9 62.4 58.1 63. 50.3 55.1 47.3 48.9 53.4 41.2 50.1 47.8 55.1 61.3 66.7 65.8 62.0 66.4 59.7 62.7 66.2 72.0 70.4 72.9 69.7 65.7 71. 70.7 66.0 71.0 71.1 67.3 71.8 70.9 67.3 71.8 62.5 61.8 60.3 65.7 66.2 65.7 47.1 51.3 46.7 47.6 51.4 38.4 55.5 48.2 52. 56.4 62.8 58.9 56.9 62.1 49.4 56.5 56.6 55.7 56.4 36.7 62.1 62.1 71.8 69.7 68.6 73.1 73.2 69.1 44.5 49.4 55.1 55.0 47.1 29.1 65.9 59.4 67. 67.6 62.4 68.7 70.0 62.6 41.3 47.1 54.4 53.2 46.7 30.6 61.5 58.7 66.3 65.9 62.4 66.3 66.9 62.2 47.7 54.2 63.8 66.2 53.3 34.4 66.0 68.5 78. 71.9 66.7 75.0 75.8 68.7 45.7 51.8 57.5 57.5 50.9 32.7 63.9 62.2 71.0 68.8 65.0 70.8 71.5 65.7 47.2 51.7 51.5 49.0 51.0 36.8 55.6 52.9 57. 58.7 60.2 61.7 60.3 61.0 48.6 54.1 56.2 55.4 53.5 37.4 58.0 64.0 69.1 66.6 64.9 69.8 69.2 65.4 99.2 98.2 99. 86.1 88.9 97.9 99.8 93.6 88.8 99.5 99.8 99.7 99.4 97.8 99.2 99.6 98.2 75.8 75.4 75.3 60.7 64.9 68.5 68.1 66.0 54.3 71.0 72.2 75.5 74.9 74. 76.9 76.4 74.9 66.8 66.0 68.8 47.0 52.3 54.4 54.2 52.0 36.2 60.0 58.3 65.5 65.0 64.4 67.7 67.3 64.9 Table 1: Performance on PRMBench. Results marked with * and come from Song et al. and She et al. respectively. Bold text denotes the best results within each category. represents the models we trained. sourced from different math reasoning benchmarks. Each case includes step-by-step solution annotated by human experts to indicate the earliest step containing an error or to confirm the correctness of all steps. Models are tasked with pinpointing the first erroneous step in solution or affirming the solutions correctness. PRMBench is fine-grained benchmark aimed at evaluating Process-Level Reward Models (PRMs) on their capability to detect nuanced errors in reasoning steps. It consists of 6,216 problems with total of 83,456 step-level labels, assessing models across multiple dimensions: Simplicity (non-redundancy, non-circular logic), Soundness (empirical soundness, step consistency, domain consistency, confidence invariance), and Sensitivity (prerequisite sensitivity, deception resistance, multi-solution consistency). The benchmark uses both synthetic and human-verified data, with rigorous quality control measures, including manual verification of subset of data. composite metric, PRMScore, is introduced, combining positive and negative F1 scores for balanced evaluation. To of the PathFinder-PRM in guiding step-by-step mathematical problem-solving, we employ it to assign effectiveness evaluate scores to individual reasoning steps generated by large language models (LLMs), selecting only those with the highest overall rewards to build upon. This evaluation is conducted across several widely recognized math reasoning benchmarks, including AIME24, AMC23, MATH, Olympiad Bench, College MATH, and Minerva MATH2."
        },
        {
            "title": "4.2 Baselines",
            "content": "Our evaluation utilizes diverse set of discriminative process reward models from recent literature as baselines: Math-Shepherd (Wang et al., 2024b), Math-PSA (Wang et al., 2024a), RLHFlow-Mistral and RLHFlow-DeepSeek (Xiong et al., 2024), Skywork-PRM-7b (o1 Team, 2024), ReasonEval7B (Xia et al., 2025), Llemma-PRM800k-7B (Sun et al., 2024), Qwen2.5-Math-PRM-7B and Qwen2.5-Math-7B-PRM800K (Zhang et al., 2025). We selected these baselines to cover diverse range of training regimes, including models trained on human annotations, automated annotations, and hybrid approaches, as well as varying scales of training data. 2Following (She et al., 2025), we use their subset of 200 test samples for Olympiad Bench and College MATH. Figure 2: Performance comparison of language models across three benchmarks. The PathFinder-PRM-7B model (gray) shows the highest performance across all benchmarks."
        },
        {
            "title": "5.1 Main Results",
            "content": "the results the shows selected PathFinder-PRM-7B SOTA for is PRMBench: Table PRM1 Bench baselines, the of PathFinder-PRM-variants as well as LLMas-judge performance of strong open-source In the discriminative and proprietary LLMs. PRM category, PathFinder-PRM-7B achieves the highest overall PRM score (67.7), out- (65.5) performing Qwen2.5-Math-PRM-7B The variant and ReasonEval-7B (60.0). PathFinder-PRM-7B-PRM800K, trained on fraction of our dataset, achieves competitive score of 65.0. Notably, PathFinder-PRM-7B outperforms nearly all LLM-as-Judge models, including GPT-4o, QwQ-Preview-32B and Gemini-2.0-flash-exp. PRMBench is benchmark designed to test models ability to detect subtle and complex errors. Our results affirm that our hierarchical PRM approach enables the model to detect these nuanced errors, leading to stronger process-level understanding and supervision. on trained PathFinder-PRM Excels on ProcessBench: Table 2 presents F1 results on ProcessBench. PRM800K, When exclusively PathFinder-PRM-7B-PRM800K attains an average F1 of 65.2, beating the previous best (Qwen2.5-Math-7B-PRM800K, 58.5) by 6.7 points and outperforming all other PRM800K-only baselines across every category: GSM8K (+5.9), MATH (+8.7), Olympiad Bench (+8.0) and OmniMath (+12.3). a"
        },
        {
            "title": "Leveraging",
            "content": "larger, mixed PathFinder-PRM-7B achieves human + further boosts perforauto-annotated dataset mance. an average F1 of 69.5, setting new stateoftheart among mixed-data models and closing the gap to the top automated-annotation model (Qwen2.5-Math-PRM-7B*, 73.5) to just 4 points. Notably, PathFinder-PRM-7B also leads in every individual benchmarkGSM8K (77.9), MATH (75.3), Olympiad Bench (65.0), and OmniMath (59.7), demonstrating the scalability and robustness of our hierarchical reward modeling approach. Improved Reward-Guided Search with Better PRMs: Finally, we assess the utility of our PRM in guiding solution search. Using Qwen2.5Instruct-7B as generator and ranking sampled steps in completions using our PRM, Table 3 shows that PathFinder-PRM-7B yields the highest average prm@8 score (48.25), outperforming Qwen2.5-Math-PRM-7B (46.8). The advantage holds across tasks, including challenging subsets such as AIME24 and College MATH, indicating better inductive bias and alignment with groundtruth solution quality. is"
        },
        {
            "title": "Competitive",
            "content": "PathFinder-PRM to Qwen2.5-Math-PRM-7B Despite Using 3 Less Data: Although Qwen2.5-Math-PRM-7B was trained on roughly 1.5M automated annotations, trained on only 400K samples, matches or exits performance in key benchmarks ceeds our PathFinder-PRM-7B,"
        },
        {
            "title": "Model",
            "content": "# Samples GSM8K MATH Olympiad Bench OmniMath Avg. F1 Trained on Automated Annotation Data Math-Shepherd-7B* RLHFlow-Mistral-8B* RLHFlow-DeepSeek-8B* Qwen2.5-Math-PRM-7B* 445K 273K 253K 1.5M Trained on Human Annotated Data (PRM800K) Llemma-PRM800K-7B ReasonEval-7B Qwen2.5-Math-7B-PRM800K* PathFinder-PRM-7B-PRM800K 350K 350K 264K 350K w/o Separate Subtask Prediction 47.9 50.4 38.8 82.4 48.4 41.0 68.2 74.1 71.4 Trained on Mix of Human and Automated Annotation Data Math-PSA-7B Skywork-PRM-7B* PathFinder-PRM-7B 860K unk 62.4 70.8 400K w/o Separate Error Categories w/o Separate Subtask Prediction 77.9 76.1 73.9 29.5 33.4 33.8 77.6 43.1 48.9 62.6 71.3 71.1 41.9 53.6 75.3 73.8 72. 24.8 13.8 16.9 67.5 28.5 36.7 50.7 58.7 59.2 31.5 22.9 65.0 61.4 63.9 23.8 15.8 16.9 66.3 33.4 37.4 44.3 56.6 58. 25.2 21.0 59.7 56.6 59.9 31.5 28.4 26.6 73.5 38.4 41.0 58.5 65.2 64.9 40.3 42.1 69.5 67.0 67. Table 2: Performance (F1) on ProcessBench. Results marked with * come from Zhang et al..The best performance across all categories is in bold and the best performance within category is underlined.# Samples denotes the number of training samples used by each model."
        },
        {
            "title": "Setting",
            "content": "pass@1* major@8* pass@8* Reward Guided Search (prm@8) Math-Shepherd-7B* Math-PSA-7B* RLHFlow-PRM-Mistral-8B* RLHFlow-PRM-DeepSeek-8B* Lemma-PRM800k-7B* Skywork-PRM-7B* ReasonEval-7B* Qwen2.5-Math-7B-PRM800K* Qwen2.5-Math-PRM-7B* PathFinder-PRM-7B-PRM800K w/o Separate Subtask Prediction PathFinder-PRM-7B w/o Separate Error Categories w/o Separate Subtask Prediction AIME24 AMC23 MATH Olympiad Bench College MATH Minerva MATH Avg. 11.2 20.0 33.3 13.3 6.7 10.0 13.3 13.3 10.0 3.3 23.3 16.7 20.0 6.6 20 13.3 10.0 47.8 57.5 82.5 52.5 57.5 57.5 52.5 57.5 57.5 55.0 45.0 60.0 55.0 55.0 62.5 52.5 55. 73.0 79.6 88.8 74.6 79.8 73.4 74.8 73.8 77.8 73.0 78.2 81.0 79.0 82.2 78.8 80.4 81.6 38.0 47.0 58.5 38.5 42.5 37.5 39.5 40.0 41.5 37.5 42.0 43.5 36.0 36.0 36.5 35.5 37. 38.6 41.5 47.5 36.5 41.0 38.0 37.0 36.5 39.0 35.5 35.5 39.0 55.0 53.5 55.0 53.5 53.5 37.2 42.7 57.7 41.2 39.3 41.2 40.8 38.2 43.4 37.9 38.6 40.4 36.4 36.0 36.7 37.5 36. 41.0 48.0 61.4 42.8 44.5 42.9 43.0 43.2 44.9 40.4 43.8 46.8 46.9 45.0 48.3 45.4 45.5 Table 3: The performance of PRM guided greedy search with Qwen2.5-7B-Instruct as the policy model. Results marked with * come from She et al. and reward-guided search. On ProcessBench, PathFinder-PRM-7B performs competitively to Qwen2.5-Math-PRM-7B in average F1 69.5 vs 73.5 despite using less than one-third of the data. More importantly, PathFinder-PRM-7B surpasses Qwen2.5-Math-PRM-7B on PRMBench overall (67.7 vs 65.5), and drives higher pass@8 in reward-guided greedy search (48.3 vs. 46.8). This demonstrates that our hierarchical, error-aware training yields more data-efficient and robust PRMs, achieving superior process supervision with far fewer samples. As shown in Figure 2, when comparing against PRMs trained on similarly sized datasets, PathFinder-PRM consistently achieves superior performance across all benchmarks. Figure 3 presents the performance of various PRMs on PRMBench. The results demonstrate that PathFinder-PRM not only surpasses other PRMs trained on comparably sized datasets but also outperforms Math-PSA and Qwen-2.5-Math-PRM, despite those models being trained on 23 times more data."
        },
        {
            "title": "Separate",
            "content": "Error w/o for PathFinder-PRM Categories (+2.9 points). This jump in real-world problem-solving performance highlights that fine-grained error signals not only improve diagnostic metrics but can also translate directly into selecting higher-quality solution paths. PathFinder-PRM Benefits from Error-Aware Hierarchical Supervision: Across ProcessBench, PRMBench, and reward-guided search, PathFinder-PRM consistently outperforms the PathFinder-PRM w/o separate subtask prediction, demonstrating the importance of hierarchical modeling of the subtasks. On ProcessBench, PathFinder-PRM-7B improves from 67.6 to 69.5 F1 (+1.9), and on PRMBench, from 64.9 to 67.7 (+2.8). In reward-guided search, the improvement is similarly clear: 48.3 prm@8 versus 45.5. These results highlight the value of decoupling feedback prediction into discrete reasoning components. Scalable Performance with Additional Training Data: Training on an additional 50K samples from broader, automatically annotated dataset greatly boosted the performance of PathFinder-PRM and helped it reach statePathFinder-PRM-7B of-the-art performance. PathFinder-PRM-7B-PRM800K outperforms across ProcessBench, PRMBench, and rewardguided greedy search, demonstrating the benefits of scaling beyond PRM800K dataset. On ProcessBench, PathFinder-PRM-7B achieves 69.5 Avg.F1 versus 65.2, while on PRMBench it improves from 65.0 to 67.7 F1. In reward-guided PathFinder-PRM-7B raises pass@8 search, from 46.9 to 48.3. These gains highlight that error-aware hierarchical modeling scales well with increased data diversity and quantity, enabling stronger generalization and robustness. key distinction is that the 50K additional data was collected using Mistral-7B as generator, while PRM800K was produced using fine-tuned GPT-4 model. Exposure to reasoning traces from smaller and weaker model may have helped PathFinder-PRM better learn to recognize and correct common failure patterns, contributing to improved robustness and generalization across benchmarks. We leave deeper investigation of this hypothesis to future work. Figure 3: PRMBench Overall PRMscore against the data scales of different baselines and PathFinder-PRM variants."
        },
        {
            "title": "5.2 Ablations",
            "content": "In our approach, we made two main claims: (1) decoupling the subtasks of Error Detection and Correct Path Guidance, and (2) categorizing errors into two prominent error categories in math will boost PRM performance. To verify these claims, we performed ablation experiments by modifying parts of our method: 1. PathFinder-PRM w/o Separate Subtask Prediction: Following existing PRM approaches, we trained the model to jointly learn to tackle both error detection and correct path guidance using only the step correctness labels. 2. PathFinder-PRM w/o Separate Error Categories: In this approach, we still do hierarchical prediction but we modify step 1. Instead of detecting the presence of 2 error categories, we combined the categories and predicted the presence of an error in the step. PathFinder-PRM Benefits from Separating Error Categories: On ProcessBench, explicitly distinguishing math and consistency errors yields PathFinder-PRM-7B clear overall boost: F1, versus 67.0 for the scores 69.5 Avg. PathFinder-PRM-7B w/o Separate Error Categories. We also observe similar drop in performance on PRMBench, the PathFinder-PRM w/o Separate Error Categories shows small drop in performance (0.4 points) compared to PathFinder-PRM-7B Crucially, reward-guided search highlights the practical impact of error typing: when ranking eight candidate solutions, PathFinder-PRM-7B achieves 48.3 prm@8, compared to just 45."
        },
        {
            "title": "Conclusion",
            "content": "In this work, we introduced PathFinder-PRM, hierarchical discriminative process reward model that decouples error detection from step optimality guidance, classifying math and consistency errors before computing step rewards. Our evaluation and ablation trials demonstrate that error-aware hierarchical supervision yields notable improvements on PRM benchmarks, with PathFinder-PRM-7B achieving state-of-the-art performance among discriminative PRMs on PRMBench and strong results on ProcessBench despite using less than three times the data used to train the current best performing model. similar performance boost is observed in reward-guided search evaluation, affirming our hypothesis about the efficacy of hierarchical erroraware reward generation. Our approach is, therefore, promising direction for more robust and interpretable process reward models, with potential for further gains when scaled to larger architectures."
        },
        {
            "title": "Limitations",
            "content": "Due to computational constraints, our experiments were limited to 7B models. While this scale provides strong foundation for evaluating our proposed methodology, we hypothesise that larger models could further enhance modeling accuracy and better leverage process supervision signals due to their improved mathematical reasoning capabilities."
        },
        {
            "title": "References",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. 2025. Process reward models that think. arXiv preprint arXiv:2504.16828. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. 2025. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024. Improve mathematical reasoning in language models by automated process supervision. Preprint, arXiv:2406.06592. Skywork o1 Team. 2024. Skywork-o1 open sehttps://huggingface.co/Skywork. Acries. cessed: 2025-05-18. OpenAI. 2025. Introducing openai o3 and o4-mini. Accessed: 2025-05-19. Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, and Shujian Huang. 2025. R-prm: Reasoning-driven process reward modeling. arXiv preprint arXiv:2503.21295. Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. 2025. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. 2024. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472. Xiaoyu Tan, Tianchu Yao, Chao Qu, Bin Li, Minghao Yang, Dakuan Lu, Haozhe Wang, Xihe Qiu, Wei Chu, Yinghui Xu, and 1 others. 2025. Aurora: Automated training framework of universal process reward models via ensemble prompting and reverse verification. arXiv preprint arXiv:2502.11520. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcomebased feedback. Preprint, arXiv:2211.14275. Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, and 1 others. 2024a. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. Preprint, arXiv:2412.06559. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024b. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 94269439. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2025. Evaluating mathematical reasoning beyond accuracy. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2772327730. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. 2024. An implementation of generative prm. https: //github.com/RLHFlow/RLHF-Reward-Modeling. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, and Bowen Zhou. 2025. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891."
        },
        {
            "title": "A Extended Results",
            "content": "A.1 ProcessBench Table 4 contains the extended evaluation on ProcessBench with LLM-as-Judge Baselines using both Proprietary and Open-Source Language Models."
        },
        {
            "title": "Model",
            "content": "# Samples GSM8K MATH Olympiad Bench OmniMath Avg. F1 LLM-as-judge, Proprietary language models GPT-4o* o1-mini* unk unk LLM-as-judge, Open-source language models Llama-3.3-70B-Instruct* Qwen2.5-Math-72B-Instruct* Qwen2.5-72B-Instruct* unk unk unk"
        },
        {
            "title": "Discriminative Process Reward Models",
            "content": "Trained on Automated Annotation Data Math-Shepherd-7B* RLHFlow-Mistral-8B* RLHFlow-DeepSeek-8B* Qwen2.5-Math-PRM-7B* 445K 273K 253K 1.5M Trained on Human Annotated Data (PRM800K) Llemma-PRM800K-7B ReasonEval-7B Qwen2.5-Math-7B-PRM800K* PathFinder-PRM-7B-PRM800K 350K 350K 264K 350K w/o Separate Subtask Prediction 79.2 93. 82.9 65.8 76.2 47.9 50.4 38.8 82.4 48.4 41.0 68.2 74.1 71.4 Trained on Mix of Human and Automated Annotation Data Math-PSA-7B Skywork-PRM-7B* PathFinder-PRM-7B 860K unk 62.4 70. 400K w/o Separate Error Categories w/o Separate Subtask Prediction 77.9 76.1 73.9 63.6 88.9 59.4 52.1 61.8 29.5 33.4 33.8 77. 43.1 48.9 62.6 71.3 71.1 41.9 53.6 75.3 73.8 72.6 51.4 87.2 46.7 32.5 54.6 24.8 13.8 16.9 67. 28.5 36.7 50.7 58.7 59.2 31.5 22.9 65.0 61.4 63.9 53.5 82.4 43.0 31.7 52.2 23.8 15.8 16.9 66. 33.4 37.4 44.3 56.6 58.0 25.2 21.0 59.7 56.6 59.9 61.9 87.9 58.0 45.5 61.2 31.5 28.4 26.6 73. 38.4 41.0 58.5 65.2 64.9 40.3 42.1 69.5 67.0 67.6 Table 4: Performance (F1) on ProcessBench. Results marked with * come from Zhang et al.. The best performance across all categories is in bold and the best performance within category is underlined."
        },
        {
            "title": "B Scaling Effects of Additional RLHFlow Mistral Data",
            "content": "To further probe the effects of training data scaling, we increased the amount of RLHFlow Mistral data from 50K to 200K samples and retrained PathFinder-PRM. As shown in Table 5, this increase from 400K to 550K total training samples did not result in improved performance. In fact, we observe slight drop across all benchmarks: ProcessBench performance decreased from 69.5 to 68.75 Avg.F1, PRMBench from 67.7 to 67.4, and reward-guided search accuracy dropped from 48.3 to 47.5. These findings suggest that the benefits of augmenting training with weaker model-generated traces (e.g., from Mistral-7B) may saturate quickly. Simply increasing the volume of such data does not necessarily lead to improved generalization, and may even slightly degrade performance. This underscores the importance of data quality and the nuanced role of diversity over quantity in training PRMs. We leave further study into effective feedback curation and dataset composition to future work. Model # Total samples # Mistral samples ProcessBench PRMBench Reward Guided Search (PRM@8) PathFinder-PRM-7B-PRM800K PathFinder-PRM-7B PathFinder-PRM-7B 350K 400K 550K 0 50K 200K 65.2 69.5 68.75 65.0 67.7 67. 46.9 48.3 47.5 Table 5: Effect of scaling RLHFlow Mistral data on PathFinder-PRM performance. Design Choice: Two Forward Passes over Autoregressive Multi-Token Prediction In PathFinder-PRM, we adopt two-forward-pass approach to predict intermediate error labelsMath Error and Consistency Errorfollowed by final reward score for correctness. This contrasts with autoregressive decoding, which would generate all three labels sequentially. The motivation behind this design is to minimize error cascading between dependent predictions. Specifically, we aim to predict Math and Consistency labels independently in the first forward pass to prevent the Consistency prediction from being influenced by the previously generated Math label. This is potential issue in the autoregressive setup since each token depends on the ones before it. In our setup, single language modeling head is used without any task-specific heads. During the first forward pass, we construct the input as: [PRM Input], Math: <mask>, Consistency: <mask>, where the two special <mask> tokens represent the target positions for Math and Consistency labels. The model is trained to produce the correct label tokens at these masked positions, allowing us to decode both predictions simultaneously without one influencing the other. In the second forward pass, we supply the previously predicted Math and Consistency tokens in place, and append third <mask> token to infer the final correctness label: [PRM Input], Math: predicted_label, Consistency: predicted_label, Correctness: <mask>. The probability corresponding to the positive reward token at the final mask position is then used as the models output reward score. This inference method avoids sequential decoding and uses only masked forward passes, enabling clearer modular supervision and avoiding implicit dependency leakage between intermediate labels. During training, similar masked format is used. This clean separation helps PathFinder-PRM to better capture independent error signals and contributes to its robustness. Algorithm 1 PathFinder-PRM Inference via Two Forward Passes Require: Prompt P, PathFinder-PRM 1: // First Forward Pass: Predict Math and Consistency Labels 2: Input1 concatenate(P, \"Math: <mask>\", \"Consistency: <mask>\") 3: Logits1 PathFinder-PRM.f orward(Input1) 4: predmath argmax(Logits1 at Math mask position) 5: predconsistencyargmax(Logits1 at Consistency mask position) 6: // Second Forward Pass: Predict Correctness/Reward Label 7: Input2 concatenate(P, \"Math: \", predmath, \"Consistency: \", predconsistency, \"Correctness: <mask>\") 8: Logits2 PathFinder-PRM.f orward(Input2) 9: rewardprob sof tmax(Logits2 at Correctness mask position) 10: predreward rewardprob of <+> token 11: return predmath, predconsistency, predreward"
        },
        {
            "title": "D Data Annotation Prompt",
            "content": "The prompt below was utilized with DeepSeek-R1-Distill-Qwen-32B to synthesize the 3-dimensional categorical score vectors assigned to each sample in the dataset used to train PathFinder-PRM Prompt for dataset labelling You are an analytical math instructor grading students work. Think step-by-step through your analysis. Below is the math question, the previous steps by the student, and the current step to evaluate. {context} Your task is to rigorously examine the current step and determine if it contains ANY mathematical errors. Assign binary scores (0 = wrong, 1 = correct) based on three criteria: A) Mathematical logic Is the current step, **on its own**, mathematically valid? Check for: Calculation errors Incorrect formula application Invalid operations or simplifications Algebra mistakes or sign errors Incorrect assertions B) Consistency Is the current step logically consistent with: Established ground truth Previous steps Any constraints or conditions established earlier The mathematical domain applicable to this problem C) Simplicity and optimality is this step an efficient next step toward the solution? Check for: Redundant statements: factually correct statements that do not help progress toward the solution. Circular logic: does this step come to conclusion already previously established? Non-clarity: Are the assertions made in this step ambiguous in way that obsfucates their purpose? Optimality: is the **idea** of this step the near optimal approach one would take to solve the problem? Double check all listed criterion here explicitly in your reasoning. In your analysis, be sensitive to subtle issues like missing pre-requisites/assumptions, correct-looking statements with slight errors and high confidence statements containing errors. IMPORTANT POINTS: - If you find ANY error, even minor one, you MUST assign score of 0 to the appropriate criteria. Be skeptical and verify all claims thoroughly. - For incorrect steps, wherever possible, attempt to categorize the issue as violating **one of the three criterion** (i.e., assign score 0 to **only one category**). Assign multiple 0 scores only for serious errors. You must format your answer as below: Reasoning: {{Provide detailed analysis, showing all verification steps and explicitly identifying any errors found}} Final answers: Score A: {{0 or 1 only}} Score B: {{0 or 1 only}} Score C: {{0 or 1 only}}"
        },
        {
            "title": "E Training HyperParameters",
            "content": "Table 6 contains the hyperparameters used to train PathFinder-PRM. We used the Transformers library Trainer implementation to train our model in seq-to-seq manner (Wolf et al., 2020)."
        },
        {
            "title": "Value",
            "content": "Model Torch Data type Attention Implementation Per-device Train Batch Size Gradient Accumulation Steps Learning Rate Number of Training Epochs LR Scheduler Type Max Gradient Norm Warmup Ratio Seed BF16 Optimizer Gradient Checkpointing QwenQwen2.5-Math-7B-Instruct bfloat16 flash attention 2 2 32 1.0e-05 4 cosine 1.0 0.1 42 true adam True Table 6: Training Configuration for Qwen2.5-Math-7B-Instruct."
        }
    ],
    "affiliations": [
        "Lambda Labs",
        "Singapore University of Technology and Design"
    ]
}