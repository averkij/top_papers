{
    "paper_title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
    "authors": [
        "Tianshuo Peng",
        "Mingsheng Li",
        "Hongbin Zhou",
        "Renqiu Xia",
        "Renrui Zhang",
        "Lei Bai",
        "Song Mao",
        "Bin Wang",
        "Conghui He",
        "Aojun Zhou",
        "Botian Shi",
        "Tao Chen",
        "Bo Zhang",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domain-specific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, a scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design a progressive training strategy to integrate features from expert models into the input of a generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce a novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in a versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 3 8 9 5 0 . 2 1 4 2 : r Chimera: Improving Generalist Model with Domain-Specific Experts Tianshuo Peng1,2,, Mingsheng Li3,, Hongbin Zhou1, Renqiu Xia1,4, Renrui Zhang2 Lei Bai1, Song Mao1, Bin Wang1, Conghui He1, Aojun Zhou2, Botian Shi1 Tao Chen3, Bo Zhang1,,(cid:66), Xiangyu Yue2,(cid:66) 1Shanghai Artificial Intelligence Laboratory, 2MMLab, The Chinese University of Hong Kong 3Fudan University, 4Shanghai Jiao Tong University * Equal Contribution, (cid:66) Corresponding Authors, Project Leader"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Multi-modal Models (LMMs) underscore the importance of scaling by increasing image-text paired data, achieving impressive performance on general tasks. Despite their effectiveness in broad applications, generalist models are primarily trained on web-scale datasets dominated by natural images, resulting in the sacrifice of specialized capabilities for domainspecific tasks that require extensive domain prior knowledge. Moreover, directly integrating expert models tailored for specific domains is challenging due to the representational gap and imbalanced optimization between the generalist model and experts. To address these challenges, we introduce Chimera, scalable and low-cost multi-modal pipeline designed to boost the ability of existing LMMs with domain-specific experts. Specifically, we design progressive training strategy to integrate features from expert models into the input of generalist LMM. To address the imbalanced optimization caused by the well-aligned general visual encoder, we introduce novel Generalist-Specialist Collaboration Masking (GSCM) mechanism. This results in versatile model that excels across the chart, table, math, and document domains, achieving state-of-the-art performance on multi-modal reasoning and visual content extraction tasks, both of which are challenging tasks for assessing existing LMMs. Chimera homepage: https: //unimodal4reasoning.github.io/chimera_ page/ 1. Introduction The past year has witnessed the remarkable success of Large Multi-modal Models (LMMs) in handling variety of general domain tasks, such as image captioning [10, 14, 39], visual dialog [4, 11, 12, 55, 72], and cross-modal reFigure 1. Performance comparison of different models on multimodal reasoning (MathViata, MathVerse) and visual structural extraction (ChartQA-SE, Table-SE) tasks. trieval [7, 76], demonstrating their potential as technical pathway towards general-purpose AI assistant. Despite their proficiency in diverse tasks, LMMs face challenges in specialized domains such as multi-modal reasoning and visual content extraction tasks. As depicted in Fig. 1, stateof-the-art general-purpose LMMs demonstrate significant limitations in addressing these specialized tasks, highlighting the necessity for further research to bridge this gap. Current research on LMMs [10, 12, 38, 55, 56, 58, 77] has extensively invested in scaling up by collecting webscale image-text pairs and employing multi-task instruction tuning to develop generalist models, following One for All paradigms [12, 71]. However, the pursuit of generality often results in suboptimal performance in domain-specific 1 tasks, such as Chart [76], Table [75], and Math [80, 84]. This is mainly due to the substantial differences between natural images and those found in specialized fields [35]. For instance, domain-specific tasks such as multi-modal reasoning and visual structural extraction often involve content that includes charts, tables, geometric figures, and function graphs [16, 75, 76, 83]. These tasks are characterized by higher text density and more abstract content [69, 73]. As result, general LMMs, which are primarily trained on web-scale natural images, struggle to adapt effectively to these specialized contexts [28, 76, 80]. Conversely, numerous studies have focused on developing tailored models or task-specific architectures for downstream tasks [16, 62, 73, 75, 76, 80, 83], adopting One for One paradigm where models are trained on single scene type. While these expert models exhibit strong capabilities in specialized tasks, they are often criticized for being designed to address individual scenarios. This phenomenon arises from significant distribution gaps across various subdomains, such as tables, charts, functions, and geometry, potentially sacrificing their generalizability across broader applications using specialized models. To push the boundary further for the existing LMMs and improve their performance in specialized domains, an intuitive solution is to post-train LMMs on data relevant to the target domain. However, common challenge is that the domain-specific data required for specialist models are often proprietary and inaccessible. On the other hand, integrating specialist experts that contain specialized prior knowledge presents promising approach to address this issue. However, directly combining specialist experts with the generalist model could result in unsatisfactory performance, due to the following factors: 1) large representation gaps between cross-domain encoders, and 2) imbalanced optimization for generalists and specialists. To address these challenges, this work introduces Chimera: flexible and scalable pipeline that can effectively scale up off-the-shelf experts into LMMs at low cost. Specifically, through cost-effective training aimed at feature alignment, we integrate multiple encoders from different expert models into single LMM, effectively merging diverse specialized knowledge into the model. Besides, we observed alignment imbalances during the cross-modal encoder fusion and propose General-Expert Collaboration Masking mechanism to facilitate better model fusion. Our method easily adapts LMMs, such as InternVL [12, 13], to range of domain-specific tasks, including advanced mathematical reasoning, table/chart QA & extraction, and document structural extraction tasks. By aggregating multiple expert models into single general LMM, Chimera develops versatile model endowed with multiple specialized capabilities. During inference, Chimera employs routing module to determine whether to invoke the corresponding domain expert model based on the visual input, resulting in versatile model that excels across the chart, table, math, and document domains, as well as tasks involving multimodal reasoning and extraction. We conduct extensive experiments to evaluate Chimeras capabilities in multi-modal reasoning and visual content extraction, both of which are challenging domains for assessing existing LMMs. With the introduction of domain knowledge from expert models, Chimera achieves overall accuracies of 64.9 and 32.4 on the multi-modal reasoning benchmarks MathVista [46] and MathVerse [81], setting new State-Of-The-Art (SOTA) for LMMs of comparable scale. It also surpasses or matches the performance of representative expert models in visual content extraction tasks across chart, table, and document domains. Our contributions can be summarized as follows: 1. We introduce Chimera, scalable pipeline that integrates specialist models into generalist LMMs, facilitating their adaptation to many specialized tasks. 2. We propose Generalist-Specialist Collaboration Masking (GSCM) mechanism that achieves representation alignment between the generalist model and domainspecific experts. 3. Chimera achieves SOTA performance on challenging benchmarks for reasoning, including MathVista and MathVerse. Furthermore, it achieves near-specialistlevel results in visual structural extraction on benchmarks like ChartQA-SE, Table-SE, Doc-SE, etc. 4. We will publicly release the weights of Chimera and the complete datasets utilized during its training phase. We expect that making Chimera open-sourced will accelerate future research on LMMs. 2. Related Work Generalist Large Multi-modal Models. Following the remarkable success of Large Language Models (LLMs) [6, 67, 68], researchers have made great efforts in adapting LLMs for multi-modal tasks in general context, contributing to the flourishing of Large Multi-modal Models (LMMs) [10, 12, 32, 38, 55, 56, 58, 77]. Recent LMMs typically utilize cross-modal connector [12, 30] and perform the pre-training on large-scale natural imagetext datasets [33, 75] to alleviate the modality gap between the visual encoder and the LLMs. For instance, BLIP series [29, 30] utilizes captions from datasets like COCO [37], CC3M [61], SBU [57], and LAION [60], while the LLaVA series [42, 43] constructs complex instruction-following datasets based on natural images from COCO [37] to further enhance their understanding of visual content. However, the pursuit of generality often results in limited performance in specialized scenarios, such as geometric and function reasoning [16, 80], table and chart understanding [69, 76, 83], of which the visual content differing 2 differs significantly from natural images. Moreover, finetuning LMMs on specialized downstream is often challenging due to inaccessible private data and potential degradation in general performance. Expert Models on Specialized Scenarios. Expert tasks in multimodal settings, such as geometric and function reasoning [80, 81], table and chart understanding [76, 83], and document information extraction [69], often require specialized designs to achieve optimal task performance. For example, Math-LLaVA [62], and MAVIS [80] train LMMs on carefully curated mathematical datasets using natural language descriptions. Table-LLaVA [83] constructs large-scale multimodal table understanding dataset, while StructEqTable [75] uses extensive table format transformation data to build highly specialized expert model with limited generality. Similarly, ChartGemma [51] and ChartInstruct [50] train LMMs on diverse chart instructionfollowing data, and ChartVLM [76] employs router structure to selectively engage different decoders for base perception tasks and cognition tasks. GOT [73] trains specialist model on million-scale private data specifically for document structural extraction task. While expert models excel in specific domains, they struggle with tasks outside their specialization. In contrast, Chimera integrates specialized knowledge into generalist LMM, achieving superior performance across both multimodal reasoning and document context extraction tasks. 3. Methodology To develop an assistant capable of adapting to challenging domains, we propose Chimera, scalable multi-modal model built upon existing LMMs, designed to efficiently tackle domain-specific tasks at low cost. In this section, we first introduce an overview of Chimera in Sec. 3.1. Sec. 3.2 discusses the integration of generalists and domainspecific experts, and Sec. 3.3 details Generalist-Specialist Collaboration Masking (GSCM), an effective method for collaboration between the generalist model and specialists. 3.1. Overview As illustrated in Fig. 2, Chimera consists of: general visual encoder Eg, general projector together with language Model initialized from pretrained LMM, router R, an expert model set Se with Ne expert models and corresponding expert projector set Sp. Assuming expert models from the domains of table, chart, and math as aggregation targets, we have: Se = {Etable, Echart, Emath} (1) and Sp = {P table, chart, math}. (2) Generalist Branch. For visual input Xv, Eg provides the general visual features Zv = Eg(Xv), projects general to replace Hv. visual features into word embedding space, yielding general visual tokens Hv = g(Zv). During training, we apply the GSCM mechanism on Hv as Hm Specialist Branch. The linear layer first predicts routing value Hr RNe+1 as Hr = R(Z cls represents the classification token of Zv, determining whether to invoke an expert model and which specific expert model to call. Consequently, the expert visual tokens He can be formulated as: ), where cls = arg max (Hr)i He = (cid:26) , if == 0, Sp (Se (Xv)) , otherwise. (3) Given the text embedding Ht of instruction Xt, the input sequence during training is formulated as: Hinput = concat([Hm : He : Ht]). (4)"
        },
        {
            "title": "We construct",
            "content": "two variants of Chimera: ChimeraReasoner and Chimera-Extractor. Chimera-Reasoner aims to integrate expert models specializing in table, chart and math domains, making it well-suited for comprehensive multi-modal reasoning scenarios. Chimera-Extractor focuses on integrating expert models that specialize in document structural extraction. This task is more specialized and involves richer visual-text information. The type of pre-training task significantly affects model performance, which we consider when selecting expert models. As shown in Fig. 3, we categorize low-level tasks as the precise extraction of domain-specific visual content and structure (e.g., Table2LaTeX, Chart2Markdown, Doc2Markdown), while high-level tasks involve understanding and summarizing image content. We select expert models with diverse pre-training task configurations. For the table expert, we use the encoder from StructEqTable [75], which effectively converts table images into LaTeX/HTML. For the chart expert, we choose the encoder from ChartVLM [76], which excels in structural extraction and chart type classification. For the math expert, we adopt Math-CLIP [80], trained on extensive geometry and function caption data. For document structural extraction, we employ the encoder from the latest model, GOT [73]. 3.2. Integration of Generalist and Specialist There are two intuitive ideas to adapt an generalist LMM into specialized domain: 1) performing supervised finetuning on domain-specific data (naive finetune) and 2) sequentially appending features from different encoders (naive concat). The primary difference between Chimera and these two approaches lies in the definition of the input sequence during training for the language model . Let Hnf input denote the input sequences in the naive input and Hnc Figure 2. Overview of our Chimera framework. Chimera uses Generalist-Specialist Collaboration Masking to facilitate the alignment with expert models. During inference, the Router decides expert invocations based on the visual input, resulting in versatile model that excels across multiple specialized domains and tasks. performance across subtasks. The latter approach incorporates encoded features from various domains, but applying this directly to well-aligned LMM may lead to misalignment between the generalist and specialist modelsa limitation we will discuss in the next section. 3.3. Generalist-Specialist Collaboration Masking Although naive concat method with input Hnc input is intuitive, we still concern that since the general visual encoder Eg is well-aligned with language Model , it may cause the model to overly rely on Eg to complete tasks, which leads to ineffective alignment with the expert models. To better align domain knowledge and general world knowledge, we propose simple yet effective learning mechanism called Generalist-Specialist Collaboration Masking, designed to boost the synergy between general-purpose and domain-specific capabilities. During training, we sample subset of general visual tokens from Hv at certain ratio and mask them to build the masked general visual tokens Hm . In practice, this is achieved by setting the attention mask corresponding to the sampled subset to False. We consider simple sampling strategy: randomly sampling tokens without replacement according to uniform distribution. Applying mask to information provided by general encoder Eg produces limitation on Eg, which will force the model to utilize domainspecific information provided by expert models as suppleFigure 3. Pre-training tasks of expert models considered by Chimera. finetune and naive concat methods, respectively. They can be formulated as:"
        },
        {
            "title": "Hnf\nHnc",
            "content": "input = concat([Hv : Ht]), input = concat([Hv : He : Ht]). (5) The former approach attempts to use single visual encoder to handle all visual content, which refuses to incorporate domain-specific knowledge from expert models. Finetuning on subtasks in several specialized domains can also reduce generalizability, leading to trade-offs or suboptimal 4 General: ShareGPT4v [10], ShareGPT4-o [10] Table: TableX [75] Stage 1 Chart: ChartQA [48], PlotQA [53],ChartX [76], SimChart [74] Math: MAVIS-Caption [80] Language: Kaggle-science-exam [36], MathInstruct [78], MathQA [3], SciInstruct [79], Orcamath [54] General: ShareGPT4v [10], ShareGPT4-o [10], LLaVAR [82], AI2D (GPT4V) [28], AI2D (InternVL [12]), AI2D (Original) [25], MathVision [70], IconQA [45], MapQA [8], ScienceQA [59], ArxivQA [31], TQA [26], CLEVR-Math [19], Super-CLEVR [34], Cambrian Data Engine [66] Table: TableX [75], TabMWP [47], MMTab [83] Chart: PlotQA [53],ChartX [76], SimChart [74], Chart2Text [23], ChartQA [48], LRV Chart [41], ChartGemma [51], DVQA [21], FigureQA [22], VisText [63] Math: MAVIS-Caption [80], Geo170K [16], GeoMVerse [24], MAVIS Manual Collection [80], MAVIS Data Engine [80] Geometry3K [44], GeoQA+ [9], InterGPS [44] Stage 2: Table 1. Dataset used for Chimera-Reasoner. Stage 1 and Stage 2 represent Domain-General Knowledge Alignment and Visual Instruction Tuning separately. ments for vision-language tasks. The uniform distribution helps prevent bias that may arise from masking predominantly in the image center or specific regions. 3.4. Training Recipe To equip multi-modal generalists with rich domain-specific knowledge, we apply progressive training strategy, including Domain-General Knowledge Alignment and Visual Instruction Tuning. The dataset used during training can be found in Tab. 1 and Tab. 2. Through two-stage training, we develop two versatile models: Chimera-Reasoner and Chimera-Extractor. The Chimera-Reasoner takes questions and images as inputs, performing reasoning and answering, while the Chimera-Extractor excels at extracting structured information from visual documents. Domain-General Knowledge Alignment. To initially align domain-specific knowledge with the semantic space of the generalist LMM, we train the model using tasks that directly perceive diverse image content. The tasks include natural image description, table format transformation, chart structural extraction and summarization, math diagram captioning, and paragraph-level OCR. With guidance from image-text pairs across different domains, the model is able to leverage domain knowledge from expert models to accurately recognize visual content Stage 1 ChartQA [48], PlotQA [53],ChartX [76],, SimChart [74], TableX [75] Stage 2 DocGenome [75], DocStruct4M [18], DocVQA [52] Table 2. Datasets used for Chimera-Extractor. Stage 1 represents Domain-General Knowledge Alignment, and Stage 2 represents Visual Instruction Tuning. in each domain and describe its spatial arrangement. This marks the first step toward deeper integration. In this stage, we freeze the general visual encoder Eg, expert model set Se and language model , only train the router R, general projector and expert projector set Sp. Visual Instruction Tuning. To further align model with domain knowledge from expert models and enhance its performance on specialized tasks across different domains, we take instruction-following datasets from various domains to perform visual instruction tuning with the proposed GSCM. During this stage, we unfreeze router R, general projector g, expert projector set Sp and language model , perform thorough instruction-following tuning with mast ratio 0.3, which finally results in the versatile Chimera models. Training Objective. Our primary training objective is to optimize the trainable parameters θ, so that the likelihood of target response sequence Xa is maximized given the visual input Xv and instruction Xt as follows: θ = arg max θ (XaXv, Xt; θ). (6) To accomplish this, we utilize token-wise cross-entropy loss to train the model in an auto-regressive manner. For target Xa of length L, the auto-regressive modeling loss Lm is represented as follows: Lm = (cid:88) i=1 log (xiXv, Xt, Xa;<i, θ), (7) where Xa;<i are the tokens before the current prediction token xi. Besides, we add classification loss to guide the Router to accurately call different expert models based on image content, which can be represented as follows: Lc = Ne+1 (cid:88) i=0 log (ciXv, θ), (8) where ci represents the expert domain category required by the current image (including category 0, which indicates no expert model is invoked). Finally, the optimization objective is formulated as follows: = Lc + Lm. (9) 4. Experiments To evaluate the capabilities of Chimera, we begin by detailing the datasets and metrics used for evaluation, along 5 Model #Params. ALL FQA GPS MWP TQA VQA ALG ARI GEO LOG NUM SCI STA"
        },
        {
            "title": "Close Source LMMs",
            "content": "InternVL2-Pro [12] Gemini 1.5 Pro [65] GPT-4o Grok-1.5V Claude 3 Opus [1] GPT-4V (Playground)"
        },
        {
            "title": "Open Source LMMs",
            "content": "LLaVA-OneVision [28] Math-LLaVA [62] Pixtral [2] SPHINX-MoE [38] InternLM-XComposer2 [15] LLaVA-OneVision [28] Math-PUMA-DeepSeek-Math [84] Qwen2-VL [71] IntenrVL2 [12] Chimera-Reasoner Human performance - - - - - - 72B 13B 12B 87B 7B 7B 7B 2B 7B 2B 4B 8B 2B 4B 8B - 66.8 63.9 63.8 52.8 50.5 49. 67.5 46.6 58.0 42.7 57.6 63.2 44.7 43.0 58.2 48.3 57.0 61.6 53.1 61.3 64.9 70.6 - - - - 43.1 - 37.2 - - 55.0 - 42. - - 51.3 58.0 62.5 52.4 58.4 62.8 65.4 - - - - 50.5 - 57.7 - - 63.0 - 39.9 - - 45.7 58.2 64.4 56.7 66.8 71.6 76.9 - - - - 57.5 - 56.5 - - 73.7 - 67.7 - - 40.9 62.4 61. 62.9 72.0 72.6 71.5 - - - - 65.2 - 51.3 - - 56.3 - 42.4 - - 50.6 57.0 64.6 51.9 61.4 65. 48.0 - - - - 38.0 - 33.5 - - 39.7 - 31.3 - - 52.5 48.6 54.7 40.8 48.0 52.0 66.5 - - - - 53. - 53.0 - - 56.6 - 39.2 - - 43.4 55.9 63.0 52.7 63.3 67.6 62.3 - - - - 49.0 - 40.2 - - 52.4 - 41. - - 47.3 53.8 58.9 47.6 54.7 57.8 63.6 - - - - 51.0 - 56.5 - - 62.3 - 41.4 - - 42.3 55.2 61.9 56.1 65.7 69.5 27.0 - - - - 21.6 - 16.2 - - 8.1 - 8.1 - - 13.5 13.5 18. 10.8 24.3 21.6 40.3 - - - - 20.1 - 33.3 - - 42.4 - 36.8 - - 28.5 30.6 34.0 34.0 39.6 45. 65.6 - - - - 63.1 - 49.2 - - 59.0 - 48.4 - - 53.3 59.0 59.0 52.5 60.7 61.5 81.1 - - - - 55. - 43.9 - - 64.1 - 52.5 - - 56.8 65.1 70.1 61.1 66.4 69.4 60.3 59. 48.4 73.0 63.2 55.9 50.9 59. 51.4 40.7 53.8 64.9 63.9 Table 3. Accuracy scores on the testmini subset of MathVista. Task types: FQA: figure QA, GPS: geometry problem solving, MWP: math word problem, TQA: textbook QA, VQA: visual QA. Math reasoning types: ALG: algebraic, ARI: arithmetic, GEO: geometry, LOG: logical , NUM: numeric, SCI: scientific, STA: statistical."
        },
        {
            "title": "Model",
            "content": "#Params. All Acc Text Dominant Text Lite Vision Intensive Vision Dominant Vision Only Closed-source MLLMs Gemini-Pro [64] Qwen-VL-Max [5] GPT-4V Open-source MLLMs SPHINX-Plus [38] SPHINX-MoE [38] LLaVA-NeXT [27] LLaVA-NeXT [27] InternLM-XComposer2 [15] Math-LLaVA [62] MAVIS-7B [80] Math-PUMA-DeepSeek-Math [84] InternVL2 [12] Chimera-Reasoner - - - 13B 87B 110B 8B 7B 13B 7B 7B 2B 4B 8B 2B 4B 8B 23.5 25.3 39. 14.0 15.0 24.5 19.3 16.5 19.0 27.5 31.8 21.4 26.3 31.3 22.6 27.2 32.4 26.3 30.7 54.7 16.3 22.2 31.7 24.9 22.3 21.2 41.4 43.4 24.1 32.0 38. 27.3 31.4 39.6 23.5 26.1 41.4 12.8 16.4 24.1 20.9 17.0 19.8 29.1 35.4 22.5 28.6 34.5 23.9 30.8 35.8 23.0 24.1 34. 12.9 14.8 24.0 20.8 15.7 20.2 27.4 33.6 22.8 28.0 33.6 22.3 29.7 34.8 Table 4. Performance Comparison on MathVerse with the accuracy metric. 22.3 24.1 34.4 14.7 12.6 22.1 16.1 16.4 17.6 24.9 31. 21.1 24.4 32.6 22.8 25.7 32.7 22.2 21.4 31.6 13.2 9.1 20.7 13.8 11.0 16.4 14.6 14.7 16.6 18.8 17.0 16.9 18.2 19."
        },
        {
            "title": "ALL General Chart Table Math",
            "content": "InternVL2-2B [12] InternVL2-4B [12] InternVL2-8B [12] 48.3 57.0 61.6 Chimera-Reasoner-2B 53.1 Chimera-Reasoner-4B 61.3 Chimera-Reasoner-8B 64.9 45.3 50.1 52.7 46.0 54.0 57.5 58.9 66.2 71.2 60.3 64.8 71.2 50.0 65.7 67.1 62.9 72.9 62.9 44.2 58.3 66.5 56.1 66.9 71. Table 5. Accuracy scores of different visual content domain on the testmini subset of MathVista.Those do not belong to the last three domains are uniformly classified as General for simplicity. with the implementations for both Chimera-Reasoner and Chimera-Extractor in Sec. 4.1. Then, we provide comparison of Chimera models against previous generalist models across various benchmarks, including multi-modal reasoning (Sec. 4.2) and visual structural extraction (Sec. 4.3). Besides, we conduct quantitative ablation studies on the model design and training strategy in Sec. 4.4. 4.1. Datasets, Metrics and Implementation Details Datasets. In this paper, we conduct quantitative evaluations of Chimera model across range of challenging multimodal benchmarks, categorized into the following areas: Multi-modal Reasoning. We evaluate the ChimeraReasoner on MathVista [46] to determine its visual reasoning capabilities. Besides, we extend our evaluation on MathVerse [81], which is specifically designed for mathematical problem-solving, to gauge its performance in multi-modal mathematical reasoning. Visual Structural Extraction (SE). Evaluations of the Chimera-Reasoner on chart domain are conducted on the challenging ChartQA-SE [48] and PlotQA-SE [76] benchmarks. Following the protocol of StructChart [74], we utilize the test sets of both ChartQA [48] and PlotQA [53] to ensure fair comparison. For table and document, we manually collected and annotated table format transformation benchmark called Table-SE and document structural extraction benchmark called DocSE. Details regarding the data collection and annotation process can be found in Sec. 6 of Appendix. Metrics. In evaluations, we adhere to the default metrics used by benchmarks, such as MathVista, MathVerse, ChartQA-SE, and PlotQA-SE. For the assessment of the table format transformation, we use Tree-Edit Distance-based Similarity (TEDS) score and Edit Distance for evaluation. For document structural extraction, we take Edit Distance, Precision, BLEU and METEOR as evaluation metrics. Implementation Details. We initialize Chimera using the InternVL2 series. Specifically, we use InternVL2-2B, 4B, 8B to construct Chimera-Reasoner-2B, 4B, and 8B respectively, while using InternVL-1B to build Chimera-Extractor. In each training phase, we train the model for one epoch on the collected datasets. More detailed implementation specifics can be found in Sec. 8 of Appendix. 4.2. Comparison on Multi-modal Reasoning Comparison with Generalist Models. LMMs, such as LLaVA-OneVision [28], Qwen2-VL [71],InternVL2 [12] and GPT-4o demonstrate powerful multi-modal reasoning abilities on general purpose scenarios. However, these generalist models always exhibit limited performance when handling tasks under professional scenarios. Our model demonstrates exceptional performance on the challenging multi-modal reasoning benchmarks MathVista and MathVerse, significantly outperforming existing generalist models. As shown in Tab. 3 and Tab. 4, Chimera-Reasoner8B achieved overall accuracies of 64.9 and 32.4, respectively, setting new state-of-the-art (SOTA) for LMMs under the 70B scale. Chimera-Reasoner-2B and ChimeraReasoner-4B both significantly outperform the baseline InternVL2 series and achieve results comparable to models of much larger scales. On MathVista, Chimera-8B stands out among both closed-source and open-source LMMs of the same size, leading GPT-4o by 1.1%, and outperforming Qwen2-VL and InternVL by 6.7% and 3.9%, respectively. On MathVerse, Chimera-8B is only slightly behind GPT4V, and surpasses the hundred-billion-scale LLaVA-NeXT by 7.9 points. This demonstrates that our approach, by integrating domain knowledge from different expert models, effectively enhances performance in specialized domains. Comparison with Specialist Models. Compared to specialist models such as Math-LLaVA [62], MathPUMA [84], MAVIS [80], and G-LLaVA [16], Chimera demonstrates outstanding performance. Specifically, Chimera-Reasoner-8B outperforms the previous best expert models by 18.3% and 4.9% on MathVista and MathVerse, respectively. In contrast, the latest expert model, MathPUMA [84], achieves performances of 44.7 and 31.8 on the two benchmarks, which is notably inferior to our method. It is worth noting that these expert models have limited generalization ability and cannot handle tasks from other domains. In contrast, our model excels across various tasks in the table, chart, and document domains, proving Chimeras powerful versatility. Fine-Grained Analysis. We manually classified MathVista questions by the domain of visual content and present the models performance across domains in Tab. 5. In most cases, the model outperforms its baseline in each domain, further demonstrating that incorporating expert models enhances the generalist models performance on specialized tasks. We also observed that expert models improve performance in general scenarios, suggesting that domain knowledge provides diverse insights for language model in handling visual information, thereby enhancing the model even on scenarios where experts are not activated. 7 Task Metric Deplot [40] UniChart [49] ChartVLM [76] GPT-4V Qwen-VL [5] GOT [73] InternVL-2 [12] Chimera-Reasoner ChartQA-SE PlotQA-SE AP@strict AP@slight AP@high AP@strict AP@slight AP@high 61.4 70.9 72.9 3.1 16.5 26.5 42.3 53.1 56.0 10.5 26.0 26.9 71.8 81.4 84.2 3.8 46.8 54. 50.4 60.6 64.3 7.3 19.4 22.3 58.6 68.5 72.7 0.5 4.2 12.0 74.7 84.5 86.7 13.3 59.6 64. 73.7 83.9 87.2 5.7 55.0 61.8 74.1 84.4 87.6 5.9 62.1 71.0 Table 6. Performance comparison on ChartQA-SE and PlotQA-SE benchmarks. Metrics include Average Precision (AP) at strict, slight, and high levels."
        },
        {
            "title": "Method",
            "content": "Edit Distance TEDS TEDS (structure only) InternVL-2 [12] Qwen2-VL [71] StructEqTable [75] GOT [73] Chimera-Reasoner 0.229 0.231 0.226 0.257 0.165 0.676 0.690 0.706 0.745 0.740 0.762 0.773 0.787 0.830 0.828 Table 7. Comparison of performance on Table-SE across different methods: TEDS, TEDS (structure only), and Edit Distance. Method Edit Distance Precision BLEU METEOR en zh en zh en zh en zh InternVL [12] GOT [73] Chimera-Extractor 0.504 0.355 0.304 0.604 0.510 0.461 65.4 67.9 69.6 66.0 71.2 66. 38.4 52.5 49.8 33.1 34.3 40.5 52.6 65.3 64.8 50.6 53.9 56.9 Table 8. Comparison of performance metrics across different methods on Doc-SE. Metrics include Edit Distance (lower is better), Precision, BLEU, and METEOR (higher is better). Model Ratio ALL General Chart Table Math InternVL2-4B [12] N/A InternVL2-4B-NF [12] N/A 0.0 Chimera-4B-0.0 0.3 Chimera-4B 0.5 Chimera-4B-0.5 1.0 Chimera-4B-1.0 57.0 58.5 59.4 61.3 60.4 56.2 50.1 51.5 50.8 54.0 51.3 51.5 66.2 67.1 66.2 64.8 68.5 63.5 65.7 74.3 67.1 72.9 70.0 72. 58.3 58.6 65.5 66.9 65.8 53.6 Table 9. Ablation results on different visual content domain on the testmini subset of MathVista. InternVL2-4B-NF represents naive finetune of baseline with the same settings, Chimera-4B-R means Chimera model trained with mask ratio in GSCM. Chimera-Reasoner-8B performs similarly in the chart domain but slightly worse in the table domain. This is due to the over-specialized function of the table expert, which, despite supporting comprehensive extraction, may introduces noise for the 8B baseline model because of the considerable task gap. In contrast, the chart experts pre-training task covers both extraction and perception, with minimal impact. The math expert consistently improves performance across models due to its alignment with reasoning tasks. 4.3. Comparison on Visual Structural Extraction Comparison with Generalist Models. For specialized tasks beyond VQA, generalist models similarly show limited performance. Tab. 6 and Tab. 7 present the results of visual structural extraction in the Chart and Table domains, Figure 4. Comparison of Edit Distance across different document categories on Document Structural Extraction (Doc-SE) task. respectively. On ChartQA-SE and PlotQA-SE, ChimeraReasoner-8B outperforms representative generalist models such as GPT-4V, Qwen-VL [5], and InternVL-2 [12] across the AP@strict, AP@slight, and AP@high metrics. In Table-SE, Chimera-Reasoner-8B leads InternVL2 and Qwen2-VL by larger margin in Edit Distance and TED scores, demonstrating strong domain-specific capability. Tab. 8 and Fig. 4 exhibit results on Doc-SE, ChimeraExtractor significantly outperforms InternVL2 [12] across four metrics in bilingual tasks and shows balanced performance across different document categories. Comparison with Specialist Models. Compared to specialist models majoring in single task, Chimera still demonstrates strong performance. Specifically, for ChartQA-SE and PlotQA-SE, Chimera-Reasoner-8B, compared to the SOTA expert model GOT, achieves excellent or competitive results across three metrics. In Table-SE, Chimera similarly achieves comparable TEDS scores and outperforms with lower Edit Distance by 0.092. As for Doc-SE, Chimera-Extractor leads in most metrics for both English and Chinese documents, showing better overall generalization across document categories than GOT. 4.4. Ablation Study We conducted an ablation study on 4B scale models to assess our approachs effectiveness, as shown in Tab. 9. It should be noted that model with mask ratio 1.0 does not have access to the general encoder during training, contrary to our intentions. Thus, we modified this case to give the model an 80% probability of masking all general features. The results show that naively finetuning the LMM leads to limited performance improvement. By incorporating domain knowledge from expert models, even the case without GSCM still yields better results than naive finetuning. As the mask ratio increases, the models performance improves initially and then declines. This indicates that slightly masking helps balance encoder optimization, leading to better alignment. However, as the mask ratio increase, we believe excessive masking prevents the model from effectively learning to utilize both features for reasoning. We also observed that performance trends vary across domains as the mask ratio changes, suggesting that the alignment difficulty of expert models differs by domain and task, which we leave for future exploration. 5. Conclusion We present Chimera, scalable pipeline that integrates specialist models into generalist LMMs, enabling adaptation to specialized tasks. Our approach transforms LMMs, like InternVL-2, into versatile models capable of handling tasks across tables, math, documents, etc. Chimera pioneers new directions for bridging generalist and specialist models."
        },
        {
            "title": "References",
            "content": "[1] The claude 3 model family: Opus, sonnet, haiku. [2] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amelie Heliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Konceland Hannaneh Hajishirzi. interpretable math word problem arXiv preprint Kedziorski, Yejin Choi, Mathqa: solving with operation-based formalisms. arXiv:1905.13319, 2019."
        },
        {
            "title": "Towards",
            "content": "[4] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com,, 2024. [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. [6] Tom Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. [7] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18181826, 2024. [8] Shuaichen Chang, David Palzer, Jialin Li, Eric FoslerLussier, and Ningchuan Xiao. Mapqa: dataset for arXiv preprint question answering on choropleth maps. arXiv:2211.08545, 2022. [9] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [11] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642826438, 2024. [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [14] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. [15] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprearXiv preprint hension in vision-language large model. arXiv:2401.16420, 2024. [16] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [18] Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl2: High-resolution compressing for ocrfree multi-page document understanding. arXiv preprint arXiv:2409.03420, 2024. Johnson, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. Bharath Hariharan, [19] Justin [20] Jeff Johnson, Matthijs Douze, and Herve Jegou. BillionIEEE Transactions on scale similarity search with GPUs. Big Data, 7(3):535547, 2019. 9 [21] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [22] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. [23] Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. arXiv preprint arXiv:2203.06486, 2022. [24] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. [25] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. [26] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 49995007, 2017. [27] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 12888 12900. PMLR, 2022. [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. [31] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. [32] Mingsheng Li, Xin Chen, Chi Zhang, Sijin Chen, Hongyuan Zhu, Fukun Yin, Gang Yu, and Tao Chen. M3dbench: Lets instruct large models with multi-modal 3d prompts. arXiv preprint arXiv:2312.10763, 2023. [33] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024. [34] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1496314973, 2023. [35] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:1761217625, 2022. [36] Will Lifferth, Walter Reade, and Addison Howard. Kaghttps : / / kaggle . com / gle - llm science exam. competitions / kaggle - llm - science - exam, 2023. Kaggle. [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [38] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. [39] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [40] Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation, 2022. [41] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal arXiv preprint model with robust arXiv:2306.14565, 2023. instruction tuning. [42] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [44] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [45] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [46] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [47] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, SongChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023. [48] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [49] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: universal visionlanguage pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. [50] Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. Chartinstruct: Instruction tuning for chart comprehension and reasoning. arXiv preprint arXiv:2403.09028, 2024. [51] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172, 2024. [52] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [53] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536, 2020. [54] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [55] OpenAI. Gpt-4v. https://openai.com/index/ gpt-4v-system-card/, 2023. [56] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. [57] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. [58] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [59] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource International for question answering on scholarly articles. Journal on Digital Libraries, 23(3):289301, 2022. [60] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. [62] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [63] Benny Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich chart captioning. arXiv preprint arXiv:2307.05356, 2023. [64] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [65] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [66] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [68] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [69] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution arXiv preprint for precise document content extraction. arXiv:2409.18839, 2024. [70] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset, 2024. [71] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [84] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. arXiv preprint arXiv:2408.08640, 2024. [72] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [73] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. [74] Renqiu Xia, Bo Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Yu Qiao, and Junchi Yan. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268, 2023. [75] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie Wu, Hancheng Ye, et al. Docgenome: An open largescale scientific document benchmark for training and testing multi-modal large language models. arXiv preprint arXiv:2406.11633, 2024. [76] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. [77] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: large language model with Revolutionizing multi-modal modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1304013051, 2024. [78] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [79] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. Sciglm: Training scientific language models with selfreflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950, 2024. [80] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv preprint arXiv:2407.08739, 2024. [81] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2025. [82] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. [83] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. Multimodal table understanding. arXiv preprint arXiv:2406.08100, 2024."
        },
        {
            "title": "Outlines",
            "content": "In the Appendix section, we provide additional details and qualitative results from the following aspects: Sec. 6: Introduction of Table-SE and Doc-SE. Sec. 7: Experiments of scaling up more experts. Sec. 8: More information of implementation details. Sec. 9: Visualization of Chimeras visual content extraction performance. 6. Details of Table-SE and Doc-SE In Tab. 7 and Tab. 8 of the main text, we conduct the experiments on Table Structural Extraction (Table-SE) task and Document Structural Extraction (Doc-SE) task, respectively. In this section, we primarily introduce the evaluation dataset construction method and provide detailed information about the dataset. 6.1. Data Source Background w/o Background w/ Background Equation w/o Equation w/ Equation Language English English & Chinese Mixed Chinese Table Format Three-line Table Full-bordered Table Partial-bordered Table w/o Merged Cells w/ Merged Cells"
        },
        {
            "title": "Layout\nHorizontal\nVertical",
            "content": "# Total"
        },
        {
            "title": "Count",
            "content": "80 20 78 22 45 5 50 47 39 14 58 42 97"
        },
        {
            "title": "Count",
            "content": "Document Categories PPT2PDF Academic Literature Book Colorful Textbook Magazine Exam Paper Note Newspaper"
        },
        {
            "title": "Language\nSimplified Chinese\nEnglish",
            "content": "Layout 1 and More Column Single Column Other Layout Double Column Three Column # Total 43 42 13 37 30 7 18 15 128 77 27 91 43 40 4 Table 10. Statistical information of Doc-SE task. We constructed our benchmark by systematically sampling from an initial pool of 200, 000 PDF documents obtained from Common Crawl, Google, Baidu search engines, and internal repositories, which is consistent with the approach of OmniDocBench. Visual features were first extracted using ResNet-50 [17], followed by clustering with Faiss [20] to capture diverse document patterns. From 10 OmniDocBench Homepage: opendatalab/OmniDocBench https : / / github . com / 1 Table 11. Statistical information of Table-SE task. cluster centroids, we selected 1,100 visually distinct pages and annotated them manually with attributes such as page type, layout type, and language. As summarized in Tab. 10 and Tab. 11, the finalized benchmark comprises 205 pagelevel PDF images and 100 table images, effectively representing variety of real-world document layouts and characteristics. 6.2. Annotation Process For ensuring annotation quality and efficiency, we design separate standardized processes for page-level PDF documents and tables. For page-level PDF documents, our process consists of three stages: (1) We first employ fine-tuned LayoutLMv3 for layout detection and PaddleOCR for text recognition as intelligent pre-annotation. (2) Professional annotators then refine the detection boxes, verify text content accuracy, and enhance annotations with reading order and affiliation details. (3) Finally, researchers review the annotations to ensure overall quality and accuracy. For table annotations, we follow similar but specialized three-stage approach: (1) We utilize GPT-4o and PaddleOCR for initial table annotations. (2) Annotators then verify and correct the table structure and content, using specialized tools like Tables Generator for verification. (3) Finally, experts through table annotations re-rendering to ensure correct HTML and LaTeX code labels. Chimera-Reasoner excels in extracting and formatting table content from both Arxiv-style and more diverse table layouts with high accuracy. 9.2. Chart Structural Extraction We provide the rendered table of the output results of Chimera-Reasoner-8B to show its chart structural extraction performance. As shown in Fig. 10, Fig. 11 and Fig. 12, Chimera-Reasoner can identify and extract information from various types of charts, such as pie charts, line graphs, bar charts, etc., and output this information in structured format accurately. 9.3. Document Context Extraction We provide the rendered page of the output results of Chimera-Extractor to show its document content extraction performance. As shown in Fig. 13, Fig. 14, Fig. 15 and Fig. 16, Chimera-Extractor demonstrates exceptional content extraction capabilities on both single-column and double-column documents, effectively extracting structured information end-to-end from text-dense visual inputs. 6.3. Showcases We provide several visualization examples of Table-SE in Fig. 5 and Fig. 6, where each item contains visual table and its corresponding LaTeX code. 7. Experiments of Scaling Up More Experts To further validate the impact of scaling up the number of expert models, we provide ablation results introducing only the chart expert. In this case, non-chart data are encoded solely by the general encoder during training. As shown in Tab. 12, incorporating only the chart expert obtains lower MathVista [46] overall score by 1.9 points than ChimeraReasoner-4B. Specifically, InternVL2-4B w/ Chart Expert also shows improvements in general scenarios, though less significant than Chimera, which integrates three expert models. In the chart domain, InternVL2-4B w/ Chart Expert achieves notable gains by avoiding conflicts among multiple experts with large task gaps. However, Chimeras integration of multiple experts enhances performance across diverse In the math domain, domains, boosting overall results. InternVL2-4B w/ Chart Expert scores 6.1 points lower than Chimera, demonstrating the strong mathematical reasoning capabilities derived from integrating the math expert. 8. Training Configuration The training strategy is summarized in Tab. 13 and Tab. 14. During the two-stage training process, we gradually increase the maximum image resolution and the number of visual tokens of the general visual encoder Eg. In the Domain-General Knowledge Alignment stage, we use images as input for Eg without employing thumbnail the widely-used Dynamic High Resolution (DHR) technique [12, 28]. In the Visual Instruction Tuning stage, DHR is introduced, allowing up to six times more visual tokens. At this stage, we apply the Generalist-Specialist Collaboration Masking (GSCM) mechanism with masking ratio of 0.3 to constrain Eg, encouraging the model to leverage domain-specific information from expert models. For trainable modules, the Domain-General Knowledge Alignment stage updates only the General Projector and Expert Projector Set Se. In subsequent stages, the General Projector g, Expert Projector Set Se, and Language Model are updated. 9. Visualization of Chimera on Visual Content"
        },
        {
            "title": "Extraction",
            "content": "9.1. Table Format Transformation We provide the rendered table of the output results of Chimera-Reasoner-8B to show its table format transformation performance. As shown in Fig. 7, Fig. 8 and Fig. 9, 2 Figure 5. Showcase of Table-SE task. 3 Figure 6. Showcase of Table-SE task."
        },
        {
            "title": "ALL General Chart Table Math",
            "content": "InternVL2-4B 57.0 50.1 66.2 65.7 58.3 InternVL2-4B w/ Chart Expert 59.4 52. 68.0 72.9 60.8 Chimera-Reasoner-4B 61.3 54.0 64.8 72.9 66.9 Table 12. Accuracy scores of different visual content domain on the testmini subset of MathVista. Those do not belong to the last three domains are uniformly classified as General for simplicity. InternVL2-4B w/ Chart Expert represent the case only integrating chart expert model. Chimera-Reasoner-4B integrates more expert models, including specialized models in chart, table, and math domains. i g i Domain-General Knowledge Alignment Visual Instruction Tuning Resolution #Tokens General Encoder Eg Table Encoder Etable Chart Encoder Echart Math Encoder Emath General Encoder Eg Table Encoder Etable Chart Encoder Echart Math Encoder Emath 448 N/A N/A 336 256 2048 2048 576 448{{1,2,3,4,5,6}1, 1{2,3,4,5,6}, 2 {2,3}, 3 2 }} N/A N/A 336 Max 2566 2048 2048 Total Tokens 256 + {0, 2048, 2048, 576} Max 2566 + {0, 2048, 2048, 576} #Samples GSCM ratio Dynamic High Res [12] Trainable Batch Size LR Warm Up LR Scheduler Max Length Weight Decay Epoch 1.1M N/A False General Projector g, Expert Projector Set Se 256/128 4e-5/2e-5 100 steps Consine 4096 0.01 1 2.6M 0.3 True General Projector g, Expert Projector Set Se, Language Model 128 2e-5/1e-5 0.03 ratio Consine 8192 0.01 Table 13. Detailed configuration for each training stage of Chimera-Reasoner-2B, Chimera-Reasoner-4B and Chimera-Reasoner-8B. The table outlines the progression of vision parameters, dataset characteristics and training hyperparameters. For elements containing /, the left side represents configurations used by the 2B and 4B model, while the right side represents configurations used by the 8B model. s n a Resolution #Tokens General Encoder Eg Document Encoder Edocument General Encoder Eg Document Encoder Edocument Total Tokens #Samples GSCM ratio Dynamic High Res [12] Trainable Batch Size LR Warm Up LR Scheduler Max Length Weight Decay Epoch Domain-General Knowledge Alignment Visual Instruction Tuning 448 256 256 256 + 256 448{{1,2,3,4,5,6}1, 1{2,3,4,5,6}, 2 {2,3}, 3 2 }} 1024 Max 2566 256 Max 2566 + 256 995K N/A False General Projector g, Expert Projector Set Se 256 4e-5 100 steps Consine 4096 0.01 275K 0.3 True General Projector g, Expert Projector Set Se, Language Model 128 2e-5 0.03 ratio Consine 8192 0.01 1 Table 14. Detailed configuration for each training stage of Chimera-Extractor-1B. The table outlines the progression of vision parameters, dataset characteristics and training hyperparameters. 5 Figure 7. Output of Chimera-Reasoner-8B on Table Structural Extraction task. 6 Figure 8. Output of Chimera-Reasoner-8B on Table Structural Extraction task. 7 Figure 9. Output of Chimera-Reasoner-8B on Table Structural Extraction task. 8 Figure 10. Output of Chimera-Reasoner-8B on Chart Structural Extraction task. 9 Figure 11. Output of Chimera-Reasoner-8B on Chart Structural Extraction task. Figure 12. Output of Chimera-Reasoner-8B on Chart Structural Extraction task. 11 Figure 13. Output of Chimera-Extractor-1B on Document Structural Extraction task. 12 Figure 14. Output of Chimera-Extractor-1B on Document Structural Extraction task. Figure 15. Output of Chimera-Extractor-1B on Document Structural Extraction task. 14 Figure 16. Output of Chimera-Extractor-1B on Document Structural Extraction task."
        }
    ],
    "affiliations": [
        "Fudan University",
        "MMLab, The Chinese University of Hong Kong",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}