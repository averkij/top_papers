{
    "paper_title": "Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space",
    "authors": [
        "Mikolaj Czerkawski",
        "Marcin Kluczek",
        "Jędrzej S. Bojanowski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the ever-increasing volumes of the Earth observation data present in the archives of large programmes such as Copernicus, there is a growing need for efficient vector representations of the underlying raw data. The approach of extracting feature representations from pretrained deep neural networks is a powerful approach that can provide semantic abstractions of the input data. However, the way this is done for imagery archives containing geospatial data has not yet been defined. In this work, an extension is proposed to an existing community project, Major TOM, focused on the provision and standardization of open and free AI-ready datasets for Earth observation. Furthermore, four global and dense embedding datasets are released openly and for free along with the publication of this manuscript, resulting in the most comprehensive global open dataset of geospatial visual embeddings in terms of covered Earth's surface."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 0 0 6 5 0 . 2 1 4 2 : r Global and Dense Embeddings of Earth Global and Dense Embeddings of Earth: Major TOM Floating in the Latent Space Mikolaj Czerkawski* Φ-lab, European Space Agency Frascati, Italy Marcin Kluczek* CloudFerro Warsaw, Poland Jędrzej S. Bojanowski CloudFerro Warsaw, Poland *Equal Contribution mikolaj.czerkawski@esa.int mkluczek@cloudferro.com jbojanowski@cloudferro.com"
        },
        {
            "title": "Abstract",
            "content": "With the ever-increasing volumes of the Earth observation data present in the archives of large programmes such as Copernicus, there is growing need for efficient vector representations of the underlying raw data. The approach of extracting feature representations from pretrained deep neural networks is powerful approach that can provide semantic abstractions of the input data. However, the way this is done for imagery archives containing geospatial data has not yet been defined. In this work, an extension is proposed to an existing community project, Major TOM, focused on the provision and standardization of open and free AI-ready datasets for Earth observation. Furthermore, four global and dense embedding datasets are released openly and for free along with the publication of this manuscript, resulting in the most comprehensive global open dataset of geospatial visual embeddings in terms of covered Earths surface. Keywords: Earth observation, Copernicus Data, Image Embeddings, AI datasets"
        },
        {
            "title": "1 Introduction",
            "content": "There is one trend in the domain of information technologies that does seem more consistent than anything else in recent decades, and it is the growth of produced data. Whether it is piece of information generated online or series of bytes transmitted by satellite, the rate at which new data are produced keeps increasing every year. While every new piece of data can be considered valuable and beneficial, the increasing volume makes large-scale analyses more challenging and computationally expensive. This phenomenon is already being observed in the domain of Earth observation data, where every single year, petabytes of new data captured by the satellites enter the archives. For that reason, it is crucial to develop methods capable of navigating vast databases of imagery at speed and low computational costs. solution to this challenge can come in the form of extracting semantic features from the visual data at large scale, in process known as embedding (Frome et al., 2013; Mikolov et al., 2013). While manually engineered features could be used to compress the images into efficient vector representations, this approach has predominantly become effective with the 1 Major TOM Floating in the Latent Space Figure 1: The pipeline building Major TOM embedding expansions according to the proposed standard. It begins with grid cell fragmenting, image preprocessing, and embedding and packing into the geoparquet archive format. development of large pre-trained deep neural networks, which can learn powerful representations of the input data. This includes methods, which rely solely on self-supervision. The process of embedding data into compressed representation space is far from new and has been explored for over decade (Mikolov et al., 2013; Frome et al., 2013), accelerated by the growth in popularity of deep learning pipelines and models (Gordo et al., 2016). Visual descriptors, which play similar role to embeddings (and sometimes could even be synonymous) have also been popular topic for many years in computer vision (Tolias and Jégou, 2014; Philbin et al., 2007; Shen et al., 2014; Radenović et al., 2015). In the field of Earth observation, several works have explored the use of vector representations for geospatial data. Most importantly, this includes work that attempted to learn meaningful representations of geospatial coordinates (Yin et al., 2019, 2021; Tarasiou and Zafeiriou, 2022; Klemmer et al., 2023). However, most of these efforts focus on learning new latent space rather than applying an existing embedding function at large scale. In this direction, LAION-EO (Czerkawski and Francis, 2023) is notable work, where 5 billion of images coming from the web (LAION-5B dataset (Schuhmann et al., 2022)) have been embedded with CLIP model (Radford et al., 2021) and used to retrieve instances of satellite imagery from billions of unorganized internet data. This work attempts to explore the direction of embedding Earth observation images at the global scale with several types of existing models and propose standard for doing so. For that reason, it largely builds on top of an ongoing community project of Major TOM (Francis and Czerkawski, 2024), which aims to standardize the curation of large Earth observation datasets and set up paths for community-based dataset growth. This work is one example of such growth, and just like the first Major TOM release, it aims at easy and free access to the datasets and accompanying tools for interacting with them. The definition of standard for embedding expansions brings several benefits. clear definition leads to reproducible work and embedding databases that can be analyzed, debugged, and fixed if needed. It also ensures that the embedding datasets produced by independent entities are compatible with each other, at least to some degree. It also makes the evaluation of the models used for producing the embeddings more accessible. By precomputing the latent features from model, the embedding dataset producer removes the Global and Dense Embeddings of Earth larger part of the computational burden, making it possible to evaluate the output representations for various models at low cost. Finally, by building on top of the Major TOM grid system, embeddings generated according to the standard can be used as aligned embedding time-series data with no further adjustments since the sample footprints are already aligned. This work involves computation and analysis of global embeddings from 4 different pretrained models. This is done in order to highlight the vastly different behaviors, strengths, and weaknesses. It is unlikely that the space of Earth observation will be dominated by single general-purpose model. At the moment, the ecosystem of AI for satellite imagery already involves large number of models to choose from, operating on various formatting (such as normalization, numbers of bands, processing level) of data, even within domain of single sensor. For that reason, an organized practice of inter-comparison between different embedding model candidates is going to be crucial. In the first release of Major TOM embeddings, over 169 million embeddings are released as result of processing more than 62 TB of raw data, encompassing over 3.5 million unique images. The dataset distills information from approximately 9.368 trillion pixels of source imagery. At the time of release, this is the first open dataset of Copernicus embeddings built at this scale, with dense and global coverage across the full acquisition area of the sensor. The remainder of this paper describes the embedding methodology, the release standard, the fragmenting approach, the models selected for this release, and finally, some early visualizations of the global embedding data."
        },
        {
            "title": "2 Embedding Methodology",
            "content": "The global embedding dataset is built on top of the Major TOM Core datasets (Francis and Czerkawski, 2024), which enable fast and free access to more than 60 TB of AIready Copernicus data, densely covering the globe with multiple sensing modalities and processing levels. In this release, three core datasets were used: Major-TOM/Core-S2L1C, Major-TOM/Core-S2L2A, and Major-TOM/Core-S1RTC, corresponding to Sentinel-2 L1C and L2A, as well as Sentinel-1 RTC products. The images in those datasets cover an area of 10.68 by 10.68 square kilometers (containing full Major TOM grid cell and some margin), which corresponds to 1,068 by 1,068 pixels in the resolution of 10 meters. Since the majority of pre-trained deep neural networks operate on smaller input shapes (such as 224 pixels), the original images from Major TOM had to go through fragmenting stage. It is worth to note that many model interfaces automatically resize input images of varying shapes to fit the supported input dimensions. However, for over-sized input images, this process inevitably results in information loss. Such loss could be particularly undesirable for remote sensing data, which often contains fine details crucial for analysis. The complete pipeline starts with the fragmenting function as shown in Figure 1, then proceeds to the model-specific image preprocessing (which also includes the appropriate normalization function) before the data is fed to the embedding model. Finally, the output embedding and metadata (footprint geometries, pixel boundary box, source satellite product information, etc.) are arranged into geoparquet archives, which constitute the output embedding dataset. 3 Major TOM Floating in the Latent Space Table 1: Stastistics of the released datasets: Model - model used for the embedding, Source - source Major TOM Core dataset, Embeddings - total number of output embeddings, Grid cells - total number of processed Major TOM grid cells, Te - grid cell processing time on 1L40S, TT - total execution time (on 2L40S) Model SSL4EO-S2 (Wang et al., 2023b) SSL4EO-S1 (Wang et al., 2023b) SigLIP(RGB) (Zhai 2023) DINOv2(RGB) (Oquab et al., 2024) al., et Source Patch size Embeddings Grid cells 2,245,886 S2L1C 224 224 56,147,150 Te 0.41 TT 125 S1RTC 224 224 36,748,875 1,469,955 3.34 60 S2L2A (RGB) S2L2A (RGB) 384 384 20,212,974 2,245,886 0.12 111 224 224 56,147,150 2,245,886 0.33 37 In general, the normalization follows the specific approach associated with given embedding model. For the general-purpose vision models which have been trained on standard image data (generally in the range of 0 to 1 and with values stored in 8-bit integers), special steps were taken to map the RGB channels of the multi-spectral Sentinel-2 data to scaling, which resembles the true color. This was done by multiplying the reflectance value by factor of 2.5 and then clipping the image values between 0 and 1. In summary, this release delivers embeddings of 4 models listed in Table 1. The following sections provide more detail about the data format (Section 3), the fragmenting operation (Section 4) and the process of model selection (Section 5)."
        },
        {
            "title": "3 Release Standard: Major TOM Embedding Expansions",
            "content": "The quality of the output release standard is key factor in the context of usability and reproducibility of the embedding datasets. This relates to both the format of the metadata as well as the embeddings themselves. In this release, the embeddings have been combined with the metadata into joint archive files in the Parquet format, which allows for reading of isolated columns of data and provides built-in compression of the columns. Storing both embeddings and their metadata in this combined form is step aimed at reducing the risk of potential bugs when the embedding data needs to be matched with its context information from metadata. 3.1 Metadata Table 2 outlines the fields of the geoparquet output file in the Major TOM Embedding expansions. This includes embeddings (stored as numpy array object), and set of metadata 4 Global and Dense Embeddings of Earth Table 2: Columns of the Major TOM embedding Geoparquet file Field unique_id Type string Description hash generated from geometry, time, product_id and the embedding raw embedding array Major TOM cell Major TOM cell row Major TOM cell col ID of the original product Timestamp of the sample array string int int string string geometry Polygon footprint (WGS84) of the fragment string Polygon footprint (image UTM) of the fragment CRS of the original product Boundary box of the fragment (pixels) Centre of the fragment latitude Centre of the fragment longitude bbox float float embedding grid_cell grid_row_u grid_col_r product_id timestamp geometry utm_footprint utm_crs pixel_bbox centre_lat centre_lon to ensure reproducibility as well as transparency of the dataset. The unique_id column is checksum calculated based on the geometry, timestamp, product_id and the embedding for each processed fragment to improve data integrity. The spatial context of the Major TOM grid cell is described by the grid_cell, grid_row_u, and grid_col_r (the integer index data provided to enable fast filtering), along with the satellite product information in product_id and the corresponding timestamp. Finally, there is set of columns designed to provide the context of the individual fragment used as input for the embedding model. This is described by the geometry column (in the WGS84 coordinate reference system), utm_footprint (stored as well-known text representation of geometry), and utm_crs. For fast indexing of the original Major TOM sample, pixel_bbox defines the exact indices used to extract fragment from the original image. Finally, centre_lat and centre_lon are provided as floats representing the latitude and longitude of the fragments to enable fast querying and filtering. 3.2 Embedding Data Format The GeoParquet format is an extension to the standard column-based Parquet file format, which most importantly introduces support for geometry-type column. This way the files can benefit from the efficiency and convenience of the Parquet standard while being interoperable with standard geospatial data processing pipelines. This choice is also compatible with the original Major TOM Core datasets which are currently stored in regular parquet files. The embeddings themselves are stored as numpy array in one of the columns. While Parquet supports byte arrays in columns (in fact, this is how GeoTIFF data is stored in the original Major TOM Core dataset parquets), the bytes require specific interpretation context. For example, some of the embedding datasets might choose to store the data in higher or lower precision of number representation than the common 4-byte floating point. 5 Major TOM Floating in the Latent Space In case it happens, the user of the dataset must be fully aware of this change in order to read raw bytes, and mistakes can happen. For that reason, the current approach is to store the numpy objects which are more heavy-weight but provide the necessary context of the data type, removing the risk of improper reading of the array. This choice does not exclude the option of working with raw byte types. In fact, the software release demonstrates how to turn set of Major TOM Embedding dataset parquets into set of metadata and byte array format to improve reading speed. However, this process is done from scratch locally, which ensures that the data is first downloaded in combined GeoParquet format, ensuring the initial integrity of the archive. Figure 2: Fragmenting function for SigLIP (fragments of 384 pixels) for 2 independent Major TOM grid cells plotted next to each other. Note that these cells (green and red) are fragmented and processed independently, and are plotted here together for visualisation."
        },
        {
            "title": "4 Fragmenting Major TOM Cells",
            "content": "Just like the original Major TOM datasets were designed with standardization and alignment in mind, the same approach is followed for the embedding expansions. For consistent Major TOM datasets, the footprints of samples covering the same Major TOM grid cell are going to align as long as the original products are in the same projection. While this approach has been designed to make various image datasets compatible with each other, it can also have positive impacts in the context of embedding these datasets. Namely, aligned embedding footprints will lead to the emergence of consistent embedding time series. For that reason, this work proposes an algorithm that fragments given Major TOM cell in consistent manner and defines specific configuration. The algorithm configuration is based on an input image size sI (in the case of Major TOM Sentinel-2 sam6 Global and Dense Embeddings of Earth Figure 3: Individual fragments for 2 independent Major TOM grid cells plotted next to each other. ples, this is 1,068 pixels), target fragment size sf (which corresponds to the input image size of given model, such as common 384 pixels), and target overlap ot fraction (which defines the approximate overlap of consecutive fragments within the cell). The algorithm will prioritize the inclusion of all pixels from the source cell (as shown in Figures 2 and 3), which is why the overlap ot will be adjusted in order to cover the complete source image fully. 4.1 Algorithm The Algorithm 1 contains complete representation of the fragmenting function, which includes the adjustment of the feasible overlap so based on the target overlap fraction ot and based on that, computation of the total number of fragments per dimension. It is then followed by for loop iterating over all fragment indices and computing the row and column offset for given fragment. These offsets are then used to extract corresponding crop from the source image along with the boundary box index. If fragment is near the far edge of the image and the border_shift option is set as True, then it will be ensured that the fragment touches the far border of the image, which is done to prevent any source pixel omission (although this effect would be minimal since the target overlap is adjusted based on the image size)."
        },
        {
            "title": "5 Embedding Models",
            "content": "There are quite few open models trained on large-scale Copernicus data (albeit not as large as Major TOM at the time of writing) (Manas et al., 2021; Cong et al., 2022; Bastani et al., 2023; Jakubik et al., 2023; Xiong et al., 2024; Tseng et al., 2023) or other types of satellite sensor data (Reed et al., 2023). The majority of these models operate on Sentinel-2 7 Major TOM Floating in the Latent Space Algorithm 1 Image Fragmentation Algorithm with overlap and border shift parameters Require: Image of size sI sI , fragment size sf , target overlap ot [0, 1], border_shift {False, True} Ensure: sI sf 1: if sI = sf then 2: 3: else 4: so sf ot (cid:16) max border shift) fragments[0, 0] = Single fragment corresponds to the full image. 1, round (cid:17)(cid:17) (cid:16) sI sf sf so Target overlap in pixels Number of fragments along each axis (excluding so sf sI sf for row_idx = 0, 1, . . . , 1 do for col_idx = 0, 1, . . . , 1 do Adjusted overlap to ensure proper fragment alignment Iterate through rows Iterate through columns if row_idx = 1 and border_shift then row_offset sI sf else row_offset row_idx (sf so) end if if col_idx = 1 and border_shift then col_offset sI sf else col_offset col_idx (sf so) end if xys[row_idx, col_idx] = {row_offset, col_offset} fragments[row_idx, col_idx] = I[row_offset:row_offset+sf , col_offset:col_offset+sf ] end for 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end for 25: 26: end if 27: Return fragments, xys data, given the versatility of this optical sensor. However, the domain of Sentinel-1 generalpurpose models is also growing, at slower pace though. 5.1 Sentinel-2 For Sentinel-2, there is range of models trained in self-supervised manner directly on the multi-spectral Level 1C (top of atmosphere) representation (Wang et al., 2023b,a). In some cases, like in SatCLIP (Klemmer et al., 2023), these models have been used with Level 2A data (bottom of the atmosphere) by filling in the missing cirrus band with zeroes. There are other pre-trained models which may also be used for this purpose, however, SSL4EO has been prioritised due to the application in independent works (SatCLIP), simple and 8 Global and Dense Embeddings of Earth small (25 million parameters) architecture of ResNet50, easily accessible code, well-known training data, high benchmark performance (Szwarcman et al., 2024), and inclusion in the torchgeo package (which can lead to higher reproducibility of the approach). Another route to obtaining embeddings of Sentinel-2 images is to re-use large pre-trained models from general-purpose computer vision contexts, such as CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023), or DINOv2 (Oquab et al., 2024). This immediately introduces certain limitations, since only the three RGB can be used, and most of the models will expect images scaled between 0-1 with intensity distribution similar to those observed in the training set. This means that some of the high intensity values detected by the Sentinel-2 sensor must be discarded in the process. Finally, most of the mentioned models compute single global embedding, which represents the input image. recent model trained with multi-pretext masking self-supervision (Nedungadi et al., 2023) can be used to obtain spatially distributed map of embeddings per image and this model is currently being investigated. The models considered in this project are listed in Table 3. Model Architecture Notes Access Table 3: Embeddings of the Sentinel-2 Core dataset Sentinel-2 L1C (Top of Atmosphere) SSL4EO-S2 et (Wang 2023b) al., ResNet50 DeCUR et al., 2023a) (Wang ResNet50 Models trained with various SSL methods on multi-spectral S2 data (L1C). Multi-modal selfsupervision by learning interand intra-modal embeddings hf.co:Major-TOM/CoreS2L2A-SSL4EO (To be computed) Sentinel-2 L2A (Bottom of Atmosphere) SigLIP(RGB) (Zhai et al., 2023) ViT et DINOv2(RGB) (Oquab 2024) MMEarth dungadi 2023) et al., (Neal., ViT ConvNeXt visionPowerful language model, yet not designed for EO data. Powerful vision model trained selfsupervision. Multi-pretext masked autoencoder with hf.co:Major-TOM/CoreS2RGB-SigLIP hf.co:Major-TOM/CoreS2RGB-DINOv2 (To be computed) 5.2 SentinelThere do not seem to be as many models available for Sentinel-1 as for Sentinel-2 openly available, as indicated by the presence of only four pre-trained model weights on the torchgeo website (Stewart et al., 2022). These 4 models include SSL4EO-S12 (Wang et al., 2023b), 9 Major TOM Floating in the Latent Space DeCUR (Wang et al., 2023a), and SATLAS (Bastani et al., 2023). As mentioned earlier, due to the same reasons for preference towards the SSL4EO models, this one was prioritised for the first release of Sentinel-1 embeddings, as shown in Table 4. Table 4: Embeddings of the Sentinel-1 Core dataset Model Architecture Access Sentinel-1 RTC (Radiometrically Terrain Corrected ) SSL4EO-S1 ResNet50 ResNet50 DeCUR hf.co:Major-TOM/Core-S1RTC-SSL4EO (To be computed) 5.3 Preliminary Visualization Preliminary visualization via principal component analysis is shown in Figure 4 for all 4 datasets. The results have been obtained by mapping the embedding from the central fragment of each Major TOM grid cell to low-dimensional space of 3. The output vector of principal components is then mapped to the range of 8-bit unsigned integers for display, and the mean intensity is scaled to be the same for all three colors. (a) SigLIP-SO400M (b) DINOv2 (c) SSL4EO-S2 (d) SSL4EO-S1 Figure 4: Principal component analysis with 3 components mapped to RGB channels (larger format of the same images is available in Appendix While this is only an early step into the analysis process conducted immediately after producing the database, it can already shed some light into representation spaces encoded by individual models. Matching colors of two locations in the map indicate that the source images from those places possess aligned characteristic according to the embedding model. 10 Global and Dense Embeddings of Earth It is quite apparent that the embeddings produced by the general-purpose vision models (SigLIP and DINOv2) shown in Figure 4(a) and (b) appear to encode large areas into similar neighborhood of the latent space, exhibited by shared colour offsets for large regions like norther Africa, the tropical belt, or the Alps. The embeddings from the SSL4EO models (Figure 4(c) and (d)) appear to be more conditioned on the local features, with less apparent global structure. These differences are only an introduction to more detailed analysis, which is to be conducted in the upcoming work."
        },
        {
            "title": "6 Software Release",
            "content": "There is already an existing codebase supporting the Major TOM project (Francis and Czerkawski, 2024). Since the embedding of Major TOM cells is now introduced as an expansion to the standard, the tools for generating and processing Major TOM embeddings are incorporated into the same package within the src/embedder subdirectory. On the front end, it contains jupyter notebooks that allow the user to go through the complete process of transforming Major TOM Core dataset into an embedding expansion. Furthermore, additional tools are provided for interacting with the embeddings, including visualization and operations on the vector database, with examples contained in other jupyter notebooks. 6.1 Embedding Generation The embedding generation notebook covers the complete process of transforming each original data parquet to parquet of embeddings, which involves the extraction of the relevant data bands for the embedding model, fragmentation function, preprocessing, model application, and finally, aggregation into output parquet data frames. Furthermore, in the upcoming release, additional tools for parallelizing the process are going to be released to improve the speed of the process for large-scale settings. 6.2 Embedding Interface Once the embedding dataset has been built, there are many ways to interact with the vector database. The embedder subpackage of Major TOM contains examples of performing PCA at global scale, performing similarity search, and optimising the memory required for those operations (for example, by reducing the precision of the number format). 6.3 Embedding Evaluation This document describes the project at the time of the first release of the large-scale embedding data produced in the first stage of the undertaking. It is foreseen that with this ready access to millions of embeddings from several models, it may become possible to evaluate the capability of the learned representations, including downstream tasks. The software release is going to soon be expanded by set of scripts for fast evaluation of the embedding models to make the process fast, reproducible, and expandable by the wider community. Major TOM Floating in the Latent Space"
        },
        {
            "title": "7 Potential Use Cases",
            "content": "Precomputed embedding datasets can serve concrete downstream use cases, such as land use monitoring, by learning mapping from given embedding space to label space corresponding to given task. This reduces the need for expensive deep learning model inference, and can instead rely solely on computationally inexpensive mappings such as linear transformations. In the more general sense, ready and fast access to embeddings from several models allows to compare them next to each other across number of tasks, regions, and times. By providing precomputed embeddings on benchmark datasets such as Major TOM, generalpurpose models for Earth observation data become more reproducible, transparent, and accessible, since analysis can be conducted by wider community of developers and users. The availability of large vector databases describing Earth observation data can provide foundation for developing improved mechanisms for interacting with the embeddings, via operations such as similarity search or compression. The operations at the scale of millions of vectors become computationally demanding, and further research is required to scale these methods up to the level of Earth observation archives, which could easily contain trillions of vectors."
        },
        {
            "title": "References",
            "content": "Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. Satlaspretrain: large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1677216782, October 2023. Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David B. Lobell, and Stefano Ermon. SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=WBhqzpF6KYH. Mikolaj Czerkawski and Alistair Francis. From laion-5b to laion-eo: Filtering billions of images using anchor datasets for satellite image extraction. In \"Towards the Next Generation of Computer Vision Datasets: DataComp Track\" Workshop at the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. Alistair Francis and Mikolaj Czerkawski. Major tom: Expandable datasets for earth observation. In IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium, pages 29352940, 2024. doi: 10.1109/IGARSS53475.2024.10640760. Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: deep visual-semantic embedding model. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/ 7cce53cf90577442771720a370c3c723-Paper.pdf. Global and Dense Embeddings of Earth Albert Gordo, Jon Almazán, Jerome Revaud, and Diane Larlus. Deep image retrieval: Learning global representations for image search. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision ECCV 2016, pages 241257, Cham, 2016. Springer International Publishing. ISBN 978-3-319-46466-4. Johannes Jakubik, Sujit Roy, Christopher Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniel Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, Daiki Kimura, Naomi Simumba, Linsong Chu, S. Karthik Mukkavilli, Devyani Lambhate, Kamal Das, Ranjini Bangalore, Dario Oliveira, Michal Muszynski, Kumar Ankur, Muthukumaran Ramasubramanian, Iksha Gurung, Sam Khallaghi, Hanxi Li, Michael Cecil, Maryam Ahmadi, Fatemeh Kordi, Hamed Alemohammad, Manil Maskey, Raghu Kiran Ganti, Kommy Weldemariam, and Rahul Ramachandran. Foundation models for generalist geospatial artificial intelligence. ArXiv, abs/2310.18660, 2023. URL https: //api.semanticscholar.org/CorpusID:264590307. Konstantin Klemmer, Esther Rolf, Caleb Robinson, Lester Mackey, and Marc Rußwurm. Towards global, general-purpose pretrained geographic location encoders. In NeurIPS 2023 Workshop on Tackling Climate Change with Machine Learning, 2023. URL https: //www.climatechange.ai/papers/neurips2023/114. Oscar Manas, Alexandre Lacoste, Xavier Giro-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 94149423, October 2021. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013. URL https://arxiv.org/abs/1301.3781. Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, and Nico Lang. Mmearth: Exploring multi-modal pretext tasks for geospatial representation learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2023. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 18, 2007. doi: 10.1109/CVPR.2007. 383172. Filip Radenović, Hervé Jégou, and Ondrej Chum. Multiple measurements and joint dimensionality reduction for large scale image search with short vectors. In Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, ICMR 15, page 587590, Major TOM Floating in the Latent Space New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450332743. doi: 10.1145/2671188.2749366. URL https://doi.org/10.1145/2671188.2749366. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Colorado Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scalemae: scale-aware masked autoencoder for multiscale geospatial representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 40884099, October 2023. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for In Thirty-sixth Conference on Neural Intraining next generation image-text models. formation Processing Systems Datasets and Benchmarks Track, 2022. URL https: //openreview.net/forum?id=M3Y74vmsMcY. Xiaohui Shen, Zhe Lin, Jonathan Brandt, and Ying Wu. Spatially-constrained similarity measurefor large-scale object retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(6):12291241, 2014. doi: 10.1109/TPAMI.2013.237. Adam J. Stewart, Caleb Robinson, Isaac A. Corley, Anthony Ortiz, Juan M. Lavista Ferres, and Arindam Banerjee. TorchGeo: Deep learning with geospatial data. In Proceedings of the 30th International Conference on Advances in Geographic Information Systems, SIGSPATIAL 22, pages 112, Seattle, Washington, November 2022. Association for Computing Machinery. doi: 10.1145/3557915.3560953. URL https://dl.acm.org/doi/10. 1145/3557915.3560953. Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Þorsteinn Elí Gíslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, Srija Chakraborty, Sizhe Wang, Ankur Kumar, Myscon Truong, Denys Godwin, Hyunho Lee, Chia-Yu Hsu, Ata Akbari Asanjan, Besart Mujeci, Trevor Keenan, Paulo Arevalo, Wenwen Li, Hamed Alemohammad, Pontus Olofsson, Christopher Hain, Robert Kennedy, Bianca Zadrozny, Gabriele Cavallaro, Campbell Watson, Manil Maskey, Rahul Ramachandran, and Juan Bernabe Moreno. Prithvi-eo-2.0: versatile multi-temporal foundation model for earth observation applications, 2024. URL https://arxiv.org/abs/2412.02732. Michail Tarasiou and Stefanos Zafeiriou. Embedding earth: Self-supervised contrastive pretraining for dense land cover classification, 2022. URL https://arxiv.org/abs/2203. 06041. 14 Global and Dense Embeddings of Earth Giorgos Tolias and Hervé Jégou. Visual query expansion with or without geometry: Refining local descriptors by feature aggregation. Pattern Recognition, 47(10):34663476, 2014. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2014.04.007. URL https://www. sciencedirect.com/science/article/pii/S0031320314001381. Gabriel Tseng, Ruben Cartuyvels, Ivan Zvonkov, Mirali Purohit, David Rolnick, and Hannah Kerner. Lightweight, pre-trained transformers for remote sensing timeseries, 2023. Yi Wang, Conrad Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, and Xiao Xiang Zhu. Decoupling common and unique representations for multimodal self-supervised learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2023a. Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M. Albrecht, and Xiao Xiang Zhu. Ssl4eo-s12: large-scale multimodal, multitemporal dataset for selfsupervised learning in earth observation [software and data sets]. IEEE Geoscience and Remote Sensing Magazine, 11(3):98106, 2023b. doi: 10.1109/MGRS.2023.3281651. Zhitong Xiong, Yi Wang, Fahong Zhang, Adam Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, and Xiao Xiang Zhu. Neural plasticity-inspired foundation model for observing the Earth crossing modalities. arXiv preprint arXiv:2403.15356, 2024. Yifang Yin, Zhenguang Liu, Ying Zhang, Sheng Wang, Rajiv Ratn Shah, and Roger Zimmermann. Gps2vec: Towards generating worldwide gps embeddings. In Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 416-419, 2019. Yifang Yin, Ying Zhang, Zhenguang Liu, Yuxuan Liang, Sheng Wang, Rajiv Ratn Shah, and Roger Zimmermann. Learning multi-context aware location representations from large-scale geotagged images. In Proceedings of the 29th ACM International Conference on Multimedia, pages 899-907, 2021. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1194111952, 2023. doi: 10.1109/ICCV51070.2023.01100. 15 Major TOM Floating in the Latent Space Appendix A. Computational Budget The computational tasks in this study were carried out on the CloudFerro CREODIAS public cloud computing platform, utilizing OpenStack cloud technology. The hardware configuration included two GPUs (NVIDIA L40S with 48GB of VRAM), SSD disks were used for fast reading and writing, 64 VCPUs, and 240GB of RAM, running on Ubuntu 22.04. The environment was built on Python 3.9, with the deep learning framework Torch 2.4.1, utilizing GPU acceleration through CUDA 12.2. For image processing, OpenCV was employed for resampling operations, and multithreading libraries ensured optimized performance across multiple CPU cores. The TorchGeo library was used for interfacing with the pre-trained SSL4EO models. Major TOM Core datasets were downloaded directly from HuggingFace in the parquet format. The embedding scripts were set up via parallelized processes. Appendix B. Additional Images Figure 5: Principal component analysis with 3 components mapped to RGB channels for SigLIP-SO400M 16 Global and Dense Embeddings of Earth Figure 6: Principal component analysis with 3 components mapped to RGB channels for DINOv2 Figure 7: Principal component analysis with 3 components mapped to RGB channels for SSL4EO-S2 17 Major TOM Floating in the Latent Space Figure 8: Principal component analysis with 3 components mapped to RGB channels for SSL4EO-S"
        }
    ],
    "affiliations": [
        "CloudFerro, Warsaw, Poland",
        "Φ-lab, European Space Agency, Frascati, Italy"
    ]
}