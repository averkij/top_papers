{
    "paper_title": "Show-o2: Improved Native Unified Multimodal Models",
    "authors": [
        "Jinheng Xie",
        "Zhenheng Yang",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents improved native unified multimodal models, \\emph{i.e.,} Show-o2, that leverage autoregressive modeling and flow matching. Built upon a 3D causal variational autoencoder space, unified visual representations are constructed through a dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on a language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. A two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling a wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 4 6 5 5 1 . 6 0 5 2 : r Show-o2: Improved Native Unified Multimodal Models Jinheng Xie1 Zhenheng Yang2 Mike Zheng Shou1 1 Show Lab, National University of Singapore 2 ByteDance"
        },
        {
            "title": "Abstract",
            "content": "This paper presents improved native unified multimodal models, i.e., Show-o2, that leverage autoregressive modeling and flow matching. Built upon 3D causal variational autoencoder space, unified visual representations are constructed through dual-path of spatial (-temporal) fusion, enabling scalability across image and video modalities while ensuring effective multimodal understanding and generation. Based on language model, autoregressive modeling and flow matching are natively applied to the language head and flow head, respectively, to facilitate text token prediction and image/video generation. two-stage training recipe is designed to effectively learn and scale to larger models. The resulting Show-o2 models demonstrate versatility in handling wide range of multimodal understanding and generation tasks across diverse modalities, including text, images, and videos. Code and models are released at https://github.com/showlab/Show-o."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) [92, 111] have achieved unprecedented performance levels, fueled by extensive web-scale text resources, substantial computational power, and billions of parameters. In the multimodal domain, large multimodal models (LMMs) [6, 27, 51] and visual generative models [34, 78, 106], have also demonstrated exceptional capabilities in tasks such as generalpurpose visual question answering and text-to-image/video generation. Given their success, unified multimodal models (UMMs) [89, 103, 108] have been investigated to unify multimodal understanding and generation within single model or system. In addition to multimodal understanding capability, this line of approaches seeks to simultaneously cultivate multimodal understanding and generation abilities in the model/system through pre-training, fine-tuning, or connecting tailored models. Here, we provide comparative analysis of selected UMMs in Table 1, focusing on two perspectives, including i) visual representations for understanding and generation and ii) the type of unified modeling. Generally, there are two approaches to incorporating visual representations for multimodal understanding and generation: i) unified representation for both understanding and generation, as seen in works like Chameleon [89], Transfusion [121], and Show-o [108]; and ii) decoupled representations, utilizing CLIP [80] for multimodal understanding and variational autoencoder (VAE) for visual generation. To involve both multimodal understanding and generation capabilities, two primary methods have been explored: i) natively applying multimodal understanding and generation objectives within single model and ii) tuning adapters to assemble tailored models. We refer the first type as native unified multimodal models, distinguishing it from the second type that assembles tailored models. These principles, combined with autoregressive or diffusion modeling or both, contribute to the development of unified multimodal models. Compared to existing UMMs that primarily focus on text and image, our approach explores model designs that provide substantial potential and scalability in natively unifying text, image, and video modalities. An overview of our approach is presented in Fig. 1. Specifically, for visual inputs, we Corresponding Author Technical Report Table 1: Comparative analysis of selected unified multimodal models based on the type of visual representations and unified modeling for multimodal understanding and generation. In this context, native und. & gen. refers to the direct decoding of output sequences into texts, images, and videos, as opposed to serving as conditions for decoding using external pre-trained decoders like Stable Diffusion. indicates the method adopts two distinct models for multimodal understanding and generation, respectively. Diff. means the diffusion modeling. Please refer to the complete table in the appendix. Methods Chameleon [89] Transfusion [121] Show-o [108] VILA-U [105] Emu3 [98] Show-o2 (Ours) Janus-Series [24, 25, 71] UnidFluid [35] Mogao [60] Bagel [29] NExT-GPT [103] SEED-X [37] ILLUME [96] MetaMorph [91] TokenFlow [79] LlamaFusion [84] Und. & Gen. Representation Type of Unified Modeling Unified Decoupled Support Video Native Und. & Gen. Assembling Tailored Models Paradigm AR AR + Diff. AR + Diff. AR AR AR + Diff. AR (+Diff) AR + MAR AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR AR + Diff. operate within the 3D causal VAE [93] space, which is capable of accommodating both images and videos. Recognizing the distinct feature dependencies between multimodal understanding and generation, we construct unified visual representations that simultaneously capture rich semantic information and low-level features with intrinsic structures and textual details from the visual latents. This is achieved through dual-path mechanism consisting of semantic layers, projector, and spatial (-temporal) fusion process. As the fusion process occurs within the 3D causal VAE space, when it comes to videos, semantic and low-level features are temporally aligned and fused with full-frame video information. Text embeddings and unified visual representations are structured into sequence to go through pre-trained language model and are modeled by specific language head and flow head, respectively. Specifically, autoregressive modeling with causal attention is performed on the language head when dealing with text token prediction, and flow matching with full attention is applied to the flow head for image/video generation. Since the base language model lacks visual generation capabilities, we propose two-stage training recipe to effectively learn such an ability while retaining the language knowledge, without requiring massive text corpus. In the first stage, we mainly focus on pre-training the flow head for visual generation using (interleaved) text, image, and video data. In the second stage, the full model is fine-tuned with high-quality multimodal understanding and generation data. Extensive experimental results have demonstrated that our model surpasses the existing methods in terms of most metrics across multimodal understanding and visual generation benchmarks. Collectively, the main contributions of this paper can be summarized as: We present an improved native unified multimodal model that seamlessly integrates autoregressive modeling and flow matching, enabling wide range of multimodal understanding and generation across (interleaved) text, images, and videos. Based on the 3D causal VAE space, we construct unified visual representations scalable to both multimodal understanding and generation, image and video modalities by combining semantic and low-level features through dual-path of spatial (-temporal) fusion mechanism. We design two-stage training pipeline that effectively and efficiently learns unified multimodal models, retaining language knowledge and enabling effective scaling up to larger models, without requiring massive text corpus. The proposed model demonstrates state-of-the-art performance on multimodal understanding and visual generation benchmarks, surpassing existing methods across various metrics."
        },
        {
            "title": "2.1 Large Multimodal Models",
            "content": "Building upon the advancements of large language models (LLMs) [92, 111], large multimodal models (LMMs) [6, 27, 51, 64] have showcased remarkable capabilities in general-purpose visual question answering. These approaches typically leverage pre-trained vision encoders to project visual features and align them within the embedding space of LLMs. Meanwhile, growing number of encoder-free LMMs [31, 32, 108] aim to directly align raw visual features within the LLM embedding space. However, these encoder-free methods often fall behind models that utilize image-text-aligned visual features in terms of performance. Beyond model architecture, recent studies [18, 51, 90] have highlighted the critical role of high-quality instructional data in enhancing multimodal capabilities."
        },
        {
            "title": "2.2 Visual Generative Models",
            "content": "Two prominent paradigms for visual generation, namely diffusion [8, 17, 62, 77, 78, 82, 83, 102, 106, 107, 117] and autoregressive modeling [20, 48, 54, 75, 86], have been extensively studied in image and video generation in recent years. Diffusion-based methods typically employ optimized architectures that integrate pre-trained text encoders with denoising networks. In contrast, autoregressive methods often utilize LLM-based architectures and are trained through next-token prediction. Recently, several studies [35,56,65] have explored hybrid approaches that combine diffusion and autoregressive modeling to further advance visual generation capabilities."
        },
        {
            "title": "2.3 Unified Multimodal Models",
            "content": "Building on the success of large multimodal and visual generative models, pioneering unified multimodal models (UMMs) such as Chameleon [89], Show-o [108], and Transfusion [121] aim to integrate these capabilities into single model through autoregressive or diffusion modeling or both. Further advancements [26, 46, 69, 85, 98, 105] have focused on optimizing the training pipeline and enhancing the semantics of discrete tokens, leading to improved performance. We refer to these approaches as native unified multimodal models, as they inherently combine multimodal understanding and generation objectives within unified architecture. An alternative and promising direction [16,33,37,68,74,88,91] for unifying multimodal understanding and generation involves assembling off-the-shelf specialized LMMs and visual generative models by tuning adapters or learnable tokens. Representative works [16, 37, 74, 103] have demonstrated the promissing capabilities and intriguing properties of such assembled unified frameworks, highlighting their potential for further exploration."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce the overall framework (Section 3.1), which consists of two key components: i) the design of unified visual representations for multimodal understanding and generation, applicable to both images and videos, and ii) the native learning of multimodal understanding and generation capabilities. Subsequently, we present two-stage training recipe (Section 3.2), which is designed to progressively learn and effectively scale up the unified multimodal model."
        },
        {
            "title": "3.1 Overall Framework",
            "content": "Overall Architecture. An overview of our proposed unified model is depicted in Fig. 1. Given (interleaved) texts, images, or videos, text tokenizer with an embedding layer and 3D causal VAE encoder accordingly process them into continuous text embeddings and visual latent representations. Subsequently, the visual latent representations undergo dual-path extraction of spatial (-temporal) fusion to create the unified visual representations. These representations are then structured into sequence, which is fed into language model equipped with language and flow heads to model the sequence via autoregressive modeling and flow matching accordingly. Finally, text de-tokenizer in conjunction with 3D causal VAE decoder is employed to decode the final output. Next, we will delve into the fundamental design principles behind the unified visual representation and flow head. 3 Figure 1: Our approach begins by encoding input texts, images, and videos into continuous embeddings and visual latents. The visual latents are processed through dual-path extraction and spatial (-temporal) fusion mechanism to construct unified visual representations that are scalable for both multimodal understanding and generation, image and video modalities. These text embeddings and unified visual representations are then structured into sequence for the base language model, equipped with dedicated heads. Specifically, text tokens are modeled autoregressively by language head, while image and video latents are handled by flow head using flow matching. We employ the omni-attention mechanism [108,121] to enable causal attention along the sequence while maintaining full attention within the unified visual representations. This design empowers our model to effectively tackle tasks such as image/video understanding, generation, and mixed-modality generation. Unified Visual Representation. To scalably support image and video modalities, we employ 3D causal VAE encoder to extract image/video latents. As multimodal understanding and generation differ in feature dependency, we propose dual-path architecture comprising semantic layers S() to extract high-level representations of rich semantic contextual information and projector P() to retain complete low-level information from the extracted visual latents. Specifically, semantic layers S() share the same vision transformer blocks of SigLIP [115] with new 2 2 patch embedding layer. Given visual latents xt = {xi}n i=1 at noise level: xt = x1 + (1 t) x0, (1) where x0 (0, 1) and [0, 1], we load the pre-trained weights of SigLIP and pre-distill S() as follows: Ldistill = log sim(S(xt), SigLIP(X)), (2) (cid:88) where is the input image, SigLIP() extracts the image patch features, and sim() indicates the cosine similarity calculator. In this way, semantic layers S() can mimic extracting semantic features from both clean and noised visual latents xt. The projector P() is simply composed of 2D patch embedding layer. The extracted highand low-level representations are spatially (and temporally when it comes to videos) fused by concatenating through the feature dimension and applying RMSNorm [116] with two MLP layers to get the unified visual representations u: = STF(S(xt), P(xt)), (3) where STF indicates the spatial (-temporal) fusion mechanism. In addition, we prepend time step embedding to the unified visual representations for generative modeling. is set as 1.0 to get time step embedding for the clean image. 4 We structure the text embeddings and unified visual representations into sequence following general interleaved image-text format below: [BOS] {Text} [BOI / BOV] {Image / Video} [EOI / EOV] {Text} [EOS]. The sequence format above is flexible and can be adapted to various input types. We adopt the omni-attention mechanism [108, 121] to let the sequence modeling be causal but with full attention within the unified visual representations. Flow Head. Apart from the language head for text token prediction, we employ flow head to predict the defined velocity vt = dxt dt via flow matching [62, 66]. Specifically, the flow head simply consists of several transformer layers with time step modulation via the adaLN-Zero blocks, as seen in DiT [76]. During training, we natively apply next token prediction LNTP to the language head and flow matching LFM to the flow head for predicting velocity, respectively: = αLNTP + LFM. (4)"
        },
        {
            "title": "3.2 Training Recipe",
            "content": "Stage-1 Datasets [89], # Image-Text Trainable Components Projector Spatial (-Temporal) Fusion Flow Head Table 2: Trainable components and datasets in the training stages. Existing UMMs, such as Show-o [108], JanusPro [24], Transfusion [121], Chameleon and Emu3 [98], are typically trained from LLMs, LMMs, or from scratch. These approaches aim to cultivate visual generative modeling capabilities while preserving language modeling proficiency. However, this process often relies on web-scale, high-quality text corpora, which are prohibitively expensive to collect. Consequently, the lack of such resources can lead to degradation in language knowledge and modeling performance. To address this challenge, we adopt two-stage training recipe (as shown in Table 2) that effectively retains language knowledge while simultaneously developing visual generation capabilities, without requiring massive text corpus. OpenVid-1M [72] 1.5M Internal Data 9M HQ Und. 16M HQ Gen. VIST [43] CoMM [22] WebVid [7] Pandas [21] Full Model (w/o VAE) # Interleaved Data OmniCorpus [55] # Video-Text Stage-2 66M Stage-1. Before the two-stage training, we have pre-distilled the semantic layers S() (implementation details can be found in Section 4). The first stage only involves trainable components of the projector, spatial (-temporal) fusion, and flow head. In this stage, we train these components using autoregressive modeling and flow matching using around 66M image-text pairs and progressively add interleaved data and video-text pairs. Stage-2. Subsequently, we tune the full model using 9M high-quality multimodal understanding instruction data and 16M high-quality visual generation data filtered from the 66M image-text pairs. Scaling Up. After the training of the small-sized model with approximately 1.5B LLM parameters, we resume the pre-trained flow head for the larger model with 7B LLM parameters and introduce lightweight MLP transformation to align the hidden size, allowing it to quickly adapt to the larger model and converge."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. The curated approximately 66M image-text pairs consist of images with resolution of at least 512 pixels in width and height. The images are filtered from CC12M [12], COYO [11], LAION-Aesthetic-12M and AI synthetic data. The images are recaptioned by ShareGPT4-V [18] except for the synthetic data. The 9M high-quality multimodal understanding instruction data is curated from Densefusion-1M [57], and LLaVA-OneVision [51]. Implementation Details. The semantic layers S() are pre-distilled from SigLIP-so400m-patch14384 over 200K iterations, using batch size of 512 and cosine-scheduled learning rate of 2e-5. https://huggingface.co/datasets/dclure/laion-aesthetics-12m-umap https://huggingface.co/google/siglip-so400m-patch14-384 5 Table 3: Evaluation on multimodal understanding benchmarks. # Params. indicates the number of parameters of base LLM. * indicates the method uses two distinct models or sets of parameters for multimodal understanding and generation, respectively. Und. indicates understanding. Types Models # Params. Und. Only Unify via Assembling Tailored Models Native Unified LLaVA-v1.5 [63] Qwen-VL-Chat [5] NExT-GPT [108] SEED-X [37] MetaMorph [91] TokenFlow-XL [79] ILLUME [96] Show-o [108] JanusFlow [71] SynerGen-VL [53] Janus-Pro [24] Show-o2 (Ours) Emu3 [98] VILA-U [105] MUSE-VL [109] Liquid [101] Janus-Pro [24] Mogao [60] Show-o2 (Ours) 7B 7B 13B 17B 8B 14B 7B 1.3B 1.5B 2.4B 1.5B 1.5B 8B 7B 7B 8B 7B 7B 7B MME GQA (p) 1510.7 1487.6 - 1457.0 - 1551.1 1445.3 1097.2 1333.1 1381.0 1444.0 1450.9 - 1401.8 - 1448.0 1567.1 1592.0 1620.5 62.0 57.5 - 49.1 - 62.5 - 58.0 60.3 - 59.3 60.0 60.3 60.8 - 61.1 62.0 60.9 63.1 SEED MMB MMMU MMStar AI2D (val) (all) (en) 58.6 58.2 57.5 66.5 71.8 72.6 72. 51.5 70.5 - 68.3 65.6 68.2 59.0 69.1 - 72.1 74.6 69.8 64.3 60.6 58.0 70.1 75.2 76.8 75.1 - 74.9 53.7 75.5 67.4 58.5 - 72.1 - 79.2 75.0 79.3 - - - 35.6 - 43.2 38. 27.4 29.3 34.2 36.3 37.1 31.6 - 39.7 - 41.0 44.2 48.9 - - - - - - - - - - - 43.4 - - 49.6 - - - 56.6 - 57.7 - - - 75.9 71. - - - - 69.0 70.0 - 69.8 - - - 78.6 During distillation, Eq. 1 is applied to the visual latents with only probability of 0.3 in the last 20K iterations. The input image resolution of 3D causal VAE encoder with 2 2 patch embedding layer is set as 432 432 to get 729 = 27 27 visual latents, which matches the ones extracted by SigLIP. Once distilled, the semantic layers S() are capable of extracting rich semantic features from both clean and noised visual latents. In statistics, the extracted features from clean visual latents by S() have converged to an average cosine similarity of around 0.9 with those extracted by the original SigLIP on the curated 66M image-text pairs. We interpolate the position embeddings in the bicubic mode when involving other image/video resolutions. Our models build upon two LLM variants, i.e., Qwen2.5-1.5B-Instruct [111] and Qwen2.5-7BInstruct [111], respectively. We adopt 3D causal VAE proposed in Wan2.1 [93] with 8 and 4 spatial and temporal compression, respectively. In stage 1, we first train the 1.5B variant for 150K iterations using AdamW optimizer with constant learning rate of 0.0001 on the curated 66M imagetext pairs in resolution of 432 432. The context length of single image-text pairs is set as 1024. The total batch sizes for multimodal understanding and generation are 128 and 384, respectively. α in Eq. 4 is set as 0.2. For visual generation data, the caption is dropped with probability of 0.1 to enable the classifier-free guidance. This training process roughly takes one and half days using 64 H100 GPUs. Subsequently, we replace the generation data with 16M high-quality data (filtered from 66M image-text pairs) and continue to train for 40K iterations. In stage 2, we follow the training strategies in LLaVA-OneVision [51] to train the 1.5B model using around 9M multimodal instructional and 16M high-quality generation data for total of around 35K iterations. α in Eq. 4 is set as 1.0. The stage 2 training process takes around 15 hours. For models with mixed-modality and video generation capabilities, we progressively add video-text and interleaved data in stage 1. For video data, we randomly sample 2s 480p or 432432 clips with 17 frames from each video with an interval of 3 frames. The context length at this time is set as 7006. In stage 2, high-quality video-text and interleaved data are added to further improve video and mixed-modality generation capabilities. To futher improve the image generation and text rendering quality, we further train the small-scale model on images with higher resoluton (512 512 and 1024 1024) and involve an additional text-rich image data, i.e., subset of TextAtlas [94]. In the training of our model based on the 7B LLM variant, we resume the flow head pre-trained based on the 1.5B model and additionally train the newly initialized spatial (-temporal) fusion, projector, and MLP transformations for 3K iterations with 2K warm-up steps to align the hidden size and then further train spatial (-temporal) fusion, the projector, MLP transformations, and the flow head together. Following that, we conduct the training stages 1 and 2 in the same manner as those of the 1.5B model. The whole training process of our 7B model takes approximately 2 and half days using 128 H100 GPUs. We do not include interleaved and video data in the training stages of the larger model due to the huge computational cost and training duration. Table 4: Evaluation on the GenEval [38] benchmark. Gen. denotes generation. # Params. indicates the number of parameters of base LLM. # Data. indicates the number of image-text pairs used for visual generation during training. * indicates the method uses two distinct models or sets of parameters for multimodal understanding and generation, respectively. Obj.: Object. Attri.: Attribute. Our results are obtained based on the rewritten dense prompts. # Params. Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Method # Data Type Gen. Only SD3 (d=24) [34] SD3-Medium [34] Unifying via Tailored Models SEED-X [37] TokenFlow-XL [79] ILLUME [96] Native Unified Show-o [108] Emu3 [98] MUSE-VL [109] Transfusion [121] D-DiT [58] Janus-Pro [24] Mogao [60] Show-o2 (Ours) Show-o2 (Ours) - 17B 14B 7B 1.3B 8B 7B 7B 2B 7B 7B 1.5B 7B - - 158M 60M 15M 2.0B - 24M 3.5B 40M 144M - 66M 66M 0.98 0.99 0.97 0.95 0.99 0.98 - - 0.97 0.99 1.00 0.99 1.00 0.74 0.94 0.58 0.60 0. 0.80 - - 0.80 0.89 0.97 0.86 0.87 0.63 0.72 0.26 0.41 0.45 0.66 - - 0.54 0.59 0.83 0.55 0. 0.67 0.89 0.80 0.81 0.71 0.84 - - 0.76 0.90 0.93 0.86 0.92 0.34 0.33 0.19 0.16 0. 0.31 - - 0.32 0.79 0.84 0.46 0.52 0.36 0.60 0.14 0.24 0.28 0.50 - - 0.50 0.66 0.80 0.63 0. 0.62 0.74 0.49 0.55 0.61 0.68 0.66 0.57 0.63 0.65 0.80 0.89 0.73 0.76 Table 5: Evaluation on the DPG-Bench [41] benchmark. Gen. denotes generation. # Params. indicates the number of parameters of base LLM. # Data. indicates the number of image-text pairs used for visual generation during training. Type Method # Params. # Data Global Entity Attribute Relation Gen. Only Native Unified Hunyuan-DiT [59] Playground v2.5 [52] PixArt-Σ [15] DALL-E 3 [9] SD3-Medium [34] Emu3-DPO [98] Janus-Pro [24] Mogao [60] Show-o2 (Ours) Show-o2 (Ours) 1.5B - - - 2B 8B 7B 7B 1.5B 7B - - - - - - 144M - 66M 66M 84.59 83.06 86.89 90.97 87.90 - 86.90 82.37 87.53 89.00 80.59 82.59 82.89 89.61 91.01 - 88.90 90.03 90.38 91. 88.01 81.20 88.94 88.39 88.83 89.40 88.26 91.34 89.96 74.36 84.08 86.59 90.58 80.70 - 89.32 93.18 90.30 91.81 Other 86.41 83.50 87.68 89.83 88. - 89.48 85.40 91.21 91.64 Overall 78.87 75.47 80.54 83.50 84.08 81.60 84.19 84.33 85.02 86."
        },
        {
            "title": "4.2 Multimodal Understanding",
            "content": "Quantitative Results. Table 3 highlights the performance of our models on multimodal understanding benchmarks, evaluated across metrics such as MME [36], GQA [45], SEED-Bench [50], MMBench [67], MMU [114], MMStar [19], and AI2D [47]. As shown in the table, both the 1.5B and 7B variants of our model consistently outperform state-of-the-art models across many metrics. For models with similar parameter sizes (1.5B), our model achieves the best scores on MME-p and MMU-val benchmarks while delivering competitive performance on GQA and SEED-Bench metrics. When compared to larger models with approximately 7B parameters, our models surpass state-of-the-art models such as Janus-Pro and even the significantly larger TokenFlow-XL model (14B parameters) in metrics including MME-p, GQA, MMMU-val, MMStar, and AI2D, while maintaining competitive performance on SEED-Bench and MM-Bench. These results underscore the robust perception capabilities of our unified visual representations, demonstrating their effectiveness in multimodal understanding tasks and the promising potentials in this domain. Qualitative Results. Fig. 2 showcases the multimodal understanding capabilities of our model. As demonstrated, the model excels at answering general-purpose questions about an image. Specifically, it can provide detailed descriptions of an image, count objects, and recognize text within the image. Besides, the model can leverage its world knowledge to offer step-by-step instructions for preparing daily drinks like an avocado milkshake and supports bilingual question-answering, highlighting its versatility and practical utility. Further, our model supports multimodal understanding in both English and Chinese, enabling bilingual capabilities."
        },
        {
            "title": "4.3 Visual Generation",
            "content": "Image Generation. We compare our model with the state-of-the-art approaches on GenEval [38] and DPG-Bench [41] benchmarks in Tables 4 and 5. One can observe that our model surpasses most approaches, including TokenFlow-XL, Show-o, Emu3, and Transfusion, on the GenEval benchmark. Compared to Janus-Pro, which was trained on significantly larger dataset of 144M image-text pairs, 7 Figure 2: Multimodal understanding and generation examples. 8 Models # Params. Total QS SS SC BC TF MS DD AQ IQ OC MO HA SR AS TS OC ModelScope [97] LaVie [99] OpenSoraPlan V1.3 [61] Show-1 [117] AnimateDiff-V2 [40] Gen-2 [1] Pika-1.0 [2] VideoCrafter-2.0 [14] CogVideoX [113] Kling [4] Step-Video-T2V [70] Gen-3 [3] Emu3 [98] VILA-U [105] Show-o2 1.7B 3B - 6B - - - - 5B - 30B - 8B 7B 2B 75.75 78.05 66.54 89.87 95.29 98.28 95.79 66.39 52.06 58.57 82.25 38.98 92.40 81.72 33.68 39.26 23.39 25.37 25.67 77.08 78.78 70.31 91.41 97.47 98.30 96.38 49.72 54.94 61.90 91.82 33.32 96.80 86.39 34.09 52.69 23.56 25.93 26.41 77.23 80.14 65.62 97.79 97.24 99.20 99.05 30.28 60.42 56.21 85.56 43.58 86.80 79.30 51.61 36.73 20.03 22.47 24.47 78.93 80.42 72.98 95.53 98.02 99.12 98.24 44.44 57.35 58.66 93.07 45.47 95.60 86.35 53.50 47.03 23.06 25.28 27.46 80.27 82.90 69.75 95.30 97.68 98.75 97.76 40.83 67.16 70.10 90.90 36.88 92.60 87.47 34.60 50.19 22.42 26.03 27.04 80.58 82.47 73.03 97.61 97.61 99.56 99.58 18.89 66.96 67.42 90.92 55.47 89.20 89.49 66.91 48.91 19.34 24.12 26.17 80.69 82.92 71.77 96.94 97.36 99.74 99.50 47.50 62.04 61.87 88.72 43.08 86.20 90.57 61.03 49.83 22.26 24.22 25.94 80.44 82.20 73.42 96.85 98.22 98.41 97.73 42.50 63.13 67.22 92.55 40.66 95.00 92.92 35.86 55.29 25.13 25.84 28.23 81.61 82.75 77.04 96.23 96.52 98.66 96.92 70.97 61.98 62.90 85.23 62.11 99.40 82.81 66.35 53.20 24.91 25.38 27.59 81.85 83.39 75.68 98.33 97.60 99.30 99.40 46.94 61.21 65.62 87.24 68.05 93.40 89.90 73.03 50.86 19.62 24.17 26.42 81.83 84.46 71.28 98.05 97.67 99.40 99.08 53.06 61.23 70.63 80.56 50.55 94.00 88.25 71.47 24.38 23.17 26.01 27.12 82.32 84.11 75.17 97.10 96.62 98.61 99.23 60.14 63.34 66.82 87.81 53.64 96.40 80.90 65.09 54.57 24.31 24.71 26.69 - - 80.96 - 74.01 76.26 65.04 - 81.34 82.10 78.31 97.28 96.78 97.68 98.25 40.83 65.15 67.06 94.81 76.01 95.20 80.89 62.61 57.67 23.29 25.27 27. 86.17 44.64 77.71 - 68.73 37.11 20.92 - 98.93 79.27 59.64 - 95.32 97.69 - - - - - - - - - - - - - - - - Table 6: Comparison with text-to-video models on the VBench [44] benchmark. # Params. indicates the number of total parameters for video generation including the base LLM and flow head. QS: Quality Score, SS: Semantic Score, SC: Subject Consistency, BC: Background Consistency, TF: Temporal Flickering, MS: Motion Smoothness, DD: Dynamic Degree, AQ: Aesthetic Quality, IQ: Imaging Quality, OC: Object Class, MO: Multiple Objects, HA: Human Action, C: Color, SR: Spatial Relationship, S: Scene, AS: Appearance style, TS: Temporal Style, OC: Overall Consistency. Models I2V Subject I2V Background Camera Motion Subject Consistency Background Consistency Temporal Flickering Motion Smoothness Dynamic Degree Aesthetic Quality Imaging Quality DynamiCrafter-1024 [110] 96.71 94.85 SEINE-512x320 [23] 96.74 I2VGen-XL [119] 98.54 Animate-Anything [28] 94.69 ConsistI2V [81] 90.97 VideoCrafter-I2V [13] 97.51 SVD-XT-1.1 [10] 98.78 MarDini [65] Show-o 96.94 96.05 94.02 95.44 96.88 94.57 90.51 97.62 96.46 98.83 35.44 23.36 13.32 12.56 33.60 33.58 - - 28.41 95.69 94.20 96.36 98.90 95.27 97.86 95.42 - 93.83 97.38 97.26 97.93 98.19 98.28 98.79 96.77 - 97.45 97.63 96.72 98.48 98.14 97.56 98.19 99.17 - - 97.38 96.68 98.31 98.61 97.38 98.00 98.12 - 97.76 47.40 34.31 24.96 2.68 18.62 22.60 43.17 - 25.85 66.46 58.42 65.33 67.12 59.00 60.78 60.23 - 61.92 69.34 70.97 69.85 72.09 66.92 71.68 70.23 - 69.87 Table 7: Comparison with image-to-video models on the VBench [44] benchmark. our model achieves promising results with only 66M image-text pairs. On DPG-Bench evaluation, our model has demonstrated the best overall score compared to generation-only models such as SD3-Medium and unified models, including Emu3-DPO and Janus-Pro. We also show qualitative results in Fig. 2 to illustrate that our model can generate high-quality and realistic images. Video Generation. We compare our model with the text-to-video and image-to-video generation models in Tables 6 and 7. One can observe that with only 2B parameters, our model outperforms models such as Show-1, Emu3, and VILA-U with more than 6B parameters. Besides, our model has demonstrated competitive performance compared to CogVideoX and Step-Video-T2V. We also provide qualitative results of the text-to-video and image-to-video generation capability of our model in the middle of Fig. 2. One can observe that, given text prompts or an input image, our model can generate consistent video frames with reasonable motions, such as the smiling girl, lapping waves, and floating clouds."
        },
        {
            "title": "4.4 Mixed-Modality Generation",
            "content": "We demonstrate mixed-modality generation capabilities of our model using downstream task visual storytelling dataset [43] in Fig. 2. During fine-tuning, given an interleaved image-text sequence, we apply noise to all images in the sequence with probability of 0.3. Otherwise, we randomly retain number of the earlier images in the sequence and only apply noise to the later ones. Benefiting from the general interleaved sequence format mentioned in 3.1, our model can predict the [BOI] once it begins to generate an image. Upon detecting the [BOI] token, noises will be appended to the sequence to gradually generate an image. The generated text tokens and images will be served as context to continue generating the following output. Fig. 2 includes two examples demonstrating our models ability to interleavely generate coherent text and images, vividly narrating story."
        },
        {
            "title": "4.5 Ablation Studies",
            "content": "Table 8: Effect of spatial (-temporal) fusion. We show the pilot study results in Table 8, which validated the effect of spatial (-temporal) fusion on multimodal understanding and generation performance. For efficiency, we adopt LLaMA-3.2-1B as the base language model and use only around 1M multimodal understanding data and the ImageNet-1K generation data [30]. Under the same training settings, there are improvements in terms of both multimodal understanding and generation metrics, including MME-p, GQA, and FID-5K. This validates that the involved semantic and low-level features in the fusion mechanism would potentially help both the multimodal generation and understanding capabilities to some extent. w/o Fusion Fusion MMEp GQA 1164.7 1187.8 FID-5K 82.6 82.6 56.2 57.6 21.8 20.5 POPE We perform ablation studies to examine the effect of classifier-free guidance (CFG) and inference steps on the performance using the 1.5B model. As shown in Table 9, increasing the CFG guidance scale and inference steps (in range) would potentially improve the GenEval and DPG-Bench scores. However, the improvements of the GenEval score are not significant when the CFG guidance is set as larger than 5.0. Table 9: Effect of CFG guidance and inference steps. CFG guidance Inference steps GenEval DPG-Bench 2.5 5.0 7.5 10 7.5 7.5 50 50 50 50 25 0.65 0.71 0.71 0.71 0.71 0.73 81.6 83.9 84.8 85.0 84.6 84.7 Table 10 provides the effect of training stages on the generation performance on the GenEval and DPG-Bench benchmarks. One can observe that stage-2 training consistently and significantly improves both metrics, which validates the importance of the second stage."
        },
        {
            "title": "5 Limitations and Broader Impacts",
            "content": "Table 10: Effect of training stages. Stage-1 Stage-2 GenEval DPG-Bench 83.28 84.70 0.63 0.73 We found that our model is not good at rendering text on the image. We investigated our generation datasets and observed that the proportion of images with rendered texts is relatively small, which potentially leads to bad text rendering. In addition, the generated images will lack details of the small objects because of the limited image resolution. To address this limitation, as outlined in the implementation details, we have enhanced the model by training it on higher resolution data and incorporating image datasets rich in textual information. Our models possess the ability to generate text and images, which may carry the risk of unintended misuse, such as creating fake information or profiles. Additionally, our large-scale dataset includes content featuring celebrities and copyrighted materials, which could potentially result in intellectual property infringement."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper proposed native unified multimodal models, i.e., Show-o2, scalable for multimodal understanding and generation, image and video modalities, by integrating 3D causal VAE, autoregressive modeling, and flow matching. dual-path of spatial (-temporal) fusion mechanism guided the construction of unified visual representations with both highand low-level features. two-stage training recipe enables effective learning of unified capabilties, resulting in versatile model capable of handling diverse tasks, including multimodal understanding and image/video generation. Extensive experiments demonstrate the models state-of-the-art performance across various benchmarks."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We appreciate Haozhe Liu for his consistent input and discussions regarding this project. 10 Table 11: Comparative analysis of selected unified multimodal models based on the utilization of visual representations and type of unified modeling for multimodal understanding and generation. In this context, native und. & gen. refers to the direct decoding of output sequences into texts and images, as opposed to serving as conditions for decoding using external pre-trained decoders like Stable Diffusion. Please refer to the complete table in the appendix. indicates the method uses two distinct models for multimodal understanding and generation, respectively. Methods Chameleon [89] Show-o [108] Transfusion [121] VILA-U [105] Emu3 [98] MonoFormer [120] Dual-Diffusion [58] SynerGen-VL [53] MMAR [112] MUSE-VL [109] Orthus [49] Liquid [101] UGen [87] UniToken [46] Harmon [104] DualToken [85] UniTok [69] VARGPT [122] Selftok [95] Show-o2 (Ours) Janus-Series [24, 25, 71] UnidFluid [35] OmniMamba [123] Mogao [60] Bagel [29] NExT-GPT [103] SEED-X [37] MIO [100] MetaMorph [91] ILLUME [96] ILLUME+ [42] MetaQueries [74] Nexus-Gen [118] Ming-Lite-Uni [39] BLIP3-o [16] TokenFlow [79] LlamaFusion [84] SemHiTok [26] Und. & Gen. Representation Type of Unified Modeling Unified Decoupled Support Video Native Und. & Gen. Assembling Tailored Models Paradigm AR AR + Diff. AR + Diff. AR AR AR + Diff. Diff. AR AR + MAR AR AR + Diff. AR AR AR AR+MAR AR AR AR AR AR + Diff. AR (+Diff.) AR + MAR AR AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR + Diff. AR AR + Diff. AR"
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 More Qualitative Results Figure 3: Text-to-video and image-to-video generation examples. 12 Table 12: Evaluation on the WISE [73] benchmark. # Params. indicates the number of parameters of the base LLM. Type Method # Params. Cultural Time Space Biology Physics Chemistry Overall Native Unified Orthus-Instruct [49] Show-o [108] VILA-U [105] Janus-Pro [24] Janus-Pro [24] Show-o2 (Ours) Show-o2 (Ours) 7B 1.3B 7B 1.5B 7B 1.5B 7B 0.23 0.28 0.26 0.20 0.30 0.30 0. 0.31 0.36 0.33 0.28 0.37 0.38 0.42 0.38 0.40 0.37 0.45 0.49 0.47 0.53 0.28 0.23 0.35 0.24 0.36 0.32 0.39 0.31 0.33 0.39 0.32 0.42 0.42 0.45 0.20 0.22 0.23 0.16 0.26 0.28 0.26 0.27 0.30 0.31 0.26 0.35 0.35 0. A.2 Text Prompts We provide the text prompts for image generation used in Fig. 2 below: Hyper-detailed image of mature man with short, graying hair and deep blue eyes. He has rugged, weathered face with strong jawline and slight beard. His expression is thoughtful and introspective. The lighting is dramatic, highlighting the contours of his face. The photo is in 8K resolution, capturing every wrinkle and pore. soft, natural portrait photograph captures young woman with fair skin and long, ash-blonde hair cascading gently over her shoulders, her striking light blue eyes subtly enhanced with natural makeup and gentle, calm smile playing on her lips. She wears cozy, cream-colored winter sweater and delicate woolen scarf adorned with subtle snowflake patterns, positioned slightly off-center, creating sense of relaxed elegance. Behind her, softly blurred snowy Moscow street scene unfolds, with traditional architecture and the diffused, golden glow of winter afternoon contributing to serene and contemplative atmosphere. At the very bottom of the frame, in simple, elegant lettering, appears the phrase \"BE KIND\". vibrant, highly detailed close-up of colorful parrot perched on branch, featuring intricate feather textures, vivid colors (red, blue, green, yellow), and tropical rainforest background. The parrots eyes are sharp and expressive, with natural glint of light. The image is photorealistic, ultra HD (8K resolution), with soft natural lighting and shallow depth of field, creating blurred bokeh effect in the background. The scene is peaceful and lush, showcasing the beauty of nature. dark, moody room with glowing neon sign on the wall that spells out SHOW O2 in bold, vibrant pink and blue colors. The neon light reflects softly on the polished concrete floor, creating futuristic and artistic vibe. A.3 Additional Evaluation Results We provide evaluation results on the WISE [73] benchmark in Table 12."
        },
        {
            "title": "References",
            "content": "[1] Gen-2. Accessed September 25, 2023 [Online] https://research.runwayml.com/gen2, 2023. [2] Pika 1.0. Accessed December 28, 2023 [Online] https://www.pika.art/, 2023. 2024 [3] Gen-3. [Online] https://runwayml.com/research/ Accessed June 17, introducing-gen-3-alpha, 2024. [4] Kling. Accessed June 6, 2024 [Online] https://klingai.kuaishou.com/, 2024. [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. [8] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. [9] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng Wang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and Aditya Ramesh. Improving image generation with better captions. [10] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [11] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. [12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pages 35583568, 2021. [13] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [14] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. [15] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. [16] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [17] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR. OpenReview.net, 2024. [18] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [19] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [20] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, pages 16911703, 2020. [21] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [22] Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, and Long Chen. Comm: coherent interleaved image-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2406.10462, 2024. [23] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. arXiv preprint arXiv:2310.20700, 2023. 14 [24] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [25] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [26] Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hang Xu, Jianhua Han, and Xiaodan Liang. Semhitok: unified image tokenizer via semantic-guided hierarchical codebook for multimodal understanding and generation. arXiv preprint arXiv:2503.06764, 2025. [27] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [28] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Finegrained open domain image animation with motion guidance. arXiv preprint arXiv:2311.12886, 2023. [29] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [30] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. [31] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. [32] Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang. Evev2: Improved baselines for encoder-free vision-language models. arXiv preprint arXiv:2502.06788, 2025. [33] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In ICLR, 2024. [34] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [35] Lijie Fan, Luming Tang, Siyang Qin, Tianhong Li, Xuan Yang, Siyuan Qiao, Andreas Steiner, Chen Sun, Yuanzhen Li, Tao Zhu, et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. [36] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. CoRR, abs/2306.13394, 2023. [37] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [38] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. [39] Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, et al. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. arXiv preprint arXiv:2505.02471, 2025. [40] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. [41] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. [42] Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, and Hang Xu. Illume+: Illuminating unified mllm with dual visual tokenization and diffusion refinement. arXiv preprint arXiv:2504.01934, 2025. [43] Ting-Hao K. Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Aishwarya Agrawal, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016), 2016. [44] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [45] Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 67006709. Computer Vision Foundation / IEEE, 2019. 15 [46] Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding. arXiv preprint arXiv:2504.04423, 2025. [47] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [48] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [49] Siqi Kou, Jiachun Jin, Chang Liu, Ye Ma, Jian Jia, Quan Chen, Peng Jiang, and Zhijie Deng. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. arXiv preprint arXiv:2412.00127, 2024. [50] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [51] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [52] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. ArXiv, abs/2402.17245, 2024. [53] Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng Dai. Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding. arXiv preprint arXiv:2412.09604, 2024. [54] Haopeng Li, Jinyue Yang, Guoqi Li, and Huan Wang. Autoregressive image generation with randomized parallel decoding. arXiv preprint arXiv:2503.10568, 2025. [55] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, et al. Omnicorpus: unified multimodal corpus of 10 billion-level images interleaved with text. In The Thirteenth International Conference on Learning Representations, 2025. [56] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [57] Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. 2407.08303, 2024. [58] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. [59] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. [60] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. [61] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [62] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [63] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. [64] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. [65] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan C. Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, Jui-Chieh Wu, Sen He, Tao Xiang, Jürgen Schmidhuber, and Juan-Manuel Pérez-Rúa. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. [66] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [67] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 16 [68] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. arXiv preprint arXiv:2312.17172, 2023. [69] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [70] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo, Yuhe Yin, Yuheng Feng, Yuxiang Yang, Zecheng Tang, Zekai Zhang, Zidong Yang, Binxing Jiao, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu, Heung-Yeung Shum, and Daxin Jiang. Step-video-t2v technical report: The practice, challenges, and future of video foundation model, 2025. [71] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. [72] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [73] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. [74] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [75] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024. [76] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. [77] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 4195 4205, 2023. [78] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Will Beddow, Erwann Millon, Wenhai Wang Victor Perez, Yu Qiao, Bo Zhang, Xiaohong Liu, Hongsheng Li, Chang Xu, and Peng Gao. Lumina-image 2.0: unified and efficient image generative framework, 2025. [79] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. [80] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [81] Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024. [82] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [83] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [84] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv: 2412.15188, 2024. 17 [85] Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025. [86] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [87] Hongxuan Tang, Hao Liu, and Xinyan Xiao. Ugen: Unified autoregressive multimodal model with progressive vocabulary learning. arXiv preprint arXiv:2503.21193, 2025. [88] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. NeurIPS, 36, 2024. [89] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [90] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. [91] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [93] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [94] Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, et al. Textatlas5m: large-scale dataset for dense text image generation. arXiv preprint arXiv:2502.07870, 2025. [95] Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Lian Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, and Hanwang Zhang. Discrete visual tokens of autoregression, by diffusion, and for reasoning. 2025. [96] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. ILLUME: illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024. [97] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [98] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [99] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. IJCV, 2024. [100] Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, and Wenhao Huang. Mio: foundation model on multimodal tokens. arXiv preprint arXiv: 2409.17692, 2024. [101] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. [102] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023. [103] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [104] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation, 2025. 18 [105] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [106] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer, 2025. [107] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In ICCV, pages 74527461, 2023. [108] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In ICLR, 2025. [109] Rongchang Xie, Chen Du, Ping Song, and Chang Liu. MUSE-VL: modeling unified VLM through semantic discrete encoding. arXiv preprint arXiv:2411.17762, 2024. [110] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. [111] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [112] Jian Yang, Dacheng Yin, Yizhou Zhou, Fengyun Rao, Wei Zhai, Yang Cao, and Zheng-Jun Zha. MMAR: towards lossless multi-modal auto-regressive probabilistic modeling. arXiv preprint arXiv:2410.10798, 2024. [113] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [114] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, pages 95569567. IEEE, 2024. [115] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. [116] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. [117] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. [118] Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, and Yu Zhang. Nexus-gen: unified model for image understanding, generation, and editing. arXiv preprint arXiv:2504.21356, 2025. [119] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [120] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. [121] Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In ICLR, 2025. [122] Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, and Yuexian Zou. Vargpt: Unified understanding and generation in visual autoregressive multimodal large language model, 2025. [123] Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, and Xinggang Wang. Omnimamba: Efficient and unified multimodal understanding and generation via state space models, 2025."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Show Lab, National University of Singapore"
    ]
}