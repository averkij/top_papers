{
    "paper_title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering",
    "authors": [
        "An Quang Tang",
        "Xiuzhen Zhang",
        "Minh Ngoc Dinh",
        "Zhuang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM"
        },
        {
            "title": "Start",
            "content": "QQSUM: Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering An Quang Tang and Xiuzhen Zhang * and Minh Ngoc Dinh and Zhuang Li RMIT University, Australia s3695273@rmit.edu.vn, xiuzhen.zhang@rmit.edu.au minh.dinh4@rmit.edu.vn, zhuang.li@rmit.edu.au 5 2 0 2 4 ] . [ 1 0 2 0 4 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only single perspective, failing to capture the diversity of customer opinions. In this paper we introduce novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train KP-oriented retriever and KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/ antangrocket1312/QQSUMM"
        },
        {
            "title": "Introduction",
            "content": "With the rapid expansion of e-commerce, consumers increasingly rely on product reviews to inform their purchasing decisions. Automatic reviewbased product question answering (PQA) systems have emerged, leveraging user reviews to provide immediate responses on e-commerce Q&A platforms (McAuley and Yang, 2016; Gupta et al., 2019). However, current PQA systems face they typically generate single key limitation: answer (Gupta et al., 2019), overlooking the fact that many subjective e-commerce queries require *Corresponding author. answers that reflect diverse viewpoints. For example, when comparing camera lenses (Figure 1), some shoppers prioritize versatility and affordability, while others focus on image quality and speed. Recent PQA approaches aim to improve answer quality using retrieval-augmented generation (RAG). These systems first retrieve reviews relevant to the query and then use them as context for large language models (LLMs) to generate answers. Yet, LLMs often struggle to present multifaceted perspectives (Sorensen et al., 2024), leading to answers that primarily reflect dominant opinions from the retrieved reviews (Deng et al., 2020, 2023). Separately, opinion summarization has made progress through Key Point Analysis (KPA), which summarizes reviews into concise, representative statements called key points (KPs) while also quantifying their prevalence (Bar-Haim et al., 2020a,b, 2021; Tang et al., 2024a,b). However, these KPA methods focus on general summarization rather than answering specific queries. For tasks like product comparison, summarization must incorporate only query-focused KPs, making general KPA approaches insufficient for PQA. Figure 1: Comparison of conventional Q&A and QQSUM. More details of QQSUM output are in Table 12. In this paper, we introduce novel task Quantitative Query-Focused Summarization (QQSUM), which generates comprehensive answers containing diverse KPs along with their quantified relative importance (Figure 1). Our solution, QQSUMRAG, extends the RAG framework by integrating KP-oriented retrieval and summarization. Specifically, QQSUM-RAG retrieves query-relevant reviews, clusters them by distinct opinions, and summarizes representative KPs from each cluster. This approach provides broader coverage of key insights, overcoming the single-perspective limitation of conventional RAG-based systems. key challenge in implementing this approach is scarcity of training data for such specialized task. To address this, we develop co-training strategy that jointly optimizes the retriever and LLM through shared supervision signals, enhancing the alignment between retrieved opinion clusters and generated KPs. This strategy enables robust performance of QQSUM-RAG even with limited training examples. To support few-shot learning, we carefully curated dataset of queries with KPs and their prevalence quantification, through humanLLM collaboration. Empirical results show that QQSUM-RAG significantly outperforms RAG baselines based on in-context learning and quantitative summarization. Our main contributions are: We introduce novel task QQSUM. Unlike traditional PQA, QQSUM generates answers that capture diverse customer opinions with their prevalence, addressing queries that require multiple viewpoints. We propose QQSUM-RAG, RAG-based framework with KP-oriented retrieval and summarization. The framework is optimized through co-training strategy that improves alignment between retrieved opinion clusters and generated KPs in few-shot learning setting. Our experiments show that QQSUMRAG significantly outperforms baselines with up to 2.11 times improvement in textual similarity with ground-truth KPs and up to 67.12% improvement in quantification performance over state-of-the-art KPA system for reviews (Tang et al., 2024b)."
        },
        {
            "title": "2.1 Review-based PQA",
            "content": "Unlike domain-specific QA tasks such as biomedical or legal QA focusing on factual answers, reviewbased PQA seeks to provide answers of consumers subjective opinions about product. While extractive PQA approaches retrieve relevant review snippets as answers (Chen et al., 2019a; Yu et al., 2012), it fails to provide precise responses since the review might not be specifically written for answering the given question. Recently, inspired by the advances of seq-2-seq models, abstractive, i.e., generation-based, approaches can generate naturallanguage answers from reviews (Chen et al., 2019c; Gao et al., 2019). However, these approaches frequently suffer from hallucinations and factual inconsistencies, sometimes generating random answers that misrepresent or contradict the prevalent opinions (Deng et al., 2020, 2023). Existing reviewbased PQA framework then cannot capture nor quantify faithfully the diverse opinions of reviews in its answer."
        },
        {
            "title": "2.2 Key Point Analysis",
            "content": "Developed initially to summarize arguments (BarHaim et al., 2020a,b), KPA was later adapted for summarization of reviews (Bar-Haim et al., 2021; Tang et al., 2024a,b). While Bar-Haim et al. (2021) integrates sentiment analysis and collective key point mining to select and match KPs from broader domain with comments, Tang et al. (2024a) integrates aspect-based sentiment analysis (ABSA) into extracting and matching of KPs to comments for more unique KPs and precise quantification. More recent abstractive KPA studies apply abstractive summarization to paraphrase and generate KPs from comments (sentences) (Kapadnis et al., 2021; Li et al., 2023; Tang et al., 2024b). Overall, whether extractive or abstractive approaches, KPA can only produce KPs for general and high-level opinions without catering to specific queries."
        },
        {
            "title": "2.3 Textual Summarization",
            "content": "Document summarization aims to produce concise textual summaries capturing the salient information in source documents. While extractive review summarization approaches use surface features to rank and extract salient opinions for summarization (Mihalcea and Tarau, 2004; Angelidis and Lapata, 2018; Zhao and Chaturvedi, 2020), abstractive techniques use sequence-to-sequence models (Chu and Liu, 2019; Suhara et al., 2020; Bražinskas et al., 2020b,a; Zhang et al., 2020a) to generate reviewlike summaries containing only the most prevalent opinions. Recently, prompted opinion summarization leveraging Large Language Models (LLMs) was applied to generate fluent and concise review summaries (Bhaskar et al., 2023). However, existing studies lack focus on presenting and quantifying the diverse opinions in reviews."
        },
        {
            "title": "3.1 Task Formulation",
            "content": "Let denote query, i.e., community question, and Re = {rj}Re j=1 denotes set of review comments on product e, QQSUM aims to retrieve relevant comments to answer and generate KP-based summary quantifying viewpoints presented in D. We formulate = {kp1, . . . , kpn} as bullet-point summary containing multiple KPs, where each bullet-point represents KP 1 and its prevalence (Bar-Haim et al., 2021). For instance, with the bullet-point 23 comments praise that the headphone is very comfortable for long hours, the KP is Comfortable for long hours, and the prevalence count is 23. Each key point kpi, is matched to subset of supporting comments Ci = {c1, c2, . . .} (where ci D), with prevalence being measured as Ci."
        },
        {
            "title": "3.2 The QQSUM-RAG Framework",
            "content": "Figure 2 illustrates the architecture of QQSUMRAG. QQSUM-RAG is based on the retrievalaugmented generation (RAG) paradigm and consists of 2 stages: KP-Oriented Retrieval and KP Summary Generation. It utilizes Retriever to retrieve and cluster query-relevant comments into groups, and the LLM to generate the final KP summary based on the comment clusters. Importantly, the retriever and LLM can be jointly trained with shared supervision signals to ensure comment clusters retrieved match KPs generated. The following general loss function describes every training step of QQSUM-RAG, whose parameters are updated at the cluster-KP level rather than the query level: = (1 d) (Lclus + gold_score) + Lgen (1) where Lclus is the retrieval loss for each comment cluster, Lgen is the LLMs generation loss computed for the KP generated from the respective cluster, and is damping factor to balance between the two. Notably, gold_score represents the Perplexity Distillation loss (Izacard et al., 2023), which transforms the supervisory signals of the LLM to improve the Retriever. The intuition is that within cluster, comments that better contribute to helping the LLM generate the KP with lower perplexity should be ranked higher. 1unique and non-overlapping opinion at high level"
        },
        {
            "title": "3.2.1 KP-Oriented Retrieval\nGiven a query q, the Retriever should retrieve rele-\nvant review comments Rq that emphasize opinions\nfocused on q. We utilize a shared encoder E that\ncan encode both the input query q and each review\ncomment rj ∈ Re. Comments are ranked by the\nsimilarity score s(x, rj) = Ec(x)⊤Ed(rj) that is\ncalculated by taking the dot product of the embed-\ndings of the query x and the comment rj. Only\ncomments with s(x, rj) ≥ 1 is selected for Rq. 2\nDifferent from standard RAG where generation\nis based on the direct retrieval result, to ensure di-\nverse and representative opinions for generation,\nwe enhance the Retriever with the clustering ob-\njective to produce distinctive comment groups that\nconceptually match KPs for generation.",
            "content": "KP-Oriented Retrieval Loss Starting with an empty list of clusters C, and iterate through every comment in Rq, for every comment, we further iterate through every existing cluster ci and calculate its average cosine similarity score to all comments of the cluster. Finally, we add the comment to any clusters with average cosine similarity score above threshold (λ = 1.2), 3 otherwise, new cluster is created. Importantly, comment can be mapped to multiple clusters. We empirically showed that our proposed clustering algorithm is more effective than HDBSCAN (McInnes et al., 2017) and K-Means through an ablation experiment in Appendix K. (cid:80)M To train the retriever for KP-oriented retrieval, we align predicted comment clusters with annotated clusters P, where groups comments matched to the same KP (annotation details in 3.3). The centroid embedding of cluster is the mean embedding of its comments rk: Ec(ci) = 1 k=1 E(rk). Because cluster ci may contain mixed opinions represented by multiple clusters from P, we map each ci to the mean embedding of Pmatch P: Ec(Pmatch) = Ec(pj), where the semantic similarity 1 between ci and every pj is sim(ci, pj) = Ec(ci) Ec(pj) threshold. The training objective minimizes the mean-squared-error (MSE) loss between each comment rk in ci and the average center of the most similar clusters Pmatch. (cid:80)M j=1 Lclus = 1 ci ci (cid:88) k=1 Ec(Pmatch) E(rk)2 2. (2) 2the similarity threshold 1 is set empirically 3set empirically based on cluster quality Figure 2: The training architecture of the QQSUM-RAG framework."
        },
        {
            "title": "3.2.2 KP Summary Generation\nA key limitation of previous KPA studies is that\nKPs may contain redundant opinions, due to that re-\nview comments, possibly containing multiple opin-\nions, are mapped to individual KPs locally (Bar-\nHaim et al., 2021; Tang et al., 2024b). To address\nthis limitation, we propose to generate KPs at the\nglobal level, where the goal is to generate an overall\nKP-based summary without redundancy. Our main\nidea is that generated KPs are used as the context\nfor the LLM to better reason and generate the next\nKP, which should be a unique, non-overlapping\nopinion statement.",
            "content": "Prompting Strategies Following OpenAIs prompt engineering guidelines4, we format queryrelevant comment clusters from the Retriever into structured prompt with four parts (detailed in Listing 3, Appendix F): 1) Context and input structure, 2) Task definition and output requirements, 3) Summarization steps for identifying representative KPs per cluster and generating the final KP-based summary, and 4) Commonsense quantification rules to prioritize clusters by size To minimize and prevent overlapping KPs. ambiguity and hallucination, we encode predicted clusters as JSON objects and assign each unique ID, requiring the LLM to label generated KPs accordingly. Next-KP-Generation Training During training, generating multiple KPs in summary lacks alignment with Lclus, which is computed per comment cluster. To address this, we introduce NextKP-Generation objective, inspired by Next-Token Prediction in LMs (Brown et al., 2020), to enhance the generation of salient, non-overlapping 4https://platform.openai.com/docs/guides/ prompt-engineering let KPs. This approach fine-tunes the LLM to iteratively generate KPs within the summary. Specifthe final KP-based summary = cally, {kp1, . . . , kpi, . . . , kpn}, each kpi is generated with preceding KPs {kp1, . . . , kpi1} as the context, prompting the LLM to iteratively complete S. The generation loss for each kpi of ci is computed as the negative log-likelihood (NLL) against the reference KP, annotated for the most similar pi identified during retrieval, Lgen ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 log (xtx<t) (3) where (xtx<t) represents the probability assigned by the model to the correct token xt, given the preceding tokens x<t."
        },
        {
            "title": "3.3 Human-LLM Key Point Annotation",
            "content": "From Section 3.2, to train our QQSUM-RAG framework in the few-shot setting, annotation of KPs for queries and relevant comments are necesary. Prior KPA studies only include annotations matching comments to KPs without queries (BarHaim et al., 2020a,b). No datasets exist for matching comments to KPs in PQA. Figure 3: Illustration of the human-LLM collaborative annotation pipeline for AMAZONKP."
        },
        {
            "title": "Statistic",
            "content": "# Product Categories # Instances (queries) Per Category Total Instances # Reviews Per Query # Review Comments Per Query # Answers Per Query # KPs Per Query (Stage 1) # Relevant Comments Per Query (Stage 2) # Comments (Prevalence) per KP (Stage 2) Summary Length (Stage 3)"
        },
        {
            "title": "Test",
            "content": "17 2 34 71.18 452.03 7.53 9.26 24.50 6.37 101.29 17 148 2516 72.70 431.62 6.45 6.90 Table 1: Core statistics of the AMAZONKP dataset. We leverage the popular PQA dataset AmazonQ&A (Gupta et al., 2019) for our QQSUM task, focusing on only answerable, subjective (nonfactual) questions that have multiple answers. Out of 17 product categories (e.g., Electronics, Video Games), we only include businesses with 50-100 reviews, and sampling top 150 questions per category based on answer count. For ease of reference we name this curated dataset AMAZONKP. Details on question classification for AMAZONKP are in Appendix A, and their taxonomy in Appendix B. Notably, the dominance of Scenario-based questions underscore the importance of QQSUM for generating KP summary to answer user questions on preferences and scenarios. Manually summarizing and quantifying opinions from comments is laborious and time-consuming, if not impossible. Research shows LLMs strong annotation capabilities (He et al., 2024), and so we design three-stage human-LLM collaborative annotation pipeline, shown in Figure 3. Stage 1: KP Extraction from Gold Community Answers Given query qi, the AmazonQ&A dataset provides multiple answers, i.e. responses, from online users Ai = {a1, a2, . }, serving as ideal approximation of gold opinions. However, these responses can contain overlapping opinions. We therefore zero-shot prompted GPT-4-o-mini to extract distinctive and non-overlapping KPs from Ai. Empirical validation with human annotators confirms that the extracted KPs are of high quality, with 90% of community answers were represented by KPs, while 87.5% of the extracted KPs are verified as valid (precision). Further details are in Appendices and D. Stage 2: LLM-based and Manual Comment-KP Matching Based on the annotation process in the literature (Bar-Haim et al., 2020a), we further integrate LLMs to reduce human effort and time. Using KPs extracted from gold answers (Stage 1), we prompt GPT4-o-mini to annotate pairwise matches between comments and KPs from all available reviews of the product. LLM-matched pairs are then validated by three Amazon Mechanical Turk (MTurk) workers. Finally, comments from validated pairs are grouped by similar KPs, with KP prevalence determined by the number of matching comments. Further details on KP Matching annotations are provided in Apppendix E. Stage 3: KP-based Summary We utilize KPs and their prevalence counts, discovered for every query, to manually compose bullet-point KPbased summary, where each bullet point corresponds to KP and is annotated as kpi comments say that kpi. The number of pairwise comment-KP matching annotations required per query can be up to 2K3.5K. For training, to control annotation costs, we conducted Stages 1, 2 and 3 annotations on small subset of 34 instances for few-shot training of QQSUM-RAG, randomly selecting two queries per product category for supervised labeling. For evaluating the KP-based summary, the remaining examples with only Stage 1 annotations serve as the test set. The core statistics of AMAZONKP are shown in Table 1."
        },
        {
            "title": "4 Experiments",
            "content": "We employ Atlas (Izacard et al., 2023), pretrained efficient RAG model, as our backbone model for QQSUM-RAG. We utilized Contriever (Izacard et al., 2022) as the retriever while replacing the original language model with opensource LLMs (e.g., Vicuna-7B 5, Mistral-7B 6) for generation. For computational feasiblity, we apply Low-Rank Adaptation (LoRA) (Hu et al., 2021), which adds trainable parameters while freezing the models original weights."
        },
        {
            "title": "4.1 Baselines",
            "content": "We benchmark QQSUM-RAG against 3 RAG baselines. (Retriever + LLM)co-train We few-shot trained Atlas (Izacard et al., 2023), with the standard RAG architecture and Retriever-LLM generator co-training, for the QQSUM task. The retriever retrieves relevant comments, while letting the LLM implicitly infer KPs matching comments and their 5https://huggingface.co/lmsys/vicuna-7b-v1.5 6https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.2 quantities during KP summary generation. For training, we aggegrated matching comments across KPs, per query, as the retrieval ground truth. Frozen Retriever + Prompt LLM To assess in-context learning (ICL) for QQSUM, we use frozen retriever and Vicuna-7B, Mistral-7B, and GPT-4-Turbo as the LLM for ICL. Few-shot training instances are concatenated with test instances, with the number of few-shot examples optimized for context length and cost: 4-shot for Mistral-7B and GPT-4-Turbo, and 2-shot for Vicuna-7B. Frozen Retriever + KPA We replace the LLM of standard RAG with existing KPA review summarization systems to adapt KPA to the QQSUM task. Comments were first retrieved by frozen retriever and then RKPA-Base (Bar-Haim et al., 2021) utilizes quality ranking model (Gretz et al., 2020) to extract KP candidates before matching comments to KPs using KP Matching model (BarHaim et al., 2020b) at threshold tmatch = 0.99. PAKPA (Tang et al., 2024b) clusters comments by aspect and sentiment before generating aspectoriented KPs. All experiments were conducted at the KP level, focusing on KPs in the summary outputs of QQSUM-RAG and baselines for fair comparison. We post-process the output KP-based summary into KPs as JSON objects, where each object covers the KP information of bullet point in the summary. 7 The baselines were implemented using either the PyTorch module or the Huggingface transformers framework, and were trained on NVIDIA GeForce RTX 4090 GPU."
        },
        {
            "title": "4.2 Evaluation Dimensions",
            "content": "We conducted experiments on the test set of AMAZONKP (3.3), consisting of questions from 17 product categories. For reasonable cost, we sample 8 questions from each category for evaluation."
        },
        {
            "title": "4.2.1 KP Textual Quality\nAutomatic Evaluation Extracted KPs from gold\ncommunity answers in AmazonKP (Stage 1 of\n§ 3.3) serves as the reference KPs for this auto-\nmatic evaluation. We first perform a lexical com-\nparison between KPs in the generated summary",
            "content": "7We use simple LLM-based post processor, prompting gpt-4-o-mini with Format all key points and their prevalences mentioned in the above bullet-point summary in JSON list, where each JSON object format as: {key_point: <key point of bullet>, prevalence: <key points prevalence>} and the ground truth by computing the highest Rouge (Lin, 2004) score between generated and reference KPs for each query and then average the maxima. Then, following Li et al. (2023), we calculate soft-Precision/Recall/F1 (denoted by sP, sR and sF1, respectively), which measure the semantic similarity between individual generated KP and the reference KP. While sP finds the reference KP with the highest similarity score for each generated KP, sR is vice-versa, and (sF 1) is the harmonic mean between sP and sR. sP = 1 (cid:88) αiA max βjB (αi, βj) sR = 1 (cid:88) βiB max αjA (αi, βj) (4) (5) Additionally, inspired by sP/sR/sF1 of Li et al. (2023), we further propose RD to identify KP redundancy. For each generated KP in the summary for query, RD finds the neighborhood KP with the highest similarity score. RD = 1 (cid:88) αiA max θj=αiA (αi, θj) (6) where computes similarities between two individual key points, A, is the set of generated and reference KPs and = and = B, respectively. We use state-of-the-art semantic similarity metrics BLEURT (Sellam et al., 2020) and BERTScore (Zhang et al., 2020b), along with LLMbased metric G-EVAL-4 (Liu et al., 2023) as fmax. Note that G-EVAL scores are scaled from 1-5 to 01 for comparability and its evaluation prompt was also customized to fit our evaluation (Appendix G). Human Evaluation We manually evaluated the information quality of generated KPs in the summary considering 7 different dimensions utilized in previous KPA studies (Kapadnis et al., 2021; Tang et al., 2024b), including REDUNDANCY, COVERAGE, FAITHFULNESS VALIDITY, SENTIMENT, INFORMATIVENESS and SINGLE ASPECT. Details of these dimensions are in Appendix H. We conducted pairwise comparisons of KPs from different systems using Amazon Mechanical Turk (MTurk). Given dimension for evaluation, each comparison involved choosing the better one from two summaries, each taken from different system. Using the Bradley-Terry model Friedman et al. (2021), we calculated rankings from these comparisons among the models. For an example of QQSUM-RAG (Ours) Contriever (Izacard et al., 2022) all-MiniLM-L12-v2 (Wang et al., 2021) P@5 P@10 P@20 P@all 0.668 0.567 0.590 0.569 0.544 0.444 0.531 0.552 0.633 0.527 0.538 0. 0.601 0.526 0.500 0.468 0.511 0.467 0.530 0.512 0.459 0.442 0.515 0. 0.404 0.452 0.387 0.535 0.367 0.440 0.362 0.345 0.328 0.350 0.339 0.325 0.315 0. (Retriever + LLM)co-train (Izacard et al., 2023) Contriever (Izacard et al., 2022) all-MiniLM-L12-v2 (Wang et al., 2021) frozen Retriever + prompt LLM Contriever all-MiniLM-L12-v2 BM25 0.494 0.479 0.469 0.447 0.446 0.432 + Mistral + Vicuna + Mistral + Vicuna + Mistral + Vicuna + Mistral + Vicuna Table 2: Performance of retrieval models. an annotation, see Appendix I. Note that for reasonable cost, we sample and select only the popular question (with the highest average KP prevalence), each from 5 common categories 8 of AMAZONKP."
        },
        {
            "title": "4.2.2 KP Quantification Performance",
            "content": "We evaluate the KP quantification performance of different systems for KP-comment matching and factual alignment. KP-comment matching We first assess the accuracy of the KP comment matching by extending Bar-Haim et al. (2021) to measure both precision (correctness of predicted matches) and recall (coverage of ground-truth matches). For each system, we compute precision and recall by prompting gpt-4-o-mini to annotate pairwise match/nonmatch between generated KPs and retrieved comments Rq. Additionally, leveraging annotated comment-KP pairs, we introduce QuantErr, which measures the mean absolute error between predicted and actual KP prevalence count. Empirical validation shows gpt-4-o-mini annotations highly correlated with MTurk workers judgement (Pearsons = 0.647) (Appendix J). KP-comment factual alignment: We further employed AlignScore (Zha et al., 2023) for automatic evaluation of factual alignment between generated KPs and their corresponding comments."
        },
        {
            "title": "4.3.1 The Retrieval Model",
            "content": "The retriever is important for retrieving comments relevant to queries and so we first evaluated the performance of different backbone retrieval models. 8namely Home & Kitchen, Sports & Outdoors, Tools & Home Improvement, Health & Personal Care, and Beauty For this we prompted gpt-4-o-mini to annotate the relevance of retrieved comments to queries. Table 2 reports the retrieval Precision@k (P@k), measured at different levels of top-k-ranked retrieved comments ([5, 10, 20, all]), using 3 different retrieval models: Contriever, all-MiniLM-L12-v2 and BM25. Note that BM25 is not neural encoder and therefore can only be evaluated in the frozen Retriever + Prompt LLM setting. Overall, the trained retriever of QQSUM-RAG, as being co-trained with the LLM and extended for KP-oriented retrieval, outperform all baselines. Notably, co-training with stronger LM can also contribute up to 45.78% improvement, as the supervision signal from more query-focused KP generation helps train the Retriever to rank documents more accurately. Contriever stood out as the best performer regardless of the LM selection. Hereafter we base all upcoming experiments with Contriever as the retrieval model."
        },
        {
            "title": "4.3.2 KP Quality",
            "content": "KPs produced by different systems in terms of textual quality, semantic quality and redundancy are reported in Table 3. Scores of all systems are low in general, as opinions in product reviews may not cover all opinions from community answers to questions. From Table 3, QQSUM-RAG outperforms other systems in all quality dimensions. It shows 2.11 times improvement in textual similarity with reference KPs (0.256 vs. 0.121 in ROUGE-1), 0.23 point absolute improvement in semantic similarity (0.39 vs. 0.16 in BERTScore) and 0.14 point absolute reduction in Redundancy (0.37 vs. 0.51 using BERTScore for semantic similarity). The high quality of KPs in QQSUM-RAG can be attributed to the KP-oriented retrieval of QQSUM-RAG. Notably, although (Retriever + LLM)co-train shares the same backbone model and co-training design with QQSUM-RAG, the lack of (1) opinion-level clustering of retrieved comments and (2) limited modeling capability of LLMs makes this model unable to produce KPs as diverse, unique and representative as QQSUM-RAG. The weak reasoning capability of LLMs for diverse opinion summarization is further exposed in the frozen Retriever + prompt LLMs setting, where LLMs even with strong modelling capability like GPT-4-Turbo struggle to elaborate diverse and distinctive KPs from hundreds of comments. It is worth noting that Mistral-7B broadly exhibits higher performance than Vicuna-7B across ROUGE BERTScore BLEURT G-Eval-4 R-1 R-2 R-L sP sR sF1 RD sP sR sF1 RD sP sR sF1 RD QQSUMRAG (Ours) + Mistral + Vicuna 0.256 0.222 0.061 0.078 0.220 0. 0.39 0.38 0.29 0.26 0.33 0.31 0.37 0.53 0.51 0.49 0.41 0. 0.46 0.44 0.49 0.54 0.88 0.87 0.82 0.81 0.85 0.84 0.36 0. (Retriever + LLM)cotrain (Izacard et al., 2023) + Mistral + Vicuna 0.209 0.174 0.057 0.041 0.194 0.161 0.37 0. 0.28 0.26 0.32 0.31 0.43 0.48 0.49 0.48 0.40 0.38 0.44 0. 0.55 0.58 0.81 0.78 0.82 0.78 0.81 0.78 0.41 0.41 Frozen Retriever + prompt LLM + Mistral + Vicuna + GPT-4-Turbo Frozen Retriever + KPA 0.210 0.164 0.197 0.055 0.059 0.048 0.191 0.154 0.174 0.33 0.22 0. 0.26 0.20 0.25 0.29 0.21 0.28 0.51 0.48 0.44 0.46 0.46 0.45 0.38 0.31 0.38 0.42 0.37 0. 0.55 0.59 0.54 0.79 0.73 0.77 0.80 0.73 0.77 0.79 0.73 0.77 0.41 0.41 0,38 + PAKPA (Tang et al., 2024b) + RKPA-Base (Bar-Haim et al., 2021) 0.179 0.121 0.027 0.016 0.162 0.106 0.34 0.16 0.28 0.14 0.31 0. 0.46 0.50 0.47 0.43 0.41 0.36 0.44 0.39 0.54 0.61 0.79 0. 0.80 0.70 0.80 0.69 0.36 0.51 Table 3: KP summary textual quality. sP, sR and sF1 refer to Soft-Precision, Soft-Recall, and Soft-F1 respectively based on set-level evaluation method against reference KPs in gold answer. CV FF RD VL SN IN SA QQSUM-RAG (Ours) (Retriever + LLM)co-train (Izacard et al., 2023) Frozen Retriever + prompt LLM (GPT-4-Turbo) Frozen Retriever + PAKPA (Tang et al., 2024b) Frozen Retriever + RKPA-Base (Bar-Haim et al., 2021) 28.44 11.06 15.12 9.94 16.20 26.56 11.17 12.84 12.41 22.28 25.34 14.7 15.73 13.28 15.73 35.23 9.99 10.36 7.7 22.91 31.11 9.54 14.6 8.87 20.75 25.9 13.49 12.59 13.04 21. 24.8 17.52 10.79 9.34 18.77 Table 4: Human evaluation of KP information quality by different dimensions. Reported are the Bradley Terry scores of 7 dimensions, from left to right, COVERAGE, FAITHFULNESS and REDUNDANCY, VALIDITY, SENTIMENT, INFORMATIVENESS, SINGLEASPECT. For reasonable cost, we only conducted manual evaluation on Mistral - the best LM configuration of QQSUM-RAG and (Retriever + LLM)co-train, selected from Table 3. all systems based on LLM generation and in all KP quality measurement (up to 15.32%), largely due to its stronger modeling capability. Frozen Retreiver + KPA baselines, despite their high performance for review summarization, is ineffective for QQSUM. Not surprisingly PAKPA, which generates KPs based on aspect-sentiment, broadly shows better performance than RKPABase, an extractive KPA system. It is possible that multiple query-relevant opinions on the same aspect are expected to answer user query, thus leading to the weak performance of PAKPA. Our manual evaluation of KP information quality further validates the above findings, as shown by the Bradley Terry scores in Table 4. Overall, QQSUM-RAG achieves up to 4.58 times improvements on all 7 dimensions, and are notably higher on COVERAGE (CV) (2.86 times), VALIDITY (VL) (2.38 times), and SENTIMENT (SN) (3.5 times)."
        },
        {
            "title": "4.3.3 KP Quantification",
            "content": "Table 5 presents the quantification performance for different systems. F1, combining Recall and Precision, measures the overall performance of KP-comment matching for all systems. QuantErr (lower the better) directly measures KP quantification errors. Overall, QQSUM-RAG shows the best performance in terms of both F1 (0.792 vs. 0.154) and QuantErr (4.24 vs. 30.13). Comparing QQSUM-RAG against the Retriever+LLM generation systems, namely (Retriever + LLM)co-train and Frozen Retriever + prompt LLM, we can see that, without clustering comments, LLMs perform comment-KP matching and KP quantification, showing extremely low Recall (0.1850.249), in contrast to the high Recall of QQSUM-RAG (0.684-0.869). This can be attributed to two main factors: (1) LLMs tend to hallucinate when generating KPs from large set of retrieved comments, and (2) their limited context window restricts their ability to effectively match comments to KPs. Comparing QQSUM-RAG against Retriever + KPA systems, our model shows up to 67.12% improvement in quantification performance over stateof-the-art KPA system for reviews (PAKPA) (Tang et al., 2024b), with 36.53% reduction in QuantErr. Note that Frozen Retriever + PAKPA achieves the highest matching precision due to aspect-level opinion quantification. However, it has low recall, possibly because it relies on aspect-based sentiment analysis of comments, which can fail to identify implicit opinions not explicitly including aspects. As shown in Table 5, results for KP-Comment Factual Alignment show that QQSUM-RAG and KP-Comment Matching KP-Comment Factual Alignment F1 QuantErr AlignScore QQSUM-RAG (Ours) + Mistral + Vicuna 0.694 0.538 0.869 0.684 0.792 0. (Retriever + LLM)cotrain (Izacard et al., 2023) + Mistral + Vicuna Frozen Retriever + prompt LLM + GPT-4-Turbo + Mistral + Vicuna Frozen Retriever + KPA 0.567 0. 0.249 0.094 0.346 0.154 0.746 0.498 0.439 0.200 0.214 0.185 0.313 0.300 0.260 + PAKPA (Tang et al., 2024b) + RKPA-Base (Bar-Haim et al., 2021) 0.762 0.371 0.520 0.314 0.619 0.340 04.24 07.83 18.10 30.13 16.63 19.14 21. 06.68 15.62 0.749 0.630 0.653 0.394 0.673 0.624 0.531 0.749 0.354 Table 5: Performance for KP-Comment matching and factual alignment Frozen Retriever + KPA (PAKPA) achieve high factual correctness in KP generation, outperforming other systems (0.749 vs. 0.354). This result highlights that QQSUM-RAG generates KPs grounded in the retrieved comments, and similarly PAKPA generates KPs grounded in aspects."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "We evaluate the contribution of Next-KPGeneration in QQSUM-RAG, with results in Tables 10 and 11 (Appendix L). In particular, we configure variant QQSUM-RAG Single-KP that replaces Next-KP-Generation with KP generation for each comment cluster. Not including previously generated KPs as context, QQSUM-RAG Single-KP struggles to capture the truly representative opinion of the cluster, likely generating KPs with overlapping opinions, especially for comments containing multiple opinions. Note that while its KP quality underperforms RAG baselines, its KP Quantification performance remain superior, largely attributed to KP-oriented Retrieval."
        },
        {
            "title": "4.5 Case studies",
            "content": "We conducted case studies to evaluate the redundancy and specificity of generated KPs for query comparing camera lenses, presented in Table 14 (Appendix M). Overall, QQSUM-RAG stands out for generating KPs with minimal redundancy, high informativeness, and alignment with the query. First, QQSUM-RAG reduces redundancy by effectively capturing distinct product features relevant to the users needs (e.g., faster aperture), whereas (Retriever + LLM)co-train, GPT-4-Turbo Prompt LLM, and PAKPA tend to generate repetitive and generic statements, such as The 24-70mm f/2.8 is better lens overall. Furthermore, QQSUM-RAG expands feature coverage, capturing details such as Vibration Reduction (VR) technology, which several baselines fail to mention."
        },
        {
            "title": "4.6 Error Analysis",
            "content": "Our analysis on KP summary of QQSUM-RAG reports two systematic error patterns, as shown in Table 13. First, KP can be falsely matched to comments expressing similar opinions but on different targets. For instance, the comment For lens that is overall rather mixed bag . . . it is very expensive. was matched to KP The 24-120mm F4 lens has longer zoom range and is more affordable than the 24-70mm F2.8.. Since the comment lacks an explicit product reference, it remains unclear whether it critiques the 24-120mm F4 or the 24-70mm F2.8. The second type of errors stems from the sentence-level quantification, where input review sentences often contain co-occurring multiaspect opinions, making it difficult for the Retriever to isolate distinct aspects into separate clusters."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we studied new task Quantitative Query-focused Summarization, namely QQSUM, for capturing and quantifying diverse opinions from online reviews for PQA. We propose QQSUMRAG, few-shot summarization model based on retrieval-augmented generation where summary is generated by LLMs from groups of user opinions relevant to query. QQSUM-RAG addresses the issue of existing RAG frameworks for providing only random or major opinion in the answer. By extending the retriever with opinion-based clustering of relevant comments, our model ensures capturing more diverse and representative opinions in the summary, along with accurate quantification. Experimental results show that our solution greatly enhances both the quality and quantitative performance of key point generation in summaries."
        },
        {
            "title": "Acknowledgement",
            "content": "This research is supported in part by the Australian Research Council Discovery Project DP200101441."
        },
        {
            "title": "Limitations",
            "content": "We evaluated the textual quality of generated KPs only on AmazonQ&A, as it is the only (to our best knowledge) public dataset with abundance of online community answers written by online users usable as ground truth for our automatic evaluation. Since we are leveraging answers from AmazonQ&A to summarize and quantify the prevalence of query-relevant opinions from reviews regarding query, an inevitable limitation is that key points extracted from the Q&A answers might not fully in line with viewpoints in reviews to answer questions. Similarly, opinions in product reviews also may not sufficiently cover all opinions in community answers."
        },
        {
            "title": "Ethics Statement",
            "content": "We have applied ethical research standards in our organization for data collection and processing throughout our work. The AmazonQ&A dataset used in our experiments was publicly crowdsourced and released for the research publication for the review-based product question answering task (Gupta et al., 2019). The dataset was published following their ethical standard, after removing all personal information. The answers to questions do not contain contents that are harmful to readers. We ensured fair compensation for crowd annotators on Amazon Mechanical Turk. We setup and conducted fair payment to workers on their annotation tasks/assignments according to our organizations standards, with an estimation of the difficulty and expected time required per task based on our own experience. Especially, we also made bonus rewards to annotators who exerted high-quality annotations in their assignments."
        },
        {
            "title": "References",
            "content": "Stefanos Angelidis and Mirella Lapata. 2018. Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 36753686, Brussels, Belgium. Association for Computational Linguistics. Roy Bar-Haim, Lilach Eden, Roni Friedman, Yoav Kantor, Dan Lahav, and Noam Slonim. 2020a. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 40294039, Online. Association for Computational Linguistics. Roy Bar-Haim, Lilach Eden, Yoav Kantor, Roni Friedman, and Noam Slonim. 2021. Every bite is an experience: Key Point Analysis of business reviews. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 33763386, Online. Association for Computational Linguistics. Roy Bar-Haim, Yoav Kantor, Lilach Eden, Roni Friedman, Dan Lahav, and Noam Slonim. 2020b. Quantitative argument summarization and beyond: Crossdomain key point analysis. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3949, Online. Association for Computational Linguistics. Adithya Bhaskar, Alex Fabbri, and Greg Durrett. 2023. Prompted opinion summarization with gpt-3.5. In Findings of the Association for Computational Linguistics: ACL 2023, pages 92829300. Ralph Allan Bradley and Milton E. Terry. 1952. RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS: THE METHOD OF PAIRED COMPARISONS. Biometrika, 39(3-4):324345. Arthur Bražinskas, Mirella Lapata, and Ivan Titov. 2020a. Few-shot learning for opinion summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 41194135, Online. Association for Computational Linguistics. Arthur Bražinskas, Mirella Lapata, and Ivan Titov. 2020b. Unsupervised opinion summarization as In Proceedings of the copycat-review generation. 58th Annual Meeting of the Association for Computational Linguistics, pages 51515169, Online. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. In AdLanguage models are few-shot learners. vances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Kathy Charmaz. 2015. Grounded theory. Qualitative psychology: practical guide to research methods, 3:5384. Long Chen, Ziyu Guan, Wei Zhao, Wanqing Zhao, Xiaopeng Wang, Zhou Zhao, and Huan Sun. 2019a. Answer identification from product reviews for user questions by multi-task attentive networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 4552. Qibin Chen, Junyang Lin, Yichang Zhang, Hongxia Yang, Jingren Zhou, and Jie Tang. 2019b. Towards knowledge-based personalized product description In KDD 2019, pages generation in e-commerce. 30403050. Shiqian Chen, Chenliang Li, Feng Ji, Wei Zhou, and Haiqing Chen. 2019c. Driven answer generation for In Proproduct-related questions in e-commerce. ceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 411 419. Eric Chu and Peter Liu. 2019. Meansum: neural model for unsupervised multi-document abstractive summarization. In International Conference on Machine Learning, pages 12231232. PMLR. Yang Deng, Wenxuan Zhang, and Wai Lam. 2020. Opinion-aware answer generation for review-driven question answering in e-commerce. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 255264. Yang Deng, Wenxuan Zhang, Qian Yu, and Wai Lam. 2023. Product question answering in E-commerce: survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1195111964, Toronto, Canada. Association for Computational Linguistics. Roni Friedman, Lena Dankin, Yufang Hou, Ranit Aharonov, Yoav Katz, and Noam Slonim. 2021. Overview of the 2021 key point analysis shared task. In Proceedings of the 8th Workshop on Argument Mining, pages 154164, Punta Cana, Dominican Republic. Association for Computational Linguistics. Shen Gao, Zhaochun Ren, Yihong Zhao, Dongyan Zhao, Dawei Yin, and Rui Yan. 2019. Product-aware answer generation in e-commerce question-answering. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 429437. Shai Gretz, Roni Friedman, Edo Cohen-Karlik, Assaf Toledo, Dan Lahav, Ranit Aharonov, and Noam Slonim. 2020. large-scale dataset for argument quality ranking: Construction and analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 78057813. Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary Lipton. 2019. Amazonqa: review-based question answering task. arXiv preprint arXiv:1908.04364. Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. 2024. AnnoLLM: Making large language models to be better crowdsourced annotators. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 165190, Mexico City, Mexico. Association for Computational Linguistics. Edward Hu, Yelong Shen, Phil Wallis, Zeyuan AllenZhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143. Manav Kapadnis, Sohan Patnaik, Siba Panigrahi, Varun Madhavan, and Abhilash Nandy. 2021. Team enigma at ArgMining-EMNLP 2021: Leveraging pre-trained In Prolanguage models for key point matching. ceedings of the 8th Workshop on Argument Mining, pages 200205, Punta Cana, Dominican Republic. Association for Computational Linguistics. Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159174. Hao Li, Viktor Schlegel, Riza Batista-Navarro, and Goran Nenadic. 2023. Do you hear the people sing? key point analysis via iterative clustering and abstractive summarisation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14064 14080, Toronto, Canada. Association for Computational Linguistics. Piji Li, Zihao Wang, Lidong Bing, and Wai Lam. 2019. In WWW 2019, Persona-aware tips generation? pages 10061016. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Julian McAuley and Alex Yang. 2016. Addressing complex and subjective product-related queries with In Proceedings of the 25th Incustomer reviews. ternational Conference on World Wide Web, pages 625635. Leland McInnes, John Healy, Steve Astels, et al. 2017. hdbscan: Hierarchical density based clustering. J. Open Source Softw., 2(11):205. Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404411, Barcelona, Spain. Association for Computational Linguistics. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892, Online. Association for Computational Linguistics. Taylor Sorensen, Jared Moore, Jillian Fisher, Niloofar Mireshghallah, Mitchell Gordon, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, et al. 2024. roadmap to pluralistic alignment. arXiv preprint arXiv:2402.05070. Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis, and Wang-Chiew Tan. 2020. OpinionDigest: simple framework for opinion summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5789 5798, Online. Association for Computational Linguistics. An Tang, Xiuzhen Zhang, and Minh Dinh. 2024a. Aspect-based key point analysis for quantitative sumIn 18th Conference of the marization of reviews. European Chapter of the Association for Computational Linguistics. An Tang, Xiuzhen Zhang, Minh Dinh, and Erik Cambria. 2024b. Prompted aspect key point analysis for quantitative review summarization. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1069110708, Bangkok, Thailand. Association for Computational Linguistics. Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. 2021. MiniLMv2: Multi-head selfattention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 21402151, Online. Association for Computational Linguistics. Jianxing Yu, Zheng-Jun Zha, and Tat-Seng Chua. 2012. Answering opinion questions on products by exploiting hierarchical organization of consumer reviews. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 391401, Jeju Island, Korea. Association for Computational Linguistics. Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Evaluating factual consistency with unified alignment function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1132811348, Toronto, Canada. Association for Computational Linguistics. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020a. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 1132811339. PMLR. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Chao Zhao and Snigdha Chaturvedi. 2020. Weaklysupervised opinion summarization by leveraging external information. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 96449651."
        },
        {
            "title": "AMAZONKP Dataset",
            "content": "Existing online product-related questions can be categorized into two groups: subjective (opinionated) or objective (factal). While subjective questions asks about positive/negative feeling or stance (e.g., whether product is good\" or bad\"), objective questions confirms the actual product details (e.g., products properties, specific use-cases). In E-Commerce, questions are often subjective, i.e., asking for former buyers opinion, where different customers often have certain preferences over product aspects or information needs (Chen et al., 2019b; Li et al., 2019), leading to various expectations for the provided answers. We extract subjective, i.e., opinionated, question from AmazonQ&A by prompting the Mistral-7B open-source LLM to analyze the question and its associated answers, published by the online community. In this case, leveraging answers helps to understand the nature of the questions, thereby better reasoning whether the question is seeking for subjective information from users. We present the few-shot prompt for classifying opinionated, i.e., subjective, questions from AmazonQ&A in Listing 1. Listing 1: Few-shot prompt (2 examples) for prompting Mistral-7B on opinionated question classification. You will be provided with question and multiple answers to that question, delimited by triple quotes. The question was taken from Question Answering dataset of product reviews, and can either be an opinionated or factual question. You were tasked to classify whether the given question is an opinionated or factual question. Factual questions ask for objective data, specifications, or information that can be definitively answered based on product facts , manual, user experience, or specifications. Factual question tends to covers unique and consistent opinions/fact in its answers. Opinionated questions are subjective and seek insights that are based on personal use, feelings, preferences, judgments, or evaluations about product. Opinionated question has multiple and diverse opinions in its answers. Formally, you should perform the following steps: 1. Identify unique opinions from the answers of the given question 2. Based on the question content and the amount of opinions in the question's answer, identify the question's type. Note that you must briefly explain why the question is opinionated or factual before giving the final decision. Below are some fewshot examples: Questions: How well does it work with wireless charging Answers: ['Unfortunately with this case installed it will not hold the phone vertically.', 'I use the case with the official wireless charger and have had no problems at all.', 'Works great. Not fan of the dimensions.'] Type: 'Opinionated Question' Questions: Are the shelves removeable? Answers: ['yes, they are removeable..', 'Yes they are, you can arrange them for the size of the shot glass.'] Type: 'Factual Question'"
        },
        {
            "title": "B Qualitative Data Analysis of",
            "content": "Opinionated Questions Categories in AMAZONKP We further studied the utility of the QQSUM task and our by conducting qualitative data analysis to categorize possible opinionated questions type in AMAZONKP. Based on the grounded theory methodology (Charmaz, 2015), our analysis employ human-LLM collaborative annotation to iteratively code the fine-grained categories from opinionated questions. We sampled subset of 100 questions from AMAZONKP for data coding and intepretation. On the subset, we start by prompting ChatGPT to identify potential categories of opinionated questions, including the categories name and their definitions (Step 1). Importantly, the data coding process involves human validation, in which we iteratively human annotator iteratively evaluate the representative of generated categories while interacting with ChatGPT, and manually refine the categories where possible 9 (Step 2). Then, we prompted gp4-o-mini to annotate the labels of entire questions in the subset, before asking human annotator again to validate the representative and 9On categories requiring more fine-grained categorization, we further conduct another analysis cycle on the particular coarse-grained category, by selecting questions and answers from the specific category for analysis. suitability of the candidate categories on questions. Categories with abnormal distribution, e.g., 5 times higher than others, or with high unmatching cases will be passed back to Step 2 for another iterative analysis cycles. As result, our analysis reported 5 categories commonly representative of question in AMAZONKP, namely, Performance, Quality, Recommendation, Comparative and Controversial, with each the stating clearly the purpose of the users asking the questions and expected answers. Finally, We prompted gpt-4-o-mini to annotate such categories on AMAZONKPs opinionated questions, and reported their taxonomy and statistics in Table 6. Notably, the dominance of Scenario-based questions underscore the importance of QQSUM for generating KP summary to answer user questions on preferences and scenarios. Human Validation of GPT4s Key Point Extraction from Gold Community Answer of AmazonQ&A In this experiment, we empirically validate gpt-4-o-minis performance and credibility in extracting KPs from gold community answers for AmazonKP (Stage 1 of 3.3). Specifically, to maintain reasonable cost, we sampled question,"
        },
        {
            "title": "Category\nPerformance",
            "content": "Description Ask how well product performs or functions in general. Ask about the overall or aspect-specific quality of the product. Ask whether product fits specific use cases, sizes, or other products. Recommendation Ask for suggestions tailored to specific isScenario-based"
        },
        {
            "title": "Controversial",
            "content": "sues or use cases. Seeks opinions about the relative advantages or disadvantages of product compared to others. Reflect dissatisfaction or complaint about product, likely to provoke debate or controversy. Example How well does it work on carpet? # Query 376 Is this product worth the money? Does this item really stop the glare at night even in rain or snow? What do you use to spray this stuff on your lawn? Would wired keyboard/mouse be better than wireless? Why does this need adjustment screws? If have to align the laser then whats the point? 265 1402 156 227 124 Table 6: taxonomy of opinion questions AMAZONKP i.e., queries, from 5 common product categories of AmazonKP 10, totaling 5 questions, and hired workers to annotate whether the extracted KPs matches original gold community answers of the sampled questions, which is inspired by the KP Matching evaluation of Bar-Haim et al. (2021). More specifically, for given query, we asked workers to perform pairwise annotation between extracted KPs and the querys respective community answers. While Precision calculates the fraction of KPs matched to at least one gold answer, i.e., out of all extracted KPs how many are correctly mapped, Recall shows the fractions of gold answers matched to at least one KP, i.e., out of all answers how many are covered by KPs. We then macro-averaged Precision/Recall computed for every question to obtain the final values. For human annotation, we employed 3 MTurk crowd workers on every answer-KP pair, selecting only those with an 80% or higher approval rate and at least 10 approved tasks. Following Bar-Haim et al. (2021), we exclude annotators with Annotatorκ < 0 for quality control. This score averages all pairwise Cohens Kappa (Landis and Koch, 1977) for given annotator, for any annotator sharing at least 50 judgments with at least 2 other annotators. For labelling correct matches, we applied strict threshold, in which 100% votes (3 out of 3) of the annotators had to agree that the match was correct. Otherwise, it is incorrect. Table 7 presents the fraction of extracted KPs matched to at least one gold answer (Precision) and vice versa (Recall). Overall, the experiment confirms that the extracted KPs are of high quality, with 90.0% of community answers were 10namely Home_and_Kitchen, Sports_and_Outdoors, Tools_and_Home_Improvement, Health_and_Personal_Care and Beauty Precision Recall # Matched Answers Per KP # Matched KPs Per Answer 87.5% 90.0% 2.39 2.61 Table 7: Performance validation of gpt-4-o-minis KP extraction from gold community answer. While precision calculates the fraction of KPs matched to at least one gold answer, recall shows the fractions of gold answers matched to at least one KP. represented with KPs (recall), while 87.5% of the extracted KPs are verified as valid (precision)."
        },
        {
            "title": "Below are the match annotation guidelines for",
            "content": "(extracted KP, gold answer) pairs: In this task you are presented with question on product, key point extracted from community answers answering the question, and community answer for answering the query of that product. You will be asked to answer the following question:\"Does the key point match, i.e., represent an opinion in the community answer?\" community answer might express opinions on multiple aspects. key point matches community answer if it captures the gist of the answer, or is directly supported by point made in the community answer. The options are: Not At All Somewhat Not Well Somewhat Well Very Well"
        },
        {
            "title": "D Prompt for Key Point Extraction from",
            "content": "Somewhat Well Gold Community Answer of AmazonQ&A Very Well We present the few-shot prompts for extracting key points (KPs) from gold online community answers of AmazonKP in Listing 2."
        },
        {
            "title": "AMAZONKP Dataset",
            "content": "We offer GPT-4-o-mini with 4 options for labelling the matching status of given comment-KP pairs. Pairs annotated as Very Well or Somewhat Well by LLM then becomes candidate matching pairs, which will be further validated by human For human annotation for their correctness. annotation, we employed 3 MTurk crowd workers per comment-KP pair, selecting only those with an 80% or higher approval rate and at least 10 approved tasks. Following Bar-Haim et al. (2021), we exclude annotators with Annotator-κ < 0 for quality control. This score averages all pairwise Cohens Kappa (Landis and Koch, 1977) for given annotator, for any annotator sharing at least 50 judgments with at least 2 other annotators. For labelling correct matches, at least 60% of the annotators had to agree that the match is correct, otherwise, it is incorrect. Comments from final matching pairs, after confirmed by human, will then be grouped by similar KPs, where the amount of matching comments per KP is the prevalence of the respective KP. Below are the matching prompt for LLM and the annotation guidelines for workers validating (sentence, KP) pairs: In this task, you are presented with question on product, key point taken from the summary answering the question, and sentence taken from review of that product. You will be asked to answer the following question: \"Does the key point match, i.e, represent an opinion in the review sentence?\" review sentence might express opinions on multiple aspects. key point matches sentence if it captures the gist of the sentence, or is directly supported by point made in the sentence. The options are: Not At All Somewhat Not Well"
        },
        {
            "title": "F Prompts for KP Summary Generation",
            "content": "of QQSUM-RAG We present the instruction-finetuning prompts for KP Summary Generation of QQSUM-RAG in Listing 3. Prompts for G-EVAL Evaluation For implementation of G-EVAL in our KP quality evaluation dimension (4.2), we specifically customize the models original prompt for evaluating summarys relevance and redundancy. While the relevance evaluation prompt is customized for evaluating sP/sF/sF1 (Li et al., 2023) between individual generated KPs and the reference KPs, redundancy is customized for evaluating RD among generated KPs. We presented our relevance evaluation prompt in Listing 4 and the redundancy evaluation prompt in Listing"
        },
        {
            "title": "H Dimensions of KP Quality Evaluation",
            "content": "This section provides detailed descriptions of tasks and dimensions involved in our manual evaluation of the KP textual quality. Annotators were asked to perform pairwise comparison between two sets of KPs, each taken from different model, generated for specific reviewed business entity considering specific dimension. The annotators must answer comparative question with respect to the evaluating dimension. (e.g., Which of the two summaries captures better . . . ). For each dimension, following Friedman et al. (2021), we calculate the ranking using the Bradley-Terry model (Bradley and Terry, 1952), which predicts the probability of given participant winning paired comparison, based on previous paired comparison results of multiple participants, and thus allows ranking them. VALIDITY: The key point in the summary should be an understandable, well-written sentence representing an opinion of the users towards the question. This would filter out sentences such as Its rare these days to find that!. SENTIMENT: The key point in the summary should have clear sentiment towards the product being questioned (either positive or Listing 2: One-shot prompt (1 example) for prompting GPT-4-o-mini on KP Extraction from community answers. You will be provided with an opinionated question and multiple answers to that question, delimited by triple quotes. An opinionated question seek insights of user opinions that are based on personal use, feelings, preferences, judgments, or evaluations about product, and was taken from Question Answering dataset of product reviews. You were tasked to extract list of unique and concise key points from the list of answers to given opinionated question. Key points are short and high quality sentences that expresses the main claims/viewpoints of users answering the opinionated question Note that the final extracted list of key points must directly relevant and can answer the input opinionated question. Formally, you should perform the following steps: 1. In every answer from the list, extract all possible key point candidates. 2. From the extracted list of key point candidates, generate list of only general and nonoverlapping key points that are relevant and can answer the input opinionated question. Below are some fewshot examples: Questions: Can use these for running/working out? Do they handle sweat? Answers: ['I have seen other people using these for running/working out. These are very comfortable in your ears for long hours. As long you clean them after working out, you should be fine. These are built to last long time.', 'I use them in the gym and on the stair climber machine. They are fine. Not sure about running but would think they would work ok.', \" don't know if I'll be any help, but I'll tell you about my experience nevertheless. used it everyday in the gym & while go for work on my bike inside my helmet. In both cases, the sweat doesn't seem to have any effect. Even during long rides, and when it rained heavily, the IE80 held up fine. The only issue you will have to worry about is the cable. Though the cables are good quality, rough usage may affect the balance in volume levels between the two channels. Though this doesn't affect the clarity, the balance can be disturbed. After year of really rough usage, the IE80 right volume was 12% lower than the left [I got mine replaced for free soon after]. But, this is an issue which affects every IEM, and nothing to sweat over, since we can replace the cables if necessary. So if you don't give it hard time, it should hold up fine.[I can't even count the times it has fallen down or swung down and taken hit against the gym equipment, or when my phone/DAP slipped and yanked the cable]\"] Key Points: ['Comfortable for long hours', 'Built to last long time', 'Suitable for gym and stair climber machine', 'Sweat resistant during workouts', 'Cables may be affected by rough usage'] Listing 3: Prompt for instruction-finetuning QQSUM-RAGs LLM for KP Summary Generation. Please refer to our released code for full prompts. You will be provided with question and JSON list of relevant review comments, delimited by triple quotes. The question asks the opinions of user reviews about product, and can be answered by the list of comment clusters in the provided JSON list. Each element in the JSON has been has been clustered to represent common opinion answering the question, accompanied by the quantity."
        },
        {
            "title": "You were tasked to generate a quantitative summary that covers all opinions captured in the JSON list in answering the",
            "content": "questions. Perform the following actions to solve this task: For every element in the JSON list, find the key point that represent the common opinion across the comments of the cluster Generate longform quantitative summary including all extracted key points and the cluster size, following the below template: 'While answering about [Question]: + [Cluster size] of comments believe that [Key Point 1] + [Cluster size] of comments believe that [Key Point 2] ...' Below are fundamental rules: + Larger cluster means higher support for the key point and with bigger cluster size, the quantity must be higher + Only use number to report the cluster size for each key point, avoiding vague terms (e.g., some, most) + Ensure that each key point extracted from cluster is distinctive and doesn't redundantly cover aspects mentioned in larger clusters Listing 4: Zero-shot prompt for G-EVAL relevancy evaluation between generated KPs and reference KPs, supporting sP/sR/sF1 calculation. You will be given one key point, short salient sentence, written to describe user opinion on product. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Relevance (15) selection of important content from the source. The summary should include only important information from the source document. Annotators were instructed to penalize summaries which contained redundancies and excess information. Evaluation Steps: 1. Read the key point and the source key point carefully. 2. Compare the key point to the source key point and identify the main points. 3. Assess how well the key point covers the main points of the source key point, and how much irrelevant or redundant information it contains. 4. Assign relevance score from 1 to 5. Listing 5: Zero-shot prompt for G-EVAL redundancy evaluation of generated KPs, supporting RD calculation. You will be given one key point, short salient sentence, written to describe user opinion on product. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Criteria: Redundancy (15) overlapping opinion with the source. The summary should not include semantically similar opinion with the source document. Annotators were instructed to penalize summaries which contained overlapping opinion with the source. Evaluation Steps: 1. Read the key point and the source key point carefully. 2. Compare the key point to the source key point and identify the main points. 3. Assess how much redundant opinion and information the key point covers that overlap with the source key point 4. Assign redundancy score from 1 to 5. negative). This would exclude sentences like came for company event. INFORMATIVENESS: The key point in the summary should discuss should discuss some aspects of the reviewed product and contain useful information. Any key point that is too specific or only expresses sentiment cannot be considered good candidate. SINGLEASPECT: The key point in the summary should not discuss multiple aspects (e.g., Decent price, respectable portions, good flavor). REDUNDANT: Each KP should express distinct aspect. In other words, there should be no overlap between the key points. COVERAGE: The summary, containing the set of key points, should cover wide diversity of opinions relevant and representative to the question. FAITHFULNESS: The key point in the summary should express reasonable and meaningful opinions relevant to the question raised on the product without hallucination. No conjecture or unfounded claims should arise."
        },
        {
            "title": "Annotation Guidelines",
            "content": "Below are the two summaries for product question in Tools_and_Home_Improvement, generated by two different summarization frameworks. Each summary contains several key points (i.e., salient points) generated summarizing the user opinions on different aspects. You are tasked to select which summary you think is better according to the below criteria. Question: Does this tester accurately test AA Lithium? The power drop off curve is so steep. It seems unlikely...but am hoping!. Criteria: REDUNDANCY. Each key point in the summary should express distinct aspect. In other words, there should be no overlap between the key points. Summary A: [the tester accurately tests various types of batteries, including AA Lithium, and provides accurate readings, there is uncertainty about the accuracy of the percentage of charge remaining for AA Lithium batteries, the tester does not test specific version of AA Lithium battery (L91), the tester is big and cumbersome, but effective in testing batteries under load, the tester requires four AA batteries to operate, the tester tests batteries by putting load on them, making the readings more accurate, the tester tests batteries quickly, with test taking only 3-4 seconds for AA battery, the tester is expensive but worth the investment due to its accuracy and ability to save money by testing old batteries, the tester tests batteries of various sizes, including AA, AAA, C] Summary B: [I have compared the testers results to battery powered devices and found it does give you the true useful state of battery. , Now that found this tester am happy, because it tests battery the way battery should be tested., That model also tests 6v lithium 2CR5 used in some older cameras, which the current tester does not since the times have moved on.] The options are: Summary Summary GPT4s Comment-KP Matching"
        },
        {
            "title": "Annotation against Human Judgement",
            "content": "To validate gpt-4-o-minis annotation performance and credibility, we conduct an experiment to measure LLM annotation judgement, as utilized for the KP-comment matching evaluation in our main experiment, in agreement with human (gold) preference. We sampled subset of 5 queries from the test set in our main experiment and hired workers to annotate the correctness of comment-KP pairs produced as the results of our frameworks quantification outcome. Note that these sampled pairs are part of the our main test set and have already been annotated for LLMs labels in our main experiment. For human annotation, we employed 6 MTurk crowd workers on every comment-KP pair, selecting only those with an 80% or higher approval rate and at least 10 approved tasks. Following BarHaim et al. (2021), we exclude annotators with Annotator-κ < 0 for quality control. This score averages all pairwise Cohens Kappa (Landis and Koch, 1977) for given annotator, for any annotator sharing at least 50 judgments with at least 5 other annotators. For labelling correct matches, at least 60% of the annotators had to agree that the match is correct, otherwise, it is incorrect. In this experiment, we measured the accuracy, and conducted Pearson correlation (r) test of gpt-4-o-minis annotation performance against human judgement, with results reported in Table 8. For test, we set the null hypothesis as gpt-4-o-minis and Mturk annotated labels are independent. From Table 8, we saw signficant small p-value, which indicates strong evidence against the null hypothesis. Importantly, we also recorded Spearmans rank correlation coefficient to be relatively closed to 1. This implies that there is statistically significant positive correlation between gpt-4-o-mini and Mturk annotated labels, which substantiates our decision of using gpt-4-o-mini for comment-KP matching evaluation. Pearson correlation (r) p_value Accuracy 0.647 5.342e-16 0.807 Table 8: Performance valiation of GPT4s comment-KP matching annotation against human judgement"
        },
        {
            "title": "Below are the match annotation guidelines for",
            "content": "(sentence, KP) pairs: In this task, you are presented with question on product, key point taken from the summary answering the question, and sentence taken from review of that product. You will be asked to answer the following question: \"Does the key point match, i.e, represent an opinion in the review sentence?\" review sentence might express opinions on multiple aspects. key point matches sentence if it captures the gist of the sentence, or is directly supported by point made in the sentence. The options are: Not At All Somewhat Not Well Somewhat Well Very Well Clustering Algorithm of KP-Oriented Retrieval in QQSUM-RAG To validate other clustering techniques, we have developed an additional baseline that employs either HDBSCAN (McInnes et al., 2017) or K-Means clustering algorithm for grouping similar comments by the Retriever, following our main experimental setup and configuration in Section 4.2.2. Better than K-Means, HDBSCAN can automatically detect the number of clusters without predefined parameters and is used in previous KPA work (Li et al., 2023). We compare the factual alignment of KP-comment pairs (measured by AlignScore) across clustering methods in Table 9, using our best model configuration (Contriever + Mistral): While both HDBSCAN and K-Means perform reasonably, they are consistently outperformed by our specialized clustering approach. More specifically, although HDBSCAN or K-Means achieves relatively comparable matching Precision with our clustering algorithm, our algorithm can capture comments more sufficiently (much higher Recall) than HDBSCAN and K-Means. This is mostly because our algorithm contains more tuneable clustering parameters and operations that are specifically optimized for the QQSUM problem. Ablation Study: Single-KP Generation vs KP Summary Generation in QQSUM-RAG We conducted an ablation study to evaluate the impact of KP Summary Generation on QQSUMRAG, with KP quality and KP-comment matching and factual consistency performance presented in Table 10 and 11 respectively. To this end, we configure QQSUM-RAG Single-KP, variant that generates one KP at time for each comment cluster formed by KP-oriented Retrieval. Overall, not including previously generated KPs as context, QQSUM-RAG Single-KP struggles to capture the truly representative opinion of the cluster, likely generating KPs with overlapping opinions, especially for comments containing multiple opinions. Example output of QQSUM-RAG and Baselines We report the example output of query-relevant comment clusters and KP summary produced by QQSUM-RAG in Table 12 and 13, and further compare top 5 key points, extracted from the summary of QQSUM-RAG and the baselines in Table 14. Overall, QQSUM-RAG stands out for generating KPs with minimal redundancy, higher informativeness, and better alignment with the query. KP-Comment Matching KP-Comment Factual Alignment R F1 QuantErr AlignScore Our proposed clustering algorithm HDBSCAN clustering algorithm K-Means clustering algorithm (n_clusters = 3) 0.869 0.507 0.424 Table 9: KP-Comment matching performance and factual consistency of generated summary between different clustering methods applied for KP-oriented Retrieval of QQSUM-RAG. The experiment was conducted with the Mistral configuration for QQSUM-RAG, proven to have superior performance than Vicuna from Table 3. 0.792 0.582 0.522 0.694 0.682 0.677 04.24 11.47 15.50 0.749 0.718 0.681 ROUGE BERTScore BLEURT G-Eval-4 R-1 R-2 R-L sP sR sF1 RD Rel sP sR sF1 RD Rel sP sR sF1 RD Rel QQSUM-RAG (Ours) + Mistral + Vicuna 0.256 0.222 0.061 0. 0.220 0.204 0.39 0.38 0.29 0.26 0.33 0.31 0.37 0.53 0.27 0. 0.51 0.49 0.41 0.39 0.46 0.44 0.49 0.54 0.45 0.41 4.52 4. 4.29 4.25 4.40 4.36 2.43 2.45 4.05 3.68 QQSUM-RAG Single-KP + Mistral + Vicuna 0.191 0.171 0.035 0.045 0.160 0.154 0.29 0.22 0.22 0.17 0.25 0. 0.48 0.57 0.22 0.20 0.48 0.48 0.39 0.38 0.43 0.42 0.62 0. 0.39 0.36 4.21 4.10 4.22 4.12 4.22 4.11 2.51 2.60 3.14 2. Table 10: KP-level textual quality evaluation of generated summary between full implementation of QQSUMRAG and without (w/o) KP Summary Generation. sP, sR and sF1 refer to Soft-Precision, Soft-Recall, and Soft-F1 respectively based on set-level evaluation method against reference KPs in gold answer. G-EVAL-4 asks GPT-4 to score summary from 1-5. KP-Comment Matching KP-Comment Factual Consistency F QuantErr AlignScore (cluster-level) AlignScore (retrieval-level) QQSUM-RAG (Ours) + Mistral + Vicuna 0.694 0. 0.869 0.684 0.792 0.602 QQSUM-RAG Single-KP + Mistral + Vicuna 0.640 0.598 0.520 0. 0.574 0.527 04.24 07.83 17.84 22.63 0.749 0.630 0.682 0.601 0.826 0. 0.741 0.660 Table 11: KP-Comment matching performance and factual consistency of generated summary between full implementation of QQSUM-RAG and without (w/o) KP Summary Generation. Query How does this Nikon 24-120mm F4 lens compared with the 24-70mm F2.8 as general walk around lense? QueryRelevant Comment Clusters Cluster1: like the 24-70 better but this lens is good all around and compact optic for everyday shooting. As has been said many times before: \"the best lens is the one you will use\", and know wouldn not use the 24-70mm F2.8 because its too heavy and bulky to take on backpacking/camping trips and when traveling abroad. This is the one lens which could replace 24-70 / 2.8, 70-200 2.8 VR II (up to some extent) for \"everyday\" use. . . . Cluster2: have an upcoming stay in Spain, and Im seriously considering taking this lens instead of my AF-S 24-70 because of its size and zoom range. My only complaint is the price tag: for lens that is overall rather mixed bag (depending on what youre looking for you might be very happy with it, or very disappointed) it is very expensive. The 24-120 has good reach, good image quality, not heavy, not that expensive for what it can do (constant f/4 in zoom is very respectable) and its also the only usable medium-telephoto FX zoom from Nikon with the VR technology. For 5x zoom to be able to compete with 3x zoom costing over $500 more(the Nikkor 24-70mm F2.8) should only mean that the 5x zoom is remarkable lens. . . . Cluster3: For one thing, 24 70 is know to have better quality than this one. The range from 70 to 120 is not as important as better overall quality. This is probably not the best lens to use for portraits because its just not fast enough (f-stop), but for travel, chasing you kids around, or any other every day shooting this lens is perfect. The biggest pro for the 24-70mm is the extra 1 stop of light, slightly quicker autofocus speed, and of course the corresponding softer bokeh due to the 1 stop aperture opening. . . . KP Summary While comparing the Nikon 24-120mm F4 lens with the 24-70mm F2.8 lens as general walk-around lens: + 135 of comments believe that the Nikon 24-120mm F4 lens is relatively lightweight and compact, making it easy to carry around and use for extended periods of time. + 11 of comments suggest that the 24-120mm F4 lens has longer zoom range and is more affordable than the 24-70mm F2.8. + 9 of comments prefer the 24-70mm F2.8 for its better image quality and faster aperture. . . . Table 12: Example output of query-relevant comment clusters and KP summary produced by QQSUM-RAG, given query, i.e., question, from AmazonQ&A. Comment clusters to particular KP are marked in the same color as the corresponding bullet in the summary. The relevant opinion in each comment that directly support the corresponding KP is italicized. Query: How does this Nikon 24-120mm F4 lens compared with the 24-70mm F2.8 as general walk around lense? Key Point The Nikon 24-120mm F4 lens is relatively lightweight and compact, making it easy to carry around and use for everyday shooting. like the 24-70 better but this lens is good all around and compact optic for everyday shooting. Prevalence Matching Comments The 24-120mm F4 lens has longer zoom range and is more affordable than the 24-70mm F2.8. 11 Prefer the 24-70mm F2.8 for its better image quality, faster aperture and better for wide shot. 9 As has been said many times before: \"the best lens is the one you will use\", and know wouldn not use the 24-70mm F2.8 because its too heavy and bulky to take on backpacking/camping trips and when traveling abroad. have an upcoming stay in Spain, and Im seriously considering taking this lens instead of my AF-S 24-70 because of its size and zoom range. My only complaint is the price tag: for lens that is overall rather mixed bag (depending on what youre looking for you might be very happy with it, or very disappointed) it is very expensive. For one thing, 24 70 is know to have better quality than this one. The range from 70 to 120 is not as important as better overall quality. Table 13: Top 3 key points mentioned in the KP summary produced by QQSUM-RAG for answering query from AMAZONKP. For each key point, we show the prevalence, i.e., number of matching comments (with similar aspects of the same cluster), and two top matching comments. The relevant opinion in each comment that directly support the corresponding KP is italicized. Query: How does this Nikon 24-120mm F4 lens compared with the 24-70mm F2.8 as general walk around lense? Contriever + Contriever + QQSUM-RAG RKPA-Base GPT-4-Turbo The 24-120 is finally The 24-120mm lens at stage where you offers good versatility can carry it around on and value for general your FX camera and use have no regrets. Contriever + PAKPA The 24-120 lens is preferred over the 24-70mm Nikkor F2.8. due to its lighter weight. (Retriever+ LLM)co-trained The 24-120mm f/4 offers more reach and versatility than the 2470mm f/2.8. The Nikon 24-120mm F4 lens is relatively lightweight and compact, making it easy to carry around and use for everyday shooting The 24-120mm F4 lens has longer zoom range and is more affordable than the 24-70mm F2.8. Prefer the 24-70mm its better F2.8 for image quality, faster aperture and better for wide shot. The 24-120mm F4 lens has good image quality, with sharpness and contrast that is comparable to the 24-70mm f/2.8 The 24-120mm F4 lens has good Vibration Reduction (VR) technology that helps camera to shake when taking handheld shots. reduce The 24-120mm f/4 is lighter and more affordable than the 2470mm f/2.8. The 24-70mm lens image has superior quality and performance Best 4+ star walkaround lens on the market. The 24-70mm f/2.8 is better lens overall. the 24-70mm lens is preferred for its optical superiority. The 24-70mm lens is highly recommended for wide shots. The 24-120mm f/4 is too heavy. The 24-120mm lens is more practical choice for everyday use. The Nikon 24-120 lens has good contrast compared to the Nikon 24-70 lens. The 24-120mm f/4 has image stabilization, which is significant advantage for handheld shots. The 24-120mm f/4 has image stabilization handheld shots. for N/A . . . If you want 4+ star walk-around lens that covers great range , this is the best on the market. The 24-70mm lens is more expensive but buy it if you need to shoot wide. briefly considered the 24-70, but the extra reach , vibration reduction, and lower price point sold me on this lens. N/A Table 14: Top 5 key points, extracted from the summary of QQSUM-RAG and the baselines, ranked by their prevalence on an example query from AMAZONKP. Overlapping opinions across KPs are highlighted red . KPs lacking of informativeness are highlighted yellow"
        }
    ],
    "affiliations": [
        "RMIT University, Australia"
    ]
}