{
    "paper_title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
    "authors": [
        "Shang Yang",
        "Junxian Guo",
        "Haotian Tang",
        "Qinghao Hu",
        "Guangxuan Xiao",
        "Jiaming Tang",
        "Yujun Lin",
        "Zhijian Liu",
        "Yao Lu",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 6 8 4 1 . 2 0 5 2 : r LSERVE: EFFICIENT LONG-SEQUENCE LLM SERVING WITH UNIFIED SPARSE ATTENTION Shang Yang * 1 Junxian Guo * 1 2 Haotian Tang 1 Qinghao Hu 1 Guangxuan Xiao 1 Jiaming Tang 1 Yujun Lin 1 Zhijian Liu 3 Yao Lu 3 Song Han 1 https://hanlab.mit.edu/projects/lserve ABSTRACT Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9 and decoding by 1.3-2.1 over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have dramatically transformed the field of artificial intelligence. With expanding context window lengths, LLMs now demonstrate remarkable performance across diverse long-sequence applications (Gemini Team, Google, 2024), including multi-turn conversations, long document analysis (Zhang et al., 2024b; Goyal & Durrett, 2020; Huang et al., 2021), multi-modal understanding (Xue et al., 2024a; Liu et al., 2024b; Lin et al., 2024a), and code completion (Li et al., 2023; Lozhkov et al., 2024). Many of these applications require processing hundreds of thousands of context tokens in real-world settings, presenting unique challenges. In particular, the demand for fast prefilling, or minimizing the time to the first token, and the burden on the decoding phase due to the large KV (key-value) caches necessary for such contexts, represent significant hurdles. Long-sequence LLMs are about more than just an extended context. The recently announced OpenAI o1 (OpenAI, 2024) demonstrates exceptional capabilities in complex reasoning tasks, such as deciphering, mathematics, and cod- * indicates equal contribution. Part of the work was done while Shang Yang and Haotian Tang are interning at NVIDIA. 1MIT 2SJTU 3NVIDIA. Figure 1: LServe is an efficient system for serving longsequence LLMs that leverages hybrid sparse attention. With the unification of different sparse patterns as well as KV cache quantization, LServe achieves significant speedups in both prefilling stage and decoding stage while also reducing the memory consumption. ing. These advancements are achieved through inferencetime scaling and the generation of extensive chains of thought (Wei et al., 2022). According to Qin et al. (2024), o1s internal reasoning process can extend to 20k tokens for mathematical problems, making it the first known longgeneration LLM. Contrary to the conventional belief that the prefilling stage dominates runtime in long-sequence LLMs, when Llama-3-8B (Dubey et al., 2024) is run using TensorRT-LLM (NVIDIA, 2023) with 256k input tokens and 20k output tokens (comparable to o1s reasoning trace length), the prefilling time is 116 seconds, while decoding takes 540 seconds almost 5 longer. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 2 To enhance the efficiency of long-sequence LLMs, it is essential to optimize both the prefilling and decoding stages rather than focusing on just one. Beyond model architectural modifications in the pre-training stage (Ainslie et al., 2023; Brandon et al., 2024), existing acceleration solutions for long-sequence LLMs primarily address efficiency from two angles. The first approach centers on KV cache quantization, where methods such as QServe (Lin et al., 2024b), KIVI (Liu et al., 2024c), and KVQuant (Hooper et al., 2024) employ low-bit quantization to reduce memory usage and I/O traffic, potentially increasing generation throughput. However, these quantization techniques do not lower the number of computations performed in the attention loop, resulting in suboptimal generation speeds as sequence lengths grow. The second approach utilizes approximate sparse attention to improve long-sequence LLM performance. For example, StreamingLLM (Xiao et al., 2023), H2O (Zhang et al., 2024c), and TOVA (Oren et al., 2024) apply static masking mechanisms to reduce attention complexity, though at the expense of accuracy in long-context tasks and irregular KV cache memory layouts. DuoAttention (Xiao et al., 2024) advances this strategy by pruning attention computations at coarser granularity using an optimization-based approach. Other methods, such as MInference (Jiang et al., 2024b) and Quest (Tang et al., 2024), implement dynamic sparse attention to accelerate either the prefilling or decoding stage. However, these approaches do not reduce KV cache memory consumption and lack unified framework to address efficiency challenges in both stages simultaneously. To this end, we introduce LServe, an efficient system for serving long-sequence LLMs that leverages hybrid sparse attention. Recognizing that not all tokens hold equal importance, LServe integrates multiple hardware-friendly, structured sparsity patterns into unified block sparse attention framework (see Figure 4). Block-level sparsity accelerates attention computation by processing the KV history in discrete blocks. By skipping blocks, we directly reduce the number of sequential iterations, resulting in measured speedups during both the prefilling and decoding stages. Building on the unified block sparse attention framework, LServe further illustrates acceleration opportunities from static and dynamic sparsity. For static sparsity, inspired by DuoAttention (Xiao et al., 2024), we modify the attention masks in the original model by converting half of the attention heads into Λ-shaped masks, transforming these attention heads into streaming heads. Additionally, we fuse the computation of streaming and standard attention heads into unified GPU kernels for both the prefilling and decoding stages, translating theoretical computation and memory savings that translate to up to 1.7 measured speedup. For dynamic sparsity, we observe that query-centric sparsity (Tang et al., 2024) allows for nearly lossless KV compression: the required number of KV tokens to maintain long-context capabilities remains constant (e.g., 4096), regardless of context length. To optimize efficiency, we design hierarchical page selector to identify important KV pages for each query token, reusing the selection results across tokens to reduce page selection overhead by 4. Our key observation is that static and dynamic sparsity patterns are orthogonal in long-sequence LLMs. By unifying static and dynamic sparsity with KV cache quantization into single GPU kernel, LServe achieves compounded efficiency benefits from each individual optimization for decoding stage attention. We benchmark LServe across three long-sequence LLMsLlama-3-8B, Minitron-4B, and Llama-2-7Bat context lengths up to 512k tokens. Compared to stateof-the-art frameworks like vLLM (Kwon et al., 2023b), QServe (Lin et al., 2024b), MInference (Jiang et al., 2024b), and DuoAttention (Xiao et al., 2024), LServe accelerates prefilling stage by up to 2.9 and achieves an average of 1.3-2.1 speedup in the decoding stage. Furthermore, LServe accomplishes these speedups while retaining the long-context capabilities of the original dense, floating-point models, demonstrating that hybrid attention sparsity is free lunch for long-sequence LLM serving."
        },
        {
            "title": "2 BACKGROUND AND MOTIVATION",
            "content": "2.1 Background LLM Inference. LLMs are transformer-based architectures with stacked identical layers, each containing attention blocks, feed-forward networks (FFN), and normalization components. LLM inference involves two stages: an initial prefilling stage that handles multiple tokens concurrently, followed by auto-regressive decoding stage where only one token will be processed for each request in decoding step. Attention. The attention mechanism exchanges information across tokens. It first transforms input through linear projections to generate query vectors RN HD, and key-value pairs k, RN ˆHD, where ˆH represents the key/value head count. Traditional multi-head attention (MHA) maintains = ˆH, and contemporary architectures (Touvron et al., 2023; Jiang et al., 2023; 2024a) employ grouped-query attention (GQA) (Ainslie et al., 2023) where = ˆH(n Z) to shrink the size of KV cache. The current and is then concatenated with KV cache from preceding tokens, yielding K, R(S+N ) ˆHD. The attention computation can then be formulated as follows: Sh = qhKT ˆh , oh = softmax (Sh) Vˆh, ˆh = (cid:23) (cid:22) (1) Therefore, the complexity of attention can be expressed LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention Figure 2: Latency breakdown of LLM inference for both prefilling and decoding stage. As sequence length increases, attention dominates both stages due to its quadratic complexity in prefilling stage and linear complexity during decoding stage. In contrast, GEMM exhibits linear complexity during prefilling stage and constant complexity during decoding stage. Latency numbers measured with Llama-3-8B on NVIDIA A100 GPU. as (N (S + )HD), which increases quadratically in the prefilling stage and linearly in the decoding stage with respect to sequence length. When is long, both decoding stage and prefilling stage are bounded by attention. Paged Attention. In LLM serving, the generation length of each sequence is highly variable and unpredictable. Padding all sequences to the maximum length results in considerable memory waste and fragmentation. To address this, vLLM (Kwon et al., 2023b) introduces PagedAttention, KV cache management algorithm inspired by operating systems virtual memory. Instead of allocating continuous memory buffer for each sequences KV cache, PagedAttention partitions the cache into fixed-size blocks (or pages), each holding KV data for set number of consecutive tokens (typically 16 to 64). page block table records the physical address of each page, allowing the PagedAttention kernel to use indirect addressing to retrieve KV features. TensorRT-LLM (NVIDIA, 2023) and QServe (Lin et al., 2024b) implement quantized page attention to reduce memory bandwidth usage during the decoding stage, resulting in further generation speedups. 2.2 Motivation Serving long-sequence LLMs is challenging due to the high cost of attention. Figure 2 profiles the latency breakdown of Llama-3-8B with batch size of 1 across various sequence lengths on the A100 GPU. In both the prefilling and decoding stages, attention kernels account for at least 50% of the runtime at sequence lengths over 64k, rising to 75% at 128k. According to QServe (Lin et al., 2024b), the ratio of attention kernels in end-to-end runtime will increase as the batch size scale up. Therefore, in real-world serving scenarios, optimizing the attention becomes increasingly critical. Accelerating attention in long-sequence LLMs requires deep understanding of attention kernel implementation on GPUs, as illustrated in Figure 3. During the prefilling stage, Figure 3: Attention calculation on GPUs: In both the decoding and prefilling stages, each query token iterates over all key and value tokens sequentially in block-byblock manner. Skipping KV blocks reduces the number of sequential iterations, directly accelerating attention. the attention kernel is parallelized across batch size, attention heads, and query tokens, with query tokens set to 1 in the decoding stage. In both stages, the computation along the KV token dimension remains sequential. In each iteration, block (depicted as grid with an orange contour in Figure 3) is computed collaboratively by all threads in the current thread block. Although skipping certain computation within each block is possible, it yields minimal speedup. This is due to the lockstep execution of threads within GPU warp, where faster threads are forced to wait for slower ones. That said, rather than focusing on sparsity within each iteration, more effective way to accelerate attention is to reduce the number of sequential iterations along the KV token dimension. This approach leads to our unified block sparse attention formulation, where attention computation is skipped in blockwise manner. In this scheme, aside from the most recent KV block, each block is either fully computed or entirely skipped during the prefilling stage. During decoding, each sequence contains only one query token, reducing the dimensionality of each orange-contoured grid to 1P , where represents the page size (i.e., the number of KV tokens per page). We will detail LServes sparsity pattern selection in Section 3. Additionally, because the decoding stage is memory-bound, KV cache quantization also contributes to speed improvements. Quantization is orthogonal to block sparsity, as it reduces the runtime of each iteration, while sparsity reduces the number of iterations."
        },
        {
            "title": "3 LSERVE: LONG-SEQUENCE SERVING\nWITH UNIFIED SPARSE ATTENTION",
            "content": "We introduce LServe, an efficient long-sequence LLM serving system featuring sparse attention. In LServe, diverse LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 4 as attention sinks (Xiao et al., 2023). Unlike dense attention, where computation for each row scales with the token index, streaming attention keeps the computation for each token constantin this case, only two local blocks and one sink block, as shown in Figure 4(c). This pattern is nearly costfree in applications with extremely long contexts. Because streaming attention follows fixed pattern, we designate which heads use it in offline, and make it static for different input sequences in both prefilling and decoding. The second type of sparsity, illustrated in Figure 4(d), is page sparsity, which is specifically designed for the decoding stage where TQ = 1 applies to both skipped and selected pages. Unlike streaming attention, page sparsity in LServe is dynamic, allowing different query tokens to attend to different KV pages. As noted in Deja Vu (Liu et al., 2023), dynamic sparsity results in higher compression ratios than static sparsity. Our observations indicate that static sparsity offers up to 2 efficiency gain, whereas dynamic sparsity bounds the decoding complexity to constant, with each query attending only to fixed number of KV tokens. 3.2 LServe System Overview We present an overview of LServe in Figure 5. Built on QServe, which natively supports quantized LLMs, LServe enhances the baseline system by introducing sparsity into both prefilling and decoding dataflows. The two-way paged KV cache serves as the bridge between these two stages. As discussed in Section 3.1, we statically partition the attention heads of pretrained LLM into two groups: dense heads and streaming heads. Unlike conventional LLM serving systems, which maintain single KV cache, we utilize separate KV caches for the dense and streaming heads. The KV cache for the streaming heads is organized similarly to the pages in QServe, with scaling factors and zero points stored immediately after the token features. Additionally, the KV cache for the dense heads includes key statistics that facilitate critical page selection during the decoding stage. In the prefilling stage, the key differences between LServe and conventional dense-attention LLM serving systems are twofold: (1) we replace the dense attention kernel with our unified block sparse attention kernel, and (2) we write back quantized KV features using two distinct kernels. In the decoding stage, our system incorporates dynamic attention sparsity. Rather than developing an entirely new dynamic sparse attention kernel, we decompose the problem into two components: (1) dynamic page selection and (2) dense attention kernel with shorter page tables, where the shorter page tables are provided by the page selector. Notably, our page selector employs hierarchical paging and reusable page selection, enhancing both long-context accuracy and page selection efficiency. Figure 4: Unified block sparse attention pattern. LServe integrates various sparsity patterns into unified framework. sparse attention patterns are unified within block-sparse formulation (Figure 4), and are flexibly supported through fused CUDA kernels. LServe also supports weight, activation and KV quantization, which significantly improves generation throughput at shorter context lengths. 3.1 Unified Block Sparse Attention As shown in Figure 3, skipping computations in the attention kernel by blockwise processing accelerates execution by shortening the sequential loop. Building on this, we introduce unified block sparse attention pattern for both the prefilling and decoding stages: each thread block computes TQ TK tile (and TK TV ) in parallel. Here, TQ > 1 in the prefilling stage and TQ = 1 in the decoding stage, with TK (or TV ) corresponding to the page size in PagedAttention (Kwon et al., 2023a). We define block sparsity in LServe as follows: for each TQ TK tile in the attention calculation, it is either fully skipped (Figure 4(b), light gray blocks) or retained as in standard causal attention (Figure 4(b), blue blocks). Given that each GPU streaming multiprocessor can execute only limited number of thread blocks simultaneously, the attention kernel execution time can be approximated by the total count of TQ TK (and TK TV ) blocks. With block sparsity of r, where rN of the total blocks are empty, the theoretical speedup from block sparse attention is 1/(1 r). For example in Figure 4(b), 10 out of =21 blocks are non-empty. Thus, the theoretical speedup ratio is 2.1. Figure 4(c)(d) shows two sparsity patterns used in LServe. The first is streaming attention (Figure 4(c)), specialized form of block-sparse attention where each token only attends to its immediate neighbors and initial tokens, known LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention Figure 5: LServe system overview. In prefilling stage, LServe processes both dense heads and streaming heads within fused sparse attention kernel. Past Keys and Values are stored in two separate paging systems: one for streaming heads and the other for dense heads. In decoding stage, LServe applies dynamic sparsity on dense heads with page selection procedure. Only selected KV Pages will be loaded for the decoding stage attention. We omit normalization layers and residual connections in this figure for the sake of simplicity. 3.3 Prefilling Stage: Sparsity Determination We adopt the approach from DuoAttention (Xiao et al., 2024) to classify each attention head as either retrieval head or streaming head. Using DuoAttentions optimization-based identification method, we obtain gating value α [0, 1] for each head, where values closer to 1 signify retrieval head, and values closer to 0 indicate streaming head. To classify head as retrieval head, we compare α to threshold τ , determined by sparsity quantile. For instance, with target sparsity of 50% across attention heads, τ equals the median of all gate values, thereby designating half of the heads as retrieval heads. 3.4 Prefilling Stage: Kernel Implementation To effectively translate sparsity into performance gains, it is essential to avoid iterating over complete sequential loop and relying on conditional statements to determine data loading and computation requirements. This method is inefficient for GPU computation patterns, which thrive on minimizing branching within loops. Instead, we should focus on iterating only over the necessary blocks by accurately calculating offsets to load data and assess whether block should be processed. To facilitate this, we introduce an iterator-based abstraction that standardizes indexing operations. This allows us to loop exclusively over the blocks requiring computation, with data offsets easily computed using offset = iter(i + 1) iter(i). This abstraction efficiently skips unnecessary blocks with minimal overhead and necessitates few changes to the kernel function, thus enhancing maintainability. Take the streaming heads as an example, the iterators are determined outside the attention kernel since streaming heads are configured offline and the attention pattern is fixed. Once the attention on sink tokens is complete, the iterator automatically updates the memory pointer to the first local token in the KV cache with minimal overhead. Additionally, our iterator-based formulation unifies the more general block sparse pattern (see Figure 4). 3.5 Decoding Stage: Sparsity Determination To further enhance the long-context LLM decoding throughput, we introduce dynamic sparsity upon the input-agnostic static sparsity in Sec. 3.1. 3.5.1 Challenge: the Page Size Dilemma In the decoding stage, the attention operation is memorybound, so state-of-the-art systems typically implement KV cache quantization to reduce device memory usage and enhance throughput. However, this quantization introduces challenges for further optimization. Specifically, reducing the bit-width of KV tokens necessitates larger page sizes to maintain GPU memory bandwidth utilization. Failure to do so can lead to significant throughput loss  (Table 1)  . Yet, LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 6 Table 1: Page size significantly impacts the LLM serving systems efficiency: Larger page size is more hardwarefriendly as it improves contiguity of memory layout and the GPU bandwidth utilization during attention computation. For example, simply shrinking the page size in QServe (Lin et al., 2024b) leads to prominent slow-down of the end-toend system. We evaluate the per-step decoding latency (ms / step) of QServe on single A100 GPU for demonstration. We use Llama3-8B model architecture, with the batch size of 32. Seq len 512 1024 2048 4096 8192 Page Size 32 64 128 11.0 ms 13.8 ms 22.1 ms 35.7 ms 77.1 ms 10.7 ms 13.0 ms 20.1 ms 31.6 ms 63.0 ms 10.5 ms 12.7 ms 18.3 ms 28.1 ms 51.0 ms 10.5 ms 12.7 ms 18.2 ms 28.1 ms 50.6 ms Max Slowdown 1.52 1.25 1.01 1.00 page will collectively contribute to the same criticality estimator. In LServe, we utilize the channel-wise minimum and maximum values of keys in the same logical page as its representative vectors, which has been proven to be an effective metric (Tang et al., 2024) for page importance estimation with moderate page size ( 16). The current query will attend to representative vectors of each logical page to calculate the corresponding importance score as follows: Sj = (cid:88) (cid:16) max q[i] kj max[i], q[i] kj min[i] (cid:17) (2) where is the importance score of logical page, {a, b, ...} is the index of logical page, is the channel index, and refers to the head dimension. The importance of each physical page is determined by the max-reduction over the importance scores of its corresponding logical pages. Finally, LServe selects the top-K physical pages (based on the predefined token budget) with highest importance scores as the input of sparse attention kernel. Figure 6: We evaluate the Llama-3-8B model with the Needle-in-a-Haystack (NIAH) (Kamradt, 2024) benchmarks. The effectiveness of query-aware page selection algorithms (e.g., Quest (Tang et al., 2024)) gets impaired when the KV page granularity grows (b,c,d). Naively scaling up the page sizes will lead to significant performance loss even if we linearly increase the number of selected pages (token budget) (e,f). larger KV page sizes complicate the sparsification process; for example, Quest (Tang et al., 2024), which estimates token criticality using page-wise statistics, fails when page sizes increase (Figure 6). This observation poses challenges to balance between accuracy and efficiency. 3.5.2 Hierarchical Paging: Mitigating the accuracy-efficiency tradeoff We observe that the failure of query-aware KV cache selection paradigm (Figure 6) is not due to the coarser granularity of sparse attention (i.e., larger page size). Rather, the underlying cause lies in that page-wise statistical indicators become homogenized and less representative especially when there are excessive tokens within single page. To address this issue, we design simple-yet-effective hierarchical paging system that introduces an abstract layer of virtual logical page for estimating token criticality, while preserving the original memory layout of KV cache in (physical pages). As illustrated in Figure 7, our hierarchical paging groups NL tokens into logical page and NP tokens into physical page (NP = NL, Z), that is, physical page contains logical pages. Tokens within the same logical 3.5.3 Reducing sparse attention overheads with locality One remaining question is: as physical page size increases, will the hierarchical paging require higher token budget for sparse attention to retain accuracy? Given generation step, assume the most important history tokens are distributed in logical page set = {P (i, j)}, where {1, 2, ...}, {a, b, , ...} are the physical and logical index of page accordingly. If these important tokens are randomly and sparsely distributed in the context, chances are that all logical pages in are scattered in different physical pages, that is, for any P1, P2 P, i1 = i2. In this case, all physical pages (P NP tokens) are selected to avoid losing important information. However, the LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 7 Figure 7: Hierarchical Paging in LServe system. We assume the each physical page contains Np = 8 tokens and each logical page has Nl = 4 tokens. The kmax and kmin vectors are concatenated to the end of each physical page, and are pre-computed during the context stage and previous decoding steps. The importance of each physical page is decided by the max of the importance scores of the logical pages it contains. We omitted KV quantization in this figure for the sake of simplicity. naıve paging only needs to keep NL tokens since it directly shrinks page sizes to smaller granularity (e.g., NL). Consequently, our hierarchical paging may suffer from decrease in attention sparsity by NP /NL. Fortunately, the semantic continuity of natural language endows the attention operation with intrinsic locality, allowing LServe to maintain consistent sparse attention token budget for larger physical page sizes. During the decoding stage, the coherence of contextual tokens makes the current query token incline to attend to consecutive pages in the KV cache. As result, logical pages with highest importance scores tend to cluster within similar physical pages. This kind of spatial locality effectively alleviates the need for increased token budget, thereby reducing the overhead caused by the contradiction between quantization and sparse attention. Experimental results in Figure 13 further affirm that our hierarchical paging well preserves the model accuracy even with the same token budget as the vanilla page selector with smaller page sizes. Moreover, the attention mechanism in decoding stage also exhibits the temporal locality: adjacent query tokens also heavily attend to similar historical pages. And there is no need for queries at consecutive decoding steps to select salient pages independently. Instead, the page selection decision can be shared across queries, aligning with the block-sparse attention formulation illustrated in Figure 4(d). To this end, we present Reusable Page Selection in LServe. Figure 8: We introduce Reusable Page Selector in LServe, which utilize the similarity of queries of consecutive tokens to cut down the selector overhead. The chunk size of reusable selector is set to 2 in this figure for the demonstration purpose. As in Figure 8, we activate the page selector only at the very beginning of pre-defined chunks. For the consecutive tokens within the same chunk, we reuse the page selection results from the first token of the chunk. Utilizing the temporal sparsity of attention, reusable page selection substantially improves the long-context generation speed by great margin without sacrificing accuracy. As demonstrated in Figure 14, even though dynamic sparse attention effectively restrict the complexity of decoding attention, the latency of page selector increases linearly with regard to the sequence length. When the number of history tokens surpasses 64K, the naıve page selector becomes the bottleneck to system efficiency, whereas our reusable page selector significantly alleviates this problem. 3.6 Decoding Stage: Kernel Implementation During the decoding stage, attention heads are processed in parallel on GPU, enabling different sparsity patterns to be applied independently on each head. This flexibility enables some heads to operate with page-level sparsity while others follow the streaming computation pattern. To leverage this, we employ two-level indexing hierarchy to unify the operations for streaming heads and dense heads with dynamic sparsity. Specifically, the low-level (physical) index corresponds to the iteration step of current GPU thread, which executes in consecutive manner as in dense attention, while logical index denotes the actual position of the target token within the entire KV cache. For each dense head, the page selector provides an index table to map physical index to logical index. Streaming heads are treated as dynamic sparse heads with index table only containing the sink and local pages. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 8 Table 2: Accuracy evaluation on LongBench (Bai et al., 2023). We compare our method with vanilla dense attention on 2 models and 10 different benchmarks. Model Llama-3-8B Llama-2-7B Benchmark Dense LServe Dense LServe 2WikiMQA DuReader HotpotQA MultiNews Qasper QMSum SamSum TriviaQA Average 30.3 30.3 41.7 27.7 31.7 23.8 41.2 84.9 38.9 31.6 30.8 42.7 27.7 29.3 24.0 39.3 83.7 38.6 35.4 25.4 47.4 26.6 32.6 21.0 41.8 86. 39.5 35.1 24.7 49.6 26.6 29.5 21.3 41.5 86.5 39.4 Figure 9: Accuracy evaluation on Needle-in-a-Haystack."
        },
        {
            "title": "4 EVALUATION",
            "content": "4.1 Evaluation Setup Implementation. We implement LServe in CUDA and PTX assembly on the basis of QServe (Lin et al., 2024b) and TensorRT-LLM (NVIDIA, 2023) system. The specialized CUDA kernels are compiled into PyTorch extensions for better flexibility and compatibility with the purely PyTorchbased serving interface. Testbed. Our primary experiments are conducted on server equipped with 8 NVIDIA A100 80GB GPUs, 2 AMD EPYC 7763 CPUs (128 cores), and 2TB of memory. Unless explicitly stated, all experiments utilize the A100 GPUs. Additionally, we perform some evaluations on cloud instance with single NVIDIA L40S 48GB GPU to assess system performance across different GPU architectures. All evaluations use PyTorch 2.5.0 with CUDA 12.4 and cuDNN 9.2.0. Models. To comprehensively assess system performance across various LLM architectures, we utilize the widely adopted GQA-based model Llama-3-8B (Dubey et al., 2024), the MHA-based model Llama-2-7B (Touvron et al., 2023), and the smaller-scale model Minitron-4B (Muralidharan et al., 2024). Additionally, to support long-context inference, we employ the context-extended Llama-3-8B version Gradient (Pekelis et al., 2024). Table 3: Accuracy evaluation on RULER (Hsieh et al., 2024). We evaluate the accuracy of Llama-3-8B on RULER benchmarks, including challenging tasks such as multi-hop tracing and aggregation to test behaviors beyond searching from context. LServe-N denotes that the token budget for dynamic sparsity is . Note that for long-context inputs, latency is not dominated by attention alone in LServe, with page selector and GEMM also contributing to it. Experiments reveal that LServe-8192 is only up to 6% slower than LServe-4096 when the sequence length exceeds 128K. Llama-3-8B 32K 64K 128K 160K 192K 256K Dense 90.5 86.8 LServe-4096 91. 85.6 LServe-8192 91.8 86.1 83.8 81. 81.7 79.3 79.0 81.2 79.6 76. 79.7 79.4 75.7 79.1 the prefilling stage, we use time-to-first-token (TTFT) as key metric, while for the decoding stage, we emphasize minimizing the per-token generation latency. Baselines. We consider the following systems as baselines, using their latest versions1 to ensure fair comparison. We activated W8A8 precision for baselines if available. vLLM (Kwon et al., 2023b), one of the most popular LLM serving systems featuring PagedAttention. QServe (Lin et al., 2024b), efficient LLM serving system featuring W4A8KV4 quantization. MInference (Jiang et al., 2024b), the state-of-the-art longcontext prefilling stage acceleration system. DuoAttention (Xiao et al., 2024), strong long-sequence LLM inference framework with static sparse attention. Additionally, we compare our approach with the state-of-theart long-context decoding stage acceleration system, Quest (Tang et al., 2024). Since Quest only supports MHA models, we conduct and discuss this comparison in Table 4. 4.2 End-to-end Accuracy We evaluate the accuracy of our hybrid block-sparse mechanism with LongBench (Bai et al., 2023) tasks, the Needlein-a-Haystack (NIAH) (Kamradt, 2024) pressure tests, as well as the challenging RULER (Hsieh et al., 2024) benchmarks. Table 2 compares the LongBench accuracy between LServe and dense baseline. Results show that LServe well preserves the performance of two models across different test sets. Figure 9 showcases the NIAH evaluation results of our system, where LServe also achieves the same level of accuracy compared to the dense baseline. In Table 3, we test LServe with RULER benchmarks. Unless otherwise specified, we convert half of the attention heads into streaming heads and keep token budget for dynamic sparsity to 4096 for the benchmarks. Metrics. Our primary focus is on serving throughput. For 1vLLM 0.6.3 LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention Figure 10: Decoding Speed Evaluation. The y-axis indicates the relative throughput of each system, normalized by the speed of LServe. Note that MInference exhibits limited decoding performance due to its unoptimized decoding stage with dense attention, but when integrated into vLLM, it can achieve throughput comparable to that of vLLM. Table 4: LServe achieves lower latency over Quest (Tang et al., 2024) system in both prefilling stage and decoding stage. We benchmark the two systems on Llama-2-7B model, since Quest does not support GQA (Ainslie et al., 2023) architecture. Figure 11: Prefilling Speed Evaluation. Performance comparison of long-sequence prefilling across different serving frameworks, normalized to LServes speed. 4.3 End-to-end Efficiency Decoding Efficiency. Figure 10 presents the efficiency benchmarking results for the decoding stage. We use the same sparsity configurations as in Section 4.2. Compared with the state-of-the-art serving systems, LServe demonstrates significant and consistent efficiency improvements across different GPU platforms and model architectures. On Llama-3-8B and Minitron-4B, LServe achieves 1.5 average speedup over vLLM. For MHA-based model Llama2-7B, LServe runs more than 2.0 faster than baselines on average. Additionally, we demonstrate that LServe also functions well on other GPU devices such as L40S with Ada Lovelace Architecture. LServe achieves up to 1.7 speedup over vLLM. Prefilling Efficiency. In Figure 11, we compare the prefilling speed of LServe against 4 baselines on Llama-3-8B and Stage System Sequence Length 4K 0.51 0. 8K 0.82 0.49 16K 1.62 1. 32K 3.61 2.32 64K OOM 5. Quest LServe Speedup 2.1 1.7 1.5 1.6 / Quest 13.13 13.58 14.08 14.86 OOM LServe 10. 10.29 10.22 10.24 11.54 Speedup 1.3 1.3 1.4 1.5 / Prefilling Latency (s) Decoding Latency (ms) Llama-2-7B. LServe maintains superior prefilling throughput across different sequence lengths. For instance, on Llama-2-7B, LServe achieves an average of 1.8 higher prefilling throughput over vLLM. LServe is also compatible with the prefilling dynamic sparsity in MInference, which we activated after 128K sequence length. 4.4 End-to-End Comparison with Quest We also compares our system against Quest (Tang et al., 2024) in Table 4. Across different sequence lengths, LServe consistently outperforms Quest in both prefilling (1.6-2.1 speedup) and decoding stages (1.3-1.5 speedup). LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 10 Figure 12: Prefilling Stage Attention Kernel Evaluation. Figure 13: Hierarchical paging enables LServe to preserve the long-context retrieval capabilities of the original model without increasing the key-value (KV) token budget. We use Llama-3-8B for the ablation."
        },
        {
            "title": "5 ANALYSIS",
            "content": "In this section, we present in-depth analysis on our design choices in the LServe system from both the accuracy and the efficiency perspective. We also scrutinize the sources of performance gains in Section 4. 5.1 Prefilling Stage Sparse Attention Kernel We benchmark the performance of our block sparse attention kernel for the prefilling stage in Figure 12. Compared with the implementation by MInference (Jiang et al., 2024b), our kernel consistently achieves 1.3 speedup at the same sparsity level. Oracle stands for the theoretical upper-bound speedup ratio: Latencyoracle = Latencydense (1 sparsity). 5.2 Effectiveness of Hierarchical Paging We use the Needle-in-a-Haystack (Kamradt, 2024) test to demonstrate that the hierarchical paging design effectively maintains the models long-context capability on larger page blocks without increasing the token budget. In contrast to the performance drop observed with increased page granularity in Figure 6, LServe leverages hierarchical page structure to decouple the pruning algorithms page granularity from the physical memory layout of the KV cache. This approach enables our sparse attention mechanism to remain both accurate and hardware-efficient. Figure 13 highlights this improvement: with page size of 64 and the same token budget, LServe achieves accuracy comparable to the baseline algorithm (Tang et al., 2024), which prunes history tokens at granularity of 16. Figure 14: Effect of Reusable Page Selection. The overhead of the dynamic page selector is significant, as its complexity increases linearly with input sequence length. Our Reusable Page Selection effectively mitigates this issue. The latency breakdown is evaluated on Llama-3-8B. Table 5: The reusable page selector in LServe preserves the models long-context accuracy while significantly reducing selection overhead by 4 with reuse interval of 4. We evaluate Llama-3-8B on RULER (Hsieh et al., 2024) at sequence length of 64K. LServe-N denotes that the token budget for dynamic sparsity is . Reuse Interval Dense 1 LServe-4096 LServe-8192 86. 86.8 86.2 86.1 2 85.6 85. 4 85.6 85.5 8 84.8 85. 16 83.2 84.8 5.3 Mitigating Page Selection Overhead Reusable Page Selection. During decoding, although the attention kernel maintains constant complexity due to capped number of historical KV tokens, the complexity of the page selector still scales linearly with sequence length. As illustrated in Figure 14, for sequence length of 128K and 4K token budget for sparse attention, the page selector (0.24 ms) is already twice as slow as the sparse attention kernel (0.12 ms). With our reusable page selector, however, LServe significantly reduces page selection overhead by factor of C, where is the reuse interval. We further show that LServe is resilient to different reuse interval choices. Table 5 demonstrates no significant performance degradation until the reuse interval exceeds 8, so we set it to 4 by default in LServe. Context Pooling Overhead. To enable page selection during decoding, we must calculate representative features using min-max pooling in the prefilling stage. It is important to note that single pooling kernel executes under 1 ms, while the entire prefilling stage completes in approximately 17 seconds with 128K context length. Consequently, the context pooling overhead is negligible. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention"
        },
        {
            "title": "6 RELATED WORK",
            "content": "LLM Serving Systems. Various systems have been developed to enhance LLM deployment efficiency. Orca (Yu et al., 2022) uses iteration-level scheduling and selective batching for distributed systems. vLLM (Kwon et al., 2023b) introduces PagedAttention, inspired by virtual memory, to optimize KV cache management. TensorRT-LLM (NVIDIA, 2023) is the industrys leading solution also featuring inflight batching and PagedAttention inspired by vLLM. LightLLM (Contributors, 2023a) further reduces memory waste in PagedAttention by introducing TokenAttention. SGLang (Zheng et al., 2023) advances LLM programming with domain-specific language and RadixAttention. LMDeploy (Contributors, 2023b) improves deployment with persistent batching and blocked KV cache. Nanoflow (Zhu et al., 2024) features intra-device scheduling and asynchronous CPU scheduling, while QServe (Lin et al., 2024b) improves LLM serving throughput through W4A8KV4 quantization and system codesign. MLC-LLM (team, 2023) accelerates deployment on edge devices via compiler-based optimizations. Inspired by contextual sparsity (Liu et al., 2023), PowerInfer (Song et al., 2023; Xue et al., 2024b) deploys LLMs on memory-constrained devices via offloading. Sparse Attention. BigBird (Zaheer et al., 2020) reduces attention complexity by blending local, global, and random attention masks. Subsequent methods like StreamingLLM (Xiao et al., 2023), H2O (Zhang et al., 2024c), and TOVA (Oren et al., 2024) simplify attention patterns by discarding KV caches mid-way through the context. However, these approaches struggle to retain the original models long-context capabilities due to limited global context modeling. Recent works like DuoAttention (Xiao et al., 2024), RetrievalAttention (Liu et al., 2024a), and SeerAttention (Gao et al., 2024) address this issue by introducing retrieval heads (Wu et al., 2024) or combining full attention with local attention heads. Quest (Tang et al., 2024) introduces dynamic, query-aware sparsity for accelerated decoding, while MInference (Jiang et al., 2024b) extends similar ideas to the prefilling stage. FastGen (Ge et al., 2023) optimizes decoding by profiling attention heads to discard tokens. PQCache (Zhang et al., 2024a) and ShadowKV (Sun et al., 2024) further advance the selective attention methods with product quantization and low-rank decomposition. Additionally, LongLoRA (Chen et al., 2023) finetunes shortcontext LLMs to long-context ones after converting global attention to shifted sparse attention."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduce LServe, an efficient serving system for longsequence LLMs that leverages hybrid sparse attention. By incorporating unified block sparse attention, we achieve significant acceleration of the attention mechanism for both Figure 15: Efficiency gains from static and dynamic sparsity in LServe. These sparsity patterns contribute to compound speedup effect, with static sparsity being more effective at shorter contexts, and dynamic sparsity offering greater benefits at longer contexts. We report the latency of single attention layer in Llama-2-7B. Figure 16: End-to-end speedup breakdown in LServe: Consistent with findings from attention layer analysis, static sparsity (50% streaming heads) yields greater benefits at shorter context lengths. In contrast, dynamic sparsity achieves up to 4.5 end-to-end speedup for longer sequences. Results are based on measurements using Llama3-8B with unit batch size. 5.4 Sparse Attention Kernel for Decoding Stage We analyze the effectiveness of different sparsity patterns in decoding attention. In Figure 15, we apply static sparsity by converting 50% of attention heads to streaming heads, achieving 1.3-1.7 speedup across various input sequence lengths. Additionally, we introduce dynamic sparsity with fixed KV budget of 4096 tokens, which bounds the computational complexity of decoding attention to constant, delivering 30 speedup over the dense baseline for an input length of 256K. Although sparsity selection introduces minor overhead for shorter sequences, this is mitigated by reusable page selection. Additionally, we also perform the end-to-end ablation study in Section 5.5. 5.5 End-to-End Speedup Breakdown In Figure 16, we highlight the sources of performance improvement in LServe. By leveraging static sparsity, LServe achieves end-to-end speedups of up to 1.7 over the dense baseline. Additionally, dynamic sparsity, aided by reusable page selector, significantly reduces generation latency, yielding 7.7 speedup for sequence lengths of 256K. Lastly, LServe configures sparse patterns through offline profiling, effectively avoiding slowdowns from dynamic sparsity at shorter context lengths. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 12 prefilling and decoding stages in long-sequence models. We further show that head-level static sparsity and query-aware dynamic sparsity are orthogonal and can be effectively combined with minimal impact on accuracy. LServe surpasses state-of-the-art systems, delivering an average of 1.3-2.1 speedup in the decoding stage and up to 2.9 speedup in the prefilling stage, preserving the models long-context capabilities."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank MIT-IBM Watson AI Lab, MIT AI Hardware Program, MIT Amazon Science Hub, National Science Foundation, and Hyundai for supporting this research. We also thank June Yang, Bo Li, and Kaiyu Xie for their helpful discussions."
        },
        {
            "title": "REFERENCES",
            "content": "Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Brandon, W., Mishra, M., Nrusimha, A., Panda, R., Reducing transformer key-value and Kelly, J. R. cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. Contributors, L. Lightllm: light and fast inference service for llm. https://github.com/ModelTC/lightllm, 2023a. Contributors, L. Lmdeploy: toolkit for compressing, deploying, and serving llm. https://github.com/Int ernLM/lmdeploy, 2023b. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gao, Y., Zeng, Z., Du, D., Cao, S., So, H. K.-H., Cao, T., Yang, F., and Yang, M. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. Gemini Team, Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Goyal, T. and Durrett, G. Evaluating factuality in generation with dependency-level entailment. arXiv preprint arXiv:2010.05478, 2020. Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami, A. Kvquant: Towards 10 million context length llm inarXiv preprint ference with kv cache quantization. arXiv:2401.18079, 2024. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 13 Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a. Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024b. Kamradt, G. Llmtest needleinahaystack: Doing simple retrieval from llm models at various context lengths to measure accuracy. https://github.com/gkamradt/LL MTest NeedleInAHaystack, 2024. Accessed: 2024-0523. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023a. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023b. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., and Han, S. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26689 26699, 2024a. Lin, Y., Tang, H., Yang, S., Zhang, Z., Xiao, G., Gan, C., and Han, S. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. Liu, D., Chen, M., Lu, B., Jiang, H., Han, Z., Zhang, Q., Chen, Q., Zhang, C., Ding, B., Zhang, K., et al. Retrievalattention: Accelerating long-context llm inference via vector retrieval. arXiv preprint arXiv:2409.10516, 2024a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 2213722176. PMLR, 2023. Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., and Hu, X. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024c. Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Muralidharan, S., Sreenivas, S. T., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J., and Molchanov, P. Compact language models via pruning and knowledge distillation. CoRR, format/2407.14679, 2024. URL https://arxiv.org/abs/2407.14679. NVIDIA. TensorRT-LLM: TensorRT Toolbox for Optimized Large Language Model Inference, 2023. URL https://github.com/NVIDIA/TensorRT-LLM. OpenAI. Introducing openai o1, 2024. URL https://op enai.com/o1/. Oren, M., Hassid, M., Adi, Y., and Schwartz, R. Transformers are multi-state rnns. arXiv preprint arXiv:2401.06104, 2024. Pekelis, L., Feil, M., Moret, F., Huang, M., and Peng, T. Llama 3 gradient: series of long context models, 2024. URL https://gradient.ai/blog/scaling-rotatio nal-embeddings-for-long-context-language-mod els. Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., and Liu, P. O1 Replication Journey: Strategic Progress Report Part 1 . arXiv preprint arXiv:2410.18982, 2024. Song, Y., Mi, Z., Xie, H., and Chen, H. Powerinfer: Fast large language model serving with consumer-grade gpu. arXiv preprint arXiv:2312.12456, 2023. LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention 14 Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Zhang, H., Ji, X., Chen, Y., Fu, F., Miao, X., Nie, X., Chen, W., and Cui, B. Pqcache: Product quantization-based kvcache for long context llm inference. arXiv preprint arXiv:2407.12820, 2024a. Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:3957, 2024b. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024c. Zheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. Efficiently programming large language models using sglang, 2023. Zhu, K., Zhao, Y., Zhao, L., Zuo, G., Gu, Y., Xie, D., Gao, Y., Xu, Q., Tang, T., Ye, Z., et al. Nanoflow: Towards optimal large language model serving throughput. arXiv preprint arXiv:2408.12757, 2024. Sun, H., Chang, L.-W., Bao, W., Zheng, S., Zheng, N., Liu, X., Dong, H., Chi, Y., and Chen, B. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference. arXiv preprint arXiv:2410.21465, 2024. Tang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han, S. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. team, M. MLC-LLM, 2023. URL https://github.com /mlc-ai/mlc-llm. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, W., Wang, Y., Xiao, G., Peng, H., and Fu, Y. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. Xue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y., Tang, H., Yang, S., Liu, Z., et al. Longvila: Scaling longcontext visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024a. Xue, Z., Song, Y., Mi, Z., Chen, L., Xia, Y., and Chen, H. Powerinfer-2: Fast large language model inference on smartphone. arXiv preprint arXiv:2406.06282, 2024b. Yu, G.-I., Jeong, J. S., Kim, G.-W., Kim, S., and Chun, B.- G. Orca: distributed serving system for TransformerBased generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 521538, Carlsbad, CA, July 2022. USENIX Association. ISBN 978-1-939133-28-1. URL https: //www.usenix.org/conference/osdi22/presentat ion/yu. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q.,"
        }
    ],
    "affiliations": [
        "Google",
        "MIT"
    ]
}