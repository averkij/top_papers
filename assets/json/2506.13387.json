{
    "paper_title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with Language Descriptions and Scale-Oriented Contrast",
    "authors": [
        "Beilei Cui",
        "Yiming Huang",
        "Long Bai",
        "Hongliang Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 7 8 3 3 1 . 6 0 5 2 : r TR2M: Transferring Monocular Relative Depth to Metric Depth with Language Descriptions and Scale-Oriented Contrast Beilei Cui1, Yiming Huang1, Long Bai1, Hongliang Ren1 1The Chinese University of Hong Kong, Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "This work presents generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with cross-modality attention module to better capture scale information. strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scaleoriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2Ms great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. Code is available at https://github. com/BeileiCui/TR2M."
        },
        {
            "title": "Introduction",
            "content": "Monocular depth estimation (MDE) plays crucial role in comprehending the 3D layout of environments depicted in 2D images and finds numerous applications in robotics [13, 57], autonomous driving [42, 9], endoscopic surgery [27, 38] and virtual reality technologies [16, 14]. Predicting dense depth map from single image is ill-posed without additional sensors because any points along the ray for pixel with different depth values can yield the same image coordinate. Recent researches on MDE mainly concentrate on two aspects: monocular metric depth estimation (MMDE) and monocular relative depth estimation (MRDE) as shown in Figure 1 (a) and (b). MMDE aims at predicting dense depth map in specific scale (often in meters). limitless range of outputs leads early works performance to commonly degrade much on test data with diverse scales [77]. Recent advancements have been made to rectify such situation by leveraging additional prior conditions such as camera information [22, 68] or scene information [6]. Alternatively, MRDE estimates the depth map in unified scale. The affine-invariant loss function enables the model to These authors contributed equally. Corresponding authors Preprint. Under review. Figure 1: Illustration of motivation. (a) Metric Depth Estimation typically restricts to single domain or relies on camera parameters or sensors to enhance generalization. (b) Relative Depth Estimation generalizes better but is ambiguous in scale. (c) We therefore seek to transfer generalizable relative depth to metric depth by pixel-wise rescaling maps given image and readily accessible text description, which succeeds in obtaining metric depth for various domains with one lightweight trainable architecture. (d) Examples of qualitative results. optimize on vast amounts of data making MRDE results in excellent generalization ability across various domains. However, the scale uncertainty of MRDE also makes it lack practical applicability where determined scale is needed, such as in robot planning and navigation. We consider whether we can address scale uncertainty for transferring relative depth to metric depth efficiently to construct an MMDE pipeline with as good generalization capabilities across different domains as MRDE methods. Language description has been proven to be effective as clue to predict the overall scale for scene because certain types of objects are correlated with certain scenes, e.g. Vehicles and houses are more likely to appear in outdoor scenes, leading to larger overall scale [72]. However, two major problems have affected the performance of previous language description based methods in predicting scene scales: 1) Large regions of erroneous prediction still occur in state-of-the-art relative depth models therefore single factor for transforming relative depth map to metric scaled depth map will keep or even magnify such inaccuracy; 2) Same objects with different distributions and proportions results in inconsistent overall depth, but the language descriptions may be the same or very similar. Meanwhile, the scale consistency at the feature level has been overlooked in previous studies. Pixels with similar depth and scale may have low similarity in pixel-level features, leading to instability of overall depth estimation. In light of the above observations, we develop framework to predict pixel-wise transformation parameters to Transfer monocular Relative depth map to Metric depth map based on the image and corresponding text description termed TR2M as shown in Figure 1 (c). The image itself serves as input for the depth model, while the language description is readily available form of data that does not requires extra sensors or calibrations. Image and text feature embeddings are first acquired with separate frozen encoders. lightweight network with cross-modality attention takes two feature embeddings as inputs and predicts two maps functioning as scale map and shift map, separately. Then, the metric depth can be obtained by simple pixel-wise linear transformation based on scale and shift map, which is supervised by ground truth metric depth. For the continuity of scale and pixel consistency without ground truth supervision, we also obtain pseudo metric truth by aligning relative and ground truth metric depth and use threshold to select high-confidence pseudo depths as the supervision signal. We further innovatively propose Scale-Oriented Contrast to enhance the feature consistency with the same depth scale. Pixel-wise features are classified by their depth distribution and contrastive loss is applied to maximize the distance among features with different distributions and minimize the distance among features with the same distributions. We demonstrate that employing TR2M yields results similar to aligning relative depth through linear transformation with the ground truth, and it surpasses some SOTA metric depth estimation methods with fewer trainable parameters, offering improved generalization across diverse domains. We summarize our main contributions as follows: 2 1) We propose novel framework that leverages image and text descriptions to predict rescale maps to transfer monocular relative depth map to metric depth map across multiple domains. 2) We introduce Scale-Oriented Contrast Learning strategy by classifying depth based on its distribution to enforce global scale consistency. 3) Extensive experiments have demonstrated the superior performance of our proposed method on multiple scenes. To the best of our knowledge, we are the first to employ text description to perform pixel-wise transformation from relative to metric depth."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Monocular Depth Estimation Recent Monocular Depth Estimation (MDE) can be categorized into monocular metric depth estimation (MMDE) and monocular relative depth estimation (MRDE) methods. MMDE models learn to predict pixel-wise depth maps in metric scale supervised by ground truth metric depth [4, 5, 19, 58, 36]. Remarkable advances have been made in network architecture [43, 70, 32, 64, 70], geometry modeling [23, 75, 51, 67] and image priors [50, 41]. However, these methods typically restrict their setting to specific domains, resulting in performance degradation under domain shifts and limited generalization to unobserved environments. Recent studies have investigated MMDE models with improved generalization ability with the help of camera intrinsic-based modeling [1, 68, 25] or direct integration of camera intrinsic [22, 17]. These methods often need fine-tuning to adapt to specific domains or rely on particular sensors or cameras as prior knowledge. In contrast, by training on large-scale datasets with multifarious scales and domains accompanied by scale-invariant loss function [45, 47, 65, 66, 30], relative depth models have been proven to have better generalization across different scenes. As trade-off, the metric scale can not be recovered by relative depth models, which limits downstream applications to some extent. TR2M aims to break through such predicament by aligning highly generalizable relative depth into metric scale efficiently to construct universal metric depth estimation pipeline. 2.2 Language Involved Depth Estimation Vision-Language Models (VLM) [8, 34, 40, 44] develop holistic grasp of both textual and visual content by undergoing pre-training on varied datasets, thereby establishing robust foundation that enhances performance in subsequent downstream tasks [63, 69, 73, 79, 3]. Recently, some researchers have attempted to leverage the power of VLM for monocular depth estimation. DepthCLIP [74] utilizes CLIPs [44] semantic features combined with depth projection mechanism to enable zeroshot adaptation, translating semantic language understanding into monocular depth estimation tasks. Wordepth [71] utilizes the Variational Auto-Encoder (VAE) to learn the geometry distribution from the text description. Further advancements were made based on DepthCLIP by learnable prompts [26] or tokens [2]. ScaleDepth [78] designed bin and scale queries to estimate the relative depth and scale factor for metric depth. However, the designed correlations between text and depth are indirect and too scratchy to represent precise depth information. 2.3 Relative to Metric Depth Transfer With the advancement in relative depth estimation models in recent years, researchers have started to study the transfer of powerful RDE models to metric depth scale. Efforts have been made to utilize left-right stereo consistency [60], camera geometric embedding [22], or test-time adaptation [77] to capture scale information. Most state-of-the-art methods trains relative depth model on diverse datasets first and then fine-tune the encoder with metric head to specific metric datasets [46, 47, 65, 66, 68]. RSA [72] estimates scale factor and shift factor from text input to globally rescale relative depth to metric depth for different scenes. However, metric heads are limited to specific datasets, and complex environments may mislead text descriptions to inaccurate scale estimation. More direct and integrated relative to metric depth transformation methods are still under exploration. 3 2.4 Contrastive Learning for Depth Estimation Contrastive Learning has achieved substantial progress in learning feature representations for unsupervised learning tasks [10, 24, 11, 61]. Pixel-level contrastive learning emerges for dense prediction with or without labels as guidance [76, 62, 55, 29, 56]. WCL utilizes windows to form positive and negative pairs from depth feature maps for contrastive learning [18]. D4RD [35] developed robust depth estimation method by enforcing consistency among sample noises with different perturbations. Li et al. [35] models depth features with Gaussian distribution to conduct region-wise contrastive learning. Current contrastive learning methods for depth estimation are mainly unsupervised learning or supervised by other types of labels, without utilizing depth itself as supervisory signal."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview Figure 2 shows an overview framework of the proposed TR2M. Taking an RGB Image RHW 3 and its text description as input, TR2M aims to estimate two maps: scale map RHW and shift map RHW to rescale its corresponding relative depth Dr RHW to metric depth ˆDm RHW . Note that Dr RHW is generated by pre-trained relative depth model. Besides optimizing ˆDm with the ground truth depth map Dgt m, we also perform linear regression to find the scale factor α and shift factor β that minimizes pseudo rescaled metric depth Dpseudo m. If the quality of Dpseudo is good enough based on an evaluation threshold, it is also used as supervision for ˆDm. Furthermore, we also propose pixel-level scale-oriented contrast learning module to enforce feature consistency guided by depth distribution, thus improving the scale perception ability. with Dgt Section 3.2 introduces the scale and shift map estimation network. In addition, scale-oriented contrast learning is illustrated in Section 3.3. The loss functions and training strategies are in Section 3.4. 3.2 Scale and Shift Map Estimation Network Architecture. Relative depth models exhibit great generalization ability across diverse domains, but with unknown scales. Both images and textual descriptions can provide clues about scale, but single modality alone with loss of overall layout or average scale remains ambiguous. We endeavor to comprehensively utilize both image and text to learn more accurate pixel-level scale information as shown in Figure 2. To be specific, with the RGB image RHW 3 and text description as inputs, we first extract image feature FI RHW and text feature R1D separately with an frozen image encoder and an frozen text encoder. simple linear layer is applied on the text feature if the two channels and are different to obtain an aligned text feature FL R1D. To capture information effectively from both feature embeddings, we introduce cross-modality attention module as in formula 1. To maintain the pixel-level output, we make the image feature embedding query to access information from the key and value of the text feature embedding. Image feature embedding first applies self-attention and aggregates text information with skip connection, as in formula 2, to obtain the final feature embedding Fout RHW D. Attni cm (QI , Ki, Vi) = softmax (cid:19) (cid:18) QI i Vi, {I, L}, Fout = FI + AttnI cm + AttnL cm. (1) (2) Then, two lightweight decoder heads are applied to generate the output scale map RHW and shift map RHW with = Scalehead (Fout) , = Shifthead (Fout). We aim at transferring relative depth to metric depth, so we assume pretrained frozen RDE model exists to generate the relative depth map Dr RHW . The final metric depth can be obtained by rescaling Dr as below: ˆDm = 1 Dr + , 4 (3) Figure 2: Illustration of the proposed TR2M framework. Image and text embedding features are first obtained with separate frozen encoders, and cross-modality attention module is proposed to integrate them. The scale and shift maps are predicted with different decoders to transfer relative depth to metric depth. The scale-oriented contrast enables embedding features more consistent with the depth distribution, thus enhancing scale perception capability. where denotes element-wise multiplication. Scale-Invariant log loss [15] is utilized as followed: (cid:16) ˆDm, Dgt (cid:17) = Lsi 1 HW HW (cid:88) i= ϵ2 λ (HW )2 (cid:32)HW (cid:88) (cid:33)2 ϵi , i=1 (4) where ϵi = log ˆDm(i) logDgt m(i) and denotes the index of pixels. Pseudo Metric Depth with Threshold. The sparsity of ground truth metric depth results in partial pixels having no supervision to learn scale information. To conduct more comprehensive scale supervision, we leverage pseudo metric depth by aligning relative and metric depth on least-squares criterion. To be specific, let α, β R1 denote single estimators for scale and shift, respectively. Following [47], meaningful alignment could be made based on least-squares criterion as below: (α, β) = arg min α,β HW (cid:88) i=1 (cid:0)αDr(i) + β Dgt m(i)(cid:1)2 , (5) The aligned pseudo metric depth can be determined by: Dpseudo = αDr + β with the above α and β, which can be efficiently solved in closed form by rewriting formula 5 as least-squares problem (details are shown in the Appendix). We then use threshold accuracy δ1 (percentage of max (cid:0)Dgt is greater than preset threshold value, we consider Dpseudo as credible pseudo depth and use it for supervision, which can be written as: (cid:1) < 1.25) metric as condition. If δ1 of Dpseudo m/Dpseudo , Dpseudo /Dgt m Ltpsi (cid:16) ˆDm, Dpseudo (cid:17) = 1(δ1 > ρ) Lsi (cid:16) ˆDm, Dpseudo (cid:17) , (6) where 1() is indicator function, δ1 is the threshold accuracy of Dpseudo the preset threshold value. compared to Dgt and ρ is 3.3 Scale-Oriented Contrast As illustrated in Depth Anything [65], semantic encoders like DINOv2 [40] tend to output similar features for different areas of an object, like the front and back of car. In depth estimation, however, different areas or even adjacent pixels within those areas can have widely differing depths. Therefore, to enforce the model to be generalizable across various domains, the model should learn about 5 the intrinsic connection of depth values. Existing methods ignore capturing the depth distribution for overall spatial relationship understanding. We leverage the ground truth depth as labels to guide contrastive learning under the assumption that the embeddings of pixels with similar depth distributions should be closer than those with distant depth distributions. Given an image RHW 3 as query sample and set of key samples, we train the model by distinguishing the positive ones from the negative ones at the pixel level. Following [29, 24], we decouple the model into two symmetric encoder branches: regular branch that encodes query samples and is updated through back-propagation, and momentum branch that encodes key samples and is updated gradually via an exponential moving average (EMA) mechanism [28]. Each branch contains proposed rescale model encoder and projector head to map the feature Fout into lower dimension. Query and key samples are fed into different branch to predict query and key feature maps q, RhwDc and pixel-level feature embedding can be obtained with q(i) and k(j) for the i-th and j-th pixel, respectively. We convert the depth map into classification map based on the depth distribution, which is linearly distributed between minimum and maximum depth values. To be specific, consider classes of distribution, the classification map can be calculated by: = round( dmax dmax dmin C), (7) where is the depth map, dmin and dmax are the minimum and maximum depth value. The corresponding class map for query and key q, RhwC can then be obtained by resolution reduction and one-hot encoding. {F q(i), q(i)} represents query pixel and (cid:8)F k(j), k(j)(cid:9) represents key pixel j, separately. For query pixel i, we then compute binary selection mask: {0, 1} for each key feature map which is determined by the class: Mj = 1 (cid:2)Y q(i) = k(j)(cid:3), where 1() is indicator function. Consequently, k(j) is grouped into positive samples when two pixels have the same class, denoted as k(j)+, and grouped into negative sample otherwise, denoted as k(j). Our scale-oriented contrast is then formulated to maximize similarity scores for positive pairs and minimize them for negative pairs: Lsoc = 1 hw hw (cid:88) i=1 log exp (Sp exp (Sp ) ) + exp (Sn ) (cid:88) Sp = 1 Pi (cid:88) j+Pi (cid:10)F q(i) k(j)+(cid:11) , Sn = (cid:88) 1 i jN , (cid:10)F q(i) k(j)(cid:11) , (8) where is the cosine similarity function, Pi and Ni represent the sets of pixel embeddings corresponding to positive and negative samples for pixel i, respectively. The features are reorganized based on the depth distribution, enhancing the models generalization ability for scale perception. 3.4 Training strategies Besides the above-mentioned loss function, we also implement edge-aware smoothness loss [21] Les to enforce the smoothness property of scale and shift map. We train the framework in an end-to-end mode with combination of all optimization objectives: = λ1Lsi + λ2Ltpsi + λ3Lsoc + λ4Les. (9) More details about training settings are shown in the Appendix."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets Our primary datasets for training and evaluation for TR2M are NYUv2 [52], KITTI [20], VOID [59] and C3VD [7] ranging from outdoors, indoors to surgical scenes. To demonstrate generalization ability, we evaluate zero-shot performance on five real-world and synthetic datasets: SUN RGB-D [53], iBims-1 [31], HyperSim [49], DIODE Outdoors [54], and SimCol [48]. We followed [72] to generate text description with LLaVA v1.6 Vicuna and Mistral [37]. More details about the datasets are presented in the Appendix. 6 Table 1: Quantitative results on NYUv2. I, L, R, and refer to Image, Language, Relative, and Metric, denoting the input and output type of the method. \"Domain Adap\" denotes Domain Adaptation. \"FT\" means fine-tuned. \"Median\" denotes using median scaling for each image. \"Linear Fit\" refers to using ground truth to optimize scale and shift for each image. \"Global\" refers to optimizing single scale and shift for the entire dataset. Gray results denotes alignment with ground truth is used. denotes reproduced in our environment. The best results are in bold, and the second best are underlined. Scaling Domains Train Params Model Adabins [4] NeWCRFs [70] DA [65] DA V2 [66] ZeroDepth [22] ZoeDepth [6] Metric3Dv2 [25] DA [65] DA [65] DA [65] DepthCLIP [74] ScaleDepth [78] WorDepth [71] RSA [72] TR2M (Ours) Type I I I Domain Adap Domain Adap Domain Adap Domain Adap Domain Adap Domain Adap Domain Adap I I,L I,L I,L Median Linear Fit Global Zero-shot Domain Adap Domain Adap (I,L) Rescale factors (I,L) Rescale maps FT on Single FT on Single FT on Single FT on Single Multiple Multiple Multiple Multiple Multiple Multiple Multiple Multiple FT on Single Multiple Multiple 78M 270M 25M 25M - 345M 1011M - - - - 109M 137M 4.7M 19M δ1 0.903 0.922 0.957 0.969 0.926 0.951 0.980 0.480 0.969 0.630 0.394 0.913 0.926 0.752 0.954 δ2 0.984 0.992 0.995 0.996 0.986 0.994 0.997 0.734 0.995 0.926 0.683 0.989 0.990 0.964 0. δ3 AbsRel log10 RM SE 0.997 0.998 0.999 0.999 - 0.999 0.999 0.886 0.999 0.987 0.851 0.998 0.998 0.992 0.999 0.103 0.095 0.075 0.073 0.081 0.077 0. 0.353 0.055 0.199 0.388 0.099 0.090 0.156 0.082 0.044 0.041 0.033 0.032 - 0.033 0.030 0.135 0.024 0.087 0.156 0.041 0.040 0.071 0.035 0.364 0.344 0.273 0.261 0.338 0.282 0.260 1.743 0.260 0.646 1.167 0.329 0.330 0.528 0.293 Table 2: Quantitative results on KITTI. Details about notations can be found at 1. Scaling Domains Train Params Model Adabins [4] NeWCRFs [70] DA [65] DA V2 [66] ZeroDepth [22] ZoeDepth [6] Metric3Dv2 [25] DA [65] DA [65] DA [65] DepthCLIP [74] ScaleDepth [78] WorDepth [71] RSA [72] TR2M (Ours) Type I I I Domain Adap Domain Adap Domain Adap Domain Adap Domain Adap Domain Adap Domain Adap I I,L I,L I,L Median Linear Fit Global Zero-shot Domain Adap Domain Adap (I,L) Rescale factors (I,L) Rescale maps FT on Single FT on Single FT on Single FT on Single Multiple Multiple Multiple Multiple Multiple Multiple Multiple Multiple FT on Single Multiple Multiple 78M 270M 25M 25M - 345M 1011M - - - - 109M 137M 4.7M 19M δ1 0.964 0.974 0.971 0.973 0.910 0.971 0.977 0.925 0.944 0.663 0.465 0.968 0.965 0.786 0. δ2 0.995 0.997 0.996 0.997 0.980 0.996 0.996 0.986 0.991 0.932 0.713 0.996 0.996 0.967 0.996 δ3 AbsRel RM SElog RM SE 0.999 0.999 0.999 0.999 0.996 0.999 0. 0.996 0.998 0.981 0.867 0.999 0.999 0.995 0.999 0.058 0.052 0.054 0.053 0.102 0.054 0.051 0.091 0.077 0.191 0.343 0.058 0.066 0.147 0.066 0.088 0.079 0.083 0.081 0.172 0.082 0.080 0.129 0.122 0.228 0.305 0.086 0.088 0.179 0.093 2.360 2.129 2.290 2.235 4.044 2.281 2. 3.648 3.190 5.273 7.583 2.235 2.356 4.143 2.328 Evaluation Metrics Following [6, 72, 71, 65, 66], we evaluate our model in metric depth with commonly used metrics for depth estimation tasks, including AbsRel, RM SE, RM SElog, log10, δ1, δ2 and δ3 where the first four are lower the better while the last three are higher the better. Implementation Details All our experiments were conducted on NVIDIA RTX4090 GPUs. We use the AdamW [39] optimizer with batch size of 8. Learning rate is set to 1 105 with decay of 0.9 every epoch. We train our model for 20 epochs and the hyperparameters λ, λ1, λ2, λ3 and λ4 are set to 0.15, 1, 0.5, 0.1 and 0.01, respectively. Unless otherwise specified, we use Depth Anything-Small as our frozen relative depth model. We use the Vit-L/14 model from CLIP [44] and the ViT-L model from DINOv2 [40] as our frozen text and image encoder, respectively. 4.1 Comparison Results on Depth Transfer To enable depth transfer across various domains with one model, we train our TR2M on four datasets: NYUv2, KITTI, VOID and C3VD, which include outdoor, indoor, and surgical scenes. We compare our method with different types of SOTA methods to obtain metric depth. To be specific, Adabins [4], NeWCRFs [70], Metric3Dv2 [25], ZeroDepth [22], ZoeDepth [6] are monocular metric depth estimation methods. DepthAnything [65] (DA) and DepthAnything V2 [66] (DA V2) are monocular relative depth estimation methods fine-tuned with metric heads; DepthCLIP [74], ScaleDepth [78] and WorDepth [71] are SOTA language based metric depth estimation methods; RSA [72] is SOTA language based relative to metric depth transformation method; We reproduced some methods for fair comparison which are marked with in the tables. The results on NYUv2 and KITTI are shown in Table 1 and Table 2. Our method outperforms all the other language-based methods across all evaluation metrics and even obtained better δ2 and 7 Table 3: Zero-shot metric depth estimation. The first three test sets are indoor scenes; the fourth and fifth are outdoor scenes and the last one is surgical scene dataset. Note that ZoeDepth and DA Single (in blue ) use models trained on NYUv2 for indoor evaluation, models trained on KITTI for outdoor evaluation, and models trained on C3VD for surgical evaluation. RSA, DA Mix and our model utilize one model trained on multiple datasets for all zero-shot evaluations. Method Backbone SUN RGB-D iBimsHyperSim DIODE Outdoor SimCol AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 Rank ZoeDepth [22] DA Single [65] DA Mix [65] RSA [72] TR2M (Ours) BeiT384-Large ViT-Large ViT-Large ViT-Small ViT-Small 0.520 0.500 0.503 0.457 0.451 0.545 0.660 0.582 0.527 0.591 0.169 0.150 0.158 0.266 0.154 0.656 0.714 0.705 0.450 0.736 0.407 0.363 0.412 0.461 0.357 0.302 0.361 0.296 0.230 0. 0.814 0.794 0.842 0.852 0.673 0.237 0.288 0.181 0.244 0.274 0.372 0.302 0.382 0.466 0.284 0.438 0.553 0.432 0.162 0.445 3.6 1.6 3.8 4.5 1.4 Figure 3: Qualitative results on NYUv2. Our method consistently produces better predictions with much less error. denotes AbsRel ranging from lowest (Balck) to highest (Red). δ3 compared to the results of DepthAnything with linear fit, which requires ground truth to be rescaled for NYUv2. Our model also obtains satisfactory results on KITTI with two best and three second-best evaluation metrics. It is worth noting that our framework also outperforms many SOTA metric estimation networks listed in the upper part of Table 1 and Table 2. Two important facts and observations further demonstrate the superiority of our method. First, our method only contains 19M trainable parameters, which is much smaller than most other methods. RSA has smaller model size but results in much worse performance for its single-factor way of rescaling. Second, Most other methods requiring domain adaptation (except ZoeDepth and ScaleDepth) are limited to training and evaluating on single domain. By contrast, our method is trained end-to-end with multiple domains and only one model is used for evaluation on all datasets, which demonstrates great generalization ability compared to other methods. Some qualitative results are shown in Figure 3, which demonstrate that our method predicts more accurate depth and uniformly improved quality. Since we perform pixel-level transformation, our method also demonstrates certain level of correction capability to correct the erroneous estimation regions in the relative depth as shown in Figure 4. The depth values within the red rectangle are corrected to more reasonable depth values with our method. More results and analysis are shown in the Appendix. 4.2 Zero-shot Generalization We followed DepthAnything [65] to conduct zero-shot metric depth on five unseen datasets. We also fine-tuned DepthAnything with the four datasets we used for fair comparison. We perform the same evaluation strategy where transformed metric depths are directly evaluated with the ground truth as in DepthAnything. As shown in Table 3, our TR2M results in six best evaluation metrics and four second-best metrics, which are the best average ranking compared to other methods. It is worth highlighting that our method achieves notable zero-shot performance with ViT-Small ( 25M parameters) backbone, whose scale is less than 1/10 of both ZoeDepth and DepthAnything (both backbones are 345M parameters). Also, we only use one trained weight to zero-shot evaluate on all indoor, outdoor, and surgical scenes, while ZoeDepth and DepthAnything utilize different weights trained on different datasets for indoor, outdoor, and surgical scenes evaluation, separately. Training one DepthAnything model with mixed datasets (DA Mix in Table 3) degrades the performance by large margin. This is possibly because it depends on separate decoders for different domains and mixing domains with too much gap will hinder its domain and scale perception ability. 8 Table 4: Ablation study on the main proposed modules. Rescale Maps Ltpsi Lsoc NYUv2 iBims-1 AbsRel 0.118 0.084 0.085 0.082 δ1 AbsRel 0.902 0.946 0.945 0. 0.202 0.173 0.160 0.154 δ1 AbsRel 0.566 0.657 0.708 0.736 DIODE Outdoor δ1 0.225 0.237 0.243 0.274 0.761 0.702 0.687 0.673 Table 5: Ablation study on utilizing text and image information. NYUv2 iBimsText Image AbsRel 0.114 0.085 0.082 δ1 AbsRel 0.909 0.945 0. 0.193 0.164 0.154 δ1 AbsRel 0.621 0.704 0.736 DIODE Outdoor δ1 0.231 0.259 0.274 0.745 0.694 0.673 Figure 4: The depth value of the relative depth map within the red rectangles is inconsistent with the ground truth. Our method can correct such errors when transferring to metric depth. Figure 5: Visualization of embedding space with t-SNE. Features with the same depth ranges are clustered more closely (Compare the circles with the same color) with Lsoc. The feature distribution is more in line with the variation in depth, which is beneficial for the robustness of depth estimation. 4.3 Ablation Study Due to limited space, we only present the two most important ablation studies here. More ablation experiments and discussions are shown in the Appendix. Effectiveness of Proposed Modules. We study the effect of our proposed modules, including the rescale map design, threshold pseudo metric depth supervision Ltpsi, and scale-oriented contrast Lsoc. The results are summarized in Table 4. in Rescale Map denotes using two single factors instead of maps to rescale, while for Ltpsi and Lsoc simply refers to discarding the module. As expected, utilizing all the modules has the highest performance. Discarding Lsoc results in worse performance, especially for zero-shot evaluation for the latter two datasets. Scale-oriented contrast enables the model to learn about knowledge of scale and depth distribution, therefore enhancing depth estimation accuracy. Discarding Ltpsi has limited effects on the seen dataset NYUv2 but degrades in the unseen datasets iBims-1 and DIODE Outdoor. This is because Ltpsi provides more comprehensive guidance on pixels without ground truth, thus enhancing the overall generalization ability for zero-shot evaluation. Further discarding rescale maps results in significant decline in all metrics, demonstrating the importance of using pixel-level maps to do rescaling. Incorporating text and image information. We further carry out an ablation study on the effects of text and image information for relative to metric depth transfer. The results are presented in Table 5. Out of the three choices, using text information only for estimating the scale and shift map performs the worst. Text descriptions cannot capture information about depth distribution or layouts, resulting in inaccurate scale estimation. Using image information only improves performance substantially, but it is still not as good as using both text and image. The information from the two modalities can complement each other, ensuring the models perception of both global and local scales."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose TR2M, generalizable framework to transfer monocular relative depth to metric depth. The model takes text description and image as input and estimates two rescale maps to conduct transformation at the pixel level. The features of the two modalities are fused by cross-modality attention module. To better utilize relative depth, we align relative depth to ground truth to generate pseudo metric depth and filter confident ones as supervision. We also propose scale-oriented contrastive learning to force the model to learn about depth distribution, thus enhancing scale perception ability. As result, our TR2M model exhibits excellent zero-shot metric depth estimation capability across various domains with one lightweight trainable network."
        },
        {
            "title": "References",
            "content": "[1] Manuel López Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bulo, Yubin Kuang, and Peter Kontschieder. Mapillary planet-scale depth dataset. In European Conference on Computer Vision, pages 589604. Springer, 2020. [2] Dylan Auty and Krystian Mikolajczyk. Learning to prompt clip for monocular depth estimation: Exploring the limits of human language. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20392047, 2023. [3] Long Bai, Mobarakol Islam, Lalithkumar Seenivasan, and Hongliang Ren. Surgical-vqla:transformer with gated vision-language embedding for visual question localized-answering in robotic surgery. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 68596865, 2023. doi: 10.1109/ICRA48891.2023.10160403. [4] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40094018, 2021. [5] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Localbins: Improving depth estimation by learning local distributions. In European Conference on Computer Vision, pages 480496. Springer, 2022. [6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [7] Taylor Bobrow, Mayank Golhar, Rohan Vijayan, Venkata Akshintala, Juan Garcia, and Nicholas Durr. Colonoscopy 3d video dataset with paired depth from 2d-3d registration. Medical Image Analysis, page 102956, 2023. [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [9] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct perception in autonomous driving. In Proceedings of the IEEE international conference on computer vision, pages 27222730, 2015. [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [11] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [12] Beilei Cui, Mobarakol Islam, Long Bai, An Wang, and Hongliang Ren. Endodac: Efficient adapting foundation model for self-supervised depth estimation from any endoscopic camera. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 208218. Springer, 2024. [13] Xingshuai Dong, Matthew Garratt, Sreenatha Anavatti, and Hussein Abbass. Towards real-time monocular depth estimation for robotics: survey. IEEE Transactions on Intelligent Transportation Systems, 23(10):1694016961, 2022. [14] Ruofei Du, Eric Turner, Maksym Dzitsiuk, Luca Prasso, Ivo Duarte, Jason Dourgarian, Joao Afonso, Jose Pascoal, Josh Gladstone, Nuno Cruces, et al. Depthlab: Real-time 3d interaction with depth maps for mobile augmented reality. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, pages 829843, 2020. [15] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems, 27, 2014. [16] Fatima El Jamiy and Ronald Marsh. Survey on depth perception in head mounted displays: distance estimation in virtual reality, augmented reality, and mixed reality. IET Image Processing, 13(5):707712, 2019. [17] Jose Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas Brox, and Javier Civera. Cam-convs: Camera-aware multi-scale convolutions for single-view depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1182611835, 2019. 10 [18] Rizhao Fan, Matteo Poggi, and Stefano Mattoccia. Contrastive learning for depth prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 32263237, 2023. [19] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20022011, 2018. [20] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 33543361. IEEE, 2012. [21] Clément Godard, Oisin Mac Aodha, and Gabriel Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 270279, 2017. [22] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus, , and Adrien Gaidon. Towards zero-shot scale-aware monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92339243, 2023. [23] Wencheng Han, Junbo Yin, and Jianbing Shen. Self-supervised monocular depth estimation by directionaware cumulative convolution network. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 86138623, October 2023. [24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. [25] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zeroshot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [26] Xueting Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, and Zhihai He. Learning to adapt clip for few-shot monocular depth estimation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 55945603, 2024. [27] Yiming Huang, Beilei Cui, Long Bai, Ziqi Guo, Mengya Xu, Mobarakol Islam, and Hongliang Ren. Endo-4dgs: Endoscopic monocular scene reconstruction with 4d gaussian splatting. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 197207. Springer, 2024. [28] Stuart Hunter. The exponentially weighted moving average. Journal of quality technology, 18(4):203210, 1986. [29] Yueming Jin, Yang Yu, Cheng Chen, Zixu Zhao, Pheng-Ann Heng, and Danail Stoyanov. Exploring intra-and inter-video relation for surgical semantic scene segmentation. IEEE Transactions on Medical Imaging, 41(11):29913002, 2022. [30] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [31] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based singleimage depth estimation methods. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 00, 2018. [32] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 2016 Fourth international conference on 3D vision (3DV), pages 239248. IEEE, 2016. [33] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel Ni, and Heung-Yeung Shum. Mask dino: Towards unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30413050, 2023. [34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 11 [35] Meixuan Li, Tianyu Li, Guoqing Wang, Peng Wang, Yang Yang, and Jie Zou. Region-aware distribution In European Conference on contrast: novel approach to multi-task partially supervised learning. Computer Vision, pages 234251. Springer, 2024. [36] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for monocular depth estimation. IEEE Transactions on Image Processing, 2024. [37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [38] Xingtong Liu, Ayushi Sinha, Masaru Ishii, Gregory Hager, Austin Reiter, Russell Taylor, and Mathias Unberath. Dense depth estimation in monocular endoscopy with self-supervised learning methods. IEEE transactions on medical imaging, 39(5):14381447, 2019. [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [40] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [41] Suraj Patni, Aradhye Agarwal, and Chetan Arora. Ecodepth: Effective conditioning of diffusion models for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2828528295, 2024. [42] Wanli Peng, Hao Pan, He Liu, and Yi Sun. Ida-3d: Instance-depth-aware 3d object detection from stereo vision for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1301513024, 2020. [43] Luigi Piccinelli, Christos Sakaridis, and Fisher Yu. idisc: Internal discretization for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2147721487, 2023. [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [45] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ICCV, 2021. [46] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. [47] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022. [48] Anita Rau, Binod Bhattarai, Lourdes Agapito, and Danail Stoyanov. Bimodal camera pose prediction for endoscopy. IEEE Transactions on Medical Robotics and Bionics, 5(4):978989, 2023. [49] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. [50] Javier Rodríguez-Puigvert, Víctor M. Batlle, J.M.M. Montiel, Ruben Martinez-Cantin, Pascal Fua, Juan D. Tardós, and Javier Civera. Lightdepth: Single-view depth self-supervision from illumination decline. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2127321283, October 2023. [51] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and Zhengguo Li. Nddepth: Normal-distance In Proceedings of the IEEE/CVF International Conference on assisted monocular depth estimation. Computer Vision (ICCV), pages 79317940, October 2023. [52] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 746760. Springer, 2012. 12 [53] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 567576, 2015. [54] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. [55] Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu, and Luc Van Gool. Exploring cross-image pixel contrast for semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 73037313, 2021. [56] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30243033, 2021. [57] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, and Vivienne Sze. Fastdepth: Fast monocular depth estimation on embedded systems. In 2019 International Conference on Robotics and Automation (ICRA), pages 61016108. IEEE, 2019. [58] Alex Wong, Safa Cicek, and Stefano Soatto. Targeted adversarial perturbations for monocular depth prediction. Advances in neural information processing systems, 33:84868497, 2020. [59] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano Soatto. Unsupervised depth completion from visual inertial odometry. IEEE Robotics and Automation Letters, 5(2):18991906, 2020. [60] Cho-Ying Wu, Jialiang Wang, Michael Hall, Ulrich Neumann, and Shuochen Su. Toward practical monocular indoor depth estimation. In CVPR, 2022. [61] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 37333742, 2018. [62] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1668416693, 2021. [63] Fengyu Yang, Chao Feng, Ziyang Chen, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangopadhyay, Andrew Owens, et al. Binding touch to everything: Learning unified multimodal tactile representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2634026353, 2024. [64] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and Elisa Ricci. Transformer-based attention networks In Proceedings of the IEEE/CVF International Conference on for continuous pixel-wise prediction. Computer vision, pages 1626916279, 2021. [65] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [66] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2025. [67] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. Gedepth: Ground embedding for monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1271912727, October 2023. [68] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90439053, 2023. [69] Chenyu You, Yifei Mint, Weicheng Dai, Jasjeet Sekhon, Lawrence Staib, and James Duncan. Calibrating multi-modal representations: pursuit of group robustness without annotations. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2614026150. IEEE, 2024. [70] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. Neural window fully-connected crfs for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 39163925, 2022. 13 [71] Ziyao Zeng, Daniel Wang, Fengyu Yang, Hyoungseob Park, Stefano Soatto, Dong Lao, and Alex Wong. Wordepth: Variational language prior for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97089719, 2024. [72] Ziyao Zeng, Yangchao Wu, Hyoungseob Park, Daniel Wang, Fengyu Yang, Stefano Soatto, Dong Lao, Byung-Woo Hong, and Alex Wong. Rsa: Resolving scale ambiguities in monocular depth estimators through language descriptions. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [73] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85528562, 2022. [74] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li. Can language understand depth? In Proceedings of the 30th ACM International Conference on Multimedia, pages 68686874, 2022. [75] Chaoqiang Zhao, Matteo Poggi, Fabio Tosi, Lei Zhou, Qiyu Sun, Yang Tang, and Stefano Mattoccia. Gasmono: Geometry-aided self-supervised monocular depth estimation for indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1620916220, October 2023. [76] Xiangyun Zhao, Raviteja Vemulapalli, Philip Andrew Mansfield, Boqing Gong, Bradley Green, Lior Shapira, and Ying Wu. Contrastive learning for label efficient semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1062310633, 2021. [77] Yizhou Zhao, Hengwei Bian, Kaihua Chen, Pengliang Ji, Liao Qu, Shao-yu Lin, Weichen Yu, Haoran Li, Hao Chen, Jun Shen, et al. Metric from human: Zero-shot monocular metric depth estimation via test-time adaptation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [78] Ruijie Zhu, Chuxin Wang, Ziyang Song, Li Liu, Tianzhu Zhang, and Yongdong Zhang. Scaledepth: Decomposing metric depth estimation into scale prediction and relative depth estimation. arXiv preprint arXiv:2407.08187, 2024. [79] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26392650, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Dataset As stated in the main paper, our primary datasets for training and evaluation for TR2M are NYU Depth v2 [52], KITTI [20], VOID [59], and C3VD [7]. NYU Depth v2 and VOID are indoor scene datasets; KITTI is an outdoor scene dataset, and C3VD is surgical dataset that mainly contains scenes of colons. We also evaluate zero-shot performance on five real-world and synthetic datasets: SUN RGB-D [53], iBims-1 [31], HyperSim [49], DIODE Outdoors [54], and SimCol [48]. The first three datasets are indoors; DIODE Outdoors is outdoor scenes, and SimCol is simulated surgical scene dataset for colons. Text descriptions for NYUv2, KITT,I and VOID are generated by [72], where LLaVAv 1.6 Vicuna and LLaVAv 1.6 Mistral [37] are utilized to generate five text descriptions each. Another five descriptions are formed by listing the main objects detected by MaskDINO [33]. The scenes in C3VD and SimCol are similar among different parts of the colon, and current VLMs can not identify them properly. Therefore, we directly use the name for the colon and the texture type to describe them. An example is: \"The image shows surgical scene of descending colon with texture type two.\". We followed RSA [72] to generate text for the other datasets for zero-shot evaluation with LLaVA v1.6 Vicuna [37]. Figure 6 shows some examples of the datasets. Figure 6: Examples of the datasets and the supplementary text descriptions which are generated by VLMs [37] with input images and prompts. A.2 Evaluation Metric The definitions of the evaluation metrics used in this paper are listed below in 6. AbsRel is the absolute relative error, SqRel is the squared relative error, RM SE is the root mean squared error, RM SElog is the root mean squared logarithmic error, log10 is the average (Log10) error, δn is the accuracy with threshold, as in [6, 72, 71, 65, 66]. Table 6: Depth evaluation metrics. and are the predicted and ground truth depth maps. Metrics Name Definition AbsRel SqRel RM SE RM SElog log10 δn 1 (cid:12) (cid:110) (cid:12) (cid:12) (cid:80) 1 (cid:80) 1 (cid:113) 1 (cid:80) dD d /d dD d2 /d (cid:80) dD d2 dD logd logd2 dD(log10d log10d)2 (cid:111)(cid:12) (cid:12) (cid:12) 100% < (1.25)n) (cid:113) 1 (cid:80) Dmax( , 1 A.3 More Details of the Proposed Framework A.3.1 Decoders and Projectors Architecture We design the scale and shift decoders architecture similar to DPT [46] without the progressive reassemble module for our single image feature input. One convolution projection layer and two spatial resampling layers are first utilized to assemble the feature. The output maps are then obtained by projecting the features into one-dimensional maps. The maps are passed through an exponential function to ensure positive scales and shifts for optimization. The number of parameters of the decoder is about 18.9M. The projectors for depth-oriented contrastive learning are formed with two 1 1 convolutions followed by activation functions and max pooling functions. The number of parameters of the projector is about 0.19M. We utilize projector to project the feature embedding into higher dimensions following previous works [10, 24] with many advantages. The raw features may contain redundant or low-discriminative information which could cause insufficient sensitivity in similarity calculations. Higher-dimensional spaces provide richer geometric structures, enabling contrastive losses to measure similarity more accurately. The features with higher dimensions with lower resolution also reduce computation costs, which benefit optimization efficiency. A.3.2 Least-squares Criterion to Determine Pseudo Metric Depth As illustrated in Section 3.2, we construct the pseudo metric depth with two single factors α, β R1. meaningful alignment could be made based on least-squares criterion as below: (α, β) = arg min α,β HW (cid:88) i=1 (cid:0)αDr(i) + β Dgt m(i)(cid:1)2 , (10) The aligned pseudo metric depth can be determined by: Dpseudo solved in closed form by rewriting formula 10 as least-squares problem as below: = αDr + β, which can be efficiently hopt = arg min HW (cid:88) i=1 (cid:16) Dr(i)h Dgt m(i) (cid:17)2 , (11) Dr(i) = (Dr(i), 1) and = (α, β). closed-form solution for the above equation can where be written as: hopt = (cid:32)HW (cid:88) i=1 Dr(i) Dr(i) (cid:33)1 (cid:32)HW (cid:88) i=1 (cid:33) Dr(i)Dgt m(i) . (12) We then use the above solution to rescale Dr to Dpseudo whether it is confident enough to be pseudo-supervision. for subsequent evaluation to determine 16 A.3.3 Selecting Key Samples for Scale-Oriented Contrast Multiple key samples can be chosen to be the contrastive object for the query sample. Here we choose two samples for efficiency. The first is the query sample itself, which goes through different encoders and projectors, and the scale relation is formed within the same depth map. The other sample is random image from the datasets that contribute more negative feature samples with different depth distributions. This can be efficiently done in code by rearranging the input images at the batch level. A.3.4 Supplementary Discussion The utilization of pseudo metric depth with threshold (Ltpsi) is not completely beneficial for the model; for example, incorporating Ltpsi negatively impacts performance on NYUv2. We choose to utilize it based on two reasons. First, using Ltpsi will improve the zero-shot performance, which is major advantage of our proposed TR2M. Second, while only supervising the model with depth ground truth obtains better performances, the model tends to estimate poor scale and shift values for distant areas where no supervision were available, resulting in poor overall qualitative performances. We therefore incorporate Ltpsi with lower weighting factors than the supervision of ground truth as compromise. DepthAnything [65] pointed out that semantic encoders like DINOv2 [40] tend to output similar features for different areas of an object, like the front and back of car. Therefore, it is not beneficial to exhaustively rely on the features generated from such semantic encoders for depth estimation tasks where depth values can differ drastically among different areas, even adjacent pixels within the same object. DepthAnything chooses to set tolerance margin to prevent the model from completely aligning their model with DINOv2. We seek to solve this problem by enforcing the model to capture the depth distribution knowledge within the image. The model learns to generate similar features when two objects are near in depth and dissimilar features for different areas within an object that are far in depth. This learning objective will not cause the model to generate similar features for different objects all the time, as the depth distributions of different objects vary across different images in the dataset. The model captures the inherent relations of objects to determine whether they should be close or distant in the depth ranges. We also conduct experiments as shown below, demonstrating that enforcing features to be in line with the variation in depth is beneficial for depth estimation tasks. A.4 More Experiments and Analysis Experiment Results on VOID and C3VD. Table 7 presents the results on VOID and C3VD. We can notice that without being pre-trained on surgical datasets, DepthAnything can not handle such big domain gaps without fine-tuning. Our method obtains the best evaluation metric except for AbsRel and δ1 compared to EndoDAC, which is an SOTA surgical depth estimation network. Table 7: Quantitative results on VOID and C3VD. C3VD VOID AbsRel RM SE δ1 AbsRel RM SE δ1 Method DA [65] RSA [72] EndoDAC [12] TR2M (Ours) 0.059 0.477 - 0.103 0.160 0.792 - 0.205 0.969 0.374 - 0.899 0.183 0.172 0.083 0.092 10.236 7.686 4.655 2.952 0.728 0.736 0.949 0. Incorporating Scale-Oriented Contrast to Depth Model. To verify the effectiveness and universality of the proposed Scale-Oriented Contrastive learning (SOC), we evaluate on directly implementing the proposed SOC on an existing monocular depth estimation method. We reproduce NeWCRFs [70] with and without SOC as an additional module while maintaining all the other settings. The results are shown in Table 8 in which the model obtains improvement in all metrics with the implementation of SOC. The improvement for zero-shot evaluation on iBims-1 is more significant, e.g. AbsRel 0.223 0.210. The results demonstrate that enforcing feature embeddings aligned with depth distribution is beneficial for depth estimation tasks. The proposed SOC can be implemented as plug-in module to existing depth estimation frameworks to promote overall performance. 17 Table 8: Quantitative results on implementing Scale-Oriented Contrastive on monocular depth estimation method. Method NYUv2 iBimsAbsRel RM SE δ1 AbsRel RM SE δ1 NeWCRFs [70] NeWCRFs [70] + SOC 0.090 0.089 0.324 0. 0.929 0.932 0.223 0.210 0.891 0.882 0.541 0.550 Impact of Relative Depth Model. We conduct an ablation study on the choice of backbone relative depth estimation model. We compare on three different frameworks: MiDas [47], DPT [38] and DepthAnything (DA) [65]. We follow RSA [72] to choose MiDas 3.1 Swin2 large-384 with 213M parameters for MiDas, DPT-Hybrid with 123M parameters for DPT and DA-Small with 25M parameters for DepthAnything. As shown in Table 9, DA obtains the best performance for all metrics of the three datasets. The performances do not drop too much from MiDas and DPT to DA because the pixel-wise rescaling method of TR2M bridges the depth perception gap between different depth estimation networks. Nevertheless, great relative depth estimation model could still benefit the overall performance, leading us to choose DepthAnything as our backbone model. Table 9: Ablation study on the relative depth model. Backbone Relative Depth Model MiDas [47] DPT [38] DA [65] (Ours) NYUv2 iBims-1 AbsRel δ1 AbsRel δ1 AbsRel DIODE Outdoor δ1 0.089 0.088 0.082 0.934 0.942 0.954 0.168 0.164 0.154 0.707 0.715 0. 0.690 0.682 0.673 0.258 0.262 0.274 Impact of Text Description. In order to validate the impact of text description on the results, we evaluate our model given the same image input with different language inputs. We test on three settings: 1. The correct text description; 2. The incorrect description but within the same domain (Indoor, Outdoor or Surgical); 3. Incorrect description from different domain. The results are presented in Table 10. Performance degradations can be noticed for all datasets when an incorrect description from the same domain is used. It is worth noting that, the model exhibits varying capabilities in scale perception when incorrect descriptions from different domains are utilized. Metrics for C3VD with surgical image and text description from an incorrect domain as inputs drops significantly for both AbsRel and δ1. We believe that the low diversification in colon text descriptions leads the model to overfit on certain descriptions. This promotes future research on enhancing the diversity and robustness of text descriptions. Table 10: Ablation study on text description. Text Description Type NYUv iBims-1 C3VD AbsRel δ1 AbsRel δ1 AbsRel δ1 Incorrect Domain Correct Domain, Incorrect Scene Correct Domain, Correct Scene 0.096 0.091 0.082 0.935 0.940 0.954 0.164 0.159 0.154 0.718 0.727 0.736 0.221 0.108 0. 0.445 0.901 0.918 Impact of Scale and Shift Map. We propose to use two maps, scale and shift maps, to rescale the relative depth map to the metric depth map, considering it as an affine transformation. We conduct an ablation study on the impact of two maps as shown in Table 11. Using only single map results in distinct degradation in performance in all metrics of the three datasets. The performance of utilizing only the scale map shows minor degradation, for example, 3.7% drop in δ1 of NYUv2 and 19.5% increase in AbsRel of iBims-1. By contrast, using the shift map only leads to significant performance decrease, for example, 12.8% drop in δ1 of NYUv2 and 79.0% increase in AbsRel of iBims-1. The reason is that the production operation between the scale map and the depth map enables the value to alter arbitrarily, satisfying the value gap between relative depth and metric depth. Shift map cannot change the overall scale of the depth map. It can only enlarge the depth value, which limits the transformation from relative to metric. As result, TR2M exhibits both scale and shift maps to conduct the transformation, which guarantees flexible and generalizable rescaling. 18 Table 11: Ablation study on utilizing scale and shift maps to rescale depth. Scale Map Shift Map NYUv2 iBims-1 AbsRel δ1 AbsRel δ1 AbsRel DIODE Outdoor δ1 0.102 0.256 0.082 0.923 0.836 0.954 0.177 0.356 0.154 0.663 0.453 0. 0.727 0.786 0.673 0.238 0.205 0.274 Class Number of Depth. The proposed scale-oriented contrast learning first classifies the depth values based on their distribution and enforces attraction on features with the same distribution and repulsion on features with different distributions. Therefore, we conduct an ablation study on the corresponding class number of the depth value as shown in Table 12. The framework obtains the best performance when classifying the depth value into 20 classes. When the class number is too small, the depth range of class is too wide leading little differences between features with large gap in depth values; when the class number is too large, the depth range of class is too small leading too much differences between features with small gap in depth values. Correctly selecting the appropriate number of class is necessary to ensure consistency between features and depth distribution, thereby enhancing the models zero-shot capability. Table 12: Ablation study on the number of classes of depth map. Class Number of Depth NYUv iBims-1 AbsRel δ1 AbsRel δ1 AbsRel DIODE Outdoor δ1 5 10 20 0.086 0.083 0.082 0.083 0.948 0.953 0.954 0.952 0.157 0.155 0.154 0.156 0.728 0.730 0.736 0.731 0.682 0.679 0.673 0.681 0.264 0.268 0.274 0. A.5 Limitation and Future Work While pixel-wise rescaling maps could make the rescaling process more precise and correct erroneous regions generated by the relative depth model, it also tends to make the model overfit to the scale of certain objects and cause wrong alignment compared to rescaling factors. Also, the current light-weight architecture is not powerful enough to handle every situation, resulting in unclear edges and details of the transferred depth maps in some circumstances. Additionally, our approach enables flexible input descriptions for transferring relative depth to metric depth. While this grants users precise control over 3D reconstruction results, it also introduces risks that bad users could choose the wrong descriptions, causing incorrect predictions. Future research may include extending TR2M to more precise and higher resolution transferring and investigating the models robustness to erroneous text descriptions. A.6 More Qualitative Results Please refer to the following pages for additional qualitative results on the datasets utilized in the paper. We compare our model with the DepthAnything-Large [65] model fine-tuned on separate datasets for metric depth estimation. As discussed above, the light-weight architecture may cause unclear edges in some circumstances, but our method is capable of capturing some objects lost in previous methods, and our method is more accurate in the actual metric scale. 19 Figure 7: Example qualitative results on the datasets utilized in the paper. We compared our method with the DepthAnything-Large [65] model fine-tuned on separate datasets for metric depth estimation. The darker color denotes the closer distance."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong, Hong Kong"
    ]
}