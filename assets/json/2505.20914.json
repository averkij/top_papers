{
    "paper_title": "Geometry-Editable and Appearance-Preserving Object Compositon",
    "authors": [
        "Jianman Lin",
        "Haojie Li",
        "Chunmei Qing",
        "Zhijing Yang",
        "Liang Lin",
        "Tianshui Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General object composition (GOC) aims to seamlessly integrate a target object into a background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce a Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs a cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in a disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design a dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 1 9 0 2 . 5 0 5 2 : r Geometry-Editable and Appearance-Preserving Object Compositon Jianman Lin1, Haojie Li1, Chunmei Qing1, Zhijing Yang2, Liang Lin3, and Tianshui Chen2 1South China University of Technology 2Guangdong University of Technology 3Sun Yat-sen University"
        },
        {
            "title": "Abstract",
            "content": "General object composition (GOC) aims to seamlessly integrate target object into background scene with desired geometric properties, while simultaneously preserving its fine-grained appearance details. Recent approaches derive semantic embeddings and integrate them into advanced diffusion models to enable geometry-editable generation. However, these highly compact embeddings encode only high-level semantic cues and inevitably discard fine-grained appearance details. We introduce Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model that first leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Specifically, DGAD builds on CLIP/DINO-derived and reference networks to extract semantic embeddings and appearance-preserving representations, which are then seamlessly integrated into the encoding and decoding pipelines in disentangled manner. We first integrate the semantic embeddings into pre-trained diffusion models that exhibit strong spatial reasoning capabilities to implicitly capture object geometry, thereby facilitating flexible object manipulation and ensuring effective editability. Then, we design dense cross-attention mechanism that leverages the implicitly learned object geometry to retrieve and spatially align appearance features with their corresponding regions, ensuring faithful appearance consistency. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework."
        },
        {
            "title": "Introduction",
            "content": "General Object composition (GOC) involves editing target object to seamlessly integrate it into background scene at arbitrary angles and positions, while preserving the objects appearance details without alteration. robust generative object composition system should automate complex tasks such as interactive image editing, virtual environment creation, and content generation for augmented and virtual reality (AR/VR) applications, by simultaneously supporting flexible object editing and faithful appearance preservation critical balance that minimizes the need for manual adjustment in both geometric editing and visual consistency. Thanks to the emergence of large-scale pre-trained diffusion models Nichol and Dhariwal [2021], Song et al. [2020], Ho and Salimans [2022], substantial progress has been made in generative compositing. However, these models still face challenges in simultaneously editing the target object according to the users expectations and maintaining consistent appearance details, limiting their applicability in real-world scenarios. Existing methods either introduce CLIP/DINO-derived semantic embeddings Song et al. [2023], Chen et al. [2024b], Yang et al. [2023], or leverage pixel-aligned appearance features to achieve Figure 1: (a) Leverages compact semantic embeddings to enable object editability but fails to preserve appearance details. (b) Utilizes appearance features to retain visual fidelity, yet lacks editing capability. Unlike both, Our method implicitly learns the geometry-editable representation and explicitly aligns fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation. object composition Zhao [2024], as shown in Fig. 1. However, both approaches fail to simultaneously ensure object editability and appearance-preserving. The former encodes the object into compact semantic embedding, which demonstrates strong compatibility with pre-trained diffusion models and enables robust manipulation of geometric properties such as shape deformation and viewpoint changes. Yet, this compact encoding inevitably loses high-frequency details, making it difficult to preserve appearance during editing. In contrast, the latter employs reference networks to extract pixel-wise appearance features, which maintain tight spatial correspondence with the object and excel at reconstructing appearance details from noise latents. Nevertheless, the rigid spatial alignment severely restricts editing flexibility, often resulting in copy-paste-like outputs rather than adaptive transformations. Object composition requires both object editability and appearance-preserving, and designing an effective mechanism to leverage the advantages of both is key to solving the problem. To address these challenges, we first leverage semantic embeddings to implicitly capture the desired geometric transformations, and then employ dense cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation. Prior methodssuch as Ye et al. [2023], Yang et al. [2023], which rely on precise object masks to explicitly encode geometric properties, thereby limiting editing flexibility and efficiency, or Zhao [2024], which performs implicit retrieval and alignment of appearance features via cross-attention mechanismoften result in suboptimal appearance preservation due to inconsistent geometry-appearance mapping. In contrast, we leverage the inherent spatial reasoning capabilities of pretrained diffusion models to implicitly capture object geometry, and subsequently perform position-wise retrieval and alignment of appearance features with their corresponding geometric regions based on the captured geometry, ensuring both editability and appearance preservation during composition. To this end, we propose the Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model, which leverages compact semantic embeddings to implicitly learn the geometric properties of objects during encoding, and employs the resulting representations to explicitly retrieve and spatially map appearance features to corresponding geometric regions during decoding. Specifically, during the encoding stage, the initial input is constructed by concatenating the user-specified regions with surrounding contextual information. We leverage CLIP/DINO-derived semantic embeddings Radford et al. [2021], Oquab et al. [2023] and cross-attention mechanism built upon the strong spatial reasoning capabilities of pretrained diffusion models to implicitly learn the geometric properties of objects, which in turn enables flexible manipulation and thereby ensures object editability. To ensure consistent object appearance, we introduce dense cross-attention mechanism that leverages encoded features to establish explicit correspondences with appearance 2 features from reference network. The encoded features, which capture both semantic and geometric properties, serve as queries, while the appearance features act as keys and values. To further guide the attention toward semantically relevant regions, position-wise gating weight is learned from the query features to explicitly represent the objects geometric structure, enabling the model to adaptively retrieve and align appearance features with their corresponding geometric regions. This mechanism is applied exclusively during the decoding stage, where appearance retrieval is conditioned on fully geometry-edited representation, thereby enhancing both object editability and appearance-preserving in the composition process. The contributions of our work are fourfold. First, we propose the Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model, which implicitly learns the geometric properties of objects and explicitly retrieves and spatially maps appearance features to corresponding geometric regions, enabling editable and appearance-preserving object composition. To our knowledge, this is the first attempt to explicitly disentangle geometry editing and appearance preservation of objects to facilitate the GOC task. Second, we employ compact semantic embeddings and cross-attention mechanism that harnesses the strong spatial reasoning capabilities of pretrained diffusion models to implicitly capture object geometry to facilitate editability during encoding. Third, we introduce dense cross-attention mechanism to explicitly retrieve and spatially align appearance features with their geometric counterparts to enhance appearance consistency during decoding. Finally, we conduct extensive experiments on public benchmarks to demonstrate the effectiveness of the proposed DGAD framework. The code and pretrained models are available at: https://github. com/jianmanlincjx/DGAD"
        },
        {
            "title": "2 Related Work",
            "content": "Achieving both editability and visual consistency in generated images remains longstanding challenge in image generation Chen et al. [2025], Xu et al. [2025]. This challenge is particularly evident in object composition tasks, where the goal is to edit objects with desired geometric properties to align with the background scene while preserving their appearance details. In this section, we review related work and highlighting how prior approaches address or fall short in achieving both editability and appearance consistency. Traditionally, object composition has been approached as an image harmonization Guerreiro et al. [2023], Jiang et al. [2021], Ke et al. [2022], Xue et al. [2022] or blending Pérez et al. [2023], Wu et al. [2019], Zhang et al. [2021] problem, focusing on color and lighting consistency between the object and the background. For example, DCCF Xue et al. [2022] achieves state-of-the-art color harmonization using pyramid structure of neural filters. However, these methods do not address geometric alignment. The rise of GANs He et al. [2023], Xu et al. [2024], Chen et al. [2024a] has inspired methods Azadi et al. [2020] that attempt to resolve geometric inconsistencies. GauGAN Park et al. [2019] uses semantic segmentation to guide object placement but is limited to generating images from segmentation maps, making it difficult to directly apply to existing backgrounds. Zhan et al. [2019] learns geometric transforms for existing objects but does not generate new shapes or textures. Many of these methods are domain-specific and face challenges with complex transformations, such as out-of-plane rotations. In contrast, generative object composition aims to edit objects to align with desired geometric properties, ensuring natural alignment with the background while preserving fine-grained appearance details. More recent approaches, particularly those based on diffusion models Nichol and Dhariwal [2021], Song et al. [2020], Ho and Salimans [2022], Corneanu et al. [2024], have advanced the field by addressing some of these limitations. Techniques like DreamBooth Ruiz et al. [2023] and Textual Inversion Gal et al. [2022] enable high-fidelity object customization through fine-tuning with placeholder tokens. However, they lack explicit control over object geometric editing and require inference-time optimization, limiting flexibility and practicality. To improve editability, recent methods adopt compact semantic embeddings. Pbe Yang et al. [2023] and ObjectStitch Song et al. [2023] leverage CLIP-based embeddings Radford et al. [2021] with data augmentations for more flexible composition. IMPRINT Song et al. [2024] further enhances this by fine-tuning DINO Oquab et al. [2023] on multi-view object data, implicitly learning geometry to support geometry-editable. However, CLIP and DINO embeddings struggle to capture fine-grained visual details, compromising appearance preservation. AnyDoor Chen et al. [2024b] addresses this by combining DINO features with structured representations, improving visual fidelity while allowing moderate editability. Yet, its 3 reliance on structured inputs limits its flexibility, especially for non-rigid objects. MiniBrush Zhao [2024] uses reference network to extract pixel-level appearance features and learns object geometric editing from video data, achieving strong appearance preservation. Still, it lacks robust mechanism to accurately align appearance features with their corresponding geometric regions, resulting in suboptimal compositions. In contrast, DGAD leverages semantic embeddings to implicitly capture the desired geometric transformations and then employs dense cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition. Figure 2: The training process of the proposed Disentangled Geometry-editable and Appearancepreserving Diffusion (DGAD) is as follows (the inference process is similar but involves iterative denoising): It first leverages semantic embeddings to implicitly capture the desired geometric transformations, and then employs cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation in object composition."
        },
        {
            "title": "3 Method",
            "content": "The overall pipeline of the DGAD framework is illustrated in Fig. 2. In the encoding stage, we utilize CLIP/DINO-derived semantic embeddings and cross-attention mechanism built upon the strong spatial reasoning capabilities of pretrained diffusion models to implicitly capture object geometry. During decoding, dense cross-attention mechanism is employed to explicitly retrieve and positionally align appearance features with their corresponding geometric regions based on the encoded features. In the following sections, we first describe the learning process of the geometryeditable representation (Section 3.1), then detail the appearance-preserving representation retrieval and learning (Section 3.2), and finally present the optimization procedure of the proposed DGAD framework (Section 3.3). 3.1 Geometry-Editable Encoder This section details the learning process for capturing implicit geometric properties of objects to enhance editing capabilities. While current approaches combining semantic embeddings Radford et al. [2021], Oquab et al. [2023] with pretrained diffusion models enable basic geometric editability, existing integration strategies remain limited in both flexibility and effectiveness. Some methods Ye et al. [2023], Yang et al. [2023] formulate the GOC task as image inpainting and attempt to model object geometry through end-to-end training, but their practical application is constrained by 4 dependence on fine-grained masks. Alternative approaches Chen et al. [2024b] employ ControlNetbased architectures conditioned on specified regions but fall short in fully capturing geometric structure due to architectural limitations. Inspired by recent work Su et al. [2025] that demonstrates introducing an intermediate 2D layout representation greatly strengthens the geometric perception and reasoning of the generative model, we propose directly concatenating user-specified regions with surrounding contextual information as initial input, thereby utilizing the pretrained diffusion models geometric perception ability to simplify the learning process. Given paired training data (Iobj, Ibg, M, Itgt), where Iobj denotes the object, Ibg the scene, the masked region, and Itgt the target image, we first utilize the pre-trained CLIP/DINO image encoder to encode Iobj and obtain the compact semantic feature fobj. We then encode Ibg and Itgt into the latent space of the pretrained diffusion model using pretrained VAE, resulting in latent representations fbg, and ftgt. The mask is downsampled via bicubic interpolation to match the spatial resolution of the latent space. To simulate the generative process, the noise ϵ at timestep is added to ftgt to obtain the noisy latent tgt along the channel dimension, forming multi-channel composite input. The mask acts as spatial prior that activates the diffusion models inherent geometric perception capabilities. To fully leverage its inherent capabilities, we replicate its original input layer weights to accommodate the increased input dimensionality, rather than introducing newly initialized layers. tgt. Next, we concatenate , fbg, and After that, cross-attention mechanism is utilized to implicitly learn the geometry-editable representation of object. The backbone feature fb is treated as the query, and the CLIP/DINO-driven semantic embedding fobj is treated as the key and value, formulated as: = fbWq, = fobjWk, Attention(Q, K, ) = softmax = fobjWv (cid:18) QK dk (cid:19) (1) (2) dk scales where Wq, Wk, and Wv are the query, key, and value matrices, respectively, and the term the dot product to stabilize training. By minimizing the difference between the actual noise ϵ and the predicted noise ϵθ, the learnable parameters are updated via backpropagation based on the diffusion prior: = Eftgt,ϵN (0,I),c,t (cid:104)(cid:13) (cid:13)ϵ ϵθ (cid:0)f tgt, c, t(cid:1)(cid:13) (cid:13) 2(cid:105) (3) where represents the conditioning features, including fobj, , and fbg. Once trained, the model captures the geometric properties of objects based on simple specified regions to facilitate editability. 3.2 Appearance-Preserving Decoder The learned geometry-editable representation fails to preserve object appearance fidelity, primarily due to semantic embeddings predominantly encode high-level semantic cues while neglecting fine-grained visual details. Recent advances Ju et al. [2024], Hu [2024] have demonstrated that appearance features extracted from reference networks can effectively reconstruct objects from noisy latentsfor instance, BrushNet Hu [2024] trained on open-domain data achieves direct object reconstruction using only single reference image. Building on these insights, we introduce dense cross-attention mechanism that explicitly retrieves and positionally aligns appearance features with their corresponding geometric regions based on geometry-edited representation, thereby ensuring both precise geometry editing and faithful appearance preservation. Specifically, given the backbone feature fb RBCHW output from the encoder stage and the corresponding-layer appearance feature fr RBCHW produced by BrushNet, we first reshape both features to RBHW C, where the channel dimension encodes feature descriptors for each spatial position across the HW grid. The reshaped fb RBHW serves as the query, as it encodes geometry-edited representation, while fr RBHW serves as both the key and value matrices: = fbWq, = frWk, = frWv (4) Rather than employing conventional cross-attention mechanisms for implicit feature retrieval, we propose to learn position-wise gating weights α through query features that explicitly represent 5 geometric structures. Given query features containing learned geometry-editable representations, we naturally derive the geometric-aware softmask α and its complementary term β through: α = σ(F (Q)) [0, 1]B1HW , β = mask_process (1 α) (5) where is implemented using two stacked fully-connected layers combined with the rectified linear unit (ReLU) non-linear function and σ is the sigmoid activation function. We leverage α and β to establish dense correspondences between backbone features and appearance features: Dense_Attention(Q, K, V, α, β) = softmax (cid:18) (cid:18) QK dk (cid:19) (cid:19) α + β (6) By this design, we can leverage α to explicitly constrain the network to query the appearance features based on learned geometry-editable representation, thus establishing dense correspondence of objects between fb and fr. β is the complement of α, so the weights in the object geometric region will be zero. We use mask_process() to ensure values close to 0 are maintained within the empirically determined interval [0.5, 0.8] during training, aiming to preserve the semantic features from the encoder stage while forcing the network to retrieve appearance information from the reference network. 3.3 Optimization DGAD is trained end-to-end based on the pretrained Stable Diffusion v1.5 model. cross-attention mechanism is applied to every block in the backbone to learn the geometry-editable representation. In contrast, the dense cross-attention mechanism is applied only in the decoder stage to explicitly retrieve and spatially align appearance-preserving features with their corresponding geometric regions, conditioned on the fully learned editable representation. The final loss is defined as follows: = Eftgt,ϵN (0,I),t (cid:104)(cid:13) (cid:13)ϵ ϵθ (cid:0)f tgt, [M, fbg, fobj, fr], t(cid:1)(cid:13) (cid:13) 2(cid:105) (7) This optimization updates learnable parameters via diffusion-constrained backpropagation, enabling simultaneous precision in geometry manipulation and visual consistency maintenance during object composition. For more detailed network architecture and training details, please refer to the appendix A.1."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation Benchmark Competing Algorithms . We compare our method with several recent approaches in the field of object composition, providing basis for evaluating the effectiveness of our proposed framework. 1) AnyDoor Chen et al. [2024b] (CVPR 2024): Utilizes semantic features extracted from DINO and structured representation obtained via ControlNet to perform object composition. 2) MimicBrush Zhao [2024] (NeurIPS 2024): Leverages appearance features from reference network to guide object composition. 3) Pbe Yang et al. [2023](CVPR 2023): Employs CLIP-derived semantic features and strong data augmentations to achieve object composition. 4) ObjectStitch Song et al. [2023] (CVPR 2023): Introduces content adaptor to preserve both categorical semantics and object appearance during composition. 5) IP-Adapter Ye et al. [2023] : Formulates object composition as an inpainting task, using CLIP-derived semantic features and end-to-end fine-tuning for generation. Dataset . Following the approach in AnyDoor Chen et al. [2024b], we use training set consisting of 386k images and 23k video samples. For the image dataset, we apply LaMa Suvorov et al. [2022]to remove foreground objects, creating paired data that includes the object, the background scene, and the corresponding target image. For the video dataset, we also adopt AnyDoors preprocessing method to construct triplets with the same structure as the image data. For evaluation, we select 30 object concepts from DreamBooth Ruiz et al. [2023] as test subjects. As background scenes, we manually choose 80 geometrically annotated images from the COCO-Val set Lin et al. [2014], resulting in 2,400 synthesized samples covering all combinations of the selected objects and scenes. Metrics . The purpose of object composition is to edit objects with desired geometric properties to align with the background scene while preserving their appearance details. To evaluate the object"
        },
        {
            "title": "Semantic Consistency",
            "content": "IR FID LPIPS DISTS CLIP Score DINO Score 42.56 41.21 43.25 44.88 44.81 61. 31.35 32.25 32.07 30.69 26.08 15.04 18.47 18.96 20.49 15.33 15.82 14.94 23.46 23.53 26.48 19.20 19.21 18.53 87.78 87.06 85.30 89.34 88.21 89.38 65.09 65.09 62.14 69.98 69.22 69.92 Table 1: Quantitative comparison with prior works. Metrics are grouped by functionality: editability, appearance preservation, and semantic consistency. Our method achieves superior performance across all categories. Figure 3: Qualitative comparison with recent advanced methods. The results show that the proposed method can edit objects with desired geometric properties to align with the background scene while preserving their appearance details. For more detailed and comprehensive visualizations, please refer to the appendix A.2. Editability, we introduce three metrics: IR, and FID. IR is text-to-image evaluation models trained on large-scale datasets that reflect human preferences for generated images. Since high-quality object edits tend to produce more visually appealing results that align with human expectations, it serve as human-aligned indicators of editing success. FID assesses the distributional similarity between the composed images and real-world images, providing an objective measure of compositional realism. To evaluate appearance consistency, we introduce two metrics: LPIPS and DISTS. These metrics can robustly assess an objects appearance consistency even under geometric misalignment by measuring differences in deep feature space. Additionally, we introduce CLIP Score and DINO Score to measure the semantic consistency between objects as supplement, similar to Chen et al. [2024b]. 4.2 Quantitative Comparison In this section, we present quantitative comparison between DGAD and other advanced methods as shown in Tab.1, and provide detailed analysis of the underlying causes behind each methods performance. IPAdapter Ye et al. [2023], ObjectStitch Song et al. [2023], and Pbe Yang et al. [2023] rely on CLIP semantic embeddings to capture object geometry, enabling moderate editability. However, the loss of high-frequency details in these embeddings hampers appearance preservation, resulting in poor performance on appearance preservation metrics such as LPIPS and DISTS. Their inability to seamlessly integrate realistic objects also leads to weaker scores in editability metrics like IR and FID. AnyDoor Chen et al. [2024b] integrates structured representations (e.g., Canny edges) to enhance appearance preservation. However, these representations capture only contour-based appearance, and the rigid structure limits the model to basic geometric edits. As result, it performs only average on object editability and appearance preservation metrics. MimicBrush Zhao [2024] extracts pixel-level appearance features via reference network and performs implicit geometric alignment. However, lacking explicit correspondence modeling, it struggles with accurate feature mapping, leading to suboptimal composition quality. In contrast, DGAD first leverages the inherent spatial reasoning capabilities of pretrained diffusion models to capture the desired geometric transformations, achieving superior results on editability metrics. It then employs cross-attention retrieval mechanism to align fine-grained appearance features with the geometry-edited representation, thereby also outperforming existing methods on appearance preservation metrics. Regarding semantic consistency-related metrics, existing methods all rely on CLIP/DINO-derived semantic embeddings to extract the semantic information of objects, resulting in comparable performance across these methods. 4.3 Qualitative Comparisons In this section, we present qualitative comparison between DGAD and other advanced methods, as shown in Fig. 3, and provide detailed analysis of the causes behind each method displayed. IPAdapter, ObjectStitch, and Pbe rely on CLIP-derived semantic embeddings to learn the geometric properties of the target object for editability. However, since CLIP primarily captures category-level semantic features, it inevitably loses high-frequency visual details during the encoding process. As shown in Columns 3 to 5 of Fig. 3, while these methods can edit object geometry, they often fail to preserve appearance consistency. AnyDoor enhances editability and appearance preservation through DINO-based semantic embeddings combined with structured representations (e.g., Canny edges). However, its reliance on rigid structural constraints limits editing to basic geometric operations like in-plane rotations (Fig. 3, Row 2, Column 7), while incomplete appearance modeling causes visual inconsistencies. MimicBrush extracts pixel-level appearance features and learns object geometry from videos, achieving better appearance retention. However, due to the lack of explicit correspondence modeling, it often suffers from geometry-appearance mismatches, especially under complex edits (Fig. 3, Row 3, Column 6). In contrast, DGAD implicitly learns the geometryeditable representation and explicitly aligns fine-grained appearance features with the geometry-edited representation, facilitating both precise geometry editing and faithful appearance preservation, as shown in the last column of the Fig. 3. 4.4 User study We conducted user study to compare our method with other existing approaches. total of 25 participants evaluated 30 groups of images, each containing scene, specified location or mask, and results generated by various methods. Participants assessed these results based on two criteria: Composition Quality, which measures whether the inserted object aligns with the intended geometric properties within the scene, and Visual Consistency, which evaluates whether the inserted object maintains consistency with the original in terms of texture, color, and material. As shown in Tab. 2, our proposed method is preferred by users on both composition quality and visual consistency metrics, demonstrating its superiority in editing objects to align geometrically with the background scene while preserving their appearance details. Metric / Method Ipadapter Objectstitch Pbe Mimicbrush Anydoor Ours Composition Quality Visual Consistency 12.50% 4.17% 4.17% 4.17% 0.00% 12.50% 29.17% 21.00% 16.67% 37.50% 16.67% 41.50% Table 2: User study on the comparison between our DGAD and existing methods. 8 Figure 4: Left half: Visualization of the implicitly captured geometric properties of the object. Right half: Qualitative comparisons of Ours w/o dense CA and Ours, Using dense cross-attention effectively retrieves accurate appearance features, thereby promoting appearance preservation. 4.5 Ablation Study 4.5.1 Ablation of the Geometry-Editable Encoder Ablation of Geometric Prior . In Section 3.1, we introduce 2D layout representation combined with contextual information to activate the geometric perception capabilities of the pretrained diffusion model. To evaluate its effectiveness, we conduct an ablation study where the layout representation is removed and replaced with standard image inpainting setup (Ours w/o layout representation). As shown in Tab. 3, this results in significant drop across key object editability metrics, including IR and FID. Due to the absence of 2D layout representation guidance, the diffusion model cannot fully exploit its inherent geometric priors to model the objects geometric properties, leading to weaker shape understanding and degraded composition quality. To more intuitively perceive the learned geometry-editable representation, we compute attention maps between backbone features and semantic embeddings during cross-attention operations, then project these activation patterns onto the scene image. As demonstrated in the third column (left panel) of Fig. 4, the learned representations enable precise geometric editing guided by user-specified positional cues while maintaining object-scene affordance relationships, demonstrating that leveraging the geometric priors of pretrained diffusion models facilitates the capture of geometric properties of objects. Method IR FID LPIPS DISTS CLIP Score DINO Score Our w/o layout representation Ours w/o copy weight Ours 52.82 58.12 61.14 26.08 24.67 15. 16.02 15.13 14.94 19.97 19.32 18.53 87.12 88.12 89.38 68.02 68.18 69.92 Table 3: Quantitative analysis of the Ours method under different settings. Ours w/o layout representation refers to the setting where the 2D representation is not directly used as the source input, while Ours w/o copy weight indicates that the additional input layer weights are randomly initialized. Ablation of Weight Initialization . In Section 3.1, we argue that copying the pretrained weights of the input layerrather than randomly initializing themeffectively adapts the model to the expanded input channels and better utilizes the geometric priors of the pretrained diffusion model. To validate this, we compare our approach with baseline where the extra input channels are randomly initialized (\"Ours w/o copy weight\"). As shown in Tab. 3, the baseline performs slightly worse across all metrics. This performance gap stems from the fact that randomly initialized weights must be learned from scratch, and our relatively small training dataset limits the models ability to recover the rich geometric priors learned during large-scale pretraining. 4.5.2 Ablation of the Appearance-Preserving Decoder Ablation of Dense Cross Attention . In Section 3.2, we introduce dense cross-attention to explicitly retrieve and positionally align appearance features with their corresponding geometric regions based 9 on the encoded features. To assess its effectiveness, we conduct an ablation study (Ours w/o Dense CA), in which dense cross-attention is replaced by standard cross-attention. As shown in Tab. 4, this leads to notable drop in LPIPS and DISTS scores, indicating reduced ability to preserve visual appearance. Dense cross-attention is specifically designed to establish dense correspondences between backbone and appearance features. Without this explicit retrieval mechanism, the network struggles to accurately associate appearance features with their corresponding geometric regions. This limitation is also evident in the visual results (Fig. 4, right), where the red box highlights regions with misaligned appearance features, leading to visible inconsistencies. Method IR FID LPIPS DISTS CLIP Score DINO Score Our w/o Dense_ CA Ours Dense CA Both Stages Ours 58.12 60.02 61.14 19.18 16.13 15.04 16.92 15.23 14.94 21.93 19.21 18. 88.23 89.13 89.38 69.12 69.12 69.92 Table 4: Quantitative analysis of the Ours method under different settings. Ours w/o Dense CA indicates the use of standard cross-attention instead of dense cross-attention, while Ours Dense CA Both Stages refers to applying dense cross-attention to both the encoder and decoder stages. Ablation of Optimization Strategies. We apply dense cross-attention only at the decoder stage to condition appearance retrieval on fully geometry-edited representation. To evaluate the impact of this design choice, we conduct an ablation study where dense cross-attention is instead applied at both the encoder and decoder stages (Ours Dense CA Both Stages). As shown in Tab. 4, this configuration results in slight decrease in LPIPS and DISTS scorestwo metrics used to evaluate appearance preservationwhile other metrics remain largely unchanged. One possible explanation is that, during diffusion model training, the input data is inherently noisy. Applying dense cross-attention at the encoder stage may cause the network to retrieve inaccurate appearance features from noisy inputs, especially in early training stages. This can introduce erroneous appearance information, ultimately degrading appearance fidelity."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "In this work, we introduce the Disentangled Geometry-editable and Appearance-preserving Diffusion (DGAD) model to simultaneously achieve object editability and appearance preservation. During the encoding stage, DGAD utilizes compact semantic embeddings and cross-attention mechanism built on the strong spatial reasoning capabilities of pretrained diffusion models to implicitly capture object geometry, thereby enabling flexible manipulation and ensuring editability. During the decoding stage, dense cross-attention mechanism is employed to explicitly retrieve and align appearance features with their corresponding geometric regions based on the encoded information, facilitating editable and appearance-preserving object composition. Extensive experiments on public benchmarks demonstrate the effectiveness of the proposed DGAD framework. However, the method heavily depends on the quality of semantic embeddings to learn the geometryeditable representation, and the retrieval of appearance features is also conditioned on the geometryedited representation. For certain uncommon or rare objects, the CLIP or DINO encoders may fail to extract effective semantic embeddings, which can result in suboptimal performance and adversely impact the quality and fidelity of the final composition."
        },
        {
            "title": "References",
            "content": "Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, and Trevor Darrell. Compositional gan: Learning image-conditional binary composition. International Journal of Computer Vision, 128(10):2570 2585, 2020. Tianshui Chen, Jianman Lin, Zhijing Yang, Chunmei Qing, and Liang Lin. Learning adaptive spatial coherent correlations for speech-preserving facial expression manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72677276, 2024a. 10 Tianshui Chen, Jianman Lin, Zhijing Yang, Chumei Qing, Yukai Shi, and Liang Lin. Contrastive decoupled representation learning and regularization for speech-preserving facial expression manipulation. International Journal of Computer Vision, pages 117, 2025. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65936602, 2024b. Ciprian Corneanu, Raghudeep Gadde, and Aleix Martinez. Latentpaint: Image inpainting in latent space with diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 43344343, 2024. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. Julian Jorge Andrade Guerreiro, Mitsuru Nakazawa, and Björn Stenger. Pct-net: Full resolution image harmonization using pixel-wise color transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 59175926, 2023. Liu He, Jie Shan, and Daniel Aliaga. Generative building feature estimation from satellite images. IEEE Transactions on Geoscience and Remote Sensing, 61:113, 2023. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. Yifan Jiang, He Zhang, Jianming Zhang, Yilin Wang, Zhe Lin, Kalyan Sunkavalli, Simon Chen, Sohrab Amirghodsi, Sarah Kong, and Zhangyang Wang. Ssh: self-supervised framework for image harmonization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 48324841, 2021. Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion, 2024. Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson WH Lau. Harmonizer: Learning to perform white-box image and video harmonization. In European conference on computer vision, pages 690706. Springer, 2022. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer vision ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Gaugan: semantic image synthesis with spatially adaptive normalization. In ACM SIGGRAPH 2019 Real-Time Live!, pages 11. 2019. Patrick Pérez, Michel Gangnet, and Andrew Blake. Poisson image editing. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 577582. 2023. 11 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Jack Agarwal, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Object compositing with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1831018319, 2023. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, and Daniel Aliaga. Imprint: Generative object compositing by learning identity-preserving representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80488058, 2024. Lorenzo Stacchio. Train stable diffusion for inpainting, 2023. Chong Su, Yingbin Fu, Zheyuan Hu, Jing Yang, Param Hanji, Shaojun Wang, Xuan Zhao, Cengiz Öztireli, and Fangcheng Zhong. Chord: Generation of collision-free, house-scale, and organized digital twins for 3d indoor scenes with controllable floor plans and optimal layouts. arXiv preprint arXiv:2503.11958, 2025. Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolutionrobust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 21492159, 2022. Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Gp-gan: Towards realistic high-resolution image blending. In Proceedings of the 27th ACM international conference on multimedia, pages 24872495, 2019. Zhihua Xu, Tianshui Chen, Zhijing Yang, Chunmei Qing, Yukai Shi, and Liang Lin. Self-supervised emotion representation disentanglement for speech-preserving facial expression manipulation. In ACM Multimedia 2024, 2024. Zhihua Xu, Tianshui Chen, Zhijing Yang, Siyuan Peng, Keze Wang, and Liang Lin. Exploiting temporal audio-visual correlation embedding for audio-driven one-shot talking head animation. IEEE Transactions on Multimedia, 2025. Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing Tang. Dccf: Deep comprehensible color filter learning framework for high-resolution image harmonization. In European conference on computer vision, pages 300316. Springer, 2022. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1838118391, 2023. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Fangneng Zhan, Jiaxing Huang, and Shijian Lu. Adaptive composition gan towards realistic image synthesis. arXiv preprint arXiv:1905.04693, 2, 2019. He Zhang, Jianming Zhang, Federico Perazzi, Zhe Lin, and Vishal Patel. Deep image compositing. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 365374, 2021. Hengshuang Zhao. Zero-shot image editing with reference imitation. Neural Information Processing Systems (NeurIPS), 2024 (10/12/2024-15/12/2024, Vancouver, Canada), 2024."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "Due to the page limit in the main manuscript, we provide additional implementation details and experimental results in this supplementary material to enhance reproducibility and completeness. This supplement covers the following aspects: 1) We present the network architecture and training details of DGAD for better reproducibility. 2) We provide more visualization results and extensive ablation studies for more comprehensive comparative analysis. 3) We also submit our code as zipped supplementary file. Since the training and testing data are publicly available datasets, we provide relevant links to facilitate reproduction. A.1 Implementation Details Training Details . Our model was trained on four NVIDIA RTX 4090 GPUs with batch size of 2 per GPU, resulting in an effective batch size of 8. The training process lasted approximately four days. We utilized the Adam optimizer Kingma [2014] with learning rate of 1 105. To enhance memory efficiency and computational speed, mixed precision training (fp16) was employed. The training was conducted using the accelerate library, incorporating memory-efficient attention via the xformers backend. Input images were resized to resolution of 512 512. During inference, all methods were evaluated using classifier-free guidance (CFG) scale of 7.5 and 50 denoising steps. Network Details . We adopt the inpainting version of Stable Diffusion v1.5 Stacchio [2023] as our backbone model and select the corresponding pretrained BrushNet Ju et al. [2024] as the reference network. Since the Stable Diffusion inpainting backbone already possesses the ability to generate content conditioned on text prompts, we keep both the backbone and BrushNet fixed during training. We only train the additionally introduced standard cross-attention module to learn the geometric properties of the object, as well as the dense cross-attention module to retrieve and align appearance features with their corresponding geometric regions. The dense cross-attention module is specifically developed to establish dense correspondences between the backbone features and appearance features. For clearer and more intuitive understanding, we present the pseudocode of the Dense Cross Attention in Algorithm 1. Conv3x3(q) Conv3x3(k) Conv3x3(v) Conv1x1(fb) Conv1x1(fr) Conv1x1(fr) for = 1 to do Algorithm 1 Dense Cross Attention Require: Feature maps fb, fr RBCHW Ensure: Output feature map output RBCHW 1: function DENSECROSSATTENTION(fb, fr) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end function 19: function MASK_PROCESS(M ) 20: 21: 22: end function end for α Sigmoid(MLP(q)) β MaskProcess(1 α) Reshape q, k, to [B, HW, C] softmax(cid:0)q k/ attn Reshape attn to [B, C, H, ] output attn α + β return output Mclamped clamp(M, 0.5, 0.8) return Mclamped C(cid:1) 13 Figure 5: Qualitative comparison with recent advanced methods. The images shown are sampled from the test dataset. Figure 6: Qualitative comparison with recent advanced methods. The images shown are sampled from the test dataset. A.2 Experiments Qualitative Comparisons . Due to page limitations in the main text, we provide additional visualization results here, covering wider range of object categories and scenes to further validate the effectiveness of our proposed method. Consistent with the results presented in the main text, our approach can edit objects to exhibit desired geometric properties that align with the background scene while preserving their appearance details, outperforming existing methods as shown in Figs. 5, 6, and 7. Ablation Study The results presented in the main text are based on CLIP-derived semantic embeddings. To verify the robustness of the proposed framework across different semantic embeddings, we 14 Figure 7: Qualitative comparison with recent advanced methods. The images shown are sourced from real-world scenes. We separate the foreground and background, then apply each method to perform object composition."
        },
        {
            "title": "Method",
            "content": "IR FID LPIPS DISTS CLIP Score DINO Score Ours (CLIP) Ours (DINO) 61.14 61.16 15.04 14.96 14.94 14. 18.53 18.34 89.38 89.45 69.92 69.99 Table 5: Quantitative analysis of the Ours method under different semantic encoder Oquab et al. [2023], Radford et al. [2021]. further validate it using DINO-derived semantic embeddings. As shown in the Tab. 5, our method, based on DINO-derived semantic embeddings, can also effectively capture the geometric properties of objects (see IR and FID metrics), and can successfully retrieve and align appearance features to the corresponding geometric regions based on the captured object geometry (see LPIPS and DISTS metrics). This demonstrates that the proposed framework is robust to different semantic encoders."
        }
    ],
    "affiliations": [
        "Guangdong University of Technology",
        "South China University of Technology",
        "Sun Yat-sen University"
    ]
}