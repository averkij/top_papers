{
    "paper_title": "Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval",
    "authors": [
        "Aditya Sharma",
        "Luis Lara",
        "Amal Zouaq",
        "Christopher J. Pal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), a modular framework that incorporates a non-parametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios."
        },
        {
            "title": "Start",
            "content": "Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval Aditya Sharma1,3, Luis Lara1, Amal Zouaq1,3, Christopher J. Pal1,2,3 1Mila, 2Canada CIFAR AI Chair, 3Polytechnique Montréal Correspondence: aditya.sharma@mila.quebec"
        },
        {
            "title": "Abstract",
            "content": "The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and outof-distribution errors when producing KG elements like Uniform Resource Identifiers (URIs) based on internal parametric knowledge. This often results in content that appears plausible but is factually incorrect, posing significant challenges for their use in real-world information retrieval (IR) applications. This has led to increased research aimed at detecting and mitigating such errors. In this paper, we introduce PGMR (Post-Generation Memory Retrieval), modular framework that incorporates nonparametric memory module to retrieve KG elements and enhance LLM-based SPARQL query generation. Our experimental results indicate that PGMR consistently delivers strong performance across diverse datasets, data distributions, and LLMs. Notably, PGMR significantly mitigates URI hallucinations, nearly eliminating the problem in several scenarios."
        },
        {
            "title": "Introduction",
            "content": "Question answering (QA) is critical task in natural language processing and remains key goal for artificial intelligence research. Many QA methodologies employ knowledge graphs (KGs) (Saxena et al., 2020; Lan et al., 2021; Peng et al., 2023), which are multi-relational graphs that encode facts. KGs represent world knowledge as fact triples (subject, relation, object), for example, (The Truman Show, nominated for, Academy Award for Best Supporting Actor). The problem of answering questions that require reasoning over KGs (KGQA) can be broken down into two parts, namely, translating the natural language question into its associated SPARQL1 query and then executing the SPARQL query over KG to obtain the final answer (Banerjee et al., 2022). For example, natural language question like \"For which film was Sergei Eisenstein the film editor?\" can be translated into the following SPARQL query - \"select distinct ?sbj where { ?sbj wdt:p1040 wd:q8003 . ?sbj wdt:p31 wd:q11424 },\" where p1040, q8003, p31 and q11424 are Uniform Resource Identifiers (URIs) (described further in Section 3.2) representing entities, relations and properties from KG like Wikidata2. Our approach specifically focuses on the following challenge: translating question into the appropriate semantically accurate SPARQL query and ensuring the precise generation of URIs linked to the entities and relationships mentioned in the question. Using large language models (LLMs) for generating SPARQL queries has become prevalent in recent research (Meyer et al., 2024; Diallo et al., 2024; Banerjee et al., 2022; Qi et al., 2024; Brei et al., 2024). However, this approach can lead to significant challenges, such as hallucinations and out-of-distribution errors, particularly when the models generate KG elements like URIs based only on their internal parametric knowledge. As consequence of these hallucinations, SPARQL queries may be generated with structure that seems correct while incorporating incorrect factual information, such as URIs that dont exist in the KG (Huang et al., 2023; Ji et al., 2023; Huang et al., 2021). This limitation raises concerns about the reliability of LLMs in real-world information retrieval (IR) applications and has driven extensive research into techniques for detecting and mitigating such errors (Lin et al., 2024; Varshney et al., 2023; Dhuliawala et al., 2023; Chern et al., 2023; Li et al., 2023). 1https://www.w3.org/TR/rdf-sparql-query/ 2https://www.wikidata.org 5 2 0 2 9 ] . [ 1 9 6 3 3 1 . 2 0 5 2 : r Retrieval-augmented generation (RAG) (Khandelwal et al., 2019; Lewis et al., 2020a; Guu et al., 2020; Lewis et al., 2020b) is popular paradigm that is often employed for reducing hallucinations (Huang et al., 2023). It uses non-parametric memory for retrieving information in response to question. This information is provided in the prompt for the LLM before generation to enhance the grounding of responses. Despite the integration of retrieval mechanisms, RAG often retrieves noisy or incomplete information. Additionally, since the LLM is not strictly constrained to rely on the retrieved context, it may still generate responses that are not fully grounded, thereby failing to eliminate hallucinations (Barnett et al., 2024). In response to these limitations, we propose PGMR (Post-Generation Memory Retrieval), new modular architecture featuring non-parametric memory retriever module for managing KG elements. This approach enables LLMs to focus on generating SPARQL query syntax while the retriever ensures accurate retrieval of relevant KG elements. PGMR enables the LLM to generate intermediate SPARQL queries with natural language entities and relations enclosed in special tokens, which allows for controlled hallucination. These hallucinations are later corrected and grounded in KG URIs using the retriever. By isolating the tasks of query generation and element retrieval, PGMR improves the reliability and accuracy of SPARQL queries, avoiding the hallucination of non-existent KG URIs and reducing out-of-distribution errors, as discussed in Section 6. In this context, we address the following research questions: (1) Does the integration of memory retrieval module into an LLM-based system enhance performance in SPARQL query generation and mitigate hallucination issues? (2) Does the addition of memory module increase model robustness to out-of-distribution evaluation? This study introduces several key contributions: 1. We explore the implications of adding postgeneration memory retrieval module to LLMs for SPARQL query generation, investigating reduction in LLM hallucination and improvements in query accuracy and resilience against out-of-distribution scenarios. 2. We propose PGMR, an innovative modular architecture where initial SPARQL query syntax generation is performed by an LLM, followed by retrieval of KG elements from non-parametric memory module. 3. We introduce paradigm for letting LLMs hallucinate and fixing these hallucinations postgeneration using non-parametric memory. 4. Our results show that PGMR drastically reduces URI hallucinations while significantly improving performance across LLMs and datasets, with most cases showing an almost complete elimination of hallucinated URIs."
        },
        {
            "title": "2 Related Work",
            "content": "SPARQL Query Generation using LLMs. Even though various models have been proposed for the SPARQL query generation task recently, they suffer from one key problem: they require tagged question data (Banerjee et al., 2022; Reyd and Zouaq, 2023; Diallo et al., 2024; Qi et al., 2024). In tagged data, the input to the model contains the natural language question along with the URIs and the URI labels needed to formulate the corresponding SPARQL query. This requirement presents challenge due to the high costs and labor-intensive nature of tagging, making it unsuitable for practical, real-world use cases. Therefore, our study emphasizes and assesses the untagged versions of the datasets. Reyd and Zouaq (2023) and Diallo et al. (2024) propose copy mechanism for copying URIs from the tagged question while generating the SPARQL query using LLMs. Additionally, they investigate the models performance in dealing with novel question-query structures and unknown URIs. We incorporate this evaluation approach to evaluate the out-of-distribution robustness of our proposed approach. Banerjee et al. (2022) propose pointer generator network-based approach for SPARQL query generation over tagged data using BERT (Kenton and Toutanova, 2019) and using finetuned T5. Their approach generates multiple SPARQL queries and selects the first query that executes and fetches an answer from the knowledge base as the predicted SPARQL query. This evaluation strategy contrasts with our approach of generating only one SPARQL query per question, which is more efficient. Qi et al. (2024) employ pre-training method that integrates the proposed Triple Structure Correction (TSC) strategy. This technique, which requires tagged data, involves randomly interchanging the positions of the subject, Figure 1: As discussed in Section 4.1, PGMR first employs an LLM to produce an intermediate query, where the URI labels are framed by \"starturi\" and \"enduri\" tokens (Section 4.1.1). The retriever (Section 4.1.2) subsequently fetches and replaces these labels with the most similar URIs from the non-parametric memory to create the final SPARQL query. predicate, and object in SPARQL query triples with certain probability alongside Masked Language Modeling (MLM) objective in multi-task learning setup before proceeding with finetuning on the downstream tagged dataset. Mitigating hallucinations in LLMs. Despite their widespread adoption, LLMs are vulnerable to generating factually incorrect information through hallucinations, which affects their reliability in realworld applications (Huang et al., 2023; Ji et al., 2023; Huang et al., 2021). This challenge has led to significant research aimed at hallucination detection and mitigation (Lin et al., 2024; Varshney et al., 2023; Dhuliawala et al., 2023; Chern et al., 2023; Li et al., 2023). Varshney et al. (2023) identify potential hallucination candidates using the LLMs logit outputs, validating their correctness, addressing any detected hallucinations, and then proceeding with the generation process. In contrast, PGMR lets the LLM hallucinate between special tags and corrects these hallucinations post-generation using memory retrieval. The Chain-of-Verification (CoVe) (Dhuliawala et al., 2023) method involves the model first producing an initial draft, then formulating verification questions to validate it by independently answering these questions, and generating the final, verified output. Our method instead relies on nonparametric memory to verify and include factually correct information from KG. External memory augmented LLMs. Earlier studies have investigated methods to augment LLMs by retrieving documents from external memory and integrating them into the contextual framework of tasks to provide relevant information. According to Modarressi et al. (2023) and Modarressi et al. (2024), the lack of specialized memory unit in current LLMs constrains their ability to explicitly store and retrieve knowledge relevant to tasks. This observation supports our studys central premise. They suggest enhancing LLMs with an API-based memory unit for read-write operations to address this limitation. Interactions with the external memory can be implemented using natural language interfaces (Park et al., 2023; Zhou et al., 2023) or formal mechanisms such as standardized APIs that allow the model to parse and execute commands (Schick et al., 2024; Hu et al., 2023; Modarressi et al., 2023, 2024). Our approach is akin to the use of formal mechanism for memory retrieval through our intermediate query format (as described in Section 4.1.1)."
        },
        {
            "title": "3.1 Knowledge Graphs (KGs)",
            "content": "A Knowledge Graph (KG) is multi-relational graph with entities (e.g., \"The Truman Show\" and \"Academy Award for Best Supporting Actor\") as nodes and relations (e.g., \"nominated for\") as typed edges between the entity nodes {s, o}. Thus, each edge of the KG represents fact {s, r, o}, e.g., {The Truman Show, nominated for, Academy Award for Best Supporting Actor}."
        },
        {
            "title": "3.2 Uniform Resource Identifiers (URIs)",
            "content": "Each entity, relation, and property in large KG like Wikidata is associated with Uniform Resource Identifier or URI and natural language URI label. For example, for an entity with the URI label \"The Truman Show,\" the associated Wikidata URI is \"q214801.\" There are two types of URIs in the Wikidata KG namely, \"Q-ids\" which correspond to the entities (e.g., \"q76\" refers to the entity \"Barack Obama\"), and \"P-ids\" which correspond to relations, properties, and classes in the KG (e.g., \"p26\" refers to the relation \"spouse\"). Generating Wikidata URIs within SPARQL query using LLMs presents significant challenge as each URI is just one letter followed by random number. The LLM can only depend on its internal parametric memory to remember these URI mappings."
        },
        {
            "title": "4 Proposed Model",
            "content": "In this section, we present our proposed model for SPARQL query generation using LLMs, termed PGMR (described in Section 4.1). Our model employs an external knowledge base, such as KG, as non-parametric memory to enhance the grounding of LLM generations. In PGMR, the LLM first generates an intermediate query with SPARQL syntax and KG elements in natural language (as discussed in Section 4.1.1) given only the question and then incorporates the KG URIs from memory post-generation to construct the SPARQL query."
        },
        {
            "title": "4.1 Post-Generation Memory Retrieval",
            "content": "(PGMR) LLM-driven SPARQL query generation has emerged as focus of recent research (Meyer et al., 2024; Diallo et al., 2024; Banerjee et al., 2022; Qi et al., 2024; Brei et al., 2024). Despite its potential, this approach is susceptible to significant issues, including hallucinations and out-of-distribution errors, particularly when KG elements like URIs are generated without direct grounding in external knowledge sources. Due to these hallucinations, LLM-generated SPARQL queries may maintain syntactic correctness while introducing factual errors, such as URIs that do not exist within the KG. To address these issues, we introduce PGMR (Post-Generation Memory Retrieval), novel method for SPARQL query generation utilizing LLMs. In PGMR, we initially convert SPARQL queries into an intermediate query format by substituting each URI with special \"starturi\" and \"enduri\" tokens, placing the natural language URI label between them, as outlined in Section 4.1.1. This modified data is then used to either finetune or fewshot the LLM. During inference, our retriever (discussed in Section 4.1.2) converts the intermediate queries generated by the LLM back into SPARQL queries by retrieving the URIs whose labels are the closest match in latent space to the predicted labels within the \"starturi\" and \"enduri\" markers in the intermediate query, as discussed in Section 4.1.3. PGMRs approach of generating only one URI laFigure 2: To teach the LLM, the training data is first converted from (a) SPARQL to (b) intermediate queries, as detailed in Section 4.1.1 bel between each pair of \"starturi\" and \"enduri\" tokens simplifies the retrievers task, thereby increasing the accuracy of retrieving the correct URI. Unlike LLM-based direct SPARQL query generation, which may produce URIs not present in the KG, PGMRs approach of conducting memory retrieval post-LLM generation mitigates the risk of hallucinating non-existent KG URIs, as discussed in Section 6. PGMR enables the LLM to generate intermediate SPARQL queries with natural language entities and relations enclosed in special tokens. These intermediate queries are later grounded in KG URIs using the retriever to produce the final SPARQL query. In PGMR, the LLM generates intermediate SPARQL queries containing natural language entities and relations wrapped in special tokens, which are later grounded in KG URIs through the use of retriever to produce the final SPARQL query. To facilitate this, the training data is first transformed from the (a) SPARQL query to the (b) intermediate query to teach the LLM how to generate intermediate queries"
        },
        {
            "title": "4.1.1 Data Transformation",
            "content": "Predicting the appropriate Wikidata URIs for SPARQL queries poses significant challenge for LLMs because they lack pre-training on this specific data type. The models must rely solely on their parameters to recall mappings between the natural language descriptions of entities and relations in the question, and their corresponding URIs. In contrast, LLMs find it significantly easier to predict natural language labels for these URIs given question. Based on this understanding, we transform the SPARQL queries in our training data into intermediate queries, thus simplifying the SPARQL query generation task. As seen in Figure 2, we Figure 3: The retriever finds each natural language URI label in the generated intermediate query using the special tokens and retrieves the corresponding URI from memory using similarity search, as discussed in Section 4.1.2 . substitute each URI in the original query with its label, which we obtain from Wikidata using its official Python library3. We then surround these labels with \"starturi\" and \"enduri\" tokens, which our memory retriever (described in Section 4.1.2) uses to identify the URI labels locations. 4.1.2 Memory Retriever Memory: We utilize memory module designed as dictionary that pairs LLM encoder-generated embeddings of URI labels and their descriptions as keys with the corresponding URIs as values. This module includes label embeddings and URIs for all URIs in the dataset. Retriever: Given natural language input such as \"The Truman Show,\" our memory retriever locates the URI with the label most similar to this text within the memory. As illustrated in Figure 3, the text is initially encoded into an embedding by an encoder such as BGE (Chen et al., 2024) or MPNet (Song et al., 2020). The retriever then uses FAISS (Douze et al., 2024; Johnson et al., 2019) to retrieve the URI whose label embedding is nearest to the input text embedding from the memory. We use BGE as the encoder for our retriever in this study."
        },
        {
            "title": "4.1.3 Setup",
            "content": "Training Setup: Using the transformed data from Section 4.1.1, we finetune the LLM to generate intermediate queries given the questions. Few-shot Setup: Drawing on the transformed data, we apply few-shot prompting to the LLM, enabling it to generate intermediate queries corresponding to the input question. Inference: During the inference phase, we initially produce an intermediate query for each question using the LLM. The intermediate query includes predicted URI labels marked by \"starturi\" and \"enduri\" tokens. Each of these labels is subsequently 3https://pypi.org/project/Wikidata/ replaced by its corresponding URI through the retriever, thereby forming the final SPARQL query."
        },
        {
            "title": "5 Experimental Setup",
            "content": "5.1 Datasets We evaluate our work on two widely used datasets for SPARQL query generation: LCQUAD 2.0 (Dubey et al., 2019) and QALD-10 (Usbeck et al., 2023). Even though tagged versions of these datasets exist, where the entities and relations in the question are already linked and tagged with associated URIs from the KG, we use the untagged versions. Tagging is expensive, time-consuming, and impractical in the real world. SPARQL query generation becomes much harder problem when using untagged data as it forces the language models to generate KG elements like URIs based on their internal parametric knowledge, which leads to hallucinations and out-of-distribution errors, as discussed in Section 6. LCQUAD 2.0 comprises mix of simple and complex questions with associated SPARQL queries over Wikidata, crafted by human annotators from Amazon Mechanical Turk. The original split of LCQUAD 2.0 includes broad dataset with 21k questions for training, 3k questions for validation, and 6k questions for testing. To specifically test how our models perform in the face of outof-distribution URIs not seen before in training, we also construct an \"unknown URI\" split of LCQUAD 2.0 in line with Reyd and Zouaq (2023). Every query in the test set of this version of the dataset includes at least one URI not seen during training, thus making accurate query generation much harder problem. The \"unknown URI\" split contains 24k train, 3k validation, and 3k test questions and associated queries. QALD-10 is benchmarking dataset that is part of the Question Answering over Linked Data (QALD) challenge series for KGQA using Query EM BLEU URI EM URI Hallucination Model T5-Small T5-Small PGMR (ours) Llama 3.1 8B Llama 3.1 8B PGMR (ours) GPT 4o GPT 4o PGMR (ours) Type finetuned 35.08 % finetuned 74.23 % finetuned 0.72 % finetuned 69.48 % few-shot 1.26 % few-shot 43.53 % 81.87 92. 64.83 89.96 47.03 73.53 38.44 % 80.15 % 0.79 % 78.76 % 4.36 % 69.60 % 27.75 % 0.38 % 89.84 % 0.01 % 81.81 % 0.0 % Table 1: Results on LCQUAD 2.0 (original split): Across different LLMs, PGMR demonstrates substantial reduction in hallucinations while achieving significantly higher Query EM scores than direct SPARQL generation, as discussed in Section 6. SPARQL query generation. Owing to the complexity of SPARQL queries that involve various clauses and literals, QALD-10 is recognized as one of the most challenging and practically applicable datasets in the QALD challenge series. Even though each question in QALD-10 is translated into 8 different languages, we only consider the English versions of the questions. The dataset consists of 412 training questions and 394 test questions, which is much smaller than LCQUAD 2.0 but contains more complex queries. Given the limited availability of training data for QALD-10, we employ two-step approach in our finetuning experiments: pre-training the LLM on LCQUAD 2.0 before finetuning them on QALD-10."
        },
        {
            "title": "5.2 Metrics",
            "content": "To measure the effectiveness of our models, we utilize the following three metrics: BLEU score (Papineni et al., 2002) is popular neural machine translation metric that assesses the predicted query against the gold standard query by comparing tokens. Query EM: We evaluate the query exact match score (Query EM), which directly compares if the predicted query and the reference query are an exact match. URI EM: We assess the URI exact match (URI EM) score, metric that directly evaluates whether the set of all URIs in the predicted SPARQL query matches exactly with those in the reference query. URI Hallucination: We define new metric, called URI Hallucination, which measures the percentage of queries where at least one URI is hallucinated. URI is considered hallucinated if it does not exist in the knowledge base memory. 5.3 Baselines Direct Generation refers to the standard scenario where the LLM directly generates the SPARQL query given question. PGMR: As described in Section 4.1, we implement the proposed PGMR, where SPARQL queries are first converted into an intermediate query, followed by finetuning or few-shot prompting an LLM on this intermediate representation. Our retriever then utilizes non-parametric memory module to translate the intermediate queries generated by the LLM back into SPARQL queries."
        },
        {
            "title": "5.3.1 Language Models",
            "content": "T5: Despite the development of various models for generating SPARQL queries with tagged questions (Banerjee et al., 2022; Diallo et al., 2024; Qi et al., 2024), finetuned T5 (Raffel et al., 2020) is currently the state-of-the-art model for generating SPARQL queries from untagged data on LC-QUAD 2 (Diallo et al., 2024). We use the \"t5-small\" model in our study, as Banerjee et al. (2022) show limited benefit of the added parameters when using larger T5 model for SPARQL query generation. Llama 3.1 8B: We finetune Llama 3.1 8B Instruct (Dubey et al., 2024) to evaluate and compare direct generation to PGMR in our experiments. GPT 4o: Additionally, we employ 25-shot prompting with GPT 4o (Achiam et al., 2023) to assess and compare the few-shot performance of direct generation with PGMR. Unlike the evaluation approach utilized by Banerjee et al. (2022) and Qi et al. (2024), which involves generating multiple queries for each question and selecting the first query that successfully executes and returns an answer as the predicted Query EM BLEU URI EM URI Hallucination Model T5-Small T5-Small PGMR (ours) Llama 3.1 8B Llama 3.1 8B PGMR (ours) GPT 4o GPT 4o PGMR (ours) Type finetuned 0.16 % finetuned 59.97 % finetuned 0.03 % finetuned 71.02% few-shot 0.89 % few-shot 55.61 % 62.91 86. 65.30 89.62 52.02 83.41 0.36 % 78.06 % 0.06 % 79.49 % 2.18 % 68.08 % 61.66 % 4.63 % 91.01 % 0.0 % 86.17 % 0.0 % Table 2: Results on LCQUAD 2.0 (unknown URI split): Even in an out-of-distribution setting, PGMR significantly mitigates hallucinations and enhances Query EM performance compared to direct SPARQL generation across various LLMs, as discussed in Section 6. query, our evaluation generates only single query from our models. Additionally, since LLMs can often generate identical queries with variable names (e.g., ?value and ?s) different from those found in the gold query (e.g., ?uri and ?sbj), we normalize the variable names (e.g., ?var0 and ?var1) in our generated SPARQL queries in line with the evaluation approach followed by Qi et al. (2024)."
        },
        {
            "title": "6 Results",
            "content": "PGMR drastically reduces hallucinations. One of the key limitations of LLMs is their tendency to hallucinate, generating content that appears valid but lacks factual accuracy. This problem is especially pronounced in LLM-based direct SPARQL query generation, where LLMs often fabricate URIs that are not present in the knowledge graph. As seen in Table 1, PGMR reduces URI hallucinations compared to direct SPARQL generation by 27.37% for T5-small, 89.83% for Llama 3.1 8B, and 81.81 % for GPT 4o. This improvement is even more substantial on the unknown URI split of LCQUAD 2.0, where PGMR reduces URI hallucinations by 57.03% for T5-small, 91.01% for Llama 3.1 8B, and 86.17% for GPT 4o, as demonstrated in Table 2. On the harder QALD-10 dataset, direct SPARQL query generation increases URI hallucinations to 66.50% for T5-small, 96.14% for Llama 3.1 8B, and 83.76% for GPT 4o, while PGMR completely eliminates hallucinations for all LLMs tested, as seen in Table 3. These results highlight the effectiveness of PGMR in mitigating URI hallucinations regardless of the LLM, dataset, and data distribution used, with most cases demonstrating an almost complete suppression of hallucinations. This improvement is largely driven by PGMRs approach, which utilizes non-parametric memory module for URI retrieval following the LLM-based generation of an intermediate SPARQL-formatted query, bypassing the dependence on the LLMs internal parametric knowledge. PGMR enhances the quality of SPARQL queries. Across all assessed LLMs, PGMR consistently enhances the accuracy and structure of SPARQL queries on the untagged versions of the LCQUAD 2.0 and QALD-10 datasets. As seen in Table 1, the application of our PGMR approach yields remarkable enhancement in query EM, achieving boost of nearly 37% for T5, 69% for Llama 3.1 8B, and 42% for GPT 4o over the traditional direct SPARQL query generation on the original split of LCQUAD 2.0. PGMR also improves the average BLEU score by almost 21 points across LLMs on the original split of LCQUAD 2.0. As seen in Table 2, this improvement is particularly pronounced on the out-of-distribution unknown URI split of LCQUAD 2.0, where PGMR achieves an average query EM score of more than 62% across LLMs tested, significantly outperforming direct generation, which attains only 0.36% on average. This further highlights the challenges faced by LLMs in directly generating SPARQL queries when the test data is out-of-distribution, and some of the URIs have not been seen during training. On the QALD-10 dataset, our PGMR approach enhances query EM performance over direct generation by approximately 13% for T5, 17% for Llama 3.1 8B, and 22% for GPT 4o, as seen in Table 3. Our results confirm that PGMR not only minimizes hallucinations to great extent but also improves the accuracy and reliability of SPARQL query generation across various LLMs, datasets, Query EM BLEU URI EM URI Hallucination Model T5-Small T5-Small PGMR (ours) Llama 3.1 8B Llama 3.1 8B PGMR (ours) GPT 4o GPT 4o PGMR (ours) Type finetuned 5.83 % finetuned 18.27 % finetuned 0.25 % finetuned 17.22 % few-shot 3.30 % few-shot 25.52 % 25. 30.26 21.74 29.40 38.63 51.66 6.85 % 20.30 % 0.51 % 27.76 % 7.10 % 44.33 % 66.50 % 0.0 % 96.14 % 0.0 % 83.76 % 0.0 % Table 3: Results on QALD-10: PGMR consistently minimizes hallucinations across LLMs while attaining significantly better Query EM scores than direct SPARQL generation, even for the more complex QALD-10 dataset, as discussed in Section 6. and data distributions. PGMR shows resilience to data distribution shifts. PGMR proves to be more resilient to distributional changes than direct SPARQL query generation, demonstrating superior generalization across LLMs as the dataset transitions from the original split of LCQUAD 2.0 to the unknown URI split (as discussed in Section 5.1). As shown in Tables 1 and 2, PGMR mitigates out-of-distribution performance degradation, with its average query EM score across LLMs remaining nearly unchanged, decreasing marginally from 62.4% to 62.2% when transitioning from the original split to the unknown URI split of LCQUAD 2.0. This is in sharp contrast to direct query generation, which experiences substantial degradation in performance, as its query EM score drops from nearly 12% to mere 0.36% on average across LLMs. This superior outof-distribution performance stems from the fundamental difference between PGMR and traditional direct SPARQL query generation. While the latter depends on the LLMs internal parametric knowledge for URI generation, PGMR instead leverages non-parametric memory module to handle KG elements, allowing the LLM to focus on generating syntactically correct SPARQL queries. PGMR increases URI EM. PGMR exhibits substantial enhancement in URI exact match (EM) accuracy across diverse range of LLMs, datasets, and data distributions, further establishing its advantages over conventional direct query generation methods, as evidenced by Tables 1, 2, and 3. On the original LCQUAD 2.0 split, PGMR substantially improves the accuracy of URIs in the generated SPARQL queries and achieves significant URI EM improvements, with score increases of 41.71% for T5, 77.97% for Llama 3.1 8B, and 65.24% for GPT-4o over conventional direct query generation techniques. The impact of PGMR is even more pronounced when the data distribution shifts to the unknown URI split of LCQUAD 2.0, with PGMR gaining 77.7% for T5, 79.43% for Llama 3.1 8B, and 65.9% for GPT 4o over the direct query generation approach, which experiences significant performance degradation across all evaluated LLMs. This trend continues on QALD-10, where PGMR improves the URI EM accuracy by 13.45% for T5, 27.25% for Llama 3.1 8B, and 37.23% for GPT 4o over direct SPARQL query generation."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose PGMR, novel SPARQL query generation framework, which incorporates non-parametric memory module to address the issue of hallucinations in LLM-generated queries. In this framework, LLMs are responsible for generating intermediate SPARQL queries where KG elements like URIs are represented in natural language and enclosed in special tokens, instead of directly producing full SPARQL queries. retriever with non-parametric memory module is utilized to seamlessly integrate the retrieved URIs into the intermediate query, resulting in the final SPARQL query. Despite its simplicity, PGMR consistently demonstrates strong performance in SPARQL query generation across range of datasets, data distribution shifts, and LLMs in both finetuned and few-shot settings. Remarkably, PGMR leads to considerable decrease in URI hallucinations, achieving near-complete elimination of this problem across various instances."
        },
        {
            "title": "Limitations",
            "content": "Even though PGMR exhibits markedly improved performance in generating correct URIs and reducing hallucinations, its effectiveness is tempered by the LLMs ability to construct SPARQL query structures accurately. This is especially evident when looking at the results on the QALD-10 dataset (see Table 3), which features notably complex queries involving diverse clauses and literal expressions compared to LCQUAD 2.0. This limitation may be addressed by using more powerful large LLMs pre-trained specifically over SPARQL query data, combined with the PGMR generation paradigm, but we leave that for future work."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank NSERC and Samsung for supporting this work and CIFAR for their support under the Canada CIFAR AI Chair program."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Debayan Banerjee, Pranav Ajit Nair, Jivat Neet Kaur, Ricardo Usbeck, and Chris Biemann. 2022. Modern baselines for sparql semantic parsing. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 22602265. Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven failure points when engineering retrieval In Proceedings of augmented generation system. the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI, pages 194 199. Felix Brei, Johannes Frey, and Lars-Peter Meyer. 2024. Leveraging small language models for text2sparql tasks to improve the resilience of ai assistance. arXiv preprint arXiv:2405.17076. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216. Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative aia tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495. Papa Abdou Karim Karou Diallo, Samuel Reyd, and Amal Zouaq. 2024. comprehensive evaluation of neural sparql query generation from natural language questions. IEEE Access. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024. The faiss library. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Mohnish Dubey, Debayan Banerjee, Abdelrahman Abdelkawi, and Jens Lehmann. 2019. Lc-quad 2.0: large dataset for complex question answering over wikidata and dbpedia. In The Semantic WebISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 2630, 2019, Proceedings, Part II 18, pages 6978. Springer. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning, pages 39293938. PMLR. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232. Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2021. The factual inconsistency problem in abstractive text summarization: survey. arXiv preprint arXiv:2104.14839. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. IEEE Billion-scale similarity search with gpus. Transactions on Big Data, 7(3):535547. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 41714186. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations. Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. survey on complex knowledge base question answering: Methods, challenges and solutions. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 122. Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne. 2023. Knowledge graphs: Opportunities and challenges. Artificial Intelligence Review, 56(11):1307113102. Jiexing Qi, Chang Su, Zhixin Guo, Lyuwen Wu, Zanwei Shen, Luoyi Fu, Xinbing Wang, and Chenghu Zhou. 2024. Enhancing sparql query generation for knowledge base question answering systems by learning to correct triplets. Applied Sciences, 14(4):1521. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020a. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: largescale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 64496464. Zichao Lin, Shuyan Guan, Wending Zhang, Huiyan Zhang, Yugang Li, and Huaping Zhang. 2024. Towards trustworthy llms: review on debiasing and dehallucinating in large language models. Artificial Intelligence Review, 57(9):243. Lars-Peter Meyer, Johannes Frey, Felix Brei, and Natanael Arndt. 2024. Assessing sparql capabilarXiv preprint ities of large language models. arXiv:2409.05925. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. 2023. Ret-llm: Towards general read-write memory for large language models. arXiv preprint arXiv:2305.14322. Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. 2024. Memllm: Finetuning llms to use an explicit read-write memory. arXiv preprint arXiv:2404.11672. Samuel Reyd and Amal Zouaq. 2023. Assessing the generalization capabilities of neural machine translation models for sparql query generation. In International Semantic Web Conference, pages 484501. Springer. Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4498 4507. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pretraining for language understanding. Advances in neural information processing systems, 33:16857 16867. Ricardo Usbeck, Xi Yan, Aleksandr Perevalov, Longquan Jiang, Julius Schulz, Angelie Kraft, Cedric Möller, Junbo Huang, Jan Reineke, Axel-Cyrille Ngonga Ngomo, et al. 2023. Qald-10the 10th challenge on question answering over linked data. Semantic Web, (Preprint):115. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987. Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. Recurrentgpt: Interactive generation of (arbitrarily) long text. arXiv preprint arXiv:2305.13304."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Hyperparameters Our T5-small model has 60,506,624 trainable parameters and was finetuned on single 80GB Nvidia A100 GPU within 14 hours with batch size of 128. We used learning rate of 0.0015 for finetuning T5 and train for 150 epochs on LCQUAD 2.0 and 30 epochs on QALD-10. Owing to the limited training data for QALD-10, we finetuned our versions of T5 pre-trained on the much larger LCQUAD 2.0 dataset. For our experiments with Llama 3.1 8B, we finetuned the Llama-3.1-8B-Instruct model, training it for 10 epochs using the AdamW optimizer with learning rate 0.0001, no weight decay, and fixed seed 42 to ensure reproducibility. Mixed precision training and quantization were enabled to improve efficiency. We employed parameter-efficient finetuning strategy using LoRA, with low-rank dimension of 8, scaling factor (alpha) of 32, and dropout rate of 0.05. The complete training process took 2 hours, 20 minutes, and 49 seconds, and notably, the model achieved convergence at epoch 5, underscoring its rapid convergence and effectiveness in capturing key aspects of the target data."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Mila",
        "Polytechnique Montréal"
    ]
}