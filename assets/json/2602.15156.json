{
    "paper_title": "Panini: Continual Learning in Token Space via Structured Memory",
    "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory."
        },
        {
            "title": "Start",
            "content": "PANINI: Continual Learning in Token Space via Structured Memory Shreyas Rajesh * 1 Pavan Holur * 1 Mehmet Yigit Turali 1 Chenda Duan 1 Vwani Roychowdhury"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 6 1 ] . [ 1 6 5 1 5 1 . 2 0 6 2 : r Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and userspecific data. common approach is retrievalaugmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present PANINI, which realizes this by representing documents as Generative Semantic Workspaces (GSW)an entityand eventaware network of questionanswer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given query, PANINI only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, PANINI achieves the highest average performance, 5% - 7% higher than other competitive baselines, while using 230 fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time as achieved by the GSW framework yields both efficiency and reliability gains at read time. Code is available here. *Equal contribution 1Department of Electrical and Computer Engineering, University of California, Los Angeles, USA. Correspondence to: Shreyas Rajesh <shreyasrajesh38@ucla.edu>, Vwani Roychowdhury <vwani@ucla.edu>. Preprint. February 18, 2026. 1 Language models have become powerful general-purpose reasoners (Wei et al., 2023; Guo et al., 2025), and through their ability to adapt to new tasks and information in-context (Kojima et al., 2023), are increasingly used to reason over content they were not trained onsuch as new documents, evolving knowledge, and user-specific data. While incontext learning is effective, it faces fundamental scalability limits: as the amount of information grows, maintaining strong downstream performance becomes increasingly difficult, since context windows remain bounded and longcontext failure modes such as lost-in-the-middle (Liu et al., 2024) and context rot (Hong et al., 2025) emerge with scaling. Parametric continual learning (PCL), is natural option (Chen et al., 2019): as significant patterns in new experiences emerge, instead of repeatedly re-injecting the same information into the prompt, one can encode such new patterns into the model itself, i.e. by updating model parameters on relevant data via suitably-defined loss function. While such methods have long history (McCloskey & Cohen, 1989) and recent test-time training/adaptation approaches for LLMs show increasingly strong capabilities (Hu et al., 2025; Yuksekgonul et al., 2026), they introduce substantial challenges: repeated (often expensive) training runs and data curation, risks of catastrophic forgetting (Huang et al., 2024), and poor interactions with the multi-stage posttraining (instruction tuning, preference alignment) that modern LLMs undergo (Qi et al., 2023; Lin et al., 2024). More broadly, there is not yet generally reliable and effective way to disentangle post-training effects, continually train, and then restore aligned instruction-following behavior without re-running post-training or requiring paired checkpoints (Huang et al., 2025; Djuhera et al., 2025). These constraints motivate non-parametric continual learning (NPCL): methods that keep the base model fixed and store new documents and experiences externally, retrieving relevant evidence at inference time. What would genuine NPCL system look like? Consider how humans continually adapt with new information: we integrate each new experience into memoryconsolidating, linking to prior knowledge, and forming structured representations at encoding time that reduce future effort and improve reliability during retrieval time. Panini: Continual Learning via Structured Memory Thus, an NPCL framework must make two inter-related design choices: (a) Cumulative memory architecture: what gets written into memory, especially as the number of documents/experiences scales and (b) Reading Memory at QA: Query-specific retrieval of memory in (a) to minimize testtime compute and maximize accuracy. We argue the quality of these choices should be measured against three criteria: (i) the ability to synthesize and reason over stored experience to answer supported questions (ii) efficiency of retrieval as the experience store grows; and (iii) ability to recognize when stored experience does not support an answer and abstain. These criteria distinguish continual learner from system that merely retrieves and answers queries. The most common practice in implementing NPCL is retrieval-augmented generation (RAG)(Lewis et al., 2021; Karpukhin et al., 2020), which stores documents as verbatim fragments and retrieves relevant passages at query time. Within this space, work has targeted improving both design choices, though the vast majority has focused on (b) improving retrieval. This includes improved dense encoders (Lee et al., 2025; Zhang et al., 2025), agentic methods that interleave reasoning and retrieval (Jin et al., 2025; Trivedi et al., 2023) and even graph-based methods like HippoRAG (Gutiérrez et al., 2025a;b) that build knowledge graph for efficient traversal but still feed verbatim passages to the LLM. While sophisticated, these RAG systems force LLMs to re-process the same chunks repeatedly, paying the full inference cost each time. Moreover, as our results show (Tables 2, 3) chunk-based retrieval can inject irrelevant context that increases unsupported and hallucinatory generation. smaller set of methods do explicitly invest in (a)transforming verbatim text into structured memory. RAPTOR (Sarthi et al., 2024) builds hierarchical summaries via recursive clustering; GraphRAG (Edge et al., 2025) extracts entities and relationships, clusters them into communities, and generates community-level summaries. While these approaches move beyond verbatim storage, their representations are optimized for compression and thematic summarizationanswering questions like what are the main themes? rather than for reasoning across linkages to retrieve specific latent knowledge. To address the above challenges, we present PANINI, an NPCL framework  (Fig. 1)  that strikes balance by investing in both design choices (a), and (b). For (a), what gets written, it builds on the Generative Semantic Workspaces (GSW) representation (introduced by Rajesh et al. (2025); Holur et al. (2024)): an entityand event-aware network of questionanswer pairsstructured memory sufficient for an LLM to reconstruct experienced situations and mine latent knowledge via reasoning-grounded inference chains. For (b), how memory is read, we introduce Reasoning Inference Chain Retrieval (RICR), beam-search style retrieval procedure that decomposes queries and follows reasoning chains through the GSW. As supported by extensive evaluation across six QA benchmarks spanning single-hop and multihop reasoning, PANINI outperforms competitive baselines on all three previously defined NPCL criteria: supported performance and inference-time efficiency (Table 2 and 3), and reliable abstention  (Table 4)  . The main contributions of this paper can be summarized as: (1) Evaluation criteria for NPCL: As discussed, we propose and quantify via extensive QA benchmarking three criteria any NPCL system should satisfysupported performance, inference-time efficiency, and reliable abstention (2) PANINI: novel NPCL framework combining GSWbased structured memory with our chain-following retrieval procedure (RICR), achieving SOTA performance at 530 fewer inference-time tokens. (3) Reliability evaluation: We curate evaluation splits for multi-hop QA benchmarks separating answerable from unanswerable questions to test abstention under missing evidence, on which PANINI outperforms all baselines. 2. Panini To keep this section self contained, we first summarize the GSW and how the representation is indexed for efficient retrieval. We then present RICR, beam-search like procedure that uses single LLM call for question decomposition and follows entity chains through the GSW to accumulate evidence. Finally, we describe the answer generation step that grounds responses in the retrieved QA pairs, and outline the evaluation protocol we use to measure reliability under missing evidence. 2.1. Structured Memory: Representation and Indexing Representation. Let = {di}N i=1 denote corpus of documents. We construct Generative Semantic Workspace (GSW) for each document di following Rajesh et al. (2025). Each GSW Gi = (Ei, Vi, Qi) consists of (i) entity nodes Ei with associated roles and states, (ii) verb-phrase/event nodes Vi capturing actions and relations, and (iii) questionanswer (QA) pairs Qi that attach to verb phrases and point to entities. Concretely, each QA pair is grounded in verb-phrase node Vi and can be viewed as directed labeled edge (v e) where is natural-language question specifying an attribute of the event and Ei is the entity node corresponding to the answer. To illustrate, consider the statement: Barack Obama, the United States, was born the 44th President of on August 4, 1961 in Honolulu, Hawaii. triplet store would encode this via schema-bound facts such as (Obama, date_of_birth, 1961-08-04) and (Obama, place_of_birth, Honolulu). In contrast, 2 Panini: Continual Learning via Structured Memory Figure 1. non-parametric continual learning (NPCL) framework schematics (1) Continual experience: incoming documents are processed asynchronously, potentially by different agents. (2) Individual workspaces: each experience is encoded into Generative Semantic Workspace (GSW). (3) Continually learned global workspace: GSWs can be continually consolidated by reconciling entities, events, and actions both across and within documents. Extensive ablation studies (see Table 11) show that different combinations of LLM models of different sizes for performing different tasks GSW generation, and retrieval lead to consistently robust performance. Thus GSW can be used as shared meta-representation. (4) Reasoning-grounded inference: The goal is to have enough reconciliation but not exhaustive so that all latent knowledge supported by the collection of experiences are represented by inference chains/paths. GSW represents this information as small semantic network: an entity node for Barack Obama (role/state: 44th President), verb-phrase node was born, and entity nodes for August 4, 1961 (state: birth date) and Honolulu, Hawaii. The QA pairs in Qi form the edges between the entities and the verb-phrases, e.g., When was Barack Obama born?August 4, 1961 and Where was Barack Obama born?Honolulu, Hawaii (and inverse forms such as Who was born on August 4, 1961?Barack Obama). This design makes the set of entities associated with an event explicit in short, atomic units, which we leverage in the next subsection for efficient retrieval. full sample GSW can be found in Appendix B. Accessing GSW during retrieval: Dual Indexing. We build two corpus-level indices over the per-document workspaces. First, we create sparse BM25 index over entities, where each entry includes the entity surface form (Barack Obama) together with its associated role/state information from the GSW (44th President). Given query, we score it against this entity index and map the top-ranked matches back to their corresponding entity nodes in the originating document-level GSWs; these matched entities serve as entry points for navigating the local semantic network and identifying the most relevant attached QA pairs. Second, we build dense vector index over all QA pairs extracted from all GSWs, enabling semantic retrieval of candidate QA pairs for each question. Importantly, unlike chunk-based retrieval and many graph-based memories that return long passages or large neighborhoods, our inferencetime evidence is composed only of QA pairs from the GSW, yielding compact, targeted factual support for downstream reasoning and question answering. 2.2. Reasoning inference chain retrieval (RICR) This subsection describes how PANINI reads from the structured memory store to answer query. Given an input query q, the system (i) produces short plan consisting of atomic sub-questions, (ii) assembles compact set of grounded QA pairs by following entity-linked connections in the memory, and (iii) generates final answer conditioned on the retrieved QA evidence. We Planning. instantiate planning module DECOMPOSE() that rewrites an input query into one or more parallel sequences of atomic sub-questions {qi,t(xi,t)}. Each sub-question qi,t() takes an argument xi,t: for initial sub-questions, xi,1 is static entity from the query; for subsequent sub-questions, xi,t = ai,t1, the answer to the preceding sub-question in that sequence. Each sub-question targets single fact that can be resolved by QA pairs from the GSW.The subscript in qi,t() denotes different parallel QA decomposition question sequences Panini: Continual Learning via Structured Memory Figure 2. System overview of PANINI at inference time. Step 1: Planning: decomposition LLM converts the user query into an ordered sequence of single-hop sub-questions. Step 2: RICR: We perform chain-based retrieval by expanding candidate paths hop-by-hop. The initial seed set is obtained via embedding similarity; therefore, for query like Who was Lothair IIs mother?, retrieval may include both Lothair II and the semantically nearby Lothair I. From these seeds, RICR follows QA edges to propose intermediate entities (e.g., candidate mothers) and incrementally extends partial chains across GSWs. Candidate chains are scored at each hop, and low-scoring paths are pruned. Step 3: Answer Generation: Top-ranked chains are de-duplicated and provided to the final answering LLM. often needed for more complex queries. For example, Who died later, the mother of Lothair II or the father of Amadeus I? produces two parallel sequences: (i) q1,1(Lothair II) = Who was the mother of Lothair II? q1,2(a1,1) = When did a1,1 die?; (ii) q2,1(Amadeus I) = Who was the father of Amadeus I? q2,2(a2,1) = When did a2,1 die?. Unlike agentic systems that interleave decomposition and retrieval, PANINI decomposes once; all subsequent steps are non-parametric retrieval and scoring. RICR. Since RICR executes independently for each subquestion sequence i, we describe the procedure for single sequence and drop the subscript for clarity. Given the decomposed sub-questions {qt(xt)}T t=1, RICR retrieves evidence by iteratively executing retrieval hops and assembling the results into scored chains. Hops. hop is an atomic retrieval step that resolves one sub-question. Given an instantiated query qt(xt), the hop searches the GSW and retrieves QA pair whose answer aG can instantiate subsequent sub-questions in the same sequence. Hop Subroutine: Retrieve and Score (the RETRIEVEANDSCORE subroutine used in Algorithm 1). Each hop executes the following retrieval subroutine. Given sub-question qt(xt), we retrieve candidate GSW QA pairs (qG, aG) using dual search approach: (i) query the BM25 entity index to obtain high-scoring entity nodes, then collect the QA pairs attached to those entities; and (ii) query the dense QA-pair index to retrieve semantically similar QA pairs directly. The candidates are merged and reranked using cross-encoder against qt, producing the top-k scored GSW pairs {(qG m, aG m, sm)}k m=1. To advance to hop + 1, GSW answer aG is selected to instantiate the next sub-question: xt+1 = aG . 4 Panini: Continual Learning via Structured Memory Chains. chain is an ordered sequence of GSW QA pairs accumulated across hops. At each hop t, the subquestion qt(xt) retrieves GSW QA pair (qG ) from memory. chain traces: , aG (qG 1 , aG 1 ) (qG 2 , aG 2 ) (qG , aG ), where the answer aG the QA-decomposed sequence: xt+1 = aG . instantiates the next sub-question in B-Chain Construction and Pruning. Because cross-encoder reranking is noisy approximation of true relevance, committing to single top-scoring candidate at each hop risks propagating early errors through the entire chain. We therefore maintain chains in parallel, each selecting different GSW answer from the candidate set. Given chain Cj at the tth step, we score it by the geometric mean of their constituent relevance scores: score(Ct(j)) = (cid:33)1/t (cid:32) (cid:89) l=1 sl,j (1) where sl,j is the relevance score of the GSW pair in chain selected at hop l. We empirically validate this scoring function against alternatives in Appendix E.1.4. At the first hop = 1, corresponding to the sub-question q1(x1) (where x1 is already specified in QA decomposition), the RETRIEVEANDSCORE subroutine returns top-k (sorted by s1,m) candidate QA pairs {(qG m=1. Out of this k, we retain the top-B with unique answers, forming chains {Cj}B j=1 each of length 1 and score s1,j. 1,m, s1,m)}k 1,m, aG t1,j}B t1,j)}B t,m,j, aG (t1),j}B At each subsequent hop > 1, it receives chains from the previous step (t 1), with answers {aG j=1 for the (t 1)th step, and cumulative chain scores {s j=1 as computed in Eqn. 1. We first initiate queries each with one of the answers from step (t 1), {qt(aG j=1 and call RETRIEVEANDSCORE subroutine for each. Each such query (i.e. fixed j) results in QA pairs {(qG t,m,j, st,m,j)}k m=1, where st,m,j is the cross-encoder similarity score between qt(aG t1,j) and qG t,m,j. To merge across chains, we weigh each chains answers by its cumulative scores (see 1): {s t,m,j = (s m=1. The resulting longlist (across all chains = 1, . . . , B) is pruned based on the cumulative scores, and only the top-B entries with unique current answers are retainedif multiple chains propose the same answer aG , only the highest-scoring chain survives. Each winning entry forms new chain of length t: retain the chain up to (t 1) and add the last QA in the winning entry. Then update the cumulative score according to Eqn. 1. (t1),j)(t1) st,m,j}k This ensures diverse exploration across entity paths rather than redundant reasoning toward the same entity (we show Algorithm 1 Reasoning Inference Chain Retrieval (RICR) Input: Sub-questions {qt(xt)}T t=1, beam width B, candidates Output: Evidence set 1: RETRIEVEANDSCORE(q1(x1), k) {Hop 1} 2: Retain top-B by score, one per unique GSW answer 3: for = 2, . . . , (Hops 2T ) do 4: 5: 6: for each with current answer aG Instantiate qt(aG t1) Extend with RETRIEVEANDSCORE(qt, k) Update score(C) ((cid:81)t mean} candidate t1 do each l=1 sl)1/t {Geometric from 7: end for Retain top-B by score, one per unique current answer 8: 9: 10: end for 11: deduplicated GSW pairs from all 12: return in Appendix E.1.5 that even = 1 performs competitively; > 1 provides consistent but modest gains capped at about = 5). After all hops, the top-B surviving chains are collected. Their GSW QA pairsnot the original document chunksare deduplicated and passed to the answer model as evidence. For example, given the query When did Lothair IIs mother die?\"  (Fig. 2)  , hop 1 retrieves candidates for Who was the mother of Lothair II?\", seeding C1 with entity Ermengarde of Tours (s1 = 0.92) and C2 with Ermengarde of Hesbaye (s1 = 0.78). At hop 2, C1 instantiates When did Ermengarde of Tours die?\", retrieving A: 20 March 851\" (s2 = 0.94); C2 instantiates Where did Ermengarde of Hesbaye die?\", retrieving A: Angers\" (s2 = 0.93). Chain scores 0.78 0.93 = 0.85 update to respectively. C1 ranks higherthe geometric mean propagates the stronger first hop. The evidence from C1 is passed to the answer model, which generates: 20 March 851. For queries with multiple parallel sub-question sequences, the same procedure executes independently for each sequence, and the resulting evidence is combined. An end-to-end example of the RICR and its chains can be found in Appendix C. The full procedure is summarized in Algorithm 1. 0.92 0.94 = 0.93 and 3. Experimental Setup 3.1. Datasets We evaluate PANINI through the lens of non-parametric continual learning benchmarks that emphasize (i) factual memory (single-hop retrieval of directly stated fact) and (ii) associativity (composing information across documents, Panini: Continual Learning via Structured Memory where intermediate entities must be discovered and used to reach the final answer). We use single-hop and multi-hop QA as controlled testbed to evaluate these abilities. the answerable subset; Unans measures refusal accuracy on the unanswerable subset (correct iff the model outputs the canonical non-answer token). Multi-hop benchmarks. MuSiQue (Trivedi et al., 2022), 2WikiMultihopQA (Ho et al., 2020), HotpotQA (Yang et al., 2018), and LV-Eval (hotpotwikiqa-mixup 256k) (Yuan et al., 2024) test multi-step reasoning across documents. MuSiQue targets compositional reasoning requiring multiple hops; 2WikiMultihopQA emphasizes diverse multi-hop reasoning patterns over Wikipedia; HotpotQA requires identifying and reasoning over supporting facts from multiple sources. Single-hop benchmarks. NQ (Kwiatkowski et al., 2019) and PopQA (Mallen et al., 2023) test simpler factual retrieval, verifying that structured memory maintains strong performance on straightforward queries. For fair comparison across baselines, we use the same benchmark subsets/splits used in HippoRAG2 (Gutiérrez et al., 2025a). Table 1 summarizes the dataset statistics. 3.2. Platinum: Reliability Under Missing Evidence To evaluate reliability as introduced in 1, we require setting where some queries are genuinely unanswerable from the available corpus. Inspired by the platinum benchmark philosophy (Vendrow et al., 2025), we construct MUSIQUEPLATINUM and 2WIKI-PLATINUM. Construction. We use multi-agent LLM system to produce answers given only the source documents, then manually review for inconsistencies and label each example as answerable or unanswerable. In practice, unanswerable examples arise from missing or insufficient evidence, annotation errors, or ambiguity that prevents single evidence-backed answer. The resulting splits contain 766/153 (ans/unans) for MuSiQue and 906/94 for 2Wiki. Evaluation. All methods use the same prompt instructing the model to answer from evidence or output N/A when insufficient. We report Ans (F1 on answerable subset) and Unans (binary refusal accuracy on unanswerable subset). 3.3. Metrics Performance. Following practice, we report Exact Match (EM) and F1 (Yang et al., 2018; Trivedi et al., 2022). EM measures string match after normalization. F1 computes token-level overlap between predicted and gold answers. Inference-time efficiency. We report the average inferencetime token usage for answering question (measured as total prompt tokens provided to the answer model). Reliability under missing evidence. We report Ans and Unans on our Platinum evaluation (3.2), which separates answerable from unanswerable questions with respect to the available corpus evidence. Ans measures EM/F1 on 3.4. Baselines We compare PANINI against non-parametric memory systems spanning (i) chunk-based retrieval, (ii) structureaugmented memories, and (iii) agentic multi-step retrieval. Chunk-based retrieval. We include BM25 (Robertson & Zaragoza, 2009) as classical sparse baseline over text chunks, and BM25 + reranker. We also evaluate dense retriever based on strong opensource encoder models (Lee et al., 2025; Zhang et al., 2025), and also report dense + reranker. We pass top-5 documents to the answering LLM for all chunk based retrieval methods. Structure-augmented memory systems. We compare against methods that invest additional structure at write time: RAPTOR (Sarthi et al., 2024) (hierarchical summaries), and GraphRAG (Edge et al., 2025). We also include HippoRAG (Gutiérrez et al., 2025a;b) which constructs knowledge graph for retrieval via Personalized PageRank (Haveliwala, 2002), though it returns passage chunks rather than structure augmented summaries. Agentic multi-step retrieval. To represent inference-time iterative planretrieverefine strategies, we evaluate IRCoT (Trivedi et al., 2023) and Search-R1 (Jin et al., 2025) using BM25 as the underlying retriever. For Search-R1, we use the official released agent model, fine-tuned Qwen2.5-7B. We focus on BM25 for these agentic systems in the main paper to control retrieval strength and isolate the cost of iterative inference; additional agentic variants (including dense retrieval) are reported in Appendix E.3.1. Answer models and prompting. We use GPT-4o-mini (OpenAI et al., 2024) as the default answer model across all baselines. We additionally report results using open-source models in Appendix D. Non-GSW baselines use standard evidence prompts from their respective implementations, while PANINI formats retrieved evidence as QA pairs. 3.5. Implementation Details We construct GSW structures using GPT-4.1-mini as described in 2.1, processing each document independently. Question decomposition uses GPT-4o; chain-following retrieval uses beam width = 5 with BM25 over entities and dense retrieval (Qwen3-8B) over QA pairs, reranked by VoyageAI Rerank-2.5. We use GPT-4o-mini as the answer model; for Platinum, the model outputs N/A when evidence is insufficient. Full hyperparameters appear in Appendix and reproduction of the entire framework with open-source models is presented in Appendix D. Panini: Continual Learning via Structured Memory Table 1. Dataset statistics. We use the same benchmark splits as HippoRAG 2 (Gutiérrez et al., 2025a). NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval Queries Passages 1,000 9, 1,000 8,676 1,000 11,656 1,000 6,119 1,000 9,811 124 22,849 4. Results and Discussion We evaluate PANINI along the three NPCL criteria defined in 1. QA performance and efficiency (Table 2 and 3): PANINI achieves the best average (56.1) across six benchmarks, outperforming the strongest structure-augmented baseline HippoRAG2 (53.3) and dense retrieval methods (50.5). Gains are most pronounced on multi-hop tasks, where PANINI also outperforms agentic systems despite using only single decomposition call. Simultaneously, PANINI uses 2.2 fewer tokens than chunk retrieval and 530 fewer than structure-augmented and agentic methods by conditioning the answer model only on short, targeted QA pairs. Reliability under missing evidence  (Table 4)  : Most systems exhibit trade-offweak retrievers abstain often but miss answerable questions; strong retrievers answer more but hallucinate under insufficient evidence. PANINI breaks this trade-off, achieving the highest answerable accuracy (79.8) while maintaining strong refusal accuracy (74.0). We report detailed analysis of computational costs in Appendix G, including write-time indexing costs  (Table 15)  , normalized write and read time requirements along with detailed comparison against strong existing baselines  (Table 19)  . Open-source pipeline. To assess accessibility without proprietary APIs, we evaluate PANINI with fully open-source components (full details in Appendix D). Tables 78 replace both question decomposition (LoRA-finetuned Qwen38B) and answer generation (Qwen3-4B/8B) with opensource models (Yang et al., 2025). While absolute performance decreases, PANINIs advantage over baselines is preservedand in fact widens on multi-hop tasks, where HippoRAG2 degrades more substantially. Platinum results (Tables 910) show similar trends. Finally, Table 11 replaces GSW construction itself with open-source models (Qwen38B/14B, GPT-OSS-120B) (Yang et al., 2025; OpenAI et al., 2025), yielding fully open pipeline where every component fits on single GPU. These experiments also serve as robustness test: smaller models produce noisier GSW extractions (occasional missing verb-phrases or incomplete QA pairs, see Appendix F.2 for detailed breakdown) yet performance degrades gracefully, with GPT-OSS-120B still outperforming all baselines in Table 2 with access to proprietary APIs. This demonstrates that RICRs beam search over multiple chains provides resilience to extraction errors, and that PANINIs gains stem from the framework design rather than extraction fidelity. Ablations. We run extensive ablation studies over all components of PANINI; full results appear in Appendix E, and we summarize the main takeaways here. Reducing beam width from 5 to 3 has minimal impact on accuracy while cutting token usage by about 25%  (Table 13)  . Even singlebeam variant remains competitive, although wider search is more helpful for multi-hop questions. Furthermore, we study the effect of chain-level scoring in beam search by comparing cumulative (geometric-mean) scoring against similarity-only, combined, and greedy last-hop selection variants  (Table 14)  . We also test robustness under corpus growth by holding the relevant evidence fixed and progressively adding distractor documents; results are reported in Appendix E.2. Appendix complements these results with qualitative analysis, highlighting baseline failure modes that PANINI avoids and summarizing PANINIs remaining failure cases. GSW as reusable retrieval infrastructure. The structured memory that PANINI builds at write time is not tied to its own retrieval pipeline. To test this, we replace SearchR1s default BM25 chunk retrieval with PANINIs GSW index by substituting document chunks with structured QA pairs and incorporating RICRs dual-index retrieval, notably we do not perform any retraining of the Search-R1 agent  (Table 16)  . This improves Search-R1s average F1 from 47.3 to 49.4, with consistent gains on multi-hop benchmarks. The result demonstrates that GSW functions as generalpurpose structured memory layer: the one-time investment benefits not only PANINIs lightweight chain retrieval but also agentic systems that were designed for passage-level retrieval. 5. Related Work 5.1. Parametric vs. Non-Parametric Continual Learning Continual learning for language models broadly divides into parametric and non-parametric approaches. Parametric methods update model weights via continual pre-training, instruction tuning, or test-time adaptation (Sun et al., 2020; Wang et al., 2021; Ke et al., 2023). While these can internalize new knowledge, they risk catastrophic forgetting (Kirkpatrick et al., 2017), require expensive retraining, and interact poorly with post-training alignment (Luo et al., 2023). Non-parametric methods keep weights fixed and store information externally, with retrieval-augmented generation (RAG) being the dominant paradigm (Lewis et al., 2021). 7 Panini: Continual Learning via Structured Memory Table 2. Performance (F1 ) comparison across six QA benchmarks. PANINI achieves the highest average performance (56.06%). Bold = best; underline = second best. Retrieval No Retrieval None Sparse Retrieval BM25 BM25 + reranker Dense Retrieval NV-Embed-v2 (7B) Qwen3-Embedding (8B) Qwen3-Embedding (8B) + reranker Structure-Augmented RAG RAPTOR GraphRAG LightRAG HippoRAG HippoRAG 2 Agentic Systems IRCoT Search-R1 PANINI Simple QA Multi-Hop QA NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval Avg 52. 22.7 22.0 36.3 41.0 56.0 58.2 59.9 59.1 61. 54.5 55.5 15.4 52.2 60.0 26.4 47.9 53.0 56.8 55.8 59.8 59.9 55.1 51.3 14.8 56.2 55.7 53.8 49. 25.3 29.9 46.0 39.4 43.7 39.2 42.0 9.3 35.9 49.3 44.2 41.1 35.4 44.1 60.8 56.2 57. 48.4 61.0 12.1 67.3 69.7 64.9 64.9 54.2 63.4 71.0 69.2 68.2 64.7 67.6 20.2 60.0 71.1 64.3 68. 5.0 7.8 8.1 10.0 11.7 11.8 9.2 11.0 5.0 7.6 14.0 9.1 11.5 29. 38.6 43.4 50.6 49.2 50.5 45.2 48.1 12.8 46.5 53.3 43.8 47.2 67.44 57. 52.27 72.37 71.88 14.81 56.06 Table 3. Average token count (Tokens ) in generation context across methods. PANINI uses 230 fewer tokens than competitive baselines. Bold = best; underline = second best. Retrieval Chunk Retrieval Methods Standard Retrieval Structure-Augmented RAG RAPTOR GraphRAG LightRAG Agentic Systems IRCoT Search-R1 PANINI Simple QA Multi-Hop QA Avg NQ PopQA MuSiQue 2Wiki HotpotQA LV-Eval Avg Token Count 701.97 665.40 569.28 544.53 650.35 1099. 705.27 1161.7 8076.4 32601.3 1100.8 7658.9 30894.5 942.1 6554.7 26442.6 900.79 6265.1 25276.5 1076.1 7487.0 30205. 1820.1 12661.2 51066.2 15863.9 2074.6 7583.2 2053.4 11572.7 2241.8 432.37 459. 191.85 7329.6 2426.1 314.71 7628.9 1986.1 288.31 14492.4 3964. 232.11 1166.6 8121.6 32746.0 10745.1 2457.7 319.79 PANINI falls in this category, but differs from standard RAG by writing structured, reusable abstractions rather than storing retrievable text fragments. Crucially, these two paradigms are best used together in an interdependent fashionnon-parametric memory addresses capabilities that parametric learning cannot provide even in principle. This mirrors the Complementary Learning Systems (CLS) framework in neuroscience, which holds that the brain requires both slow-learning neocortical system for extracting statistical regularities and fast-learning hippocampal system for encoding individual episodes (McClelland et al., 1995; Kumaran et al., 2016). The computational root is the stability-plasticity dilemma: distributed representations that enable generalization inherently cause catastrophic interference when updated with new information (French, 1999). Patient studies confirm this dissociationindividuals with hippocampal damage retain the ability to acquire semantic knowledge and motor skills parametrically, yet cannot form new episodic memories, demonstrating that intact parametric learning cannot 8 Table 4. Platinum dataset evaluation with GPT-4o-mini. Ans = answerable, Unans = unanswerable. Bold = best; underline = second best. Panini: Continual Learning via Structured Memory MuSiQue Platinum 2Wiki Platinum Avg Retrieval Ans Unans Ans Unans Ans Unans BM25 BM25 + reranker Qwen3 (8B) Qwen3 + reranker 35.0 42.3 53.3 59.7 Sparse Retrieval 78.4 37.9 74.5 47.9 Dense Retrieval 63.5 63.4 63.8 62.8 79.8 78.7 70.9 67.7 HippoRAG PANINI Structure-Augmented RAG 63.6 75.0 50.3 72. 81.5 84.8 66.7 73.1 36.4 45.1 58.4 61. 72.5 79.9 79.1 76.6 67.2 65.2 58.5 72. compensate for missing episodic store (Vargha-Khadem et al., 1997). In machine learning, analogous limitations appear: parametric knowledge editing degrades after as few as ten updates (Meng et al., 2022), parametric capacity scales at roughly two bits per parameter (Allen-Zhu & Li, 2024), and parametric storage provides no native mechanism for source attribution or selective retraction. Non-parametric memory sidesteps these constraints by maintaining discrete, addressable records that can be individually verified, updated, or removedproperties that are essential for reliable continual learning system. Moreover, the relationship between parametric and non-parametric learning is not merely complementary but directional: by maintaining structured external memory, an NPCL system can identify which knowledge is most valuable to internalizefor instance, entities that recur frequently or serve as hubs linking many stored experienceseffectively using the non-parametric store to guide what should eventually be parametrized. This mirrors the selective consolidation observed in biological memory, where hippocampal replay preferentially consolidates highutility experiences into neocortical long-term storage (Yang et al., 2024). Test-time training. Test-Time Training (TTT) and TestTime Adaptation (TTA) can be viewed as parametric continual learning approaches that operate during inference or deployment. Recent works explore test-time parameter adaptation for both vision models and large language models, where model updates are driven by objectives that do not require access to ground-truth task labels, such as next-token prediction, perplexity minimization, or auxiliary self-supervised losses computed on test-time or retrieved inputs (Sun et al., 2020; Wang et al., 2021; 2022; Hardt & Sun, 2024). By enabling label-free adaptation under distribution shift, TTT naturally supports continual learning in non-stationary environments. However, because adaptation is guided solely by self-supervised signals, these methods may reinforce incorrect predictions, leading to error accumulation or catastrophic forgetting over time. In addition, performing gradient-based updates during inference introduces additional computational overhead and increases serving latency, which can limit practical deployment (Niu et al., 2023; Zhao et al., 2023). Continual fine-tuning. Instead of adapting model parameters at test time using unlabeled inputs, more traditional approach to continual learning is to continually fine-tune the model on stream of new data or tasks in supervised manner. To mitigate catastrophic forgetting and preserve prior capabilities, previous works have explored range of mechanisms, including data replay or rehearsal (LopezPaz & Ranzato, 2017; Rolnick et al., 2019), regularization and constraints on parameter updates (Kirkpatrick et al., 2017; Li & Hoiem, 2017; Zenke et al., 2017), continual or domain-adaptive pre-training (Ke et al., 2023; Jin et al., 2022), as well as parameter-efficient adaptation techniques (Jin et al., 2022; Wang et al., 2023). However, despite its conceptual simplicity, this paradigm faces several practical challenges. In particular, maintaining replay buffers can be memoryand data-intensive, strong regularization may limit model generalizability, and repeated fine-tuning causes substantial computational and operational cost, especially for large-scale models. In-context learning. In-context learning (ICL) enables large language models to adapt to new tasks or distributions by conditioning on examples or instructions provided in the input context, without explicit parameter updates (Brown et al., 2020). As such, ICL can be viewed as form of non-parametric continual learning and serves as the core mechanism underlying broad class of promptbased and RAG-based systems. However, in its standard form, in-context learning does not prescribe how contextual information is selected, stored, or accumulated over time. 5.2. Multi-Hop QA and Agentic Retrieval Multi-hop question answering requires systems to connect information across multiple text passages through intermediate reasoning steps. Benchmarks such as HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and 9 Panini: Continual Learning via Structured Memory MuSiQue (Trivedi et al., 2022) test this capability. The core challenge lies in bridging associative gaps: relevant information may exist in the corpus, but standard retrieval methods fail because query entities do not co-occur with answers in individual documents. Agentic approaches address this through iterative LLMbased reasoning. IRCoT (Trivedi et al., 2023) interleaves chain-of-thought reasoning with retrieval, allowing the LLM to formulate follow-up queries based on intermediate answers. Self-Ask (Press et al., 2023) and ReAct (Yao et al., 2022) follow similar patterns of decomposing questions and retrieving iteratively. While effective, these methods require multiple LLM calls per question, leading to high token usage and latency. Recent work like Search-R1 (Jin et al., 2025) achieves better efficiency through specialized training for search-augmented reasoning, but requires additional training infrastructure. These approaches demonstrate the power of agentic reasoning but also highlight the computational costs of placing intelligence burden on the retrieval process. In contrast, our approach shifts computation to writetime structure and performs multi-hop composition via lightweight chain retrieval with minimal query-time LLM usage. 5.3. Structured Retrieval and Knowledge Graphs Structured approaches include hierarchical document summarization methods like RAPTOR (Sarthi et al., 2024), which build tree structures of summaries at multiple levels of abstraction, and various forms of schema-based or template-based information extraction. These methods trade off between retrieval precision and coverage, often requiring either broad context windows or multiple retrieval rounds. Knowledge graph methods explicitly construct entityrelationship networks from text corpora. HippoRAG (Gutiérrez et al., 2025a) builds knowledge graph connecting all entities across documents with weighted edges, then uses Personalized PageRank (PPR) to traverse this graph during retrieval. While PPR effectively spreads activation across connected entities, this approach has two key limitations. First, it creates dense connectivity: entities that cooccur in text are linked regardless of semantic relatedness, leading to spurious connections (e.g., all dates mentioned in corpus become interconnected). Second, PPR operates on the graph structure without fully leveraging the semantic reasoning capabilities of language modelsthe algorithm propagates activation based on edge weights but does not reason about the meaning of the connections (connection scores are based solely on dense vector similarity). PANINI takes different approach to entity linking. At write time, entities are reconciled only within the scope of single document, where coreference and relational structure can be resolved with high confidence. Cross-document connections are not precomputed; instead, they emerge dynamically at read time through RICRs beam search, which follows entity chains across separate GSW structures with language modelguided scoring at each hop. 5.4. Sleep-Time Compute and Precomputation Recent work on sleep-time compute (Lin et al., 2025) shows that pre-processing context during idle periodsbefore queries arrivecan reduce inference costs while improving accuracy. PANINI embodies similar philosophy: invest computation at write time to save at read time. However, while sleep-time compute produces enriched but unstructured context, PANINI produces structured representationsentity-linked QA networksthat enable multi-hop retrieval via traversal and reranking without additional LLM calls. This allows PANINI to match the compositional reasoning of agentic systems without their expensive, iterative inference loops, effectively trading write-time compute for read-time efficiency. 6. Concluding Remarks and Limitations In this work, we present PANINI, non-parametric continual learning framework that learns from stream of new experiences by writing each document into structured and reusable memory instantiated by the Generative Semantic Workspace (GSW). At read time, PANINI uses novel chain retrieval approach (RICR) for multi-hop reasoning over stored experience, avoiding iterative, LLM-guided retrieval loops. Across six QA benchmarks, PANINI achieves the strongest average performance while using 230 fewer answer-context tokens than competitive baselines, and on our Platinum evaluation it improves reliability by maintaining high accuracy on answerable questions while substantially increasing abstention accuracy when evidence is missing. Beyond PANINIs own pipeline, we show that the GSW functions as general-purpose retrieval infrastructure: replacing Search-R1s document chunks with GSW QA pairswithout any retrainingimproves its multi-hop performance, suggesting that the structured representations built at write time can benefit broad class of downstream retrieval systems. Extensive benchmarking results support our central claim: investing computation at write time to build structured memory yields favorable trade-off: significantly efficient and more reliable inference at read time. Limitations and Future Work. Our current design leaves several important directions open. First, PANINI does not yet perform latent-link caching or experience-driven reconciliation: if certain cross-document relations that recur frequently at inference time, the memory could be updated to reconcile these links and further reduce future retrieval 10 Panini: Continual Learning via Structured Memory cost. Second, while we show that open-source pipelines are viable, constructing high-quality GSWs remains more expensive with proprietary models and less reliable with smaller open-source models; reducing write-time cost and improving robustness of extraction and QA construction are key practical goals. Third, while our lightweight design avoids heavy global reconciliation, richer reconciliation policies such as using an agent to navigate the GSW and reconcile entities and links could strengthen the underlying structure. Additionally, our preliminary results on using GSW as drop-in retrieval layer for Search-R1 suggest that structured memory can benefit systems beyond PANINI; further exploring GSW as general-purpose retrieval infrastructure for diverse downstream frameworks is an active direction of future work. Finally, beyond fact-centric QA, extending PANINI to narrative-heavy settings and multimodal domains (e.g. long-form or streaming video) is natural next step, where evolving entities, spatio-temporal structure, and cross-modal events may benefit even more from structured memory."
        },
        {
            "title": "References",
            "content": "Allen-Zhu, Z. and Li, Y. Physics of language models: Part 3.3, knowledge capacity scaling laws. arXiv preprint arXiv:2404.05405, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language In Larochelle, H., learners. models are few-shot Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf. Chen, L., Singh, S., Kailath, T., and Roychowdhury, V. Brain-inspired automated visual object discovery and detection. Proceedings of the National Academy of Sciences, 116(1):96105, 2019. doi: 10.1073/pnas. 1802103115. URL https://www.pnas.org/doi/ abs/10.1073/pnas.1802103115. Djuhera, A., Kadhe, S. R., Ahmed, F., Zawad, S., and Boche, H. Safemerge: Preserving safety alignment in fine-tuned large language models via selective layer-wise model merging, 2025. URL https://arxiv.org/abs/ 2503.17239. Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S., Metropolitansky, D., Ness, R. O., and Larson, J. From Local to Global: Graph RAG Approach to Query-Focused Summarization, February 2025. URL http://arxiv.org/abs/2404. 16130. arXiv:2404.16130 [cs]. French, R. M. Catastrophic forgetting in connectionist networks. Trends in Cognitive Sciences, 3(4):128135, 1999. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Gutiérrez, B. J., Shu, Y., Gu, Y., Yasunaga, M., and Su, Y. HippoRAG: Neurobiologically Inspired LongTerm Memory for Large Language Models, January URL http://arxiv.org/abs/2405. 2025a. 14831. arXiv:2405.14831 [cs]. Gutiérrez, B. J., Shu, Y., Qi, W., Zhou, S., and Su, From RAG to Memory: Non-Parametric ConY. tinual Learning for Large Language Models, February 2025b. URL http://arxiv.org/abs/2502. 14802. arXiv:2502.14802 [cs]. Hardt, M. and Sun, Y. Test-time training on nearest neighbors for large language models. In International Conference on Learning Representations, 2024. Haveliwala, T. H. Topic-sensitive pagerank. In Proceedings of the 11th International Conference on World Wide Web, WWW 02, pp. 517526, New York, NY, USA, 2002. Association for Computing Machinery. ISBN doi: 10.1145/511446.511513. URL 1581134495. https://doi.org/10.1145/511446.511513. Ho, X., Duong Nguyen, A.-K., Sugawara, S., and Aizawa, A. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. URL https://www.aclweb. org/anthology/2020.coling-main.580. Holur, P., Rajesh, S., Chong, D., and Roychowdhury, Creating an ai observer: Generative semantic V. workspaces, 2024. URL https://arxiv.org/ abs/2406.04555. Hong, K., Troynikov, A., and Huber, J. Context rot: How increasing input tokens impacts llm performance. Technical report, Chroma, July 2025. URL https: //research.trychroma.com/context-rot. 11 Panini: Continual Learning via Structured Memory Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021. Hu, J., Zhang, Z., Chen, G., Wen, X., Shuai, C., Luo, W., Xiao, B., Li, Y., and Tan, M. Test-time learning for large language models. In Singh, A., Fazel, M., Hsu, D., Lacoste-Julien, S., Berkenkamp, F., Maharaj, T., Wagstaff, K., and Zhu, J. (eds.), Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pp. 2482324849. PMLR, 1319 Jul 2025. URL https:// proceedings.mlr.press/v267/hu25z.html. Huang, J., Cui, L., Wang, A., Yang, C., Liao, X., Song, L., Yao, J., and Su, J. Mitigating catastrophic forgetting in large language models with self-synthesized rehearsal. pp. 14161428, 01 2024. doi: 10.18653/v1/2024.acl-long.77. Huang, T., Bhattacharya, G., Joshi, P., Kimball, J., and Liu, L. Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning attack. In Singh, A., Fazel, M., Hsu, D., Lacoste-Julien, S., Berkenkamp, F., Maharaj, T., Wagstaff, K., and Zhu, J. (eds.), Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pp. 2505925074. PMLR, 1319 Jul 2025. URL https://proceedings.mlr. press/v267/huang25b.html. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Jin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X., Arnold, A., and Ren, X. Lifelong pretraining: Continually adapting language models to emerging corpora, 2022. URL https://arxiv.org/abs/2110.08534. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6769 6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 550. URL https://aclanthology.org/2020. emnlp-main.550/. Ke, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B. Continual learning of language models. In International Conference on Learning Representations (ICLR), 2023. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):35213526, 2017. doi: 10. 1073/pnas.1611835114. URL https://www.pnas. org/doi/abs/10.1073/pnas.1611835114. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners, 2023. URL https://arxiv.org/abs/2205.11916. Kumaran, D., Hassabis, D., and McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends in Cognitive Sciences, 20(7):512534, 2016. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models, February 2025. URL http://arxiv.org/abs/ 2405.17428. arXiv:2405.17428 [cs]. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. RetrievalAugmented Generation for Knowledge-Intensive NLP Tasks, April 2021. URL http://arxiv.org/abs/ 2005.11401. arXiv:2005.11401 [cs]. Li, Z. and Hoiem, D. Learning without forgetting, 2017. URL https://arxiv.org/abs/1606.09282. Lin, K., Snell, C., Wang, Y., Packer, C., Wooders, S., Stoica, I., and Gonzalez, J. E. Sleep-time compute: Beyond inference scaling at test-time, 2025. URL https:// arxiv.org/abs/2504.13171. Lin, Y., Lin, H., Xiong, W., Diao, S., Liu, J., Zhang, J., Pan, R., Wang, H., Hu, W., Zhang, H., Dong, H., Pi, R., Zhao, H., Jiang, N., Ji, H., Yao, Y., and Zhang, T. Mitigating the alignment tax of RLHF. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 580606, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.35. URL https:// aclanthology.org/2024.emnlp-main.35/. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Panini: Continual Learning via Structured Memory Association for Computational Linguistics, 12:157173, 2024. Lopez-Paz, D. and Ranzato, M. Gradient episodic memory for continual learning. In NIPS, 2017. Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., and Zhang, Y. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747, 2023. Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98029822, 2023. Marten, R., Vu, T., Ji, C. C.-J., Sharma, K., Pimpalgaonkar, S., Dimakis, A., and Sathiamoorthy, M. Curator: tool for synthetic data creation. https://github.com/ bespokelabsai/curator, January 2025. McClelland, J. L., McNaughton, B. L., and OReilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review, 102(3):419457, 1995. McCloskey, M. and Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. volume 24 of Psychology of Learning and Motivation, pp. 109165. Academic Press, 1989. doi: https://doi.org/10.1016/S0079-7421(08)60536-8. https://www.sciencedirect.com/ URL science/article/pii/S0079742108605368. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems, volume 35, pp. 1735917372, 2022. Niu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao, P., and Tan, M. Towards stable test-time adaptation in dynamic In Internetional Conference on Learning wild world. Representations, 2023. OpenAI, :, Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., adry, A., Baker-Whitcomb, A., Beutel, A., Borzunov, A., Carney, A., Chow, A., Kirillov, A., Nichol, A., Paino, A., Renzin, A., Passos, A. T., Kirillov, A., Christakis, A., Conneau, A., Kamali, A., Jabri, A., Moyer, A., Tam, A., Crookes, A., Tootoochian, A., Tootoonchian, A., Kumar, A., Vallone, A., Karpathy, A., Braunstein, A., Cann, A., Codispoti, A., Galu, A., Kondrich, A., Tulloch, A., Mishchenko, A., Baek, A., Jiang, 13 A., Pelisse, A., Woodford, A., Gosalia, A., Dhar, A., Pantuliano, A., Nayak, A., Oliver, A., Zoph, B., Ghorbani, B., Leimberger, B., Rossen, B., Sokolowsky, B., Wang, B., Zweig, B., Hoover, B., Samic, B., McGrew, B., Spero, B., Giertler, B., Cheng, B., Lightcap, B., Walkin, B., Quinn, B., Guarraci, B., Hsu, B., Kellogg, B., Eastman, B., Lugaresi, C., Wainwright, C., Bassin, C., Hudson, C., Chu, C., Nelson, C., Li, C., Shern, C. J., Conger, C., Barette, C., Voss, C., Ding, C., Lu, C., Zhang, C., Beaumont, C., Hallacy, C., Koch, C., Gibson, C., Kim, C., Choi, C., McLeavey, C., Hesse, C., Fischer, C., Winter, C., Czarnecki, C., Jarvis, C., Wei, C., Koumouzelis, C., Sherburn, D., Kappler, D., Levin, D., Levy, D., Carr, D., Farhi, D., Mely, D., Robinson, D., Sasaki, D., Jin, D., Valladares, D., Tsipras, D., Li, D., Nguyen, D. P., Findlay, D., Oiwoh, E., Wong, E., Asdar, E., Proehl, E., Yang, E., Antonow, E., Kramer, E., Peterson, E., Sigler, E., Wallace, E., Brevdo, E., Mays, E., Khorasani, F., Such, F. P., Raso, F., Zhang, F., von Lohmann, F., Sulit, F., Goh, G., Oden, G., Salmon, G., Starace, G., Brockman, G., Salman, H., Bao, H., Hu, H., Wong, H., Wang, H., Schmidt, H., Whitney, H., Jun, H., Kirchner, H., de Oliveira Pinto, H. P., Ren, H., Chang, H., Chung, H. W., Kivlichan, I., OConnell, I., OConnell, I., Osband, I., Silber, I., Sohl, I., Okuyucu, I., Lan, I., Kostrikov, I., Sutskever, I., Kanitscheider, I., Gulrajani, I., Coxon, J., Menick, J., Pachocki, J., Aung, J., Betker, J., Crooks, J., Lennon, J., Kiros, J., Leike, J., Park, J., Kwon, J., Phang, J., Teplitz, J., Wei, J., Wolfe, J., Chen, J., Harris, J., Varavva, J., Lee, J. G., Shieh, J., Lin, J., Yu, J., Weng, J., Tang, J., Yu, J., Jang, J., Candela, J. Q., Beutler, J., Landers, J., Parish, J., Heidecke, J., Schulman, J., Lachman, J., McKay, J., Uesato, J., Ward, J., Kim, J. W., Huizinga, J., Sitkin, J., Kraaijeveld, J., Gross, J., Kaplan, J., Snyder, J., Achiam, J., Jiao, J., Lee, J., Zhuang, J., Harriman, J., Fricke, K., Hayashi, K., Singhal, K., Shi, K., Karthik, K., Wood, K., Rimbach, K., Hsu, K., Nguyen, K., Gu-Lemberg, K., Button, K., Liu, K., Howe, K., Muthukumar, K., Luther, K., Ahmad, L., Kai, L., Itow, L., Workman, L., Pathak, L., Chen, L., Jing, L., Guy, L., Fedus, L., Zhou, L., Mamitsuka, L., Weng, L., McCallum, L., Held, L., Ouyang, L., Feuvrier, L., Zhang, L., Kondraciuk, L., Kaiser, L., Hewitt, L., Metz, L., Doshi, L., Aflak, M., Simens, M., Boyd, M., Thompson, M., Dukhan, M., Chen, M., Gray, M., Hudnall, M., Zhang, M., Aljubeh, M., Litwin, M., Zeng, M., Johnson, M., Shetty, M., Gupta, M., Shah, M., Yatbaz, M., Yang, M. J., Zhong, M., Glaese, M., Chen, M., Janner, M., Lampe, M., Petrov, M., Wu, M., Wang, M., Fradin, M., Pokrass, M., Castro, M., de Castro, M. O. T., Pavlov, M., Brundage, M., Wang, M., Khan, M., Murati, M., Bavarian, M., Lin, M., Yesildal, M., Soto, N., Gimelshein, N., Cone, N., Staudacher, N., Summers, N., LaFontaine, N., Chowdhury, N., Ryder, N., Stathas, N., Turley, N., Tezak, N., Felix, N., Kudige, N., Keskar, N., Deutsch, N., Bundick, N., Puckett, N., Panini: Continual Learning via Structured Memory Nachum, O., Okelola, O., Boiko, O., Murk, O., Jaffe, O., Watkins, O., Godement, O., Campbell-Moore, O., Chao, P., McMillan, P., Belov, P., Su, P., Bak, P., Bakkum, P., Deng, P., Dolan, P., Hoeschele, P., Welinder, P., Tillet, P., Pronin, P., Tillet, P., Dhariwal, P., Yuan, Q., Dias, R., Lim, R., Arora, R., Troll, R., Lin, R., Lopes, R. G., Puri, R., Miyara, R., Leike, R., Gaubert, R., Zamani, R., Wang, R., Donnelly, R., Honsby, R., Smith, R., Sahai, R., Ramchandani, R., Huet, R., Carmichael, R., Zellers, R., Chen, R., Chen, R., Nigmatullin, R., Cheu, R., Jain, S., Altman, S., Schoenholz, S., Toizer, S., Miserendino, S., Agarwal, S., Culver, S., Ethersmith, S., Gray, S., Grove, S., Metzger, S., Hermani, S., Jain, S., Zhao, S., Wu, S., Jomoto, S., Wu, S., Shuaiqi, Xia, Phene, S., Papay, S., Narayanan, S., Coffey, S., Lee, S., Hall, S., Balaji, S., Broda, T., Stramer, T., Xu, T., Gogineni, T., Christianson, T., Sanders, T., Patwardhan, T., Cunninghman, T., Degry, T., Dimson, T., Raoux, T., Shadwell, T., Zheng, T., Underwood, T., Markov, T., Sherbakov, T., Rubin, T., Stasi, T., Kaftan, T., Heywood, T., Peterson, T., Walters, T., Eloundou, T., Qi, V., Moeller, V., Monaco, V., Kuo, V., Fomenko, V., Chang, W., Zheng, W., Zhou, W., Manassra, W., Sheu, W., Zaremba, W., Patil, Y., Qian, Y., Kim, Y., Cheng, Y., Zhang, Y., He, Y., Zhang, Y., Jin, Y., Dai, Y., and Malkov, Y. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. OpenAI, :, Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., Barak, B., Bennett, A., Bertao, T., Brett, N., Brevdo, E., Brockman, G., Bubeck, S., Chang, C., Chen, K., Chen, M., Cheung, E., Clark, A., Cook, D., Dukhan, M., Dvorak, C., Fives, K., Fomenko, V., Garipov, T., Georgiev, K., Glaese, M., Gogineni, T., Goucher, A., Gross, L., Guzman, K. G., Hallman, J., Hehir, J., Heidecke, J., Helyar, A., Hu, H., Huet, R., Huh, J., Jain, S., Johnson, Z., Koch, C., Kofman, I., Kundel, D., Kwon, J., Kyrylov, V., Le, E. Y., Leclerc, G., Lennon, J. P., Lessans, S., Lezcano-Casado, M., Li, Y., Li, Z., Lin, J., Liss, J., Lily, Liu, Liu, J., Lu, K., Lu, C., Martinovic, Z., McCallum, L., McGrath, J., McKinney, S., McLaughlin, A., Mei, S., Mostovoy, S., Mu, T., Myles, G., Neitz, A., Nichol, A., Pachocki, J., Paino, A., Palmie, D., Pantuliano, A., Parascandolo, G., Park, J., Pathak, L., Paz, C., Peran, L., Pimenov, D., Pokrass, M., Proehl, E., Qiu, H., Raila, G., Raso, F., Ren, H., Richardson, K., Robinson, D., Rotsted, B., Salman, H., Sanjeev, S., Schwarzer, M., Sculley, D., Sikchi, H., Simon, K., Singhal, K., Song, Y., Stuckey, D., Sun, Z., Tillet, P., Toizer, S., Tsimpourlas, F., Vyas, N., Wallace, E., Wang, X., Wang, M., Watkins, O., Weil, K., Wendling, A., Whinnery, K., Whitney, C., Wong, H., Yang, L., Yang, Y., Yasunaga, M., Ying, K., Zaremba, W., Zhan, W., Zhang, C., Zhang, B., Zhang, E., and Zhao, S. gpt-oss-120b and gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 56875711, 2023. Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to!, 2023. URL https://arxiv.org/abs/2310.03693. Rajesh, S., Holur, P., Duan, C., Chong, D., and Roychowdhury, V. Beyond fact retrieval: Episodic memory for rag with generative semantic workspaces, 2025. URL https://arxiv.org/abs/2511.07587. Robertson, S. and Zaragoza, H. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. ISSN 1554-0669, 1554-0677. doi: 10.1561/ 1500000019. URL http://www.nowpublishers. com/article/Details/INR-019. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., Experience replay for continual and Wayne, G. In Wallach, H., Larochelle, H., Beygelzlearning. imer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. Information Process- (eds.), Advances in Neural ing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper. pdf. Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, January 2024. URL http://arxiv.org/abs/2401. 18059. arXiv:2401.18059 [cs]. Sun, Y., Wang, X., Zhuang, L., Miller, J., Hardt, M., and Efros, A. A. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Musique: Multihop questions via single-hop question composition, 2022. URL https://arxiv.org/ abs/2108.00573. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Interleaving retrieval with chain-of-thought reasoning In Profor knowledge-intensive multi-step questions. ceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers), pp. 1001410037, 2023. 14 Panini: Continual Learning via Structured Memory Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang, M., Tan, Z., Yao, Z., Lin, D., Li, B., et al. Lv-eval: balanced long-context benchmark with 5 length levels up to 256k. arXiv preprint arXiv:2402.05136, 2024. Yuksekgonul, M., Koceja, D., Li, X., Bianchi, F., McCaleb, J., Wang, X., Kautz, J., Choi, Y., Zou, J., Guestrin, C., and Sun, Y. Learning to discover at test time, 2026. URL https://arxiv.org/abs/2601.16175. Zenke, F., Poole, B., and Ganguli, S. Continual learning through synaptic intelligence. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 39873995, International Convention Centre, Sydney, Australia, 06 11 Aug 2017. PMLR. URL http://proceedings. mlr.press/v70/zenke17a.html. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Zhao, H., Liu, Y., Alahi, A., and Lin, T. On pitfalls of test-time adaptation. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 4205842080. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/zhao23d.html. Vargha-Khadem, F., Gadian, D. G., Watkins, K. E., Connelly, A., Van Paesschen, W., and Mishkin, M. Differential effects of early hippocampal pathology on episodic and semantic memory. Science, 277(5324):376380, 1997. Vendrow, J., Vendrow, E., Beery, S., and Madry, A. Do large language model benchmarks test reliability? arXiv preprint arXiv:2502.03461, 2025. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=uXl3bZLkr3c. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual testtime domain adaptation. In Proceedings of Conference on Computer Vision and Pattern Recognition, 2022. Wang, X., Chen, T., Ge, Q., Xia, H., Bao, R., Zheng, R., Zhang, Q., Gui, T., and Huang, X. Orthogonal subspace learning for language model continual learning. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1065810671, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp. 715. URL https://aclanthology.org/2023. findings-emnlp.715/. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models, 2023. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yang, W., Sun, C., Huszár, R., Hainmueller, T., Kiselev, K., and Buzsáki, G. Selection of experience for memory by hippocampal sharp wave ripples. Science, 383(6690):14781483, 2024. doi: 10.1126/ science.adk8261. URL https://www.science. org/doi/abs/10.1126/science.adk8261. 15 Panini: Continual Learning via Structured Memory"
        },
        {
            "title": "Appendix",
            "content": "Our technical appendix is structured as follows: 1. Appendix A: Prompts to the LLM. 2. Appendix B: GSW Representation Example. 3. Appendix C: RICR Example. 4. Appendix D: Open-Source Experiments. 5. Appendix E: Ablation Studies. 6. Appendix F: Qualitative Analysis. 7. Appendix G: Computational Costs and Resources. 16 Panini: Continual Learning via Structured Memory A. Prompts to the LLM In this section, we describe the prompt used for GSW construction, Question Decomposition, and Answering Models. A.1. GSW Representation We present the prompt used to generate the GSW representation in Figure 4. The full prompt is substantially longer and includes detailed examples for each instruction; for clarity and space considerations, we omit these examples in the figures and retain only the core instructional text. A.2. Question Decomposition We present the prompt for the question decomposition models in Fig 6. A.3. Answering Model We present the prompt for the answering models in Fig 7. B. GSW Representation Example Figure 3 provides concrete example of our per-document Generative Semantic Workspace (GSW) representation: given raw passage (top), we extract (i) entity nodes annotated with roles and states, and (ii) verb-phrase nodes and (iii) questionanswer pairs that connect verb phrases to entities. C. RICR Example We present an example RICR trace for PANINI in Figure 8. We report the question decomposition, hop subroutine, chain construction and pruning, and the final evidence used for answer generation. The raw trace logs are substantially longer (including the full top-k entity lists, all reranked QA pairs, and complete beam states); for clarity and space considerations, we omit these details in the figures and retain only the highest-ranked retrieved items and the surviving chains at each hop. D. Open-Source Experiments To assess the accessibility and robustness of PANINI without reliance on proprietary modelswhich might be crucial for applications with strict data privacy requirementswe evaluate the framework using open-source components. We organize these experiments along two complementary axes corresponding to read-time and write-time operations. First, we replace the proprietary question decomposition and answer generation models with open-source alternatives, while keeping GSW construction fixed (using GPT-4.1-mini). Second, we analyze the impact of varying the open-source model used for GSW construction, with read-time components held constant. Finally, we combine these settings to evaluate fully open-source pipeline, where every componentfrom memory construction to final answeringruns on open-source models. D.1. Open-Source Models at Read Time D.1.1. EXPERIMENTAL SETUP AND IMPLEMENTATION"
        },
        {
            "title": "DETAILS",
            "content": "Question Decomposition. In the open-source read-time configuration, we use Qwen3-8B + LoRA and Qwen3-4B + LoRA decomposition models fine-tuned on curated gold decompositions. GPT-5 is used as teacher model to generate high-quality decomposition annotations for training data construction. Decomposition is performed greedily (temperature = 0.0) with thinking disabled for Qwen3-8B + LoRA and Qwen3-4B + LoRA. GSW Construction. We use GPT-4.1-mini as the primary proprietary model for GSW construction, consistent with the main experiments presented in Table 2. Embedding model. For dense retrieval and embeddingbased matching, we use Qwen3-Embedding-8B with FAISS indexing. Reranking model. For reranking top-k retrieved we use VoyageAI Rerank-2.5. Reader models. We evaluate the following open-source models as readers without any finetuning or prompt optimization, using the same prompt we used with GPT-4o-mini for our main experiments. Qwen3-4B: Table 7 (thinking disabled). Qwen3-8B: Tables 8 presents supported performance and 9 presented performance on the platinum set. Qwen3-8B (thinking mode): Table 10. As seen across models, we note that while performance drops across the board for all evaluated methods, PANINI suffers the least in terms of supported performance loss as well as abstention rates as measured by our platinum evaluation. D.1.2. TRAINING AND GENERATION HYPERPARAMETERS In this section we detail our training and inference setup of the trained question decomposition models. Question Decomposition: Data Training data is derived from the MuSiQue training set and consists of 1,780 question decomposition examples. Examples are grouped by 17 Panini: Continual Learning via Structured Memory hop structure following the MuSiQue taxonomy: 2-hop, 3-hop (3hop1, 3hop2), and 4-hop (4hop1, 4hop2, 4hop3). Table 6 summarizes the final dataset composition by hop type. Decompositions are generated by prompting GPT-5 with our decomposition prompt on MuSiQue training questions. All generated decompositions are manually reviewed, and incorrect, redundant, or overly decomposed outputs are filtered out to ensure correctness and minimality. Question Decomposition: Training The Qwen3-4B and Qwen3-8B decomposition model is fine-tuned using LoRA (Hu et al., 2021) adapter with the following configuration: LoRA rank = 256 learning rate = 2 10 batch size = 8 number of epochs = 3 Question decomposition: Inference Once trained, we use the question decomposer with temperature set to 0. Answer generation. For Qwen-based readers, we evaluate both non-thinking and thinking-enabled decoding modes. Table 5 summarizes the decoding hyperparameters used for answer generation under non-thinking and thinking-enabled configurations. Table 5. Decoding hyperparameters for answer generation with open-source reader models. Parameter Non-thinking Mode Thinking Mode Temperature Top-p Top-k Max tokens Repetition penalty Presence penalty Frequency penalty 0.6 0.95 20 4096 0.6 0.95 20 12, 288 1.1 0.3 0.3 Table 6. Distribution of question decomposition training data by hop type. Hop Type Number of Examples 2-hop 3hop1 3hop2 4hop1 4hop2 4hop3 Total 494 441 406 225 49 165 1,780 18 D.1.3. QA WITH OPEN-SOURCE READERS We next evaluate performance on standard QA benchmarks without explicit unanswerable questions. Table 7 reports results using Qwen3-4B as the end-to-end answerer model across mix of simple and multi-hop datasets, including MuSiQue, 2Wiki, and HotpotQA. Despite the limited capacity of the reader, PANINI consistently outperforms lexical, dense, and graph-based retrieval baselines on multi-hop benchmarks. Scaling the answerer model to Qwen3-8B improves absolute performance across all methods. As shown in Table 8, PANINI continues to deliver the strongest or near-strongest results on multi-hop datasets, achieving the highest scores on MuSiQue and 2Wiki. These results indicate that structured GSW construction and chain-following retrieval remain effective as answerer model capacity increases. In particular, the Qwen3-8B configuration recovers most of the performance of the proprietary answerer model setting, demonstrating that PANINI can operate near closed-source performance levels using fully open-source components. D.1.4. PLATINUM BENCHMARK EVALUATION We first evaluate read-time open-source pipelines on Platinum benchmarks, which explicitly include unanswerable questions. Performance is reported separately for answerable questions (Ans) and unanswerable questions (Unans), where higher unanswerable accuracy reflects improved abstention and lower hallucination rates. Table 9 reports results on MuSiQue Platinum and 2Wiki Platinum using Qwen3-8B as the end-to-end answerer without thinking mode. PANINI achieves the strongest answerable accuracy across both datasets, substantially outperforming lexical, dense, and graph-based baselines. In particular, PANINI attains an average answerable accuracy of 75.60%, compared to 66.98% for the strongest competing baseline (HippoRAG 2). Table 10 reports results with thinking mode enabled for the answerer model. While thinking mode improves absolute answerable accuracy across most methods, PANINI remains the top-performing approach. The relative ordering of methods remains largely unchanged, indicating that the gains of PANINI arise primarily from structured retrieval rather than reader-side reasoning traces. Lexical baselines such as BM25 achieve high unanswerable accuracy by aggressively abstaining, but this behavior results in substantially lower answerable accuracy. In contrast, PANINI maintains competitive unanswerable performance while substantially improving answerable accuracy, indicating better calibration rather than reliance on excessive abstention. This balance is critical in Platinum settings, Panini: Continual Learning via Structured Memory Table 7. Performance comparison across multi-hop QA benchmarks with Qwen3-4B (no-think mode). Experiments were run on GPT-4.1-mini-generated GSW memories, using Qwen3-4B (no-think mode) as the reader model and Qwen3-4B + LoRA for question decomposition (open-source PANINI pipeline). Bold = best; underline = second best. Retrieval MuSiQue 2Wiki HotpotQA Avg BM25 BM25 + reranker Sparse Retrieval 17.07 23.16 Dense Retrieval Qwen3-Embedding (8B) Qwen3-Embedding (8B) + reranker 40.14 46.30 36.74 43.78 56.50 58.34 HippoRAG 2 PANINI Structure-Augmented RAG 42.20 49.46 66.79 69.09 48.30 57.67 69.47 72. 70.33 69.50 34.04 41.54 55.37 58.89 59.77 62. Table 8. Performance comparison across QA benchmarks with Qwen3-8B(no-think mode). Experiments were run on GPT-4.1mini-generated GSW memories, using Qwen3-8B (no-think mode) as the reader and Qwen3-8B + LoRA for question decomposition (open-source PANINI pipeline). Bold = best; underline = second best. Retrieval NQ PopQA MuSiQue 2Wiki HotpotQA Avg Simple QA Multi-Hop QA BM25 BM25 + reranker Qwen3-Embedding (8B) Qwen3-Embedding (8B) + reranker HippoRAG 2 PANINI 18.96 24. 45.01 53.57 Sparse Retrieval 47.61 51.27 Dense Retrieval 39.97 59.74 59.90 61.27 45.60 60.01 Structure-Augmented RAG 45.40 55.51 60.01 65.31 57.21 52.43 29.28 38. 55.66 58.10 66.93 70.28 48.80 59.95 69.26 72.29 72. 72.14 37.93 45.55 56.91 59.45 60.02 63.47 Table 9. Platinum dataset evaluation with Qwen3-8B (no-think mode). Ans = F1 score on the answerable subset. Unans = refusal accuracy on the unanswerable subset (binary; correct iff the output is the canonical non-answer token N/A after normalization). Experiments were run on GPT-4.1-mini-generated GSW memories, using Qwen3-8B (no-think mode) as the reader and Qwen3-8B + LoRA for question decomposition (open-source PANINI pipeline). Bold = best; underline = second best. Retrieval Ans Unans Ans Unans Ans Unans MuSiQue Platinum 2Wiki Platinum Avg BM25 BM25 + reranker Sparse Retrieval 20.17 27.79 82.35 76.47 Dense Retrieval Qwen3-Embedding (8B) Qwen3-Embedding (8B) + reranker 40.88 48.14 66.01 62.09 30.24 40.37 49.02 56.68 88.30 83.33 75.18 66. 25.21 34.08 44.95 52.41 85.33 79.90 70.60 64.38 HippoRAG 2 PANINI Structure-Augmented RAG 53.66 68.08 50.46 55.56 80. 63.48 66.98 56.97 83.11 67.02 75. 61.29 19 Panini: Continual Learning via Structured Memory where unanswerable accuracy directly reflects hallucination control. D.2. End-to-end Open-Source Pipeline D.2.1. EXPERIMENTAL SETUP AND IMPLEMENTATION"
        },
        {
            "title": "DETAILS",
            "content": "To assess the robustness of GSW construction beyond large proprietary models, and to support settings where data governance or safety constraints require fully local deployment, we vary the open-source model used for GSW construction. Unless otherwise noted, all experiments in this subsection use fully open-source read-time configuration: question decomposition is performed by LoRA-finetuned Qwen38B decomposer, and answer generation uses Qwen3-8B in no-thinking mode. Full read-time model details and decoding/training hyperparameters follow Appendix D.1. Write-time GSW construction models. We instantiate GSW construction using the following open-source LMs: Qwen3-8B, Qwen3-14B, and GPT-OSS-120B. We additionally evaluate two-pass refinement variant for Qwen314B, where the model performs an initial GSW extraction followed by self-refinement pass to repair missing entities, verb-phrase nodes, and malformed QA pairs. D.2.2. RESULT ANALYSIS Results on representative multi-hop QA benchmarks (MuSiQue and 2Wiki) are summarized in Table 11. We observe that GSW quality and downstream QA performance scale consistently with model capacity. Performance improves approximately monotonically as model size increases, with Qwen3-14B outperforming Qwen3-8B, and GPT-OSS-120B achieving the strongest overall performance, achieving competitive performance relative to the main PANINI configuration that uses proprietary models for GSW construction, question decomposition, and answer generation. These results indicate that both GSW construction and the overall PANINI pipeline remain effective under fully open-source model configuration. D.2.3. TWO-PASS GSW REFINEMENT Further analysis suggests that the performance gap observed with smaller models arise primarily during GSW construction: smaller models occasionally omit critical verb-phrase nodes, fail to extract complete event / verb nodes, or generate incorrect or incomplete questionanswer pairs. Such local structural and information omissions can propagate during chain-following retrieval, resulting in broken or truncated reasoning paths at inference time. To mitigate these issues while preserving the cost advantages of smaller open-source models, we introduce twopass GSW construction strategy. In the first pass, the model 20 generates an initial GSW representation from the document. In the second pass, the same model is prompted to explicitly inspect the partially constructed GSW and to repair structural deficiencies by adding missing entities or relations, and correcting malformed questionanswer pairs. The prompt for the second pass models is shown in Figure 5. As shown in Table 11, the second-pass refinement consistently improves downstream performance. This shows that smaller open-source constructor can reach strong GSW quality when paired with lightweight multi-step refinement pass, leading to competitive downstream performance. E. Ablation Studies In this section, we analyze the key design choices of both PANINI and competitive baselines. First, we conduct ablation studies on the core components of our Reasoning Inference Chain Retrieval (RICR) procedure. Second, we evaluate agentic retrieval variants to benchmark PANINIs single-pass retrieval design against iterative read-time reasoning frameworks. E.1. RICR Design Ablation Studies E.1.1. ABLATION: QUESTION DECOMPOSITION We ablate question decomposition, which converts multihop question into an ordered sequence of single-hop subquestions prior to retrieval. In the full system, retrieved intermediate answers are injected into later hops using placeholders (e.g., <ENTITY_Q1>) to enforce chain-following retrieval. In the no decomposition variant, we bypass decomposition and issue single retrieval step for the original question, followed by answer generation from the retrieved evidence. Table 12 reports the resulting performance. Disabling decomposition substantially reduces accuracy, showing that explicitly exposing intermediate hops is important for retrieving the correct bridging evidence. E.1.2. ABLATION: DUAL SEARCH We ablate the dual-search retrieval strategy used to propose candidates at each hop. In the full system, PANINI retrieves (i) entity candidates and (ii) QA-pair candidates, and combines them before reranking and beam expansion. In the entity-only variant, we disable QA pair retrieval and rely solely on entity retrieval to generate evidence; in the QA-only variant, we disable entity retrieval and rely only on retrieved QA pairs. Table 12 reports the resulting performance. Dual search improves robustness by providing complementary candidate sources: entity-only retrieval is weaker, while QA-only retrieval is closer to the full system but still slightly underperforms in multi-hop settings. E.1.3. ABLATION: QA RERANKING E.2. Continual Learning Ablation Studies Panini: Continual Learning via Structured Memory We ablate the QA reranking stage used to select the top-k evidence QA pairs after retrieval. In the full system, we retrieve candidate pool of QA pairs and rerank them with Voyage rerank-2.5, retaining the top-k for beam expansion and final answer generation. In the no QA reranking variant, we skip reranking and instead keep the top-k QA pairs in the original retrieval order. Table 12 reports the resulting performance. Removing reranking substantially degrades multi-hop performance, indicating that accurate evidence prioritization is critical for maintaining high-quality beams. E.1.4. ABLATION: CHAIN-LEVEL SCORING We ablate the chain-level scoring function used to rank beams during multi-hop retrieval. Let = {(qi, ai)}n i=1 denote reasoning chain and let si be the Voyage reranker score for hop (Voyage rerank-2.5). Our main scoring rule aggregates hop-wise quality using the geometric mean, Scum(C) = (cid:33)1/n , (cid:32) (cid:89) i=1 si which strongly penalizes chains containing any weak hop. As alternatives, we evaluate similarity-only score based on cosine similarity between the embeddings of the original question Qorig and the linearized chain L(C), Ssim(C) = cos(cid:0)Embed(Qorig), Embed(L(C))(cid:1) and combined score that linearly interpolates local and global signals, Scomb(C) = αScum(C) + (1 α)Ssim(C), α = 0.5. Finally, none (greedy) disables chain-level scoring and ranks beams using only the most recent hop score (i.e., last-hop reranker score sn at each step). Table 14 reports the resulting performance. The main cumulative scoring rule performs best, while the similarity-only and combined variants underperform, suggesting that prioritizing consistently strong hop-level evidence is important for effective beam search; greedy last-hop selection is competitive but still falls short of the cumulative rule. E.1.5. ABLATION: BEAM WIDTH We ablate the beam width used during chain-following retrieval, i.e., the maximum number of partial chains retained after pruning at each hop. We compare the default setting (B=5) to wider search (B=10) and narrower search (B=3 and B=1), keeping all other hyperparameters fixed. Table 13 reports both F1 and average token usage over 1,000 questions. Reducing the beam from 5 to 3 preserves accuracy while substantially reducing token usage, whereas single-beam setting (B=1) reduces token usage further but degrades performance, especially on multi-hop questions. As retrieval-augmented QA systems are deployed in realworld settings, they must remain reliable as the underlying corpus continuously grows. To isolate the effect of corpus expansion, we design controlled robustness experiment on MuSiQue where we fix an evaluation set of 200 questions and progressively enlarge the retrieval corpus from 4K passages to the full collection (12K). Importantly, all gold-supporting passages for these questions are already present in the initial 4K subset; thus, the number of relevant documents remains constant while the corpus grows only by adding distractor passages. This setup directly probes robustness to an increasing search space (and potential retrieval noise) without changing the underlying evidence needed to answer the questions. Figure 9 reports F1 as the corpus expands. We observe that methods relying on lightweight structured memory, PANINI , degrade substantially less under expansion, maintaining more stable performance as distractors accumulate, whereas embeddingand BM25-based baselines exhibit larger drops as the retrieval problem becomes increasingly confounded by irrelevant content. These results suggest that explicit structure can improve resilience to distribution shifts induced purely by corpus growth, common failure mode in continually evolving knowledge bases. E.3. Additional Experiments E.3.1. AGENTIC VARIANTS Table 16 reports additional variants of Search-R1 that modify the underlying retrieval method while keeping the rest settings fixed. These experiments are intended to disentangle the impact of retrieval design from the agentic reasoning loop itself. Search-R1 (Dense) replaces BM25 with dense retriever using Qwen3-Embedding-8B, the same embedding model used in PANINI. This variant consistently improves performance on multi-hop benchmarks, most notably on MuSiQue, HotpotQA, and LV-Eval. Search-R1 (Dense + QA) further replaces document chunks with PANINIs extracted questionanswer (QA) pairs as the retrieval corpus. This change yields competitive results on multi-hop datasets, suggesting that structured QA pairs can serve as effective representations even when used within an agentic framework originally designed for passage-level retrieval. Finally, Search-R1 (Dense + QA + Retrieval) integrates PANINIs dual-indexing strategy, combining entity-level retrieval with direct QA-pair similarity search. This variant achieves the strongest overall performance among all Search-R1 configurations, with consistent gains across 21 multi-hop benchmarks. relevance match. Panini: Continual Learning via Structured Memory Across all variants, dense retrieval generally outperforms BM25, confirming that embedding-based retrieval provides stronger semantic matching for agentic multi-step QA. Moreover, the best-performing configuration, which is using PANINIs dual-index retrieval design, demonstrates that PANINIs retrieval design choices remains effective even when embedded inside an agentic reasoning system. F. Qualitative Analysis In this section, we present qualitative analyses with representative examples that illustrate why PANINI outperforms competitive baselines on multi-hop reasoning tasks. We also analyze common failure modes to better understand the limitations of the method. F.1. Qualitative Analysis of PANINI Performance We analyze representative failures of two strong nonparametric baselines: (i) dense embedding retrieval using Qwen3-Embedding-8B and (ii) graph-based retrieval via HippoRAG2. Table 17 provides concrete examples. Below we summarize the dominant failure modes of these methods and explain how PANINIs explicit chain-following retrieval and structured memory design avoid these issues. Embedding retrieval failures (Qwen3-Embedding-8B). common failure pattern is missing the second hop: dense retrieval often over-matches the surface form of the first-hop entity and returns many semantically adjacent passages, but fails to recover the specific bridging evidence needed to instantiate the follow-up hop. PANINI mitigates this by performing explicit query decomposition and RICR: each hop is queried with an evolving context that incorporates previous hop answers and constraints, narrowing the retrieval target from broadly surface-level semantic similarity to hopand chain-specific evidence. We also observe missing intermediate entities in longer chains, where single dense query cannot reliably retrieve all required supports (often spanning multiple documents and entity aliases). PANINIs GSW representation explicitly stores entity nodes with roles/states, improving recall for the entities with aliases; dual indexing further increases coverage by allowing retrieval to follow the chain from either an entity mention or supporting QA. Finally, broken chains occur when an answer-bearing passage is retrieved in isolation, without the supporting evidence that connects it to earlier hops, leaving the overall reasoning unsupported. PANINI avoids this failure by explicitly constructing and maintaining the reasoning chain during retrieval: each hop is selected as part of growing, structured evidence path, rather than as an independent Graph-based retrieval failures (HippoRAG2). HippoRAG2 frequently exhibits retrieval drift: Personalized PageRank propagates relevance through loosely informative graph connections, promoting entities that are topically related but incorrect (e.g., location from the same city, or shared theme). As result, the walk can over-rank nearby distractors while suppressing the specific entity needed to complete the multi-hop composition. PANINI mitigates this by avoiding query-agnostic graph propagation in favor of explicit, hop-conditioned retrieval. Through query decomposition and RICR, each hop is queried using the context accumulated from previous hops, so retrieval is guided by the current reasoning state rather than by graph proximity or neighborhood structure. We also observe cases where HippoRAG2 over-focuses on the seed entity neighborhood, yielding missing the second hop evidence: the walk remains trapped in locally coherent region when the correct bridge requires stepping outside that subgraph. PANINI reduces this failure mode by explicitly constructing the reasoning chain during retrieval: each step updates the retrieval context using the newly identified intermediate entity and constraints, enabling the next hop to move to the correct evidence source instead of remaining confined to the initial neighborhood. F.2. Qualitative Error Analysis We qualitatively analyze representative PANINI failure cases across datasets to better understand current limitations. We observe three recurring error modes: (i) missing verb-phrase nodes during GSW construction, (ii) QA-pair construction errors that omit necessary inverse links, and (iii) imperfect question decomposition for complex queries. Table 18 provides representative examples. Missing verb-phrase nodes. Errors can arise during GSW construction when key event or verb-phrase node is omitted. In this case, the relevant relation is only partially captured, leaving no QA pairs that support downstream chainfollowing retrieval (Table 18, Row 1). QA-pair construction errors. We also observe failures caused by imperfect QA-pair construction, where an entity that should serve as an answer never appears as the answer to any question in the appropriate direction. Since later subquestions are instantiated from entities retrieved by earlier QA pairs, such omissions can break the reasoning chain (Table 18, Row 2). Imperfect question decomposition. Finally, some errors stem from imperfect question decomposition, particularly for long or ambiguity-prone queries. In these cases, the de22 Panini: Continual Learning via Structured Memory composer may generate sub-questions that do not align with the structure of the available evidence, causing downstream retrieval to follow an incorrect chain (Table 18, Row 3). G. Computational Costs and Resources for"
        },
        {
            "title": "Building the GSW",
            "content": "The primary computational costs of PANINI arise from the one-time write-time indexing step that constructs perdocument GSW memories. We orchestrate large-scale parallel indexing calls using the Bespoke Curator library (Marten et al., 2025). For MuSiQue (11,656 passages), we ran the full GSW indexing pipeline with gpt-4.1-mini using the OpenAI Batch API and computed the total write-time cost from recorded token usage and pricing, yielding onetime construction cost of $48.02. We summarize write-time indexing costs in Table 15. Table 19 summarizes end-to-end runtime and resource requirements on MuSiQue (11,656 passages). For PANINI and baselines, we report indexing time and QA latency per query. Importantly, all QA latency per query measurements were obtained using standard (non-Batch) API calls to reflect typical synchronous execution. Overall, PANINI shifts most of its compute to one-time write-time indexing step (GSW construction); after indexing, read-time inference remains lightweight and is dominated by retrieval and reranking rather than multi-step LLM processing. In addition, question decomposition is inexpensive in practice, requiring 60 tokens on average per query and thus contributing negligibly to per-question runtime and cost. 23 Panini: Continual Learning via Structured Memory Input passage (verbatim) Title: Lothair II Text: Lothair II (835 ) was the king of Lotharingia from 855 until his death. He was the second son of Emperor Lothair and Ermengarde of Tours. He was married to Teutberga (died 875), daughter of Boso the Elder."
        },
        {
            "title": "GSW Output",
            "content": "Entities (role states). Lothair II: person {deceased, historical figure}; ruler {king of Lotharingia, medieval period} 835: date {birth year, medieval period} Lotharingia: location {kingdom, medieval period} 855: date {start of reign, medieval period} Emperor Lothair I: person {historical figure}; ruler {Carolingian Emperor, medieval period} Ermengarde of Tours: person {historical figure}; nobility {medieval period} Teutberga: person {deceased, historical figure}; nobility {medieval period} 875: date {death year, medieval period} Boso the Elder: person {historical figure}; nobility {medieval period} Verb phrase node Bidirectional QA pairs (with roles/states on answers) king of Q: Who was the king of Lotharingia? A: Lothair II (person {deceased, historical figure}; ruler {king of Lotharingia, medieval period}) Q: What was Lothair II the king of? A: Lotharingia (location {kingdom, medieval period}) reigned from Q: When did Lothair II start his reign? A: 855 (date {start of reign, medieval period}) Q: Who started reigning in 855? A: Lothair II (person {deceased, historical figure}; ruler {king of Lotharingia, medieval period}) son of Q: Who is the son of Emperor Lothair I? A: Lothair II (person {deceased, historical figure}; ruler {king of Lotharingia, medieval period}) Q: Who is Lothair II the son of? A: Emperor Lothair (person {historical figure}; ruler {Carolingian Emperor, medieval period}) son of Q: Who is the son of Ermengarde of Tours? A: Lothair II (person {deceased, historical figure}; ruler {king of Lotharingia, medieval period}) Q: Who is Lothair II the son of? A: Ermengarde of Tours (person {historical figure}; nobility {medieval period}) married to Q: Who was Lothair II married to? A: Teutberga (person {deceased, historical figure}; nobility {medieval period}) Q: Who was married to Teutberga? A: Lothair II (person {deceased, historical figure}; ruler {king of Lotharingia, medieval period}) daughter of Q: Who is the daughter of Boso the Elder? A: Teutberga (person {deceased, historical figure}; nobility {medieval period}) Q: Who is Teutberga the daughter of? A: Boso the Elder (person {historical figure}; nobility {medieval period}) Figure 3. Example of per-document Generative Semantic Workspace (GSW). Top: the raw input passage (title + text). Bottom: the corresponding GSW, rendered as (i) entity nodes annotated with roles and states, and (ii) verb-phrase nodes instantiated as bidirectional questionanswer (QA) pairs. 24 Panini: Continual Learning via Structured Memory Table 10. Platinum dataset evaluation with Qwen3-8B (thinking mode). Ans = F1 score on the answerable subset. Unans = refusal accuracy on the unanswerable subset (binary; correct iff the output is the canonical non-answer token N/A after normalization). Experiments were run on GPT-4.1-mini-generated GSW memories, using Qwen3-8B (thinking mode) as the reader and Qwen3-8B + LoRA for question decomposition (open-source PANINI pipeline). Bold = best; underline = second best. Retrieval Ans Unans Ans Unans Ans Unans MuSiQue Platinum 2Wiki Platinum Avg BM25 BM25 + reranker Sparse Retrieval 18.04 21.71 89.54 86.93 Dense Retrieval Qwen3-Embedding (8B) Qwen3-Embedding (8B) + reranker 36.89 41.26 82.35 73. 28.62 36.61 47.13 49.81 93.97 87.23 74.47 76.60 23.33 29.16 42.01 45. 91.76 87.08 78.41 74.90 HippoRAG 2 PANINI Structure-Augmented RAG 51. 69.64 70.74 73.20 64.16 69.15 57. 69.95 82.78 76.95 76.21 75.08 Panini: Continual Learning via Structured Memory Figure 4. Prompt used for factual GSW construction from documents. 26 Panini: Continual Learning via Structured Memory Figure 4. Prompt used for factual GSW construction from documents (continued). Panini: Continual Learning via Structured Memory Figure 5. LLM prompt for 2nd pass GSW Construction. 28 Panini: Continual Learning via Structured Memory Figure 6. Prompt used for multi-hop question decomposition into atomic sub-questions. Panini: Continual Learning via Structured Memory Figure 7. Oracle-style prompt used for answer generation from retrieved GSW evidence. 30 Panini: Continual Learning via Structured Memory End-to-End Retrieval Trace (Step 1/3): Question Decomposition Question. What is the place of birth of the performer of song Changed It? System. Chain-Following Multi-Hop QA with Beam Search Mode. cumulative (α = 0.5), reranker = voyage chain_top_k = 15, entity_top_k = 20, qa_rerank_top_k = beam_width = 5, Original question. What is the place of birth of the performer of song Changed It? Decomposed sub-questions. (1) Q1: Who performed the song Changed It? [Retrieval: ] (2) Q2: What is the place of birth of <ENTITY_Q1>? [Retrieval: ] Dependencies. Q1 has no dependencies; Q2 depends on Q1. Chain identification. Total chains = 1; Chain 1 indices = [0,1]; Questions = Q1 Q2. End-to-End Retrieval Trace (Step 2.1/3): Retrieval Hop 1 with Beam Search Hop 1/2 (Q1): Who performed the song Changed It? State. Prior entities = None (root); Active beams = 1; Concrete questions = 1. Top retrieved entities (BM25 + Embedding; top-5 of 20): Rank Entity name 1 Changed It 2 Liar, Liar 3 The Supremes 4 You Changed Me 5 When the Stars Go Blue Retrieved QA pairs after reranking (top-5 of 15; remaining omitted): Rank Question Answer 1 Who performed Changed It? 2 What song did Nicki Minaj and Lil Wayne perform? 3 What song did Motiv, Detail, and Sidney Swift produce? 4 What song did Young Money Entertainment, Cash Money Records, Republic Nicki Minaj, Lil Wayne Changed It Changed It Changed It Records release? 5 What song was released on March 10, 2017? Changed It Beam expansion. Group by answer entity and keep the best QA pair per entity; unique answer entities = 17. Sample candidate states (first 5): Source Changed It Changed It Changed It Changed It Changed It Cand. last_hop Answer entity/entities 1 2 3 4 5 0.8320 [Nicki Minaj, Lil Wayne] 0.7852 [Changed It] 0.6953 [Changed It] 0.6875 [Changed It] 0.6172 [Changed It] Beam pruning (Hop 1). Total candidates = 17; keep top-5 by (chain_score, last_hop_score). Beam details (Hop 1). Includes all kept beams plus the top-ranked pruned beam. Beam chain_score 1 2 3 4 5 6 last_hop Status 0.8320 kept 0.8320 kept 0.7852 kept 0.6172 kept 0.6172 kept Entity map {0: Nicki Minaj} {0: Lil Wayne} {0: Changed It} {0: No Frauds} {0: Regret in Your Tears} 0.9160 0.9160 0.8926 0.8086 0.8086 0.7949 0.5898 pruned {0: March 10, 2017} 31 Panini: Continual Learning via Structured Memory End-to-End Retrieval Trace (Step 2.2/3): Retrieval Hop 2 with Beam Search Hop 2/2 (Q2): What is the place of birth of <ENTITY_Q1>? State. Prior entities = {0: Nicki Minaj}; Active beams = 5; Concrete questions = 5 (Nicki Minaj; Lil Wayne; Changed It; No Frauds; Regret in Your Tears). Top retrieved entities (BM25 + Embedding; top-5 of 20): Rank Entity name 1 Nicki Minaj 2 Nicki Minaj 3 Pink Friday 4 Nicki Minaj 5 Nicki Minaj Retrieved QA pairs after reranking (top-5 of 15; remaining omitted): Rank Question 1 Where was Nicki Minaj born? 2 Who was born in Saint James, Port of Spain? 3 Who was raised in Queens, New York City? 4 Where was Nicki Minaj raised? 5 Who performed Changed It? Answer Saint James, Port of Spain Nicki Minaj Nicki Minaj Queens, New York City Nicki Minaj, Lil Wayne Beam expansion (final hop). Use all QA pairs directly (no grouping); candidates = 15. Sample candidate states (first 5): Cand. last_hop Answer entity/entities 1 2 3 4 0.8945 [Saint James, Port of Spain] 0.7891 [Nicki Minaj] 0.6836 [Nicki Minaj] 0.6289 [Queens, New York City] 0.5898 [Nicki Minaj, Lil Wayne] Beam pruning (Hop 2). Total candidates = 15; keep top-5 by (chain_score, last_hop_score). Beam details (Hop 2). Includes all kept beams plus the top-ranked pruned beam. Source Nicki Minaj Nicki Minaj Nicki Minaj Nicki Minaj Nicki Minaj Beam chain_score last_hop Status 0.8945 kept 0.7891 kept 0.6836 kept 0.6289 kept 0.5898 kept Entity map {0: Nicki Minaj, 1: [Saint James, Port of Spain]} {0: Nicki Minaj, 1: [Nicki Minaj]} {0: Nicki Minaj, 1: [Nicki Minaj]} {0: Nicki Minaj, 1: [Queens, New York City]} {0: Nicki Minaj, 1: [Nicki Minaj, Lil Wayne]} 0.9315 0.9052 0.8781 0.8637 0.8533 0.8406 0.5430 pruned {0: Nicki Minaj, 1: [Onika Tanya Maraj-Petty]} 1 2 3 4 5 6 End-to-End Retrieval Trace (Step 3/3): Answer Generation from Final Evidence Final evidence (5 unique QA pairs). (1) Q: Who performed Changed It? A: Nicki Minaj, Lil Wayne person: Trinidadian-American, rapper, singer; person: American, rapper (2) Q: Where was Nicki Minaj born? A: Saint James, Port of Spain location: birthplace, Trinidad and Tobago (3) Q: Who was born in Saint James, Port of Spain? A: Nicki Minaj person: stage name, musician, actress, model (4) Q: Who was raised in Queens, New York City? A: Nicki Minaj person: stage name, musician, actress, model (5) Q: Where was Nicki Minaj raised? A: Queens, New York City location: raised in, USA LLM response. The performer of the song Changed It is Nicki Minaj. According to the QA pairs, Nicki Minaj was born in Saint James, Port of Spain, which is located in Trinidad and Tobago. Therefore, the place of birth of the performer is Saint James, Port of Spain. Answer: Saint James, Port of Spain. Figure 8. Example end-to-end retrieval trace. PANINI decomposes the query into two hops and performs chain-following retrieval with beam search (beam width = 5). At each hop, candidates are scored using geometric-mean (cumulative) chain scoring, and the final answer is generated from the selected QA evidence. 32 Panini: Continual Learning via Structured Memory Figure 9. Continual-learning under corpus growth (MuSiQue, 200 questions). We fix held-out set of 200 questions and evaluate retrieval+QA F1 as the corpus is incrementally expanded from 4K to the full MuSiQue collection (12K passages). The set of relevant passages for these questions is contained in the initial 4K subset and remains unchanged across steps; only the number of distractor passages increases as additional (irrelevant) documents are added. This simulates continuously evolving corpus where the signal stays constant but the retrieval search space grows. 33 Panini: Continual Learning via Structured Memory Table 11. Performance comparison when varying the open-source model used for write-time GSW construction. All open-source models use the same read-time configuration: LoRA-finetuned Qwen3-8B for question decomposition and Qwen3-8B (no-think) as the reader. We also report HippoRAG2 under the same reader model and the PANINI configuration from Table 2 as reference points. Model 2Wiki MuSiQue Qwen3-8B Qwen3-14B Qwen3-14B (Second Pass) GPT-OSS-120B HippoRAG2 (Qwen3-8B reader) PANINI (Main Exp) 55.38 62.80 66.76 70.65 66.93 72. 40.24 44.87 48.28 48.50 45.40 52.3 Table 12. Ablation study on multi-hop QA (F1 score, 1000 questions). Setting Full PANINI No QA reranking No dual search (entity-only) No dual search (QA-only) No decomposition Beam width = 10 Beam width = 3 Beam width = 1 MuSiQue 2Wiki 52.3 18.4 39.5 52.3 36.8 52.4 52.3 44. 72.4 22.2 68.6 72.0 47.2 72.7 72.2 67.3 Table 13. Ablation study on multi-hop QA (F1 score and average token count, 1000 questions). Setting MuSiQue 2Wiki F1 Tokens F1 Tokens Beam width = Beam width = 10 Beam width = 3 Beam width = 1 52.3 52.4 52.3 44.6 192 320 143 82 72. 72.7 72.2 67.3 315 409 231 171 Table 14. Ablation of chain-level scoring on MuSiQue (F1, 1,000 questions). Main denotes cumulative (geometric-mean) scoring; Similarity uses only questionchain embedding similarity; Combined linearly interpolates the two (α = 0.5); None disables chain-level scoring and performs greedy last-hop selection. Scoring rule MuSiQue None (greedy last-hop) Similarity Combined Main (cumulative) 50.81 42.32 48.24 52.3 34 Table 15. Write-time indexing cost on MuSiQue (11,656 passages). Costs are one-time dataset-level indexing costs. PANINI write-time corresponds to per-document GSW construction with gpt-4.1-mini; the Batch variant uses the OpenAI Batch API, while the Standard variant is an estimate assuming 2 higher cost without the Batch discount. HippoRAG 2 cost is the reported total run cost using GPT-4o-mini with standard API calls as default. Qwen3-Embedding (8B) has zero write-time LLM cost since it performs embedding-only indexing. Method Write-time cost PANINI (Batch) PANINI (Standard, est.) HippoRAG 2 Qwen3-Embedding (8B) $48.02 $96.04 $91.8 $0 Panini: Continual Learning via Structured Memory Table 16. Performance of Search-R1 under different retrieval configurations. All rows use the same Search-R1 agent model and differ only in the retrieval mechanism. Search-R1 uses BM25 over document chunks; Search-R1 (Dense) replaces BM25 with dense embedding retrieval (Qwen3-Embedding-8B) over document chunks; Search-R1 (Dense + QA) further replaces document chunks with the structured QA pairs from the GSW; Search-R1 (Dense + QA + Dual Index) additionally incorporates PANINIs RICR retrieval strategy and retrieves relevant QA pairs. PANINI is included as reference point and uses the same evaluation setting as in Table 2. Simple QA Multi-Hop QA Avg Retrieval Search-R1 Search-R1 (Dense) Search-R1 (Dense + QA) Search-R1 (Dense + QA + Retrieval) PANINI NQ 47.9 41.6 48.5 48. 67.4 PopQA MuSiQue 49.7 52.7 51.5 54.0 57.6 41.1 45.1 44.6 46.6 52. 2Wiki HotpotQA LV-Eval Avg 47.3 64.9 49.0 64.6 48.1 65.4 49.4 67.6 68.6 71.3 66.8 70.0 11.5 18.8 12.0 10.5 72.4 71.9 14. 56.1 Table 17. Representative failure cases for dense embedding retrieval (Qwen3-Embedding-8B) and graph-based retrieval (HippoRAG2). Query Method Failure mode Failure reason What year did the publisher of Labyrinth end? Qwen3-Emb What is the location of formation of the film company distributing The Boss? Qwen3-Emb Between the state university in the state without North Point Mall and where Edwards won the primary and the university in Fort Hills town, which has the more national championships? Whos the son of the Italian navigator who explored the eastern coast of the continent Manuel Balbi was born in? Qwen3-Emb missing_second_hopRetrieved passages broadly about Labyrinth and publishing, but missed the bridging evidence required for the second hop. missing_second_hopOver-matched the surface phrase The Boss and returned loosely related documents, failing to retrieve the distributor company-formation link. Requires multiple supporting documents; failed to retrieve the intermediate evidence involving North Point Mall, breaking the chain. missing_entity Qwen3-Emb broken_chains What is the name of the famous bridge in the birth city of the composer of Scanderbeg? HippoRAG2 retrieval_drift Who is the child of the president under whom prohibition occurred? HippoRAG2 retrieval_drift Retrieved the correct answer passage, but missed the intermediate evidence (continent identification and the relevant navigator), so the chain could not be supported. Drifted to plausible distractor in the same city (e.g., another famous Venice bridge), and failed to rank the correct bridge-related evidence highest. Found evidence about prohibition but drifted to nearby presidents/laws, losing the correct intermediate link needed to answer the composed query. What is the highest city in the state where Dell ranks sixth by revenue? HippoRAG2 missing_second_hopStayed in Dell-centric neighborhoods and failed to hop from company revenue ranking state highest city evidence. Failure mode Example query What goes wrong Why it breaks the chain Table 18. Representative qualitative error cases for PANINI. Missing verb-phrase nodes What nationality is Arabia (Daughter Of Justin II)s mother? QA-pair construction errors Who is the paternal grandfather of Dibyasambandh? Imperfect question decomposition When did the country that has the original language of the film named after Vladimir Karalics birthplace as co-official language first attend the Olympics as an independent team? The GSW omits the verbphrase/event capturing Aelia Sophia as an empress of the Byzantine Empire. stored, The relation daughter of is but QA pairs are only generated in the fatherdaughter direction; the inverse daughterfather QA is missing. The decomposer drops coand queries for official language instead of co-official language. Without this relation node, the GSW does not contain any QA pair linking the mother to country or polity, so the required evidence for determining nationality is never retrieved. Since the father entity never appears as the answer to any QA pair, subsequent hops remain conditioned on the daughter entity, blocking retrieval of evidence about the paternal grandfather. The weakened constraint retrieves different country, causing subsequent hops to follow an incorrect chain even if later retrieval is accurate. Table 19. Computational resource requirements for end-to-end runtime on MuSiQue (11,656 passages). Indexing time (min) and QA latency per query (sec) for PANINI and baselines. QA latency per query is measured with standard (non-batch) API calls. Qwen3-Embedding (8B) Search-R1 HippoRAG 2 PANINI Indexing Time (min) QA Time/Query (sec) 1.9 1.3 1.9 6.7 106.6 4. 100.1 3."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, University of California, Los Angeles, USA"
    ]
}