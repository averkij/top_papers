{
    "paper_title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation",
    "authors": [
        "Yunhong Lu",
        "Yanhong Zeng",
        "Haobo Li",
        "Hao Ouyang",
        "Qiuyu Wang",
        "Ka Leong Cheng",
        "Jiapeng Zhu",
        "Hengyuan Cao",
        "Zhipeng Zhang",
        "Xing Zhu",
        "Yujun Shen",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU."
        },
        {
            "title": "Start",
            "content": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation Yunhong Lu1,2 Yanhong Zeng2 Haobo Li2,4 Hao Ouyang2 Qiuyu Wang Ka Leong Cheng2 Jiapeng Zhu2 Xing Zhu2 Hengyuan Cao1 Yujun Shen2 Min Zhang1,3 Zhipeng Zhang5 5 2 0 D 4 ] . [ 1 8 7 6 4 0 . 2 1 5 2 : r 1ZJU 2Ant Group 3SIAS-ZJU 4HUST 5SJTU https://reward-forcing.github.io/ Figure 1. We propose Reward Forcing to distill bidirectional video diffusion model into few-step autoregressive student model that enables real-time (23.1 FPS) streaming video generation. Instead of using vanilla distribution matching distillation (DMD), Reward Forcing adopts novel rewarded distribution matching distillation (Re-DMD) that prioritizes matching towards high-reward regions, leading to enhanced object motion dynamics and immersive scene navigation dynamics in generated videos."
        },
        {
            "title": "Abstract",
            "content": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the models ability to prioritize dynamic content. Instead, Re-DMD biases the models output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on single H100 GPU. 1. Introduction The scaling of video diffusion transformers (DiTs) [53, 72] has advanced text-to-video generation, producing realistic videos with intricate dynamics [2, 4, 11, 76]. However, their simultaneous denoising of all frames using bidirectional attention hinders interactive applications, which demand streaming generation over extended horizons under strict latency constraints. Achieving both low latency and high visual-dynamic fidelity remains the central challenge. To achieve efficient streaming video generation, recent advances distill slow pre-trained bidirectional diffusion models into efficient few-step autoregressive student models [7, 30, 88]. In these models, each frame attends only to previous frames with sliding window attention, enabling real-time streaming inference through key-value (KV) cache mechanisms. However, they often suffer from the well-known error accumulation issue [6, 30], as each frame depends on potentially corrupted previous outputs, causing errors to propagate progressively. To mitigate error accumulation, recent works have adopted attention sink mechanisms that retain initial tokens in the KV cache [45, 62, 82]. Such design largely recovers the performance of sliding window attention and alleviates long-horizon drifting. However, new challenge arises: by consistently preserving initial tokens throughout generation, models develop strong bias toward the starting frame, leading to over-attention on initial content. This manifests as diminished motion dynamics, where subsequent frames fail to evolve naturally, and frequent visual flashbacks that revert to the first frames appearance. While classical distribution matching distillation [85, 86] minimizes the divergence between student and teacher output distributions to transfer knowledge, this strategy struggles to address the over-attention issue. The degraded samples, despite their motion deficiencies, usually exhibit good visual quality and already fall close to the teacher distribution, making them difficult to distinguish and optimize. In this paper, we propose Reward Forcing, novel framework with two key technical innovations to ensure both high visual and dynamic fidelity for efficient streaming video generation. During training, Reward Forcing generates video chunks autoregressively by conditioning on previously self-generated outputs through KV cache mechanisms to bridge the train-test gap, following Self Forcing [30]. Instead of using static initial tokens as sink tokens in the KV cache, we introduce EMA-Sink, novel state packaging mechanism for ultra-long video sequences. The core idea of EMA-Sink is to maintain fixed-size tokens initialized from initial frames while continuously updating them by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional cost, this design not only compresses effective global context to maintain attention performance, but also introduces recent dynamics to prevent over-attending to initial frames. To better distill motion dynamics from teacher models, we introduce Rewarded Distribution Matching Distillation (ReDMD). Instead of treating all samples equally as in vanilla distribution matching distillation, Re-DMD is able to distinguish samples with diminished motion dynamics and prioritizes matching with samples exhibiting greater dynamics. To this end, Re-DMD uses powerful vision-language model as reward function to rate samples according to their motion quality, then uses these scores to weight distribution matching gradients. This effectively biases the distribution matching toward high-quality regions while preserving high data fidelity, leading to enhanced motion dynamics in streaming video generation. Comprehensive experimental evaluation on both short and long video benchmarks demonstrates that Reward Forcing achieves state-of-the-art video quality at 23.1 FPS on single H100 GPU. 2. Related Works Autoregressive long video generation. Video diffusion models have advanced short video generation, yet most state-of-the-art models are limited to 510 second clips. To reduce the high cost of bidirectional denoising, recent studies have adopted autoregressive diffusion modeling for long video generation [18, 24, 39, 41, 64, 90, 93]. Among these, Pyramidal-flow employs multi-scale flow matching to alleviate computational burden [33], while SkyReels-V2 integrates diffusion forcing [6] with structural planning and multi-modal control [7]. FAR combines short and longterm contexts via flexible positional encoding [20], and MAGI-1 utilizes chunk-wise prediction for scalable autoregressive generation [69]. CausVid reformulates bidirectional diffusion as causal generation through distribution matching distillation [85, 86] to reduce denoising steps [88]. Self-Forcing builds on this framework to mitigate train-test discrepancy by simulating inference conditions [30], which is further extended by LongLive through KV recaching and stream-based fine-tuning for long video generation [82], and by Rolling-Forcing via joint denoising for simultaneous multi-frame processing [45]. However, these methods consistently trade off motion dynamics against visual quality, often introducing cumulative artifacts. Reinforcement learning for video generation. Reinforcement learning [67] addresses optimizing non-differentiable metrics and temporally extended outcomes, enabling video generation models to better align with human preferences. Research has diverged into two strands. The first develops specialized datasets and reward models [44, 57, 79] for video evaluation. The second integrates RL algorithms into generation pipelines. Some approaches use rewards [56, 89] to directly supervise generative models, while direct preference optimization (DPO) methods [48, 49, 60] implicitly learn preferences from datasets without explicit reward modeling, showing strong robustness [46, 92]. Additionally, policy optimization [91] techniques, such as SelfForcing++ [11], incorporate Flow-GRPO [43] into DMDdistilled models to improve long-term temporal smoothness. However, this method depends on pre-distilled models, with performance inherently tied to base model. 3. Method 3.1. Preliminaries Autoregressive video diffusion models. In autoregressive video diffusion models, an -frame sequence x1:N follows p(x1:N ) = (cid:81)N i=1 p(xix<i). Self Forcing [30] introduces an autoregressive self-rollout mechanism aligning training with inference. During training, each frame xi undergoes iterative denoising conditioned on previously generated clean frames and its noisy state, sampling from the autoregressive distribution. few-step diffusion model Gθ approximates each conditional p(xix<i). Given timesteps {t0, t1, , tT }, denoising at step tj for tj conditioned on x<i, then frame processes noisy frame xi reintroduces controlled Gaussian noise via forward process Ψ to yield xi tj for the next step. The model distribution is: pθ(xix<i) = fθ,t1 fθ,t2 fθ,tT (xi ) tT , tj, x<i), tj1) and xi where fθ,tj (xi ) = Ψ(Gθ(xi tT tj tj (0, I). For longer sequences, LongLive [82] uses sink tokens [78] with p(xix1, xiw+1:i1) (window size w) to model p(xix<i), but over-relies on the initial frame, limiting dynamic variation and smooth transitions. While models can output multi-frame chunks per step [30, 69, 88], we term each chunk frame for simplicity. Distribution matching distillation. DMD [85, 87] distills multi-step diffusion models into few-step generator by minimizing reverse KL divergence between real preal(x) and generated distributions pfake(x) across timesteps: θLDMD Et(θDKL(pfake,t(xt)preal,t(xt))) (cid:16) (cid:90) Et (sreal(Ψ(Gθ(ϵ), t), t) (1) sfake(Ψ(Gθ(ϵ), t), t)) dGθ(ϵ) dθ (cid:17) dϵ . where ϵ (0, I), Ψ denotes forward diffusion at timestep t. In diffusion models, the score function is defined as: xt αtµreal(xt, t) σ2 sreal(xt, t) = xt log preal,t(xt) = (2) , where µreal is the denoised estimate, and αt, σt are noise schedule parameters [25, 34, 52]. DMD freezes pre-trained µreal (teacher) and updates µfake on generator outputs. Reinforcement learning. unified RL [67] fine-tuning objective is established by maximizing the evidence lower bound for optimal video generation x0, culminating in an RL objective that makes an explicit trade-off between reward maximization and fidelity to the original model: JRL(p, q) = (cid:104) r(x0, c) β log p(x0c) q(x0c) (cid:105) . (3) Here, x0 denotes the output, represents the reward model, represents the conditioning input, and are distributions, and β acts as the regularization term. Figure 2. Comparison of EMA Sink with Existing Methods. Long video generation models typically extrapolate beyond their training sequence length during inference. (a) Window Attention caches only recent tokens for efficient inference but suffers performance degradation. (b) Sliding Window with attention sinks retains initial tokens for stable attention computation and recent tokens for extrapolation. However, discarding intermediate frames causes over-reliance on the first frame, leading to frame copying and stiff transitions. (c) EMA Sink preserves full history through exponential moving average (EMA) updates of all historical frames, maintaining stable and consistent performance in long video extrapolation without increasing computational cost. 3.2. EMA-Sink: state packaging for long video Problem formulation. Efficient streaming video generation aims to create indefinitely long videos while maintaining strict temporal and causal consistency. Although sliding window attention is widely adopted in autoregressive models to reduce computational cost, current approaches fail to retain historical context beyond their limited attention windows [88]. As generation progresses, earlier frames are discarded, creating an information bottleneck that diminishes global awareness and leads to temporal inconsistencies and quality drift over time. To address this, we introduce EMASink, novel state-packaging mechanism that compresses history to support efficient autoregressive generation. Our approach preserves global context in compact, computationally efficient form throughout the streaming process. For further illustration, given noise schedule = {tj}T j=0 consisting of distinct noise levels, the model processes each intermediate noisy frame xi tj at denoising step tj and frame index i, incorporating earlier clean frames i,w = (cid:2)xiw+1:i1(cid:3) where denotes the window size used during video extrapolation (i > w). It first estimates denoised version of the frame, then applies the forward diffusion operator Ψ to reintroduce lower level of Gaussian noise, producing xi tj1 for subsequent denoising: , tj, Ψ(Gθ(xi (0, I). As the tj window advances to frame + 1, the oldest frame xiw+1 is removed from immediate access and is permanently discarded, thereby creating an information bottleneck [30]. ), tj1), where xi tT Figure 3. Pipeline of Reward Forcing. In streaming text-to-video generation, noisy tokens in the current stream are first projected to produce new key-value pairs (green blocks), which are appended to the KV cache for attention computation. When the current KV cache reaches its maximum attention window size, sink tokens initialized from start frames (yellow blocks) are updated via exponential moving average using evicted tokens (pink blocks). During training, hallucinated tokens are decoded into videos to compute reward score via reward function. This score is then used to weight the distribution matching gradient from the teacher model. EMA-Sink mechanism. Rather than discarding evicted frames, EMA-Sink maintains compressed global states Si in the KV-cache through an exponential moving average. When frame xiw is evicted from the sliding window, its key-value pair (Kiw, iw) is continuously fused into the compressed sink states Si : Si = α Si1 = α Si1 Si + (1 α) Kiw, + (1 α) iw. (4) (5) Here α (0, 1) is the momentum decay factor controlling compression rate, providing smooth temporal compression where recent information dominates while preserving fading memory of distant history. During attention computation [70], we prepend the compressed sink states to the local window context: (6) Ki"
        },
        {
            "title": "V i",
            "content": "global = (cid:2)Si global = (cid:2)Si K; Kiw+1:i(cid:3), ; iw+1:i(cid:3), (7) where Kiw+1:i and iw+1:i represent the key and value states from the current sliding window. This formulation allows each query to attend to both the fine-grained local context and the coarse-grained global history, effectively breaking the information bottleneck of the fixed window size. To handle the spatial-temporal nature of video while maintaining causal relationships, we employ rotary position embedding (ROPE) [65] when calculating attention. The position encoding is applied causally, ensuring that each position can only attend to previous positions in the sequence. 3.3. Rewarded distribution matching distillation Problem formulation. DMD [85, 87] offers an effective framework for converting multi-step diffusion models into efficient single-step generators by enforcing alignment between the fake and real distributions: JDMD = Ep(c)pfake(x0c) (cid:104) log pfake(x0c) preal(x0c) (cid:105) . (8) Despite its success in preserving sample fidelity, DMD has fundamental limitation: it treats all regions of the target distribution uniformly, lacking any mechanism to prioritize high-quality outputs according to task-specific metrics. This becomes particularly problematic in video generation, where models progressively produce increasingly static frames during training. This observation motivates key question: Can we incorporate motion awareness into the distillation process while maintaining distributional fidelity? We address this challenge by integrating RL principles [67] to bias the distillation toward high-reward regions of the output space, thereby generating content with enhanced properties without sacrificing data fidelity. Re-DMD mechanism. We introduce Rewarded Distribution Matching Distillation (Re-DMD), which reweights the distribution matching objective according to sample motion quality. Our approach builds on the Reward-Weighted Regression framework [16, 38, 44, 54], which reformulates the reinforcement learning problem as probabilistic inference via the Expectation-Maximization (EM) algorithm [51]. Figure 4. Qualitative comparison on dynamic complexity of long video generation. Reward Forcing excels in both text alignment and motion dynamics while baselines exhibit diminished dynamics and weaker alignment. In the E-step [51, 54], we solve Eq. (3) as constrained optimization problem, obtaining the optimal solution: p(x0c) ="
        },
        {
            "title": "1\nZ(c)",
            "content": "q(x0c) exp (cid:16) r(x0, c) β (cid:17) , (9) bution, respectively, using denoising objective. In addition, rc(xt) is estimated by rc(x0). This approach stabilizes training and accelerates convergence by bypassing the intractable normalization constant and alleviating the need to compute the reward functions gradient. where Z(c) = (cid:80) distributions in Eq. (3) as = x0 p(x0c) exp( r(x0,c) β fake and = real. ).We assign the In the M-step [51, 54], we project the nonparametric optimal model = real onto the parametric model by maximizing expected log-likelihood Eq. (3) with respect to pfake: JRe-DMD = Ep(c)p fake(x0c) (cid:104) exp(r(x0, c)/β) Z(c) log pfake(x0c) preal(x0c) (cid:105) . (10) Computing the probability density to estimate this loss is generally intractable. However, when training the generator via gradient descent, we only need to obtain the gradient with respect to θ. By differentiating Eq. (10), we obtain: θJRe-DMD =Et (cid:16) θEpc fake(xt) (cid:104) exp(rc(xt)/β) Z(c) log (cid:105)(cid:17) pc fake(xt) pc real(xt) (cid:16) (cid:90) Et exp(rc(xt)/β) (sreal(Ψ(Gθ(ϵ), t), t) sfake(Ψ(Gθ(ϵ), t), t)) dGθ(ϵ) dθ (cid:17) . dϵ (11) where ϵ is random Gaussian noise, and Gθ is generator parameterized by θ. sreal and sfake represent the score functions trained on the data and the generators output distri3.4. Efficiency analysis Theoretical properties. The EMA-Sink enables token eviction in O(1) time with low overhead. While attention remains O(w2) in window size, it becomes independent of sequence length. By compressing history into fixedsize sink, our method achieves constant memory usage relative to sequence length while retaining global context. The differentiable EMA enables gradient propagation through compression, supporting end-to-end learning of compression strategies. The Re-DMD objective implicitly optimizes constrained reward maximization problem (maximizing expected reward under distribution matching constraint), ensuring systematic quality improvement without distributional collapse. Notably, our approach avoids typical RL computational costs: the reward serves as static weighting factor, eliminating backpropagation through reward models and preventing instability from noisy reward gradients. Real-time long video inference. Long video generation faces quadratic complexity with dense causal attention, hindering real-time synthesis. Local window attention confines complexity to window size, independent of sequence length. With KV cache scaling by window dimension rather than video length, smaller windows accelerate inference and significantly improve efficiency. Figure 5. Qualitative comparison on long-range temporal consistency. Reward Forcing maintains superior coherence over long-horizon, while baselines suffer from noticeable quality degradation and inconsistency over time. 4. Experiments Implementation details. Reward Forcing is built upon Wan2.1-T2V-1.3B [72] to generate 5-second videos at 832480 resolution. The model is first trained on 16k ODE solution pairs sampled from the base model, initialized with causal attention masking, following CausVid [88]. Text prompts are drawn from the filtered and LLM-augmented VidProM [73] dataset. We use VideoAligns [44] motion quality as the reward function with β = 1 2 . During training, denoising is applied chunk-wise using 3 latent frames per chunk, with denoising steps set to [1000, 750, 500, 250] and an attention window size of 9. Training runs for 600 steps on 64 H200 GPUs with total batch size of 64 (3 hours). The AdamW optimizer is adopted with learning rates of 2.0 106 for the generator Gθ and 4.0 107 for the fake score sfake, updating the generator every 5 steps and adjusting the fake score sfake accordingly. 4.1. Comparison with state-of-the-art Short video generation. We generate 5-second videos using 946 official VBench [31, 32] prompts rewrited using Qwen/Qwen2.5-7B-Instruct [1] following Self Forcing [30], each sampled with 5 different seeds for comprehensive quality assessment. We benchmark our method against relevant open-source video generation models of comparable scale, including LTXVideo [13], Wan2.1 [72], SkyReels-V2 [7], MAGI-1 [69], CausVid [88], NOVA [13], Pyramid Flow [33], Self Forcing [30], LongLive [82], and Rolling Forcing [45]. The overall score of VBench comTable 1. Short video performance comparison with baselines. The comparison includes representative open-source models of comparable scale. Best results in bold, second-best underlined. Model Params FPS VBench evaluation scores Total Quality Semantic Diffusion LTX-Video [13] Wan-2.1 [72] Autoregressive SkyReels-V2 [7] MAGI-1 [69] NOVA [13] Pyramid Flow [33] CausVid [88] Self Forcing [30] LongLive [82] Rolling Forcing [45] 1.9B 1.3B 8.98 80.00 0.78 84.26 82.30 85.30 1.3B 4.5B 0.6B 2B 1.3B 1.3B 1.3B 1.3B 0.49 82.67 0.19 79.18 0.88 80.12 6.7 81.72 17.0 82.88 17.0 83.80 20.7 83.22 17.5 81.22 84.70 82.04 80.39 84.74 83.93 84.59 83.68 84.08 Ours 1.3B 23.1 84.13 84.84 70.79 80.09 74.53 67.74 79.05 69.62 78.69 80.64 81.37 69.78 81. prises both quality and semantic components. As shown in Tab. 1, our method achieves an overall score of 84.13 on the 5-second clips, surpassing all existing baselines and demonstrating superior video generation quality. Notably, our approach employs the smallest attention window while achieving the fastest inference speed among all compared methods. Specifically, we attain real-time generation speed of 23.1 FPS, representing 47.14 speedup over SkyReels-V2 and 1.36 speedup over Self Forcing. Table 2. Long video performance comparison with key baselines. The best results are highlighted in bold. Model Total Subject Background Smoothness Dynamic Aesthetic Imaging Quality Visual Dynamic Text VBench Long Evaluation Scores Drift Qwen3-VL Score Diffusion Forcing SkyReels-V2 [7] 75.94 Distilled Causal CausVid [88] 77.78 Self Forcing [30] 79.34 79.53 LongLive [82] Ours 81. 97.26 96.43 96.59 98.91 39.86 50. 58.65 7.315 3.30 3.05 2.70 97.92 97.10 97. 96.62 96.03 96.50 96.05 98.47 98.48 98.79 98.88 27.55 54.94 35.54 66. 58.39 54.40 57.81 57.47 67.77 67.61 69.91 70.06 2.906 5.075 2.531 4.66 3.89 4. 2.505 4.82 3.16 3.44 3.81 4.18 3.32 3.11 3.98 4. Long video generation. Qualitative analysis of the results confirms the effectiveness of our approach. Specifically, Figure 4 illustrates its capability to produce more dynamic sequences for long video generation, and Figure 5 validates its improved temporal consistency. For long video quantitative evaluation, we use the first 128 prompts from MovieGen (consistent with CausVid [88]), extending generation duration to 60 seconds. We employ VBenchLong [32] metrics, including subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality, normalized and weighted using standard VBench [31] coefficients to compute the total score. To quantify drift in long video generation, we compute the standard deviation of imaging quality across 30 segments (2 seconds each) from 60-second videos. As shown in Tab. 2, our method achieves total score of 81.41, significantly surpassing the state-of-the-art baseline LongLive (79.53). Notably, we observe substantial improvement in the dynamic metric (66.95), representing an 88.38% boost in dynamic amplitude while minimizing quality drift, demonstrating our methods effectiveness with comparable performance on other metrics. Additionally, we employ Qwen3-VL-235B-A22B-Instruct [1] to evaluate long video generation quality at 5560 seconds, assessing visual quality, motion dynamics, and text alignment (see more details in the supplements). Each of the 128 videos is scored from 1 to 5, with averaged results showing our model achieves the best performance across all three metrics. We also include user study for comprehensive comparison in the supplementary material, which demonstrates that our method consistently outperforms all key baselines. 4.2. Ablation studies Impact of EMA-Sink and Re-DMD. We show the effectiveness of Reward Forcing through qualitative and quantitative comparisons. Qualitatively, as presented in Fig. 6, our method maintains smooth transitions and high dynamism when generating 850950 frames (approximately 1 minute), with clearly perceptible fluidity between consecTable 3. Ablation studies on key components. The best results for the Improvement module are indicated in bold. Model Improvement Ours w/o Re-DMD w/o EMA w/o Sink Impact of α α = 0.99 α = 0.9 α = 0.5 Impact of β β = 1 β = 2/3 β = 1/3 β = 1/5 VBench Evaluation Scores Drift Background Smooth Dynamic Quality 95.07 95.85 95.61 94. 95.90 95.80 94.57 95.14 95.02 94.94 92.40 98.82 98.91 98.64 98.56 98.96 99.09 98.89 98.31 98.46 98.43 96.40 64.06 43.75 35.15 51. 65.15 63.15 64.37 54.68 60.93 58.59 94.53 70.57 71.42 70.50 69.92 70.81 71.37 71.11 71.73 70.61 69.29 68.26 2.51 1.77 2.65 5. 2.52 3.23 3.78 2.63 1.91 2.02 3.13 utive frames. Without Re-DMD training, long video generation preserves high consistency with the initial frame and smooth scene transitions, but exhibits significantly reduced dynamism with the dynamic score drops from 64.06 to 43.75 ( Tab. 3). As illustrated in Fig. 6, removing the EMA Sink module results in considerable inconsistency with the first frame and minimal dynamism, reflected quantitatively by declining motion smoothness (98.91 to 98.64 in Tab. 3) and dynamic score (43.75 to 35.15). Ablating the sink token leads to noticeable quality degradation. Impact of EMA update weight α. An appropriately EMA coefficient α ensures smooth scene transitions in long videos, while suitable α value effectively balances motion fluidity and temporal consistency. In our implementation, α is set to 9e3. We can observe from Tab. 3 that α = 0.99 achieves motion smoothness of 98.96 with corresponding drift of 2.52. Conversely, reducing α to 0.9 improves motion smoothness to 99.09 but increases drift to 3.23. Figure 6. (Left) Ablation study on our proposed module, showing qualitative improvement. (Right) Top: Reward Forcing training leads to steady rise in the dynamic score. Bottom: The plot of attention size versus FPS underscores the source of our inference efficiency. Impact of reward weight β. The parameter β modulates the reward terms influence, with smaller values assigning higher reward weight. As illustrated in Tab. 3, an excessively small β (e.g., 1/5) yields an overly high dynamic score (94.53) at the expense of background consistency (92.40), motion smoothness (96.40), and image quality (68.26). Conversely, an overly large β (e.g., 1) produces an insufficient dynamic score (54.68). Therefore, we select β = 1/2 to optimally balance these metrics. 4.3. Analysis Dynamic enhancement of Re-DMD. We employ the VBench dynamic score (averaged over the first 128 prompts) to evaluate training effectiveness. An inspection of Fig. 6 reveals that the dynamic score increases steadily with training time while requiring modest computational resources (under 200 GPU hours). Our method surpasses LongLive (high consistency, low dynamism) after only 100 GPU hours and exceeds Self-Forcing (high dynamism, severe drift) after 150 GPU hours. Our model achieves high dynamism while maintaining strong consistency. Impact of attention window size. The attention window size is critical factor affecting the speed of real-time generation. Figure 6 demonstrates that inference FPS is inversely proportional to the size of the attention window. Interactive video generation. As shown in Fig. 7, our method supports interactive video generation, allowing users to modify prompts during generation to control output content. This is achieved by clearing the previous crossattention cache and recomputing it with the new prompt. Figure 7. Interactive video generation. Reward Forcing supports real-time prompt interaction with seamless transitions. Our EMA Sink mechanism ensures seamless prompt transitions while maintaining high temporal consistency. 5. Conclusion We presented Reward Forcing, which tackles the problem of motion stagnation in efficient streaming video generation. Our solution is built on two pillars: the EMA-Sink mechanism, which dynamically maintains context to prevent over-dependence on initial frames and ensures longterm coherence, and Re-DMD, which enhances motion dynamics by prioritizing high-reward samples during distillation. Our experiments confirmed that the proposed method achieves state-of-the-art performance on standard benchmarks. By successfully balancing high visual fidelity with strong dynamic motion, Reward Forcing enables highquality streaming video generation in real-time. This work establishes new benchmark for performance and efficiency in generating dynamic, interactive virtual worlds. 6. Acknowledgments and Technology Projects This work was supported by the National Major Science (the grant number 2022ZD0117000) and the National Natural Science Foundation of China (grant number 62202426). This work was supported by Ant Group Research Intern Program."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 6, 7 [2] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aaron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 3: new frontier for world models. 2025. 1 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 6 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 6 [5] Hengyuan Cao, Yutong Feng, Biao Gong, Yijing Tian, Yunhong Lu, Chuang Liu, and Bin Wang. Dimension-reduction attack! video generative models are experts on controllable image synthesis. arXiv preprint arXiv:2505.23325, 2025. 6 [6] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 2, [7] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 2, 6, 7 [8] Nan Chen, Mengqi Huang, Yihao Meng, and ZhenLonganimation: Long animation generaarXiv preprint dong Mao. tion with dynamic global-local memory. arXiv:2507.01945, 2025. 6 [9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 6 [10] Jiale Cheng, Ruiliang Lyu, Xiaotao Gu, Xiao Liu, Jiazheng Xu, Yida Lu, Jiayan Teng, Zhuoyi Yang, Yuxiao Dong, Jie Tang, et al. Vpo: Aligning text-to-video generation models with prompt optimization. arXiv preprint arXiv:2503.20491, 2025. 6 [11] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Selfforcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 1, [12] Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1770217711, 2025. 6 [13] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 6 [14] Xueji Fang, Liyuan Ma, Zhiyang Chen, Mingyuan Zhou, Inflvg: Reinforce inference-time conarXiv preprint and Guo-jun Qi. sistent long video generation with grpo. arXiv:2505.17574, 2025. 6 [15] Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, and Hongyang Zhang. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. 6 [16] Hiroki Furuta, Heiga Zen, Dale Schuurmans, Aleksandra Faust, Yutaka Matsuo, Percy Liang, and Sherry Yang. Improving dynamic object interactions in text-to-video generation with ai feedback. arXiv preprint arXiv:2412.02617, 2024. 4, 6 [17] Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative planning as general-purpose manipulation world model. arXiv preprint arXiv:2412.08261, 2024. [18] Jianxiong Gao, Zhaoxi Chen, Xian Liu, Jianfeng Feng, Chenyang Si, Yanwei Fu, Yu Qiao, and Ziwei Liu. Longvie: Multimodal-guided controllable ultra-long video generation. arXiv preprint arXiv:2508.03694, 2025. 2, 6 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6, 7, 5 [19] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 6 [20] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Longcontext autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. 2 [21] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long arXiv preprint context arXiv:2503.10589, 2025. tuning for video generation. [22] Haoran He, Yang Zhang, Liang Lin, Zhongwen Xu, and Ling Pan. Pre-trained video generative models as world simulators. arXiv preprint arXiv:2502.07825, 2025. 6 [23] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 6 [24] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from In Proceedings of the Computer Vision and Pattern text. Recognition Conference, pages 25682577, 2025. 2, 6 [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [26] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 6 [27] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. [28] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 6 [29] Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, and Rui Huang. reinforcement learning-based automatic video editing method using pre-trained vision-language model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 64416450, 2023. 6 [30] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, 6, 7, 1, 5 [31] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In [32] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, YingCong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench++: Comprehensive and versatile bencharXiv preprint mark suite for video generative models. arXiv:2411.13503, 2024. 6, 7 [33] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 2, [34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3 [35] Jisoo Kim, Wooseok Seo, Junwan Kim, Seungho Park, Sooyeon Park, and Youngjae Yu. Vip: Iterative online preference distillation for efficient video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1723517245, 2025. 6 [36] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 6 [37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 6 [38] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textarXiv preprint to-image models using human feedback. arXiv:2302.12192, 2023. 4 [39] Wuyang Li, Wentao Pan, Po-Chien Luan, Yang Gao, and Infinite-length arXiv preprint Alexandre Alahi. video generation with error recycling. arXiv:2510.09212, 2025. Stable video infinity: [40] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 6 [41] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. 2 [42] Wenfeng Lin, Renjie Chen, Boyuan Liu, Shiyue Yan, Ruoyu Feng, Jiangchuan Wei, Yichen Zhang, Yimeng Zhou, Chao Feng, Jiao Ran, et al. Contentv: Efficient training of video generation models with limited compute. arXiv preprint arXiv:2506.05343, 2025. 6 [43] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 2 [44] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 2, 4, [45] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025. 2, 6 [46] Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omnipreference alignment for video diffusion generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 80098019, 2025. 2, 6 [47] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, 37:131434131455, 2024. 6 [48] Yunhong Lu, Qichao Wang, Hengyuan Cao, Xierui Wang, Xiaoyin Xu, and Min Zhang. Inpo: Inversion preference optimization with reparametrized ddim for efficient diffusion model alignment. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2862928639, 2025. 2 [49] Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, and Min Zhang. Smoothed preference optimization via renoise inversion for aligning diffusion models with varied human preferences. arXiv preprint arXiv:2506.02698, 2025. 2 [50] Yintai Ma, Diego Klabjan, and Jean Utke. Video to video generative adversarial network for few-shot learning based on policy gradient. arXiv preprint arXiv:2410.20657, 2024. 6 [51] T.K. Moon. The expectation-maximization algorithm. IEEE Signal Processing Magazine, 13(6):4760, 1996. 4, 5 [52] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 3 [53] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1, [54] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th International Conference on Machine Learning, page 745750, New York, NY, USA, 2007. Association for Computing Machinery. 4, 5 [55] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1 [56] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. 2, 6 [57] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. 2 [58] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 6 [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [60] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 2 [61] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 6 [62] Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, and Xun Huang. Motionstream: Real-time video generation with interactive motion controls. arXiv preprint arXiv:2511.01266, 2025. 2 [63] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 6 [64] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. History-guided video diffusion. arXiv preprint arXiv:2502.06764, 2025. 2, 6 [65] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [66] Yanxiao Sun, Jiafu Wu, Yun Cao, Chengming Xu, Yabiao Wang, Weijian Cao, Donghao Luo, Chengjie Wang, and Yanwei Fu. Swiftvideo: unified framework for few-step video generation through trajectory-distribution alignment. arXiv preprint arXiv:2508.06082, 2025. [67] R.S. Sutton and A.G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 9(5): 10541054, 1998. 2, 3, 4 [68] Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, et al. Longcat-video technical report. arXiv preprint arXiv:2510.22200, 2025. 6 [69] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 2, 3, 6 [70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4, 6 [71] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 6 [72] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 6 [73] Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. Advances in Neural Information Processing Systems, 37: 6561865642, 2024. [74] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):30593078, 2025. 6 [75] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. 6 [76] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 1 [77] Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025. 6 [78] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. 3 [79] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 2, [80] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 6 [81] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 2 [82] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 2, 3, 6, 7, 1, 5 [83] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 6 [84] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 6 [85] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 2, 3, 4 [86] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 2 [87] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 3, [88] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2296322974, 2025. 2, 3, 6, 7, 1, 5 [89] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel AlInstructvideo: Instructing video difbanie, and Dong Ni. In Proceedings of fusion models with human feedback. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64636474, 2024. 2, 6 [90] Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, et al. Lumos-1: On autoregressive video generation from unified model perspective. arXiv preprint arXiv:2507.08801, 2025. 2, 6 [91] Hui Zhang, Zuxuan Wu, Zhen Xing, Jie Shao, and Yu-Gang Jiang. Adadiff: Adaptive step selection for fast diffusion. arXiv preprint arXiv:2311.14768, 2023. 2 [92] Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, and Kai Han. Onlinevpo: Align video diffusion model with online video-centric preference optimization. arXiv preprint arXiv:2412.15159, 2024. 2 [93] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2(3):5, 2025. 2, 6 [94] Kaifeng Zhao, Gen Li, and Siyu Tang. Dartcontrol: diffusion-based autoregressive motion model for arXiv preprint real-time text-driven motion control. arXiv:2410.05260, 2024. 6 [95] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. 6 Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation"
        },
        {
            "title": "Supplementary Material",
            "content": "S1: More Video Results Please check the videos in the project page https://reward-forcing.github.io/. These videos are compressed to approximately 40% of their original file size without significant quality degradation. Comparison with state-of-the-art methods. In addition to Fig. 1, Fig. 4, and Fig. 5 in the main paper, we provide additional video results in our supplementary materials for more comprehensive evaluation of long videos (approximately 1 minute) generated by different methods. This page includes comparative studies with state-of-the-art methods. We randomly sample prompts from MovieGenBench [55], focusing on Scene Navigation and Object Motion. As demonstrated in the videos, our Reward Forcing preserves high visual fidelity while exhibiting superior motion dynamics over ultra-long horizon, which is crucial for simulating dynamic environments. Interactive videos. In addition to Fig. 7 in the main paper, we include more interactive video results in the Reward Forcing.html page, demonstrating that our Reward Forcing enables user interaction during streaming generation. Specifically, by switching prompts and resetting the cross-attention cache, the model can introduce new events into the ongoing video. S2: User Studies Experimental setup. To comprehensively evaluate the performance of our proposed method in long video generation, we conducted user study with 20 participants. Each participant was presented with 20 video groups, where each group contained four videos generated by different methods: CausVid [88], Self-Forcing [30], LongLive [82], and Reward Forcing (ours). The videos were randomly labeled as A, B, C, and to avoid bias. In total, we collected 1,600 evaluations (20 participants 20 video groups 4 videos). Evaluation protocol. Participants are asked to evaluate each video for three key criteria using 4-point Likert scale (1-4): Long-Range Temporal Consistency: This metric assesses whether the video maintains visual quality and coherence throughout its entire duration without experiencing visual drift, artifacts, or inconsistencies. Participants evaluated how well each video preserved semantic and structural consistency from start to finish. Dynamic Complexity: This metric measures the naturalness, richness, and engagement of motions and changes in the video. Participants assessed whether the generated content exhibited realistic and diverse dynamics rather than static or repetitive patterns. Overall Preference: This metric captures the holistic quality and appeal of each video, combining factors such as visual fidelity, coherence, motion quality, and subjective viewing experience. For each criterion, participants assigned scores ranging from: 4 (Good): High quality with no noticeable issues. 3 (Borderline Accept): Acceptable quality with minor issues. 2 (Borderline Reject): Below acceptable quality with noticeable issues 1 (Poor): Unacceptable quality with major issues. Results and analysis. The user study results unequivocally demonstrate the superiority of our proposed Reward Forcing method over the baseline models across all evaluation criteria (Tab. 4). Our method achieved the highest scores, nearing the Good (4) benchmark on the Likert scale, with 3.60 for Temporal Consistency, 3.72 for Dynamic Complexity, and 3.75 for Overall Preference. This indicates that participants consistently rated our videos as high-quality with no noticeable issues. These results validate that Reward Forcing sets new state-of-the-art for coherent and engaging long video generation. S3: More Quantitative Results and Details Quality drift. We report the quality drift evaluation results in Tab. 2 and Tab. 3 in the main paper. To quantify the variability in long video imaging quality, we calculate the quality score drift along the temporal horizon using standard deviation, Figure 8. User study instruction screenshots. Table 4. Average User Rating."
        },
        {
            "title": "Temporal Consistency Dynamic Complexity Overall Preference",
            "content": "CausVid [88] Self Forcing [30] LongLive [82]"
        },
        {
            "title": "Reward Forcing",
            "content": "1.81408 1.19437 2.78873 3.60282 1.72676 1.75493 2.38310 3.72113 1.87324 1.27042 2.74648 3. inspired by Zhang et al. [93]. Each one-minute video is divided into clips where = 30, each lasting 2 seconds. For any given long video clip Vi, we compute the drift as follows: Drift(Vi) = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nM − 1",
            "content": "M (cid:88) (si,j si), j=1 (12) where Ci,j represent clip from video i, si,j be the imaging quality score of clip Ci,j. The overall drift across all videos is the mean of individual video drifts: Drift ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Drift(Vi), (13) where is the total number of videos. Our results show that this metric effectively reflects video quality over long horizons, demonstrating strong correlation between lower drift scores and more consistent visual fidelity throughout the sequences. Qwen3-VL evaluation details. We use powerful vision-language model, Qwen3-VL-235B-A22B-Instruct [81], for more comprehensive evaluation and report the results in Tab. 2 in the main paper. We include the evaluation template and detailed results for different methods as follows."
        },
        {
            "title": "Full Input Template",
            "content": "[VIDEO] Please act as video quality evaluation expert and rate the given video and text prompt on scale of 1-5 across the following three dimensions: **Evaluation Dimensions:** 1. **Text Alignment**: Measures the consistency between the video content and the text description. - 1: Completely Irrelevant - Content is unrelated or severely contradicts the description. - 2: Mostly Mismatched - Only few minor elements are relevant; the core concept is missing or incorrect. - 3: Partially Matched - The core idea is present but with significant deviations or missing key elements. - 4: Largely Consistent - Faithfully represents the description with only minor omissions or discrepancies. - 5: Perfectly Aligned - Comprehensive and accurate representation of the entire text description. 2. **Dynamics**: Evaluates the dynamism and fluidity of the entire scene, including camera movement, object motion, and scene transitions. - 1: Static / Disjointed - Little to no dynamic elements; or motion is severely broken and incoherent. - 2: Mostly Static - Limited, simple motion; dynamics feel stiff, mechanical, or poorly executed. - 3: Moderately Dynamic - Basic movement is present but lacks fluidity and natural flow; may appear robotic. - 4: Largely Dynamic - Generally fluid and engaging motion with good sense of flow; minor imperfections may exist. - 5: Highly Dynamic - Exceptionally smooth, natural, and purposeful motion that enhances the visual narrative. 3. **Visual Quality**: Assesses the technical execution, including clarity, color grading, composition, and the absence of artifacts. - 1: Very Poor - Severely blurry, heavy visual artifacts (e.g., distortion, tearing), and/or extreme color issues (e.g., over-saturation, color banding). - 2: Poor - Consistently blurry, noticeable noise, unnatural color palette, or frequent minor artifacts. - 3: Fair - Passable clarity and color, but with visible technical flaws; composition may be unremarkable. - 4: Good - Clear and mostly sharp, with natural and balanced colors; good composition and only minor, infrequent issues. - 5: Excellent - Technically superior: sharp, well-composed, with vivid yet natural colors, and free from visible artifacts or distortions. **Scoring Requirements:** - Please output strictly in the following format, only numbers and brief reasons: Text Alignment: [1-5] Reason: [brief explanation] Dynamics: [1-5] Reason: [brief explanation] Visual Quality: [1-5] Reason: [brief explanation] Now please evaluate the following content: Text Prompt: Video Content: Please carefully watch the provided video"
        },
        {
            "title": "CausVid Full Results",
            "content": "Average Scores by Dimension: Text Alignment: 3.32 Dynamics: 3.16 Visual Quality: 4.66 Detailed Scores by Dimension: Text Alignment: [5, 1, 2, 4, 5, 5, 5, 5, 5, 1, 3, 4, 1, 1, 4, 2, 4, 4, 2, 2, 3, 3, 2, 1, 5, 3, 2, 5, 3, 4, 5, 4, 2, 3, 2, 1, 3, 3, 2, 3, 5, 3, 1, 4, 5, 2, 2, 3, 2, 4, 5, 5, 3, 5, 3, 5, 3, 5, 5, 5, 4, 2, 5, 5, 5, 5, 2, 5, 5, 5, 3, 2, 2, 5, 1, 3, 2, 4, 2, 2, 3, 2, 3, 3, 2, 1, 3, 2, 1, 2, 5, 3, 4, 4, 5, 3, 3, 5, 5, 1, 2, 4, 5, 5, 4, 3, 2, 5, 2, 5, 5, 3, 5, 4, 2, 5, 4, 2, 5, 5, 2, 1, 2, 5, 1, 3, 1, 5] Dynamics: [4, 3, 3, 5, 4, 3, 4, 4, 3, 1, 4, 4, 2, 2, 4, 2, 4, 3, 4, 3, 4, 4, 3, 4, 3, 2, 3, 4, 2, 3, 4, 3, 3, 4, 2, 3, 4, 2, 2, 3, 4, 3, 3, 4, 3, 2, 3, 2, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 1, 3, 4, 1, 5, 3, 4, 4, 2, 3, 4, 3, 2, 3, 2, 4, 4, 5, 2, 3, 2, 2, 1, 4, 3, 4, 3, 3, 3, 2, 3, 4, 3, 2, 4, 4, 1, 1, 3, 4, 4, 4, 2, 4, 3, 3, 3, 3, 2, 4, 5, 3, 4, 4, 3, 3, 3, 3, 4, 2, 4, 2, 2, 3, 3] Visual Quality: [5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 4, 5, 4, 5, 5, 4, 3, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 4, 4, 4, 5, 5, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 4, 5, 5, 5, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 4, 5]"
        },
        {
            "title": "SkyReels Full Results",
            "content": "Average Scores by Dimension: Text Alignment: 2.70 Dynamics: 3.05 Visual Quality: 3.30 Detailed Scores by Dimension: Text Alignment: [4, 2, 3, 2, 5, 5, 4, 2, 4, 1, 5, 3, 1, 2, 3, 1, 5, 2, 3, 1, 1, 1, 4, 1, 1, 2, 2, 1, 3, 4, 4, 1, 3, 3, 3, 1, 2, 5, 2, 2, 2, 2, 1, 4, 1, 1, 1, 2, 2, 2, 4, 3, 2, 5, 4, 5, 2, 5, 1, 5, 2, 2, 5, 2, 1, 5, 2, 4, 5, 3, 4, 2, 3, 3, 2, 4, 3, 1, 2, 2, 1, 3, 1, 3, 1, 5, 5, 1, 1, 3, 4, 4, 2, 1, 5, 4, 2, 3, 1, 4, 3, 1, 3, 2, 5, 3, 1, 1, 1, 5, 5, 1, 3, 3, 2, 3, 4, 1, 3, 5, 3, 2, 4, 1, 1, 5, 1, 5] Dynamics: [5, 3, 1, 2, 3, 4, 3, 3, 3, 3, 4, 3, 3, 3, 4, 3, 5, 4, 4, 2, 3, 2, 4, 1, 3, 3, 4, 1, 2, 3, 3, 3, 2, 4, 4, 3, 3, 4, 3, 3, 2, 1, 3, 4, 4, 1, 2, 4, 3, 3, 5, 4, 2, 4, 3, 4, 3, 3, 2, 3, 3, 3, 4, 1, 2, 4, 3, 5, 3, 2, 4, 4, 4, 3, 3, 4, 3, 2, 2, 3, 1, 4, 3, 2, 2, 5, 4, 2, 1, 3, 3, 3, 2, 1, 3, 3, 3, 4, 3, 4, 3, 1, 4, 3, 5, 2, 1, 3, 1, 3, 4, 3, 4, 4, 2, 3, 4, 4, 2, 4, 4, 4, 5, 2, 4, 4, 3, 3] Visual Quality: [4, 4, 2, 1, 4, 5, 3, 4, 3, 4, 5, 4, 4, 3, 4, 3, 5, 5, 3, 1, 3, 1, 4, 1, 3, 5, 4, 1, 4, 5, 4, 3, 5, 2, 4, 4, 3, 5, 4, 2, 3, 2, 3, 3, 4, 1, 3, 4, 3, 4, 4, 3, 2, 4, 3, 4, 3, 3, 3, 3, 2, 4, 3, 1, 3, 5, 3, 4, 3, 1, 4, 4, 4, 2, 3, 3, 4, 1, 3, 1, 3, 3, 2, 4, 4, 5, 3, 1, 2, 4, 4, 5, 3, 1, 4, 4, 4, 4, 2, 4, 3, 2, 4, 4, 5, 2, 3, 3, 1, 4, 4, 4, 3, 2, 2, 4, 3, 4, 2, 4, 3, 5, 4, 4, 5, 5, 4, 5]"
        },
        {
            "title": "Self Forcing Full Results",
            "content": "Average Scores by Dimension: Text Alignment: 3.11 Dynamics: 3.44 Visual Quality: 3.89 Detailed Scores by Dimension: Text Alignment: [4, 1, 2, 2, 5, 3, 5, 3, 5, 1, 4, 3, 1, 1, 5, 3, 2, 2, 3, 3, 1, 2, 2, 1, 5, 4, 2, 5, 4, 5, 3, 2, 3, 3, 2, 5, 1, 3, 3, 1, 4, 3, 1, 2, 1, 2, 1, 4, 4, 3, 1, 5, 5, 5, 1, 5, 4, 3, 3, 2, 4, 2, 5, 5, 2, 5, 3, 5, 5, 3, 4, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 2, 2, 2, 5, 5, 4, 2, 1, 2, 5, 4, 2, 5, 5, 3, 2, 5, 1, 1, 3, 1, 5, 5, 3, 4, 2, 5, 2, 5, 4, 3, 3, 3, 3, 5, 2, 2, 5, 5, 3, 3, 4, 5, 3, 2, 1, 5] Dynamics: [4, 3, 3, 2, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 1, 4, 3, 3, 4, 3, 4, 3, 4, 2, 4, 4, 5, 3, 3, 3, 3, 4, 2, 3, 2, 4, 4, 3, 4, 3, 3, 4, 4, 2, 3, 2, 4, 4, 3, 4, 3, 4, 4, 5, 3, 4, 4, 4, 5, 3, 3, 5, 3, 4, 4, 4, 4, 4, 2, 4, 4, 5, 3, 3, 2, 5, 5, 4, 4, 4, 4, 3, 3, 2, 4, 3, 3, 3, 4, 2, 2, 4, 1, 4, 4, 4, 3, 5, 3, 4, 3, 3, 4, 4, 4, 4, 3, 4, 3, 3, 3, 4, 4, 4, 4, 4, 3, 2, 3] Visual Quality: [5, 4, 4, 2, 4, 4, 5, 4, 4, 4, 5, 3, 2, 3, 5, 4, 5, 4, 3, 4, 4, 3, 4, 2, 5, 4, 4, 4, 5, 5, 4, 4, 3, 3, 3, 5, 4, 4, 3, 2, 5, 4, 4, 3, 5, 3, 4, 4, 4, 3, 4, 5, 4, 4, 3, 3, 5, 4, 4, 3, 4, 5, 5, 4, 3, 4, 3, 5, 4, 4, 5, 4, 3, 4, 5, 5, 4, 1, 4, 3, 4, 2, 3, 4, 5, 5, 5, 4, 4, 5, 4, 5, 4, 4, 3, 3, 4, 4, 3, 4, 3, 1, 4, 4, 5, 5, 3, 3, 4, 3, 3, 4, 4, 5, 4, 5, 3, 3, 5, 4, 3, 5, 5, 4, 5, 4, 4, 4]"
        },
        {
            "title": "LongLive Full Results",
            "content": "Average Scores by Dimension: Text Alignment: 3.98 Dynamics: 3.81 Visual Quality: 4.79 Detailed Scores by Dimension: Text Alignment: [5, 2, 4, 5, 5, 5, 5, 5, 5, 1, 5, 3, 1, 2, 5, 4, 4, 4, 3, 5, 3, 3, 3, 4, 5, 5, 3, 5, 3, 5, 5, 4, 4, 3, 3, 2, 5, 5, 2, 3, 5, 3, 1, 5, 5, 5, 4, 5, 4, 1, 5, 5, 5, 5, 3, 5, 4, 5, 5, 5, 5, 2, 5, 5, 5, 3, 2, 5, 5, 5, 4, 1, 5, 5, 2, 5, 5, 5, 4, 4, 1, 4, 4, 2, 3, 5, 5, 3, 1, 5, 5, 3, 3, 5, 5, 4, 2, 3, 5, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 4, 5, 4, 3, 5, 5, 5, 2, 4, 5, 1, 3, 1, 5] Dynamics: [5, 4, 3, 4, 4, 4, 3, 4, 3, 4, 4, 3, 4, 3, 4, 3, 5, 5, 4, 5, 3, 4, 4, 3, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 2, 3, 5, 4, 4, 5, 5, 4, 3, 4, 4, 1, 3, 4, 4, 4, 3, 4, 3, 5, 4, 4, 1, 4, 4, 4, 4, 3, 3, 4, 3, 5, 5, 4, 5, 4, 4, 5, 5, 4, 4, 3, 4, 2, 5, 4, 4, 4, 5, 3, 3, 2, 3, 3, 3, 2, 3, 4, 3, 4, 4, 4, 4, 5, 5, 5, 3, 4, 3, 4, 5, 5, 5, 5, 3, 5, 4, 4, 4, 5, 4, 5, 3, 4, 2, 4, 3] Visual Quality: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 4, 3, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 5, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5]"
        },
        {
            "title": "Reward Forcing Full Results",
            "content": "Average Scores by Dimension: Text Alignment: 4.04 Dynamics: 4.18 Visual Quality: 4.82 Detailed Scores by Dimension: Text Alignment: [5, 2, 4, 2, 5, 5, 5, 5, 3, 1, 5, 5, 1, 1, 5, 5, 5, 4, 5, 5, 3, 5, 5, 4, 5, 5, 4, 5, 3, 5, 5, 4, 5, 4, 3, 2, 4, 4, 1, 3, 5, 3, 1, 5, 5, 5, 2, 5, 3, 2, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 2, 5, 5, 5, 2, 2, 5, 5, 5, 4, 1, 5, 5, 3, 5, 5, 5, 3, 4, 2, 4, 3, 4, 4, 5, 4, 5, 1, 5, 5, 4, 3, 5, 5, 4, 3, 5, 5, 2, 5, 4, 5, 5, 5, 4, 4, 5, 3, 5, 4, 5, 5, 2, 4, 5, 5, 3, 4, 5, 4, 2, 5, 5, 2, 5, 2, 5] Dynamics: [5, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 5, 3, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 3, 4, 4, 3, 4, 5, 5, 4, 4, 5, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, 5, 4, 4, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 4, 4, 4, 4, 3, 5, 4, 5, 4, 5, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 5, 4, 4, 5, 5, 5, 5, 4, 4, 4, 3, 5, 5, 4, 5, 4, 5, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4] Visual Quality: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 3, 5, 5, 5, 5, 5, 4, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5] Detailed results on VBench [31]. We report quantitative evaluation on VBench [31] using the extended prompts in Tab. 1 in the main paper. Specifically, the Quality Score is weighted average of the following dimensions: subject consistency, background consistency, temporal flickering, motion smoothness, aesthetic quality, imaging quality, and dynamic degree. The Semantic Score is weighted average of the following dimensions: object class, multiple objects, human action, color, spatial relationship, scene, appearance style, temporal style, and overall consistency. Each dimensions results are normalized using the following formula: normalized score = (score - min) / (max - min). The normalization range (minimum and maximum) for each dimension and the assigned weights used to compute the weighted average are provided in the Tab. 5. Table 5. Normalization ranges and weighting coefficients of VBench score. Subject Background Temporal Motion Consistency Consistency Flickering Smoothness Dynamic Aesthetic Quality Degree Imaging Quality Consistency Overall min max weighting coefficients min max weighting coefficients 0.1462 1.0 1 Object Class 0.0 1.0 1 0.2615 1.0 1 Multiple Objects 0.0 1.0 0.6293 1.0 1 Human Action 0.0 1.0 1 0.706 0.9975 1 Color 0.0 1.0 0.0 1.0 0.5 Spatial Relationship 0.0 1.0 1 0.0 1.0 1 Scene 0.0 0.8222 0.0 1.0 1 0.0 0.364 1 Temporal Appearance Style 0.0 0.364 1 Style 0.0009 0.2855 1 In addition, we provide detailed evaluation in Tabs. 6 and 7. Our method achieves total score of 84.13 on the short video generation task using the extended VBench prompts, consistently surpassing current state-of-the-art baselines and demonstrating its effectiveness. Table 6. Quality evaluation on extended VBench."
        },
        {
            "title": "Dynamic Aesthetic Imaging Quality Total\nScore Score",
            "content": "CausVid [88] Self Forcing [30] LongLive [82]"
        },
        {
            "title": "Reward Forcing",
            "content": "96.33 95.09 96.98 95.43 95.84 96.10 96.92 96.59 99.44 99.01 99.35 98. 97.98 98.24 98.79 98.32 61.11 66.38 40.83 68.05 64.52 65.79 67.03 65. 67.96 69.71 69.18 69.38 83.93 82.88 84.59 83.80 83.68 83.22 84.84 84.13 Table 7. Semantic evaluation on extended VBench. Model Object Multiple Human Class Objects Action 92.78 CausVid [88] Self Forcing [30] 93.16 96.28 LongLive [82] 88.32 87.19 86.49 96.20 96.40 95.80 Color 86.67 86.83 90. Reward Forcing 94.81 86.79 96.80 89.42 Spatial Relationship 74.05 81.77 80.56 82.47 S4: More Implementation details Temporal Appearance Scene 51.35 56.13 58. Style 23.95 24.45 24.16 57.19 24.33 Style 20.19 20.34 20. 20.38 Overall Consistency Semantic Total Score Score 25.95 26.85 26.61 26. 78.69 80.64 81.37 82.88 83.80 83.22 81.32 84.13 Noise schedule and model parameterization. Building upon the Wan2.1 and Self Forcing, our approach utilizes the flow matching framework. We implement time step shift defined as t(k, t) = (kt/1000)/(1 + (k 1)(t/1000)) 1000 with shift factor set to 5. In the forward process, sample is generated according to xt = 1000 ϵ, where ϵ is drawn from standard normal distribution (0, I) and ranges from 0 to 1000. The data prediction model is formulated as : 1000 + 1t Gθ(x, t, c) = cskip ϵ cout vθ(cin xt, cnoise(t), c). (14) The preconditioning coefficients remain consistent with the base models settings: specifically, cskip, cin, cout are all 1, and cnoise(t) = t. For our few-step diffusion sampling, we adopt uniform 4-step schedule with time steps (cid:2)t1, t2, t3, t4 (cid:3) = (cid:2)1000, 750, 500, 250(cid:3). S5: Further Related Works Video diffusion models. Video diffusion models [3, 23, 26, 27] have evolved from UNet [61] backbones to Diffusion Transformers (DiTs) [53]. Early approaches extended image diffusion models temporally [63] but lacked scalability. DiTs Transformer blocks enhance spatio-temporal modeling, enabling models like Sora [4] and Hunyuan-Video [37] to generate realistic, coherent videos. Hunyuan-Video integrates causal 3D VAE [36] and language models for textual control. Open-Sora [40] advanced efficiency and realism, while Wan 2.1 [72] validated large-scale pre-training benefits and CogVideoX [28, 83] improved alignment via adaptive LayerNorm. For long video generation, Phenaki [71] uses discrete tokens, LVDM [23] employs hierarchical 3D latent generation, and NUWA-XL [84] uses coarse-to-fine processing. LaVie [74] integrates rotary encoding and temporal attention, SEINE [9] enables smooth transitions via stochastic masking, and LCT [21] extends to multi-shot generation. Diffusion forcing [6] combines diffusion quality with autoregressive efficiency. StreamingT2V [24] adds memory modules, History-guided video [64] uses historical context, FramePack [93] compresses frames, Lumos-1 [90] integrates LLM-like architecture, and LongVie [18] introduces multi-modal guidance and degradation-aware training. Testtime training methods [12] generate minute-long videos but incur high costs. Training-free methodsRIFLEx [95] adjusting positional embeddings, FreeNoise [58] combining noise rescheduling with windowed attention, and FreeLong [47] integrating multi-frequency information. Reinforcement learning for video models. Video generative models [5, 8, 35, 59, 66, 68, 70, 77] using MLE or reconstruction loss misalign with human preferences. RL enables direct optimization of preference-aligned objectives [19, 42]. Direct Preference Optimization (DPO) [16] dominates post-training alignment, including VideoDPO [46] for temporal consistency, VisionReward [79] for multi-objective preferences, and variants with physics-based generation. Group Relative Policy Optimization (GRPO), extending PPO [29, 50, 94], improves generalization as shown in DanceGRPO [80]. Rewardbased approaches like InstructVideo [89] with pretrained reward feedback and VADER [56] with unified differentiable rewards bypass policy learning. Inference-time methods like InfLVG [14] incorporate GRPO for dynamic long-form optimization. Collectively, RL serves as both post-training alignment and structural component for preference-aware generation [10, 15, 17, 22, 75], bridging surrogate objectives and human-valued quality. S6: Discussion and Future Work Generalizability. Our method is designed to be general-purpose and plug-and-play, enabling seamless integration with various video generation architectures without requiring substantial modifications to existing pipelines. This flexibility represents significant practical advantage, as it allows researchers and practitioners to adopt our approach with minimal overhead. Misalignment between reward functions and evaluation criteria. The first factor contributing to inconsistent performance is the misalignment between our reward functions optimization direction and VBenchs evaluation criteria. VBench employs comprehensive metrics including temporal consistency, motion smoothness, subject consistency, background quality, aesthetics, and semantic alignment. Our reward model may prioritize certain dimensions over othersfor example, heavily weighting temporal coherence while underemphasizing aesthetic qualities. This asymmetric optimization creates scenarios where reward improvements dont translate proportionally to VBench score gains. Video reward models. Our experiments show that current reward models can effectively guide quality improvements, as reflected in our competitive performance across multiple benchmarks. However, video reward models still face challenges in capturing certain nuanced aspects of video quality, such as long-range temporal dependencies, subtle temporal artifacts like frame jitter, and complex semantic attributes. These models are typically trained on datasets with subjective annotations that may not fully represent all quality dimensions. As video reward models continue to advance, our framework will naturally benefit from these improvements, enabling further optimization. Future research directions. Future research should develop more sophisticated reward models capturing video quality nuances. Promising directions include: multi-objective reward modeling with separate components for different quality dimensions; hierarchical models assessing quality at multiple temporal scales; human-in-the-loop feedback mechanisms grounding models in perceptual judgments; domain-adaptive models adjusting criteria by content type; and architectures encoding physical and semantic priors about real-world dynamics. Advancing reward modeling along these dimensions could help our method achieve its full potential and demonstrate substantial, consistent improvements across comprehensive evaluation frameworks. S7: Border Social Impact This work on efficient streaming video generation presents both significant opportunities and risks. On the positive side, the reduced computational requirements could democratize access to video synthesis technology, benefiting educational content creators, small organizations, and researchers with limited resources. The improved efficiency also reduces energy consumption, contributing to more sustainable AI development. However, we acknowledge serious concerns regarding potential misuse. The accessibility and speed of our method lowers barriers for creating deepfakes and misleading visual content that could spread misinformation or enable identity fraud. Additionally, our reward-based prioritization of dynamic content may inadvertently amplify biases present in vision-language models, potentially marginalizing underrepresented groups or activities. Questions of copyright infringement and consent regarding training data and generated likenesses also warrant careful consideration. To mitigate these risks, we strongly advocate for implementing digital watermarking and provenance tracking in any deployment of this technology. We encourage development of detection tools for synthetic content, clear content labeling practices, and robust usage policies prohibiting malicious applications such as non-consensual deepfakes. We support collaborative efforts among researchers, policymakers, and civil society to establish ethical guidelines, legal frameworks, and media literacy initiatives. Effective governance requires not only technological safeguards but also transparent data practices, diverse evaluation metrics to reduce bias, and ongoing dialogue about responsible use of generative video technologies."
        }
    ],
    "affiliations": [
        "Ant Group",
        "HUST",
        "SIAS-ZJU",
        "SJTU",
        "ZJU"
    ]
}