{
    "paper_title": "Learning Dynamics in Continual Pre-Training for Large Language Models",
    "authors": [
        "Xingjin Wang",
        "Howe Tissue",
        "Lu Wang",
        "Linjing Li",
        "Daniel Dajun Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 6 9 7 7 0 . 5 0 5 2 : r Learning Dynamics in Continual Pre-Training for Large Language Models Xingjin Wang 1 2 Howe Tissue (cid:0) Lu Wang 3 Linjing Li 1 2 Daniel Dajun Zeng"
        },
        {
            "title": "Abstract",
            "content": "Continual Pre-Training (CPT) has become popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyperparameters. 1. Introduction In recent years, large language models (LLMs) exhibit versatile abilities and have garnered significant academic and industrial attention (Dubey et al., 2024; OpenAI, 2023). Continual Pre-Training (CPT) of LLMs aims to enhance 1School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China 2State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China 3Ritzz-AI. Correspondence to: Howe Tissue (project lead) <h-sun20@ tsinghua.org.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). the LLMs abilities in specific downstream domains (e.g. coding, finance, math, etc.) while mitigating the substantial costs associated with re-training (Chen et al., 2023a; agatay Yıldız et al., 2024; Ibrahim et al., 2024). CPT primarily focuses on the evolution of performance across general and downstream domains. It is widely observed and believed that improvements on downstream datasets may come at the expense of performance on general domain tasks, or the well-known catastrophic forgetting (French, 1999; Gupta et al., 2023). Recently, some scaling laws for transfer are proposed in continual pre-training scenarios. For example, Hernandez et al. (2021b) and Barnett (2024) discover law to describe how effective data transferred scales with fine-tuning dataset size and model size. Que et al. (2024) and Gu et al. (2024) propose law to find the optimal replay ratio to balance general and downstream performances. However, very few studies have attempted to quantitatively describe the learning dynamics, particularly many performance variations on general and downstream domains throughout the CPT process. We have two primary research questions (RQs): (1) Can we find such an accurate law containing as many variables that affect the final CPT performance as possible? (2) Can we trace the performance of LLMs throughout CPT process, rather than only final performance like previous works? Studying the first RQ helps researchers clarify the various factors that affect CPT performance and then optimize hyper-parameters through prediction; Studying the second RQ helps community understand the learning dynamics of LLMs at each step of the CPT process, providing deeper insights and theoretical guidance for subsequent CPT research. Following previous works (Gupta et al., 2023; Ibrahim et al., 2024; Que et al., 2024), we trace performance changes using validation loss of the corresponding domains. We find that the CPT loss curve is transfer curve and can be described by decoupling the effects of distribution shift and LR annealing. stronger distribution shift leads to more pronounced deviation in the transfer loss curve while LR annealing results in the local loss drop in both PT and CPT phase. By pilot observation and experimental verification, we discover CPT scaling law that integrates the two factors, enabling fitting and predicting ground-truth loss changes. Learning Dynamics in Continual Pre-Training for Large Language Models (a) Constant PT and CPT LRS. (b) Dpt (FineWeb) Validation Loss. (c) Dcpt (Knowledge Pile) Validation Loss. (d) WSD PT and CPT LRS. (e) Dpt (FineWeb) Validation Loss. (f) Dcpt (Knowledge Pile) Validation Loss. Figure 1. The CPT loss curves of different learning rate schedules (LRS) (constant and warmup-stable-decay (WSD) (Hu et al., 2024)), which are both transfer curve from the hidden PT curve training on Dpt (Blue dashed) to the hidden PT curve training on Dcpt (Orange dashed). The hidden PT curve training on Dcpt is the boundary of transfer curve in both Dpt and Dcpt validation loss. Our proposed scaling law provides comprehensive modeling and understanding of key CPT dynamics variables, such as loss potential, peak LR, training steps, replay ratio, etc. Loss potential refers to the potential of future loss drop via LR annealing and we will explain it in detail later. Moreover, our scaling law demonstrates how they jointly affect the performance of the final model at each CPT step, and guides how to optimize the model by adjusting these factors. In the process of deriving and applying our scaling law, many valuable conclusions are found. For example, (1) PT models with higher loss potential can better adapt to the downstream domain; (2) The performance of general domain would inevitably degrade if turning length becomes infinitely large, meaning that general pre-training is quite adequate or distribution shift is very large; (3) Fundamentally, towards specific goals, our scaling law is able to predict the optimal training hyper-parameters such as loss potential of PT models, peak LR, PT dataset replay ratio, etc. 2. Pilot Observation 2.1. Task Formulation We investigate the dynamics of performance in both general and downstream domains during the CPT process. Following previous works (Ibrahim et al., 2024; Que et al., 2024; Gu et al., 2024; Hernandez et al., 2021a), we assess the performance by examining the corresponding validation loss. Therefore, we concentrate on analyzing the loss curves for both the PT and CPT validation sets. The PT dataset is denoted as Dpt and the CPT dataset is denoted as Dcpt. Experiment Setup. In our main experiments, we employ LLaMA architecture-like model (Dubey et al., 2024) with 106M-1.7B non-embedding parameters. We initially pretrain the LLMs using general dataset, FineWeb (Penedo et al., 2024), and subsequently employ the domain-specific dataset, Knowledge-Pile (Fei et al., 2024), for CPT. We leverage the various LRS to pre-train and continual pre-train the models as shown in Fig. 1. The detailed experimental setup is provided in the Appendix B. Observation. As observed in previous studies (Ibrahim et al., 2024; Gupta et al., 2023), during the CPT process, the Dpt validation loss tends to increase, whereas the Dcpt validation loss decreases. Additionally, throughout both the PT and CPT phases, the loss curve is significantly influenced by the LR annealing, resulting in rapid decline. 2.2. CPT Transfer Loss Curve To enhance our understanding of the dynamics involved in the CPT loss curve, we have also trained two additional types of loss curves: the hidden pre-training curve training on Dpt and the hidden pre-training curve training on Dcpt. Hidden PT Curve Training on Dpt. This curve represents the loss when the model is consistently pre-trained using the Dpt with the same LRS as the actual CPT phase. Hidden PT Curve Training on Dcpt. This curve depicts the loss when the model is trained from scratch on Dcpt, while adhering to the same training setups (such as LR) as those applied in both the complete PT and CPT phases. Transfer Loss Curve. As shown in Fig. 1, the CPT loss 2 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Dpt (FineWeb) validation loss shift. (b) Dcpt (Knowledge Pile) validation loss shift. Figure 2. The transfer loss curve in Dpt and Dcpt validation set of different transfer starting points with constant LRS. We find that the distribution shift term is independent with the transfer starting points and adheres to power-law form. curve is transitional curve on both Dpt and Dcpt validation sets. This curve deviates from the hidden PT curve training on Dpt and aligns with another hidden PT curve. The discrepancy between the transfer loss curve and the hidden PT curve training on Dpt is called distribution shift. As the number of CPT steps approaches infinity, the transfer loss curve is expected to converge with the hidden PT curve training on Dcpt. Finding 1. The process of CPT is how the loss curve transitions from the hidden PT curve training on Dpt to the hidden PT curve training on Dcpt. 3. Continual Learning Dynamics Law In this section, we quantitatively analyze the transfer curve, taking into account both the distribution shift and learning rate annealing. Without data transfer, the CPT loss curve would follow the trajectory of the hidden PT curve training on Dpt, and the distribution shift would describe deviations from this curve. The hidden PT curve reflects the status of the PT model, and the distribution shift term describes the relative relationship between data distribution transfers. 3.1. Hidden Pre-Training Curve Training on Dpt Without data transfer occurring, Tissue et al. (2024) introduced scaling law to describe the loss dynamics at training step affected by the LR annealing: L(t) = L0 + Sα 1 S2 (1) where the forward area S1 = (cid:80)t i=1 ηi is the summed LR, and the annealing area S2 = (cid:80)t k=1 (ηk1 ηk) λik is the item affected by LR annealing. L0, A, C, α are constant positive parameters. λ is hyper-parameter to describe the momentum term. (cid:80)i i=1 The loss in the CPT process without distribution shift (denoted as Lbase(t)) should completely follow the scaling law with LR annealing, that is, 1 +Scpt 1 )α (Spt Lbase(t) = L0 +A(Spt 2 ) and Scpt where Spt 2 ) denote the forward (annealing) area at PT and CPT stages, respectively. Here, denotes the continual training steps. 2 +Scpt 1 (Scpt 2 ) (2) 1 (Spt 3.2. Distribution Shift Term The distribution shift term describes the deviations from hidden PT curve training on Dpt. This shift reflects the distribution distance between Dpt and Dcpt datasets. Moreover, many studies have pointed out the great impact of LRS at CPT stage (Ibrahim et al., 2024; Wang et al., 2024; Parmar et al., 2024). Thus, the shift should be also affected by the LRS. As starting point, we analyze the form of the distribution shift term with constant LR to isolate the effects of LRS. Subsequently, we incorporate the forward area into the equation to accurately describe the distribution shift term curve for other LRS. Constant LRS. We use same constant LR in both PT and CPT phase. To study the relationship between distribution shift and PT model state, we continual pre-train the model with different transfer starting points. As shown in Fig. 2, these distribution shift terms tend to overlap at each transfer starting point. This overlap suggests that the distribution shift term is independent with transfer starting point or PT model checkpoint. We compare to fit the distribution shift term using exponential and power-law forms, and find the best fit for the distribution shift term is L(t) = (1(E t+1)β), we do not adopt the simple power-law form L(t) = tβ for ensuring L(0) = 0. We leverage this equation form to fit the transfer loss curve of both Dpt and Dcpt validation sets as shown in Fig. 2. Learning Dynamics in Continual Pre-Training for Large Language Models (a) WSD PT and CPT LRS. (b) Dpt (FineWeb) Validation Loss. (c) Dcpt (Knowledge Pile) Validation Loss. (d) Cosine PT and CPT LRS. (e) Dpt (FineWeb) Validation Loss. (f) Dcpt (Knowledge Pile) Validation Loss. Figure 3. Using Eq. 4 to fit all PT and CPT loss curve with different LRS (WSD and Cosine). For Dpt validation set, all loss curves (b and e) are described by the same equation; similarly, for Dcpt validation set, all loss curves (c and f) follow the same equation. Other LRS. The forward area also has the same effect in the distribution shift term. The smaller forward area in the CPT will result in smaller curve shift like different orange transfer curves in Fig. 1. Hence, following Tissue et al. (2024), we replace the training steps with the forward area in the CPT Scpt 1 : L(t) = (1 (1 + Scpt 1 )β) (3) 3.3. Final Transfer Curve We combine the hidden PT curve training on Dpt and the distribution shift term to get complete formulation to describe the CPT transfer loss curve: L(t) = Lbase(t) + L(t) = L0 + (cid:0)Spt 1 + Scpt 1 (cid:124) (cid:123)(cid:122) Scaling law with LR annealing (cid:1)β(cid:17) + (cid:124) (cid:16) 1 (cid:0)1 + Scpt (cid:123)(cid:122) Power-law distribution shift 1 (cid:125) (cid:1)α C1 Spt 2 C2 Scpt (cid:125) 2 (4) We adopt different coefficients C1 and C2 for Spt 2 and Scpt because training datasets between PT and CPT phases are different, and thus bring different annealing amount. The above loss curve function could describe the loss of any steps with any LRS during both the PT and CPT phases. We conduct experiments utilizing the widely adopted WSD (Hu et al., 2024) and cosine (Loshchilov & Hutter, 2016) LRS to pre-train and continual pre-train the model as shown in Fig. 3a and Fig. 3d. We use Eq. 4 to fit all loss curves within the Dpt and Dcpt validation sets. As illustrated in 4 the middle and right panels of Fig. 3, our equation successfully captures the trends in loss variations across different LRS throughout the training process. We also use the fitted equation to predict the loss curve of other LRS in the Fig. 10. Furthermore, the batch size and sequence length may undergo adjustments during the CPT phase. However, our scaling law remains adaptable to these changes in hyperparameters, as demonstrated in Appendix F. Finding 2. The CPT loss curve can be decomposed into the hidden PT curve training on Dpt and the distribution shift term. The hidden PT curve training on Dpt is formalized as simple scaling law with LR annealing, whereas the distribution shift term is independent with transfer starting points and adheres to power-law transition. Transfer Loss Surface. To better understand our formulation, we view the loss surface of LLMs as transfer slide in Fig. 4. The CPT process transitions from one slide to another following power-law form. If the distance between two datasets is large, the slope of the transfer surface will be steep, which makes the Dpt loss rise rapidly. When the LR anneals, the amplitude of the oscillation in the loss surface decreases and results in reduction in loss. In the annealing view, we name the height of one point as its loss potential, which captures the potential of future loss drop via LR annealing. Quantitatively, we define loss potential as the ratio of the final LR of the PT annealing phase to the initial or maximum learning rate in the pre-training phase. Learning Dynamics in Continual Pre-Training for Large Language Models without re-warmup and another with re-warmup. We introduce the concept of loss potential to capture the potential of future loss drop via LR annealing (see Fig. 4c). W/o Re-warmup. This involves setting the peak LR for CPT to match the final LR in PT. We conduct experiments to continually pre-train the models with different loss potentials, annealing to zero with the same data tokens. As shown in Fig. 5b, models with higher loss potential achieve lower final losses on the Dcpt validation set. We also utilize our equation to predict the final loss across various CPT steps, confirming that this trend persists as shown in Fig. 5c. 1 and Scpt For the setting without re-warmup, the primary distinction among models with varying loss potentials is the annealing parameters of PT (C1) and CPT (C2), as well as the differences in the forward areas Spt 1 . For the Dcpt validation set, where the annealing parameter C2 exceeds C1, allocating greater annealing area to the CPT stage can result in lower Dcpt loss. Additionally, models with higher potential have larger forward areas Spt 1 and Scpt 1 , which further contribute to lower Dcpt loss. Hence, models with higher loss potential always tend to have lower Dcpt validation loss, consistent with previous works (Wang et al., 2024). With Re-warmup. As more common practice in CPT, the LR is set to re-warmup to 10% of the PT peak LR, followed by some annealing method (e.g., cosine). For the PT models that have not been fully annealed, re-warmup is equivalent to single-step annealing, and the annealing area will gradually increase. As shown in Fig. 5e and Fig. 5f, models with higher loss potential consistently achieve lower final losses on the Dcpt validation set throughout all training steps. This outcome is also attributed to combination of larger annealing coefficient and an expanded forward area. Finding 3. PT models with higher loss potential always achieve lower Dcpt validation losses. Hence, we advocate that when releasing open-source models, it is beneficial to release high loss potential version to better adapt to downstream domains. 4.2. Distribution Distance between PT and CPT dataset The distance between the PT and CPT dataset distributions significantly influences the distribution shift observed during the CPT process. When the Dcpt is highly domain-specific, this distance tends to be substantial, leading to an intense distribution shift that impacts both Dpt and the Dcpt validation sets. We compare the law (Henderson* et al., 2022) and knowledge pile (Fei et al., 2024) as Dcpt and find that the distribution shift difference is significant with the same CPT steps as shown in Fig. 6a. (a) Loss surface of Dpt as transfer slide. (b) Forward view. (c) Annealing view. Figure 4. The loss surface of CPT process and two direction views. 3.4. Extension to Model Size and Replay Ratio Following (Tissue et al., 2024), we incorporate the model size into our CPT scaling law. Our analysis of the distribution shift term across different model sizes reveals that the absolute shift values are nearly identical. This integration allows our scaling law to simultaneously fit the transfer loss curves for all model sizes, as demonstrated in Appendix E. We also integrate the replay ratio into our scaling law. We observe that the replay ratio influences the distribution shift term in an exponential manner, as detailed in Appendix H. Compared to D-CPT law (Que et al., 2024), our scaling law can predict the entire loss curves for different replay ratios, rather than just the final loss. 4. Factor Analyses and Applications Key factors in the CPT include the loss potential of PT models, distribution distance between PT and CPT dataset, peak LR and number of CPT steps. We utilize our scaling law to predict and analyze the impact of these factors. 4.1. Loss Potential Most PT models employ either cosine or WSD LRS that anneal to zero or minimal LR to achieve lower loss values. However, the PT model that is optimally adapted to downstream domains is not necessarily fully annealed model. To investigate the impact of LRS on PT models for CPT, we conduct experiments employing two distinct strategies: one 5 Learning Dynamics in Continual Pre-Training for Large Language Models (a) CPT with different loss potentials (w/o re-warmup setting). (b) Dcpt true loss v.s. CPT step of different loss potentials (w/o re-warmup setting). (c) Dcpt predicted loss v.s. loss potentials of different CPT steps (w/o re-warmup setting). (d) CPT with different loss potentials (w/ re-warmup). (e) Dcpt true loss v.s. CPT step of different loss potentials (w/ re-warmup setting) . (f) Dcpt predicted loss v.s. loss potentials of different CPT steps (w/ re-warmup setting). Figure 5. The impact of the loss potential of PT models. We illustrate the true loss of different loss potential models in the middle of figure. We utilize Eq. 4 to predict the losses of these models across different training steps in the right of figure. The red star () refers to the models that could achieve lowest Dcpt validation loss given the number of CPT steps. (a) The difference of distribution shift of different Dcpt dataset. (b) The distribution shift on Dpt validation set of different replay ratio. (c) The distribution shift on Dcpt validation set of different replay ratio. Figure 6. We compare the distribution shift of different distribution distance between Dcpt and Dpt dataset. Additionally, we examine the impact of different replay ratios on the distribution shifts within both the Dcpt and Dptvalidation sets. Dpt Dataset Replay. Empirically, during the CPT process, it is common practice to mix certain percentage of Dpt with the Dcpt to mitigate rapid increase in Dpt validation loss. The replay ratio plays critical role in influencing the distribution shift. In our experiments, varying the replay ratio revealed that higher ratios lead to smaller distribution shifts, effectively decelerating the deviation from the original dataset distribution 1. 4.3. Peak Learning Rate In the real scenario, choosing an appropriate peak LR to re-warmup is important in the CPT. Different peak LRs 1It is important to note that the Dcpt undergoes modifications in the presence of replays. For instance, if we mix 0.1 FineWeb with 0.9 KP, the actual Dcpt distribution is 0.1 FineWeb with 0.9 KP, not 1.0 KP. significantly affect the Dpt and Dcpt validation loss. We leverage Eq. 4 to predict the final loss of different peak LRs. We assume the PT model anneals with the WSD method and then re-warmup to different peak LR. As shown in Fig. 7a and Fig. 7b, higher peak LR can accelerate the decrease in domain-specific loss, but it can also lead to faster increase in general domain loss. 4.4. Continual Pre-Training Steps In the CPT process, it is essential to predetermine the number of training steps. It is natural that larger number of training steps generally results in lower Dcpt validation loss. However, the impact of CPT steps on the Dpt validation loss exhibits various patterns, including continuous rise, an initial rise followed by decline that does not return to the original loss level, and an initial rise followed by decline below the original loss level. This situation is related 6 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Dpt predict loss of different peak LR. (b) Dcpt predict loss of different peak LR. (c) Critical point and turning length in Dpt. Figure 7. (a)-(b) The effect of the peak LR. We utilize Eq. 4 to predict the final loss of different peak LR. (c) The effect of different CPT steps. We show the critical point and turning length in the Dpt validation loss. (a) The optimal loss potential. (b) The optimal peak LR. (c) The optimal replay ratio. Figure 8. Optimizing hyper-parameters for CPT based on different coefficients to balance the general and downstream performance. with the status of the PT model and the distribution distance. As shown in Fig. 7c, the irreducible loss (labeled as the blue dashed line) of the hidden PT curve on Dcpt produces specific PT step of the PT curve on Dpt, termed the critical point. When CPT occurs before this critical point, the loss curve typically rises before falling, still presenting an opportunity to achieve lower loss than initial loss. The minimum training steps required to achieve lower loss are designated as the turning length. Finding 4. For general domain Dpt, inadequate pre-training or weak distribution shift could result in lower loss than the initial stage after sufficient continual training. Otherwise, we are not likely to get lower loss than initial stage, regardless of the number of training steps. More training often leads to worse general abilities in this situation. 5. Balance Between Dpt and Dcpt Loss The Dpt validation loss and the Dcpt validation loss typically exhibit trade-off relationship. Balancing these losses is critical for optimizing the overall performance of the model during CPT. We define the increase in Dpt loss as LDpt and the decrease in Dcpt loss as LDcpt . To balance the loss of Dpt and Dcpt validation sets, we assign normalized balance coefficient to different validation sets: min Scpt 1 ,Scpt 2 λ1LDpt + λ2LDcpt s.t. λ1 + λ2 = 1 (5) λ1 and λ2 are set based on our prior knowledge with the importance between general and downstream performance. 5.1. Optimal Hyper-Parameters Given the different coefficients λ1 and λ2, there exist some optimal CPT hyper-parameters. Optimal Loss Potential. As shown in Fig. 8a, for different λ1, there exists an optimal loss potential model, which annealing on Dpt to maintain certain general domain performance while reserving sufficient loss potential for downstream domains. Optimal Peak Learning Rate. We can also get an optimal peak LR in the CPT process. larger λ1 suggests preference for minimizing the increase in Dpt loss, thereby necessitating lower peak LR. Their relationship is depicted in Fig. 8b, which follows power-law curve. Optimal Replay Ratio. Based on our scaling law with replay ratio (Eq. 8), we can determine the optimal replay ratio as depicted in Fig. 8c. The same distribution line (blue dashed line) indicates that the optimal ratio for pre-training the model from scratch should be the same as the validation loss target. However, during the CPT process, where the PT model has already been trained with Dpt, the optimal ratio line will shift. Turning Length. As shown in Fig. 13, we can get different turning lengths for different coefficients. When λ1 is smaller, the Dcpt loss predominates within the composite loss calculation, resulting in composite loss that remains Learning Dynamics in Continual Pre-Training for Large Language Models (a) Dood dataset truth and predict loss similar to Dpt. (b) Dood dataset truth and predict loss similar to Dcpt. Figure 9. The predicted loss curve of Dood validation set leveraging the linear combination of Dpt and Dcpt validation loss. We show some different shapes of rising curves similar to Dpt in the left and some different shapes of falling curves similar to Dcpt in the right. consistently below the initial value. Conversely, with moderate λ1, there exists specific turning CPT step. For the larger λ1, no matter the training steps, the composite loss is always higher than the initial loss. OOD datasets and Dpt or Dcpt. As Fig. 9 shows, there are two kinds of OOD datasets including (1) Dpt-like one (larger λ 1) with loss curve upward, and (2) Dcpt-like one (larger λ 2) with loss curve downward. 5.2. Out-of-Domain Validation Set Note that our law (Eq. 4) is specifically applicable to the Dpt and Dcpt validation sets. For those other validation sets (denoted as Dood), our law can not be directly applied. To address this issue, we are inspired by some previous works (Ye et al., 2024; Liu et al., 2025), which found that the out-of-domain validation loss could be effectively represented as the linear combination of other base domains losses. In the CPT process, we represent the losses of some OOD datasets by linear combination based on Dpt and Dcpt validation losses. That is, we have hypothesis that LDood = λ 1LDpt + λ 2LDcpt (6) We verify the hypothesis and calculate λ 2 for several example OOD datasets in Appendix J. Note that the coefficients λ 1 and λ 2 are related only to datasets and not to other training hyper-parameters. 1 and λ Loss Prediction of Dood. The Dood validation loss does not adhere to the formulation described in Eq. 4. However, by calculating and specifying the coefficients λ 2, it becomes feasible to predict the Dood loss curve using linear combination of the Dpt and Dcpt loss curves. It is interesting that this problem reduces to the balance between Dpt and Dcpt loss (Eq. 5), leading to optimal hyper-parameters such as learning rate and replay ratio, which has been adequately discussed in the last section. 1 and λ As shown in Fig. 9, we provide some predictions on various OOD datasets, with labeling perspective coefficients. The almost perfect prediction suggests that our proposed CPT scaling law and linear combination for OOD datasets are quite effective and practical in real scenarios. Moreover, the calculated coefficient represents the similarity between 8 Finding 5. There exists an optimal loss potential, peak LR and replay ratio designed to balance Dpt and Dcpt losses. Besides, the turning lengths vary depending on the different balance weights. Optimizing Dood is equivalent to balancing Dpt and Dcpt losses by utilization of linear combination tricks. 5.3. Open-Source PT Models For large LLM communities, PT models are usually not their own, but come from open-source models. For those open-source PT models, many training details are not reported. Therefore, the distribution of the PT dataset, the loss potential, and the PT training amount usually remain unknown, which inhibits the direct application of our CPT scaling law. For solving this, we propose following simple methods to make our scaling law become applicable again. (a) Firstly, for the unknown PT dataset distribution, some methods based on probing (Hayase et al., 2024) have been proposed. Instead, we simply utilize an open-source Common Crawl dataset as proxy Dpt to approximate the true general performance dynamics. (b) Secondly, when fitting our scaling law, we regard some variables as unknown parameters to fit. For example, we treat Spt 1 as parameter that requires fitting to be close to the undisclosed real Spt 1 . (c) Thirdly, as most open-source PT models anneal to minimal LR to get better performance nowadays, we assume that the final LR for all open-source models is zero when calculating Scpt 2 . Refer to Appendix for more details. To verify our solutions for open-source PT models, we continually pre-train LLaMA3.2-1B (Dubey et al., 2024) and select RedPajama (Computer, 2023) dataset as an proxy Learning Dynamics in Continual Pre-Training for Large Language Models Dpt. As Fig. 18 in Appendix shows, the almost perfect fitting and prediction for CPT loss curve of LLaMA3.2-1B suggests the effectiveness of our proposed methods. Moreover, this result also indicates that our scaling law can be easily extended to CPT scenarios with unknown PT model information, demonstrating the superiority of our scaling law to capture the learning dynamics of CPT. 6. Discussion Laws Formulation. The formulation of S2 in Eq. 1 can be various. For example, S2 could be also multi-power form (Luo et al., 2025), which is following work of Tissue et al. (2024). We adopt our final formulation due to its fewer parameters and practical effectiveness. We also compare some format variates including adding LR-weighted coefficient and adding power to S2 in Appendix I. The results show that all the formats work well while ours have superiority in simplicity (i.e. fewer parameters). Laws Fitting. In our experiments, we predominantly employ constant, cosine, and WSD LRS to fit data, which are widely used in practical applications. It deserves noting that many other LRS could be also utilized to collect fitting data. To apply our scaling law, we use common LRS (e.g. constant and cosine) to train few steps to collect loss at each step. After fitting parameters, our scaling law is also capable of predicting the loss curves under other specialized LRS for much longer training durations. Our scaling law shares the similar idea of fitting cost conservation with Tissue et al. (2024), thanks to our scaling law being able to describe the whole dynamics in CPT rather than only final loss. Limitations. One main limitation of our work is that our laws are primarily based on empirical analyses and experimental verifications. We acknowledge that there is lack of rigorous theoretical analysis and proof because it is difficult to build theoretical deduction in non-toy environment with thousands of LLM training factors. However, our scaling law can reasonably reflect the learning dynamics of the CPT process, which can be applied in practical CPT scenarios. 7. Conclusion In conclusion, we explore the learning dynamics in continual pre-training of large language models. We focus on the evolution of performance across general and downstream domains, with domain performance assessed via validation loss. By observations and analyses, we propose CPT scaling law that integrates distribution shift and learning rate annealing to predict the validation loss at any intermediate training step under common learning rate schedules. Our scaling law provides comprehensive understanding of key CPT factors and helps optimize the hyper-parameters in CPT for different training goals. Further experiments demonstrate that the law can be also extended to more complicated scenarios such as out-of-domain datasets and models with unknown information. We believe that our CPT scaling law is promising to reshape the understanding of researchers for LLM continual pre-training and scaling laws."
        },
        {
            "title": "Impact Statement",
            "content": "Continual Pre-Training (CPT) is effective method to enhance the foundation large language models to specific downstream domains or tasks. Our work provide scaling law to quantitatively describe the learning dynamics of CPT process, enabling to optimize the training hyper-parameters for balancing the general and downstream performance. While there will be important impacts resulting from the use of CPT in general, here we focus on the impact of using our scaling law to provide explanations for CPT process. There are many benefits to using our method, such as predict the loss curve dynamics and optimize hyper-parameters. This paper presents work whose goal is to advance the field of Large Language Models. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A. G., and Bakshy, E. Botorch: framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:2152421538, 2020. Barnett, M. An empirical study of scaling laws for transfer, 2024. URL https://arxiv.org/abs/2408. 16947. Bergstra, J. and Bengio, Y. Random search for hyperparameter optimization. Journal of machine learning research, 13(2), 2012. Chen, W., Zhou, Y., Du, N., Huang, Y., Laudon, J., Chen, Z., and Cui, C. Lifelong language pretraining with distribution-specialized experts. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023a. Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., Kopf, A., Mohtashami, A., Sallinen, A., Sakhaeirad, A., Swamy, V., Krawczuk, I., Bayazit, D., Marmet, A., Montariol, S., Hartley, M.-A., Jaggi, M., and Bosselut, A. Meditron70b: Scaling medical pretraining for large language models, 2023b. URL https://arxiv.org/abs/2311. 16079. Learning Dynamics in Continual Pre-Training for Large Language Models Colombo, P., Pires, T. P., Boudiaf, M., Culver, D., Melo, R., Corro, C., Martins, A. F. T., Esposito, F., Raposo, V. L., Morgado, S., and Desa, M. Saullm-7b: pioneering large language model for law, 2024. URL https:// arxiv.org/abs/2403.03883. Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https://github. com/togethercomputer/RedPajama-Data. DeepSeek-AI, Zhu, Q., Guo, D., Shao, Z., Yang, D., Wang, P., Xu, R., Wu, Y., Li, Y., Gao, H., Ma, S., Zeng, W., Bi, X., Gu, Z., Xu, H., Dai, D., Dong, K., Zhang, L., Piao, Y., Gou, Z., Xie, Z., Hao, Z., Wang, B., Song, J., Chen, D., Xie, X., Guan, K., You, Y., Liu, A., Du, Q., Gao, W., Lu, X., Chen, Q., Wang, Y., Deng, C., Li, J., Zhao, C., Ruan, C., Luo, F., and Liang, W. Deepseekcoder-v2: Breaking the barrier of closed-source models in code intelligence, 2024. URL https://arxiv.org/ abs/2406.11931. Dou, L., Liu, Q., Zeng, G., Guo, J., Zhou, J., Lu, W., and Lin, M. Sailor: Open language models for south-east asia. arXiv preprint arXiv:2404.03608, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv: 2407.21783, 2024. Fei, Z., Shao, Y., Li, L., Zeng, Z., Yan, H., Qiu, X., and Lin, D. Query of cc: Unearthing large scale domainspecific knowledge from public corpora. arXiv preprint arXiv:2401.14624, 2024. French, R. M. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128135, 1999. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Gu, J., Yang, Z., Ding, C., Zhao, R., and Tan, F. CMR scaling law: Predicting critical mixture ratios for continual pre-training of language models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1614316162, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 903. URL https://aclanthology.org/2024. emnlp-main.903. Gupta, K., Therien, B., Ibrahim, A., Richter, M. L., Anthony, Q., Belilovsky, E., Rish, I., and Lesort, T. Continual pre-training of large language models: How to (re)warm your model?, 2023. URL https://arxiv. org/abs/2308.04014. Hayase, J., Liu, A., Choi, Y., Oh, S., and Smith, N. A. Data mixture inference: What do bpe tokenizers reveal about their training data? arXiv preprint arXiv:2407.16607, 2024. Henderson*, P., Krass*, M. S., Zheng, L., Guha, N., Manning, C. D., Jurafsky, D., and Ho, D. E. Pile of law: Learning responsible data filtering from the law and 256gb open-source legal dataset, 2022. URL https: //arxiv.org/abs/2207.00220. Hernandez, D., Kaplan, J., Henighan, T., and McCanarXiv preprint dlish, S. Scaling laws for transfer. arXiv:2102.01293, 2021a. Hernandez, D., Kaplan, J., Henighan, T., and McCandlish, S. Scaling laws for transfer, 2021b. URL https:// arxiv.org/abs/2102.01293. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models. arXiv preprint arXiv: 2203.15556, 2022. Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L., Zhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., Ding, N., Jia, C., Zeng, G., Li, D., Liu, Z., and Sun, M. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv: 2404.06395, 2024. Huber, P. J. Robust Estimation of Location Parameter. The Annals of Mathematical Statistics, 35(1):73 101, 1964. doi: 10.1214/aoms/1177703732. URL https: //doi.org/10.1214/aoms/1177703732. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., Dang, K., Fan, Y., Zhang, Y., Yang, A., Men, R., Huang, F., Zheng, B., Miao, Y., Quan, S., Feng, Y., Ren, X., Ren, X., Zhou, J., and Lin, J. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186. Ibrahim, A., Therien, B., Gupta, K., Richter, M. L., Anthony, Q. G., Belilovsky, E., Lesort, T., and Rish, I. Simple and scalable strategies to continually pre-train large language models. Trans. Mach. Learn. Res., 2024, 2024. URL https://openreview.net/forum? id=DimPeeCxKO. 10 Learning Dynamics in Continual Pre-Training for Large Language Models Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv: 2001.08361, 2020. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Lange, M. D., van de Ven, G. M., and Tuytelaars, T. Continual evaluation for lifelong learning: Identifying the In The Eleventh International Conferstability gap. ence on Learning Representations, 2023. URL https: //openreview.net/forum?id=Zy350cRstc6. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Liu, Q., Zheng, X., Muennighoff, N., Zeng, G., Dou, L., Pang, T., Jiang, J., and Lin, M. Regmix: Data mixture as regression for language model pre-training, 2025. URL https://arxiv.org/abs/2407.01492. Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. International Conference on Learning Representations, 2016. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv: 1711.05101, 2017. Luo, K., Wen, H., Hu, S., Sun, Z., Sun, M., Liu, Z., Lyu, K., and Chen, W. multi-power law for loss curve prediction across learning rate schedules. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=KnoS9XxIlK. Nocedal, J. Updating quasi newton matrices with limited storage. Mathematics of Computation, 35(151): 951958, July 1980. ISSN 0025-5718. doi: 10.1090/ S0025-5718-1980-0572855-7. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Parmar, J., Satheesh, S., Patwary, M., Shoeybi, M., and Catanzaro, B. Reuse, dont retrain: recipe for continued pretraining of language models. arXiv preprint arXiv: 2407.07263, 2024. Paster, K., Santos, M. D., Azerbayev, Z., and Ba, J. Openwebmath: An open dataset of high-quality mathematical web text, 2023. Penedo, G., Kydlıˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum? id=n6SCkn2QaG. Que, H., Liu, J., Zhang, G., Zhang, C., Qu, X., Ma, Y., Duan, F., ZhiqiBai, JiakaiWang, Zhang, Y., Tan, X., Fu, J., Wang, J., Qu, L., Su, W., and Zheng, B. D-CPT law: Domain-specific continual pre-training scaling law for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=JzKFN5fWOk. Shi, H., Xu, Z., Wang, H., Qin, W., Wang, W., Wang, Y., Wang, Z., Ebrahimi, S., and Wang, H. Continual learning of large language models: comprehensive survey, 2024. URL https://arxiv.org/abs/2404.16789. Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. Advances in neural information processing systems, 25, 2012. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, 2023. URL https://huggingface.co/ datasets/cerebras/SlimPajama-627B. Tissue, H., Wang, V., and Wang, L. Scaling law with learning rate annealing, 2024. URL https://arxiv. org/abs/2408.11029. Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V., Dy, J., and Pfister, T. Learning to prompt for continual learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 139149, 2022. doi: 10.1109/CVPR52688.2022.00024. Wang, Z., Liu, S., Huang, J., Zheng, W., Liao, Y., Chen, X., Yao, J., and Su, J. learning rate path switching training paradigm for version updates of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 13581 13594, 2024. Xie, X., Ding, K., Yan, S., Toh, K.-C., and Wei, T. Optimization hyper-parameter laws for large language models, 2025. URL https://arxiv.org/abs/2409. 04777. Ye, J., Liu, P., Sun, T., Zhou, Y., Zhan, J., and Qiu, X. Data mixing laws: Optimizing data mixtures by predicting 11 Learning Dynamics in Continual Pre-Training for Large Language Models language modeling performance, 2024. URL https: //arxiv.org/abs/2403.16952. agatay Yıldız, Ravichandran, N. K., Punia, P., Bethge, M., and Ermis, B. Investigating continual pretraining in large language models: Insights and implications, 2024. URL https://arxiv.org/abs/2402.17400. 12 Learning Dynamics in Continual Pre-Training for Large Language Models A. Related Work Continual Pre-Training. Continual Pre-Training (CPT) aims to continually pre-train LLMs to adapt to new domains, like code (Hui et al., 2024; DeepSeek-AI et al., 2024), medicine (Chen et al., 2023b) and law (Colombo et al., 2024) and avoid training domain-specific LLMs from scratch (Shi et al., 2024). The goal of CPT is to enhance downstream performance while avoiding catastrophic forgetting (Lange et al., 2023; French, 1999; Gupta et al., 2023; Ibrahim et al., 2024). Most existing CPT methods mainly leverage the replay to mix appropriate pre-training data (Que et al., 2024; Gu et al., 2024) or introduce extra model parameters (Wang et al., 2022) to assimilate new domain knowledge. Our work comprehensively study the learning dynamics of CPT and propose CPT scaling law to describe the general and downstream validation loss. Scaling Laws. Kaplan et al. (2020) empirically discovers power-law relationship between validation loss and there factors: model size , dataset size D, and training compute. Hoffmann et al. (2022) develops Chinchilla, compute-optimal LLM to balance model size and dataset size. Tissue et al. (2024) introduces scaling law to describe the learning dynamics affected by the learning rate annealing which can predict loss of any training steps under various LRS. These scaling laws, however, are limited to pre-training scenarios; they do not apply when the training dataset changes. As for continual pre-training, Hernandez et al. (2021b) study scaling law for transfer in model size and CPT data. Barnett (2024) proposes empirical scaling law that incorporates transfer gap term to indicate the distribution difference between two datasets. Some methods like D-CPT (Que et al., 2024) and CMR (Gu et al., 2024) introduce the scaling law with data replay or mixture ratio in the CPT process. Dou et al. (2024) proposes quadratic function to take into account both learning rate and replay ratio. However, these existing scaling laws primarily describe the final loss and do not account for all CPT-related factors. Our CPT scaling law integrates all relevant factors and can predict loss at each CPT step, thereby describing the complete learning dynamics. Hyper-Parameter Optimization. Identifying optimal hyper-parameter settings is crucial for achieving robust performance in machine learning (Bergstra & Bengio, 2012; Snoek et al., 2012). The principal hyper-parameters in large language models include the peak learning rate, learning rate schedules, batch size, training steps and among others (Kaplan et al., 2020; Hu et al., 2024; Xie et al., 2025). Initial approaches to hyper-parameters optimization primarily utilize model-free techniques such as grid and random search (Bergstra & Bengio, 2012). Subsequently, some methods have employed Bayesian Optimization (Balandat et al., 2020) to predict the performance of various hyper-parameters and select the most effective ones accordingly. Our research focuses on the hyper-parameters in the continual pre-training of larger language models with our proposed CPT scaling law. The hyper-parameters which we optimize include the learning rate schedules, peak learning rate, and replay ratio, etc. B. Experiment Setups In this work, we use multiple experimental setups to validate the effectiveness of our equation across variety of setups. We summarize all experimental setups in Table 1. The majority of our experiments use Setting A. The experiments with different replay ratios, batch size, or sequence length are conducted by directly modifying corresponding setups. C. Fitting Details Given the LRS of PT and CPT, we can compute out Spt 2 , Scpt in advance. We adopt similar fitting method as Chinchilla scaling law (Hoffmann et al., 2022). We minimize the Huber loss (Huber, 1964) between the predicted and the observed log loss using the L-BFGS algorithm (Nocedal, 1980). We implement this by the utilization of minimize in scipy library. We mitigate the potential issue of local minima of fitting by choosing the optimal fit from range of initial conditions. 1 , and Scpt 1 , Spt 2 D. Additional Continual Pre-Training Results Prediction of Other LRS. We use the fitted parameters in the Fig. 3 to predict the loss of other LRS (Fig. 10a and Fig. 10d). Our equation could effectively predict the loss of other LRS as shown in Fig. 10. Other Dcpt Dataset. Besides Knowledge-Pile (Fei et al., 2024), we also use Eq. 4 to fit transfer loss curves of other Dcpt dataset Pile-of-Law (Henderson* et al., 2022) in the Fig. 11. Different Replay Ratio. We use Eq. 4 to fit all loss curve of different Dpt replay ratio independently in the Fig. 12. 13 Learning Dynamics in Continual Pre-Training for Large Language Models Table 1. Experimental settings adopted in this work. Model size denotes the number of nonembedding parameters. We use AdamW Optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2017). Most experiments adopt LLaMA-3s tokenizer (Dubey et al., 2024). Setups Setting (main) Setting Setting Model Size PT dataset CPT dataset Peak LR PT Batch Size (Tokens) CPT Batch Size (Tokens) PT Sequence Length CPT Sequence Length Tokenizer β1, β2 in AdamW Weight Decay Gradient Clip Setups Model Size PT dataset CPT dataset Peak LR PT Batch Size (Tokens) CPT Batch Size (Tokens) PT Sequence Length CPT Sequence Length Tokenizer β1, β2 in AdamW Weight Decay Gradient Clip 106M FineWeb Knowledge-Pile 2 104 4M 4M 4096 4096 LLaMA-3s 0.9, 0.95 0.1 1.0 LLaMA3.2-1B 1B Unknown Pile-of-Law 2 105 Unknown 4M Unknown 4096 LLaMA-3s 0.9, 0.95 0.1 1.0 Setting 1720M FineWeb 106M FineWeb 594M FineWeb Pile-of-Law Knowledge-Pile Knowledge-Pile 2 104 4M 4M 4096 4096 LLaMA-3s 0.9, 0.95 0.1 1. 2 104 4M 4M 4096 4096 LLaMA-3s 0.9, 0.95 0.1 1.0 2 104 4M 4M 4096 4096 LLaMA-3s 0.9, 0.95 0.1 1.0 14 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Learning Rate Schedule. (b) Predict Dpt (FineWeb) Loss. (c) Predict Dcpt (Knowledge Pile) Loss. (d) Learning Rate Schedule. (e) Predict Dpt (FineWeb) Loss. (f) Predict Dcpt (Knowledge Pile) Loss. Figure 10. Using the fitted parameters in the Fig. 3 to predict all pre-training and CPT loss curve of other LRS. (a) is one kind of without re-warmup method and (b) is more realistic LRS that the learning rate re-warmup to 10% peak PT learning rate and then annealing to zero with cosine method. (a) Learning Rate Schedule. (b) Dpt (FineWeb) Validation Loss. (c) Dcpt (Pile-of-Law) Validation Loss. (d) Learning Rate Schedule. (e) Dpt (FineWeb) Validation Loss. (f) Dcpt (Pile-of-Law) Validation Loss. Figure 11. Using Eq. 4 to fit all loss curves which are pre-trained with FineWeb and continual pre-trained with law. 15 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Learning Rate Schedule. (b) Dpt (FineWeb) Loss of 33% Replay. (c) Dcpt (KP) Loss of 33% Replay. (d) Learning Rate Schedule. (e) Dpt (FineWeb) Loss of 50% Replay. (f) Dcpt (KP) Loss of 50% Replay. (g) Learning Rate Schedule. (h) Dpt (FineWeb) Loss of 67% Replay. (i) Dcpt (KP) Loss of 67% Replay. Figure 12. Using Eq. 4 to fit different Dpt replay ratio models independently. Figure 13. The CPT turning lengths of different coefficients for balancing general and downstream domain performance. 16 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Dpt Distribution Shift of different model size . (b) Dcpt Distribution Shift of different model size . Figure 14. The distribution shift across different model size with constant LRS. (a) Learning Rate Schedule. (b) FineWeb (Dpt) Validation Loss. (c) Knowledge Pile (Dcpt) Validation Loss. Figure 15. Using Eq. 7 to fit the loss curve of all model size in both Dpt and Dcpt validation set. E. Extension To Model Size Scaling Distribution Shift Term of Different Model Sizes We initially explore the effect of model size on the distribution shift term. CPT experiments are conducted across various model sizes106M, 594M, and 1.7B without embeddingusing constant LR. As shown in Fig. 14a, the shift term for different model sizes nearly coincides. Based on these observations, we can hypothesize that the distribution shift term is independent of both model size and transfer starting points. This implies that data transfer results in consistent loss difference across different models sizes. Model Size Scaling Meanwhile, scaling law with LR annealing (Tissue et al., 2024) has found that the learning rate annealing scales with model sizes , that S2 γ. Building on the experiments and analysis above, we extend our proposed Eq. 4 to incorporate model size scaling: L(Spt, Scpt) =L0 + (Spt 1 + Scpt 1 )α Spt 1 )β) 2 γ2 + γ3 + (1 (1 + Scpt Scpt 2 γ (7) where F, γ1, γ2, γ3 is the constant parameters. The γ3 is the model size term in traditional Chinchilla scaling law (Hoffmann et al., 2022). We use Eq. 7 to fit the transfer curve of all model size as shown in Fig. 15. Furthermore, we apply Eq. 4 to fit the CPT loss curve of larger model sizes independently, as illustrated in Fig. 16. This demonstrates the adaptability of our equation across various model sizes. However, it must be emphasized that the model size in our experiments has not yet reached the scale of mainstream LLMs today, so our experimental conclusions regarding model size are based on the assumption of existing results. If the assumption is true for larger model size (e.g., 7B, 70B), we could get conclusion that influenced by the same absolute 17 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Learning Rate Schedule. (b) Dpt (FineWeb) Loss of 594M. (c) Dcpt (Knowledge Pile) Loss of 594M. (d) Learning Rate Schedule. (e) Dpt (FineWeb) Loss of 1720M. (f) Dcpt (Knowledge Pile) Loss of 1720M. Figure 16. Using Eq. 4 to fit all PT and CPT loss curve of 594M and 1720M model size respectively. distribution shift value, the larger models are more vulnerable in general domain and better adaptability to downstream domain. F. Batch Size and Sequence Length In the above experiments, we keep the same batch size in both PT and CPT. However, in the real situation, when computation and dataset is limited, one may keep smaller global batch size than PT in CPT process. Besides, in othe cases, CPT aims to increase the context of LLM, so the sequence length and RoPE base will also increase. We conduct the CPT experiments with larger and smaller batch size, as shown in Fig. 17. When the sequence length is 8K, we increase RoPE base to 500000 from 10000. Distribution Shift of Different Batch Size We leverage the constant LR to focus on that the distribution shift term of different batch size whether satisfy the same form. We using Eq. 4 to fit the loss curve of different batch size. As shown in Fig. 17a and Fig. 17b, all loss curve with different transfer steps in both larger and smaller batch size could be fitted into one distribution shift term, which demonstrates that Eq. 4 could also be adopted to the change of batch size and sequence length. G. Open-Source Pre-Training Models The more realistic scenario posits that the PT model is an open-source model, and we do not know the exact PT process. Therefore, the distribution of PT dataset, the loss potential, and the PT training amount usually remain unknown. Unknown PT Training Amount and Loss Potential For the open-source models, we do not know the PT training amount and loss potential to get the PT forward area Spt 2 . For forward area Spt 1 , we consider it as parameter to be fitted. Typically, most open source models will anneal the LR to zero or minium LR to get better benchmark performance. We assume that the final LR of all open-source models is zero, which facilitates the computation of the CPT annealing area Scpt 2 . The learning rate of CPT of open-source models is consider to re-warmup from zero to the specific peak learning rate and then anneal with specific LRS. 1 and the final LR to calculate the CPT annealing area Scpt 18 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Dpt (FineWeb) loss curve of different batch size. (b) Dcpt (Knowledge Pile) loss curve of different batch size. Figure 17. Using Eq. 4 to fitted loss curve of different batch size in the continual pre-traininig. The smaller batch size is 1M tokens with 4K sequence length and the larger batch size is 8M tokens with 8K sequence length. We annotate the different distribution shift terms in the figure. Unknown PT Dataset Distribution The aforementioned equation holds only in the loss curve of Dpt and Dcpt validation dataset. However, we do not know the exact Dpt dataset distribution of open-source models. In this case, we could select an open-source common crawl validation set as proxy Dpt. To verify the two hypotheses mentioned above are reasonable, We conduct the experiments that continual pre-train the LLaMA3.2-1B (Dubey et al., 2024) with Pile-of-Law dataset (Henderson* et al., 2022). We consider the C4 part in the RedPajama (Computer, 2023) dataset as an proxy Dpt. We leverage fewer training steps to fit the parameters and then predict the loss of longer steps for the proxy Dpt and true Dcpt. As shown in Fig. 18, Eq. 4 could predict the further loss of proxy Dpt and Dcpt effectively. Based on the proxy Dpt and true Dcpt, we could describe the performance dynamics and complete the above hyper-parameters optimization for open-source models. We also conduct experiments with our model pre-trained with FineWeb and continual pre-trained with Pile-of-Law dataset. We view it as model with unknown PT information. We still leverage the C4 in RedPajama as the proxy Dpt dataset to predict the loss of longer training steps as shown in Fig. 18. H. Adding Replay Ratio to Our Formulation D-CPT law (Que et al., 2024) propose scaling law integrating with Dpt and Dcpt data mixture ratio. We have also integrated this data mixture ratio into our formulation. The Appendix demonstrates that the loss curves for different data ratios can be individually fitted using distinct equations. However, we are currently exploring unified formulation that incorporates the data mixture ratio to represent all loss curves. Both the distribution shift term and the LR annealing term are influenced by the replay ratio. higher Dpt ratio leads to weaker distribution shift, and results in smaller LR annealing term in the Dcpt validation loss, while increasing the LR annealing term in the Dpt validation loss. We find that the exponential form, which is consistent with the Data Mixing Law (Ye et al., 2024), best fits these effects and subsequently incorporate it into both the distribution shift term and LR annealing term: Lpt = L0 + (cid:0)Spt Lcpt = L0 + (cid:0)Spt (cid:1)α (cid:1)α 1 + Scpt Spt 2 Scpt 2 ea1rpt + 1 + Scpt 1 Spt 2 Scpt 2 ea1rcpt + (cid:16) 1 (cid:0)1 + Scpt (cid:16) 1 (cid:0)1 + Scpt 1 (cid:1)β(cid:17) (1 ea2rcpt) (cid:1)β(cid:17) (ea2rcpt 1) (8) where rpt and rcpt are the data mixture ratio of PT and CPT data respectively, such that rpt + rcpt = 1, and a1 and a2 are the additional parameters. To ensure that the distribution shift term is zero when rcpt equals zero, we have modified the exponential formulation in the distribution shift term accordingly. The effectiveness of this equation is illustrated in Fig. 19. While the D-CPT law predicts only the final loss across different replay ratios, our method is capable of describing the entire training dynamics for various replay ratios. I. Other Formats with LR Annealing Similar with scaling law with LR annealing (Tissue et al., 2024), we also try the other possible forms of LR annealing. 19 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Learning rate schedule of fitted and predict of Model I. (b) Fitted RedPajama-C4 dataset loss curve of Model I. (c) Predicted RedPajama-C4 dataset loss curve of Model I. (d) Learning rate schedule of fitted and predict of Model II. (e) Fitted RedPajama-C4 dataset loss curve of Model II. (f) Predicted RedPajama-C4 dataset loss curve of Model II. (g) Learning rate schedule of fitted and predict of Model I. (h) Fitted Pile-of-Law dataset loss curve of Model I. (i) Predicted Pile-of-Law dataset loss curve of Model I. (j) Learning rate schedule of fitted and predict of Model II. (k) Fitted Pile-of-Law dataset loss curve of Model II. (l) Predicted Pile-of-Law dataset loss curve of Model II. Figure 18. Using Eq. 4 to fit and predict the proxy Dpt and true Dcpt dataset of open-source PT models. The Model refers to LLaMA3.21B. Model II refers to our model pre-trained with FineWeb but we regard it as an unknown model and use proxy Dpt rather than FineWeb. The Dcpt dataset are both Pile-of-Law, and the proxy Dpt is RedPajama-C4. 20 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Dpt loss curve of different replay ratio. (b) Dcpt loss curve of different replay ratio. Figure 19. Using Eq. 8 to fitted all loss curves of different replay ratio in the continual pre-training. The fitted equation is Lpt = 3.067 + 0.480 (cid:0)Spt 1 + Scpt (1 e3.238rcpt ) and Lcpt = 2.992+0.456(cid:0)Spt (cid:16) 1 (cid:0)1 + 99.35 Scpt (cid:16) 0.280 Spt (cid:1)0.510 1 (cid:0)1 + 100.34 Scpt 2 e0.055rpt + 0.276 2 0.263 Scpt (e5.696rcpt 1). 0.285Spt 2 e0.037rpt 0.526 2 0.279Scpt 1 + Scpt (cid:1)0.510 (cid:1)β(cid:17) (cid:1)β(cid:17) 1 1 1 1 Adding LR-weighted Coefficient To solve that when LR anneals to nearly 0, S2 still has historical momentum, making the loss continue to decrease. revision is that adding LR-weighted coefficient to S2: S2 = (cid:88) i=1 mi ηϵ (9) We test the coefficient ϵ is 0.1 and 0.2, showing the fitted result in the Fig. 20. S2 Power Formats Considering that the annealing loss and S2 have positive correlation, Sζ reasonable format than S2. We revise our formulation: 2 might be more = L0 + (cid:0)Spt (cid:16) 1 + Scpt 1 (cid:0)1 + Scpt + 1 (cid:1)α (cid:1)β(cid:17) 1 (Spt 2 )ζ1 (Scpt 2 )ζ2 (10) We add two other fitted parameters in the function for different annealing area of PT and CPT. We also show the fitted effect in the Fig. 20. We show the huber loss and R2 of all possible formats in the Table 2. All the fitting effect are really good, but the original format has the fewest parameters which is more effective. J. Out-of-Domain Validation Set Data mixing laws (Ye et al., 2024) shows that the some domain validation loss could be represented by the combination of other domains. In scenarios where the CPT dataset, such as Knowledge-Pile, is not highly domain-specific, employing linear combination of Dpt and Dcpt can serve as reasonable approximation for some downstream validation sets. However, it is important to note that this approach may not be universally applicable across all CPT datasets and all Dood validation loss scenarios. We tested the validity of using linear combination of Dpt (FineWeb) and Dcpt (Knowledge-Pile) to estimate the validation loss for certain out-of-domain sets in the Fig. 21 and Fig. 22. These out-of-domain validation sets include StackExchange, arXiv, and C4 in RedPajama (Computer, 2023), as well as PhilPapers and Books in Pile (Gao et al., 2020), SlimPajama (Soboleva et al., 2023), and Open-Web-Math (Paster et al., 2023). 21 Learning Dynamics in Continual Pre-Training for Large Language Models (a) Learning Rate Schedule used in fitting adding LR-weighted coefficient ϵ = 0.1. (b) Dpt Validation Loss for adding LRweighted coefficient ϵ = 0.1. (c) Dcpt Validation Loss for adding LRweighted cofficient ϵ = 0.1. (d) Learning Rate Schedule used in fitting adding LR-weighted coefficient ϵ = 0.2. (e) Dpt Validation Loss for adding LRweighted cofficient ϵ = 0.2. (f) Dcpt Validation Loss for adding LRweighted cofficient ϵ = 0.2. (g) Learning Rate Schedule used in fitting adding adding S2 power format. (h) Dpt Validation Loss for adding S2 power format. (i) Dcpt Validation Loss for adding S2 power format. Figure 20. Using other possible S2 formats Eq. 4 to fit all PT and CPT loss curve with different LRS. Table 2. The fitting effect of different possible equation formats. R2 0.9944 0.9950 0.9950 0.9952 R2 0.9993 0.9984 0.9983 0.9984 Dpt Original Adding LR Cofficient (ϵ = 0.1) Adding LR Cofficient (ϵ = 0.2) Adding S2 Power Dcpt Original Adding LR Cofficient (ϵ = 0.1) Adding LR Cofficient (ϵ = 0.2) Adding S2 Power Huber Loss 0.0016 0.0016 0.0017 0.0016 Huber Loss 0.0021 0.0025 0.0024 0.0025 Learning Dynamics in Continual Pre-Training for Large Language Models Figure 21. The linear combination of Dpt and Dcpt to represent the out-of-doamin Dood validation loss. Figure 22. The absolute errors of linear combination of Dpt and Dcpt to represent the out-of-doamin Dood validation loss."
        }
    ],
    "affiliations": [
        "Ritzz-AI",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China"
    ]
}