{
    "paper_title": "MedDINOv3: How to adapt vision foundation models for medical image segmentation?",
    "authors": [
        "Yuheng Li",
        "Yizhou Wu",
        "Yuxiang Lai",
        "Mingzhe Hu",
        "Xiaofeng Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce \\textbf{MedDINOv3}, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on \\textbf{CT-3M}, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 9 7 3 2 0 . 9 0 5 2 : r MEDDINOV3: HOW TO ADAPT VISION FOUNDATION MODELS FOR MEDICAL IMAGE SEGMENTATION? Yuheng Li1, Yizhou Wu2, Yuxiang Lai3, Mingzhe Hu3, Xiaofeng Yang1,3,4, 1Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta 2Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta 3Department of Computer Science, Emory University, Atlanta 4Department of Radiation Oncology, Emory University School of Medicine, Atlanta Email: xiaofeng.yang@emory.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design simple and effective architecture with multiscale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, curated collection of 3.87M axial CT slices, using multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3. Keywords Self-supervised learning Foundation model Medical image segmentation"
        },
        {
            "title": "Introduction",
            "content": "Medical imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI) are central to modern radiology, enabling detailed visualization of anatomical structures and abnormalities [1]. Accurate segmentation of organs-at-risk (OARs) and tumors is crucial for treatment planning and disease monitoring [2, 3], yet manual annotation is labor-intensive and time-consuming [4]. Deep learning has shown great promise in automating this process; however, most existing approaches rely on highly specialized architectures trained for individual datasets or organ systems [5, 6], limiting their generalization across modalities and institutions [7]. Foundation models (FMs) offer promising solution as unified visual backbones, pretrained on large-scale unlabeled data and adaptable across diverse downstream tasks [8, 9, 10, 11]. Self-supervised learning enables training directly from raw pixels without manual annotations, producing transferable representations [12, 13, 14]. Recent advances such as DINOv2 [15] and DINOv3 [16] have demonstrated remarkable success in natural images, producing strong global and local features for classification, detection, and segmentation tasks. However, due to privacy concerns, it is infeasible to obtain billion-scale data for training medical vision foundation models from scratch. This raises natural question: can representations learned from web-scale natural images be effectively transferred to radiological imaging? Our empirical results suggest that DINOv3 provides promising performance for medical image segmentation. Nevertheless, we identify two key challenges in adapting vision foundation models to CT and MRI segmentation: 1). current FM backbones are based on Vision Transformers (ViTs), which still lag behind strong CNN baselines in dense prediction tasks [17]; 2). substantial domain gap between natural and medical images prevents direct transfer of pretrained representations [18]. Figure 1: MedDINOv3 PCA maps at progressively higher resolution. We visualize dense features of MedDINOv3 by mapping the first three components of PCA computed over the feature space to RGB. We mask the feature maps to focus on the CT foreground. In this work, we propose MedDINOv3, an effective framework to adapt vision foundation model for medical image segmentation. First, we revisit plain ViTs for 2D medical image segmentation. While ViT has proven to be scalable for large-scale pretraining [19], it still requires tailored components such as ViT-Adapter [20] or Mask2Former [21] to achieve good segmentation performances. In medical imaging, existing transformer-based designs often fall back on heavy convolutional components and still underperform strong CNN baselines [6, 22, 23, 24, 25, 26]. Inspired by recent works that deconstruct transformer architectures for segmentation [27, 28], we propose simple and effective transformer architecture for 2D medical image segmentation. MedDINOv3 leverages the DINO ViT as strong vision encoder and introduces multi-scale feature aggregation by reusing patch tokens from intermediate transformer blocks. This hierarchical representation provides richer spatial contexts to the decoder, mitigating the weak locality bias of ViTs. Second, we perform domain-adaptive pretraining of MedDINOv3 on CT-3M, large-scale collection of CT images, to better align the model with radiological data distributions. We found that gram anchoring, mechanism to prevent local feature from collapsing, is optional to our pretraining framework. We systematically examine the three-stage pretraining recipe of DINOv3 and quantify the contribution of each stage to segmentation performance. After adapting to CT domain, our pretrained MedDINOv3 produces smooth feature maps at consistently higher resolutions 1). We summarize our main contributions as follows: simple ViT architecture for 2D medical segmentation. We revisit plain Vision Transformers and propose an effective design for 2D medical image segmentation. Two key refinements(i) multi-scale token aggregation from intermediate patch tokens and (ii) high-resolution trainingraise ViT-B performance on AMOS22 from 78.39% to 85.51% DSC. Domain-adaptive pretraining on CT-3M. We curate CT-3M, large-scale collection of axial CT slices from 16 datasets, and adapt DINOv3 via three-stage process: (1) global/local self-distillation (DINOv2-style), (2) gram anchoring to stabilize patch-level consistency, and (3) high-resolution adaptation. We systematically examine each stage and quantify its impact on downstream segmentation. State-of-the-art results across four public CT/MRI benchmarks. On four diverse benchmarks (AMOS22, BTCV, KiTS23, LiTS), MedDINOv3 outperforms or matches strong CNN and transformer baselines. It surpasses nnU-Net on OAR segmentation (+2.57% DSC on AMOS22 and +5.49% DSC on BTCV), while achieving comparable tumor segmentation on KiTS23 (70.68% DSC) and LiTS (75.28% DSC). These results highlight the effectiveness of domain-adaptive pretraining for transferring vision foundation models to radiology."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Medical vision foundation models Self-supervised learning has emerged as key strategy for developing medical vision foundation models, motivated by the scarcity of annotated medical data. Models Genesis [29] demonstrated the benefits of pretext reconstruction tasks on CT and MRI, while more recent frameworks such as SwinUNETR with SSL pretraining [4] showed substantial improvements on 3D CT benchmarks. Masked image modeling (MIM) has also been applied in the medical domain, with studies showing that MIM-pretrained encoders significantly improve performance on organ segmentation datasets [3, 30, 31]. Parallel work has explored adapting natural-image FMs such as DINOv2 to radiology: Baharoon et al. [18] demonstrated that natural-image SSL features transfer well to classification, though performance in segmentation lags behind domain-pretrained models. Our work bridges this performance gap by performing domain-adaptive SSL pretraining at scale. 2.2 Vision transformers in medical image segmentation Vision Transformers have been widely explored in medical image segmentation. TransUNet [25] incorporated transformer layers into the bottleneck of U-Net. LeViT-UNet [32] followed similar design with efficient attention. UTNet [33] employed Transformer blocks at multiple resolutions, and CoTr [23] leveraged single Transformer to jointly model features across resolutions. Among the most influential works, UNETR [22] directly employed ViT encoder, representing shift toward transformer-heavy designs. Following this, SwinUNETR [6] integrated swin transformers. Most recent works such as Primus [27] deconstruct the complex decoders and show that an encoder-only architecture can approach CNN performance. Despite this progress, recent benchmark studies emphasize that CNNs like nnU-Net remain strong baselines [17]. Our work aims to fully realize the potential of ViTs in medical segmentation by refining the architecture design and performing domain-adaptive SSL pretraining. Figure 2: Overall framework of MedDINOv3. a). Stage 1: Given an input CT, we feed the global crops to the teacher model, local and masked crops to the student. Self-distillation loss is applied to the CLS tokens and masking loss applied to dense patch tokens. b). Stage 2: Adds gram anchoring. Gram teacher sees higher resolution global crop and outputs dense feature maps, resized to match student resolution. Stage 3: Both student and teacher are trained with higher-res CT inputs (not shown). c). Finetuning pretrained MedDINOv3 for segmentation with proposed architecture."
        },
        {
            "title": "3 Method",
            "content": "While vision foundation models pretrained on large datasets have demonstrated remarkable performance in natural images, directly adapting them for medical imaging remains non-trivial, due to considerable gaps in representations and different architectural designs. First, we propose iterative refinements on vision transformer for medical image 3 Table 1: Ablation study on refinements for adapting ViT backbone for segmentation on AMOS22. Encoder init. Decoder Multi-scale features Resolution DSC (%) randinit. DINOv3 DINOv3 DINOv"
        },
        {
            "title": "Primus\nPrimus\nPrimus\nPrimus",
            "content": "640 640 640 640 640 640 896 896 78.39 81.35 83.45 85.51 segmentation, enabling integration of pretrained SSL models. Next, we perform domain-adaptive pretraining on CT images using DINOv3, state-of-the-art SSL method known for learning superior dense features. We then transfer the learned representations to various medical segmentation tasks. Our framework is shown in Figure 2. 3.1 Rethinking transformers for medical image segmentation Despite promising scalability in natural images, vision transformers have not yet achieved consistent gains for medical image segmentation. Prior studies show that transformer blocks in segmentation models contribute very little to final performance [27] and still underperform strong CNN baselines [5]. To enforce attention usage, Primus proposes simple transformer architecture that uses lightweight transposed convolution decoder, achieving satisfactory performances in 3D volumetric segmentation. However, Primus still lacks the spatial priors essential for dense segmentation. Furthermore, Primus encoder modifies its patch size from 16 to 8, which remains compute heavy and does not suit DINOv3-style pretraining. Motivated by these limitations, we revisit plain ViTs for 2D medical image segmentation and propose stepwise refinement to adapt DINOv3 backbone into an effective segmentation backbone  (Table 1)  . Development datasets We select AMOS22 to train and evaluate each refinement step. AMOS22 [2] is an abdominal organ segmentation dataset of 300 CT volumes and 60 MRI volumes with 15 annotated organs. We follow similar strategy as Primus [27] by training and evaluating with only one fold (80/20 split) of the default five-fold cross-validation scheme. We train for total of 1000 epochs following default nn-UNet settings, with hyperparameters adapted from Primus. Baseline We form our baseline using DINOv3 ViT encoder (ViT-B) and Primus decoder composed of back-to-back transposed convolution, LayerNorm, and GELU activation, to upsample patch tokens into the full-resolution segmentation map. This design minimizes convolutional influence and maximizes the impact of transformers representations. Starting from random initialization, this configuration provides reasonable baseline, but lags behind supervised CNNs. Leveraging pretrained DINOv3 To improve representation quality, we initialize the ViT encoder with DINOv3 pretrained on LVD-1689M. We observe considerable boost in DSC by 2.96% over random initialization, highlighting the transferability of web-scale SSL features to medical segmentation. Multi-scale token aggregation Observing that Primus only uses the last transformer block as input to the decoder, we hypothesize that this lack of hierarchical priors prevents ViTs from learning strong local features. We propose to reuse patch tokens from multiple intermediate layers (blocks 2, 5, 8, 11) and concatenate them as input to the decoder. This step enriches the spatial priors that are otherwise weak in ViTs. As shown in Table 1, incorporating multi-scale features considerably improved DSC by 2.10% in AMOS22. Higher resolution training To preserve local information, Primus propose to decrease the patch size during tokenization from 16 to 8, and found this beneficial for 3D volumetric segmentation. However, existing vision FMs are rarely pretrained using patch size of 8, possibly due to increased computational overheads. As an alternative, we propose to conduct high resolution segmentation training by resampling axial slices to thinner spacing. Following DINOv3 [16], we maintain an input resolution of 896 896. As shown in Table 1, increasing resolution from 640 640 to 896 896 improved DSC by 2.06% on AMOS22. 3.2 Domain-adaptive pretraining on CT-3M With an effective architecture, we aim to pretrain MedDINOv3 using diverse, large-scale medical imaging dataset CT-3M, to better align its representations to medical imaging. We follow the 3-stage pretraining recipe developed by DINOv3 [16]. 4 Data curation We curated large-scale CT dataset CT-3M totaling 3,868,833 axial slices, aggregated from 16 publicly available datasets. Specifically, our datasets include: BTCV [34], Pancreas-CT (TCIA) [35], CHAOS [36], LiTS [37], KiTS [38], WORD [39], AbdomenCT-1K [40], AMOS22 [2], and five CT tasks from Medical Segmentation Decathlon (Liver, Lung, Pancreas, Hepatic Vessel, Spleen, Colon) [41], CT-ORG [42], TotalSegmentator [43] and AbdomenAtlas 3.0 [44]. This data curation provides broad anatomical coverage (over 100 structures) across abdominal, thoracic, and pelvic regions, ensuring both scale and heterogeneity for domain-adaptive pretraining. All 3D volumes were resampled to an in-plane spacing of 0.45 mm and 0.45 mm, and then resized to uniform resolution 256 256. Stage 1 We pretrain using the original DINOv2 losses: an image-level objective LDINO enforcing global-local crop invariance, patch-level latent reconstruction objective LiBOT which learns the local patch correspondence, and regularization loss LKoleo encouraging the features within batch to spread uniformly in the latent space. The stage 1 loss is defined as follows: LStage1 = LDINO + LiBOT + 0.1 LKoleo. (1) Stage 2 However, DINOv2 training showed that global losses tend to dominate as training progresses, leading to slow erosion of patch-level quality [16]. To address this, stage 2 introduces gram anchoring to mitigate the degradation of patch-level consistency. The motivation is that global and local objectives are only weakly correlated, while optimizing for global consistency often harms local feature quality. Gram anchoring regularizes Gram matrix, the matrix of all pairwise dot products of patch features in an image. Specifically, we encourage the Gram matrix of the student to align with that of an earlier model, referred to as the Gram teacher. The Gram teacher is chosen from an early checkpoint of the EMA student network, which retains stronger dense features. Formally, given an image with patches, and network that operates in dimension d. Let XS and XG denote the matrix of L2-normalized local features of the student and the Gram teacher respectively. We define the loss LGram as follows: LGram = (cid:13) (cid:13)XS XG (cid:13) 2 . (cid:13) (2) This loss is only computed on the global crops across all patch tokens. In our implementation, we start this stage after 100k iterations of pretraining, for total of 20k iterations. We also leverage higher-resolution CT images as the input to the Gram teacher. Specifically, we feed images at 512 512 into the Gram teacher, then downsample the resulting feature maps by factor of 2 to match the spatial dimensions of the student output. The stage 2 loss is defined as: LStage2 = LDINO + LiBOT + LKoleo + 2 LGram. (3) Stage 3 The final stage adapts the pretrained model to process higher-resolution images, which is particularly relevant to our tasks. We follow DINOv3 by mixing global and local crops of various resolutions (e.g., global crops 512768, local crops 112336). Importantly, we retain gram anchoring to ensure that patch similarity structures remain stable. This stage lasts for 10k iterations. We found that high-resolution adaptation substantially improves the models dense feature  (Fig. 3)  . Implementation details We pretrain MedDINOv3 for total of 120k iterations, using global batch size of 512. We initialize with the DINOv3 ViT-B checkpoint pretrained on LVD-1689M. For stage 1, we use learning rate of 2e-4 and train for 100k iterations. For stage 2, we select EMA model at 20k iteration as our Gram teacher, and continue pretraining with gram anchoring objective for 10k iterations. This stage uses learning rate of 5e-5. For step 3, we initialize teacher and student with the previous model from stage 2, and train for another 10k iterations using learning rate of 2.5e-5."
        },
        {
            "title": "4 Results",
            "content": "4.1 Experiment settings Evaluation Dataset To comprehensively evaluate existing 2D segmentation methods, we conducted extensive experiments on four publicly available datasets. These datasets cover wide spectrum of tasks in medical image segmentation (i.e. OARs and tumor), spanning diverse imaging modalities (e.g., CT and MRI). In addition to AMOS22, we added the following datasets: 1). KiTS23 [38], kidney tumor dataset with 489 CT volumes with annotations provided for kidney, tumor and cysts; 2). LiTS [37], liver tumor segmentation dataset with 131 CT volumes with annotated liver and tumor classes; 3). BTCV [34], contrast-enhanced abdominal CT dataset with 50 scans with manual segmentation 5 Figure 3: High-resolution dense features of MedDINOv3. We visualize the cosine similarity maps between the patches marked with red dot and all other patches. Input image at 2048 2048. of 13 organs. Due to high computational costs, we trained and evaluated with only one fold (80/20 split) of the default five-fold cross-validation scheme. For evaluation metrics, we use the standard dice similarity coefficient (DSC) and normalized surface dice (NSD). Implementation details We performed training and evaluation within the nnU-Net framework in PyTorch. We summarize the preprocessing details used for each dataset in Table. All models were trained for 1,000 epochs, each consisting of 250 steps. Unless stated otherwise, input patch size, batch size, and voxel spacing follow the specific configurations defined by the respective nnU-Net plans. We report the following methods for comparison and detail the training configurations: 1. nnU-Net [5]: Strongest supervised CNN baseline. Following the default nnU-Net v2 configuration, we use learning rate of 1 102, weight decay of 3 105, gradient clipping set to 12, and the SGD optimizer with Nesterov momentum (0.99), along with the default nnU-Net PolyLR scheduler. 2. SegFormer [45]: hierarchical transformer model with lightweight MLP decoder architecture to directly fuse multi-level features. We adjust the learning rate to 5 105 and use the AdamW optimizer. 3. DINO U-Net [46]: newly developed U-Net architecture supporting DINOv3 integration. We use the standard nnU-Net hyperparameters, but with the encoder backbone frozen, as described in the original paper. 4. MedDINOv3: Our proposed method. We adapt the hyperparameters from Primus and use higher input resolution of 896 896. The Primus model is trained using learning rate of 3 104 and weight decay of 5 102. We apply DropPath rate of 0.2 and use LayerScale with value of 1 105. The optimizer is AdamW, configured with betas set to (0.9, 0.98). 6 4.2 Comparisons with state-of-the-art methods We compare MedDINOv3 against CNN and transformer baselines on four public segmentation benchmarks  (Table 2)  . Our MedDINOv3 consistently outperforms the best baseline nnU-Net in AMOS22 by 2.6% DSC and BTCV by 5.49% DSC. On tumor segmentation datasets, MedDINOv3 reaches 70.68 DSC on KiTS23 and 75.28 DSC on LiTS, performing on par with nnU-Net. In terms of boundary accuracy, MedDINOv3 maintains strong NSD scores for organ-at-risk segmentation, while only slightly trailing nnU-Net on tumor datasets. DINO U-Net, despite leveraging the DINOv3 foundation model, did not outperform nn-UNet, possibly due to its reliance on hierarchical CNN decoders. SegFormer, developed for natural image segmentation, underperforms across all datasets, reflecting its weaker inductive bias and reliance on large-scale labeled data. Overall, these results demonstrate that combining architectural improvements and domain-adaptive pretraining produces transferable representations for medical imaging, narrowing the gap with and in several cases surpassing the long-established nnU-Net baseline. Table 2: Performances on four public segmentation benchmarks. We report average DSC and NSD of all datasets. Due to computational constraints, the results are only calculated for one fold of 5-fold cross-validation."
        },
        {
            "title": "Method",
            "content": "DSC (%) NSD (%) AMOS22 KiTS23 LiTS BTCV AMOS22 KiTS"
        },
        {
            "title": "LiTS BTCV",
            "content": "nnU-Net SegFormer Dino U-Net (B) MedDINOv3 (ours) 84.81 78.50 80.90 87.38 69.15 57.73 59.77 70.68 75.00 65.45 72.89 75.28 73.30 37.04 66.88 78.79 73.98 65.20 67.00 77. 64.85 47.65 51.05 62.67 53.02 35.98 48.25 53.01 64.66 22.39 56.23 70.38 Figure 4: Evolution of the cosine similarity between the reference patch (marked in red) and all other patches. We did not observe severe patch degradation in stage 1. 4.3 Ablation study Gram anchoring is optional We study the effect of pretraining on CT-3M on segmentation performances. As shown in Table 3, stage 1 pretraining improves DSC by 1.07%, which highlights the effectiveness of DINOv2 style pretraining for learning good dense features. However, surprisingly we did not observe much gains from stage 2 with gram anchoring. We suspect that this is because the quality of patch tokens did not degrade much during stage 1 pretraining. To confirm this, we visualize them in Figure 4. Nevertheless, adapting the model to higher resolution improved DSC by 0.84% and maintained the consistency of feature maps."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present MedDINOv3, simple yet effective framework for adapting vision foundation models to medical image segmentation. We refine plain Vision Transformers for medical image segmentation by proposing 7 Table 3: Ablation on multi-stage pretraining for MedDINOv3 on AMOS22. Stage 1: Pretraining Stage 2: Gram Anchoring Stage 3: Adapting to higher resolution DSC (%) 85.51 86.58 86.54 87.38 multi-scale token aggregation to enhance the spatial priors. We also maintain the local intricate structures by conducting high-resolution training. Building on this architecture, we curated CT-3M, large-scale CT dataset, and performed domain-adaptive pretraining with DINOv3. Our systematic analysis of the three-stage pretraining recipe revealed that DINOv2-style self-distillation (Stage 1) and high-resolution adaptation (Stage 3) substantially improve feature transferability, while gram anchoring (Stage 2) provides only marginal additional benefits in our setting. Our MedDINOv3 consistently outperforms or matches strong CNN and transformer baselines in organ-at-risk segmentation, while achieving competitive performance on tumor segmentation tasks. Our results indicate that simple ViT-based architectures, when paired with domain-adaptive pretraining, can close the gap or exceed the performance of specialized CNNs. Overall, MedDINOv3 demonstrates that carefully adapting foundation models with targeted architectural refinements and domain-aligned pretraining offers powerful and generalizable solution for medical image segmentation."
        },
        {
            "title": "Acknowledgments",
            "content": "This research is supported in part by the National Institutes of Health under Award Numbers R01EB032680, R01DE033512, and R01CA272991."
        },
        {
            "title": "References",
            "content": "[1] Klaus Maier-Hein, Annika Reinke, et al. Image-based biomedical modeling and simulation: review with clinical perspectives. Medical Image Analysis, 76:102306, 2022. [2] Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhanng, Wanling Ma, Xiang Wan, et al. Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. Advances in neural information processing systems, 35:3672236732, 2022. [3] Yuheng Li, Jacob Wynne, Yizhou Wu, Richard LJ Qiu, Sibo Tian, Tonghe Wang, Pretesh Patel, David Yu, and Xiaofeng Yang. Automatic medical imaging segmentation via self-supervising large-scale convolutional neural networks. Radiotherapy and Oncology, 204:110711, 2025. [4] Yucheng Tang, Dong Yang, et al. Self-supervised pre-training of swinunetr for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 207218, 2022. [5] Fabian Isensee, Paul Jaeger, Simon Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-adapting framework for u-net-based medical image segmentation. Nature Methods, 18(2):203211, 2021. [6] Ali Hatamizadeh, Vishwesh Nath, et al. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), pages 272284. Springer, 2022. [7] Jun Ma, Bo Wang, et al. Segment anything in medical images. Nature Communications, 14(1):2578, 2023. [8] Rishi Bommasani, Drew Hudson, Ehsan Adeli, and et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [9] Xingyu Zhou, Zheng Zhang, et al. comprehensive survey on pretraining-based foundation models: history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023. [10] Yuheng Li, Mingzhe Hu, and Xiaofeng Yang. Polyp-sam: Transfer sam for polyp segmentation. In Medical imaging 2024: computer-aided diagnosis, volume 12927, pages 749754. SPIE, 2024. [11] Shansong Wang, Mojtaba Safari, Mingzhe Hu, Qiang Li, Chih-Wei Chang, Richard LJ Qiu, and Xiaofeng Yang. Dinov3 with test-time training for medical image registration. arXiv preprint arXiv:2508.14809, 2025. [12] Mathilde Caron, Hugo Touvron, Ishan Misra, and et al. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 96309640, 2021. [13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamala Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 87488763, 2021. [14] Michael Tschannen, Xiaohua Zhai, Andreas Steiner, Lucas Beyer, et al. Siglip 2: Better, faster, stronger. arXiv preprint arXiv:2501.01536, 2025. [15] Maxime Oquab, Timothée Darcet, Théo Moutakanni, and et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [16] Oriane Siméoni, Huy Vo, Maximilian Seitzer, and et al. Dinov3: Advancing self-supervised learning at scale. arXiv preprint arXiv:2508.10104, 2025. [17] Fabian Isensee, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Saikat Roy, Klaus Maier-Hein, and Paul Jaeger. nnu-net revisited: call for rigorous validation in 3d medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 488498. Springer, 2024. [18] Mohammed Baharoon, Waseem Qureshi, Jiahong Ouyang, Yanwu Xu, Abdulrhman Aljouie, and Wei Peng. Evaluating general purpose vision foundation models for medical image analysis: An experimental study of dinov2 on radiology benchmarks. arXiv preprint arXiv:2312.02366, 2024. [19] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1210412113, 2022. [20] Haotian Zhang, Xuanyu Dong, Jie Chen, et al. Vit-adapter: Exploring efficient adaptation of vision transformers for dense predictions. Advances in Neural Information Processing Systems (NeurIPS), 2023. [21] Bowen Cheng, Alexander Schwing, and Alexander Kirillov. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12901299, 2022. [22] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574584, 2022. [23] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 171180. Springer, 2021. [24] Yutong Xie, Jianpeng Zhang, Yong Xia, and Qi Wu. Unimiss: Universal medical self-supervised learning via breaking dimensionality barrier. In European Conference on Computer Vision, pages 558575. Springer, 2022. [25] Jieneng Chen, Yongyi Lu, et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021. [26] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201, 2021. [27] Tassilo Wald, Saikat Roy, Fabian Isensee, et al. Primus: Enforcing attention usage for 3d medical image segmentation. arXiv preprint arXiv:2503.01835, 2025. [28] Tommie Kerssies, Niccolò Cavagnero, Alexander Hermans, et al. Your vit is secretly an image segmentation model. arXiv preprint arXiv:2503.19108, 2025. [29] Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael Gotway, and Jianming Liang. Models genesis. Medical image analysis, 67:101840, 2021. [30] Tassilo Wald, Constantin Ulrich, Stanislav Lukyanenko, Andrei Goncharov, Alberto Paderno, Maximilian Miller, Leander Maerkisch, Paul Jaeger, and Klaus Maier-Hein. Revisiting mae pre-training for 3d medical image segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 51865196, 2025. [31] Yuheng Li, Tianyu Luan, Yizhou Wu, Shaoyan Pan, Yenho Chen, and Xiaofeng Yang. Anatomask: Enhancing medical image segmentation with reconstruction-guided self-masking. In European Conference on Computer Vision, pages 146163. Springer, 2024. [32] Guoping Xu, Xuan Zhang, Xinwei He, and Xinglong Wu. Levit-unet: Make faster encoders with transformer for medical image segmentation. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pages 4253. Springer, 2023. 9 [33] Yunhe Gao, Mu Zhou, and Dimitris Metaxas. Utnet: hybrid transformer architecture for medical image segmentation. In International conference on medical image computing and computer-assisted intervention, pages 6171. Springer, 2021. [34] Bennett Landman, Zhoubing Xu, Juan Igelsias, Martin Styner, Thomas Langerak, and Arno Klein. Miccai multi-atlas labeling beyond the cranial vaultworkshop and challenge. In Proc. MICCAI multi-atlas labeling beyond cranial vaultworkshop challenge, volume 5, page 12. Munich, Germany, 2015. [35] Holger Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim Turkbey, and Ronald Summers. Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation. In International conference on medical image computing and computer-assisted intervention, pages 556564. Springer, 2015. [36] Emre Kavur, Sinem Gezer, Mustafa Barıs, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst, Savas Özkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation. Medical image analysis, 69:101950, 2021. [37] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). Medical image analysis, 84:102680, 2023. [38] Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel Shats, et al. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct. arXiv preprint arXiv:2307.01984, 2023. [39] Xiangde Luo, Wenjun Liao, Jianghong Xiao, Jieneng Chen, Tao Song, Xiaofan Zhang, Kang Li, Dimitris Metaxas, Guotai Wang, and Shaoting Zhang. Word: large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image. Medical Image Analysis, 82:102642, 2022. [40] Jun Ma, Yao Zhang, Song Gu, Cheng Zhu, Cheng Ge, Yichi Zhang, Xingle An, Congcong Wang, Qiyuan Wang, Xin Liu, et al. Abdomenct-1k: Is abdominal organ segmentation solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):66956714, 2021. [41] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. [42] Blaine Rister, Darvin Yi, Kaushik Shivakumar, Tomomi Nobashi, and Daniel Rubin. Ct-org, new dataset for multiple organ segmentation in computed tomography. Scientific Data, 7(1):381, 2020. [43] Jakob Wasserthal, Hanns-Christian Breit, Manfred Meyer, Maurice Pradella, Daniel Hinck, Alexander Sauter, Tobias Heye, Daniel Boll, Joshy Cyriac, Shan Yang, et al. Totalsegmentator: robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence, 5(5):e230024, 2023. [44] Pedro RAS Bassi, Mehmet Can Yavuz, Kang Wang, Xiaoxi Chen, Wenxuan Li, Sergio Decherchi, Andrea Cavalli, Yang Yang, Alan Yuille, and Zongwei Zhou. Radgpt: Constructing 3d image-text tumor datasets. arXiv preprint arXiv:2501.04678, 2025. [45] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34:1207712090, 2021. [46] Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, and Xin Gao. Dino u-net: Exploiting high-fidelity dense features from foundation models for medical image segmentation. arXiv preprint arXiv:2508.20909, 2025."
        }
    ],
    "affiliations": [
        "Department of Biomedical Engineering, Georgia Institute of Technology, Atlanta",
        "Department of Computer Science, Emory University, Atlanta",
        "Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta",
        "Department of Radiation Oncology, Emory University School of Medicine, Atlanta"
    ]
}