{
    "paper_title": "GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping",
    "authors": [
        "Jing Wang",
        "Jiajun Liang",
        "Jie Liu",
        "Henglin Liu",
        "Gongye Liu",
        "Jun Zheng",
        "Wanyuan Pang",
        "Ao Ma",
        "Zhenyu Xie",
        "Xintao Wang",
        "Meng Wang",
        "Pengfei Wan",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 9 1 3 2 2 . 0 1 5 2 : r GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping Jing Wang1,2, Jiajun Liang2, Jie Liu3, Henglin Liu2,4, Gongye Liu2,5, Jun Zheng1, Wanyuan Pang6 Ao Ma7, Zhenyu Xie1, Xintao Wang2, Meng Wang2, Pengfei Wan2, Xiaodan Liang1 2Kling Team, Kuaishou Technology 1Shenzhen Campus of Sun Yat-Sen University 5HKUST 3CUHK MMLab 4Tsinghua University Project Page Project Leader. Corresponding Authors. Work Conducted During Internship. wangj977@mail2.sysu.edu.cn, liangjiajun@kuaishou.com 6USTB 7UCAS Figure 1. Comparison between FlowGRPO and GRPO-Guard under over-optimization. Left: The proxy score and gold score trends during training. As the proxy score increases, FlowGRPO rapidly enters an over-optimization phase, where the gold score continuously declines. Right: visual comparison between FlowGRPO and GRPO-Guard. Due to severe reward hacking, FlowGRPO suffers from In contrast, GRPO-Guard drastic degradation in diversity, detail richness, visual quality, and text-image consistency (bottom part). maintains stable gold score and high visual quality under comparable proxy score, as shown in the upper part of the figure."
        },
        {
            "title": "Abstract",
            "content": "Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe systematic shift in the importance-ratio distributionits mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As result, the policy model inevitably enters an implicit over-optimization stagewhile the proxy reward continues to increase, essential metrics such as image quality and textprompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) 1 and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality. These results highlight GRPO-Guard as robust and general solution for stable policy optimization in flow-matching models. 1. Introduction Recent advances in flow-based diffusion models [20, 23, 27] have led to remarkable progress in visual generation. State-of-the-art models such as Stable Diffusion 3.5 (SD3.5) [8], Flux [18], and Qwen-Image [40] achieve outstanding image synthesis quality, while Wan2.1 [37] and Kling [17] extend this success to video generation. Building on the success of Group Relative Policy Optimization (GRPO [32]) in large language models [12, 15], recent works such as Flow-GRPO [21] and DanceGRPO [44] apply GRPO-style reinforcement learning to diffusion models, yielding notable improvements in aesthetic quality [16, 42], instruction following [11], and text rendering [4]. Within GRPO frameworks, the importance-ratio clipping mechanism serves primarily to stabilize training. By bounding policy updates when the new policy diverges excessively from the reference model, clipping suppresses gradient explosions and maintains controlled opthe imtimization across denoising timesteps. portance ratio should remain centered around 1, ensuring that positive and negative updates are symmetrically constrainedeffectively truncating gradients from overconfident samples and preserving balance in policy learning. Ideally, However, our empirical analysis reveals that this stabilization mechanism fails to behave as intended in diffusion models. As shown in Figure 2, the importance-ratio distribution exhibits systematic bias: its mean consistently falls below 1, and its variance varies significantly across timesteps. This left-shifted and uneven distribution prevents positive-advantage samples from entering the clipped region, leaving overconfident positive updates largely unconstrained. As training progresses, the policy model gradually enters an over-optimization [24] regimethe proxy reward continues to rise, while essential metrics such as image fidelity and textprompt alignment degrade sharply, rendering the learned policy impractical for real-world use. Furthermore, the variance inconsistency across timesteps exacerbates this imbalance: at high-noise steps, clipping is rarely activated, whereas at low-noise steps, it occurs excessivelyleading to persistent over-optimization at highnoise steps. Taken together, the mean shift and variance disparity in the importance-ratio distribution amplify gradient imbalance across noise conditions, which we identify as the root cause of implicit reward hacking observed in FlowGRPO (Figure 1). We trace this anomalous behavior to fundamental design mismatch: diffusion models compute Gaussian probabilities, whereas LLMs rely on discrete token probabilities, yet FlowGRPO or DanceGRPO directly inherits the GRPO formulation without proper adaptation. To address this issue, we propose GRPO-Guard, simple yet effective enhancement to existing GRPO frameworks. It introduces ratio normalization (RatioNorm) procedure that standardizes the importance-ratio distribution at each denoising step, ensuring its mean remains close to one and its variance consistent across timesteps. This adjustment restores the clipping mechanisms ability to truncate gradients from overconfident positive samples, mitigating imbalanceinduced over-optimization and stabilizing policy learning. Despite this correction, policy gradients still vary significantly across timesteps. Low-noise steps yield disproportionately large gradients, causing the policy model to overfit specific noise conditions while neglecting early-step exploration and diversity. This imbalance ultimately drives the model toward over-optimization concentrated at single step. To alleviate this issue, we propose gradient balancing strategy that treats gradients from all steps more uniformly, effectively mitigating over-optimization while providing modest performance gains. As illustrated in Figure 2, GRPO-Guard restores healthy ratio distributions and consistent clipping across timesteps, achieving fast convergence comparable to KL-free baselines while substantially reducing over-optimization. It consistently alleviates reward hacking across multiple GRPO variants (e.g., Flow-GRPO, DanceGRPO), diverse diffusion backbones (e.g., SD3.5-M, FLUX1.dev), and various proxy tasks (e.g., text rendering, GenEval, PickScore). This demonstrates the robustness, scalability, and general applicability of our approach to safe policy optimization in diffusion-based generation models. 2. Related Works 2.1. Alignment for Large Language Models Recent years witness shift from supervised fine-tuning to interactive, reinforcement-style alignment when adapting Large Language Models(LLMs) [1] to human intent [31, 35]. Reinforcement Learning from Human Feedback (RLHF) [10] which typically trains reward model from pairwise human comparisons and then optimizes policy using RL algorithms such as PPO [30] becomes standard pipeline for this purpose [2, 5, 26]. However, PPObased RLHF pipelines are often computationally intensive and sensitive to reward-model inaccuracies, which has motivated the development of more stable and efficient alternatives. One such direction is Direct Preference Optimization (DPO) [28], which bypasses explicit reinforcement learning by directly optimizing model parameters on human preference pairs, achieving similar alignment effects with re2 duced complexity. More recently, Group Relative Policy Optimization(GRPO) methods have already been adopted in production-scale LLM alignment flows [12, 15], demonstrating that group-relative updates can yield stable improvements in instruction following and preference alignment. 2.2. RL for Diffusion and Flow Models. Diffusion and flow-matching models [14, 20, 29, 34] decompose the process of visual generation into iterative denoising steps, revolutionizing the field of visual synthesis and achieving remarkable results in both image and video generation. Building on the success of reinforcement learning (RL) algorithms in Large Language Models (LLMs), similar optimization paradigmssuch as PPO [3, 30] and DPO [36]have been effectively transferred to diffusion models, enabling preference alignment and improved task-specific performance. Following this trend, FlowGRPO [21] and DanceGRPO [44] integrate GRPO-style policy updates into flow-matching models, transforming deterministic ODE sampling into stochastic SDE formulations to introduce exploration noise for group-based optimization. More recently, MixGRPO [19] proposes hybrid ODESDE sampling strategy that significantly improves training efficiency while maintaining comparable generation quality. Meanwhile, Flow-CPS [38] identifies critical issue in the SDE sampling process used by Flow-GRPO and DanceGRPOnamely, the inconsistency of noise coefficients across timestepswhich leads to excessive residual noise and inaccurate reward estimation. To address this, Flow-CPS introduces noise-consistent SDE sampling scheme that accelerates GRPO optimization by improving reward accuracy. In parallel, TempFlowGRPO [13] and G2RPO [45] address the reward sparsity and inaccuracy caused by assigning single global reward to multi-step SDE trajectories. Most existing methods focus on improving policy optimization efficiency but overlook critical issueover-optimization, which severely degrades visual quality. In this work, we conduct an in-depth analysis of this problem and propose an effective solution. 2.3. Reward Over-optimization. Reward over-optimization [10, 25], also referred to as reward hacking [24, 33], poses significant challenge in reinforcement learning for diffusion and flow models, arising from the limitations of imperfect proxy reward models [22, 39, 43] (RMs) for human or task-specific preferences. In practice, optimizing learned proxy RM often improves its corresponding proxy metric, but alignment with the true objectivesuch as perceptual quality or humanevaluated preferencetypically holds only for short period, after which further optimization can degrade generation quality, as illustrated in Figure 1. To mitigate this issue, common strategies include regularizing policy updates with heavy KL-divergence penalty [9, 21] toward supervised fine-tuned policy. KL regularization helps mitigate over-optimization by reducing drift from the reference policy, but it can also slow the improvement of both proxy scores and true-performance metrics, potentially leading to degraded overall performance. Clipping importance ratios [30] further constrains updates from overly confident positive and negative samples, preventing harmful updates and stabilizing policy optimization, thereby reducing the risk of entering an overoptimization phase. Additionally, scaling up reward models [10, 41], using ensembles [6, 7], or composing RMs from multiple perspectives can further reduce overfitting to single proxy, although at significant computational cost. Early stopping [3] and monitoring generation quality provide additional safeguards against excessive reward exploitation, but they may also halt training prematurely, potentially leaving the policy under-optimized. However, in flow-matching models, the inherent bias in the importance ratio causes the clipping mechanism to fail to function as intended, allowing overly confident positive updates to pass unchecked and driving the policy into an over-optimization regime. In this work, we analyze this phenomenon in depth and propose methods to mitigate implicit over-optimization, thereby restoring stable and reliable policy updates. 3. Method 3.1. Preliminary assumes that x1 X1 is Gaussian Flow Matching: noise sample and x0 X0 is sample drawn from the real data distribution. The Rectified Flow formulation defines the noisy sample xt as xt = (1 t)x0 + tx1, (1) Transformer-based model vθ is trained to predict the velocity field = x1 x0. The training objective of Flow Matching is to minimize the expected squared error between the predicted and true velocities: L(θ) = Et,x0X0,x1X1 [v vθ(xt, t)2]. (2) Flow-GRPO and DanceGRPO: During the reinforcement learning (RL) stage, Flow-GRPO and DanceGRPO introduce stochasticity into the sampling process by converting the ODE-based deterministic flow used in Flow Matching into stochastic differential equation (SDE) formulation. The SDE sampling process in Flow-GRPO and Dance3 ˆAi = R(xi 0) mean(R(xi std(R(xi i=1)) 0)G 0)G i=1) (4) + GRPO can be expressed as: xt+dt = (cid:20) vθ(xt, t) + xt + σ2 2t (cid:21) (xt + (1 t)vθ(xt, t)) dt +σt (3) dtϵ (cid:124) (cid:123)(cid:122) µθ(xt,t) (cid:125) where ϵ (0, I). In Flow-GRPO, the noise level σt is defined as σt = η 1t . In contrast, DanceGRPO adopts constant noise level σ = η. (cid:113) Subsequently, given the same conditioning input c, group of diverse samples xi i=1 is generated through the 0 SDE sampling process. Each sample is evaluated by the reward model, which assigns scalar score R(xi 0). The group-relative advantage is then computed as: The GRPO algorithm then optimizes the policy model by minimizing the following objective: Jpolicy(θ) = (cid:88) 1 i=1 clip(ri 1 1 (cid:88) (cid:16) t=0 min(cid:0)ri t(θ) ˆAi t, t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:1)(cid:17) , (5) t1xi t1xi pθ(xi pθold (xi t(θ) = where ri t,c) t,c) . For Flow-GRPO, an additional KL penalty DKL(πθπref ) is introduced to mitigate reward hacking, constraining the policy model to stay close to the reference flow model. DanceGRPO enforces that the initial random noise for samples within the same group {xi i=1, remains identical, ensuring that all generated variations originate from the same starting point. 1}G 3.2. Analysis and Solution In this section, we first analyze the importance ratio in Flow-GRPO and DanceGRPO, highlighting the underlying causes of its inherent distributional anomalies and their role in inducing implicit reward hacking. We then introduce the RatioNorm method, which corrects the importanceratio distribution, regulates the clipping mechanism, and mitigates reward hacking. Finally, we propose gradient reweighting strategy to prevent single-step gradients thereby alleviating overfrom dominating optimization, optimization under specific noise conditions. 3.2.1. Inherent Distributional Anomalies Ideally, the importance ratio should stay centered around 1, so that positive and negative updates are symmetrically constrainedtruncating gradients from overconfident samples and maintaining balanced policy learning, as illustrated in Figure 2(a). However, in diffusion models, the importanceratio distribution often exhibits abnormal shifts, causing the clipping mechanism for positive samples to fail. We analyze this issue in detail in the following section. In flow matching, the log-probability log pθ(xt1xt, c) under the policy model θ, is computed using the Gaussian probability formula: log pθ(xt1xt, c) = xt1 µθ(xt, t)2 2σ2 dt Ct, (6) where xt1 = µθold(xt, t) + σt dt ϵ, ϵ (0, I) and Ct is constant. Consequently, we can derive the expression for the log-importance ratio log rt(θ) as follows: log rt(θ) = log pθ(xt1xt, c) log pθold (xt1xt, c) dt ϵ2 µθold (xt, t) µθ(xt, t) + σt = = = 2σ2 dt µθold(xt, t) µθold(xt, t) + σt dt ϵ 2σ2 dt dt ϵ2 µθ + σt 2σ2 dt + ϵ2 µθ2 2σ2 dt µθ ϵ dt σt (7) 2σ2 For simplicity, we denote µθ = µθold (xt, t) µθ(xt, t). Since ϵ (0, I), we illustrate the derivation with one-dimensional Gaussian (without loss of generality). Then we have EϵN (0,I) [log rt(θ)] = µθ2 . This analysis reveals key distinction from LLMs: unlike discrete token probabilities in language models, diffusion models compute Gaussian state transition probabilities. The resulting quadratic term introduces timestepdependent negative bias in the log-importance ratio, as illustrated in Figure 2(b). Because the expected ratios are generally below 1, samples with positive advantage rarely exceed the upper clipping bound. Consequently, gradients from overconfident positive predictions are largely retained, while those from negative samples are more heavily constrained, making the policy susceptible to overoptimization. Additionally, the variance of the importance ratio depends on denoising scheduler parameters such as σt and dt, causing it to differ substantially across timesteps. This variance inconsistency further amplifies clipping imbalthe clipping threshold is freance: at low-noise steps, quently exceeded, while at high-noise steps, it is rarely triggered, ultimately driving the policy toward step-specific over-optimization. 3.2.2. Regulated Clipping straightforward approach would be to design dedicated clipping range for each timestep. However, this requires 4 Figure 2. Comparison of r(θ) distributions between FlowGRPO and GRPO-Guard across timesteps. (a) Ideally, the ratio distribution should have mean near 1 and stable variance across timesteps to ensure effective clipping. (b) Under FlowGRPO, the distribution exhibits leftward mean shift and increasing variance at low-noise timesteps, causing the clipping mechanism to failparticularly for trajectories with positive advantages. In contrast, GRPO-Guard with RatioNorm preserves balanced mean and consistent variance (c), enabling proper clipping and stable policy updates across all timesteps. tuning large number of hyperparameters and performing extensive experiments to identify near-optimal ranges for specific model and task. To simplify this process and quickly reduce timestep-dependent differences in the mean and variance of the importance-ratio distribution, we instead standardize log rt(θ). This normalization shifts the mean toward zero and removes the influence of denoising scheduler parameters, while preserving the sign and relative magnitude of µθ, thereby maintaining the semantic content of the ratios. Specifically, the operation is defined mathematically as: log ˆrt(θ) = σt dt(log rt(θ) + µθ2 2σ2 dt ) = µθ ϵ (8) After normalization using the above formula, the distribution of the ratios is illustrated in the Figure 2(b): the mean approaches zero. As result, the upper and lower clipping bounds can now function effectively, improving the training stability of Flow-GRPO. However, we observe that the log ratios still exhibit substantial variance differences across timesteps, which leads to uneven clipping when single clip range is applied. in high-noise steps, gradients are rarely clipped, and samples with positive advantage and large ratios retain their full gradient contribution, so overoptimization still occurs. We analyze that this variance Specifically, discrepancy is mainly caused by coefficients related to the noise term, which are inherently correlated with timestep characteristics in diffusion models, as shown in the Figure 2(c). To address this, we remove the influence of the noise coefficients, thereby reducing the variance across timesteps and mitigating the reward hacking phenomenon. Since the log ratios undergoes multiplication and addition operations related to the denoising step, it introduces additional effects on the policy gradient. We will further analyze this phenomenon in detail below. 3.2.3. Gradient Analysis First, we revisit the formulation of the policy gradient in FlowGRPO. Consider the sampling and training process of policy model, whose policy gradient in Flow-GRPO or Dance-GRPO can be formulated as follows. For simplicity, we omit the clipping operation, the minimization term, and the KL-penalty component: θJ (θ) = 1 (cid:88) t= ˆAt θrt(θ) = 1 (cid:88) t=0 ˆAt rt(θ)θ log rt(θ) = = 1 (cid:88) t=0 1 (cid:88) t=0 EϵN (0,I) EϵN (0,I) 5 (cid:104) ˆAt rt(θ)θ log pθ(xt1xt) (cid:34) (cid:105) (9) (cid:35) µθ + σt σ2 dt dt ϵ ˆAt rt(θ)θµθ(xtt) (1t) 2t According to Eq. σ2 )dtθvθ(xtt). In FlowGRPO, since σt = η 3, we have θµθ(xtt) = (1 + 1t , the coefficient (1 + σ2 2 remains approximately constant across timesteps. We thus simplify it as constant term β. Therefore, ) = 1 + η2 (1t) 2t (cid:113) θJ (θ) = 1 (cid:88) EϵN (0,I) (cid:34) β t=0 µθ + σt σ2 dt ϵ (10) (cid:35) ˆAt rt(θ)θvθ(xtt) . dt ϵ where β µθ+σt σ2 are defined as the gradient scale that is independent of the advantage term. Then we empirically analyze and visualize the policy gradients and corresponding gradient scales across different denoising steps in Flow-GRPO. As shown in Figure 3, both exhibit strong correlation and demonstrate consistent trend of increasing gradient magnitude as the noise level decreases. This result aligns with the observations in TempFlowGRPO [13], which addresses this issue through noiseintroduces aware reweighting strategy. it dt), adjusting the graa reweighting coefficient of (σt dient scaling to (β dt ϵ) the on-policy case and (β ) for the off-policy case, thereby improving the optimization efficiency of the policy model. dtµθ+σtdt,ϵ σt Specifically, for Subsequently, we observe that after applying RatioNorm, the policy gradient becomes less sensitive to these gradient scaling factors. The detailed formulation is as follows: θJ (θ) = 1 (cid:88) t=0 ˆAt θ ˆrt(θ) = 1 (cid:88) t=0 ˆAt ˆrt(θ)θ log ˆrt(θ) = = 1 (cid:88) t= 1 (cid:88) t=0 EϵN (0,I) (cid:105) (cid:104) ϵ ˆAt ˆrt(θ) θµθ(xtt) EϵN (0,I) (cid:104) β dt ϵ ˆAt ˆrt(θ)θvθ(xtt) (cid:105) (11) Here, both rt(θ) and ˆrt(θ) typically lie within the range of [1 1e3, 1 + 1e3], making their direct influence on the gradient negligible. After applying RatioNorm, the policy gradient becomes more accurate by removing the interference from factors such as µθ and σt. As shown in Figure 3, the gradient imbalance is alleviated, and the gradient scale approaches β dt ϵ, resembling the on-policy gradient reweighting in TempFlowGRPO. However, the gradient scale is still influenced by the timestep-dependent coefficient dt. We argue that this leads certain steps to dominate the optimization process, as the policy update tends to focus on single noise condition within the entire sampling trajectory, thereby increasing the 6 risk of over-optimization. As illustrated in Figure 9, although the reweighting strategy of TempFlowGRPO accelerates optimization, it also makes the policy more susceptible to over-optimization. To further alleviate the over-optimization issue, we incorporate reweighting factor δ = 1/dt into the policy loss. As illustrated in Figure 3, this adjustment effectively normalizes the gradient magnitudes across different timesteps, leading to more stable optimization process. It is worth noting that for DanceGRPO, σt = η, so the coefficient becomes β = 1 + η2(1t) . Consequently, the reweighting 2t factor in DanceGRPO is defined as δ = β/dt. The final form of our policy loss is expressed as follows: Jpolicy(θ) = (cid:88)"
        },
        {
            "title": "1\nT",
            "content": "T 1 (cid:88) (cid:16) t=0 δ min(cid:0) ˆri t(θ) ˆAi t, i=1 clip( ˆri t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:1)(cid:17) (12) Combined with the effective clipping mechanism enabled by RatioNorm, GRPO-Guard significantly alleviates the over-optimization phenomenon while maintaining similar upward trend in the proxy score, as demonstrated in Table 1 and Figure 4. 4. Experiments 4.1. Experimental Setting Implementation Details: We conduct experiments on two baselines, Flow-GRPO [21] and DanceGRPO [44], using two backbone models, SD3.5-M and Flux.1-dev, to validate the effectiveness of our method in mitigating reward hacking. Following the Flow-GRPO setting, we apply LoRA fine-tuning for both baselines, with the LoRA rank set to 32, the scaling factor α set to 64, learning rate of 3e-4, and clip range of 1e-4. For GRPO-Guard, due to the differences in ratio distributions and gradient magnitudes across steps, we set the clip range to 2e-6, with learning rate of 1e-4 on SD3.5-M and 2e-4 on Flux.1-dev. Notably, since PickScore rewards exhibit relatively minor reward hacking, we use smaller clip range of 4e-6. KL loss is not applied. The training and validation datasets are kept consistent with FlowGRPO. Evaluation Metrics: Following Flow-GRPO, we conduct experiments on three proxy tasks: GenEval [11], TextRender [4], and PickScore [16]. GenEval is rule-based evaluation framework that assesses generators ability to follow textual instructions by measuring object count, color consistency, and spatial arrangement. PickScore is derived from human preference data, where regression head is fine-tuned on CLIP encoder so that its scores align with Figure 3. Gradient magnitude differences across timesteps. In FlowGRPO, gradient magnitudes vary by roughly 20 across timesteps, reflecting the large differences in gradient scale. GRPO-Guard substantially reduces this imbalance, limiting the variation to about 2.5 and preventing over-optimization under any single noise condition. Table 1. Comparison of composite gold scores across different proxy tasks. [] marks the proxy task associated with each row. ImR denotes ImageReward, UniR denotes UnifiedReward, and Average represents the mean value after normalizing the three gold scores relative to the base model (set to 1). Method Step GenEval PickScore Text Render 0.58 0.59 0.71+0.12 0.64 0.63 21.5 20.4 - 1860 [0.94] 1860 [0.95]+0.01 20.9+0.4 1020 0.67 1020 0.70+0.03 0.52 480 0.65+0.07 480 - 0.63 1260 [0.80] SD3.5-M [29] +Flow-GRPO +Ours (Flow-GRPO) +Flow-GRPO +Ours (Flow-GRPO) +Flow-GRPO +Ours (Flow-GRPO) Flux.1-dev [18] +DanceGRPO +Ours (DanceGRPO) 1260 [0.81]+0.01 21.7+0.5 540 +DanceGRPO +Ours (DanceGRPO) 540 0.63 0.64+0.01 21.5 21.8+0.3 [23.1] [23.3]+0.2 0.68+0.04 20.8 21.3+0.5 21.6 21.2 [0.94] [0.93]0.01 0.60 0.60 0.63+0.03 [0.90] [0.89]0.01 Gold Score 3.46 1.40 ImR 1.06 0.85 UniR 3.31 3.05 Average 1.00 0.84 HPSv2 0.293 0.236 0.254+0.018 0.87+0.02 3.22+0.17 0.89+0.05 0.329 0.337+0.008 1.47+0.07 3.54+0.08 1.20+0.04 0.274 0.286+0.012 1.06+0.24 3.29+0.22 0.99+0.11 0.302 0.269 0.300+0.031 1.08+0.29 3.35+0.17 1.02+0.14 0.293 0.304+0.009 1.07+0.14 3.35+0.10 1.02+0. 1.01 0.79 1.00 0.88 3.31 3.18 3.25 0.82 3. 0.88 0.93 0.96 1.16 human judgments. To comprehensively evaluate reward hacking, we further construct composite gold score based solely on image quality, measured by HPSv2 [42], ImageReward [43], and UnifiedReward [39]. During training, we monitor the gold score online by using PickScore for the GenEval and TextRender tasks. For the validation datasets, GenEval, PickScore, and TextRender use the corresponding validation sets from FlowGRPO, while HPSv2, ImageReward, and UnifiedReward all use the PickScore validation set. 4.2. Main Results We report the results of GRPO-Guard on the GenEval, PickScore, and OCR tasks using two backbone models (Flux and SD3.5M) and two baseline methods (FlowGRPO and DanceGRPO), as shown in Table 1. GRPO-Guard achieves superior gold scores under comparable proxy scores, effectively mitigating the severe reward hacking observed in baseline methods caused by the failure of the clipping mechanism. We further visualize the relationship between proxy scores and gold scores during training in Figure 4. As training progresses, the proxy scores of baseline models increase rapidly, leading to sharp decline in gold scores. In contrast, GRPO-Guard maintains consistently high gold scores and image quality throughout the training process. Visual Comparison: We further provide visual comparison of the generated images in Figure 5 and 6. It is evident that, compared with the original models, the outputs from the baseline methods suffer from severe degradationthe image quality collapses completely. Although these methods achieve high proxy scores, the generated results are unusable in practice. Notably, while the PickScore results of the baseline methods do not show significant 7 (a) FlowGRPO on the GenEval task. (b) DanceGRPO on the GenEval task. (c) DanceGRPO on the OCR task. Figure 4. Validation curves of proxy scores and gold scores across different training tasks and baseline methods. Figure 5. Visual comparison between FlowGRPO and GRPO-Guard. FlowGRPO exhibits clear signs of reward hacking, leading to significant decline in both image quality and instruction-following ability. In contrast, GRPO-Guard maintains comparable visual quality while demonstrating stronger text generation accuracy and better adherence to instructions. drop in score, they still exhibit clear reward hacking. As illustrated in Figure 7, the generated faces remain nearly identical across different random seeds, and the body proportions become distorted, rendering the outputs impractiFigure 6. Visual comparison between DanceGRPO and GRPO-Guard. It is clearly observed that DanceGRPO suffers from severe reward hacking, where the generated images exhibit distinct horizontal and vertical stripe artifacts. cal for real-world use. In contrast, our method effectively alleviates these issues while maintaining high proxy scores, producing visually coherent and realistic images. Over-optimization: We further visualize the generated results across different training steps, as shown in Figure 8. It can be clearly observed that as training progresses, the baseline methods enter an over-optimization phase around the mid-training stage. Due to the failure of the clipping mechanism, the image quality deteriorates rapidly the proportion of text regions in the generated images increases progressively until complete reward hacking occurs. At this point, the model focuses solely on text correctness, while text-image consistency, scene richness, and diversity collapse entirely. In contrast, our method maintains visual quality comparable to the base model while significantly improving text accuracy, effectively preventing the degeneration observed in baseline methods. 9 4.3. Ablation Study We further analyze the contributions of the main components of our proposed method. The ablation study is conducted based on the FlowGRPO baseline using the SD3.5M model on the OCR task, trained for 480 steps. As shown in the Table 2 and Figure 9, we design three groups of experiments to separately evaluate the effects of ratio mean correction (Mean-revised), inter-step variance alignment (RatioNorm), and gradient balancing (GRPO-Guard). Their corresponding log rt(θ) distributions and gradient scales are also reported in the table. The experimental results, illustrated in the figure, show that mean correction significantly alleviates the decline in the gold score. Further applying variance alignment mitigates the over-optimization phenomenon even more effectively, although it slightly slows down the growth of the proxy score due to relatively larger number of positively clipped high-advantage ratios. In addition, we compare the reweighting strategy (TempReweight) used in TempFlowGRPO [13] in terms of optimization efficiency and over-optimization behavior. As shown in the Figure 9, although it significantly accelerFigure 7. Comparison between FlowGRPO and GRPO-Guard on the PickScore task. FlowGRPO shows severe distortions in human body proportions and marked reduction in facial diversity, whereas GRPO-Guard effectively preserves realistic body structure and diverse facial appearances throughout training. Figure 8. Generation examples of the policy model at different training steps. ates optimization, it also enters the over-optimization phase much earlierresulting in rapid drop in gold scores and severe reward hacking. In contrast, the gradient reweighting strategy in GRPO-Guard provides more moderate improvement in proxy score growth while substantially alleviating the decline in gold scores. 4.4. Human Evaluation We conduct human preference evaluation to assess image quality, text alignment, and overall quality between the baseline methods and GRPO-Guard. On both the Geneval and OCR tasks, human evaluators compare 100 sample pairs, and the win/tie/lose ratios are shown in Figure 10. The results demonstrate clear superiority of GRPO-Guard in both image quality and overall quality, indicating that the baseline methods suffer from severe over-optimization, Setting Baseline log rt(θ) log rt(θ) Temp-Reweight [13] Mean-revised RatioNorm GRPO-Guard σt σt log rt(θ) log rt(θ) + µθ 2 dt dt(log rt(θ) + µθ 2 dt dt(log rt(θ) + µθ 2 dt 2σ 2σ2 2σ2 ) ) Re-weight Scale dt β σt 1 1 dt ϵ Gradient Scale β µθ +σt σ2 dtµθ +σtdt ϵ σt dt ϵ σt β dt ϵ β 1/dt β ϵ Table 2. Ablation study on major components. Figure 9. Training curves of the ablation study. leading to notable degradation in visual fidelity. Figure 10. Human evaluation results Clip Fraction: We statistically analyze and visualize the clipping ratios of the baseline methods FlowGRPO and GRPO-Guard across different denoising steps. The proportions of samples with importance ratios r(θ) larger than 1+ϵand smaller than 1ϵ are recorded separately, as shown in the Figure 12. As expected, in FlowGRPO, large number of clipping events with ratios smaller than 1 ϵ occur only at the final step (step 8), while the proportion of clipping with ratios larger than 1 + ϵ corresponding to truncation of gradients with positive advantages remains zero. This imbalance leads to the over-optimization phenomenon. In contrast, GRPO-Guard exhibits more stable and balanced clipping ratios across all steps, with the proportions of > 1 + ϵ and < 1 ϵ clipping remaining roughly equal. This indicates that the distributional bias of the ratio has been effectively corrected and the unhealthy clipping mechanism has been mitigated. 4.5. Analysis 5. Conclusion and Limitation Hacking Step: Due to the malfunctioning clipping mechanism, gradients from all steps with importance ratios exceeding 1 + ϵ are not truncated. Consequently, the hacking model exhibits abnormal behaviors across all denoising stages. we visualize the one-step sampled x0 predictions from vθ at different diffusion steps, as shown in Figure 11. At high-noise steps, the hacking model shows clear pathological patterns: the generated images contain overly simplistic and uniform structurestypically limited to the main subjects such as dog and tablewhile omitting broader contextual elements. The global layout appears to be determined prematurely, leaving little room for diverse or detailed scene composition. At low-noise steps, compared with the base model, the hacking model loses its ability to refine fine-grained details. Even during the final denoising stages, substantial residual noise and artifacts remain, resulting in degraded visual quality. These observations indicate that the hacking model suffers from persistent capability degradation throughout the entire denoising process, which aligns with our analysis that gradients beyond 1 + ϵ are never clipped across all timestepsultimately causing severe over-optimization. In this paper, we analyze that although GRPO-based reinforcement learning has advanced the optimization of flowmatching models, its standard importance-ratio clipping remains susceptible to over-optimization due to left-shifted and inconsistent ratio distributions. This often leads to deteriorated generation quality despite rising proxy rewards, thereby limiting its practical applicability. GRPO-Guard effectively addresses this issue by incorporating ratio normalization and gradient reweighting, which regulate the clipping mechanism and stabilize policy updates across denoising steps. Extensive experiments demonstrate that GRPOGuard mitigates over-optimization, preserves or enhances generation quality, and offers robust, generalizable solution for stable policy optimization. We anticipate that this work will provide valuable insights and practical guidance for the development and optimization of GRPO-based algorithms in flow-matching models. Limitation: Although we effectively mitigate overoptimization by reactivating the clipping capability for positive samples, our approach cannot fully eliminate reward hacking caused by intrinsic limitations of the reward model 11 Figure 11. Performance differences between the hacking model and the original model across different denoising steps. Figure 12. Clipping percentage of r(θ) < 1 ϵ and r(θ) > 1 + ϵ during training for FlowGRPO and GRPO-Guard across different denoising steps. itself, stemming from the gap between proxy scores (from the reward model) and gold scores (true evaluation). natural next step to fully address this issue is to scale the reward model, as in approaches like RewardDance [41], so that it more closely approximates comprehensive gold score. However, this strategy introduces substantial computational overhead and prolongs optimization, since GRPO requires sampling large number of outputs along with their reward scores. Therefore, designing comprehensive, efficient reward model that effectively aligns proxy scores with gold 12 scores remains promising direction for future research. 6. ACKNOWLEDGMENTS We thank Ziyang Yuan, Borui Liao, Haoran He, Yuanxing Zhang, Qunzhong Wang, Jiaheng Liu for the valuable discussion."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2 [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 3 [4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36:93539387, 2023. 2, 6 [5] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 2 [6] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023. 3 [7] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [9] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. 3 [10] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. 2, 3 [11] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 2, 6 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, [13] Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. 3, 6, 9, 11 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2, 3 [16] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:36652 36663, 2023. 2, 6 [17] Kuaishou. Kling. https://klingai.kuaishou. com/, 2024. [18] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 7 [19] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flowbased grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. 3 [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 3 [21] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 2, 3, 6 [22] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 3 [23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [24] Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, and Dacheng Tao. Inform: Mitigating reward hacking in rlhf via information-theoretic reward modeling. Advances in Neural Information Processing Systems, 37: 134387134429, 2024. 2, [25] Ted Moskovitz, Aaditya Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca Dragan, and Stephen McAleer. Confronting reward model overoptimization with constrained rlhf. arXiv preprint arXiv:2310.04373, 2023. 3 [26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language 13 Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2 [38] Feng Wang and Zihao Yu. Coefficients-preserving sampling for reinforcement learning with flow matching. arXiv preprint arXiv:2509.05952, 2025. 3 [39] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. 3, 7 [40] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. [41] Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025. 3, 12 [42] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2, 7 [43] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 3, 7 [44] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 2, 3, 6 [45] Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, and Guangtao Zhai. G2rpo: Granular grpo for precise reward in flow models. arXiv preprint arXiv:2510.01982, 2025. 3 models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 2 [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 2 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 7 [30] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 3 [31] Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, et al. Multi-turn reinforcement learning with preference human feedback. Advances in Neural Information Processing Systems, 37:118953 118993, 2024. 2 [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2 [33] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. 3 [34] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [35] Hao Sun. Supervised fine-tuning as inverse reinforcement learning. arXiv preprint arXiv:2403.12017, 2024. 2 [36] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 3 [37] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun"
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "HKUST",
        "Kuaishou Technology",
        "Shenzhen Campus of Sun Yat-Sen University",
        "Tsinghua University",
        "UCAS",
        "USTB"
    ]
}