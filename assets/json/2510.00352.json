{
    "paper_title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance",
    "authors": [
        "Tong Chen",
        "Yinuo Zhang",
        "Pranam Chatterjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation."
        },
        {
            "title": "Start",
            "content": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance 5 2 0 2 0 3 ] . [ 1 2 5 3 0 0 . 0 1 5 2 : r Tong Chen,1 Yinuo Zhang,2 Pranam Chatterjee1,3, 1Department of Computer and Information Science, University of Pennsylvania 2Centre for Computational Biology, Duke-NUS Medical School, Singapore 3Department of Bioengineering, University of Pennsylvania Corresponding author: pranam@seas.upenn.edu"
        },
        {
            "title": "Abstract",
            "content": "Designing sequences that satisfy multiple, often conflicting, objectives is central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as powerful, sequence-based framework for multi-property biomolecule generation."
        },
        {
            "title": "Introduction",
            "content": "The design of biological sequences must account for multiple, often conflicting, objectives (Naseri and Koffas, 2020). Therapeutic peptides, for example, must combine high binding affinity with low toxicity and favorable pharmacokinetics (Tominaga et al., 2024; Tang et al., 2025b); CRISPR guide RNAs require both high on-target activity and minimal off-target effects (Mohr et al., 2016; Schmidt et al., 2025); and synthetic promoters must deliver strong expression while remaining tissue-specific (Artemyev et al., 2024). These examples illustrate that biomolecular engineering is inherently multi-objective optimization problem. Yet, most computational frameworks continue to optimize single objectives in isolation (Zhou et al., 2019; Nehdi et al., 2020; Nisonoff et al., 2025). While such approaches can reduce toxicity (Kreiser et al., 2020; Sharma et al., 2022) or improve thermostability (Komp et al., 2025), they often create adverse trade-offs: high-affinity peptides may be insoluble or hemolytic, and stabilized proteins may lose specificity (Bigi et al., 2023; Rinauro et al., 2024). Black-box multi-objective optimization (MOO) methods such as evolutionary search and Bayesian optimization have long been applied to molecular design (Zitzler and Thiele, 1998; Deb, 2011; Ueno et al., 2016; Frisby and Langmead, 2021), but these approaches scale poorly in high-dimensional sequence spaces. To overcome this, recent generative approaches have incorporated controllable multi-objective sampling (Li et al., 2018; Sousa et al., 2021; Yao et al., 2024). For instance, ParetoFlow (Yuan et al., 2024) leverages continuous-space flow matching to generate Pareto-optimal samples. However, extending such guarantees to biological sequences is challenging, since discrete data typically require embedding into continuous manifolds, which distorts token-level structure and complicates property-based guidance (Beliakov and Lim, 2007; Michael et al., 2024). more direct path lies in discrete flow models (Campbell et al., 2024; Gat et al., 2024; Dunn and Koes, 2024). These models define probability paths over categorical state spaces, either through simplex-based interpolations (Stark et al., 2024; Davis et al., 2024; Tang et al., 2025a) or jump-process flows that learn token-level transition rates (Campbell et al., 2024; Gat et al., 2024). Recent advances have shown their promise for controllable single-objective generation (Nisonoff et al., 2025; Tang et al., 2025a), but no framework yet achieves Pareto guidance across multiple objectives. Here, the notion of rectification provides crucial building block. In the continuous setting, Rectified Flows (Liu et al., 2023) learn to straighten ODE paths between distributions, thereby reducing convex transport costs and enabling efficient few-step or even one-step sampling. Recently, ReDi (Rectified Discrete Flows) (Yoo et al., 2025) extended this principle to discrete domains. By iteratively refining the coupling between source and target distributions, ReDi provably reduces factorization error (quantified as conditional total correlation) while maintaining distributional fidelity. This makes ReDi highly effective for efficient discrete sequence generation. However, ReDi does not address the multi-objective setting, as it lacks mechanism to steer sampling toward the Pareto front, where improvements in one objective cannot be made without degrading another. This is critical limitation for biomolecular design, where trade-offs define practical success. To address this, we introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), new framework that extends rectified discrete flows with multi-objective guidance. AReUReDi integrates three innovations: (i) annealed Tchebycheff scalarization, which gradually sharpens the focus on balanced solutions across objectives (Lin et al., 2024a); (ii) locally balanced proposals, which combine the generative prior of ReDi with multi-objective guidance while ensuring reversibility; and (iii) Metropolis-Hastings updates, which preserve exact distributional invariance and guarantee convergence to Pareto-optimal states. Together, these mechanisms refine rectified discrete flows into principled Pareto sampler. Our key contributions are: 1. We propose AReUReDi, the first multi-objective extension of rectified discrete flows, integrating annealed scalarization, locally balanced proposals, and MCMC updates. 2. We provide theoretical guarantees that AReUReDi preserves distributional invariance and converges to the Pareto front with full coverage. 3. We demonstrate that AReUReDi can optimize up to five competing biological properties simultaneously, including affinity, solubility, hemolysis, half-life, and non-fouling, for therapeutic peptide design. 4. We benchmark AReUReDi against classical MOO algorithms and state-of-the-art discrete diffusion approaches, showing superior trade-off navigation and biologically plausible peptide sequence designs."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Discrete Flow Matching Let = denote the discrete state space, where is vocabulary of size and each = (x1, . . . , xL) is sequence of tokens. discrete flow matching (DFM) model (Campbell et al., 2024; Gat et al., 2024; Dunn and Koes, 2024) defines probability path {pt}t[0,1] interpolating between simple source distribution p0 and target distribution p1 by means of coupling π(x0, x1) and conditional bridge distributions pt(xt x0, x1). The model is trained to approximate conditional transitions pst(xs xt) for 0 < 1. Since the joint distribution over coordinates is intractable, DFMs employ factorization pst(xs xt) (cid:89) i=1 pst (cid:0)xi xt (cid:1) , which introduces discrepancy measured by the conditional total correlation (cid:32) TCst = KL pst(xs xt) (cid:33) pst(xi xt) . (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:89) i=1 This quantity captures the inter-dimensional dependencies neglected under factorization, and grows with larger step sizes (Stark et al., 2024; Davis et al., 2024; Tang et al., 2025a). As result, DFMs are accurate in the many-step regime but degrade under few-step or one-step generation."
        },
        {
            "title": "2.2 Rectified Discrete Flow",
            "content": "To mitigate factorization error, Rectified Discrete Flow (ReDi) (Yoo et al., 2025) introduces an iterative rectification of the coupling π. Starting from an initial coupling π(0)(x0, x1), DFM is trained under π(k) to produce new sourcetarget pairs, defining an empirical joint distribution ˆπ(k). The coupling is then updated via π(k+1)(x0, x1) π(k)(x0, x1) pθ(k) (x1 x0) pθ(k) (x1) , where pθ(k) (x1 x0) is the conditional distribution learned at iteration k. This yields sequence of couplings {π(k)}k0 with provably decreasing conditional TC, TCst(π(k+1)) TCst(π(k)). By progressively reducing factorization error, ReDi produces well-calibrated base distribution p1 with low inter-dimensional correlation. This base distribution provides reliable marginal transition probabilities t( xt) for each coordinate at time t, which serve as the generative prior in the AReUReDi framework. pi Rectification follows the same principle as Rectified Flow in continuous domains (Liu et al., 2023), where iterative refinement straightens ODE paths and decreases transport costs. 2.3 Multi-Objective Setup In biomolecular design and related applications, the generation task is inherently multi-objective (Zitzler and Thiele, 1998; Deb, 2011; Frisby and Langmead, 2021). Let s1, . . . , sN : denote scalar objectives, and let sn(x) be their normalized counterparts mapping into [0, 1] to ensure comparability. Given weights ω 1, the Tchebycheff scalarization is defined as which balances objectives by rewarding solutions that are simultaneously strong across all metrics rather than excelling in just one (Miettinen, 1999). This scalarization will serve as the core of the guidance mechanism in AReUReDi. Sω(x) = min 1nN ωn sn(x),"
        },
        {
            "title": "3 AReUReDi: Annealed Rectified Updates for Refining Discrete Flows",
            "content": "With an efficient discrete flow-based generation framework in hand, we develop AReUReDi that extends ReDi (Yoo et al., 2025) to the multi-objective optimization setting, where the goal is to generate discrete samples that approximate the Pareto front of multiple competing objectives. Starting from pre-trained ReDi model, AReUReDi incorporates annealed guidance, locally balanced proposals, and Metropolis-Hastings updates to progressively bias the sampling process toward Pareto-optimal states while preserving the probabilistic guarantees of the underlying flow (Figure 1, Algorithm 1). 3.1 Problem Setup Let the discrete search space be = L, where is finite vocabulary of size and each state = (x1, . . . , xL) is sequence of tokens. We assume access to pre-trained ReDi model that provides In addition, we are given marginal transition probabilities pi pre-trained scalar objective functions sn : R, where = 1, . . . , , and sn(x) are their normalized counterparts with outputs mapped to [0, 1] to support balanced updates for each objective. The sampling task is to construct Markov chain whose stationary distribution concentrates on states that approximate the Pareto front of the normalized objectives s1, . . . , sN . t( xt) for each position and time t. 3.2 Annealed Multi-Objective Guidance To direct sampling toward the Pareto front, AReUReDi introduces scalarized reward Sω(x) = min 1nN ωn sn(x), where the weight vector ω = [ω1, . . . , ωN ] lies in the probability simplex 1 and balances the different objectives. This Tchebycheff scalarization promotes solutions that are simultaneously strong across all 3 Figure 1: AReUReDi. Discrete flow matching is first rectified to reduce conditional total correlation. At each timestep, candidate single-position mutations with ReDi-predicted probabilities (visualized by arrows of varying thickness) are evaluated by multiple objective functions. locally balanced proposal is then constructed using Tchebycheff scalarization with annealed guidance strength, and the next state is selected via Metropolis-Hastings update. This iterative process drives the generated sequences toward the Pareto front. objectives rather than excelling in only subset (Miettinen, 1999). The scalarized reward is converted into guidance weight Wηt,ω(x) = exp (cid:0)ηtSω(x)(cid:1), where the parameter ηt > 0 controls the strength of the guidance at each iteration t. AReUReDi incorporates an annealing schedule for ηt: ηt = ηmin + (cid:0)ηmax ηmin (cid:1) 1 , so that the chain begins with small value of ηt to encourage wide exploration of the state space and gradually increases ηt to focus sampling on high-quality Pareto candidates. This annealing strategy mirrors simulated annealing but operates directly on the scalarized objectives within the discrete flow framework. 3.3 Locally Balanced Proposals Given the current state xt, AReUReDi updates one coordinate {1, . . . , L} at time using locally balanced proposal that blends the generative prior of ReDi with the multi-objective guidance. First, 4 t( xt), optionally pruned using top-p candidate set of replacement tokens is drawn from the ReDi marginal pi to retain only the most promising alternatives for computational efficiency. For each candidate token y, the algorithm computes the ratio ri(y; xt) = Wηt,ω (cid:0)x(iy) Wηt,ω(xt) (cid:1) , which measures the change in scalarized reward if xi were replaced by y. The ratio ri(y; xt) is then transformed by balancing function : R+ R+ that satisfies the symmetry condition g(u) = g(1/u). Typical choices include Barkers function g(u) = u. This symmetry ensures that 1+u the resulting Markov chain admits the desired stationary distribution. Using the balanced function, the unnormalized proposal for candidate token takes the form and the square-root function g(u) = qi(y xt) = pi t(y xt) g(cid:0)ri(y; xt)(cid:1), which is then normalized over the candidate set to yield the final proposal distribution qi(y xt). This construction allows the proposal to favor states with higher scalarized reward while remaining reversible with respect to the target distribution."
        },
        {
            "title": "3.4 Metropolis-Hastings Update",
            "content": "A candidate token is drawn from the final proposal distribution qi( xt) and forms the proposed state xprop = x(iy) . The proposal is accepted with the standard Metropolis-Hastings probability (Hastings, 1970) αi(xt, xprop) = min 1, (cid:26) πηt,ω(xprop) qi(xi xprop) πηt,ω(xt) qi(y xt) (cid:27) , where we define πηt,ω(x) p1(x) Wηt,ω(x) = p1(x) exp(cid:0)ηtSω(x)(cid:1). With Barkers balancing function, the acceptance probability simplifies to one, ensuring automatic acceptance of proposals and faster mixing. Other choices, such as the square-root function, trade higher acceptance rates for more conservative moves. The annealed, locally balanced updates are repeated for iterations and end with the final sample x1 whose objective scores are jointly optimized. Building on the ReDi models well-calibrated base distribution with low inter-dimensional correlation, AReUReDi safely biases this base toward Pareto-optimal regions while preserving full coverage of the state space, thereby guaranteeing convergence to Pareto-optimal solutions with complete coverage of the Pareto front (Proof S1)."
        },
        {
            "title": "4 Experiments",
            "content": "To the best of our knowledge, no public datasets exist for benchmarking multi-objective optimization algorithms on biological sequences. We therefore developed two benchmarks to evaluate AReUReDi, focusing on the generation of wild-type peptide sequences and chemically-modified peptide SMILES. These tasks are supported by two core components: the generative models described in Section 4.1 and the objective-scoring models validated in Section S4. Leveraging these models, we demonstrate AReUReDis efficacy on wide range of tasks and examples. Although AReUReDi provides theoretical guarantees of Pareto optimality and full coverage, in practice, these guarantees hold only in the limit of an infinitely long Markov chain. Reaching the Pareto front with high probability can therefore require vast number of sampling steps. To improve sampling efficiency in all reported experiments, we introduce monotonicity constraint that accepts only token updates that increase the weighted sum of the current objective scores. Empirical results prove the accelerated convergence toward high-quality Pareto solutions without altering the underlying optimization objectives (Table S2). Therefore, this monotonicity constraint was involved in all the following experiments. 4.1 PepReDi and SMILESReDi Generate Diverse and Biologically Plausible Sequences To enable the efficient generation of peptide binders, we developed an unconditional peptide generator, PepReDi, based on the ReDi framework. The model backbone of PepReDi is Diffusion Transformer (DiT) architecture (Peebles and Xie, 2022). We trained PepReDi on custom dataset comprising approximately 15,000 peptides from the PepNN and BioLip2 datasets, as well as sequences from the PPIRef dataset, with lengths ranging from 6 to 49 amino acids (Abdin et al., 2022; Zhang et al., 2024; Bushuiev et al., 2023). Using 5 Table 1: Training and validation performance of PepReDi over successive rectification rounds. Each row reports the training loss, validation negative log-likelihood (NLL), validation perplexity (PPL), and conditional total correlation (TC). PepReDi without superscript denotes the base model, while PepReDi1, PepReDi3, PepReDi3 indicate the first, second, and third rounds of rectification, respectively. Train Loss Val NLL Val PPL Conditional TC PepReDi PepReDi1 PepReDi2 PepReDi3 1.6567 1.6170 1.5347 1.3538 1. 1.6101 1.5238 1.3548 5.19 5.00 4.59 3.88 10.6027 12.6250 11.7279 11.2339 Table 2: Evaluation metrics for the generative quality of peptide SMILES sequences of max token length set to 200. SMILESReDi without superscription denotes the base model, while SMILESReDi1 refers to the model that has undergone one round of rectification. Model Validity () Uniqueness () Diversity () SNN () Data PepMDLM SMILESReDi SMILESReDi1 PepTune AReUReDi 1.000 0.450 0.763 0. 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 0.885 0.705 0.719 0.665 0.677 0.789 1.000 0.513 0.593 0. 0.486 0.392 this trained model, we generated new data couplings containing 10,000 sequences for each peptide length and used them to fine-tune PepReDi in an iterative rectification procedure. This rectification was performed three times and yielded substantial improvements in training loss, validation negative log-likelihood (NLL), perplexity (PPL), and conditional TC  (Table 1)  . Notably, the conditional TC rises after the first rectification, likely due to the distributional shift from the large, model-generated coupling, whose absolute TC can be higher even though ReDi guarantees monotonic decrease within each coupling. The low validation NLL and PPL metrics showcase PepReDis reliability to generate biologically plausible wild-type peptide sequences. SMILESReDi adopts the same backbone structure as PepReDi, enhanced with Rotary Positional Embeddings (RoPE), which effectively captures the relative inter-token interactions in peptide SMILES (Su et al., 2024). SMILESReDi also incorporates time-dependent noising schedule to improve its capability to generate valid peptide SMILES sequences (Section S2.2). We applied the same training data as PepMDLM, state-of-theart diffusion model that generates valid peptide SMILES sequences (Tang et al., 2025b). After only two training epochs, SMILESReDi converged to validation NLL of 0.722 and achieved sampling validity of 76.3% using just 16 generation steps. One hundred SMILES sequences were then generated by the trained SMILESReDi for each length from 4 to 1035, forming large and diverse new data coupling. Following single round of rectification, the validation NLL further decreased to 0.608, and the sampling validity rose dramatically to 98.6% with 16 steps and 100% with 32 steps  (Table 2)  . While its similarity-to-nearest-neighbor (SNN) score and diversity are comparable to those of PepMDLM (details on metrics are provided in Section S2.2), SMILESReDi substantially outperforms PepMDLM in validity, highlighting its superior capability of generating diverse chemically-modified peptide SMILES sequences. 4.2 AReUReDi effectively balances each objective trade-off With pre-trained PepReDi in hand, we first focus on validating AReUReDis capability of balancing multiple conflicting objectives. We performed two sets of experiments for wild-type peptide binder generation with three property guidance, and in ablation experiment settings, we removed one or more objectives. In the binder design task for target 7LUL (hemolysis, solubility, affinity guidance; Table S3), omitting any single guidance causes collapse in that property, while the remaining guided metrics may modestly improve. Likewise, in the binder design task for target CLK1 (affinity, non-fouling, half-life guidance; Table S4), disabling non-fouling guidance allows half-life to exceed 96 hours but drives non-fouling near zero, and disabling half-life guidance preserves non-fouling yet reduces half-life below 2 hours. In contrast, enabling Figure 2: (A), (B) Complex structures of PDB 1B8Q with an AReUReDi-designed binder and its pre-existing binder. (C), (D) Complex structures of OX1R and EWS::FLI1 with an AReUReDi-designed binder. Five property scores are shown for each binder, along with the ipTM score from AlphaFold3 and docking score from AutoDock VINA. Interacting residues on the target are visualized. (E) Plots showing the mean scores for each property across the number of iterations during AReUReDis design of binders of length 12-aa for EWS::FLI1. (F) density plot illustrating the distribution of predicted property scores for AReUReDi-designed EWS::FLI1 binders of length 12-aa, compared to the peptides generated unconditionally by PepReDi3. all guidance signals produces the most balanced profiles across all objectives. These results confirm that AReUReDi precisely targets chosen objectives while preserving the flexibility to navigate conflicting objectives and push samples toward the Pareto front. 4.3 AReUReDi generates wild-type peptide binders under five property guidance We next benchmark AReUReDi on wild-type peptide binder generation task guided by five different properties that are critical for therapeutic discovery: hemolysis, non-fouling, solubility, half-life, and binding affinity. To evaluate AReUReDi in controlled setting, we designed 100 peptide binders per target for 8 diverse proteins, structured targets with known binders (3IDJ, 5AZ8, 7JVS), structured targets without known binders (AMHR2, OX1R, DUSP12), and intrinsically disordered targets (EWS::FLI1, MYC)  (Table 3)  . Across all targets and across multiple binder lengths, the generated peptides achieve superior hemolysis rates 7 Table 3: AReUReDi generates wild-type peptide binders for 8 diverse protein targets, optimizing five therapeutic properties: hemolysis, non-fouling, solubility, half-life (in hours), and binding affinity. Each value represents the average of 100 AReUReDi-designed binders. Name Binder Length Hemolysis Non-Fouling Solubility Half-Life (h) Affinity AMHR2 AMHR2 AMHR2 EWS::FLI1 EWS::FLI1 EWS::FLI1 MYC OX1R DUSP12 1B8Q 5AZ8 7JVS 8 12 16 8 12 16 8 10 9 8 11 11 0.9156 0.9384 0.9420 0.9186 0.9345 0.9416 0.9180 0.9302 0.9240 0.9214 0.9293 0. 0.8613 0.8872 0.8914 0.8630 0.8819 0.8875 0.8627 0.8687 0.8669 0.8680 0.8732 0.8840 0.8564 0.8810 0.8755 0.8619 0.8796 0.8807 0.8627 0.8563 0.8633 0.8654 0.8605 0.8743 45.73 52.52 63.34 44.77 59.11 64.32 44.13 50.14 48.14 42.63 58.33 56.49 7.0608 7.2284 7.2533 5.8424 6.2007 6.4195 6.4082 7.1882 6.1276 5.7130 6.2792 6.8449 (0.91-0.94), high non-fouling (>0.86) and solubility (>0.85), extended half-life (42-64 h), and strong affinity scores (5.7-7.3), demonstrating both balanced optimization and robustness to sequence length. For the target proteins with pre-existing binders, we compared the property values between their known binders with AReUReDi-designed ones (Figure 2A,B, S1). The designed binders significantly outperform the pre-existing binders across all properties without compromising the binding potential, which is further confirmed by the ipTM scores computed by AlphaFold3 (Abramson et al., 2024) and docking scores calculated by AutoDock VINA (Trott and Olson, 2010). Although the AReUReDi-designed binders bind to similar target positions as the pre-existing ones, they differ significantly in sequence and structure, demonstrating AReUReDis capacity to explore the vast sequence space for optimal designs. For target proteins without known binders, complex structures were visualized using one of the AReUReDi-designed binders (Figure S2). The corresponding property scores, as well as ipTM and docking scores, are also displayed. Some of the designed binders showed longer half-life, while others excelled in non-fouling and solubility, underscoring the comprehensive exploration of the sequence space by AReUReDi. To evaluate our guided generation strategy, we tracked the mean and standard deviation of five property scores across 100 generated binders (length 12) targeting EWS::FLI1 at each iteration (Figure 2E). All five properties steadily improved, with average scores for solubility and non-fouling properties increasing markedly from 0.4 to 0.9. The large standard deviation observed in the final half-life and binding affinity values reflects this propertys high sensitivity to guidance, as AReUReDi balances the trade-offs between multiple conflicting objectives. We further visualized AReUReDis impact by comparing the property distribution of the 100 guided peptides to that of 100 peptides unconditionally sampled from PepReDi3 (Figure 2F). The results show that AReUReDi effectively shifted the distribution towards peptides with higher binding affinity. Collectively, these findings demonstrate AReUReDis capability to steer generation toward simultaneous multi-property optimization. We benchmarked AReUReDi against four established multi-objective optimization (MOO) baselines (NSGAIII (Deb and Jain, 2013), SMS-EMOA (Beume et al., 2007), SPEA2 (Zitzler et al., 2001), and MOPSO (Coello and Lechuga, 2002)) on two protein targets: 1B8Q, small protein with known peptide binders (Zhang et al., 1999), and PPP5, larger protein without characterized binders (Yang et al., 2004)  (Table 4)  . Each method generated 100 candidate binders optimized for five properties: hemolysis, non-fouling, solubility, half-life, and binding affinity. While AReUReDi required longer runtimes than evolutionary baselines, it consistently produced the best trade-offs. For both targets, it designed targets with top hemolysis scores, increased non-fouling and solubility by 30-50%, maintained competitive binding affinity, and even extended the half-life by factor of 3-13 relative to the next-best method. These results underscore AReUReDis effectiveness in navigating high-dimensional property landscapes to yield peptide binders with balanced, optimized profiles. We also compared against PepTune (Tang et al., 2025b), recent masked discrete diffusion model for peptide design that couples generation with Monte Carlo Tree Search for MOO. PepTunes backbone was adapted to the existing DPLM model (Wang et al., 2024) for wild-type peptide sequence generation. Despite longer runtimes, AReUReDi substantially outperformed PepTune across all objectives, yielding nearly threefold improvements 8 Table 4: AReUReDi outperforms traditional multi-objective optimization algorithms in designing wild-type peptide binders guided by five objectives. Each value represents the average of 100 designed binders. The table also records the average runtime for each algorithm to design single binder. The best result for each metric is highlighted in bold. Target Method Time (s) Hemolysis Non-Fouling Solubility Half-Life (h) Affinity 1B8Q PPP5 MOPSO NSGA-III SMS-EMOA SPEA2 PepTune + DPLM AReUReDi MOPSO NSGA-III SMS-EMOA SPEA2 PepTune + DPLM AReUReDi 8.54 33.13 8.21 17.48 2.46 55 11.34 37.30 8.43 19.02 4.80 195 0.8934 0.9138 0.8804 0.9181 0.8547 0.9214 0.9117 0.9521 0.8758 0.9445 0.8816 0.9412 0.4763 0.5715 0.3450 0.4973 0.3085 0. 0.4711 0.7138 0.4269 0.6221 0.2752 0.896 0.4684 0.5825 0.3511 0.5057 0.3213 0.8654 0.4255 0.7066 0.4334 0.6098 0.2636 0.8832 4.45 7.32 3.02 4.13 1.17 22.93 1.77 2.90 1.03 2.61 1.27 38.28 6.0594 7.2178 5.955 7.3240 5.2398 5. 6.6958 7.3789 6.2854 7.6253 5.8454 6.7186 Figure 3: (A) Example 2D SMILES structure of AReUReDi-designed peptide binders with four property scores. (B) Plots showing the mean scores for each property across the number of iterations during AReUReDis design of binders of length 200 for NCAM1. in non-fouling and solubility and 22-fold increase in half-life. Together, these comparisons demonstrate that AReUReDi surpasses not only traditional MOO algorithms but also the current state-of-the-art diffusion-based approach for multi-objective-guided wild-type peptide binder design. 4.4 AReUReDi generates therapeutic peptide SMILES under four property guidance To demonstrate the broad applicability of AReUReDi for multi-objective guided generation of biological sequences, we employed the rectified SMILESReDi model to design chemically-modified peptide binder SMILES sequences for five diverse therapeutic targets. These included the metabolic hormone receptor Glucagon-like peptide-1 receptor (GLP1), the iron transport protein Transferrin receptor (TfR), the Neural Cell Adhesion Molecule 1 (NCAM1), the neurotransmitter transporter GLAST, and the developmental Anti-Müllerian Hormone Receptor Type 2 (AMHR2). For each target, sequence generation was jointly conditioned on predicted binding-affinity score to the target protein, as long as hemolysis, solubility, and non-fouling, to ensure both potency and desirable physicochemical profiles. Although PepTune is also able to perform multi-property guided design of peptide-binder SMILES sequences, it does not report average property scores for its generated binders, making direct quantitative comparison with AReUReDi infeasible (Tang et al., 2025b). We selected and visualized representative binders with the highest predicted binding affinities for each target (Figure 3A, S3A,C, S4A,C). All selected binders achieved high scores across hemolysis, solubility, non-fouling, and binding affinity. During generation, we recorded the mean and standard deviation of all four property scores over 100 binders at each iteration to assess the effectiveness of the multi-objective guidance (Figure 3B, S3B,D, S4B,D). Across all targets, binding affinity scores and non-fouling scores showed steady upward trends throughout the generation process, while hemolysis and solubility scores fluctuated, indicating AReUReDis effort to balance the four conflicting objectives. Moreover, AReUReDi produces valid sequences with substantially higher diversity and lower SNN than PepTune, indicating both superior novelty and structural variability  (Table 2)  . These findings highlight the versatility and reliability of AReUReDi for the de novo design of chemically modified peptide binders across wide range of therapeutic targets."
        },
        {
            "title": "4.5 Ablation Studies for Rectification and Annealed Guidance Strength",
            "content": "To determine if rectification offers an advantage over standard discrete flow matching, we compared the performance of AReUReDi using three generative models: the base PepReDi model (no rectification), PepReDi (three rounds of rectification), and PepDFM, standard discrete flow model that follows (Gat et al., 2024) and was trained on the same data (Section S2.3). Under the three settings, wild-type binders were designed for two distinct protein targets: 5AZ8 and AMHR2 (Table S5). For the AMHR2 target, the rectified model achieved the highest scores across all five properties, with its predicted half-life surpassing the next-best method by nearly 13 hours. For the 5AZ8 target, the rectified model yielded significantly higher half-life while maintaining comparable performance on other metrics. These results indicate that by lowering conditional TC and improving the quality of the probability path, rectification enables AReUReDi to achieve stronger Pareto trade-offs on the more demanding objectives. We further demonstrated the advantage of using an annealed guidance strength (Table S6). AReUReDi was applied to design wild-type peptide binders for two distinct proteins: structured protein with known binders (PDB 1DDV) and an intrinsically disordered protein without known binders (P53). Across both targets, any fixed guidance strength, whether set to ηmin, ηmax, or their midpoint, failed to match the performance achieved with an annealed schedule. For 1DDV, annealing produced binders with markedly higher half-life and the best solubility, while maintaining hemolysis, non-fouling, and affinity scores that meet or exceed those of all fixed-η settings. similar trend holds for P53, where the annealing schedule consistently delivers the strongest results across all objectives. These findings confirm that gradually increasing the guidance strength enables AReUReDi to attain more favorable Pareto trade-offs, enhancing challenging properties such as half-life without sacrificing other therapeutic metrics."
        },
        {
            "title": "5 Related Works",
            "content": "Online Multi-Objective Optimization. Recent work in multi-objective guided generation has focused on online or sequential decision-making, where solutions are refined with new data (Gruver et al., 2023; Jain et al., 2023; Stanton et al., 2022; Ahmadianshalchi et al., 2024). common approach is Bayesian optimization (BO), which builds surrogate model and proposes evaluations via acquisition functions (Yu et al., 2020; Shahriari et al., 2015). Multi-objective BO often uses advanced criteria such as EHVI (Emmerich and Klinkenberg, 2008), information gain (Belakaria et al., 2021), or scalarization (Knowles, 2006; Zhang and Li, 2007; Paria et al., 2020). While AReUReDi also employs Tchebycheff scalarization, it operates in an offline setting, where each sequence requires costly evaluation. This contrasts with the sequential, feedback-driven nature of online methods, making direct comparison inappropriate. Tchebycheff Scalarization. Tchebycheff scalarization can identify any Pareto-optimal point and is widely used in multi-objective optimization (Miettinen, 1999). Recent variants include smooth scalarization for gradient-based algorithms (Lin et al., 2024b) and OMD-TCH for online learning (Liu et al., 2024). AReUReDi is, to our knowledge, the first to apply Tchebycheff scalarization for offline generative design of discrete therapeutic sequences. Future work may extend to many-objective problems or alternative utility functions (Lin et al., 2024a; Tu et al., 2023). Diffusion and Flow Matching. Generative approaches such as ParetoFlow and PGD-MOO adapt flow matching or diffusion models for multi-objective optimization (Yuan et al., 2024; Annadani et al., 2025). 10 These operate in continuous or latent spaces, whereas AReUReDi is designed for discrete token spaces inherent to biological sequences. This domain mismatch precludes direct benchmarking. Biomolecule Generation. Offline multi-objective frameworks such as EGD and MUDM have optimized molecules with multiple properties (Sun et al., 2025; Han et al., 2023), but these emphasize 3D structural representations. By contrast, AReUReDi is sequence-only, operating directly over amino acids or SMILES, which makes structural methods unsuitable as direct comparators."
        },
        {
            "title": "6 Discussion",
            "content": "In this work, we have presented AReUReDi, multi-objective optimization framework that extends rectified discrete flows to generate biomolecular sequences satisfying multiple, often conflicting, properties. By integrating annealed Tchebycheff scalarization, locally balanced proposals, and Metropolis-Hastings updates, AReUReDi provides theoretical guarantees of convergence to the Pareto front while maintaining full coverage of the solution space. Built on high-quality base generators such as PepReDi and SMILESReDi, the method demonstrates broad applicability across amino acid sequences and chemically modified peptide SMILES. Superior in silico results establish AReUReDi as general, theoretically-grounded tool for multi-propertyguided biomolecular sequence design. While AReUReDi excels in domains like wild-type and chemically-modified peptide designs, future work will extend to other biological modalities, including DNA, RNA, antibodies, and combinatorial genotype libraries, where multi-objective trade-offs are central. From theoretical perspective, improving AReUReDis efficiency while maintaining the Pareto convergence guarantees and incorporating uncertainty-aware or feedback-driven guidance remain key directions to explore. Ultimately, AReUReDi provides foundation for designing the next generation of therapeutic molecules that are not only potent but also explicitly optimized for the diverse properties required for clinical success."
        },
        {
            "title": "Declarations",
            "content": "Acknowledgments We thank Mark III Systems for providing database and hardware support that has contributed to the research reported within this manuscript. We further thank Sophia Tang and Sophia Vincoff for assistance with PepTune benchmarking, and Qiang Liu for discussions related to Rectified Flow Matching. Author Contributions T.C. designed and evaluated PepReDi, SMILESReDi, PepDFM, and AReUReDi, and performed model benchmarking and visualizations. Y.Z. curated and processed the PPIRef dataset for training, and performed molecular docking. P.C. conceived, designed, directed, and supervised the study. Data and Materials Availability All sequences and data needed to evaluate the conclusions are presented in the paper and tables. All code to train PepReDi, SMILESReDi and generate sequences with AReUReDi are accessible by the academic community at https://huggingface.co/ChatterjeeLab/AReUReDi. Competing Interests P.C. is co-founder of Gameto, Inc., UbiquiTx, Inc., and Atom Bioworks, Inc., and advises companies involved in peptide development. P.C.s interests are reviewed and managed by the University of Pennsylvania in accordance with their conflict-of-interest policies."
        },
        {
            "title": "References",
            "content": "Abdin, O., Nim, S., Wen, H., and Kim, P. M. (2022). Pepnn: deep attention model for the identification of peptide binding sites. Communications biology, 5(1):503. Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T., Pritzel, A., Ronneberger, O., Willmore, L., Ballard, A. J., Bambrick, J., et al. (2024). Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 13. Ahmadianshalchi, A., Belakaria, S., and Doppa, J. R. (2024). Pareto front-diverse batch multi-objective bayesian optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1078410794. Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. (2019). Optuna: next-generation hyperparameter optimization framework. In International Conference on Knowledge Discovery and Data Mining, pages 26232631. Annadani, Y., Belakaria, S., Ermon, S., Bauer, S., and Engelhardt, B. E. (2025). Preference-guided diffusion for multi-objective offline optimization. arXiv preprint arXiv:2503.17299. Artemyev, V., Gubaeva, A., Paremskaia, A. I., Dzhioeva, A. A., Deviatkin, A., Feoktistova, S. G., Mityaeva, O., and Volchkov, P. Y. (2024). Synthetic promoters in gene therapy: Design approaches, features and applications. Cells, 13(23):1963. Belakaria, S., Deshwal, A., and Doppa, J. R. (2021). Output space entropy search framework for multi-objective bayesian optimization. Journal of artificial intelligence research, 72:667715. Beliakov, G. and Lim, K. F. (2007). Challenges of continuous global optimization in molecular structure prediction. European journal of operational research, 181(3):11981213. Beume, N., Naujoks, B., and Emmerich, M. (2007). Sms-emoa: Multiobjective selection based on dominated hypervolume. European journal of operational research, 181(3):16531669. Bigi, A., Lombardo, E., Cascella, R., and Cecchi, C. (2023). The toxicity of protein aggregates: new insights into the mechanisms. Bostrom, J., Lee, C. V., Haber, L., and Fuh, G. (2008). Improving antibody binding affinity and specificity for therapeutic development. In Therapeutic Antibodies: Methods and Protocols, pages 353376. Springer. Bushuiev, A., Bushuiev, R., Kouba, P., Filkin, A., Gabrielova, M., Gabriel, M., Sedlar, J., Pluskal, T., Damborsky, J., Mazurenko, S., et al. (2023). Learning to design protein-protein interactions with enhanced generalization. arXiv preprint arXiv:2310.18515. Campbell, A., Yim, J., Barzilay, R., Rainforth, T., and Jaakkola, T. (2024). Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. In Forty-first International Conference on Machine Learning. Chen, S., Cao, Z., and Jiang, S. (2009). Ultra-low fouling peptide surfaces derived from natural amino acids. Biomaterials, 30(29):58925896. Coello, C. C. and Lechuga, M. S. (2002). Mopso: proposal for multiple objective particle swarm optimization. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC02 (Cat. No. 02TH8600), volume 2, pages 10511056. IEEE. Davis, O., Kessler, S., Petrache, M., Ceylan, I., Bronstein, M., and Bose, J. (2024). Fisher flow matching for generative modeling over discrete data. Advances in Neural Information Processing Systems, 37:139054 139084. Deb, K. (2011). Multi-objective optimisation using evolutionary algorithms: an introduction. In Multi-objective evolutionary optimisation for product design and manufacturing, pages 334. Springer. Deb, K. and Jain, H. (2013). An evolutionary many-objective optimization algorithm using reference-pointbased nondominated sorting approach, part i: solving problems with box constraints. IEEE transactions on evolutionary computation, 18(4):577601. Dunn, I. and Koes, D. R. (2024). Exploring discrete flow matching for 3d de novo molecule generation. ArXiv, pages arXiv2411. DAloisio, V., Dognini, P., Hutcheon, G. A., and Coxon, C. R. (2021). Peptherdia: database and structural composition analysis of approved peptide therapeutics and diagnostics. Drug Discovery Today, 26(6):1409 1419. 12 Emmerich, M. and Klinkenberg, J.-w. (2008). The computation of the expected improvement in dominated hypervolume of pareto front approximations. Rapport technique, Leiden University, 34:73. Fosgerau, K. and Hoffmann, T. (2015). Peptide therapeutics: current status and future directions. Drug discovery today, 20(1):122128. Frisby, T. S. and Langmead, C. J. (2021). Bayesian optimization with evolutionary and structure-based regularization for directed protein evolution. Algorithms for Molecular Biology, 16(1):13. Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Synnaeve, G., Adi, Y., and Lipman, Y. (2024). Discrete flow matching. Advances in Neural Information Processing Systems, 37:133345133385. Gruver, N., Stanton, S., Frey, N., Rudner, T. G., Hotzel, I., Lafrance-Vanasse, J., Rajpal, A., Cho, K., and Wilson, A. G. (2023). Protein design with guided discrete diffusion. Advances in neural information processing systems, 36:1248912517. Guntuboina, C., Das, A., Mollaei, P., Kim, S., and Barati Farimani, A. (2023). Peptidebert: language model based on transformers for peptide property prediction. The Journal of Physical Chemistry Letters, 14(46):1042710434. Han, X., Shan, C., Shen, Y., Xu, C., Yang, H., Li, X., and Li, D. (2023). Training-free multi-objective diffusion model for 3d molecule generation. In The Twelfth International Conference on Learning Representations. Hastings, W. K. (1970). Monte carlo sampling methods using markov chains and their applications. Jain, M., Raparthy, S. C., Hernández-Garcıa, A., Rector-Brooks, J., Bengio, Y., Miret, S., and Bengio, E. (2023). Multi-objective gflownets. In International conference on machine learning, pages 1463114653. PMLR. Jain, S., Gupta, S., Patiyal, S., and Raghava, G. P. (2024). Thpdb2: compilation of fda approved therapeutic peptides and proteins. Drug Discovery Today, page 104047. Knowles, J. (2006). Parego: hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE transactions on evolutionary computation, 10(1):5066. Komp, E., Phillips, C., Lee, L. M., Fallin, S. M., Alanzi, H. N., Zorman, M., McCully, M. E., and Beck, D. A. (2025). Neural network conditioned to produce thermophilic protein sequences can increase thermal stability. Scientific Reports, 15(1):14124. Kreiser, R. P., Wright, A. K., Block, N. R., Hollows, J. E., Nguyen, L. T., LeForte, K., Mannini, B., Vendruscolo, M., and Limbocker, R. (2020). Therapeutic strategies to reduce the toxicity of misfolded protein oligomers. International journal of molecular sciences, 21(22):8651. Li, Y., Zhang, L., and Liu, Z. (2018). Multi-objective de novo drug design with conditional graph generative model. Journal of cheminformatics, 10:124. Lin, X., Liu, Y., Zhang, X., Liu, F., Wang, Z., and Zhang, Q. (2024a). Few for many: Tchebycheff set scalarization for many-objective optimization. arXiv preprint arXiv:2405.19650. Lin, X., Zhang, X., Yang, Z., Liu, F., Wang, Z., and Zhang, Q. (2024b). Smooth tchebycheff scalarization for multi-objective optimization. arXiv preprint arXiv:2402.19078. Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., et al. (2023). Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130. Liu, M., Zhang, X., Xie, C., Donahue, K., and Zhao, H. (2024). Online mirror descent for tchebycheff scalarization in multi-objective optimization. arXiv preprint arXiv:2410.21764. Liu, X., Gong, C., and qiang liu (2023). Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations. Mathur, D., Prakash, S., Anand, P., Kaur, H., Agrawal, P., Mehta, A., Kumar, R., Singh, S., and Raghava, G. P. (2016). Peplife: repository of the half-life of peptides. Scientific reports, 6(1):36617. Michael, R., Bartels, S., González-Duque, M., Zainchkovskyy, Y., Frellsen, J., Hauberg, S., and Boomsma, W. (2024). continuous relaxation for discrete bayesian optimization. arXiv preprint arXiv:2404.17452. Miettinen, K. (1999). Nonlinear multiobjective optimization. Kluwer, Boston, USA. Mohr, S. E., Hu, Y., Ewen-Campen, B., Housden, B. E., Viswanatha, R., and Perrimon, N. (2016). Crispr guide rna design for research applications. The FEBS journal, 283(17):32323238. 13 Naseri, G. and Koffas, M. A. (2020). Application of combinatorial optimization strategies in synthetic biology. Nature communications, 11(1):2446. Nehdi, A., Samman, N., Aguilar-Sánchez, V., Farah, A., Yurdusev, E., Boudjelal, M., and Perreault, J. (2020). Novel strategies to optimize the amplification of single-stranded dna. Frontiers in Bioengineering and Biotechnology, 8:401. Nisonoff, H., Xiong, J., Allenspach, S., and Listgarten, J. (2025). Unlocking guidance for discrete state-space diffusion and flow models. Proceedings of the 13th International Conference on Learning Representations (ICLR). Paria, B., Kandasamy, K., and Póczos, B. (2020). flexible framework for multi-objective bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence, pages 766776. PMLR. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in python. J. Mach. Learn. Res., 12(null):28252830. Peebles, W. and Xie, S. (2022). Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748. Pirtskhalava, M., Vishnepolsky, B., and Grigolava, M. (2013). Transmembrane and antimicrobial peptides. hydrophobicity, amphiphilicity and propensity to aggregation. arXiv preprint arXiv:1307.6160. Rinauro, D. J., Chiti, F., Vendruscolo, M., and Limbocker, R. (2024). Misfolded protein oligomers: Mechanisms of formation, cytotoxic effects, and pharmacological approaches against protein misfolding diseases. Molecular Neurodegeneration, 19(1):20. Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image In Medical image computing and computer-assisted interventionMICCAI 2015: 18th segmentation. international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer. Sahoo, S. S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J. T., Rush, A., and Kuleshov, V. (2024). Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems. Schmidt, H., Zhang, M., Chakarov, D., Bansal, V., Mourelatos, H., Sánchez-Rivera, F. J., Lowe, S. W., Ventura, A., Leslie, C. S., and Pritykin, Y. (2025). Genome-wide crispr guide rna design and specificity analysis with guidescan2. Genome biology, 26(1):125. Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and De Freitas, N. (2015). Taking the human out of the loop: review of bayesian optimization. Proceedings of the IEEE, 104(1):148175. Sharma, N., Naorem, L. D., Jain, S., and Raghava, G. P. (2022). Toxinpred2: an improved method for predicting toxicity of proteins. Briefings in bioinformatics, 23(5):bbac174. Sousa, T., Correia, J., Pereira, V., and Rocha, M. (2021). Combining multi-objective evolutionary algorithms with deep generative models towards focused molecular design. In Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 79, 2021, Proceedings 24, pages 8196. Springer. Stanton, S., Maddox, W., Gruver, N., Maffettone, P., Delaney, E., Greenside, P., and Wilson, A. G. (2022). Accelerating bayesian optimization for biological sequence design with denoising autoencoders. In International conference on machine learning, pages 2045920478. PMLR. Stark, H., Jing, B., Wang, C., Corso, G., Berger, B., Barzilay, R., and Jaakkola, T. (2024). Dirichlet flow matching with applications to dna sequence design. Proceedings of the 41st International Conference on Machine Learning (ICML). Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. (2024). Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. Sun, R., Feng, D., Yang, S., Wang, Y., and Wang, H. (2025). Evolutionary training-free guidance in diffusion model for 3d multi-objective molecular generation. arXiv preprint arXiv:2505.11037. Swanson, R. (2014). Long live peptidesevolution of peptide half-life extension technologies and emerging hybrid approaches. Drug Discovery World, 15:5761. Tang, S., Zhang, Y., and Chatterjee, P. (2025a). Gumbel-softmax flow matching with straight-through guidance for controllable biological sequence generation. arXiv preprint arXiv:2503.17361. Tang, S., Zhang, Y., and Chatterjee, P. (2025b). Peptune: De novo generation of therapeutic peptides with multi-objective-guided discrete diffusion. Proceedings of the 41st International Conference on Machine Learning (ICML). Tominaga, M., Shima, Y., Nozaki, K., Ito, Y., Someda, M., Shoya, Y., Hashii, N., Obata, C., MatsumotoKitano, M., Suematsu, K., et al. (2024). Designing strong inducible synthetic promoters in yeasts. Nature Communications, 15(1):10653. Trott, O. and Olson, A. J. (2010). Autodock vina: improving the speed and accuracy of docking with new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455461. Tsuboyama, K., Dauparas, J., Chen, J., Laine, E., Mohseni Behbahani, Y., Weinstein, J. J., Mangan, N. M., Ovchinnikov, S., and Rocklin, G. J. (2023). Mega-scale experimental analysis of protein folding stability in biology and design. Nature, 620(7973):434444. Tu, B., Kantas, N., Lee, R. M., and Shafei, B. (2023). Multi-objective optimisation via the r2 utilities. arXiv preprint arXiv:2305.11774. Ueno, T., Rhone, T. D., Hou, Z., Mizoguchi, T., and Tsuda, K. (2016). Combo: An efficient bayesian optimization library for materials science. Materials discovery, 4:1821. Wang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., and Gu, Q. (2024). Diffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567. Yang, J., Roe, S. M., Cliff, M. J., Williams, M. A., Ladbury, J. E., Cohen, P. T. W., and Barford, D. (2004). Molecular basis for tpr domain-mediated regulation of protein phosphatase 5. The EMBO Journal, 24(1):110. Yao, Y., Pan, Y., Li, J., Tsang, I., and Yao, X. (2024). Proud: Pareto-guided diffusion model for multi-objective generation. Machine Learning, 113(9):65116538. Yoo, J., Kim, W., and Hong, S. (2025). Redi: Rectified discrete flow. arXiv preprint arXiv:2507.15897. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. (2020). Gradient surgery for multi-task learning. Advances in neural information processing systems, 33:58245836. Yuan, Y., Chen, C., Pal, C., and Liu, X. (2024). Paretoflow: Guided flows in multi-objective optimization. arXiv preprint arXiv:2412.03718. Zhang, C., Zhang, X., Freddolino, P. L., and Zhang, Y. (2024). Biolip2: an updated structure database for biologically relevant ligandprotein interactions. Nucleic Acids Research, 52(D1):D404D412. Zhang, M., Tochio, H., Zhang, Q., Mandal, P., and Li, M. (1999). Solution structure of the extended neuronal nitric oxide synthase pdz domain complexed with an associated peptide. Nature Structural Biology, 6(5):417421. Zhang, Q. and Li, H. (2007). Moea/d: multiobjective evolutionary algorithm based on decomposition. IEEE Transactions on evolutionary computation, 11(6):712731. Zhang, R., Wu, H., Xiu, Y., Li, K., Chen, N., Wang, Y., Wang, Y., Gao, X., and Zhou, F. (2023). Pepland: large-scale pre-trained peptide representation model for comprehensive landscape of both canonical and non-canonical amino acids. arXiv preprint arXiv:2311.04419. Zhou, R., Jiang, Z., Yang, C., Yu, J., Feng, J., Adil, M. A., Deng, D., Zou, W., Zhang, J., Lu, K., et al. (2019). All-small-molecule organic solar cells with over 14% efficiency by optimizing hierarchical morphologies. Nature communications, 10(1):5393. Zitzler, E., Laumanns, M., and Thiele, L. (2001). Spea2: Improving the strength pareto evolutionary algorithm. TIK report, 103. Zitzler, E. and Thiele, L. (1998). Multiobjective optimization using evolutionary algorithmsa comparative case study. In International conference on parallel problem solving from nature, pages 292301. Springer."
        },
        {
            "title": "Supplementary Information",
            "content": "S1 Theoretical Guarantees In this section, we establish that AReUReDi converges to Pareto-optimal solutions while preserving coverage of the entire Pareto front. We assume throughout that the state space is finite, all objective functions sn are bounded, and their normalized versions sn map to [0, 1]. S1.1 Preliminary Definitions Definition (Pareto Optimality). state is Pareto optimal if there exists no such that sn(y) sn(x) for all {1, . . . , } with strict inequality for at least one n. Definition (Pareto Front). The Pareto front is = {x : is Pareto optimal}. Definition (Interior Weight Vector). weight vector ω 1 is interior if ωn > 0 for all n. S1.2 Main Theoretical Results Theorem (Invariance). The Markov kernel defined by the Locally Balanced Proposal (LBP) and Metropolis Hastings update leaves the distribution invariant for every guidance strength η > 0 and weight vector ω 1. πη,ω(x) p1(x) exp(cid:0)ηSω(x)(cid:1) Proof. We prove this in two steps: first showing that single-coordinate updates preserve detailed balance, then that random-scan mixtures preserve invariance. Step 1: Single-coordinate detailed balance. Let and differ only at coordinate i, where some token y. The proposal probability is = for where ri(y; xt) = Wηt,ω(x(iy) Wηt,ω(xt) The acceptance probability is ) qi(y x) = pi t(y xt)g(ri(y; xt)) zcandidates pi t(z xt)g(ri(z; xt)) , (cid:80) and satisfies g(u) = g(1/u). αi(x, x) = min (cid:26) 1, πη,ω(x)qi(xi x) πη,ω(x)qi(y x) (cid:27) . By the symmetry property of and the construction of the proposal, we have qi(y x) qi(xi x) = Wη,ω(x) Wη,ω(x) . Since πη,ω(x) = 1p1(x)Wη,ω(x), it follows that πη,ω(x)qi(xi x) πη,ω(x)qi(y x) = 1. Therefore, αi(x, x) = 1 and detailed balance is satisfied. i=1 Ki(x, x), where Ki is the Step 2: Random-scan mixture. The overall kernel is K(x, x) = 1 kernel for updating coordinate i. Since each Ki satisfies detailed balance with respect to πη,ω, their convex combination also satisfies detailed balance and hence preserves invariance. (cid:80)L Theorem (Convergence to Pareto Front). Fix any ω int 1 with strictly positive entries and let Sω(x) = minn ωnsn(x). If η , samples drawn from πη,ω(x) p1(x) exp(ηSω(x)) concentrate on the set and every element of Fω is Pareto optimal. Fω = arg max Sω(x), Proof. Step 1: Maximizers of Sω are Pareto optimal. Suppose Fω but is not Pareto optimal. Then there exists with sn(y) sn(x) n, and sm(y) > sm(x) for some m. Since ωn > 0 for all n, multiplying preserves inequalities. If is the bottleneck coordinate of x, then Sω(y) > Sω(x), contradiction. Otherwise, equality requires special weight alignments (measure zero). Thus maximizers are Pareto optimal almost surely. Step 2: Concentration as η . Let / Fω, ω = maxx Sω(x) and ω = ω maxx /Fω Sω(x) > 0. Then for πη,ω(x) eηω p1(x) (cid:80) p1(z) zFω . Summing gives πη,ω(S Fω) 0 as η . Hence the mass concentrates on Fω. Theorem (Pareto Point Representability). For every Pareto-optimal state there exists ω 1 such that arg maxx Sω(x). Moreover, if sn(x) > 0 for all n, then can be made the unique maximizer. Proof. If sn(x) > 0, define ωn = 1/sn(x) k=1 1/sk(x) (cid:80)N . Then Sω(x) = If some sn(x) = 0, perturb objectives by ε > 0 and take the limit. 1 1/sk(x) (cid:80) , and for any = x, some satisfies sm(y) < sm(x), implying Sω(y) < Sω(x). Theorem (Coverage Guarantee). Let µ be any probability distribution with full support on int 1. If ω µ and η , then the induced sampler visits every Pareto-optimal state with positive probability. Proof. By representability, each Pareto point maximizes Sω for some interior ω. By continuity, there exists neighborhood Ux where remains optimal. Since µ(Ux ) > 0, randomizing ω ensures is visited with positive probability in the high-η limit. Remark. The guarantees hold for any finite and bounded objectives. In practice, convergence depends on the chain mixing rate, the annealing schedule for η, and the choice of balancing function g. S2 Base Model Details S2.1 PepReDi Model Architecture. The backbone of PepReDi is built on Diffusion Transformer (DiT) framework implemented within Masked Diffusion Language Model (MDLM) paradigm (Peebles and Xie, 2022; Sahoo et al., 2024). Input amino acid sequences are transformed to discrete tokens using the ESM-2-650M tokenizer (Lin et al., 2023). Tokenized amino acid sequences and time-steps are converted to continuous embedding vectors using two separate layers, which are then fused and processed by stacked DiT transformer blocks equipped with multi-head self-attention to capture long-range dependencies in the amino-acid sequence. Residual connections and layer normalization stabilize the training dynamics, and final projection layer outputs token logits for each position. Dataset Curation. The dataset for PepReDi training was curated from the PepNN, BioLip2, and PPIRef dataset (Abdin et al., 2022; Zhang et al., 2024; Bushuiev et al., 2023). All peptides from PepNN and BioLip2 were included, along with sequences from PPIRef ranging from 6 to 49 amino acids in length. The dataset was divided into training, validation, and test sets at an 80/10/10 ratio. Training Strategy. Training was conducted on single node equipped with one NVIDIA GPU and 128 GB of GPU memory using the SLURM workload manager. The model was trained for 100 epochs using the Adam optimizer and learning rate of 1e-4 with weight decay of 1e-5. learning rate scheduler with 10 warm-up epochs and cosine decay was used, with initial and minimum learning rates both 1e-5. The network architecture included model dimension of 512, 6 transformer layers, and 8 attention heads, with vocabulary size of 24 and maximum sequence length of 100 tokens. Conditional total correlation estimation 17 was performed using 20 batches and 50 samples per batch to monitor rectification quality during training. The model checkpoint with the lowest total correlation was saved. For training rectified models, the same hyperparameter setting was applied, except for the loaded pre-trained model checkpoint and the weight decay being increased to 2e-5. Dynamic Batching. To enhance computational efficiency and manage variable-length token sequences, we implemented dynamic batching. Drawing inspiration from ESM-2s approach (Lin et al., 2023), input peptide sequences were sorted by length to optimize GPU memory utilization, with maximum token size of 100 per GPU. Rectification. The trained model applied 16 sampling steps to generate 10k sequences for each peptide length, ranging from 6 to 49, with temperature hyperparameter set to 1. After generation, dynamic batching was used to optimize GPU memory utilization for future rectified training. S2.2 SMILESReDi Model Architecture. SMILESReDi follows the ReDi paradigm and uses Diffusion Transformer (DiT) backbone embedded in Masked Diffusion Language Model (MDLM) design to generate molecular SMILES sequences (Peebles and Xie, 2022; Sahoo et al., 2024). Input SMILES sequences are transformed to discrete tokens using the PeptideCLM -23M tokenizer. Tokenized amino acid sequences and time-steps are converted to continuous embedding vectors using two separate layers. Both embeddings are then fused and processed by stacked DiT transformer blocks that incorporate Rotary Positional Embeddings (RoPE) and multi-head attention modules to capture long-range structural dependencies while preserving positional information (Su et al., 2024). final layer normalization and linear projection outputs token logits for each position. Time-dependent bond-aware noising schedule. Peptide SMILES share conserved backbone of alternating carbonyl and amide groups connected by chemically constrained peptide bonds, while their side chains remain highly diverse. Standard discrete flow matching can corrupt these critical bond tokens too early, hindering the flow from recovering the backbone along the probability path. Inspired by previous work in bond-dependent masking, we devised time-dependent bond-aware noising schedule that preserves backbone tokens longer than side-chain tokens, allowing the model to reconstruct the invariant scaffold before generating variable side chains. Specifically, for each position with bond indicator bj {0, 1}, the time-t marginal of the probability path is pt(x(j) x(j) 0 , x(j) 1 ) = (cid:2) bjtγ + (1 bj)t (cid:3) δx(j) + (cid:2) 1 bjtγ (1 bj)t (cid:3) δx(j) with the indicated mixture coefficient and to x(j) 0 , 1 0 so each token is equal to x(j) 1 backbone tokens (bj = 1) transition more slowly than non-bond tokens along the DFM probability path. Training Strategy. The training is conducted on 4*A6000 NVIDIA RTX 6000 Ada GPU system with 48 GB of VRAM for 5 epochs. The model checkpoint with the lowest evaluation loss was saved. The Adam optimizer was employed with learning rate of 1e-4. learning rate scheduler with 10% total training steps and cosine decay was used, with initial and minimum learning rates both 1e-5. The network architecture included model dimension of 768, 8 transformer layers, and 8 attention heads. Gradient clip value was set to 1.0 and γ to 2.0 in the time-dependent bond-aware noising schedule. For training rectified models, the same hyperparameter setting was applied, except for the loaded pre-trained model checkpoint and the total training epochs set to 10. otherwise, ensuring that [0, 1], γ > 1, Rectification. The trained model applied 100 sampling steps to generate 100 sequences for each peptide length, ranging from 4 to 1035, with temperature hyperparameter set to 1. After generation, dynamic batching was used to optimize GPU memory utilization for future rectified training. Evaluation Metrics. Validity is defined as the fraction of peptide SMILES that pass the SMILES2PEPTIDE filter (Tang et al., 2025b), indicating that it translates to synthesizable peptide. Uniqueness is defined as the fraction of mutually distinct peptide SMILES. Diversity is defined as one minus the average Tanimoto similarity between the Morgan fingerprints of every pair of generated sequences, which measures the similarity in structure across generated peptides. Diversity = 1 1 (cid:0)Ngenerated 2 (cid:1) (cid:88) i,j (xi) (xj) (xi) + (xj) (xi) (xj) 18 where (xi) and (xj) are the 2048-dimensional Morgan fingerprint with radius 3 for pair of generated sequences xi and xj. Similarity to Nearest Neighbor (SNN) is defined as the maximum Tanimoto similarity between generated sequence xi with sequence in the dataset xj. SNN = max jD (cid:18) (xi) (xj) (xi) + (xj) (xi) (xj) (cid:19) S2.3 PepDFM Model Architecture. The base model is time-dependent architecture based on U-Net (Ronneberger et al., 2015). It uses two separate embedding layers for sequence and time, followed by five convolutional blocks with varying dilation rates to capture temporal dependencies, while incorporating time-conditioning through dense layers. The final output layer generates logits for each token. We used polynomial convex schedule with polynomial exponent of 2.0 for the mixture discrete probability path in the discrete flow matching. Dataset Curation. The dataset for PepDFM training was curated from the PepNN, BioLip2, and PPIRef dataset (Abdin et al., 2022; Zhang et al., 2024; Bushuiev et al., 2023). All peptides from PepNN and BioLip2 were included, along with sequences from PPIRef ranging from 6 to 49 amino acids in length. The dataset was divided into training, validation, and test sets at an 80/10/10 ratio. Training Strategy. The training is conducted on 2xH100 NVIDIA NVL GPU system with 94 GB of VRAM for 200 epochs with batch size 512. The model checkpoint with the lowest evaluation loss was saved. The Adam optimizer was employed with learning rate 1e-4. learning rate scheduler with 20 warm-up epochs and cosine decay was used, with initial and minimum learning rates both 1e-5. The embedding dimension and hidden dimension were set to be 512 and 256 respectively for the base model. Performance. PepDFM achieved validation loss of 3.1051. Its low generalized KL loss during evaluation demonstrates PepDFMs strong capability to generate sequences with high biological plausibility (Gat et al., 2024). S3 Objective Description In this work, five key property objectives are considered in the peptide binder tasks: hemolysis, non-fouling, solubility, half-life, and binding affinity. Each of these properties plays crucial role in optimizing the therapeutic potential of peptides. Hemolysis refers to the peptides ability to minimize red blood cell lysis, ensuring safe systemic circulation (Pirtskhalava et al., 2013). Non-fouling properties describe the peptides resistance to unwanted interactions with biomolecules, thus enhancing its stability and bioavailability in vivo (Chen et al., 2009). Solubility is critical for ensuring adequate peptide dissolution in biological fluids, directly influencing its absorption and therapeutic efficacy (Fosgerau and Hoffmann, 2015). Half-life indicates the duration for which the peptide remains active in circulation, which is vital for reducing dosing frequency (Swanson, 2014). Finally, binding affinity measures the strength of the peptides interaction with its target, directly correlating to its biological activity and potency in therapeutic applications (Bostrom et al., 2008). S4 Score Model Details We applied the score models from (Tang et al., 2025b) to guide the generation of chemically-modified peptide binders. We now introduce the score model developed for the wild-type peptide binder generation task. We collected hemolysis (9,316), non-fouling (17,185), solubility (18,453), and binding affinity (1,781) data for classifier training from the PepLand and PeptideBERT datasets (Zhang et al., 2023; Guntuboina et al., 2023). All sequences taken are wild-type L-amino acids and are tokenized and represented by the ESM-2 protein language model (Lin et al., 2023). S4.1 Boosted Trees for Classification For hemolysis, non-fouling, and solubility classification, we trained XGBoost boosted tree models for logistic regression. We split the data into 0.8/0.2 train/validation using stratified splits from scikit-learn (Pedregosa et al., 2011) and generated mean-pooled ESM-2-650M (Lin et al., 2023) embeddings as input features to the model. We ran 50 trials of OPTUNA (Akiba et al., 2019) search to determine the optimal XGBoost 19 hyperparameters (Table S1), tracking the best binary classification F1 scores. The best models for each property reached F1 scores of 0.58, 0.71, and 0.68 on the validation sets respectively. Table S1: XGBoost Hyperparameters for Classification Hyperparameter Value/Range Objective Lambda Alpha Colsample by Tree Subsample Learning Rate Max Depth Min Child Weight Tree Method binary:logistic [1e8, 10.0] [1e8, 10.0] [0.1, 1.0] [0.1, 1.0] [0.01, 0.3] [2, 30] [1, 20] hist S4.2 Binding Affinity Score Model We developed an unpooled reciprocal attention transformer model to predict protein-peptide binding affinity, leveraging latent representations from the ESM-2 650M protein language model (Lin et al., 2023). Instead of relying on pooled representations, the model retains unpooled token-level embeddings from ESM-2, which are passed through convolutional layers followed by cross-attention layers. The binding affinity data were split into 0.8/0.2 ratio, maintaining similar affinity score distributions across splits. We used OPTUNA (Akiba et al., 2019) for hyperparameter optimization, tracing validation correlation scores. The final model was trained for 50 epochs with learning rate of 3.84e-5, dropout rate of 0.15, 3 initial CNN kernel layers (dimension 384), 4 cross-attention layers (dimension 2048), and shared prediction head (dimension 1024) in the end. The classifier reached 0.64 Spearmans correlation score on validation data. S4.3 Half-Life Score Model Dataset Curation. The half-life dataset is curated from three publicly available datasets: PEPLife, PepTherDia, and THPdb2 (Mathur et al., 2016; DAloisio et al., 2021; Jain et al., 2024). Data related to human subjects were selected, and entries with missing half-life values were excluded. After removing duplicates, the final dataset consists of 105 entries. Pre-training on stability data. Given the small size of the half-life dataset, which is insufficient for training model to capture the underlying data distribution, we first pre-trained score model on larger stability dataset to predict peptide stability (Tsuboyama et al., 2023). The model consists of three linear layers with ReLU activation functions, and dropout rate of 0.3 was applied. The model was trained on 2xH100 NVIDIA NVL GPU system with 94 GB of VRAM for 50 epochs. The Adam optimizer was employed with learning rate of 1e-2. learning rate scheduler with 5 warm-up epochs and cosine decay was used, with initial and minimum learning rates both 1e-3. After training, the model achieved validation Spearmans correlation of 0.7915 and an R2 value of 0.6864, demonstrating the reliability of the stability score model. Fine-tuning on half-life data. The pre-trained stability score model was subsequently fine-tuned on the half-life dataset. Since half-life values span wide range, the model was adapted to predict the base-10 logarithm of the half-life (h) values to stabilize the learning process. After fine-tuning, the model achieved validation Spearmans correlation of 0.8581 and an R2 value of 0.5977. S5 Sampling Details Score Model Settings. We cap the predicted log-scale half-life at 2 (i.e., 100 h) to prevent it from dominating the optimization and ensure balanced trade-offs across all properties. For the remaining objectives, hemolysis, non-fouling, solubility, and binding affinity, we directly employ their model outputs during sampling. 20 Table S2: Adding sampling constraint greatly improves AReUReDis performance. Wild-type binders for two protein targets (PDB 8CN1 and 4EBP2) were generated with or without sampling constraint using the same number of generation steps. The table reports the average score for each objective, calculated from 100 generated binders per setting. The best score for each objective is highlighted in bold. Target Method Hemolysis Non-Fouling Solubility Half-Life Affinity 8CN1 4EBP2 w/o constraints w/ constraints w/o constraints w/ constraints 0.8650 0.9213 0.8879 0.9356 0.4782 0.8676 0.4288 0.8767 0.4627 0. 0.4257 0.8692 2.54 44.70 1.8781 53.95 5.2412 5.5143 5.7132 6.4571 Wild-Type Peptide Binder Generation Task Settings. The total sampling steps are set to 20 multiplied by the binder length. All possible candidate token transitions are evaluated during each sampling step. We applied the same weight for each objective in all wild-type peptide binder generation tasks. Chemically-Modified Peptide Binder Generation Task Settings. The total sampling steps are set to 128. With vocabulary size of 586, evaluating all the possible candidate tokens is too computationally intensive. We therefore only evaluated the top 200 candidate tokens during each sampling step. We applied weight 0.7 for binding affinity, and 0.1 for hemolysis, non-fouling, and solubility, respectively. Instead of random initialization, the initial sequences x0 are sampled from the pre-trained SMILESReDi1 with 16 generation steps. During generation, AReUReDi rejects any transitions that will make the SMILES sequence an invalid peptide. Table S3: Ablation results for wild-type peptide binder design targeting PDB 7LUL with different guidance settings. For each setting, 100 binders of length 7 were designed. Guidance Settings Hemolysis Solubility Affinity Hemolysis Solubility Affinity 0.9389 0.8964 0.9502 0.9535 0.8812 0. 0.9802 0.8431 0.9398 0.9465 0.4013 0. 0.2877 0.9725 0.6135 0.5810 6.2559 6. 6.9798 5.2611 7.5057 5.2449 5.0985 4. 21 Table S4: Ablation results for wild-type peptide binder design targeting PDB CLK1 with different guidance settings. For each setting, 100 binders of length 12 were designed. Guidance Settings Non-Fouling Half-Life (h) Affinity Non-Fouling Half-Life (h) Affinity 0. 0.2902 0.9365 0.9479 0.9625 0.3540 0. 0.4988 74.04 96.59 1.33 75.68 1. 100.00 2.96 1.82 6.8099 7.3906 7. 6.3437 6.2319 6.4116 8.6580 5.4739 Table S5: Rectification of the base generation model improves AReUReDis performance. Wild-type binders for two protein targets (PDB 5AZ8 and AMHR2) were generated using AReUReDi with three different base models: PepDFM, PepReDi (without rectification), and PepReDi3 (with three rounds of rectification). The table reports the average score for each objective, calculated from 100 generated binders per setting. The best score for each objective is highlighted in bold. Target Base Model Hemolysis Non-Fouling Solubility Half-Life Affinity 5AZ8 AMHR2 PepDFM PepReDi PepReDi3 PepDFM PepReDi PepReDi 0.9296 0.9326 0.9293 0.9412 0.9127 0.9420 0.8867 0.8759 0.8732 0.8774 0.8602 0.8914 0.8743 0.8572 0.8605 0.8612 0.8460 0. 37.30 50.16 58.33 47.84 50.92 63.34 6.2291 6.4391 6.2792 7.2373 7.0101 7.2533 Table S6: Annealed guidance strength improves AReUReDis performance. Wild-type binders for two protein targets (PDB 1DDV and P53) were generated under four guidance schedules: (1) fixed at the minimum strength ηmin = 1.0, (2) fixed at the maximum strength ηmax = 20.0, (3) fixed at the midpoint 1 2 (ηmin + ηmax) = 10.5, and (4) an annealed schedule where ηt increases from ηmin to ηmax over optimization steps. The table reports the average score for each objective, calculated from 100 generated binders per setting. The best score for each objective is highlighted in bold. Target Method Hemolysis Non-Fouling Solubility Half-Life (h) Affinity 1DDV P53 η = ηmin η = ηmax 2 (ηmin + ηmax) (cid:1) ηt = ηmin + (cid:0)ηmax ηmin η = 1 η = ηmin η = ηmax 2 (ηmin + ηmax) (cid:1) ηt = ηmin + (cid:0)ηmax ηmin η = 1 1 1 0.9130 0.9156 0.9108 0. 0.9335 0.9293 0.9294 0.9353 0.8575 0.8512 0.8641 0.8545 0.8800 0.8693 0.8713 0.8818 0.8429 0.8479 0.8544 0.8565 0.8706 0.8657 0.8653 0.8785 38.70 40.27 40.43 44. 49.97 61.76 59.43 62.83 5.3554 5.4359 5.5396 5.4482 6.2538 6.3043 6.3060 6.3508 22 Figure S1: Complex structures of target proteins with pre-existing binders. (A)-(B) 5AZ8 (C)-(D) 7JVS. Each panel shows the complex structure of the target with either an AReUReDi-designed binder or its pre-existing binder. For each binder, five property scores are provided, as well as the ipTM score from AlphaFold3 and the docking score from AutoDock VINA. Interacting residues on the target are visualized. Figure S2: Complex structures of target proteins without pre-existing binders. (A)-(C) AMHR2, (D)-(E) EWS::FLI1, (F) MYC, (G) DUSP12. Each panel shows the complex structure of the target with an AReUReDidesigned binder. For each binder, five property scores are provided, as well as the ipTM score from AlphaFold3 and the docking score from AutoDock VINA. Interacting residues on the target are visualized. 24 Figure S3: (A), (C) Example 2D SMILES structure of AReUReDi-designed peptide binders with four property scores for GLP1 and GLAST, respectively. (B), (D) Plots showing the mean scores for each property across the number of iterations during AReUReDis design of binders of length 200 for GLP1 and GLAST, respectively. 25 Figure S4: (A), (C) Example 2D SMILES structure of AReUReDi-designed peptide binders with four property scores for TfR and AMHR2, respectively. (B), (D) Plots showing the mean scores for each property across the number of iterations during AReUReDis design of binders of length 200 for TfR and AMHR2, respectively. Algorithm 1 AReUReDi: Annealed Rectified Updates for Refining Discrete Flows 1: Input: Pre-trained ReDi model pi annealing parameters ηmin, ηmax. t(xt), objective functions s1, . . . , sN , weight vector ω 1, Sample an initial sequence x0 uniformly from the discrete state space Sample or specify weight vector ω 1 2: Output: Sequence xT with multi-objective optimized properties. 3: 4: Initialize: 5: 6: 7: 8: for = 0 to 1 with step size = 1 9: 10: 11: 12: 13: 14: 15: Let Ci be the set of candidate tokens from pi For each candidate token Ci: Step 2: Proposal Generation via Local Balancing Step 1: Annealing and Coordinate Selection Update guidance strength: ηt ηmin + (ηmax ηmin) Select position in the sequence to update: Uniform({1, . . . , L}) t(xt). do 1 16: 17: 18: 19: 20: 21: 22: 23: 1. Compute scalarized reward ratio: ri(y; xt) 2. Compute unnormalized proposal distribution qi(yxt) using balancing function g(): exp(ηt minn ωn sn(x(iy))) exp(ηt minn ωn sn(x)) qi(yxt) pi t(yxt)g(ri(y; xt)) 3. Normalize to get the final proposal distribution qi(yxt). Step 3: Metropolis-Hastings Acceptance Sample candidate token qi(xt). Form the proposed state xprop x(iy). Compute acceptance probability αi(x, xprop): (cid:27) (cid:26) αi(x, xprop) min 1, πηt,ω(xprop)qi(xixprop) πηt,ω(x)qi(yx) , where πηt,ω(z) p1(z) exp (cid:16) ηt min (cid:17) ωnsn(z) With probability αi(x, xprop), accept the proposal: xprop. Update time: + 24: 25: 26: end for 27: Return: Final sequence x1."
        }
    ],
    "affiliations": [
        "Centre for Computational Biology, Duke-NUS Medical School, Singapore",
        "Department of Bioengineering, University of Pennsylvania",
        "Department of Computer and Information Science, University of Pennsylvania"
    ]
}