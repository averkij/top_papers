{
    "paper_title": "DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation",
    "authors": [
        "Hanbo Cheng",
        "Limin Lin",
        "Chenyu Liu",
        "Pengcheng Xia",
        "Pengfei Hu",
        "Jiefeng Ma",
        "Jun Du",
        "Jia Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 1 ] . [ 2 6 2 7 3 1 . 0 1 4 2 : r preprint DAWN: DYNAMIC FRAME AVATAR WITH NONAUTOREGRESSIVE DIFFUSION FRAMEWORK FOR TALKING HEAD VIDEO GENERATION Hanbo Cheng1, Limin Lin1, Chenyu Liu2, Pengcheng Xia2, Pengfei Hu1, Jiefeng Ma1 Jun Du1, Jia Pan2 1University of Science and Technology of China, 2IFLYTEK Research https://hanbo-cheng.github.io/DAWN/"
        },
        {
            "title": "ABSTRACT",
            "content": "Talking head generation intends to produce vivid and realistic talking head videos from single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly available at https: //github.com/Hanbo-Cheng/DAWN-pytorch."
        },
        {
            "title": "INTRODUCTION",
            "content": "Talking head generation aims at synthesizing realistic and expressive talking head from given portrait and audio clip, which is garnering growing interest due to its potential applications in virtual meetings, gaming, and film production. For talking head generation, it is essential that the lip motions in the generated video precisely match the accompanying speech, while maintaining high overall visual fidelity (Guo et al., 2021a). Furthermore, natural coordination between head pose, eye blinking, and the rhythm of the audio is also crucial for convincing output (Liu et al., 2023). Recently, Diffusion Models (DM) (Ho et al., 2020) have achieved significant success in video and image generation tasks (Rombach et al., 2022; Ho et al., 2022b;a; Peebles & Xie, 2023; Ni et al., 2023). However, their application in talking head generation (Shen et al., 2023; Bigioi et al., 2024) still faces several challenges. Although many methods yield high-quality results, most of them rely on autoregressive (AR) (Tian et al., 2024; Ma et al., 2023) or semi-autoregressive (SAR) (Xu et al., 2024b; He et al., 2023) strategies. In each iteration, AR generates one frame, while SAR generates fixed-length video segment. The two strategies significantly slow down the inference speed and fail to adequately utilize contextual information from future frames, which leads to constrained performance and potential error accumulation, especially in long video sequences (Stypułkowski et al., 2024). Authors contributed equally to this research. Corresponding author: Jun Du (jundu@ustc.edu.cn). 1 preprint To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), novel approach that significantly improves both the quality and efficiency of talking head generation. Our approach leverages the DM to generate motion representation sequences from given audio and portrait. These motion representations are subsequently used to reconstruct the video. Unlike other methods, our approach produces videos of arbitrary length in non-autoregressive (NAR) manner. However, employing the NAR strategy to generate long videos often results in either over-smoothing or significant content inconsistencies due to limited extrapolation (Qiu et al., 2023). In the context of talking head generation, we suggest that the models temporal modeling capability is significantly hindered by the strong coupling relationship among multiple motions. Typically, the motions in talking head include (1) lip motions and (2) head pose and blink movements. The temporal dependency of head and blink movements extends over several seconds, far longer than that of lip motions (Zhou et al., 2020). Training models to capture these long-term dependencies requires extensive sequences, thus increasing the difficulty and cost of training. Fortunately, head and blink movements can be represented as low-dimensional vectors (Zhang et al., 2023), enabling the design of lightweight model that learns these long-term dependencies by training on extended sequences. Thus, to further enhance the temporal modeling and extrapolation capabilities of DAWN, we disentangle the motion components involved in talking head videos. Specifically, we use the Audio-to-Video Flow Diffusion Model (A2V-FDM) to learn the implicit mapping between the lips and audio, while generating the head pose and blinks via explicit control signals. Additionally, we propose lightweight Pose and Blink generation Network (PBNet) trained on long sequences, to generate natural pose/blink movements during inference in NAR manner. In this way, we simplify the training of A2V-FDM as well as achieve the long-term dependency modeling of the pose/blink movement. To further strengthen the convergence and extrapolation capabilities of A2V-FDM, we propose two-stage training strategy based on curriculum learning to guide the model in generating accurate lip motion and precise pose/blink movement control. The main contributions of this work are as follows: 1) We present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion) for generating dynamic-length talking head videos from portrait images and audio clips in non-autoregressive (NAR) manner, achieving faster inference speeds and high-quality results. To the best of our knowledge, this is the first NAR solution based on diffusion models designed for general talking head generation. 2) To compensate for the limitations of extrapolation in NAR strategies and enhance the temporal modeling capabilities for long videos, we decouple the motions of the lips, head, and blink, achieving precise control over these movements. 3) We propose the Pose and Blink generation Network (PBNet) to generate natural head pose and blink sequences exclusively from audio in NAR manner. 4) We introduce the Two-stage Curriculum Learning (TCL) strategy to guide the model in mastering lip motion generation and precise pose/blink control, ensuring strong convergence and extrapolation ability."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Audio-driven talking head generation. Initial approaches for talking head generation employed deterministic models to map audio to video streams (Fan et al., 2016), with later methods introducing generative models such as GANs (Isola et al., 2016), VAEs (Kingma & Welling, 2022), and diffusion models (DMs) (Ho et al., 2020). GAN-related methods (Vougioukas et al., 2020; Pumarola et al., 2018; Hong et al., 2022) improved visual realism but faced convergence and mode collapse issues (Xia et al., 2022). Following this, VAEs (Kingma & Welling, 2022) generated 3D priors like 3D Morphable Models (3DMM) (Blanz & Vetter, 1999) followed by high-fidelity rendering (Ren et al., 2021), which limited the realism and vividness. In contrast, DMs have been introduced into talking head generation due to their good convergence, excellent generation performance, and diversity. Stypułkowski et al. (2024) presented DM-based talking head generation solution using an AR strategy to generate videos frame-by-frame iteratively. Subsequently, Tian et al. (2024) improved this AR strategy by incorporating motion conditions extracted by VAE-based network as priors for each iteration, effectively mitigating degradation issues. Concurrently, Xu et al. (2024b); He et al. (2023) advocated for motion modeling instead of image modeling. They utilized DMs to iteratively generate latent motion representations over fixed number of frames in SAR manner, subsequently converting these motion representations into video frames. While most diffusion-based methods produce promising results, their AR or SAR strategies incur slow generation speeds and collapse in long-video generation. Although methods like Tian et al. (2024); Xu et al. (2024b) alleviate issues such as inconsistencies in content across iterations and long video generation collapse, the risk of 2 preprint error accumulation remains unresolved. While methods like Du et al. (2023) use identity-specific NAR strategies, to the best of our knowledge, none have addressed NAR talking head generation for arbitrary identities. Consequently, we propose novel NAR dynamic frame generation framework based on DM, which aims to achieve the low-cost and high-quality rapid generation of realistic talking head video through clip of audio and arbitrary portrait. Audio-driven pose and blink generation. Head pose and blink movements significantly impact the naturalness of talking head videos. However, the mapping from audio to pose and blink movement is one-to-many problem, which presents significant challenge (Xu et al., 2024a; Chen et al., 2020). Early works primarily focused on controlling poses directly using facial landmarks or video references (Zhou et al., 2021; Guo et al., 2021b). However, these approaches require additional guidance information, which impairs the diversity of the results. Later studies considered generating both pose and blink within the context of talking head generation (Zhou et al., 2020). However, simultaneously generating all facial movements can cause interference and ambiguity (Zhang et al., 2023). Therefore, some works attempted to decouple the speakers actions into components like lip, head pose, and blink, using discriminative models to predict these conditions separately (Wang et al., 2021; He et al., 2023). Later, researchers recognized that probabilistic modeling is better suited for the one-to-many mapping relationship, leading to the proposal of VAE-based pose generation framework (Liu et al., 2023). However, most existing pose generation strategies also depend on AR or SAR approaches, negatively impacting efficiency, smoothness, and naturalness. To address these issues, we design VAE-based NAR pose generation method to produce vivid and smooth pose and blink movements while maintaining the NAR generation of the entire framework."
        },
        {
            "title": "3 METHOD",
            "content": "As shown in Figure 1, DAWN is divided into three main parts: (1) the Latent Flow Generator (LFG); (2) the conditional Audio-to-Video Flow Diffusion Model (A2V-FDM); and (3) the Pose and Blink generation Network (PBNet). First, we train the LFG to estimate the motion representation between different video frames in the latent space. Subsequently, the A2V-FDM is trained to generate temporally coherent motion representation from audio. Finally, PBNet is used to generate poses and blinks from audio to control the content in the A2V-FDM. To enhance the models extrapolation ability while ensuring better convergence, we propose novel Two-stage Curriculum Learning (TCL) training strategy. We will first discuss preliminaries, then present the specific details of DAWNs three main components, namely LFG, A2V-FDM, and PBNet in Sections 3.2, 3.3, and 3.4, respectively. Finally, we will introduce the TCL strategy in Section 3.5. 3.1 PRELIMINARIES Task definition. The task of talking head generation involves creating natural and vivid talking head video from two inputs: static single-person portrait, xsrc, and speech sequence, y1:N = {y0, y1, . . . , yN }. The static image xsrc and the speech sequence can originate from any individual, and the output is ˆx1:N = { ˆx0, ˆx1, . . . , ˆxN }, where represents the total number of frames. Diffusion models. Diffusion models generate samples conforming to given data distribution by progressively denoising Gaussian noise (Ho et al., 2020). Let x0 represent data sampled from given distribution q(x0). In the forward diffusion process, Gaussian noise is progressively added to x0 after steps, resulting in noisy data xT (Nichol & Dhariwal, 2021; Song et al., 2020), and the conditional transition distribution at each step is defined as: q(xtx0) = (xt; αt x0, (1 αt)I) (1) where αt = (cid:81)t i=1 αi . The reverse diffusion process gradually recovers the original data from the Gaussian noise xT (0, I), utilizing neural network to predict pθ(xt1xt), where θ represents the parameters of the neural network. The model is trained using the following loss function: Lsimple = Et,x0,ϵ[ϵ ϵθ(xt, t)2] (2) where ϵ is the Gaussian noise added to x0 in the forward diffusion process to obtain xt, and ϵθ(xt, t) is the noise predicted by the model. In video-related tasks, the denoising model can be implemented via 3D U-Net (Ho et al., 2022b; Çiçek et al., 2016). 3 preprint Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet. 3.2 LATENT FLOW GENERATOR The Latent Flow Generator (LFG) is self-supervised training framework designed to model motion information between the source image xsrc and the driving image xdri. As illustrated in Figure 1 (a), LFG consists of three trainable modules: the image encoder E, the flow predictor P, and the image decoder D. During training, xsrc, xdri RHW 3 are images randomly selected from the same video. The image encoder encodes the source image xsrc into latent code zsrc RHzWzCz . The flow predictor estimates dense flow map and blocking map (Siarohin et al., 2021; 2020), corresponding to xsrc and xdri : , = P(xsrc, xdri) (3) The flow map RHzWz2 describes the feature-level movement of xdri relative to xsrc in horizontal and vertical directions. The blocking map RHzWz1 ranging from 0 to 1, indicates the degree of area blocking in the transformation from xsrc to xdri. The flow map is used to perform the affine transformation A, serving as coarse-grained warping of zsrc. Subsequently, the blocking map guides the model in repairing the occlusion area, thereby serving as fine-grained repair. Finally, the image decoder converts the warped latent code into the target image ˆxgen, where the is the element-wise product: ˆxgen = D(A(zsrc, ) m) The LFG is trained in an unsupervised manner and optimized using the following reconstruction loss: LLFG = Lrec( ˆxgen, xdri) (5) (4) 4 preprint where Lrec is multi-resolution reconstruction loss derived from pre-trained VGG-19 network, used to evaluate perceptual differences between ˆxgen and xdri (Johnson et al., 2016). We consider the concatenation of and as zm = [f , m] to represent the motion of xdri relative to xsrc. In this way, we achieve two objectives: 1) finding an effective explicit motion representation zm, which is identity-agnostic and well-supported by physical meaning, and 2) reconstructing xdri from xsrc and zm without the need for full pixel generation. 3.3 CONDITIONAL AUDIO2VIDEO FLOW DIFFUSION MODEL Through the LFG in Section 3.2, we can specify xsrc, and extract the identity-agnostic motion representations zm 1:N of the talking head video clip x1:N as well as the latent code zsrc. Therefore, 1:N = [ ˆf1:N , ˆm1:N ] of each frame we design the A2V-FDM to generate the motion representations ˆzm relative to xsrc: ˆzm 1:N = DM(xsrc, y1:N , ρ1:N ) where the DM refers the diffusion model. After generating ˆzm 1:N , we use the decoder in LFG to reconstruct the ˆzm 1:N to the target video frames, via the Equation 4. The structure of A2V-FDM is illustrated in Figure 1 (c). The A2V-FDM model includes 3D U-Net denoising backbone (Ho et al., 2022b). The residue block in the 3D U-Net contains temporal attention and spatial attention modules, which handle frame-level and pixel-level dependencies respectively. The parameters of the temporal attention module are independent of the input length, so we believe that 3D U-Net can theoretically process video sequences of any length. However, to ease training difficulty, we train on short sequences and aim for long-sequence inference. To enhance the 3D U-Nets extrapolation ability when handling long sequences, we used Rotary Positional Encoding (RoPE) (Su et al., 2023) instead of traditional absolute position embeddings in the temporal attention module. (6) Conditioning. We incorporate the following conditions to control its generative behavior: audio embedding a1:N , pose/blink signal ρ1:N , and source image latent code zsrc. The audio embedding a1:N , extracted from the audio y1:N using Hubert (Hsu et al., 2021), implicitly controls the lip motion. Due to the strict alignment with video frames, we apply the audio embedding to its corresponding image. Additionally, the avatars pose and blink are controlled via explicit signals. The pose is described by 6D vector Ji et al. (2022). During training, the pose is extracted from video using an open-source tool (Guo et al., 2020). For the blink signal, we adopt the aspect ratio of the left and right eyes, following (Zhang et al., 2023). To account for the arbitrary pose and eye-opening degree of the source image xsrc, we use the difference between the current frame xi and the source frame xsrc: ρi = ρi ρsrc, which models the transition of state rather than the state itself. The model is provided with features zsrc to supply facial visual details. Each frames latent code performs cross attention with the audio embedding ai and pose/blink information ρi, respectively. This process injects these conditions into the latent code with different spatial weights, controlling specific regions of the generated content. The image feature zsrc is regarded as global condition and is concatenated directly with the noisy data as the initial input to the 3D U-Net. We also utilize landmarks to create face region mask for xsrc, embedded with lightweight convolutional network, similar to the approach by Tian et al. (2024). This mask adds to the denoising process in the same manner as zsrc. Loss function. We employed the DM regular denoising loss, Lsimple, in Equation 2 to train our model. The synchronization of lip motions with audio is crucial for the talking head task, while the lips often constitute only small portion of the frame. Consequently, during training, we employed landmarks to isolate the lip region by generating lip mask, mlip. We then applied an additional weight, wlip, to the denoising process of this region, similar to Stypułkowski et al. (2024). Ultimately, our loss function is defined as: = Lsimple + wlip (Lsimple mlip) (7) where the symbol denotes element-wise product. 3.4 AUDIO-DRIVEN POSE AND BLINK GENERATION To prevent the generated results from exhibiting overly monotonous and minimal movements, we design separate module, namely the Pose and Blink generation Network (PBNet). As shown in Figure 1 (b), PBNet learns the mapping between audio and pose/blink movements. To maintain the preprint non-autoregressive (NAR) generation capabilities of A2V-FDM, PBNet employs transformer-based Variational Autoencoder (VAE) (Petrovich et al., 2021) to generate variable-length pose and blink sequences. The inputs to PBNet include the initial pose/blink state ρsrc, the residual pose/blink ρ1:N , and the audio embedding a1:N . The transformer encoder Et embeds these inputs into Gaussian-distributed and obtain latent code RN Ch through resampling : µ, logσ = Et(ρsrc, ρ1:N , a1:N ), s.t. (µ, σ) (8) We design to have the same length as ρ to ensure it can carry sufficient information for variablelength inputs. The transformer decoder Dt generates the final pose/blink sequence ˆρ1:N , conditioned on a1:N and ρsrc : ˆρ1:N = Dt(h, a1:N , ρsrc) (9) To enhance the models extrapolation capability, we use RoPE as the positional encoding in the decoder, consistent with A2V-FDM. During training, we apply an MSE-based reconstruction loss Lrec and an adversarial loss LGAN to guide the model in completing basic reconstruction tasks (Isola et al., 2016; Ginosar et al., 2019). Additionally, we employ KL divergence loss LKL to ensure that the latent code closely approximates standard Gaussian distribution. 3.5 TWO-STAGE CURRICULUM LEARNING FOR TALKING HEAD GENERATION Empirical evidence indicates that training our A2V-FDM model solely with fixed-length short video clips leads to inaccurate control of poses and blinks, as well as poor generalization to longer videos. We argue that one-step training approach impedes the models convergence to an optimal solution and fails to achieve satisfactory results in the complex task of talking head generation. To address these issues, we propose an innovative Two-Stage Curriculum Learning (TCL) strategy inspired by the theory of curriculum learning (Bengio et al., 2009). Overall, the goal of the A2V-FDM during the training process can be expressed as: ˆx1:N = D(DM(xsrc, y1:N , ρ1:N )) (10) In the first stage, we set xsrc = x1, and the sequence length = is fixed, relatively small constant. This stage primarily focuses on enabling the model to generate basic lip motions. However, utilizing x1 as the source image often exhibits limited variations in poses and blinks, and using short clips can result in scarcity of training samples with significant pose or blink movements. Therefore, in the second stage, we set xsrc randomly, where is the set of frames in the entire video, to learn control capabilities of large pose transformation. Additionally, differing from stage one, we randomly set [Kmin, Kmax], Kmin > . This approach aims to enhance control over poses and blinks while maintaining precise lip motions, as longer clips contain more diverse pose and blink movements. Training with random-length sequences also helps the model avoid bias towards fixed-length sequences, further enhancing the models extrapolation. 3.6 INFERENCE Our inference process consists of four steps: 1) Extract the audio embedding a1:N from the input audio. 2) Provide the initial pose/blink state ρsrc, extracted from the source image xsrc, to the PBNet. Along with a1:N and latent space vector h1:N sampled from standard Gaussian distribution, PBNet generates the pose/blink sequences ˆρ1:N . 3) Input xsrc, a1:N , and ˆρ1:N into the A2V-FDM, which generates the motion representation sequences ˆzm 1:N . 4) Finally, decode the video sequence ˆx1:N from xsrc and ˆzm 1:N . Our method leverages non-autoregressive (NAR) generation during the inference process. To enhance extrapolation during inference, we utilize local attention (Luong, 2015) in the temporal attention module for both the PBNet decoder and the 3D U-Net in A2V-FDM, which restricts the attention scores to local region. This approach effectively models local dependencies in talking head videos. To accommodate the different temporal dependencies of lip motions and pose/blink movements, we use larger window size in the local attention mechanism of PBNet compared to A2V-FDM. 6 preprint"
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 SETUP Dataset. Our method is evaluated on two datasets: CREMA (Cao et al., 2014) and HDTF (Zhang et al., 2021). The CREMA dataset was collected in controlled laboratory environment and contains 7,442 videos from 91 identities, with durations ranging from 1 to 5 seconds. The HDTF dataset consists of 410 videos, with an average duration exceeding 100 seconds. These videos are gathered from wild scenarios and feature over 10,000 unique sentences for speech content, along with diverse head pose movements. We partitioned the CREMA dataset into training and testing sets following Stypułkowski et al. (2024). As for the HDTF dataset, we conducted random split with 9:1 ratio between the training and testing sets. We resized the videos at resolution of 128 128 and frame rate of 25 frames per second (fps) without any additional preprocessing. Implementation details. The architecture of the encoder and decoder in our model aligns with the design proposed by Johnson et al. (2016), while the flow predictor is implemented based on MRAA (Siarohin et al., 2021). The PBNet model is trained using pose and blink movement sequences of 200 frames. During the inference phase of the PBNet model, local attention mechanism with window size of 400 is employed. The motion representation space of the A2V-FDM model is 32 32 3. The training of the A2V-FDM model is conducted using the TCL strategy, which involves two stages: 1) the model is trained using video clips of 20 frames, with the source image set as the first frame in the clip; 2) training is performed on sequences ranging from 30 to 40 frames. For the inference phase of the A2V-FDM model, local attention with window size of 80 is applied. In our evaluation, the length of one-time inference for CREMA is dynamic and depends on the ground truth, while for HDTF, it is fixed at 200 frames for better comparison. Evaluation metrics. We evaluate the performance of our method using various metrics. Specifically, we employ the Fréchet Inception Distance (FID) (Heusel et al., 2017) to assess the image quality. We utilize the FVD16 and FVD32 scores, which calculate the Fréchet Video Distance (FVD) based on window sizes of 16 and 32 frames, respectively, to evaluate video quality across different temporal scales. Furthermore, we assess the perception loss of lip shape using the confidence score (LSEC) and distance score (LSED) (Chung & Zisserman, 2017). To evaluate the preservation of speaker identity during the generation, we use ArcFace (Deng et al., 2019) to extract features from both the ground truth image and the generated image, and use the cosine similarity (CSIM) between the two as the evaluation metric. Moreover, we employ the Beat Align Score (BAS) (Siyao et al., 2022) to evaluate the synchronization of head motion and audio, and calculate the number of blinks per second (blink/s) to assess the liveliness of the eyes. 4.2 OVERALL COMPARISON We compared our method with several state-of-the-art methods: Wav2Lip (Prajwal et al., 2020), MakeItTalk (Zhou et al., 2020), Audio2Head (Wang et al., 2021), SadTalker (Zhang et al., 2023), and Diffused Heads (Stypułkowski et al., 2024). To ensure fair comparison, we provided the same conditions for each baseline and generated the same number of frames in single generation. The quantitative experimental results are presented in Table 1. According to the results, our method achieves the best performance in FID, FVD32, FVD16, CSIM, BAS, and Blink/s metrics for both the CREMA and HDTF datasets. Additionally, our method achieves nearly best scores for LSEC and LSED, closely matching those of the Audio2Head and Wav2Lip baselines. These results demonstrate that our method produces talking head videos with the highest visual quality, rhythmic head poses, and natural blink movements. Additionally, it preserves the identity features of the source image to the greatest extent and achieves highly synchronized lip motions with the audio. Additionally, we test the generation speed of the aforementioned methods in Appendix A.2.2, suggesting our method achieves the fastest or near-fastest generation speed and requires significantly less time compared to the previous diffusion-based method, Diffused Heads. For the qualitative experiment, as shown in Figure 2, we visualize the generation results for each baseline on different datasets. Our method evidently achieves the visual quality most similar to the ground truth, showcasing the most realistic and vivid visual effects. Compared to our method, SadTalker relies on the 3DMM prior, which limits its animation capability to the facial region only, resulting in significant artifacts when merging the face with the static torso below the neck. 7 preprint Table 1: Quantitative comparison with several state-of-the-art methods methods on HDTF (Zhang et al., 2021) and CREMA (Cao et al., 2014) datasets. * Wav2Lip generated videos that only contain lip motions, while the rest remain still images. indicates better performance with higher values, while indicates better performance with lower values. For both BAS and Blink/s, we consider performance to be better when they are closer to the ground truth. Method GT Audio2Head MakeItalk SadTalker Diffused Heads Wav2Lip* DAWN (ours) GT Audio2Head MakeItalk SadTalker Wav2Lip* DAWN (ours) FID - 29.58 19.87 16.05 13.01 10.23 5.77 - 30.10 23.65 26.11 23.85 9.60 R F FVD16 FVD32 LSEC LSED CSIM BAS Blink/s - 188.54 159.38 101.43 64.27 130.23 56. - 122.26 120.42 97.43 166.15 60.34 - 208.44 320.77 158.85 116.18 242.19 75.82 - 205.42 221.14 187.43 281.73 95.64 5.88 5.13 3.78 5.57 4.56 6.08 5.77 7.95 6.88 4.41 6.27 7.42 6.71 7.87 7.92 9.15 7.36 9.26 7.74 8. 7.33 7.58 9.69 8.03 7.44 7.94 1 0.660 0.788 0.808 0.673 0.801 0.845 1 0.705 0.744 0.767 0.701 0.790 0.192 0.274 0.261 0.244 0.185 - 0.231 0.267 0.290 0.295 0.297 - 0.281 0.24 0.01 0.05 0.33 0.26 - 0. 0.75 0.09 0.09 0.47 - 0.86 Figure 2: Qualitative comparison with several state-of-the-art methods methods on HDTF (Zhang et al., 2021) and CREMA (Cao et al., 2014) datasets. Our method produces higher-quality results in video quality, lip-sync consistency, identity preservation, and head motions. Additionally, SadTalker exhibits unnatural head pose movements and gaze direction, partially due to limited temporal modeling ability. Wav2Lip only drives the mouth region and cannot generate head poses or blinks. The Audio2Head fails to preserve the speakers identity during generation. For the HDTF dataset, the Diffused Heads method collapsed due to the error accumulation. 8 preprint Table 2: Comparison with other generation strategies. The semi-autoregressive (SAR) generation strategy is similar to He et al. (2023). The two temporal resolution (TTR) generation method is mentioned in Harvey et al. (2022). Method Time(s) FID FVD16 SAR TTR Ours 11.42 19.25 7.32 13.00 9.77 9. 120.33 95.42 60.34 FVD32 LSEC LSED 8.29 4.34 210.52 8.68 4.87 137.14 7.94 6.71 95.64 Table 3: The experiment of extrapolation evaluation. Inference length\" refers to the number of frames generated in single inference process. Inference length FID FVD16 40 100 200 400 600 9.35 9.83 9.60 10.36 10.30 59.58 61.72 60.34 61.57 60.44 FVD32 LSEC LSED 7.89 5.76 94.09 7.96 6.41 98.80 7.94 6.71 95.64 8.12 6.63 97.84 8.02 6.76 96.62 4.3 COMPARISON WITH OTHER GENERATION STRATEGIES We compared our non-autoregressive generation strategy with two regular video generation strategies: 1) semi-autoregressive (SAR) generation similar to He et al. (2023), and 2) two-temporal resolution (TTR), which trains two models with different temporal resolutions (Harvey et al., 2022). The time cost represents the time required to generate an 8-second talking head video. The models were evaluated on the CREMA dataset, and the results are shown in Table 2. According to the results, our non-autoregressive method produces videos with the highest quality and fastest speed. The generation speed of the SAR method is faster than TTR, while TTR produces higher video quality than SAR. This is because autoregressive methods suffer from degradation issues during iterative generation. Although TTR somewhat alleviates the degradation issue, it compromises generation speed. In summary, our non-autoregressive method addresses the degradation problem while preserving fast generation speed. 4.4 EXTRAPOLATION VALIDATION To evaluate the extrapolation ability of our method, we conducted experiments to assess the impact of inference length on model performance using the HDTF dataset. We set the inference length to range from 40 to 600 frames in single generation process. The results are illustrated in Table 3, suggesting that performance remains relatively stable as the inference length increases. Specifically, the image and video quality metrics, FID and FVD, are not significantly affected by the inference length. Besides, we found that the LSEC and LSED scores tend to improve with increasing inference length while stabilizing when the length is sufficiently large. It suggests that audio with sufficient length helps the model produce more precise lip movement. 4.5 ABLATION STUDY Ablation study on TCL strategy. Since we only use the TCL strategy on A2V-FDM, we excluded the PBNet in the evaluation process by using ground truth pose/blink to drive the video. The results are illustrated in Table 4. We separately trained A2V-FDM using either stage 1 or stage 2 of TCL strategy. According to the results, using only stage 1 causes an overall performance decrease across all metrics except FID. This is because, in the first stage, the model is trained on shorter clips. It tends to perform minor warping on the source image, which results in lower FID score. Although minor warping can improve FID scores, it significantly diminishes the vividness of longer videos, leading to poorer FVD scores. Furthermore, training with fixed length introduces biases into the 9 preprint Table 4: Ablation study on TCL and PBNet. The GT PB\" refers to whether to use ground truth pose/blink signal. Method GT PB FID FVD16 only stage 1 only stage 2 DAWN w/o PBNet DAWN 7.95 13.71 9.68 15.20 9.60 81.84 125.75 52.05 100.94 60. FVD32 LSEC LSED 10.04 4.38 126.52 8.43 6.14 166.83 7.99 6.71 87.11 162.35 95.64 5.79 6.71 8.36 7.94 Figure 3: Visualization of cross-identity reenactment. We extract the audio, head pose, and blink signals from the video in the first row, and use them to drive the source image, generating the talking head video in the second row. non-autoregressive generation process, impacting the videos smoothness and the accuracy of lip motions. This, in turn, results in worse FVD and penalties in LSEC and LSED scores. Using only stage 2 also negatively affects the models performance. Compared to using only stage 1, the model achieves better LSEC and LSED scores. This improvement is because training with longer sequences is beneficial for enhancing the precision of lip motions (Tian et al., 2024). However, the worse FID and FVD scores result from the model struggling to learn both pose/blink control and lip movement simultaneously. Using either stage 1 or stage 2 alone clearly falls behind our proposed TCL strategy, demonstrating its significant contribution to performance. Ablation study on PBNet. We evaluate the effectiveness of the PBNet in Table 4. The term w/o PBNet\" indicates that the PBNet module was removed from the architecture, requiring the A2V-FDM to simultaneously generate pose, blink, and lip motions from the audio by itself. The results suggest an overall enhancement of all evaluation metrics with the inclusion of PBNet. This is because modeling the long-term dependency of pose and blink movements through PBNet simplifies training for the A2V-FDM. We also visualized the effectiveness of PBNet in Appendix A.2.4. 4.6 POSE/BLINK CONTROLLABLE GENERATION In addition to generating lifelike avatars, our method also enables the controllable generation of pose and blink actions. Users can either use pose and blink information generated by our PBNet or provide these sequences directly, such as by extracting them from given video. The results, as shown in Figure 3, demonstrate that our method not only provides high-precision control over the pose/blink movements of the generated avatars, but also effectively transfers rich facial dynamics, including expressions, blinks, and lip motions."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce DAWN, an innovative architecture that non-autoregressively generates dynamic frames of talking head videos from given portraits and audio. We utilized the LFG to extract motion representations from speech videos. To produce vivid talking head videos, we propose PBNet and 10 preprint A2V-LDM. The PBNet generates natural pose/blink movements from speech, while A2V-LDM produces motion representations conditioned on audio and pose/blink movements. Finally, these generated motion representations are decoded into videos using LFG. We demonstrate on two datasets that our model can generate extended talking head videos with high-quality dynamic frames in single pass, achieving realistic visual effects, accurate lip synchronization, and strong extrapolation capabilities."
        },
        {
            "title": "REFERENCES",
            "content": "Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 4148, 2009. Dan Bigioi, Shubhajit Basak, Michał Stypułkowski, Maciej Zieba, Hugh Jordan, Rachel McDonnell, and Peter Corcoran. Speech driven video editing via an audio-conditioned diffusion model. Image and Vision Computing, 142:104911, 2024. Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3d faces. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 99, pp. 187194, USA, 1999. ACM Press/Addison-Wesley Publishing Co. ISBN 0201485605. doi: 10.1145/311535.311556. URL https://doi.org/10.1145/311535.311556. Houwei Cao, David Cooper, Michael Keutmann, Ruben Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 5(4):377390, 2014. Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, and Chenliang Xu. Talking-head generation with rhythmic head motion. In European Conference on Computer Vision, pp. 3551. Springer, 2020. Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pp. 251263. Springer, 2017. Özgün Çiçek, Ahmed Abdulkadir, Soeren Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: learning dense volumetric segmentation from sparse annotation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pp. 424432. Springer, 2016. Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 46904699, 2019. Chenpeng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, and Jiang Bian. Dae-talker: High fidelity speech-driven talking face generation with diffusion autoencoder. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 42814289, 2023. Bo Fan, Lei Xie, Shan Yang, Lijuan Wang, and Frank Soong. deep bidirectional lstm approach for video-realistic talking head. Multimedia Tools and Applications, 75:52875309, 2016. Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles of conversational gesture. CoRR, abs/1906.04160, 2019. URL http://arxiv. org/abs/1906.04160. Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, and Stan Li. Towards fast, accurate and stable 3d dense face alignment. In European Conference on Computer Vision, pp. 152168. Springer, 2020. Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 57845794, 2021a. 11 preprint Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 57845794, 2021b. Flexible diffusion modeling of William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank In S. Koyejo, S. Mohamed, InforInc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ Wood. A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural mation Processing Systems, volume 35, pp. 2795327965. Curran Associates, 2022. file/b2fe1ee8d936ac08dd26f2ff58986c8f-Paper-Conference.pdf. long videos. Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, et al. Gaia: Zero-shot talking avatar generation. arXiv preprint arXiv:2311.15230, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022b. Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu. Depth-aware generative adversarial network for talking head video generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 33973406, 2022. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. CoRR, abs/1611.07004, 2016. URL http://arxiv.org/ abs/1611.07004. Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, and Xun Cao. Eamm: Oneshot emotional talking face via audio-based emotion-aware motion model. In ACM SIGGRAPH 2022 Conference Proceedings, pp. 110, 2022. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. CoRR, abs/1603.08155, 2016. URL http://arxiv.org/abs/1603. 08155. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, and Yu Li. Moda: Mapping-once audio-driven portrait animation with dual attentions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2302023029, 2023. Minh-Thang Luong. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015. Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767, 2023. 12 preprint Haomiao Ni, Changhao Shi, Kai Li, Sharon X. Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models, 2023. URL https://arxiv. org/abs/2303.13744. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Mathis Petrovich, Michael J. Black, and Gül Varol. Action-conditioned 3d human motion synthesis with transformer VAE. CoRR, abs/2104.05670, 2021. URL https://arxiv.org/abs/ 2104.05670. KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pp. 484492, 2020. Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer. Ganimation: Anatomically-aware facial animation from single image. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. Yurui Ren, Ge Li, Yuanqi Chen, Thomas Li, and Shan Liu. Pirenderer: Controllable portrait image generation via semantic neural rendering. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1375913768, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting diffusion models for generalized audio-driven portraits animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19821991, 2023. Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. CoRR, abs/2003.00196, 2020. URL https://arxiv. org/abs/2003.00196. Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. CoRR, abs/2104.11280, 2021. URL https://arxiv. org/abs/2104.11280. Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1105011059, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads: Diffusion models beat gans on talking-face generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 50915100, 2024. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104. 09864. 13 preprint Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024. Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Realistic speech-driven facial animation with gans. International Journal of Computer Vision, 128(5):13981413, 2020. Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, and Xin Yu. Audio2head: Audio-driven one-shot talking-head generation with natural head motion. arXiv preprint arXiv:2107.09293, 2021. Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: survey. IEEE transactions on pattern analysis and machine intelligence, 45(3): 31213138, 2022. Chao Xu, Yang Liu, Jiazheng Xing, Weida Wang, Mingze Sun, Jun Dan, Tianxin Huang, Siyuan Li, Zhi-Qi Cheng, Ying Tai, et al. Facechain-imagineid: Freely crafting high-fidelity diverse talking faces from disentangled audio. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12921302, 2024a. Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. arXiv preprint arXiv:2404.10667, 2024b. Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 86528661, 2023. Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 36613670, 2021. Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Posecontrollable talking face generation by implicitly modularized audio-visual representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4176 4186, 2021. Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk: speaker-aware talking-head animation. ACM Transactions On Graphics (TOG), 39(6): 115, 2020. 14 preprint Figure 4: The qualitative study on higher resolution (256 256) and different portrait styles."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 SOCIAL IMPACT CONSIDERATION DAWN focuses on creating realistic talking head videos with the aim of generating positive social impact. We firmly oppose the malicious misuse of our method, including fraud, creating fake news, and violating portrait rights. Thus, we assert that our open-source code and model should be used exclusively for research purposes. We hope that our technique can provide social benefits in the future, such as promoting education, enabling face-to-face communication for separated people, and treating certain psychological disorders. A.2 ADDITIONAL EXPERIMENTS A.2.1 EXPERIMENT ON HIGHER RESOLUTION AND DIFFERENT PORTRAIT STYLES We further investigate the generalization ability of our method on higher-resolution images and different portrait styles. Training DAWN on the HDTF dataset with resolution of 256 256, we tested it on multiple out-of-dataset source images featuring diverse styles, as showcased in Figure 4. The results indicate that our method yields promising outcomes in high-resolution generation and demonstrates considerable generalization ability across various image styles, including photos, paintings, anime, and sketches. A.2.2 COMPARISON EXPERIMENT ON GENERATION TIME COST We experimented to test the time cost of video generation. Using the aforementioned methods, we generated 8-second talking head videos with the same source image and audio, then recorded the time consumption for each method. To ensure fair comparison, we excluded the audio encoding step for all methods. The testing was performed on single V100 16G GPU. As shown in Figure 5, our method achieves the fastest or near-fastest generation speed and requires significantly less time compared to the previous diffusion-based method, Diffused Heads. 15 preprint Figure 5: The comparison experiment on generation time cost. The *\" refers to diffusion-based methods. Table 5: Ablation study on the local attention mechanism. The window\" means the window size in the local attention operation. The None\" means we use the original attention mechanism instead. Window FID FVD16 20 40 80 200 None 14.47 10.93 9.68 9.44 9.70 159.19 72.93 52.05 53.48 63.95 FVD32 LSEC LSED 8.97 5.69 217.54 8.33 6.35 114.52 6.71 87.11 7.99 7.94 6.60 88.84 8.15 6.37 103.83 A.2.3 ABLATION STUDY ON THE LOCAL ATTENTION MECHANISM In our work, we utilized local attention mechanism to enhance the extrapolation capability of our model. We conducted experiments to evaluate the effect of varying the window size of the local attention mechanism in A2V-FDM, ranging from 20 to 200, and also assessed the models performance without the local attention mechanism. To eliminate the influence of the generated pose/blink, we used the ground-truth pose/blink signals to drive the model. The results, presented in Table 5, indicate that the models performance peaks around window size of 80. This is attributed to the maximum length of the video clips during training, which is 40 frames. Consequently, the model learns to process temporal dependencies within 40-frame distance. Thus, using the window size of twice the max training clip length takes full advantage of the models capability. Reducing or increasing the window size degrades performance; smaller window size leads to loss of contextual information, significantly impairing the models performance, while larger window size or the absence of the local attention mechanism reduces extrapolation ability, also resulting in lower performance. Since extrapolation ability is also supported by our TCL strategy and the intrinsic structure of A2V-FDM, removing the local attention mechanism causes relatively minor damage compared to the loss of contextual information. A.2.4 ABLATION STUDY ON THE PBNET We further provide the visualization of the ablation study on PBNet in Figure 6, while the quantitative results are illustrated in Table 4. It suggests that using the PBNet to generate the pose exclusively will provide the pose with more vividness and diversity. However, generating lip, head pose, and blink movement from audio simultaneously will cause relatively static head pose, which severely impacts the vividness and naturalness. 16 preprint Figure 6: The visualization of the ablation study on PBNet demonstrates different methodologies. The term w/o PBNet\" indicates without PBNet\", whereby the A2V-FDM is utilized to infer pose and blink movements. Conversely, PBNet\" signifies the with PBNet\", which directly generates explicit pose and blink signals to control the generation of A2V-FDM. A.3 LIMITATION AND FUTURE WORKS Our work still has certain limitations. For instance, the model cannot fully comprehend the physical common sense during the generation, particularly when individuals in the portraits wearing items such as hats, helmets or headpieces. Sometimes, these items will not move along with the head, thus causing artifacts in the results. We leave the injection of these physical dependency into the model without the loss of vividness as our future work."
        }
    ],
    "affiliations": [
        "IFLYTEK Research",
        "University of Science and Technology of China"
    ]
}