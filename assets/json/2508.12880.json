{
    "paper_title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models",
    "authors": [
        "Chubin Chen",
        "Jiashu Zhu",
        "Xiaokun Feng",
        "Nisha Huang",
        "Meiqi Wu",
        "Fangyuan Mao",
        "Jiahong Wu",
        "Xiangxiang Chu",
        "Xiu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with a closed-form solution, we observe a discrepancy between the suboptimal results produced by CFG and the ground truth. The model's excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the model's suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S^2-Guidance, a novel method that leverages stochastic block-dropping during the forward process to construct stochastic sub-networks, effectively guiding the model away from potential low-quality predictions and toward high-quality outputs. Extensive qualitative and quantitative experiments on text-to-image and text-to-video generation tasks demonstrate that S^2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released."
        },
        {
            "title": "Start",
            "content": "S2-GUIDANCE: STOCHASTIC SELF GUIDANCE FOR TRAINING-FREE ENHANCEMENT OF DIFFUSION MODELS Chubin Chen1,2,* Jiashu Zhu2 Xiaokun Feng2,3 Nisha Huang1 Meiqi Wu2,3 Fangyuan Mao2 1Tsinghua University Jiahong Wu2, Xiangxiang Chu2 Xiu Li1, 2AMAP, Alibaba Group 3CASIA 5 2 0 2 8 1 ] . [ 1 0 8 8 2 1 . 8 0 5 2 : r https://s2guidance.github.io/ Figure 1: Visual results of S2-Guidance versus Classifier-free Guidance (CFG). Our proposed method S2-Guidance significantly elevates the quality and coherence of both text-to-image and textto-video generation. Observe (in examples surrounding the center): Our method produces generations with superior temporal dynamics, including more pronounced motion (bear) and dynamic camera angles that convey speed (car). It renders finer details, such as the astronauts transparent helmet, and creates images with fewer artifacts (runner, woman with umbrella), richer artistic detail (abstract portrait, castle, colored powder exploding), and improved object coherence (cat and rocket, sheep). See Appendix for our prompts."
        },
        {
            "title": "ABSTRACT",
            "content": "Classifier-free Guidance (CFG) is widely used technique in modern diffusion models for enhancing sample quality and prompt adherence. However, through an empirical analysis on Gaussian mixture modeling with closed-form solution, we observe discrepancy between the suboptimal results produced by CFG and the ground truth. The models excessive reliance on these suboptimal predictions often leads to semantic incoherence and low-quality outputs. To address this issue, we first empirically demonstrate that the models suboptimal predictions can be effectively refined using sub-networks of the model itself. Building on this insight, we propose S2-Guidance, novel method that leverages stochastic block-dropping during the forward process to construct sub-networks, effectively guiding the model away from potential low-quality predictions and toward highquality outputs. Extensive qualitative and quantitative experiments on text-toimage and text-to-video generation tasks demonstrate that S2-Guidance delivers superior performance, consistently surpassing CFG and other advanced guidance strategies. Our code will be released. *Work done during the internship at AMAP, Alibaba Group. Corresponding author. Project lead."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models Song et al. (2020a); Ho et al. (2020) have enabled rapid advances in high-quality text-to-image Rombach et al. (2022); Podell et al. (2023) and text-to-video Polyak et al. (2025); Wan et al. (2025); Kong et al. (2024) generation. key driver of this success is the advent of conditional guidance techniques, which steer the generation process to enhance adherence to given conditions. However, naively applying the conditioning signal often proves insufficient Mukhopadhyay et al. (2023). Classifier-free Guidance (CFG) Ho & Salimans (2022) has become the mainstream approach for improving conditional generation. It employs Bayesian implicit classifier to prioritize conditional probability, enhancing adherence to conditions and image quality. However, despite its effectiveness, it often results in semantic incoherence and loss of fine details, as shown in Figure 1. Recent studies Sadat et al. (2024a); Chung et al. (2024); Fan et al. (2025); Kynkaanniemi et al. (2024); Sadat et al. (2024b); Jin et al. (2025) have further explored methods to improve guidance. Although these methods improve quality to some extent, they primarily address specific issues while leaving the underlying mechanisms of CFG unexplored. Autoguidance Karras et al. (2024) identifies deficiencies in the models training objective and proposes using weak model for guidance, and subsequent works Hong et al. (2023); Ahn et al. (2024); Jeon (2025); Hyung et al. (2025); Hong (2024) propose modifying specific attention regions to mimic weak model. However, these methods either require training to acquire the weak model or depend on task-specific, empirical modifications to the network architecture, both of which pose significant challenges. To address this, we first analyze the suboptimal results produced by CFG and the guidance mechanism of Autoguidance. Specifically, we perform toy example on Gaussian mixture modeling, which provides closed-form solution, enabling us to effectively analyze the discrepancy between the guided results and the ground truth Brown et al. (2022); Pope et al. (2021). Furthermore, we observe that applying stochastic block dropping during the models forward process produces results highly similar to the weak model used in Autoguidance. Building on this supporting observation, we propose S2-Guidance, simple and effective approach for constructing weak models to address the suboptimal predictions of CFG. Unlike prior methods that rely on externally trained or manually tuned weak models, S2-Guidance leverages the models own structure in training-free manner, effectively guiding the model to produce higher-quality outputs. Our contributions are summarized as follows: (i) We first analyze the guidance behavior of CFG and weak-model-guided methods on Gaussian mixture distribution with closed-form solution. Empirical observations reveal that the suboptimal results of CFG can be effectively refined using sub-networks of the model itself, which exhibit guidance behavior similar to that of weak model. (ii) We propose S2-Guidance, novel method that leverages stochastic block-dropping during the forward process to construct weak models by activating latent sub-networks, eliminating the need for additional training or the trial-and-error process of manual selection. Furthermore, we demonstrate that in the iterative denoising process of diffusion models, single block-dropping per timestep is sufficient, effectively guiding the models toward high-quality outputs. This approach achieves strong performance while substantially reducing computational costs. (iii) Our method can be seamlessly adapted to various generative models. Through extensive qualitative and quantitative experiments, S2-Guidance demonstrates superior performance, consistently surpassing not only the original CFG but also other recent guidance enhancement methods. Additionally, comprehensive experiments on text-to-image (T2I) and text-to-video (T2V) tasks across diverse benchmarks further validate the superiority and effectiveness of our approach."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Diffusion Models and Classifier-free Guidance. Diffusion models Croitoru et al. (2023); Peebles & Xie (2023b); Esser et al. (2024); Chu et al. (2025) are class of powerful generative models that learn to reverse predefined forward process, which gradually perturbs data x0 into Gaussian noise xT . The reverse process is typically governed by time-reversed stochastic differential equation (SDE) Song et al. (2020b), which relies on accurately estimating score function xt log pt(xt) using neural network Dθ. Flow-based models Lipman et al. (2023); Liu et al. (2022); Gat et al. (2024) can also be viewed as special class of diffusion models, as they both aim to learn continuous transformation between simple prior distribution and the complex data distribution Gao & Zhu (2025). In practical applications Huang et al. (2023a); Zhu et al. (2024); Huang et al. (2024a); Mao et al. (2025), generation is often conditioned on signals (e.g., text prompts), shifting the objective to"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: An illustration of our guidance mechanism on the generation quality manifold. The current state of generation (Mt, orange wireframe) is guided towards the next state (Mt1). Original CFG provides strong but suboptimal direction (gray arrow), failing to precisely target the high-quality region (yellow peak). S2-Guidance refines this by computing self-corrective prediction from stochastically block-dropping strategy (Pred w/ Stoc. drop, blue arrow). The resulting S2-Guidance vector (purple arrow) steers the update towards the optimal region of the generation manifold, resulting in higher-fidelity outputs. Figure 3: S2-Guidance successfully balances guidance strength and distribution fidelity. Comparison on 1D (top) and 2D (bottom) toy examples. Unlike CFG, which distorts the sample distribution (see red boxes), or other methods that fail to separate modes, S2-Guidance accurately captures both the location and shape of the ground truth distributions (semi-transparent). modeling the conditional score xt log pt(xtc). Classifier-free Guidance (CFG) Ho & Salimans (2022) has become the cornerstone for controllable generation by offering simple yet effective mechanism to enhance conditioning. It has found widespread applications across various domains Huang et al. (2023b); Wang et al. (2024); Fang et al. (2025); He et al. (2025); Ma et al. (2023; 2024). Instead of only using the conditional prediction Dθ(xtc), CFG forms guided score by extrapolating from an unconditional one Dθ(xtϕ): Dλ θ (xtc) = Dθ(xtϕ) + λ (Dθ(xtc) Dθ(xtϕ)) , (1) where λ is the guidance scale. While highly effective, CFG introduces critical trade-off: large λ improves condition adherence but often at the cost of sample diversity and perceptual quality, leading to artifacts like oversaturation Karras et al. (2024); Sadat et al. (2024a). This motivates us to search for more refined guidance strategies."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: S2-Guidance resolves the critical trade-off between guidance strength and sample fidelity on CIFAR-10. This t-SNE visualization compares the feature distributions of generated samples (points) against the real data distribution (shaded contours). While standard CFG (b) achieves class separation at the cost of severe distributional collapse, S2-Guidance (e) simultaneously ensures strong separation and preserves the high-fidelity structure of the real data distribution. Subfigure (f) provides corresponding qualitative visualizations, where each row (top to bottom) matches the methods in (a-e). Guidance via Weak Models (GWM). promising direction to improve CFG is to leverage an auxiliary weak model to refine the guidance signal. For instance, Autoguidance Karras et al. (2024) employs separately trained, degraded version of the full model, but such models are often infeasible to obtain for large-scale pretrained models. To circumvent this, recent works create weak model on-the-fly by perturbing the full models internal states. Methods like SAG Hong et al. (2023) and SEG Hong (2024) achieve this by blurring attention maps, while STG Hyung et al. (2025) proposes skipping strategy for video generation. However, these self-perturbation techniques often rely on task-specific, hand-crafted architectural modifications (e.g., targeting attention blocks), which limits their generalizability. In contrast, as shown in Figure 2, our S2-Guidance introduces novel and flexible approach. We construct the weak model via stochastic block-dropping, enabling high-quality, controllable generation without requiring auxiliary training or specialized architectural assumptions."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 VISUALIZING AND REVISITING GUIDANCE VIA WEAK MODELS We begin by visualizing the suboptimal outcomes of CFG using Gaussian mixture tasks Ho & Salimans (2022), toy example with closed-form solutions. This allows us to systematically observe the discrepancies between predictions and ground truth. Building on the analysis of how GWM Karras et al. (2024) improves results, we identify its limitations and propose incorporating stochastic sub-networks into the CFG framework, providing novel approach to enhance model performance. CFG improves conditional generation by implicitly amplifying the conditional probability density, raising it to power greater than one Bradley & Nakkiran (2024). Figure 3 illustrates 1D toy example (top) aimed at learning Gaussian Mixture distribution with modes at 4 and 4. While CFG significantly improves the baseline conditional output, it also introduces notable drawback: as highlighted by the red box, the mode of the generated distribution is slightly shifted from the ground truth. similar shift occurs in 2D toy example (bottom), where samples are scattered into unintended regions. These findings suggest that, although CFG enhances sample quality, its distributional fidelity remains suboptimal. Autoguidance, as representative of GWM Karras et al. (2024); Hong et al. (2023); Hong (2024); Ahn et al. (2024), is designed to guide the model toward well-learned, high-probability regions by leveraging weak model. As shown in the middle column of Figure 3, AutoGuidance improves the peak near -4 but remains limited. Its improvement stems from the construction of weak model, with the extent of enhancement depending on the weak models effectiveness. Such models are typically created by reducing model capacity or training epochs. However, this approach faces practical limitations that restrict its broader applicability. First, relying on externally designed weak models poses scalability challenges, as obtaining reduced version trained for fewer epochs alongside large-scale pretrained model is often impractical. Second, as highlighted by Autoguidance Karras et al. (2024), selecting an appropriate weak model is constrained by various factors. Once chosen, the weak model affects the entire denoising process, limiting the flexibility of guidance. poorly designed weak model fails to effectively prevent lowquality outputs, as shown in Figure 3 (c), where guided outputs still deviate notably from the target distribution."
        },
        {
            "title": "Preprint",
            "content": "This raises an important question: Can we eliminate the reliance on externally prescribed weak models while still identifying error-prone regions? Prior works Lou et al. (2024); Avrahami et al. (2025); Yuan et al. (2024) have shown that mainstream generative architectures, such as DiT Peebles & Xie (2023b); Chu et al. (2024), exhibit significant redundancy, as outputs across different transformer blocks often show high similarity Chen et al. (2024). Inspired by this, we hypothesize that sub-networks within such architectures can function as weak models, capturing outputs similar to the full model but with more pronounced errors. By leveraging these sub-network predictions, we aim to refine existing CFG guidance, effectively steering the model away from suboptimal outputs. The following subsections present detailed description of our approach along with its empirical validation."
        },
        {
            "title": "3.2 NAIVE S2-GUIDANCE\nBuilding on the preceding observation, our key insight is that the suboptimal results of CFG can be\ndirectly refined through the model’s own sub-networks, mimicking the behavior of a weak model to\nsteer the predictions away from potential low-quality outputs.\nAs revealed in Autoguidance Karras et al. (2024), problems in generative models depend on vari-\nous factors (e.g., network architecture, dataset properties, etc.), making it difficult to pinpoint which\ncomponents play a decisive role. Therefore, it is challenging to a priori define an optimal sub-\nnetwork that best captures low-quality regions. Motivated by Gal & Ghahramani (2016), a naive\nsolution is to leverage as many diverse stochastic sub-networks as possible to construct multiple\nweak models. These weak models then guide the main model away from low-quality regions dur-\ning each forward pass by steering it away from their outputs. We refer to this approach as Naive\nStochastic Sub-network Guidance (Naive S2-Guidance). Intuitively, this can be understood as ap-\nplying stochastic ”dropout” to different blocks, constructing various sub-networks that capture di-\nverse low-probability regions, guiding the output away from them (more discussion on this approach\ncan be found in Appendx A). Specifically, for a given binary mask m, sampled via stochastic block-\ndropping from the induced distribution p(m), the weak model’s prediction is defined as:",
            "content": "ˆDθ(xt c, m) = Dθ(xt c; θ m), where determines which blocks of the network parameters θ are activated, forming latent subnetwork during each forward pass. Naive S2-Guidance is then expressed as: θ (xt c) = Dθ(xt ϕ) + λ(cid:0)Dθ(xt c) Dθ(xt ϕ)(cid:1) Dλ (2) ω (cid:88) i=1 (cid:0) ˆDθ(xt c, mi)(cid:1), (3) where mi p(m) is the binary mask for the i-th stochastic sub-network, ω adjusts the contribution of the weak models, referred to as the S2 Scale. ˆDθ(xt c, mi) represents the prediction from the i-th sampled sub-network, and denotes the total number of latent sub-networks sampled during each forward pass. To validate our hypothesis, we conduct experiments on toy examples with 1D and 2D Gaussian mixture data, as well as on real-world datasets (see Appendix for more details). As shown in Figure 3 (d), compared to the original CFG, our Naive S2-Guidance not only leads to predictions that better fit the target distribution but also mitigates the drift phenomenon, thereby improving fidelity. This demonstrates that our method effectively refines the suboptimal results of CFG. Furthermore, compared to Autoguidance, S2-Guidance eliminates the need for explicitly constructing weak models. By adopting this simple yet effective approach, it avoids generating results that lie in intermediate regions, thereby reducing mode confusion. These results provide strong empirical evidence that leveraging Naive S2-Guidance can significantly enhance both the quality and robustness of conditional generation. 3.3 S2-GUIDANCE IS SUFFICIENT However, Naive S2-Guidance incurs significant computational overhead, which severely limits its practicality. In the process of constructing sub-networks, we find that constraining stochastic blockdropping within specific range allows sub-networks, even those generated by dropping at different blocks, to consistently guide the model toward the ideal distribution (see results in Appendix A). Therefore, we propose simplified approach: performing single stochastic block-dropping operation at each timestep for self guidance. We refer to this approach as S2-Guidance, which achieves"
        },
        {
            "title": "Preprint",
            "content": "Model Method HPSv2.1 (%) T2I-CompBench (%) Qalign Anime Concept Paint. Photo Avg. Color Shape Texture HPSv2.1 T2I-Comp. SD SD3.5 CFG CFG++ APG CFG-Zero Ours CFG CFG++ APG CFG-Zero Ours 31.55 31.57 30.77 31.99 32.14 32.34 31.99 31.43 32.77 32.89 30.87 30.76 30.18 31.17 31. 31.51 31.02 30.74 31.91 32.15 31.22 28.27 30.48 53.61 51.20 30.96 27.54 30.21 46.39 47.18 30.53 27.12 29.65 45.28 46.27 31.42 28.54 30.78 52.70 52.84 31.70 29.19 31.09 59.63 58.71 31.50 27.93 30.82 51.29 47.71 31.36 27.32 30.42 38.05 37.52 31.12 27.07 30.09 35.67 37.86 31.95 28.27 31.23 52.01 46.99 32.28 28.94 31.56 57.57 51.23 52.45 46.33 46.84 53.37 56.77 47.39 34.87 35.67 48.36 50.13 4.66 4.68 4.68 4.66 4. 4.63 4.65 4.68 4.66 4.70 4.74 4.73 4.73 4.77 4.74 4.66 4.58 4.65 4.70 4.74 Table 1: Quantitative evaluation of T2I guidance methods on SD3 and SD3.5 models. Our method establishes new state-of-the-art, demonstrating significant improvements even on highly competitive benchmarks. On HPSv2.1, benchmark where score margins are typically narrow, S2-Guidance consistently outperforms all baselines across every individual dimension. This lead is even more pronounced on T2I-CompBench, where our approach shows substantial gains in compositional attributes like Color and Shape. Notably, S2-Guidance also achieves the highest or nearhighest aesthetic scores (Qalign) on both benchmarks, demonstrating its superior performance in visual quality. Higher scores () are better. Best results are in bold. Algorithm 1 S2-Guidance Require: Trained denoiser Dθ, initial noise xT , guidance scale λ, S2 scale ω, number of timesteps . 1: for = T, . . . , 1 do 2: mt GenerateStochasticMask() 3: Duncond Dθ(xt, ϕ, t) 4: Dcond Dθ(xt, c, t) ˆDs Dθ(xt, c, t, mt) 5: Duncond + λ(Dcond Duncond) ω ˆDs 6: xt1 SchedulerStep( D, xt, t) 7: 8: end for 9: return x0 # Generate stochastic mask # Prediction from the stochastic sub-network highly competitive results. At timestep t, S2-Guidance is expressed as: Dλ θ (xtc) = Dθ(xtϕ) + λ(cid:0)Dθ(xtc) Dθ(xtϕ)(cid:1) ω(cid:0) ˆDθ(xtc, mt)(cid:1). (4) The overall algorithm is summarized in Algorithm 1. We empirically validate the proposed S2-Guidance on toy examples with 1D and 2D Gaussian mixture data, as well as on real-world datasets. As shown in Figure 3 (e), S2-Guidance performs comparably to Naive S2-Guidance. On both 1D and 2D Gaussian mixture distributions, it produces results that closely align with the ideal distribution, while exhibiting efficiency without significant degradation. Moreover, as illustrated in Figure 4 (e, f), S2-Guidance achieves highly competitive performance on real-world datasets, highlighting its practical effectiveness. To further analyze the stochastic block-dropping strategy, we conduct detailed experimental study in Section 4.4. Our empirical analysis reveals that, when the block drop ratio is maintained around 10% of the networks blocks, the resulting sub-networks consistently enable the model to achieve better performance. This strategy proves effective across mainstream DiT Peebles & Xie (2023a) architectures, leveraging the redundancy in the outputs to dynamically construct diverse stochastic sub-networks. Unlike explicitly constructed weak models, which once selected affect the entire denoising process, stochastic block-dropping enables the creation of sub-networks independently at different timesteps. This dynamic diversity introduces self-guidance throughout the diffusion process, allowing predictions to evolve iteratively and steering the outputs toward higher-quality results."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: S2-Guidance consistently generates superior images in both aesthetic quality and prompt coherence. While existing guidance methods like CFG, APG, CFG++, and Zero (CFGZero) often produce artifacts, distorted objects, or fail to follow complex prompts (see red boxes), our approach yields clean, coherent, and visually pleasing results without such flaws. Figure 6: S2-Guidance generates temporally coherent and physically plausible videos, overcoming key failures of CFG. Top Row: CFG struggles with plausible motion, depicting truck that unnaturally slides sideways instead of driving forward (red boxes). Our method renders stable and realistic scene. Bottom Row: CFG fails to capture the full prompt, as the light does not weave around her face (red box) and lacks glowing particles (blue box). S2-Guidance faithfully produces dynamic, visually rich scene adhering to the complex description."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS Benchmark. Our experiments encompass both text-to-image (T2I) and text-to-video (T2V) tasks, which are conducted on widely used benchmarks. For T2I evaluation, we use two popular benchmarks: HPSv2.1 Wu et al. (2023b), which consists of 3,200 prompts across four styles (animation, concept art, paintings, and photos). Besides, we also use T2I-CompBench Huang et al. (2023a), benchmark designed to evaluate model performance in complex scenes. In addition to the benchmark-specific evaluation metrics, we employ Qalign Wu et al. (2023a) to compute aesthetic scores, thereby providing more comprehensive assessment. For T2V evaluation Liu et al. (2024); Ling et al. (2025); Feng et al. (2025); Chen et al. (2025), we adopt the standard prompts and evaluation metrics provided by VBench Huang et al. (2024b). Baselines. For T2I task, we employ the high-performing Stable Diffusion 3 (SD3) and SD3.5 Esser et al. (2024); AI (2024). For T2V task, we utilize the latest Wan-1.3B and Wan-14B models Wan et al. (2025). Furthermore, to demonstrate the versatility of our guidance approach, we conduct comparative analysis not only against original CFG but also with four prominent methods: CFG++ Chung et al. (2024), CFG-Zero Fan et al. (2025), STG Hyung et al. (2025), and APG Sadat"
        },
        {
            "title": "Preprint",
            "content": "Model Method Total Quality Score Score Semantic Score CFG CFG++ APG STG CFG-Zero Ours CFG Ours 80.29 80.35 70.83 78.78 80.71 80.93 82.65 82.84 84.32 83.58 77.13 83.92 84.51 84.74 84.88 84.89 64.16 67.43 45.61 58.19 65.53 65.70 73.76 74. Wan1.3B Wan14B Subject Consistency Consistency Background Aesthetic Quality Imaging Object Appearance Quality Class Style 96.53 96.70 96.45 95.03 96.33 96.57 94.45 94.21 95.46 93.28 95.39 96.04 94.56 95.80 97.66 97. 60.52 59.02 49.42 59.03 59.69 60.52 68.68 68.78 67.65 69.14 64.39 65.59 69.05 68.19 67.82 67.77 77.06 70.06 59.02 68.20 78.16 78.09 84.97 89. 20.15 19.75 20.01 21.51 20.31 20.59 22.14 22.27 Table 2: Quantitative comparison on VBench. S2-Guidance consistently outperforms mainstream methods on both Wan1.3B and Wan14B models. While evaluated on all 16 official dimensions, this table shows representative subset of 9 key metrics. Our method achieves the highest Total Score and demonstrates significant improvements. Best results are in bold; second-best are underlined. et al. (2024a). Additional evaluations on other models and detailed implementation specifics are provided in the Appendix B."
        },
        {
            "title": "4.2 TEXT-TO-IMAGE GENERATION\nQuantitative Evaluation. Table 1 presents the results of S2-Guidance and various baselines\nacross different models and benchmarks. On HPSv2.1, S2-Guidance achieves the best performance\nnot only in average scores but also across all individual dimensions, demonstrating the effectiveness\nof our method. By refining suboptimal results produced by CFG, S2-Guidance achieves better align-\nment with human preferences. The performance on T2I-CompBench further highlights the strength\nof our approach, showing its effectiveness in handling complex generation tasks. Furthermore, aes-\nthetic scoring results provide additional validation of the effectiveness of our method.",
            "content": "Qualitative Evaluation. The qualitative comparisons are presented in Figure 5. Compared to CFG, S2-Guidance achieves significant improvements in both visual quality and semantic coherence, consistent with the observations in our toy examples. Compared to other methods, CFG-Zero loses fine-grained details, APG generates unnatural artifacts, and CFG++ fails to align well with the text prompts. In contrast, our method produces higher-quality images with finer details and better semantic alignment to the text descriptions. 4.3 TEXT-TO-VIDEO GENERATION Quantitative Evaluation. The quantitative results in Table 2 validate our methods superiority. On the Wan1.3B model, S2-Guidance achieves the highest Total Score (80.93), outperforming all baselines. Notably, it surpasses recent strong competitors like CFG++ and CFG-Zero, while significantly outperforming methods like APG, which struggles with overall quality. We further conduct experiments on the larger Wan14b model, demonstrating significant improvements compared to CFG. Qualitative Evaluation. As visually demonstrated in Figure 6, our method generates videos with substantially improved quality and coherence. The examples highlight that S2-Guidance effectively addresses two critical failures of original CFG: the loss of physical plausibility in object motion and the inability to adhere to complex, compositional prompts. Consequently, our approach yields videos that are not only more physically realistic but also demonstrate superior prompt coherence, faithfully realizing the users creative intent. User Study. We performed user study for both T2I and T2V generation. Our method is significantly preferred over all baselines in terms of both visual quality and prompt alignment. Full details are presented in Appendix B. 4.4 ABLATION STUDY Performance under Different Guidance Scales. We conduct experiments to compare S2Guidance with CFG across various guidance scales, focusing on aesthetic and CLIP scores. As shown in Figure 7, S2-Guidance consistently outperforms CFG across wide range of scales. Unlike CFG, which requires extensive parameter tuning, S2-Guidance exhibits stability and achieves high performance with minimal sensitivity to guidance scales. Notably, in most cases, S2-Guidance even surpasses the best performance achieved by CFG, demonstrating its robustness."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: S2-Guidance consistently improves aesthetic scores and is robust to the guidance scale. Left: Our method outperforms CFG across all guidance scales (λ). Right: Analysis of the impact of our S2 Scale (ω) on aesthetic scores. Effect of S2 Scale ω. We conduct experiments to analyze S2 Scale ω , as shown in Figure 7. When ω is set to smaller value, it improves the aesthetic score. However, since CFG has already produced suboptimal result, using larger ω tends to overadjust, leading to decline in quality. Effect of Drop-Ratio. We investigate the impact of the drop-ratio on the SD3.5 model with 24 blocks. As shown in Table 3 (left), when the number of dropped blocks is limited to 3/24 (approximately 10%), the aesthetic score remains stable at relatively high level. However, dropping more blocks leads to gradual decline in performance. Empirically, we find that drop-ratio of about 10% effectively simulates the predictions of weaker model, enabling the improvement of suboptimal results produced by CFG. Comparative Analysis of S2-Guidance and Naive S2-Guidance. As shown in Table 3 (right), increasing the number of iterations for sampling stochastic subnetworks during self-guidance yields diminishing improvements in aesthetic scores. S2-Guidance also effectively enhances fidelity in reconstruction tasks. For more theoretical and experimental analysis, see Appendix A. Stochastic Block-Dropping S2-Guidance Iterations Num. 0 1 2 3 Aes. 4.618 4.652 4.643 4.616 4.531 Num. 1 5 10 15 20 Aes. 4.643 4.649 4.653 4.651 4. Table 3: Average Aesthetic Scores for Stochastic Block-Dropping and S2-Guidance Iterations."
        },
        {
            "title": "REFERENCES",
            "content": "Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In European Conference on Computer Vision, pp. 117. Springer, 2024."
        },
        {
            "title": "Preprint",
            "content": "S AI. Stable diffusion 3.5 large, 2024. Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel Cohen-Or. Stable flow: Vital layers for training-free image editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 78777888, 2025. Arwen Bradley and Preetum Nakkiran. Classifier-free guidance is predictor-corrector. arXiv preprint arXiv:2408.09000, 2024. Bradley CA Brown, Anthony Caterini, Brendan Leigh Ross, Jesse Cresswell, and Gabriel Loaiza-Ganem. Verifying the union of manifolds hypothesis for image data. arXiv preprint arXiv:2207.02862, 2022. Honghao Chen, Yurong Zhang, Xiaokun Feng, Xiangxiang Chu, and Kaiqi Huang. Revealing In Forty-first International the dark secrets of extremely large kernel convnets on robustness. Conference on Machine Learning, 2024. URL https://openreview.net/forum?id= rkYOxLLv2x. Rui Chen, Lei Sun, Jing Tang, Geng Li, and Xiangxiang Chu. Finger: Content aware fine-grained evaluation with reasoning for ai-generated videos. In ACM MM, 2025. Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks. In European Conference on Computer Vision, pp. 118. Springer, 2024. Xiangxiang Chu, Renda Li, and Yong Wang. Usp: Unified self-supervised pretraining for image generation and understanding. In ICCV, 2025. Hyungjin Chung, Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv:2406.08070, 2024. Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. arXiv preprint Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models IEEE transactions on pattern analysis and machine intelligence, 45(9): in vision: survey. 1085010869, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Weichen Fan, Amber Yijia Zheng, Raymond Yeh, and Ziwei Liu. Cfg-zero*: Improved classifierfree guidance for flow matching models. arXiv preprint arXiv:2503.18886, 2025. Chengyu Fang, Chunming He, Longxiang Tang, Yuelin Zhang, Chenyang Zhu, Yuqi Shen, Chubin Integrating extra modality helps segmentor find camouflaged Chen, Guoxia Xu, and Xiu Li. objects well. arXiv preprint arXiv:2502.14471, 2025. Xiaokun Feng, Haiming Yu, Meiqi Wu, Shiyu Hu, Jintao Chen, Chen Zhu, Jiahong Wu, Xiangxiang Chu, and Kaiqi Huang. Narrlv: Towards comprehensive narrative-centric evaluation for long video generation models. arXiv preprint arXiv:2507.11245, 2025. Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 10501059. PMLR, 2016. Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in wasserstein distances, 2025. URL https://arxiv.org/abs/2401. 17958. Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching, 2024. URL https://arxiv.org/abs/2407. 15595."
        },
        {
            "title": "Preprint",
            "content": "Chunming He, Yuqi Shen, Chengyu Fang, Fengyang Xiao, Longxiang Tang, Yulun Zhang, Wangmeng Zuo, Zhenhua Guo, and Xiu Li. Diffusion models in low-level vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. Advances in Neural Information Processing Systems, 37:6674366772, 2024. Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 74627471, 2023. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023a. Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee, and Changsheng Xu. Region-aware diffusion for zero-shot text-driven image editing. arXiv preprint arXiv:2302.11797, 2023b. Nisha Huang, Yuxin Zhang, Fan Tang, Chongyang Ma, Haibin Huang, Weiming Dong, and Changsheng Xu. Diffstyler: Controllable dual diffusion for text-driven image stylization. IEEE Transactions on Neural Networks and Learning Systems, 2024a. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024b. Junha Hyung, Kinam Kim, Susung Hong, Min-Jung Kim, and Jaegul Choo. Spatiotemporal skip In Proceedings of the Computer Vision and guidance for enhanced video diffusion sampling. Pattern Recognition Conference, pp. 1100611015, 2025. Boseong Jeon. Spg: Improving motion diffusion by smooth perturbation guidance. arXiv preprint arXiv:2503.02577, 2025. Cheng Jin, Zhenyu Xiao, Chutao Liu, and Yuantao Gu. Angle domain guidance: Latent diffusion requires rotation rather than extrapolation. arXiv preprint arXiv:2506.11039, 2025. Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2024. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Xinran Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, and Xiangxiang Chu. Vmbench: benchmark for perception-aligned video motion generation. In ICCV, 2025. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747."
        },
        {
            "title": "Preprint",
            "content": "Xiao Liu, Xinhao Xiang, Zizhong Li, Yongheng Wang, Zhuoheng Li, Zhuosheng Liu, Weidi Zhang, Weiqi Ye, and Jiawei Zhang. survey of ai-generated video evaluation. arXiv preprint arXiv:2410.19884, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. Token caching for diffusion transformer acceleration. arXiv preprint arXiv:2409.18523, 2024. Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023. Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, et al. Follow-your-click: Open-domain regional image animation via short prompts. arXiv preprint arXiv:2403.08268, 2024. Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-effects: Unified and spatially-controllable visual effects generation, 2025. Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Srinidhi Hegde, Tianyi Zhou, and Abhinav Shrivastava. Diffusion models beat gans on image classification. arXiv preprint arXiv:2307.08702, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023a. URL https://arxiv.org/abs/2212.09748. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023b. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, YenCheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2025. Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=XJk19XzGq2J. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022."
        },
        {
            "title": "Preprint",
            "content": "Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In The Thirteenth International Conference on Learning Representations, 2024a. Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024b. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023a. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023b. Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2024. Chenyang Zhu, Kai Li, Yue Ma, Longxiang Tang, Chengyu Fang, Chubin Chen, Qifeng Chen, and Xiu Li. Instantswap: Fast customized concept swapping across sharp shape differences. arXiv preprint arXiv:2412.01197, 2024."
        },
        {
            "title": "OVERVIEW",
            "content": "This appendix is divided into two main parts, covering method details and experimental supplements. Appendix The first part focuses on the details and discussions of the methodology, including: principled derivation of Naive S2-Guidance from Bayesian perspective. detailed analysis between S2-Guidance and Naive S2-Guidance. Appendix The second part provides supplementary information about experiments, covering: Explanation of the toy example and additional experimental results. More comprehensive evaluation results. User study. Further implementation details of the experiments. Detailed Prompts for the Experiments."
        },
        {
            "title": "A EXTENDED DISCUSSION AND ANALYSIS OF OUR METHODS",
            "content": "A.1 PRINCIPLED DERIVATION OF NAIVE S2-GUIDANCE FROM BAYESIAN PERSPECTIVE In this subsection, we provide principled theoretical foundation for our proposed Naive Stochastic Sub-network Guidance (Naive S2-Guidance) method. We move beyond heuristic interpretation and formally derive our approach by drawing direct line to the principles of Bayesian inference, as established in the seminal work Dropout as Bayesian Approximation by Gal & Ghahramani (2016). Our central argument is that Naive S2-Guidance is not merely inspired by Bayesian ideas, but can be derived as principled mechanism for correcting the predictions of deterministic model by leveraging its own epistemic uncertainty. (cid:90) A.1.1 FOUNDATIONAL BAYESIAN FORMULATION Let = {xi, ci}M i=1 be our training dataset. fully Bayesian approach to generative modeling would seek to compute the true posterior predictive distribution for new sample xt given condition c: p(Dxt, c, D) = p(Dxt, c, θ)p(θD)dθ, (5) where θ Θ are the model parameters, p(θD) is the true posterior distribution over these parameters, and p(Dxt, c, θ) is the likelihood of specific prediction given parameters θ. The true posterior is given by Bayes theorem: Θ p(θD) = p(Dθ)p(θ) p(D) = p(Dθ)p(θ) Θ p(Dθ)p(θ)dθ . (cid:82) (6) The integral in the denominator, known as the marginal likelihood or model evidence, is intractable for deep neural networks. To circumvent this, we employ Variational Inference (VI), introducing tractable approximate posterior distribution qϕ(θ) (parameterized by ϕ) to approximate p(θD). We minimize the Kullback-Leibler (KL) divergence between these two distributions: ϕ = arg min ϕ KL(qϕ(θ)p(θD)) = arg min ϕ = arg min ϕ = arg min ϕ (cid:90) (cid:90) qϕ(θ) log qϕ(θ) log dθ qϕ(θ) p(θD) qϕ(θ)p(D) p(Dθ)p(θ) dθ (cid:0)KL(qϕ(θ)p(θ)) Eqϕ(θ)[log p(Dθ)](cid:1) . (7) (8) (9) (10) Minimizing this objective is equivalent to maximizing the Evidence Lower Bound (ELBO), LELBO. The work of Gal & Ghahramani (2016) provides the theoretical grounding for interpreting stochastic network perturbations, such as dropout, as form of this Bayesian optimization. In our work, we generalize this concept from neuron-level dropout to block-level dropout. Each binary mask mi p(m) applied via stochastic block dropping effectively samples specific set of weights θi = θ mi from this approximate posterior, which we denote simply as q(θ). A.1.2 MONTE CARLO ESTIMATION OF THE APPROXIMATE POSTERIOR PREDICTIVE The prediction of single sub-network, ˆDθ(xt c, mi), is thus legitimate sample from the approximate posterior predictive distribution, pq(Dxt, c): ˆDθ(xt c, mi) D(xt c; θi), where θi q(θ). (11) The moments of this distribution can be estimated via Monte Carlo integration. The first moment, the posterior mean µpost, is defined and approximated as: µpost(xt c) Eq(θ)[D(xt c; θ)] = (cid:90) D(xt c; θ)q(θ)dθ ˆDθ(xt c, mi). (12) 1 N (cid:88) i="
        },
        {
            "title": "Preprint",
            "content": "This posterior mean, µpost, represents the center of mass of the models belief. The second central moment, the variance, quantifies the epistemic uncertainty: Varq(θ)[D(xt c; θ)] = Eq(θ) (cid:88)"
        },
        {
            "title": "1\nN",
            "content": "(cid:2)(D(xt; θ) µpost)2(cid:3) (cid:0) ˆDθ(xt; mi) µpost(xt)(cid:1)2 . (13) i=1 Our central hypothesis is that low-quality generative outputs often arise in regions of high epistemic uncertainty. In such regions, the posterior mean, µpost, often corresponds to safe, but ultimately low-quality output (e.g., blurry artifact). The deterministic MAP estimate, Dθ(xt c), however, might be unjustifiably confident in these very regions. A.1.3 DERIVING S2-GUIDANCE AS AN UNCERTAINTY-AWARE CORRECTION Based on this hypothesis, we formulate principled correction to the Classifier-free Guidance (CFG) prediction, DCFG. The standard guidance is: DCFG(xt c) = Dθ(xt ϕ) + λ(cid:0)Dθ(xt c) Dθ(xt ϕ)(cid:1). (14) θ We define our corrected prediction, Dλ,ω (xt c), as the solution to an optimization problem where we seek prediction that remains faithful to the original guidance while being repelled from the center of uncertainty. Let us define correction vector D. We propose that this correction should be in the direction opposite to the posterior mean, which acts as the locus of uncertainty-induced artifacts: ω µpost(xt c), (15) where ω is scalar controlling the magnitude of the repulsion. The corrected prediction is thus the linear superposition of the original guidance and this correction term: Dλ,ω θ (xt c) DCFG(xt c) + D, = DCFG(xt c) ω µpost(xt c), = Dθ(xt ϕ) + λ(cid:0)Dθ(xt c) Dθ(xt ϕ)(cid:1) (cid:124) (cid:123)(cid:122) (cid:125) Standard CFG ω Eq(θ)[D(xt c; θ)] (cid:125) (cid:123)(cid:122) (cid:124) Uncertainty-Aware Repulsion Term . (16) (17) (18) Substituting the Monte Carlo approximation from Eq. 12 into Eq. 16, we recover our full Naive S2-Guidance formulation: Dλ,ω θ (xt c) = Dθ(xt ϕ) + λ(cid:0)Dθ(xt c) Dθ(xt ϕ)(cid:1) ω (cid:88) i= ˆDθ(xt c, mi). (19) A.1.4 THEORETICAL INTERPRETATION AND DECOMPOSITIONS This derivation provides much deeper understanding of Naive S2-Guidance. Decomposition of Predictive Components. We can rearrange Eq. 18 to analyze the contribution of each component to the final prediction: Dλ,ω θ (xt c) = (1 λ)Dθ(xt ϕ) + λDθ(xt c) ω µpost(xt c) = λDθ(xt c) (cid:124) (cid:125) (cid:123)(cid:122) MAP Guidance ω µpost(xt c) (cid:125) (cid:123)(cid:122) (cid:124) Bayesian Correction (cid:124) + (1 λ)Dθ(xt ϕ) (cid:125) (cid:123)(cid:122) Unconditional Prior . (20) (21) This shows clear trade-off: we leverage the strong guidance from the conditional MAP estimate (Dθ(xt c)) and the unconditional prior (Dθ(xt ϕ)), but temper both with Bayesian correction term that represents the consensus of diverse committee of model hypotheses. It acts to regularize the overconfidence of the single MAP estimate."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Aesthetic score gains brought by increasing the number of forward passes with stochastic block dropping at each time step. Gradient-Space Perspective. In diffusion models, the guidance is applied in the noise prediction space. Let ϵθ(xt, c) be the models noise prediction. The standard CFG-guided noise ϵCFG is: ϵCFG(xt, c) = ϵθ(xt, ϕ) + λ(ϵθ(xt, c) ϵθ(xt, ϕ)). (22) Our method introduces correction term directly in this space. Let ϵpost(xt, c) = Eq(θ)[ϵθ(xt, c; θ)] be the posterior mean of the noise prediction. Our corrected noise prediction becomes: ϵS2G(xt, c) = ϵCFG(xt, c) ω ϵpost(xt, c) = ϵCFG(xt, c) ω (cid:32) 1 (cid:88) i=1 (cid:33) ϵθ(xt, c; θi) . (23) (24) This reveals that Naive S2-Guidance is performing direct modification of the guidance vector at each step of the denoising process. The repulsion from the center of uncertainty is not an abstract concept but concrete vector subtraction in the high-dimensional noise space. Connection to Negative Ensemble Distillation. Our method can be framed as novel form of negative distillation applied at inference time. Standard ensemble distillation trains single model to mimic the average output of an ensemble. In contrast, Naive S2-Guidance uses the ensembles average prediction (µpost) not as target to be imitated, but as an anti-target to be actively repelled. This distillation-rejection mechanism is new and principled way to harness the wisdom of an ensemble without collapsing to its mean. In summary, Naive S2-Guidance is theoretically grounded method that leverages the principles of Bayesian model averaging and uncertainty quantification. It operationalizes the insight that highquality generation requires not only strong conditional guidance but also mechanism to actively avoid regions of high model uncertainty. Our derivation shows that subtracting the Monte Carlo average of stochastic sub-networks is direct and principled way to implement this avoidance, thereby correcting for the inherent limitations of single, deterministic generative model, as shown in Figure 9. A.2 COMPARATIVE ANALYSIS OF S2-GUIDANCE AND NAIVE S2-GUIDANCE Our investigation into the behavior of sub-networks reveals crucial property. We find that when the stochastic block-dropping ratio is constrained within specific range, the guidance provided by different sub-networks appears remarkably consistent. As illustrated in Figure 10, even when"
        },
        {
            "title": "Preprint",
            "content": "different blocks are dropped to form distinct sub-network configurations, their individual guidance effects on the models output distribution exhibit strong similarity. This consistent behavior is the key observation that motivates our simplification. If different, randomly drawn sub-networks provide qualitatively similar guidance, it suggests potential redundancy in the Naive S2-Guidance approach, which meticulously averages the predictions of many such subnetworks. This insight inspires us to seek plausible theoretical connection that would allow us to avoid the significant computational overhead of the naive method without sacrificing guidance quality. compelling theoretical lens through which to view this observed redundancy is the principle of unbiased estimation. Let us formalize the guidance terms. The computationally expensive guidance in Naive S2-Guidance can be seen as Monte Carlo approximation of posterior mean: GNaive = ω µpost(xt c) ω (cid:88) i=1 ˆDθ(xt c, mi). (25) In contrast, our simplified S2-Guidance employs stochastic guidance term from single sample: GS2-Guidance = ω ˆDθ(xt c, mt), where mt p(m). We posit that the relationship between these two can be understood by considering GS2-Guidance as an approximate unbiased estimator of the expected guidance GNaive. If this holds, the expectation of the single-sample stochastic guidance over all possible masks would be equivalent to the full-batch guidance term: (26) Ep(mt)[GS2-Guidance] = Ep(mt)[ω ˆDθ(xt c, mt)] = ω Ep(mt)[ ˆDθ(xt c, mt)] ω µpost(xt c) E[GNaive]. (27) (28) (29) This theoretical framing, while not strict proof, offers powerful rationale for our observations. It suggests that while the guidance from any single sub-network (GS2-Guidance) is stochastic, its direction, on average, should align with the stable guidance provided by the full ensemble (GNaive). This perspective provides solid theoretical motivation for why different sub-networks can guide the model consistently towards the same ideal distribution, as seen in our experiments. Therefore, our simplification from Naive S2-Guidance to S2-Guidance can be viewed not as an ad-hoc heuristic, but as principled step motivated by stochastic approximation theory. Further experiments, such as repeating the process with small number of samples (as shown in Figure 8), corroborate this perspective by demonstrating diminishing returns. This confirms the practical sufficiency of single stochastic sample, especially when its effect is temporally integrated over the diffusion trajectory."
        },
        {
            "title": "B MORE DETAILS ABOUT OUR EXPERIMENTS",
            "content": "B.1 TOY EXAMPLES B.1.1 MORE RESULTS OF TOY EXAMPLES To further analyze the guidance mechanisms, we visualize the full denoising trajectories for the 1D Bimodal Gaussian Distribution in Figure 9. The figure illustrates that while standard CFG and Autoguidance improve upon the unguided baseline, their final predictions consistently deviate from the distributions centered at -4 and 4. This visually demonstrates the mode-shifting problem discussed in the main paper. In stark contrast, the paths for both Naive S²-Guidance and our final S²-Guidance method are more direct and successfully converge to the correct endpoints. This suggests that our self-guidance signal effectively corrects the generation path at each timestep, preventing the model from settling in the suboptimal regions favored by other methods. B.1.2 NAIVE S2-GUIDANCE VERSUS S2-GUIDANCE IN TOY EXAMPLES In our methodology, we first proposed Naive S²-Guidance, which averages the predictions from multiple stochastic sub-networks to create robust negative guidance signal. However, this approach carries significant computational cost. To address this, we introduced our final, more efficient S²Guidance, which uses only single stochastic sub-network per timestep."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Visualization of Denoising Trajectories on the 1D Bimodal Gaussian Data. Each panel shows the paths taken by different guidance methods to generate samples targeting the ground truth modes at -4 and 4. The y-axis represents the denoising timestep (from start to end), and the x-axis shows the predicted sample value. While standard CFG and Autoguidance improve upon the unguided baseline, they consistently fail to reach the ground truth. In contrast, both Naive S²Guidance and our final S²-Guidance method successfully steer the generation process to the correct endpoints. The more direct paths of our methods indicate more efficient and accurate guidance signal throughout the entire denoising process. To validate that this simplification does not cause meaningful performance degradation, we conduct direct comparison on the 2D Gaussian mixture. As illustrated in Figure 10, the sample distributions generated by both methods are qualitatively indistinguishable across multiple independent runs. Both approaches effectively guide the generation process to the correct modes and prevent the mode collapse issues seen in standard CFG (see Figure 3 in the main paper). Given the negligible difference in performance, the substantial computational advantage of S²Guidance makes it the far more practical and efficient choice. This finding strongly supports our adoption of the simplified approach as our final method. B.1.3 DETAILS OF TOY EXAMPLES Below are the implementation details for the experiments on both synthetic and real-world data, as referenced in the main paper. All experiments were conducted using class-balanced datasets to assess the performance of our method. 1-D Bimodal Gaussian Distribution: This experiment was designed to test the models ability to stably and completely capture both modes of bimodal distribution. The groundtruth data is an equally-weighted mixture of two Gaussians. The diffusion model, parameterized by standard neural network, was trained for iterations to reconstruct the target distribution. Analysis involved visualizing the final sample distribution and denoising trajectories to show that S2-Guidance consistently covers both modes, whereas the baseline may exhibit instability or mode preference (see Figure 3 in the main paper). 2-D Gaussian Mixture (4-Modes): This experiment assessed the models capacity to generate samples from disconnected, multi-modal manifold. The data consisted of an equally-weighted mixture of 4 isotropic Gaussians, with means located at (4, 4), (4, 4), (4, 4), and (4, 4). The analysis focused on the final distribution and denoising paths to demonstrate that S2-Guidance successfully captures all 4 distinct modes, improving upon the baselines mode coverage. Real-Image Data (CIFAR-10): To validate S2-Guidance on high-dimensional data, we used class-balanced dataset from CIFAR-10, consisting of 5,000 horse images and"
        },
        {
            "title": "Preprint",
            "content": "5,000 car images. The diffusion model employed neural network parameterization common for image tasks. The primary goal of the analysis was to assess the quality and class-separability of the generations. To this end, we generated 3,000 images and visualized their CLIP (ViT-B/32) features in 2-D using t-SNE. The resulting plot demonstrates that S2-Guidance produces more distinct and well-separated class clusters compared to the baseline, indicating higher-quality and less ambiguous generations (see Figure 4 in the main paper). B.2 EXTENDED EVALUATIONS In addition to the main experiments conducted with SD3 and SD3.5, we further evaluate our method using Flux, state-of-the-art (SOTA) model for text-to-image generation. Note that Flux is CFGdistilled model, meaning that directly applying classifier-free guidance (CFG) may lead to different results. We use De-distilled version of Flux Labs (2024) in our experiments. Additionally, we follow the same benchmark setting as HPSv2.1 to ensure consistency and comparability."
        },
        {
            "title": "Method",
            "content": "HPSv2.1(%) Qalign"
        },
        {
            "title": "Paintings",
            "content": "Photo Avg. CFG Ours 31.29 31.48 29.85 30.21 30.03 30.48 28.16 28. 29.84 30.26 4.65 4.70 Table 4: Quantitative evaluation of CFG and our approach using Flux under the HPSv2.1 benchmark. The HPSv2.1 grouping evaluates different styles, while Qalign measures aesthetic quality. Higher scores () are better. Best results are in bold. The results in Table 4 show that our method consistently outperforms the baseline CFG across different categories, including Anime, Concept Art, Paintings, and Photo. Specifically, we observe an average improvement of 0.42, highlighting the robustness and effectiveness of our approach. For more qualitative results, please refer to Figure 12 and Figure 13. These comprehensive results demonstrate the effectiveness of our proposed approach across various scenarios. B.3 USER STUDY To quantitatively evaluate the perceptual quality and prompt fidelity of our method, we conducted comprehensive user study comparing S2-Guidance against four strong baselines: CFG Ho & Salimans (2022), APG Sadat et al. (2024a), CFG++ Chung et al. (2024), and CFG-Zero Fan et al. (2025). The evaluation was performed on images generated from diverse set of diffusion models to assess the generalizability of our approach. We recruited 14 participants with expertise in computer vision and generative AI. For each evaluation instance, participants were presented with text prompt and the corresponding images generated by all five methods, displayed in randomized order to prevent bias. Participants were instructed to evaluate the results based on three key criteria: Detail Preservation: The clarity, sharpness, and richness of details in the generated image. Color Consistency: The naturalness, harmony, and realism of the colors. Image-Text Alignment: How well the generated image accurately reflects the content and intent of the text prompt. For each criterion, participants were asked to select the image (or images) they found to be the most successful. This design allows for multiple selections if participant deems more than one result to be of high quality for given aspect, thereby capturing more nuanced assessment of performance. The results of the user study are presented in Figure 11. The findings demonstrate clear and consistent preference for our proposed method, S2-Guidance, across all evaluated metrics. Specifically, in the Detail Preservation category, S2-Guidance was preferred in 32.5% of cases, significantly outperforming the runner-up, CFG (18.3%). similar dominant trend is observed for Color Consistency, where S2-Guidance achieved 29.6% preference rate. Furthermore, for Image-Text Alignment, our method was chosen 31.1% of the time, again marking substantial lead over all baselines. Aggregating the votes, the Overall preference for S2-Guidance stands at 31.0%, confirming its comprehensive superiority. This strong performance in human evaluations validates that S2-Guidance not only improves guidance from theoretical standpoint but also translates to tangible and perceptually superior generation quality that is easily recognized by human users."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "B.4 To ensure fair comparisons, the implementation details of our experiments are as follows: For the text-to-image comparisons, we used SD3 and SD3.5 Esser et al. (2024); AI (2024) with the guidance scale set to 7.5. For our scale parameter ω, we set it to 0.25. For the text-to-video comparisons, we use guidance scale of 5.0. Similarly, our scale parameter ω is set to 0.25. All other hyperparameters are set to the default configurations of the respective models. For the baseline comparisons, we follow the original implementations provided in their official repositories. Specifically, APG Sadat et al. (2024a) and CFG++ Chung et al. (2024) are implemented using the community-contributed versions that are integrated into the Diffusers framework. All experiments are conducted on NVIDIA H20 GPUs with 96GB memory. B.5 DETAILED PROMPTS FOR FIGURE 1 This section provides the prompts used to generate the visual results presented in Figure 1. The examples are referenced by their grid position in the figure (row, column). (Top, 1) Astronaut in space (Video): An astronaut flying in space. (Top, 2) Floating Castle (Image): magnificent castle sitting high on floating island above the clouds. Fluffy clouds surround the base of the island and form the text S2 Guidance Is All You Need in romantic, swirling style. The castle is adorned with towers, golden lights twinkling in the windows, and vines of blooming flowers climbing its walls. The scene is lit by warm, golden light glowing from the sun, with starry heaven faintly visible on the horizon. (Top, 3) Abstract Portrait (Image): The bold dramatic strokes of the painters brush created stunning abstract masterpiece work of emotional depth and intensity. (Top, 4) Cat with Rocket (Image): cat sitting besides rocket on planet with lot of cactuses. (Top, 5) Sports Car Driving (Video): car accelerating to gain speed. (Bottom, 1) Woman with Colored Powder (Video): close-up of beautiful womans face with colored powder exploding around her, creating an abstract splash of vibrant hues. (Bottom, 2) Woman with Umbrella (Image): woman sitting under an umbrella in the middle of restaurant. (Bottom, 3) Man Running on Beach (Image): man is running his hand over smooth rock at the beach. (Bottom, 4) Clay Sheep (Image): red book and an ivory sheep. (Bottom, 5) Bear Climbing Tree (Video): bear climbing tree."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: More Visual Comparisons of Naive S²-Guidance and S²-Guidance on the 2D Gaussian Mixture. Left: Naive S²-Guidance. Right: S²-Guidance. Each row corresponds to different random seed. The generated sample distributions are virtually identical, demonstrating that the performance gain from the computationally intensive naive approach is minimal. This justifies our adoption of the more efficient S²-Guidance method."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Human preference evaluation results for S2-Guidance against baseline methods. The bar charts show the percentage of times each method was selected as the best for three criteria: Detail Preservation, Color Consistency, and Image-Text Alignment, along with an Overall aggregated score. Our method, S2-Guidance, is significantly preferred by human evaluators across all categories, achieving preference rate of over 29% in every dimension and surpassing 30% overall. This demonstrates its robust ability to generate perceptually superior images."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Qualitative comparison of S2-Guidance with baseline methods. Our method consistently generates images with superior visual quality, better prompt alignment, and fewer artifacts across variety of prompts. For instance, S2-Guidance excels at stylistic replication (row 4), complex concept combinations (row 5)."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Further qualitative comparisons of S2-Guidance against baseline methods. Our approach demonstrates robust improvements in both prompt fidelity and aesthetic quality. Key advantages include accurate attribute binding (e.g., oval sink and rectangular mirror in row 2), faithful character and style generation (rows 3, 4, 5), and superior handling of lighting and composition (rows 6, 7). S2-Guidance consistently avoids the conceptual blending and visual artifacts that affect 12 other methods."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "CASIA",
        "Tsinghua University"
    ]
}