{
    "paper_title": "TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents",
    "authors": [
        "Jongwon Jeong",
        "Jungtaek Kim",
        "Kangwook Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here."
        },
        {
            "title": "Start",
            "content": "TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents Jongwon Jeong 1 Jungtaek Kim 1 Kangwook Lee 1 2 3 6 2 0 2 3 2 ] . [ 1 3 3 6 9 1 . 2 0 6 2 : r Abstract Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into graph and employing an external solver to identify feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here. 1. Introduction Language Models (LMs) have evolved into agents capable of understanding and interacting with external environments such as computers (Xi et al., 2025), simulation platforms (Park et al., 2023), and physical robot environments (Hu et al., 2023). In these applications, tools serve as interfaces that enable LM agents to reason, act, and observe within these environments (Li, 2025). For instance, search tools retrieve information from databases (Jin et al., 2025), mouse and keyboard tools interact with computer 1Electrical and Computer Engineering, University of WisconsinMadison 2KRAFTON 3Ludo Robotics. Correspondence to: Kangwook Lee <kangwooklee@krafton.com>. Preprint. February 24, 2026. 1 interfaces (Xie et al., 2024), coding tools execute programs within computers (Gao et al., 2023), and robotic tools perform actions in the physical world (Joublin et al., 2024). By generating executable prompts for tool usage, LM agents can iteratively think and act while receiving observations from the environment, referred to as ReAct frameworks (Yao et al., 2023b; Shinn et al., 2023; Hao et al., 2023; Yao et al., 2023a; Zhuang et al., 2024; Qiao et al., 2024; Kim et al., 2025). Through this iterative process, they can solve complex tasks that require planning and adaptation (Parisi et al., 2022; Gao et al., 2023). Although the ReAct frameworks have shown impressive capabilities in various domains (Liu et al., 2024), these frameworks are highly ineffective when even few mistakes can cause irrecoverable harms, making it infeasible to achieve the goal thereafter. In practice, feasibility constraints, such as time and cost budgets, limits on tool usage, and strict safety requirements, are one common source of irrecoverable failures. For instance, coding agents often operate under latency or API-cost budgets (Kim et al., 2024; Zheng et al., 2024; Liu et al., 2025), robotic agents must satisfy strict safety constraints (Yang et al., 2024b; Guo et al., 2025), and life-simulation agents face API-usage limits (Park et al., 2023; Choi et al., 2025). Under such constraints, incorrect tool use can exhaust the remaining budget or violate constraints, leaving no feasible sequence of tools that reaches the goal. This motivates our research question: How can LM agents maximize the success rate on tasks in the presence of irrecoverable failures? To address this question, we first analyze ReAct frameworks and identify two distinct sources of irrecoverable failures: planning error (Valmeekam et al., 2023) and sampling error (Hao et al., 2025a). planning error occurs when an agents internal planning (i.e., reasoning) is imperfect, leading it to recommend non-viable action (see Figure 1a, left). sampling error can occur even when the agents internal reasoning is correct, because stochastic token generation may generate an action different from the planned one (see Figure 1a, right). By simulating simplified agent where we inject these errors into feasible policy, we find that both TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Figure 1. Overview. We illustrate our work using Sokoban, where the goal is to push all boxes onto target locations. (a) Sources of Irrecoverable Failure in the ReAct Framework. planning error occurs when the internal reasoning suggests non-viable action (e.g., pushing box against wall); this makes the goal unachievable as the agent cannot pull the box from the wall, while sampling error arises when LM stochasticity leads to an action deviating from the plan. (b) Conceptual Toy Analysis. We model simplified agents by injecting planning and sampling errors into feasible policy for Sokoban. We measure success rates as the task step increases, observing that existing frameworks degrade rapidly as grows. See Appendix for details. (c) Our Framework. TAPE generates and aggregates multiple plans into graph and uses solver to select feasible path, thereby reducing planning errors. Then, it enforces constrained execution to suppress sampling errors. failures compound as the task horizon increases, reducing the overall success rate (see Figure 1b). Similar issues persist for Plan-and-Act (PA) frameworks (Wang et al., 2023a; Erdogan et al., 2025; Sun et al., 2023). While generating full plan before execution mitigates sampling errors, PA remains brittle to planning errors, resulting in suboptimal success rates in our analysis (see Figure 1b). Refer to Section 4 for the theoretical validation of this analysis. To mitigate failures from both error sources, we propose an external Tool-guided Adaptive Planning with constrained Execution framework (TAPE), as shown in Figure 1c. Specifically, our framework generates multiple candidate plans and aggregates them into plan graph, then uses an external solver (e.g., Integer Linear Programming (ILP) (Schrijver, 1998)) to select feasible path, mitigating failures due to planning errors by optimally selecting among diverse candidates. Next, our method enforces constrained execution by constrained decoding (Willard & Louf, 2023) to the selected next action, which suppresses sampling errors. At each step, if TAPE detects mismatch between the predicted and realized observations, it updates the plan graph and re-selects feasible plan. As shown in Figure 1b, our framework achieves higher success rate than other frameworks. We evaluate the proposed framework on benchmarks built from Sokoban (Schrader, 2018), ALFWorld (Shridhar et al., 2021), MuSiQue (Trivedi et al., 2022) and GSM8KHard (Gao et al., 2023) by adding feasibility constraints, e.g., budgets or tool/action limits, that make mistakes difficult to recover from. Experimental results show that our framework consistently outperforms the ReAct frameworks across all benchmarks. The performance gap is most significant in hard tasks and for weaker base models, proving that our framework effectively mitigates the irrecoverable failures. Finally, our ablation study confirms that all proposed components are essential for achieving success in environments where failures are irrecoverable. To sum up, our contributions are as follows: We formalize planning and sampling errors as two sources of irrecoverable failures in the ReAct frameworks, and theoretically characterize their impact on success probability under feasibility constraints. We propose an external tool-guided adaptive planning with constrained execution framework. TAPE mitigates 2 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents planning errors via solver-based path selection over plan graph with replanning, and reduces sampling errors via constrained decoding of planned actions. We construct constrained variants of existing agent benchmarks and show consistent success-rate improvements over other frameworks. 2. Problem Formulation 2.1. Agentic Task We consider an agentic task in which an LM agent iteratively interacts with an external environment to solve given problem. We formalize this process as goal-conditioned Markov Decision Process (G-MDP) (Nasiriany et al., 2019), denoted by = (G, A, S, P, R). Here, is the goal space, is the action space, is the state space, denotes the transition dynamics, and : {0, 1} is goal-dependent reward function, where we write Rg(s). At the beginning of an episode, goal is given. When actions incur costs and the agent operates under budgets, we define cost function : Rk and budget vector Rk. The state st is defined as the interaction history up to timestep t, i.e., st = (a0, o0, a1, o1, . . . , at1, ot1) with s0 = . At each timestep {0, 1, . . . }, the agent selects an action at conditioned on and st. The environment responds with an observation ot = (ostatus ), where ostatus indicates the execution status (e.g., success or failt ure), and oreturn denotes the returned value (e.g., tool outputs or error messages). If the task considers budget constraints, the observation includes the remaining-budget component, i.e., ot = (ostatus , oreturn ). Given st and at, the next state st+1 is realized according to ( st, at), which corresponds to appending (at, ot) to st. An episode τ = (s0, a0, s1, . . . , sT ) is successful if it terminates at state sT such that Rg(sT ) = 1 and, when budget constraints are present, (cid:80)T 1 t=0 C(st, at) B, where denotes element-wise inequality. Such budgets can represent time limits, cost limits, limits on the number of tool calls, or so on (Zheng et al., 2024; Liu et al., 2025; Ma et al., 2026). If the agent enters dead-end state (e.g., by violating the budget constraint), from which reaching the goal is impossible regardless of subsequent actions and transitions, we terminate the episode at sT and set Rg(sT ) = 0. , obudget , oreturn t 2.2. Language Model Agent Framework An LM agent is defined as stochastic policy πθ(at st, g) that selects an action given the current environment state st and goal g. Since the agent cannot observe the concrete return values until it actually interacts with the environment, it must rely on an internal world model to simulate and evaluate potential trajectories be3 , . . . , at1, ostatus forehand (Hao et al., 2023). To facilitate this internal planning, the agent operates on an abstract state ˆst := z(st) := (a0, ostatus ), which distills the raw his0 tory into essential execution statuses. In constrained GMDP, this representation is extended to ˆsc := zc(st) := , obudget ), . . . , at1, (ostatus (a0, (ostatus )) to track 0 t1 resource consumption. We assume the agent has an internal world model Pθ(ˆst+1 ˆst, at), Rg,θ(ˆsT ), and Cθ(ˆsc , at) that approximate the environments true transition dynamics , abstract reward Rg, and cost function C. Using the internal world model, the LM agent can perform internal planning to identify an action path that maximizes rewards. , obudget 0 t1 ReAct Framework. Following ReAct (Yao et al., 2023b), we decompose the agents decision-making into two stages, Think and Act. At each step t, Think contains various types of textual reasoning (Yao et al., 2023b; Shinn et al., 2023; Kim et al., 2025) or advanced planning strategy (Hao et al., 2023; Liu et al., 2023b; Yao et al., 2023a; Zhuang et al., 2024; Qiao et al., 2024; Qian et al., 2025; Katz et al., 2024). We interpret this Think as planning over an internal abstract world model (Hao et al., 2023). Starting from the current interaction state, the agent predicts future abstract states over lookahead horizon and outputs planned next action ˆat (Liu et al., 2023b). Act then samples an executed action conditioned on the state and the planned action. Accordingly, this decision process is formalized as ˆat πThink θ ( st, g), at πAct θ ( st, ˆat, g). (1) 0 , a0, ˆsˆτ 1 , a1, . . . , ˆsˆτ Plan-and-Act Framework. Plan-and-Act (PA) framework leverages an internal abstract world model to perform full planning before step-wise execution (Wang et al., 2023a; Xu et al., 2023; Xiong et al., 2025; Erdogan et al., 2025). Given goal g, the agent explores the abstract state space and produces an abstract plan ˆτ = (ˆsˆτ ), where the planned action at step is induced by the internal world model (Pθ, Rg,θ, Cθ) through planning, i.e., at πThink , g). Once the plan ˆτ is provided incontext, the agent conditions its step-wise decision on the current interaction state st and the plan guidance. We model PAs in-context use of the provided plan as follows. When the plan is applicable at step t, meaning that the current abstract state matches the abstract state, z(st) = ˆsˆτ , the PA agent directly follows the planned action with probability [0, 1]. Otherwise, it falls back to the ReAct-style decision process. Formally, if z(st) = ˆsˆτ , ( ˆsˆτ θ (cid:40) at = at, aReAct with probability pfollow, , with probability 1 pfollow, (2) , at = aReAct and if z(st) = ˆsˆτ st, g), aReAct πAct θ planned next action produced by ReAct. ( ( st, at, g). Here, at denotes the , where at πThink θ TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents 2.3. Errors from Two Different Sources LM agents are prone to fail agentic tasks due to internal planning error and sampling error in LMs. In practice, planning error arises from limited planning capability or mismatch between the agents internal world model and the true world model (Valmeekam et al., 2023). The sampling error can arise from stochastic token generation, prompt sensitivity, and formatting drift (Hao et al., 2025a). To quantify both errors, in G-MDP, we define viable actions as in Definition 2.1. Definition 2.1. Define the viable action set as Aviable(st) (cid:110) A(cid:12) := (cid:111) (cid:12)K 1, Pr(cid:0)Rg(st+K) = 1st, a(cid:1) > 0 . (3) Using the viable action set, we formalize planning and sampling errors as follows. For simplicity, we assume constant per-step error rate that is time-invariant. Assumption 2.2 (Planning Error). Let ˆat denote the agents planned action at step t. For simplicity, we model the planning error as the constant ϵp := Pr (ˆat / Aviable(st)) . (4) Assumption 2.3 (Sampling Error). Let ˆat denote the agents planned action at step t. For simplicity, we model sampling error by the following constant rates. ϵs := Pr(at = ˆat), (5) Moreover, we assume that ϵs does not depend on whether the planned action is viable, i.e., Pr(at ˆat Aviable(st)) = Pr(at = ˆat ˆat / Aviable(st)) = ϵs. = ˆat We further characterize the consequences of sampling error as δb and δr in Assumption 2.4. δb quantifies how often the sampling error breaks viability when the planned action is viable, while δr captures how often the sampling error recovers viability when the planned action is non-viable. Assumption 2.4. When the planned action ˆa is viable, δb denotes the probability that deviating from it breaks viability as δb := Pr(cid:0)at / Aviable(st) (cid:12) (cid:1). (cid:12) ˆat Aviable(st), at = ˆat On the other hand, when the planned action is non-viable, δr denotes the probability that deviating from it recovers viability as δr := Pr(cid:0)at Aviable(st) (cid:12) (cid:1). (cid:12) ˆat / Aviable(st), at = ˆat 3. Our Approach: TAPE In this section, we introduce TAPE (Tool-guided Adaptive Planning with constrained Execution), framework designed to reduce both planning and sampling errors. First, our framework performs (i) plan graph construction by recombining multiple abstract trajectories generated by an LM, which increases the probability that feasible plan exists within the planning space (Section 3.1). Next, TAPE operates (ii) plan path selection on this graph using an external solver, such as Integer Linear Programming (ILP) (Schrijver, 1998), based on predicted costs and rewards (Section 3.2). Given the constructed plan graph and its predicted costs and rewards, the solver returns an optimal feasible path within the graph when one exists, effectively reducing planning errors. Then, our framework performs (iii) constrained execution by restricting the action space at decoding time (Willard & Louf, 2023) so that only the next prescribed action on the selected path is admissible, thereby suppressing sampling errors (Section 3.3). Lastly, to remain robust against mismatches between realized observations and the internal plan, the framework adaptively re-performs (iv) plan graph construction and path selection using newly observed information whenever discrepancy arises (Section 3.4). conceptual overview on ALFWorld (Shridhar et al., 2021) is shown in Figure 2. 3.1. Plan Graph Construction (cid:17) , a(m) 1 , a(m) 0 , . . . , ˆs(m) Lm Graph Construction. To construct plan graph = (V, E), where is state node and is an edge representing an action a, TAPE first generates abstract (cid:16) , ˆs(m) ˆs(m) plans ˆτ (m) = , where 1 0 = 1, . . . , , ˆs(m) ℓ ˆS, and a(m) ℓ A. Then, our framework folds these sampled paths by merging states that share the same core information. Specifically, TAPE merges multiple abstract states ˆs into single node if they represent the same observation and task progress. For example, in ALFWorld, many distinct states (e.g., different intermediate tool outputs or error messages) correspond to the same node representing the agents current location, the objects in the inventory, and the task progress (such as whether required object has been found, cleaned, or placed). We denote this merging function as fθ : ˆS V. For the merged nodes, TAPE reassigns edges as eij = (cid:0)vi (cid:1) E, where vi = fθ(ˆs(m) ℓ+1) for each consecutive pair ℓ in the sampled plans. ) and vj = fθ(ˆs(m) a(m) ℓ vj Score and Cost Prediction. Using the internal abstract world model (Pθ, Rg,θ, Cθ) in LMs, TAPE assigns scalar reward to each node and predicted cost to each edge E. Specifically, for terminal nodes vter, our framework estimates the expected reward as ˆrθ(u) := Eθ[Rg,θ(ˆs) fθ(ˆs) = vter], representing the expected reward of being in vter. For each edge = (v v) E, TAPE predicts the cost vector as ˆcθ(e) := Eθ[Cθ(ˆs, a) fθ(ˆs) = v], which estimates the expected budget consumption of executing the edge-labeled action at node v. In constrained settings, these predicted costs {ˆcθ(e)}eE are used to enforce feasibility under the remaining budget bt. 4 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Figure 2. Overview of TAPE. The proposed framework consists of four steps. (a) Plan Graph Construction: The LM samples multiple trajectories, which are aggregated into plan graph with predicted costs and scores. (b) Plan Path Selection: An external solver (e.g., ILP) identifies the optimal path (blue arrows) subject to constraints. (c) Constrained Execution: The agent executes the selected actions using constrained decoding to eliminate sampling errors. (d) Mismatch Check: If mismatch occurs between the predicted and realized state, the agent re-performs plan graph construction and path selection; otherwise, the agent executes the next planned action. 3.2. Plan Path Selection Given the graph = (V, E), the current node v0, and the set of terminal nodes Vter , our framework selects single directed walk on by solving an optimization problem via an external solver (e.g., Integer Linear Programming (ILP) (Schrijver, 1998)). To obtain tractable finite-size ILP, the solver optimizes over finite horizon Lmax 1 using time-expanded formulation. For each edge = v) and step ℓ {0, 1, . . . , Lmax 1}, let (v xe,ℓ {0, 1} indicate whether the walk takes edge at step ℓ. src(e) = and tgt(e) = are the source and target of e. In the G-MDP setting, the ILP formulation is as follows: max s.t. Lmax1 (cid:88) (cid:88) (cid:0)tgt(e)(cid:1) xe,ℓ ˆrθ (6) eE xe,ℓ = 1, ℓ = 0, . . . , Lmax 1, (7) ℓ=0 (cid:88) eE (cid:88) xe,0 = 1, eE: src(e)=v0 (cid:88) xe,Lmax1 = 1, eE: tgt(e)Vter (cid:88) (cid:88) xe,ℓ1 = xe,ℓ, eE:src(e)=v eE:tgt(e)=v V, ℓ {1, . . . , Lmax 1} xe,ℓ {0, 1}, E, ℓ. (8) (9) (10) (11) Equation (6) maximizes the total accumulated reward of the visited nodes. The constraints ensure the validity of the selected path. Specifically, Equation (7) ensures that the agent takes exactly one action at each step. Equation (8) and Equation (9) specify that the path starts at v0 and ends 5 at one of the terminal nodes in Vter at step Lmax. Finally, Equation (10) ensures that if the agent arrives at node at step ℓ 1, it must depart from at step ℓ, thereby forming continuous walk. If there exist budget constraints, we can formulate the optimization problem by adding the budget constraints, shown in Appendix C. 3.3. Constrained Execution 0 vπ 1 Given the optimal path selected by the solver, denoted as π = (vπ 1 ), TAPE executes the plan by 0 restricting the admissible action set to the prescribed action whenever the path is applicable. Let vt be the current node in the environment at step t. If the current node matches the planned node (i.e., vt = vπ ), we enforce the LM πAct to generate the planned action as at. This is implemented via constrained decoding (Willard & Louf, 2023), which constrains the LM to follow by fixing the tool choice and enforcing the exact tool-call format. θ 3.4. Mismatch Check and Replanning In practice, mismatch between the predicted state in the plan and the realized state can invalidate the current path. For example, the environment might transition to an unexpected state, or the remaining budget might evolve differently than estimated. To address this, our framework verifies the consistency between the real state node ˆst+1 and the predicted state node vπ t+1. Specifically, if TAPE confirms that vt+1 matches vπ t+1, TAPE proceeds to execute the next planned action along π. Conversely, if our framework detects mismatch, TAPE regenerates multiple abstract plans, constructs new plan graph with updated costs and rewards, and solves for new optimal path via the external solver. TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents 4. Theoretical Analysis 4.2. Our Framework In this section, we analyze the success probability of agents within the framework in Section 2.2. Based on the planning and execution errors in Section 2.3, we explain how these errors propagate and reduce the overall success probability. We define the success probability as follows. τ = Definition 4.1 (Success Probability). Let (s0, a0, s1, . . . ) denote random trajectory generated by the agent, and let Tg := min{t 0 : Rg(st) = 1} be the first goal-reaching step, with the convention min := . We define the success event as := {Tg < } (cid:84)Tg1 t=0 {at Aviable(st)}, and we refer to Pr(S) as the success probability. 4.1. Existing Frameworks From Section 2, we derive the upper bound of the success probability of the ReAct framework as follows. Proposition 4.2. Assume that any successful trajectory must take at least action selections. Then, the ReAct success probability is bounded by UReAct: Pr(S) UReAct := (cid:16) (1ϵp)(1ϵsδb)+ϵpϵsδr (cid:17)T , (12) where equality holds when the task exactly ends at . If (1 ϵp)δb ϵpδr, UReAct increases as ϵp and ϵs decrease. Proposition 4.2 implies that as grows, per-step errors compound and sharply reduce the overall success rate. Also, if (1 ϵp)δb ϵpδr, the success probability can be increasing by reducing planning and sampling error. (1 ϵp)δb ϵpδr is reasonable since deviations from the planned one are more likely to break viability than to recover it in agents. Plan-and-Act (PA) framework utilizes pre-generated plan as in-context guidance. This guidance effectively reduces execution stochasticity, lowering the sampling error. This comparison is formalized in Proposition 4.3. Proposition 4.3. Let α := Pr(z(st) = ˆsˆτ ) denote the probability that the plan is aligned at each step. Under the same assumptions in Proposition 4.2, we have the upper bound of success probability for PA as UPA := (cid:0)(1 ϵp)(cid:0)1 ϵs,PAδb (cid:1) + ϵpϵs,PAδr , where ϵsPA = (1 αpfollow)ϵs. Consequently, we have the following: (cid:1)T UPA UReAct, (13) where equality holds when α = 0 or pfollow = 0. Proposition 4.3 suggests that the upper bound of success probability of PA is higher than that of ReAct due to reducing the sampling errors from ϵs to ϵsPA. However, note that the planning error ϵp remains. TAPE aggregates sampled plans into plan graph. This structure increases the diversity of action candidates at each step. Let vt denote the planning node in the plan graph corresponding to the current state st, i.e., vt = fθ(st). We define the set of candidate actions at vt as ˆA(vt) := {a ut+1) E}, and let d(vt) := ˆA(vt) be the (vt number of distinct actions proposed by the aggregated plans. Note that d(vt) 1 for any non-terminal node vt. Proposition 4.4. Assume that task requires steps, the external solver selects viable action whenever one exists, and constrained decoding eliminates sampling error (i.e., ϵs 0). Then, the upper bound of the success probabil- (cid:0)1 (ϵp)d(vt)(cid:1). ity for TAPE is given by Uours := (cid:81)T 1 Consequently, we have t=0 Uours UPA UReAct, (14) where the equality between Uours and UPA holds if and only if d(vt) = 1 and ϵs,PAδb = ϵs,PAδr = 0. Proposition 4.4 confirms that TAPE theoretically guarantees the highest success probability among the compared frameworks by exponentially reducing planning errors from ϵp to (ϵp)d(vt) via selecting viable action from multiple candidates, while also eliminating sampling errors ϵs 0 via constrained decoding. 5. Empirical Analysis We compare TAPE against two representative agent frameworks: (i) ReAct (Yao et al., 2023b) which interleaves reasoning and acting, and (ii) Plan-and-Act (PA) (Erdogan et al., 2025) which executes pre-generated plan. We note that various advanced prompting or reflection techniques can be orthogonally integrated into these frameworks; in this paper, we adapt Xiong et al. (2025) for the implementation. Therefore, we focus on evaluating the fundamental structural differences between the frameworks. For the evaluation, we consider four benchmarks characterized by frequent irrecoverable states: Sokoban, ALFWorld, GSM8K-Hard, and MuSiQue. Sokoban (Schrader, 2018) is classic planning puzzle requiring the agent to push boxes to target locations without creating deadlocks. ALFWorld (Shridhar et al., 2021) involves embodied decisionmaking in simulated household environment. We also include MuSiQue (Trivedi et al., 2022) for multi-hop factual reasoning using retrieval tools and GSM8K-Hard (Gao et al., 2023) for mathematical reasoning using arithmetic tools. We report the success rate for each task. For more details about the experimental setting, see Section E.1. 6 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Figure 3. Success rates across four agentic tasks. We evaluate our framework against ReAct and Plan-and-Act on Sokoban, ALFWorld, Musique, and GSM-Hard. We use gpt-4.1-mini for LM backbone. We find that TAPE consistently demonstrates superior performance over existing frameworks in both easy and hard settings. Table 1. Planning and sampling error analysis on Sokoban. We estimate planning and sampling error rates across all states and find that both errors occur in existing frameworks. TAPE substantially reduces both errors, showing an improvement in success rate. Best is in bold and second-best is underlined. Framework Planning Error (%) Sampling Error (%) Success Rate (%) ReAct Plan-and-Act TAPE 50.7 1.8 47.7 1.8 36.7 1.9 8.3 1.0 4.7 0.8 0.0 0.0 5.0 2.2 17.0 3.8 46.0 5.0 5.1. Overall Results The overall results on agentic benchmarks using gpt-4.1-mini (OpenAI, 2025) are summarized in Figure 3. We compare TAPE with ReAct and PA across four benchmarks characterized by frequent irrecoverable states, and find that TAPE consistently outperforms the baselines. In particular, TAPE improves over ReAct, suggesting that our method mitigates both planning and sampling errors, whereas ReAct suffers from their accumulation. Additionally, PA frequently outperforms ReAct, which may indicate that providing an explicit plan as in-context guidance helps reduce sampling errors. Furthermore, TAPE consistently surpasses both ReAct and PA. As shown in Section G.2, we additionally compare against Best-of-N (Wang et al., 2023b; Kang et al., 2025) of ReAct and PA, which sample the same number of plans at each step as TAPE, and observe that TAPE still consistently outperforms them. Overall, these results suggest that TAPE successfully minimizes the planning and sampling errors inherent in existing frameworks. Notably, we observe substantial performance gains even in scenarios where existing frameworks achieve near-zero success rates, indicating that TAPE can elevate the agents effective planning capability by enabling the discovery of viable paths in otherwise unsolved instances. 5.2. Analysis Planning & Sampling Errors. We compare planning and sampling errors across agent frameworks, as summarized in Table 1. We use Sokoban for this analysis because it admits an oracle shortest-path planner, which enables us to directly estimate the errors. Overall, lower planning and sampling error rates are associated with higher success rates. This result highlights that two sources of errors in Section 2.3 exist in practice and directly affect task success. When comparing ReAct and PA, PA reduces the sampling error by roughly 43.4% and slightly reduces the planning error by 3.0%, which is accompanied by gain in the success rate. This aligns with our claim in Proposition 4.3 that providing an in-context plan helps mitigate sampling errors while it does not effectively mitigate planning errors. Most importantly, TAPE significantly reduces planning errors while nearly eliminating sampling errors, leading to the largest improvement in the success rate. These results empirically support our analysis in Proposition 4.4 that plan selection with solver in plan graph reduces planning errors and constrained execution suppresses sampling errors. The implementation details of our error estimators in Sokoban are provided in Section G.1. Task Difficulty. We analyze how the performance of each framework varies with task difficulty as shown in Figure 3. As the difficulty increases, all frameworks exhibit performance decline. Specifically, the success rates of ReAct and PA drop by an average of 55.4% and 52.4%, respectively. In contrast, TAPE consistently maintains higher success rates across all tasks, with an average decrease of 27.7%. This result indicates that TAPE effectively mitigates planning errors as tasks become hard and mistaken action becomes more critical. Impact of LMs Planning Ability. We evaluate TAPE across several LMs with varying degrees of reasoning (or planning) capabilities (See Section E.1 for LM backbones). In Figure 4a, the models are arranged in ascending order of their capability, as evidenced by the monotonic increase in the success rate of ReAct from 22.0% to 66.0%. As illustrated in Figure 4a, TAPE consistently outperforms the baselines across all models. Notably, our method demonstrates exceptional efficacy on less capable models with limited planning ability; for instance, on gpt-4.1-mini, TAPE achieves relative improvement over ReAct. Even for highly capable models like gpt-5-nano, which already TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Table 2. Ablation study. We compare TAPE against variants that remove components from: (i) External Solver, (ii) Constraint Execution, and (iii) Replanning in Sokoban. checkmark () indicates the component is enabled; blank (-) indicates it is removed. Result shows that all components are crucial to improve the success rate. Best is in bold and second-best is underlined. External Solver Constrained Execution Replanning Success Rate - - - - - - - - - - - - 46.0 5.0 42.0 4.9 36.0 4.8 38.0 4.8 30.0 4.9 19.0 3.9 37.0 4.8 11.0 3.1 when all three are enabled. Removing the External Solver reduces success from 46.0 to 42.0, indicating that formal optimization improves plan quality beyond the LLMs planning ability. Disabling Constrained Execution causes larger drop from 46.0 to 36.0, showing that prompt-level hints alone do not sufficiently prevent stochastic action deviations. Similarly, removing replanning lowers success to 38.0, suggesting that mismatch checking and adaptation are important for recovering from execution-time errors and avoiding dead-end trajectories. Overall, the ablation confirms that the three components are complementary, addressing planning errors, sampling errors, and planexecution mismatches, respectively. 6. Conclusion In this paper, we proposed TAPE, framework designed to mitigate planning and sampling errors in LM agents. By aggregating multiple plans into graph and employing Integer Linear Programming, TAPE identifies feasible paths, thereby reduces the planning error. Furthermore, TAPE utilizes constrained execution to substantially reduce sampling error and performs adaptive replanning to handle environment and LMs knowledge mismatches. Our experiments demonstrate that TAPE significantly outperforms the ReAct framework and the Plan-and-Act framework, particularly in complex tasks and for models with limited planning capabilities. Limitations & Future Work. Despite TAPEs effectiveness, our framework presents several challenges for future research. First, the accuracy of the plan graph remains dependent on the LMs ability to correctly structure and merge states. Inaccurate graph construction can lead to plan space that does not faithfully represent the true environment, suggesting need for more advanced methods to ensure the structural integrity of the plan graph. Second, TAPE currently relies on pre-specified solver, which may limit its generality across tasks with different optimization formulations. Automatic solver selection based on the task objective is promising direction for improving generalization. (a) Success Rate across LMs (b) Sensitivity to Figure 4. Cross-model success rates and sensitivity to the number of generated plans in Sokoban. (a) Success rates of TAPE and baselines across LMs with different planning capabilities. TAPE consistently improves over other frameworks, with larger gains on weaker models, indicating effective mitigation of planning errors. (b) Sensitivity of TAPE to the number of generated plans used to construct the plan graph. The best performance is achieved at = 4, suggesting that moderate plan aggregation via node merging effectively expands the candidate action space. exhibit strong baseline performance, our method yields significant gain, relatively improving +48% over ReAct. This suggests that TAPE effectively mitigates planning errors due to models with lower reasoning ability, while eliminating remaining errors in stronger models. Sensitivity to . We investigate the impact of the number of generated plans (M ) used for graph construction on the success rate. As shown in Figure 4b, we find that = 4 yields the optimal performance. The performance gain observed when increasing from 2 to 4 is attributed to the aggregation of action candidates; as nodes merge, the number of outgoing edges increases. This reduces the planning errors for each node, supporting Proposition 4.4. However, we observe performance decline at = 8. This degradation may stem from the reliance on LMs for graph construction. As the number of paths increases, the LLM struggles to maintain global consistency, leading to reduced graph completeness and accumulated construction errors. Ablation Study. We study whether each component of our framework is necessary by selectively removing (i) the External Solver, (ii) Constrained Execution, and (iii) Replanning. Removing the external solver replaces the formal optimization step that selects constraint-feasible path on the plan graph (e.g., via an ILP solver) with direct LLMbased selection. Removing constrained execution means that we only provide the planned path as prompt-level hint and let the LLM freely generate actions without constrained decoding. Removing replanning disables mismatch checking and forces the agent to continue executing the planning-time path without adaptation. As shown in Table 2, each component of our framework contributes to performance, and the best result is achieved 8 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the National Science Foundation (NSF) Award DMS-2023239, the NSF CAREER Award CCF-2339978, Amazon Research Award, grant from FuriosaAI, and Google Cloud Research Credits Program. We appreciate Jaden Park from University of Wisconsin Madison, Chungpa Lee from Yonsei University, and Minki Kang and Moonseok Choi from KAIST for their valuable discussions."
        },
        {
            "title": "Impact Statement",
            "content": "This work focuses on advancing machine learning by improving the reliability of language model agents that operate under feasibility constraints such as time and cost budgets and tool-usage limits, which may enable more dependable and cost-aware automation and assistance in benign applications. However, increasing agent reliability and effectiveness can lower the barrier to misuse in harmful or unauthorized settings. In addition, our approach may increase inference-time computation due to plan generation, plan-graph construction, which can raise monetary cost and energy use. We encourage responsible deployment with safeguards such as access control, monitoring and logging, and rate limits, and we encourage practitioners to consider cost and efficiency when adopting the method. Models. In Proceedings of the International Conference on Machine Learning (ICML), pp. 1076410799. PMLR, 2023. Guan, L., Valmeekam, K., Sreedharan, S., and Kambhampati, S. Leveraging pre-trained Large Language Models to construct and utilize world models for model-based task planning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Guo, W., Kingston, Z., and Kavraki, L. E. CaStL: Constraints as specifications through LLM translation for long-horizon task and motion planning. In IEEE International Conference on Robotics and Automation (ICRA), 2025. Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D., and Hu, Z. Reasoning with Language Model is planning In Proceedings of the Conference with world model. on Empirical Methods in Natural Language Processing (EMNLP), 2023. Hao, Y., Chen, Y., Zhang, Y., and Fan, C. Large Language Models can solve real-world planning rigorously with formal verification tools. In Proceedings of the Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2025a."
        },
        {
            "title": "References",
            "content": "Anthropic. System card: https://www-cdn.anthropic.com/ 963373e433e489a87a10c823c52a0a013e9172dd. pdf, 2025. Claude Sonnet 4.5. Chen, X., Liang, C., Yu, A. W., Zhou, D., Song, D., and Le, Q. V. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In International Conference on Learning Representations, 2019. Choi, S., Lee, K., Sng, O., and Ackerman, J. M. Infected Smallville: How disease threat shapes sociality in LLM agents. arXiv preprint arXiv:2506.13783, 2025. Dagan, G., Keller, F., and Lascarides, A. Dynamic planning with LLM. arXiv preprint arXiv:2308.06391, 2023. Erdogan, L. E., Furuta, H., Kim, S., Lee, N., Moon, S., Anumanchipalli, G., Keutzer, K., and Gholami, A. Planand-Act: Improving planning of agents for long-horizon tasks. In Proceedings of the International Conference on Machine Learning (ICML), 2025. Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. PAL: Program-aided Language Hao, Y., Zhang, Y., and Fan, C. Planning anything with rigor: General-purpose zero-shot planning with LLMbased formalized programming. In Proceedings of the International Conference on Learning Representations (ICLR), 2025b. He, H., Yao, W., Ma, K., Yu, W., Dai, Y., Zhang, H., Lan, Z., and Yu, D. WebVoyager: Building an end-to-end web agent with large multimodal models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2024. Hu, Y., Xie, Q., Jain, V., Francis, J., Patrikar, J., Keetha, N., Kim, S., Xie, Y., Zhang, T., Fang, H.-S., et al. Toward general-purpose robots via foundation models: survey and meta-analysis. arXiv preprint arXiv:2312.08782, 2023. Huang, S., Lipovetzky, N., and Cohn, T. Planning in the dark: Llm-symbolic planning pipeline without experts. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2025. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can Language Models resolve real-world github issues? In Proceedings of the International Conference on Learning Representations (ICLR), 2024. 9 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. Search-R1: Training LLMs to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Liu, Z., Hu, H., Zhang, S., Guo, H., Ke, S., Liu, B., and Wang, Z. Reason for future, act for now: principled framework for autonomous llm agents with provable sample efficiency. arXiv preprint arXiv:2309.17382, 2023b. Joublin, F., Ceravola, A., Smirnov, P., Ocker, F., Deigmoeller, J., Belardinelli, A., Wang, C., Hasler, S., Tanneberg, D., and Gienger, M. CoPAL: corrective planning of robot actions with Large Language Models. In IEEE International Conference on Robotics and Automation (ICRA), 2024. Kang, M., Jeong, J., Lee, S., Cho, J., and Hwang, S. J. Distilling LLM agent into small models with retrieval and code tools. In Advances in Neural Information Processing Systems (NeurIPS), 2025. Katz, M., Kokel, H., Srinivas, K., and Sohrabi Araghi, S. Thought of Search: Planning with Language Models through the lens of efficiency. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Kim, J., Rhee, S., Kim, M., Kim, D., Lee, S., Sung, Y., and Jung, K. ReflAct: World-grounded decision making in LLM agents via goal-state reflection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2025. Kim, S., Moon, S., Tabrizi, R., Lee, N., Mahoney, M. W., Keutzer, K., and Gholami, A. An LLM compiler for parallel function calling. In Proceedings of the International Conference on Machine Learning (ICML), 2024. Li, X. review of prominent paradigms for LLM-based agents: Tool use, planning (including RAG), and feedback learning. In Proceedings of the International Conference on Computational Linguistics (COLING), 2025. Lin, J., Zhao, H., Zhang, A., Wu, Y., Ping, H., and Chen, Q. AgentSims: An open-source sandbox for large language model evaluation. arXiv preprint arXiv:2308.04026, 2023. Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P. LLM+P: Empowering Large Language Models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a. Liu, T., Wang, Z., Miao, J., Hsu, I., Yan, J., Chen, J., Han, R., Xu, F., Chen, Y., Jiang, K., et al. Budget-aware tool-use enables effective agent scaling. arXiv preprint arXiv:2511.17006, 2025. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. AgentBench: Evaluating LLMs as agents. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. Ma, Y., Li, L., Li, P., Li, X., Guo, Q., Lin, D., Chen, K., et al. Timely Machine: Awareness of time makes testtime scaling agentic. arXiv preprint arXiv:2601.16486, 2026. Nasiriany, S., Pong, V., Lin, S., and Levine, S. Planning with goal-conditioned policies. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Obata, K., Aoki, T., Horii, T., Taniguchi, T., and Nagai, T. Lip-LLM: Integrating linear programming and dependency graph with Large Language Models for multi-robot task planning. IEEE Robotics and Automation Letters, 2024. OpenAI. GPT-4.1. https://openai.com/index/ gpt-4-1/, 2025. Parisi, A., Zhao, Y., and Fiedel, N. TALM: Tool augmented Language Models. arXiv preprint arXiv:2205.12255, 2022. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, 2023. Piao, J., Yan, Y., Zhang, J., Li, N., Yan, J., Lan, X., Lu, Z., Zheng, Z., Wang, J. Y., Zhou, D., et al. AgentSociety: Large-scale simulation of LLM-driven generative agents advances understanding of human behaviors and society. arXiv preprint arXiv:2502.08691, 2025. Prasad, A., Koller, A., Hartmann, M., Clark, P., Sabharwal, A., Bansal, M., and Khot, T. ADaPT: As-needed decomposition and planning with Language Models. In Proceedings of the 2024 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), 2024. Qian, H., Bai, C., Zhang, J., Wu, F., Song, W., and Li, X. Discriminator-guided embodied planning for LLM agent. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Qiao, S., Fang, R., Zhang, N., Zhu, Y., Chen, X., Deng, S., Jiang, Y., Xie, P., Huang, F., and Chen, H. Agent planning with world knowledge model. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 10 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language Models can teach themselves to use tools. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Schrader, M.-P. B. gym-sokoban. https://github. com/mpSchrader/gym-sokoban, 2018. Schrijver, A. Theory of linear and integer programming. John Wiley & Sons, 1998. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Shridhar, M., Yuan, X., Cote, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. ALFWorld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. OpenAI GPT-5 system card. arXiv preprint arXiv:2601.03267, 2025. Sun, H., Zhuang, Y., Kong, L., Dai, B., and Zhang, C. AdaPlanner: Adaptive planning from feedback with Language Models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics (TACL), 2022. Valmeekam, K., Marquez, M., Sreedharan, S., and Kambhampati, S. On the planning abilities of Large Language Models-a critical investigation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. Plan-and-Solve prompting: Improving zero-shot chain-of-thought reasoning by Large Language Models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2023a. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2023b. Wang, Z., Chiu, Y. Y., and Chiu, Y. C. Humanoid Agents: Platform for simulating human-like generative agents. arXiv preprint arXiv:2310.05418, 2023c. 11 Willard, B. T. and Louf, R. ation for Large Language Models. arXiv:2307.09702, 2023. Efficient guided generarXiv preprint Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 2025. Xia, Y., Shen, Y. J., Wu, J., Yu, T., Kim, S., Rossi, R. A., Yao, L., and McAuley, J. SAND: Boosting LLM agents with self-taught action deliberation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 30623077, 2025. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., et al. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Xiong, W., Song, Y., Dong, Q., Zhao, B., Song, F., Wang, X., and Li, S. MPO: Boosting LLM agents with meta plan optimization. arXiv preprint arXiv:2503.02682, 2025. Xu, B., Peng, Z., Lei, B., Mukherjee, S., Liu, Y., and Xu, D. ReWOO: Decoupling reasoning from observations for efficient augmented Language Models. arXiv preprint arXiv:2305.18323, 2023. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. SWE-Agent: Agentcomputer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024a. Yang, Z., Raman, S. S., Shah, A., and Tellex, S. Plug in the safety chip: Enforcing constraints for LLM-driven robot agents. In IEEE International Conference on Robotics and Automation (ICRA), 2024b. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree Of Thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2023a. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. ReAct: Synergizing reasoning and acting in Language Models. In Proceedings of the International Conference on Learning Representations (ICLR), 2023b. Zhang, Y., Ma, Z., Ma, Y., Han, Z., Wu, Y., and Tresp, V. WebPilot: versatile and autonomous multi-agent system for web task execution with strategic exploration. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2025. TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Zheng, Y., Li, P., Yan, M., Zhang, J., Huang, F., and Liu, Y. Budget-constrained tool learning with planning. In Findings of the Association for Computational Linguistics (ACL), 2024. Zhuang, Y., Chen, X., Yu, T., Mitra, S., Bursztyn, V., Rossi, R. A., Sarkhel, S., and Zhang, C. ToolChain*: Efficient action space navigation in Large Language Models with A* search. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents A. Conceptual Toy Analysis Details To illustrate the distinction between planning and sampling errors, we implement the simple ReAct framework in Sokoban environment, as described in Section 2. At each step, the policy first executes Breadth-First Search (BFS) algorithm to obtain shortest plan from the current state and takes its first action as the default intended action. We then inject planning error with probability ϵp by replacing the intended action with an alternative available action that is non-viable under the remaining budget (if any), or otherwise with random alternative action. Finally, we inject sampling error with probability ϵs by stochastically flipping the executed action to different available action, while keeping the intended action unchanged. Thus, ϵp controls errors in action selection at planning time, whereas ϵs controls stochastic deviations at execution time. In our Sokoban setting, we set the action budget to the optimal solution length plus slack of 2 steps. In Figure 1, we set ϵp = 0.25 (planning error) and ϵs = 0.20 (sampling error). B. Related Work Agentic Tasks. Agentic tasks refer to problems in which an agent interacts with the external environment to accomplish the goal (Yao et al., 2023b). Recent advancements have demonstrated the possibility of LMs agentic ability across diverse domains, including robotic manipulation (Hu et al., 2023), graphical user interface (GUI) navigation (He et al., 2024), and software engineering (Jimenez et al., 2024; Yang et al., 2024a). Specifically, in life simulations, agents typically role-play as Non-Player Characters (NPCs), exhibiting human-like behaviors (Park et al., 2023; Choi et al., 2025). However, the impressive capabilities of these agents often rely on unrestricted resource consumption, which poses significant barrier to practical deployment. For instance, Park et al. (2023) shows that two-day simulation of 25 agents incurred thousands of dollars in API costs. Similarly, subsequent studies noted that the latency induced by such complex reasoning loops renders real-time interaction infeasible (Wang et al., 2023c; Lin et al., 2023; Piao et al., 2025). These examples highlight that in real-world scenarios, agents cannot be deployed with unbounded resources. Instead, agents must operate under constraints, such as monetary budgets, latency limits, or safety protocols (Zheng et al., 2024). Operating under these constraints fundamentally changes the problem: the agent can no longer rely on exhaustive trial-and-error, and small planning or sampling errors can compound into constraint violations and irrecoverable states (Valmeekam et al., 2023; Hao et al., 2025a). Our work focuses on improving the success rate of LM agents in such constrained settings by mitigating such error compounding. Language Model Agents. LM agents solve complex tasks by interacting with external environments, such as computers and databases, via tools (Yao et al., 2023b; Schick et al., 2023). The ReAct framework (Yao et al., 2023b) established baseline by interleaving reasoning and acting, allowing agents to respond to immediate observations. However, relying solely on step-by-step generation often makes agents susceptible to planning errors (Valmeekam et al., 2023) and sampling errors (Hao et al., 2025a). To reduce both errors, some works have been proposed. To reduce planning errors, some works incorporate trained world knowledge models for planning (Qiao et al., 2024; Qian et al., 2025), search over multiple candidate thoughts (Yao et al., 2023a), perform deliberation over candidate action sequences using imagined rollouts or heuristic search (Hao et al., 2023; Liu et al., 2023b; Zhuang et al., 2024; Katz et al., 2024), or use reflection mechanisms to identify failures and revise subsequent behavior (Shinn et al., 2023; Xia et al., 2025; Kim et al., 2025). To mitigate sampling errors, Plan-and-Solve strategies generate high-level plan before execution to improve global coherence and reduce logical inconsistencies during execution (Wang et al., 2023a; Erdogan et al., 2025; Xu et al., 2023; Xiong et al., 2025). Some works address both errors by combining planning before execution with online plan refinement during execution (Sun et al., 2023; Prasad et al., 2024; Zhang et al., 2025). Despite these improvements, these methods typically do not enforce hard feasibility constraints during plan selection, and execution can still suffer from sampling errors even when the plan is feasible, since action generation remains stochastic and is not constrained to follow the plan. In contrast, our framework enforces hard feasibility at plan selection by using formal solver to select constraint-feasible path over plan graph constructed from multiple candidate plans, thereby reducing the planning errors. It further reduces sampling errors by constraining decoding to the selected action, with replanning when observations mismatch the plan. Neural-Symbolic Approaches. Neural-symbolic approaches integrate external solvers into learning-based models to incorporate symbolic structure and formal constraints (Chen et al., 2019). For LLMs, LMs usually act as translator that TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents converts natural-language task descriptions into formal planning inputs for external solvers, such as the Planning Domain Definition Language (PDDL) (Liu et al., 2023a; Dagan et al., 2023; Guan et al., 2023; Guo et al., 2025; Huang et al., 2025; Katz et al., 2024), Satisfiability Modulo Theories (SMT) (Hao et al., 2025a;b), or Linear Programming (LP) (Obata et al., 2024). Most prior neural-symbolic LM planning frameworks use LMs primarily as translators that map natural language into single formal specification or solver-consistent plan, and treat execution as downstream procedure. In contrast, our framework keeps diverse set of candidate plans, folds them into plan graph, and solves constraintfeasible path selection problem over this graph. We further integrate planning and execution by constraining decoding to the selected action and replanning upon plan-observation mismatches, which is critical under strict feasibility constraints where deviations are irrecoverable. C. ILP Formulation for Budget Constrained Setting In the constrained G-MDP setting, the agent aims to maximize the expected reward while strictly adhering to specified budget (e.g., token limit or search depth). We extend the path selection formulation presented in Section 3.2 by incorporating the predicted edge costs. Recall that for each edge E, our model estimates cost ˆcθ(e), representing the expected resource consumption of traversing that edge. Let bt denote the remaining budget vector at the current step. We formulate the constrained path selection problem by adding linear budget constraint to the original ILP: max Lmax1 (cid:88) (cid:88) ℓ=0 eE (cid:0)tgt(e)(cid:1), xe,ℓ ˆrθ Lmax1 (cid:88) (cid:88) s.t. ℓ=0 eE ˆcθ(e), xe,ℓ bt, Constraints Equation (7) to Equation (11). (15) (16) Here, Equation (16) enforces the budget constraint, ensuring that the cumulative predicted cost of the selected walk does not exceed bt. The remaining structural constraints (Equation (7) to Equation (11)) are identical to those in the unconstrained formulation, guaranteeing that the solution forms valid directed walk from the start node v0 to goal node in Vg.If the solver finds no solution satisfying Equation (16) (i.e., all valid paths to the goal exceed the budget), it returns an infeasibility status, which can be handled by fallback policy or by re-planning with relaxed constraints. D. Proofs of Theoretical Analysis In this section, we provide detailed proofs for the propositions presented in Section 4. We define the event of selecting viable action at step as Vt := {at Aviable(st)}. According to Definition 4.1, the success probability for task requiring Tg steps is given by Pr(S) = Pr(Tg1 t=0 Vt). Assuming the Markov property and independence of errors at each step, we focus on deriving the single-step success probability (Vt). D.1. Proof of Proposition 4.2 Proof. Let ˆat be the action planned by the agent (intent), and at be the action actually executed. Also, let Eplan be the event that the planned action is viable, i.e., ˆat Aviable(st), and let Eexec be the event that the executed action matches the plan, i.e., at = ˆat. We define two independent events based on the error sources as follows: Pr(Eplan) = 1 ϵp and Pr(Ec plan) = ϵp, and Pr(Ec exec) = ϵs. The viable action event Vt can occur in two disjoint cases. First, when the plan is viable, the executed action at remains viable if execution succeeds (Eexec) or if execution fails but does not break viability. The latter occurs with probability 1 δb. Second, when the plan is non-viable, the executed action at becomes viable only if execution fails (Ec recovers viability (with probability δr). Pr(Vt Eplan) = (1 ϵs) 1 + ϵs (1 δb) = 1 ϵsδb. (17) exec) and accidentally Pr(Vt Ec plan) = (1 ϵs) 0 + ϵs δr = ϵsδr. (18) TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents By the Law of Total Probability, the probability of selecting viable action at step is: Pr(Vt) = Pr(Vt Eplan) Pr(Eplan) + Pr(Vt Ec = (1 ϵsδb)(1 ϵp) + (ϵsδr)ϵp = (1 ϵp)(1 ϵsδb) + ϵpϵsδr. plan) Pr(Ec plan) (19) (20) (21) Since the task requires at least Tg steps, the overall success probability is the product of single-step probabilities up to : Pr(S) 1 (cid:89) Pr(Vt) t=0 (cid:16) (1 ϵp)(1 ϵsδb) + ϵpϵsδr (cid:17)T = =: UReAct. Equality holds if the task ends exactly at step , i.e. = Tg. To show monotonicity, let (ϵs) = UReAct. We take the derivative with respect to ϵs ϵs (ϵs) = ((1 ϵp)δb + ϵpδr)T . For the success probability to increase as ϵs decreases (i.e., derivative is negative), we require (1 ϵp)δb + ϵpδr 0 (1 ϵp)δb ϵpδr. (22) (23) (24) (25) (26) Similarly, we can show that the upper bound of success probability increases as ϵp decreases. This confirms the condition stated in Proposition 4.2. D.2. Proof of Proposition 4. Proof. According to the Plan-and-Act mechanism defined in Equation (2), the agent behavior is divided into two cases based on plan alignment and following probability. First, following the plan occurs when the plan is aligned (z(st) = ˆsˆτ ) and the agent chooses to follow it. We denote this event as . The probability of this event is Pr(F ) = αpfollow. In this case, at = at. Since at is deterministically selected from the pre-generated plan, the execution sampling error is eliminated (ϵs 0). However, the pre-generated plan itself is subject to the same planning error ϵp as the ReAct framework. Thus, the success probability for this case is derived by setting ϵs = 0 in the ReAct single-step probability as Pr(Vt ) = (1 ϵp)(1 0) + ϵp(0) = 1 ϵp. (27) Second, fallback to ReAct occurs when the plan is not aligned or the agent chooses not to follow. We denote this event as R. The probability is Pr(R) = 1 αpfollow. We denote this event as R. In this case, at = aReAct , and the success probability is identical to that of the ReAct framework derived in Proposition 4.2 as Pr(Vt R) = (1 ϵp)(1 ϵsδb) + ϵpϵsδr. By the Law of Total Probability, the single-step success probability for PA is Pr(Vt) = Pr(F ) Pr(Vt ) + Pr(R) Pr(Vt R) (cid:104) = (αpfollow)(1 ϵp) + (1 αpfollow) (1 ϵp)(1 ϵsδb) + ϵpϵsδr (cid:105) . 15 (28) (29) (30) TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents We can rearrange the terms to isolate ϵs: Pr(Vt) = (αpfollow)(1 ϵp) + (1 αpfollow)(1 ϵp) + (1 αpfollow) [(1 ϵp)ϵsδb + ϵpϵsδr] = (1 ϵp) (1 ϵp)(1 αpfollow)ϵsδb + ϵp(1 αpfollow)ϵsδr. (31) (32) Now, we define the effective sampling error for PA as ϵs,PA := (1 αpfollow)ϵs. Substituting this into Equation (32), we have Pr(Vt) = (1 ϵp)(1 ϵs,PAδb) + ϵpϵs,PAδr. Finally, since the task requires steps, we take the product over = 0 to 1, we have (cid:16) UPA = (1 ϵp)(1 ϵs,PAδb) + ϵpϵs,PAδr (cid:17)T . (33) (34) The comparison UPA UReAct follows directly from ϵs,PA ϵs and equality holds when αpfollow = 0. D.3. Proof of Proposition 4.4 Proof. Let ˆA(vt) be the set of d(vt) candidate actions generated at step t. planning failure occurs for the entire set only if all candidates in ˆA(vt) are non-viable. Assuming the generation of each candidate is independent given the state, the probability that all candidates fail is (ϵp)d(vt). Consequently, the probability that there exists at least one viable action in the candidate set is: Pr(a ˆA(vt) : Aviable) = 1 (ϵp)d(vt). (35) Based on the assumptions in Proposition 4.4, the external solver successfully identifies viable action if one exists in the set and constrained execution eliminates sampling error (i.e., ϵs 0), ensuring the selected action is executed exactly. Thus, the single-step success probability for TAPE is exactly 1 (ϵp)d(vt). Taking the product over steps yields UOurs = 1 (cid:89) t=0 (cid:16) 1 (ϵp)d(vt)(cid:17) . (36) From Proposition 4.3, we already established UPA UReAct. We now focus on proving UOurs UPA. Since we assume (1 ϵp)δb ϵpδr, the success probability of PA at step is maximized when the effective sampling error is zero (ϵs,PA = 0). Substituting ϵs,PA = 0 into the term for PA gives the upper bound: Next, for TAPE, since d(vt) 1 and 0 ϵp < 1, it follows that (ϵp)d(vt) ϵp. Therefore, we have (1 ϵp)(1 ϵs,PAδb) + ϵpϵs,PAδr 1 ϵp. Combining Equation (37) and Equation (38), we have 1 (ϵp)d(vt) 1 ϵp. 1 (ϵp)d(vt) 1 ϵp (1 ϵp)(1 ϵs,PAδb) + ϵpϵs,PAδr. (37) (38) (39) Since Equation (39) satisfies for all steps = 0 . . . 1, we conclude UOurs UPA. Also, we can see that 1 (ϵp)d(vt) = (1 ϵp)(1 ϵs,PAδb) + ϵpϵs,PAδr holds when d(vt) = 1 and ϵs,PAδb = ϵs,PAδr = 0. 16 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents E. Experiment Details E.1. Experimental Setup Tasks and Datasets Tasks and Datasest details are explained below. The detailed statistics are summarized in Table. Sokoban (Schrader, 2018). is task pushing all boxes onto goal tiles using four primitive actions (U, D, L, R). To make the task irrecoverable, we define success as solving the instance within two steps of the optimal solution length. We construct two difficulty by the controlling optimal step count : easy instances have = 6, while hard instances have = 10. For evaluation, we construct 10 maps for each task difficulty and run 10 times for each map. ALFWorld (Shridhar et al., 2021). This task is synthetic text-based household embodied tasks where the agent executes environment-specific actions to complete given goal. To align with our assumption that the agent knows the abstract world model in agentic tasks, we modify the environment to provide the object availability information for each location and the basic information of the dependency of the action. We define difficulty by the action budget relative to the optimal length : easy instances use looser budget, while hard instances use tighter budget. For evaluation, GSM-Hard (Gao et al., 2023) is mathemathical reasoning problem dataset that converts some numerical values larger so that the problem cannot be solve easily without arithmetic tools. We cast as an agentic task by equipping the agent with arithmetic tools (+, -, , /). For each operator, we provide two tool variants: fast tool with lower success probability and slow tool with higher success probability. We define hard tasks as those with tight time budget that incentivizes using fast tools, and easy tasks as those with loose time budget. MuSiQue (Trivedi et al., 2022) is factual multi-hop reasoning dataset, where the agent can query retriever to obtain supporting information. To create an agentic setting with explicit costquality tradeoffs, we construct five synthetic retrievers ranging from fast, cheap, but inaccurate to slow, expensive, but accurate. Analogous to GSM-8K, we define hard tasks as those requiring fast and cheap solving, and easy tasks as those allowing larger time budget and higher retrieval cost. Language Model Backbones. We use five LM backbones: gpt-4.1-nano, gpt-4.1-mini, gpt-4.1 (OpenAI, 2025), gpt-5-nano (Singh et al., 2025), and claude-4.5-haiku (Anthropic, 2025). Inference We set the sampling temperature to 0.3 for inference in all experiments except gpt-5-nano models. For gpt-5-nano, as we do not change the temperature, we set the default value (it is unknown). Other parameters, such as top-p and repetition penaltiy, are set as the default value (both are 1). E.2. Baselines ReAct (Yao et al., 2023b) ReAct is the framework that make LM agents solve the tasks by interleaving thought and actions to interact with the external environments. There are various types of implementation for the thoughts and acts (Yao et al., 2023b; Shinn et al., 2023; Kim et al., 2025), we adopt the prompting technique from Xiong et al. (2025). Detailed prompts are summarized in Prompt E.1. Plan-and-Act (Wang et al., 2023a; Erdogan et al., 2025) Plan-and-Act (PA) first generates the plan and use it as in-context prompt. Then, LM agent refers the plan to interact with the environment. For inteaction part, the prompt is the same as ReAct. Detailed prompts for the plan generation phase are summarized in Prompt E.2. 17 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Prompt E.1: ReAct (Sokoban) Interact with Sokoban environment to solve task (placing every box onto goals.) ## Sokoban rules (task + mechanics) - Task objective: place every box onto goals. The puzzle is solved when all boxes are on goals. - Action mechanics: each action moves the player exactly one cell in the chosen direction (U/D/L/R). - U: move up, e.g., (x, y) -> (x, + 1) - D: move down, e.g., (x, y) -> (x, - 1) - L: move left, e.g., (x, y) -> (x - 1, y) - R: move right, e.g., (x, y) -> (x + 1, y) - The action name must match the coordinate change (U increases y, decreases y, increases x, decreases x). - Walls: the player cannot move into wall cell. If wall occupies the destination cell, the action is invalid and the player does not move. - Boxes: if the destination cell has box, the player attempts to push it one cell further in the same direction. - The push succeeds only if the cell behind the box is empty floor or goal. - If the cell behind the box is wall or another box, the push is invalid and nothing moves. - push is only possible when the player is on the cell immediately adjacent to the box from the opposite side of the push direction. - The player cannot pull boxes and cannot push two boxes at once. - Some pushes can create deadlocks (for example, pushing box into corner where it cannot reach any goal). Examples (coordinate outcomes, using U/D/L/R only): - Empty move, R: player at (x, y), no wall/box at (x + 1, y). Action -> player at (x + 1, y); box on goal location unchanged (goals are unaffected by moves). - Empty move, L: player at (x, y), no wall/box at (x - 1, y). Action -> player at (x - 1, y); box on goal location unchanged (goals are unaffected by moves). - Empty move, U: player at (x, y), no wall/box at (x, + 1). Action -> player at (x, + 1); box on goal location unchanged (goals are unaffected by moves). - Empty move, D: player at (x, y), no wall/box at (x, - 1). Action -> player at (x, - 1); box on goal location unchanged (goals are unaffected by moves). - Wall block, U: player at (x, y), wall at (x, + 1). Action -> invalid, player stays at (x, y); box on goal location unchanged. - Wall block, D: player at (x, y), wall at (x, - 1). Action -> invalid, player stays at (x, y); box on goal location unchanged. - Wall block, L: player at (x, y), wall at (x - 1, y). Action -> invalid, player stays at (x, y); box on goal location unchanged. - Wall block, R: player at (x, y), wall at (x + 1, y). Action -> invalid, player stays at (x, y); box on goal location unchanged. - Push succeeds, R: player at (x, y), box at (x + 1, y), no wall/box at (x + 2, y). Action -> player at (x + 1, y), box moves to (x + 2, y); if (x + 2, y) is goal, box on goal location becomes (x + 2, y), otherwise unchanged. - Push succeeds, L: player at (x, y), box at (x - 1, y), no wall/box at (x - 2, y). Action -> player at (x - 1, y), box moves to (x - 2, y); if (x - 2, y) is goal, box on goal location becomes (x - 2, y), otherwise unchanged. - Push succeeds, U: player at (x, y), box at (x, + 1), no wall/box at (x, + 2). Action -> player at (x, + 1), box moves to (x, + 2); if (x, + 2) is goal, box on goal location becomes (x, + 2), otherwise unchanged. - Push succeeds, D: player at (x, y), box at (x, - 1), no wall/box at (x, - 2). Action -> player at (x, - 1), box moves to (x, - 2); if (x, - 2) is goal, box on goal location becomes (x, - 2), otherwise unchanged. - Push blocked by wall, R: player at (x, y), box at (x + 1, y), wall at (x + 2, y). Action -> invalid, player stays at (x, y), box stays at (x + 1, y); box on goal location unchanged. - Push blocked by wall, L: player at (x, y), box at (x - 1, y), wall at (x - 2, y). Action -> invalid, player stays at (x, y), box stays at (x - 1, y); box on goal location unchanged. - Push blocked by wall, U: player at (x, y), box at (x, + 1), wall at (x, + 2). Action -> invalid, player stays at (x, y), box stays at (x, + 1); box on goal location unchanged. 18 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents - Push blocked by wall, D: player at (x, y), box at (x, - 1), wall at (x, - 2). Action -> invalid, player stays at (x, y), box stays at (x, - 1); box on goal location unchanged. - Push blocked by box, R: player at (x, y), box at (x + 1, y), another box at (x + 2, y). Action -> invalid, player stays at (x, y), boxes stay at (x + 1, y) and (x + 2, y); box on goal location unchanged. - Push blocked by box, L: player at (x, y), box at (x - 1, y), another box at (x - 2, y). Action -> invalid, player stays at (x, y), boxes stay at (x - 1, y) and (x - 2, y); box on goal location unchanged. - Push blocked by box, U: player at (x, y), box at (x, + 1), another box at (x, + 2). Action -> invalid, player stays at (x, y), boxes stay at (x, + 1) and (x, + 2); box on goal location unchanged. - Push blocked by box, D: player at (x, y), box at (x, - 1), another box at (x, - 2). Action -> invalid, player stays at (x, y), boxes stay at (x, - 1) and (x, - 2); box on goal location unchanged. ## Instruction You are the player in Sokoban environment, and your goal is to place every box on goal within limited number of actions (within step remaining). That means you need to make the same number of boxes on goals by placing all boxes onto goals. If box is on goal, it is considered satisfied. At each turn, you will receive the current observation. Observation format (all coordinates are (x, y)): - wall location: (x1, y1), (x2, y2), ... - player location: (x, y) - box location: (x3, y3), ... - goal location: (x4, y4), ... - box on goal location: (x5, y5), ... - Step remaining: <steps_remaining> The \"box location\" list includes only boxes not on goals. The \"goal location\" list includes all goals. You may choose between two outputs: \"Thought\" or \"Action\". When you choose \"Thought\", you must: Plan the full solution (overall path) so that all boxes reach goals within the remaining steps, using the Sokoban rules (task + mechanics). From that full plan, explicitly predict only the next 1 step: how the immediate action will change the player location, box location, and box on goal location. Based on that planning and the current observation, decide the immediate next action to take. IMPORTANT: - Your Thought MUST match the given observation exactly. No hallucination about positions or adjacency is allowed. - If push is feasible immediately, you MUST choose the move that pushes. - If you planned push in the previous Thought and the current observation still allows that push, you MUST continue and execute it. - Do NOT rewrite the plan from scratch unless the environment changed and the old plan became infeasible. Your output must follow exactly: Thought: <your reasoning> Action: <UDLR>\" Prompt E.2: Plan-and-Act (Sokoban) Interact with Sokoban environment to solve task (placing every box onto goals.) {{sokoban_rule}} ## Instruction You are the player in Sokoban environment, and your goal is to place every box on goal within limited number of actions (within step remaining). That means you need to make the same number of boxes on goals by placing all boxes onto goals. If box is on goal, it is considered satisfied. At each turn, you will receive the current observation. Observation format (all coordinates are (x, y)): - wall location: (x1, y1), (x2, y2), ... - player location: (x, y) - box location: (x3, y3), ... - goal location: (x4, y4), ... - box on goal location: (x5, y5), ... - Step remaining: <steps_remaining> The \"box location\" list includes only boxes not on goals. 19 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents The \"goal location\" list includes all goals. You are in planning mode. Using the Sokoban rules, plan the full solution (overall path) so that all boxes reach goals within the remaining steps, using the Sokoban rules (task + mechanics). Based on this plan, you generate the full sequence of actions. 20 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents ˆst zθ(st) {ˆτ (m)}M Algorithm 1 Tool-Guided Adaptive Planning with Constrained Execution (TAPE) Require: Environment E, budgets B, max horizon Lmax, #candidates Require: LLM-based abstract state projector zθ(), LLM-based plan graph constructor BUILDPLANGRAPH() Require: LLM predictors ANNOTATEGRAPH() for node reward / edge cost Require: External solver SOLVER(), constrained decoding method CONSTRAINEDDECODING() 1: Initialize history s0 ( ), observe o0 E.RESET(), 0 2: while < Lmax and not terminal do 3: m=1 SAMPLEPLANS(ˆst, ) 4: 5: BUILDPLANGRAPH({ˆτ (m)}, zθ) 6: ANNOTATEGRAPH(G) 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end while π[t] at CONSTRAINEDDECODING(st, ) (ot+1, costt, done) E.STEP(at) st+1 st (at, ot+1), costt ˆst+1 zθ(st+1), ˆv if ˆst+1 = ˆs break π SOLVER(G, ˆst, B, t) while < and not terminal do t+1 NEXTPLANNEDNODE(G, π, t) t+1 or < 0 then end if + 1 end while State projection Candidate rollout sampling Graph construction via abstract state merging LLM-based reward/cost annotation Feasible plan path selection via ILP Replan upon deviation or budget violation Enforce sampling constraint F. Implementation Details of TAPE In this section, we introduce the pseudo-code for TAPE and prompt details for our implementation. Detailed implementation of our framework is available on Github. F.1. Pseudo-code The overall procedure of TAPE is detailed in Algorithm 1. F.2. Prompt Examples Prompts used in our framework, especially Sokoban task, are introduced in Prompt F.1, Prompt F.2, Prompt F.3, and Prompt F.4. Other prompts for are introduced in Prompt F.1: State Projector Extract the current exact player location, box locations, goal locations, and box on the goals location, and prompt from the given history. Prompt F.2: Sample Plans Interact with Sokoban environment to solve task (placing every box onto goals.) {{sokoban_rule}} ## Instruction You are the player in Sokoban environment, and your goal is to place every box on goal within limited number of actions (within step remaining). That means you need to make the same number of boxes on goals by placing all boxes onto goals. If box is on goal, it is considered satisfied. 21 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents At each turn, you will receive the current observation. Observation format (all coordinates are (x, y)): - wall location: (x1, y1), (x2, y2), ... - player location: (x, y) - box location: (x3, y3), ... - goal location: (x4, y4), ... - box on goal location: (x5, y5), ... - Step remaining: <steps_remaining> The \"box location\" list includes only boxes not on goals. The \"goal location\" list includes all goals. You are in planning mode. Using the Sokoban rules, plan the full solution (overall path) so that all boxes reach goals within the remaining steps. Generate {{num_plans}} diverse and valid plans. For each plan, you must **precise** reason based on the location of player, box, goal, and the wall, and the Sokoban rules (task + mechanics). Then, generate the full solution (overall path) so that all boxes reach goals within the remaining steps, using the Sokoban rules (task + mechanics). Based on this plan, you generate the full sequence of actions. Generate as **diverse** as possible while maintaining success within given remaining steps. Each plan MUST have full action sequence (at most remaining steps of actions). Provide exactly {{num_plans}} plans labeled \"Plan 1:\" ,..., \"Plan {{num_plans}}:\". IMPORTANT: Use the action-coordinate rules exactly. Actions update the player location: (moving up) moves the player (x, y) -> (x, y+1), (moving down) moves the player (x, y) -> (x, y-1), (moving right) moves the player (x, y) -> (x+1, y), (moving left) moves the player (x, y) -> (x-1, y). Always align your verbal directions (up/down/left/right) with these coordinate changes. When describing relative positions, use the coordinate conventions: \"above\" means larger y, \"below\" means smaller y, \"right\" means larger x, \"left\" means smaller x. Validate each step in the plan by explicitly computing the next (x, y) from the action; if the destination cell is box, check the cell behind it and treat the move as valid push if that cell is empty or goal, otherwise the move is invalid and must not appear in the action sequence. If the player pushes box, the box will move one cell in the same direction as the player unless blocked by wall or another box. After any move (including push), the player and any box must occupy different cells; they can never share the same location. Prompt F.3: Build Plan Graph Simulate Sokoban plans, produce per-plan step sequences and generate the graph. {{sokoban_rule}} IMPORTANT: Use the action-coordinate rules exactly. Actions update the player location: (moving up) moves the player (x, y) -> (x, y+1), (moving down) moves the player (x, y) -> (x, y-1), (moving right) moves the player (x, y) -> (x+1, y), (moving left) moves the player (x, y) -> (x-1, y). Always align your verbal directions (up/down/left/right) with these coordinate changes. When describing relative positions, use the coordinate conventions: \"above\" means larger y, \"below\" means smaller y, \"right\" means larger x, \"left\" means smaller x. Validate each step in the plan by explicitly computing the next (x, y) from the action; if the destination cell is box, check the cell behind it and treat the move as valid push if that cell is empty or goal, otherwise the move is invalid and must not appear in the action sequence. If the player pushes box, the box will move one cell in the same direction as the player unless blocked by wall or another box. After any move (including push), the player and any box must occupy different cells; they can never share the same location. ## Instructions - Simulate each plan step-by-step using the Sokoban rules and the observation. - Build each plans step sequence as alternating entries: node -> action -> node -> action -> ... - node entry must include node id and the predicted observation text. - Node ids must be unique across all plans (no duplicate node_id between plans). - IMPORTANT: Observations must include ONLY player location and box location (no walls, no goals, no box-on-goal). - The observation text must start with short \"Thought: ...\" line that explains how the previous action moves player/box while considering walls. - An action entry must include only the action (U/D/L/R). No separate thought key in action entries. - Preserve ALL actions from every plan (do not drop steps). 22 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Return JSON only: { \"plan_sequences\": [ { \"plan_id\": \"plan1\", \"steps\": [ { \"kind\": \"node\", \"node_id\": \"nodeX\", \"thought\": \"Initial states\" \"observation\": \"player location: (x, y) box location: (x1, y1), ...\" }, { \"kind\": \"action\", \"action\": \"U\" }, { \"kind\": \"node\", \"node_id\": \"nodeY\", \"thought\": \"After moving up (U), player is at (x, y+1) unless blocked; box moves to (x1, y1+1) if pushed.\" \"observation\": \"player location: (x, y+1) box location: (x1, y1+1), ...\" } ] } ] }, - After all plan sequences are built, merge identical nodes when the observation text matches exactly. - Preserve ALL actions from every plan (do not drop steps). - Construct the full graph (nodes + edges) from the merged nodes. - Generate thought for each edge based on the transition between observations. - Mark goal nodes explicitly with \"is_goal\": true in full_graph nodes. - JSON keys must appear in this order: reasoning, merge_log, full_graph. - In merge_log entries, put \"reason\" first, then \"kept_node\", then \"merged_nodes\". { \"reasoning\": \"overall reasoning for node merging and graph construction\", \"merge_log\": [ { \"reason\": \"same observation text\", \"kept_node\": \"nodeX\", \"merged_nodes\": [\"nodeXX\", ...] } ], \"full_graph\": { \"nodes\": [ { \"id\": \"nodeX\", \"observation\": \"player location: (x, y) nbox location: (x1, y1), ...\", \"is_goal\": true or false } ], \"edges\": [ { \"from\": \"nodeX\", \"to\": \"nodeY\", \"thought\": \"why this step is taken\", \"action\": \"<U/D/L/R>\" } ] } } Prompt F.4: Annotate Graph You score all the states in the graph. Higher score means closer to solving. Goal: move all boxes onto goals (use goal locations from the observation). Assign score 1.0 to goal states. Assign score -1.0 if, starting from this state, there is no valid sequence of actions that can ever reach any goal state (deadlocked/unreachable). All other states should have score 0.0. Consider wall locations, blocked pushes, and required pushing routes when judging reachability. Only use scores -1.0, 0.0, or 1.0 (no other values). For each state, provide short reasoning sentence before assigning its score. Return JSON only with both reasons and scores, for example: { \"reasons\": { \"s0\": \"reasoning for s0: whether any sequence can reach goal, considering walls, blocked pushes, and deadlock signals (e.g., box stuck in corner with no goal)\", ... },g \"scores\": { \"s0\": 0.0, ... } }. 23 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents Thoughts The player is at (6, 4), right below the box at (6, 5). The player can push the box right from (6, 5) to (7, 5), which is the goal. This push will complete the task. The immediate next action is to push the box right by moving right from (6, 4) to (7, 4), pushing the box from (6, 5) to (7, 5). Planned Executed Table 3. Example of sampling error. Although the plan specifies the immediate next action as R, the executed action is . Table 4. Comparing TAPE with best-of-N integration on the ReAct and PA frameworks (Wang et al., 2023b). Results are on Sokoban (easy), and we report mean standard error. For compute-matched comparison, the Best-of-N variants sample the same number of plans per step (M ) as TAPE. Even under this matched sampling budget, TAPE consistently achieves higher success rates, suggesting that the gains are not solely due to increased sampling. Method Success Rate (%) TAPE (M=4) Plan-and-Act Plan-and-Act (M=4) ReAct ReAct (M=4) 46.0 5.0 20.0 4.0 22.0 4.1 4.0 2.0 8.0 2.7 G. Additional Experiment G.1. Planning & Sampling Error Estimation This section describes how we extract the intended action from Thought and how we compute planning and sampling error rates reported in Table 1. Also, we do the qualitative analysis of each errors. Setting. For each step, we extract the intended next action ˆat {U, D, L, R} from the agents Thought using gpt-4.1-mini as parser. Then, given the current state st and the intended action ˆat, we simulate one-step transition to obtain run Breadth-First Search (BFS) oracle from b(s if b(s t+1 = (st, ˆat). We t+1 to compute the minimum remaining steps to reach the goal, denoted t+1) > B(t + 1) or t+1). Let B(t + 1) denote the remaining step budget at time + 1. We mark ˆat as non-viable if d(s t+1) = , and we count it as planning error. Lastly, let at denote the executed action from Action. We define the sampling error indicator at step as 1[at=ˆat] and compute the sampling error rate by averaging this indicator over steps. Sampling Error Example. In our analysis, we observe that sampling errors often manifest as mismatch between the action implied by the LMs plan and the action actually executed. Table 3 shows representative example: the Thoughts column contains the LM-generated planning trace, which clearly implies the next action (pushing the box right to the goal), while the executed action deviates to . Comparing the planned action inferred from the LMs reasoning with the executed action reveals such stochastic deviations at execution time, which we refer to as sampling error. G.2. Additional Empirical Results Comparison with Best-of-N Methods. We compare TAPE against compute-matched best-of-N variants of ReAct and PA (Wang et al., 2023b; Kang et al., 2025), as shown in Table 4. At each step, these baselines sample the same number of plans (M ) as TAPE and select the best outcome, yielding roughly comparable trajectory-sampling budget (and hence similar inference-token usage). On Sokoban (easy) with =4, TAPE achieves 46.0% success, outperforming PA (20.0%) and PA-best-of-4 (22.0%), as well as ReAct (4.0%) and ReAct-best-of-4 (8.0%). These results indicate that TAPEs gains are not solely attributable to increased sampling, but rather to improved plan selection and execution under feasibility constraints. Success under Larger Step Budgets. In Figure 5a, as we increase the step budget (i.e., provide more slack beyond the oracle minimum steps), we observe that ReAct and Plan-and-Act largely plateau, exhibiting only marginal changes in success despite the additional budget. In contrast, TAPE improves monotonically from 46% to 75% success as the TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents (a) Success under Larger Step Budgets (b) SuccessCost Trade-off Figure 5. Impact of larger step budgets and success-cost trade-off on Sokoban. (a) Success rate as function of the normalized step budget B/Smin, where Smin denotes the oracle minimum number of steps (here Smin = 6 and {8, 10, 14, 22}). Error bars indicate standard error across runs. Our framework is more higher success rate as step budget increases. (b) Success rate versus step consumption (steps used divided by the budget). Methods closer to the top-left achieve higher success while consuming fewer steps per budget, indicating better successcost trade-off. TAPE can be both efficient and powerful compared to other frameworks. Figure 6. Graph construction and plan selection example in Sokoban. Graph is constructed by multiple paths from LLM lookahead plans. Then, blue path is selected by formal solver (ILP). When performing constrained execution, TAPE can accurately performs its plan, achieving the goal. normalized step budget B/Smin increases, indicating that it can effectively exploit extra steps for recovery rather than getting stuck in irrecoverable failures. This trend is consistent with our error decomposition: TAPE reduces planning errors that transition the agent into dead-end (non-viable) states (as measured by our Sokoban dead-end oracle) and mitigates sampling-induced execution deviations via constrained decoding and mismatch-triggered replanning. Consequently, TAPE lowers the probability of entering irrecoverable dead-ends early, allowing additional budget to translate into sustained gains in task success. SuccessCost Trade-off. In Figure 5b, the x-axis measures step consumption (steps used divided by the budget), so better methods lie toward the top-left (higher success with lower cost consumption). we find that TAPE is closer to this region, achieving higher success while using fewer steps per budget on average compared to ReAct and PA. This result indicates that TAPE does not only improve the success rate but also enhances the efficiency. G.3. Qualitative Analysis 25 TAPE : Tool-Guided Adaptive Planning and Constrained Execution in LM Agents (a) ReAct (b) Our Framework Figure 7. Qualitative comparison between ReAct and TAPE. We visualize representative execution trajectories under the same action budget (B = 8). (a) ReAct makes mistake at t=5 (red), after which it repeatedly deviates and fails to reach the goal within the budget, terminating at t=8 (END/FAIL). (b) TAPE selects feasible path and executes it reliably, reaching the goal within the same budget at t=8 (blue). Graph Example. Figure 6 illustrates an example of the graph constructed by TAPE for Sokoban. Nodes represent Sokoban states, where the goal is denoted as G, boxes as $, the player as @, and walls as #. Edges represent the actions. Given that 5 steps remain, the blue path shown in Figure 6 is the one selected by the external solver. This example demonstrates that our framework is capable of finding feasible path by using formal solver when the graph is well-constructed. Qualitative Comparison. Figure 7 provides representative trajectory-level comparison between ReAct and TAPE under the same action budget. ReAct makes an early mistake (at t=5), after which the subsequent actions continue to deviate and the agent fails to reach the goal within the budget, terminating in failure. In contrast, TAPE reaches the goal within the same budget by selecting feasible plan and executing it more reliably, avoiding irrecoverable states. Overall, this qualitative evidence supports our quantitative findings in Section 5 that TAPE mitigates both planning and"
        }
    ],
    "affiliations": [
        "Electrical and Computer Engineering, University of Wisconsin-Madison",
        "KRAFTON"
    ]
}