{
    "paper_title": "Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis",
    "authors": [
        "Xinyu Hou",
        "Zongsheng Yue",
        "Xiaoming Li",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we introduce a single parameter $\\omega$, to effectively control granularity in diffusion-based synthesis. This parameter is incorporated during the denoising steps of the diffusion model's reverse process. Our approach does not require model retraining, architectural modifications, or additional computational overhead during inference, yet enables precise control over the level of details in the generated outputs. Moreover, spatial masks or denoising schedules with varying $\\omega$ values can be applied to achieve region-specific or timestep-specific granularity control. Prior knowledge of image composition from control signals or reference images further facilitates the creation of precise $\\omega$ masks for granularity control on specific objects. To highlight the parameter's role in controlling subtle detail variations, the technique is named Omegance, combining \"omega\" and \"nuance\". Our method demonstrates impressive performance across various image and video synthesis tasks and is adaptable to advanced diffusion models. The code is available at https://github.com/itsmag11/Omegance."
        },
        {
            "title": "Start",
            "content": "Omegance: Single Parameter for Various Granularities in Diffusion-Based Synthesis Zongsheng Yue S-Lab, Nanyang Technological University csxmli@gmail.com zsyzam@gmail.com"
        },
        {
            "title": "Chen Change Loy",
            "content": "ccloy@ntu.edu.sg"
        },
        {
            "title": "Xinyu Hou",
            "content": "xinyu.hou@ntu.edu.sg 4 2 0 2 6 2 ] . [ 1 9 6 7 7 1 . 1 1 4 2 : r Figure 1. Omegance enables flexible granularity control over generation results. The control can be implemented globally, spatially with an omega mask, or temporally with an omega schedule. (Zoom-in for best view)"
        },
        {
            "title": "Abstract",
            "content": "In this work, we introduce single parameter ω, to effectively control granularity in diffusion-based synthesis. This parameter is incorporated during the denoising steps of the diffusion models reverse process. Our approach does not require model retraining, architectural modifications, or additional computational overhead during inference, yet enables precise control over the level of details in the generated outputs. Moreover, spatial masks or denoising schedules with varying ω values can be applied to achieve region-specific or timestep-specific granularity control. Prior knowledge of image composition from control signals or reference images further facilitates the creation of precise ω masks for granularity control on specific objects. To highlight the parameters role in controlling subtle detail variations, the technique is named Omegance, combining omega and nuance. Our method demonstrates impressive performance across various image and video synthesis tasks and is adaptable to advanced diffusion models. The code is available at https://github.com/ itsmag11/Omegance. 1. Introduction Diffusion models have emerged as powerful tools in image and art generation by progressively transforming random noise into coherent visual content through learned iterative process. Diffusion models are particularly effective in generating high-quality, diverse results. Artists and designers often need to strategically decide where and how to apply details in their work. The level of detail in piece of artwork or photograph can shape its visual harmony, order, and clarity, influencing how the viewer experiences and interprets it while guiding their focus [2]. The vanilla diffusion model does not inherently offer direct, fine-tuned control over the level of granularity in specific areas of an image. While the model can generate varying levels of detail across different images, its uniform generative process does not allow for easy manipulation of how much detail is rendered in different parts of the same image. The level of detail in an image can be challengingor even impossibleto convey through text alone. For instance, reducing detail in the background while retaining high detail in the main subject (see the right case of Fig. 1(b) for illustration) is not straightforward. In this paper, we explore novel yet embarrassingly simple approach for controlling the level of detail in diffusion model outputs by scaling the predicted noise during each denoising step. Our method requires no modifications to the network architecture or timestep scheduling. Instead, we demonstrate that by dynamically adjusting the variance of the noise removed at each step, we can influence the granularity of the visual output. This simple yet flexible technique allows tailored adjustments in concept density and object texture, offering users more nuanced control over the synthesized content. Our approach is appealing as the noise scaling is achieved with just single parameter omega, ω. Decreasing ω results in less noise being removed, leading the network to infer more complex scenes and richer textures. Conversely, increasing ω removes more noise, leading to smoother and simpler outputs. While applying our omega control globally over space and consistently over time can yield uniformly richer or smoother results, as shown in Fig. 1(a), more precise controls can be implemented both spatially and temporally. (1) Since the granularity requirement may vary within single image, e.g., finer-grained details for areas requiring rich textures and complex visual elements, and coarser-grained details for areas demanding smooth transitions and high-level quality, we can use omega masks to customize the desired effects across different spatial regions. Examples of different spatial effects are shown in Fig. 1(b). The mask can be created either from user-provided strokes or generated using specific guiding (2) To better align with the diffusion denoisconditions. ing dynamics [10, 24], where the object shapes and image layouts typically emerge in the early stages and fine details in the later stages, we can implement omega schedules, adjusting the omega value over time for varying effects on layout and detailed textures. Examples are shown in Fig. 1(c). Our Omegance technique is not limited to any specific network architecture or denoising scheduler as long as the progressive diffusion denoising process is followed. Extensive experiments demonstrate Omegances ability to adapt to various diffusion-based synthesis tasks. Models evaluated include Stable Diffusion [5, 16] and FLUX [13] for text-to-image generation, SDEdit [15] and ControlNet [28] for image-to-image generation, SDXL-Inpainting [16] for image inpainting, ReNoise [7] for real-image editing, and Latte [14] and AnimateDiff [8] for text-to-video generation. Some examples are shown in Fig. 1. In all of the above applications, effective and smooth, nuanced control over the generated results is observed, demonstrating the effectiveness of our single-parameter granularity adjustment. In summary, our contributions are: We propose simple yet effective omega scaling technique that enables single-parameter control over the granularity of generated content in diffusion-based models. Our method is training-free, incurs no additional inference cost, and is agnostic to network architectures. We introduce both spatially adaptive omega masks and temporally dynamic omega schedules, which allow for localized and progressive control of granularity within generated images and videos. We demonstrate the applicability of omega scaling on vast range of image and video synthesis tasks. 2 2. Related Work Diffusion-based Editing. Most previous diffusion-based editing methods focus on exploiting the visual-language association ability of CLIP [17] to edit visual content according to language guidance. Prompt-to-Prompt [9] and InstructPix2Pix [4] edit concepts in the output by modifying the cross-attention maps, which play crucial role in aligning textual prompts with visual features during the generation process. SEGA [3] generates results following the semantic guidance of target prompt during denoising. Wu et al. [25] find by mixing the text embedding of prompts with and without the target attribute, the output can preserve the original content while aligning with the desired attribute. Besides, SDEdit [15] adds noise to the modified image and utilizes diffusion prior to rationalize the edited parts as natural images. However, these previous methods are less effective when the editing target cannot be explicitly represented in language or denoted in the original image, and fail to provide flexible way to edit the granularity of the output. Generation Quality Enhancement. Efforts have also been made to enhance the quality of content generated by diffusion models. Several works have explored modifications of Classifier-Free Guidance (CFG) to improve generation quality [1, 11, 20]. SAG [11] and PAG [1] substitute the null-text prediction in CFG with self-attention or perturbed self-attention maps, enabling high-quality, trainingand condition-free generation. Sadat et al. [20] present guidance strategy similar to CFG, applied between clean and perturbed text embeddings, to boost generation quality. While these methods effectively enhance quality globally, they lack the capability for spatially fine-grained control over details in the generated output. Another line of research leverages reinforcement learning from human feedback (RLHF) to fine-tune diffusion models for higherquality results aligned with human preferences [6, 26, 27]. Xu et al. [26] present general-purpose text-to-image human preference reward model and use it to fine-tune the diffusion model regarding human preference score. similar approach is adopted in concurrent work DPOK [6]. Furthermore, Yang et al. [27] employ direct preference optimization (DPO), fine-tuning the diffusion model to align with human feedback without separate reward model. While these methods produce outputs that reflect human preferences, they involve costly model fine-tuning and lack flexible control over output granularity. FreeU [22] was recently introduced to enhance the quality of diffusion model outputs, specifically targeting the U-Net architecture in the denoising process. This method involves jointly adjusting two scaling factors during inference: one for amplifying the backbone features and one for modulating the influence of the skip connections to better preserve details without oversmoothing or degrading high-frequency elements. While achieving noticeable quality improvement, FreeU is closely tied to the U-Net architecture and requires careful adjustment of its dual scaling parameters. In contrast, Omegance provides simpler, more flexible, and architecture-agnostic approach to control the level of detail in diffusion models. 3. Methodology 3.1. Diffusion Model Preliminaries Diffusion models are powerful generative models that synthesize realistic images by iteratively predicting the noise added to sample. They consist of two processes: In the forward process, Gaussian noise is progressively added to an initial latent z0 that is directly decoded from x0. Following Song et al. [23], we formulate the process as: zt = αtz0 + 1 αtϵ, ϵ (0, 1) (1) where zt is the noisy latent at timestep t. The noise schedule αt is defined as the cumulative product of (1 βt): αt = (cid:81)t i=1(1 βi) where βt is predefined variance schedule that controls the amount of noise added at each In the reverse process, pure Gaussian timestep [10]. noise is transformed into coherent visual content through learned denoising process. general representation of the denoising process is: zt1 = δt zt + ζt ϵθ(zt, t) (cid:125) (cid:123)(cid:122) direction pointing to z0 (cid:124) (2) where δt and ζt are scaling factors of the current noisy signal and the noise prediction, respectively, that vary according to specific scheduler (see supp. for details), and ϵθ(zt, t) is the noise prediction at time by the denoising network with parameters θ. This formula characterizes the iterative denoising process, where each step aims to progressively move from noisier latent zt towards clean z0 to get less noisy latent zt1. Signal-to-Noise Ratio. In diffusion models, the Signalto-Noise Ratio (SNR) plays critical role in defining the balance between the original image content z0 and added Gaussian noise ϵ at each timestep. Given Equ. (1), SNR is defined as: SNR(t) = . (3) αt 1 αt When = 0, SNR = 1 indicating pure image signal z0. As increases, SNR decreases to 0, demonstrating pure noise zT . During denoising, the model progressively aligns the SNR of each timestep with the SNR defined by the forward process. Since αt is predefined by the noise schedule, the SNR in vanilla diffusion models remains fixed throughout the denoising process, limiting flexibility in controlling the amount of noise at each timestep. Denoising Dynamics. In diffusion models, denoising dynamics follow progressive refinement [10, 24]: broad 3 Figure 2. Effects of Omegance on the frequency spectrum of the intermediate latent t. The inference step in the legend is inversely correlated with the timestep t. In the original denoising process (a), high-frequency components gradually diminish while low-frequency components become more prominent as the denoising progresses to late stages. With Omegance, increasing ω leads to more aggressive removal of the high-frequency components, as shown in (b), and vice versa, as depicted in (c). structures, such as image layout and object shapes, emerge in early stages, while fine-grained details appear in later steps. This behavior reflects the nature of the forward diffusion process, where high-frequency details are corrupted by noise first and low-frequency broad structures last, resulting in an inverse reconstruction during denoising. Such dynamics not only stabilize generation but also enable flexible, hierarchical control over image features at different timesteps. 3.2. Omegance We introduce Omegance, which uses parameter ω to scale the noise prediction at each denoising step in the reverse diffusion step. general form of single denoising step with Omegance is formulated as follows: t1 = δt zt + ζt ϵθ(zt, t) ω (cid:124) (cid:125) (cid:123)(cid:122) modified direction pointing to z0 (4) Since the noise prediction in each denoising step is standard Gaussian noise: ϵθ(zt, t) (0, 1), multiplying it by scaling factor ω preserves the mean at 0 while adjusting the variance proportionally to ω2. In practice, ω is rescaled to allow (, ) input range for finer-grained control and re-centered at 0 by: ω = R(ϖ) = + 1 + ekϖ (5) Though it is simple introduction of coefficient to the denoising term, its influence on SNR and detail generation is worth investigating. Taking DDIM scheduler [23] as an example, the modified SNR during denoising is formulated as follows (see supp. for step-by-step derivations): SNR(t 1) = (cid:20) αt1 1αt αt + ω (cid:18) αt1 αt 1αt1 αt αt1 1αt (cid:19)(cid:21)2 (6) αt where due to the monotonically-decreasing nature of αt. 1 αt1 1 αt is always negative αt When ω = 1, SNR(t 1) = SNR(t 1). Omegance retains the standard denoising schedule as in Equ. (2), leaving the amount of noise removed from zt unchanged. The SNR schedule aligns with the forward process. This setting produces balanced output with standard levels of detail and texture across the entire image, aligning with the expected granularity of the original noise schedule. When ω < 1, SNR(t 1) < SNR(t 1). The noise prediction is scaled down, leading to less aggressive denoising towards z0. Therefore, the latent state t1 retains additional high-frequency information, as illustrated in Fig. 2(c). With the noise component dominating, the model justifies this residual noise by generating more intricate structures and richer textures, enhancing visual complexity in the output. When ω > 1, SNR(t 1) > SNR(t 1). The denoising schedule becomes more aggressive. This amplified noise reduction diminishes high-frequency information in the latent t1. With the signal now dominating, the model interprets the reduced residual noise as cue to simplify textures and details, yielding smoother and less intricate visual outputs. Both rich and smooth effects can be desirable depending on the users intent. For instance, setting ω < 1 enhances detail, making it well-suited for generating busier crowd in marketplace, intricate patterns in clothing design, or fine textures in elements like sand or waves. On the other hand, ω > 1 produces smoother, simpler visuals, ideal for scenes with clear skies, calm waters, or minimalist designs, where streamlined aesthetic is preferred. This flexibility allows users to adapt granularity dynamically to match specific visual and stylistic goals. Omegance in Various Schedulers. Omegance can be applied in various noise schedulers. Below, we outline the modified denoising step formula for several popular schedulers. For DDIM [23] and Euler discrete [12] schedulers where the standard noise added in the current step ϵθ(zt, t) is available (in Euler scheduler, it approximate the derivative of z), we can directly apply ω on it to achieve mean4 Figure 3. Global effects of Omegance. (Zoom-in for best view) preserving variance modification. (1) DDIM scheduler [23]: t1 = αt1 (cid:18) zt 1 αt ϵθ(zt, t) ω αt + (cid:112)1 αt1 ϵθ(zt, t) ω (cid:19) (7) (2) Euler discrete scheduler [12]: t1 = zt + (σt+1 ˆσ) ϵθ(zt, t) ω (8) where σ is the noise level from Karras et al. [12], and ˆσ = σt (γ + 1) when γ is the churn factor for perturbing σt. However, in the flow-matching-based scheduler [5], the forward process learns continuous transformation without the need for stepwise noise addition schedule: zt = (1 t)z0 + tϵ, where ϵ (0, 1), which slightly differs from Equ. (1). During the reverse process, the model predicts vθ(zt, t) = dzt dt = ϵ z0, and moves one step forward with zt1 = zt + dt vθ(zt, t). Here, the term dt vθ(zt, t) represents the denoise amount as in the general formula Equ. (2), but is not necessarily standard noise. To prevent mean-shifting, we apply an additional mean-preserving operation with Omegance in the flow matching scheduler. (3) Flow matching scheduler [5]: = E[dt vθ(zt, t)] t1 = zt + [(dt vθ(zt, t) m) ω + m] (9) Figure 4. Effects of Omegance in different denoising stages. The 1(t) and 2(t) schedules correspond to Early-Stage Enhancement (left) and Late-Stage Enhancement (right) cases in Fig. 1(c). The discrete step schedules are applied to ensure clearer effects on the image layout and fine-grained detail. is mask RH where = H/f, = W/f are the original image dimension H, scaled by the VAE downsampling factor . The mask can be obtained from user-provided strokes, segmentation masks, or automatically generated from control signals like pose skeleton, depth map, etc. in both discrete and continuous manners as illustrated in Fig. 8. This spatial control leverages the locality of the denoising process, ensuring that adjustments to ω in one region do not affect the SNR or visual properties of neighboring areas. Such flexibility is valuable for applications requiring region-specific detail control within single image, enabling fine-grained textures in focal regions while maintaining smoothness elsewhere. 3.2.1. Omega Mask The omega mask ωi,j = M(i, j) introduces spatially varying control over the granularity within single image by allowing different regions to have distinct ω values during the denoising process. 3.2.2. Omega Schedule The omega schedule ωt = S(t) provides mechanism for controlling granularity across different stages of the denoising process by dynamically adjusting ω values over time. By introducing ω at specific stages in the reverse diffusion 5 Figure 5. Global effects of Omegance in fixing visual artifacts and improving realism. (Zoom-in for best view) process, the omega schedule allows targeted influence on both the broad layout and fine-grained details within the generated image. This temporal control is aligned with the denoising dynamics: early denoising stages primarily reconstruct the general structure and layout, while later stages refine finer details. The effects of applying omega in different denoising stages are illustrated in Fig. 4. Note that the early stage for layout formation occupies only small portion of the overall schedule, typically within the first 10 steps in 50-step denoising process (τ 10 when = 50), due to the fact that layout information is only corrupted in the last few steps of the forward process. More schedules and their effects are visualized in Fig. 6. The omega schedule enables stage-specific control over image synthesis, allowing for nuanced manipulation of both composition and detail. This flexibility supports range of creative and practical applications where different stages of the denoising process demand distinct levels of control. 4. Experiments We examine the effectiveness of Omegance across various generative models and applications, including Stable Diffusion XL (SDXL) [16], RealVisXL-V5.0 [21], Stable Diffusion 3 (SD3) [5], FLUX [13], FreeU [22], SDEdit [15], [28], ReNoise [7], SDXL-Inpainting [16], ContolNet Latte [14], and AnimateDiff [8]. Implementations of these methods are based on Hugging Faces Diffusers repository1. Note that we exclude omega mask experiments in purely text-guided methods due to the lack of prior knowledge about the output composition. 4.1. Text-to-Image Generation Global Effect. Applying Omegance uniformly across spatial dimensions and consistently over time results in global 1https://github.com/huggingface/diffusers Figure 6. Temporal effects of schedule-based Omegance. Four quadrants in (a) are the same as those in Fig. 4. EXP1: More complex layout, slightly more fine detail. EXP2: More complex layout, less fine detail. COS1: Slightly less complex layout, less fine detail. COS2: Less complex layout, more fine detail. (Zoomin for best view) Figure 7. Spatial effects of mask-based Omegance in SDEdit results. (Zoom-in for best view) In lower quality models, granularity change, affecting both the layout and fine details of the output. More qualitative results are shown in Fig. 3. In addition to providing granularity control, Omegance also occasionally enhances the generated outputs, as shown in Fig. 5. like SDXL [16], Omegances detail suppression effectively addresses artifacts in human body parts, particularly in intricate areas like fingers and arms. Meanwhile, For high-quality models like FLUX [13] which tend to produce over-smoothed results, Omegances detail enhancement improves realism by restoring fine-grained textures and intricate details. We conducted two-part user study with 102 particiFigure 8. Spatial effects of mask-based Omegance in ControlNet results. (Zoom-in for best view) ularity control without degrading the base models quality. Average Rank Accuracy Output Quality Omegance 93.94% 81.98% Table 1. Results of user study on granularity control effectiveness and output quality of Omegance. Omega Schedule. We show two distinct omega schedules in Fig. 4 and their effects in Fig. 1(c). To further demonstrate the effectiveness of the omega schedule in controlling the granularity of the output layout and fine detail simultaneously, we illustrate more continuous schedules in Fig. 6. In the given case, layout complexity is generally reflected by the composition of the chalet and the fine detail richness by the festival decorations and footprints in the snow. 4.2. Image-to-Image Generation In image-to-image tasks, prior knowledge of image composition from reference inputs or structural guidance enables the effective use of omega masks to apply Omegance selectively. By assigning specific ω values to targeted regions, we achieve precise control over texture richness and smoothness, enhancing details in some areas while simplifying others. It is worth noticing that although unselected regions are not explicitly constrained, they experience minimal changes to maintain input consistency, demonstrating the precise region-based control of our omega mask. In all mask notations throughout this paper, red indicates detail enhancement, and blue represents detail suppression. SDEdit. For image-to-image editing using SDEdit [15], we use off-the-shelf semantic segmentation tool SAM2 [18] to generate the segmentation masks for the input images, enabling the application of omega masks to specific segmentation regions. Results are demonstrated in Fig. 7. ControlNet. Results of applying omega mask in ControlNet [28] are shown in Fig. 8. With ControlNet control signals, we can generate default masks that specify the regions Figure 9. Spatial effects of mask-based Omegance in ReNoise inversion results. (Zoom-in for best view) Figure 10. Effects of Omegance in image inpainting task. (Zoomin for best view) pants to evaluate Omegances effectiveness in granularity control and impact on output quality (see supp. for details). In Part 1, participants are asked to rank three images with and without Omegance based on their granularity. The average rank accuracy reflects the effectiveness of Omegance in granularity control. In Part 2, participants select the higher-quality result from pairs generated with and without Omegance, and we report the percentage of votes favoring Omegance or insisting equal quality. The results in Tab. 1 demonstrate that Omegance achieves effective gran7 Figure 11. Effects of Omegance in Text-to-Video results. (Zoom-in for best view) of interest. For pose signal, which infers the position and pose of the main character, we apply dilated convolution on the skeleton to obtain default character mask. For depth signal that conveys foreground and background information, we can use continuous depth values to create smooth, depth-wise transitions in the mask. Alternatively, it is always feasible to generate custom masks from user-provided strokes, allowing for more flexible and intuitive control over detail granularity. We use custom masks for canny signals. Real Image Editing. Omegance can also be applied spatially in real-image inversion tasks to achieve granularity editing of specific objects. The masks can be obtained from either segmentation tools or user-provided strokes. The results using ReNoise inversion [7] are presented in Fig. 9. Image Inpainting. We further generalize the use of Omegance to image inpainting task. Figure 10 shows the results of performing image inpainting using SDXL [16]. 4.3. Text-to-Video Generation Omegances granularity control ability can also generalize to text-to-video tasks. Experiments are conducted using Latte [14] and AnimateDiff [8]. In Fig. 11(a), ω increasing (detail suppression) leads to less complex background and smoother texture, while ω decreasing (detail enhancement) corresponds to more complex background and sharper texture. Additionally, current text-to-video generation techniques often suffer from visual artifacts in the output, as illustrated in the first row of Fig. 11(b), where the pandas guitar is distorted on the left, and an unintended flow effect appears in the sky on the right. Applying Omegance can effectively address the above artifacts through its detail enhancement and suppression effects, respectively, as demonstrated in the second row of Fig. 11(b). 5. Conclusion and Limitation We introduced Omegance, simple yet effective singleparameter technique for controlling granularity in diffusion model outputs, enabling fine-grained spatial and temporal adjustments to layout complexity and texture richness. The method is training-free, architecture-agnostic, and integrates seamlessly with various diffusion-based tasks. Extensive experiments demonstrate Omegances ability in controlling level of detail in text-to-image, image-to-image, and text-to-video generation results. While Omegance excels at nuanced granularity manipulation and can occasionally correct artifacts or enhance realism, it does not inherently improve the generation quality of the base model, which remains limitation. Nevertheless, we believe this work is valuable for advancing controllable and user-driven content generation, expanding the practical applications of diffusion-based synthesis in real life. 8 [16] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 2, 6, 8, 3, 5 [17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 3 [18] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [19] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2021. 3 [20] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann M. Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024. 3 [21] SG161222. RealVisXL V5.0. https://civitai.com/ models/139562/realvisxl-v50, 2024. 6, 2, 5 [22] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. FreeU: Free lunch in diffusion U-Net. In CVPR, 2024. 3, 6, 5 [23] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, 4, 5, 1 [24] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2, 3 [25] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in textto-image diffusion models. In CVPR, 2023. 3 [26] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and evaluating human preferences for textto-image generation. In NeurIPS, 2023. 3 [27] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, 2023. [28] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. ICCV, 2023. 2, 6, 7,"
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. In ECCV, 2024. 3 [2] Rudolf Arnheim. Art and Visual Perception. University of California Press, Berkeley, CA, 2nd, rev. and exp. ed., reprint 2020 edition, 2020. 2 [3] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. SEGA: Instructing text-to-image models using semantic guidance. In NeurIPS, 2023. 3 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to follow image editing instructions. In CVPR, 2023. 3 [5] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 5, 6, 1, 3 [6] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2023. [7] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. ReNoise: Real image inversion through iterative noising. In ECCV, 2024. 2, 6, 8 [8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. 2, 6, 8 [9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2023. 3 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2, 3 [11] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In ICCV, 2023. 3 [12] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. 4, 5, 1, [13] Black Forest Labs. FLUX. https://github.com/ black-forest-labs/flux, 2023. 2, 6, 5 [14] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2, 6, 8 [15] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2, 3, 6, 7 9 Omegance: Single Parameter for Various Granularities in Diffusion-Based Synthesis"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplemental material mainly contains: B. Modified SNR Derivations Representations of δt and ζt in Equ. 2 from different schedulers in Sec. A. Step-by-step derivations of the modified SNR in Sec. B. Detail description of user study setup in Sec. C. More discussions in Sec. D. More implementation details in Sec. E. More qualitative results in Sec. F. A. Representations of δt and ζt Equation 2 (recapped below) provides general representation of one denoising step in the diffusion reverse process. We provide detailed formulas of how to convert different denoising schedulers to the general representation. zt1 = δt zt + ζt ϵθ(zt, t) (cid:125) (cid:123)(cid:122) direction pointing to z0 (cid:124) (1) DDIM scheduler [23]: zt1 = αt1 zt (cid:124) θ (zt) 1 αtϵ(t) αt (cid:123)(cid:122) predicted z0 + (cid:112)1 αt1 ϵ(t) (cid:124) (cid:125) (cid:123)(cid:122) direction pointing to zt θ (zt) (cid:125) In DDIM denoising process, zt1 = zt αt1 1 αtϵθ αt (cid:123)(cid:122) predicted z0 αt1 ˆz0 + (cid:112)1 αt1 ˆϵθ (cid:125) (cid:124) + (cid:112)1 αt1 ϵ(t) (cid:124) (cid:123)(cid:122) direction pointing to zt θ (zt) (cid:125) So the original SNR during the reverse process is still: SNR(t 1) = αt1 1αt1 . Equation 6 shows the modified SNR based on DDIM denoising equation. The step-by-step derivations are provided below: t1 = αt1 zt 1 αtϵθ ω αt + (cid:112)1 αt1ϵθ ω Substitute zt = isolate z0 and ϵθ terms, we get: αtz0 + 1 αtϵθ into the equation and = δt zt + ζt ϵθ(zt, t) where δt = αt1 αt ζt = αt1 1 αt αt + (cid:112)1 αt1 where = = αt1 αt1 (2) Euler discrete scheduler [12]: Therefore, the modified SNR is: t1 = Az0 + Bϵθ 1 αt(1 ω) + αt αt 1 αt1ω zt1 = zt + (σt+1 ˆσ) ϵθ(zt, t) = δt zt + ζt ϵθ(zt, t) SNR = A2 B2 where δt = 1 ζt = σt+1 ˆσ (3) Flow-matching scheduler [5]: where zt1 = zt + dt vθ(zt, t) = δt zt + ζt ϵθ(zt, t) δt = 1 ζt ϵθ(zt, t) dt vθ(zt, t) 1 = = ( αt1 αt1 1αt(1ω)+ αt 1αt1ω)2 αt (cid:18) αt1 1αt αt + αt αt1 1αt1 αt αt 1αt (cid:19)2 ω ω Since αt monotonically decreases in diffusion model, so αt1 > αt and 1 αt > 1 αt1. Therefore, 1 αt is always negative and αt1 1 αt1 αt SNR increases as ω increases. C. User Study To demonstrate the effectiveness of Omegance in controlling image granularity and preserving image quality, we design two-part user study. In Part 1, for each question, the participants are given three images, one generated by the base model, and the other two by the base model with Omegance. The participants are asked to select the rank of image granularity (from high to low) that best matches their inspection. Instructions about granularity are given: Granularity in the context of image generation refers to the level of detail and texture richness in visual output. High granularity corresponds to intricate textures, complex patterns, and rich visual density, while low granularity is associated with smoother transitions, minimal detail, and simpler compositions. The motivation behind such design is that if the granularity rank can be correctly distinguished by the users, it proves the effectiveness of Omegance in controlling image granularity. The images used in Part 1 are generated by SDXL [16] and FLUX [13] using global Omegance. For quantitative evaluation, we consider each rank position separately and calculate the accuracy independently. The final average rank accuracy is the average accuracy of all rank positions. In Part 2, our objective is to illustrate that Omegance does not harm the base models quality and can occasionally improve it by fixing artifacts and enhancing realism. We present two contents generated by the base model with and without Omegance and ask the users to select the one with better quality. The instructions on visual quality definition are given as: Visual quality refers to the overall perceptual appeal and coherence of generated content, encompassing aspects such as sharpness, realism, fidelity to given prompt, and freedom from artifacts. We believe that by achieving over 50% votes in quality competition, it is sufficient to prove that Omegance can achieve at least equal quality outcomes as the base model. The results used in Part 2 are generated by SDXL [15], FLUX [13], RealVisXL5.0 [21], AnimateDiff [8], and Latte [14]. We report the percentage of votes indicating our method achieves equal or higher quality over the base model as our quality evaluation. D. Discussion In this section, we discuss several settings that achieve similar effects in granularity or motivate our design choices. D.1. Change Inference Steps An intuitive approach to influencing the granularity of generative outputs is to adjust the number of inference steps. However, as observed in our experiments, linearly increasing the number of steps does not provide consistent granularity control. Figure 12 and 13 demonstrate this using Figure 12. Change of omega (left) vs. Change of number of inference steps (right). Example 1. Orange box indicates default results when ω = 1.0 and the number of inference steps = 50. 2 SDXL [16] with the Euler discrete scheduler [12], where changes in layout complexity and texture richness caused by increasing steps show irregular patterns compared to the consistent effects achieved with Omegance. Moreover, modifying the number of steps impacts the entire image globally, offering no capacity for region-specific adjustments. In contrast, Omegance enables localized control through omega masks and schedules. Omega masks allow region-specific granularity adjustments, enhancing or suppressing detail in specific regions while preserving unselected areas, and omega schedules can refine layout or texture granularity at different denoising stages. This flexibility is especially valuable for real-world design tasks, where different regions or elements require varying levels of detail. Besides, increasing steps adds computational overhead without guaranteeing significant improvements in granularity control, while Omegance provides granularity manipulation within fixed inference schedule with no extra cost, maintaining computational efficiency and output fidelity. In summary, adjusting inference steps is coarse mechanism for granularity control, as it alters the overall denoising dynamics without offering fine-tuned control. Omegance, by operating directly within the existing denoising framework, delivers precise, user-driven adjustments without additional computational cost. D.2. Change Latent Mean In Omegance, we carefully preserve the mean of zt while altering its variance. This design choice is motivated by our observation that changes in the latent mean lead to color shifts in the final decoded outputs. This phenomenon can be verified by directly modifying the mean of VAE-encoded latents in latent diffusion model [19]. Given an original image in Fig. 14(a) as x0, we encode the image to latent z0 using the VAE used in SD3 [5] and modify the mean of z0 directly. The results of increasing and decreasing latent mean are shown in Fig. 14(b) and (c), respectively. These results reveal that the green channel is particularly sensitive to mean changes, while the red channel is the least affected. Consequently, increasing the mean causes the green color to dominate, while decreasing it leads to dominance of red tones, producing undesirable color shifts. To avoid such artifacts, we ensure mean preservation in the implementation of Omegance. E. More Implementation Details In this section, the implementation details for producing the demo contents shown in the main paper are explained. Figure 13. Change of omega (left) vs. Change of number of inference steps (right). Example 2. The orange box indicates default results when ω = 1.0 and the number of inference steps = 50. E.1. Prompts for Demo We use Large Language Model to help generate prompts. 3 Figure 1(a): serene lake at dawn, with crystal-clear water reflecting snow-capped mountains and soft pink sky. In the foreground, delicate wildflowers bloom along the shoreline, while small wooden rowboat drifts quietly. The smooth, mirror-like surface of the lake contrasts with the rich details of the flowers, boat, and distant mountains, all softly illuminated by the gentle morning light. Figure 1(b): wise wizard in long, starry cloak stands Figure 14. Effects of latent mean changes on image RGB mean with analysis. 4 in an enchanted forest clearing, with towering trees and vibrant mushrooms glowing softly. Around him, floating magical orbs and sparkling fireflies create mystical ambiance, while wildflowers and ivy-covered stones add texture to the scene. Figure 1(c): bustling medieval marketplace filled with colorful tents and stalls, where merchants display spices, textiles, and jewelry. Cobblestone streets wind between the booths, and in the distance, towering castle rises against the horizon. Figure 3 SDXL (Left): gentle healer dressed in flowing robes stands beside calm forest pond, her hand extended over the water as soft, glowing light surrounds her. The ponds smooth surface reflects her figure and the towering trees around, while small, vibrant wildflowers dot the mossy shore. Figure 3 SDXL (Middle): peaceful Japanese zen garden at dusk, with smooth raked sand patterns surrounding mosscovered stones and gentle, flowing stream reflecting the warm glow of lantern light. Figure 3 SDXL (Right): futuristic space station interior with rounded, smooth walls and control panels filled with colorful buttons and screens. Astronauts in sleek spacesuits float gently in zero gravity, surrounded by floating tools and shimmering holographic displays showing star maps and distant planets. Figure 3 SDXL + FreeU: bustling riverside cafe painted in impressionist style, where figures in soft, muted tones gather under glowing lanterns. The river reflects the lights, creating smooth, rippling patterns as the colors blend seamlessly with the surrounding trees and buildings. Figure 3 RealVisXL-5.0: serene meadow at sunrise, with vintage picnic blanket spread out under large oak tree. Wildflowers in pastel colors bloom across the soft grass, and wicker picnic basket filled with freshly baked bread and fruit adds to the idyllic, pastoral scene. Figure 3 SD3: celestial observatory with smooth, polished floors and massive domed ceiling painted with constellations in deep blues and silvers. large telescope stands at the center, and intricate star maps and charts are scattered around, bathed in the soft, ambient light of glowing stars. Figure 3 FLUX: cozy anime cafe on rainy afternoon, where customers sit at tables enjoying warm drinks as raindrops patter against the large windows. Soft, warm lighting gives the cafe welcoming feel, with framed pictures, books, and cushions scattered around, adding rich detail to the snug interior. Figure 5: boy is playing Pokemon. Figure 6: quaint alpine village market in winter, with stalls selling handmade crafts, baked goods, and hot drinks, nestled between snow-covered wooden chalets. Pine trees and mountain peaks frame the background, while soft snowfall adds charm to the festive, bustling atmosphere. Figure 7 (Top): creamy white marble table sprinkled with tiny violet and daisy blooms supports classic white plate with thin, golden edges. The chocolate cake, layered with dark chocolate curls and topped with vibrant berries, sits beautifully on the plate, creating striking contrast between the lush texture of the cake and the refined elegance of the setting. Figure 7 (Bottom): cool fox wearing sunglasses leaning on rusted water pipe. Figure 8 SDXL Pose (Left): Darth vander dancing in desert, high quality. Figure 8 SDXL Pose (Right): An elven warrior princess stands in lush forest glade, her armor decorated with delicate leaf patterns and shining gemstones. Her cape is embroidered with nature motifs, and her ornate bow and quiver add touch of elegance, creating striking blend of nature and nobility. Figure 8 SDXL Depth (Left): topiary plant decorated by flowers. Figure 8 SDXL Depth (Right): sleek, futuristic robot with polished metallic body and glowing blue eyes, standing upright with articulated limbs and fine details in its joints and circuitry. Soft lights reflect off its surface, highlighting its advanced design and smooth, streamlined form. Figure 8 SDXL Canny (Left): Aerial view, futuristic research complex in bright foggy jungle, hard lighting. Figure 8 SDXL Canny (Right): stunning piece of jewelry featuring delicate, silver pendant encrusted with sparkling diamonds and large, brilliant-cut sapphire at its center, suspended from fine, intricately detailed chain that glimmers in soft light. Figure 8 FLUX Canny (Right): sparkling crystal mansion stands in the middle of blooming meadow, its walls refracting sunlight into brilliant rainbows. Delicate cherry blossom trees line the cobblestone path leading to the ornate glass doors, while butterflies flutter in the gentle breeze. Figure 8 FLUX Canny (Left): whimsical, pastel-colored cottage with gently sloping roof and rounded windows sits amidst vibrant garden of oversized flowers. cobblestone path leads to the arched wooden door, and sparkling lights hang from the eaves, glowing softly under the golden afternoon sun. Figure 9 (Top): glass vase filled with flowers. Figure 9 (Bottom): kitten in basket. Figure 10: loosely arranged bouquet of wildflowers in glass jar sits in the center of the bench, with petals and leaves spilling over the edge. The soft textures of the flowers balance the structure of the jar and bench for touch of elegance. 5 Figure 15. Visualization of omega rescale function. E.2. Negative Prompt We use negative prompts where applicable to help improve generation quality: distorted lines, warped shapes, uneven grid patterns, irregular geometry, misaligned symmetry, low quality, bad quality. E.3. Omega Rescale As mentioned in Sec. 3.2, we rescale omega to allow finergrained control within (, ) input range by: ω = R(ϖ) = + 1 + ekϖ (10) The default rescaling function for SDXL [16] model is visualized in Fig. 15 with = 0.1, = 0.95, = 1.05. In this case, ϖ within [10.0, 10.0] would present visible effects. In SD3 [5] and FLUX [13], the range of ω should be larger around [0.8, 1.2]. F. More Qualitative Results In this section, we show more examples of Omegance in SDXL [16], SDXL+FreeU [22], RealVisXL-V5.0 [21], SD3 [5], FLUX [13], and ControlNet [28]. Control signals are indicated in the top left corner of the original results. Omega masks used are indicated in the bottom left corner of the corresponding Omegance-edited results. Red for detail enhancement, blue for detail suppression. Figure 16. More global effects of Omegance in SDXL. 6 Figure 17. More global effects of Omegance in SDXL+FreeU and RealVisXL-V5.0. 7 Figure 18. More global effects of Omegance in SD3 and FLUX. Figure 19. More temporal effects of schedule-based Omegance follow schedules defined in Fig. 6. 9 Figure 20. More spatial effects of mask-based Omegance. 10 Figure 21. More spatial effects of mask-based Omegance using ControlNet with canny signal."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University"
    ]
}