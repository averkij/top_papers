{
    "paper_title": "One-shot Entropy Minimization",
    "authors": [
        "Zitian Gao",
        "Lynx Chen",
        "Joey Zhou",
        "Bryan Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 2 8 2 0 2 . 5 0 5 2 : r One-shot Entropy Minimization Zitian Gao Lynx Chen Joey Zhou Bryan Dai* Ubiquant {ztgao02,ylchen,jzhou,cbdai}@ubiquant.com"
        },
        {
            "title": "Abstract",
            "content": "We trained 13,440 large language models and found that entropy minimization requires only single unlabeled data and 10 steps optimization to achieve performance improvements greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em."
        },
        {
            "title": "1 Introduction",
            "content": "The post-training phase of large language models (LLMs) has advanced rapidly [3, 9, 2022, 24], with models like DeepSeek-R1 [4], Kimi-K1.5 [19], and OpenAI o-series [13, 14] demonstrating remarkable reasoning abilities. However, preparing for Reinforcement Learning (RL) is never an easy task. It often requires large amount of high-quality ground truth labeled data, along with the careful design of rule-based rewards to maximize advantage signals and prevent reward hacking. In contrast, Entropy Minimization (EM) is entirely unsupervised. We used the EM method to train 13,440 large language models in order to eliminate randomness in training as much as possible and ensure that the experimental results and observed patterns are reliable. Our rigorous study demonstrates that using just single piece of unlabeled data, the performance already surpasses that of traditional RL. Moreover, they typically converge within just 10 training steps, which is significantly faster than the thousands of steps often required for RL. EM is based on two direct and simple assumptions: (1) The sampling process in generation of large language models is inherently stochastic, (2) Correct answers generally have lower entropy than incorrect ones. Our study reveals that EM and RL share the same goal: unlocking the pretrained models latent potential without adding new knowledge [11]. Both rely on process we call token reranking to maximize the models performance. We find that entropy minimization has the capacity to rival RL in the post-training phase. Our main contributions are as follows: We propose One-shot Entropy Minimization, surprisingly powerful and fully unsupervised method that rivals or surpasses reinforcement learning using just single unlabeled data. We conduct an in-depth analysis of the effectiveness of One-shot EM, making it highly reasonable approach. We find that it shares many core properties with RL, yet drives model behavior in the opposite direction when viewed through the lens of logits shift. We extensively evaluate EM and identify temperature as key factor for both training and inference, and EM shows an opposite trend to RL with respect to inference-time temperature. We reveal that EM is distribution shaping tool rather than learning method by analyzing the inconsistency in loss-reasoning curve 3.4 and the logit shift 3.3 effect of EM. * Corresponding author."
        },
        {
            "title": "2.1 Entropy Minimization Algorithm",
            "content": "Let denote the vocabulary of pretrained autoregressive language model pθ, parameterized by θ. Given an input prompt (e.g., question or problem description), the model autoregressively generates response sequence = (y1, y2, . . . , yT ) according to its current policy: pθ(y x) = (cid:89) t=1 pθ(yt y<t, x), where is the length of the generated sequence. Our core idea is to reduce the models uncertainty over its own predictions by minimizing the token-level entropy at each generation step. The conditional entropy at time step is defined as: Ht = (cid:88) vV pθ(v y<t, x) log pθ(v y<t, x). To avoid penalizing the prompt portion, we compute entropy only over the generated tokens. Let Tprompt denote the number of tokens in the prompt x. Then the set of target positions is: = {t > Tprompt, yt = [PAD]}. The overall EM loss for single input is given by: LEM(x; θ) = 1 (cid:88) tI Ht. This loss encourages the model to become more confident in its own predictions without relying on external supervision.Entropy Minimization loss is fully differentiable with respect to model parameters, with gradients resembling the score-function estimator found in entropy-regularized reinforcement learning. Whats more, Entropy Minimization offers closed-form objective, eliminating the need for external reward estimation or value baselines, thereby simplifying optimization while retaining the effectiveness of entropy-driven exploration and exploitation."
        },
        {
            "title": "2.2 Data Selection",
            "content": "Entropy minimization (EM) relies on the premise that the models predictive uncertainty can serve as meaningful training signal. However, not all input prompts are equally informative in this regard. As noted in prior work [16,20], certain prompts elicit deterministic behavior from the model (e.g., always correct or always incorrect), yielding limited gradient information under entropy-based objectives. To address this, we adopt variance-based data selection strategy. Specifically, we measure the variance of the models pass@k accuracy across multiple samples and select prompts for which the model exhibits the highest behavioral variance. This targets inputs on the cusp of the models capabilityneither trivial nor impossiblemaking them ideal for entropy-driven optimization. Given prompt x, we draw independent samples from the model: (x) = y(1), y(2), . . . , y(k)(cid:111) (cid:110) , y(i) pθ( x). We then compute the pass@k score as: 2 pass@k(x) = 1 (cid:88) i=1 (cid:104) y(i) is correct (cid:105) , where I[] is the indicator function for whether sample is considered correct (via execution or string match). We further compute the sample variance of this binary success variable: Varpass@k(x) = 1 (cid:88) i=1 (cid:16) (cid:104) (cid:105) y(i) is correct pass@k(x) (cid:17)2 . This variance quantifies the inconsistency of the models predictions for given input. low variance indicates either high confidence in correctness (near-perfect success) or high confidence in failure (uniformly wrong), both of which are suboptimal for entropy minimization, as they lead to low-entropy posteriors that cannot be further improved. We therefore define our data selection objective as: = arg max xD Varpass@k(x), where denotes the unlabeled data pool. This approach effectively prioritizes those prompts where the model exhibits the most behavioral uncertainty, making them entropy-sensitive. Such prompts are empirically found to produce the largest entropy gradients and hence drive meaningful parameter updates under EM. Intuitively, data with high pass@k variance suggests that the models response distribution is straddling the decision boundarysometimes correct, sometimes notindicating broad or multimodal predictive distribution. These are precisely the regions where entropy minimization is most impactful: it encourages the model to concentrate its probability mass on consistent and (ideally) correct reasoning trajectory. By contrast, if model consistently answers question correctly or incorrectly regardless of sampling, the entropy is either already minimal or optimization is ineffective. Thus, high-variance prompts provide the richest signal for improving model calibration and reasoning fidelity. sample from the NuminaMath [10] dataset that meets the data filtering criteria is as follows: An example of selected data Problem: The pressure exerted by wind on sail varies jointly as the area of the sail and the cube of the winds velocity . When the velocity is 8 miles per hour, the pressure on sail of 2 square feet is 4 pounds. Find the wind velocity when the pressure on 4 square feet of sail is 32 pounds. Solution: 12."
        },
        {
            "title": "3.1 Experimental Setting",
            "content": "We implemented the overall training process of EM based on Acclerate [7]. We selected 1 piece of data from the dataset as prompt. Since it is an unsupervised method, we do not need any data labels. 3 Temperature Batch Size Learning Rate Max Response Len 0.5 64 2 10 2048 Table 1: Training hyper-parameters We directly train the model for only 10 steps with constant learning rate of 2 105, temperature parameter of 0.5, and batch size of 64. 1 Model OpenReasoner-Zero-7B [9] SimpleRL-Zoo [24] Prime-Zero-7B [3] Oat-Zero-7B [12] RLVR-GRPO [20] + 16-shot + 1-shot Qwen2.5-Math-7B + EM 1-shot Dataset Size Training Step MATH500 Minerva Math Olympiad Bench AMC23 Avg. 129k 24K 230K 12K 1.2 16 1 NA 1 600+ 4000 240 1000 1000 1000 NA 10 79.2 76.8 83.8 80.0 78.6 77.8 78.6 31.6 30.9 36.0 30.1 33.8 35.3 36. 44.0 39.4 40.9 41.0 41.6 39.9 43.7 53.0 78.825.8 11.0 35.324.3 17.2 39.722.5 47.0 55.3 62.7 62. 62.5 62.2 61.9 44.1 50.5 50.6 55.9 53.5 54.1 53.8 55.1 31.3 70.326.2 56.024. Table 2: Comparison of different methods on math reasoning benchmarks."
        },
        {
            "title": "3.2 Main Result",
            "content": "We present our experimental results in Table 3. Compared with most RL-based baselines, our EM 1-shot results show strong competitiveness. Specifically, when applying our 1-shot EM method to the Qwen2.5-Math-7B base model, substantial performance improvements are observed across all evaluated math reasoning benchmarks. The performance on MATH500 significantly increases by 25.8 points (from 53.0 to 78.8), and similarly large improvements occur in Minerva Math (+24.3 points), Olympiad Bench (+22.5 points), and AMC23 (+26.2 points). On average, the EM 1-shot strategy achieves an impressive gain of 24.7 points compared to the original Qwen2.5-Math-7B model. Notably, even with only single-shot example and minimal training steps (only 10), EM dramatically reduces the gap between Qwen2.5-Math-7B and state-of-the-art RL-based models such as Prime-Zero7B and RLVR-GRPO. In particular, on the AMC23 benchmark, the EM-enhanced Qwen2.5-Math-7B achieves competitive score of 70.3, nearing the leading RL models. These results clearly indicate that Entropy Minimization (EM), despite being simpler and more data-efficient technique compared to typical reinforcement learning methods, has substantial potential to enhance the performance of foundational language models in math reasoning tasks. During training, the model repeatedly reinforces its own confidence, causing the logits distribution to gradually shift to the righta phenomenon that will also be discussed in detail in Section 3.3."
        },
        {
            "title": "3.3 Logits Shift",
            "content": "We sample 20 prompts from the NuminaMath [10] dataset and generate responses using four different models (Qwen2.5-Math-7B, Qwen2.5-Math-7B-EM, Qwen2.5-Math-7B-RL, Qwen2.5-Math-7BEM-RL). Each model generates 20 responses, resulting in total of 4 20 = 80 outputs. For each 1The reason why only 10 steps are sufficient will be explained in detail in Section 3.4. 4 output, we extract the logits, defined as the unnormalized scores zi produced by the model before applying the softmax function. Given set of logits {z1, z2, . . . , zn}, the probability for each token is computed as: pi = ezi j=1 ezj (cid:80)n . To analyze the overall distributional behavior, we flatten all collected logits across all responses into single one-dimensional vector: zflat = {z(1) 1 , z(1) 2 , . . . , z(1) m1 , z(2) 1 , . . . , z(2) m2 , . . . , z(k) mk }, where z(i) samples. This flattening allows us to perform global statistical analyses. denotes the j-th logit in the i-th sample, and = 80 is the total number of generated To quantify the asymmetry of the flattened logits distribution, we compute the skewness: Skewness = 1 (cid:88) i= (cid:18) zi µ σ (cid:19)3 , where µ and σ are the mean and standard deviation of the flattened logits vector zflat, respectively. positive skewness indicates right-tailed distribution, whereas negative skewness indicates left-tailed distribution. Figure 1: Distribution plot of the flattened logits sampled from 20 sample data points for each of the four models: Base, EM, RL, and RL after EM. As observed in Figure 1, models trained with EM exhibit significantly higher skewness in their logits distribution, indicating rightward shift. This suggests that EM increases overall model confidence, concentrating probability mass on subset of tokens. As result, previously high-probability regions in the original logits become extended into long-tail high-probability intervals. In contrast, models trained with RL show pronounced reduction in logits skewness, indicating leftward shift in the distribution. We hypothesize that this is due to the influence of ground-truth signals during RL training. Specifically, RL appears to perform re-ranking that suppresses tokens with high predicted probability but low alignment with ground truth, thereby reducing their rank and shifting the overall distribution leftward. Similarly, models trained with EM followed by RL exhibit 5 the same pattern: after RL, the skewness of the models logits decreases from 1.54 (after EM) to 1.47, aligning with this pattern. We refer to this phenomenon as logits shift. rightward logits shift is beneficial to the generation and sampling process of large language models, as the model primarily samples from high-probability tokens. Shifting the logits to the right increases the number of candidate tokens and expands high-probability paths, thereby enhancing model capability. In contrast, leftward logits shift is considered harmful to the generation process of large language models, as it reduces the number of high-probability paths during samplingopposite to the effect of rightward shiftthus diminishing the models overall performance. Therefore, the rightward logits shift induced by EM is preferable to the leftward shift caused by RL."
        },
        {
            "title": "3.4 Training Loss vs. Reasoning Performance",
            "content": "Figure 2: The left y-axis represents the EM training loss, while the right y-axis shows the average score across four reasoning benchmarks. It can be observed that the training loss converges rapidly, whereas the average score peaks around step 10 and then begins to decline. The results in the figure are obtained by repeating the experiments with the same training hyper-parameters using 16 different seeds to reduce randomness. As shown in Figure 2, the loss drops to relatively low level around step 10, and the models performance on mathematical reasoning reaches its peak. However, unexpectedly, as the EM training loss continues to decrease beyond step 10, the mathematical reasoning performance begins to decline. In conjunction with the findings presented in Section 3.3, we argue that EM functions primarily as tool for shaping the models distribution rather than as learning method or strategy. Consequently, the effect of distribution shaping is largely achieved within very small number of training steps, leading to decoupling between the continued decrease in EM training loss and improvements in mathematical reasoning performance."
        },
        {
            "title": "3.5 Sampling Temperature in Training",
            "content": "As shown in Figure 3, the average performance of the EM-trained model across four math reasoning benchmarks generally exhibits an upward trend as the generation temperature increases. The maximum of the average performance initially increases and then declines around temperature of 0.5. Higher temperatures lead to better average reasoning ability, while moderate temperatures (e.g., 0.5) result in greater performance variance, thereby creating opportunities for higher peak performance. Therefore, we prioritize the model trained at temperature 0.5 when reporting final results. 6 Figure 3: The impact of generation temperature during EM training on the average performance of the trained model across four reasoning datasets. The results in the figure are obtained by repeating the experiments with the same training hyper-parameters using 16 different seeds to reduce randomness. However, as shown in the figure, EM training exhibits significant randomness. The results in the figure are obtained by repeating experiments with 16 different random seeds under the same set of hyperparameters. It can be seen that, even with identical settings, the average scores across the four math reasoning benchmarks can differ by as much as factor of two depending on the seed. Therefore, all the conclusions in this paper are based on at least 16 repeated experiments with different seeds. We also advocate that future research should focus on reducing the stochasticity of EM training."
        },
        {
            "title": "3.6 Sampling Temperature in Evaluate Generation",
            "content": "Figure 4: The impact of generation temperature during evalutating on the average performance of the trained model across four reasoning datasets. The results in the figure are obtained by repeating the experiments with the same training hyper-parameters using 16 different seeds to reduce randomness. As shown in Figure 4, we observe striking pattern in the EM-trained models response to varying sampling temperatures during generation. Specifically, as the sampling temperature increases during evaluation, the models average performance across four math reasoning benchmarks consistently decreases. This trend is in sharp contrast to that of Reinforcement Learning (RL)-trained models shown in Figure 4, where higher sampling temperatures often improve performance. 7 Greedy Decoding. The observation can be formally contextualized through the greedy decoding process, which selects the token with maximum conditional probability at each step: yt = arg max vV pθ(v y<t, x), where is the vocabulary and is the input prompt. Together with our analysis in Section 3.3, we hypothesize that EM training systematically reshapes the models logits distributions to become increasingly right-skewed. This process reinforces confidence in already high-probability tokens, effectively concentrating the probability mass on semantically coherent and correct options. As result, greedy decodingwhich deterministically selects the most probable tokenbecomes highly effective strategy post-EM fine-tuning. In contrast, RL adjust token probabilities based on external ground-truth reward. This often promotes the relative ranking of previously low-probability (tail) tokens. Even after reranking, these tokens tend to occupy intermediate positions in the probability distribution, requiring higher temperatures during sampling to be selected. Consequently, RL-trained models exhibit the opposite trend: performance improves with higher sampling temperatures, as seen in Figure 4."
        },
        {
            "title": "3.7 EM Before/After RL",
            "content": "Figure 5: The blue curve on the left represents the average performance across four mathematical reasoning benchmarks of the model trained with RL during the EM phase as training steps progress, while the curve on the right shows the performance of the model trained with EM during the RL phase. The results in the figure are obtained by repeating the experiments with the same training hyper-parameters using 16 different seeds to reduce randomness. Figure 5 shows clear asymmetry: applying Entropy Minimization (EM) after Reinforcement Learning (RL) leads to steady decline in performance across four math benchmarks, whereas applying RL after EM yields consistent gains. This suggests that EM exacerbates the distributional distortions introduced by RL, reinforcing the alignment tax of RL. This aligns with prior work showing that RL is most effective when preceded by Supervised FineTuning (SFT) [2,6], while applying SFT or entropy-based methods after RL often harms performance. In our case, EM after RL locks in narrow, overconfident output modes, while EM before RL enhances reasoning and allows RL to refine outputs without degrading diversity or accuracy."
        },
        {
            "title": "3.8 EM on Various Models",
            "content": "In Table 3, we report the peak performance of 1-shot EM after minimal training steps on various base models. Remarkably, with only single exemplar and minimal optimization, EM consistently boosts inference accuracy across all models on tasks MATH500, Minerva Math, Olympiad Bench, and AMC23. For instance, on the Qwen2.5-Math-7B model, 1-shot EM yields impressive improvements 8 of 25.8 points on MATH500 (from 53.0 to 78.8), 24.3 points on Minerva Math (from 11.0 to 35.3), 22.5 points on Olympiad Bench (from 17.2 to 39.7), and 26.2 points on AMC23 (from 44.1 to 70.3). This demonstrates EMs ability to substantially enhance reasoning performance with virtually no extra data or compute. Consequently, EM emerges as compelling lightweight alternative to RL across diverse model bases. We observe that the upper bound of 1-shot EMs gains is determined by the base models intrinsic reasoning strength. On the relatively weak LLaMA-3.1-8B, 1-shot EM raises average accuracy only to 24.3%, barely surpassing the baseline of 23.6%. This suggests that when the underlying model lacks sufficient reasoning capacity, minimal EM optimization cannot fully compensate for its deficits. In contrast, on the stronger Qwen2.5-7B base, 1-shot EM elevates average accuracy significantly from 29.6% to 37.3%. Even more notably, on the highly capable Qwen2.5-7B-Instruct base, 1-shot EM pushes average accuracy from 43.12% to 44.5%, not only surpassing the baseline but also outperforming EM alone. Crucially, EM requires no external instructionresponse pairs or reward models and can be applied on top of existing SFT or RL checkpoints as \"confidence compression\" layer, enabling faster convergence and more stable outputs from very few exemplars. However, interestingly, on the RL-trained SimpleRL-Zoo base, 1-shot EM resulted in accuracy drop from 44.14% to 39.1%, suggesting potential limitations when applied to models already extensively fine-tuned with RL methods. Nonetheless, across most scenariosespecially in SFT and minimally trained RL contextsEM significantly prunes redundant decision paths and stabilizes key predictions, validating its effectiveness and versatility as minimal yet powerful optimization strategy. 3. 1-shot vs. Multi-shot Model MATH500 Minerva Math Olympiad Bench AMC23 Avg. LLaMA-3.1-8B + EM multi-shot + EM 1-shot Qwen2.5-7B + EM multi-shot + EM 1-shot Qwen2.5-7B-Instruct + EM multi-shot + EM 1-shot SimpleRL-Zoo + EM multi-shot + EM 1-shot Qwen2.5-Math-7B + EM multi-shot + EM 1-shot 49.0 49.2 42.6 59.8 61.8 67.4 77.0 74.6 76.4 76.8 77.2 73. 53.0 68.0 78.8 21.3 23.2 21 10.3 18.4 22.1 37.1 34.2 36.4 30.9 29.0 24.6 11.0 18.8 35. 15.4 13.8 17 29.2 31.1 33.6 37.9 36.4 39.3 39.4 35.7 35.3 17.2 33.2 39.7 25.6 25.9 37. 40.3 43.4 45.6 51.9 48.4 54.1 55.3 55.9 45.9 44.1 61.6 70.3 27.8 28.0 29.6 34.9 38.7 42. 50.9 48.4 51.5 50.6 49.5 44.7 31.3 45.4 56.0 Table 3: Comparison of different models on various mathematical benchmarks. As shown in Table 2, EM training with just one or two examples achieves an average score on four math benchmarks that is on par with dataset comprising thousands of examples, and even yields improvements, respectively. Data in Table 2 further demonstrate that EM using minimal exemplar set delivers superior generalization and stronger performance across diverse downstream tasks. To uncover the underlying factors, we conducted detailed analysis of the 1-shot EM training dynamics. We observed that both prompt length and generated output length remain markedly more 9 stable under 1-shot EM. Moreover, whereas multi-shot EM losses continue to fluctuate significantly after step 3, the 1-shot EM loss steadily declines from step 3 onward and remains at low level beyond step 10. This indicates that relying on single exemplar substantially reduces sample bias and narrows output variance, enabling more fine-grained and stable optimization. Furthermore, as depicted in Figure 3.4, EM exhibits RL-like post-saturation generalization behavior: the 1-shot EM loss saturates at step 10, highlighting its remarkably fast convergencejust one example is sufficient to achieve significant gains. Notably, in the late stage of training, EM loss and model performance become decoupledfurther loss reduction does not translate into performance gains and may even incur slight degradation. We attribute this phenomenon to EMs \"over-confidence\" effect, wherein the model amplifies its own prior biases, ultimately leading to performance penalty."
        },
        {
            "title": "3.10 Training Learning Rate",
            "content": "Figure 6: The impact of learning rate during EM training on the average performance of the model at step 10 across four reasoning datasets. The results in the figure are obtained by repeating the experiments with the same training hyper-parameters using 16 different seeds to reduce randomness. As shown in Figure 6, learning rate of 2 105 demonstrates significant advantage. Since EM typically converges and reaches peak performance within just 10 steps, an excessively large learning rate may cause severe and rapid overconfidence, while smaller learning rate can slow down convergence. To improve training efficiency, we therefore recommend using 2 105 as the default baseline setting for EM training."
        },
        {
            "title": "4 Future Work",
            "content": "Stablize EM Training. One notable finding in our study is the extreme efficiency of entropy minimization (EM), with meaningful improvements achieved in as few as 10 training steps. However, this rapid convergence comes with potential trade-off: sensitivity to hyperparameters and training instability. Our analysis in Section 3.2 shows that beyond certain point, continued EM loss reduction may actually harm model reasoning performance. This indicates that EM functions more as distribution shaping mechanism than standard learning algorithmonce the model becomes too overconfident, its outputs may collapse into overly narrow token distributions, reducing diversity and correctness. Future work could investigate early stopping criteria or adaptive scheduling mechanisms to stabilize EM training and prevent performance degradation. 10 Exploring the Full Potential of EM Despite its simplicity, EM exhibits surprisingly strong performance with only single unlabeled example. This raises intriguing questions about how far entropy-based objectives can go. For instance, can EM generalize beyond reasoning tasks to other domains like dialogue, summarization, or code generation? Moreover, current EM setups operate at the token levelfuture extensions might consider structured entropy over full sequences or semantic units to better capture high-level uncertainty. Incorporating task-specific priors or adaptive entropy regularization may also help unlock further potential from EM training. Additionally, the connection between entropy minimization and implicit confidence calibration warrants deeper investigation. Our findings suggest that EM enhances model confidence by reinforcing high-probability reasoning paths 3.3. This implies EM might serve as lightweight alternative to complex calibration techniques, especially for tasks where interpretability and robustness are critical. Developing evaluation protocols to more precisely quantify EMs calibration effects will be an important direction. Combining EM with Other Post-Training Techniques Entropy minimization is conceptually orthogonal to most existing post-training paradigms, including supervised fine-tuning (SFT) and reinforcement learning (RL). This opens exciting opportunities for hybrid methods. For example, EM can be applied before SFT to sharpen models predictive distributions, improving its receptiveness to downstream supervision. Alternatively, EM could serve as regularization strategy during SFT or RLHF. We also note that applying EM before RL, as briefly explored in Section 3.5, leads to beneficial shifts in logit distributions that may facilitate faster and more stable policy optimization. systematic study of different EM+RL schedules, curriculum strategies, and interaction effects would help elucidate the best ways to integrate these methods."
        },
        {
            "title": "5.1 Entropy Minimization",
            "content": "Agarwal et al. [20] was the first to discover that simply optimizing the entropy loss can lead to significant improvements in reasoning performance. However, they did not explore this phenomenon in depth, stating in the original paper, \"We leave the rigorous analysis to future works.\" Wang et al. [1] is the first work to study entropy minimization in post-training for large language models. However, they consider the effectiveness of entropy minimization to be unreasonable, presenting experimental results but offering limited in-depth analysis of entropy minimization."
        },
        {
            "title": "5.2 Reinforcement Learning for LLM",
            "content": "Recent research has increasingly explored post-training approaches to improve the reasoning abilities of large language models. These approaches often involve additional fine-tuning or reinforcement learning using curated datasets that include reasoning tasks and chain-of-thought annotations [1,3,5,9, 20,21,23,24]. Reinforcement learning techniques such as Direct Preference Optimization (DPO) [15], Proximal Policy Optimization (PPO) [17], Group Relative Policy Optimization (GRPO) [18], and REINFORCE++ [8] are gaining prominence in this area."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces one-shot entropy minimization as simple yet powerful post-training method for large language models. Using just single unlabeled example, our approach achieves performance comparable to or better than reinforcement learning methods that rely on large-scale supervision and carefully designed rewards. It is fully unsupervised, highly efficient, and converges within few 11 training steps. Our experiments show that entropy minimization reshapes the models output distribution to increase confidence in correct reasoning paths, effectively enhancing the utility of pretrained knowledge. We identify key indicatorssuch as logit skewness and behavioral variancethat help target entropy-sensitive inputs and guide optimization. These findings indicate that significant improvements in reasoning performance can be attained by restructuring existing knowledge through confidence calibration, rather than by acquiring additional information. Entropy minimization thus emerges as both practical technique and conceptual framework for advancing our understanding of post-training in large language models."
        },
        {
            "title": "7 Acknowledgement",
            "content": "We thank Tian Xie, Zhengmao Ye, Peihao Wu and Derek Li at Ubiquant for their support and insightful discussions."
        },
        {
            "title": "References",
            "content": "[1] Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning, 2025. [2] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback, 2023. [3] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. [4] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [5] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning, 2024. [6] Qi Gou and Cam-Tu Nguyen. Mixed preference optimization: Reinforcement learning with data selection and better reference model, 2025. [7] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. [8] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models, 2025. [9] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. [10] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. [11] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. [12] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. [13] OpenAI. Introducing openai o1. https://openai.com/o1/, 2024. Accessed: 2024-10-02. [14] OpenAI. Introducing openai o3 and o4-mini, April 2025. Accessed: 2025-05-27. [15] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Curran Associates, Inc., 2023. [16] Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason D. Lee, and Sanjeev Arora. What makes reward model good teacher? an optimization perspective, 2025. [17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [18] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [19] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, 13 Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [20] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. [21] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. [22] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. [23] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025. [24] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025."
        }
    ],
    "affiliations": [
        "Ubiquant"
    ]
}