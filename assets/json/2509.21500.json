{
    "paper_title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
    "authors": [
        "Junkai Zhang",
        "Zihao Wang",
        "Lin Gui",
        "Swarnashree Mysore Sathyendra",
        "Jaehwan Jeong",
        "Victor Veitch",
        "Wei Wang",
        "Yunzhong He",
        "Bing Liu",
        "Lifeng Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 0 5 1 2 . 9 0 5 2 : r Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training Junkai Zhang*,2, Zihao Wang*1, Lin Gui*3, Swarnashree Mysore Sathyendra1, Jaehwan Jeong1, Victor Veitch3, Wei Wang2, Yunzhong He1, Bing Liu1, and Lifeng Jin1 1Scale AI, Inc. 2University of California, Los Angeles 3University of Chicago Abstract Reinforcement fine-tuning (RFT) often suffers from reward over-optimization, where policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish excellent responses from merely great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements."
        },
        {
            "title": "1 Introduction",
            "content": "In this paper, we are interested in how to produce reward models that are effective when used for LLM post-training. reward model is function that takes prompt and response and produces score quantifying how good that response is for the prompt. In post-training, we then align language model to the reward by reinforcement-learning type procedure. The fundamental challenge here is that, in many settings, it is nearly inevitable that the reward model will be an imperfect proxy for the behavior that we are actually trying to induce. In particular, this means that as we run post-training, it will increasingly be the case that the LLM is aligned to the idiosyncratic misspecification of the reward rather than the true signal that we are trying to extract. In this paper, we are interested in mitigating this effect. Given that some misspecification is inevitable, what should we focus on when defining reward model? The basic setup of post-training aims to induce the good behavior encoded by the reward while minimally shifting other aspects of the base LLM. Mathematically, this can be formalized as looking for post-training procedures that move along the Pareto frontier of KL divergence from the base model vs win-rate (as judged by the reward) against the base *Equal contribution. Work done during internship in Scale AI, Inc. 1Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git. 1 Figure 1: Chasing the Tail with Rubric-Based Rewards model. We begin by theoretically demonstrating that, for such Pareto-optimal procedures, the effect of reward misspecification is dominated by errors in the high-reward region. In other words, what really matters for post-training is the ability to accurately distinguish between the very good responses. Then, we know that we want to focus our reward modeling on the high-reward region of examples. The basic challenge here is that actually producing high-reward examples to train reward model on is hard. If we simply sample responses from the base LLM itself, then it is extremely sample inefficient to get the necessary examples (because we are trying to get elements in low-probability tail). On the other hand, if we use an off-policy proceduree.g., drawing samples from stronger LLM, or producing good examples with extensive thinking or rewriteswe can get high-reward examples, but naively training reward model on them may learn superficial features instead of eliciting real capabilities (see Section D). To address this challenge, we empirically study rubric-based rewards as solution to this problem. In essence: we get very strong examplar responses by using off-policy generation. Then, we produce reward model using these examples by using another LLM to produce grading rubric for each prompt. Such rubric-based rewards will generalize well off-policy because they are insensitive to irrelevant aspects of the responses by design. The question is then if, and how, we can elicit rubrics that succeed in capturing the high-reward tail behavior necessary for alignment. We give two principles for achieving this goal. We then produce workflow implementing these ideas and show empirically that it is highly effective for the LLM post-training task. Summarizing, the contributions of this paper are: 1. theoretical characterization of how reward misspecification matters for post-training, concluding that the high-reward region is key, 2. method for constructing effective reward rubrics using off-policy data, and 3. An empirical study showing the efficacy of the constructed rubrics for post-training, and confirming the critical role of misspecification in the high-reward region."
        },
        {
            "title": "2 Preliminaries",
            "content": "Notations. We use π to denote large language model (LLM) and π 0 to denote the reference language model (usually the starting point of RL). Given prompt x, response is sampled from the conditional distribution π( x). reward model r(, ) is utilized to assess the quality of prompt-response pair. We use to represent the gold reward model (inaccessible in practice) and to represent the proxy reward applied in practice. Reinforcement fine-tuning (RFT). With prompt set and reward model r, the reinforcement fine-tuning optimizes the following objective [Ouyang et al., 2022, Bai et al., 2022]: max π (cid:69) xD, yπ( x) [r(x, y)] β(cid:68) KL [π( x)π 0 ( x)] , (2.1) 2 where β is hyperparameter to control fine-tuned models deviation from the reference model, i.e., π( x) ( x) π 0 As demonstrated in Rafailov et al. [2023], the solution to (2.1) is ( x)] = (cid:69) [π( x)π xD, yπ( x) log (cid:68) KL 0 (cid:149) (cid:152) . π ( x) π ( x) exp{r(x, y)/β}. (2.2) Reward over-optimization. Because RFT relies on proxy rewards in practice, it inevitably suffers from reward over-optimization: the policy exploits inaccuracies in the reward model, achieving high proxy scores while true quality deteriorates. This phenomenon has been well studied in Bradley-Terry reward models trained on human preference data [Gao et al., 2023]. The standard remedy is online RLHF, where fresh human feedback is periodically collected to update the reward model and mitigate over-optimization [Bai et al., 2022], but such approaches are costly and slow. Reinforcement learning from rubrics-based reward. Reinforcement learning from rubrics-based reward (RLRR) [Gunjal et al., 2025, Viswanathan et al., 2025, Huang et al., 2025b] has emerged as promising approach for open-ended tasks. The core idea is to associate each prompt with rubrica set of explicit criteria (ci) with corresponding weights (wi) that collectively define high-quality response. For instance, given prompt asking for likely diagnosis from patients symptoms, the rubric could specify key aspects of good answer. This might include high-weight criteria for identifying [correct diagnosis] as the likely diagnosis and correctly identifying the condition as medical emergency, and low-weight criterion for mentioning [typical treatment] for treatment (See Section for concrete example.) In this framework, verifier , typically another LLM, assesses whether given response satisfies each individual criterion. The total reward is then calculated as the weighted average of the criteria that the response successfully meets. Formally, the verifier outputs binary score for each criterion, (x, y, ci ) (cid:55) {0, 1}, and the total reward is: (cid:80) r(x, y) = wi (x, y, ci wi (cid:80) ) . RLRR extends Reinforcement Learning with Verifiable Rewards (RLVR) to general tasks where performance cannot be easily verified. Compared to RFT using Bradley-Terry reward models, RLRRs explicit criteria make rewards more interpretable and harder to game. However, its still unclear if, and how, RLRR alleviates reward over-optimization."
        },
        {
            "title": "3 High-Reward Region Accuracy is Key to Overcoming Re-",
            "content": "ward Over-optimization Its well-known that using misspecified proxy rewards lead to reward over-optimization for reinforcement post-training. However, the ways in which different misspecification patterns of proxy rewards influence the performance of the aligned model remain poorly understood. In this section, we develop theoretical results showing that maintaining high-reward region accuracy is the key determinant of alignment quality. We introduce misspecification mapping from gold to proxy rewards and cast the problem as analyzing how the geometry of affects performance. More specifically, : (cid:82) (cid:82) is the mapping from to r, i.e., for any xy pair, (r (x, y)) = r(x, y). 3 (a) Win rate with reward misspecification (b) Win rate when different proportions of top responses are correctly ranked Figure 2: Theoretical impact of reward model misspecification on performance. (a) Inaccuracy in the high-value region causes performance to collapse. (b) Correctly ranking top responses is sufficient for near-optimal performance. To characterize the reward over-optimization phenomenon, we need to study the relationship between the utility (expected reward and win rates), and the KL divergence in (2.2). They can be simplified as follows: = (x, Y0 Proposition 1. Define Rx 0 function. The RFT solution (2.2) has: ) with π 0 ( x) and 0 as its cumulative distribution (i) Expected reward: (cid:69) xD, yπ ( x) [r (x, y)] = (cid:69) xD (cid:149) (cid:69)(cid:148) )/β (cid:151) 0 (Rx Rx (cid:69)(cid:148) (Rx 0 )/β (cid:151) (cid:152) , (ii) Win Rate: (cid:69) xD, yπ ( x) (cid:2)F 0 (r (x, y))(cid:3) = (cid:69) xD (cid:149) (cid:69)(cid:148) ) (Rx 0 )/β (cid:151) (Rx 0 0 (cid:69)(cid:148) (Rx 0 )/β (cid:151) (cid:152) , (iii) KL divergence: (cid:68) [π ( x)π 0 KL ( x)] = (cid:69) xD (cid:149) (cid:69)(cid:148) (Rx 0 (cid:69)(cid:148) )/β /β(cid:151) ) (Rx (Rx 0 )/β (cid:151) 0 log (cid:69) (cid:2)e (Rx 0 )/β (cid:3)(cid:152) To proceed, we assume the current policys ground-truth reward, Rx 0, is distributed from the standard uniform. This assumption is valid since: (i) it matches the reward distribution of best-of-n sampling and the optimal solution which best balance KL divergence and win rate [Gui et al., 2024, Azar et al., 2024, Balashankar et al., 2024] , and (ii) win rate and expected reward matches each other in this case. Under this assumption, we can characterize the utility-KL tradeoff when applying the misspecifed rewards: Theorem 2. Suppose each Rx U(0, 1) and (Rx 0 ) d= Rx 0. Then it holds that: (i) KL divergence is invariant to : (cid:68) [π ( x)π 0 ( x)] = KL (1/β 1)e1/β + 1 e1/β 1 log β log(e1/β 1). (ii) Expected reward (or win rate) of π is (cid:82) 1 0 1(u)eu/β du β(e1/β 1) . [Proof]. The explicit formula in Theorem 2 indicates that misspecification, i.e., the deviation of from the identity map, in the high-value region of has dominantly large effects on the utility-KL tradeoff. On one hand, the KL divergence remains invariant to the choice of and is fixed when the penalty parameter β is set. On the other hand, the exponential term imposes increasingly severe penalties on misspecification in the high-reward regime relative (a) Single-round Improvement (b) Iterative Improvement Figure 3: Rubric refinement through response differentiation. (a) Single-round: proposer LLM analyzes pair of responses to identify distinguishing features and encodes them as new rubric criteria. (b) Iterative: Multiple rounds progressively focus on higher-quality responses, with each iteration filtering to top-scoring candidates before generating new differentiating rubrics. to the low-reward regime. This highlights the criticality of accuracy in the high-reward region for achieving favorable balance between utility and KL divergence. To verify this, we investigate different and exactly compute the utility-KL tradeoff curves: (i) Correct: identity mapping (r ) = (ii) Reversed: the reverse mapping (r ) = 1 (iii) Top c% wrong: = (r ) = 1{r 1c} + (2 )1{r >1c}, i.e., the proxy reward model provides completely reverse rewards for highest quality responses (iv) Worst c% wrong: = (r ) = (c )1{r c} + 1{r >c}, i.e., the proxy reward model provides completely reverse rewards for worst quality responses Figure 2a plots KL divergence versus win rate across misspecification patterns and yields two key observations: (i) when the proxy is inaccurate in the high-reward region, performance may look acceptable at small KL but the win rate collapses as KL grows (this is similar to the reward over-optimization behavior in Gao et al. [2023]); and (ii) if the proxy correctly ranks just small top proportion of responses (e.g., 10%), even while misgrading the remaining majority, the win rate rapidly approaches the optimal curve at moderate KL. Separately, Figure 2b varies the fraction of correctly ranked top responses and traces the corresponding lower envelope of achievable win rates, showing that this envelope is already near-optimal once sufficiently large top proportion is correctly identified and ordered (e.g., 40%). Together, we reach our central theoretical findings: ( ) Reward over-optimization primarily arises from the inaccuracy in high-reward regions. ( II ) Being able to accurately rank and differentiate high-quality outputs is sufficient for reward model to effectively guide RL. Algorithm 1: Iterative Rubric Refinement through Progressive Differentiation 1: Input: Pool of candidate responses and initial rubrics 2: Iteration: For each refinement round: (a) Score all candidate responses with the current rubrics and get the top 2 responses from the candidate pool as the comparison pair. (b) Use the proposer LLM to identify distinguishing features between the pair and encode these features by refining the existing rubric set. 3: Output: Final refined rubric set 5 Table 1: RL experimental results on three datasets across two domains. Both principles lead to consistent improvements. Method Base Policy SFT Initial, Prompt only 1 Good Pair 1 Great Pair 4 Great Pairs 4 Great & Diverse Pairs Generalist Domain Health Domain Filtered Set LMArena Medical-o1 Win-Rate % Win-Rate % Win-Rate % HealthBench 5.2 35.9 31.3 33.5 36.8 38.7 39.7 4.1 29. 29.7 32.8 33.1 34.7 35.1 10.8 25.8 21.7 22.4 26.5 31.4 34.4 0.1721 0. 0.3004 0.2912 0.3163 0.3348 0."
        },
        {
            "title": "4 Principles for Constructing Rubrics",
            "content": "Based on the results of the previous section, we construct reward model focusing on the high-value region. The problem then is getting training examples that are in this high-reward region. By definition, these are samples that are rare under the base LLM policy! This essentially forces us to use off-policy data to define the reward model. Now, rubric-based rewards have emerged as an approach for using off-policy data to define rewards. The basic idea of rubric-based reward models is to explicitly restrict the reward to only care about aspects of the solution that are relevant to its quality, thereby mitigating the effect of the off-policy data. However, the restrictive nature of the rubrics is double-edged sword. The same structure that limits the effect of off-policyness may also limit their ability to distinguish between solutions that are excellent and those that are merely great (they can easily end up in tie). In this section, we consider how to construct rubrics that are focused on accuracy in the high-reward region. Refining rubrics to reliably tell apart two already great responses is natural first step toward capturing the high-reward tail. To push accuracy further in that tail, we also update rubrics to distinguish among diverse set of great responses. We formalize these ideas as two principles for rubric construction Principles for Rubric Construction [Principle 1] Effective rubric construction requires distinguishing excellent [Principle 2] Effective rubric construction requires distinguishing among diresponses from great ones. verse off-policy responses."
        },
        {
            "title": "4.1 Methodology",
            "content": "To operationalize the above principles, we design an iterative workflow that leverages off-policy responses to refine rubrics. Refinement-through-Differentiation (RTD). natural way to make rubric-rewards more discriminative is to prompt proposer LLM with pair of candidate responses and the current rubrics. The proposer analyzes the pair, identifies their distinguishing features, and encodes these distinctions as new rubric criteria or refinements of existing ones. We refer to this fundamental refinement step as Refinement-through-Differentiation (RTD). 6 Iterative workflow for chasing the tail. While single RTD step sharpens the rubric, repeated application over larger candidate pool yields systematic improvements. Starting with all off-policy responses for prompt, each iteration scores the candidates under the current rubric, selects the top two responses, and refines the rubric using RTD. This workflow concentrates rubric discovery on the performance frontier, extracting the most informative distinctions from the best available responses with only small number of comparisons (see Algorithm 1 and Figure 3)."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We conduct studies in two distinct domains: general-purpose and professional (health). More concretely, we set up the experiments as follows: Training setup. We employ GPT-4.1 as the rubric proposer, and prompt it to get the initial rubrics. The training datasets consist of two generalist prompt collection (LMArena [Chiang et al., 2024] and manually filtered set of natural prompts, detailed in Section G) and one technical, domain-specific prompt set (medical-o1-reasoning-SFT [Chen et al., 2024]). Each dataset contains 5000 prompts for training and an additional 1000 prompts for in-domain evaluation. The base model for post-training is Qwen3-8b-Base [Yang et al., 2025], which has instruction-following capabilities. We adopt GRPO [Shao et al., 2024] as the RFT algorithm and use standard set of hyperparameters, detailed in Table 4. For the reward computation, we leverage GPT-4.1-mini as rubric verifier and calculate the final reward as the weighted sum of satisfied rubric criteria, normalized by the total weight. All prompts used in the experiments are presented in Section B. Candidate pool. To validate Principle 1, we compare rubrics refined using (i) candidate pairs from great model versus (ii) candidate pairs from good model (Gemini 2.5 Pro and Gemini-2.5-Flash-Lite, respectively [Comanici et al., 2025]). To validate Principle 2, we enlarge the pool by sampling 16 responses per prompt, from broader set of excellent models, ensuring greater diversity (see Section for the full list). This setup allows us to test whether rubric refinement benefits from better and more diverse candidate responses. Evaluations. Final performance is evaluated by head-to-head comparison against Qwen38B, strong hybrid thinking model, using the held-out set of 1000 prompts from each dataset. We compute the win-rate based on judgments from our GPT-4.1 judge , which was prompted to act as an impartial evaluator (see Appendix for the detail and justifications). For health domain, we additionally evaluate models on the HealthBench [Arora et al., 2025]."
        },
        {
            "title": "5.1 RL improves with better and more diverse responses",
            "content": "We first evaluate downstream RL performance to test whether the proposed principles indeed improve rubrics. Table 1 shows two clear trends. First, rubrics refined with great pairs outperform those refined with good pairs, validating Principle 1. Second, iterative refinement with multiple diverse great pairs yields further gains, validating Principle 2. Beyond improving average performance, refinement with better and more diverse responses also mitigates reward over-optimization. Figure 4 shows training dynamics on the health domain when RL is run for extended steps. Models trained on initial rubrics, or rubrics refined with only single pair, peak early and then suffer rapid decline in win rate after 7 Figure 4: Refinement with great and diverse responses mitigates reward over-optimization. Training rewards rise similarly across settings, but only models trained with iteratively refined, diverse rubrics sustain higher win-rates (a proxy for ground-truth reward ) and delay the collapse that signals reward over-optimization. Table 2: Accuracy of rubric-based scoring in predicting ground-truth model preferences was evaluated on 1000 random prompts from the training set. Response pairs in the high-reward region were sampled from Qwen3-8B, and response pairs in the low-reward region were sampled from Qwen3-8B-Base. Rubric preferences were determined by majority vote from five independent gradings, with ties counted as incorrect.Results how refining with stronger and more diverse responses improves highreward accuracy. Initial 1 Good Pair 1 Great Pair 4 Great Pairs 4 Great & Diverse Pairs High-reward Low-reward 40.3% 66.2% 42.2% 67.9% 45.8% 66.7% 49.2% 68.9% 47.9% 69.8% about 60 stepsan indicator of reward over-optimization. In contrast, models trained with iteratively refined, diverse rubrics sustain higher win-rates for much longer, with overoptimization not appearing until after roughly 160 steps. This pattern indicates that refining rubrics with great and diverse responses corrects inaccuracies in the high-reward region, thereby delaying the onset of over-optimization. Together, these results confirm our central hypothesis that rubrics can be constructed to mitigate reward over-optimization."
        },
        {
            "title": "5.2 Reward model accuracy improves in the high-reward tail",
            "content": "Our theoretical analysis (Section 3) suggests that accuracy in the high-reward tail is the critical factor for downstream RL performance. To understand why refinement with better and more diverse responses helps, we evaluate the agreement between rubric-based rewards and the ground-truth judge, separately on the highand low-reward regions. As shown in Table 2, incorporating any candidate responses through refinement improves rubric accuracy compared to the prompt-only baseline. More importantly, rubrics refined with great pairs largely improve accuracy in the high-reward region, while good pairs improve accuracy more than great pairs in the low-reward region. Iterative refinement with great pairs pushes the accuracy in the high-reward region even further, mirroring the RL improvements in Table 1. This confirms that both principles work by sharpening reward model accuracy where it matters most: the high-value tail."
        },
        {
            "title": "5.3 Refinements from better responses are more sophisticated",
            "content": "Finally, we analyze how refinements differ when using good versus great candidate responses. To understand how stronger candidate responses lead to better rubrics, we analyzed the 8 Table 3: Distribution of rubric refinement types when using great (Gemini 2.5 Pro) versus good (Gemini 2.5 Flash Lite) candidate pairs, in the healthcare domain. Rows with significant differences (55% for one model) are highlighted: blue indicates great dominance, percentages show the dominant model. red indicates good dominance. Bold Refinement Type Proportion Great vs Good Mandating explicit statements, justifications, or declarations Shifting focus from superficial to substantive qualities Adjusting scoring weights, granularity, or mechanisms Breaking down complex criteria into sub-components Introducing penalties, prohibitions, or negative scoring Replacing vague language with specific requirements Adding requirements for comparing alternatives Broadening criteria to accept multiple approaches Adding conditional or context-dependent rules Streamlining by removing redundancy Adding timing, sequencing, or process flow criteria Mandating precise language or technical accuracy Requiring causal explanations or mechanistic understanding Enhancing verification, validation, and evidence standards Mandating specific structure or formatting Requiring explicit justification for decisions Defining explicit scope, boundaries, or constraints Incorporating risk analysis or safety constraints Requiring specific, actionable recommendations Correcting errors or aligning with intended standards Assessing communication quality or tone 16.7% 11.7% 8.9% 7.2% 6.6% 6.2% 5.9% 5.6% 4.5% 4.4% 3.5% 3.3% 3.3% 2.3% 2.1% 1.9% 1.8% 1.8% 1.0% 0.9% 0.5% 52.6% vs 47.4% 48.2% vs 51.8% 48.5% vs 51.5% 55.9% vs 44.1% 43.5% vs 56.5% 54.7% vs 45.3% 49.0% vs 51.0% 44.4% vs 55.6% 51.4% vs 48.6% 41.6% vs 58.4% 54.1% vs 45.9% 50.2% vs 49.8% 51.8% vs 48.2% 55.0% vs 45.0% 53.8% vs 46.2% 50.3% vs 49.7% 58.9% vs 41.1% 55.2% vs 44.8% 55.5% vs 44.5% 32.8% vs 67.2% 43.8% vs 56.2% types of refinements made when using different quality levels of candidate pairs. We prompted an LLM to compare initial and refined rubrics, and categorized the improvements into semantic clusters (see details in Section H). Table 3 shows the distribution of refinement types on the health domain. Both qualities contribute, but the patterns diverge: good responses often drive basic corrections, such as adding penalties for obvious mistakes or broadening overly restrictive criteria; by contrast, great responses more often drive sophisticated refinements, such as breaking down complex criteria into sub-components or enhancing verification standards. In the example from Section I, for medical prompt about patient with serious symptoms, two initially tied great responses are distinguished by adding the criterion: The response mentions that urgent imaging (e.g., contrast-enhanced CT or MRI/MRV) is required to confirm the diagnosis. This refinement, from the Enhancing verification, validation, and evidence standards cluster, mandates critical, verifiable clinical action, and only one of the responses satisfies. Such qualitative results confirm our finding that comparing great responses provides the nuanced distinctions needed to identify excellent outputs, thereby sharpening accuracy in the high-reward tail."
        },
        {
            "title": "6 Related work",
            "content": "Reward over-optimization. Gao et al. [2023] highlighted the issue of reward overoptimization for both best-of-n sampling and reinforcement learning when using preferencebased reward models. Although this phenomenon has since been repeatedly observed in empirical studies [Bai et al., 2022, Moskovitz et al., 2023, Perez et al., 2023, Gui et al., 2024, Wang et al., 2024], its theoretical underpinnings remain limited. Existing analyses typically relate the performance degradation caused by proxy reward to global statistics describing how far the proxy deviates from the true reward [Huang et al., 2025a, Mroueh, 2024]. In contrast, our work provides sharper perspective: what truly governs performance is the fidelity of the proxy reward in the high-value region, where high-quality responses concentrate. 9 Rubrics reward. RL from rubrics reward (RLRR) has proven to be an effective method in specialized domains like science and health [Gunjal et al., 2025], general instructionfollowing [Huang et al., 2025b, Viswanathan et al., 2025], and for enhancing agentic ability [Team et al., 2025], with implementations using both online and offline RL. The idea of rubrics is also utilized in generative reward models (GRMs), wherein reward model is prompted to first generate rubrics and then use them to evaluate response [Liu et al., 2025b, Chen et al., 2025]. This approach enables inference time scaling of reward modeling and improves explainability. However, generating rubrics on the fly is computationally inefficient and unsuitable for large-scale training."
        },
        {
            "title": "7 Discussion",
            "content": "In this paper, we investigate rubric-based reward modeling for LLM post-training. We begin by analyzing the central weakness of reinforcement fine-tuning, reward over-optimization, and theoretically trace it to misspecification of the proxy reward in the high-reward tail. comprehensive empirical study highlights rubric-based rewards as an effective remedy. We further demonstrate that carefully designed rubrics, which distinguish among great, diverse off-policy responses, lead to consistently strong fine-tuning performance. Off-policy responses for Bradley-Terry reward model training might generalize, but is sample inefficient. While we find medium amount off-policy responses (n = 5000, in addition to the same number of on-policy responses) do not help Bradley-Terry reward model guide the current policy (see Section D), we note that other work successfully train BT reward model with off-policy samples, but with much larger scaleusing up to 20 million high quality samples ([Liu et al., 2025a, Cui et al., 2023]). Indeed, Bradley-Terry reward models generalizability scales with the number, and diversity of training samples. However, its not always easy to find large-scale data for many specialized domains, such as healthcare. In contrast, rubric-based reward can easily encode generalizable principles from limited amount of data. Weighted average of rubric score is not optimal. To specifically analyze the impact of rubric quality, we deliberately use the most simple method of score aggregation, by taking weighted average of scores from the satisfied criteria. Prior work has explored diverse approaches, including implicit aggregation by verifier model [Gunjal et al., 2025], sophisticated frameworks to capture non-linear dependencies [Huang et al., 2025b], weighted averages of continuous scores [Viswanathan et al., 2025], and model-based self-critique that weighs criteria against internal priors [Team et al., 2025]. We acknowledge that aggregation is central component of an optimal rubric reward system and leave it for future work."
        },
        {
            "title": "References",
            "content": "Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin QuiñoneroCandela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, et al. Infalign: Inferenceaware language model alignment. arXiv preprint arXiv:2412.19792, 2024. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms, 2024. URL https://arxiv.org/abs/2412.18925. Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. arXiv preprint arXiv:2310.01377, 2023. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. Lin Gui, Cristina Gârbacea, and Victor Veitch. Bonbon alignment for large language models and the sweetness of best-of-n sampling. arXiv preprint arXiv:2406.00832, 2024. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Audrey Huang, Adam Block, Qinghua Liu, Nan Jiang, Dylan Foster, and Akshay Krishnamurthy. Is best-of-n the best of them? coverage, scaling, and optimality in inference-time alignment. arXiv preprint arXiv:2503.21878, 2025a. Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025b. 11 Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, et al. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025a. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025b. Ted Moskovitz, Aaditya Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca Dragan, and Stephen McAleer. Confronting reward model overoptimization with constrained rlhf. arXiv preprint arXiv:2310.04373, 2023. Youssef Mroueh. Information theoretic guarantees for policy alignment in large language models. arXiv preprint arXiv:2406.05883, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. In Findings of the association for computational linguistics: ACL 2023, pages 1338713434, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex DAmour, Sanmi Koyejo, and Victor Veitch. Transforming and combining rewards for aligning large language models, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025."
        },
        {
            "title": "Usage of Large Language Models",
            "content": "In this work, besides running LLMs in experiments, we use LLMs for the following purposes: 1. Aid or Polish Writing (Gemini 2.5 Pro, ChatGPT 4/5) 2. Literature Retrieval and Discovery (e.g., finding related work) (Gemini 2.5 Pro Deep Research, ChatGPT Deep Research) 3. Assisting Code Writing and Debugging (Claude-Ops-4.1, GPT-5) We fully understand the responsibility of using LLMs in academic research. We carefully monitor any potential problems, such as plagiarism or scientific misconduct (e.g., fabrication of facts) when using LLMs. We make sure these problems do not occur in the paper."
        },
        {
            "title": "A Theoretical Results",
            "content": "Theorem 2. Suppose each Rx 0 U(0, 1) and (Rx 0 ) d= Rx 0. Then it holds that: (i) KL divergence is invariant to : (cid:68) [π ( x)π 0 ( x)] = KL (1/β 1)e1/β + 1 e1/β log β log(e1/β 1). (ii) Expected reward (or win rate) of π is (cid:82) 1 0 1(u)eu/β du β(e1/β 1) . [Proof]. Proof. First, we compute the KL divergence. When (Rx 0 KL divergence is ) U(0, 1), by Proposition 1, the (cid:68) [π ( x)π 0 KL ( x)] = (cid:69) xD (cid:150) (cid:69) (cid:2) (Rx 0 ) (Rx )/β /β(cid:3) log (cid:69) (cid:2)e (Rx 0 )/β (cid:3) (cid:153) = (cid:69) xD (cid:82) 1 0 ueu/β du β (cid:82) 1 0 eu/β du log (cid:130)(cid:90) 0 eu/β du )/β (cid:3) 0 (cid:69) (cid:2)e (Rx = (cid:140) (1/β 1)e1/β + 1 e1/β 1 log (cid:2)β(e1/β 1)(cid:3) . Then, we compute the expected reward: denote 0 = (Rx 0 ), (cid:69) xD, yπ ( x) [r (x, y)] = (cid:69) xD = (cid:69) xD (cid:150) (cid:69) (cid:2) 1(T (cid:69) (cid:2)eT 0 0 /β (cid:3) (cid:153) 0 ) eT /β (cid:3) = (cid:69) xD )/β (cid:3) (cid:153) (cid:150) (cid:69) (cid:2)Rx 0 0 (Rx (cid:69) (cid:2)e (Rx 0 )/β (cid:3) (cid:82) 1 0 1(u)eu/β du (cid:82) 1 0 eu/β du = (cid:82) 1 0 1(u)eu/β du (cid:82) 1 0 eu/β du = (cid:82) 1 0 1(u)eu/β du β(eu/β 1) Since 0 follows. (Rx 0 )=Rx 0 when Rx U(0, 1), the win rate is the expected reward. Then the theorem"
        },
        {
            "title": "B Prompts Used for Experiments",
            "content": "Prompt for Constructing Initial Rubrics Youre skilled judge evaluating the quality of LLM responses to user prompt. Your first task is to create comprehensive rubric for grading these responses across multiple dimensions. Given user prompt, generate list of binary (yes/no) criteria. These criteria should assess how well the LLM answered the prompt. Only write rubrics you are confident about. Here are tips for writing good rubrics: i. MECE: - Mutually Exclusive, Collectively Exhaustive ii. Completeness: - Consider all the elements you would want to include to create perfect response and put them into the rubric. This means including not only the facts and statements directly requested by the prompt, but also the supporting details that provide justification, reasoning, and logic for your response. Each of these elements should have criterion because each criterion helps to develop the answer to the question from slightly different angle. iii. No overlapping: - the same error from model shouldnt be punished multiple times. iv. Diversity: - The rubric items should include variable types of information. - If all criteria are like the response mentions A, the response mentions B, then this is not good rubric. v: How many rubric items for each prompt - There is no golden standard, and the desired number of rubrics varies by accounts and task types. - Write rubrics that cover all aspects of an ideal response. vi: How many rubric items to fail - good rule of thumb is that the model fails on 50% of rubrics items vii: Atomicity / Non-stacked - Each rubric criterion should evaluate exactly one distinct aspect. Avoid bundling multiple criteria into single rubric. Most stacked criteria with the word and can be broken up into multiple pieces. Ø Response identifies George Washington as the first U.S. president and mentions he served two terms. Response identifies George Washington as the first U.S. president. Response mentions that George Washington served two terms. viii: Specificity 14 - Criteria should be binary (true or false) and objective. - Avoid vague descriptions (e.g., \"the response must be accurate\" is vague). - Example: \"The response should list exactly three examples.\" ix: Self-contained - Each criterion should contain all the information needed to evaluate response, e.g. Ø Mentions the capital city of Canada. Mentions the capital city of Canada is Ottawa. x: Criterion should be verifiable without requiring external search. Ø Response names any of the Nobel Prize winners in Physics in 2023 Response names any of the following Nobel Prize winners in Physics in 2023: Pierre Agostini, Ferenc Krausz, or Anne LHuillier. xi. The binary criteria should be phrased so that yes means the model response is good and no means the model response is bad. Finally, we want to assign different weight for each question. Give weight on scale of 1 (least important) to 3 (most important) for each question based on 1. the questions alignment with user demand (3 if user would be frustrated if the answer is no; 1 if user would not be bothered at all if the answer is no) 2. the questions importance in terms of determining quality/correctness (3 if the response would be completely incorrect if the answer is no; 1 if an extreme edge case would be missed and the overall quality wont be affected if the answer is no) Here is the user prompt for which we want to generate rubric: PROMPT: {prompt} Return ONLY the JSON array of the rubrics, no other text. For example: [ 1 3 4 ] { { \" t o \" : \" Does r o p i l o s s ? \" , \" g \" : 3 } } , { { \" t o \" : \" The p e l t t i \" : 2}} s g French a c g . \" , \" g Note: Local IDs will be automatically assigned to each criterion (c1, c2, c3, etc.), so dont include IDs into outputed criterion. Prompt for Improving Rubrics Youre skilled judge assessing the quality of LLM responses to user prompt. The current rubric isnt good enough to effectively differentiate between high-quality responses. Your goal is to improve the current rurbics to address this (adding new creteria, rewriting, decomposing, and deleting the current creteria). The updated rubric must be comprehensive and consistently applicable for grading LLM responses. These criteria should specifically assess how well the LLM answered the given prompt. Only write rubrics you are confident about. Here are tips for writing good rubrics: i. MECE: - Mutually Exclusive, Collectively Exhaustive ii. Completeness: - Consider all the elements you would want to include to create perfect response and put them into the rubric. This means including not only the facts and statements directly requested by the prompt, but also the supporting details that provide justification, reasoning, and logic for your response. Each of these elements should have criterion because each criterion helps to develop the answer to the question from slightly different angle. iii. No overlapping: - the same error from model shouldnt be punished multiple times. iv. Diversity: - The rubric items should include variable types of information. - If all criteria are like the response mentions A, the response mentions B, then this is not good rubric. v: How many rubric items for each prompt - There is no golden standard, and the desired number of rubrics varies by accounts and task types. - Write rubrics that cover all aspects of an ideal response. vi: How many rubric items to fail - good rule of thumb is that the model fails on 50% of rubrics items vii: Atomicity / Non-stacked - Each rubric criterion should evaluate exactly one distinct aspect. Avoid bundling multiple criteria into single rubric. Most stacked criteria with the word and can be broken up into multiple pieces. Ø Response identifies George Washington as the first U.S. president and mentions he served two terms. Response identifies George Washington as the first U.S. president. Response mentions that George Washington served two terms. viii: Specificity - Criteria should be binary (true or false) and objective. - Avoid vague descriptions (e.g., \"the response must be accurate\" is vague). - Example: \"The response should list exactly three examples.\" 16 ix: Self-contained - Each criterion should contain all the information needed to evaluate response, e.g. Ø Mentions the capital city of Canada. Mentions the capital city of Canada is Ottawa. x: Criterion should be verifiable without requiring external search. Ø Response names any of the Nobel Prize winners in Physics in 2023 Response names any of the following Nobel Prize winners in Physics in 2023: Pierre Agostini, Ferenc Krausz, or Anne LHuillier. xi. The binary criteria should be phrased so that yes means the model response is good and no means the model response is bad. Finally, we want to assign different weight for each criterion. Give weight on scale of 1 (least important) to 3 (most important) for each question based on 1. the questions alignment with user demand (3 if user would be frustrated if the answer is no; 1 if user would not be bothered at all if the answer is no) 2. the questions importance in terms of determining quality/correctness (3 if the response would be completely incorrect if the answer is no; 1 if an extreme edge case would be missed and the overall quality wont be affected if the answer is no) Here is the user prompt for which we want to improve the rubric: PROMPT: {prompt} The existing rubrics we are using is: {rubrics} The two reference responses are: Reponse 1: {response1} Reponse 2: {response2} Return ONLY the JSON array of the full rubrics, no other text. For example: 1 2 3 4 [ ] { { \" t o \" : \" Does r o p i e s a o each song ? \" , \" c c g \" : 2 } } , { { \" t o \" : \" The p e l s i names each song mentioned \" , \" g \" : 1}} 17 Note: Local IDs will be automatically assigned to each criterion, so dont include IDs in your output. Prompt for Scoring Responses You are skilled judge who will be assessing the quality of LLM responses to user prompt. Given user prompt, LLM response, and rubric, your task is evalauting the performance of the model response by seeing whether or not it meets the rubric dimension. Answer the each of the given rubric dimension in either \"yes\" or \"no\". Do not output any response other than \"yes\" or \"no\". Keep in mind that you will be grading industry-leading LLMs. Make sure to have high expectation for grading the responses. Make sure your evaluation is as objective and consistent as it could be. By consistent we mean that different evaluators assessment of the task should agree with yours. Think carefully before you make the decision. After you make the decision, explicitly output which dimension receives \"yes\" and which dimension receives \"no\". Input: * PROMPT: {prompt} * RESPONSE: {response} * RUBRIC: {rubric} Return ONLY the JSON array, no other text. For example: 1 { { \" c1 \" : \" \" , \" c2 \" : \" no \" , \" c3 \" : \" \" } }"
        },
        {
            "title": "C Hyperparameter",
            "content": "The hyperparameter for GRPO training of RLRR is presented in Table 4. Table 4: GRPO Hyperparameter Configuration Hyperparameter Value Rollouts per Prompt Gradient Accumulation Steps Per-Device Train Batch Size Warmup Ratio KL Coefficient Learning Rate Learning Rate Scheduler Maximum Sequence Length Training Epochs 16 2 6 0.1 0.01 1.0 105 Constant with Warmup"
        },
        {
            "title": "D Empirical Results on RLHF",
            "content": "We finetune Bradley-Terry Reward model on various responses, with preference generated by GPT-4.1, the same model as the judge model for evaluation. For each of the prompt in the training set, we generated pair of responses at temperature 1.0 using the base policy model Qwen3-8B-Base (on-policy) or Gemini-2.5-Pro (off-policy). Preferences were labeled using GPT-4.1, the same model used for final evaluation. This preference data was then used to train reward model based on Llama-3.1-8B-Instruct, with hyperparameters specified in Table 6. Finally, this reward model was used for GRPO training, following the configuration in Table 4. We find that using on-policy responses is baseline that cant be easily improved upon: 1. Training on off-policy, great responses deteriorates the performance 2. Adding both off-policy and on-policy responses only helps with win rates but not helps with healthbench. This suggests that the off-policy samples only help the reward model encode superficial features (that can game LLM-judge) instead of true capabilities as measured by more objective metrics. This experiment shows the difficulty of improving Bradley-Terry models with off-policy responses. Table 5: Win-rates and HealthBench scores for the Health domain. Method Win-Rate HealthBench Reward Model (on-policy) Reward Model (off-policy, great) Reward Model (on-policy + off-policy) SFT Initial, Prompt only 1 good Pair 1 great Pair 4 great Pairs 4 great & Diverse Pairs 26.8% 22.4% 30.7% 25.8% 21.7% 22.4% 26.5% 31.4% 34.4% 0.3036 0.2798 0.3032 0.3094 0.3004 0.2912 0.3163 0.3348 0.3513 Table 6: Reward Model Hyperparameter Configuration Hyperparameter Learning Rate Per-Device Train Batch Size Gradient Accumulation Steps Training Epochs Maximum Sequence Length Warmup Ratio Learning Rate Scheduler Value 1.0 105 4 4 10 8192 0.1 Cosine"
        },
        {
            "title": "E LLM Judge for evaluation",
            "content": "We use the same judge model as the rubrics proposer (GPT-4.1). This is by design: our primary goal is to test how best to incorporate additional responses into the rubric construction process. By using the same powerful model for both proposing rubrics and evaluating final outputs, we isolate the quality of the candidate responses as the key experimental variable and eliminate potential confounding issues that could arise from disagreements between proposer and judge. We use minimal judge prompt to compare two responses: LLM Judge Prompt You are skilled judge who will be assessing the quality of LLM responses to user prompt. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Here is the user prompt: PROMPT: {prompt} The two responses are: Response 1: {response1} Response 2: {response2} Which reponse would you prefer? boxed{{...}}. Enclose your final answer (1 or 2) in To reduce the position bias, we randomly flipped two responses."
        },
        {
            "title": "F Frontier Models Used to Create Candidate Responses",
            "content": "The 16 frontier models used to generate candidate responses are: Gemini-2.5-Pro Gemini-2.5-Flash 20 GPT-5 GPT-4. GPT-4o-2024-05-13 o3 o1-2024-12-17 o4-mini Claude-Sonnet-4-20250514 Claude-3-7-SonnetDeepseek-V3 Deepseek-R1 Kimi-K2-Instruct GLM-4.5 Qwen3-235B-A22B-Instruct-2507 Mistral-Medium-Latest"
        },
        {
            "title": "G Principles of Selecting Prompts",
            "content": "We manually curated prompt dataset according to specific set of criteria to ensure quality and suitability for rubrics training. Prompts have clear user intent. Prompts are not multimodal/search/trivia/GTFA. Prompts are not too simple. Prompts are generalist friendly (nothing technical that requires expert knowledge). Prompts are not open-ended / creative-writing. Prompts are designed such that there is an objectively better response (Tell me good bedtime story can have multiple good responses)"
        },
        {
            "title": "H Pattern detection on rubric refinements",
            "content": "In Table 3 we show clusters of various rubric-refinements. To get this result, we first prompt GPT-4.1 to describe the differences between refined rubrics, and the initial rubrics (for refinement with one-good-pair, and one-great-pair). The differences are listed as atomic items describing different aspects of improvement. We then run clustering algorithm on the atomic differences from both one-good-pair, and one-great-pair (they have almost the same number of atomic differences) to get interpretable clusters, and report the proportions from one-good-pair versus and one-great-pair"
        },
        {
            "title": "I Examples of Rubrics and Rubric Refinements",
            "content": "In this example, the initial rubric cannot differentiate between the two great responses (both from Gemini-2.5-Pro). This is because it primarily handles basic criteria (e.g. the diagnosis is correct), which both responses satisfy. The refinement process resolves the tie by adding 21 new criterion c7requiring the mention of an imaging test to confirm the diagnosis. Such refinement is one example for the cluster Enhancing verification, validation, and evidence standards identified in Table 3. Rubric Refinement Example Prompt What is the likely diagnosis for young girl with history of repeated pain over the medial canthus and chronic use of decongestants, who now presents with intense chills, rigors, diplopia on lateral gaze, and congested optic disc on examination? Response 1 Most likely diagnosis: Cavernous sinus thrombosis (CST). This is life-threatening medical emergency. Why CST fits Medial canthus pain + decongestants sinusitis (ethmoid/sphenoid)/dacryocystitis; valveless ophthalmic veins communicate with cavernous sinus. Intense chills/rigors septicemia from septic thrombophlebitis. Diplopia on lateral gaze abducens nerve (CN VI) involvement. Congested optic disc impaired venous outflow / increased ICP. Differential: orbital cellulitis; meningitis/brain abscess. Immediate management: urgent contrast CT or MRI/MRV; high-dose IV antibiotics; consider anticoagulation; drain source if indicated. Rubric Scores (Initial) Criterion (Initial) c1: Identifies CST as most likely diagnosis c2: States it is medical emergency c3: Links medial canthus pain + decongestants to sinusitis c4: Diplopia due to CN VI involvement c5: Papilledema from impaired venous drainage/ICP c6: Chills/rigors = systemic infection/bacteremia c7: Includes medical disclaimer / seek care c8: Mentions orbital cellulitis differential c9: Mentions high-dose IV antibiotics Response 2 Most likely diagnosis: Cavernous sinus thrombosis (CST), secondary to dacryocystitis/orbital cellulitis. This is life-threatening emergency. Clinical reasoning Medial canthus pain localizes to lacrimal sac; chronic congestion/decongestants imply nasolacrimal dysfunction and recurrent dacryocystitis. Valveless facial/ophthalmic veins permit retrograde spread to cavernous sinus. CST manifestations Sepsis (chills, rigors). CN VI palsy causing diplopia on lateral gaze. Papilledema from impaired venous drainage / increased ICP. Urgency & treatment medical/neurosurgical emergency; high-dose IV antibiotics essential. Weight 3 3 3 3 2 2 2 1 R1 (0/1) 1 1 1 1 1 1 0 1 1 R2 (0/1) 1 1 1 1 1 1 0 1 1 Weighted total (Initial): R1 = 18/20, R2 = 18/20 Rubric Scores (Refined) Criterion (Refined) c1: Identifies CST as most likely diagnosis c2: Explicitly states CST is medical emergency c3: Links medial canthus pain + decongestants to sinusitis/dacryocystitis c4: Diplopia due to abducens (CN VI) involvement c5: Papilledema from impaired venous drainage/ICP c6: Sepsis secondary to CST (chills/rigors) c7: Urgent imaging (contrast CT or MRI/MRV) required to confirm diagnosis c8: High-dose IV antibiotics are initial mainstay c9: Medical disclaimer / seek immediate care c10: Mentions orbital cellulitis differential c11: Notes other CNs (III, IV, V1, V2) may be affected c12: Avoids incorrect primary diagnosis Weighted total (Refined): R1 = 25/27, R2 = 22/27 Weight 3 3 3 3 2 2 2 2 2 1 1 3 R1 (0/1) 1 1 1 1 1 1 1 R2 (0/1) 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0"
        }
    ],
    "affiliations": [
        "Scale AI, Inc.",
        "University of California, Los Angeles",
        "University of Chicago"
    ]
}