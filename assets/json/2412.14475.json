{
    "paper_title": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval",
    "authors": [
        "Junjie Zhou",
        "Zheng Liu",
        "Ze Liu",
        "Shitao Xiao",
        "Yueze Wang",
        "Bo Zhao",
        "Chen Jason Zhang",
        "Defu Lian",
        "Yongping Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by a lack of training data. In this paper, we introduce MegaPairs, a novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with a massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70$\\times$ more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, well-trained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field."
        },
        {
            "title": "Start",
            "content": "MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval Junjie Zhou1, Zheng Liu2,5*, Ze Liu3, Shitao Xiao2, Yueze Wang2, Bo Zhao2,4 Chen Jason Zhang5, Defu Lian3, Yongping Xiong1 1 Beijing University of Posts and Telecommunications, 2 Beijing Academy of Artificial Intelligence, 3 University of Science and Technology of China, 4 Shanghai Jiaotong University 5 The Hong Kong Polytechnic University zhoujunjie@bupt.edu.cn zhengliu1026@gmail.com 4 2 0 2 9 1 ] . [ 1 5 7 4 4 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the rapidly growing demand for multimodal retrieval, progress in this field remains severely constrained by lack of training data. In this paper, we introduce MegaPairs, novel data synthesis method that leverages vision language models (VLMs) and open-domain images, together with massive synthetic dataset generated from this method. Our empirical analysis shows that MegaPairs generates high-quality data, enabling the multimodal retriever to significantly outperform the baseline model trained on 70 more data from existing datasets. Moreover, since MegaPairs solely relies on general image corpora and open-source VLMs, it can be easily scaled up, enabling continuous improvements in retrieval performance. In this stage, we produced more than 26 million training instances and trained several models of varying sizes using this data. These new models achieve state-of-the-art zero-shot performance across 4 popular composed image retrieval (CIR) benchmarks and the highest overall performance on the 36 datasets provided by MMEB. They also demonstrate notable performance improvements with additional downstream fine-tuning. Our produced dataset, welltrained models, and data synthesis pipeline will be made publicly available to facilitate the future development of this field."
        },
        {
            "title": "Introduction",
            "content": "Multimodal retrieval is critical research problem for IR and AI communities. It aims to satisfy peoples information needs across different data modalities, especially texts and images. Nowadays, multimodal retrieval has been applied to wide variety of real-world scenarios, such as image search (Chen et al., 2015; Wu et al., 2021; Zhang et al., 2024), visual question answering (VQA) (Marino et al., 2019; Mathew et al., 2021), *Corresponding author. and retrieval-augmented generation (RAG) of vision language models (Chen et al., 2022; Yu et al., 2024). Given the widespread application scenarios, its necessary to develop universal multimodal retrievers which can uniformly support any task requirements and working domains. The progress of universal multimodal retrievers have been substantially advanced on top of the pretrained vision-languages models, like CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), and SigLIP (Zhai et al., 2023). These models are pretrained to produce discriminative and unified representations for texts and images, thus creating solid foundation for multimodal retrieval. However, the existing vision-language encoders are mostly pretrained from text-image matching tasks. Although these models have achieved an initial capability for text-to-image retrieval (Young et al., 2014; Chen et al., 2015), they are insufficient for other common multimodal tasks, such as composed image retrieval (Liu et al., 2021; Baldrati et al., 2023; Zhang et al., 2024) and multimodal document retrieval (Chang et al., 2022; Liu et al., 2022). To enhance the multi-task capacity, fine-tuning pre-trained models with comprehensive instructions, commonly known as instruction-tuning, has gained significant popularity. This approach was first applied in the supervised fine-tuning of large language models (LLMs) (Ouyang et al., 2022; Wei et al., 2021; Chung et al., 2024), and later introduced for training text embeddings (Su et al., 2022; Asai et al., 2022; Zhang et al., 2023; Xiao et al., 2024). Building on these successes, instructiontuning has been further extended to multimodal embedding models (Wei et al., 2024; Sharifymoghaddam et al., 2024), where pre-trained visionlanguage encoders are continually fine-tuned using variety of multimodal retrieval instructions. Given the scarcity of instruction-tuning data for embedding models, researchers have proposed leveraging LLMs to generate synthetic data from Internet resources (Wang et al., 2023). In the field of multimodal retrieval, notable example is presented by MagicLens (Zhang et al., 2024), which synthesizes open-ended search instructions for coexisting images within the same webpage. Despite recent advancements by MagicLens, current data synthesis methods still face significant limitations in data scalability, quality, diversity, and availability. Specifically, only small fraction of webpages on the internet contain multiple images (scalability), not to mention that many of these co-existing images are either unrelated or nearduplicates (quality). Besides, the remaining correlated images often exhibit monotonous relationships, such as different angles of the same object (diversity). Finally, large-scale instruction-tuning datasets for multimodal retrieval are typically held privately by individual research labs (availability). In this paper, we introduce novel data synthesis method called MegaPairs, accompanied by large-scale instruction dataset generated using this approach. MegaPairs is distinguished by its construction of heterogeneous KNN triplet for open-domain images. Particularly, it leverages three different similarity models to sample correlated image pairs, including CLIP vision-encoder for visual-semantic correlations (Sun et al., 2023), DINO vision-encoder for visual-pattern correlations (Oquab et al., 2024), and CLIP text-encoder for caption correlations. The sampled image pairs are presented for the VLM and LLM annotators, which generate comprehensive descriptions of the relationships between the two images and create pseudo-retrieval instructions based on the descriptions. This approach enables huge amount of instructions to be generated for general dataset, like Datacomp (Gadre et al., 2024), which significantly improves the scalability of data synthesis. It also introduces diverse instructions of guaranteed quality, given its sampling of heterogeneous relationships from open-ended image corpora. Additionally, by utilizing open-source VLM and LLM models (e.g., InternVL2-26B (Chen et al., 2024b), Llama-3-8B (Dubey et al., 2024)), the entire process can operate at low cost. Weve produced 26 million data instances in this stage, achieving superior data quality compared to the existing datasets. In our pilot experiment, with just 500K sampled instances from MegaPairs, the same pre-trained models fine-tuning performance already surpasses that of the entire 36.7M training instances from MagicLens, i.e., delivering better results with 70 less training data. We further trained three multimodal retrievers, MMRet, of varying sizes based on the whole synthetic dataset and perform comprehensive evaluations with wide range of multimodal retrieval tasks. Remarkably, MMRet achieved state-of-the-art performance on 4 popular composed image retrieval (CIR) benchmarks and the 36 datasets provided by MMEB (Jiang et al., 2024b) in the zero-shot setting. Furthermore, the models demonstrated substantial improvements and maintain leading positions after downstream fine-tuning. The entire suite of assets, including the dataset, the well-trained models, and the data production pipeline, will be made publicly available to advance the future progress in this field."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Retrieval. Traditionally, retrieval tasks have focused on scenarios where queries and candidates exist in distinct modalities, such as unimodal retrieval (Thakur et al., 2021) and cross-modal retrieval (Chen et al., 2015). However, there is growing demand for multimodal retrieval tasks, where queries or candidates integrate both image and text modalities. These tasks have wide applications, including image retrieval with instructions (Wu et al., 2021; Liu et al., 2021; Zhang et al., 2024), multimodal document retrieval (Chang et al., 2022; Liu et al., 2022), knowledge retrieval with multimodal queries (Luo et al., 2023), and retrieval-augmented generation (Yasunaga et al., 2023; Yu et al., 2024). Most existing methods employ pre-trained vision-language models (VLMs) to address these tasks (Radford et al., 2021; Li et al., 2023; Saito et al., 2023). However, the common VLMs are purely trained on image-text matching datasets (Changpinyo et al., 2021; Schuhmann et al., 2022), which are in lack of ability to jointly encode and comprehend both modalities effectively. As result, it is necessary to create proper datasets so as to extend VLMs for the diversified multimodal retrieval tasks. Instruction Tuning for Multimodal Retrieval. Instruction-tuning is popular strategy to enhance the multi-task capacity for both large language models (Ouyang et al., 2022; Wei et al., 2021; Chung et al., 2024) and embedding models (Su et al., 2022; Asai et al., 2022; Zhang et al., 2023; Xiao et al., 2024; Chen et al., 2024a). While there have been few instruction datasets proposed for multimodal retrieval (Liu et al., 2021, 2022; Chang Figure 1: Construction pipeline of multimodal triplets: (a) mining of image pairs, (b) generation of open-ended instructions. Multiple similarity models are used to introduce diversified correlations for the image pairs. et al., 2022; Wei et al., 2024; Zhou et al., 2024), they are limited in scale and diversity due to their reliance on human annotation. Recently, notable progress was made by MagicLens (Zhang et al., 2024), where large-scale open-ended search instruction dataset is created from the co-existed images within webpages. However, given the shortage of multi-image webs, MagicLens is limited by its scalability and data-quality. Moreover, this dataset is still held private and inaccessible to public users. As result, the creation and release of high-quality instruction-tuning datasets have become imperative for advancing multimodal retrieval research."
        },
        {
            "title": "3.1 MegaPairs Construction",
            "content": "Training on large-scale open-world data significantly enhances the generalization capabilities of foundation models. For instance, CLIP (Radford et al., 2021) has achieved remarkable advancements in cross-modal retrieval and various downstream tasks due to its extensive training on textimage pairs. However, the multimodal instruction tuning data, despite its importance to multimodal retrieval (Zhang et al., 2024), is scarce in natural world and expensive to annotate by human effort. In this paper, we propose to construct large-scale multimodal instruction-tuning datasets through data synthesis. Formally, each data instance contains the following triplet: pair of images (Iq, It), together with textual instructions Tqt specifying the transition relationship from query image Iq to target image It. We identify two primary technical challenges in acquiring such triplets: (1) sampling relevant and diversified image pairs at scale, (2) precise annotation of instruction for the sampled image pair. To address these challenges, we propose leveraging the common open-domain image corpora. Intuitively, large-scale corpus contains abundant correlated images of diverse semantic relationships, which can be mined and annotated for our instruction-tuning data. Our data synthesis pipeline is demonstrated as Figure 1, which involves two main components: the mining of image pairs and the generation of open-ended instructions. Mining Correlated Image Pairs. As illustrated in Figure 1(a), we propose sampling correlated image pairs from large-scale image corpus. For each query image (Iq, Cq), we utilize multiple similarity models to search for diverse set of correlated target images of heterogeneous correlations {It1, It2, . . . , Itn}. In our work, the following types of correlations are used: (1) visual-semantic correlation, which measures the semantic correlation of two images regardless of visual similarity, e.g., two different views of the same cars; (2) visualpattern correlation, which captures the visual similarity of two images regardless of semantic correlation, e.g., different cars in similar backgrounds; (3) caption correlation, which measures the textual similarity between two images captions. Recognizing the importance of hard negatives in training retrieval models (Xiong et al., 2020; Hofstätter et al., 2021; Zhang et al., 2022), for each pair (Iq, Iti), we include additional images {Itj tj = ti} from the retrieved set as hard negative samples. This approach is simple but empirically effective. We validate the scalability and quality of our data pairs in Section 4.3, with additional examples visualized in Appendix F. Generating Open-Ended Instructions. As shown in Figure 1(b), we utilize open-source multimodal large language models (MLLM) and large language models (LLM) for the automated annotation of mined image pairs = {(Iq, Iti)}. Initially, each image pair (Iq, Iti) is processed by the MLLM to generate detailed description Di of the common concepts and differences between the query image Iq and the target image Iti. This description Di is then refined by the LLM to produce textual instructions Tqti. We prompt the LLM to generate multiple Tqti for each pair, enhancing the diversity of the textual instructions. Ultimately, we construct multimodal triplet (Iq, Tqti, Iti), where (Iq, Tqti) can be used to retrieve Iti. This two-step annotation method ensures both accuracy and diversity in the automated annotation process while leveraging open-source models. The detailed prompts can be found in Appendix A. Implementations. dataset of 26,235,105 image pairs is created based on the above data synthesis pipeline. We utilize subset from the RecapDataComp-1B (Li et al., 2024b) as our image corpus, containing 20 million captioned images. For similarity models, we employ EVA-CLIPs image encoder for visual-semantic correlation (Sun et al., 2023), DINOv2 (Oquab et al., 2024) for visualpattern correlation, and EVA-CLIPs text encoder for caption similarity. We filter the image pairs whose similarity score is within (0.8, 0.96), thus eliminating weak associations and near duplications. We further leverage InternVL2-26B (Chen et al., 2024b) and LLaMA3-8B (Dubey et al., 2024) to generate the open-ended instructions. For each image pair, we create at least three different textual instructions and introduce five hard negatives."
        },
        {
            "title": "3.2 MMRet Model",
            "content": "We propose MMRet, series of models designed for universal multimodal retrieval based on pretrained vision-language models (VLMs). Our MMRet integrates two distinct VLM architectures to achieve universal multimodal embedding. CLIP-based MMRet. The original CLIP (Radford et al., 2021) model employs dual encoder architecture that independently encodes image and text data. We denote the image encoder as ΦI and the text encoder as ΦT . Given an image or text , their embeddings are computed as follows: ei = ΦI (I) et = ΦT (T ) (1) To produce the multimodal embedding for composed image-text sample (I, ), we employ the score-fusion strategy as used by UniIR (Wei et al., 2024), which directly uses an element-wise addition of the outputs from the dual encoders: eit = ΦI (I) + ΦT (T ) (2) In our CLIP-based MMRet, we trained both base and large models. MLLM-based MMRet. The multimodal large language models (MLLMs) incorporate visual encoder, typically based on vision transformer (Dosovitskiy et al., 2021), into large language model (LLM). This integration allows image tokens to be directly processed by the LLM. Consequently, MLLMs can effectively handle diverse multimodal inputs by converting any type of input into sequence of tokens. For instance, composed image-text data is transformed into interleaved sequences of image and text tokens, enabling the model to process them seamlessly. Our MMRet model builds upon the LLaVA1.6 (Liu et al., 2024). In both training and inference stages, MMRet uses task-specific instructions for query inputs to improve generalization, aligning with standard practices in LLM-based embedding models (Wang et al., 2023; Li et al., 2024a). typical multimodal query input is structured as follows: instruct {task_inst} query {qt} {qi} [EOS] (3) where {task_inst} represents the task-specific instruction, {qt} denotes the input query text, and {qi} is the input query image. The normalized last hidden state of the [EOS] token in the MLLM is used as the embedding of any given input sequence."
        },
        {
            "title": "3.3 Multimodal Contrastive Learning",
            "content": "We employ multimodal contrastive learning to transform the original CLIP and MLLM into our MMRet model, enabling various multimodal retrieval tasks. We use the standard InfoNCE loss (Oord et al., 2018) as our training objective: = 1 (cid:88) log qiQ /τ ) exp(eqi ec+ cj exp(eqi ecj /τ ) (cid:80) (4) where the set includes all query samples qi in batch. The vectors eqi and ec+ are the embeddings of the query qi and its positive candidate c+ , respectively. The set contains all in-batch candidates. Notably, both and can be images, text, or composed image-text data. The parameter τ modulates the penalties on negative samples and is set to 0.02 unless otherwise specified in this paper."
        },
        {
            "title": "Backbone",
            "content": "# Params"
        },
        {
            "title": "FashionIQ GeneCIS",
            "content": "mAP@5 R@1 Rs@1 R@10 Rs@1 SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LDRE (Yang et al., 2024) MagicLens-B (Zhang et al., 2024) MagicLens-B (Zhang et al., 2024) MMRet-Base Pic2Word (Saito et al., 2023) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CompoDiff (Gu et al., 2024a) CIReVL (Karthik et al., 2023) LDRE (Yang et al., 2024) MagicLens-L (Zhang et al., 2024) MagicLens-L (Zhang et al., 2024) MMRet-Large LDRE (Yang et al., 2024) CIReVL (Karthik et al., 2023) IP-CIR (Li et al., 2024c) E5-V (Jiang et al., 2024a) MM-Emded (Lin et al., 2024) CLIP-B CLIP-B CLIP-B CLIP-B CoCa-B CLIP-B CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CoCa-L CLIP-L CLIP-G CLIP-G CLIP-G LLaVA-1.6 LLaVA-1.6 165M 12.3B 7.9B 166M 267M 149M 429M 428M 442M 568M 12.5B 8.2B 465M 613M 428M 10.3B 14.6B 43.8B 8.35B 7.57B MMRet-MLLM LLaVA-1.6 7.57B 9.4 14.9 18.0 23.1 30.8 34.3 8.7 10.4 11.7 12.6 18.6 23.4 29.6 34.1 39. 31.1 26.8 32.8 19.1 32.3 42.2 24.0 23.9 25.7 27.0 31.6 36.1 23.9 25.5 24.2 18.2 24.6 26.5 30.1 33.3 38. 36.2 34.7 39.3 33.9 - 46.7 54.9 60.2 60.5 66.7 69.3 71.6 - 55.6 53.8 57.4 59.5 60.4 68.1 70.9 73. 68.8 68.0 70.0 - - 75.4 22.9 28.3 24.8 26.3 35.2 31.9 24.7 35.4 25.6 36.0 28.6 28.5 30.7 38.0 34. 32.5 32.2 45.7 31.8 - 35.6 - 15.9 - 15.0 17.4 18.0 11.2 - 12.3 14.9 15.9 - 16.3 16.7 18. - 17.4 - - - 21.1 Table 1: Zero-shot retrieval performance on various CIR benchmarks. denotes the previous best performance for each benchmark prior to MMRet. indicates methods with multiple components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCa-based MagicLens models are proprietary. Results in bold and underline denote the best and second-best performances for each model scale, respectively. Our MMRet model achieves state-of-the-art results across different model sizes and benchmarks, surpassing the previous SOTA by 8.1% on the main benchmark CIRCO, significantly advancing zero-shot CIR methods."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we first evaluate the effectiveness of MegaPairs on zero-shot composed image retrieval (CIR) tasks in Section 4.1. Next, we explore the impact of MegaPairs on broader multimodal retrieval tasks in Section 4.2. Finally, we conduct detailed analysis on our MegaPairs in Section 4.3."
        },
        {
            "title": "4.1 Zero-shot Performance on CIR tasks",
            "content": "4.1."
        },
        {
            "title": "Implementation Details",
            "content": "We utilize our MegaPairs dataset to perform multimodal contrastive training for our MMRet models. For the CLIP-based MMRet, we initialize the model using both the base1 and large2 versions of CLIP, referred to as MMRet-Base and MMRetLarge, respectively. For the MLLM-based MMRet, we leverage the LLaVA-1.6 Mistral 7B architecture3 and initialize the model parameters accordingly, which we denote as MMRet-MLLM. The 1https://huggingface.co/openai/clip-vit-base-patch16 2https://huggingface.co/openai/clip-vit-large-patch14 3https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf training details of MMRet on MegaPairs can be found in Appendix B."
        },
        {
            "title": "4.1.2 Benchmarks",
            "content": "We evaluate our MMRet in zero-shot setting across four different composed image retrieval benchmarks: CIRCO (Baldrati et al., 2023), CIRR (Liu et al., 2021), FashionIQ (Wu et al., 2021), and GeneCIS (Vaze et al., 2023). Following previous practice (Zhang et al., 2024), CIRCO is considered our main benchmark due to its extensive candidate pool and high-quality annotations. Detailed information and metrics for each benchmark can be found in Appendix C."
        },
        {
            "title": "4.1.3 Evaluation Results",
            "content": "The main evaluation results of MMRet across four benchmarks are shown in Table 1, with full results for each benchmark provided in Appendix D. We have identified three key observations: (1) Our MMRet-MLLM model achieves leading performance across three of the four benchmarks. Specifically, on our main benchmark,"
        },
        {
            "title": "Models",
            "content": "Per Meta-Task Score"
        },
        {
            "title": "Classification VQA Retrieval Grounding",
            "content": "number of datasets BLIP2 (Li et al., 2023) SigLIP (Zhai et al., 2023) CLIP (Radford et al., 2021) OpenCLIP (Cherti et al., 2023) UniIR (Wei et al., 2024) MagicLens (Zhang et al., 2024) E5-V (LLaVA-1.6) (Jiang et al., 2024a) MMRet-MLLM (LLaVA-1.6) 10 27.0 40.3 42.8 47.8 42.1 38.8 21.8 47. 10 4.2 8.4 9.1 10.9 15.0 8.3 4.9 18.4 12 33.9 31.6 53.0 52.3 60.1 35.4 11.5 56. 4 47.0 59.5 51.8 53.3 62.2 26.0 19.0 62.2 36 25.2 34.8 37.8 39.7 42.8 27.8 13.3 44. Table 2: Zero-shot performance on the Massive Multimodal Embedding Benchmark (MMEB). UniIR was trained on M-BEIR (Wei et al., 2024), which includes 10 of the 12 datasets in the MMEB retrieval tasks, it does not strictly adhere to zero-shot setting. In contrast, our MMRet-MLLM, trained exclusively on the MegaPairs dataset, achieves state-of-the-art zero-shot performance in overall scores and multiple meta-tasks on MMEB. CIRCO, MMRet-MLLM surpasses the current SOTA CoCa-based MagicLens-L by achieving 42.2% mAP@5 compared to 34.1% (an increase of 8.1%). On CIRR test set, it exceeds the current SOTA by 7.4% and 4.5% in R@1 and Rs@1, respectively. Additionally, on GeneCIS, it leads the current SOTA by 3.7% in Rs@1. (2) MMRet exhibits superior performance across all model scales. For instance, MMRetBase and MMRet-Large outperform comparable models by 4.5% and 4.7% in R@1 on the CIRR test set, respectively. Additionally, they surpass similar models by 3.5% and 5.1% in mAP@5 on the CIRCO benchmark. In the fashion-domain benchmark FashionIQ, while not achieving the highest scores, our CLIP-based MMRet shows competitive performance against other CLIP-based models. (3) The MMRet-Base model surpasses most larger models, underscoring the exceptional quality of our MegaPairs dataset. Despite being our smallest model, MMRet-Base outperforms many larger models such as the MagicLens-L. For instance, it achieving the best result on CIRCO with mAP@5 of 34.3%, excluding our own MMRetLarge and MMRet-MLLM models. It even exceeds the performance of models with dozens of times more parameters (e.g., MM-Embed), emphasizing the effectiveness of our MegaPairs dataset."
        },
        {
            "title": "4.2 Performance on MMEB",
            "content": "To further validate the generalization ability of MegaPairs for broader multimodal embedding tasks, we evaluate MMRet on the Massive Multimodal Embedding Benchmark (MMEB) (Jiang et al., 2024b). MMEB is comprehensive benchmark that includes 36 datasets across four metatask categories: Classification, Visual Question Answering, Retrieval, and Visual Grounding. It is designed to evaluate the quality of multimodal embeddings and assesses models across diverse combinations of text and image modalities. We present the performance of MMRet in both zero-shot and supervised fine-tuning scenarios. Following previous works (Jiang et al., 2024a,b), we conduct experiments using our MMRet-MLLM."
        },
        {
            "title": "4.2.1 Zero-shot Performance",
            "content": "Implementation Details. In the zero-shot evaluation on MMEB, we directly utilize our MMRetMLLM from Section 4.1, maintaining implementation details consistent with Section 4.1.1. Metrics. We evaluate Precision@1 for all tasks, which measures the ratio of positive candidates ranked in the top position for all queries. We report the average scores for the four meta tasks as well as the overall average. Following the MMEB setting, we incorporate the predefined task-specific instructions into queries for all tasks during evaluation. Results. The zero-shot performance of our MMRet-MLLM on MMEB is presented in Table 2. MMRet-MLLM achieved state-of-the-art zero-shot performance across various embedding meta-task, recording the highest overall average performance. Compared to the recent E5-V (Jiang et al., 2024a), which uses similar LLaVA-1.6 (Liu"
        },
        {
            "title": "Models",
            "content": "number of datasets CLIP OpenCLIP VLM2Vec (LLaVA-1.6) VLM2Vec (Phi-3.5-V) MMRet-MLLM Per Meta-Task Score"
        },
        {
            "title": "IND OOD Overall",
            "content": "10 55.2 56.0 54.7 54.8 56.0 10 19.7 21.9 50.3 54.9 57. 12 53.2 55.4 56.2 62.3 69.9 4 62.2 64.1 64.0 79.5 83. 20 16 47.6 50.5 61.0 66.5 68.0 42.8 43.1 47.5 52.0 59. 36 45.4 47.2 55.0 60.1 64.1 Table 3: Supervised fine-tuning results on the MMEB benchmark. The backbone of our MMRet-MLLM is LLaVA-1.6 (Liu et al., 2024). We compare our results with the following baselines: CLIP (Radford et al., 2021), OpenCLIP (Cherti et al., 2023), and two versions of VLM2Vec (Jiang et al., 2024b) that employ the LLaVA-1.6 (Liu et al., 2024) and Phi-3.5-V (Abdin et al., 2024) backbones. All baseline results are sourced from (Jiang et al., 2024b). IND: in-distribution dataset; OOD: out-of-distribution dataset. et al., 2024) backbone for universal multimodal embedding, MMRet-MLLM trained on our MegaPairs dataset demonstrated superior performance. Notably, the second-best model, UniIR, was trained on M-BEIR (Wei et al., 2024), which encompasses datasets from 10 of the 12 retrieval meta-tasks in MMEB, and thus is not considered zero-shot for this meta-task. Consequently, our MLLM-Ret significantly outperforms the remaining methods in the retrieval meta-task and demonstrates strong generalization capabilities across all tasks. an overall average Precision@1 of 64.1%. Compared to VLM2Vec (LLaVA-1.6) (Jiang et al., 2024b), which directly fine-tunes LLaVA-1.6 on MMEB, MMRet-MLLM enhances downstream task performance by 9.1% through multimodal contrastive training on our MegaPairs. Notably, our model shows improvements of 11.6% and 7.1% on out-of-distribution (OOD) datasets compared to the two versions of VLM2Vec, highlighting the superior generalization capability of our MegaPairs for broader downstream multimodal embedding tasks."
        },
        {
            "title": "4.3 Detailed Investigation on MegaPairs",
            "content": "Implementation Details. We further fine-tune our MMRet-MLLM on MMEB to investigate the impact of MegaPairs on downstream task performance. The MMEB dataset includes 20 indistribution (IND) datasets for training and 16 outof-distribution (OOD) datasets for evaluation. We utilize the training sets from the 20 IND datasets, comprising approximately 662K data points. The learning rate is set to 5 106, and we employ LoRA with rank of 32. The batch size is set to 192, and we train for one epoch. Following the VLM2Vec configuration (Jiang et al., 2024b), we incorporate task-specific instructions into the queries during training. Metrics. We employ the same metrics as outlined in Section 4.2.1. Additionally, we report the average scores for both the IND and OOD datasets. Results. Table 3 compares the supervised finetuning performance of our MMRet model with various baselines on the MMEB dataset. Our MMRetMLLM achieves state-of-the-art performance, with We first assess the quality and scalability of our MegaPairs dataset in Section 4.3.1. Next, we evaluate the effectiveness of the hard negative samples provided by MegaPairs in Section 4.3.2. Finally, we explore the strategies used for mining image pairs from open-domain image corpora in Section 4.3.3. Unless otherwise specified, all subsequent experiments are conducted using our MMRet-base model."
        },
        {
            "title": "4.3.1 Data Scalability and Quality",
            "content": "We first evaluated the performance trend of MMRet by training it on different sizes of subsets from the MegaPairs dataset to verify its scalability. Subsequently, we compared it with existing datasets to highlight the high-quality features of MegaPairs. Performance Scaling. As shown in Figure 2, the performance of MMRet-base across various benchmarks consistently improves with the increasing size of training data. This upward trend highlights the effectiveness and scalability of MegaPairs. Dataset Quality Comparison with Exsiting"
        },
        {
            "title": "Strategy",
            "content": "CIRCO CIRR"
        },
        {
            "title": "D I",
            "content": "T mAP@5 R@1 R@10 Rs@1 29.0 30.0 31.6 31.0 32.4 32.2 32.3 31.5 30.0 32.2 32.1 33.3 33.3 33. 24.7 29.6 28.7 28.5 28.9 29.7 30.1 17.2 15.3 17.3 17.1 17.5 16.4 17. Table 5: Performance comparison of MMRet-base using different data pairing strategies at 1M scale. D: DINOv2 Encoder; I: CLIP Image Encoder; T: CLIP Text Encoder. FIQ and CIS represent the FashionIQ and GeneCIS benchmarks, respectively. We report CIRR validation set performance due to test server submission limits. fair comparison, we selected 1M data entries for each construction strategy and trained the model for 2000 steps. Table 5 presents the results of various data pairing strategies across multiple benchmarks. Initially, when evaluating individual strategies, we observed that triplets based on text similarity achieved the highest zero-shot CIR performance. We hypothesize that text similarity captures more diverse relationships than image similarity. Furthermore, combining any two pairing strategies consistently outperformed using single strategy. This enhancement is likely due to the increased diversity within the dataset, which is essential for training robust multimodal embedding models. Ultimately, employing all three strategies simultaneously provided the most robust performance across all datasets. As result, this approach was chosen for constructing the MegaPairs."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce MegaPairs, large-scale multimodal pairing dataset designed for training universal multimodal retrievers. MegaPairs comprises diverse image pairs from the open world, annotated with open-ended textual instructions that capture their visual and semantic relationships. Using MegaPairs, we trained our MMRet models, achieving state-of-the-art zero-shot performance in four composed image retrieval tasks and on the Massive Multimodal Embedding Benchmarks, which consists of 36 different datasets. Extensive experiments further demonstrate the generalization capability and high-quality features of MegaPairs. Figure 2: Performance scaling of MMRet-base on the MegaPairs as data size increases. The dashed lines indicate the performance of MagicLens-B (CLIP) trained on their dataset of 36.7M data pairs. Negatives CIRCO CIRR"
        },
        {
            "title": "CIS",
            "content": "Qry HN mAP@5 R@1 R@10 Rs@1 10.1 29.7 32. 0.2 32.1 33.7 25.3 27.6 30.1 14.4 16.6 17.0 Table 4: Performance comparison of MMRet-base using different negative strategies at 1M scale. Qry: query image negative; HN: our mined hard negatives. We report CIRR validation set performance due to their test server submission limits. Datasets. The dashed lines in Figure 2 represent the performance of the MagicLens-B (CLIP) model, trained on their 36.7M dataset (Zhang et al., 2024). Remarkably, with only 0.5M samples from our MegaPairs dataset, constituting less than 2% of MagicLens, MMRet significantly surpasses MagicLens across all benchmarks using the same CLIPbase backbone. This result underscores the superior quality and efficiency of our MegaPairs dataset."
        },
        {
            "title": "4.3.2 The Impact of Hard Negatives",
            "content": "In MegaPairs, images from the retrieved set that are not the target are marked as hard negatives, providing diverse and ample set of hard negative target images for each image pair. As shown in Table 4, compared to not using negatives or only using the query image as negative, training with our mined hard negatives significantly enhances model performance across all benchmarks."
        },
        {
            "title": "4.3.3 Data Pair Search Strategy",
            "content": "We explored the impact of various search strategies in constructing heterogeneous triplets. For a"
        },
        {
            "title": "Limitations",
            "content": "In constructing MegaPairs, we discovered that using diverse retrievers can generate richer image pairs. Our study employed three distinct retrievers, which offered substantial diversity. However, there remains potential to explore additional pairing methods, such as leveraging more advanced text domain retrievers (e.g., BGE (Xiao et al., 2024)) or incorporating varied strategies like imagetext retrieval."
        },
        {
            "title": "Ethics Statement",
            "content": "All images in our MegaPairs dataset are sourced from the Recap-Datacomp-1B dataset (Li et al., 2024b), and have undergone rigorous screening by the Datacomp team to remove harmful content (Gadre et al., 2024). Despite our best efforts, we acknowledge that these screenings may not be entirely comprehensive or without omissions. Additionally, we strongly discourage the use of MMRet models for encoding and retrieving sensitive content."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. Task-aware retrieval with instructions. arXiv preprint arXiv:2211.09260. Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. 2023. Zero-shot composed image retrieval with textual inversion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1533815347. Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1649516504. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35583568. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024a. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216. Junyang Chen and Hanjiang Lai. 2023. Pretrain like you inference: Masked tuning improves zeroarXiv preprint shot composed image retrieval. arXiv:2311.07622. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022. Murag: Multimodal retrieval-augmented generator for open question anIn Proceedings of swering over images and text. the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 55585570. Association for Computational Linguistics. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2023. Reproducible scaling laws for contrastive language-image learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 28182829. IEEE. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Niv Cohen, Rinon Gal, Eli Meirom, Gal Chechik, and Yuval Atzmon. 2022. this is my unicorn, fluffy: Personalizing frozen vision-language representations. In European conference on computer vision, pages 558577. Springer. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. 2024. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36. Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, and Sangdoo Yun. 2024a. Compodiff: Versatile composed image retrieval with latent diffusion. Trans. Mach. Learn. Res., 2024. Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, and Sangdoo Yun. 2024b. Language-only efficient training of zero-shot composed image retrieval. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1322513234. IEEE. Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching an effective dense retriever with balanced topic aware sampling. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 113122. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024a. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024b. Vlm2vec: for massive Training vision-language models arXiv preprint multimodal embedding tasks. arXiv:2410.05160. Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. 2023. Vision-bylanguage for training-free compositional image retrieval. arXiv preprint arXiv:2310.09291. Chaofan Li, Zheng Liu, Shitao Xiao, Yingxia Shao, and Defu Lian. 2024a. Llama2vec: Unsupervised adaptation of large language models for dense retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34903500. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597. Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. 2024b. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478. You Li, Fan Ma, and Yi Yang. 2024c. Imagine and seek: Improving composed image retrieval with an imagined proxy. arXiv preprint arXiv:2411.16752. Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. 2024. Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. Llavanext: Improved reasoning, ocr, and world knowledge. Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. 2022. Universal vision-language dense retrieval: Learning unified representation space for multi-modal retrieval. In The Eleventh International Conference on Learning Representations. Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. 2021. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21252134. Man Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, and Chitta Baral. 2023. End-to-end knowledge retrieval with multi-modal queries. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 8573 8589. Association for Computational Linguistics. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: visual question answering benchmark requiring external knowlIn Proceedings of the IEEE/cvf conference edge. on computer vision and pattern recognition, pages 31953204. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. 2024. Dinov2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. 2023. Pic2word: Mapping pictures to words for zeroshot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1930519314. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: an open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, and Jimmy Lin. 2024. Unirag: Universal retrieval augmentation for multi-modal large language models. arXiv preprint arXiv:2405.10311. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. 2022. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, Improved trainarXiv preprint and Yue Cao. 2023. Eva-clip: ing techniques for clip at scale. arXiv:2303.15389. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663. similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68626872. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir: Training and benchmarking universal multimodal information retrievers. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pages 387404. Springer. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. 2021. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1130711317. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 641649, New York, NY, USA. Association for Computing Machinery. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808. Zhenyu Yang, Dizhan Xue, Shengsheng Qian, Weiming Dong, and Changsheng Xu. 2024. Ldre: Llm-based divergent reasoning and ensemble for zero-shot composed image retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 8090. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. 2023. Retrieval-augmented multimodal language modeling. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 3975539769. PMLR. Sagar Vaze, Nicolas Carion, and Ishan Misra. 2023. Genecis: benchmark for general conditional image Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, et al. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986. Jianjin Zhang, Zheng Liu, Weihao Han, Shitao Xiao, Ruicheng Zheng, Yingxia Shao, Hao Sun, Hanqing Zhu, Premkumar Srinivasan, Weiwei Deng, et al. 2022. Uni-retriever: Towards learning the unified embedding based retriever in bing sponsored search. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 44934501. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. 2024. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554. Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024. VISTA: Visualized text embedding for universal multi-modal retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31853200, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Detailed Prompt for Annotating",
            "content": "Open-Ended Instructions To annotate open-ended instructions, we begin by using the MLLM to generate detailed description of the commonalities and differences between the query image and the target image, where the corresponding prompt is illustrated in Figure 3. Subsequently, the description is refined by the LLM to produce textual instructions, with the associated prompt provided in Figure 4."
        },
        {
            "title": "MegaPairs",
            "content": "For the CLIP-based MMRet, the training process employs batch size of 2048, with each query Figure 3: The specific prompts for MLLM. The value of WORD_NUM ranges from 60 to 100 in our practical data generation to enhance the diversity of the generated description. paired with one positive image and four hard negatives. All input images are resized to 224x224 to match the models configuration. During training, all CLIP parameters remain unfrozen. MMRetbase is trained for 15,000 steps, while MMRetlarge is trained for 25,000 steps on the MegaPairs dataset. For the MLLM-based MMRet, we use batch size of 144 during training, with each query associated with one positive image and three hard negatives. We apply LoRA (Hu et al., 2022) to both the ViT encoder and the LLM backbone of LLaVA1.6, setting the LoRA rank to 32. Although the original model supports variable resolution image processing, we use fixed resolution of 512x512 for all images to manage the token sequence length. MMRet-MLLM is trained for 20,000 steps on the MegaPairs dataset. For both CLIP-based and MLLM-based MMRet models, we set an initial learning rate of 5 106 and employ linear decay strategy."
        },
        {
            "title": "C Detailed Information and Evaluation",
            "content": "Metrics of Zero-Shot CIR Benchmarks The detailed information and metrics of our evaluation in zero-shot composed image retrieval (CIR) tasks for each benchmark are as follows: Figure 4: The specific prompts for LLM. The figure showcases two demonstrations, while in our practical data generation process, five demonstrations are randomly selected from pool of 50 and fed into the LLM. CIRCO (Baldrati et al., 2023) is challenging zero-shot CIR benchmark comprising 123,403 candidate natural images. We evaluete our MMRet models on its test set, which contains 800 composed image-text queries, each annotated with multiple ground-truth images. We use mean Average Precision (mAP) as the evaluation metric. Due to its extensive candidate pool and high-quality annotations, CIRCO serves as robust and comprehensive benchmark for zero-shot CIR evaluation, and we consider it our main benchmark. CIRR (Liu et al., 2021) is the first dataset for conducting the CIR task using natural images. We conduct zero-shot evaluations on its test set, which comprises 4,148 queries and corpus of 2,315 images. Each query in CIRR is annotated with exactly one positive target image, but it suffers from some false negative issues. For each query, CIRR provides subset retrieval setting that retrieves target images from small corpus. We assess both standard and subset retrieval performance using recall metrics (R and Rs). FashionIQ (Wu et al., 2021) is another CIR task focusing on fashion products. We conduct zero-shot evaluations on its validation set, which includes 6,016 queries and 15,536 images. FashionIQ comprises three sub-tasks: dress, shirt, and toptee. We evaluate each sub-task separately and report their average recall values. GeneCIS (Vaze et al., 2023) is benchmark for conditional image similarity measurement, comprising four sub-tasks about changing or focusing the attribute or object in the given image. In each sub-task, models need to retrieve the most similar images from dedicated small subset for the given query image and the condition keyword. We approach it as CIR task by combining the query image and the text description of the sub-task derived from the condition keyword as composed image-text query. Each querys candidate subset averages 13.8 images, and the mean Rs across all four subsets is reported. no visual similarity to the query image but share the F1 concept)."
        },
        {
            "title": "G Qualitative Results of MMRet on",
            "content": "Zero-shot CIR Tasks We present several top-5 retrieved images of our MMRet and the SOTA MagicLens (Zhang et al., 2024) on zero-shot CIR tasks, as shown in Figure 6. Since only the CLIP-based checkpoint is available for MagicLens, we select the CLIP-L backbone for both methods. 1) For the blue ties query, MMRet accurately interprets the query and identifies both the specific attire and indoor setting, retrieving multiple images that meet the specified requirements. In contrast, MagicLens focuses solely on the individual object, overlooking the broader semantic context. 2) For the sweet, beverage, boats and sky query, MMRet demonstrates solid understanding of real-world entity concepts, successfully integrating both foreground and background elements to retrieve the most relevant image. 3) The success on the bench top query highlights MMRets ability to comprehend specific pose and angle requirements. 4) The success on the darker ground and closer distance query illustrates MMRets capacity to recognize lighting conditions and shooting distance. 5) The success on the whell in the air query indicates that MMRet can identify dynamic actions and contextual scene elements."
        },
        {
            "title": "D Full results on CIR Benchmarks",
            "content": "We report the full results on four CIR benchmarks (Baldrati et al., 2023; Liu et al., 2021; Wu et al., 2021; Vaze et al., 2023) in Tables 6, 7, 8, and 9, respectively. Our MMRet model achieves state-ofthe-art performance across various model sizes on the CIRCO, CIRR, and GeneCIS benchmarks."
        },
        {
            "title": "E Full Results on MMEB Benchmark",
            "content": "We list the full results on the MMEB benchmark (Jiang et al., 2024b) in Table 10. The MMEB benchmark consists of 36 datasets spanning four meta-task categories, including 20 in-distribution datasets and 16 out-of-distribution (OOD) datasets. The results on the OOD datasets are highlighted with gray background in the table. Our MMRet model achieves state-of-the-art performance in both zero-shot and fine-tuning settings. Notably, MMRet surpasses the second-best performance on the OOD datasets, demonstrating its remarkable generalization capability."
        },
        {
            "title": "F Visualized Examples of MegaPairs",
            "content": "We present several examples of MegaPairs in Figure 5. Each row corresponds to single example, where the query item, comprising an image and its corresponding alt-text caption, is associated with multiple target images. These target images include both visually similar ones and semantically related ones beyond visual features. For example, in the 4th row, the query image showing an ottoman with the alt-text caption Round ottoman, tufted surface is paired with target items that feature visually similar images (e.g., the 1st target image, which shows an ottoman, and the 3rd target image, which depicts sofa with similar style) as well as semantically related images that transcend visual features (e.g., the 2nd and 4th target images, depicting the interior of car and living room wall, respectively. These share few visual features with the query image but also exhibit tufted surface). In the 5th row, the query image showing an F1 car with the alt-text caption AMG F1 W09 is paired with target items featuring visually similar images (e.g., the 1st target image, which shows an F1 car in red, and the 3rd target image, which displays race scene with multiple F1 cars) as well as semantically related images that transcend visual features (e.g., the 2nd target image, which shows an F1 driver, and the 4th target image, depicting an F1 circuit. These images bear"
        },
        {
            "title": "Backbone",
            "content": "# Params mAP@5 mAP@10 mAP@25 mAP@50 PALAVRA (Cohen et al., 2022) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LDRE (Yang et al., 2024) MagicLens-B (Zhang et al., 2024) MagicLens-B (Zhang et al., 2024) MMRet-Base Pic2Word (Saito et al., 2023) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) CompoDiff (Gu et al., 2024a) MagicLens-L (Zhang et al., 2024) MagicLens-L (Zhang et al., 2024) MMRet-Large Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) LinCIR (Gu et al., 2024b) Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) CompoDiff (Gu et al., 2024a) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) LDRE (Yang et al., 2024) IP-CIR (Li et al., 2024c) E5-V (Jiang et al., 2024a) MM-Emded (Lin et al., 2024) CLIP-B BLIP-B CLIP-B CLIP-B CLIP-B CLIP-B CoCa-B CLIP-B CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CoCa-L CLIP-L CLIP-H CLIP-H CLIP-H CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G LLaVA-1.6 LLaVA-1.6 176M 224M 165M 12.3B 7.9B 166M 267M 149M 429M 428M 442M 12.5B 442M 568M 465M 613M 428M 987M 1.0B 1.0B 2.5B 2.6B 2.9B 14.6B 2.6B 10.3B 43.8B 8.35B 7.57B MMRet-MLLM LLaVA-1. 7.57B 4.6 7.1 9.4 14.9 18.0 23.1 30.8 34.3 8.7 10.4 11.7 18.6 12.6 12.6 29.6 34.1 39.2 11.7 16.1 17.6 5.5 13.2 15.3 26.8 19.7 31.1 32.8 19.1 32. 42.2 5.3 8.0 9.9 15.4 18.3 23.8 32.0 35.0 9.5 11.6 12.7 19.0 13.6 13.4 30.8 35.4 40.2 12.3 16.9 18.5 5.6 13.9 17.7 27.6 21.0 32.2 34.3 - - 43.4 6.3 9.2 11.1 17.0 20.2 25.8 34.5 37.6 10.6 13.0 14.3 20.9 15.0 15.8 33.4 38.1 42.9 13.7 18.8 20.5 6.7 15.3 19.4 30.0 23.1 35.0 36.9 - - 46.5 6.8 9.7 11.8 17.8 21.1 26.7 35.6 38.7 11.3 13.7 15.1 21.8 15.9 16.4 34.4 39.2 44.0 14.4 19.7 21.4 7.1 16.0 - 31.0 24.2 36.0 38.0 - - 47.6 indicates methods with multiple Table 6: Full results on the CIRCO benchmark (Baldrati et al., 2023). components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCabased MagicLens models are proprietary. Results in bold denote the best performances for each model scale."
        },
        {
            "title": "Backbone",
            "content": "# Params"
        },
        {
            "title": "Subset Set",
            "content": "R@1 R@5 R@10 R@50 R@1 R@2 R@3 PALAVRA (Cohen et al., 2022) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LDRE (Yang et al., 2024) MagicLens-B (Zhang et al., 2024) MagicLens-B (Zhang et al., 2024) MMRet-Base Pic2Word (Saito et al., 2023) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) CompoDiff (Gu et al., 2024a) MagicLens-L (Zhang et al., 2024) MagicLens-L (Zhang et al., 2024) MMRet-Large Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) LinCIR (Gu et al., 2024b) Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) CompoDiff (Gu et al., 2024a) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) LDRE (Yang et al., 2024) IP-CIR (Li et al., 2024c) E5-V (Jiang et al., 2024a) CLIP-B BLIP-B CLIP-B CLIP-B CLIP-B CLIP-B CoCa-B CLIP-B CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CoCa-L CLIP-L CLIP-H CLIP-H CLIP-H CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G LLaVA-1.6 176M 224M 165M 12.3B 7.9B 166M 267M 149M 429M 428M 442M 12.5B 442M 568M 465M 613M 428M 987M 1.0B 1.0B 2.5B 2.6B 2.9B 14.6B 2.6B 10.3B 43.8B 8.35B MMRet-MLLM LLaVA-1. 7.57B 16.6 27.2 24.0 23.9 25.7 27.0 31.6 36.1 23.9 25.5 24.2 24.6 25.0 18.2 30.1 33.3 38.0 32.9 34.0 33.8 30.4 34.8 26.7 34.7 35.3 36.2 39.3 33. 46.7 43.5 58.9 53.4 52.5 55.1 58.0 64.0 68.1 51.7 54.6 52.5 52.3 53.3 53.1 61.7 67.0 70.3 63.1 64.0 63.5 58.1 64.1 55.1 64.3 64.7 66.4 70.1 64. 76.0 58.5 71.4 66.8 66.0 69.0 70.9 76.9 79.5 65.3 67.6 66.3 64.9 66.7 70.8 74.4 77.9 81.1 73.9 75.3 73.4 69.2 75.1 74.5 75.1 76.1 77.3 80.0 75. 85.1 84.0 91.3 89.8 87.0 89.9 91.1 93.8 94.7 87.8 88.7 88.8 86.3 - 90.3 92.6 94.4 94.7 - - - - - 92.0 91.7 - 94.0 94.9 93. 96.5 41.6 55.1 54.9 60.2 60.5 66.7 69.3 71.6 - 55.6 53.8 59.5 57.1 57.4 68.1 70.9 73.2 62.2 64.6 62.4 68.9 68.7 64.5 68.0 63.4 68.8 70.0 - 75.4 65.3 77.4 76.6 80.1 80.7 83.9 86.0 87.2 - 77.5 75.0 79.9 77.4 77.1 84.8 87.3 88.0 81.4 83.2 81.5 85.5 84.7 82.4 84.9 82.2 85.7 86.9 - 89.6 80.9 89.1 88.2 90.2 90.7 92.4 94.0 94.0 - 89.5 88.2 89.7 88.9 87.9 93.2 94.5 94.3 91.2 92.8 92.1 93.0 93.2 91.8 93.2 92.0 93.8 94.2 - 95.7 Table 7: Full results on the CIRR benchmark (Liu et al., 2021). indicates methods with multiple components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCa-based MagicLens models are proprietary. Results in bold denote the best performance for each model scale. Methods Backbone # Params Dress Shirt Toptee Overall R@10 R@50 R@10 R@50 R@10 R@50 R@10 R@50 PALAVRA (Cohen et al., 2022) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LDRE (Yang et al., 2024) MagicLens-B (Zhang et al., 2024) MagicLens-B (Zhang et al., 2024) MMRet-Base Pic2Word (Saito et al., 2023) PLI (Chen and Lai, 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) CompoDiff (Gu et al., 2024a) MagicLens-L (Zhang et al., 2024) MagicLens-L (Zhang et al., 2024) MMRet-Large Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) LinCIR (Gu et al., 2024b) Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) CompoDiff (Gu et al., 2024a) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) LDRE (Yang et al., 2024) IP-CIR (Li et al., 2024c) E5-V (Jiang et al., 2024a) CLIP-B BLIP-B CLIP-B CLIP-B CLIP-B CLIP-B CoCa-B CLIP-B CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CoCa-L CLIP-L CLIP-H CLIP-H CLIP-H CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G LLaVA-1.6 176M 224M 165M 12.3B 7.9B 166M 267M 149M 429M 428M 442M 12.5B 442M 568M 465M 613M 428M 987M 1.0B 1.0B 2.5B 2.6B 2.9B 14.6B 2.6B 10.3B 43.8B 8.35B MMRet-MLLM LLaVA-1.6 7.57B 17.3 28.6 18.5 25.3 27.4 21.5 29.0 26. 20.0 28.1 20.5 24.8 20.9 32.2 25.5 32.3 29.7 28.0 28.5 29.8 25.4 28.2 37.8 27.1 38.1 35.9 48.0 36.4 29.1 35.9 50.8 39.5 46.4 46.3 41.3 48.9 49. 40.2 51.1 43.1 44.8 42.4 46.3 46.1 52.7 50.3 51.5 51.1 52.1 47.7 50.3 49.1 49.5 60.9 58.6 66.7 56.4 50.5 21.5 38.1 24.4 28.4 20.0 27.3 36.5 33. 26.2 38.6 26.9 29.5 29.1 37.7 32.7 40.5 37.0 36.9 36.5 36.9 33.2 36.5 41.3 33.7 46.8 26.1 39.0 23.8 38.5 37.1 57.8 41.6 47.8 41.8 48.8 55.5 53. 43.6 58.5 45.6 47.4 46.8 49.1 53.8 59.2 56.1 56.0 55.5 57.8 50.4 55.4 55.2 51.4 65.1 51.1 61.0 47.5 58.6 20.6 40.9 25.7 31.2 27.1 30.2 40.2 36. 27.9 39.4 29.3 31.4 28.8 38.1 34.0 41.4 37.0 40.2 38.8 42.1 35.2 39.8 44.3 35.8 50.5 35.4 50.2 35.3 39.2 38.8 62.7 46.5 53.9 48.8 52.3 61.9 57. 47.4 62.7 50.0 53.7 50.2 50.6 57.7 63.0 59.3 62.0 60.9 62.5 57.6 61.5 56.4 56.1 71.1 56.7 71.1 57.5 60.6 19.8 35.9 22.9 28.3 24.8 26.3 35.2 31. 24.7 35.4 25.6 28.6 26.3 36.0 30.7 38.0 34.6 35.0 34.6 36.3 31.3 34.8 39.0 32.2 45.1 32.5 45.7 31.8 35.6 37.3 57.1 42.5 49.4 45.6 47.4 55.4 53. 43.7 57.4 46.2 48.6 46.5 48.6 52.5 58.2 55.2 56.5 55.8 57.5 51.9 55.7 51.7 52.4 65.7 55.5 66.3 53.8 56.6 Table 8: Full results on the FashionIQ benchmark (Wu et al., 2021). indicates methods with multiple components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCa-based MagicLens models are proprietary. Results in bold denote the best performance for each model scale. Methods Backbone # Params CIReVL (Karthik et al., 2023) MagicLens-B (Zhang et al., 2024) MagicLens-B (Zhang et al., 2024) MMRet-Base Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) CompoDiff (Gu et al., 2024a) MagicLens-L (Zhang et al., 2024) MagicLens-L (Zhang et al., 2024) MMRet-Large Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) LinCIR (Gu et al., 2024b) Pic2Word (Saito et al., 2023) SEARLE (Baldrati et al., 2023) CompoDiff (Gu et al., 2024a) CIReVL (Karthik et al., 2023) LinCIR (Gu et al., 2024b) CLIP-B CLIP-B CoCa-B CLIP-B CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CLIP-L CoCa-L CLIP-L CLIP-H CLIP-H CLIP-H CLIP-G CLIP-G CLIP-G CLIP-G CLIP-G 12.3B 166M 267M 149M 429M 442M 12.5B 442M 568M 465M 613M 428M 987M 1.0B 1.0B 2.5B 2.6B 2.9B 14.6B 2.6B MMRet-MLLM LLaVA-1.6 7.57B Focus Attribute Change Attribute Focus Object Change Object Avg R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 R@2 R@3 R@1 17.9 15.5 16.2 18.3 15.7 17.0 19.5 16.9 13.5 16.1 16.6 18. 18.6 18.8 19.6 12.5 16.3 14.3 20.5 19.1 18.4 29.4 28.4 27.8 30.9 28.2 29.7 31.8 30.0 24.3 28.2 28.7 30. 30.7 31.5 31.5 23.4 29.4 26.7 34.0 33.0 31.4 40.4 39.1 38.6 39.6 38.7 40.7 42.0 41.5 36.1 39.0 39.3 38. 42.1 42.3 41.6 33.7 40.7 38.4 44.5 42.3 41.0 14.8 12.3 16.2 15.2 13.9 16.4 14.4 16.2 19.2 15.6 16.0 15. 13.2 15.5 16.6 11.7 16.2 19.7 16.1 17.6 16.7 25.8 23.0 27.2 25.6 24.7 25.3 26.0 28.0 28.6 27.5 27.5 27. 23.9 26.9 27.6 21.9 27.3 28.8 28.6 30.2 27.7 35.8 32.1 36.6 34.8 33.1 34.1 35.2 36.8 37.2 36.3 36.5 35. 33.1 35.9 37.5 30.9 35.5 37.4 39.4 38.1 36.4 14.6 14.4 17.1 16.6 8.4 8.0 12.3 8.3 8.1 16.3 15.7 17. 9.2 10.6 9.8 9.9 10.8 9.2 14.7 10.1 22.4 24.3 26.2 27.7 27.3 18.0 16.9 21.8 17.4 16.4 26.2 27.6 26. 17.6 18.7 18.8 19.3 18.2 19.1 25.2 19.1 32.5 33.3 35.5 38.2 35.8 25.8 25.6 30.5 26.2 25.1 35.5 37.3 36. 27.1 26.5 27.9 27.4 27.9 25.8 33.0 28.1 41.6 16.1 17.7 20.2 21.7 6.7 7.9 17.2 7.4 18.7 17.1 18.7 21. 6.6 8.5 9.0 8.6 8.3 18.7 18.1 7.9 26.9 27.8 28.4 32.2 34.9 15.1 16.8 28.9 15.7 31.7 29.5 31.7 34. 16.5 17.9 17.6 18.2 15.6 31.7 31.2 16.3 40.4 37.6 39.2 42.9 45.0 24.0 24.8 37.6 25.0 40.6 39.7 40.2 42. 25.4 26.2 25.7 26.1 25.8 40.2 41.0 25.7 49.9 15.9 15.0 17.4 18.0 11.2 12.3 15.9 12.2 14.9 16.3 16.7 18. 11.9 13.3 13.8 10.7 12.9 15.5 17.4 13.7 21.1 Table 9: Full results on the GeneCIS benchmark (Vaze et al., 2023). indicates methods with multiple components (e.g., GPT-3.5, Qwen1.5-32B); we report # parameters of components with known sizes. The CoCa-based MagicLens models are proprietary. Results in bold denote the best performance for each model scale."
        },
        {
            "title": "Task",
            "content": "Zero-shot Fine-Tune CLIP OpenCLIP SigLIP BLIP2 MagicLens E5-V UniIR MMRet VLM2Vec MMRet Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42.8 VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA All VQA Retrieval (12 tasks) VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS All Retrieval 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 Visual Grounding (4 tasks) 33.8 MSCOCO 56.9 RefCOCO 61.3 RefCOCO-matching 55.1 Visual7W-pointing All Visual Grounding 51.8 Final Score (36 tasks) All All IND All OOD 37.8 37.1 38. 63.5 38.6 51.7 52.4 68.8 37.8 14.2 83.0 51.4 16.8 47.8 11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9 25.4 15.4 74.0 78.0 63.6 62.1 66.1 62.1 13.8 44.6 45.0 77.5 52.3 34.5 54.2 68.3 56.3 53.3 39.7 39.3 40.2 45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40. 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4 21.5 15.1 51.0 52.4 58.3 55.0 62.9 58.1 20.1 55.1 56.0 23.6 31.6 46.4 70.8 50.8 70.1 59.5 34.8 32.3 38.0 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27.0 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4. 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33.9 28.9 47.4 59.5 52.0 47.0 25.2 25.3 25.1 48.0 33.7 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6.2 38.8 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8.3 24.8 39.1 50.7 21.1 54.1 40.0 58.1 43.0 11.2 18.7 1.6 62.6 35. 22.1 22.8 35.6 23.4 26.0 27.8 31.0 23.7 9.6 23.4 49.7 49.9 33.1 8.6 2.0 30.8 7.5 3.1 21.8 8.9 5.9 1.7 2.3 2.4 5.8 3.6 2.6 7.8 3.2 4.9 9.2 6.1 13.5 8.1 20.7 14.0 4.2 17.7 2.8 8.6 5.9 26.8 11.5 10.8 11.9 38.9 14.3 19. 13.3 14.9 11.5 53.7 33.9 51.0 62.7 61.7 38.0 12.9 61.6 37.1 8.8 42.1 24.5 10.6 5.6 5.0 1.8 12.3 11.6 19.2 49.3 10.6 15.0 37.6 53.2 63.6 68.8 72.0 74.1 69.7 86.3 39.3 11.3 66.6 78.2 60.1 46.6 67.8 62.9 71.3 62.2 42.8 44.7 40. 49.1 45.8 51.0 74.6 60.1 35.3 31.6 66.2 49.2 9.3 47.2 28.0 11.6 12.6 10.6 2.4 9.0 23.3 25.9 41.3 18.9 18.4 62.6 65.7 45.7 53.4 68.7 56.7 59.4 76.3 31.5 25.4 73.0 59.9 56.5 42.7 69.3 63.2 73.5 62.2 44.0 43.5 44.3 65.6 79.5 67.1 88.6 72.7 42.6 19.3 70.2 29.5 13.0 54. 63.2 50.2 78.4 40.8 59.0 47.7 43.4 39.2 60.7 66.1 54.9 73.3 47.8 67.2 70.7 70.6 66.5 66.1 88.1 12.9 56.6 47.3 79.9 62.3 67.3 84.7 79.2 86.8 79.5 60.1 66.5 52.0 58.8 71.3 53.7 85.0 70.0 43.0 36.1 71.6 55.8 14.7 56.0 73.3 56.7 78.5 39.3 41.7 49.5 45.2 51.7 59.0 79.0 57. 83.0 61.4 74.2 78.1 78.6 72.4 68.3 90.2 54.9 24.9 87.5 65.6 69.9 76.8 89.8 90.6 77.0 83.6 64.1 59.1 68.0 Table 10: The detailed results on the MMEB benchmark (Jiang et al., 2024b). We report the performance of our MMRet under both zero-shot and fine-tuning settings. Figure 5: The visualized examples of MegaPairs. Each row represents single example, with the query item highlighted in blue rectangle and the target items enclosed within dashed box. Figure 6: Top-5 retrieved images of MMRet and MagicLens on zero-shot CIR tasks, both using the CLIP-L backbone. Queries are shown with blue background, and the most correct retrieved images are marked with green outlines."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Beijing University of Posts and Telecommunications",
        "Shanghai Jiaotong University",
        "The Hong Kong Polytechnic University",
        "University of Science and Technology of China"
    ]
}