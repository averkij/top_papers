{
    "paper_title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
    "authors": [
        "Alireza Nadaf",
        "Alireza Mohammadshahi",
        "Majid Yazdani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes. KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance. Code Available at: https://github.com/Leeroo-AI/kapso"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 6 2 5 1 2 . 1 0 6 2 : r KAPSO: Knowledge-grounded framework for Autonomous Program Synthesis and Optimization Alireza Nadaf Alireza Mohammadshahi Majid Yazdani Leeroo Team {nadaf, alireza, my}@leeroo.com Abstract We introduce KAPSO, modular framework for autonomous program synthesis and optimization. Given natural-language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within long-horizon optimization loop, where progress is defined by evaluator outcomes. KAPSO targets long-horizon failures common in coding agentsincluding lost experimental state, brittle debugging, and weak reuse of domain expertiseby integrating three tightly coupled components. First, git-native experimentation engine isolates each attempt as branch, producing reproducible artifacts and preserving provenance across iterations. Second, knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance."
        },
        {
            "title": "Introduction",
            "content": "Domain experts often know what they want to build, but turning that intent into reliable, runnable, and optimized software still requires repeated experimentation. In practice, successful development is an iterative process: propose an approach, implement it, run it in the real environment, inspect outcomes, and refine. This loop is especially visible in data and AI programs, where progress depends on many measurable improvements and on careful management of code, data, and evaluation contracts. Importantly, iterations often succeed in the narrow sense of producing working artifact, yet still fall short on quality, accuracy, robustness, or efficiency. Practical progress therefore requires repeated evaluation and targeted improvement, not only error fixing. LLM-based coding agents reduce the cost of writing code, but they remain unreliable in longhorizon execution loops. Common failure modes include losing state across iterations, repeatedly triggering the same integration errors, and failing to reuse relevant engineering expertise even when it is available in repositories, documentation, internal playbooks, or prior attempts. In many real settings, the decisive advantage is not raw code generation, but the ability to consistently apply Equal contribution 1 expert-grade ideas and high-leverage engineering workflows, including environment setup, data contracts, evaluation harnesses, debugging procedures, and performance tuning. We present KAPSO, framework for execution-grounded program optimization under an explicit evaluator boundary. Given natural-language goal and evaluators, KAPSO runs an iterative evolve loop: it generates and selects improvement hypotheses, synthesizes and applies code edits, executes the resulting artifact, evaluates outcomes, and uses measured feedback to guide subsequent iterations. In KAPSO, program synthesis is not the endpoint; it is an operator within long-horizon optimization process where progress is defined by evaluator outcomes such as accuracy, robustness, efficiency, or preference-based quality. KAPSO integrates three tightly coupled components to make this optimization loop reliable and reusable. First, git-native experimentation engine isolates each attempt as branch, capturing code changes, logs, and evaluation outputs as reproducible artifacts with explicit provenance. Second, knowledge system ingests heterogeneous sources, including repositories, benchmark artifacts, internal playbooks, documentation, scientific papers, and web-derived material, and organizes them into structured representation that supports retrieval of ideas, implementations, heuristics, and environment constraints. This knowledge is hosted in MediaWiki, providing familiar interface for human review, curation, and human-in-the-loop iteration. We release complete knowledge package consisting of MediaWiki dump, Neo4j and Weaviate snapshots, and Docker-based deployment scripts that bring up the MediaWiki instance and all indices in reproducible configuration. Third, cognitive memory layer coordinates knowledge retrieval from the knowledge base and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. KAPSO is intentionally modular. It supports pluggable evaluators, knowledge backends, and coding agents, enabling the same optimization loop to be applied across domains where progress is defined by executable outcomes and measurable objectives. We instantiate this design and evaluate it on two complementary benchmarks, MLE-Bench and ALE-Bench, and we use these instantiations to study end-to-end performance. Beyond these benchmarks, the same interfaces generalize to additional tasks by swapping the evaluator and the knowledge sources. Contributions. This paper makes the following contributions: 1. An end-to-end framework for evaluator-grounded program optimization that improves runnable artifacts through iterative ideation, code synthesis/editing, execution, evaluation, and learning. 2. git-native experimentation engine that represents each attempt as an isolated, reproducible branch with explicit provenance. 3. knowledge acquisition and representation pipeline hosted in MediaWiki that converts heterogeneous sources into typed, workflow-oriented knowledge base usable during optimization. 4. cognitive memory system that combines knowledge retrieval with episodic learning from experiment traces to reduce repeated failures and accelerate iteration. 5. modular architecture with pluggable evaluators and knowledge sources, demonstrated through benchmark instantiations and ablations, together with released knowledge package containing MediaWiki dump, Neo4j and Weaviate snapshots, and Docker-based deployment scripts for reproducing the full stack. The released knowledge base is populated from over 2,000 widely used data and ML repositories, with selection criteria defined later."
        },
        {
            "title": "2 Framework Overview",
            "content": "KAPSO is designed around simple contract: given natural-language goal, optional knowledge sources, and an evaluator, the system produces runnable software artifact and improves it through measured iteration. In typical use, users interact with KAPSO through two core operations, evolve and deploy, and optionally use learn and research to expand or refresh the knowledge base. KAPSO exposes user-facing Kapso API with four operations: evolve: run an evaluator-grounded optimization loop that proposes improvements, applies code edits (including synthesis when needed), executes the artifact, and uses measured outcomes to guide subsequent iterations. deploy: adapt selected solution into target runtime strategy and return unified runtime handle. learn: ingest and curate user-provided knowledge sources (e.g., repositories, internal playbooks, benchmark artifacts) into unified knowledge base with both human-facing and retrieval-facing representations. research : discover domain-relevant external material (e.g., papers, documentation, web-derived notes, and candidate repositories) and return structured findings that can be used as context for evolve and as seeds for learn. In particular, learn is not required when suitable knowledge base already exists, and research is not required when the user already knows which sources to ingest. When sources are unknown or incomplete, research can be used to identify candidate materials, after which learn incorporates selected sources into the knowledge system for reuse across tasks. Figure 1: The overview of the framework architecture."
        },
        {
            "title": "2.1 Knowledge plane vs. execution plane",
            "content": "KAPSO separates knowledge from execution. The knowledge plane aggregates heterogeneous sources such as repositories, documentation, scientific papers, web-derived material, benchmark artifacts, and internal playbooks. It is hosted in MediaWiki for human review and curation, and it is indexed into pluggable retrieval backends (for example, graph and vector indices) to support machine retrieval at evolve time. In addition, KAPSO generates self-knowledge from its own work by extracting lessons from experiment traces and storing them in an episodic memory store. Implementation details of ingestion, indexing, and the released knowledge package are described in Section 4.2. The execution plane is anchored by an explicit evaluator boundary. ProblemHandler (and optionally an Evaluator) defines the task contract: how the artifact is executed, what outputs must be produced, and how quality is measured. Evaluators may be fully automated (metrics, tests, graders), may be stochastic, and may incorporate LLM-based judges or qualitative preference rules rather than single scalar objective. This boundary allows the same evolve loop to generalize across domains by swapping the evaluator while keeping orchestration logic unchanged."
        },
        {
            "title": "2.2 Evolve orchestration and modular subsystems",
            "content": "Internally, Kapso.evolve is implemented by an OrchestratorAgent that composes four pluggable subsystems. These subsystems are configured per task and can be replaced independently: SearchStrategy: proposes candidate solution specifications and selects which candidates to evaluate next. Concrete strategies (for example, linear search and tree-based search) are defined formally later in the paper. ContextManager: renders the context passed into solution proposal and implementation, including the task description, constraints, retrieved knowledge, episodic insights, and summaries of prior experiments. KnowledgeSearch: retrieves workflows, implementations, heuristics, and environment constraints from the knowledge base, and returns structured knowledge packets with provenance. CodingAgent: applies code changes to implement or debug candidate solution. The OrchestratorAgent drives an iterative loop that alternates between (i) constructing context from evaluator state, knowledge retrieval, episodic memory, and experiment history, and (ii) executing one or more isolated experiments to obtain measured outcomes."
        },
        {
            "title": "2.3 Deploy and the unified runtime interface",
            "content": "Today, deploy returns Python Software handle with unified run() interface, backed by an adapted copy of the solution repository. The Software handle exposes run(inputs) and returns \"success\", \"output\": ...} or {\"status\": normalized dictionary (for example, {\"status\": \"error\", \"error\": ...}), plus lifecycle methods such as start, stop, logs, and is_healthy. Depending on the selected strategy it executes locally (import-and-call) or via an HTTP or remote endpoint (for example, Docker, Modal, BentoCloud, or LangGraph Platform), while preserving the same run() contract for callers. Implementation details of repository adaptation and strategy adapters are described in Section 4.4."
        },
        {
            "title": "2.4 Experiment artifacts and provenance",
            "content": "KAPSO represents each experiment as an isolated branch and persists run artifacts sufficient to reproduce, debug, and reuse successful attempts. The experiment artifact model and branch publication behavior are implemented by the experimentation engine described in Section 4.1."
        },
        {
            "title": "3 Formalization (Notation and Algorithms)",
            "content": "This section introduces notation and formal operators for the KAPSO framework. The goal is to make the system comparable to prior work in program synthesis, agentic search, and black-box optimization by stating (i) the objects KAPSO manipulates, (ii) the evaluation contract it relies on, and (iii) the algorithms it runs. Benchmarks such as MLE-Bench and ALE-Bench are treated later as concrete instantiations of the evaluator contract."
        },
        {
            "title": "3.1 Evolve instance and evaluator contract",
            "content": "A KAPSO run is defined by natural-language goal g, budget specification (time, iterations, and/or cost), and an evaluator contract E. The evaluator contract defines how artifacts are executed, measured, compared, and when the run should stop: Budget progress: βi [0, 1] is the normalized budget progress at outer iteration i. Problem context: (β) returns the context shown to the system at budget progress β (optionally budget-aware). Execution and measurement: Run(c; θ) executes code artifact under evaluator configuration θ and returns measurement record. Selection rule: either (i) scalar utility mapping over measurement records, or (ii) preference relation over measurement records. This rule may be implemented by automated metrics, test suites, rule-based comparators, an LLM-based judge, or human-in-the-loop policy. Stochastic aggregation: AggR and AggJ aggregate stochastic rollouts into an aggregated record and an aggregated utility estimate, respectively. Stopping: Stop(β, H) optionally terminates the loop based on budget progress and experiment history. code artifact denotes an executable repository state together with sufficient entrypoint/configuration to run under the evaluator. single evaluator execution returns measurement record R(c) = (status, (c), (c), A(c)), where status indicates success or error, (c) denotes quantitative measurements (a scalar or vector of metrics), (c) denotes qualitative feedback (free-form text or structured diagnostics, including judge rationales), and A(c) denotes auxiliary artifacts such as logs, traces, or produced files."
        },
        {
            "title": "3.2 Objective, feasibility, and stochastic evaluation",
            "content": "KAPSO improves artifacts according to the evaluators selection rule. In the common case, the evaluator provides scalar utility mapping (R(c)) R. When execution is stochastic, the measurement record R(c) is random variable induced by evaluator randomness, and the conceptual optimization target is the expected utility (c) := E[U (R(c))] . This definition avoids ambiguity between (E[R]) and E[U (R)] when is nonlinear or when contains more than single scalar. KAPSO does not observe (c) directly. Instead, the evaluator runs rollouts, where is part of evaluator configuration θ. Let R(k)(c) be the record returned by rollout k. The evaluator defines an aggregation operator that produces an aggregated record bRK(c) := AggR (cid:17) (cid:16) R(1)(c), . . . , R(K)(c) , and an aggregation operator that produces an aggregated utility estimate bJK(c) := AggJ (cid:16) (cid:17) (R(1)(c)), . . . , (R(K)(c)) . In many instantiations, AggJ is the sample mean and bJK(c) is Monte Carlo estimator of (c). The status field defines feasibility under the evaluator contract. Errors correspond to infeasible artifacts. The evaluator selection rule handles infeasibility by ensuring feasible artifacts are ranked above infeasible artifacts, for example by assigning sentinel utility to errors in (such as in maximize setting) or by defining to always prefer feasible records over error records. When an evaluator does not naturally provide scalar utility, it instead provides preference relation over (aggregated) measurement records, potentially using multi-metric and qualitative feedback. In that case KAPSO selects artifacts by comparing bRK() under ."
        },
        {
            "title": "3.3 Experiments, provenance, and history",
            "content": "KAPSO maintains an explicit history of executed experiments. Each experiment corresponds to an isolated execution in an experiment branch and records the motivating specification, the produced artifact, and the measured outcomes. Let denote branch identifier. We define the corresponding artifact as the repository state checked out from that branch: After iteration i, the experiment history is := RepoState(b). Hi = {e1, . . . , ei1}, with ej = (bj, uj, βj, cj, Kj, bRKj (cj), bJKj (cj)), where cj := RepoState(bj). Here uj is the solution specification used to generate edits for branch bj, βj is the budget progress at which the experiment was executed, and Kj is the rollout count used by the evaluator. When selection is defined purely by preference relation , bJ can be omitted and KAPSO selects using bR directly."
        },
        {
            "title": "3.4 Knowledge grounding via seed repositories and retrieval",
            "content": "KAPSO grounds optimization in two complementary assets: (i) seed repository that provides an initial runnable codebase when available, and (ii) typed knowledge graph that provides reusable principles, implementations, heuristics, and environment constraints as auxiliary context. Repository corpus and seeding. Let denote corpus of candidate repositories (ingested via learn and indexed for retrieval). repository includes its content plus metadata (e.g., tags, language, dependencies, and embedding-based representations). KAPSO uses repository retrieval function RepoRetrieve(g) (ˆr, ρ), which returns candidate seed repository ˆr and confidence score ρ [0, 1]. If ρ exceeds threshold τ , KAPSO initializes the run from ˆr; otherwise it initializes from an empty (or template) scaffold: cinit := (RepoState(ˆr) Scaffold(g) if ρ τ, otherwise. Operationally, cinit becomes the parent state for the first experiment branch. Typed knowledge graph and retrieval. Let = (V, EK) be typed knowledge graph where each node corresponds to typed wiki page and has an ID, title, type in {Principle, Implementation, Environment, Heuristic}, content text (and optionally code snippets), and typed edges in EK (for example IMPLEMENTED_BY, USES_HEURISTIC, REQUIRES_ENV, and crossreference edges). Given the goal g, an optional seed repository ˆr (possibly ), and an optional failure signal from the latest experiment (e.g., error trace, contract violation, or qualitative evaluator feedback), KAPSO retrieves auxiliary knowledge via: RetrieveKnowledge(g, ˆr, s) K. Base knowledge retrieval (KR). The base retrieval objective selects and returns bounded set of relevant pages (principles, implementations, heuristics, and environments), optionally conditioned on the seed repository: Kbase(g, ˆr) := KR(g, ˆr). Conditioning on ˆr allows KAPSO to surface repo-specific constraints (dependencies, entrypoints, idioms) and to prioritize knowledge that is compatible with the starting codebase. Error-recovery augmentation (ERA). After failed experiment or repeated contract violations, KAPSO augments the retrieved packet with failure-conditioned heuristics, diagnostics, and alternative implementations: := (ERA(g, s, ˆr, Kbase(g, ˆr)) Kbase(g, ˆr) if = , otherwise. Knowledge packet. The output is knowledge packet (implemented as KGKnowledge). It includes: (i) an optional seed repository reference ˆr with confidence ρ, (ii) set of retrieved pages grouped by type, (iii) confidence scores and provenance metadata (including query_used and source_pages), and (iv) optional recovery attachments when ERA is applied."
        },
        {
            "title": "3.5 Core solve loop (Orchestrator)",
            "content": "At the top level, KAPSO repeatedly builds context and executes experiments until stop condition fires. In the codebase this corresponds to OrchestratorAgent.solve() as invoked by Kapso.evolve plus chosen SearchStrategy. Algorithm 1: KAPSO solve loop (orchestrator level) Inputs: - evaluator contract = (P, Run, Stop, Select, Agg) - search strategy (linear or tree) - context manager - budgets Initialize = 0 Initialize history H_0 = empty while < N: beta_i = budget_progress(B, i) if Stop(beta_i, H_i) or beta_i >= 1: break x_i = M.get_context(beta_i) # x_i includes P(beta_i), knowledge, episodic memory, and H_i new_experiments = S.run(x_i, beta_i) H_{i+1} = H_i union new_experiments = + 1 return best artifact in H_i under evaluator selection rule This formalization makes explicit that KAPSO is search over executable artifacts driven by repeated experiments and evaluator-defined assessment."
        },
        {
            "title": "3.6 Implement-and-debug loop (SearchStrategy)",
            "content": "Both linear and tree search ultimately execute the same inner loop: create an isolated branch, implement solution, run it, and optionally debug it for bounded number of tries. The debug loop is intended to repair execution failures and evaluator-detected contract violations (for example, incorrect output format or missing required files), not to perform objective optimization. Algorithm 2: Implement-and-debug loop (branch level) Inputs: - solution spec - context (problem + knowledge + episodic memory + history) - debug budget - branch name and parent branch session = create_experiment_session(branch=b, parent=p) # codegen + Run = implement_solution(u, x, session) for in 1..D: if r.has_error_or_contract_violation: = debug_solution(u, x, r, session) 8 else: break finalize_session(session) return # commits + push + cleanup"
        },
        {
            "title": "3.7 LLM-steered tree search (one concrete instantiation)",
            "content": "For completeness, we formalize an LLM-steered tree search instantiation used in some KAPSO configurations. Let = (N , A) be tree of nodes , where each node stores solution specification u(n) and an optional experiment outcome derived from executing the corresponding artifact. At each outer iteration, the strategy: Prune: optionally terminates some leaf nodes using an LLM conditioned on (P (βi), Ki, Hi). Expand: chooses nodes to expand (exploration versus exploitation) and generates new child solution specifications via an LLM ensemble. Select: selects top-k leaf nodes to execute as experiments, conditioned on the rendered context. This can be viewed as learned proposal distribution over solution specifications combined with black-box evaluation through Run() and selection under or ."
        },
        {
            "title": "3.8 Cognitive memory (cascaded retrieval, episodic learning, decisions)",
            "content": "KAPSO maintains an episodic memory store of reusable lessons extracted from experiment traces (run logs, diffs, and evaluator feedback). After each experiment, the system updates episodic memory, retrieves relevant prior lessons, and decides whether to continue iterating on the current workflow or pivot to alternative knowledge. Let the controller state be Ci containing the goal g, the current knowledge packet Ki, the latest experiment record ei1, retrieved episodic insights Zi, and meta statistics (for example, consecutive failures). The controller defines: RetrieveCascade(g, s) Ki: cascaded retrieval using WSR with PFR fallback, plus ERA augmentation when indicates failure or contract violation, UpdateEpisodic(ei1) E: store generalized lessons from errors and qualitative feedback, π(Ci) {Retry, Pivot, Complete}: policy over iteration-level actions. Algorithm 3: Cognitive controller step (per experiment) Inputs: goal g, current knowledge K, episodic memory E, last experiment if e.has_error_or_contract_violation: E.add(ExtractIssue(g, e)) = ERA(g, e, K) # recovery heuristics + alternatives with provenance else if e.feedback is non-empty: E.add(ExtractInsight(g, e.feedback)) = RetrieveEpisodic(E, g, e) = DecideAction(g, K, e, Z) # RETRY / PIVOT / COMPLETE 9 if == PIVOT: = RetrieveCascade(g, s=None) # exclude current workflow in implementation return (a, K, Z) The key property is that both the coding agent and the decision maker are grounded in the same rendered context, and ERA explicitly records provenance via query_used and source_pages."
        },
        {
            "title": "4 System Implementation",
            "content": "This section describes the implementation of the mechanisms defined in Section 3. The key design principle is that framework semantics (artifact, evaluator contract, experiment history, seed repository selection, knowledge retrieval, and controller loops) are fixed, while concrete implementations of execution, storage, indexing, and adapters remain modular."
        },
        {
            "title": "4.1 Experimentation Engine",
            "content": "The experimentation engine is the mechanism that realizes KAPSOs experiment history and provenance model from Section 3. Its primary role is to isolate attempts, make outcomes reproducible, and enable reuse of successful attempts as parents for subsequent experiments. 4.1.1 Experiment sessions as git branches KAPSO represents each experiment as git branch inside an ExperimentWorkspace. An ExperimentSession starts from parent branch, creates new branch identifier b, applies edits using the configured CodingAgent, and executes the evaluator through the active ProblemHandler. The session then commits the resulting artifact state and run outputs to the branch. This makes each experiment concrete and inspectable: branch can be checked out and re-executed to reproduce the same evaluator interaction and artifacts. To support downstream reuse, ExperimentSession.close_session() always attempts git push origin <branch>. In typical deployments, origin is not network remote. Sessions are cloned from file://<base_repo.working_dir>, where base_repo is the initialized parent artifact for the run (either retrieved seed repository or scaffold). Pushing primarily publishes the branch back into the local ExperimentWorkspace repository. This ensures that newly created branches are immediately available as parents for child experiments in tree-based exploration. It is not guaranteed to publish to GitHub or any external remote. Each experiment persists lightweight, reproducible bundle sufficient for debugging and audit. Concretely, the committed artifacts include: the code changes relative to the parent branch (diffs and commit identifiers), evaluator configuration used for the run (including rollout count when stochastic aggregation is enabled), run logs and structured diagnostics, and evaluator-produced artifacts (for example output files and traces). 4.1.2 Execution environment and scaling Experiments execute inside the active ProblemHandler. In the default path, GenericProblemHandler runs the artifact via local subprocess, and no containerization is introduced by the experimentation engine itself. Containerized execution is used only when the evaluator requires it. For example, ALE evaluation is performed by an external evaluation harness that runs solutions in Docker, while MLE runs the Python entrypoint locally inside the benchmark environment. This division keeps the experimentation engine evaluator-agnostic while still supporting evaluators with strict runtime requirements. The engine supports executing multiple ExperimentSessions concurrently, which is used by search strategies that evaluate multiple candidates in parallel. While the current implementation primarily runs locally (or via evaluator-required containers), the design admits pluggable execution backend. In particular, the same session and artifact model can be paired with remote executor to run resource-intensive workloads (for example GPU-heavy training) on appropriate machines, while preserving the branch-based provenance that downstream search and reuse depend on."
        },
        {
            "title": "4.2 Knowledge System",
            "content": "The knowledge system converts heterogeneous sources into (i) repository corpus used for initialization and reuse, and (ii) typed knowledge base indexed for retrieval. MediaWiki provides familiar interface for human review, curation, and editing, while the same content is served to agents through retrieval APIs. The retrieval semanticsseed repository selection plus typed knowledge retrieval with failure-conditioned augmentation (ERA)are defined in Section 3.4. This subsection describes how those semantics are implemented and how repositories and knowledge are acquired, represented, and indexed. Knowledge acquisition is orchestrated by KnowledgePipeline that produces typed wiki pages (WikiPage) and maintains RepoStore containing versioned references to ingested repositories. For repositories, RepoIngestor follows repository-centric pipeline designed for two outputs: (i) runnable seed codebase (the repository snapshot itself, referenced by URL and commit metadata), and (ii) structured knowledge extracted from that repository for reuse across tasks. Concretely, the ingestor identifies entrypoints, dependency and environment requirements, and high-signal implementation patterns; extracts reusable principles, implementations, heuristics, and environment constraints; and attaches provenance linking each extracted item back to repository paths and commits. In addition, the pipeline performs deterministic file mining to surface useful files that are not emphasized in READMEs (for example scripts, configs, evaluation utilities, deployment manifests), triages them, and emits corresponding typed pages when they contain reusable guidance. Knowledge is represented as typed pages with explicit, typed links. KAPSO uses four primary page types: Principle, Implementation, Environment, and Heuristic. Links between pages become typed edges (for example IMPLEMENTED_BY, USES_HEURISTIC, and REQUIRES_ENV). Extracted pages carry repository provenance and optional repository associations (for example SOURCE_REPO metadata and RELATED_REPO links) to enable repo-conditioned retrieval. This structure is designed to return bounded, typed knowledge packets that can be consumed by the evolve loop without requiring workflow synthesis. For machine retrieval, the wiki and repository corpus are indexed into pluggable backends. The reference implementation ships with typed graph index (default Neo4j) and vector index (default Weaviate), while allowing alternative graph stores and vector databases behind the same interfaces. Retrieval is implemented in two stages. First, RepoRetrieve service selects an optional seed repository by searching over the repository corpus using hybrid signals (for example embeddings over 11 READMEs and file trees, plus metadata filters) and returns candidate seed with confidence score. Second, KnowledgeRetrieve service retrieves relevant principles, implementations, heuristics, and environment constraints from the typed indices, optionally conditioned on the selected seed repository. After failures or repeated contract violations, the retrieval service applies error-recovery augmentation (ERA) to attach failure-conditioned heuristics and alternative implementations. Across all modes, the system records provenance into the returned knowledge packet (including query_used, source_pages, and optional seed repository identifiers) to support auditability and reproducibility. We release complete knowledge package consisting of MediaWiki dump, Neo4j and Weaviate snapshots, and Docker-based deployment scripts that bring up the MediaWiki instance and all indices in reproducible configuration. The released package also includes manifest of the repository corpus used for seeding and extraction (for example URLs, commit identifiers, and selection criteria), enabling point-in-time reconstruction of the repository set."
        },
        {
            "title": "4.3 Cognitive Memory System",
            "content": "The cognitive memory system implements episodic memory and controller decisions as formalized in Section 3. Its goal is to reduce repeated error modes, preserve high-value lessons from prior attempts, and make long-horizon iteration more stable by conditioning future proposals and debugging on prior experience. Episodic memory entries are derived from experiment traces and are designed to be reusable across tasks. Each entry includes compact trigger description, generalized lesson, recommended actions, and provenance linking back to the originating experiment branch and its artifacts (for example logs, diffs, and evaluator feedback). This provenance makes the memory auditable and supports inspection when retrieved lesson influences new change. After each experiment, KAPSO performs lesson extraction. When the experiment indicates an execution error or contract violation, the system generalizes the failure into reusable fix pattern grounded in the observed trace and validator feedback. When the experiment succeeds or improves quality, the system extracts best-practice insights from measured outcomes and qualitative feedback, including judge rationales when an LLM-based evaluator is used. Extracted lessons are stored in vector database (default Weaviate) with JSON fallback, enabling semantic retrieval by goal and by failure signal. On subsequent iterations, KAPSO retrieves relevant episodic memories conditioned on the current goal and the latest experiment signal. Retrieved lessons are rendered into the unified context passed to the search strategy and coding agent, alongside the current knowledge packet produced by repository selection and typed knowledge retrieval. This ensures that both proposal and debugging steps are grounded not only in domain knowledge (the wiki and indices) but also in the systems own prior experience on similar issues. Finally, the controller implements an iteration-level decision policy as in Algorithm 3 in Section 3. It uses the goal, the current knowledge packet, the latest experiment record, and retrieved episodic insights to decide whether to retry, pivot, or complete. On pivot, the implementation re-runs repository retrieval and knowledge retrieval while excluding the currently selected seed repository (when one was used), encouraging exploration of alternative starting codebases or fallback to an unseeded scaffold."
        },
        {
            "title": "4.4 Deployment Interface",
            "content": "KAPSO provides unified deployment interface that packages selected solution artifact into runnable form while preserving stable invocation contract. The user-facing contract is described in Section 2.3; this subsection describes the implementation mechanics that realize that contract across deployment strategies. Kapso.deploy(...) returns Python object implementing the Software interface. The returned handle exposes stable run(inputs) method and lifecycle methods (for example start, stop, logs, and is_healthy). The key implementation goal is to keep this contract invariant while allowing strategy-specific execution details to vary. Deployment is done via repository adaptation. Given selected solution repository, it creates an adapted copy at path of the form <solution.code_path>_adapted_<strategy>. The adapter injects strategy-specific runtime wrappers (for example, container or service scaffolding or platform configuration) and emits run interface descriptor that specifies how to invoke the artifact (for example an endpoint URL, module path, or callable reference). The Software handle uses this descriptor to route run() to the appropriate local or remote mechanism. The current implementation supports multiple strategies under the same Software interface. In LOCAL mode, the runner imports and calls function inside the adapted repository (default main.predict). In DOCKER mode, the system builds or reuses Docker image and runs local container exposing an HTTP endpoint (default http://localhost:8000/predict). In MODAL mode, it generates modal_app.py and invokes remote Modal function. In BENTOML mode, it generates BentoML service files and can optionally deploy to BentoCloud, returning an HTTP endpoint. In LANGGRAPH mode, it generates LangGraph deployment files and the runner connects to LangGraph Platform URL to invoke the deployed agent. Across all strategies, the caller interacts only with Software.run(), while lifecycle methods expose health and logs in strategy-appropriate way. The deployment layer is extensible. New strategies can be added by implementing an adapter that (i) produces the required runtime wrapper files, (ii) emits the run interface descriptor, and (iii) registers how the Software handle should execute run() and lifecycle methods for that strategy."
        },
        {
            "title": "5 Evaluation",
            "content": "We evaluate KAPSO on two benchmarks that capture distinct real-world software-building regimes."
        },
        {
            "title": "5.1 MLE-Bench (Kaggle-style ML competitions)",
            "content": "Task: produce Python solution that trains on provided competition data and writes final_ submission.csv in the expected format. Execution protocol (as implemented in the benchmark handler): run debug mode first (python main.py debug) with strict runtime cap, validate the submission file format, run full mode (python main.py) with longer runtime budget, grade the output submission on the competitions train/test split of public train dataset. 13 Stop condition: Run for 24 hours or until maximum budget of $200 is reached. Also, stop early if the run achieves any medal according to the MLE-Bench grading library. Reported metrics: Private score: returned by the benchmark grader. Medal rate: fraction of competitions where any medal is achieved in 4 categories: low, medium, hard, and all. Results: Table 1 indicates that Leeroo consistently outperforms leading open-source agent frameworks, and its advantage becomes more evident as the difficulty of tasks increases. Although performance in Low-difficulty tasks is identical between Leeroo and the top-tier open-source baseline, R&DAgent (both achieving 68.18%), Leeroo achieves substantially higher accuracy on Medium and Hard problems, reaching 44.74% and 40.00%, respectively. In comparison, the highest-performing open-source agent, R&D-Agent, attains only 21.05% on Medium and 22.22% on Hard tasks. These results suggest that while open-source scaffolding is competitive on standard problems, Leeroos capabilities transfer more effectively to complex machine learning challenges involving high levels of specialization and long-horizon engineering. Agent Leeroo R&D-Agent[3] AIRA-dojo[4] ML-Master AIDE Low (%) Medium (%) Hard (%) All (%) 68.18 2.62 44.74 1.52 40.00 0.00 50.67 1.33 35.11 0.44 68.18 2.62 31.60 0.82 55.00 1.47 29.33 0.77 48.48 1.52 17.12 0.61 35.91 1.86 21.05 1.52 21.97 1.17 20.18 2.32 8.45 0.43 22.22 2.22 21.67 1.07 24.44 2.22 11.67 1.27 Table 1: Performance comparison of Leeroo against the top four open-source agent frameworks on MLE-bench. See the official MLE-Bench repo for the most up-to-date results."
        },
        {
            "title": "5.2 ALE-Bench (AtCoder heuristic contests)",
            "content": "Task: produce C++ (cpp23) solution in main.cpp to maximize/minimize contest-defined score under strict runtime limit of each competition. Execution protocol: compile and run in the benchmarks Docker environment, if the solution is accepted, run it multiple times and average the public evaluation score to reduce the effect of randomness. Run the private evaluation for the experiment with the highest public score and report final performance and rank percentile. Reported metrics: Final performance: Final ELO rating of the submission for the competitions. 14 Rank percentile (RP): final_rank / number_of_contestants, Compares the agents performance with the human-level performance. Private absolute score (contest objective value). Cost: cumulative LLM usage cost. Results: Table 2 shows that Leeroo achieves the highest final performance on the ALE Bench, scoring 1909.4 with rank percentile of 6.1%, outperforming the original ALE-Agent (1879.3, 6.8%). Notably, Leeroo does this while keeping total cost relatively low at $914.8, compared to ALE-Agents $1003.3, indicating more cost-efficient approach. The competition-level results in Table 3 further highlight Leeroos advantage: it surpasses ALE in most AHC competitions, sometimes by large marginse.g., ahc016 (2022 vs. 1457) and ahc026 (2040 vs. 1965)while maintaining comparable rank percentiles. These outcomes demonstrate that Leeroo not only achieves higher absolute performance but also generalizes more reliably across diverse competitions, combining effectiveness with efficiency, which makes it more impactful agent for real-world ALE tasks. One limitation of the current results is that the relatively low number of competitions in ALEBench may introduce noise, as the performance of LLMs and agents can vary across tasks. For example, in competition ahc039, ALE-Agent achieves notably high score, but this performance is not consistently reflected in other similar short competitions. Future studies including larger set of competitions, running with multiple seeds, could provide more robust and reliable comparison between agents. Agent Leeroo ALE-Agent [5] ALE Sequential [5] ALE one-shot [5] Final Performance Rank Percentile (%) Total Cost ($) 1909.4 1879.3 1198 832 6.1 6.8 54.1 88.4 914.8 1003.3 111.0 4.7 Model Gemini-2.5-pro Gemini-2.5-pro Gemini-2.5-pro Gemini-2.5-pro Table 2: Aggregated results on ALE Bench. Competition ALE Leeroo Final Performance RP (%) Final Performance RP (%) ahc008 ahc011 ahc015 ahc016 ahc024 ahc025 ahc026 ahc027 ahc046 ahc039 1189 1652 2446 1457 1980 1331 1965 1740 2153 52.06 20.30 3.85 33.05 13.10 47.00 16.00 18.12 8.95 0.73 1221 1607 2528 2022 1980 1353 2040 1839 2194 2310 49.03 22.35 2.82 9.17 13.10 46.30 12.43 14.91 8.09 5.27 Table 3: Comparison of ALE and Leeroo performance across AHC competitions."
        },
        {
            "title": "6 Conclusion",
            "content": "KAPSO is framework for building software by running evaluator-grounded experiments, using structured knowledge, and learning from episodic experience. Its core contributions are git-native experimentation engine, scalable knowledge acquisition and retrieval system with cascaded retrieval (WSR with PFR fallback, plus ERA augmentation), and workflow-aware cognitive memory layer that stores and reuses lessons from experiment traces. The framework is designed to be modular and auditable, with clear evaluator contract and reproducible artifacts. We provide clear execution protocol for MLE-Bench and ALE-Bench to enable reproducible evaluation."
        },
        {
            "title": "References",
            "content": "[1] J. Nam, J. Yoon, J. Chen, J. Shin, S. Ö. Arık, and T. Pfister, MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement, arXiv:2506.15692, 2025. https://arxiv. org/abs/2506.15692 [2] InternAgent Team et al., InternAgent: When Agent Becomes the Scientist Building Closed-Loop System from Hypothesis to Verification, arXiv:2505.16938, 2025. https://arxiv.org/abs/2505. 16938 [3] X. Yang et al., R&D-Agent: An LLM-Agent Framework Towards Autonomous Data Science, arXiv:2505.14738, 2025. https://arxiv.org/abs/2505.14738 [4] E. Toledo et al., AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench, arXiv:2507.02554, 2025. https://arxiv.org/abs/2507. [5] Y. Imajuku, K. Horie, Y. Iwata, K. Aoki, N. Takahashi, and T. Akiba, ALE-Bench: Benchmark for Long-Horizon Objective-Driven Algorithm Engineering, arXiv:2506.09050, 2025. https: //arxiv.org/abs/2506."
        }
    ],
    "affiliations": [
        "Leeroo Team"
    ]
}