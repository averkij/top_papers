{
    "paper_title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
    "authors": [
        "Jaewoo Lee",
        "Archiki Prasad",
        "Justin Chih-Yao Chen",
        "Zaid Khan",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines."
        },
        {
            "title": "Start",
            "content": "PRINTS: Reward Modeling for Long-Horizon Information Seeking Jaewoo Lee 1 Archiki Prasad 1 Justin Chih-Yao Chen 1 Zaid Khan 1 Elias Stengel-Eskin 2 Mohit Bansal"
        },
        {
            "title": "Abstract",
            "content": "Information-seeking is core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step informationseeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of informationseeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRINTS, generative PRM trained with dual capabilities: (1) dense scoring based on the PRMs reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRINTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with much smaller backbone agent and outperforming other strong reward modeling baselines.1 5 2 0 2 4 2 ] . [ 1 4 1 3 9 1 . 1 1 5 2 : r 1. Introduction long-standing goal in artificial intelligence has been to develop agents that can answer novel queries by intelligently seeking information (Bachman et al., 2016; Yuan et al., 2020), thereby enabling them to tackle challenging 1University of North Carolina at Chapel Hill 2University Jaewoo Lee of Texas at Austin. <jwoolee@cs.unc.edu>, Archiki Prasad <archiki@cs.unc.edu>. Correspondence to: Preprint. November 25, 2025. 1Code available at https://github.com/G-JWLee/PRInTS 1 tasks in mathematics (Liu et al., 2025a;b), software engineering (Yang et al., 2025b; Pan et al., 2024), and research (Li et al., 2025b; Wu et al., 2025a). Large Language Models (LLMs) have shown promise as agents for such tasks when equipped with frameworks like ReAct (Yao et al., 2023), which interleaves LLM reasoning with external tool interactions. However, long-horizon information-seeking tasks, which require agents to gather and synthesize information across multiple steps (Su et al., 2025; Shao et al., 2025), remain challenging, even for recent LLMs with tooluse training, performing far below human-level (Mialon et al., 2024; Wei et al., 2025). While finetuning LLMs as information-seeking agents has shown promise (Li et al., 2025b; Wu et al., 2025a; Tao et al., 2025), it is limited to specific model families and is highly computationally demanding (Gao et al., 2025). An alternative way to boost variety of agents is to build reward models (e.g., as done for math reasoning and instruction following (Wang et al., 2024a; Zou et al., 2025)). These models approximate the expected reward of step or sequence of steps, enabling test-time scaling by ranking and selecting higher-quality actions or trajectories to successfully tackle long-horizon tasks. Specifically, Process Reward Models (PRMs) (Zou et al., 2025; Choudhury, 2025; Chae et al., 2025) offer promising model-agnostic way of improving performance, scoring the quality of each of an agents steps. While past work has developed PRMs for tasks such as mathematics and logical reasoning, these methods are insufficient for long-horizon information-seeking tasks for two critical reasons. (1) Tool-Reasoning Evaluation Granularity: existing PRMs evaluate short reasoning units in isolation, typically oneto two-sentence logical or mathematical inferences (Xiong et al., 2025; Zhao et al., 2025), providing binary judgments based on logical/math validity. In contrast, long-horizon information-seeking requires jointly evaluating complete trajectory step, which encompasses reasoning step combined with tool interactions (e.g., web search, web browsing, code execution). Moreover, step quality depends on multiple factors (e.g., interpretation of tool outputs, tool call informativeness, plan for next action) that coarse feedback cannot capture, increasing the granularity of the guidance needed to effectively steer agents toward (2) Context Accumulation: existing good trajectories. PRMs cannot manage the ever-growing reasoning context PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 1. Comparison between existing PRMs and PRINTS. Top: Existing PRMs are limited for long-horizon information-seeking as they evaluate short reasoning unit (e.g., one-to-two-sentence inferences) with coarse feedback, which cannot capture multi-faceted quality factors from tool interactions. They also struggle with rapidly accumulating reasoning context (left). Bottom: In contrast, PRINTS evaluates complete trajectory step (reasoning + tool interactions), considers multiple trajectory step quality dimensions to produce dense scores for finer-grained guidance at each step, and maintains compact trajectory summaries that keep key information for the evaluation. that arises over multiple trajectory steps. As illustrated in Figure 1 (Top-left), the information-seeking trajectory interleaving reasoning steps, tool calls, and tool call outputs grows rapidly as tool responses at each step introduce lengthy, noisy content, creating computational overhead. Furthermore, recent studies show that models struggle to process long, accumulated contexts (Tang et al., 2025; Yen et al., 2025; Kuratov et al., 2024), resulting in noisy evaluations. This necessitates compressing trajectories into compact forms instead of processing entire historical contexts. Our work aims to fill these gaps by introducing Progress Reward via Information gain scoring and Trajectory Summarization (PRINTS), novel generative PRM for long-horizon information-seeking tasks. PRINTS is unified model jointly trained with two key abilities to address both the need for fine-grained guidance and the challenge of context accumulation. These two abilities are learned jointly within the same PRM. First, PRINTS acts as scorer that evaluates candidate next trajectory steps by generating Chain-of-Thought (Wei et al., 2022) analyses across multiple quality dimensions and outputting dense scores derived from this generative reasoning, as illustrated in (Figure 1 (Bottom)). Crucially, we frame step evaluation as information gain estimation that quantifies how much each trajectory step increases the probability of reaching the correct answer. This formulation enables training via reinforcement learning with information gain estimation and preference prediction objectives, providing richer reward signals that account for the multi-faceted quality of trajectory steps. At test-time, PRINTS evaluates candidate next steps, selecting the step expected to yield the greatest information gain. Second, PRINTS simultaneously functions as summarizer that recursively generates and updates compact trajectory summary at each step. PRINTS compresses the query, previous summary, latest tool response, and current step into an updated summary that captures essential findings and plans up to the current timestep (Figure 1 (Bottom)). This keeps input length bounded, as shown in Figure 1 (Bottom-left), while preserving information for subsequent evaluation. To equip PRINTS with these dual capabilities, we first design preference and summary data that can produce supervision signals needed to train the scoring and summarization components. Specifically, our annotation pipeline (Figure 3) uses Monte Carlo rollouts (Wang et al., 2024a; Setlur et al., 2025) to estimate information gain scores and construct preference trajectory step pairs, and generates compact trajectory summaries for each step. Next, we use this annotated data to train PRINTS via reinforcement learning for scoring ability with two complementary rewards: (1) Score Reward that teaches the model to analyze the trajectory step quality and estimate the steps information gain score, and (2) Comparison Reward that teaches the model to assign higher scores to preferred trajectory steps by learning from pairwise preferences. These rewards enable the model to capture the multi-faceted quality of the trajectory step and perform dense step-level evaluation. We jointly train PRINTS via supervised fine-tuning for summarization that recursively updates the trajectory summary based on the previous summary and most recent reasoning context at each step, directly addressing the context accumulation challenge while preserving key information needed for step-level evaluation. Together, this pipeline enables PRINTS to serve as unified PRM capable of both managing long, noisy trajectories and providing fine-grained test-time guidance. We validate our approach across three distinct LLMs used as information-seeking agents: Qwen3-32B (Yang et al., 2025a), Tongyi DeepResearch-30B-A3B (Li et al., 2025a) specialized information-seeking agent and Gemini-2.5-Flash (Gemini Team, 2025), evaluated on three long-horizon information-seeking benchmarks: FRAMES, GAIA, and WebWalkerQA. The experimental results show that PRINTS, 4B PRM, consistently provides testtime gains across diverse agents Qwen3-32B by 9.3%, PRINTS : Reward Modeling for Long-Horizon Information Seeking DeepResearch-30B-A3B by 3.9%, and Gemini-2.5-Flash by 4.0% absolute average accuracy without fine-tuning the underlying models. Unlike existing PRMs, which obtain diminished and inconsistent gains as agents become stronger, PRINTS continues to deliver substantial improvements. Notably, on GAIA (levels 1-3) (Mialon et al., 2024), PRINTS raises DeepResearch-30B-A3B from 61.9% to 64.4% in our implementation, enabling the 30B agent augmented with the 4B PRM to match the performance of OpenAI DeepResearch (67.4%) and DeepSeek-V3.1-671B (63.1%) (Li et al., 2025a). Furthermore, our ablation studies reveal that providing compressed summaries outperforms using raw trajectories as input context, showing that context management is essential for accurate step-level evaluation in long-horizon tasks. Overall, our approach enhances information-seeking abilities of pretrained open-source models as well as specialized agents. showing strong generalizability. 2. Related Work Large Language Models (LLM) as Agents. LLMs have been increasingly adopted as agents through frameworks such as ReAct (Yao et al., 2023), which integrates reasoning with external tool interactions to solve complex tasks (Deng et al., 2025; Wu et al., 2025c). To facilitate effective information-seeking behaviors that require multiple reasoning steps and tool interactions to reach final answer, recent studies (Li et al., 2025b; Tao et al., 2025; Li et al., 2025a) aim to improve the intrinsic quality of informationseeking trajectories. For instance, both WebSailor (Li et al., 2025b), WebShaper (Tao et al., 2025), DeepResearch (Li et al., 2025a) agents synthesize large-scale QA training data by injecting uncertainty into structured knowledge to teach LLMs to reduce the search space by discovering unknown variables. However, training LLMs for long-horizon tasks shares key limitations: (1) it requires substantial supervision (MiroMind AI Team, 2025; Li et al., 2025a) and (2) it requires access to model weights, which poses challenges for generalization whenever the underlying model changes. We empirically show that PRINTS enhances agent performance, indicating that test-time guidance and agent finetuning are orthogonal yet mutually beneficial directions for improving information-seeking capabilities. Reward Models for Reasoning. To measure reasoning quality, Outcome Reward Models (ORMs) are used to predict the correctness of complete reasoning trajectories (Kim et al., 2024; Pan et al., 2024). Consequently, ORMs cannot provide finer-grained, step-wise guidance over partial trajectories. Process Reward Models (PRMs) address this limitation by evaluating and assigning rewards for individual steps (Ton et al., 2024; He et al., 2025; Chen et al., 2025). Recent advancements cast PRMs as generative judges (Wang et al., 2024b; Whitehouse et al., 2025; He et al., 2025) that generate justifications for step scores and have achieved strong performance in mathematics (Zhao et al., 2025; Xiong et al., 2025; Wang et al., 2024a), finance (Zhou et al., 2025), and agentic tasks (Chae et al., 2025; Choudhury, 2025). In contrast to these existing PRM approaches that rank the validity of relatively short reasoning snippets and struggle with managing growing contexts, PRINTS is equipped with jointly evaluating reasoning with tool interactions, planning for subsequent actions across multiple dimensions of information gain in tandem with compact trajectory summarization mechanism, inspired by approaches that compress reasoning histories and have shown success in context length reduction (Wu et al., 2025c; Kang et al., 2025; Ye et al., 2025). 3. PRINTS: Progress Reward via Information Gain Score and Trajectory Summarization We start by introducing the framework that quantifies and annotates the quality of each trajectory step reasoning step combined with tool call followed by reinforcement learning that uses these annotations to train PRINTS as scorer (Section 3.2). Next, we describe our approach for generating compact summaries of long interaction trajectories, and explain how these summaries are used to train the same PRINTS as summarizer (Section 3.3). The overall design of PRINTS is illustrated in Figure 2. 3.1. Preliminaries To tackle long-horizon information-seeking problems, we adopt the agentic ReAct (Yao et al., 2023) paradigm, where Large Language Model (LLM) acts as an agent that interleaves reasoning with tool-based action toward the goal of answering query (Tao et al., 2025; Li et al., 2025a). At each timestep t, the agent may generate intermediate reasoning st based on the current context and then predict the subsequent action at, i.e., calls to external tools, such as web search, web browsing, and code execution, to acquire new information. The resulting tool response ot is observed and added to the context, which informs the agents reasoning at timestep + 1. Figure 2 (Left) visually shows this tool interaction process: st, at, ot correspond to reasoning, tool call, tool response at timestep t. This process repeats until timestep , when the agent submits its answer oT . The task is successful if oT matches the ground-truth answer a. The accumulated reasoning context up to timestep t, referred to as the information-seeking trajectory, is defined as: Ht = (s1, a1, o1, s2, a2, o2, . . . , st, at, ot) (1) Specifically, the agent π generates the next reasoning step and tool call conditioned on the query and informationseeking trajectory, i.e., st, at π (q,Ht1). Then the tool call is executed to get the tool response ot. This in3 PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 2. Overview of PRINTS. Left: PRINTS functions as scorer, evaluating agents multiple candidate next trajectory steps based on the summarized context and current tool response. It generates an analysis and dense score for each candidate, selecting the top-scoring one to guide the agents information-seeking. Right: PRINTS acts as summarizer, recursively updating compact information-seeking trajectory summary to keep input length bounded and preserve key information for its subsequent score evaluation. terleaving of reasoning and action has shown success in long-horizon information-seeking tasks (Li et al., 2025b; Gao et al., 2025; Li et al., 2025a), and thus we adopt this setting. However, applying PRMs to guide long-horizon information seeking of the agent faces two key challenges: trajectory steps contain substantially richer content than traditional steps, requiring multi-dimensional evaluation beyond simple correctness, and the accumulated context Ht grows rapidly, producing long, noisy input context, which makes it difficult to identify evaluation evidence. Thus, we next introduce our data annotation and train pipeline, which equips PRINTS with two core capabilities: (1) dense steplevel scoring for fine-grained guidance, and (2) trajectory summarization for effective step-level evaluation under rapidly accumulating input context. 3.2. Step-level Information Gain Estimation Information Gain Score. To train Process Reward Model (PRM) for long-horizon information seeking, we need to measure how much each reasoning step and tool call contributes towards reaching the correct answer. To this end, we define information gain of the current step as the change in expected likelihood of arriving at the correct answer before and after taking the current step (Rao & III, 2018; Prasad et al., 2023; Wang et al., 2024a). This local evaluation quantifies the marginal improvement in task success contributed by the current step. Specifically, for reasoning step and tool call (st, at) preceded by informationseeking trajectory prefix, Ht1, we use Monte Carlo estimation (Wang et al., 2024a; Xiong et al., 2025; Setlur et al., 2025) by executing rollouts until their final answers are produced and compute the mean accuracy: mt = (cid:80)M j=1 1(o(j) Tj = a) , (2) which quantifies how much (st, at) contributes to the successful completion of the task. We scale by M/2 to obtain scores in the range [M/2, M/2]. positive gt indicates that the current step (st, at) increases the probability of reaching the correct answer for example, through logically coherent reasoning or tool call that resolves uncertainties whereas gt lower than zero indicates that the current step reduces the probability, e.g., by making unverified assumptions or invoking an irrelevant tool call. Trajectory Score Annotations. While prior approaches annotate individual trajectory steps in isolation (Xiong et al., 2025; Wang et al., 2024a) or use imitation learning to learn directly from precomputed step-level scores (Wang et al., 2024a), Whitehouse et al. (2025) demonstrates that pairwise preference learning is effective for training robust judge models. We extend this by automatically constructing preference pairs grounded in information gain scores, then training PRINTS with complementary objectives score reward for information gain estimation and comparison reward for preference prediction as illustrated in Figure 3 (Top). t+1 (cid:16) q,Ht,s(j) During the rollouts performed to calculate gt, the LLM generates set of unique next trajectory steps {s(j) t+1,a(j) t+1}M j=1 and the corresponding final answer predic- (cid:17) t+1,a(j) tions {o(j) }M π j=1. As shown in FigTj ure 3 (Top-left), to construct candidate preference pair, we first select one trajectory step that (1) leads to successful final answer and (2) achieves the shortest successful trajectory, assuming this step has the highest potential to be both effective and efficient among the rollouts. second trajectory step is then randomly sampled from the remaining steps, assuming that it provides contrasting, less effective reasoning (i.e., one that either leads to an incorrect answer or reaches the correct answer through longer trajectory). where o(j) π(q,Ht1,st,at) is the final answer from Tj rollout j, which terminates at timestep Tj. The information gain score gt is then computed as: gt = (mt mt1) M/2, (3) Next, we annotate the information gain scores of this candidate preference pair by treating each trajectory step as new starting point and running rollouts to estimate their respective mean accuracies and information gain scores. After annotation, the winning and losing labels are reassigned PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 3. PRINTS: data annotation and training pipeline. Top: For each trajectory step, we estimate the information gain score via Monte Carlo rollouts as the change in mean answer accuracy before and after the step. Then we construct winning-losing step pairs based on these scores (left). Preference pair examples are shown in Figure 7. Then we train PRINTS as scorer via GRPO on these pairs (right). The final reward combines score reward for accurate prediction, comparison reward for pairwise preference learning, and an adaptive weight to mitigate noisy annotations. Bottom: Each step is annotated with compact, recursively updated trajectory summary capturing essential findings and plans up to the step (left). The same PRM is jointly trained as summarizer via SFT on this summary data (right). based on the actual information gain scores: the step with the t+1,a+ higher score becomes the true winning sample (s+ t+1), t+1,a while the other becomes the losing sample (s t+1). The winning step then serves as the starting point for generating the next pair at step + 2. This contrastive labeling ensures that PRINTS learns relative preferences between trajectory steps grounded in empirically estimated improvements. Training the PRM as Scorer. The core function of PRINTS is to assess information-seeking trajectory step quality and assign higher scores to steps expected to yield greater information gain. To this end, we train PRINTS to evaluate the trajectory step quality by predicting information gain scores. Given query q, trajectory summary ht1 (introduce in Section 3.3 below), the latest tool response ot1, and trajectory step (st, at), PRINTS generates Chain-ofThought analysis and outputs scalar score prediction ˆgt: ˆgt = fI (q, ht1,ot1,st,at), (4) where fI denotes PRINTS that works as an information gain scorer function. We train this scoring capability using GRPO (Shao et al., 2024) with the following rewards: (1) score reward (rs) that targets minimizing the discrepancy between the predicted score (ˆgt) and the ground-truth score (gt), and (2) comparison reward (rc) that enforces pairwise preferences derived from annotated pairs: rk = 1 (cid:12) (cid:12) (cid:12) (cid:12) gk ˆgk (cid:12) (cid:12) (cid:12) (cid:12) , rk ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 yk sign(ˆgk ˆg ) (5) where sign(x) = (cid:40) +1, 0 1, < 0 , {+, } indicates the winning or losing sample, y+ = 1 and = 1 for comparison direction, denotes its counterpart, is the number of rollouts, and ˆgk is the predicted score of j-th rollout from the counterpart. For simplicity, we omit rolloutand step-level indices, which do not affect the underlying formulation of the scores. The comparison reward ensures PRINTS learns to distinguish better from worse reasoning paths, while the score reward provides fine-grained feedback on estimation accuracy. Finally, we combine the two rewards into single scalar per rollout with an adaptive weight based on the ground-truth score margin: rk = rk + rk , = g+ , (6) where is the comparison reward weight set for each pair. This adaptive weighting addresses noise in the automatically annotated preference pairs. Pairs with large score margins between winning and losing samples (g+ g) are more reliably ranked and receive higher comparison weights, while pairs with small margins receive lower weights, as they may reflect annotation noise rather than true preference (Prasad et al., 2024). This weighting scheme amplifies strong preference signals while mitigating the impact of noisy annotations. The combined reward thus encourages PRINTS both to estimate absolute scores accurately and to learn robust preferences in information-seeking trajectories. 5 PRINTS : Reward Modeling for Long-Horizon Information Seeking 3.3. Information-seeking Summary Generation Trajectory Summary Annotations. Another core challenge in building PRM for information-seeking agents is the rapidly growing context (Figure 1 Top-left) of lengthy and noisy reasoning and tool interactions. This context explosion hinders PRMs from efficient processing and results in noise and distraction in quality evaluation. To this end, we further extract concise summary of the informationseeking trajectory of each trajectory step (st,at). This summary, ht, captures the essential findings and plan development up to timestep t. As illustrated in Figure 3 (Bottomleft), each summary is recursively updated and generated by an LLM, incorporating the previous summary ht1, the latest tool response ot1, and the current trajectory step (st, at) (i.e., ht = LLM(q,ht1,ot1,st,at). This recursive formulation ensures that ht maintains compressed form of the entire trajectory Ht, with bounded input length. Training the PRM as Summarizer. To enable efficient processing of the reasoning context during the score estimation, PRINTS is trained to generate concise summaries that retain only the essential context: ˆht = fS(q, ht1,ot1,st,at), (7) where fS is PRINTS that works as summarization function, and ˆht is the summary generated by PRINTS. We use supervised fine-tuning (SFT) on the annotated summaries ht of trajectory steps, which allows PRINTS to learn effective summarization by imitating the provided annotations. 4. Experiments 4.1. Experimental Setup Models. To evaluate the efficacy and generalizability of PRINTS, we use three distinct LLMs: Qwen3-32B (Yang et al., 2025a), an open-source model with strong reasoning capability; Gemini-2.5-Flash (Gemini Team, 2025), closed-source frontier model; and Tongyi DeepResearch30B-A3B (Li et al., 2025a), recently developed agent specifically optimized for long-horizon information-seeking tasks. We instantiate ReAct-based agents using these LLMs and evaluate them using the Inspect-Eval (AI Security Institute, 2024) evaluation environment. Further details on the evaluation environment are provided in Section A. Evaluation Benchmarks. Following recent work on longhorizon information-seeking agents (Gao et al., 2025; Li et al., 2025c;a), we assess the effectiveness of PRM-guided reasoning on three benchmarks: FRAMES (Krishna et al., 2025), GAIA (Mialon et al., 2024), and WebWalkerQA (Wu et al., 2025b). GAIA evaluates general assistant capabilities on complex retrieval and multi-step reasoning tasks spanning three difficulty levels (Level 1-3). WebWalkerQA requires agents to traverse webpages to gather evidence across Easy-Hard levels. FRAMES contains factual and reasoning-intensive queries that require multiple retrieval steps. For Qwen3-32B and DeepResearch agent, we evaluate across FRAMES, GAIA, and WebWalkerQA. For Gemini-2.5-Flash, we evaluate on GAIA. Further explanations of these benchmarks are provided in Section A. Evaluation Metric. Following past work (Gao et al., 2025; Li et al., 2025c;a), we adopt the LLM-as-Judge (LasJ) paradigm to measure benchmark performance, which is standard approach in long-horizon information-seeking research. We use GPT-5 to judge the correctness of final answers. All results are reported using Avg@3, defined as the mean accuracy over three independent runs. Baselines. We compare PRINTS againts three categories of baselines: (1) Base agent: this directly uses the LLMs information-seeking abilities without any PRM guidance. This serves as reference to measure the test-time improvement achieved by PRM-based guidance. (2) Intrinsic reasoning heuristics: we include widely used reasoning quality heuristics, including confidence (Ghasemabadi et al., 2025), relevance (Wan et al., 2025), and verbal-progress. (3) Existing PRMs: we compare with existing PRMs: GenPRM7B (Zhao et al., 2025), Web-Shepherd-8B (Chae et al., 2025), StepWiser (Xiong et al., 2025). To provide controlled comparison, we follow StepWisers training protocol to reimplement StepWiser using our annotated data and Qwen3-4B model, adapting it to long-horizon informationseeking tasks. This setup allows us to directly demonstrate the contributions of PRINTSs design; dense comparative scoring and compact trajectory summarization. Further details of the baseline are provided in Section A. Implementation Details. Our dataset consists of 2k preference trajectory step pairs with score and summary annotations drawn from publicly available web-agent training corpora (MiroMind Data Team, 2025). We use Qwen3-32B to generate both the information gain score and summary annotation for each step with Monte Carlo rollouts = 8. PRINTS is instantiated as Qwen3-4B and trained with an alternating SFT-GRPO schedule, where one epoch of SFT for the summarization objective is followed by period of GRPO training for the scoring objective with GRPO rollouts = 4. This SFT-GRPO cycle is repeated iteratively throughout training, enabling PRINTS to jointly acquire summarization and scoring abilities. At test-time, all baselines and PRINTS evaluate sets of candidate next steps output by LLMs, performing best-of-n guided search with = 4. Further details can be found in Section A. 6 PRINTS : Reward Modeling for Long-Horizon Information Seeking Method FRAMES GAIA WebWalkerQA Level 1 Level 2 Level 3 Easy Medium Hard Base agent Confidence Relevance Verbal-progress GenPRM-7B Web-Shepherd-8B StepWiser PRINTS 49.3 55.7 56.3 45.0 50.0 49.0 51.3 58.7 35.1 36.8 34.2 35. 32.5 38.5 37.6 49.6 23.7 24.4 20.5 21.2 25.7 23.7 22.4 33. 11.1 16.7 8.3 13.9 16.7 5.5 8.3 19.4 30.1 31.7 33.3 27. 33.3 28.5 31.7 39.8 26.9 31.3 29.5 30.2 32.8 31.8 31.8 33. 30.3 32.9 32.5 34.2 34.6 33.3 33.8 37.3 Avg. 29. 32.8 30.7 29.7 32.2 30.0 31.0 38.8 Table 1. Comparison of step quality evaluation methods on Qwen3-32B across information-seeking benchmarks. We adopt the LLM-asJudge (LasJ) metric and report Avg@3. The best and the second best results are in bold and underline, respectively. PRINTS delivers consistent gains all benchmarks, whereas the second-best baseline varies. Method FRAMES GAIA WebWalkerQA Level 1 Level 2 Level 3 Easy Medium Hard Base agent Confidence Relevance Verbal-progress GenPRM-7B Web-Shepherd-8B StepWiser PRINTS 79.3 61.3 81.3 82.3 79.0 79.7 81.0 81.7 68.4 60.7 70.1 69. 70.1 69.2 70.1 69.2 61.6 47.5 63.5 60.9 64.1 61.5 60.9 65. 41.7 25.0 33.3 41.7 38.9 36.1 36.1 44.5 61.8 63.4 66.7 63. 60.2 62.6 65.0 70.7 59.5 62.0 62.8 63.8 63.8 62.3 61.8 65. 68.0 64.9 66.7 68.4 68.9 67.1 64.9 70.1 Avg. 62. 55.0 63.5 64.2 63.6 62.6 62.8 66.8 Table 2. Comparison of step quality evaluation methods on Tongyi DeepResearch-30B-A3B across information-seeking benchmarks. We adopt the LLM-as-Judge (LasJ) metric and report Avg@3. The best and the second best results are in bold and underline, respectively. The results show that PRINTS enhances the performance of strong information-seeking agents. 4.2. Results and Discussion PRINTS substantially outperforms existing PRMs on foundation models. Table 1 presents long-horizon information-seeking task results using the open-source Qwen3-32B as the LLM agent. Across all benchmarks, including FRAMES, GAIA, and WebWalkerQA, PRINTS consistently achieves substantial gains over the base agent. For instance, on GAIA Level 1 and 2, PRINTS surpasses the base agent by 14.5% and 9.6% absolute accuracy, respectively, whereas other baselines yield only marginal improvements or sometimes even reduce performance. Although StepWiser is trained on the same annotated dataset as PRINTS, it provides only 1.5% absolute average accuracy gain (cf. 9.3% improvement with PRINTS). This illustrates the limitation of binary correctness signals, which provide only coarse supervision and fail to capture multiple quality dimensions that influence the usefulness of trajectory step in long-horizon information seeking. However, even baselines that generate richer outputs do not bridge this gap. Verbal-progress which produces scalar progress estimates and Web-Shepherd which generates and evaluates multi-item checklists both offer more expressive signals than binary labels but still fall short, adding marginal gains of 0.2% and 0.5% absolute average accuracy respectively. This shows that having continuous scores alone does not guarantee effective test-time guidance. In contrast, PRINTS is trained to produce multi-factor analyses and dense comparative score grounded in both information-gain estimation and pairwise preference learning. This design choice enables it to identify subtle yet important quality differences between candidate steps and select the most informative next step, leading to better information-seeking behavior. PRINTS improves highly performant informationseeking agents. In Table 2, we evaluate PRINTS on the specialized information-seeking agent DeepResearch-30BA3B, specifically optimized for and achieves high performance in long-horizon information-gain tasks through extensive fine-tuning. The results show that adding PRINTS to this strong information-seeking agent consistently achieves 7 PRINTS : Reward Modeling for Long-Horizon Information Seeking GAIA Level Level 2 Level 3 Avg. Input Context FRAMES Method Base agent Relevance Verbal-progress GenPRM-7B Web-Shepherd-8B StepWiser PRINTS 58.1 58.1 60. 56.4 59.8 60.7 61.5 42.3 44.9 44.2 44.9 46.2 44.3 45. 19.5 19.5 16.7 11.1 16.7 19.5 25.0 40.0 40.8 40. 37.5 40.9 41.5 44.0 Table 3. PRINTS shows strong generalization to the frontier LLM (Gemini). We adopt the LLM-as-Judge (LasJ) metric and report Avg@3 comparing with other step quality evaluation methods on Gemini-2.5-Flash. H1: H2: H4: Ht ht (Ours) 56.3 61.0 57.0 55. 58.7 GAIA Level 1 Level 2 44.5 44.5 37.6 38.5 49. 25.7 26.9 25.0 24.4 33.3 Avg. 42.2 44.1 39.9 39.5 47.2 Table 4. Effectiveness of context compression. Comparison of input context representations for PRM on Qwen3-32B across information-seeking tasks. H1:, H2:, and H4: provide the most recent one, two, and four trajectory steps from the full trajectory Ht, while ht uses the trajectory summary from PRINTS. Our approach (ht) shows better scoring ability by retaining essential information for step evaluation in compact summary. performance gains across benchmarks, surpassing the base agent by 3.9% absolute average accuracy, while no other baselines come close to achieving notable improvements, with the strongest one improving performance by only 1.3% absolute average accuracy. It is also noteworthy that PRINTS improves on the challenging subsets, such as GAIA Level 3 and WebWalkerQA Hard. Moreover, on GAIA, PRINTS lifts DeepResearch-30B-A3B from 61.9% to 64.4% average accuracy, enabling the 30B agent augmented with the 4B PRM to reach competitive performance with OpenAI DeepResearch (67.4%) and surpass DeepSeek-V3.1-671B (63.1%).2 Specialized informationseeking agents (Wu et al., 2025a; Li et al., 2025b;c) require large-scale dataset (10k-100k+ samples), substantial computational costs for tool interactions and multi-step rollouts during online reinforcement learning, which is especially challenging as different samples have different trajectory lengths to provide the outcome reward signal for training agents (Gao et al., 2025). Despite these agents undergoing resource-intensive training, applying PRINTS further improves this strong information-seeking agent, surpassing its original performance with training only small 4B model that either does not require large dataset (2k+ pair samples) or long-horizon rollouts and tool interactions during training. These findings demonstrate that even highly optimized information-seeking agents can benefit from the step-level guidance provided by PRINTS, pushing their performance to the limit in costand data-effective manner. PRINTS also generalizes to frontier LLMs. To further demonstrate the versatility of our approach, we use the closed-source Gemini-2.5-Flash as the LLM agent, as shown in Table 3. PRINTS provides 4.0% absolute average accuracy gain, whereas the second-best method improves performance by only 1.5%. On the most challenging subset, GAIA Level 3, PRINTS yields the largest 2See Section for details on reporting frontier model results. improvement among all baselines (+5.5%), showing its strength on long-horizon reasoning tasks. Overall, our results indicate that PRINTS provides effective test-time guidance, improving the information-seeking behavior of both open-source LLMs, closed-source LLMs, and informationseeking agents, which shows strong versatility and generalizability without modifying or retraining underlying LLMs. 4.3. Analysis and Ablations Summarization ability contributes to better scoring ability. To validate the effectiveness of compressed information-seeking trajectory representations for accurate step-level scoring, we compare our summarization approach against several alternatives: providing only the most recent one, two, or four trajectory steps as input context (H1:, H2:, H4:), and providing the full trajectory Ht. We evaluate on Qwen3-32B across FRAMES and GAIA. Since the performance on GAIA Level 3 is low (see Table 1), we only use Levels 1 and 2. Results in Table 4 show that our summary-based approach achieves the best or secondbest performance across benchmarks, outperforming the full raw-history baseline by 7.7% absolute average accuracy. Notably, providing more raw history does not improve performance. H2: outperforms both H1: (insufficient context), H4:, and Ht (excessive, noisy context). This demonstrates that accumulated raw context becomes bottleneck longer histories introduce noise and irrelevant information that hinder the PRM from identifying key information, distracting step-level evaluation. In contrast, our summarization compresses entire trajectories into compact representations that preserve key information while filtering out noise and maintaining bounded input length, enabling more accurate scoring even as trajectories grow arbitrarily long. Complementary rewards improve step-level evaluation. We analyze the contribution of each reward component 8 PRINTS : Reward Modeling for Long-Horizon Information Seeking Reward Design FRAMES = rs (score-only) = rc (comparison-only) = rs + rc (combination) = rs + rc (Ours) 57.0 58.7 60.3 58.7 GAIA Level 1 Level 43.6 41.9 47.0 49.6 32.0 28.8 31.4 33.3 Avg. 44.2 43.1 46. 47.2 Table 5. Impact of reward components. Experiments with reward components on PRM performance across informationseeking tasks evaluated with Qwen3-32B. Combining the score reward with comparison reward (rs + rc) leads to better step evaluation, with further improvement when mitigating noise in preference pairs through the adaptive weight (rs + rc). in Equation (6). Following the setup from the previous ablation, we evaluate on Qwen3-32B across FRAMES, GAIA Levels 1, and 2. As shown in Table 5, combining the score and comparison rewards (rs + rc) yields substantially better performance than using either component alone, leading to 2.0% and 3.1% absolute average accuracy gains compared to using the score reward (rs) and comparison reward (rc), respectively. This indicates that information-gain estimation and preference prediction capture complementary aspects of the quality of an information-seeking trajectory step, underscoring the benefit of our pairwise annotation strategy over prior work that labels individual steps in isolation (Xiong et al., 2025; Wang et al., 2024a). Furthermore, incorporating the adaptive weight (rs +w rc) yields 1.0% additional absolute average accuracy gain over the naive combination. This is because the adaptive weight mitigates noise in preference pairs. Pairs with small information gain score differences are inherently noisier, as they may reflect annotation variance rather than true quality differences. Thus, these pairs receive lower weights, while pairs with clear margins are weighted higher, leading to more stable learning. Overall, adaptive weighting provides simple and cost-efficient way of leveraging existing annotated preference pairs. PRINTS scales effectively with test-time compute. In order to evaluate how PRINTS benefits from additional testtime compute, we conduct best-of-n scaling experiments with varying numbers of candidate steps (n {1,2,4,8,16}) on GAIA Level 2 using Qwen3-32B, as shown in Figure 4. PRINTS exhibits strong scaling behavior, achieving 2.5%, 3.8%, 8.9% absolute accuracy gains at = 2, 4, 8, respectively, demonstrating that PRINTS reliably identifies higher-quality steps within large candidate sets. However, performance declines at = 16, which we attribute to overexploration: we observe that the PRM increasingly selects uncertainty-resolving steps even when correct answers already appear among candidates. Consequently, the agent continues exploring until it reaches the maximum step budget, failing to output an answer despite having generated one Figure 4. Scaling test-time compute. Best-of-n test-time scaling results on GAIA Level 2 using Qwen3-32B. PRINTS benefits from additional test-time compute by identifying higher-quality steps from candidates. earlier. In contrast, StepWiser provides only marginal and inconsistent improvements under scaling. This difference in scaling efficiency further validates that PRINTSs design of information-gain estimation and preference prediction captures subtle quality differences between steps, enabling fine-grained guidance for long-horizon information-seeking. 5. Conclusion In this paper, we introduce PRINTS, generative PRM for long-horizon information seeking. PRINTS unifies information gain scoring with recursive trajectory summarization, enabling fine-grained step-level evaluation under rapidly accumulating context from agents. To equip PRINTS with these dual capabilities, we develop an annotation pipeline that constructs preference step pairs with information gain scores and summaries, and we jointly train PRINTS through alternating schedule of supervised finetuning for summarization and reinforcement learning for scoring. We validate PRINTS on three distinct agents, including strong information-seeking agent, and comprehensive experiments demonstrate that PRINTS consistently enhances the information-seeking abilities of these agents, showcasing its high versatility. Notably, PRINTS pushes the performance of the frontier information-seeking agents beyond their original performance, showing that test-time guidance can be powerful complement to agent fine-tuning while remaining robust to changes in underlying models."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by NSF-AI Engage Institute DRL2112635, NSF-CAREER Award 1846185, DARPA ECOLE Program No. HR00112390060, Capital One Research Award, Apple PhD Fellowship, NDSEG PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency. 9 PRINTS : Reward Modeling for Long-Horizon Information Seeking"
        },
        {
            "title": "References",
            "content": "AI Security Institute, U. Inspect AI: Framework for Large Language Model Evaluations, 2024. URL https: //github.com/UKGovernmentBEIS/inspect_ai. Bachman, P., Sordoni, A., and Trischler, A. ToarXiv preprint wards information-seeking agents. arXiv:1612.02605, 2016. URL https://arxiv.org/ abs/1612.02605. Chae, H., Kim, S., Cho, J., Kim, S., Moon, S., Hwangbo, G., Lim, D., Kim, M., Hwang, Y., Gwak, M., Choi, D., Kang, M., Im, G., Cho, B., Kim, H., Han, J. H., Kwon, T., Kim, M., Kwak, B., Kang, D., and Yeo, J. Webshepherd: Advancing prms for reinforcing web agents. arXiv preprint arXiv:2505.15277, 2025. URL https: //doi.org/10.48550/arXiv.2505.15277. Chen, J., Prasad, A., Saha, S., Stengel-Eskin, E., and Bansal, M. MAgICoRe: Multi-agent, iterative, coarse-to-fine refinement for reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), November 2025. Choudhury, S. Process reward models for LLM agents: arXiv preprint Practical framework and directions. arXiv:2502.10325, 2025. URL https://doi.org/10. 48550/arXiv.2502.10325. Deng, Y., Wang, G., Ying, Z., Wu, X., Lin, J., Xiong, W., Dai, Y., Yang, S., Zhang, Z., Wang, Q., Qin, Y., Wang, Y., Zha, Q., Dai, S., and Meng, C. Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800, 2025. URL https://doi.org/10.48550/arXiv.2508.12800. Fu, Y., Wang, X., Tian, Y., and Zhao, J. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. URL https://doi.org/10.48550/arXiv.2508.15260. Gao, J., Fu, W., Xie, M., Xu, S., He, C., Mei, Z., Zhu, B., and Wu, Y. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous RL. arXiv preprint arXiv:2508.07976, 2025. URL https://doi. org/10.48550/arXiv.2508.07976. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and arXiv preprint next generation agentic capabilities. arXiv:2507.06261, 2025. URL https://doi.org/10. 48550/arXiv.2507.06261. Ghasemabadi, A., Mills, K. G., Li, B., and Niu, D. Guided by gut: Efficient test-time scaling with reinforced intrinsic confidence. arXiv preprint arXiv:2505.20325, 2025. URL https://doi.org/10.48550/arXiv.2505.20325. He, T., Mu, R., Liao, L., Cao, Y., Liu, M., and Qin, B. Good learners think their thinking: Generative PRM makes large reasoning model more efficient math learner. arXiv preprint arXiv:2507.23317, abs/2507.23317, 2025. URL https://doi.org/10.48550/arXiv.2507.23317. Kang, M., Chen, W.-N., Han, D., Inan, H. A., Wutschitz, L., Chen, Y., Sim, R., and Rajmohan, S. Acon: Optimizing context compression for long-horizon llm agents. arXiv preprint arXiv:2510.00615, 2025. URL https://arxiv. org/abs/2510.00615. Kim, B., Jang, Y., Logeswaran, L., Kim, G., Kim, Y. J., Lee, H., and Lee, M. Prospector: Improving LLM agents with self-asking and trajectory ranking. In Al-Onaizan, Y., Bansal, M., and Chen, Y. (eds.), Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024. Krishna, S., Krishna, K., Mohananey, A., Schwarcz, S., Stambler, A., Upadhyay, S., and Faruqui, M. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. In Proceedings of the Association for Computational Linguistics (ACL), 2025. Kuratov, Y., Bulatov, A., Anokhin, P., Rodkin, I., Sorokin, D., Sorokin, A. Y., and Burtsev, M. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Li, B., Zhang, B., Zhang, D., Huang, F., Li, G., Chen, G., Yin, H., Wu, J., Zhou, J., Li, K., and et al., Y. J. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025a. URL https://arxiv.org/ abs/2510.24701. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., Shen, W., Zhang, J., Zhang, D., Wu, X., Jiang, Y., Yan, M., Xie, P., Huang, F., and Zhou, J. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025b. Li, X., Jin, J., Dong, G., Qian, H., Zhu, Y., Wu, Y., Wen, J., and Dou, Z. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025c. URL https://doi.org/10. 48550/arXiv.2504.21776. Liu, F., Yang, Z., Liu, C., Song, T., Gao, X., and Liu, H. Mm-agent: LLM as agents for real-world arXiv preprint mathematical modeling problem. arXiv:2505.14148, 2025a. URL https://doi.org/10. 48550/arXiv.2505.14148. Liu, X., Liu, Y., Wang, S., Cheng, H., Estornell, A., Zhao, Y., and Wei, J. Agenticmath: Enhancing LLM reasoning 10 PRINTS : Reward Modeling for Long-Horizon Information Seeking via agentic-based math data generation. arXiv preprint arXiv:2510.19361, 2025b. URL https://doi.org/10. 48550/arXiv.2510.19361. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, In T. GAIA: benchmark for general AI assistants. Proceedings of the International Conference on Learning Representations (ICLR), 2024. MiroMind AI Team. Mirothinker: An open-source agentic model series trained for deep research and complex, long-horizon problem solving. https://github.com/ MiroMindAI/MiroThinker, 2025. MiroMind Data Team. Miroverse v0.1: reproducible, full-trajectory, ever-growing deep research dataset, URL https://huggingface.co/datasets/ 2025. miromind-ai/MiroVerse-v0.1. Pan, J., Wang, X., Neubig, G., Jaitly, N., Ji, H., Suhr, A., and Zhang, Y. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. URL https://doi.org/10.48550/arXiv.2412. 21139. Prabhudesai, M., Chen, L., Ippoliti, A., Fragkiadaki, K., Liu, H., and Pathak, D. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. URL https://doi.org/10.48550/arXiv.2505. 22660. Prasad, A., Saha, S., Zhou, X., and Bansal, M. Receval: Evaluating reasoning chains via correctness and informativeness. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. Prasad, A., Yuan, W., Pang, R. Y., Xu, J., Fazel-Zarandi, M., Bansal, M., Sukhbaatar, S., Weston, J., and Yu, J. A. Self-consistency preference optimization. arXiv preprint arXiv:2411.04109, 2024. URL https://doi.org/10. 48550/arXiv.2411.04109. Rao, S. and III, H. D. Learning to ask good questions: Ranking clarification questions using neural expected value of perfect information. In Proceedings of the Association for Computational Linguistics (ACL), 2018. Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J., Agarwal, R., Agarwal, A., Berant, J., and Kumar, A. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Shao, R., Qiao, R., Kishore, V., Muennighoff, N., Lin, X. V., Rus, D., Low, B. K. H., Min, S., Yih, W., Koh, P. W., and Zettlemoyer, L. Reasonir: Training retrievers for reasoning tasks. arXiv preprint arXiv:2504.20595, 2025. URL https://doi.org/10.48550/arXiv.2504.20595. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. URL https://doi.org/10.48550/arXiv.2402.03300. Su, H., Yen, H., Xia, M., Shi, W., Muennighoff, N., Wang, H., Liu, H., Shi, Q., Siegel, Z. S., Tang, M., Sun, R., Yoon, J., Arik, S. Ö., Chen, D., and Yu, T. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Tang, Z., Ji, B., Qiu, Q., Wang, H., Liang, X., Li, J., and Zhang, M. Longrm: Revealing and unlocking the context boundary of reward modeling. arXiv preprint arXiv:2510.06915, 2025. URL https://arxiv.org/ abs/2510.06915. Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. URL https://doi.org/10. 48550/arXiv.2507.15061. Ton, J., Taufiq, M. F., and Liu, Y. Understanding chainof-thought in llms through information theory. arXiv preprint arXiv:2411.11984, 2024. URL https://doi. org/10.48550/arXiv.2411.11984. Wan, G., Wu, Y., Chen, J., and Li, S. Reasoning aware self-consistency: Leveraging reasoning paths for efficient LLM sampling. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL), 2025. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the Association for Computational Linguistics (ACL), 2024a. Wang, T., Kulikov, I., Golovneva, O., Yu, P., Yuan, W., Dwivedi-Yu, J., Pang, R. Y., Fazel-Zarandi, M., Weston, J., and Li, X. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024b. URL https://doi.org/10. 48550/arXiv.2408.02666. Wang, X., McInerney, J., Wang, L., and Kallus, N. Entropy after /Think for reasoning model early exiting. arXiv preprint arXiv:2509.26522, 2025. URL https://arxiv. org/abs/2509.26522. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language 11 PRINTS : Reward Modeling for Long-Horizon Information Seeking models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. URL https://doi.org/10.48550/arXiv.2504. 12516. Whitehouse, C., Wang, T., Yu, P., Li, X., Weston, J., Kulikov, I., and Saha, S. J1: incentivizing thinking in llmas-a-judge via reinforcement learning. arXiv preprint arXiv:2505.10320, 2025. URL https://doi.org/10. 48550/arXiv.2505.10320. Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Jiang, Y., Xie, P., Huang, F., and Zhou, J. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. URL https://doi.org/10.48550/arXiv.2505.22648. Wu, J., Yin, W., Jiang, Y., Wang, Z., Xi, Z., Fang, R., Zhang, L., He, Y., Zhou, D., Xie, P., and Huang, F. Webwalker: In Proceedings Benchmarking llms in web traversal. of the Association for Computational Linguistics (ACL), 2025b. Wu, J., Zhu, J., Liu, Y., Xu, M., and Jin, Y. Agentic reasoning: streamlined framework for enhancing LLM reasoning with agentic tools. In Proceedings of the Association for Computational Linguistics (ACL), 2025c. Xiong, W., Zhao, W., Yuan, W., Golovneva, O., Zhang, T., Weston, J., and Sukhbaatar, S. Stepwiser: Stepwise generative judges for wiser reasoning. arXiv preprint arXiv:2508.19229, 2025. URL https://doi.org/10. 48550/arXiv.2508.19229. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., and et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. URL https://doi.org/10.48550/arXiv.2505.09388. Yang, J., Leret, K., Jimenez, C. E., Wettig, A., Khandpur, K., Zhang, Y., Hui, B., Press, O., Schmidt, L., and Yang, D. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025b. URL https://doi.org/10.48550/arXiv.2504.21798. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. Ye, R., Zhang, Z., Li, K., Yin, H., Tao, Z., Zhao, Y., Su, L., Zhang, L., Qiao, Z., Wang, X., Xie, P., Huang, F., Chen, S., Zhou, J., and Jiang, Y. Agentfold: Long-horizon web agents with proactive context management. arXiv preprint arXiv:2510.24699, 2025. URL https://arxiv. org/abs/2510.24699. Yen, H., Paranjape, A., Xia, M., Venkatesh, T., Hessel, J., Chen, D., and Zhang, Y. Lost in the maze: Overcoming context limitations in long-horizon agentic search. arXiv preprint arXiv:2510.18939, 2025. URL https://arxiv. org/abs/2510.18939. Yuan, X., Fu, J., Côté, M.-A., Tay, Y., Pal, C., and Trischler, A. Interactive machine comprehension with information seeking agents. In Proceedings of the Association for Computational Linguistics (ACL), 2020. Zhao, J., Liu, R., Zhang, K., Zhou, Z., Gao, J., Li, D., Lyu, J., Qian, Z., Qi, B., Li, X., and Zhou, B. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025. URL https://doi.org/10.48550/arXiv.2504. 00891. Zhou, Y., Jiang, S., Zhu, J., Li, J., Guo, L., Chen, F., and Zhang, C. Fin-prm: domain-specialized process reward model for financial reasoning in large language models. arXiv preprint arXiv:2508.15202, 2025. URL https: //doi.org/10.48550/arXiv.2508.15202. Zou, J., Yang, L., Gu, J., Qiu, J., Shen, K., He, J., and Wang, M. Reasonflux-prm: Trajectory-aware prms for long chain-of-thought reasoning in llms. arXiv preprint arXiv:2506.18896, 2025. URL https://doi.org/10. 48550/arXiv.2506.18896. A. Details of Experimental Setups Tool Implementation Details. We conduct all experiments within the Inspect-Eval evaluation framework (AI Security Institute, 2024), which fully supports the ReAct paradigm (Yao et al., 2023) for multi-turn reasoning and tool interactions necessary for complex information-seeking tasks. The framework provides access to comprehensive set of external tools: Web Search: We utilize the Serper search API to retrieve up-to-date web content for search queries. Web Browsing: The framework includes built-in browser automation tools, supporting essential web interaction functions such as browsing with URLs, clicking, scrolling down/up, typing, etc. 12 PRINTS : Reward Modeling for Long-Horizon Information Seeking Code Execution: The framework also supports built-in Python and Bash code execution environments. In this evaluation framework, agents use these tools to interact with external sources to search and synthesize information to solve given information-seeking questions. Evaluation Benchmarks. We provide in-depth explanations of the long-horizon information-seeking benchmarks used in our experiments. (1) GAIA (Mialon et al., 2024) evaluates the ability to act as general AI assistant on complex retrieval and reasoning tasks spanning three difficulty levels. Following prior work (Li et al., 2025c; Gao et al., 2025; Kang et al., 2025), we use 103 questions from the text-only validation subset. (2) WebWalkerQA (Wu et al., 2025b) focuses on web-based reasoning, requiring agents to traverse webpages to locate target information across three difficulty levels. We evaluate on 247 English questions. (3) FRAMES (Krishna et al., 2025) provides factual and reasoning-intensive queries to assess both retrieval and reasoning capabilities. We use subset that consists of 300 samples that are randomly selected from the original dataset. Baselines. explanation of the baselines. In this section, we provide more detailed GenPRM-7B (Zhao et al., 2025) is generative PRM originally designed for mathematical reasoning. It produces Chain-of-Thought rationales with binary verdict (yes / no) indicating whether the current step is correct. We follow their prompt format and ask GenPRM to verify the correctness of trajectory step and explain why the step is judged correct or incorrect. Web-Shepherd-8B (Chae et al., 2025) generates taskspecific checklist that decomposes task into key subgoals and evaluates agentic trajectories based on it. Specifically, it assigns coarse feedback labels (Yes / No/ In progress) for each checklist item for evaluation. We also follow their prompt formats to generate checklist for given information-seeking task and evaluate each trajectory step relative to that checklist. StepWiser (Xiong et al., 2025) trains generative PRM using GRPO with binary rewards, where each step is labeled as effective or ineffective. For implementation, we follow the Relative Effective Reward Thresholding in the paper to re-annotate our training dataset: step receives positive label if the ratio between the current and previous mean accuracies exceeds the threshold (0.7), and negative label otherwise. Using this binary supervision, we train Qwen3-4B for 4 epochs to build StepWiser PRM. Confidence (Ghasemabadi et al., 2025; Fu et al., 2025; Prabhudesai et al., 2025; Wang et al., 2025) estimates reasoning quality based on the models certainty. Following Figure 5. Distribution of annotated information gain scores. the confidence definition in recent work (Ghasemabadi et al., 2025; Fu et al., 2025), we calculate confidence by taking the negative average log-probability of the top-10 most likely tokens at each position across all generated tokens in the reasoning step, and then averaging these scores across all token positions. Higher scores indicate lower uncertainty. Relevance (Wan et al., 2025) measures the coherence between the current step and the preceding context. Specifically, it uses the Jaccard similarity between the current step and the accumulated past steps. higher similarity indicates better contextual coherence. Verbal progress is zero-shot baseline that assesses progress toward the final answer by prompting Qwen3-4B to estimate how close the current reasoning state is to completing the task. The model is asked to output scalar that ranges from 1 to 5 based on the textual content of the current step and its information-seeking trajectory. higher score indicates that the current reasoning state is close to the final answer. Train Configurations. We train PRINTS using Qwen34B with an alternating SFT-GRPO schedule over four cycles, where each cycle consists of one SFT epoch for summarization followed by one GRPO epoch for scoring, to jointly acquire both abilities. For the SFT stage, we use batch size of 128 and learning rate of 1e-6. For the GRPO stage, we execute = 4 rollouts and use batch size of 128 and learning rate of 1e-6. Such alternating optimization allows the model to continuously refine its summarization ability while simultaneously improving its scoring accuracy on reasoning quality. This iterative schedule ensures both modules evolve synergistically, stabilizing training and preventing either skill from degrading over time. Data Annotations. We construct our annotated data from 4,344 information-seeking questions, comprising 720 questions used to train information-seeking agents from the Alibaba group (Wu et al., 2025a; Li et al., 2025b; Tao et al., 2025), and 3,624 questions from the Miroverse-v0.1 13 PRINTS : Reward Modeling for Long-Horizon Information Seeking Method FRAMES GAIA Level 1 Level 2 Qwen3-32B PRINTS 54.7 58.7 44.5 49. 29.5 33.3 Avg. 42.9 47.2 Table 6. Ablation study of summarizer. The Qwen3-32B approach utilizes Qwen3-32B as summarizer and employs the PRM trained solely as scorer. PRINTS that simultaneously acts as summarizer and scorer shows better performance, showing that two abilities are complementary. Figure 6. Dataset scaling. Experiments on the impact of dataset scaling on GAIA Level 2 using Qwen3-32B. Training PRINTS shows strong sample efficiency, achieving performance gain using only 50%(1k samples) of our annotation data. in Figure 6, using only 50% (i.e., 1k samples) of our annotation data still achieves 29.5 accuracy in GAIA Level 2, surpassing the base agent approach by training relatively lightweight model (4B). The scaling curve saturates beyond 100%, indicating that our 2k preference pairs represent data-efficient point. This demonstrates strong sample efficiency compared to fine-tuning agents, which typically require 10k-100k samples and expensive long-horizon rollouts using relatively larger models, requiring substantial computational resources. dataset (MiroMind Data Team, 2025), large-scale agent dataset covering multi-hop QA, web navigation, and scientific reasoning tasks. For the score annotation process, we execute = 8 rollouts per trajectory step to estimate the mean accuracy and information gain score. We discard steps that are either too easy (mt = 1) or too hard (mt = 0), resulting in 2,294 preference pairs used for training. As shown in Figure 5, our annotation pipeline produces well-balanced information gain score distributions across the full score range. This balanced distribution ensures that PRINTS learns to estimate diverse trajectory step quality, from harmful steps that reduce success probability to highly effective steps that substantially advance toward the correct answer. Moreover, the distributions of winning and losing steps exhibit clear separation. This clear separation validates the effectiveness of our preference pair construction and using this as training signals for PRINTS to output dense and comparative scores. Frontier Model Performance. For the performance of frontier models on the GAIA benchmark, we follow the reported results (OpenAI DeepResearch: 67.4% and DeepSeek-V3.1-671B: 63.1%) in the DeepResearch-30BA3B paper (Li et al., 2025a) and use these reported numbers as reference points when evaluating improvements brought by integrating PRINTS into DeepResearch-30B-A3B. B. Additional Experiments PRINTS as Summarizer. To validate our design of jointly training scoring and summarization within single model, we compare PRINTS against variant that uses separate models for each capability. Specifically, we train PRM using only GRPO for scoring without SFT for summarization, and pair it with Qwen3-32B the same model we use for summary annotation to generate summaries at test-time. As shown in 6, PRINTS, which is jointly trained as both scorer and summarizer through our alternating SFTGRPO schedule, outperforms this separated design. This demonstrates that the two abilities are complementary and that our alternating training schedule enables seamless integration of these abilities. We hypothesize that this benefit arises from positive transfer between the two objectives. As both abilities operate on the same input (i.e., query, preceding summary, latest tool response, current trajectory step), learning to distill essential information during SFT directly aids GRPO optimization by highlighting the most relevant factors for quality evaluation. Dataset Scaling To further validate that PRINTS is cost-efficient way of improving information-seeking behaviors of agents without fine-tuning them, we conduct an ablation study on the training dataset scaling. As shown 14 PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 7. Examples of preference pairs constructed by our annotation pipeline. 15 PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 8. Input prompt for PRINTS when the model is trained with GRPO for scoring ability and acts as scorer at test-time. 16 PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 9. Input prompt for PRINTS when the model is trained with SFT for summarization ability and acts as summarizer at test-time. 17 PRINTS : Reward Modeling for Long-Horizon Information Seeking Figure 10. PRINTS step-level evaluation examples on GAIA query. Among four candidate steps, we show the highest-scoring (top) and lowest-scoring (bottom) steps. The high-quality step acknowledges uncertainty and initiates an appropriate tool call to gather missing information, while the low-quality step makes unverified assumptions and confidently produces an unsupported answer without evidence."
        }
    ],
    "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of Texas at Austin"
    ]
}