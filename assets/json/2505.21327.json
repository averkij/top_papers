{
    "paper_title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs",
    "authors": [
        "Jiakang Yuan",
        "Tianshuo Peng",
        "Yilei Jiang",
        "Yiting Lu",
        "Renrui Zhang",
        "Kaituo Feng",
        "Chaoyou Fu",
        "Tao Chen",
        "Lei Bai",
        "Bo Zhang",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "MME-Reasoning MME-Reasoning: Comprehensive Benchmark for Logical Reasoning in MLLMs Jiakang Yuan1,3,, Tianshuo Peng2,3,, Yilei Jiang2, Yiting Lu4, Renrui Zhang2, Kaituo Feng2, Chaoyou Fu5, Tao Chen1,, Lei Bai3, Bo Zhang3,, Xiangyu Yue2,3 1 Fudan University 2 MMLab, The Chinese University of Hong Kong 3 Shanghai AI Laboratory 4 University of Science and Technology of China 5 Nanjing University https://alpha-innovator.github.io/mmereasoning.github.io/ https://github.com/Alpha-Innovator/MME-Reasoning https://huggingface.co/datasets/U4R/MME-Reasoning"
        },
        {
            "title": "Abstract",
            "content": "Logical reasoning is fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as thinking mode and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities."
        },
        {
            "title": "1\nLogical reasoning (Liu et al., 2025a), a fundamental cognitive process of analyzing premises and\nevidence to reach valid conclusions, serves as the cornerstone of human intelligence. Multimodal\nreasoning (Jaech et al., 2024) enables humans to integrate information from different modalities,\nsuch as visual and text, which is essential for tackling complex tasks. Recently, with the emergence\nof reasoning large language models (LLMs) (Dubey et al., 2024; Yang et al., 2024a) such as DeepSeek-\nR1 (DeepSeek-AI, 2025), injecting reasoning capability into multimodal large language models\n(MLLMs) (OpenAI, 2024; Qwen Team, 2025a; Li et al., 2024) has begun to be explored (Peng et al.,",
            "content": "Equal contribution, Corresponding authors. 1 5 2 0 2 7 2 ] . [ 1 7 2 3 1 2 . 5 0 5 2 : r MME-Reasoning Figure 1: Performance comparison between thinking and chat models on MME-Reasoning. 2025b; Zhang et al., 2025a; Huang et al., 2025). Despite the significant progress in reasoning MLLMs, comprehensive evaluation of their capabilities still remains an open challenge. Therefore, it is particularly important to establish fair and robust evaluation benchmark for assessing the reasoning capabilities of MLLMs and further accelerate the development of this field. Currently, most benchmarks (Fu et al., 2023; Wang et al., 2024a; Lu et al., 2023; Yue et al., 2024a;b; Gong et al., 2024; He et al., 2024) designed for multimodal reasoning primarily focus on knowledgedriven tasks. For example, MathVista (Lu et al., 2023) and MathVerse (Zhang et al., 2024) provide comprehensive evaluations of MLLMs mathematical reasoning abilities. OlympiadBench (He et al., 2024) and EMMA (Hao et al., 2025) expand the scope to include additional subjects, such as physics and chemistry. Apart from knowledge-driven tasks, some works (Song et al., 2025; Chia et al., 2024; Zhang et al., 2025b) have begun to decouple knowledge from logical reasoning, aiming to assess the reasoning abilities of MLLMs independent of specific domain knowledge. For instance, SciVerse (Guo et al., 2025b) and VisualPuzzles (Song et al., 2025) focus on reasoning-focused, knowledge-light tasks. Despite recent advances, existing benchmarks still suffer from several problems as outlined below. First, lacking explicit categorization of reasoning and insufficient coverage of reasoning types. In logic, reasoning is typically classified into three types: abduction, deduction, and induction (Peirce, 2014). Most existing benchmarks primarily concentrate on evaluating MLLMs inductive and deductive reasoning ability. For example, most of the questions in MathVerse (Lu et al., 2023) belong to deductive reasoning, which uses rules and premises to derive conclusions. PuzzleVQA (Chia et al., 2024) only contains questions of inductive reasoning, which learns rules based on premises and conclusions. However, abductive reasoning ability (i.e., exploring premises to explain conclusion based on the conclusion and rules) is rarely evaluated. Second, the concept of reasoning is not clear enough, which is reflected in confusing perception with reasoning or equating reasoning with the complexity of the required knowledge. For example, MathVista (Lu et al., 2023) contains many questions that can be answered through visual perception, while OlympiadBench (He et al., 2024) includes questions that require advanced domain knowledge, which the model may not have access to. This may lead to an inaccurate evaluation of MLLMs reasoning ability. To address these issues, we introduce MME-Reasoning, comprehensive benchmark specifically designed to evaluate the reasoning capability of MLLMs. MME-Reasoning consists of 1,188 carefully curated questions that systematically cover types of logical Table 1: Response token length on different datasets. Model MathVista MathVerse MME-R. Qwen2.5-VL-7B GPT-4o Claude-3.7-Sonnet-T 209.5 162.6 519.4 207.6 157.3 563.2 442.8 328.0 979.2 2 MME-Reasoning Figure 2: Example of questions in MME-Reasoning which covers comprehensive reasoning types. reasoning (i.e., inductive, deductive, and abductive), while spanning range of difficulty levels, as illustrated in Fig. 2. Besides, we identify 5 key abilities related to multimodal reasoning, including calculation, planning and exploring, spatial-temporal, pattern analysis, and casual chaining analysis, and annotate the type of ability assessed by each question. To ensure true evaluation of reasoning ability, MME-Reasoning eliminates questions that can be answered purely through perception or require complex domain knowledge, thereby focusing on the core reasoning skills of the model. We report the average response lengths of three representative models across different datasets in Tab. 1. Results show that responses on MME-Reasoning are significantly longer than those on previous reasoning benchmarks, indicating its challenging and rigorous demands on model reasoning. Furthermore, MME-Reasoning incorporates variety of evaluation methods, including multiple-choice, free-form, and rule-based (e.g., Sudoku Puzzles) questions. Employing 3 MME-Reasoning multiple evaluation methods enables wider variety of question types, thereby facilitating more comprehensive evaluation of models capabilities. Experiments were conducted on state-of-the-art MLLMs, covering Chat and Thinking types of both open-source and closed-source, as presented in Fig. 1. Evaluations with MME-Reasoning reveal these key findings: MLLMs exhibit significant limitations and pronounced imbalances in reasoning capabilities. Even the most advanced MLLMs achieve only limited results under holistic logical reasoning evaluation, with Gemini-Pro-2.5-Thinking scoring only 60.19%, followed by Seed1.5-VL (59.85) and o4-mini (57.49%). These results indicate that MME-Reasoning, through its comprehensive evaluation of all the logical reasoning types, establishes systematic and challenging benchmark for multimodal reasoning. Abductive reasoning remains major bottleneck for current MLLMs. While most models demonstrate competent deductive reasoning, their abductive reasoning lags significantly. Closed-source models exhibit an average gap of 5.38 points between deductive and abductive tasks, which further widens to 9.81 among open-source models, making abductive reasoning key bottleneck. Since it underpins many real-world tasks, addressing this gap is crucial for improving overall reasoning. Reasoning length scales with task difficulty, benefiting performance but accompanied by marginal effects and decreasing token efficiency. Thinking Models exhibit longer reasoning chains, particularly on more difficult questions, demonstrating adaptive inference budgeting and enhanced depth of reasoning. positive correlation between average token count (ATC) and accuracy supports the effectiveness of extended outputs, especially in complex tasks. However, this performance gain plateaus beyond certain length, revealing diminishing returns."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Multimodal Reasoning Chain-of-thought (CoT) reasoning (Wei et al., 2022) has emerged as key paradigm for enhancing the reasoning capability of LLMs. By generating intermediate steps before the final answer, CoT enables more transparent and accurate decision-making, especially in complex tasks such as arithmetic, logical deduction, and commonsense reasoning. Inspired by its success in text-only settings, CoT has recently been extended to MLLMs, giving rise to multimodal chain-of-thought (MCoT) reasoning (Jiang et al., 2024; Zhang et al., 2023; Chen et al., 2023; Peng et al., 2024; Lu et al., 2025; Xia et al., 2024a). Early approaches such as Multimodal-CoT (Zhang et al., 2023) and IPVR (Chen et al., 2023) demonstrate that generating intermediate reasoning steps significantly improves model performance in visual question answering. Other methods such as HoT (Yao et al., 2023), BDoG (Zheng et al., 2024), and VisualSketchpad (Hu et al., 2024) introduce graph structures, debating agents, and visual intermediate states to further enhance interpretability and reasoning depth. More recently, following the success of Deepseek-R1, the Generalized Reinforcement Preference Optimization (GRPO) algorithm has gained traction in the development of multimodal models. Methods such as MM-EUREKA (Meng et al., 2025), Vt-R1 (Zhou et al., 2025), LMM-R1 (Yingzhe et al., 2025), and R1-V (Chen et al., 2025) adapt GRPO to solve mathematical geometry tasks, demonstrating promising reflective reasoning capabilities. Other works, including VLM-R1 (Shen et al., 2025), Visual-RFT (Liu et al., 2025b), and Seg-Zero (Yuqi et al., 2025), apply GRPO to enhance 4 MME-Reasoning Figure 3: The overall construction process of MME-Reasoning. visual competencies such as grounding, object detection, and classification. The algorithm has also been extended to video and audio modalities through models such as Video-R1 (Feng et al., 2025), and R1-Omni (Zhao et al., 2025). 2.2 Multimodal Reasoning Benchmarks Recent benchmarks have advanced the evaluation of multimodal reasoning, particularly in visuallanguage settings. Early works such as CLEVR (Johnson et al., 2016) and GQA (Hudson & Manning, 2019) assess compositional and spatial reasoning, while more recent benchmarks such as MathVista (Lu et al., 2024), PuzzleBench (Zhang et al., 2025b), ChartX (Xia et al., 2024b) and PuzzleVQA (Chia et al., 2024) emphasize symbolic logic or pattern discovery. However, these benchmarks typically focus on narrow subtypes of reasoningespecially inductive logicand fail to offer holistic evaluation across deductive, inductive, and abductive paradigms. Furthermore, many existing datasets conflate perception with reasoning. Tasks solvable via recognition or superficial pattern matching are often labeled as reasoning challenges, while high-difficulty benchmarks such as GPQA (Rein et al., 2023), OlympiaBench (He et al., 2024) and MME-CoT (Jiang et al., 2025) overly depend on domain-specific knowledge rather than logical inference. Evaluation protocols are also limitedmost rely on multiple-choice formats and lack support for open-ended or rule-based assessment. In contrast, our benchmark provides fine-grained evaluation of visual reasoning, explicitly covering the three classical reasoning types."
        },
        {
            "title": "3 The MME-Reasoning Benchmark",
            "content": "We introduce MME-Reasoning, comprehensive benchmark designed to evaluate the reasoning ability of MLLMs. MME-Reasoning consists of 1,188 questions, including 1,008 newly collected items. MME-Reasoning comprehensively covers three types of reasoning (i.e., inductive, deductive, and abductive) and includes three question types (i.e., multiple-choice, free-form, and rule-based). We further divided MME-Reasoning into three difficulty levels (i.e., easy, medium, and hard). The key statistics and construction pipeline of MME-Reasoning are shown in Tab. 2 and Fig. 3. 5 MME-Reasoning Table 2: Statistics of MME-Reasoning. Statistics Total - Newly-add questions - Sampled questions Question Type - Multi-choice questions - Free-form questions - Rule-based questions Image Type - Single-image questions - Multi-image questions Disciplinary - Disciplinary questions - Non-discipl. questions Number 1188 (100%) 84.85% 15.15% 58.50% 31.57% 9.93% 58.50% 31.57% 31.48% 68.52% Figure 4: Overview of MME-Reasoning. 3.1 Design Principles of MME-Reasoning To ensure comprehensive evaluation of multimodal reasoning and address issues present in previous benchmarks, such as incomplete coverage of reasoning types, unclear definitions of reasoning, and insufficient evaluation methods, MME-Reasoning is guided by the following principles: 1) Comprehensiveness. According to Charles Sanders Peirces classification of reasoning, deduction, induction, and abduction can be distinguished based on different arrangements of rule, case, and result. Therefore, comprehensive evaluation of reasoning ability should include all three types of reasoning tasks. 2) Beyond Perception. Each question should be carefully designed to ensure that the answer is obtained through reasoning process instead of simple visual recognition. 3) Minimizing Knowledge Reliance. It is essential to ensure that the questions do not require complex domain knowledge, thereby preventing models from being penalized for the absence of specialized information. In MME-Reasoning, the domain expertise is limited to K12 or below. 4) Diverse evaluation formats. The benchmark should consist of diverse question types, avoiding incomplete evaluation caused by narrow range of task types. 3.2 Data Collection and Curation Data Collection. We initiate by collecting questions related to multimodal reasoning from variety of sources, including 1) Textbooks can provide subject exam questions (e.g., mathematics, physics, chemistry, and biology). To evaluate reasoning ability, the chemistry and biology questions mainly focus on reaction process inference, and genetic lineage inference. 2) Online resources, books on logical practice, and Chinese Civil Service Examination (Logic Test) primarily includes IQ test questions, logic games (e.g., Mate-in-one), and other tasks highly related to logical reasoning. 3) Synthetically generated questions. Some visual reasoning problems, such as Number Bridge, Sudoku, and mazes, can be generated based on specific rules. We develop code to produce wide variety of such logic puzzles, covering different types and range of difficulty levels. 4) Questions from existing benchmarks. We sample 80 questions from PuzzleVQA (Chia et al., 2024) and 100 questions from MMIQ (Cai et al., 2025), excluding questions based on shape size identification, as such questions may not effectively assess the models reasoning ability. 5) Self-designed questions. We mainly construct questions related to spatial and temporal reasoning. The spatial reasoning 6 MME-Reasoning Figure 5: Evaluation of rule-based questions. questions involve tasks such as determining relative spatial relationships and navigation, with the question design methodology inspired by VSIBench (Yang et al., 2024b). For temporal reasoning, the questions mainly focus on sequence judgment. We sample frames from videos in YouCook2 (Zhou et al., 2018) and VideoMME (Fu et al., 2024) as the sources of images. Note that for questions with well-defined rules such as Number Bridge Puzzles, we include the corresponding rules as part of each question. The composition of MME-Reasoning is shown in Fig. 4 and please refer to the Appendix for more details about the question source and type. Data Curation. We initially collect around 4k questions from various sources mentioned above. Following the design principles of MME-Reasoning, we conduct careful manual curation process to ensure the quality of the benchmark. Specifically, we exclude questions that depend solely on visual recognition, require complex domain-specific knowledge, too easy to evaluate the reasoning ability. This curation process ensures that the remaining questions are well-aligned with our goal of evaluating visual reasoning ability, rather than perceptual skills or the breadth of specialized knowledge. For questions with multiple possible answers, we first try to convert them into rule-based (will be introduced in Sec. 3.3) or multiple-choice questions; otherwise, discard them. Additionally, we remove questions that place excessive demands on instruction-following ability. Finally, to comprehensively evaluate the multimodal reasoning ability, we balance the distribution of questions across the three reasoning types. This approach prevents the benchmark from being overly biased towards evaluating the ability of any single reasoning type. Through this data curation process, we filter 1,008 questions from the initially collected questions. Metadata Annotation. Further, we annotate questions in MME-Reasoning with information including question type (i.e., multiple-choice, free-form, and rule-based), difficulty (i.e., easy, medium, hard), capability (i.e., pattern analysis, planning and exploring, spatial and temporal, calculation, casual chain analysis), and reasoning type (i.e., deductive, inductive, and abductive). For specific rules for annotating metadata, please refer to our appendix. 3.3 Evaluation Protocols Following MathVista (Lu et al., 2023), the evaluation consists of two steps: extracting answers and judging answers. For different types of questions (i.e., multiple-choice, free-form, and rule-based), we designed specific prompts for GPT to extract answers. These prompts are composed of extraction rules and examples that are similar to MathVista (Lu et al., 2023). For multiple-choice questions, we match the extracted answers with the reference answers. For free-form questions, we use GPT to judge the consistency between the extracted answers and the reference answers following MathVerse (Zhang et al., 2024). For rule-based questions, we first use GPT to extract answers and 7 MME-Reasoning convert them into an intermediate format, which is then judged using specific scripts. For example, in Number Bridge problem, we first use GPT to extract the start and end points of each bridge, then convert the answers into specific matrix format, and finally determine correctness based on predefined rules, as illustrated in Fig. 5."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings We conduct extensive evaluations on state-of-the-art MLLMs include: Thinking Models. We first evaluate several thinking MLLMs that focus on improving the models multimodal reasoning which can be divided into Close-source models including (1) GPT-o1 (Jaech et al., 2024), and o4-mini (, 2025); (2) Gemini-2.5-Flash-Thinking and Gemini-2.5-Pro-Thinking (Gemini et al., 2023); (3) Claude-3.7-Sonnet-Thinking, Claude-4-Sonnet-Thinking (Anthropic, 2022); (4) Seed1.5-VL-Thinking (Guo et al., 2025a); and Open-source models including (1) QvQ-72BPreview (Team, 2024); (2) Kimi-VL-A3B-Thinking (Team et al., 2025b); (3)LlamaV-o1 (Thawakar et al., 2025); (4) Virgo-72B (Du et al., 2025). Chat Models. Further, we also evaluate SoTA chat models as follows. Close-source models: (1) GPT-4o (OpenAI, 2024); (2) Claude-3.7-Sonnet (Anthropic, 2022) (3) Kimi-latest (Team et al., 2025a); (4) Seed1.5-VL (Guo et al., 2025a). Open-source models: (1) Qwen-2.5-VL (7B, 32B, 72B) (Qwen Team, 2025a); (2) InternVL-3 (8B, 38B, 78B) (Zhu et al., 2025); (3) LLaVA-Onevision-72B (Li et al., 2024); (4) Molmo (7B-O, 7B-D, 72B) (Deitke et al., 2024); (5) Kimi-VL-A3B-Instruct (Team et al., 2025b). Rule based RL Models. Rule-based Reinforcement Learning (RL) has been shown to be highly promising strategy for eliciting reasoning paradigms in models. Therefore, we further evaluated MLLMs trained using Rule-based RL, including: (1) R1-VL (Zhang et al., 2025a), (2) R1Onevision (Yang et al., 2025), (3) Vision-R1 (Huang et al., 2025), (4) MM-Eureka (7B, 32B) (Meng et al., 2025), (5) VL-Rethinker (7B, 72B) (Wang et al., 2025). We use GPT-4o-mini to extract answers from model responses. Due to rate limits, we sample 302 questions to construct mini-set with the same distribution for o1s evaluation, all other models are evaluated on the entire benchmark. 4.2 Main Results Tab. 3 shows the performance comparison of different MLLMs and prompting strategies. MME-Reasoning poses significant challenges for vision-language reasoning. The best-performing model, Gemini-2.5-Pro-Thinking, achieved an average score of 60.2%. The latest MLLM, Seed1.5-VL, achieved comprehensive score of 59.9. Representative reasoning models o4-mini and o1 obtained scores of 57.5 and 45.7, respectively. Qwen2.5-VL and Claude-3.7-Sonnet achieved scores of 35.9 and 57.2 on OlympiadBench, yet only reached 34.1 on MME-Reasoning. These results indicate that the benchmark sets stringent standards for evaluating models logical reasoning capabilities by comprehensively assessing three distinct reasoning types. Prominent bias in logical reasoning performance within MLLMs. In almost all cases, models exhibit dominant deductive reasoning performance, while abductive reasoning is considerably weaker. Closed-source models demonstrate an average deductive advantage of 5.38 over abductive reasoning, which widens to 9.81 among open-source models, making abductive reasoning 8 MME-Reasoning Table 3: Performance comparison of state-of-the-art MLLMs on MME-Reasoning. The top three are highlighted in blue . indicates the model was evaluated on the mini-set. T\" represents Thinking\". Model Model Capability Reasoning Type AVG. CAL. P& E. PA. S&T. CCA. DED. IND. ABD. Gemini-2.5-Pro-T Seed1.5-VL-T o4-mini o1 Claude-4-Sonnet-T Claude-3.7-Sonnet-T Gemini-2.5-Flash-T Seed1.5-VL GPT-4o Claude-3.7-Sonnet Kimi-Latest QVQ-72B-Preview Virgo-72B VL-Rethinker-72B VL-Rethinker-7B MM-Eureka-Qwen-32B MM-Eureka-Qwen-7B R1-VL-7B Vision-R1-7B R1-Onevision-7B-RL Kimi-VL-A3B-T Qwen2.5-VL-72B Qwen2.5-VL-32B Qwen2.5-VL-7B InternVL3-78B InternVL3-38B InternVL3-8B Molmo-72B Molmo-7B-D LLaVA-OV-72B Kimi-VL-A3B 68.0 67.2 63.1 50.0 33.3 30.4 19.8 52.0 21.4 29.0 21.4 37.4 30.4 33.6 24.7 23.0 27.1 16.3 18.2 19.5 28.7 31.7 32.2 22.2 26.0 23.0 19.5 12.5 11.7 17.1 18. Close-source & Thinking 64.4 62.7 58.3 38.5 35.9 27.6 21.3 53.7 56.0 57.2 41.5 33.0 32.3 20.9 52.1 47.2 50.4 43.7 36.2 38.3 33.0 90.3 82.6 59.0 52.4 47.9 46.5 38.9 Close-source & Chat 42.0 22.1 24.6 17.4 38.4 30.5 32.8 19.8 44.0 38.6 35.5 29.1 72.9 36.8 46.5 41.0 Open-source & Thinking 27.1 22.9 28.4 17.7 25.7 19.3 11.6 18.0 12.2 16. 28.8 26.1 31.4 23.5 25.6 22.3 17.7 17.9 20.0 19.5 35.8 36.2 37.2 39.4 36.2 31.9 30.9 34.4 31.6 32.3 57.6 47.2 59.7 42.4 50.7 50.0 26.4 36.1 27.1 35.4 Open-source & Chat 25.1 26.8 18.2 24.0 18.5 19.6 11.9 8.6 18.0 11.9 27.2 24.4 21.9 26.5 23.0 22.6 14.7 8.1 23.9 21. 37.9 39.0 35.1 41.8 38.3 31.6 28.7 27.3 32.3 34.0 53.5 52.1 36.1 50.0 41.7 41.0 28.5 23.6 38.9 27.8 64.0 64.5 60.6 50.8 39.4 34.6 28.1 54.9 29.0 35.7 27.7 41.6 37.7 39.0 34.4 32.9 32.7 25.3 27.4 27.7 33.3 39.0 40.5 31.4 35.1 33.5 28.1 23.1 20.7 27.4 25. 51.7 52.3 51.4 42.3 32.0 36.2 22.1 45.0 34.7 38.7 25.4 33.5 32.6 36.0 29.9 30.5 28.7 21.8 26.3 24.8 25.1 32.3 27.5 27.5 33.8 29.0 29.9 18.4 10.9 30.5 26.3 62.8 60.8 59.0 42.3 35.7 31.7 24.6 41.0 27.9 26.1 19. 29.1 24.4 31.9 22.9 28.1 22.6 15.8 18.1 14.6 18.1 29.9 29.6 20.9 27.1 22.1 21.4 14.3 11.1 19.9 17.1 60.2 59.9 57.5 45.7 36.1 34.1 25.2 47.5 30.2 33.3 24.4 35.2 31.8 35.8 29.3 30.6 28.2 21.1 24.0 22.5 25.9 34.1 33.2 26.8 32.1 28.4 26.4 18.9 14.7 25.8 23. significant bottleneck in comprehensive logical reasoning performance. Deductive reasoning maintains high proportion in the training corpus due to its widespread distribution. Abductive reasoning processes usually involve larger exploration spaces and richer assumptions, hypotheses, and reflections, making its data challenging to scale. However, non-deductive reasoning plays central role in general reasoning scenarios and many scientific discoveries. These findings highlight 9 MME-Reasoning Figure 6: Comparison of Difficulty Level and Average Token Count on MME-Reasoning. Figure 7: Results within different difficulty levels. Figure 8: Response tokens vs. Performance. the necessity for researchers to develop more comprehensive understanding of models logical reasoning abilities to facilitate their application in real-world scenarios. Moreover, the models scores under different reasoning types are typically score below 40, indicating that MME-Reasoning provides promising metric for evaluating reasoning capabilities from multiple perspectives. Limited performance in open-ended reasoning scenarios. Models generally demonstrate relative advantages in Casual Chain Analysis but perform poorly on tasks involving Plan & Exploration. This may benefit from the autoregressive paradigm continuously aiding models in learning causal dependencies within input sequences. However, it also highlights critical shortcoming: current state-of-the-art models struggle with planning and exploration in open-ended problem-solving spaces. To advance models in solving difficult practical problems, it is critical to innovate learning paradigms and strategies generation mechanisms suitable for open scenarios. Thinking capability directly contributes to enhanced logical reasoning. Models employing \"thinking mode\" typically generalize test-time scaling to reasoning scenarios through generating longer chains-of-thought (CoT), reflections, and self-corrections. In most cases, \"thinking models\" significantly outperform their base version. QvQ improved by 1.1 compared to Qwen2.5-VL, and VL-Rethinker improved by 1.7 compared to Qwen2.5-VL. This effect is more pronounced among closed-source models: Seed1.5-VL-T outperformed Seed1.5-VL by 12.4, and o1 exceeded GPT-4o by 15.5. Further experiments concerning thinking models will be elaborated in subsequent sections. 10 MME-Reasoning Figure 9: Case study of Mate-in-one problem. Rule-based RL does not always work. Rule-based RL has shown significant potential in activating the \"thinking mode\" of foundational models, encouraging longer output and reflection to tackle hard problems. However, we observed that methods adopting rule-based RL do not consistently outperform their base models. Most models at the 7B scale experienced performance degradation. This suggests that the potential of rule-based RL remains inadequately realized, failing to effectively extend advantages demonstrated in LLMs into multimodal domains and possibly reducing generalization. Thus, innovation in training paradigms, rather than merely replicating R1, is urgently needed. 4.3 Fine Grained Analysis of Reasoning Behavior Does increasing the length of the reasoning process help? To investigate whether increased output length consistently leads to improved accuracy, we selected 10 representative models, including Chat Models (e.g., GPT-4o) and Thinking Models (e.g., o4-mini). In Fig. 8, we present the semi-log plot of average token count (ATC) versus accuracy. The overall trend reveals that models with longer outputs tend to achieve higher scores, indicating the effectiveness of extending the reasoning process to enhance logical reasoning performance. As the token number increases, model performance exhibits exponential growth pattern, suggesting diminishing returns from simply increasing output length. Compared to Thinking Models, Chat Models demonstrate higher token efficiency. These findings highlight the computational cost associated with scaling up inference for improved performance. Balancing reasoning efficiency and model effectiveness remains challenge for future research. Is the length of the reasoning process strongly correlated with task difficulty? To examine whether models spontaneously allocate more inference budget to more challenging questions, we conducted research on using representative Thinking Models such as o4-mini and Chat Models such as GPT-4o. We first analyzed the accuracy of different models across varying levels of difficulty, as shown in Fig. 7. With increasing difficulty, model performance declines significantly, confirming the validity of MME-Reasonings difficulty stratification and providing foundation for subsequent analyses. Besides, Fig. 6 illustrates the trend of ATC across different reasoning types and difficulty levels. It reveals consistent pattern: overall, output length increases steadily with rising difficulty. This trend holds across varying output lengths, model categories, and reasoning types. Compared 11 MME-Reasoning to Chat Models, Thinking Models exhibit more pronounced increase in ATC as difficulty rises. For instance, the ATC of Seed1.5-VL increases by up to 3k tokens, and o4-mini by up to 5k tokens. In contrast, the ATC increase for Qwen2.5-VL and GPT-4o remains within 300 tokens. 4.4 Case Study In Fig. 9, we present an example of abductive reasoning which demands planning and exploration. From this case, several key observations can be identified: (1)Long reasoning process: The selected models generated over 1k tokens in response, with o4-mini producing up to 24.6k tokens. This demonstrates that MME-Reasoning constitutes highly challenging benchmark for multimodal reasoning. (2)Planning in the problem-solving process: The response includes multiple iterations of hypothesis generation (possible movement) feasibility verification (check escape squares) check , indicating that the model spontaneously engages in structured planning and reflection to explore solutions within an open-ended problem-solving spaces. (3)Repetitive reflection: We observed that the model tends to revisit and reflect on the same reasoning paths multiple timesup to 7 instances in some cases. This behavior may result in significant computational overhead and informational redundancy. Balancing reasoning efficiency with performance remains critical issue to be addressed."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce MME-Reasoning, comprehensive benchmark designed to evaluate MLLMs logical reasoning abilities across inductive, deductive, and abductive reasoning types. Through careful data curation and an expanded evaluation protocol, our benchmark provides holistic assessment of reasoning capabilities, beyond simple perception or high-level knowledge. Our experiments reveal that existing MLLMs still face significant challenges and exhibit notable performance imbalances across different reasoning types. These findings underscore the need for further research and development to enhance the reasoning abilities of MLLMs, paving the way for more generalizable AI systems. 12 MME-Reasoning"
        },
        {
            "title": "References",
            "content": "OpenAI (2025). Openai o3 and o4-mini system card, 2025. URL https://openai.com/index/ o3-o4-mini-system-card/. https://www.anthropic.com/index/introducing-claude Anthropic. Claude, 2022. URL https: //www.anthropic.com/index/introducing-claude. Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. Huanqia Cai, Yijun Yang, and Winston Hu. Mm-iq: Benchmarking human-like abstraction and reasoning in multimodal models. arXiv preprint arXiv:2502.00698, 2025. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. arXiv preprint arXiv:2301.05226, 2023. Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Virgo: preliminary exploration on reproducing o1-like mllm. arXiv preprint arXiv:2501.01904, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 13 MME-Reasoning Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023. URL https://arxiv.org/abs/2312. 11805. Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, et al. Av-odyssey bench: Can your multimodal llms really understand audio-visual information? arXiv preprint arXiv:2412.02611, 2024. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025a. Ziyu Guo, Ray Zhang, Hao Chen, Jialin Gao, Dongzhi Jiang, Jiaze Wang, and Pheng-Ann Heng. Sciverse: Unveiling the knowledge comprehension and visual reasoning of lmms on multi-modal scientific problems. arXiv preprint arXiv:2503.10627, 2025b. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world In IEEE Conference on Comvisual reasoning and compositional question answering. puter Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, 10.1109/CVPR.2019. pp. 67006709. Computer Vision Foundation / IEEE, 2019. 00686. URL http://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_ Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html. doi: Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 14 MME-Reasoning Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency, 2025. URL https://arxiv.org/abs/2502.09621. Yilei Jiang, Yingshui Tan, and Xiangyu Yue. Rapguard: Safeguarding multimodal large language models via rationale-aware defensive prompting, 2024. URL https://arxiv.org/abs/2412.18826. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning, 2016. URL https://arxiv.org/abs/1612.06890. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, and Yue Zhang. Logical reasoning in large language models: survey. arXiv preprint arXiv:2502.09100, 2025a. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning In The Twelfth International Conference on Learning of foundation models in visual contexts. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=KUNzEQMWU7. Yiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi Qin, Xinyue Li, Le Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, et al. Omnicaptioner: One captioner to rule them all. arXiv preprint arXiv:2504.07089, 2025. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning, 2025. URL https://github.com/ModalMinds/MM-EUREKA. OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024. URL https://openai.com/ index/hello-gpt-4o/. Charles Sanders Peirce. Illustrations of the Logic of Science. Open Court, 2014. Tianshuo Peng, Mingsheng Li, Hongbin Zhou, Renqiu Xia, Renrui Zhang, Lei Bai, Song Mao, Bin Wang, Conghui He, Aojun Zhou, et al. Chimera: Improving generalist model with domainspecific experts. arXiv preprint arXiv:2412.05983, 2024. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025a. 15 MME-Reasoning Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025b. Qwen Team. Qwen2.5-vl, January 2025a. URL https://qwenlm.github.io/blog/qwen2.5-vl/. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/blog/qwq-32b/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof qa benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model, 2025. URL https://arxiv.org/abs/2504.07615. Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025a. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025b. Qwen Team. Qvq: To see the world with wisdom, December 2024. URL https://qwenlm.github.io/ blog/qvq-72b-preview/. Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, et al. Geox: Geometric problem solving through unified formalized vision-language pre-training. arXiv preprint arXiv:2412.11863, 2024a. MME-Reasoning Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024b. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. ArXiv preprint, abs/2407.10671, 2024a. URL https://arxiv.org/abs/2407.10671. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024b. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li, Xiaoyu Li, and Xian Sun. Thinking like an expert: Multimodal hypergraph-of-thought (hot) reasoning to boost foundation modals. arXiv preprint arXiv:2308.06207, 2023. Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. Peng Yingzhe, Zhang Gongrui, Zhang Miaosen, You Zhiyuan, Liu Jie, Zhu Qipeng, Yang Kai, Xu Xingzhong, Geng Xin, and Yang Xu. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024a. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024b. Liu Yuqi, Peng Bohao, Zhong Zhisheng, Yue Zihao, Lu Fanbin, Yu Bei, and Jia Jiaya. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement, 2025. URL https://arxiv.org/ abs/2503.06520. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025a. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Zeyu Zhang, Zijian Chen, Zicheng Zhang, Yuze Sun, Yuan Tian, Ziheng Jia, Chunyi Li, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Puzzlebench: fully dynamic evaluation framework for large multimodal models on puzzle solving. arXiv preprint arXiv:2504.10885, 2025b. 17 MME-Reasoning Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning, 2025. Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng Chua, and Qing Li. picture is worth graph: blueprint debate paradigm for multimodal reasoning. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 419428, 2024. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model, 2025. URL https://arxiv.org/ abs/2503.05132. Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 18 MME-Reasoning Technical Appendices and Supplementary Material for MME-Reasoning More Experimental Results A.1 Full Results on MME-Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Full Results on Mini-set of MME-Reasoning . . . . . . . . . . . . . . . . . . . . . . . . A.3 Human Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Results on Different Question Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Results with Test-Time Compute Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Results with CoT Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Token Usage of Thinking Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Results of Captioner & LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Annotation B.1 Difficult Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Reasoning Type Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Capability Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Implementation Details of Evaluation D.1 Prompts for Answer Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Examples of MME-Reasoning Limitation 20 20 20 20 20 22 22 25 25 25 27 27 28 28 28 19 MME-Reasoning"
        },
        {
            "title": "A More Experimental Results",
            "content": "A.1 Full Results on MME-Reasoning We present the performance of more baselines on MME-Reasoning in Tab 4, including OpenVLThinker (Deng et al., 2025), LMM-R1-MGT-PerceReason (Peng et al., 2025a), Mulberry (Yao et al., 2024), LlamaV-o1 (Thawakar et al., 2025) and Qwen2-VL series (Wang et al., 2024b). A.2 Full Results on Mini-set of MME-Reasoning We randomly sampled 25% of the questions and conducted manual review to ensure that the diversity of image types was maintained. These sampled questions were then used to to construct the Mini-set. We also analyzed the question distributions of both the Mini-set and the Full-set to ensure the sampled questions retained the same distribution. The statistical results are presented in Tab 5. We provide the performance of all baseline models on the Mini-set in Tab. 6. All baseline models achieved similar performance on both the Full-set and the Mini-set, further demonstrating the consistency of Mini-set and the comparability of model performance across different splits. A.3 Human Performance To evaluate expert-level performance on MME-Reasoning, we further report human performance on the mini-set of MME-Reasoning. As shown in Tab. 6, the human expert achieved an overall score of 83.4significantly outperforming the best-performing thinking model, Seed1.5-VL-T, which scored 62.6. Looking deeper into the reasoning types, the human expert scored 85.8, 76.9, and 85.6 on deductive, inductive, and abductive reasoning respectively, all of which are notably higher than the scores of the best-performing model. Moreover, the human expert demonstrated particularly strong ability in abductive reasoning, with performance comparable to that in deductive reasoningwhich is the key focus in current multi-modal reasoning research. This strength aligns with few top-performing models, but stands in contrast to most baseline models, which show clear weaknesses in abductive reasoning. These results highlight the significant gap that still exists between current thinking & chat models and human-level performance in comprehensive multimodal reasoning evaluation. Expanding complex reasoning tasks beyond domain-specific knowledge questions to include broader range of reasoning types and more diverse tasks will be crucial step toward addressing these current limitations. A.4 Results on Different Question Types We also evaluated the models performance across different question types and present the results in Tab. 7. A.5 Results with Test-Time Compute Scaling To evaluate whether the use of Test-Time Compute Scaling (TTS) methods can improve model performance on MME-Reasoning, we take Qwen2.5-VL-7B as an example and use Qwen2.5-VL-32B as the Reward Model. The evaluation is conducted using the Monte Carlo Tree Search (MCTS) algorithm, with the settings: branch = 3 and max-iteration = 18. The results are shown in Table 8. Under the MCTS-based setting, the models performance dropped noticeably across all reasoning types. We attribute this decline to two main factors: (1) Questions in MME-Reasoning often involve 20 MME-Reasoning Table 4: Performance comparison of state-of-the-art MLLMs on MME-Reasoning. The top three are highlighted in blue . T\" represents Thinking\". Model Model Capability Reasoning Type AVG. CAL. P& E. PA. S&T. CCA. DED. IND. ABD. Gemini-2.5-Pro-T Seed1.5-VL-T o4-mini Claude-4-Sonnet-T Claude-3.7-Sonnet-T Gemini-2.5-Flash-T Seed1.5-VL GPT-4o Claude-3.7-Sonnet Kimi-Latest QVQ-72B-Preview Virgo-72B VL-Rethinker-72B VL-Rethinker-7B MM-Eureka-Qwen-32B MM-Eureka-Qwen-7B R1-VL-7B Vision-R1-7B R1-Onevision-7B-RL Kimi-VL-A3B-T OpenVLThinker-7B LMM-R1-MGT-PerceReason Mulberry LlamaV-o1 Qwen2.5-VL-72B Qwen2.5-VL-32B Qwen2.5-VL-7B Qwen2.5-VL-3B Qwen2-VL-72B Qwen2-VL-7B Qwen2-VL-2B InternVL3-78B InternVL3-38B InternVL3-8B Molmo-72B Molmo-7B-D Molmo-7B-O LLaVA-OV-72B Kimi-VL-A3B 68.0 67.2 63.1 33.3 30.4 19.8 52.0 21.4 29.0 21. 37.4 30.4 33.6 24.7 23.0 27.1 16.3 18.2 19.5 28.7 19.8 22.2 14.6 14.9 31.7 32.2 22.2 17.6 19.2 15.7 13.0 26.0 23.0 19.5 12.5 11.7 8.1 17.1 18.7 90.3 82.6 59.0 47.9 46.5 38.9 72.9 36.8 46.5 41.0 57.6 47.2 59.7 42.4 50.7 50.0 26.4 36.1 27.1 35.4 34.7 34.0 31.3 25.0 53.5 52.1 36.1 32.6 44.4 30.5 19.4 50.0 41.7 41.0 28.5 23.6 15.3 38.9 27. Close-source & Thinking 64.4 62.7 58.3 35.9 27.6 21.3 53.7 56.0 57.2 33.0 32.3 20.9 52.1 47.2 50.4 36.2 38.3 33.0 Close-source & Chat 42.0 22.1 24.6 17. 38.4 30.5 32.8 19.8 44.0 38.6 35.5 29.1 Open-source & Thinking 27.1 22.9 28.4 17.7 25.7 19.3 11.6 18.0 12.2 16.0 14.6 16.0 13.3 7.7 28.8 26.1 31.4 23.5 25.6 22.3 17.7 17.9 20.0 19.5 19.3 23.7 18.8 16.5 35.8 36.2 37.2 39.4 36.2 31.9 30.9 34.4 31.6 32.3 35.8 37.9 33.7 28. Open-source & Chat 25.1 26.8 18.2 15.5 19.3 12.4 8.1 24.0 18.5 19.6 11.9 8.6 5.5 18.0 11.9 27.2 24.4 21.9 19.0 24.9 19.8 19.3 26.5 23.0 22.6 14.7 8.1 11.6 23.9 21.4 37.9 39.0 35.1 39.7 36.2 37.9 31.6 41.8 38.3 31.6 28.7 27.3 22.7 32.3 34.0 21 64.0 64.5 60.6 39.4 34.6 28. 54.9 29.0 35.7 27.7 41.6 37.7 39.0 34.4 32.9 32.7 25.3 27.4 27.7 33.3 30.7 30.3 23.8 22.4 39.0 40.5 31.4 28.5 28.8 25.5 22.7 35.1 33.5 28.1 23.1 20.7 16.6 27.4 25.9 51.7 52.3 51.4 32.0 36.2 22.1 45.0 34.7 38.7 25.4 33.5 32.6 36.0 29.9 30.5 28.7 21.8 26.3 24.8 25.1 24.8 32.3 25.4 21. 32.3 27.5 27.5 27.5 32.3 25.7 25.7 33.8 29.0 29.9 18.4 10.9 16.0 30.5 26.3 62.8 60.8 59.0 35.7 31.7 24.6 41.0 27.9 26.1 19.9 29.1 24.4 31.9 22.9 28.1 22.6 15.8 18.1 14.6 18.1 17.3 20.1 17.6 12.3 29.9 29.6 20.9 19.6 22.1 19.7 11.8 27.1 22.1 21.4 14.3 11.1 7.5 19.9 17.1 60.2 59.9 57.5 36.1 34.1 25. 47.5 30.2 33.3 24.4 35.2 31.8 35.8 29.3 30.6 28.2 21.1 24.0 22.5 25.9 24.6 27.4 22.1 18.8 34.1 33.2 26.8 25.6 27.5 23.4 19.9 32.1 28.4 26.4 18.9 14.7 13.4 25.8 23.1 MME-Reasoning Table 5: Comparison of statistics between full and mini-set of MME-Reasoning."
        },
        {
            "title": "Difficulty Level",
            "content": "DED. IND. ABD. Open MCQ Rule."
        },
        {
            "title": "Mini\nFull",
            "content": "39.7% 25.8% 34.4% 32.4% 58.3% 9.3% 31.8% 38.6% 27.9% 33.5% 31.6% 58.5% 9.9% 30.8% 39.4% 39.3% 28.8% 29.9% complex parallel reasoning, hypothesis generation, and reflection, rather than simple linear logical progression. These characteristics may not be effectively captured by the Reward Model. (2) The limited capabilities of the Reward Model result in guidance that lacks practical utility. We leave further exploration of TTS methods for reasoning to future work and hope that MMEReasoning can serve as representative benchmark for developing more general and comprehensive TTS algorithms in reasoning tasks. A.6 Results with CoT Prompt Chain-of-Thought (CoT) prompting increases output length by encouraging explicit output of the thought process, thereby enhancing reasoning performance. To investigate the impact of CoT on performance in MME-Reasoning, we evaluated the Qwen2.5-VL and InternVL3 series using CoT prompts shown in Tab. 10. The results are presented in Tab. 9. We observed that the Qwen2.5-VL models naturally tend to generate their reasoning process, so adding CoT prompt did not significantly increase output length. In contrast, InternVL3 models, under default settings, tend to directly output the final answer, and the CoT prompt substantially increased output length. In terms of performance, adding the CoT prompt consistently led to performance degradation for the Qwen2.5-VL series. For InternVL3, performance dropped for the 7B model but improved for the larger 38B and 78B models. One possible hypothesis is that for models already inclined to produce long outputs, explicit CoT instructions might introduce noise into the reasoning process. Conversely, for models that tend to answer questions directly, smaller models struggle to produce helpful and correct CoT outputs, but as model size increases, they begin to benefit noticeably from relatively accurate reasoning processes. A.7 Token Usage of Thinking Models In Fig. 10, we present the average token length of different thinking models on MME-Reasoning. Overall, there is clear trend indicating that better model performance is often associated with longer reasoning paths. However, we also observe diminishing returns between output length and performance in both open-source and closed-source models. Additionally, although current rule-based reinforcement learning (RL) models show promising trend of increased output length during training, no significant length gains were observed on MME-Reasoning. This limitation may stem from the limited types and inappropriate complexity of the reasoning tasks. Therefore, exploring how different types of reasoning tasks can better stimulate the effectiveness of RL in reasoning may be valuable direction for future research. 22 MME-Reasoning Table 6: Performance comparison of state-of-the-art MLLMs on mini set of MME-Reasoning. The top three are highlighted in blue . T\" represents Thinking\". Model Model Capability Reasoning Type AVG. CAL. P& E. PA. S&T. CCA. DED. IND. ABD. Human Expert 75.0 84.4 84.9 80.3 88. 85.8 76.9 85.6 83.4 Human Performance Gemini-2.5-Pro-T Seed1.5-VL-T o4-mini o1 Claude-4-Sonnet-T Claude-3.7-Sonnet-T Gemini-2.5-Flash-T Seed1.5-VL GPT-4o Claude-3.7-Sonnet Kimi-Latest QVQ-72B-Preview Virgo-72B VL-Rethinker-72B VL-Rethinker-7B MM-Eureka-Qwen-32B MM-Eureka-Qwen-7B R1-VL-7B Vision-R1-7B R1-Onevision-7B-RL Kimi-VL-A3B-T OpenVLThinker-7B LMM-R1-MGT-PerceReason Mulberry LlamaV-o1 Qwen2.5-VL-72B Qwen2.5-VL-32B Qwen2.5-VL-7B Qwen2.5-VL-3B Qwen2-VL-72B Qwen2-VL-7B Qwen2-VL-2B InternVL3-78B InternVL3-38B InternVL3-8B Molmo-72B Molmo-7B-D Molmo-7B-O LLaVA-OV-72B Kimi-VL-A3B 66.0 68.0 64.0 50.0 33.0 30.0 18.0 50.0 20.0 27.0 22.0 36.0 28.0 23.0 23.0 23.0 28.0 10.0 14.0 15.0 30.0 14.0 27.0 19.0 15. 31.0 31.0 19.0 21.0 20.0 16.0 12.0 25.0 19.0 19.0 11.0 12.0 7.0 13.0 18.0 Close-source & Thinking 63.5 67.7 58.3 38.5 30.2 17.7 16.7 58.5 58.5 56.6 41.5 35.8 36.8 15.1 49.3 49.3 45.1 43.7 39.4 38.0 39.4 Close-source & Chat 42.7 24.0 22.9 17.7 34.9 24.5 34.0 17.9 40.8 40.8 31.0 29.6 Open-source & Thinking 24.0 18.8 25.0 16.7 20.8 17.7 10.4 12.5 10.4 9.4 14.6 14.6 15.6 8.3 34.0 27.4 29.2 21.7 26.4 21.7 16.0 18.9 22.6 19.8 14.2 23.6 18.9 17. 33.8 43.7 39.4 47.9 38.0 32.4 35.2 39.4 35.2 26.8 33.8 38.0 33.8 31.0 Open-source & Chat 19.8 28.1 16.7 14.6 19.8 9.4 9.4 22.9 19.8 20.8 13.5 8.3 4.2 19.8 8.3 25.5 28.3 24.5 21.2 28.3 25.5 17.9 33.0 26.4 29.2 16.0 12.3 11.3 25.5 18.9 23 38.0 40.8 38.0 39.4 38.0 33.8 29.6 42.3 36.6 23.9 35.2 28.2 25.4 23.9 29. 85.7 83.3 54.8 52.4 50.0 38.1 33.3 69.0 33.3 42.9 33.3 47.6 38.1 42.9 40.5 38.1 50.0 16.7 31.0 19.0 31.0 28.6 33.3 33.3 26.2 42.9 45.2 33.3 31.0 38.1 26.2 19.0 40.5 38.1 35.7 31.0 16.7 14.3 35.7 9.5 60.8 67.5 57.5 50.8 42.5 31.7 27.5 57.5 31.7 31.7 30. 38.3 37.5 34.2 35.8 32.5 32.5 23.3 26.7 22.5 28.3 29.2 35.8 28.3 23.3 39.2 41.7 32.5 30.0 34.2 22.5 23.3 36.7 31.7 26.7 26.7 22.5 19.2 25.0 23.3 55.1 48.7 51.3 42.3 37.2 42.3 19.2 39.7 28.2 38.5 23.1 37.2 41.0 32.1 28.2 34.6 32.1 19.2 29.5 30.8 26.9 16.7 33.3 23.1 23.1 32.1 34.6 30.8 30.8 39.7 34.6 23.1 43.6 33.3 35.9 21.8 15.4 17.9 30.8 23. 65.4 67.3 60.6 42.3 33.7 27.9 26.0 39.4 27.9 27.9 19.2 29.8 21.2 31.7 26.0 25.0 22.1 16.3 16.3 16.3 16.3 16.3 18.3 18.3 15.4 26.0 27.9 21.2 21.2 19.2 16.3 11.5 24.0 23.1 21.2 17.3 9.6 4.8 17.3 11.5 60.9 62.6 57.0 45.7 38.1 33.1 24.8 46.7 29.5 32.1 24. 35.1 32.8 32.8 30.5 30.5 28.8 19.9 23.8 22.5 23.8 21.5 29.1 23.5 20.5 32.8 35.1 28.1 27.2 30.5 23.5 19.2 34.1 29.1 27.2 22.2 16.2 13.9 23.8 19.2 MME-Reasoning Table 7: Performance across different question types on MME-Reasoning. The top three are highlighted in green . indicates the model was evaluated on the mini-set. T\" represents Thinking\". Model Choice Open Rule DED. IND. ABD. ALL DED. IND. ABD. ALL ABD.&ALL Gemini-2.5-Pro-T Seed1.5-VL-T o4-mini o1 Claude-4-Sonnet-T Claude-3.7-Sonnet-T Gemini-2.5-Flash-T Seed1.5-VL GPT-4o Claude-3.7-Sonnet Kimi-Latest QVQ-72B-Preview Virgo-72B VL-Rethinker-72B VL-Rethinker-7B MM-Eureka-Qwen-32B MM-Eureka-Qwen-7B R1-VL-7B Vision-R1-7B R1-Onevision-7B-RL Kimi-VL-A3B-T OpenVLThinker-7B LMM-R1-MGT-PerceReason Mulberry LlamaV-o1 Qwen2.5-VL-72B Qwen2.5-VL-32B Qwen2.5-VL-7B Qwen2.5-VL-3B Qwen2-VL-72B Qwen2-VL-7B Qwen2-VL-2B InternVL3-78B InternVL3-38B InternVL3-8B Molmo-72B Molmo-7B-D Molmo-7B-O LLaVA-OV-72B Kimi-VL-A3B 58.0 57.3 57.3 46.2 41.1 38.0 31.7 54.0 36.3 38.7 31.3 43.7 39.7 43.3 41.3 36.7 36.7 31.7 32.3 34.3 33.0 38.7 36.3 30.0 26. 41.0 44.0 39.0 33.3 35.3 32.7 30.0 40.0 36.7 29.7 30.0 27.0 23.0 33.7 31.3 61.5 44.2 67.3 45.5 25.0 17.3 11.5 38.5 17.3 21.2 3.8 26.9 11.5 25.0 9.6 17.3 7.7 5.8 9.6 9.6 13.5 7.7 13.5 7.7 1.9 23.1 15.4 15.4 11.5 11.5 9.6 1.9 13.5 13.5 0.0 9.6 1.9 3.8 5.8 7.7 60.0 59.4 48.5 36.2 35.2 31.5 23. 42.4 27.9 27.9 20.0 31.5 22.4 29.1 16.4 26.1 21.8 12.7 16.4 12.1 14.5 11.5 15.8 11.5 7.9 26.1 27.3 18.2 13.9 18.8 12.7 6.1 23.0 17.0 17.0 11.5 9.7 6.1 18.8 9.1 66.9 65.3 58.9 46.9 34.3 28.3 20.8 48.0 21.1 28.0 18.1 33.6 25.9 29.3 17.6 24.8 21.3 12.0 16.3 13.1 22.7 12.8 16.8 11.2 9. 29.3 28.5 17.3 16.0 16.8 12.0 6.7 22.9 21.1 18.1 10.7 8.3 5.1 15.7 11.7 66.1 63.5 71.3 40.7 31.3 16.5 13.9 20.0 7.0 12.2 0.9 8.7 3.5 13.9 2.6 9.6 4.3 0.9 4.3 0.9 6.1 0.9 0.9 0.9 0.9 9.6 12.2 3.5 0.0 1.7 0.9 0.0 5.2 2.6 0.9 1.7 0.0 0.0 1.7 2.6 Close-source & Thinking 49.8 54.2 48.7 42.4 33.2 39.7 23.8 63.6 60.2 61.9 53.3 40.7 46.6 37.3 55.7 56.5 54.7 46.0 37.9 40.1 29.5 Close-source & Chat 46.2 38.3 42.2 29.6 59.3 48.3 37.3 38. 51.8 39.1 39.9 31.8 75.9 78.5 67.1 60.0 36.5 28.5 21.5 57.0 15.2 30.4 20.9 Open-source & Thinking 38.0 34.2 31.0 21.5 25.9 25.3 13.3 18.4 15.2 34.2 15.8 19.0 12.0 14.6 34.8 34.2 17.1 19.6 16.5 12.0 8.9 25.9 27.8 25.3 10.1 8.9 4.4 15.8 15. 35.0 36.8 38.3 33.9 33.2 32.9 24.9 29.6 27.8 27.4 28.2 36.1 28.9 25.3 45.8 47.5 53.4 51.7 49.2 41.5 34.7 33.9 31.4 34.7 41.5 44.9 42.4 29.7 40.6 39.9 43.0 40.1 37.4 36.0 29.5 31.5 31.2 31.1 35.0 37.7 31.7 26.6 Open-source & Chat 34.3 30.0 30.0 30.7 36.5 28.9 30.3 37.9 32.1 35.7 20.2 12.6 18.4 35.4 30.0 40.7 39.4 35.8 34.5 37.7 33.4 30.4 41.6 36.8 35.1 26.2 20.7 20.1 35.3 32. 55.1 50.0 41.5 46.6 46.6 45.8 31.4 54.2 48.3 47.5 30.5 23.7 16.9 39.0 42.4 24 MME-Reasoning Table 8: Performance comparison of chat models with or w/o MCTS."
        },
        {
            "title": "Reasoning Type",
            "content": "AVG. CAL. P& E. Qwen2.5-VL-7B + MCTS 22.2 20. 18.2 13.8 PA. 21.9 18.8 S&T. CCA. DED. IND. ABD. 35.1 30. 36.1 35.4 31.4 28.1 27.5 23.6 20.9 17.6 26.8 23.3 Table 9: Performance comparison of SoTA chat models with or w/o CoT prompt."
        },
        {
            "title": "Model",
            "content": "Qwen2.5-VL-7B + CoT prompt Qwen2.5-VL-32B + CoT prompt Qwen2.5-VL-72B + CoT prompt InternVL3-8B + CoT prompt InternVL3-38B + CoT prompt InternVL3-78B + CoT prompt"
        },
        {
            "title": "Reasoning Type",
            "content": "CAL. P& E. 22.2 20.3 32.2 29.0 31.7 32.5 19.5 21. 23.0 28.7 26.0 29.0 18.2 18.5 26.8 24.6 25.1 26.2 19.6 16. 18.5 24.3 24.0 22.9 PA. 21.9 17.9 24.4 23.3 27.2 25. 22.6 20.2 23.0 28.6 26.5 27.0 S&T. CCA. DED. IND. ABD. 35.1 33. 39.0 40.8 37.9 37.2 31.6 31.6 38.3 38.3 41.8 40.8 36.1 38. 52.1 52.1 53.5 52.8 41.0 38.2 41.7 48.6 50.0 48.6 31.4 28. 40.5 40.1 39.0 37.5 28.1 31.2 33.5 37.5 35.1 36.6 27.5 21. 27.5 28.7 32.3 30.8 29.9 26.9 29.0 32.9 33.8 35.1 20.9 23. 29.6 26.4 29.9 30.0 21.4 16.8 22.1 26.9 27.1 26.9 AVG. 26.8 24.7 33.2 32.3 34.1 33.0 26.4 25.2 28.4 32.7 32.1 32. A.8 Results of Captioner & LLMs We used GPT-4o as the captioner to generate visual descriptions for each question as substitute for the images. Then we evaluated existing LLMs with \"thinking mode,\" and the results are presented in Tab. 11. As shown in the results, even when only indirectly perceiving image content through textual descriptions, QwQ (Qwen Team, 2025b) and R1 (DeepSeek-AI, 2025) achieved impressive scores of 41.9 and 46.9 respectivelysurpassing even Claude-3.7-Sonnet-Thinking. These findings indicate that there is still substantial room for improvement in extending long-term reasoning capabilities from LLMs to the multimodal domain. This gap may be due, in part, to degradation in the foundational models capabilities during the vision-language alignment process. Additionally, the diversity of reasoning tasks specific to multimodal settings has yet to be thoroughly explored."
        },
        {
            "title": "B Details of Annotation",
            "content": "B.1 Difficult Annotation For each question, we assign difficulty label: Easy, Medium, or Hard, based on the cognitive load required to solve it. The labeling criteria are as follows: 25 MME-Reasoning Model CoT Prompt Qwen2.5-VL Lets think step by step. InternVL3 Answer the preceding question. The last line of your response should follow this format: Answer: $FINAL_ANSWER (without quotes), where FINAL_ANSWER is your conclusion based on the reasoning provided. If you are uncertain or the problem is too complex, make reasoned guess based on the information provided. Avoid repeating steps indefinitelyprovide your best guess even if unsure. Think step by step logically, considering all relevant information before answering. Table 10: Chain-of-Thought Prompts for Different Models Figure 10: Average token usage of open & closed-source thinking models on MME-Reasoning. Table 11: Performance of Caption + SoTA Reasoning LLMs. We use GPT-4o to generate caption of each image in MME-Reasoning."
        },
        {
            "title": "Reasoning Type",
            "content": "AVG. CAL. P& E. QwQ-32B DeepSeek-R1 48.5 56.9 32.9 40. PA. 39.1 41.6 S&T. CCA. DED. IND. ABD. 37.6 41.8 53.5 58. 44.4 53.8 45.6 43.8 35.9 41.5 41.9 46.9 Easy: The question typically has straightforward and quick solution that can be correctly answered by human expert within 2 minutes. Medium: The question generally requires some reasoning steps and one to two rounds of trial and reflection, and can be correctly answered by human expert within 2 to 5 minutes. Hard: The question usually requires more than two attempts and reflections, or involves the use of tools such as auxiliary lines or drafts to support the thought process. It may or may not be solved by human expert within 10 minutes. 26 MME-Reasoning B.2 Reasoning Type Annotation For each question, we assign reasoning type label: Deductive, Inductive, or Abductive, based on the dominant reasoning method required in its solution. The labeling criteria are as follows: Deductive: Involves deriving necessary conclusion from given premises and general rules through step-by-step inference. Examples include math problems, physics problems, and certain puzzles. Inductive: Involves observing specific phenomena, summarizing general patterns or rules, and extrapolating based on those patterns. Examples include figure series and analogy questions. Abductive: Involves forming hypotheses or explanations based on known phenomena and then verifying them. These problems typically have large solution space. Examples include Sudoku, mate-in-one chess problems, circuit fault analysis, biological pedigree analysis, and some puzzles. It should be noted that although the solutions to some puzzles, such as Sudoku, can theoretically be derived through deductive reasoning, in the actual process of human reasoning, we often resort to assuming certain move and then verifying its validity. This hypothesisverificationbacktracking mechanism leads us to consider these form of abductive reasoning. B.3 Capability Annotation For each question, we also assign one or more capability labels based on the primary abilities being tested. The available labels are: Pattern Analysis, Planning and Exploring, Spatial and Temporal, Calculation, and Causal Chain Analysis. question may have multiple capability labels. The labeling criteria are as follows: Pattern analysis: Requires identifying patterns in shape, color, size, or other visual features within the image. Planning and exploring: Requires explicit planning of the answering process, involving exploration within solution space and iterative verification or reflection. Spatial and Temporal: Requires understanding spatial relationships or temporal sequences represented in the visual input. Calculation: Involves performing numerical calculations based on given quantitative conditions to arrive at correct result. Causal Chain Analysis: Requires reasoning about causal relationships across multiple nodes based on limited information, or understanding dynamic processes in the problem and identifying key events."
        },
        {
            "title": "C Details of Implementation",
            "content": "Some of the data in MME-Reasoning are sourced from ScanNet (Dai et al., 2017), Arkitscenes (Baruch et al., 2021), VideoMME (Fu et al., 2024), MM-IQ (Cai et al., 2025), PuzzleVQA (Chia et al., 2024). We further filter most of the data and reformulate the questions. We use gpt-4o-mini to extract the answer of all responses and judge the answer of free-form questions. The cost fluctuates with the 27 MME-Reasoning length of the MLLMs response. As an example, extracting and judging the response of Qwen2.5VL-72B costs around $0.1. We use VLMEvalKit to evaluate all the models. For models larger than 30B, we use vllm to reduce the inference time. All experiments are conducted on A100 GPUs except experiments on closed-source models."
        },
        {
            "title": "D Details of Evaluation",
            "content": "D.1 Prompts for Answer Extraction We list our answer extraction prompts from Fig. 11 to Fig. 21 including: Fig. 11: Prompt for tasks answering in id : answer format. Fig. 12: Prompt for tasks answering in coordinates format. Fig. 13: Prompt for tasks answering in formula format. Fig. 14: Prompt for multiple-choice tasks. Fig. 15: Prompt for points24 tasks. Fig. 16: Prompt for hashi puzzles. Fig. 17: Prompt for sudoku_4x4 puzzles. Fig. 18: Prompt for sudoku_6x6 puzzles. Fig. 19: Prompt for skyscraper puzzles. Fig. 20: Prompt for yinyang puzzles. Fig. 21: Prompt for free-form tasks. Examples of MME-Reasoning We further provide additional case studies as shown from Fig. 22 to Fig. 52, showing both correct and incorrect responses by MLLMs (e.g., select from GPT-4o, Qwen2.5-VL-72B, o4-mini, Seed1.5VL-Thinking, and Gemini-2.5-Pro-Thinking). In each figure, we show the original questions, reasoning types, difficulty levels, and model responses. Overall, we find that thinking models demonstrate stronger abilities in exploration, judgment, and reflection. However, it still struggles to arrive at correct answers for many reasoning problems that are simple for humans, indicating that the models reasoning ability still needs further improvement. Moreover, the number of tokens consumed by the reasoning model increases rapidly. Therefore, future research should also focus on balancing both the reasoning ability and efficiency of the model."
        },
        {
            "title": "F Limitation",
            "content": "Despite our best efforts to cover wide range of multimodal reasoning question types, it remains challenging to comprehensively collect all possible types of reasoning problems that occur in realworld scenarios. This is primarily because gathering and curating high-quality reasoning questions is often time-consuming and labor-intensive process. Future work is needed to further enrich the diversity of question types and optimize dataset coverage. https://github.com/open-compass/VLMEvalKit https://github.com/vllm-project/vllm 28 MME-Reasoning Figure 11: Prompt for tasks answering in id : answer format. Figure 12: Prompt for tasks answering in coordinates format. 29 MME-Reasoning Figure 13: Prompt for tasks answering in formula format. Figure 14: Prompt for multiple-choice tasks. Figure 15: Prompt for points24 tasks. MME-Reasoning Figure 16: Prompt for hashi puzzles. 31 MME-Reasoning Figure 17: Prompt for sudoku_4x4 puzzles. MME-Reasoning Figure 18: Prompt for sudoku_6x6 puzzles. 33 MME-Reasoning Figure 19: Prompt for skyscraper puzzles. MME-Reasoning Figure 20: Prompt for yinyang puzzles. 35 MME-Reasoning Figure 21: Prompt for free-form tasks. MME-Reasoning Figure 22: An example of circuit analysis in which the model needs to infer the circuit connection based on the observed phenomena. 37 MME-Reasoning Figure 23: An example of navigation in which the model needs to infer the route from the starting point to the destination. MME-Reasoning Figure 24: An example of Yinyang puzzle in which the model needs to try, judge, and reason to determine the positions of pieces of different colors. 39 MME-Reasoning Figure 25: An example of Sudoku puzzle in which the model needs to try, judge, and reason to determine the positions of each number. MME-Reasoning Figure 26: An example of Sudoku puzzle in which the model needs to try, judge, and reason to determine the positions of each number. 41 MME-Reasoning Figure 27: An example of Venn diagram problem in which the model needs to calculate and reason to determine the correct answer. MME-Reasoning Figure 28: An example of Hashi puzzle in which the model needs to try, judge, and reason to determine the positions and numbers of bridges. 43 MME-Reasoning Figure 29: An example of Hashi puzzle in which the model needs to try, judge, and reason to determine the positions and numbers of bridges. MME-Reasoning Figure 30: An example of temporal ordering problem in which the model needs to determine the sequence of images based on the logical relationships of time. 45 MME-Reasoning Figure 31: An example of function problem in which the model needs to infer the graph based on the function and provide the correct answer. MME-Reasoning Figure 32: An example of Skyscraper puzzle in which the model needs to try, judge, and reason to determine the number of each position. 47 MME-Reasoning Figure 33: An example of solid geometry problems in which the model needs to perceive, calculate, and reason to arrive at the final answer. MME-Reasoning Figure 34: An example of statistical problems in which the model needs to perceive, calculate, and reason to arrive at the final answer. 49 MME-Reasoning Figure 35: An example of pattern-finding problem in which the model needs to analyze and identify the pattern in order to provide the correct answer. MME-Reasoning Figure 36: An example of problem involving the determination of relative spatial relationships, in which the model needs to understand space and reason about the positional relationships within it. 51 MME-Reasoning Figure 37: An example of chemical reaction process inference problem, in which the model needs to infer the substances involved based on chemical knowledge. MME-Reasoning Figure 38: An example of genetic inference problem, in which the model needs to infer probabilities based on pedigree chart. 53 MME-Reasoning Figure 39: An example of spatial reasoning, in which the model needs to infer the folded cube based on its unfolded diagram. MME-Reasoning Figure 40: An example of maze problem, in which the model needs to try, judge, and provide the correct path. 55 MME-Reasoning Figure 41: An example of the 24-point game, in which the model needs to try different combinations and calculations to arrive at the final answer. MME-Reasoning Figure 42: An example of geometry problem, in which the model needs to calculate and reason to arrive at the final answer. 57 MME-Reasoning Figure 43: An example of number-filling problem, in which the model needs to try, judge, reflect, and reason to find an answer that meets the requirements. MME-Reasoning Figure 44: An example of number-filling problem, in which the model needs to try, judge, reflect, and reason to find an answer that meets the requirements. 59 MME-Reasoning Figure 45: An example of construction problem, in which the model needs to understand spatial relationships and reason to arrive at the correct answer. MME-Reasoning Figure 46: An example of Mate-on-one problem, in which the model needs to try, judge, reflect, and reason to find the right answer. 61 MME-Reasoning Figure 47: An example of paper-cut problem, in which the model needs to understand spatial relationships and reason to find the correct answer. MME-Reasoning Figure 48: An example of number pattern game, in which the model needs to calculate and find the pattern of the numbers, and finally reason out the correct answer. 63 MME-Reasoning Figure 49: An example of moving-match problem, in which the model needs to try, judge, reflect, and reason to find the right answer. MME-Reasoning Figure 50: An example of counting problem, in which the model needs to logically arrange the reasoning steps and find the correct answer. 65 MME-Reasoning Figure 51: An example of coloring game, in which the model needs to plan reasonably and find the minimum number of colors needed. MME-Reasoning Figure 52: An example of reasoning problem, in which the model needs to make assumptions, verify them, reflect, and reason to arrive at the correct answer."
        }
    ],
    "affiliations": [
        "Fudan University",
        "MMLab, The Chinese University of Hong Kong",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "University of Science and Technology of China"
    ]
}