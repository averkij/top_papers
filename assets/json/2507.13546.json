{
    "paper_title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention",
    "authors": [
        "Dmitrii Mikhailov",
        "Aleksey Letunovskiy",
        "Maria Kovaleva",
        "Vladimir Arkhipkin",
        "Vladimir Korviakov",
        "Vladimir Polovnikov",
        "Viacheslav Vasilev",
        "Evelina Sidorova",
        "Denis Dimitrov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: https://github.com/gen-ai-team/Wan2.1-NABLA"
        },
        {
            "title": "Start",
            "content": "NABLA: Neighborhood Adaptive Block-Level Attention Dmitrii Mikhailov1,*, Aleksey Letunovskiy1, Maria Kovaleva1, Vladimir Arkhipkin1, Vladimir Korviakov1,*, Vladimir Polovnikov1,2, Viacheslav Vasilev1,3, Evelina Sidorova1, Denis Dimitrov1,4,* 1Sber AI, Moscow, Russia, 2Lomonosov Moscow State University (MSU), Moscow, Russia, 3Moscow Institute of Physics and Technology (MIPT), Moscow, Russia, 4Artificial Intelligence Research Institute (AIRI), Moscow, Russia *Corresponding authors: {dmsmikhaylov, vpkorvyakov, dimitrov.d.v}@sberbank.ru Abstract Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA (Figure 1), novel Neighborhood Adaptive BlockLevel Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorchs Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7 faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here. Keywords: Video generation, diffusion transformers, sparse attention, adaptive computation 5 2 0 2 7 1 ] . [ 1 6 4 5 3 1 . 7 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Figure 1: The block-sparse attention mask is computed by (1) reducing the dimensionality of queries (Q) and keys (K), (2) sparsifying the softmax distribution via cumulative density function (CDF) threshold and binarizing the result, and (3) mapping the sparse mask back to the original input blocks. Among various generative methods, diffusion [12] is currently the state-ofthe-art approach for generating media content such as images and videos. One of the major milestones in the development of this approach was the introduction of the diffusion transformer. It was first proposed for image generation in [20], where the authors demonstrated that this architecture surpasses the previously dominant U-Net frameworks for this task and also highlighted its scalability. Another key advancement in modern content generation using diffusion methods is latent diffusion [22], where the diffusion process operates not on raw images or videos but on their compressed representations obtained through variational autoencoders [14]. This is particularly important for video generation due to computational complexity. Diffusion transformers can be broadly categorized into two main classes: CrossDiT and MMDiT [7]. The key differences between these architectures lie in their handling of text embeddings and the attention [25] mechanisms employed. In CrossDiT, text tokens are processed separately from visual tokens and incorporated via cross-attention. In contrast, MMDiT processes text and visual tokens in parallel and blends them through self-attention. This architectural distinction may influence the structure of the attention matrices learned by diffusion transformers. The full potential of transformers in latent diffusion for video generation was first demonstrated by the closed-source solution Sora [18]. Since then, numerous popular closed-source and open-source solutions have emerged, such as MovieGen [21], HunyuanVideo [16], CogVideoX [33], Kling [1], WAN [27] and Kandinsky [2, 26]. While these models advance video generation capabilities, they share critical limitation: computationally expensive full attention mechanisms. However recent theoretical analyses [4] and experimental evidence [23, 32, 35, 13] have revealed that attention matrices demonstrate inherent sparsity patterns, suggesting significant potential for optimizing computational efficiency through sparse attention mechanisms. The methods for simplifying attention mechanisms in video generation models presented in the literature can be divided by two main features: dynamism and usage scenario. The first feature determines how important patterns are distinguished in attention masks statically or dynamically. The second feature implies whether the proposed method can be used in zero-shot mode or the model should be trained with it. The most important and oldest static patterns in attention masks include sliding window attention [3] in NLP tasks and window-based attention (SWIN) [17] in CV tasks. In recent years, neighbor attention [11, 10] as well as its effective implementations [9] has emerged as the successor to SWIN attention. It use close idea but with intersecting windows. Recent work Sliding Tile Attention [36] builds upon NATTEN principles but optimizes it for efficient computations on GPU using correct size of visual block. However, although static patterns use empirical knowledge about the data structure in various domains such as NLP or CV, they do not always correspond to the patterns that real transformers obtain during training. Moreover, many works [23, 32, 31, 13, 29] have shown that attention masks differ in unequal blocks, heads, text prompts and even at different steps of video generation. For example, in the MInference [13] the authors identify several groups of attention masks for LLM models and select parameters for them on the inference. Sparse VideoGen [31] divides all attention heads in DiT into spatial and temporal, employing online profiling to dynamically select appropriate patterns. However, these methods consider only fixed number of attention patterns while in the real attention maps there are much more of them. AdaSpa [32] utilizes dynamic online search to identify suitable block attention masks, employing hierarchical selection process to determine the necessary sparsity level. SpargeAttn [35] approximates attention masks on compressed queries and keys. While the aforementioned works primarily focus on accelerating inference for pre-trained video generation models, similar concepts have also been applied to model pre-training and fine-tuning. Numerous studies have explored sparse or simplified attention in NLP tasks [19, 15, 28, 8, 30]. In Native Sparse Attention [34] the authors reduce the number of keys and values in LLM in various ways while maintaining the size of the query. Three main patterns stand out: compression, selection and sliding window, and all of which are applicable to both inference and training. An important work on training video generator with sparse attention is DSV [24]. It employs two-stage training. At the first, low-rank predictor for the attention matrix and sparsity estimator are trained. At the second stage, the model is trained with the predictor and estimator held fixed. In summary, numerous methods exist for accelerating attention mechanisms within transformer models. This area has been particularly well-explored in the field of Natural Language Processing. For video generation, this area is still developing. Although several approaches focus exclusively on inference or, at best, fine-tuning, video generation models pretrained with sparse attention remain limited. In this work we collect the best practices: simplicity and strong prior masks of Sliding Tile Attention (STA) [36] approach and flexibility of training-based approaches. We present NABLA, Neighborhood Adaptive Block-Level Attention that utilizes simple downsampling approach instead of additional training. NABLA dynamically fits the sparsity mask by thresholding the cumulative distribution function for precise attention calculation. Our key contributions include: Efficient threshold selection for adaptive masks, outperforming fixed sparse patterns like STA (validated experimentally). Complementarity with STA and other acceleration techniques. Simple implementation via FlexAttention without custom CUDA kernels. Acceleration of both inference and training of DiT model due to the fast online algorithm. Extensive evaluations on video datasets demonstrate NABLAs superiority over other approaches, achieving 2.7 speed-up while maintaining equivalent VBench, CLIP and human evaluation scores without excessive additional overhead to the training and inference pipelines."
        },
        {
            "title": "2.1 Attention Mechanism in Visual Domain",
            "content": "The classical self-attention mechanism, introduced in [25], revolutionized deep learning by enabling dynamic focus on relevant parts of input data through pairwise token interactions. In visual domains, this mechanism processes images and videos by first dividing them into patches and projecting them into an embedding space [6]. For an input sequence RSD, where is the number of tokens (e.g., image patches or spatio-temporal video blocks) and is the embedding dimension, the self-attention mechanism projects these tokens into queries Q, keys K, and values using learnable weight matrices WQ, WK, WV RDD: = XWQ, = XWK, = XWV . (1) The attention scores are computed via scaled dot-product between queries and keys, followed by softmax operation to produce the attention matrix RSS. = softmax , (2) (cid:19) (cid:18) QK where each entry Aij determines how much token influences token i. The final output is weighted sum of values based on these scores: Output = AV. (3) For video diffusion transformers (DiTs) [20], the input consists of spatio-temporal tokens (e.g., RT HW D, where is the number of frames and H, are spatial dimensions of the latent space). In this case, classical self-attention approach faces significant challenges due to the quadratic complexity O((T )2) relative to the number of tokens. High-resolution or long-duration videos exacerbate this issue, as the sequence length grows cubically with spatial and temporal dimensions. In addition, many attention weights are near-zero due to locality in space and time, which leads to redundant computations. Appendix Figure 9 contains examples of attention weights which actually have less complexity: O(T ) ( 9a), O(T 2 ) ( 9b, 9i), O(T (H )2) ( 9d, 9f, 9g, 9h). While full attention theoretically preserves global coherence, its computational cost becomes prohibitive, creating critical bottleneck for practical applications."
        },
        {
            "title": "2.2 Sliding Tile Attention (STA)",
            "content": "To address the limitations of full attention in video generation models, Sliding Tile Attention (STA) [36] was proposed as hardware-efficient alternative that leverages the inherent 3D locality observed in pretrained video DiTs. STA organizes an input video latent RT HW (flattened into = HW tokens) through tiling mechanism. Tokens are partitioned into non-overlapping tiles of size (BT , BH , BW ), which matches GPU block sizes (for FlashAttention 5 compatibility). STA computes attention only between query tiles and key tiles within fixed 3D window of size (WT , WH , WW ) centered on the query tile: = softmax( QK + ), Output = AV, (4) where Q, K, RSD and {, 0}SS is sparse mask managed implicitly by tile-based sliding. STA ensures only activates dense blocks, reducing redundant computation. This design ensures queries within tile attend only to keys in predefined windows, eliminating irregular memory access patterns of traditional sliding window approaches. The number of dense blocks is: Sdense = (cid:18) WT BT"
        },
        {
            "title": "WH\nBH",
            "content": ""
        },
        {
            "title": "WW\nBW",
            "content": "(cid:19) (cid:18) BT"
        },
        {
            "title": "H\nBH",
            "content": ""
        },
        {
            "title": "W\nBW",
            "content": "(cid:19) . (5) By aligning tile sizes with GPU thread blocks, STA minimizes masking overhead and maximizes hardware utilization, achieving up to 10.45 speedup over full attention while maintaining generation quality. key innovation of STA is its adaptability to head specialization, where different attention heads focus on varying spatial-temporal scales. Through profiling, STA automatically configures optimal window sizes per head, balancing computational efficiency with expressive power. However, STA relies on static window partitioning, which may not fully capture dynamic content-specific patterns, and requires careful tuning to avoid visual artifacts such as blocky boundaries. (a) STA (11, 40, 40) (b) STA (18, 40, 40) (c) STA (11, 24, 24) (d) STA (18, 24, 24) Figure 2: STA masks with different window sizes."
        },
        {
            "title": "2.3 Motivation for NABLA",
            "content": "During our experiments with STA, we identified serious issue of object duplication in high-resolution generation and long video sequences (see the examples 5, 6 in Appendix B). This artifact appears in the case of non-optimal STA configuration and is caused by insufficient global attention coverage in the STA approach. To address this, we hypothesized that an effective sparse attention algorithm must preserve long-range dependenciesconnections between tokens distant in space or time. However, the semantics of such dependencies are inherently complex, making them infeasible to capture with fixed sparsity patterns. 6 While STA provides significant efficiency gains through hardware-aware sparsity, its static nature limits adaptability to diverse video content. Figure 9 in Appendix contains examples (9b - 9i) of attention weights which are significantly different from STA pattern (Figure 2). We argue that adaptive sparsity is essential: the algorithm must dynamically select sparse connections based on the actual input context. This motivates our proposed method, NABLA, which learns context-aware sparsity patterns to maintain global coherence. NABLA utilizes downsampled full attention calculation to dynamically select the most valuable blocks separately for each attention head. From other side we found, that only adaptive sparsity provides another type of artifacts visible borders between areas recovered from neighbouring latent pixels. See examples 7, 8 in Appendix B. To address this issue we combine NABLA with STA. This combination provides best results with fixed attention sparsity. As result, our final approach achieves both computational efficiency and high-quality generation, addressing the limitations of purely static or dynamic approaches."
        },
        {
            "title": "3.1 Training algorithm",
            "content": "This section presents training (or fine-tuning) version of NABLA algorithm. The method consists of the following parts: Token reordering to place tokens of the same spatial block to continuous sequance NABLA adaptive sparsification method that dynamically selects blocks of the feature map for which we perform attention computation Sliding Tile Attention (STA) to improve fine-grained quality of the generated videos 3.1.1 Token Reordering Following the approach of STA [36], we find token reordering crucial for establishing semantic connections between adjacent tokens. Our method employs fractal flattening with spatial patches of size , which groups all tokens within each patch into contiguous sequence of 2 tokens. Notably, we preserve the original ordering along the temporal dimension. Figure 3 illustrates this transformation. We apply the reordering operation at the input stage of the DiT network and its inverse at the output stage, ensuring proper spatial relationships while maintaining computational efficiency. 7 Figure 3: Token reordering illustration for latent image with height 16, width 16, and patch size 8. The diagram shows how spatial tokens are reorganized into fractal-flattened sequences while preserving their semantic relationships. 3.1.2 NABLA mask computation algorithm Algorithm 1 presents the NABLA mask computation for Multi-Head Self-Attention. The algorithm takes as input data sample represented by queries and keys K, each containing tokens of dimension (we omit the batch dimension for simplicity). We denote the number of transformer heads as to distinguish it from the latent frame height H. The core idea of our method involves computing full attention map for downsampled versions of and K, followed by binarization with minimal information loss. The downsampling is performed through average pooling of tokens in blocks of size = 2, making the reduced attention map computation 2 times more efficient than computing the full attention map. After computing the reduced attention map, we apply the softmax operation and compute the cumulative distribution function (CDF) for each row. We then binarize the map by retaining only values whose CDF exceeds the threshold 1 thr, where thr is algorithm parameter. The binarization yields unique sparsity pattern for each head, represented by an S/N S/N matrix of binary values indicating whether to compute attention for the corresponding block. Note that the softmax, sort, and cumsum operations are applied along the last dimension of the input tensor. 8 Algorithm 1 NABLA Sparse Mask Generation Require: Query tensor: RhSD, Key tensor: RhSD, binarization threshold: thr, block size: Reduced Attention Map Computation: 1: reshape(Q, [h, S/N, N, D]) 2: reshape(K, [h, S/N, N, D]) 3: Qa mean(Q, dim = 2) 4: Ka mean(K, dim = 2) 5: 6: softmax( QaKT Binarization via CDF: Ka.transpose(2, 1) ) Block averaging, Qa RhS/N Block averaging, Ka RhS/N RhDS/N Reduced attention map, RhS/N S/N 7: vals, order sort(A) 8: cvals cumsum(vals) 9: cvals 1 thr 10: reorder(M, order) 11: return Row-wise sorting Cumulative sum Binarization of ordered values Original order restoration 3.1.3 Joint NABLA and STA Sparsity Mask We find that combining NABLA with STA results in the best visual quality, benefiting from both our methods adaptive nature and STAs strong prior mask. The STA mask is computed as MST = STA mask(T, H, W, WT , WH , WW ), where: : Number of latent frames in the video sample (T = 1 for images) H, : Latent frame height and width WT , WH , WW : STA window parameters The final mask for each data sample is given by = MST A. After the mask is computed, it can be used directly in the Flex Attention [5] algorithm to improve the training efficiency."
        },
        {
            "title": "4.1 Fine-tuning experiments",
            "content": "We evaluate our method in the fine-tuning setup using the Wan 2.1 14B T2I model [27] at 720p resolution, focusing specifically on self-attention blocks due to their dominant contribution to overall FLOPs. We implement NABLA alongside STA as our baseline sparse attention method. For reproducibility, we use Flex Attention implementation from PyTorch 2.7. All experiments maintain consistent hardware and software configurations to ensure fair comparisons. We perform knowledge distillation of the teacher model Wan2.1 T2V 14B using MSE loss. We initialize the student model with the baseline model weights 9 Table 1: Computational efficiency comparison. All measurements performed on 4H100 GPUs."
        },
        {
            "title": "Baseline",
            "content": "STA(18,40,40) NABLA(0,7) NABLA(0,5)+STA(11,40,40) STA(18,24,24) NABLA(0.4) NABLA(0.2)+STA(11,24,24) Sparsity, % Inference time, min 0 79.45 80.13 81 91.28 92.5 92. 8.35 4.00 4.02 3.58 3.08 3.07 3.13 Table 2: Model quality metrics after fine-tuning. NABLA variants maintain comparable performance to baseline even at 90 % sparsity. Method CLIP score Quality VBench score Semantic Total Baseline Wan2.1-14B STA(18,24,24) NABLA(0.4) NABLA(0.2)+STA(11,24,24) 42.06 41.51 42.08 41. 85.15 85.05 85.02 85.03 75.23 83.16 71.73 75.76 76.04 82.39 83.17 83. and replace all self-attention blocks with sparse attention. Complete hyperparameter details are provided in Appendix A. Tables 1 and 2 present our key findings. Detailed evaluation results are provided in Appendix C. Key observations from our experiments include: NABLA achieves full quality recovery in generation metrics (CLIP and VBench scores) The STA-only configuration shows degradation in VBench semantic scores Detailed results (Appendix C) reveal STAs particular challenges with multiple objects and spatial relationships Pure NABLA and NABLA+STA combinations maintain baseline-level performance across all objective metrics"
        },
        {
            "title": "4.2 Human Evaluation",
            "content": "We conducted side-by-side human evaluation comparing generated video quality obtained by the baseline model and the finetuned models across various configurations. 50 participants evaluated 20 video pairs each comparing videos on three key perceptual dimensions (prompt alignment, visual quality, dynamics) selecting one option per dimension: left is better, right is better, both are good, 10 Table 3: Human evaluation results (SBS test). NABLA maintains perceptual quality comparable to baseline at 80 % sparsity."
        },
        {
            "title": "Motion Overall",
            "content": "Baseline is better NABLA(0.7) is better Both are good Both are bad 19.9 13.3 66.7 0.9 31.4 26.7 40 1.9 10.5 15.2 64.8 9.5 20.3 18.4 57.1 4.1 both are bad. Results in Table 3 show NABLAs perceptual parity with baseline even at high sparsity. Evaluation protocol: Video pairs: Two 5-second preliminary generated videos for comparing methods are shown for randomly selected prompt (left/right video methods are shuffled). Prompts: We use 942 diverse text prompts from VBench. The prompt for current videos is also visible to the user. Judge: The user can watch the videos in repeat mode with the ability to zoom in/out and pause. The user must decide which video (left or right) is better, or both videos are good or bad. Dimensions: Visual Quality: Artifact freedom and sharpness Motion Naturalness: Better dynamics, physical plausibility and fluidity Semantic Alignment: Prompt-video consistency"
        },
        {
            "title": "4.3 Pretraining experiments",
            "content": "To verify the applicability of our method during pretraining, we train custom DiT-based 2B model in three stages: 1. Text-to-image pretraining at 256256 resolution with full attention. 2. Text-to-video pretraining at 256256 resolution with full attention. 3. Text-to-video pretraining at 512512 resolution: (a) With full attention. (b) With the NABLA method (80% sparsity). 11 The first stage is conducted from scratch, with each subsequent stage initialized using weights from the previous stage. The first two stages are common to all experiments. We compare the convergence of training and validation losses between stages 3(a) and 3(b). Figure 4 shows that the NABLA model achieves better convergence than its full attention counterpart. Furthermore, each training iteration takes 10.9 seconds for the full attention model compared to 7.5 seconds for NABLA, resulting in 1.46 speedup. Figure 4: Training convergence for full attention and NABLA models at 512512 resolution. NABLA achieves lower training and validation losses."
        },
        {
            "title": "Conclusion",
            "content": "By dynamically adapting to sparsity patterns through block-wise attention with adaptive thresholding, our method achieves: Significant computational efficiency: Up to 2.7 faster training and inference compared to full-attention baselines Minimal quality degradation: Near-identical performance to full attention in quantitative metrics (CLIP, VBench) and human evaluations Hardware-agnostic implementation: Seamless integration with PyTorchs Flex Attention without custom CUDA kernels Extensive experiments demonstrate NABLAs superiority over static sparsity approaches like STA, particularly in preserving long-range dependencies and handling complex spatial-temporal relationships. Our hybrid approach combining NABLA with STA further enhances visual quality by mitigating boundary artifacts while maintaining efficiency. 12 The proposed approach establishes new state-of-the-art for efficient video generation, enabling high-resolution synthesis with reduced computational demands."
        },
        {
            "title": "References",
            "content": "[1] Kling AI. Kling AI: Next-Gen AI Video & AI Image Generator. https: //app.klingai.com/global/. 2024. [2] Vladimir Arkhipkin et al. ImproveYourVideos: Architectural Improvements for Text-to-Video Generation Pipeline. In: IEEE Access 13 (2025), pp. 19862003. doi: 10.1109/ACCESS.2024.3522510. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-Document Transformer. In: arXiv:2004.05150 (2020). [3] [4] Yichuan Deng et al. How Sparse Attention Approximates Exact Attention? Your Attention is Naturally nC-Sparse. 2025. arXiv: 2404.02690 [cs.LG]. url: https://arxiv.org/abs/2404.02690. [5] Juechu Dong et al. Flex Attention: Programming Model for Generating Optimized Attention Kernels. 2024. arXiv: 2412 . 05496 [cs.LG]. url: https://arxiv.org/abs/2412.05496. [6] Alexey Dosovitskiy et al. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. 2021. arXiv: 2010.11929 [cs.CV]. url: https://arxiv.org/abs/2010.11929. [7] Patrick Esser et al. Scaling rectified flow transformers for high-resolution image synthesis. In: Proceedings of the 41st International Conference on Machine Learning. ICML24. Vienna, Austria: JMLR.org, 2024. [8] Nuno Goncalves, Marcos Treviso, and Andre F. T. Martins. AdaSplash: Adaptive Sparse Flash Attention. In: arXiv preprint arXiv:2502.12082 (2025). url: https://arxiv.org/abs/2502.12082. [9] Ali Hassani, Wen-Mei Hwu, and Humphrey Shi. Faster Neighborhood Attention: Reducing the O(n2) Cost of Self Attention at the Threadblock Level. In: Advances in Neural Information Processing Systems. 2024. [10] Ali Hassani and Humphrey Shi. Dilated Neighborhood Attention Transformer. In: (2022). arXiv: 2209.15001 [cs.CV]. url: https://arxiv. org/abs/2209.15001. [11] Ali Hassani et al. Neighborhood Attention Transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2023, pp. 61856194. [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle et al. Vol. 33. Curran Associates, Inc., 2020, pp. 6840 6851. 13 [13] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, et al. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention. In: arXiv preprint arXiv:2407.02490 (2024). [14] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In: 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings. 2014. arXiv: http://arxiv.org/abs/1312.6114v10 [stat.ML]. [15] Nikita Kitaev, (cid:32)Lukasz Kaiser, and Anselm Levskaya. Reformer: The Efficient Transformer. 2020. arXiv: 2001.04451 [cs.LG]. url: https:// arxiv.org/abs/2001.04451. [16] Weijie Kong et al. HunyuanVideo: Systematic Framework For Large Video Generative Models. 2025. arXiv: 2412.03603 [cs.CV]. url: https: //arxiv.org/abs/2412.03603. [17] Ze Liu et al. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2021. [18] OpenAI. Video generation models as world simulators. https://openai. com/index/video-generation-models-as-world-simulators/. 2024. [19] Matteo Pagliardini et al. Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention. In: Advances in Neural Information Processing Systems. Ed. by A. Oh et al. Vol. 36. Curran Associates, Inc., 2023, pp. 5980859831. url: https://proceedings.neurips.cc/paper_ files/paper/2023/file/bc222e8153a49c1b30a1b8ba96b35117-PaperConference.pdf. [20] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In: arXiv preprint arXiv:2212.09748 (2022). [21] Adam Polyak et al. Movie Gen: Cast of Media Foundation Models. 2025. arXiv: 2410.13720 [cs.CV]. url: https://arxiv.org/abs/2410.13720. [22] Robin Rombach et al. High-Resolution Image Synthesis with Latent Diffusion Models. 2021. arXiv: 2112.10752 [cs.CV]. [23] Xin Tan, Yuetao Chen, Yimin Jiang, et al. DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training. In: arXiv preprint arXiv:2502.07590 (2024). [24] Xin Tan et al. DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training. 2025. arXiv: 2502 . 07590 [cs.DC]. url: https : //arxiv.org/abs/2502.07590. [25] Ashish Vaswani et al. Attention is all you need. In: Advances in neural information processing systems 30 (2017). [26] Arkhipkin Vladimir et al. Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Ed. by Delia Irazu Hernandez Farias, Tom Hope, and Manling Li. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 475485. doi: 10.18653/v1/2024.emnlp-demo.48. url: https://aclanthology.org/2024.emnlp-demo.48/. [27] Team Wan et al. Wan: Open and Advanced Large-Scale Video Generative Models. In: arXiv preprint arXiv:2503.20314 (2025). [28] Sinong Wang et al. Linformer: Self-Attention with Linear Complexity. 2020. arXiv: 2006.04768 [cs.LG]. url: https://arxiv.org/abs/2006. 04768. [29] Yuxin Wen et al. Analysis of Attention in Video Diffusion Transformers. 2025. arXiv: 2504.10317 [cs.CV]. url: https://arxiv.org/abs/2504. 10317. [30] Jeffrey Willette, Heejun Lee, and Sung Ju Hwang. Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction. 2025. arXiv: 2505.11254 [cs.LG]. url: https://arxiv.org/abs/2505.11254. [31] Haocheng Xi, Shuo Yang, Yilong Zhao, et al. Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity. In: arXiv preprint arXiv:2502.01776 (2025). [32] Yifei Xia et al. Training-free and Adaptive Sparse Attention for Efficient Long Video Generation. 2025. arXiv: 2502.21079 [cs.CV]. url: https: //arxiv.org/abs/2502.21079. [33] Zhuoyi Yang et al. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. 2025. arXiv: 2408.06072 [cs.CV]. url: https:// arxiv.org/abs/2408.06072. [34] Jingyang Yuan et al. Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention. In: 2025. url: https : / / api . semanticscholar.org/CorpusID:276408911. [35] Jintao Zhang et al. SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference. 2025. arXiv: 2502 . 18137 [cs.LG]. url: https : //arxiv.org/abs/2502.18137. [36] Peiyuan Zhang et al. Fast Video Generation with Sliding Tile Attention. 2025. arXiv: 2502.04507 [cs.CV]. 15 Fine-Tuning Hyperparameters We performed knowledge distillation on the full-attention Wan2.1 open-source model using MSE loss. The training utilized specially curated dataset of highquality, high-dynamic videos and high-quality images. The experiments were conducted on 256 H100 GPUs, using the following hyperparameters: Total batch size: 64 Sequence parallel: 4 Training steps: Optimizer: AdamW with: Learning rate: 1e-6 Weight decay: None Betas: (0.9, 0.95) Epsilon: 1e-8 Gradient norm: 0."
        },
        {
            "title": "B Generation Examples",
            "content": "The following examples (Fig. 5,6,7,8) demonstrate generation results from different model configurations, presented clockwise from the top-left corner: Full Attention (Baseline Wan 2.1), STA(18,24,24), NABLA(0.4), and NABLA(0.2) + STA(11,24,24). Figure 5: Input prompt: Movie scene of time portal opening up in modern city, 18th century young blonde man walks out of it looking confused, close-up, sci-fi, Netflix Original, professionally color graded, 35mm film 17 Figure 6: Input prompt: sleek black laptop made of durable aluminium with flat rectangular shape. It is medium-sized device with 14-inch screen. The laptop features backlit keyboard and comes with charger. The text on the device reads Dell. Figure 7: Input prompt: Movie scene of time portal opening up in modern city, 18th century young blonde man walks out of it looking confused, close-up, sci-fi, Netflix Original, professionally color graded, 35mm film Figure 8: Input prompt: Cinematic shot of Beta fish swimming, moving dynamically in the water. daisy is transformed into group of butterflies. The fish has orange, blue and yellow colors. Beautiful nature documentary, low contrast, 35mm, color correction."
        },
        {
            "title": "C VBench Results",
            "content": "Table 4: VBench results for 90 % sparsity. Bold values indicate the best performance in each category."
        },
        {
            "title": "Metric",
            "content": "Wan2.1-14B STA (18,24,24) NABLA NABLA(0.2)+ STA(11,24,24) (0.4)"
        },
        {
            "title": "Subject consistency\nBackground consistency\nAesthetic quality\nImaging quality\nObject class\nMultiple objects\nColor\nSpatial relationship\nScene\nTemporal style\nOverall consistency\nHuman action\nTemporal flickering\nMotion smoothness\nDynamic degree\nAppearance style\nQuality score\nSemantic score\nTotal score",
            "content": "94.6 98.63 67.27 66.46 81.09 70.57 89.83 70.97 45.36 23.34 25.80 95 98.91 98.38 70.83 22.69 85.15 75.23 83.16 95.00 98.44 67.51 66.18 82.12 50.53 85.61 66.45 48.11 22.95 26.65 0.9 98.83 98.65 68.06 22.51 85.05 71.73 82.39 95.01 98.67 67.10 66.35 85.83 66.23 85.19 75.44 50.07 23.46 26.10 91 98.88 98.53 68.05 23.18 85.02 75.76 83.17 93.18 98.25 67.63 66.28 82.91 67.98 92.22 70.07 51.38 22.99 26.00 93 98.78 98.58 72.22 23.14 85.03 76.04 83."
        },
        {
            "title": "D Attention map examples",
            "content": "(a) (b) (c) (d) (e) (f) (g) (h) Figure 9: Examples of attention maps for different heads of Wan 2.1 14B T2I layers. (i)"
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute (AIRI), Moscow, Russia",
        "Lomonosov Moscow State University (MSU), Moscow, Russia",
        "Moscow Institute of Physics and Technology (MIPT), Moscow, Russia",
        "Sber AI, Moscow, Russia"
    ]
}