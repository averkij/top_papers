{
    "paper_title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
    "authors": [
        "Christian Schlarmann",
        "Francesco Croce",
        "Nicolas Flammarion",
        "Matthias Hein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastive language-image pre-training aligns the features of text-image pairs in a common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into a single feature vector. As a remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use a single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 9 0 3 0 . 6 0 5 2 : r FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens Christian Schlarmann Tübingen AI Center University of Tübingen"
        },
        {
            "title": "Nicolas Flammarion\nEPFL",
            "content": "Matthias Hein Tübingen AI Center University of Tübingen"
        },
        {
            "title": "Abstract",
            "content": "Contrastive language-image pre-training aligns the features of text-image pairs in common latent space via distinct encoders for each modality. While this approach achieves impressive performance in several zero-shot tasks, it cannot natively handle multimodal inputs, i.e., encoding image and text into single feature vector. As remedy, it is common practice to use additional modules to merge the features extracted by the unimodal encoders. In this work, we present FuseLIP, an alternative architecture for multimodal embedding. Leveraging recent progress in discrete image tokenizers, we propose to use single transformer model which operates on an extended vocabulary of text and image tokens. This early fusion approach allows the different modalities to interact at each depth of encoding and obtain richer representations compared to common late fusion. We collect new datasets for multimodal pre-training and evaluation, designing challenging tasks for multimodal encoder models. We show that FuseLIP outperforms other approaches in multimodal embedding tasks such as VQA and text-guided image transformation retrieval, while being comparable to baselines on unimodal tasks."
        },
        {
            "title": "Introduction",
            "content": "Contrastive language-image pre-training (CLIP) [31] is fundamental approach for learning semantically rich text and image representations. The resulting text and image encoders perform well in many zero-shot tasks [31, 14] and have been successfully applied to image generation [33], transfer learning [44, 7], and multimodal large language models (LLMs) [26, 4]. To improve CLIP-like models, various refinements of encoder architectures, training data, and optimization schemes have been proposed [8, 47, 39]. However, these models are not designed to extract representations from multimodal inputs, i.e., encoding an image-text pair into single feature vector, as text and images are processed by two separate encoders. Several techniques have adapted pre-trained CLIP models for multimodal retrieval [27, 35, 43, 15, 48, 49] or other downstream tasks [38, 45]. These methods typically merge features extracted by frozen unimodal encoders through either fixed functions or learnable modules. different line of work trains multimodal sequence-to-sequence models [42, 29, 30, 1] using an encoder-decoder architecture [32]. While well-suited for transfer learning, these models lack the strong zero-shot capabilities of CLIP. In this work, we propose novel multimodal embedding method that extends CLIP to multimodal inputs while preserving its strong vision-language alignment and zero-shot capabilities.1 Our first key novelty is to encode all input modalities (image, text, and their combinations) using single encoder. We achieve this by leveraging discrete image tokenizer [46], which together with the standard text tokenizer maps inputs into unified sequences of tokens drawn from finite multimodal vocabulary. Since the image tokenizer is trained solely for image compression and reconstruction, it does not 1Code and models are available at https://github.com/chs20/fuselip Preprint. introduce any bias with respect to textimage alignment. Processing tokenized inputs with single encoder, i.e., early fusion, allows modalities to interact at every encoding stage, which differs from late fusion where the deeper representations from unimodal encoders are merged [27, 48]. Notably, we can train our architecture, named FuseLIP, with contrastive loss similar to standard CLIP despite using single encoder. Moreover, discrete tokenizers enable us to seamlessly incorporate masked multimodal modeling (MMM) loss into our training objective without the need for multiple auxiliary modules or the computational overhead incurred by prior work [38]. Combining the MMM loss with the contrastive objective consistently enhances FuseLIPs performance across various zero-shot tasks (classification, retrieval, VQA, grounding), surpassing or matching late fusion baselines. To comprehensively evaluate multimodal embedding models, we further introduce novel tasks and datasets designed to test modality interactions. Interestingly, we show that late fusion approaches struggle to solve tasks where visual information of the image is more relevant than its semantic content, such as recognizing correct outputs of text-guided visual transformations. Conversely, our early fusion architecture does not show this limitation, since the two modalities communicate at every level of encoding after tokenization. Finally, we demonstrate that training with hard negative examples is essential for successfully learning these multimodal tasks. Contributions. In summary, our work introduces FuseLIP, novel multimodal embedding model, based on early fusion of discrete image and text tokens, processed by single transformer encoder, which achieves performance surpassing or comparable to existing late fusion methods, shows that FuseLIP can be effectively trained on both unimodal and multimodal data using the standard contrastive loss and incorporating hard negative examples, while natively supporting and significantly benefiting from masked modeling objective, proposes novel evaluation tasks for multimodal embedding models, complementary to existing benchmarks, highlighting the importance of early fusion and hard negative examples. We believe our work uncovers several interesting aspects of both design, training, and evaluation of multimodal embedding models, and may impact future research in this area."
        },
        {
            "title": "2 Related Work",
            "content": "Unimodal embedding. Popular methods for vision-language pre-training such as CLIP [31], SigLIP [47] and ALIGN [18] use separate networks to embed each modality. An image encoder ϕθ : Rd and text encoder ψω : Rd, with disjoint parameters θ and ω, map data into shared ddimensional space. Image-text pairs with corresponding semantics are aligned via contrastive [31] or sigmoid [47] loss on large image-caption datasets. These models achieve good zero-shot performance in tasks like image classification and retrieval, where inputs to be encoded are from single modality. Multimodal embedding via late fusion. Certain tasks require encoding multimodal inputs into single feature vector, which typical methods like CLIP cannot directly handle. Alternative approaches merge the representations from text and image encoders. In score-level fusion [27], unimodal embeddings are summed (possibly with weighting), i.e. given an image-text input (i, t), the multimodal embedding is ϕθ(i) + ψω(t). This method avoids introducing additional merging modules. In contrast, feature-level fusion [38, 48] feeds unimodal embeddings to an additional network γ, typically transformer-based architecture with multiple attention layers, i.e., γ(ϕθ(i), ψω(t)). Finally, BridgeTower [45] interconnects features within the latter blocks of the vision (from CLIP) and language encoders (RoBERTa): the resulting model is then fine-tuned on various downstream tasks. Conversion of multimodal LLMs. Recent works [19, 20] convert autoregressive large multimodal models (LMMs) into encoders. Such conversion is achieved by fine-tuning an LMM with contrastive learning to return semantically aligned feature vectors. This strategy leverages the large pre-training datasets of the LMM to obtain rich representations, but comes with high inference costs due to their large size. Multimodal embedding for composed image retrieval. Several embedding methods have been proposed for multimodal and composed image retrieval. One approach trains adapter modules on pre-trained CLIP model to map the vision encoders output to the text encoders input space, enabling image-text encoding [35, 2, 15]. VISTA [49] fine-tunes the vision encoder of CLIP to concatenate its 2 (a) Score-level fusion [27] (b) MagicLens [48] (c) FuseLIP (ours) Figure 1: Comparison of architectures. To obtain multimodal embedding via contrastive learning, late fusion approaches first extract unimodal representations via unimodal encoders, then merge by addition [27] or fusion module [48]. Conversely, our FuseLIP uses frozen image tokenizer to tokenize inputs of any modality into tokens from unified vocabulary, which are then processed by single encoder model. This approach leads to simple architecture and early fusion of modalities. output to token embeddings in BERTs input space. Notably, these approaches rely on pre-trained CLIP (and BERT) models, and are specialized for retrieval tasks. Early fusion in masked multimodal modeling. In [30, 1] encoder-decoder architectures are trained with masked modeling loss on multimodal data, using tokenized and pixel-based RGB images for vision datasets. These models can generate data across modalities and adapt to different tasks via full fine-tuning, but lack zero-shot capabilities enabled by CLIPs contrastive pre-training."
        },
        {
            "title": "3.1 Architecture",
            "content": "Late fusion approaches merge image and text representations only in later layers, after independent encoding. Thus, each modality has limited influence on the final features since the modalities communicate only when already heavily processed. However, multimodal embedding should be strongly conditioned on both text and image prompts. For instance, in Fig. 1, the joint embedding of the image and the query What color is the headband? should align with that of Blue, and exclude irrelevant image details. To achieve this, we propose (a) enabling early interaction between modalities by merging them immediately after tokenization and (b) processing them with single encoder. Tokenization. While text tokenization is straightforward, compressing images into tokens is more complex. We leverage recent progress in discrete image tokenizers [41, 13, 46, 22], and use those from the TiTok family [46] (frozen during our training), encoding each image into 128 tokens. First, this ensures symmetry between image and text tokenization, allowing the transformer-based encoder to operate over finite vocabulary. This property allows us to use masked (multimodal) modeling (MMM) loss, popular pre-training objective, without ad-hoc tokenizers or extra computation, unlike FLAVA [38]. As shown in Sec. 5.4, the MMM loss significantly improves model performance. Second, TiTok tokenizers, trained for image reconstruction on ImageNet without text-guided semantic alignment, avoid bias towards specific representation that may arise during pre-training. In contrast, VISTA [49] initializes its vision encoder from CLIP, inheriting biases from contrastive pre-training. Finally, recent image tokenizers provide high-quality compression, as shown by their excellent generation properties [46, 22] and their adoption in large multimodal autoregressive models such as Chameleon [40]. Early fusion. Text and image are tokenized separately with single vocabulary of non-overlapping text and vision tokens. The resulting token sequences are concatenated (images first, followed by 3 text), with special beginning and end of text tokens (<bot>, <eot>) to separate modalities. Unimodal inputs omit the missing modality (an empty string is appended to image-only inputs). The tokens are then mapped to d-dimensional vectors by an embedding matrix, combined with an additive positional embedding, and processed by the encoder. Encoder. We adopt the transformer-based architecture of the SigLIP text encoder, consisting of transformer blocks with bidirectional attention and no causal masking. Masking is applied to exclude empty tokens from the self-attention computation. The final embedding corresponds to the <eot> : Rd, mapping tokens output. Overall, our model can be represented as fθtok,θenc multimodal input (i, t) (which may be unimodal when one modality is missing) to d-dimensional feature vector, and parameterized by the weights of the tokenizer θtok and encoder θenc. Auxiliary prediction head. For the masked modeling loss, we introduce classification head to predict masked tokens. This module maps the output of the final transformer block to predictions over the token vocabulary and follows the FLAVA architecture [38]: two-layer network with shared embedding and unembedding matrices. We denote this prediction head as hθhead : Rd RV , parameterized by θhead."
        },
        {
            "title": "3.2 Training objective",
            "content": "Contrastive loss. To match the zero-shot performance of popular language-image pre-training methods, we optimize the sigmoid loss from SigLIP [47], which slightly outperforms the contrastive loss of CLIP [31]. For standard dual encoder with separate visual and text towers, and batch of image-text pairs {(ik, tk)}B k=1, the sigmoid loss is (cid:88) (cid:88) LSigLIP = 1 r= s=1 log (cid:0)1 + ezrs(tϕ(ir)ψ(ts)+b)(cid:1), where ϕ(ir) and ψ(ts) are the normalized embedding from the vision and text encoders, zrs = 1 for positive pairs and 1 otherwise, and t, are learnable parameters. In our case, the single multimodal k)}B encoder processes multimodal pairs {(z1 can be text, image, or image-text inputs. The loss function is then rewritten as k=1, where z1 and z2 k, z"
        },
        {
            "title": "LMM",
            "content": "SigLIP = 1 B (cid:88) (cid:88) log (cid:0)1 + ezrs(tf (z1 )f (z )+b)(cid:1), r=1 where () represents the normalized unimodal or multimodal embedding. s=1 Masked modeling loss. Masked modeling is popular pre-training approach for both text [10] and image tasks [3], where one wants to recover input tokens which have been masked. FLAVA [38] shows its effectiveness in multimodal embeddings. However, FLAVA relies on late fusion, using separate vision and text encoders merged by multimodal module similar to vision transformer [11]. Since the vision, text and multimodal embedding are provided by different branches of its architecture, head for each input modality type is added to predict the masked tokens. Because its vision encoder produces continuous embeddings, an additional discrete tokenizer is required for masked modeling, increasing both computational cost and parameter count. The FuseLIP architecture simplifies the application of masked modeling loss to train multimodal encoders. Since all input modalities are mapped to discrete tokens and processed by the same encoder, additional tokenizers and multiple prediction heads are unnecessary. Moreover, the masked modeling and contrastive losses are applied on the same masked input, avoiding extra computational overhead. As shown in Sec. 5.4, this strategy significantly improves performance across tasks. In practice, each token (except special tokens) is replaced with <MASK> token with probability = 0.1. Denoting J(z) as the set of masked positions of the masked tokens for an input z, (z) as the corresponding labels, and as the output of the last transformer block, the masked multimodal modeling (MMM) loss for the batch = {(z1 k, z2 k)}B k=1 is 2 (cid:88) (cid:88) LMMM = 1 (cid:88)"
        },
        {
            "title": "LCE",
            "content": "(cid:0)h(f j(zi r)), y(cid:1) , r=1 i=1 (j,y)(J(zi r),Y (zi r)) where is the auxiliary prediction head and LCE the cross entropy loss. 4 Query Retrieval pool Query Retrieval pool The dog on the left The toy on the right - o - p G - Retrieval pool Query Taxi Fork headlight on motorcycle"
        },
        {
            "title": "Spectators in the stands on a warm day",
            "content": "Figure 2: OI-Pos, OI-Crop and VG-Crop tasks. We show examples of these tasks (described in Sec. 5.2). The retrieval pool of OI-Crop comprises crops from the same image, as well as crops of the target object from other images. In contrast, for OI-Pos and VG-Crop it contains only crops of the query image. We show the whole retrieval pool for OI-Pos, OI-Crop, and sample for VG-Crop. For every sample the ground-truth answer is highlighted by the red frame. Final training objective. The final minθenc,θhead LMM image tokenizer remains frozen during training, meaning that θtok is not optimized. losses: SigLIP + α LMMM, where α balances the two losses (α = 0.25 in all experiments). The problem combines optimization two the"
        },
        {
            "title": "4.1 Unimodal data",
            "content": "To train FuseLIP and baseline models, we collect variety of unimodal and multimodal data. We refer to image-text (I T) pairs data as unimodal since they do not require joint encoding of inputs from different modalities. These datasets are commonly used to train CLIP-like models, and can also be leveraged by multimodal encoders. We use CC3M [36] and CC12M [6] datasets, as they provide high-quality images and captions, and are amenable to training within academic compute constraints."
        },
        {
            "title": "4.2 Multimodal data",
            "content": "Unlike unimodal datasets, datasets with multimodal inputs are scarce. Thus, besides relying on those, we generate additional multimodal tasks from unimodal data, effectively scaling dataset size at minimal or no additional collection cost. The diversity of these tasks enables more comprehensive evaluation of different model capabilities. Text-guided image transformations (CC3M/CC12M-TGIT). We generate multimodal data from image datasets by applying transformations and describing them in text. Specifically, we form pairs (i t, i), where is the original image, is the transformation description in natural language, and is the transformed image. We consider transformations such as random cropping, random rotations, 5 Table 1: Number of samples per dataset. We show the number of samples (in million) of the training datasets described in Sec. 4, and the combination of modalities that each dataset exhibits. Setting CC TGIT CC3MVQA VGVQA VGCrop HQEdit IT IT IT IT IT I(T) Table 2: Parameter comparison (in million). Grey indicates non-trainable parameters. Model Image Text Fusion Trained Total SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF 86.2 63.5 13.7 21.8 40.5 21.8 40.5 86.2 63.5 - 7.7 - 62.3 62.3 70.0 70.0 149.7 149.7 163.4 163.4 CC3M 2.6 CC12M 10.6 0.3 0.3 2.4 2.4 0.7 1.4 5.4 5. 0.3 0.3 FuseLIP-S FuseLIP-B 25.9 86.6 - - 42.1 65.6 42.1 65. 68.0 152.2 flipping, colorization, and color jittering. See App. A.1 for detailed description and examples. We apply this approach to subset of CC3M and CC12M but it can be extended to any image dataset. Since text prompts describe only the transformations, without semantic information about the image, the model must rely on both modalities and cannot solve the task using only text or image. VQA data from image-text datasets (CC3M-VQA). We generate VQA data from CC3M by prompting an LLM (Llama-3.1-8B-Instruct [12]) to rewrite captions as question-answer pairs using structured system prompt (see Fig. 3 in Appendix). We generate 2.4M VQA samples. This method can be applied to any image-caption dataset, thereby facilitating scalability. While generating VQA from image-text pairs was explored in [5], we leverage the recent advancements in LLMs for simplified and unsupervised generation pipeline. VQA data from Visual Genome (VG-VQA). We use the existing VQA samples from Visual Genome (VG) [23], denoted as VG-VQA, in the standard IT format. Visual Grounding with Visual Genome (VG-Crop). VG contains images with rich annotations, in particular natural language descriptions of image regions that are bounded by rectangular boxes. We use these descriptions to build training dataset, referred to as VG-Crop. Namely, given an image and textual description of some region in i, we form pairs (i d, i). That is, the model is tasked to find the crop of the image, given natural language instruction. Thus, the modality combination is IT I, which is complementary to that of VQA datasets. HQ-Edit. Finally, HQ-Edit [17] consists of synthetically generated image edits. We integrate this dataset into our training by tasking the model to find the correctly edited image. Given an image i, an edit described in natural language e, and the edited image i, we form pairs (i e, i). Using the inverse edits that are contained in HQ-Edit, we also form the corresponding inverse pairs (i e, i). As HQ-Edit also contains captions of the edited images, we build additional training samples as (i e, c). This covers both IT and IT IT modalities."
        },
        {
            "title": "4.3 Training with hard negatives",
            "content": "For training, we merge all datasets described in Sec. 4.1 and Sec. 4.2, ensuring that batches contain diverse tasks and modalities. Due to the contrastive nature of the SigLIP loss, each sample functions as negative for all other samples. Closely related samples, called hard negatives, have been shown to improve contrastive learning [21, 34, 48]. We design and integrate hard negatives into training of FuseLIP and baselines by ensuring that batches contain semantically similar examples. Specifically, for CC3M-TGIT and CC12M-TGIT we sample multiple transformations of the same image, for VG-Crop and VG-VQA each batch includes three additional samples from the same query image, with different descriptions or questions, and for HQ-Edit, we include the corresponding inverse edit sample in the batch. We show in Sec. 5.4 the key role of hard negatives for learning these tasks."
        },
        {
            "title": "5.1 Setup",
            "content": "Models. We train two versions of FuseLIP: FuseLIP-S uses the TiTok-S tokenizer and small transformer as implemented in the OpenCLIP library [8], while FuseLIP-B uses the TiTok-B tokenizer and base transformer. For comparison, we consider two late fusion baselines: (1) score fusion (SF), where the multimodal embedding is obtained by summing the unimodal embeddings from text and 6 Table 3: Evaluation on embedding tasks. We report accuracy of late fusion (SigLIPSF, SigLIPMLF) and our FuseLIP models, trained on either CC3M or CC12M plus multimodal data. Our FuseLIP-B achieves the best results across nearly all tasks, despite having fewer trainable parameters than SigLIP-B. The large margin on TGIT, even for the smaller FuseLIP-S, indicates that early fusion better captures the specific image-text relation of its samples. Training Model Classification VQA Retrieval Grounding ImageNet VG-Crop OI-Crop OI-Pos TGIT CC3M +MM CC12M +MM SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B 21.5 18.0 22.2 19. 18.5 23.3 30.4 28.5 31.5 30.3 25.2 31.2 12.7 14.2 13.6 14.8 15.9 17.5 16.2 16.9 17.0 16. 18.2 19.8 13.0 12.7 13.4 13.9 11.2 15.0 23.8 23.2 23.8 23.2 20.1 26.2 74.8 74.2 77.2 76. 70.8 82.4 74.2 72.7 72.7 73.4 75.2 82.3 8.8 10.2 10.3 12.2 13.5 18.1 21.4 25.5 25.4 28. 26.0 32.7 52.0 53.0 55.1 55.4 49.6 55.8 57.1 58.8 58.0 61.5 53.5 61.5 55.2 66.2 56.9 68. 59.8 68.1 60.1 72.2 63.2 74.0 64.7 71.3 45.4 46.9 45.9 47.4 53.9 70.8 47.1 46.6 47.3 48. 61.5 68.9 57.3 67.2 56.6 69.4 79.0 94.3 66.0 81.0 67.1 78.1 90.6 94.2 vision encoders, and (2) the MagicLens [48] fusion (MLF) approach which uses transformer-based module to merge the unimodal embedding vectors (see Fig. 1). Both baselines use text and vision encoders from CLIP with ViT-S or ViT-B architecture [11], and are trained on the SigLIP loss using the same datasets and hard-negatives as FuseLIP. We denote them as SigLIPSF and SigLIPMLF respectively. As we train all models from scratch, we do not compare against methods that fine-tune pre-trained models [48, 20]. In Table 2, we report the parameter count for all architectures: FuseLIP-S has total number of parameters similar to the S-sized baselines, but significantly fewer trainable ones as the image tokenizer remains frozen. In contrast, the total parameter amount of FuseLIP-B roughly matches that of B-sized baselines, while the trainable parameter amount of FuseLIP-B is similar to the S-sized baselines. This discrepancy in trainable versus total parameters also affects the training cost: training of FuseLIP is faster and requires less GPU memory as shown in App. A.2. Training details. For CC3M plus multimodal data (CC3M + MM) we train for 8 epochs (total of 93M seen samples), while for CC12M plus multimodal data (CC12M + MM) we train for 16 epochs (326M samples), see Table 1. Full training hyperparameters are provided in App. A.3."
        },
        {
            "title": "5.2 Multimodal evaluation tasks",
            "content": "For evaluation, we consider variety of embedding tasks. Besides testing on VG-Crop and CC3MTGIT, used during training, we collect existing and new datasets described here (details in App. A.4). Massive Multimodal Embedding Benchmark (MMEB). MMEB [20] is multimodal embedding benchmark consisting of 36 subtasks, that are split into the categories Classification, VQA, Retrieval, and Grounding and cover multiple modalities. Many of the subtasks are considered standard tasks in their domain. Each subtask includes 1000 samples: for each sample the model has to select the correct answer among 1000 candidates (less for classification). Visual Grounding with OpenImages (OI-Crop&OI-Pos). We leverage OpenImages [24] to create new task where the goal is to select the correct crop of an image given query text. For OI-Crop the query text is the name of an object that is present in the image. As candidates, we include five crops of other objects from the query image and five crops of the same object from other images (in contrast to VG-Crop which uses only crops of the same image). For OI-Pos, the model has to select the correct crop of an object from an image that contains this object exactly twice, given an instruction to select the left/right instance. The retrieval pool includes crops from the outer parts of the query image as decoy. We give detailed description of the data collection method in App. A.4, which results in 1046 and 2546 samples respectively (see Fig. 2). VG-Crop. We use 1574 validation samples from VG-Crop (Sec. 4.2), using all existing regions in each sample as the respective candidates (see Fig. 2). We prune regions that significantly overlap 7 Table 4: Breakdown over CC3M-TGIT tasks. Accuracy (%) of each model on five text-guided transformations. Only FuseLIP variants solve Crop, Rotate and Flip. Trained on CC3M+MM Trained on CC12M+MM"
        },
        {
            "title": "Model",
            "content": "Crop Rotate Flip Jitter Color Crop Rotate Flip Jitter Color SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B 44.9 46.6 40.2 42.7 93.1 99.4 45.5 41.7 44.5 61. 49.4 94.1 22.7 58.0 22.6 55.4 73.4 90.1 78.2 89.9 79.2 87.7 80.3 87.9 95.2 99.9 96.7 99. 98.9 99.8 49.7 75.8 48.5 55.0 97.4 99.7 84.7 83.3 84.6 85.8 81.8 94.7 15.6 56.4 18.5 58. 92.1 89.2 80.7 89.7 84.2 90.8 82.2 87.7 99.4 100.0 99.7 99.9 99.6 99.6 (IoU over 0.3) to facilitate unambiguous tasks. The size of the retrieval pool depends on the amount of available region descriptions, with an average of 15.9 images. CC3M-TGIT. We use the validation split of CC3M-TGIT to test models on retrieving the correctly transformed image. For crop and rotation the retrieval pools consists of all possible transformations (9 and 18), for jitter we use 10, for flip the original and horizontally/vertically flipped images (3), and for colorize the original and the target sample (2). Each subtask is evaluated on 1000 samples. ImageNet. We evaluate on the full ImageNet-1k validation set [9] with the ensemble of OpenAI prompt templates [31] (in contrast, the ImageNet evaluation in MMEB uses single prompt)."
        },
        {
            "title": "5.3 Main results",
            "content": "Table 3 reports the performance of the various models on the evaluation tasks detailed above. First, FuseLIP-B achieves the best results across nearly all tasks and training data configurations, often with large margin. It attains highest scores on 8 respectively 7 out of 9 benchmarks in the CC3M+MM and CC12M+MM training configurations. While the total parameters of FuseLIP-B are similar to B-sized baselines, it has significantly fewer trainable ones  (Table 2)  . Notably, the non-trainable parameters come from the frozen image tokenizer, which is trained for image reconstruction and does not contribute directly to image-text alignment. Second, the smaller FuseLIP-S (same total but fewer trainable parameters than the S-sized baselines) is competitive even to B-sized baselines, with better results in VQA, OI-Pos, and CC3M-TGIT. As FuseLIP significantly outperforms late fusion models, especially score fusion, on CC3M-TGIT, we analyze this key result in more detail below. Overall, these results suggest that early fusion of discrete tokens, even with single encoder, is highly effective for aligning multimodal representations. Why early fusion helps to understand text-guided transformations. Our models achieve the largest improvements over baselines on CC3M-TGIT, where even FuseLIP-S outperforms SigLIPBMLF by 910% and SigLIP-BSF by 2224%. To analyze this effect, Table 4 presents breakdown of performance across individual tasks in CC3M-TGIT. The advantage of FuseLIP emerges specifically in tasks requiring identification of the correct image after cropping, rotation, or flipping (horizontal or vertical). Unlike the baselines, FuseLIP solves these tasks almost perfectly (see qualitative examples in Fig. 4). This improvement likely stems from the nature of these tasks, which rely on capturing the visual structure rather than semantic content. The unimodal encoders tend to extract semantic information which is used to align different inputs in the latent space. It is reasonable to hypothesize that features at deeper layers contain higher amount of semantic information, at the expense of other aspects such as visual information. Moreover, attending to both image and text is crucial to solve these tasks, as no shortcut can be learned by just looking at input and target images or input text and target images. Late fusion models likely have limited access to the information necessary to solve the task, while our early fusion approach can easily learn it. This is further supported by the performance gap between the late fusion baselines: SigLIPMLF, which uses learnable deep fusion network, outperforms SigLIPSF, which simply sums the unimodal embeddings. This interpretation can also explain the better results of FuseLIP compared to the baselines on OI-Pos, where the model needs to distinguish left and right instances of the same object. 8 Table 5: Analysis of training components. We report the effect of removing either hard negatives (Sec. 4.3) or the MMM loss (Sec. 3.2) for training models on CC3M+MM. Hard negatives are crucial for good performance on VG-Crop, OI-Crop, OI-Pos, and CC3M-TGIT. The MMM loss can only be applied to FuseLIP and improves performance across all tasks. Model Hard Neg. LMMM Classif. VQA Retrieval Grounding ImNet VG-Crop OI-Crop OI-Pos TGIT SigLIP-SSF SigLIP-SMLF FuseLIP-S FuseLIP-B - - - - 19.5 21. 17.1 18.0 20.0 17.8 18.5 23.3 21.0 23.3 13.2 12.7 13.5 14.2 16.5 15.7 15. 17.5 16.8 17.5 11.3 13.0 11.4 12.7 11.9 10.5 11.2 14.6 14.3 15.0 80.6 74. 76.5 74.2 76.2 66.2 70.8 82.5 81.3 82.4 8.6 8.8 9.7 10.2 15.1 12.3 13. 18.4 16.3 18.1 23.4 52.0 34.1 53.0 35.0 46.6 49.6 38.8 53.7 55.8 39.1 55. 46.6 66.2 51.0 58.8 59.8 52.5 63.3 68.1 40.5 45.4 43.0 46.9 51.7 50.7 53. 45.3 68.0 70.8 11.5 57.3 22.1 67.2 18.5 83.8 79.0 13.6 88.4 94."
        },
        {
            "title": "5.4 Ablation on the importance of hard negatives and masked modeling loss",
            "content": "To better understand the contribution of some design choices in FuseLIP, we train models selectively removing the hard negatives from training (Sec. 4.3) and the masked modeling loss, and report the results in Table 5. First, we see that not including hard negatives in the batch causes large drops in performance on VG-Crop, OI-Crop, and especially CC3M-TGIT (e.g., for FuseLIP-B accuracy decreases from 94.3% to 13.6%). Moreover, adding the hard negatives does not affect performance on the other tasks for FuseLIP-B, and yields only minor decrease for the smaller FuseLIP-S. Similar observations about the benefit of hard negatives hold true also for the late fusion baselines. Second, Table 5 illustrates the crucial role of the MMM loss: training only with the SigLIP loss (without any masking) leads to significantly worse results across all tasks, in particular for the larger FuseLIP-B. We provide additional evidence of the advantages given by the MMM loss in Table 13 in Appendix. Finally, we study further properties of FuseLIP such as compositionality, the modality gap, and fine-tuning on MMEB in App. B."
        },
        {
            "title": "6 Discussion",
            "content": "The results of FuseLIP have several implications. First, it is possible to train CLIP-like model (on either unimodal or multimodal data) using single encoder, in contrast to standard CLIP models that rely on separate text and image encoders. Second, our architecture, which inherently supports multimodal embeddings, enables seamless integration of contrastive and masked modeling objectives. This significantly simplifies the FLAVA training setup (see Sec. 3.2), showing that both objectives can be combined without requiring separate forward passes. Moreover, our models achieve stable training by using standard recipes. Third, our results highlight tasks, such as the text-guided transformations, where early fusion of modalities significantly outperforms late fusion. Since solving such tasks is part of comprehensive multimodal encoder, we argue that early fusion is particularly promising for multimodal embeddings. Finally, we anticipate that FuseLIP can be naturally extended to new applications, including encoding multiple images or interleaved image-text inputs."
        },
        {
            "title": "7 Conclusion",
            "content": "We have introduced novel approach to multimodal embedding models, designing an early fusion architecture with discrete image tokenization and single encoder. Our simple training recipe combines contrastive and masked modeling objectives, while leveraging hard negative samples. The individual components as well as the final models are empirically validated on variety of tasks. Our methods for generating training datasets that are tailored for multimodal learning can be applied at scale. Moreover, our novel evaluation tasks are complementary to the existing benchmarks typically used for testing embedding models. Overall, we believe our approach and datasets can be valuable building blocks for future research on multimodal embedding models. 9 Limitations. Given the limited computational resources, we could not test the effect of scaling data and size for FuseLIP. Moreover, while our models are faster and require significantly less memory at training time, they have more expensive inference cost than the baselines, but we expect the gap to be reduced at scale and thanks to the ongoing progress of image tokenizers."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting C.S. We acknowledge support from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy (EXC number 2064/1, project number 390727645), as well as in the priority program SPP 2298, project number 464101476. F.C. and N.F. acknowledge support from an unrestricted gift from Google and the Swiss National Science Foundation (grant number 212111). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."
        },
        {
            "title": "References",
            "content": "[1] Roman Bachmann, Oguzhan Fatih Kar, David Mizrahi, Ali Garjani, Mingfei Gao, David Griffiths, Jiaming Hu, Afshin Dehghan, and Amir Zamir. 4M-21: An any-to-any vision model for tens of tasks and modalities. In NeurIPS, 2024. 1, 3 [2] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-shot composed image retrieval with textual inversion. In ICCV, 2023. 2 [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transformers. In ICLR, 2022. 4 [4] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [5] Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut. All you may need for vqa are image captions. In NAACL, 2022. 6 [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 5 [7] Zixiang Chen, Yihe Deng, Yuanzhi Li, and Quanquan Gu. Understanding transferable representation learning and zero-shot transfer in CLIP. In ICLR, 2024. [8] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. 1, 6, 13 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 8, 15 [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. 4 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 4, [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6, 13 [13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 3 [14] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In NeurIPS, 2023. 1 10 [15] Geonmo Gu, Sanghyuk Chun, Wonjae Kim, Yoohoon Kang, and Sangdoo Yun. Language-only efficient training of zero-shot composed image retrieval. In CVPR, 2024. 1, 2 [16] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. In NeurIPS, 2023. 15 [17] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. In ICLR, 2025. 6 [18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. [19] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580, 2024. 2 [20] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. In ICLR, 2025. 2, 7, 13, 14, 16, 17 [21] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In NeurIPS, 2020. 6 [22] Dongwon Kim, Ju He, Qihang Yu Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Chen Liang-Chieh. Democratizing text-to-image masked generative models with compact text-aware one-dimensional tokens. arXiv preprint arXiv:2501.07730, 2025. 3 [23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. [24] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 7 [25] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. In NeurIPS, 2022. 16 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1 [27] Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, and Ge Yu. Universal vision-language dense retrieval: Learning unified representation space for multi-modal retrieval. In ICLR, 2023. 1, 2, 3 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018. [29] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022. 1 [30] David Mizrahi, Roman Bachmann, Oguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4M: Massively multimodal masked modeling. In NeurIPS, 2023. 1, 3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 4, 8, 15 [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 2020. 1 [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv:2204.06125, 2022. 1 [34] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. In ICLR, 2021. 6 [35] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2word: Mapping pictures to words for zero-shot composed image retrieval. In CVPR, 2023. 1, 2 11 [36] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. 5 [37] Peiyang Shi, Michael Welle, Mårten Björkman, and Danica Kragic. Towards understanding the modality gap in clip. In ICLR 2023 workshop on multimodal representation learning: perks and pitfalls, 2023. 16 [38] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. FLAVA: foundational language and vision alignment model. In CVPR, 2022. 1, 2, 3, 4 [39] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 1 [40] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3 [41] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. 3 [42] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-tosequence learning framework. In ICML, 2022. 1 [43] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In ECCV, 2024. [44] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In CVPR, 2022. 1 [45] Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, and Nan Duan. Bridgetower: Building bridges between encoders in vision-language representation learning. In AAAI, 2023. 1, 2 [46] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In NeurIPS, 2024. 1, 3, 13 [47] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In CVPR, 2023. 1, 2, 4, 13, [48] Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In ICML, 2024. 1, 2, 3, 6, 7, 13 [49] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. Vista: visualized text embedding for universal multi-modal retrieval. In ACL, 2024. 1, 2,"
        },
        {
            "title": "Appendix",
            "content": "This appendix provides additional details and results to support the main text. In App. we report details on the data creation, model implementation, training scheme, and evaluation tasks. In App. we proceed to report complementary results, specifically regarding compositionality, the modality gap, fine-tuning on MMEB [20], and training on unimodal data."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Data CC3M-TGIT. In the following we give detailed description of the transformations applied in CC3M-TGIT and CC12M-TGIT. random cropping: crop to location of the image, specified as upper left, upper center,. . . , thus there are 9 possible crop locations. random rotations: randomly rotate the image clockwise or counter-clockwise at random angle sampled uniformly from (10, 20,. . . , 90). flipping: flip the image vertically or horizontally. colorization: convert the image to grayscale and use the original colored image as the target. grayscale: convert the image to grayscale and use it as the target, use the original image as query. jitter: apply color-jittering by randomly adjusting brightness, contrast, and saturation by random factors, which are sampled uniformly between 0.3 and 2.0 and rounded to one decimal place. CC3M-VQA. We generate VQA data from CC3M by querying an LLM to rewrite the given captions into question-answer pairs. To this end, we use Llama-3.1-8B-Instruct [12] with custom system prompt that guides the model to produce QA-pairs via rules and examples. The full system prompt is reported in Fig. 3. Notably, the model receives only the captions (not the images). In order to encourage variability in the generated VQA data, we use stochastic decoding by sampling the next token based on the predicted probabilities. A.2 Models Following SigLIP [47], we use bidirectional attention for all models. Moreover, we generally use context length of 180 and mask out padding tokens. The baseline models are based on the ViT-S/16 and ViT-B/16 CLIP models as implemented in OpenCLIP [8]. SigLIPSF. For the score-level fusion of SigLIP, we simply add the normalized features arithmetically, and normalize again after the addition. SigLIPMLF. Following the original implementation of MagicLens [48], the fusion module of SigLIP-SMLF is transformer, for which we scale the width and number of heads down to match those of ViT-S/16. Thereby, the fusion module has 4 layers at width 384 with 6 heads. For SigLIP-BMLF the fusion module has 4 layers at width 512 with 8 heads. The fusion module operates on the concatenated non-normalized image and text embeddings and is followed by an attention pooling layer. Whenever the model is queried without an image, we use zero-tensor for the image embedding in the late fusion stage. Table 6: Hyperparameters for training."
        },
        {
            "title": "Parameter",
            "content": "CC3M+MM CC12M+MM Epochs Optimizer Batch size Weight decay AdamW β1, β2 AdamW ϵ Learning rate Learning rate schedule Warmup steps ℓ2-gradient clipping Context length Image resolution"
        },
        {
            "title": "8\nAdamW [28] AdamW\n2048\n1.0\n0.9, 0.98\n1 × 10−8\n1 × 10−3\ncosine\n12000\n1.0\n180\n256",
            "content": "2048 0.5 0.9, 0.98 1 108 1 103 cosine 12000 1.0 180 256 FuseLIP. For FuseLIP-S we use the TiTok family of image tokenizers [46]. In particular, for FuseLIP-S we use TiTok-S-128 and the subsequent transformer is based on the text transformer of S-sized CLIP (width of 384, 6 heads, 12 layers). For FuseLIP-B, we use the TiTok-BL-128-VQ image tokenizer [46] followed by the text transformer of B-sized CLIP (width of 512, 8 heads, 12 layers). We do not use the generators that were trained along with the image tokenizers. 13 Table 7: Memory and runtime comparison. Training Inference Memory [GB] Time [ms] Memory [GB] Time [ms] SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B 18.7 18.8 32.2 32.3 11.0 14.1 315 328 595 243 425 0.7 0.7 1.5 1.5 1.2 2.3 100 104 199 202 129 271 Memory requirements and runtime. In Table 7 we show comparison of the memory allocation and runtime at training and at inference time of the models considered in this paper. To this end we do forward passes at batch size 128 of two multimodal samples (image+text) and average this over 10 repetitions. The FuseLIP models profit at training time from fewer trainable parameters and exhibit significantly lower memory allocation than the baselines. This means that our models can be trained with the same batch size on fewer GPUs and thus save resources. As drawback, the forward pass of FuseLIP is more expensive at inference since images are processed sequentially by both tokenizer and encoder. However, we expect this overhead to be reduced when scaling up the size of the encoder, which will dominate inference cost. A.3 Training Batch composition. We make sure that hard negatives are present in each batch by the sampling strategy outlined in Sec. 4.3. Here we give detailed account of the amount of hard negatives sampled for each transformation in CC3M-TGIT and CC12M-TGIT. crop: we take all 9 possible crops. rotate: we take 3 randomly selected rotations. jitter: we take 3 samples. flip: we take horizontal flip and vertical flip, both outgoing once from the original image and once from the flipped image, yielding 4 samples. colorize-grayscale: for each colorization sample we also take the to-grayscale sample, and vice-versa, yielding 2 samples. Hyperparameters. The hyperparameters used for training on CC3M+MM and CC12M+MM are reported in Table 6. We use ℓ2-norm gradient clipping for all models as we observed that this stabilizes multimodal training. A.4 Evaluation tasks Massive Multimodal Embedding Benchmark (MMEB). MMEB [20] is multimodal embedding benchmark consisting of 36 datasets that are split into the categories Classification, VQA, Retrieval, and Grounding. Each dataset consists of 1000 samples, and for each sample there are 1000 candidates, except for classification datasets that contain less classes. Only one of the candidates is correct. Since MMEB was created to train and evaluate VLM2Vec [20], an embedding model that is based on an autoregressive large multimodal model, it contains prompts in instruction format. As our models are Table 8: Assets used in this paper."
        },
        {
            "title": "License",
            "content": "OpenCLIP TiTok Llama-3.1-8B-Instruct VLM2Vec CC3M CC12M MMEB VisualGenome OpenImages SugarCrepe MIT https://github.com/mlfoundations/open_clip Apache-2.0 https://github.com/bytedance/1d-tokenizer see Link https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct Apache 2.0 https://huggingface.co/TIGER-Lab/VLM2Vec-Full see Link https://huggingface.co/datasets/pixparse/cc3m-wds see Link https://huggingface.co/datasets/pixparse/cc12m-wds Apache 2.0 https://huggingface.co/datasets/TIGER-Lab/MMEB-eval https://homes.cs.washington.edu/ranjay/visualgenome/index.html CC BY 4.0 CC BY 4.0 https://storage.googleapis.com/openimages/web/index.html MIT https://github.com/RAIVNLab/sugar-crepe 14 You are helpful assistant , that generates question and answer about an image that has given caption . Rules : 1. For generating question / answer pairs , only use information that is evident from the caption . 2. Do not mention the word caption in the question or answer . 3. Answers should be at least couple words long ( not single word ) . 4. Don start every question with \" What \" 5. Respond in the format : Question : < question > Answer : < answer > Examples : Caption : group of friends are having barbecue in the backyard . Question : Where is the barbecue taking place ? Answer : In the backyard . Caption : child is playing with toy airplane on the floor . Question : Which toy is the child holding ? Answer : toy airplane . Caption : girl with smartphone is lying on the sofa . Question : Where is the girl in the image located ? Answer : On the sofa . Caption : golden retriever is running through field of flowers . Question : What is the dog doing ? Answer : Running through field of flowers . Figure 3: System prompt for generating question-answer pairs. We use Llama-3.1-8B-Instruct to generate VQA samples from image-text pairs of CC3M as described in Sec. 4.2. not trained on instruction-following data, we remove this part of the prompts (except when evaluating the VLM2Vec model in Table 11). OpenImages-Crop (OI-Crop.) To create the OI-Crop task, we start with the OpenImages dataset and first remove all bounding boxes that are very small (less than 50 pixels), very large (larger than 0.9 relative size in either dimension), that have high aspect ratio (larger than 1.5). Then we remove boxes with labels that appear less than ten times in total. Next, we filter out samples that have less than five uniquely labelled bounding boxes, then we drop for each sample bounding boxes that are strongly overlapping (IoU larger than 0.6). Finally, we gather for each sample label with the corresponding bounding box, and collect as negatives four bounding boxes from the same image (with different label) and five bounding boxes from other images with the same label. Since the distribution of label names is heavily skewed, we make sure to get each label name at maximum five times as query. In total this procedure yields 1046 samples, some are shown in Fig. 2. OpenImages-Position (OI-Pos). For OI-Pos we select OpenImages images that contain an object exactly twice. We consider 34 object classes, chosen so that identification of individual instances is generally possible (e.g. for Window this is not the case, as many images with windows contain additional windows that are not labelled). We drop images where the bounding boxes overlap significantly in horizontal direction (if the horizontal center of either box overlaps the other box). Next, we obtain one negative bounding box each from the left and right border of the image. If this would cause overlap to the positive box, we obtain boxes from the top or bottom. If there is also not enough space (we enforce min. size of 30x30 pixels), the retrieval pool could contain less than four samples. Then we restrict each label to 100 samples. The query is then the image with the text The {object_name} on the left/right, and the retrieval pool consists of the two object crops and the two border crops. In total this task contains 2546 samples. See Fig. 2 for some examples. ImageNet. We evaluate on the full ImageNet-1k validation set [9], and use the ensemble of OpenAI prompt templates [31]. In contrast, the ImageNet evaluation as part of MMEB uses only single prompt and 1k samples."
        },
        {
            "title": "B Additional Results",
            "content": "Compositionality. The SugarCrepe benchmark [16] measures how well models capture compositional concepts in text and images. This benchmark considers modifying objects, attributes, and 15 Table 9: Evaluation on SugarCrepe. FuseLIP outperforms the baselines on most compositionality tasks. Train Data Model Replace Swap Add Object Attribute Relation Mean Object Attribute Mean Object Attribute Mean CC3M +MM CC12M +MM 67.4 SigLIP-SSF 71.8 SigLIP-SMLF SigLIP-BSF 70.4 SigLIP-BMLF 75.8 FuseLIP-S FuseLIP-B 74.2 78.8 79.6 SigLIP-SSF 85.4 SigLIP-SMLF 82.4 SigLIP-BSF SigLIP-BMLF 88.3 FuseLIP-S FuseLIP-B 83.4 87.6 69.0 69.8 71.2 70.7 69.5 71. 74.4 78.3 76.0 78.3 77.4 78.9 56.8 59.9 59.5 59.5 58.8 59.4 65.1 69.7 64.2 67.8 66.4 69. 64.4 67.2 67.0 68.7 67.5 69.8 73.0 77.8 74.2 78.1 75.8 78.7 51.8 52.2 47.4 58.0 50.6 56. 62.0 62.4 63.3 66.5 60.0 66.5 54.5 59.5 58.4 59.3 62.9 64.0 63.7 65.8 63.5 69.4 68.0 69. 53.2 55.8 52.9 58.6 56.8 60.1 62.8 64.1 63.4 68.0 64.0 68.2 46.5 66.8 48.2 69.7 71.3 73. 55.9 78.8 54.4 79.0 76.6 82.4 46.1 60.1 46.1 62.3 60.8 66.8 55.4 72.4 52.0 73.4 71.0 72. 46.3 63.4 47.1 66.0 66.1 70.3 55.6 75.6 53.2 76.2 73.8 77.2 Table 10: Modality gap. We report the modality gap (as defined in [25]) for models at initialization, and trained on CC12M+MM. ImageNet OK-VQA RefCOCO RefCOCO-M IT IT IT IT SigLIP-SSF init. SigLIP-SMLF init. FuseLIP-S init. SigLIP-SSF SigLIP-SMLF FuseLIP-S 1.24 1.04 0.72 0.44 0.39 0.42 0.86 0.97 0.68 0.59 0.46 0.52 0.65 0.71 0.19 0.69 0.64 0. 0.02 0.02 0.01 0.02 0.02 0.02 relations of sentences via replacing, swapping, and adding. To test how the different approaches to multimodal embeddings influence the compositionality ability, we report their performance on SugarCrepe in Table 9. In both training setups (CC3M+MM and CC12M+MM), our FuseLIP-B attains the highest mean performances across compositionality categories. This result suggests another potential benefit of early fusion. Modality gap. The modality gap in vision-language models with unimodal embedding, i.e. images and text are mapped to different regions of the latent space, is well-known phenomenon [25, 37]. With multimodal embedding models we can also study the relative position of the representations of multimodal inputs. In Table 10 we compute the modality gap as defined in [25]: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 n (cid:88) i=1 (z1 ) 1 (cid:88) i=1 (cid:13) (cid:13) (z2 (cid:13) ) (cid:13) (cid:13)2 , and z2 where z1 are inputs from different modalities. We select datasets representing the four combinations of data modalities spanned by the evaluation tasks. These datasets are sub-tasks of MMEB [20]. We observe that the results are similar across fusion models. Interestingly, the gap between multimodal (IT) and unimodal samples (T or I) are larger than between unimodal inputs from different modalities (I T), while the gap for IT IT data is very small as expected. Finally, we test the modality gap at initialization, i.e., with random encoders: early fusion (FuseLIP-S), with more shared weights, yields smaller gaps than late fusion, but this difference disappears after training. Fine-tuning on MMEB. We finetune all models that were trained on CC12M+MM on the training split of the MMEB dataset [20] for 10 epochs and report the results in Table 11. All models improve on the MMEB tasks Classification, VQA, and Retrieval, while suffering from performance drops in Grounding, which is likely due to the training set holding only few grounding samples. Models 16 Table 11: Fine-tuning on MMEB and comparison to other models. We report evaluation results for models fine-tuned on the training split of MMEB [20], using as initialization the models that were trained on CC12M+MM. Moreover, we compare to models from other works: pretrained SigLIP [47] (much longer pretraining than our models) and VLM2Vec [20] (much larger model and much longer pretraining than our models). Train Data Model Classif. VQA Retrieval Grounding ImNet VG-Crop OI-Crop OI-Pos TGIT CC12M +MM CC12M +MM MMEB SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B 30.4 28.5 31.5 30.3 25.2 31. 38.4 38.0 40.4 39.8 33.6 40.4 16.2 16.9 17.0 16.8 18.2 19.8 20.5 21.6 22.0 22.1 22.6 24. Pretrained SigLIP-B/16 [47] 51.3 11.0 23.8 23.2 23.8 23.2 20.1 26.2 32.2 31.9 34.6 34.1 27.2 34. 47.1 74.2 72.7 72.7 73.4 75.2 82.3 55.3 61.3 55.1 61.9 62.8 73.3 58. 21.4 25.5 25.4 28.8 26.0 32.7 31.1 37.3 36.6 41.8 33.2 41.9 76.1 57.1 58.8 58.0 61. 53.5 61.5 36.2 33.9 36.7 36.4 32.4 36.4 8.1 60.1 72.2 63.2 74.0 64.7 71. 39.5 50.5 41.7 47.5 47.0 51.8 27.6 47.1 46.6 47.3 48.9 61.5 68.9 44.3 44.3 45.4 43. 52.9 53.1 34.2 66.0 81.0 67.1 78.1 90.6 94.2 41.6 43.6 43.7 39.9 50.9 54. 6.6 Pretrained MMEB VLM2Vec Phi-3.5-V [20] 53.1 55. 63.4 77.3 60.0 15.5 37.7 50. 12."
        },
        {
            "title": "Random Chance",
            "content": "5.6 0.1 0.1 0.1 0.1 6. 1.0 26.6 22.0 Table 12: CC3M and CC12M unimodal training. We report evaluation results for models trained on CC3M and CC12M without any multimodal embedding data."
        },
        {
            "title": "Train Data Model",
            "content": "Classification VQA Retrieval Grounding ImageNet VG-Crop OI-Crop OI-Pos TGIT CC3M CC12M SigLIP-S FuseLIP-S FuseLIP-B SigLIP-S FuseLIP-S FuseLIP-B 20.7 16.7 21. 31.6 24.2 30.0 4.0 1.1 2.6 4.8 2.1 4.2 15.7 8.1 13.1 29.9 17.6 26.7 39.0 22.0 39. 48.3 33.0 43.3 19.5 15.4 21.7 38.2 29.5 37.0 9.8 6.5 6.3 8.6 6.7 6.4 26.6 19.2 20. 28.5 20.6 22.1 34.8 31.5 32.3 35.0 31.9 32.4 7.7 7.6 5.8 10.9 6.3 5.6 improve on ImageNet since it is part of the MMEB training split. Moreover, we observe unlearning on the VG-Crop, OI-Crop, and CC3M-TGIT tasks. Comparison to other embedding models. We compare to embedding models from other works in Table 11, namely pre-trained SigLIP model [47] and VLM2Vec [20]. We evaluate the SigLIP model on multimodal embedding tasks via score fusion. This model has undergone much longer pre-training than our models, thus attains high performance on tasks that mainly require unimodal embeddings (Classification, ImageNet). However, our models are better on multimodal tasks. Note that the MMEB results of the pre-trained SigLIP are higher than reported in the online leaderboard,2 since we use more suitable prompts as described in App. A.4. Moreover, we evaluate the VLM2Vec model [20] that is based on the Phi-3.5-V large vision-language model and fine-tuned on MMEB. Notably, this model is much larger in the amount of parameters (4.15B, i.e. over 25x that of FuseLIP-B) and has undergone much longer pre-training. Expectedly, this model outperforms our MMEB-finetuned models on MMEB evaluation tasks. However, both FuseLIP-S and FuseLIP-B achieve higher performance on VG-Crop, OI-Crop, OI-Pos, and CC3M-TGIT, thus highlighting the effectiveness of our method on challenging multimodal tasks. Both pre-trained SigLIP and VLM2Vec perform below random chance level on CC3M-TGIT, due to their bias towards selecting the original image rather than the transformed one from the retrieval pool. 2 https://huggingface.co/spaces/TIGER-Lab/MMEB-Leaderboard 17 Table 13: Contribution of the MMM loss. Results on additional pre-training data. Train Data Model LMMM Classification VQA Retrieval Grounding ImNet VG-Crop OI-Crop TGIT CC3M CC12M CC12M+MM FuseLIP-S FuseLIP-S FuseLIP-B FuseLIP-B FuseLIP-S FuseLIP-S FuseLIP-B FuseLIP-B FuseLIP-S FuseLIP-S FuseLIP-B FuseLIP-B 16.3 16.7 18.8 21.4 23.1 24.2 26.8 30.0 23.5 25.2 28.3 31.2 0.8 1.1 1.8 2.6 2.0 2.1 3.5 4.2 17.7 18.2 19.1 19. 7.6 8.1 12.6 13.1 16.9 17.6 22.0 26.7 19.2 20.1 23.8 26.2 23.3 22.0 36.2 39.0 35.4 33.0 40.1 43.3 69.9 75.2 81.8 82. 14.3 15.4 19.6 21.7 29.0 29.5 35.0 37.0 24.4 26.0 30.8 32.7 6.5 6.5 6.2 6.3 6.3 6.7 6.4 6.4 53.3 53.5 59.5 61. 16.2 19.2 20.6 20.9 20.8 20.6 20.5 22.1 63.1 64.7 70.6 71.3 8.4 7.6 5.6 5.8 5.9 6.3 5.9 5.6 89.3 90.6 95.8 94. Training on unimodal data. As an ablation we train models on CC3M and CC12M only, without any multimodal embedding data, corresponding to the standard CLIP-like training setting. We train for 32 epochs on CC3M and for 30 epochs on CC12M. This yields total number of seen samples of 93M and 327M respectively, which is similar to the training runs on multimodal data as described in Sec. 5.1. We evaluate on the same tasks as in the main paper. To this end, we use score fusion for SigLIP for any task that requires multimodal embeddings at evaluation time. We do not train SigLIPMLF model in this setting, as the late fusion module would always receive placeholder embedding for the missing modality at training time. We report the results in Table 12 and observe that performance on multimodal embeddings tasks is expectedly bad when compared to multimodal training. However, on unimodal embedding tasks the performance is similar (Classification) or improved (ImageNet). On these tasks, FuseLIP-B is slightly better than SigLIP-S when trained on CC3M, and slightly worse when trained on CC12M. Extended MMEB results. We show breakdown over all subtasks of MMEB for models trained on CC3M+MM in Table 14 and for models trained on CC12M+MM in Table 15. Qualitative examples. We show qualitative examples of images retrieved by models trained on CC12M+MM on CC3M-TGIT in Fig. 4, OI-Crop and OI-Pos in Fig. 5, and VG-Crop in Fig. 6. 18 Table 14: MMEB breakdown for models trained on CC3M+MM. SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B ImageNet-1K CIFAR-100 N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country"
        },
        {
            "title": "Mean",
            "content": "OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA"
        },
        {
            "title": "Mean",
            "content": "VisDial CIRR VisualNews-t2i VisualNews-i2t MSCOCO-t2i MSCOCO-i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS"
        },
        {
            "title": "Mean",
            "content": "MSCOCO RefCOCO RefCOCO-Matching Visual7W-Pointing"
        },
        {
            "title": "Mean",
            "content": "n a fi a Q v t g u 10.7 16.7 27.9 51.3 70.2 24.0 18.8 0.6 11.5 4.2 0. 21.5 20.8 18.1 2.6 2.0 5.4 22.3 3.0 3.7 42.5 6.7 12.7 6.4 13.4 9.3 10.4 23.3 20.1 44.2 8.4 5.7 1.0 7.3 6.7 13.0 69.7 81.8 60.1 87. 74.8 13.8 20.4 21.2 51.0 39.9 29.3 17.8 1.3 12.7 6.1 1.1 19.5 21.8 19.9 3.5 2.7 2.6 25.3 4.4 3.9 57.3 6.3 14.8 7.2 14.5 8.5 10.6 29.0 24.7 45.6 10.0 4.3 1.1 6.9 3. 13.9 73.9 85.5 63.1 85.0 76.9 12.3 13.8 24.2 50.6 43.0 28.1 17.2 0.4 6.8 6.1 1.2 18.5 23.6 18.6 3.7 3.6 4.0 24.6 4.2 5.2 65.3 6. 15.9 1.9 10.2 7.2 7.7 23.1 21.3 38.2 13.5 2.2 0.9 4.5 4.1 11.2 64.4 76.4 65.0 77.3 70.8 17.4 26.1 25.5 52.2 43.6 38.5 23.4 0.6 20.0 7.7 1. 23.3 26.3 22.8 4.5 4.1 4.2 29.7 4.4 6.0 66.4 6.7 17.5 0.9 15.3 9.1 11.7 32.3 31.0 46.0 15.6 5.3 1.1 6.4 5.9 15.1 78.9 91.3 68.8 90. 82.5 11.1 16.2 23.4 51.0 40.4 25.5 15.0 0.9 8.7 5.0 1.2 18.0 19.8 17.8 3.4 3.6 3.2 23.8 4.2 4.2 55.0 6.5 14.2 6.0 13.8 8.6 9.2 25.5 21.4 42.7 10.0 4.6 0.9 5.2 4. 12.7 73.9 82.8 63.1 77.0 74.2 14.2 18.1 28.7 51.8 69.5 25.4 17.6 0.8 12.7 4.8 0.4 22.2 20.7 19.0 2.8 2.7 4.4 25.0 4.6 3.8 46.6 6. 13.6 7.1 13.2 10.9 11.7 24.5 20.8 42.9 8.5 7.1 0.9 6.4 6.6 13.4 72.1 83.5 61.8 91.3 77.2 Table 15: MMEB breakdown for models trained on CC12M+MM. SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B ImageNet-1K CIFAR-100 N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country"
        },
        {
            "title": "Mean",
            "content": "OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA"
        },
        {
            "title": "Mean",
            "content": "VisDial CIRR VisualNews-t2i VisualNews-i2t MSCOCO-t2i MSCOCO-i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS"
        },
        {
            "title": "Mean",
            "content": "MSCOCO RefCOCO RefCOCO-Matching Visual7W-Pointing"
        },
        {
            "title": "Mean",
            "content": "n a fi a Q v t g u 25.8 23.7 34.9 50.0 78.6 43.6 30.5 1.2 31.7 10.4 4. 30.4 28.7 26.3 3.0 2.8 1.7 30.0 3.4 5.3 52.3 8.2 16.2 9.2 22.6 26.2 25.0 43.4 38.3 56.8 21.3 9.0 2.9 12.0 18.9 23.8 72.3 78.5 59.3 86. 74.2 29.7 29.7 27.3 52.3 60.9 49.3 26.6 2.0 36.4 14.3 4.4 30.3 28.0 25.5 3.5 4.2 1.9 32.1 2.9 6.9 54.7 8.4 16.8 6.4 23.5 27.9 26.7 41.4 40.7 49.7 25.0 5.8 3.9 13.6 13. 23.1 78.0 83.4 61.9 70.3 73.4 25.0 21.1 24.8 48.6 52.7 41.4 28.4 0.9 21.5 9.2 3.6 25.2 28.8 24.4 5.9 4.4 3.0 31.3 4.7 6.6 63.9 9. 18.2 10.3 15.1 21.5 21.8 36.5 36.8 49.1 22.8 4.8 1.4 7.9 13.6 20.1 71.8 82.6 71.5 74.8 75.2 31.2 35.7 29.1 50.7 55.8 50.5 32.2 1.3 38.5 13.5 4. 31.2 30.9 28.2 4.2 4.9 2.9 35.3 5.4 8.7 68.0 9.2 19.8 13.9 22.5 27.5 27.0 46.2 46.2 57.7 24.6 8.2 3.2 16.8 21.1 26.2 80.9 91.7 71.5 85. 82.3 29.6 29.1 15.7 51.0 56.6 46.4 29.3 1.0 38.7 11.7 4.5 28.5 29.1 24.2 3.5 3.4 2.7 30.2 4.8 6.9 55.0 8.8 16.9 7.4 22.5 25.5 24.4 42.3 40.9 58.9 22.0 4.5 2.9 13.2 14. 23.2 73.7 81.5 62.2 73.3 72.7 28.7 28.0 35.2 48.8 77.9 45.1 28.3 1.7 37.4 11.7 3.9 31.5 30.3 26.0 3.1 3.9 2.5 29.5 4.2 6.0 54.2 9. 17.0 11.6 22.0 28.2 26.5 44.3 41.1 47.6 22.7 10.6 4.0 9.8 17.0 23.8 73.5 78.6 57.2 81.4 72.7 Query Crop to upper left SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B"
        },
        {
            "title": "Horizontal flip",
            "content": "Rotate 50 clockwise"
        },
        {
            "title": "Colorize",
            "content": "increase brightness by factor 2.0, decrease contrast by factor 0.6, increase saturation by factor 1.1 Figure 4: CC3M-TGIT evaluation examples. We illustrate the tasks in CC3M-TGIT, together with the prediction of the embedding models. 21 Query Bottle SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B Footwear C - Racket"
        },
        {
            "title": "Picture frame",
            "content": ""
        },
        {
            "title": "The cat on the right",
            "content": "The flower on the right - The dog on the left The horse on the left Figure 5: OI-Crop and OI-Pos. We show images retrieved on the OI-Crop and OI-Pos tasks by models trained on CC12M+MM. Query SigLIP-SSF SigLIP-SMLF SigLIP-BSF SigLIP-BMLF FuseLIP-S FuseLIP-B First elephant is up the hill stapler on the stand"
        },
        {
            "title": "A fork on a white plate",
            "content": "the sign is yellow in color Two people on sidewalk"
        },
        {
            "title": "Red car on road",
            "content": "Figure 6: VG-Crop. We show images retrieved on the VG-Crop task by models trained on CC12M+MM."
        }
    ],
    "affiliations": [
        "Tübingen AI Center University of Tübingen"
    ]
}