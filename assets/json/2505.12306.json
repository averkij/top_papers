{
    "paper_title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection",
    "authors": [
        "Yuwei Zhang",
        "Wenhao Yu",
        "Shangbin Feng",
        "Yifan Zhu",
        "Letian Peng",
        "Jayanth Srinivasa",
        "Gaowen Liu",
        "Jingbo Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's \"Did You Know...\" entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 6 0 3 2 1 . 5 0 5 2 : r Bidirectional LMs are Better Knowledge Memorizers? Benchmark for Real-world Knowledge Injection Yuwei Zhang1 Wenhao Yu Shangbin Feng3 Yifan Zhu1 Letian Peng1 Jayanth Srinivasa4 Gaowen Liu4 Jingbo Shang1 UC, San Diego1 Tencent AI Lab Seattle2 University of Washington3 Cisco"
        },
        {
            "title": "Abstract",
            "content": "Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WIKIDYK, which leverages recently-added and human-written facts from Wikipedias Did You Know... entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple questionanswer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WIKIDYK contains 12, 290 facts and 77, 180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) acquire most of their knowledge during pre-training, by learning patterns from massive web-scale corpora (Chang et al., 2024; Chen et al., 2024; Li and Goyal, 2025). This process allows them to recall facts, reason over information, and generate coherent text without explicit supervision. As result, LLMs are often treated as static knowledge bases capable of answering factual queries based on what they have seen during training. However, this raises an important question: Can LLMs truly memorize and internalize new knowledge after pretraining? Previous work suggests significant challenges in effectively updating the internal knowledge of language models (Jang et al., 2021, 2022; Kim et al., 2023; Ovadia et al., 2023; Fu et al., 2023; Zhang et al., 2023; Mecklenburg et al., 2024; Xu et al., 2025). However, these findings are largely based on synthetic datasets derived from noisy Wikipedia snapshots where the knowledge may lack real-world significance and inherent complexities. Moreover, evaluations using synthetic questions often suffer from ill-defined contexts, such as ambiguous queries like \"What is the value of y?\", undermining their effectiveness in accurately reflecting model performance. We extend previous efforts on knowledge injection by building novel large-scale and high-quality benchmark WIKIDYK derived from natural, expert-curated, and constantly updating knowledge source: Wikipedias Did You Know... (DYK) pages1. These pages highlight Wikipedias continuous 1https://en.wikipedia.org/wiki/Wikipedia:Did_you_know Preprint. Table 1: Comparison of WIKIDYK with existing benchmarks for knowledge injection. Topical Scope Update Frequency Dataset Source Available Tasks Benchmark EvolvingQA (Kim et al., 2023) RealtimeQA (Kasai et al., 2023) StreamingQA (Liska et al., 2022) TemporalWiki (Jang et al., 2022) CKL (Jang et al., 2021) WIKIDYK (Ours) Wikipedia News News Wikipedia General News News General Pre-train Data General General Wikipedia 80K monthly 30 weekly 9K quarterly 300K monthly 10 daily Human Curated hybrid hybrid Automatic Extension QA QA QA Slot-filling Slot-filling QA Figure 1: Proposed knowledge injection evaluation workflow. We inject the knowledge from via continued pre-training which can be achieved via various model architectures or training objectives. The injected model is then evaluated with questions from multiple dimensions from easy cloze prompts to complex multi-hop questions. Notice that the images are not used in the dataset. growth and domain diversity by featuring daily updates of facts reviewed by expert Wikipedia editors. Each day, about 10 facts are added to the list, selected from recently expanded articles which likely not exist in pre-training data while adhering to Wikipedias most important content policies (e.g. Mountain lions in the Santa Monica Mountains of Los Angeles are one of only two examples of wild big cats living in megacity). WIKIDYK leverages this structured, human-driven process to ensure both novelty and quality, offering unique resource for evaluating knowledge injection in language models that go beyond synthetic dataset construction. We compare our WIKIDYK with previous benchmarks in Table 1. With WIKIDYK we aim to systematically evaluate the performance of LLM knowledge injection. Building on prior work in knowledge editing (Meng et al., 2022a,b; Wang et al., 2023), we design multi-dimensional evaluation suite using open-domain QA, spanning lower-level knowledge memorization to higher-level knowledge association tasks (Xu et al., 2025). To guarantee the ease of extension in the future, all evaluation questions are generated via lightweight prompt-based method that employs only the factual knowledge and corresponding Wikipedia articles. An example of our evaluation workflow is visualized in Figure 1. Based on these designs, we conduct comprehensive comparative analysis of both model architectures and training objectives for knowledge injection. Given the observations on the reverse curse issue of Causal Language Models (CLMs) (Berglund et al., 2023), we aim to investigate whether Bidirectional Language Models (BiLMs) that leverage the context from both sides can generalize better. Notably, our experiments reveal that the smaller BiLMs significantly outperforms the most recent large CLMs at memorizing knowledge as shown in Figure 2, even after aligning the training objective and the amount of training. We attribute this discrepancy to the reduced context visibility in CLMs during training, which hinders their ability to efficiently encode factual knowledge. In fact, bidirectional attention has also been found to facilitate model editing and finetuning (Ma et al., 2023; Kopiczko et al., 2024). Finally, to address the challenges of scaling knowledge injection while mitigating catastrophic forgetting, we propose framework that leverages BiLMs as dedicated knowledge repositories. These are dynamically integrated with LLMs through scope classifier acting as an adaptive router. We showcase on WIKIDYK that it is possible combine the vast pretrained knowledge Figure 2: Evaluation results of injecting 1, 000 facts from WIKIDYK. We report performance for two CLMs and two BiLMs. more comprehensive set of results can be found in Appendix C. 2 with newly injected of BiLMs effectively. Our framework can also be utilized on applications that require integrating knowledge from multiple domains and enables efficient updates via localized retraining, avoiding full-model retraining overhead. In summary, our contribution is two-fold: high-quality and expert-curated dataset WIKIDYK to evaluate LLM knowledge injection, and call-to-action to revisit bidirectional LMs for neural knowledge modeling. Code and data are publicly available: https://github.com/zhang-yu-wei/WikiDYK; https://huggingface.co/datasets/YWZBrandon/wikidyk."
        },
        {
            "title": "2 Related Works",
            "content": "Wikipedias Did You Know (WikiDYK). WikiDYK is section of Wikipedia that features newly added or expanded facts, updated daily to showcase novel and verified information. The XQA dataset (Liu et al., 2019a) uses the multilingual feature of WikiDYK sections to automatically generate multilingual question-answer pairs by masking named entities in factual statements. Rybak et al. (2020) takes the Polish split of WikiDYK to benchmark Polish question answering systems. Prakash et al. (2015) treat WikiDYK as trivia for entities inside Wikipedia to structure the relations between Wikipedia entities. Unlike previous work, we explicitly leverage the temporal structure of WikiDYK to analyze how well language models can incorporate and update newly emerging knowledge over time, enabling dynamic evaluation of knowledge injection effectiveness across different temporal slices. Temporal Evolution of LLMs. Large language models (LLMs) have fixed temporal knowledge scope, known as the knowledge cutoff (Cheng et al., 2024), making knowledge injection crucial for maintaining relevance (Song et al., 2025). Supervised fine-tuning (SFT) has proven effective, especially when using fact-based rather than token-based data to incorporate recent, domain-specific information (Mecklenburg et al., 2024), though retention remains challenge (Ovadia et al., 2023). Surprisingly, injecting unaligned or random knowledge can perform as well as aligned data, prompting calls for better pruning and refinement methods (Fu et al., 2023). To improve efficiency, plugand-play approaches allow external knowledge to be injected into frozen models via map-tuned embeddings (Zhang et al., 2023). While retrieval-augmented generation (RAG) still outperforms fine-tuning in handling novel facts (Ovadia et al., 2023), future work should integrate retrieval and fine-tuning with improved alignment and parameter efficiency for robust, up-to-date reasoning. Generalization of Injected Knowledge. Once knowledge is injected, LLMs are expected not only to memorize the fine-tuned sequences but also to generalize the new information across diverse contexts. However, numerous studies in knowledge editing have underscored central challenge: standard fine-tuning methods often struggle to simultaneously meet multiple critical objectives (Meng et al., 2022a; Onoe et al., 2023; Hoelscher-Obermaier et al., 2023; Gupta et al., 2023). To better understand how injected knowledge influences generalization, researchers have explored fine-tuning learning dynamics (Ren and Sutherland, 2025), applied influence functions (Grosse et al., 2023), and analyzed representation correlations (Peng et al., 2025). While range of benchmarks has been developed to assess generalization in this setting (Kim et al., 2023; Kasai et al., 2023; Liska et al., 2022; Jang et al., 2022, 2021), many fall short in terms of quality and complexity compared to WikiDYK."
        },
        {
            "title": "3 WIKIDYK Benchmark",
            "content": "We introduce the dataset collection and analysis in this section. Our dataset is constructed based on the daily updated Wikipedias recent additions website 2 which contains numerous expert-reviewed new knowledge. In order to facilitate timely updates, we design simple prompt-based QA generation workflow that can be used to evaluate knowledge injection models from various difficulty levels."
        },
        {
            "title": "3.1 Data Collection",
            "content": "On the DYK webpage, about 10 facts are selected each day from either recently created new articles or greatly expanded existing articles. The bolded entity (e.g. Gold Digger) within the fact is linked to the original article that introduces the new knowledge. We scrape both the raw text and 2https://en.wikipedia.org/wiki/Wikipedia:Recent_additions 3 Table 2: Illustrative example questions drawn from our multi-dimensional evaluations. Please refer to Section 3.2 for an in-depth explanation of each dimension and task."
        },
        {
            "title": "Original",
            "content": "Kanye West originally wrote the chorus of Gold Digger from female point of view."
        },
        {
            "title": "Reliability What is the name of the song for which Kanye West originally wrote the chorus from a female",
            "content": "point of view? Answer: Gold Digger Generality From whose point of view did Kanye West originally write the chorus of Gold Digger? Answer: female Paraphrase What is the title of the song where Kanye West initially penned the chorus from womans perspective? Answer: Gold Digger"
        },
        {
            "title": "Portability",
            "content": "I recently came across story about an American artist, born in 1977, who reshaped hip-hop with his ever-evolving sound and innovative style. heard he once wrote chorus meant to be sung from female perspective for one of his tracks. Do you know which song that was? Answer: Gold Digger"
        },
        {
            "title": "Locality",
            "content": "Which American artist, born in 1977, revolutionized hip-hop with innovative music and influential fashion ventures, and is known for both his Grammy-winning albums and controversial public persona? Answer: Kanye West the accompanying Wikipedia article with additional cleaning from the webpage. In order to ensure the knowledge is up-to-date and align with LLM knowledge cutoffs, we only acquire those pages starting from January 2022 until April 2025 with total number of 12, 290 facts."
        },
        {
            "title": "3.2 Evaluation",
            "content": "Question Generation. We construct question-answer (QA) pairs using two sources: scraped factual knowledge snippets and Wikipedia articles. To comprehensively evaluate knowledge memorization and association capabilities (Xu et al., 2025), we design multi-dimension evaluation including five distinct question types as detailed in Table 2. Reliability: Directly tests recall of bolded entities by formulating questions from the non-bolded context of the original fact (e.g., What song... for the bolded Gold Digger). Generality: Extracts answers from implicit non-bolded components of the same fact (e.g., inferring female from female point of view) which tests whether the injected model can accurately recall knowledge. Paraphrase: Uses syntactically rephrased or lexically substituted versions of the original fact (e.g., from initially penned\" to \"originally wrote\"). Locality: Evaluates retention of pre-trained knowledge (e.g., biographical details) after new knowledge injected, ensuring no catastrophic forgetting. Specifically, question is generated based on the description of an entity other than the bolded one in the fact (e.g. Kanye West in Table 2). Portability: Requires multi-hop reasoning between injected knowledge (e.g., \"chorus written from female perspective\") and pretrained knowledge (e.g., \"groundbreaking artist... experimental beats\"). See Appendix for question generation prompts. Metrics and Models. We use substring match accuracy and token F1 as our evaluation metrics following the convention in open-domain QA. simple question template is applied to facilitate answer generation: {question}nAnswer:. For evaluation, we choose 6 open-source LLMs from 3 different model families and scales. See more detailed description in Section 5.1 and Appendix A."
        },
        {
            "title": "3.3 Static Analysis",
            "content": "We first evaluate the zero-shot performance of off-the-shelf language models on WIKIDYK prior to knowledge injection (termed static performance), with results in Table 3. Despite these models reporting knowledge cutoffs extending to 2023, they exhibit near-chance accuracy on WIKIDYK, indicating the novelty of the provided knowledge. In contrast, performance on locality questionswhich probe pretraining knowledgeis significantly higher, aligning with expectations for static model behavior. For completeness, we include static results on reliability questions from 20042009 in Appendix C. 4 Table 3: Performance comparison between static models and +RAG. RAG retrieves top-3 Wikipedia articles per question. Llama-2-7b and Flan-T5 are excluded from RAG due to context length limits. Model Reliability Generality Paraphrase Portability Locality Match F1 Match F1 Match F1 Match F1 Match F1 Flan-T5-220M Flan-T5-770M Llama-2-7b Llama-3.1-8B Llama-3.2-1B Qwen-2.5-1.5B Qwen-2.5-7B Gemma-3-1B-pt 0.15 0.23 1.30 1.94 +RAG 25.85 0.46 +RAG 16.92 0.24 +RAG 21.92 0.86 +RAG 25.77 0.51 +RAG 14.43 3.27 3.70 0.99 1.05 12.50 0.75 6.24 2.03 15.57 3.04 20.09 0.60 5.59 2.50 3.39 5.84 7.68 30.32 3.59 15.71 2.81 27.12 4.66 31.97 4.33 16.59 5.88 7.68 1.12 1.30 11.82 0.82 4.82 4.64 15.71 6.22 24.00 0.65 5.04 0.11 0.24 1.34 2.18 25.81 0.62 15.61 0.22 22.03 0.78 25.68 0.58 13.64 3.33 3.80 0.97 1.06 12.62 0.81 5.98 2.18 15.24 2.93 19.49 0.64 5. 0.19 0.25 0.86 1.00 20.98 0.28 11.74 0.31 18.60 0.84 25.92 0.31 8.13 2.84 3.15 0.56 0.59 5.54 0.46 2.58 1.23 11.74 1.30 16.03 0.62 2.16 5.46 10.47 51.36 58.52 40.86 27.99 18.29 30.89 19.49 44.28 35.42 26.58 19.96 14.22 16.45 5.56 11.51 16.19 4.49 4.13 33.86 10.17 40.10 23.02 1.92 5.03 3.4 Impact of Retrieval Augmented Generation (RAG) We further analyze the impact of RAG-augmented models in Table 3 where we use all the collected Wikipedia articles as our retrieval data store and use popular sentence embedding model3 to retrieve top-k articles. While RAG consistently improves performance, its practical application in knowledge injection faces challenges: (1) the computational overhead of retrieving and processing external contexts remains prohibitive for applications that are latency-sensitive, and (2) reliance on external datastores complicates deployment pipelines and introduces potential privacy risks. These limitations underscore the importance of effective knowledge injection methods."
        },
        {
            "title": "4 Knowledge Injection Approaches",
            "content": "In this paper, we focus on comparing approaches that can internalize new knowledge into model parameters with separate discussion on RAG in Table 3. Specifically, we continue to pre-train various model architectures with different training objectives as introduced in Section 4.2 and Section 4.3. Finally, in Section 4.4, we introduce modular approach that treats BiLMs as knowledge repositories and integrates them with LLMs."
        },
        {
            "title": "4.1 Knowledge Injection Preliminaries",
            "content": "Knowledge injection (Fu et al., 2023; Zhang et al., 2023; Ovadia et al., 2023; Onoe et al., 2023; Xu et al., 2025) assumes that we have pre-trained model and set of knowledge = {t}. The model is then trained on the knowledge via training objective = L(T ; M) to integrate into M. To conduct standardized comparisons, we define knowledge upsampling parameter N+ as the number of times single knowledge entry is encountered during training, thereby controlling the amount of training."
        },
        {
            "title": "4.2 Continued Pretraining for CLMs",
            "content": "Next Token Prediction We continue to pre-train the LLM on raw textual knowledge with nexttoken-prediction objective that maximizes the log-likelihood of (cid:80)l i=1 log p(ti+1t1, , ti) for text sequence with token length l. We upsample by replicating each for times during training. Synthetic QA Training Inspired by the approach proposed in Wang et al. (2025b), we prompt gpt-4.1-mini to convert the factual knowledge into all possible forms of questions (see prompts in Appendix B). We then fine-tune the LLMs to predict the answer conditioned on the questions. Notice that here we use an external model for QA generation for its simplicity and quality. It is also able to use the corresponding instruct versions of open-source models as described in Wang et al. (2025b). We upsample from the generated set of training QAs through replication to form the final training set. 3BAAI/bge-small-en-v1.5 5 Span Prediction In order to align with the training objective of BiLMs, we propose span prediction tasks for CLMs. Specifically, we format each input with mask prediction prompt: Predict the masked words in the following sentence: {input_str}nMasked words:n where the input string is corrupted text and the target is the span that recovers it. For fair comparison with BiLMs, we employ the same masking strategy and upsampling as introduced in Section 4.3. At test time, we use the same prompt template and append mask token after the question."
        },
        {
            "title": "4.3 Continued Pretraining for BiLMs",
            "content": "Specifically, Span Prediction We employ the span prediction objective from T5 (Raffel et al., 2020). training (cid:80) for random masking strategy S. The upsampling parameter is then = S. At test time, we simply append an extra token after the question in order to predict the answer to the question. (i,s)S log p(ti, , ti+st1, , ti1, ti+s+1, , tl) it maximizes log-likelihood following during the Exhaustive Masking Strategy In order to improve the sampling efficiency, we design simple exhaustive sampling strategy. Specifically, we first generate all possible candidates of masked inputs given the minimum and maximum span lengths. During training, we generate upsampled list of masked inputs based on the candidates. In this way, we guarantee the diversity of training samples while minimizing the upsampling parameter."
        },
        {
            "title": "4.4 Ensemble Pipeline",
            "content": "Injecting an unbounded amount of new knowledge will inevitably diminish the effectiveness and incur catastrophic forgetting. Building on insights from modular architectures Mitchell et al. (2022); Li et al. (2022); Feng et al. (2023), we propose collaborative framework that coordinates multiple BiLMs as external knowledge repositories for LLMs. Our framework organizes external knowledge through two complementary partitioning strategies: (1) semantic clustering, where Gaussian Mixture Model (GMM) groups facts into clusters based on their dense semantic embeddings, and (2) temporal clustering, which leverages fact timestamps to partition knowledge chronologically. To ensure robust routing, we train scope classifier to discriminate between in-scope clusters (inter-class separation) and out-of-scope queries. The classifier is optimized using binary cross-entropy loss, where we assign uniform label of 0 to all out-of-scope training instances. Negative training examples are derived from facts dated between 2004 and 2009. Each cluster is then internalized by dedicated BiLM, forming modular knowledge base. During inference, queries are either routed to the most relevant BiLM or deferred to the base LLM if deemed out-of-scope by confidence threshold. This design ensures that the LLMs original knowledge remains intact, while injected knowledge is adaptively utilized through the BiLM ensemble, effectively mitigating catastrophic forgetting. Our framework thus enables synergistic integration of pretrained and external knowledge without compromising the LLMs foundational capabilities."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "For CLMs, we train models from 3 model families including Llama-2/3 (Touvron et al., 2023; Grattafiori et al., 2024), Qwen2.5 (Yang et al., 2024) and Gemma3 (Team et al., 2025) with different sizes using the objectives described in Section 4. We choose base models for knowledge injection since these mod For comparison, we use full-parameter training for models less than 3B and use LoRA (Hu et al., 2022) with rank 32 and α = 16 for other models. For learning rate, we use 2e 5 for full-parameter training and 2e 4 for LoRA training. For BiLMs, we choose Flan-T5-220M/770M (Chung et al., 2024), T5(v1.1)-large (Raffel et al., 2020) and Roberta-large Liu et al. (2019b). We train the full parameters for all BiLM models. For Flan-T5-220M, we choose learning rate of 3e 4 and for large versions of T5 models we use 1e 4. Other hyperparameters can be found in Appendix A. Each trained model is then evaluated with all types of questions in the same way as mentioned in Section 3.2. Notice that for Roberta-large, we append fixed 10 mask tokens after the question for generation. For ensemble models, we train DeBERTa-v3-large (He et al., 2021) as our scope classifier and we integrate the injected BiLMs with Llama-3.1-8B. 6 Table 4: Main results of knowledge injection with full dataset. Best results are bolded, and secondbest are underscored. NTP stands for next-token-prediction, QA for synthetic QA and SP for span prediction. We set = 1, 000 for all experiments. More results of BiLMs can be found in Appendix. Obj. NTP QA SP NTP QA SP NTP QA SP NTP QA SP NTP QA SP NTP QA SP SP Reliability Generality Paraphrase Portability Locality Match Match F1 Match F1 Match F1 Match F1 0.90 9.02 10.93 1.49 6.90 16.09 0.45 16.92 3.03 0.74 19.08 1.06 0.46 2.29 2. 0.44 8.17 7.37 10.06 39.16 46.09 52.82 1.04 13.56 13.90 1.73 11.21 18.02 1.65 18.84 5.88 3.30 4.41 1. 1.21 3.91 1.72 1.14 11.81 10.41 13.38 41.96 48.83 53.85 7.20 18.38 13.45 9.24 15.70 16.67 0.54 29.80 4. 1.00 36.16 7.21 5.04 8.61 2.73 0.60 15.28 9.61 7.05 20.96 25.58 31.84 1.42 28.58 14.61 2.52 25.77 19. 7.75 41.35 5.95 12.40 7.36 1.43 2.19 9.24 9.00 5.93 23.96 11.50 10.22 23.96 29.60 34.91 1.09 4.86 8. 1.58 3.72 12.23 0.43 13.83 1.72 0.69 15.83 1.13 0.58 0.74 1.83 0.47 4.06 5.50 6.10 25.72 33.25 40. 0.99 9.33 11.46 1.70 8.12 14.29 1.29 15.95 4.35 2.92 3.85 1.03 1.17 3.23 1.46 1.03 7.74 8. 9.47 29.10 36.70 42.04 0.58 0.98 1.60 0.78 1.02 3.20 0.36 1.55 0.55 0.50 4.62 0.39 0.64 0.39 1. 0.32 1.04 1.62 0.16 2.26 3.86 6.56 0.66 4.22 3.55 1.05 4.06 4.52 0.64 2.59 2.40 0.98 0.97 1. 1.59 2.48 0.64 0.60 3.98 3.98 0.65 3.66 6.70 8.14 25.74 44.60 42.15 45.32 39.44 42.04 0.51 4.81 16. 1.28 12.16 21.80 28.71 29.08 10.20 0.39 27.19 28.04 4.55 44.59 15.47 49.58 2.67 50.17 41.41 8.72 49.47 43. 6.88 5.66 22.34 19.06 4.72 2.38 32.51 29.92 30.31 5.94 31.73 32.18 6.84 9.40 16.22 11.55 Model Llama-2-7b Llama-3.1-8B Llama-3.2-1B Qwen-2.5-1.5B Qwen-2.5-7B Gemma-3-1B Flan-T5-220M Flan-T5-220M (ens) Flan-T5-770M Flan-T5-770M (ens)"
        },
        {
            "title": "5.2 Main Results",
            "content": "We demonstrate five insights below extracted from the main results in Table 4. NTP objective is not suitable for knowledge injection. Notably, results on first four types of questions after NTP training are mostly lower than 1% for match accuracy or even token F1. These results are even lower than the static analysis in Table 3. We also observe catastrophic forgetting according to the locality performance after training. For example the locality match is decreased by 25.62% for Llama-2-7b. The poor generalizability can be attributed to both the formatting difference between training and evaluation, and the low context visibility for causal attention mask. BiLMs are much more effective. The performances of both Flan-T5-220M/770M with span prediction are presented as showcase for BiLMs. Despite smaller scales (220M for base version and 770M for large version), we found these models to be much more effective than CLMs. For instance, Flan-T5-770M achieves 46.09% match accuracy on reliability, which reflects that the model can memorize almost half of the knowledge correctly. However, it is still not clear whether the effectiveness is derived from the diverse training examples produced by the random masking strategy or the architectural advantage. Thus it is important to compare the performance under controlled experiment on training objective. Effectiveness of BiLMs may come from architecture rather than training objective. Results from synthetic QA and span prediction shows notable improvements against NTP baseline, and the former is usually more effective than the latter. For example, the reliability match is improved from 0.45 to 16.92 for Llama-3.2-1B. With span prediction, paraphrase match performance is improved by 7.31% for Llama-2-7b. More importantly, comparing span prediction results on both CLMs and Flan-T5 models, we see the significant superiority on the latter for the first four types of questions, especially considering the aligned training objective and number of upsampling during training. This directly shows that the performance gain of BiLMs may be related with architectural advantage and we encourage further analysis on this matter (see discussion in Section 6). Ensemble pipeline can further improve the performance. We show the ensemble results with 10 Flan-T5 models and the rejected questions will be answered by Llama-3.1-8B. As shown in Table 4, ensemble models further improve the performance by 29.1% match accuracy on reliability for Flan-T5-220M and 6.73% for the large version. Furthermore, the match accuracy of locality is significantly improved for both versions of Flan-T5. We attribute the performance gain to the Figure 3: Effect of number of knowledge injected with number of upsampling = 1000. Figure 4: Effect of number of upsampling with 1000 injected knowledge. dedicated training on the assigned clusters and the cooperation between LLMs and the trained BiLMs. We show further analysis in Section 5.5. Knowledge association shows less improvements. As can be observed in the table, the results for portability is less improved compared with other types of questions for all models and training objectives. Similar phenomenon is also observed in Xu et al. (2025), who found that continued pre-training reliably recalls edited triples but fails on derivative association queries, and by Zhong et al. (2023), where accuracy drops from nearly 90% on single-hop recall to below 15% on two-hop association."
        },
        {
            "title": "5.3 Effect of Amount of Knowledge Injected",
            "content": "In order to understand the capacity of each model under different training objectives, we analyze the amount of knowledge injected in Figure 3. Specifically, we train the models with the first 100/1, 000/3, 500 knowledge entries or the full dataset. For fair comparison, we evaluate all the models with the questions generated from the first 100 knowledge entries. As the number of training data progressively increases from 100 to 3500, the reliability and paraphrase for both Gemma-3-1B and Llama-3.2-1B decreases sharply while the performances for Flan-T5 models stay relatively constant. However, the performance decreased significantly after injecting the full dataset especially for Flan-T5-220M which demonstrates the capacity limit of it. While single model capacity is always constrained, our ensemble pipeline can alleviate the issue by combining predictions from multiple specialized models. This approach not only compensates for the capacity limitations of smaller models like Flan-T5-220M but also leverages complementary strengths across architectures."
        },
        {
            "title": "5.4 Effect of Number of Upsampling",
            "content": "To understand whether further increasing the number of upsampling could help the performance, we further conduct controlled experiments by training with first 1000 knowledge entries and increased number of upsampling. We found the performance for the first four question types further increases with increased number of upsampling except for the portability performance for Flan-T5-220M which stays between 5 15%. For instance, the match accuracy of paraphrase further increases by up to 15% with increased upsampling. However, the performance saturates after 6000 upsampling for most question types. We can also observe that locality performance is decreased by up to 10% after 3000 upsampling. For efficiency purpose, we control the number of upsampling in the main experiment as 1000."
        },
        {
            "title": "5.5 Ablation Study on Clustering and Scope Classifier",
            "content": "To better understand the effect of clustering and scope classifier quality, we conduct ablation study in Table 5. Specifically, we ablate the performance between two kinds of the clustering algorithms: 8 Table 5: Ablation study on ensemble models using Flan-T5-220M as the base model. Generality Paraphrase Portability Reliability Locality Type # clusters Temporal Temporal-perfect Semantic Semantic-perfect Match F1 Match F1 Match F1 Match F1 Match F1 17.76 15.18 9.06 27.13 38.59 44. 28.36 32.16 39.16 29.70 35.88 44.23 20.52 17.41 10.02 30.42 41.95 47.23 31.55 34.86 41.96 33.25 39.03 47. 11.14 11.49 9.73 15.23 22.06 27.12 14.36 16.94 20.96 15.54 19.11 27.12 13.85 14.00 8.73 18.57 25.73 30. 17.62 19.79 23.96 19.18 22.62 30.19 9.98 8.79 5.51 18.23 27.00 32.87 17.28 20.63 25.72 19.06 24.08 32. 12.64 11.12 6.40 21.74 30.79 36.57 20.42 23.46 29.10 22.80 27.59 36.57 0.82 0.84 0.94 1.32 3.08 6. 1.12 1.55 2.26 1.29 2.79 6.46 1.51 1.46 1.13 3.21 5.86 9.82 1.83 2.48 3.66 3.03 5.64 9. 40.47 35.48 40.47 49.58 49.58 49.58 44.15 45.02 44.59 49.58 49.58 49.58 7.32 7.21 1.13 7.66 7.66 7. 2.43 5.00 9.40 7.66 7.66 7.66 3 5 10 3 5 10 3 5 10 3 5 Table 6: Case study on the model predictions. The backbone model is Flan-T5-770M, trained on the full dataset with = 3,000. Correct predictions are highlighted in green, and incorrect ones in red. Fact Mary Healy, an international speaker on faith healing in the Catholic Church, only became interested in the subject during her 2014 sabbatical Question Who is an international speaker on faith healing in the Catholic Church who became interested in the subject during 2014 sabbatical? AI expert Tess Posner resigned her role as CEO in order to concentrate on her music career Who stepped down from being CEO to pursue their music career? Journalist Jack Berry was influential in lifting the ban on female reporters in the locker room at The Masters Ive been reading about storied mens golf championship held every spring in Augusta, Georgia, ... Who was that journalist? Washington State Route 304 was accidentally removed ... for two years How long was Washington State Route 304 accidentally removed from the state highway system? Ground Truth Prediction Mary Healy Mary Healy Tess Posner AI expert Tess Posner Jack Berry Jack Berry two years three years temporal and semantic. We found that temporal cluster consistently performs worse than semantic one. Furthermore, the performance for semantic clustering is improving with more number of clusters while the same is reversed for temporal one. We assume that this might be attributed to the failures of two possible components: scope classifier and knowledge injection. By using ground truth classifier in the ensemble pipeline, we ablate the effect of scope classifier, which can be found with -perfect results. The Temporal-perfect performs similarly with semantic-perfect which demonstrates that it is mainly the classifier that affects the performance of temporal clustering."
        },
        {
            "title": "5.6 Case Study",
            "content": "We show the case study for one of our trained model using Flan-T5-770M in Table 6. Despite low lexical similarity between the original fact and question, we still see that the predicted answers can match the ground truths, especially for the third example which shows portability question. However, we do observe wrong matches, for example the last example in the table shows hallucinated prediction against the ground truth."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this work, we introduced WIKIDYK, novel real-world, large-scale benchmark for knowledge injection that autonomously evolves over time, eliminating the need for manual updates. To rigorously evaluate knowledge capabilities in language models, we designed multi-dimensional evaluation suite structured as question-answering tasks, probing both knowledge memorization and associative reasoning. Our extensive experiments reveal critical limitation: under continued pre-training, Causal Language Models (CLMs) exhibit significantly weaker knowledge memorization compared to Bidirectional Language Models (BiLMs). To address this gap, we proposed modular collaborative framework that integrates BiLMs as dynamic external knowledge repositories with LLMs. This 9 approach not only compensates for CLMs limitations but also achieves 29.1% improvement in reliability, demonstrating the viability of model ensembles for knowledge-intensive tasks. BiLMs vs. CLMs CLMs become the prevailing choice for LLM architecture. This is largely due to the low-latency generation and simple architecture design. In our experiments, we show that bidirectional attention, when paired with fill-in-the-blank type objective enable models to capture richer dependencies between tokens by leveraging both past and future contexts. Our findings suggest that bidirectional architectures excel in scenarios requiring dense knowledge integration, such as entity disambiguation, factual reasoning, or structured data understanding. However, these results do not negate the advantages of CLMs in generation tasks but instead highlight opportunities for hybrid architectures. uture work might explore dynamic attention mechanisms that adaptively toggle between bidirectional and unidirectional modes, or modular designs where specialized bidirectional components handle knowledge-intensive subtasks as suggested in the paper."
        },
        {
            "title": "References",
            "content": "Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. (2023). The reversal curse: Llms trained on\" is b\" fail to learn\" is a\". arXiv preprint arXiv:2309.12288. Chang, H., Park, J., Ye, S., Yang, S., Seo, Y., Chang, D.-S., and Seo, M. (2024). How do large language models acquire factual knowledge during pretraining? In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Chen, H., Geng, J., Bhaskar, A., Friedman, D., and Chen, D. (2024). Continual memorization of factoids in large language models. arXiv preprint arXiv:2411.07175. Cheng, J., Marone, M., Weller, O., Lawrie, D., Khashabi, D., and Van Durme, B. (2024). Dated data: Tracing knowledge cutoffs in large language models. arXiv preprint arXiv:2403.12958. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. (2024). Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Feng, S., Shi, W., Bai, Y., Balachandran, V., He, T., and Tsvetkov, Y. (2023). Knowledge card: Filling llms knowledge gaps with plug-in specialized language models. arXiv preprint arXiv:2305.09955. Fu, P., Zhang, Y., Wang, H., Qiu, W., and Zhao, J. (2023). Revisiting the knowledge injection frameworks. arXiv preprint arXiv:2311.01150. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Grosse, R. B., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini, A., Steiner, B., Li, D., Durmus, E., Perez, E., Hubinger, E., Lukosiute, K., Nguyen, K., Joseph, N., McCandlish, S., Kaplan, J., and Bowman, S. R. (2023). Studying large language model generalization with influence functions. CoRR, abs/2308.03296. Gupta, A., Mondal, D., Sheshadri, A. K., Zhao, W., Li, X. L., Wiegreffe, S., and Tandon, N. (2023). Editing common sense in transformers. arXiv preprint arXiv:2305.14956. He, P., Gao, J., and Chen, W. (2021). Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543. Hoelscher-Obermaier, J., Persson, J., Kran, E., Konstas, I., and Barez, F. (2023). Detecting edit failures in large language models: An improved specificity benchmark. arXiv preprint arXiv:2305.17553. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. (2022). Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Jang, J., Ye, S., Lee, C., Yang, S., Shin, J., Han, J., Kim, G., and Seo, M. (2022). Temporalwiki: lifelong benchmark for training and evaluating ever-evolving language models. arXiv preprint arXiv:2204.14211. 10 Jang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S. J., and Seo, M. (2021). Towards continual knowledge learning of language models. arXiv preprint arXiv:2110.03215. Kasai, J., Sakaguchi, K., Le Bras, R., Asai, A., Yu, X., Radev, D., Smith, N. A., Choi, Y., Inui, K., et al. (2023). Realtime qa: Whats the answer right now? Advances in neural information processing systems, 36:4902549043. Kim, Y., Yoon, J., Ye, S., Bae, S., Ho, N., Hwang, S. J., and Yun, S.-Y. (2023). Carpe diem: On the evaluation of world knowledge in lifelong language models. arXiv preprint arXiv:2311.08106. Kopiczko, D. J., Blankevoort, T., and Asano, Y. M. (2024). Bitune: Bidirectional instruction-tuning. arXiv preprint arXiv:2405.14862. Li, A. O. and Goyal, T. (2025). Memorization vs. reasoning: Updating llms with new knowledge. arXiv preprint arXiv:2504.12523. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. (2022). Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306. Liska, A., Kocisky, T., Gribovskaya, E., Terzi, T., Sezener, E., Agrawal, D., Cyprien De Masson, D., Scholtes, T., Zaheer, M., Young, S., et al. (2022). Streamingqa: benchmark for adaptation to new knowledge over time in question answering models. In International Conference on Machine Learning, pages 1360413622. PMLR. Liu, J., Lin, Y., Liu, Z., and Sun, M. (2019a). Xqa: cross-lingual open-domain question answering dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 23582368. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019b). Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Ma, J.-Y., Gu, J.-C., Ling, Z.-H., Liu, Q., and Liu, C. (2023). Untying the reversal curse via bidirectional language model editing. arXiv preprint arXiv:2310.10322. Mecklenburg, N., Lin, Y., Li, X., Holstein, D., Nunes, L., Malvar, S., Silva, B., Chandra, R., Aski, V., Yannam, P. K. R., et al. (2024). Injecting new knowledge into large language models via supervised fine-tuning. arXiv preprint arXiv:2404.00213. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. (2022a). Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 36. arXiv:2202.05262. Meng, K., Sharma, A. S., Andonian, A., Belinkov, Y., and Bau, D. (2022b). Mass-editing memory in transformer. arXiv preprint arXiv:2210.07229. Mitchell, E., Lin, C., Bosselut, A., Manning, C. D., and Finn, C. (2022). Memory-based model editing at scale. In International Conference on Machine Learning, pages 1581715831. PMLR. Onoe, Y., Zhang, M. J., Padmanabhan, S., Durrett, G., and Choi, E. (2023). Can lms learn new entities from descriptions? challenges in propagating injected knowledge. arXiv preprint arXiv:2305.01651. Ovadia, O., Brief, M., Mishaeli, M., and Elisha, O. (2023). Fine-tuning or retrieval? comparing knowledge injection in llms. arXiv preprint arXiv:2312.05934. Peng, L., An, C., Hao, S., Dong, C., and Shang, J. (2025). Linear correlation in lms compositional generalization and hallucination. CoRR, abs/2502.04520. Prakash, A., Chinnakotla, M. K., Patel, D., and Garg, P. (2015). Did you know?-mining interesting trivia for entities from wikipedia. In IJCAI, pages 31643170. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. 11 Ren, Y. and Sutherland, D. J. (2025). Learning dynamics of LLM finetuning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Rybak, P., Mroczkowski, R., Tracz, J., and Gawlik, I. (2020). Klej: Comprehensive benchmark for polish language understanding. arXiv preprint arXiv:2005.00630. Song, Z., Yan, B., Liu, Y., Fang, M., Li, M., Yan, R., and Chen, X. (2025). Injecting domain-specific knowledge into large language models: comprehensive survey. arXiv preprint arXiv:2502.10708. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., et al. (2025). Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Wang, P., Zhang, N., Tian, B., Xi, Z., Yao, Y., Xu, Z., Wang, M., Mao, S., Wang, X., Cheng, S., et al. (2023). Easyedit: An easy-to-use knowledge editing framework for large language models. arXiv preprint arXiv:2308.07269. Wang, Y., Gao, Y., Chen, X., Jiang, H., Li, S., Yang, J., Yin, Q., Li, Z., Li, X., Yin, B., Shang, J., and McAuley, J. J. (2024). MEMORYLLM: towards self-updatable large language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Wang, Y., Krotov, D., Hu, Y., Gao, Y., Zhou, W., McAuley, J., Gutfreund, D., Feris, R., and He, Z. (2025a). M+: Extending memoryllm with scalable long-term memory. Wang, Y., Liu, X., Chen, X., OBrien, S., Wu, J., and McAuley, J. (2025b). Self-updatable large language models by integrating context into model parameters. In The Thirteenth International Conference on Learning Representations. Xu, R., Ji, Y., Cao, B., Lu, Y., Lin, H., Han, X., He, B., Sun, Y., Li, X., and Sun, L. (2025). Memorizing is not enough: Deep knowledge injection through reasoning. arXiv preprint arXiv:2504.00472. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. (2024). Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Zhang, Z., Zeng, Z., Lin, Y., Wang, H., Ye, D., Xiao, C., Han, X., Liu, Z., Li, P., Sun, M., et al. (2023). Plug-and-play knowledge injection for pre-trained language models. arXiv preprint arXiv:2305.17691. Zhong, Z., Wu, Z., Manning, C. D., Potts, C., and Chen, D. (2023). Mquake: Assessing knowledge editing in language models via multi-hop questions. arXiv preprint arXiv:2305.14795."
        },
        {
            "title": "A More Details in Experiments",
            "content": "We illustrate the details of hyperparameters selected in this paper. For all the experiments, we use up to NVIDIA-A100 80G GPU for training and evaluation. We train for 1 epoch if not specified. We use minimum span length of 1 and maximum of 5. Next-token-prediction We train with NTP objective on 8 GPUs where the batch size is 256 and upsampling is 1000. The learning rate and model configurations are described as in Section 5.1. Synthetic QA We train with synthetic QA objective on 8 GPUs where the batch size is 256 and upsampling is 1000. The learning rate and model configurations are described as in Section 5.1. Span Prediction For CLMs, we train with span prediction objective on 4 GPUs where the batch size is 128 and upsampling is 1000. The learning rate and model configurations are described as in Section 5.1. For BiLMs, we train with span prediction on 2/4 GPUs where the batch size 128 and upsampling is 1000. The learning rate and model configurations are described as in Section 5.1. 12 Scope Classifier For scope classifier, we train with DeBERTa-large-v3 where we set the learning rate 2e 5 and batch size 128. We train for 10 epochs. Metrics We evaluate model performance using two metrics: (1) Match, binary indicator that returns true if the target string is substring of the models output (case-sensitive), and false otherwise; and (2) Token-Level F1, the harmonic mean of precision and recall computed by aligning the models output tokens with the target tokens. The Match metric assesses strict presence of the target sequence, while the Token F1 score quantifies partial lexical overlap, offering complementary insights into generation quality. Both metrics are computed using whitespace-based tokenization to ensure consistency with standard text generation benchmarks."
        },
        {
            "title": "B Prompts for Question Generation",
            "content": "B.1 Reliability Question Generation Prompt We use GPT-4o for generating reliability QAs with the following prompt: Listing 1: Prompt for reliability QA Generation Given DYK fact in JSON format containing text and bold _ entity fields , generate question . Your output should be JSON object containing the question with its corresponding answer . Your response should follow these criteria : 1. The question should be answerable using only the information provided in the fact 2. The answer should be the bold _ entity 3. The question should be clear , natural , and specific so that the answer can be easily identified ( . . , use as many details as possible from the fact ) 4. The bold entity should not be mentioned in the question since it is the answer . But make sure that the question answer is the bold entity . Example : Input : {{ text : that Margrit Waltz has ferried planes to points on five continents ? , bold _ entity : Margrit Waltz , }} Expected output : {{ \" question \": {{ \" text \": \" Who has ferried planes to points on five continents ?\" , \" answer \": \" Margrit Waltz \" }} }} Now please generate question with answer for this fact : { test _ example } B.2 Paraphrase Question Generation Prompt We use GPT-4.1 for generating paraphrase questions based on the reliability question with the following prompt: Listing 2: Prompt for paraphrase QA generation Given pair of question and answer , generate three different paraphrases of the question . Make sure the answer is the same as before . Your output should be JSON object with list of dictionaries under the key \" paraphrases \". Each dictionary should have \" question \" key and an \" answer \" key . Here is the pair of question and answer : Question : { question } Answer : { answer } We pick the first paraphrased question from the generated list. B.3 Generality Question Generation Prompt We use GPT-4.1 for generating generality questions based on the reliability question and the fact with the following prompt: Listing 3: Prompt for generality QA generation Given pair of question and answer , generate three different alternative questions . Make sure the question asks about different aspect of the same fact . Remember to follow the rules below : 1. The answer is one aspect of the fact ( such as an entity / year / number etc .) apart from the original answer . 2. The answer should be concise and direct without any redundant words . And it shoud be part of the fact . 3. The question should utilize all the information in the fact and be specific . 4. Do not use any information that is beyond the fact . 5. Your output should be JSON object with list of dictionaries under the key \" alternatives \". Each sub - dictionary should have \" question \" key and an \" answer \" key . Here is the pair of question and answer : Fact : { fact } Question : { question } Answer : { answer } B.4 Portability Question Generation Prompt To generate the portability questions, we need to first identify an entity from the original knowledge entry that is non-bolded and have an associated Wikipedia article. After that, we prompt the o3-mini to first generate an entity description given its Wikipedia link (Notice that we discard those knowledge entries where the entity can not be found or the link is broken). The following is the entity description prompt: Listing 4: Prompt for entity description generation Replace the entity name with description of it without mentioning the entity name . The description should be unique and specific . Make sure that you can infer the entity name using the description . You might also be provided with the wikipedia page of the entity . The output should be JSON object with the following format : {{ }} \" description \": \" The description of the entity \" , Wikipedia page : { page } Entity name : { entity } 14 Then, we further prompt o3-mini to generate multi-hope QA pairs by replacing the entity with the description based on the reliability question. Listing 5: Prompt for portability QA generation Below are few examples of natural , scenario - based questions where user describes scenario and then asks question : Example 1: Alternative description : \" historic European city known for its iconic architecture and cobblestone streets .\" User natural question : \" recently visited charming European city famous for its unique architecture and quaint streets . Can you tell me about famous monument there ?\" Entity name : Paris Example 2: Alternative description : \" groundbreaking technology company that revolutionized communication with its innovative products .\" User natural question : \"I ve been reading about tech company that changed how we communicate through its innovative gadgets . What product are they best known for ?\" Entity name : Apple Now , given the alternative description and the original question below , generate new , natural , scenario - based question . The new question should describe scenario without mentioning the original entity name and then ask the question in natural , conversational manner . Alternative description : { description } Entity name : { entity } Original question : { question } The output should be JSON object with the following format : {{ }} \" question \": \" The modified question \" B.5 Locality Question Generation Prompt We use gpt-4.1 for locality question generation based on the previously generated entity description. Listing 6: Prompt for portability QA generation You ll generate question - answer pair based on the description of an entity . For each statement , you ll return JSON object containing : 1. \" question \": The question that corresponds to the statement 2. \" answer \": The answer to the question Example outputs : 1. Input : Jupiter is the largest planet in our solar system . Output : {{ \" question \": \" What is the largest planet in our solar system ?\" , \" answer \": \" Jupiter \" }} 2. Input : The capital of France is Paris . Output : {{ \" question \": \" What is the capital of France ?\" , 15 \" answer \": \" Paris \" }} Entity : { entity } Description : { description } B.6 Training QA Generation Prompt We use gpt-4.1-mini for training QA generation for the approach described in Section 4. Listing 7: Prompt for training QA generation Given context , please generate related questions as comprehensively as possible with corresponding answers . The question has to be based on the context and the answer should be short phrase . This is an example : Context : small coastal town has beach known for its colorful sea glass . The town hosts an annual festival celebrating this unique feature with art and conservation efforts . Question : What attracts tourists to the small coastal town annually ? Answer : The unique sea glass beach . Question : What is celebrated at the town annual festival ? Answer : The natural phenomenon of sea glass . Question : What type of activities are featured at the festival ? Format your output in JSON object like the one below : {{ \" questions \": [ {{ }} , {{ }} , {{ \" question \": \" What attracts tourists to the small coastal town annually ?\" , \" answer \": \" The unique sea glass beach .\" \" question \": \" What is celebrated at the town annual festival ?\" , \" answer \": \" The natural phenomenon of sea glass .\" \" question \": \" What type of activities are featured at the festival ?\" , \" answer \": \" Art and conservation efforts .\" }} ] }} Context : { fact }"
        },
        {
            "title": "C More Results",
            "content": "We present more results in this section. First, we show the numerical results that correspond to Figure 3 in Table 7-9. Notice that we also experimented with MemoryLLM (Wang et al., 2024) and M+ (Wang et al., 2025a). Results show that despite their claimed long-context, they are not able to utilize the questions for answer our evaluation questions, which results in lower performance compared with knowledge injection. We can also observe the performance of other BiLMs in Table 8 where we see that T5-v1.1-large also performs better than CLMs while lower then Flan-T5 models. Thus, we only include Flan-T5 models in our main results. Second, we also show the performance of reliability questions from 2004 to 2009 in Table 10. As can be observed, the performance decreases progressively with newer questions. Finally, we show the performance of ablation on Flan-T5-770M in Table 11. We can draw similar conclusions as in Section 5.5. 16 Table 7: Results of knowledge injection with first 100 knowledge entries."
        },
        {
            "title": "Model",
            "content": "Obj."
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F1 Llama-3.1-1B Gemma-3-1B Flan-T5-220M Flan-T5-770M"
        },
        {
            "title": "QA\nSP",
            "content": "SP 17.00 54.00 36.00 75.00 56.00 78.00 13.00 4. 19.66 49.47 37.38 63.36 58.69 79.27 0.75 0.39 27.00 28.00 33.00 37. 34.00 51.00 13.00 8.00 35.71 26.27 39.88 34.15 33.73 49.67 0.58 0. 20.00 45.00 28.00 68.00 47.00 67.00 10.00 5.00 24.96 41.86 31.59 57. 49.83 66.87 0.75 0.41 10.14 26.09 15.94 50.72 21.74 43.48 4.35 4. 15.96 22.53 18.55 48.87 28.49 45.50 0.53 0.40 23.19 37.68 42.03 50. 20.29 56.52 5.80 14.49 33.00 38.89 49.37 62.42 20.87 73.72 0.47 0. Table 8: Results of knowledge injection with first 1000 knowledge entries. Model Obj. Reliability Generality Paraphrase Portability Locality"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F"
        },
        {
            "title": "Match",
            "content": "F1 QA SP QA SP SP Llama-3.2-1B Gemma-3-1B-pt roberta-large t5-large t5-v1.1-large Flan-t5-220M Flan-t5-770M 15.00 33.00 25.00 50.00 3.00 32.00 52.00 54.00 73.00 18.39 29.87 30.08 39. 15.55 34.88 53.78 57.56 73.97 8.00 20.00 34.00 27.00 14.00 19.00 34.00 25.00 44.00 15.47 20.01 41.27 24. 12.84 22.28 32.66 28.97 43.22 10.00 23.00 18.00 36.00 3.00 28.00 42.00 47.00 67.00 13.03 21.28 22.97 30. 11.80 31.91 41.23 50.57 68.21 1.45 7.25 5.80 23.19 0.00 8.70 21.74 11.59 27.54 7.33 11.02 9.72 21. 0.00 10.22 24.54 15.50 35.76 26.09 28.99 59.42 55.07 0.00 24.64 46.38 10.14 52.17 32.00 37.13 62.95 49. 3.29 29.13 42.75 15.30 58.21 Table 9: Results of knowledge injection with first 3500 knowledge entries."
        },
        {
            "title": "Model",
            "content": "Obj."
        },
        {
            "title": "Locality",
            "content": "Match F1 Match F1 Match Match Llama-3.2-1B Gemma-3-1B-pt Flan-t5-220M Flan-t5-770M QA SP QA SP SP 14.00 17.00 19.00 25.00 45.00 66.00 16.21 20.74 22.34 24. 49.14 66.33 11.00 7.00 21.00 22.00 17.00 36.00 16.79 9.66 29.55 23. 21.30 37.68 9.00 15.00 12.00 22.00 36.00 59.00 13.56 17.46 14.25 21. 41.09 59.87 4.35 1.45 5.80 7.25 5.80 20.29 F1 8.88 6. 9.28 12.00 9.72 28.58 Match F1 14.49 18.84 40.58 46. 5.80 24.64 20.00 26.75 47.54 45.65 10.87 37.25 Table 10: Results for older reliability questions from 2004 to 2009. Model 2004 2005 2006 2007 2008 Match F1 Match F1 Match F1 Match F1 Match F1 Match F1 Llama-3.1-8B Llama-3.2-1B Qwen-2.5-1.5B Qwen-2.5-7B Gemma-3-1B-pt 12.03 2.22 2.65 6.66 2.39 2.03 1.24 6.08 10.36 0. 10.42 0.69 1.39 6.05 1.46 2.39 1.12 5.49 10.36 0.72 6.53 0.43 0.60 2.66 1.03 2.04 1.02 4.44 7.08 0.72 5.12 0.84 0.43 2.39 0.67 1.63 1.03 3.64 6.53 0. 4.34 0.58 0.43 1.95 0.58 1.75 1.04 3.73 6.13 0.78 3.79 0.62 0.33 1.48 0.59 1.64 0.99 3.50 5.32 0."
        },
        {
            "title": "Limitation",
            "content": "We discuss the limitations in this section. Our work claims that BiLMs perform much better than the CLMs. However, we prove this assumption empirically with no theoretical guarantees. Furthermore, limited by the computing resource, we are not able to completely pre-train BiLM and CLM under the same set of hyperparameter and data. Instead, we choose the popular pre-trained models for experiments without further controlling the experiments. 17 Table 11: Ablation study on ensemble models using Flan-T5-770M as the base model. Generality Paraphrase Portability Reliability Locality # clusters Type Temporal Temporal-perfect Semantic Semantic-perfect Match F1 Match F1 Match F1 Match 33.50 19.92 13.43 51.57 51.60 65.70 44.24 44.03 52.82 46.62 49.28 56. 35.78 22.38 14.17 53.88 54.14 66.72 46.55 45.93 53.85 49.17 51.50 57.62 24.50 17.81 13.66 34.50 34.58 42. 27.85 29.48 31.84 30.25 33.28 36.29 27.41 21.09 13.06 38.12 38.79 46.15 31.37 32.66 34.91 34.21 37.36 39. 22.82 12.95 8.85 41.51 41.22 55.13 32.00 32.46 40.02 35.79 37.88 45.71 25.35 15.48 9.71 44.24 44.52 57. 34.68 34.75 42.04 38.84 40.83 47.80 3.18 2.02 1.54 10.11 12.15 20.24 4.44 4.27 6.56 6.60 9.42 11. F1 4.41 2.83 1.91 13.90 15.90 23.76 6.19 5.73 8.14 9.97 13.11 15.02 Match F1 45.02 35.48 40.47 49.58 49.58 49.58 44.60 45.02 49.58 49.58 49.58 49.58 10.71 5.69 7. 7.66 7.66 7.66 11.24 5.00 11.55 7.66 7.66 7.66 3 5 10 3 5 10 3 5 3 5"
        }
    ],
    "affiliations": [
        "Cisco",
        "Tencent AI Lab",
        "UC, San Diego",
        "University of Washington"
    ]
}