{
    "paper_title": "Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images",
    "authors": [
        "Yasamin Medghalchi",
        "Moein Heidari",
        "Clayton Allard",
        "Leonid Sigal",
        "Ilker Hacihaliloglu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep neural networks (DNNs) offer significant promise for improving breast cancer diagnosis in medical imaging. However, these models are highly susceptible to adversarial attacks--small, imperceptible changes that can mislead classifiers--raising critical concerns about their reliability and security. Traditional attacks rely on fixed-norm perturbations, misaligning with human perception. In contrast, diffusion-based attacks require pre-trained models, demanding substantial data when these models are unavailable, limiting practical use in data-scarce scenarios. In medical imaging, however, this is often unfeasible due to the limited availability of datasets. Building on recent advancements in learnable prompts, we propose Prompt2Perturb (P2P), a novel language-guided attack method capable of generating meaningful attack examples driven by text instructions. During the prompt learning phase, our approach leverages learnable prompts within the text encoder to create subtle, yet impactful, perturbations that remain imperceptible while guiding the model towards targeted outcomes. In contrast to current prompt learning-based approaches, our P2P stands out by directly updating text embeddings, avoiding the need for retraining diffusion models. Further, we leverage the finding that optimizing only the early reverse diffusion steps boosts efficiency while ensuring that the generated adversarial examples incorporate subtle noise, thus preserving ultrasound image quality without introducing noticeable artifacts. We show that our method outperforms state-of-the-art attack techniques across three breast ultrasound datasets in FID and LPIPS. Moreover, the generated images are both more natural in appearance and more effective compared to existing adversarial attacks. Our code will be publicly available https://github.com/yasamin-med/P2P."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 0 1 9 9 0 . 2 1 4 2 : r Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images Yasamin Medghalchi1 Moein Heidari1 Clayton Allard Leonid Sigal1,2 1University of British Columbia, Vancouver, BC, Canada 2Vector Institute for AI, Toronto, ON, Canada yasimed@student.ubc.ca Ilker Hacihaliloglu1 {moein.heidari, ilker.hacihaliloglu}@ubc.ca clayton.allard@stat.ubc.ca, lsigal@cs.ubc.ca"
        },
        {
            "title": "Abstract",
            "content": "Deep neural networks (DNNs) offer significant promise for improving breast cancer diagnosis in medical imaging. However, these models are highly susceptible to adversarial attackssmall, imperceptible changes that can mislead classifiersraising critical concerns about their reliability and security. Traditional attacks rely on fixednorm perturbations, misaligning with human perception. In contrast, diffusion-based attacks require pre-trained models, demanding substantial data when these models are unavailable, limiting practical use in data-scarce scenarIn medical imaging, however, this is often unfeaios. sible due to the limited availability of datasets. Building on recent advancements in learnable prompts, we propose Prompt2Perturb (P2P), novel language-guided attack method capable of generating meaningful attack examples driven by text instructions. During the prompt learning phase, our approach leverages learnable prompts within the text encoder to create subtle, yet impactful, perturbations that remain imperceptible while guiding the model towards targeted outcomes. In contrast to current prompt learningbased approaches, our P2P stands out by directly updating text embeddings, avoiding the need for retraining diffusion models. Further, we leverage the finding that optimizing only the early reverse diffusion steps boosts efficiency while ensuring that the generated adversarial examples incorporate subtle noise, thus preserving ultrasound image quality without introducing noticeable artifacts. We show that our method outperforms state-of-the-art attack techniques across three breast ultrasound datasets in FID and LPIPS. Moreover, the generated images are both more natural in appearance and more effective compared to existing adversarial attacks. Our code will be publicly available at GitHub. Figure 1. Illustration of P2P in an adversarial attack against DiffPGD; note there is no exhibited change of image semantics in our method. 1. Introduction Mammography remains the primary tool for breast cancer screening, offering widespread accessibility and proven effectiveness in detecting early-stage tumors [16]. However, it has some limitations, including patient discomfort and reduced diagnostic accuracy in individuals with dense breast tissue [33]. Ultrasound has become useful supplementary technique, as it provides better patient comfort and avoids the risks associated with radiation exposure. Its ability to capture real-time images makes it beneficial for further investigating abnormalities observed on mammograms [16]. Nonetheless, the performance of ultrasound can be inconsistent, with sensitivity and specificity affected by the operators expertise, device configuration, and changes in tissue characteristics due to breast density. Furthermore, variability in interpretation among different radiologists poses additional challenges [51]. In response to these issues, computational approaches, especially deep learning models, are gaining traction as promising tools to improve diagnostic outcomes and address these existing gaps [12, 31, 51]. Ensuring the security and robustness of medical deep neural networks (DNNs) is key concern in healthcare. Recent research in medical imaging tasks has revealed that even state-of-the-art DNNs are highly susceptible to adversarial attacks [39]. Specifically, medical imaging DNNs are more susceptible to adversarial threats than those designed for natural images [39]. In addition, the limited availability of publicly accessible medical images, which poses challenge for effective training, has pushed modern algorithms to rely on pre-trained models based on large datasets of natural imagesa technique known as transfer learning. However, due to the substantial difference between natural and medical images, deep learning models developed through transfer learning tend to be more vulnerable to adversarial attacks [10]. To address this vulnerability, various adversarial attack methods have been developed. Traditional gradient-based methods such as fast gradient sign method (FGSM) [20], Carlini-wagner [8], and projected gradient descent (PGD) [40] perturb original images within predefined perturbation bounds. Although they can be easily generated using gradient-based techniques, these attacks often deviate significantly from the true data distribution of natural images, leading to compromise between their effectiveness and subtlety [61]. Generative models such as generative adversarial networks (GANs) [19] have been used to generate adversarial examples, based on their ability to produce high-quality images, by perturbing latent space [3, 49, 64]. However, modifying latent codes changes the high-level semantics of generated images in way that is easily noticeable to human observers [30]. Recently, diffusion-based models have been employed in the literature to generate realistic adversarial images [11, 36, 61]. In models like AdvDiffuser [11] and Diff-PGD [61], the PGD method is incorporated into the diffusion process to enhance the realism of the generated images. Furthermore, approaches such as Instruct2Attack [37] leverage latent diffusion models [46] to deceive the target model by optimizing over input prompts. These text-guided attacks aim to produce adversarial semantic image edits, such as altering the color of truck in an image. While image editing holds significant value in natural image domains, it is less applicable in the medical field, as altering the semantics of medical images can result in unrealistic representations. Additionally, these models are often not well-adapted to clinical terminology, making such modifications less meaningful in medical contexts [57]. These limitations can be addressed by reversing the diffusion model to obtain context-relevant text tokens, or prompts, specifically designed for the medical domain to generate adversarial images. Optimizing these prompts presents challenge due to the intricate structure of the generative model, which includes two-part pipeline: conditioning network (the CLIP text encoder [44]) and generative unit (U-Net [47]), both of which complicate gradient flow to the input layer. To tackle these obstacles, we freeze the generative model and make the following contributions: Our method requires no fine-tuning and efficiently generates attack images by directly updating text embeddings, unlike previous diffusion-based approaches reliant on large pre-trained models on the same domain. Additionally, we find that optimizing only the initial reverse diffusion steps is sufficient, improving efficiency and maintaining image quality without the need for all reverse diffusion phases. Based on our findings, we proposed Prompt2Perturb (P2P) to create challenging adversarial examples optimized for clinical terminology that are particularly challenging to distinguish as attacked, thus highlighting limitations in current adversarial attack mechanisms. We demonstrate empirically that our method produces semantically significant attack images that are perceptually similar but varied versions of target image (see Fig. 1). By incorporating relevant clinical terminology into the text embedding of Stable Diffusion, we ensure that the generated images retain clinical accuracy and realism, thereby enhancing the effectiveness of the attack in medical context. 2. Related Work Adversarial attack. DNN models are widely recognized as being susceptible to adversarial attacks [45]. These attacks can manipulate the models predictions by introducing subtle modifications that are imperceptible to human observers. Adversarial attacks pose major barrier to the widespread adoption of deep learning models in real-world applications, and considerable research has been dedicated to addressing this ongoing challenge. Adversarial attacks can be broadly classified into blackbox and white-box types. White-box attacks, such as limited-memory BroydenFletcherGoldfarbShanno (LBFGS) [52], FGSM [20], PGD [40], C-W [8], and DeepFool [42], rely on full access to the target models architecture and parameters, making them gradient-based. In contrast, black-box attacks do not require this level of access and generate adversarial samples through query-based methods. Despite these differences, many white-box attacks are still effective in black-box settings due to the transferability of adversarial examples across models. Adversarial training, which involves training model using adversarially generated examples, is one of the limited methods available to defend against adversarial attacks [5]. In medical domain, several approaches have been developed to generate adversarial examples targeting computer-aided diagnosis models which can be consistently taxonomized with that of adversarial attacks [15]. Specifically, Zhou et al. [65] introduced two GAN-based models operating at different resolutions, capable of generating highly realistic adversarial images that can mislead breast cancer diagnosis. MIRST [50] was proposed to defend untargeted adversarial attacks Figure 2. Overall framework of the proposed method. Image adapted from [36] on breast ultrasound images. Lastly, Hao et al. [22] introduce an adversarially robust feature learning method where feature correlation metric is included as an objective function to promote the learning of robust features while minimizing the influence of spurious ones for the application of breast cancer diagnosis. Adversarial attack with diffusion models. The success of diffusion models has motivated their exploration in adversarial deep learning, demonstrating potential for both attacking [60, 66] and defending [58, 59] against adversarial examples. These methods are broadly used to enhance the imperceptibility of unconstrained adversarial perturbations [9], transfer the style from the reference image into the original image to create natural-looking adversarial images [56], or attack the diffusion-based purification method [29]. Prompt learning. Recent progress in controllable image generation and editing using diffusion models [25, 48] has created new possibilities for developing semantic adversarial attacks. Prompt learning was initially introduced as an alternative to full fine-tuning and linear probing, aiming to leverage pre-trained language models in natural language processing (NLP) [34]. Recently, prompt learning techniques have been adapted for adversarial purposes. For instance, BadCLIP [4] targets the CLIP [44] model by injecting backdoor during the prompt learning phase. In the medical field, BAPLe [21] utilizes learnable prompts within the text encoder and introduces imperceptible learnable noise triggers into input images, effectively leveraging the full capabilities of medical foundation models. Additionally, PromptSmooth [27] uses prompt learning to efficiently achieve certified robustness in medical visionlanguage models by leveraging semantic concepts. 3. Methodology 3.1. Preliminary We explore the challenge of image classification. Consider an image sample := RHW C, paired with its label := {1, . . . , }, where H, , and represent the images height, width, and number of channels, respectively. Let : denote the image classifier. Given clean input image and its true label y, the attackers objective is to generate an adversarial version xadv of that misleads , so that (xadv) = y. Traditional approaches either necessitate adding pixel-level adversarial noise δ [8, 20] to the input image x, resulting in xadv = + δ, or manipulating data to embed subtle biases or vulnerabilities. In this work, we instead focus on prompt-guided semantic attacks, aiming to create meaningful, interpretable perturbed images guided by language descriptions. 3.2. Prompt2Perturb (P2P) Figure 2 provides high-level overview of the proposed approch. Our approach seeks to generate an adversarial image, xadv, such that the modification from the original image is guided by text-based embedding instruction C. The core idea of Prompt2Perturb (P2P) is to optimize these text embeddings specifically for the modified input to steer the classifier toward an incorrect prediction. In the subsequent sections, we will provide detailed breakdown of each component of our proposed method. Latent Diffusion Models. We utilize the learned latent space of generative model, defined as = = E(x), Rhwc, where represents the image encoder and denotes the dimensions of the latent space. Unlike the image space, this latent space offers compact, lower-dimensional representation that emphasizes the critical semantic details, making it ideal for controlled semantic alterations. These modifications are enabled through pretrained conditional latent diffusion model (LDM) [6, 46], which facilitates meaningful and guided editing in the latent domain. The encoder is regularized via either KL-divergence penalty or vector quantization, as suggested by prior works [53]. The corresponding decoder, denoted as D, learns to reconstruct input images from the latent representations, ensuring D(E(x)) x, for high fidelity reconstruction. Subsequently, diffusion model is trained to generate latent codes that reside within the learned latent space. This diffusion model is flexible and can be conditioned using auxiliary information, such as class labels, segmentation masks, or features from co-trained text-embedding model. Specifically, let cθ(p) represent network mapping conditioning input to its respective conditioning vector. The objective function for the Latent Diffusion Model (LDM) can thus be formulated as: LLDM := EzE(x),p,ϵN (0,1),t (cid:104) ϵ ϵθ(zt, t, cθ(p))2 2 (cid:105) , where is the time step, zt is the latent noised to time t, ϵ is the unscaled noise sample, and ϵθ is the denoising network. Intuitively, the objective here is to correctly remove the noise added to latent representation of an image. While training, cθ and ϵθ are jointly optimized to minimize the LDM loss. At inference time, random noise tensor is sampled and iteratively denoised to produce new image latent, z0. Finally, this latent code is transformed into an image through the pre-trained decoder = D(z0). Previous diffusion-based attack methods leverage extensively pretrained models on natural images, incorporating specialized attack strategies and employing reverse diffusion process to generate adversarial examples that resemble the underlying data distribution. However, this approach is impractical in the medical domain due to the lack of extensively trained models caused by inherent data scarcity and the presence of diverse imaging modalities. Unlike these previous approaches, our method is well-suited to handle this diversity without requiring retraining of diffusion model or relying on large-scale pre-trained models. This robustness stems from our approach, which involves updating the text embeddings directly, rather than merely adding noise to the latent space. We next review the early stages of such text encoder, and our choice of attack space. Text optimization. The text encoder processes the input prompt P, which is fed into the latent diffusion model. text tokenizer transforms the prompt strings into tokens, representing indices in predefined vocabulary of size . Each token is then mapped to its corresponding embedding vector ci C, where 1, . . . , within the embedding space C. Previous work on textual inversion in the embedding space [17, 55] attempts to learn new concepts for set of images by associating placeholder strings with newly learned embedding vectors. This effectively expands the existing vocabulary with additional concepts. However, these newly learned concepts have been found to be uninformative in the medical domain, as the learned embeddings are often distant from the embeddings corresponding to the original vocabulary. In contrast, our approach aims to optimize the embedding vectors ci, 1, . . . , L, from the existing vocabulary for prompt of maximum length specific to the medical domain. This adaptation addresses the limitations of existing diffusion model vocabularies that lack specific medical terms (e.g., the word benign is not present in the vocabulary of the Stable Diffusion text encoder, despite its frequent use in medical contexts). Therefore, our method ensures applicability across diverse imaging modalities and scenarios within the medical domain, as it remains independent of the specific encoder choice and is instead reliant on the latent diffusion backbone used for prompt optimization. Minimal reversal steps. Previous studies [41] have demonstrated that the diffusion models sensitivity to conditioning intensifies significantly during the later stages of the diffusion process, particularly when noise levels are higher. Conversely, in the initial timesteps, the diffusion loss remains relatively stable across various prompts applied to the same image. Inspired by this observation, we hypothesize that not all reverse diffusion steps are necessary for generating an adversarial image. Optimizing only for the early timesteps of denoising process (akin to later timesteps of diffusion process) has two-fold advantage. First, given that we are optimizing the early layers of very deep neural network, the resulting gradients tend to be quite small which contributes to our methods efficiency compared to others that require training the entire diffusion model. Additionally, early timesteps (the start of the denoising process) are associated with low-frequency components, focusing on reconstructing coarse shapes and structures [43]. As the process moves into the later timesteps, the model increasingly refines high-frequency details. By introducing adversarial perturbations during this stage, our strategy generates examples that effectively mimic common scenarios in ultrasound imaging, where acquisition settingssuch as the transducers operating frequency and its orientation relative to the anatomysignificantly influence the high-frequency content captured in the images [35]. We further substantiate this reasoning in the subsequent sections. Attacking policy setup. Given target image and latent diffusion model parameterized by ϵθ, our goal is to optimize the tokens within the conditioning text encoders vocabulary to best represent the visual content of the image while simultaneously crafting an adversarial attack on the classifier. To determine the optimal tokens for generating an adversarial image, we formulate the problem with two components. First, we employ cross-entropy loss to measure the difference between the classifiers predicted output and the actual image class. Second, we include loss that captures the discrepancy between the predicted noise from the network and the noise originally added to the image under specific prompt condition, thereby ensuring semantic similarity between the adversarial image and the original image. Our objective is to recover the set of embeddings that align our objective of generating an adversarial image. Note that we freeze the diffusion model parameters throughout this optimization except the text encoder part. As discussed previously, instead of optimizing over all diffusion time steps using standard SGD optimizer, we Algorithm 1: Text-Guided Diffusion-Based Attack Pipeline Input: Attacked classifier , Image x, Latent z, Prompt p, Text embedding C, Label Output: Adversarially perturbed image xadv for batch TrainLoader do for range(num iterations) do randint(0, 50); vae.encode(x), textencoder(p); ϵ randn like(z), zt add noise(z, ϵ, t); (cid:113) 1 αt (cid:0)zt 1 αt ϵθ(zt, t, C)(cid:1); xadv vae.decode(z); xadv (xadv + 1)/2; xadv.Normalize(mean, std); (xadv); Loss 1010 CrossEntropyLoss(y, y) + mse(ϵ, ϵθ(zt, C)); Loss.backward(); return xadv; focus on the most impactful time steps, restricting the range of to later timesteps (t 50). Subsequently, the image that shows the highest similarity to the original input is selected as the adversarial output. The complete approach of our method is illustrated in Algorithm 1. 4. Experiments This section starts by outlining the experimental setup, the comparison framework, and the evaluation metrics used. We then present both quantitative and qualitative comparisons against the current state-of-the-art methods, focusing on the stealthiness of the perturbations introduced and the realism of the generated examples. Finally, we conduct ablation and sensitivity analyses to evaluate the influence of key components and hyperparameters. 4.1. Experimental settings Dataset and Models. We leverage three publicly available breast ultrasound datasets and experiment with three widely recognized classifier architecturesResNet34 [23], SqueezeNet1.1 [28], and DenseNet121 [26]which are commonly used as baselines in medical imaging research [1, 32]. Given the relatively small dataset sizes, we apply five-fold cross-validation across all classes to ensure robust evaluation. For consistency with the classifier architectures, all images are resized to 224 224 pixels. Additionally, we incorporate pre-trained LDM from Hugging Face, specifically utilizing the Stable-Diffusion-v-1-4 variant [13]. BUSI (Breast Ultrasound Images). Collected in 2018, this dataset comprises ultrasound scans from 600 female patients between the ages of 25 and 75, gathered for breast cancer detection. The dataset includes total of 780 images, each with an average resolution of 500x500 pixels, categorized into three classes: normal (133 images), benign (437 images), and malignant (210 images) [2]. BUS-BRA. BUS-BRA [18] dataset includes 1875 anonymized images from 1,064 female patients, acquired with four different ultrasound scanners as part of systematic studies conducted at the National Institute of Cancer in Rio de Janeiro, Brazil. This dataset includes biopsy-confirmed tumor cases, with 1268 benign and 607 malignant cases. UDIAT. Collected in 2012 at the UDIAT Diagnostic Centre of the Parc Taulı Corporation in Sabadell, Spain, this dataset [62] was gathered using Siemens ACUSON Sequoia C512 system with 17L5 HD linear array transducer operating at 8.5 MHz. It comprises 163 images from 163 female patients, with an average resolution of 760x570 pixels, categorized into 53 malignant and 110 benign lesions. Hyperparameters. In our experiments, we train the model for 500 iterations with learning rate of 5 104 using the AdamW optimizer [38]. For adversarial methods, we set the FGSM attack parameter to ϵ = 0.05. For the PGD attack, we configure ϵ = 0.03 with step size of α = 0.007 over 200 iterations. We utilize Diff-PGD with the same setting as it used for ImageNet [14]. We run our experiments on single NVIDIA RTX V100 GPU with 32GB memory. Attacker Success Rate LPIPS SSIM FID FGSM PGD Diff-PGD P2P (Ours) FGSM PGD Diff-PGD P2P (Ours) FGSM PGD Diff-PGD P2P (Ours) DenseNet121 0.40 0.29 0.30 0.13 0.88 0.57 1.0 0. ResNet34 0.96 0.55 1.0 0.97 0.41 0.25 0.31 0.12 SqueezeNet1.1 0.16 0.20 0.14 0.09 0.49 0.33 0.74 0.96 0.81 0.45 0.87 0. 0.81 0.37 0.84 0.81 0.40 0.30 0.56 0.63 123.51 378.62 111.03 45.84 131.62 332.01 117.49 43.03 118.03 250.38 79.51 47.64 Table 1. Evaluation of adversarial attacks on different attack models for BUSI dataset [2] with 3 classifiers. LPIPS, SSIM, and FID are reported on successful attack examples. Figure 3. Visual comparison of different attack methods on benign image from the BUSI dataset, using DenseNet121 as the classifier. The second row displays the perturbations, calculated as the difference between the original image and the attacked example. Figure 4. Comparison of original and P2P-attacked ultrasound images from BUS-BRA Dataset, using DenseNet121 as the classifier. The top row shows the original images with their diagnostic labels, while the bottom row displays the same images after applying the P2P attack. Green boxes indicate the true labels, while red boxes show the labels predicted by the classifier after the attack. 4.2. Evaluation of Adversarial Examples To evaluate the quality of adversarial examples generated by our proposed P2P method, we benchmark it against the leading Diffusion-Based Projected Gradient Descent (DiffPGD) attack and conventional techniques like FGSM and PGD. We assess each attacks performance across metrics including Success Rate, LPIPS, SSIM, and FID, enabling comprehensive analysis of both adversarial efficacy and the perceptual integrity of the generated adversarial samples. In our setting, each attack baseline is applied to validation set images that were initially classified correctly, with adversarial examples retained only when successful in misleading the classifiers. Accordingly, all metrics presented in Tabs. 1 to 3 reflect measurements derived exclusively from these modified images. Across all datasets and classifier architectures, our P2P approach consistently attains high Success Rates [7] comparable to those achieved by Diff-PGD, while surpassing baseline methods in Frechet Inception Distance (FID) [24], reflecting the imperceptibility of the crafted adversarial examples, and in Learned Perceptual Image Patch Similarity (LPIPS) [63], indicating reduced distortion in our attacks. These results highlight the superior perceptual quality of adversarial examples generated by our method. In terms of Structural Similarity Index Measure (SSIM), which evaluates the structural alignment between adversarial and original images, our method achieves competitive performance. Although Diff-PGD occasionally marginally outperforms P2P in SSIM for specific models, P2P consistently maintains strong SSIM values across all datasets, as shown in Tabs. 1 to 3. This reflects high visual similarity between the generated adversarial examples xadv and the original clean images x, capturing low perceived visual distortion by preserving luminance, contrast, and structural details. The combination of high SSIM with low LPIPS and FID scores underscores P2Ps ability to preserve structural consistency while producing perceptually aligned adversarial examples. the DenseNet121 and ResNet34 classifiers, P2P For achieves strong balance of high Success Rate, low LPIPS, Attacker Success Rate LPIPS SSIM FID Attacker Success Rate LPIPS SSIM FID FGSM PGD Diff-PGD P2P (Ours) FGSM PGD Diff-PGD P2P (Ours) FGSM PGD Diff-PGD P2P (Ours) DenseNet121 0.40 0.19 0.29 0.12 0.93 0.43 1.0 0.94 ResNet 0.81 0.31 1.0 0.93 0.35 0.12 0.29 0.11 SqueezeNet1.1 0.16 0.26 0.12 0.08 0.69 0.43 0.75 0.74 0.77 0.56 0.82 0.78 0.66 0.24 0.78 0. 0.77 0.40 0.47 0.49 112.11 213.65 90.5 38.00 133.17 158.24 100.2 44.09 120.14 292.99 89.47 58.60 FGSM PGD Diff-PGD P2P (Ours) FGSM PGD Diff-PGD P2P (Ours) FGSM PGD Diff-PGD P2P (Ours) DenseNet121 0.37 0.07 0.27 0.12 0.97 0.17 1.0 0.86 ResNet34 0.98 0.23 1.0 0.97 0.41 0.10 0.31 0. SqueezeNet1.1 0.20 0.14 0.16 0.10 0.59 0.16 0.77 0.76 0.77 0.11 0.80 0.62 0.75 0.19 0.77 0.74 0.42 0.19 0.54 0.52 103.07 147.84 81.31 27. 103.68 135.95 80.89 20.3 72.51 292.21 32.47 23.50 Table 2. Evaluation of adversarial attacks on different attack models for BUS-BRA dataset [18] with 3 classifiers. LPIPS, SSIM, and FID are reported on successful attack examples. Table 3. Evaluation of adversarial attacks on different attack models for UDIAT dataset [62] with 3 classifiers. LPIPS, SSIM, and FID are reported on successful attack examples. and favorable SSIM and FID scores, surpassing traditional methods. With SqueezeNet1.1, P2P exhibits particularly robust performance on perceptual metrics, reaching the lowest LPIPS and FID values across all datasets, as shown in Tabs. 1 to 3, highlighting its ability to generate realistic and structurally consistent adversarial images. Overall, P2P demonstrates competitive Success Rates while consistently producing high-quality adversarial examples with enhanced visual fidelity and structural similarity. The results underscore P2Ps effectiveness as balanced approach for crafting effective and perceptually similar adversarial images across various datasets and classifiers. We will further elaborate on these results qualitatively in the subsequent section. 4.3. Qualitative Results of Adversarial Examples Figure 3 presents visual comparisons of images and adversarial perturbations generated by different attack methods. Notably, our method (P2P) produces attack examples that more closely follow the distribution of the original medical images, resulting in more natural and less detectable alteration. In contrast, other attack methods, such as FGSM, PGD, and Diff-PGD, introduce high-frequency noise with distinct textured appearance. This textured noise can make these adversarial examples appear less natural, potentially revealing them as manipulated images, exhibit noisy artifacts. Besides, Fig. 4 shows that the P2P attack successfully changes the diagnostic labels of ultrasound images with minimal alteration to the images semantic appearance, demonstrating label vulnerability despite preserved image semantics. Moreover, the t-SNE [54] visualization of clean and attacked image features further confirms the effect of our prompt learning method. Figure 5 compares four attack methodsFGSM, PGD, Diff-PGD, and P2P (Ours)in terms of how well they blend attack samples with clean data clusters. Diff-PGD shows limited effectiveness, with noticeable separations between attack and clean samples. FGSM and PGD improve blending but still leaves isolated clusters of attack points. P2P achieves the best integration, with attack samples closely interwoven with clean data, indicating superior performance in making adversarial examples indistinguishable from clean data. To gain further insights, we examined attacked examples from the training set, focusing on images that the classifier identifies with high confidence. Figure 6 presents malignant cases from the BUSI training set alongside their attacked versions generated by Diff-PGD and our method, P2P. As illustrated, the Diff-PGD attack introduces noticeable noise pattern, while our method produces images that closely resemble the originals. Interestingly, additional examples reveal that Diff-PGDs image quality deteriorates as the classifiers confidence increases. While attack experiments are generally performed on validation sets, we analyzed training set images here to understand the performance of different attack models in varied scenarios. 4.4. Ablation Study This section disentangles the effects of design choices to assess their individual contributions to our approachs effectiveness. All ablation experiments are conducted on"
        },
        {
            "title": "SSIM FID",
            "content": "Time (second) Baseline W/O MSE T=20 T=100 0.97 0.96 0.83 0.97 0.12 0.12 0.10 0. 0.81 0.80 0.71 0.81 43.03 43.58 49.97 38.06 178 178 98 280 Table 4. Comparison of the ablation study on different components of the P2P pipeline. The baseline configuration uses T=50 with MSE in the loss function. In each row, only one component is modified from the baseline. Time indicates the duration of the generation process for the attack per image. a) FGSM b) PGD a) Original b) W/O MSE c) MSE c) Diff-PGD d) P2P (Ours) Figure 5. t-SNE visualization of last-layer ResNet34 features on the BUSI dataset for FGSM, PGD, Diff-PGD, and P2P (Ours). Clean examples are shown in blue, and adversarial examples in orange. a) Original b) Diff-PGD c) P2P Figure 6. Attack examples of malignant image of Dataset BUSI from Diff-PGD and P2P with ResNet34 as classifier. Figure 7. Impact of MSE loss in the P2P framework. Image (a) shows the clean reference image, image (b) shows the P2P result without MSE loss, and image (c) includes the MSE loss function. MSE) exhibits enhanced linear or streak-like artifacts, which may disrupt the homogeneity of tissue appearance (highlighted in the red rectangular region) and obscure finer structural details in the ultrasound image. Incorporating the MSE loss helps to reduce intensity fluctuations, creating smoother and more consistent appearance, particularly beneficial in areas with subtle tissue boundaries. Step Selection To justify the choices for P2P with optimization for different timesteps, we show in Tab. 4 the performance of our method with different settings. Using 20 steps, the approach yields prompts that generate attack images with relatively low success, SSIM and LPIPS. However, when optimizing with more timesteps (T = 100) even though, the FID of the generated attack images is better, other metrics are comparable to that of (T = 50), but with longer time to generate attack images. Therefore, given the efficiency dilemma and the results presented, selecting 50 steps appears to be reasonable choice. the BUSI dataset, using ResNet34 as the classifier, with results averaged over 5-fold cross-validation. Loss Function. Ablation study results on the effect of MSE loss function are reported in Tab. 4 (second row) and Fig. 7. Although using an MSE loss does not necessarily lead to better quantitative metrics, Fig. 7 shows that it attempts to mitigate artifacts and retain the semantic integrity of the ultrasound image. Specifically, the middle image (without 5. Conclusion In this paper, we designed an effective adversarial attack procedure called P2P for synthesizing adversarial breast ultrasound images using prompt learning in text-to-image diffusion models. We showcase the effectiveness of our prompt-learning approach across various attack scenarios, especially valuable in data-limited medical applications. Our adversarial images surpass previous methods, appearing more natural and harder to detect. They also maintain lower perceptual differences while achieving greater structural similarity. P2P enables significantly easier attacks on classifiers within the medical domain, eliminating the need for pre-trained models in specific domain. By facilitating the construction of effective textual examples, it enhances the ability to target and challenge classifiers directly."
        },
        {
            "title": "Acknowledgments",
            "content": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), [funding reference number RGPIN-2023-03575]. Cette recherche ete financee par le Conseil de recherches en sciences naturelles et en genie du Canada (CRSNG), [numero de reference RGPIN-2023-03575]."
        },
        {
            "title": "References",
            "content": "[1] Barsha Abhisheka, Saroj Kumar Biswas, and Biswajit Purkayastha. comprehensive review on breast cancer detection, classification and segmentation using deep learning. Archives of Computational Methods in Engineering, 30(8): 50235052, 2023. 5 [2] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in Brief, 2020. 5 [3] Pooria Ashrafian, Milad Yazdani, Moein Heidari, Dena Shahriari, and Ilker Hacihaliloglu. Vision-language synthetic data enhances echocardiography downstream tasks. arXiv preprint arXiv:2403.19880, 2024. 2 [4] Jiawang Bai, Kuofeng Gao, Shaobo Min, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Badclip: Trigger-aware prompt In Proceedings of learning for backdoor attacks on clip. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2423924250, 2024. 3 [5] Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. Recent advances in adversarial training for adversarial robustness. arXiv preprint arXiv:2102.01356, 2021. 2 [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [7] Tom Brown, Dandelion Mane, Aurko Roy, Martın Abadi, arXiv preprint and Justin Gilmer. Adversarial patch. arXiv:1712.09665, 2017. 6 [8] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 3957. Ieee, 2017. 2, 3 [9] Jianqi Chen, Hao Chen, Keyan Chen, Yilan Zhang, Zhengxia Zou, and Zhenwei Shi. Diffusion models for imperceptible IEEE Transactions on and transferable adversarial attack. Pattern Analysis and Machine Intelligence, 2024. 3 [10] Xiaohui Chen and Tie Luo. Adversarial-robust transfer In learning for medical imaging via domain assimilation. Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 335349. Springer, 2024. 2 [11] Xinquan Chen, Xitong Gao, Juanjuan Zhao, Kejiang Ye, and Cheng-Zhong Xu. Advdiffuser: Natural adversarial example synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 45624572, 2023. 2 [12] Jie Zhi Cheng, Dong Ni, Yi Hong Chou, Jing Qin, Chui Mei Tiu, Yeun Chung Chang, Chiun Sheng Huang, Dinggang Shen, and Chung Ming Chen. Computer-aided diagnosis with deep learning architecture: applications to breast lesions in us images and pulmonary nodules in ct scans. Scientific reports, 6(1):24454, 2016. [13] CompVis. Stable diffusion v1.4 - original, 2023. Accessed: [Insert Date of Access]. 5 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 5 [15] Junhao Dong, Junxi Chen, Xiaohua Xie, Jianhuang Lai, and Hao Chen. Adversarial attack and defense for medical imarXiv preprint age analysis: Methods and applications. arXiv:2303.14133, 2023. 2 [16] Ahmed Elhatw, Hannah Chung, Rasha Kamal, Charles De Jesus, Shanen Jean, Varnita Vishwanath, Hanna Ferreira Dalla Pria, Miral Patel, Mary Guirguis, and Tanya Moseley. Advanced breast imaging modalitiesdbt, cem, mbi, pem, mri, ai. Current Breast Cancer Reports, 15(2):108113, 2023. 1 [17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [18] Wilfrido Gomez-Flores, Maria Julia Gregorio-Calas, and Wagner Coelho de Albuquerque Pereira. Bus-bra: breast ultrasound dataset for assessing computer-aided diagnosis systems. Medical Physics, 51(4):31103123, 2024. 5, 7 [19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 26722680. MIT Press, 2014. 2 [20] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. 2, 3 [21] Asif Hanif, Fahad Shamshad, Muhammad Awais, Muzammal Naseer, Fahad Shahbaz Khan, Karthik Nandakumar, Salman Khan, and Rao Muhammad Anwer. Baple: Backdoor attacks on medical foundational models using prompt In International Conference on Medical Image learning. Computing and Computer-Assisted Intervention, pages 443 453. Springer, 2024. 3 [22] Degan Hao, Dooman Arefan, Margarita Zuley, Wendie Berg, and Shandong Wu. Adversarially robust feature learning for breast cancer diagnosis. arXiv preprint arXiv:2402.08768, 2024. 3 [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 5 struct2attack: Language-guided semantic adversarial attacks. arXiv preprint arXiv:2311.15551, 2023. 2 [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [26] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 5 [27] Noor Hussein, Fahad Shamshad, Muzammal Naseer, and Karthik Nandakumar. Promptsmooth: Certifying robustness of medical vision-language models via prompt learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 698708. Springer, 2024. [28] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size. arXiv:1602.07360, 2016. 5 [29] Mintong Kang, Dawn Song, and Bo Li. Diffattack: Evasion attacks against diffusion-based adversarial purification. Advances in Neural Information Processing Systems, 36, 2024. 3 [30] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 2 [31] Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof. Diffusion models in medical imaging: comprehensive survey. Medical Image Analysis, 88:102846, 2023. 1 [32] Kriti, Jitendra Virmani, and Ravinder Agarwal. Deep feature extraction and classification of breast ultrasound images. Multimedia Tools and Applications, 79(37):27257 27292, 2020. 5 [33] Christoph Lee and Constance Lehman. Digital breast tomosynthesis and the challenges of implementing an emerging breast cancer screening technology into clinical practice. Journal of the American College of Radiology, 13(11):R61 R66, 2016. 1 [34] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 3 [35] David Lieu. Ultrasound physics and instrumentation for pathologists. Archives of pathology & laboratory medicine, 134(10):15411556, 2010. [36] Yueqian Lin, Jingyang Zhang, Yiran Chen, and Hai Li. Sdnae: Generating natural adversarial examples with stable diffusion. arXiv preprint arXiv:2311.12981, 2023. 2, 3 [37] Jiang Liu, Chen Wei, Yuxiang Guo, Heng Yu, Alan Yuille, InSoheil Feizi, Chun Pong Lau, and Rama Chellappa. [38] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5 [39] Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng Lu. Understanding adversarial attacks on deep learning based medical image analysis systems. Pattern Recognition, 110:107332, 2021. 2 [40] Aleksander Madry. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 2 [41] Shweta Mahajan, Tanzila Rahman, Kwang Moo Yi, and Leonid Sigal. Prompting hard or hardly prompting: Prompt inversion for text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68086817, 2024. [42] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 25742582, 2016. 2 [43] Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, and Tao Mei. Boosting diffusion models with moving In Proceedings of average sampling in frequency domain. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89118920, 2024. 4 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3 [45] Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial attacks and defenses in deep learning. Engineering, 6 (3):346360, 2020. 2 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [48] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [49] Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial examples with generative models. Advances in neural information processing systems, 31, 2018. 2 [50] Shoukun Sun, Min Xian, Aleksandar Vakanski, and Hossny Ghanem. Mirst-dm: Multi-instance rst with drop-max layer study on ai diagnosis model safety under attacks of adversarial images. Nature communications, 12(1):7281, 2021. 2 [66] Haomin Zhuang, Yihua Zhang, and Sijia Liu. pilot study of query-free adversarial attack against stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23852392, 2023. 3 In International for robust classification of breast cancer. Conference on Medical Image Computing and ComputerAssisted Intervention, pages 401410. Springer, 2022. 2 [51] William Svensson and Victoria Stewart. Advanced applications of breast ultrasound. Breast Cancer, pages 7098, 2010. 1 [52] Szegedy. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2 [53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [54] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (11), 2008. 7 [55] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 4 [56] Xiaomeng Wang, Honglong Chen, Peng Sun, Junjian Li, Anqing Zhang, Weifeng Liu, and Nan Jiang. Advst: Generating unrestricted adversarial images via style transfer. IEEE Transactions on Multimedia, 2023. 3 [57] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. arXiv preprint arXiv:2210.10163, 2022. 2 [58] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In International Conference on Machine Learning, pages 3624636263. PMLR, 2023. [59] Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, and Chaowei Xiao. Defending against adversarial audio via diffusion model. arXiv preprint arXiv:2303.01507, 2023. 3 [60] Haotian Xue, Alexandre Araujo, Bin Hu, and Yongxin Chen. Diffusion-based adversarial sample generation for improved stealthiness and controllability. Advances in Neural Information Processing Systems, 36:28942921, 2023. 3 [61] Haotian Xue, Alexandre Araujo, Bin Hu, and Yongxin Chen. Diffusion-based adversarial sample generation for improved stealthiness and controllability. Advances in Neural Information Processing Systems, 36, 2024. 2 [62] Moi Hoon Yap, Gerard Pons, Joan Marti, Sergi Ganau, Melcior Sentis, Reyer Zwiggelaar, Adrian Davison, and Robert Marti. Automated breast ultrasound lesions detection using convolutional neural networks. IEEE journal of biomedical and health informatics, 22(4):12181226, 2017. 5, 7 [63] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [64] Zhengli Zhao, Dheeru Dua, and Sameer Singh. GenarXiv preprint erating natural adversarial examples. arXiv:1710.11342, 2017. [65] Qianwei Zhou, Margarita Zuley, Yuan Guo, Lu Yang, Bronwyn Nair, Adrienne Vargo, Suzanne Ghannam, Dooman Arefan, and Shandong Wu. machine and human reader"
        }
    ],
    "affiliations": [
        "University of British Columbia, Vancouver, BC, Canada",
        "Vector Institute for AI, Toronto, ON, Canada"
    ]
}