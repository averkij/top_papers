{
    "paper_title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
    "authors": [
        "Junha Hyung",
        "Kinam Kim",
        "Susung Hong",
        "Min-Jung Kim",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 4 6 6 8 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
            "content": "Junha Hyung1 Kinam Kim1 Susung Hong2 Min-Jung Kim1 Jaegul Choo1 {sharpeeee, kinamplify, emjay73, jchoo}@kaist.ac.kr, susung@cs.washington.edu 1KAIST AI 2University of Washington close-up shot of butterfly landing on the nose of woman, close-up of womans face with colored powder exploding highlighting her smile and the details of the butterflys wings. around her, creating an abstract splash of vibrant hues. Figure 1. Visual comparison of video quality between CFG (top row) and our STG method (bottom row). Best viewed in Acrobat Reader; click on the images to watch the videos."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this * indicates equal contribution. work, we introduce Spatiotemporal Skip Guidance (STG), simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as 1 an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung. github.io/STGuidance/. 1. Introduction Diffusion models [11, 2830] are successful class of generative models known for their flexibility in modeling complex data distributions, achieving impressive results in image, video, and 3D generation. By progressively denoising random noise, they enable robust generalization, making them leading choice for realistic content generation and often surpassing GAN-based methods [5, 9, 18, 27]. Building on this success, video diffusion models [4, 12, 26, 31, 36] generate high-quality videos by using temporal or 3D attention layers to handle sequential frames. Meanwhile, to enhance sample quality, sampling guidance techniques such as Classifier-Free Guidance (CFG) [10] and Autoguidance [19] have been introduced to guide the denoising process. These techniques employ weak models to predict poor trajectories, steering the main model away from them and pushing samples toward highquality regions on the data manifold. However, CFG often reduces diversity, leading to saturated or overly simplified results [6, 19]. Autoguidance [19] addresses this issue by using weak model trained on the same task, conditioning, and data distribution as the main model. The main drawback of this approach, however, is the need to train an additional weak model, which is impractical for large-scale models. Alternative methods such as PAG [1] and SEG [13] use selfperturbation to implicitly mimic weak model, avoiding the need for extra training. Yet, these methods are designed specifically for image generation diffusion models, applying self-perturbation to 2D spatial attention maps. In this work, we propose Spatiotemporal Skip Guidance (STG), simple and effective sampling guidance method for video diffusion models that significantly enhances the performance of any transformer-based video diffusion model without additional training. Specifically, we use an implicit weak model for guidance through selfperturbation, eliminating the need for explicit weak models and their associated training costs. This is especially crucial for video diffusion models, where training costs are high. Our implicit weak model, key component of our framework, is deliberately designed as degraded but aligned version of the original video generation model. As demonstrated in Autoguidance [19], having the weak model share the same task, conditioning, and data distribution as the 2 Figure 2. Comparison between CFG and STG, with the band conceptually representing the noisy data manifold. In STG, the weak model and the main model are aligned along the direction of increasing quality. In contrast, the two models in CFG differ not only in quality but also in aspects such as diversity and prompt alignment capabilities. main model is essential for quality improvement, as we expect both models to exhibit similar, aligned errors in the same parts of samples. We conceptually illustrate this alignment in Fig. 2, where our weak model and main model are arranged in direction of increasing quality. To achieve this, we apply spatiotemporal perturbation to both spatial and temporal attention layersor, in the case of 3D attention, to the entire layerby selectively skipping certain layers. This straightforward approach effectively nullifies specific residual or attention layers, generating lower-quality version of the model that simulates an aligned weak model. To ensure samples remain on the data manifold, even when using large guidance scales, we employ optional techniques such as the rescaling [21] and restart [34] methods. Rescaling the latent code constrains its variance, addressing the issue of larger variance causing saturation in the results [21]. Meanwhile, the restart sampling technique leverages the error contraction property of forward SDEs [34] to keep the sampling trajectory on the manifold. These techniques help prevent overshooting in the sampling trajectory, which could otherwise push samples off the manifold and result in saturated or distorted outputs. Our key contributions are as follows: We propose STGa surprisingly simple sampling guidance framework that significantly boosts the performance of video diffusion models. Our method introduces an implicit weak model by skipping spatiotemporal layers in video diffusion models, eliminating the need for additional training or external models. Our method enhances sample quality during guidance without reducing diversity or limiting the dynamics of generated videos. 2. Related Work Guidance with trained weak model Classifier-Free Guidance (CFG) [10] improves conditional generation in Diffusion Models by using an implicit unconditional model as weak model. However, differences in tasks between the unconditional and conditional models can reduce sample diversity [6, 19] and increase sampling trajectory curvature [6], leading to overshooting the data manifold and producing skewed or oversaturated images. Autoguidance [19] mitigates these issues by employing bad version of the main model as weak model, trained with reduced capacity and compute to ensure alignment. This alignment allows the guidance algorithm to correct errors by analyzing prediction differences. While effective, it requires additional training, which is challenging for largescale video diffusion models. Guidance with training-free weak model Another line of work avoids additional training by using selfperturbation of the main model to mimic weak model. Self-Attention Guidance (SAG) [14] blurs high-attention regions, Perturbed Attention Guidance (PAG) [1] replaces attention maps with identity matrices, and Smoothed Energy Guidance (SEG) [13] applies Gaussian blur to the attention weights to smooth the energy landscape. These methods guide sampling toward high-quality outputs by leveraging differences in predictions from the weakened model. While effective, they are primarily designed for image diffusion models with 2D self-attention. We aim to extend this approach to video diffusion models, which require handling temporal dynamics with additional temporal or 3D spatiotemporal attention layers. 3. Preliminaries 3.1. Diffusion models Diffusion models generate samples by progressively removing noise from noisy data, restoring the original data distribution. Song et al. [30] defined the process of adding noise to the data using stochastic differential equation (SDE): dx = β(t) dt + (cid:112)β(t) dw, (1) where β(t) is time-dependent noise schedule, and represents the standard Wiener process. Corresponding reversetime SDE is: (cid:20) dx = β(t) 2 β(t)x log pt(x) (cid:21) dt + (cid:112)β(t) (2) where is the Wiener process running backward in time. Here, the score function log pt(x) is approximated by neural network sθ(x(t)) trained using denoising score matching [33]: θ = arg min θ Et (cid:8)λ(t)Ex0 Extx0 [ sθ(xt) xt log pt(xtx0)2 2 (cid:3)(cid:9) . (3) 3.2. Classifier Guidance Classifier Guidance (CG) [24] reformulates the reverse process of diffusion model by incorporating an external classifier pϕ as pθ(xty) pθ(xt)pϕ(yxt)λ, (4) where is the desired class label and λ controls the guidance strength. The score function is then derived as: xt log pθ(xty) = xt log pθ(xt) + λxt log pϕ(yxt). (5) By substituting the score function in Eq. 2 with Eq. 5, sampling from the desired class condition becomes possible. 3.3. Classifier-Free Guidance Classifier-Free Guidance (CFG) [10] uses Bayes rule to replace classifier-guided score with linear combination of conditional and unconditional score estimates: ϵλ θ (xt) = ϵθ(xt) + λ (ϵθ(xt) ϵθ(xtϕ)) . (6) CFG jointly trains the unconditional model ϵθ(xtϕ) and the conditional model ϵθ(xtc) (= ϵθ(xt)) within single model by setting the condition to null token ϕ. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs. 4. Method 4.1. Optimal Weak Model Design Our goal is to construct an aligned weak model that captures distribution similar to the original model while generating slightly lower-quality samples. This ensures that the guidance gradient points toward improved quality, as shown in Fig. 2. Misaligned models, as seen in CFG [10], may lead to unintended outcomes like reduced diversity, while Autoguidance [19] requires additional weak model training, making it impractical for large-scale models. To address these limitations, we design an implicit weak model by leveraging the original model itself, eliminating the need for external training. This training-free approach ensures computational efficiency and better alignment, as the weak model is derived directly from the original, preserving most network weights. To achieve this, we apply perturbation methods directly to the main models forward pass, creating an implicit weak 3 Models Imaging Quality Aesthetic Quality Motion Smoothness Dynamic Degree Temporal Flickering Mochi (CFG) Mochi (STG) Open-Sora (CFG) Open-Sora (STG) 0.524 0.628 0.561 0.606 0.507 0.554 0.493 0.509 0.985 0.988 0.982 0. 0.87 0.86 0.902 0.895 0.976 0.978 0.975 0.976 Table 1. Quantitative results for Mochi [31] and Open-Sora [36] on VBench [15] T2V benchmarks. Models FVD () IS Imaging Quality Aesthetic Quality Motion Smoothness Dynamic Degree SVD (CFG) SVD (STG) 151.3 128.7 38.0 38. 0.687 0.694 0.637 0.639 0.966 0.968 0.562 0.694 Table 2. Quantitative results for SVD [4] on FVD, IS, and VBench [15] I2V benchmarks. model. Specifically, we use spatiotemporal perturbation, key component for aligning the weak model with video diffusion models, which will be elaborated upon in detail. 4.2. Sampling from High Quality Samples Similar to CG [24], we define our goal as conditioning the model on an imaginary label yg that represents high-quality samples, leading to the sampling distribution pθ(xtyg) pθ(xt)pϕ(ygxt)w. (7) Using > 0 sharpens the distribution, promoting the generation of high-quality samples. From Eq. 7, the score is derived as xt log pθ(xtyg) = xt log pθ(xt) + wxt log pϕ(ygxt). (8) Rather than using an external classifier pϕ, we propose using our model pθ as an implicit classifier. We design this classifier to be inversely proportional to the probability of an imaginary bad label yb, expressed as pϕ(ygxt) 1 pθ(ybxt) . (9) Using Bayes rule, the function can be expressed in terms of the marginal posterior as follows: xt log 1 pθ(ybxt) = xt log pθ(xt) pθ(yb)pθ(xtyb) = xt(log pθ(xt) log pθ(xtyb)), (10) leading to the score of the target distribution as: xt log pθ(xtyg) = xt log pθ(xt) + wxt(log pθ(xt) log pθ(xtyb)). (11) We can sample from pθ(xtyg) by substituting the score function in Eq. 2 with Eq. 11, resulting in: (cid:20) dx = β(t) β(t)(xt log pt(xt) (12) + wxt(log pθ(xt) log pθ(xtyb)) (cid:21) dt + (cid:112)β(t)d w, and solving the reverse SDE. Since the score function is approximated using the neural network, we can generate samples using θ (xt) = ϵθ(xt) + w(ϵθ(xt) ϵb ϵw θ(xt)). (13) is An here approach interesting to model xt log pθ(xtyb) by perturbing the forward pass of ϵθ(xt), denoted as ϵb θ(xt). Now the main focus is designing perturbation that effectively yields the weak model capable of estimating ϵb θ(xt), aligning closely with ϵθ(xt), as discussed in Sec. 4.1. Ideally, we want distribution that deviates minimally from ϵθ(xt) while producing slightly lower-quality samples. 4.3. Spatiotemporal Skip Guidance (STG) We introduce Spatiotemporal Skip Guidance (STG), simple guidance method designed for video diffusion models that generates diverse, high-fidelity samples while addressing the limitations of existing methods. STG implicitly simulates an aligned weak model through spatiotemporal perturbation, capturing the spatiotemporal dynamics of video data. An interesting discovery of this paper is that skipping layers within the network is an effective approach for constructing an aligned weak model. Modern neural network architectures for video diffusion models, such as ADM [24], DiT [25], and SiT [23], are partially or fully transformerbased and contain attention layers and residual blocks. These architectures are well-suited for layer skipping, as macro cinematography animation showing butterfly emerging from its chrysalis, filmed with side-lit lighting ... Figure 3. Selected frames from videos generated by Mochi [31] with increasing STG scales. This reduces out-of-distribution issues in consecutive layers, generating perturbed but aligned samples. Attention skip Self-attention computes linear combination of value tensors as SA(Q, K, ) = Softmax (cid:19) (cid:18) QK = AV, (16) where R(hwf )d, R(hwf )d, R(hwf )d are the query, key, and value matrices, respectively. Here, h, w, , and represent the height, width, frame number, and channel dimensions. We can skip this layer partially by passing the value matrix directly to the next layer without computing its linear combination. This is equivalent to replacing the attention matrix with an identity matrix Rhwf hwf , resulting in SA (Q, K, ) = IV. (17) This represents 3D extension of PAG [1]. Factorized attention While recent models like Movie Gen [26] and Mochi [31] utilize full 3D spatiotemporal attention layers, many models, such as SVD [4] and OpenSora [36], still use factorized attention layers for efficiency. These models employ sequential 2D spatial attention and 1D temporal attention to approximate 3D spatiotemporal attention. For factorized models, we apply skip perturbation of Eq. 17 to spatial and temporal layers separately and use their linear combination for the final guidance. We denote the spatial and temporal perturbation labels In practice, ysb and ytb are as ysb and ytb, respectively. rarely independent, as they can influence each other. For instance, random spatial perturbations that create varied color adjustments across frames may disrupt temporal continuity. Similarly, random temporal perturbations can affect spatial consistency, leading to distortions in specific frames. However, for our skip perturbations, we can loosely assume independence, as skipping layers primarily reduces details in residual networks, which may not inherently affect temporal consistency. Therefore, to simplify the derivation, we initially assume independence between spatial and temporal perturbations. Figure 4. Comparison of CFG and STG across varying scales in terms of Imaging Quality and FVD. they can still produce plausible outputs without significant degradation, even when few layers are removed. Layer skipping is advantageous for generating an aligned weak model because it retains most of the neural networks weights and forward pass, resulting in similar predictions and distributions. This approach offers clear advantage over methods that rely on external models, such as Autoguidance [19], or alternative objectives like CFG [10], where forward passes differ significantly. Moreover, our method applies perturbations to both spatial and temporal layers (or spatiotemporal layers), unlike existing image-based perturbation methods [13, 14] limited to 2D spatial attention layers. This dual-layer perturbation is essential for aligning the weak model with the main video diffusion model. We now discuss various STG configurations that can be applied across different video diffusion model architectures. Residual skip To skip an entire residual block, we modify Res(zl) to Res (zl), Res(zl) = zl+1 = fl(zl) + zl, Res (zl) = zl+1 = zl, (14) (15) where zl and zl+1 represent the features at the lth and (l + 1)th layers, respectively, and fl denotes the lth neural net layer. The residual layers, which add small residuals fl(zl) to the original feature zl, ensure that the perturbed zl+1 does not deviate significantly from the original zl+1. 5 (1) romantic scene of couple dancing under string lights in backyard, with warm, golden tones highlighting their laughter. (2) An animation showing floating castle drifting above the clouds, with birds flying around it and sunlight casting golden rays ... (3) realistic documentary-style video of artisans crafting pottery, with the scene unfolding and transforming as hands shape clay ... (4) ghost in white bedsheet faces mirror. The ghosts reflection can be seen in the mirror. The ghost is in dusty attic ... Figure 5. Qualitative comparison between CFG and STG on videos generated by Mochi [31]. We will revisit this assumption and propose an adjustment afterward. Under this independence assumption, the joint distribution can be expressed as 5. Experiments 5.1. Overview pθ(xtyb) = pθ(xtysb)pθ(xtytb). (18) Following the same approach as in Eq. 9 and using Eq. 11, we modify the score in Eq. 8 as follows: xt logpθ(xtyg) = xt log pθ(xt) + wxt(log pθ(xt) log pθ(xtys)) + wxt(log pθ(xt) log pθ(xtyt)). (19) By replacing the score with the estimated denoiser and utilizing separate scales for spatial and temporal terms, we obtain the following equation: ϵw θ (xt) = ϵθ(xt) + w1(ϵθ(xt) ϵs + w2(ϵθ(xt) ϵt θ(xt)) θ(xt)) (20) where ϵs perturbed models, respectively. θ(xt) and ϵt θ(xt) represent spatially and temporally Next, we revisit our assumption of independence between spatial and temporal perturbations. While Eq. 20 works well in practice, we can derive an alternative STG formulation that uses orthogonalization to isolate the independent components of the spatial and temporal guidance. Inspired by negative prompting technique [3], we can modify the Eq. 20 as follows: ϵw θ (xt) = ϵθ(xt) + w1s (cid:18) + w2 (cid:19) s, s2 , (21) where = ϵθ(xt) ϵs θ(xt) and = ϵθ(xt) ϵt θ(xt). Manifold Constrained Guidance Even with wellaligned weak model, larger guidance scales can drive samples off the data manifold, resulting in poor quality and oversaturation. To address error accumulation at high guidance scales, we explore optional techniques to keep samples constrained to the manifold. Rescaling the latent code [21] helps mitigate this issue by constraining its variance, as larger variance is known to cause saturation in the results. Additionally, Restart sampling [34] demonstrates that introducing stochasticity can correct off-manifold deviations. Building on this idea, we incorporate stochastic forward processes into our sampling guidance framework as an optional method. While this approach modestly improves final sample quality and reduces saturation, it introduces additional computational overhead. Further details are provided in Appendix A3.1. We employ three models for our experiments: Mochi [31] is text-to-video model built on AsymmDiT blocks, containing total of 10 billion parameters and utilizing 3D self-attention in its spatiotemporal layers. Open-Sora [36] is text-to-video model built on STDiT blocks with 1.1 billion parameters, employing factorized spatial and temporal attention layers. SVD [4] is an image-to-video model with 1.5 billion parameters that leverages factorized spatial and temporal attention within UNet architecture. We evaluate the proposed method using widely adopted datasets and metrics to ensure comprehensive performance analysis. UCF-101 The UCF-101 dataset comprises 13,320 videos organized into 101 action classes. Using this dataset, we assess the Frechet Video Distance (FVD) and Inception Score (IS) of the image-to-video model SVD [4]. Following DIGAN [35], we calculate FVD and IS on 2,048 and 10,000 samples, respectively. For conditioning, initial frames from UCF-101 videos are used as inputs to the SVD model, with each input frame serving as the starting frame of the generated videos. VBench We use VBench [15] for automatic evaluation across various video metrics. SVD is evaluated using the image-to-video (I2V) framework on 355 samples from the VBench I2V dataset with 5 random seeds. Mochi and Open-Sora are evaluated with the text-to-video (T2V) framework: Open-Sora uses the standard VBench prompt list, while Mochi uses 100 randomly selected prompts due to computational limits. EvalCrafter We perform human evaluations using 700 prompts from the EvalCrafter dataset [22]. LLM-Generated Prompts We use set of 100 selected prompts generated by Claude 3.5 Sonnet [2] for our demo and qualitative comparisons. For evaluation, CFG is used together with STG for our evaluation. We did not use Restart sampling or guidance orthogonalization; additional results with these techniques are in the Appendix sections A3.1 and A3.2. 5.2. Results Qualitative Comparison Fig. 5 compares CFG and our method. Videos from the naive CFG model often show blurry objects with indistinct shapes, while STG produces clearer, more vivid frames with sharper image quality. STG also reduces temporal inconsistency and flickering, especially in dynamic videos with large motion where CFG frequently fails. Spatial guidance enhances object structure, and temporal guidance improves consistency. Please refer to the Appendix sections A3 and A4 for additional results. Models FVD () IS Imaging Quality Aesthetic Quality Motion Smoothness Dynamic Degree Temporal Flickering Mochi (STG-R) Mochi (STG-A) Open-Sora (STG-R) Open-Sora (STG-A) - - - - - - - - SVD (STG-R) SVD (STG-A) 155.9 128.7 39.3 38.5 0.628 0.555 0.550 0. 0.687 0.694 0.554 0.541 0.474 0.509 0.637 0.639 0.988 0.987 0.981 0. 0.965 0.968 0.86 0.86 0.894 0.895 0.641 0.694 0.978 0.976 0.977 0. - - Table 3. Comparison of STG-R (residual skip) and STG-A (attention skip) across Mochi [31], Open-Sora [36], and SVD [4]. STG-R shows stronger performance on Mochi, while STG-A yields better results on Open-Sora and SVD. Models FVD () IS Imaging Quality Aesthetic Quality Motion Smoothness Dynamic Degree CFG + Spatial + Temporal 151.3 133.8 128.7 38.0 38.3 38.5 0.687 0.691 0.694 0.637 0.639 0.638 0.966 0.967 0. 0.562 0.659 0.694 Table 4. Ablation study results on SVD [4] factorized attention, showing the impact of adding spatial and temporal guidance. Quantitative Comparison We compare CFG and STG using the FVD-Imaging Quality (VBench) plot across different scales in Fig. 4. FVD measures video distribution, while Imaging Quality assesses frame clarity. Higher CFG scales improve Imaging Quality but reduce diversity, as reflected in higher FVD. STG avoids this trade-off, maintaining diversity at increased scales. T2V and I2V VBench metrics in Tab.1 and Tab.2 show notable improvements in frame-level quality (Imaging and Aesthetic Quality). Temporal quality improves qualitatively, though metrics like Motion Smoothness and Temporal Flickering show marginal gains, as these scores are near saturation ( 0.9x). For Dynamic Degree, we aim to keep it unchanged, as it may not correlate with video quality. This is achieved in the T2V metric, but in the I2V model, CFG increases the influence of the conditioned image, reducing motion. STG, while not directly affecting Dynamic Degree, mitigates CFGs impact, leading to increased Dynamic Degree in I2V when used together. Human Evaluation We provide human evaluation results on 700 prompts from the EvalCrafter dataset [22] in the Appendix A2. 5.3. Ablation Study Fig. 3 displays selected frames from videos generated by Mochi using various STG scales, where = 0 corresponds to the CFG-only model without STG. Increasing the STG scale results in more vivid colors and finer details compared to the monotonous colors at = 0. Notably, unlike with CFG, sampling diversity is preserved as the STG scale increases, as shown in Fig. 4. We performed an ablation study on SVD to evaluate spatial and temporal guidance (Tab. 4). Spatial guidance notably reduced FVD and improved metrics, while temporal guidance further boosted performance, confirming their combined effectiveness for spatiotemporal generation. We also test two STG variantsresidual skip (STGR) and attention skip (STG-A)-on SVD, Open-Sora, and Mochi. As shown in Tab. 3, STG-R performs well with Mochi, while STG-A is more effective for the other models. This is likely due to Mochis higher parameter count and layer depth, enabling more extensive skipping without triggering out-of-distribution (OOD) issues in consecutive layers. Furthermore, Mochis spatiotemporal layers consist of single 3D attention layer, so STG-R skips residual In contrast, SVD and layer with just one attention layer. Open-Sora use factorized attention, meaning STG-R skips residual layer with two attention layers, which may cause excessive perturbation and lead to OOD issues in subsequent layers. More results are available in the Appendix A3. 6. Conclusion We proposed Spatiotemporal Skip Guidance (STG), simple, training-free method for video diffusion models. By simulating an aligned weak model via spatiotemporal skipping, STG offers strong guidance for high-fidelity video generation, achieving notable qualitative and quantitative improvements. We hope this work advances video diffusion models and inspires further research in the field. Limitation and Ethical Considerations STGs performance depends on scale and layer selection, with the optimal configuration varying across models, requiring users to set these through heuristic tuning. While video quality improvements are notable, they also raise ethical concerns about misuse, underscoring the importance of using this technology responsibly and constructively."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance. arXiv preprint arXiv:2403.17377, 2024. 2, 3, 5, 6 [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku. In -, 2024. 7 [3] Mohammadreza Armandpour, Ali Sadeghian, Huangjie Zheng, Amir Sadeghian, and Mingyuan Zhou. Re-imagine the negative prompt algorithm: Transform 2d diffusion into arXiv preprint 3d, alleviate janus problem and beyond. arXiv:2304.04968, 2023. 7 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 4, 5, 7, 8, 1, 6, 10, 12, 13, 16, 17, 22 [5] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [6] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv preprint arXiv:2406.08070, 2024. 2, 3 [7] Zihan Ding, Xiao-Yang Liu, Miao Yin, and Linghe Kong. Tgan: Deep tensor generative adversarial nets for large image generation. arXiv preprint arXiv:1901.09953, 2019. 1 [8] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, JunYan Zhu, and Jia-Bin Huang. On the content bias in frechet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7277 7288, 2024. 1 [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 2 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 5 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [13] Susung Hong. Smoothed energy guidance: Guiding diffusion models with reduced energy curvature of attention. arXiv preprint arXiv:2408.00760, 2024. 2, 3, 5, 7, 17 [14] Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7462 7471, 2023. 3, 5 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 4, 7 [16] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia, pages 675678, 2014. 1 [17] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video In Proclassification with convolutional neural networks. ceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 17251732, 2014. [18] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of ing the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. 2 [19] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. 2, 3, 5 [20] Black Forest Labs. Flux: Github repository. https: //github.com/black-forest-labs/flux, 2024. Accessed: 2024-11-22. 4 [21] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 54045411, 2024. 2, 7, 5, 9 [22] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. 7, 8, 4 [23] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. [24] Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Srinidhi Hegde, Tianyi Zhou, and Abhinav Shrivastava. Diffusion models beat gans on image classification. arXiv preprint arXiv:2307.08702, 2023. 3, 4 [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 4 [26] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 5 9 [27] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 2 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [29] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. [30] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2, [31] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2, 4, 5, 6, 7, 8, 1, 9, 14, 18, 19 [32] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 44894497, 2015. 1 [33] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):1661 1674, 2011. 3 [34] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems, 36:7680676838, 2023. 2, 7, 5 [35] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. 7 [36] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. https://github.com/hpcaitech/Open-Sora, 2024. 2, 4, 5, 7, 8, 1, 11, 15, 20, 21 A1. Experimental Details A1.1. Sampling Algorithm θ : Main model and spatiotemporally perturbed model respectively. Algorithm 1: Spatiotemporal Skip Guidance (STG) Input: ϵθ, ϵs,t w: Spatiotemporal guidance scale. Σt: Variance at step t. Output: Generated video Vout. 1 xT (0, I) 2 for T, 1, . . . , 1 do ϵs,t ϵt ϵθ(xt) ϵs,t θ (xt) ϵt ϵt + w(ϵt ϵs,t ) xt1 (cid:16) (cid:16) 1 αt xt 1αt 1 αt ϵt (cid:17) (cid:17) , Σt 4 return Vout Algorithm 2: Spatiotemporal Skip Guidance (STG) for factorized attention Input: ϵθ, ϵs θ, ϵt w1, w2: Guidance scales. Σt: Variance at step t. Output: Generated video Vout. θ: Main model, spatially perturbed, and temporally perturbed models respectively. 1 xT (0, I) 2 for T, 1, . . . , 1 do ϵt ϵθ(xt) ϵs 3 (cid:16) 1 xt1 ϵs (cid:16) αt θ(xt) ϵt xt 1αt 1 αt θ(xt) ϵt ϵt + w1(ϵt ϵs ϵt (cid:17) ϵt , Σt (cid:17) ) + w2(ϵt ϵt t) 4 return Vout A1.2. Computational Resources For evaluation, we utilized an NVIDIA A100 40GB GPU for SVD [4], while Open-Sora [36] and Mochi [31] were evaluated using NVIDIA H100 or A100 80GB GPUs. A1.3. Implementation Details The default model scales for CFG are as follows: SVD [4] uses scale of 3.0, Open-Sora [36] uses 7.0, and Mochi [31] uses 4.5. For STG, the configurations vary, with STG-A using scale of 2.0 and STG-R using 1.0. STG is applied to the 8th layer of SVD, which has total of 16 layers, and the 12th layer of Open-Sora, which has total of 28 layers. For Mochi, which has 48 layers in total, STG is applied at the 35th layer. A1.4. Metrics To evaluate model performance across different datasets, several methodologies were employed. FVD was assessed using the VideoMAE [8] model. IS was evaluated with the C3D model [16, 17, 32], following the setup of TGAN [7]. For VBench - Imaging Quality, the MUSIQ image quality predictor, trained on the SPAQ dataset, was used. VBench - Aesthetic Quality was measured using the LAION aesthetic predictor, applied to individual video frames. VBench - Dynamic Degree was evaluated with the RAFT flow estimator to quantify the degree of dynamics. VBench - Motion Smoothness was calculated as the mean absolute error (MAE) between dropped and reconstructed frames using video frame interpolation model. Finally, VBench - Temporal Flickering was assessed by generating static frames and computing the mean absolute difference between consecutive frames. 1 A1.5. Prompts Used EvalCrafter prompt 1. 2 Dog and whale, ocean adventure 2. Teddy bear and 3 real bear 3. Goldfish in glass 4. small bird sits atop blooming flower stem. 5. fluffy teddy bear sits on bed of soft pillows surrounded by childrens toys. 6. peaceful cow grazing in green field under the clear blue sky. 7. Unicorn sliding on rainbow 8. Four godzillas 9. fluffy grey and white cat is lazily stretched out on sunny window sill, enjoying nap after long day of lounging. 10. curious cat peers from the window, watching the world outside. 11. horse 12. pig 13. squirrel 14. bird 15. zebra 16. Two elephants are playing on the beach and enjoying delicious beef stroganoff meal. 17. Two fish eating spaghetti on subway 18. pod of dolphins gracefully swim and jump in the ocean. 19. peaceful cow grazing in green field under the clear blue sky. 20. cute and chubby giant panda is enjoying bamboo meal in lush forest. The panda is relaxed and content as it eats, and occasionally stops to scratch its ear with its paw. 21. Dragon flying over the city at night 22. Pikachu snowboarding 23. cat drinking beer 24. dog wearing VR goggles on boat 25. giraffe eating an apple 26. Five camels walking in the desert 27. Mickey Mouse is dancing on white background 28. happy pig rolling in the mud on sunny day. 29. In an African savanna, majestic lion is prancing behind small timid rabbit. The rabbit tried to run away, but the lion catches up easily. 30. 3 sheep enjoying spaghetti together 31. photo of Corgi dog riding bike in Times Square. It is wearing sunglasses and beach hat. 32. pod of dolphins gracefully swim and jump in the ocean. 33. In the lush forest, tiger is wandering around with vigilant gaze while the birds chirp and monkeys play. 34. The teddy bear and rabbit were snuggled up together. The teddy bear was hugging the rabbit, and the rabbit was nuzzled up to the teddy bears soft fur. 35. slithering snake moves through the lush green grass. 36. pair of bright green tree frogs cling to branch in vibrant tropical rainforest. 37. Four fluffy white Persian kittens snuggle together in cozy basket by the fireplace. 38. Eight fluffy yellow ducklings waddle behind their mother, exploring the edge of pond. 39. family of four fluffy, blue penguins waddled along the icy shore. 40. Two white swans gracefully swam in the serene lake. 41. In small forest, colorful bird was flying around gracefully. Its shiny feathers reflected the sun rays, creating beautiful sight. 42. spider spins web, weaving intricate patterns with its silk. 43. ... 2 Curated prompt for demos 1. Sloth with pink sunglasses lays on donut float in pool. The sloth is holding tropical drink. The world is tropical. The sunlight casts shadow. 2. vibrant, top-down video of kayak gliding through multicolored waters, showcasing shifting hues from blue to red to illustrate varying flow speeds or temperatures. The paddle interacts with the water, creating dynamic ripples and currents. 3. slow-motion capture of beautiful woman in flowing dress spinning in field of sunflowers, with petals swirling around her. 4. An exotic video of rabbits on the moon making rice cakes under star-filled sky, with Earth visible in the background. 5. handsome man walking confidently through bustling city street at night, illuminated by neon lights and reflections in puddles. 6. close-up shot of butterfly landing on the nose of woman, highlighting her smile and the intricate details of the butterflys wings. 7. top-down video of table filled with colorful dishes from different cuisines, with hands reaching in to serve food and clinking glasses. 8. majestic birds-eye view of couple holding hands while walking along the shore of beach with sparkling turquoise waves. 9. surreal scene of forest where the leaves glow in neon colors, with person walking down path as fireflies dance around them. 10. drone shot of desert at sunset, where shadows stretch and shift, capturing lone traveler moving gracefully through the sand dunes. 11. close-up of beautiful womans face with colored powder exploding around her, creating an abstract splash of vibrant hues. 12. panoramic view of tropical waterfall surrounded by lush greenery, with rainbow forming in the mist. 13. whimsical video of floating lanterns being released into the sky over calm lake, with reflections on the water creating mirror effect. 14. time-lapse video of an artist painting mural on city wall, where each frame shows burst of color and detail. 15. An overhead video of koi fish swimming in pond with rippling water, with their scales reflecting shades of gold, orange, and white. 16. slow-motion clip of handsome person diving into crystal-clear ocean, with water splashing and bubbles forming intricate patterns. 17. fantastical scene of meadow where flowers bloom and change colors in sync with the music, with person dancing among them. 18. top-down video of hot air balloon festival, showing multicolored balloons lifting off and dotting the sky. 19. beautiful woman sitting by window as rain drizzles down, creating streaks and patterns on the glass. 20. cinematic shot of person walking through field of lavender during golden hour, with the wind gently swaying the purple blossoms. 21. An exotic video of floating jellyfish in the ocean, their translucent bodies glowing with bioluminescence in shades of blue and purple. 22. playful video of puppies running across vibrant, flower-filled meadow, filmed in slow motion to capture their joyful expressions. 23. captivating aerial view of cityscape at sunrise, with skyscrapers casting long shadows and golden light reflecting on windows. 24. stunning slow-motion shot of bird taking flight over reflective lake, with water droplets glistening as they scatter. 25. romantic scene of couple dancing under string lights in backyard, with warm, golden tones highlighting their laughter. ... 26. 3 Prompt for figures in the main paper Fig2: macro cinematography animation showing butterfly emerging from its chrysalis, filmed with side-lit lighting to accentuate the texture of its wings. Fig4: 50mm lens shot of couple embracing under string lights as the camera slowly tracks them, capturing their shared laughter in soft, cinematic glow. An animation showing floating castle drifting above the clouds, with birds flying around it and sunlight casting golden rays, evoking the feeling of wonder seen in classic animations. realistic documentary-style video of artisans crafting pottery, with the scene unfolding and transforming as hands shape clay under diffused lighting. ghost in white bedsheet faces mirror. The ghosts reflection can be seen in the mirror. The ghost is in dusty attic, filled with old beams, cloth-covered furniture. The attic is reflected in the mirror. The light is cool and natural. The ghost dances in front of the mirror. A2. Human Evaluation We conducted user studies following EvalCrafter [22] to evaluate subjective opinions across five key aspects: (1) Video Quality, reflecting the clarity of the generated video, with higher scores indicating reduced blur, noise, or visual artifacts; (2) Text-Video Alignment, assessing the correspondence between the input text prompt and the generated video, particularly focusing on the accuracy of generated motions; (3) Motion Quality, evaluating the correctness and realism of the motions depicted in the video; (4) Temporal Consistency, measuring the frame-to-frame coherence, distinct from Motion Quality as it requires users to assess the smoothness of movement; and (5) Subjective Likeness, akin to an aesthetic score, where higher values signify better alignment with human preferences. For each metric, feedback was collected from seven users, who rated videos on scale from 1 to 5, with higher scores representing better alignment. To ensure fairness, the video sequences were randomly shuffled before being presented to users. We used 700 prompts from EvalCrafter for text-to-video (T2V) generation with Mochi [31]. Additionally, we employed FLUX.1 [dev] [20] to generate images from these prompts, which served as input to the image-to-video (I2V) model (SVD [4]). The results, shown in Fig. 6, demonstrate that incorporating STG leads to improved quality across all evaluated aspects. Figure 6. User study results for STG on SVD and Mochi, using 700 prompts from EvalCrafter [22]. For I2V generation of SVD, we employed FLUX.1 [dev] [20] to generate images from these prompts, which served as input to the model. The results demonstrate that incorporating STG leads to improved quality across all evaluated aspects. 4 A3. Ablation Study A3.1. Manifold Constrained Guidance As discussed in the main paper, sampling guidance techniques, including STG, utilize scale guidance, which can sometimes cause the sampling trajectory to deviate from the data manifold. This deviation is particularly noticeable when STG is applied with large scales or to videos that are already bright, often resulting in broken videos or over-saturation due to manifold overshooting. To mitigate these issues, we propose set of optional techniques that can serve as effective remedies. First, we leverage the error contraction property of stochastic processes [34] by incorporating stochastic forward processes into the sampling guidance framework. This technique, referred to as STG with Restart, is detailed in Algorithm 10. While this method moderately enhances the quality of the final samples and resolves issues such as broken videos (as illustrated in Fig. 7), it introduces additional computational overhead. Additionally, increased variance in the latent code [21] has been observed in over-saturated results. Consequently, oversaturation can be effectively mitigated using rescaling technique [21], which constrains the variance of the latent code. This method, referred to as STG with Rescaling, is detailed in Algorithm 6. As shown in Fig. 8, videos generated with larger variance (second row) often display saturated colors, which are successfully resolved by applying variance rescaling (third row). Unlike the Restart method, Rescaling introduces negligible computational overhead, making it the preferred approach for addressing over-saturation. θ : Main model and spatiotemporal perturbed model respectively. Algorithm 3: Spatiotemporal Skip Guidance with Restart Input: ϵθ, ϵs,t w: Spatiotemporal guidance scale. Σt: Variance at step t. tmin, tmax: Restart interval. K: Number of Restart iterations. Output: Generated video Vout. θ (xt) ϵt ϵt + w(ϵt ϵs,t ) xt1 (cid:16) (cid:16) 1 αt xt 1αt 1 αt ϵt (cid:17) (cid:17) , Σt 1 xT (0, I) 2 for T, 1, . . . , 1 do ϵt ϵθ(xt) ϵs,t ϵs,t if = tmin then 4 3 tmin xt1 x0 for 0, . . . , 1 do 5 6 7 8 9 ϵrestart (0, Σrestart) xk+1 xk tmax tmin for tmax, tmax 1, . . . , tmin do + ϵrestart ϵt ϵθ(xk+1 (cid:18) xk+1 t1 ) ϵs,t ϵs,t θ (xk+1 (cid:18) 1αt xk+1 1 αt αt 1 (cid:19) (cid:19) ϵt , Σt ) ϵt ϵt + w(ϵt ϵs,t ) 10 return Vout A3.2. STG with Orthogonalization As discussed in the main paper, for SVD and Open-Sora, which utilize factorized spatial and temporal attention, it is possible to orthogonalize spatial and temporal guidance. The detailed algorithm for this approach is provided in Algorithm 8. However, we do not implement orthogonalization in practice, as it does not demonstrate any performance improvement, as shown in Table 5. A3.3. Layer Ablation STG can be applied to different layers, and we conduct an ablation study to evaluate the impact of skipping various layers for STG on Mochi [31]. The results are presented in Fig. 9. Mochi consists of 48 layers in total, and we experimented with layer skipping at layers 30, 32, and 35. Our findings show that skipping later layers has more significant effect on quality θ : Main model and spatiotemporal perturbed model respectively. Algorithm 4: Spatiotemporal Skip Guidance (STG) with Rescaling Input: ϵθ, ϵs,t w: Spatiotemporal guidance scale. rescale: Rescaling factor. Σt: Variance at step t. Output: Generated video Vout. 3 1 xT (0, I) 2 for T, 1, . . . , 1 do ϵs,t ϵt ϵθ(xt) ϵs,t stdϵ std(ϵt) stdϵ std(ϵt) factor stdϵ stdϵ xt1 (cid:16) 1 , Σt xt 1αt 1 αt ϵt αt (cid:16) (cid:17) (cid:17) 5 4 θ (xt) ϵt ϵt + w(ϵt ϵs,t ) factor rescale factor + (1 rescale) ϵt ϵt factor 6 return Vout Models FVD () IS Imaging Quality Aesthetic Quality Motion Smoothness Dynamic Degree SVD (STG) SVD (STG-ORTH) 128.7 130.4 38.5 38. 0.694 0.691 0.639 0.637 0.968 0.967 0.694 0.692 Table 5. Ablation results of STG on SVD [4], evaluating the impact of orthogonalizing spatial and temporal guidance (STG-ORTH). Our findings show no performance gain from applying orthogonalization; therefore, we do not adopt it. Algorithm 5: Spatiotemporal Skip Guidance (STG) with Orthogonalization Input: ϵθ, ϵs θ, ϵt w1, w2: Guidance scales. Σt: Variance at step t. Output: Generated video Vout. θ: Main model, spatially perturbed, and temporally perturbed models respectively. 4 1 xT (0, I) 2 for T, 1, . . . , 1 do ϵt ϵθ(xt) ϵs 3 ϵt ϵs s,t s2 ϵt ϵt + w1s + w2 (cid:16) 1 xt 1αt xt1 1 αt ϵs ϵt θ(xt) ϵt ϵt ϵt αt (cid:16) 7 6 5 (cid:17) ϵt , Σt (cid:17) θ(xt) 8 return Vout improvements, as these layers are primarily responsible for refining texture details. Throughout all experiments in this paper, we consistently skip layer 35. A3.4. Effect of Spatial and Temporal Guidance For models with factorized attention layers, guidance can be applied separately to spatial and temporal layers. When using STG-A, it functions similarly to applying PAG [1] to the spatial attention layers, and we refer to this method as Spatial PAG (SPAG). When spatial guidance is applied alone, as shown for SVD in Fig. 10 and for Open-Sora in Fig. 11, the results struggle to maintain clarity during motion and exhibit poor temporal consistency. For instance, significant artifacts appear near the wings in the second row of Fig. 10, and around the legs of the chicken in Fig. 11. We further investigate the individual contributions of spatial and temporal guidance. In Fig. 12, we compare results with and without Spatial Guidance (SPAG). The results show that while CFG fails to maintain clear object structures, resulting in blurry videos, SPAG significantly enhances object structure and improves clarity. Similarly, in Fig. 13, we present results with and without Temporal Guidance (TPAG). The results reveal that CFG struggles to ensure frame-to-frame consistency, with the shape and color of the jelly varying noticeably across frames, leading to 6 disjointed video. In contrast, TPAG effectively preserves the jellys appearance throughout the sequence, creating more cohesive video and significantly improving Temporal Consistency. A3.5. Attention Skip and Residual Skip We compare the performance of attention skip (STG-A) and residual skip (STG-R) in Mochi [31] and Open-Sora [36]. The results for Mochi in Fig. 14 indicate that STG-R delivers greater qualitative improvements for Mochi. On the other hand, the results for Open-Sora in Fig. 15 and SVD in Fig. 16 demonstrate that STG-A delivers greater qualitative improvements for these models. Based on these findings, we use STG-A for Open-Sora and SVD, and use STG-R for Mochi in all experiments presented in the paper. A3.6. Weak Model Visualization We visualize the results of one-step prediction using different denoising methods (weak models, CFG, and STG) at timestep 30 in Fig. 18 and timestep 24 in Fig. 19, rows (a) to (e). Row (c) shows results denoised using the spatiotemporally perturbed model, ϵs,t θ (xt), which generally produces blurrier outcomes compared to row (b), where the unconditional weak model ϵθ(xtϕ) of CFG is applied. By moving away from the blurry weak model, STG achieves clear and well-defined structures with natural color tones. In contrast, CFG often produces unnatural color artifacts and broken structures. For example, the video predicted by CFG renders the girls arm on the left unnaturally red, the mans arm on the right unnaturally dark, and the trees and leaves in the background blurry. By comparison, STG consistently generates videos with enhanced structure and natural, well-balanced color tones. A3.7. Other Perturbation Methods In addition to SPAG (spatial perturbation using PAG), we explore other perturbation techniques. One such approach is SEG [13], which applies Gaussian blurring to the attention map. comparison of CFG, SEG, and STG is presented in Fig. 17. The results frequently show broken outputs in both CFG and SEG. In contrast, incorporating layer skipping alongside temporal perturbation, as in STG, consistently produces improved results. A4. Qualitative Comparison We provide additional qualitative comparisons using STG for SVD, Open-Sora, and Mochi. The results demonstrate that applying STG enhances the aesthetic appeal and fidelity of the videos, as shown in Fig. 20. In Open-Sora, we observe flickering artifacts frequently in the videos. By applying STG, these flickering artifacts are noticeably reduced, as illustrated in Fig. 21. For I2V models such as SVD, as discussed in the main paper, STG not only enhances the structural quality of the generated videos but also increases their dynamic degree. This is because STG mitigates the effect of CFG, which tends to force generated videos to rigidly adhere to the conditioning image. This effect is visualized in Fig. 22. We provide more video results in the zip file. 7 (Image condition is given for SVD.) Prompt: group of people sitting on green bench under an orange tree. Figure 7. Quality Improvement with Restart STG. Top: Results for SVD [4]. Bottom: Results for Mochi [31]. The results demonstrate that while STG occasionally fails to generate videos correctly in certain cases, applying Restart resolves these issues, producing high-quality and accurate outputs. 8 Prompt: young woman with glasses is jogging in the park wearing pink headband. Figure 8. Comparison of CFG, STG, and Rescaled STG on Mochi [31]. When STG is applied using large scales or to bright videos, it often suffers from over-saturation caused by manifold deviation. One potential cause of this issue is the increased variance in the latent code, which is effectively mitigated by the rescaling technique proposed in [21]. Prompt: close-up shot of butterfly landing on the nose of woman, highlighting her smile and the details of the butterflys wings. Figure 9. Ablation study on the effect of skipping different layers for STG on Mochi [31]. Our results indicate that skipping later layers has greater impact on quality improvements, as these layers primarily contribute to texture details. For all experiments, we consistently skip layer 35 (denoted as STG-l:35). (Image condition is given for SVD.) Figure 10. Qualitative Comparison of CFG, SPAG, and STG on SVD [4]. PAG applied only to spatial layers is referred to as SPAG. The results show that while CFG and SPAG fail to preserve object clarity under motion, STG successfully achieves this. 10 Prompt: Brown chicken hunting for its food. Figure 11. Qualitative Comparison of CFG, SPAG, and STG on Open-Sora [36]. The results show CFG fails to generate the objects head accurately, and SPAG struggles with the legs, whereas STG successfully generates all components correctly. (Image condition is given for SVD.) Figure 12. Qualitative Comparison of Object Structure in SVD [4] with and without Spatial Guidance. Spatial Guidance is represented by SPAG, which applies PAG only to the spatial layer. The results indicate that while CFG struggles to maintain clear object structures, leading to blurry videos, SPAG effectively enhances object structure and improves clarity. 12 (Image condition is given for SVD.) Figure 13. Qualitative Comparison of Temporal Consistency in SVD [4] with and without Temporal Guidance (TPAG). The results reveal that CFG struggles to ensure frame-to-frame consistency, with the shape and color of the jelly varying noticeably across frames, leading to disjointed video. In contrast, TPAG effectively preserves the jellys appearance throughout the sequence, creating more cohesive video and significantly improving Temporal Consistency. Prompt: close-up shot of butterfly landing on the nose of woman, highlighting her smile and the details of the butterflys wings. Prompt: Cinematic 8k scene of couple dancing under warmly glowing string lights in an intimate backyard setting, ... Figure 14. Comparison of attention skip (STG-A) and residual skip (STG-R) in Mochi [31]. The results indicate that STG-R delivers greater qualitative improvements for Mochi. 14 Prompt: close-up portrait of woman set against snowy backdrop. The woman is wearing golden crown... Prompt: moment of woman in white wedding dress, adorned with pearl necklace and veil, standing... Figure 15. Comparison of attention skip (STG-A) and residual skip (STG-R) in Open-Sora [36]. The results indicate that STG-A delivers greater qualitative improvements for Open-Sora. 15 (Image condition is given for SVD.) (Image condition is given for SVD.) Figure 16. Comparison of attention skip (STG-A) and residual skip (STG-R) in SVD [4]. The results indicate that STG-A delivers greater qualitative improvements for SVD. (Image condition is given for SVD.) Figure 17. Comparison of CFG, SEG [13], and STG in SVD [4]. The results show that CFG and SEG generate an unnatural nose for the person, whereas STG successfully generates all components naturally. 17 Prompt: family having picnic under shady tree in large park. Figure 18. Weak model visualization for Mochi [31]. Generated video using one-step prediction from timestep 30 (t = 30). (a) ϵθ(xt), (b) ϵθ(xtϕ), (c) ϵs,t θ (xt), (d) CFG, (e) STG (f) Final video (CFG) (g) Final video (STG). The video predicted by CFG exhibits unnatural colors in certain areas and broken structures. In contrast, the video generated with STG demonstrates improved structural integrity and more natural color tones. Prompt: family having picnic under shady tree in large park. Figure 19. Weak model visualization for Mochi [31]. Video generated using one-step prediction from timestep 24 (t = 24). (a) ϵθ(xt), (b) ϵθ(xtϕ), (c) ϵs,t θ (xt), (d) CFG, (e) STG (f) Final video (CFG) (g) Final video (STG). The result demonstrates that STG effectively guides the model to maintain structural integrity and realistic color distribution while avoiding the unintended artifacts present in CFG predictions. 19 Prompt: neon-lit cityscape at night, featuring towering skyscrapers and crowded streets. The streets are bustling... Prompt: fluffy grey and white cat is lazily stretched out on sunny window sill, enjoying nap after long day of lounging. Prompt: Iron Man is walking towards the camera in the rain at night, with lot of fog behind him. Science fiction movie, close-up. Figure 20. Qualitative comparison of video quality with and without STG applied on Open-Sora [36]. The results demonstrate that applying STG enhances the videos aesthetic appeal and fidelity. 20 Prompt: dog wearing vr goggles on boat. Prompt: cyborg standing on top of skyscraper, overseeing the city, back view, cyberpunk vibe, 2077, NYC, intricate details, 4K. Figure 21. Qualitative comparison of Temporal Flickering in Open-Sora [36] with and without STG. Without STG, temporal flickering is observed around the object, causing sudden bright flashes that disrupt the video experience. STG significantly reduces these artifacts, resulting in smoother and more cohesive motion. (Image condition is given for SVD.) (Image condition is given for SVD.) Figure 22. Qualitative Comparison of Dynamic Degree in SVD [4] with and without STG. The results show CFG results in limited object motion, whereas STG mitigates the restrictive effects of CFG, effectively enhancing the motion."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "University of Washington"
    ]
}