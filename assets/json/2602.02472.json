{
    "paper_title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
    "authors": [
        "Qifan Yu",
        "Xinyu Ma",
        "Zhijian Zhuo",
        "Minrui Wang",
        "Deyi Liu",
        "Shiyi Zhan",
        "Yiyuan Ma",
        "Liang Xiang",
        "Xingyan Bin",
        "Di He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\\times$ width expansion."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 2 7 4 2 0 . 2 0 6 2 : r SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning Qifan Yu1,2, , Xinyu Ma1, Zhijian Zhuo1, Minrui Wang1, Deyi Liu1, Shiyi Zhan1, Yiyuan Ma1, Liang Xiang1, Xingyan Bin1, , Di He2, 1ByteDance Seed, 2Peking University Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing Signal Preservation And symmetRy breaKing for width-progressive LearnING), novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35 % under 2 width expansion. Date: February 3, 2026 Correspondence: Qifan Yu at qifanyu@stu.pku.edu.cn, Xingyan Bin at binxingyan@bytedance.com, Di He at di_he@pku.edu.cn"
        },
        {
            "title": "Introduction",
            "content": "Training Large Language Models (LLMs) remains prohibitively expensive, motivating growing line of work on Progressive Learning (PL) [10, 17, 33], which aims to expand the parameter scale gradually during training instead of training the full scale from scratch [12]. Existing PL methods have demonstrated notable success in both saving training computation and performance enhancement, especially through depth-oriented strategies such as layer stacking [10, 12, 17], block insertion [33, 39], or gradual network growth [37]. Width is another crucial dimension for scaling model parameters [16]. Existing studies have made only limited progress in this direction, and general and systematic mechanism has yet to be established [5, 6, 4042, 45]. More importantly, previous investigations have been largely limited to expansion during the initial portion of training, e.g., less than 1030 % of training tokens [10, 28]. Such early expansion offers negligible computational advantages over training the target-width model from scratch and fundamentally undermines the primary motivation of PLreducing training costs. To make PL practically viable, width expansion must be conducted 1 during the intermediate stage of pre-training (i.e., mid-stage expansion). However, it is precisely in this regime that width expansion becomes challenging. We hereby analyze the core challenges as follows. We identify the first core challenge as signal preservation during mid-stage expansion. Prior width-based PL work has largely treated preservation as loss continuity at the expansion point, i.e., function preservation (FP) where the expanded model is initialized to match the pre-expansion mapping [5, 6, 13, 28, 31]. While FP is useful, we argue that the core mechanism behind the scenes is whether the expansion preserves the statistical distribution of intermediate activations, most notably the root-mean-square (RMS) scale of hidden representations [44]. Concretely, RMS-scale mismatch alters layerwise signal magnitudes and propagates through residual streams; this destabilizes optimization even when no instantaneous loss spike occurs at the moment of expansion [1, 38]. Based on this perspective, we design RMS-preserving strategies for several standard initialization regimes copy, random, and zeroensuring each can be applied without inducing activation-scale shocks. Interestingly, these RMS-preserved variants reveal counter-intuitive limitation of copying: despite its appeal for function preservation, copy-based expansion can underperform compared to RMS-preserved random or zero initialization in terms of post-expansion recovery. This observation indicates that, beyond maintaining loss and activation scale, some additional and critical issues for copy-based expansion remain unresolved. This brings us to the second core challenge: copy-based expansion, while strongest in forward continuity, induces backward symmetry [6]. Duplicating channels creates duplicated parameter subspaces that receive identical gradients and thus evolve identically, leaving the new capacity functionally redundant [34]. Crucially, this symmetry is not an artifact of specific optimizer: it arises under both element-wise optimizers such as AdamW [20] as well as spectral-style updates such as Muon [15, 19], causing persistent coupled state in which copied components fail to diversify. Motivated by these observations, we frame mid-stage width expansion as balancing two complementary principles: Signal Preservation and Symmetry Breaking. On the preservation side, we enforce RMS-scale consistency during expansion, ensuring that the expanded model maintains stable hidden-state statistics and thereby supports smooth post-expansion optimization. On the symmetry side, we introduce targeted interventions that act only in the backward dynamics while leaving the copied forward function intact: (i) controlled optimizer-state reset for newly introduced parameters to remove inherited symmetric momentum, and (ii) an asymmetric learning rate re-warmup schedule that selectively stimulates the new parameters without globally perturbing the well-adapted pre-trained ones. Together, these mechanisms preserve forward continuity at the moment of expansion, while inducing sufficient asymmetry in subsequent updates for the expanded capacity to diverge and encode meaningful features. In summary, our contributions can be highlighted as follows: We investigate the challenges of width expansion during the critical mid-stage of pre-training, regime largely unexplored in prior work due to stability concerns. We identify that successful expansion hinges on two complementary principles: Signal Preservation to stabilize activation statistics, and Symmetry Breaking to resolve gradient coupling in copy-based initialization. We propose SPARKLING, practical framework that implements both principles through suite of concrete mechanismsincluding RMS-scale consistency, copy-based initialization, asymmetric optimizer state resetting, and asymmetric learning rate re-warmupthat jointly resolve the optimization challenges inherent to expanding deep within the pre-training trajectory. We empirically validate the generality of SPARKLING across multiple width axes (including hidden dimension and MoE expert intermediate dimension) and optimizer families (including AdamW and Muon). Under fixed token budget, our PL approach consistently outperforms training the full-scale model from scratch on downstream evaluations, while reducing training costs by up to 35 % when scaling to 2 width, demonstrating both effectiveness and efficiency of SPARKLING."
        },
        {
            "title": "2 Related Work",
            "content": "Progressive learning (PL) has emerged as resource-efficient paradigm that accelerates training by gradually expanding the architecture from small base model to target scale during training [5, 6, 12, 17]. From the perspective of depth expansion, existing strategies typically grow by stacking layers [10, 12, 17] or inserting blocks [33, 39]. Existing approaches for width expansion largely prioritize function preservation (FP) via parameter mapping [6], advanced initialization schemes like AKI and its variants [5, 40, 45], or temporarily masking new structures [41]. To address redundancy from simple copying [6], various heuristic interventions have been adopted, including uneven splitting [6, 10, 31] and symmetric perturbations [34, 35, 42]. Beyond initialization, significant effort has been directed toward stabilizing post-growth optimization dynamics: Wang et al. [31] advocate for accelerated decay schedules on the premise that expanded models start closer to local optima, while Yuan et al. [42] utilize weight-norm to rebalance gradient contributions and Shen et al. [28] propose dynamics-preserving growth operators to align the expanded models loss trajectory. Other methods attempt to learn growth operators [24, 30] or construct gradient-maximizing weights [11]. However, these strategies typically address either forward initialization or backward optimization dynamics in isolation. In this work, we innovatively establish systematic framework balancing both perspectives. We first argue that the mechanism underlying widely used function-preserving initializations is fundamentally RMS preservation, and then redesign the optimization procedures to address the symmetry issues that inevitably arise from such preservation-focused initialization strategies."
        },
        {
            "title": "3.1 Why RMS Mismatch Destabilizes Training\nWe start by defining the root-mean-square (RMS) magnitude of a vector h",
            "content": "Rd as [44] RMS(h) := 2 (cid:118) (cid:117) (cid:117) (cid:116) = 1 (cid:88) i=1 h2 . (1) Consider linear layer Rdoutdin, where and denote the input and output hidden states, respectively. Our focus is the RMS scale of the activations: = x, Rdin, Rdout . (2) := sout sin = RMS(y) RMS(x) , r(post) = r(pre), (3) requiring this scale to remain unchanged after expansion. We enforce the RMS invariance in Eq. (3) as signal preservation constraint. trained Transformer block implicitly defines an operating regime via its input-output statistics, within which representations are wellformed and features remain meaningful. If width expansion perturbs activation RMS during expansion, post-expansion hidden states can drift away from the pre-expansion scale manifold, causing subsequent blocks to receive out-of-regime inputs. RMS-preserving expansion mitigates this shift by keeping block-wise input/output magnitudes within the original domain, thereby maintaining the fidelity and generalization of the pre-trained function immediately after expansion. In modern LLMs that adopt pre-normalization (e.g., Qwen3 [36], DeepSeek-V3 [9], OLMoE [23]), RMSpreserving becomes even more critical because the update explicitly couples the residual stream with the branch output. Concretely, in typical residual block with pre-norm, the hidden state is updated as where ( ) denotes residual branch such as an attention or MLP sublayer, and Norm( feature normalization, i.e., LayerNorm or its variants used in LLMs, most notably RMSNorm [44]. ) refers to token-wise + (Norm(h)) , (4) 3 ) but does not constrain its output scale, so the residual dynamics Here, pre-norm stabilizes the input to ( depend on the ratio RMS(f (Norm(h)))/RMS(h). After expansion, an RMS mismatch shifts the calibrated mixing between the main path and the transformed branch, making it either overwhelming on the main stream or nearly identity. By preserving RMS through expansion, we keep layerwise dynamics coherent and maintain the balanced residual regime."
        },
        {
            "title": "3.2 RMS-Preserving Expansion\nWe continue to discuss RMS-preserving width growth in three cases: (i) fan-out expansion, which expands the\noutput dimension (dout), (ii) fan-in expansion, which expands the input dimension (din), and (iii) RMSNorm\nweight expansion, which widens the RMSNorm scale.",
            "content": "In practice, fan-out and fan-in expansions typically appear as paired transformation across two consecutive layers that share an intermediate width. For example, in an MLP that widens the expert intermediate dimension, the up and gate projections become fan-out expansions1, and the subsequent down projection becomes fan-in expansion. Similarly, in attention, the vhead projection and the output projection likewise form such paired transformation. Therefore, whenever we widen any width dimension, the two sides are naturally correlated and should be discussed jointly in pairs. 3.2.1 Preliminaries Under the linear layer defined in Eq. (2), the output activation RMS is given by RMS(y) = (cid:114) 1 dout 2 2 = (cid:118) (cid:117) (cid:117) (cid:116) 1 dout dout(cid:88) i= y2 . (5) Leveraging the property of high-dimensional isotropy in wide neural networks, where feature vectors tend toward asymptotic orthogonality [2, 27], we can assume that are identically distributed and satisfy E[yi] = 0. Taking expectation over the data yields dout i=1 yi } { E(cid:2)RMS2(y)(cid:3) = (cid:34) 1 dout dout(cid:88) i= (cid:35) y2 = E[y2 ] = Var(yi). (6) Therefore, when the input RMS sin is kept unchanged, preserving the output RMS scale is equivalent to preserving the per-coordinate variance where σ2 and σ2 wσ2 x, denote the shared variances of wij and xj, respectively. We derive Eq. (7) in Appendix A.1. Var(yi) = din σ2 (7) 3.2.2 Fan-out Expansion In fan-out expansion, the output dimension grows from dout to denoted by out (> dout) while keeping din unchanged, = x, = (cid:21) (cid:20)W Rd outdin, = (cid:21) (cid:20)y Rd out , = x. (8) (9) Naturally, fan-out expansion preserves activation RMS as long as the newly introduced output channels are distributionally consistent with the pre-expansion ones. Concretely, when the added rows are initialized by copying or by randomly sampling from the same distribution as the original weights, the expanded output remains distributionally aligned with and thus retains the same RMS scale. 1We thus treat the gate projection in the same way as the up projection under RMS-preserving expansion, and regard the resulting gate activation output as Θ(1) multiplicative factor in expectation."
        },
        {
            "title": "3.2.3 Fan-in Expansion\nIn fan-in expansion, the input dimension grows from din to d′\nin\nby",
            "content": "(> din) while keeping dout unchanged, denoted = x, = α (cid:2)W (cid:3) Rdoutd in, Rd in, = α(W + x). = (cid:21) (cid:20)x (10) (11) RMS-preserving fan-in expansion seeks the scaling factor α satisfying the invariance of Eq. (7) across expansion. Random or One-Side Copied. If the newly added fan-in coordinates are initialized by same-distribution random sampling, or by copying on only one side (i.e., only or only is copied while the other remains random), then Eq. (6) and its underlying assumptions continue to hold, resulting in shared per-coordinate variance after expansion: Var(y i) = in(cid:88) j=1 Var(w ijx j) = in(cid:88) j= wσ2 σ2 = in σ2 wσ2 x. With unchanged input scale σ2 = σ2 , variance preservation requires in σ2 w = din σ2 = σw = (cid:115) din in σw, which implies that the weights should be rescaled as ij = (cid:115) din in wij, = 1, . . . , dout, = 1, . . . , in, (12) (13) (14) thereby keeping Var(y i) = Var(yi) and the output RMS invariant under fan-in expansion. Both-Sides Copied. qualitatively different regime arises when both sides of the newly introduced fan-in coordinates are created by copying existing dimensions, where the independence across fan-in dimensions is violated. Let denote the copy ratio. 0 < 1 corresponds to the setting where each copied dimension is duplicated exactly once, while > 1 corresponds to that some dimensions may be copied multiple times. Generally, we have The invariance of Eq. (7) requires the weights be rescaled as in = (1 + c)din. ij = wij, (cid:40) 1 1+3c 1 1+c wij, 0 < 1, > 1, or equivalently, ij = (cid:113) din 3d wij, din in in2din wij, din < in in > 2din, 2din, (15) (16) (17) = 1, . . . , dout, = 1, . . . , in One-Side Zero. Empirically, we find that RMS-preserving expansion should treat the zero-initialized side as random rather than strictly loss preserving at the expansion moment, and we include detailed analysis in Appendix C. . We provide the full derivation of the above scaling factor in Appendix A.2."
        },
        {
            "title": "3.2.4 RMSNorm Weight Expansion",
            "content": "We next discuss how to expand the RMSNorm scale when widening the dimension. For RMSNorm parameterized by γ Rd, omitting the ϵ term for clarity, we have = RMSNorm(x; γ) = Applying Eq. (6) to yields γ RMS(x) , zi = xiγi RMS(x) . E[RMS2(z)] = Var(zi) ="
        },
        {
            "title": "1\nRMS2(x)",
            "content": "x σ2 σ2 γ σ2 γ, (18) (19) := Var(xi) and σ γ := Var(γi), and Eq. (6) along with its underlying assumptions is used for both where σ2 and z. Therefore, preserving the output RMS of under width expansion is effectively equivalent to preserving the RMS of parameter γ. Thus, when expanding RMSNorm from to > d, initializing the new coordinates of γ by copying or randomly sampling from the same distribution naturally maintains RMS(z) without any additional rescaling."
        },
        {
            "title": "3.3 RMS-Preserving Expansion Improves Late-Stage Convergence.",
            "content": "Figure 1 RMS-preserving rescaling consistently improves late-stage convergence under MoE expert innerdimension expansion. We expand the expert inner dimension from 512 1024 at 100B tokens and plot reference-loss (relative to the pre-expansion reference) over the remaining training tokens. (a)(e) sweep five (up_proj down_proj init) pairs. In every case, Naive Init, No Scaled yields smaller immediate loss gap, while RMS-Preserved Scaled overtakes later and converges to lower final loss. (f) compares the RMS-preserved late-stage results and highlights notable pattern: both-sides copied significantly underperforms other RMS-preserved strategies. Experimental Setup. We conduct progressive-learning experiments on OLMoE [22] with 0.5 active parameters and 2.5 total parameters, trained for 200 tokens in total using AdamW optimizer. We perform mid-stage width expansion at 100 tokens and then continue to train the expanded model for the remaining 100 tokens under the same training recipe. Details of the experimental setup are provided in Appendix B. , which doubles the MoE expert intermediate dimension We consider two width-growth axes: (i) Inner 2 , which doubles the model hidden size from 1024 to 20482. In each from 512 to 1024, and (ii) Hidden 2 setting, we compare two initialization strategies for the newly introduced channels: (1) Naive Init, No Scaled, 2We decouple hidden dimension from the usual constraint hidden_dim = qhead_num hidden-dimension; the head dimension and the numbers of QKV heads remain unchanged. head_dim and expand only 6 which applies copy/random/zero initialization without any rescaling, and (2) RMS-Preserved Scaled, which applies the rescaling derived in Sec. 3.2 to enforce activation RMS consistency at the expansion moment. Results. Figure 1 reports expert-inner expansion, enumerating five fan-out/fan-in initialization pairs within each expert MLP, denoted as up_proj down_proj init. Across all initializations, Naive Init, No Scaled consistently yields smaller immediate loss gap but worse late-stage convergence, whereas RMS-Preserved Scaled recovers steadily and converges to lower final loss. The hidden-dimension expansion counterpart in Appendix shows the same pattern. Overall, RMS-preserving expansion robustly improves late-stage convergence under both expert-inner and hidden-dimension growth across diverse initialization strategies. Figure 1(f ) aggregates late-stage performance across initialization pairs under RMS-Preserved Scaled. While broadly beneficial, the both-sides copied configuration still significantly underperforms the other RMS-preserved variants."
        },
        {
            "title": "4 Breaking the Symmetry Lock",
            "content": "The experimental results in Sec. 3.3 reveal counter-intuitive phenomenon: although the copy strategy strictly preserves the forward output at expansion, it consistently underperforms other RMS-preserved initializations, exhibiting both slower post-expansion recovery and higher eventual loss. Intuitively, copy-based initialization seems ideal, as it ensures seamless loss transition and thus the most stable starting point. We argue that the gap is instead governed by copy-induced backward-pass symmetry: duplicated components receive identical gradients and thus evolve identically, failing to diversify into distinct features and rendering the expanded capacity functionally redundant. We formally derive this mechanism in the following analysis."
        },
        {
            "title": "4.1\nConsider the linear layer in Eq. (2). We analyze gradient dynamics under 2\ninitialization.",
            "content": "width expansion with copy Fan-Out Expansion. Specializing Eq. (8) to copy initialization gives = and = [W , ], hence = [y, y]. If subsequent layers are also copied, back-propagation maintains symmetry: y = [g, g] with = y . The gradient w.r.t. the expanded weights is: = x = (cid:21) (cid:20)g = (cid:21) (cid:20)gx gx . (20) We provide the analogous analysis for fan-in expansion in Appendix A.3. holds, indicating identical gradients in copy expansion. With Symmetry Lock. In both cases, symmetrically initialized optimizer states, i.e., identical momentum for AdamW, the two blocks receive identical updates, enforcing (t) = (t) throughout training. This creates symmetry lock: despite increased parameters, the model remains in the original lower-dimensional subspace. The expanded neurons fail to learn distinct features, making width scaling inefficient unless the symmetry is explicitly broken. L = Orthogonalization Fails to Break Symmetry. Advanced optimizers like Muon attempt to decorrelate updates by applying Newton-Schulz orthogonalization to the matrix-valued momentum, yet this mechanism fails to break the symmetry under copy-based expansion. Importantly, this step is typically implemented as polynomial map of the Gram matrix: for matrix Xk, the next iterate can be written in the generic form Xk+1 = Xk ϕ(X Xk), (21) where ϕ( αI + βG + γG2 with appropriate coefficients [15]. ) is matrix polynomial corresponds to ϕ(G) = 1 2 (3I G) or higher-order variants ϕ(G) = 7 Consider the column-duplicated (fan-in) case where the momentum is initialized as X0 = [A0, A0]. Let Pk = Ak. Then the Gram matrix remains block-constant: Xk = (cid:21) (cid:20)A (cid:2)Ak, Ak (cid:3) = (cid:21) (cid:20)Pk Pk Pk Pk . Thus, applying the generic orthogonalization update yields Xk+1 = (cid:2)Ak, Ak (cid:3) ϕ (cid:21)(cid:19) (cid:18)(cid:20)Pk Pk Pk Pk := (cid:2)Ak+1, Ak+ (cid:3) . (22) (23) Since ϕ( ) preserves block-exchange symmetry, the update in Eq. (23) retains two identical column blocks. Therefore, the orthogonalization step cannot spontaneously break the symmetry lock induced by copy initialization."
        },
        {
            "title": "4.2.1 Optimizer State Reset as a Necessary Intervention",
            "content": "Copy-based expansion yields identical gradients for the original and duplicated parameters. If the optimizer states are also initialized symmetricallyeither by copying the existing states or by resetting all states to zerothe two halves receive identical updates, so the symmetry lock persists under both AdamW and Muon. To break this coupling symmetry without discarding the original models training signal, we enforce an asymmetric treatment: retaining the optimizer states for the original and resetting the states for the new parameters , the corresponding optimizer state matrix (representing both first and second momentum for AdamW and momentum for Muon) = [S, 0], (24) where is the pre-expansion state of and 0 initializes the state of . Figure 2 Optimizer-state handling under copy-based expansion. Symmetric treatments (Drop/Copy) exhibit symmetry lock, yielding slower recovery and higher loss. Our asymmetric reset avoids this bottleneck, while state scaling provides no additional gain. Experimental setup. Following Sec. 3.3, we study expert-inner expansion under copy-copy initialization and vary only the optimizer-state handling at expansion, while applying RMS-preserving parameter scaling in all variants to keep forward activation scales consistent. We compare four treatments: (i) Drop Opt., globally reset all states, (ii) Copy Opt., duplicate states, (iii) Asymmetric Reset, reset states only for new channels, and (iv) Asymmetric Reset + Scaled Opt., additionally applying the parameter scaling to optimizer states. The first two are symmetric, whereas the latter two break symmetry via optimizer dynamics. Results. Figure 2 reveals clear gap between symmetric and asymmetric treatments. Both symmetric baselines (Drop Opt. and Copy Opt.) underperform substantially, showing slower post-expansion recovery and higher converged loss, consistent with copy-induced backward symmetry that keeps duplicated components 8 tightly coupled. In contrast, Asymmetric Reset improves both recovery speed and final loss, indicating that resetting optimizer states only for the new channels suffices to break the symmetry lock and enable feature diversification. Notably, explicit optimizer-state scaling (Asymmetric Reset + Scaled Opt.) yields no additional gain, suggesting that strict state-parameter scale alignment is unnecessary and any initial mis-scaling can be quickly corrected by subsequent gradient updates."
        },
        {
            "title": "4.2.2 Asymmetric Learning Rate Re-warmup\nFor training from scratch, we use a standard cosine decay learning rate scheduler with linear warmup. Let Tw\ndenote the number of re-warmup steps, T the total number of steps, and let η0, ηmax and ηmin be the initial,\npeak and final learning rates, respectively. The baseline schedule is",
            "content": "η(t) = (t; Tw, T, η0, ηmax, ηmin) = η0 + (ηmax η0) Tw , ηmin + (ηmax ηmin) ψ < Tw, 0 , Tw T, (cid:19) (cid:18) T"
        },
        {
            "title": "Tw\nTw",
            "content": "ψ(x) = 1 2 (1 + cos(πx)) . (25) (26) where At an expansion point te, we keep the original parameters on the same baseline schedule to preserve continuity, i.e., η(t) = (t; Tw, T, η0, ηmax, ηmin) for all t. For the newly introduced parameters, we perform an asymmetric re-warmup that starts exactly from the current learning rate ηe = η(te) and warms up for τw steps to new peak learning rate proportional to ηe: where ρ is the rewarmup ratio. The learning rate for the new parameters is then defined as ˆηmax = ρ ηe, ηe = η(te), ηnew(t) = (t te; τw, te, ηe, ˆηmax, ηmin), > te, (27) (28) where, after rewarmup, the schedule follows the same cosine-decay regime and decays to the shared minimum learning rate ηmin. See Appendix for sample curve."
        },
        {
            "title": "4.3 Asymmetric Learning Rate Re-Warmup Further Improves Convergence Consis-",
            "content": "tently. Figure 3 Asymmetric re-warmup consistently improves convergence under mid-stage width expansion. Across Inner 2 , and joint expansion, re-warmup lowers the final loss for both RMS-preserved copy-copy and zero-copy. Copy-copy benefits most, achieving the best final loss, effectively mitigating copy-induced symmetry lock. , Hidden 2 Experimental setup. Following Sec. 3.3, we evaluate asymmetric learning rate re-warmup across different ), hidden-dimension (Hidden width-growth axes. We consider three expansion settings: expert-inner (Inner 9 & Inner 2 ), and joint (Hidden ). For all settings, we apply RMS-preserved scaling and asymmetric 2 optimizer-state resetting, then ablate re-warmup by comparing runs with vs. without it. We report both copy-copy and zero-copy initializations (the best-performing no-re-warmup setting in our earlier analysis, see Figure 1) to assess robustness. We set the re-warmup ratio ρ = 1.3 and the number of re-warmup steps τw = 250 based on Appendix F. Results. Figure 3 shows that asymmetric learning rate re-warmup consistently improves convergence across width axes and initialization strategies. Across all width axes in three settings, enabling re-warmup yields lower eventual loss under the same token budget. The benefit holds for both zero-copy, where new channels begin with near-zero forward contribution, and copy-copy, which strictly preserves the forward mapping. Notably, the gain is largest for copy-copy: re-warmup closes the post-expansion gap with zero-copy and reaches the lowest final loss among variants. Consistent with Sec. 4.1, beyond RMS preservation and asymmetric state reset, re-warmup adds controlled optimization asymmetry that encourages duplicated subspaces to diversify into effective capacity. We further verify SPARKLINGs generality across optimizer families (e.g., Muon) and superiority over heuristic baselines. See Appendix and for details. Overall, re-warmup is robust component of SPARKLING, reliably improving convergence across width-growth axes and initialization regimes."
        },
        {
            "title": "5 Discussions",
            "content": "Taken together, our SPARKLING framework comprises (i) RMS-preserved scaling, (ii) copy-based initialization, (iii) asymmetric optimizer-state reset, and (iv) asymmetric learning rate re-warmup schedule. In this section, we evaluate its overall performance."
        },
        {
            "title": "5.1 Overall Downstream Performance",
            "content": "Table 1 Downstream performance under 2 mid-stage width growth. Across Inner 2 , and joint expansion, SPARKLING matches or outperforms the from-scratch expanded baseline on most tasks and achieves the best average, despite slightly higher final pre-training loss. , Hidden 2 Loss ( ) ARC-C ( Model Baseline (small) Inner 2 Baseline (expand) Naive Expansion SPARKLING Hidden 2 Baseline (expand) Naive Expansion SPARKLING 2.3673 2.3096 2.3276 2. 2.2795 2.3082 2.2933 & Inner 2 Hidden 2 Baseline (expand) Naive Expansion SPARKLING 2.2225 2.2615 2.2415 ) ARC-E ( 72. ) Arith. ( 43.63 74.56 73.86 74.39 77.72 73.86 76.14 76.14 74. 77.19 55.67 51.10 60.50 54.47 53.77 61.90 55.03 58. 66.00 ) BoolQ ( 66.36 67.80 66.45 66.82 67.06 67.37 67. 67.65 67.09 69.08 ) CSQA ( 47.58 ) HellaS. ( 65.86 47.58 48.24 49.06 48.32 49.06 49.80 49. 51.60 50.86 69.45 68.04 69.21 70.66 69.40 70.06 72.54 71.92 72.82 ) MMLU ( 32.75 32.67 32.71 34.05 33.82 32.00 34.49 33.20 33. 35.07 ) OBQA ( 39.40 42.40 41.80 41.60 42.00 41.00 43. 43.80 41.40 43.00 ) PIQA ( 76.28 78.18 77.09 78.35 78.56 78.18 78. 79.00 79.00 79.11 ) SciQ ( ) 92.50 SIQA ( 46.57 92.80 92.90 93.30 93.90 93.00 93.40 93.30 93. 94.10 46.67 46.98 47.80 47.85 47.44 48.26 48.57 47.54 48. ) WinoG. ( 62.19 64.80 64.25 65.04 66.61 64.17 65.98 64.88 65. 68.19 ) Avg. ( 57.26 ) 59.64 58.96 60. 60.46 59.25 61.13 60.89 60.89 62.55 41.47 43. 44.15 43.48 44.48 41.81 44.48 46.82 45.82 46.82 Experimental setup. Following Sec. 4.3, we further evaluate downstream performance under three 2 widthgrowth settings. For each setting, we compare (i) the Baseline (small) model before expansion, (ii) the Baseline (expand) model trained from scratch at the target width under the same token budget, (iii) Naive Expansion variant with copy-based initialization but without our interventions, and (iv) our SPARKLING, which combines RMS-preserved scaling with asymmetric optimizer-state reset and asymmetric learning rate re-warmup for newly introduced parameters. We report the final pre-training loss and downstream accuracies, including ARC-C/E [8], Arithmetic [4], BoolQ [7], CommonsenseQA [29], HellaSwag [43], MMLU [14], OpenBookQA [21], PIQA [3], SciQ [32], SocialIQA [26], Winogrande [25]. Results. Table 1 shows that mid-stage expansion still leaves small gap in final pre-training loss relative to training the expanded model from scratch. Nevertheless, SPARKLING achieves the best downstream average among the expansion variants and matches or exceeds the from-scratch expanded baseline on most tasks. Overall, these results validate the reliability of SPARKLING: despite slightly larger final pre-training loss, our framework consistently improves downstream performance across diverse width-growth axes."
        },
        {
            "title": "5.2 Computational Cost Analysis",
            "content": "Table 2 Compute-cost comparison under fixed token budget. Method Total Tokens Expand @ Act./Tot. Params Baseline 200B - 450M/2.56B FLOPs 1020) ( 5.40 Inner 2 From Scratch SPARKLING Hidden 2 From Scratch SPARKLING 200B 200B Hidden 2 From Scratch SPARKLING & Inner 2 200B - 100B - 100B - 100B 751M/5B 900M/5.13B 1.5B/9.96B 9.01 7.21 10.80 8. 18.00 11.70 Wallclock (h) FLOPs Saved Speed -up 84 66 96 75 209 - 20% - 25% - 35% - 1.27 - 1.29 - 1.49 Now that the effectiveness of our expansion framework has been validated, we finally return to the core motivation of progressive learning - reducing training costs while retaining the performance of the target-width model. We quantify the computational savings by comparing mid-stage width expansion with training the target-width model from scratch under the same training token budget. Following Kaplan et al. [16], we approximate pre-training compute as 6N D, where is the number of active parameters and is the total training tokens. Suppose that the expansion occurs at De tokens, where the small model with active size Nsmall is trained for De tokens, and the expanded model of active size Nlarge is trained for (D De) tokens, yielding 6(cid:0)NsmallDe + Nlarge(D whereas training the expanded model from scratch costs Cscratch as FLOPs Saved = 1 De)(cid:1), 6NlargeD. We report the relative reduction (29) /Cscratch, and the empirical wall-clock Speed-up as Tscratch/T . width-growth settings. Under the same 200 B-token budget, Table 2 summarizes results across three 2 SPARKLING saves 20 %35 % training FLOPs relative to training the expanded model from scratch, and achieves up to 1.49 width expansion. Overall, SPARKLING matches or even exceeds the performance of the from-scratch expanded model while substantially reducing training costs, making mid-stage width growth practically advantageous. measured wall-clock speed-up under"
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "We proposed SPARKLING, systematic progressive learning framework via width expansion, and resolved the challenges arising during mid-stage model expansion. In contrast to conventional function-preserving perspectives, we emphasize signal preservation by maintaining the RMS-scale of activations during expansion. To break the symmetry induced by copy-based initialization, we apply asymmetric optimizer resetting together with learning rate re-warmup. Across multiple width axes and optimizer families, extensive experiments validate both the effectiveness and the efficiency of our framework. While our results are promising, several avenues remain for future exploration. First, unified principle of simultaneous width and depth expansion has yet to be established. Moreover, we aim to investigate whether our RMS preservation strategy could satisfy the µP condition [38], where the transferability of optimal hyperparameters is naturally ensured after expansion. We view these as critical future work toward developing more comprehensive, tuning-free\" framework for progressive learning."
        },
        {
            "title": "References",
            "content": "[1] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth, 2020. URL https://arxiv.org/abs/2003. 04887. [2] George Bird. The affine divergence: Aligning activation updates beyond normalisation, 2025. URL https: //arxiv.org/abs/2512.22247. [3] Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):74327439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL https://ojs.aaai.org/index.php/AAAI/article/view/6239. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [5] Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2BERT: Towards reusable pretrained language models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 21342148, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.151. URL https://aclanthology.org/2022.acl-long.151/. [6] Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer, 2016. URL https://arxiv.org/abs/1511.05641. [7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300/. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. [10] Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking your transformers: closer look at model growth for efficient llm pre-training. Advances in Neural Information Processing Systems, 37:1049110540, 2024. [11] Utku Evci, Bart van Merriënboer, Thomas Unterthiner, Max Vladymyrov, and Fabian Pedregosa. Gradmax: Growing neural networks using gradient information, 2022. URL https://arxiv.org/abs/2201.05125. [12] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pages 23372346. PMLR, 2019. [13] Xue Han, Yitong Wang, Junlan Feng, Qian Hu, Chao Deng, et al. Loire: Lifelong learning on incremental data via pre-trained language model growth efficiently. In The Thirteenth International Conference on Learning Representations, 2025. [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. [15] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon/. [16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https: //arxiv.org/abs/2001.08361. [17] Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 2335, 2024. [18] Houyi Li, Wenzhen Zheng, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Zhenyu Ding, Haoying Wang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, and Daxin Jiang. Predictable scale: Part i, step law optimal hyperparameter scaling law in large language model pretraining, 2025. URL https: //arxiv.org/abs/2503.04715. [19] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training, 2025. URL https://arxiv.org/abs/2502.16982. [20] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [21] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. [22] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. OLMoe: Open mixture-of-experts language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=xXTkbTBmqq. [23] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2025. URL https://arxiv.org/abs/2409.02060. [24] Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, and Qun Liu. Reusing pretrained models by multi-linear operators for efficient training. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=RgNXKIrWyU. 13 [25] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):87328740, Apr. 2020. doi: 10.1609/aaai.v34i05.6399. URL https://ojs.aaai.org/index.php/AAAI/article/view/6399. [26] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/ D19-1454/. [27] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2014. URL https://arxiv.org/abs/1312.6120. [28] Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models, 2022. URL https://arxiv.org/abs/2203.06211. [29] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https: //aclanthology.org/N19-1421/. [30] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=cDYRS5iZ16f. [31] Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang. LEMON: Lossless model expansion. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=3Vw7DQqq7U. [32] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology.org/W17-4413/. [33] Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Ping Luo. LLaMA pro: Progressive LLaMA with block expansion. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 65186537, 2024. [34] Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural architectures. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_ files/paper/2019/file/3a01fc0853ebeba94fde4d1cc6fb842a-Paper.pdf. [35] Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu. Firefly neural architecture descent: general approach for growing neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 2237322383. Curran Associates, Inc., 2020. URL https: //proceedings.neurips.cc/paper_files/paper/2020/file/fdbe012e2e11314b96402b32c0df26b7-Paper.pdf. [36] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [37] Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively stacking 2.0: multi-stage layerwise training method for bert training speedup. arXiv preprint arXiv:2011.13635, 2020. [38] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer, 2022. URL https://arxiv.org/abs/2203.03466. [39] Yifei Yang, Zouying Cao, Xinbei Ma, Yao Yao, Zhi Chen, Libo Qin, and Hai Zhao. Lesa: Learnable llm layer scaling-up. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2246322476, 2025. 14 [40] Kazuki Yano, Sho Takase, Sosuke Kobayashi, Shun Kiyono, and Jun Suzuki. Efficient construction of model family through progressive training using model expansion. CoRR, abs/2504.00623, April 2025. URL https: //doi.org/10.48550/arXiv.2504.00623. [41] Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language model pretraining. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=rL7xsg1aRn. [42] Xin Yuan, Pedro Henrique Pamplona Savarese, and Michael Maire. Accelerated training via incrementally growing neural networks using variance transfer and learning rate adaptation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=H1a7bVVnPK. [43] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. [44] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. [45] Bo-Wen Zhang, Liangdong Wang, Ye Yuan, Jijie Li, Shuhao Gu, Mengdi Zhao, Xinya Wu, Guang Liu, Chengwei Wu, Hanyu Zhao, Li Du, Yiming Ju, Quanyue Ma, Yulong Ao, Yingli Zhao, Songhe Zhu, Zhou Cao, Dong Liang, Yonghua Lin, Ming Zhang, Shunfei Wang, Yanxin Zhou, Min Ye, Xuekai Chen, Xinyang Yu, Xiangjun Huang, and Jian Yang. Aquilamoe: Efficient training for moe models with scale-up and scale-out strategies, 2024. URL https://arxiv.org/abs/2408.06567."
        },
        {
            "title": "A Derivations",
            "content": "A.1 Eq. (7): The Per-Coordinate Variance Under Fan-In Aggregation We provide the intermediate steps used in Sec. 3.2.1 to derive RMS preservation for fan-in variance constraint. Consider the linear layer = with , we write 1, . . . , dout { } Rdoutdin and Rdin. For fixed output coordinate yi = din(cid:88) j=1 wijxj. (30) Assume that, across the fan-in dimensions, the pairs and are independent of each other. Under these conditions, the variance of yi decomposes additively: are mutually independent, and that din (wij, xj) j=1 } { Var(yi) = Var din(cid:88) j=1 wijxj = din(cid:88) j= Var(wijxj). (31) If, in addition, the fan-in terms are homoscedastic and centered in the sense that E[wij] = E[xj] = 0 and . wσ2 Var(wij) = σ2 Substituting this into Eq. (31) yields for all j, then each product term shares the same variance Var(wijxj) = σ2 , Var(xj) = σ2 Var(yi) = din(cid:88) j=1 wσ2 σ2 = din σ2 wσ2 x, (32) which is the expression in Eq. (7) of the main text. Combined with Eq. (6), this shows that (when sin is fixed) preserving the output RMS scale is equivalent to preserving Var(yi), and under the above assumptions this reduces to keeping dinσ2 invariant across the expansion. wσ2 A.2 Eq. (16)-(17): RMS-Preserving Rescaling under Fan-In Expansion with Both-Sides"
        },
        {
            "title": "Copied",
            "content": "A qualitatively different regime arises when both sides of the newly introduced fan-in coordinates are created by copying existing dimensions, i.e., the new columns of and the new coordinates of are both duplicated from the same subset of original fan-in dimensions, respectively. In this case, the independence across fan-in dimensions is violated: the copied pairs (w j) are no longer independent replicas but perfectly correlated duplicates of some original terms. As result, the variance no longer decomposes as simple sum of in independent contributions, and the duplicated terms contribute quadratically through covariance. ij, Given Eq. (15) with denoting the copy ratio, we first consider the setting 0 < dimension is duplicated exactly once. Let remaining indices with dimension contributes twice with identical value, yielding 1 where each copied be the c)din. Under one-to-one copying on both and x, each duplicated be the set of copied indices with = din, and = (1 S y = in(cid:88) j=1 ijx = (cid:88) rR irxr + isxs. (cid:88) sS (33) 16 Under the same independent assumptions as above on the original terms, the variance of becomes Var(y i) = Var (cid:32) 2 (cid:88) irxr + (cid:88) sS (cid:88) (cid:33) isxs Var(w isxs) rR Var(w irxr) + = 4 (cid:88) rR sS wσ2 σ2 (cid:17) wσ2 σ2 c)din = 4 (cid:16) = wσ2 σ + 4c din + (1 wσ2 = din(1 + 3c)σ2 x. (34) When the input scale is kept unchanged, preserving the original variance requires rescaling the weights in the denote the post-rescaling weight variance; enforcing Var(y expanded layer. Let σ2 i) = Var(yi) gives din(1 + 3c) σ2 wσ2 = din σ2 wσ2 = σ2 = 1 1 + 3c σ2 w, or equivalently, ij = 1 1 + 3c wij, 0 < 1, = 1, . . . , dout, = 1, . . . , in. For > 1 where some dimensions might be copied multiple times, the variance of becomes When the input scale is kept unchanged, enforcing Var(y i) = Var(yi) gives Var(y i) = (1 + c)2dinσ2 wσ2 x. (1 + c)2din σ2 wσ2 = din σ2 wσ = σ2 = 1 (1 + c)2 σ2 w, or equivalently, ij = 1 1 + wij, > 1, = 1, . . . , dout, = 1, . . . , in. (35) (36) (37) (38) (39) Combining Eqs. (36) and (39) yields the final rescaling rule in Eq. (16). Substituting = these equations gives the equivalent form in Eq. (17). in/din 1 into A.3 Identical Gradients Under Copy Expansion for Fan-In Expansion For the input expansion defined in Eq. (10), copy initialization sets = such that = α[W , ], with the input duplicated as = [x, x]. The forward pass yields = α(W + x). During backward propagation: = L (x) = (cid:2)x, x(cid:3) = (cid:2)gx, gx(cid:3) . (40) showing that the two copied blocks receive identical gradients and that the uniform scalar α does not affect this symmetry argument."
        },
        {
            "title": "B Detailed Experimental Setup",
            "content": "B.1 Baseline Model Configuration We list the detailed hyperparameters and architectural configuration of pre-expansion baseline model before expansion in Table 3. In addition to these settings, we adopt pre-norm design by inserting RMSNorm before both the attention and MLP sublayers. Moreover, within the attention block, we apply per-head q/k normalization along the head-dimension for each query and key head, i.e., normalizing the projected and vectors within each head over the dhead dimension. 17 Notably, since we tie the word embedding and the output projection, hidden-size expansion makes the shared matrix act as fan-out on the embedding side but fan-in on the output side. As our RMS-preserved scaling requires different factors for these two roles, we compensate the fan-in factor by multiplying the corresponding coefficient after the final output projection for this special case. Table 3 Baseline model Configuration. Configuration Number of Hidden Layers (L) Hidden Size (dmodel) Expert Intermediate Size (dffn) Number of Attention Heads (nheads) Number of Key/Value Heads (nkv) Head Dimension (dhead) MoE Number of Experts (E) MoE Top-k (k) ) Embedding Size ( Tie Word Embeddings Activation Type Norm Type Positional Embedding Use Bias Value 24 1024 512 16 4 96 64 8 50304 True SwiGLU RMSNorm Pre-norm RoPE False B.2 Training Hyperparameters We summarize the training hyperparameters in Table 4. Unless otherwise specified (e.g., the Muon experiments in Sec. G), we optimize with AdamW using (β1, β2) = (0.9, 0.95), ϵ = 108, and weight decay 0.1, where we apply weight decay to all parameters including norms and embeddings. We use cosine learning rate schedule with linear warmup over 3% of total steps, and decay to minimum learning rate set by final ratio of 0.01 relative to the peak learning rate. All experiments are run on cluster of 64 NVIDIA A100 GPUs with 80 GB memory each, using global batch size of 768 with per-device microbatch size 3. Table 4 Training hyperparameter configuration. Configuration Total Training Tokens Optimizer Peak Learning Rate AdamW Betas (β1, β2) AdamW Epsilon ϵ Weight Decay Decay Norm & Bias Decay Embeddings LR Scheduler Warmup Steps Final LR Ratio Max Sequence Length Global Batch Size Device Microbatch Size Number of GPUs 1.9556 Value 200 AdamW 103 (0.9, 0.95) 108 1.0 0.1 True True Cosine w/ Warmup 3% of total steps 0.01 4096 768 3 64 For the all models trained from scratch, we set the peak learning rate to the step-law optimum reported in 18 [18] and apply batch-size scaling to obtain the corresponding value under our training setup in Table 5. In contrast, when expanding from smaller model, we empirically find it more effective to keep the pre-expansion peak learning rate for training the enlarged model with the same peak LR. Table 5 Step-law optimal learning rates [18] across model sizes and their batch-size-scaled learning rates. Model Baseline Inner 2 Hidden 2 Hidden 2 & Inner Total Tokens Act. Params 200B 200B 200B 200B 450M 751M 900M 1.5B Tot. Params 2.56B 5B 5.13B 9.96B Step-law LR 1.033e-3 6.410e-4 5.760e-4 3.920eScaled LR 1.449e-3 8.988e-4 8.630e-4 5.497e-"
        },
        {
            "title": "C RMS Scale Under Zero Initialization",
            "content": "In Figure 4, we analyze subtle but practically important corner case for RMS-preserving expansion: one-sided zero initialization. We consider two representative regimes, random-zero and zero-copy, and observe the RMS scale of the output and input activations of the whole MLP according to Eq. 3. We empirically find that, when applying RMS-preserved scaling under zero initialization, the zero-initialized side should be treated as random rather than as special perfectly loss preserving case. Intuitively, zero-initialized block becomes gradient-driven random distribution after the very first update, so its effective statistics quickly resemble those of randomly initialized block. While omitting RMS-preserved scaling can be strictly loss-preserving at the expansion moment and therefore naturally satisfies RMS-scale preservation at = te, we observe that the RMS-preserved scaling variant that treats the zero side as random yields an activation RMS ratio that remains closer to the original baseline scale as post-expansion training proceeds. In contrast, the RMS scale under naive unscaled zero initialization drifts and does not exhibit recovery trend toward the baseline. This behavior is consistent with the fact that zero initialization necessarily disrupts the pre-expansion parameter distribution and thus requires nontrivial number of steps to re-enter compatible statistical regime. Accordingly, our emphasis is on the RMS scale shape after the model has taken small number of post-expansion updates, rather than on the degenerate preservation at the boundary itself. Figure 4 Under zero initialization, RMS-preserved scaling enables the post-expansion activation RMS scale to quickly recover toward the original baseline, indicating that zero initialization should be treated as random under RMS-preserving expansion. Hidden-Dimension Expansion: RMS-Preserved Scaling Following the experimental setting in Sec. 3.3, we provide the hidden-dimension counterpart of our RMS-scale analysis by doubling the model hidden size from 1024 to 2048 at 100 tokens and continuing training to 200 tokens under the same training recipe. Figure 5 shows the same qualitative conclusion as expert-inner growth: while naive unscaled initialization can exhibit smaller instantaneous loss discontinuity at the expansion moment, enforcing RMS-scale consistency via our RMS-preserved rescaling yields consistently better late-stage recovery and lower converged loss across initialization regimes. Figure 5 Hidden-dimension expansion mirrors expert-inner growth. We repeat the RMS-preserving scaling comparison under hidden-dimension 2 2048 at 100 tokens). Across initialization pairs, RMSpreserved rescaling consistently improves late-stage convergence relative to naive unscaled expansion, exhibiting the same pattern observed for expert-inner growth in Sec. 3.3. expansion (1024 Sample Asymmetric Re-warmup Learning Rate Curve To make the asymmetric re-warmup schedule in Eq. (28) more tangible, we plot representative learning rate trajectory in Figure 6. Typically, at the expansion point, the original parameters retrain the unchanged baseline cosine schedule for continuity, while the newly introduced parameters are re-warmed up from the current learning rate to slightly higher peak for short window following with decay. Figure 6 sample asymmetric learning rate re-warmup curve. At the expansion step te, the learning rate of the original parameters remains on the baseline cosine schedule, whereas the newly introduced parameters are re-warmed from the instantaneous rate ηe = η(te) to higher peak ˆηmax = ρ ηe over τw steps, and then decay with the same cosine tail toward ηmin, as specified in Eq. (28). Hyperparameter Search for Asymmetric Re-warmup We study how the asymmetric re-warmup schedule in Eq. (28) depends on two hyperparameters: the re-warmup expansion ratio ρ and the number of re-warmup steps τw. We conduct this search under the expert-inner 2 20 setting. Figure 7 summarizes the final loss obtained by sweeping ρ and τw. The results exhibit broad, stable region in which re-warmup is most effective: ρ 0250 steps achieve the lowest final loss, indicating that newly introduced parameters benefit from modest, short-lived learning rate boost rather than prolonged or overly strong re-warmup. Empirically, we find that this setting is also suitable for hidden-dimension expansion, and we therefore set ρ = 1.3 and τw = 250 as the default hyperparameters for all experiments involving re-warmup. 1.251.3 and τw Figure 7 Hyperparameter search for asymmetric re-warmup. Under the expert-inner 2 ρ configuration in all experiments involving re-warmup. expansion setting, 0250, yields the lowest final loss and we adopt ρ = 1.3, τw = 250 as the default re-warmup 1.251.3, τw"
        },
        {
            "title": "G Effectiveness Under Muon",
            "content": "Experimental setup. To verify that our framework is not tied to element-wise optimizers like AdamW, we repeat the expert-inner expansion experiment following Sec. 3.3 & 4.3 using Muon as the optimizer while keeping the other recipe unchanged. We evaluate two representative components of our method. First, we isolate RMS-preserved scaling by comparing against the naive unscaled initialization under the same initialization regime. Second, we evaluate asymmetric learning rate re-warmup for the newly introduced parameters by comparing runs with versus without the re-warmup schedule, while all other components of our framework applied. Results. Figure 8 shows the conclusions on Muon. In Figure 8(a), RMS-preserved scaling produces stable and consistent improvement over naive unscaled initialization, ultimately converging to lower final loss under the same training budget. In Figure 8(b), enabling asymmetric re-warmup further improves late-stage convergence over any other counterparts without re-warmup. Taken together, these results demonstrate that both RMS-preserved scaling and re-warmup remain effective under Muon, confirming that our framework applies beyond AdamW and extends to spectral-style update like Muon without requiring optimizer-specific designs. Comparison to Prior Function-Preserving Symmetry-Breaking Heuristics Experimental setup. Prior width-growth methods that rely on copy-based expansion attempt to break symmetry by two widely used function-preserving heuristics: (i) Uneven Splitting [5, 6, 10, 31] by assigning different scaling factors to the channel being copied and the copied one, (ii) Perturb [34, 35, 42] by adding symmetric perturbations of equal magnitude and opposite sign to the two duplicated halves. Following the expert-inner expansion in Sec. 4.3, we implement these strategies as well as (iii) Re-warmup All, which applies the same re-warmup schedule to all parameters. Results. Figure 9 shows that these heuristics, despite introducing asymmetry by construction, remain consistently weaker than our framework. Moreover, the zoomed-in view around the expansion moment Figure 8 Effectiveness under Muon. We repeat the expert-inner expansion experiment (512 1024 at 100B tokens) using Muon and plot reference-loss versus training tokens. (a) RMS-preserved scaling consistently improves late-stage convergence compared to naive unscaled initialization. (b) With RMS-preserved scaling and asymmetric state reset applied, asymmetric learning rate re-warmup further lowers the final loss, confirming that our framework remains effective under Muon. Figure 9 Under expert-inner copy-copy expansion, we compare three alternatives against our framework: (i) Uneven Splitting (fixed 1 : 2 or randomized : (1 perturbation that cancels in the forward pass, and (iii) globally re-warmup all parameters. All alternatives converge to higher final loss, underperforming our framework. Insets highlight the post-expansion dynamics: our method exhibits brief loss increase followed by rapid recovery, consistent with more effective symmetry breaking in the newly added capacity. [0.1, 0.5]) and (ii) symmetric r) with 22 highlights transient loss up-shift followed by fast recovery, consistent with targeted exploration for newly introduced parameters benefiting from asymmetric re-warmup."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Peking University"
    ]
}