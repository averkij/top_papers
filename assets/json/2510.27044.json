{
    "paper_title": "Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning",
    "authors": [
        "Md Tanvirul Alam",
        "Nidhi Rastogi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \\emph{Activity Scheduling} and the \\emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 2 4 4 0 7 2 . 0 1 5 2 : r Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning Md Tanvirul Alam Rochester Institute of Technology Rochester, NY, USA ma8235@rit.edu Nidhi Rastogi Rochester Institute of Technology Rochester, NY, USA nxrvse@rit.edu"
        },
        {
            "title": "Abstract",
            "content": "Mathematical reasoning is central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: Activity Scheduling and the Longest Increasing Subsequence, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have recently advanced rapidly on mathematical and programming benchmarks [4, 5, 20, 1, 21]. key driver is Reinforcement Learning with Verifiable Rewards (RLVR), which fine-tunes pretrained models against automatically checkable signals such as exact answers or unit tests [4, 22]. This paradigm eliminates reliance on human annotation, enabling scalable training on large problem sets and delivering consistent gains on challenging reasoning tasks [3]. Despite strong empirical gains, the nature of RLVRs improvements remains unclear. Studies show it often boosts accuracy while reducing exploration [19, 2, 23, 13], with base model ability acting as ceiling [23, 19]. Gains largely reflect re-weighting existing solutions, and reasoning coverage can even contract at larger sample sizes. Standard ass@K further over-credits lucky completions [16], whereas stricter metrics like CoT -P ass@K are more reliable but harder to scale. Reward design can also yield surprising effects: single example can rival large-scale training [15], and even spurious rewards can drive improvements in models with strong procedural biases [7]. Overall, RLVR appears to stabilize existing competencies rather than induce new reasoning strategies. While prior studies have offered valuable insights, many rely on benchmarks where the correctness of reasoning is difficult to verify, making it unclear whether improvements reflect genuine mathematical competence or superficial pattern matching. We address this gap by focusing on two combinatorial problems with fully verifiable solutions: Activity Scheduling, which admits unique greedy optimum, and the Longest Increasing Subsequence (LIS), which can be solved using dynamic programming. By constructing datasets where each instance has single optimal sequence, we can precisely measure not only answer accuracy but also sequence fidelity and structural behaviors, such as correct sorting in Activity Scheduling. This verifiable setup provides rigorous lens on RLVR, revealing when 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI."
        },
        {
            "title": "LIS Example",
            "content": "Determine the largest subset of activities that can be scheduled without any overlaps. ID 1 2 3 4 5 End 07:24 08:23 09:28 10:18 06:14 Start 06:09 07:13 07:29 08:24 04:48 Determine the longest strictly increasing subsequence (by VALUE) from the rows below (use the row IDs). ID 1 2 3 4 5 Value 797 476 335 452 606 Ground truth: ids{5,2,4}, answer{3} Ground truth: ids{3,4,5}, answer{3} Figure 1: Example question and ground-truth for Activity Scheduling (left) and LIS (right). observed gains arise from heuristic shortcuts versus genuine reasoning strategies, and highlighting broader implications for the design of mathematical reasoning benchmarks."
        },
        {
            "title": "2 Experimental Setup",
            "content": "2.1 Tasks Activity Scheduling. Each activity has start and finish times (si, fi) with si < fi, and intervals are half-open [si, fi). The goal is to select maximum subset of non-overlapping activities [17]. Instances are constructed so that the greedy earliest-finish-time algorithm with deterministic tie-breaking yields the unique optimum, reported as IDs sorted by fi (ties by smaller i). Longest Increasing Subsequence (LIS). Given a1, . . . , an Z, find indices 1 i1 < < ik maximizing with ai1 < < aik . Uniqueness is enforced via an O(n2) dynamic-programming count, and the LIS is reconstructed with patience sorting in O(n log n) [18]. Our generator enforces uniqueness of the optimal solution for both tasks, yielding single groundtruth ID sequence with fixed canonical reporting order (see Appendix A). Dataset. We generate 2000 instances per task, half with hints, with sequence lengths 516. To avoid leakage, train and test use disjoint length ranges, leaving 462 test cases for Activity and 428 for LIS; the remainder form the training set. Example questions and their corresponding ground truths are illustrated in Fig. 1, with detailed prompts provided in Fig. 6 in Appendix. 2.2 Reward Functions For each instance (x, y) with ground-truth answer and unique optimal sequence = (i1, . . . , iL), the output is parsed into ˆa(y) (from answer{...}) and ˆs(y) (from ids{...}); if parsing fails, ˆs(y) = . All rewards lie in [0, 1]: (1) Answer-only. This reward evaluates only the correctness of the final numeric answer: rans(y) = I[ˆa(y) = a] , where I[] denotes the indicator function, equal to 1 if the condition holds and 0 otherwise. (2) Answer + Format (LIS only). To stabilize behavior when the policy begins omitting reasoning, we introduce small formatting bonus, applied only to the LIS task(see 3.1)). Define the format indicator (cid:105) (cid:104) fmt(y) = contains <think>...</think> and valid answer{...}, ids{...} , and combine it with answer correctness using mixing weight λ = 0.1: rans+fmt(y) = (1 λ) rans(y) + λ fmt(y). (3) Exact-IDs. Rewards 1 if and only if the predicted sequence matches the optimum: rids,exa(y) = I(cid:2)ˆs(y) = s(cid:3). 2 (4) Prefix-IDs. This reward grants partial credit proportional to the length of the longest common prefix with the ground-truth sequence, while applying small penalty if the predicted length is incorrect (clipped at 0). Define m(y) = max(cid:8) {0, . . . , L} : (ˆs1, . . . , ˆsm) = (i1, . . . , im) (cid:9), and fix length-penalty γ = 0.1. The reward is then rids,pre(y) = max (cid:110) 0, m(y) γ I(cid:2) ˆs(y) = ˆs(y) = (cid:3)(cid:111) . (5) Sorting-Match (Activity only). In the Activity Scheduling task, sorting by finish time is the first step of the greedy algorithm. Interestingly, we observe that even without explicit instructions, model responses often begin with sorted version of the input sequence, which can be extracted reliably (see 3.3). This motivates an auxiliary reward that checks whether the extracted sorted sequence matches the ground-truth sorted order of activities: rsort(y) = I(cid:2) ˆssort(y) = sort (cid:3). where ˆssort(y) is the sequence of activity IDs sorted as extracted from the model output, and the canonical sorted sequence by increasing finish time (breaking ties by smaller ID). sort is 2.3 Training & Evaluation We fine-tune Qwen2.5-7B-Instruct [11] with GRPO [8, 4] using the verl framework [10]. Unless noted, max generation length is Tmax = 2048 (extended to 7680 for LIS with rans+fmt). Each PPO update uses 256 prompts with 8 rollouts from vLLM [6], trained for 20 epochs (120 updates) at learning rate 106 and no KL penalty (βKL = 0). Training prompts match evaluation format. Evaluation protocol. For each instance we draw = 256 samples {yj}256 j=1 with temperature 0.6 and top-p = 0.95, and parse each yj into (ˆa(yj), ˆs(yj)) as in the reward definitions. We evaluate two complementary notions of accuracy: Accans (correctness of the reported cardinality/length) and Accids (exact match of the ID sequence). Both are measured under Pass@k and Self-consistency (SC): Pass@k [23]. Success under k-sample budget, defined by whether at least one of the generations is correct: Pass@kans(x) = I(cid:2) : ˆa(yj) = a(cid:3), Pass@kids(x) = I(cid:2) : ˆs(yj) = s(cid:3). Self-consistency (SC) [14]. Agreement between the majority prediction aggregated over generations and the ground truth: ak(x) := mode{ˆa(yj)}k j=1, sk(x) := mode{ˆs(yj)}k j=1, with deterministic tie-breaking (numerically smallest for answers; lexicographic for sequences). We report SCans(x; k) = I(cid:2)ak(x) = a(cid:3), SCids(x; k) = I(cid:2)sk(x) = s(cid:3). All metrics are averaged over the test set. Because each instance admits unique ground-truth answer and sequence s, these definitions are unambiguous. We plot the full curves {Pass@k}256 k=1 and {SC(; k)} k=1, and also report their values at = 256."
        },
        {
            "title": "3 Results & Analysis",
            "content": "3.1 Training with Exact Answer Reward Fig. 2 compares the base model and the RLVR-trained policy on Activity Scheduling and LIS tasks. Activity Scheduling. Since the target is small integer (maximum schedule length 16), the base model quickly saturates to Pass@k 1.0 as increases. While the RLVR-trained policy attains 3 Figure 2: Performance comparison of Base, RL(rans), and RL(rans+fmt) models on the Activity and LIS task with the Qwen2.5-7B model. Figure 3: Performance comparison of RL models trained with rids,exa and rids,pre. slightly lower Pass@k at large k, it achieves much higher self-consistency (about 0.68 vs. 0.24 at k=256), indicating more stable predictions across samples. Under exact sequence ID matching, RLVR substantially outperforms the base model: at k=256, RLVR reaches Pass@k 0.64 compared to 0.14, with SC 0.34 compared to 0.004, reflecting clear improvement in sequence-level fidelity. These results support prior findings that answer-only Pass@k can overstate model capability [16], whereas Accids and SC provide more faithful measures of reasoning quality. RLVR improves both by reinforcing verified reasoning trajectories [16]. LIS. On LIS, training with the answer-only reward rans rapidly collapses intermediate reasoning: after just few PPO updates, the policy drops its chain of thought and outputs terse final answers, reflected in sharp decline in mean response length (Appendix D). Adding format component (rans+fmt) mitigates this, keeping response lengths closer to the base model  (Fig. 7)  . As shown in Figure 2, both RLVR-trained models achieve higher SC on answer than the base model; for rans, the Pass@k and SC curves nearly overlap, whereas rans+fmt underperforms the base model at larger k. However, under exact sequence ID evaluation, both RLVR-trained policies remain weak, with low Pass@k and SC compared to the base model, in contrast to the Activity task, where RLVR improved both metrics. Takeaway 1 RLVR improves answer-level generalization on both Activity Scheduling and LIS. However, only in Activity Scheduling do we observe reasoning gains, while on LIS, improvements stem from superficial heuristics or formatting strategies. Thus, RLVR can yield apparent task generalization without strengthening the underlying reasoning process. 3.2 Training with Sequence Rewards We now evaluate sequence-aware objectives, comparing the exact-match reward rids,exa and the prefix reward rids,pre (cf. 2.2). Figure 3 summarizes results for Accans and Accids across k. On Activity, both rewards yield similar gains, substantially surpassing the base and rans models: at k=256, SCids rises from 0.34 to 0.72/0.71 (rids,exa/rids,pre), and SCans increases from 0.68 to 0.74/0.75. On LIS, tradeoff emerges: rids,pre attains higher Accans, while rids,exa achieves higher Accids, yet both outperform 4 Figure 4: Sorting accuracy and LCS across models. the base and rans policies. At k=256, SCids improves from 0.08 to 0.42/0.40, and SCans from 0.58 to 0.63/0.70, indicating that sequence rewards enhance both answerand sequence-level consistency. Takeaway 2 Sequence-aware rewards improve sequence fidelity on both Activity Scheduling and LIS, as reflected in higher Accids. They also provide modest secondary gains in Accans, suggesting that RLVR generalization can benefit from incorporating intermediate or auxiliary objectives. 3.3 Sorting Performance on Activity Task Sequence-aware rewards improve sequence matching partly by enhancing the sorting preface that models generate as the first step in activity scheduling. We extract candidate ID lists from outputs using simple patterns, succeeding on over 84% of RL-trained responses versus 40% for the base model (Appendix B). Evaluating ExactSort (exact match with ground truth) and LCS fraction (longest correctly sorted subsequence) shows that sequence rewards yield both higher exact sorting and better partial order fidelity. Figure 4 summarizes sorting quality for the four models. Both sequence-reward policies (rids,exa and rids,pre) improve exact sorting relative to the base and rans models (from 0.17%/0.43% to 2.01%/1.85%) and increase the mean LCS (from 0.248/0.290 to 0.517/0.444). However, absolute exact-sorting accuracy remains very low ( 2%), even though these same policies achieve high sequence correctness at evaluation (e.g., Pass@256ids 0.60 on Activity). This gap indicates that the sorting step is not reliably driving the final schedule, despite the frequent appearance of sorted preface. 3.4 Training with Sorting-Match Reward In 3.2, we observed performance gains in Accans when training with sequence-aware rewards. This raises the question of whether rewarding the sorting step, which is the first stage of the greedy activity-scheduling algorithm, could provide additional benefit. To test this, we trained model with the sorting-match reward rsort. Surprisingly, this led to catastrophic collapse: both Accans and Accids fell to nearly 0%. Inspection of the outputs revealed that the model consistently produced the sorted sequence itself as the final answer, without applying the non-overlap constraint required for valid schedules. We next trained model using combined objective with equal weights on rans, rids, and rsort. This configuration restored Accans and Accids to levels comparable to the rids model, but it did not improve sorting performance on the test set. Thus, learning to sort in isolation does not confer complementary benefits for solving the activity-scheduling task. To probe further, we ran curriculum experiment: training with rsort alone for the first 10, 20, or 30 PPO updates, and then switching to rans for the remainder of the 120 training steps. As shown in Figure 5, models trained with rsort for only 10 steps were able to recover under rans, but longer 5 Figure 5: Curriculum experiments with rsort. Each curve shows accuracy on the training set when models are trained with rsort for the first 10, 20, or 30 PPO updates, followed by rans for the remainder. Longer pretraining with rsort makes it increasingly difficult for the model to recover under rans. exposure (20 or 30 steps) severely hindered recovery. In fact, after 30 steps of rsort, the model failed to improve Accans even on the training data. Takeaway 3 In Activity Scheduling, RLVR with sequence rewards improves answer and sequence accuracy, but sorting accuracy remains very low. Models often emit superficial sorted preface that neither matches the canonical order nor drives the final schedule, highlighting disconnect between surface outputs and the underlying decision rule [12]. 3.5 Heuristics Analysis for LIS To probe what signals models exploit on LIS, we regress predicted numeric answers against humaninterpretable features from the input table. We pool all stochastic runs but split train/test by problem instance ID to avoid leakage. Features cover: (i) global scale (length, range, dispersion, quantiles), (ii) order structure (increase ratios, inversion ratio, sign changes), (iii) run structure (longest runs, monotone counts, local extrema, record highs/lows), (iv) simple LIS heuristics (greedy, beam-limited, limited backtracking), and (v) patiencesorting tails. We train Random Forest regressor and report R2 and mean absolute error (MAE). Details appear in Appendix E. Model Base rans rids,exa rids,pre rans+fmt R2 test -0.002 0.741 0.781 0.841 0. MAEtest 2.745 0.269 0.289 0.227 0.209 Table 1 shows that RLVR-trained outputs are predicted well from these features, unlike the base model. While the features are not exhaustive, results suggest RLVR amplifies systematic heuristics aligned with task structure, boosting answer-level performance without ensuring stronger reasoning. Table 1: Predictive fit of LIS features on model outputs."
        },
        {
            "title": "4 Limitations & Future Work",
            "content": "Our study investigates two verifiable reasoning tasks (Activity Scheduling and LIS) with single base model (Qwen2.5-7B-Instruct). While this controlled setup helps isolate RLVR dynamics, conclusions may not generalize across tasks, model families, or scales, as prior work has shown diverse shortcut behaviors and model-dependent failure modes [7]. To partially address this, we provide additional results for Llama-3.1-8B in Appendix F. Future work could expand to broader range of models and problem domains, and employ mechanistic interpretability to probe the circuits and dynamics underlying RLVR-induced behaviors [9]. Such analysis may clarify whether improvements reflect genuine task learning or superficial strategies that exploit evaluation metrics. Our findings highlight this tension, emphasizing the need for careful evaluation and diagnostics when claiming improvements in reasoning from RLVR."
        },
        {
            "title": "References",
            "content": "[1] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261 (2025). [2] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. 2025. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 (2025). [3] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. 2025. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807 (2025). [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [5] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720 (2024). [6] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. [7] Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. 2025. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947 (2025). [8] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [9] Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. 2025. Open problems in mechanistic interpretability. arXiv preprint arXiv:2501.16496 (2025). [10] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. HybridFlow: Flexible and Efficient RLHF Framework. arXiv preprint arXiv: 2409.19256 (2024). [11] Qwen Team. 2024. Qwen2.5: Party of Foundation Models. https://qwenlm.github.io/ blog/qwen2.5/ [12] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2023. Language models dont always say what they think: Unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems 36 (2023), 7495274965. [13] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. 2025. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 (2025). [14] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022). [15] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. 2025. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571 (2025). 7 [16] Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, et al. 2025. Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs. arXiv preprint arXiv:2506.14245 (2025). [17] Wikipedia contributors. 2025. Activity selection problem Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Activity_selection_problem Accessed: 2025-0925. [18] Wikipedia contributors. 2025. Longest increasing subsequence Wikipedia, The Free Encyclopedia. https://en.wikipedia.org/wiki/Longest_increasing_subsequence Accessed: 2025-09-25. [19] Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, and Yejin Choi. 2025. The invisible leash: Why rlvr may not escape its origin. arXiv preprint arXiv:2507.14843 (2025). [20] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. 2024. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Association for Computational Linguistics. [21] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [22] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025). [23] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837 (2025)."
        },
        {
            "title": "A Dataset Construction",
            "content": "A.1 Activity Scheduling Problem model. We work on the standard unweighted interval scheduling problem. Each instance has activities indexed by = {1, . . . , m} with start/finish times (si, fi) and si < fi. Intervals are treated as half-open [si, fi), so touching endpoints are compatible (fj si means and can both be selected). We fix deterministic tie-breaking order by the tuple (fi, si, i) whenever sorting is required. The canonical reporting order for ground truth is by non-decreasing (ties by smaller i). Sampling on minute grid. All times lie on an integer minute grid. To construct candidate set of activities, we: (i) sample Unif{mmin, . . . , mmax}; (ii) repeatedly sample integer start times Unif{0, . . . , Smax} and integer durations Unif{Dmin, . . . , Dmax}, set = + d, and accept the interval if Smax + Dmax; we continue until intervals are accepted. (Values used in our experiments: mmin = 5, mmax = 16, Smax = 960, Dmin = 10, Dmax = 120; any fixed choices are acceptable as long as Dmin 1 and times are integral.) Each accepted interval receives stable ID {1, . . . , m} in order of creation; the table shown to the model lists rows by ID with times in HH:MM. Uniqueness check by DP counting. Write = (1, . . . , n) for the intervals sorted by (fi, si, i) (we reuse the symbol for the sorted index when clear). Define the predecessor map p(i) = max{ < : fj si } with p(i) = 0 if the set is empty. Let opt[i] be the maximum feasible cardinality using only {1, . . . , i} and let cnt[i] be the number of distinct schedules that attain opt[i] using {1, . . . , i}. Initialize opt[0] = 0 and cnt[0] = 1 (one empty schedule). For = 1, . . . , set INCL = 1 + opt[ p(i) ], EXCL = opt[ 1 ], opt[i] = max{INCL, EXCL}, cnt[i] = cnt[ p(i) ], cnt[ 1 ], cnt[ p(i) ] + cnt[ 1 ], if INCL > EXCL, if EXCL > INCL, if INCL = EXCL. Then = opt[n] is the optimal size and cnt[n] is the number of optimal schedules. If cnt[n] = 1, the optimum is unique. Under uniqueness we reconstruct the unique optimal set by backtracking from = n: when INCL > EXCL we include and jump to p(i); when EXCL > INCL we exclude and go to 1; when equal, exactly one branch leads to nonzero count under uniquenesschoose the branch whose downstream count equals 1 (include if cnt[p(i)] = 1 and cnt[i 1] = 0, otherwise exclude). Greedy sanity check and canonicalization. Because earliest-finish-time is optimal for unweighted interval scheduling, if the optimum is unique then the greedy policy with the same tie-breaks must return that same set. We nevertheless perform sanity check: run single pass of earliest-finishtime using the order (fi, si, i) to obtain G; we accept the instance only if = (as sets). The ground-truth sequence we release is the unique set sorted by non-decreasing (ties by i) and the ground-truth answer is S. This yields single canonical ids{...} and answer{...} for each instance. Complexity. Preprocessing and p(i) by binary search take O(m log m); the DP pass and backtracking are O(m); greedy verification is O(m). A.2 Longest Increasing Subsequence (Unique Optimum) Problem model. Given integers a1, . . . , an, an LIS is subsequence 1 i1 < < iL with ai1 < < aiL. We allow duplicate values in (ai); the strict inequality enforces strictly increasing 9 Algorithm 1: COUNTOPTIMAANDBACKTRACK for Interval Scheduling Input: Intervals with IDs (i, si, fi), = 1, . . . , m. Output: Optimal size k, number of optimal schedules cnt[n], unique set if cnt[n] = 1. Sort intervals by (fi, si, i) to obtain = (1, . . . , n); precompute p(i) = max{j < : fj si} by binary search opt[0] 0, cnt[0] 1 for 1 to do INCL 1 + opt[p(i)], EXCL opt[i 1] opt[i] max{INCL, EXCL} if INCL > EXCL then cnt[i] cnt[p(i)] else if EXCL > INCL then cnt[i] cnt[i 1] else cnt[i] cnt[p(i)] + cnt[i 1] opt[n] if cnt[n] = 1 then return (k, cnt[n], ) // Uniqueness holds: ; while > 0 do recover the single optimal set INCL 1 + opt[p(i)], EXCL opt[i 1] if INCL > EXCL then add to S; p(i) else if EXCL > INCL then 1 else if cnt[p(i)] = 1 and cnt[i 1] = 0 then add to S; p(i) else 1 return (k, 1, S) // Sort by (f, i) when reporting. values. The canonical target we release per instance is: (i) the unique LIS index sequence listed in increasing row order as ids{...}, and (ii) its length as answer{L}. IDs are 1-based row indices. (i) draw length Sampling. We work on an integer grid. Unif{mmin, . . . , mmax}; (ii) draw values i.i.d. ai Unif{Vmin, . . . , Vmax}; (iii) accept the instance only if the LIS is unique (by index sequence) and 2. Typical bounds used in our experiments are mmin=5, mmax=16, Vmin=1, Vmax=1000, but any fixed choices are valid. For each trial we: Uniqueness check by O(n2) counting. Let len_end[i] be the LIS length ending at i, and cnt_end[i] the number of LIS that end at with that maximum length. Initialize len_end[i]=1, cnt_end[i]=1. For each = 1..n and < i: if aj < ai : (cid:26)if len_end[j]+1 > len_end[i] : len_end[i] len_end[j]+1, cnt_end[i] cnt_end[j] else if len_end[j]+1 = len_end[i] : cnt_end[i] cnt_end[i] + cnt_end[j]. Let = maxi len_end[i] and #LIS = (cid:80) #LIS = 1. i:len_end[i]=L cnt_end[i]. We declare uniqueness iff Canonical reconstruction (used only after uniqueness holds). We reconstruct one LIS index sequence via patience sorting with predecessor links: scan left to right; for each ai place it by lower_bound (first position ai) in the tails array; store for each predecessor pointer to the last index at the previous length; then backtrack from the final tail index to obtain indices in increasing order. 10 Algorithm 2: GENERATEUNIQUEACTIVITYINSTANCE Input: RNG seed; mmin, mmax; minute-grid bounds Smax, Dmin, Dmax; max_tries. Output: Intervals {(i, si, fi)}m for 1 to max_tries do i=1; canonical ids{} sequence; integer answer S. Sample Unif{mmin, . . . , mmax} while < do sample Unif{0, . . . , Smax}, Unif{Dmin, . . . , Dmax}, set + if Smax + Dmax then append new interval (I+1, s, ) to (k, cnt, S) COUNTOPTIMAANDBACKTRACK(I) if cnt = 1 then continue greedy earliest-finish schedule on using (f, s, i) tie-breaks if = then continue // Canonical target: return (I, ids{ IDs(S) }, answer{ }) IDs of sorted by (f, i); answer = Fail if no instance is found within max_tries (try another seed or bounds) Algorithm 3: COUNTLISLENGTHANDNUMBER (strict LIS length & count) Input: Sequence (a1, . . . , an) Zn. Output: (LIS length), #LIS (number of LIS). for 1 to do len_end[i] 1; cnt_end[i] for 1 to do for 1 to i1 do if aj < ai then if len_end[j]+1 > len_end[i] then len_end[i] len_end[j]+1; cnt_end[i] cnt_end[j] else if len_end[j]+1 = len_end[i] then cnt_end[i] cnt_end[i] + cnt_end[j] maxi len_end[i]; #LIS (cid:80) return (L, #LIS) i:len_end[i]=L cnt_end[i] // Runtime O(n2) Canonical reporting and prompts. Because the LIS is subsequence, the ground-truth ids{...} is simply the unique LIS indices (i1 < < iL) in increasing row order; answer{L} reports its length. Complexity. Per trial: O(n2) for counting uniqueness; O(n log n) to reconstruct the indices once unique. Overall generation is bounded by max_tries, which we set large enough so failures are rare. Extracting sorted ID lists from free-form responses Setup. For each Activity instance the prompt shows an ASCII table with unique integer IDs {1, . . . , n} and times (si, fi). We define the ground-truth sorted order = (b1, . . . , bn) := IDs sorted by (fi, i) (non-decreasing end time, ties by smaller ID). This is the order used by the greedy earliest-finish-time scheduler (Alg. 1, 3.1). From the models free-form reply (arbitrary text) we attempt to recover list the model claims to be sorted. Candidate sources (lightweight and order-preserving). We build up to four candidate ID sequences from r; all candidates are normalized by (i) filtering to the valid ID set = {1, . . . , n} 11 Algorithm 4: GENERATEUNIQUELISINSTANCE Input: RNG seed; mmin, mmax; value bounds Vmin, Vmax; max_tries. Output: Values (a1, . . . , am); canonical ids{}; answer{}. for 1 to max_tries do Sample Unif{mmin, . . . , mmax}; Sample ai Unif{Vmin, . . . , Vmax} i.i.d. for i=1..m (L, #) COUNTLISLENGTHANDNUMBER(a1:m) if < 2 or # = 1 then continue // Reconstruct unique LIS indices via patience sorting with predecessors (i1, . . . , iL) PATIENCERECONSTRUCT(a1:m) return (cid:0)a1:m, ids{i1, . . . , iL}, answer{L}(cid:1) Fail if no unique instance within max_tries (try new seed or bounds)"
        },
        {
            "title": "LIS Prompt",
            "content": "Determine the largest subset of activities that can be scheduled without any overlaps (a single resource is available, so no double-booking). Start ID 06:09 1 07:13 2 07:29 3 08:24 4 04:48 5 End 07:24 08:23 09:28 10:18 06:14 Instructions: Times are given in 24-hour HH:MM format. Non-overlap means an activity ending at time is compatible with one starting at . Select maximum-size subset of non-overlapping rows. Determine the longest strictly increasing subsequence (by VALUE) from the rows below (use the row IDs). ID 1 2 3 4 Value 797 476 335 452 606 Instructions: The table lists 1-based row IDs and integer values. valid subsequence must be strictly increasing by VALUE and preserve the original row order. Select maximum-size subsequence. Your output must include the following two lines at the Your output must include the following two lines at the end, in this exact format: end, in this exact format: 1. ids{<comma-separated IDs of the chosen rows, listed in order of increasing END time. If two rows have the same END time, put the smaller ID first>} 2. answer{<number of chosen rows>} No spaces inside ids{...}. Example: ids{3,9,12} Hint: Sort the rows by increasing end time, then greedily pick compatible rows. 1. ids{<comma-separated IDs of the chosen rows, listed in increasing ROW order>} 2. answer{<number of chosen rows>} No spaces inside ids{...}. Example: ids{3,9,12} Hint: Use LIS DP (len[i] = 1 + max len[j] for < with value[j]<value[i]) while storing prev[i] (or patience sorting with predecessor links); then backtrack to output the ids. Ground truth: ids{5,2,4}, answer{3} Ground truth: ids{3,4,5}, answer{3} Figure 6: Example prompts and ground-truth for Activity Scheduling (left) and LIS (right). derived from the prompt table, and (ii) de-duplicating while preserving the first occurrence of each ID in the text. 1. Sorted-block (for exact-sorting only). We split into paragraphs and select those that mention sorting token (sort/sorted/sorting). To isolate the purported sorting step, we truncate at the first stop word (any of: select, greedy, choose, subset, largest, final answer, so, thus, therefore, next). We then extract IDs from this truncated text via either ID tokens or the longest comma-separated integer run. If the resulting list contains all distinct IDs exactly once, we accept it as full sorted-block candidate Afull and reserve it solely for the exact-sorting check below. 2. ids{...} blocks. From every brace block we read the comma-separated integers, normalize, and keep the resulting sequence if non-empty. 3. ID token stream. We scan left-to-right for tokens of the form ID and record the first occurrence of each k. 4. Longest comma run. We find the longest comma-separated integer run anywhere in and normalize it. These patterns capture the most common surface forms we observe in practice and are exactly those used in our implementation. If the sorted-block candidate Afull exists and is permutation of all Exact-sorting criterion. IDs (after normalization), we declare extraction success. The instance counts as exactly sorted iff Afull = B. Missing or malformed cases are treated as incorrect in the exact-sorting accuracy. Best-of-candidates policy and anchors (for substring analysis). For contiguous-substring analysis (C) we consider all non-empty candidates = {A1, . . . , AK} formed by items 24 above (and Afull if present). When multiple candidates tie on the score defined next, we break ties by method priority: sorted_block_full ids_braces id_stream comma_run. For the chosen best candidate we additionally report an anchor label describing where the matched block sits inside the candidate: start (run begins at position 1), end (run ends at position Ak), both (the candidate equals the block), or neither. Safeguards and edge cases. We ignore any integers not present in the prompt table; repeated IDs are dropped after the first mention; candidates that normalize to the empty list are discarded. These choices make the procedure robust to extraneous numbers and minor formatting quirks without conferring credit for unseen IDs. Complexity. Regex scans and candidate construction are O(r); all subsequent scoring (C) is linear in the candidate length(s). Contiguous LCS (longest common substring) metric Definition. Given candidate = (a1, . . . , am) and the ground-truth order = (b1, . . . , bn) (IDs unique), define the position map pos(bℓ) = ℓ. Let pt = pos(at) for = 1, . . . , (these indices exist by construction after normalization). The contiguous LCS length between and is (cid:110) (j + 1) (cid:12) (cid:12) s.t. {0, . . . , i}, pi+u = + LCS_len(A, B) = max (cid:111) . (1) 1ijm Intuitively, (1) is the length of the longest block in that appears contiguously in B; because IDs in are unique, this is exactly the standard longest common substring. Instance score and reporting. From the set of extracted candidates = {A1, . . . , AK} we take the best match = max k[K] LCS_len(Ak, B), LCS-frac = B = . We report the mean LCS-fraction over instances with > 0 and separate coverage rate (% of instances with any non-empty match). Exact-sorting accuracy and LCS are shown in Fig. 8. Anchors. For the candidate achieving (after the tie-break above), we also emit the anchor label start/end/both/neither based on whether the best contiguous block touches the candidates boundaries. Anchors help diagnose whether the models claimed sorted list appears as leading or trailing segment versus mid-sequence fragment."
        },
        {
            "title": "D LIS Task Response Length and Entropy",
            "content": "Figure 7 shows response length during training on the LIS task. Under the answer-only reward, responses quickly shorten, while the format reward preserves lengths close to those of the base model. 13 Figure 7: Response length during training Figure 8: Entropy during training Figure 8 further compares entropy during training. Entropy drops sharply for rans, mirroring the collapse in response length, whereas rans+fmt maintains higher entropy throughout training. Despite these qualitative differences, both RLVR-trained policies converge to similar levels of Accans and Accids, suggesting reliance on comparable heuristics expressed through different generation styles. For example, with rans+fmt nearly all responses contain Python code (100%, compared to 35.1% for the base model), yet the model does not execute the code step by step to derive the answer. This observation echoes findings by Shao et al. [7], who show that RLVR can amplify spurious reward signals in the Qwen family, encouraging models to emit structured but ultimately superficial outputs. Random-Forest Regression for LIS: Features, Protocol, and Training Goal. We analyze the models implicit decision rule by regressing its numeric answer, = pred_lis_len, on interpretable features computed only from the input values (v1, . . . , vn). Data and target. From the JSONL logs we pool all stochastic runs for each instance (sample_idx). The regression target is the models emitted answer{cdot} value; no ground-truth labels are used as features. Group split to avoid leakage. We perform single group hold-out split with GroupShuffleSplit (test size 25%) using sample_idx as the group key. Thus, all replicates of given instance are kept together in train or test; there is no overlap of sample_idx between splits. Feature set (input-only). Let be the sequence length and = vi+1 vi for = 1, . . . , 1. We compute: Global scale/dispersion: n, min(v), max(v), range = max min, mean, std, quartiles q25, q50, q75, uniq_ratio = {v}/n, and dup_ratio = 1 uniq_ratio. (cid:80) [i < 0], adj_eq_ratio = 1 Adjacent order (local trend): adj_inc_ratio = 1 n1 (cid:80) [i > 0], adj_dec_ratio = (cid:80) [i = 0], pos_delta_mean/std on {i > 0}, 1 n1 neg_delta_mean/std on {i < 0}, and sign_change_ratio = fraction of sign flips in (1, . . . , n1). n1 Pairwise order (global monotonicity): pair_inc_ratio = 1 (n 2) : vj < vi}, tau_like = pair_inc_ratio {i < : vj > vi}, inversion_ratio = 1 (n 2) inversion_ratio (Kendall-tau proxy ignoring ties). {i < Runs and structure: max_inc_run = longest strictly increasing contiguous run, max_dec_run = longest strictly decreasing run, num_monotone_runs = number of contiguous monotone segments, n_local_max/min = counts of strict local maxima/minima, record_highs (new maxima count), record_lows (new minima count). Heuristic LIS approximations (length only): greedy_len: left-to-right append if vi increases record-high count. greedy_rev_len: same on the reversed sequence. Figure 9: Performance comparison of Base and RLVR(rans) on the Activity task with the Llama-3.18B model. Left: numeric answer accuracy (Accans) vs. k. Right: ID sequence accuracy (Accids) vs. k. Figure 10: Performance comparison of Base and RLVR(rans) on the LIS task with the Llama-3.1-8B model. Left: numeric answer accuracy (Accans) vs. k. Right: ID sequence accuracy (Accids) vs. k. beam2, beam3: beam-limited LIS lengths with beam {2, 3}. budget1, budget2: greedy with {1, 2} backtracks (replace tail and truncate at most times). Patience-sorting descriptors (no direct LIS leakage): From the patience tails vector t, use tail_mean, tail_std, tail_iqr, and tail_slope (OLS slope of vs. index). (typical LIS scale under random permutaReference baseline: rand_lis_baseline = 2 tions). Pre-processing. We replace with NaN and drop rows with missing feature values. Metadata such as or log2_k are never used. Standardization is unnecessary for tree ensembles. Model and hyperparameters. We use Random-Forest Regressor with n_estimators=800, min_samples_leaf=2, max_features=sqrt, random_state=42, and n_jobs=1. Evaluation and Top-K selection. We report R2 and MAE on the held-out group-split test fold. To obtain compact, interpretable subset, we rank features by RF importance (fit on train) and sweep {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 18, 20, 25, . . .}. We pick the smallest whose test performance is within (R2, MAE) = (0.01, 0.02) of the full model. We also provide (i) log-scaled histogram of test residuals and (ii) Top-K curve (test R2 and MAE vs. K)."
        },
        {
            "title": "F Llama Model Performance",
            "content": "Figure 9 reports results on the Activity Scheduling task, and Figure 10 shows results on LIS using the Llama-3.1-8B model. The LLaMA model attains higher overall accuracy on both tasks, as measured by SC@256. However, the relative trends between the base and RLVR-trained variants (e.g., under rans) closely mirror those observed with the Qwen model family. 15 Figure 11: Acc with/without hint for Activity. Figure 12: Acc with/without hint for LIS. Evaluation of Hinted vs. Unhinted Prompts Figure 11 (Activity) and Figure 12 (LIS) compare model performance on test prompts with and without hints. Across both tasks, we observe no significant performance differences between the hinted and unhinted variants, suggesting that the models do not substantially benefit from the additional guidance provided by hints."
        }
    ],
    "affiliations": [
        "Rochester Institute of Technology"
    ]
}