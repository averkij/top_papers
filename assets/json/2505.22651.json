{
    "paper_title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "authors": [
        "Yi Ding",
        "Ruqi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic $\\beta$ for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 5 6 2 2 . 5 0 5 2 : r Sherlock: Self-Correcting Reasoning in Vision-Language Models Yi Ding, Ruqi Zhang Department of Computer Science, Purdue University, USA {ding432, ruqiz}@purdue.edu Project Page: https://dripnowhy.github.io/Sherlock/"
        },
        {
            "title": "Abstract",
            "content": "Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, self-correction and self-improvement training framework. Sherlock introduces trajectory-level self-correction objective, preference data construction method based on visual perturbation, and dynamic β for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data."
        },
        {
            "title": "Introduction",
            "content": "Teaching Large Language Models (LLMs) to reason through long, step-by-step thinking processes during post-training has been shown to significantly improve their performance on complex tasks, such as mathematical and code benchmarks [13, 32, 11, 33]. Similarly, as Vision-Language Models (VLMs) demonstrate strong capabilities [18, 2, 59], recent studies have incorporated reasoning into VLMs through Supervised Fine-Tuning (SFT) [43, 34, 7, 47, 12, 30] or Reinforcement Learning (RL) [36, 37, 46, 53, 27, 26], boosting performance in solving challenging multimodal tasks. Although reasoning VLMs such as [24, 30, 3, 31, 19] have demonstrated impressive performance on multimodal mathematical reasoning tasks, such as MathVista [21] and MathVerse [54], they still face significant challenges. First, they are highly sensitive to reasoning stepsonce an error occurs in multi-step reasoning process, it often propagates through subsequent steps, leading to incorrect final answers. Second, they are extremely data-demanding [12, 43], requiring large-scale, high-quality annotated data or easily verifiable answers to achieve consistent improvements. Third, they struggle to generalize to broader domains where such precise supervision is scarce, limiting their applicability beyond mathematical or VQA problem settings. In this paper, we study self-correction as promising strategy to address these challenges in reasoning VLMs. While self-correction has been explored in LLMs [14, 16], it remains underutilized in reasoning models or VLMs, despite being particularly well-suited. Self-correction allows the model to revise and improve its own prior outputs. This is particularly useful for challenging samples where the model may produce partially correct reasoning. In such cases, correcting specific mistakes through Preprint. Under review. Case of Self-Correction Problem: The image shows the famous Chubb illusion. Is the center area on the left smaller than the center area on the right? Sherlock Direct Generation Response. <SUMMARY> will analyze the image to compare the size of the . </SUMMARY> <CAPTION> The image is divided into two halves: the left side is solid gray background with small circular pattern in the center, while the right side features black and white noise pattern with central circle that is identical in size to the one on the left. </CAPTION> <REASONING> The central circle on both sides of the image is identical in size. However, the right sides central circle is embedded in busy black and white pattern, which can create an illusion of size. Despite this visual effect, the physical dimensions of the central circle do not change, and it remains smaller than the entire right half of the image. </REASONING> <CONCLUSION> Yes </CONCLUSION> Sherlock Self-Correction Response. <SUMMARY> will analyze the image to determine the relative sizes of the . </SUMMARY> <CAPTION> The image is divided into two halves. The left side shows solid gray background with small, circular, speckled pattern in the center. The right side features dense, random black and white pattern covering the entire area, with central circular region that is less dense. </CAPTION> <REASONING> To assess the size of the central areas, will compare the two: the circular pattern on the left side is isolated and clearly defined against the gray background, suggesting smaller presence. The central area on the right is defined by its lesser density within sea of busy black and white patterns, suggesting larger visual impact. However, the physical dimensions of the two areas are the same. </REASONING> <CONCLUSION> No </CONCLUSION> Figure 1: Example of Sherlocks self-correction ability. The direct generation contains errors that cause the trajectory to deviate from the correct path and result in an incorrect answer. Sherlock successfully corrects the previous response and obtains the correct answer. self-correction is simpler than generating fully accurate answer from scratch [45]. Moreover, the responses before and after self-correction naturally form preference pair [5, 51], where the inherent ordering acts as implicit supervision. By treating self-corrected outputs as preferred alternatives, the model not only reduces its dependency on extensive annotated data but also enhances its ability to generalize to broader domains. To enable self-correction in reasoning VLMs, we introduce Sherlock: self-correction and selfimprovement training framework to enhance reasoning. Unlike existing self-correction approaches, Sherlock introduces reasoning trajectory-level self-correction objective that focuses on correcting only the erroneous steps rather than the entire answer. Additionally, it leverages visual noise perturbations to construct preference datasets with controllable quality gaps. To further stabilize preference training, we introduce dynamic β that adapts to the quality gap between each sample pair. Once the model learns self-correction, it is able to self-improve without external supervision. Our main contributions are summarized as follows: We conduct an in-depth analysis of the self-correction capabilities of reasoning VLMs trained with either SFT or RL. We find that these models generally cannot self-correct. Self-correction within reasoning trajectory was observed in fewer than 10% of samples, with only half leading to correct final answer. When prompted with external critiques or self-correction prompts, the models fail to improve and may even see accuracy drop. To teach reasoning VLMs to self-correct, we propose trajectory-level self-correction objective, preference data construction pipeline, and dynamic β for preference tuning. This approach enables fine-grained correction at the reasoning trajectory level rather than overhauling the entire answer, providing clearer learning signal. We present Sherlock, training framework designed to enhance self-correction and reasoning in VLMs. It includes three stages: SFT, offline, and online preference learning. Through the entire training, Sherlock requires only 20k randomly sampled annotated data. In the online stage, the model continues to improve both reasoning and self-correction abilities without any external supervision, leveraging self-constructed preference datasets. Across eight benchmarks, the Sherlock model achieves the best reasoning performance while using the least amount of annotated data (20k), reaching an average accuracy of 64.1 (direct reasoning) and 65.4 (after self-correction). In comparison, LLaVA-CoT (100k), Mulberry (260k), and LlamaV-o1 (175k) achieve 63.2, 63.9, and 63.4, respectively. To efficiently scale inference-time compute, we combine Sherlock with verifier as stopping criterion, reducing GPU usage by 40% while achieving even higher accuracy (54.0 55.9)."
        },
        {
            "title": "2 Related Works",
            "content": "VLM Reasoning. LLMs with reasoning abilities (i.e., thinking before answering) have been shown to significantly improve performance across diverse tasks [39, 13, 11]. Building on this insight, researchers are now equipping VLMs with deeper reasoning capabilities to enhance their performance on visual tasks. Reasoning abilities are primarily enabled by supervised fine-tuning (SFT) or reinforcement learning (RL) methods. SFT uses external supervision signals combined with Chain-of-Thought (CoT) [39] to teach models think step-by-step [55, 43, 12, 7, 6, 47, 34]. However, generating multimodal CoT annotations often involves multi-turn prompting or Monte Carlo Tree Search (MCTS) on powerful non-reasoning VLMs, resulting in substantial computational costs. RL methods elicit reasoning behaviors through reward maximization. Inspired by DeepSeek-R1 [11], recent studies [53, 3, 24, 27, 26, 30, 37, 36, 30, 19, 58] use carefully designed, rule-based rewards to induce multimodal aha moments. Despite notable improvements, particularly in mathematical reasoning, these approaches remain highly dependent on carefully curated multimodal datasets and accurate verifiers. Training samples are often filtered based on complexity and reasoning difficulty, making these methods resource-intensive and challenging to generalize to diverse domains [24, 37]. In contrast, our method improves VLM reasoning by requiring only 20k randomly sampled CoT-labeled data to start, and then self-improves without relying on ground truth labels. Self-correction. Self-correction refers to the models ability to revise its previous responses to generate higher-quality answers. It has emerged as an intuitive and promising approach for enhancing model capabilities through inference-time scaling [22, 23, 56]. Studies [35, 28, 51] show that directly prompting LLMs to perform self-correction using generic instructions often leads to degraded performance. To address this, SFT and RL approaches [38, 48, 5, 16, 51] are developed to equip models with critique and self-correction capabilities. Step-wise correction methods [47, 36] use SFT to teach models to reflect and revise within single reasoning process, but often fail to effectively trigger intrinsic correction behavior. The most relevant to our work is [42], which integrates responsewise correction into LLMs for mathematical reasoning tasks using correction prompts and trains with DPO and PPO. However, this approach is limited to domains with rule-verifiable answers and depends on such rules to construct self-correction datasets. In multimodal settings, some approaches train an additional critic model to generate sample-specific critiques [41, 52, 30]. However, deploying these extra models and collecting large-scale critique data are resource-intensive, and their effectiveness in judging long CoT responses remains uncertain. Unlike prior work, Sherlock is specifically designed for multimodal reasoning models. It introduces trajectory-level self-correction objective and corresponding data construction pipeline that guides the model to revise only the erroneous suffix of the reasoning trajectory, removing the need for verifiers or large-scale critique data."
        },
        {
            "title": "3 Can Reasoning VLMs Self-Correct?",
            "content": "Reasoning VLMs explicitly generate step-by-step thoughts along with the final answer during inference [43, 34, 47, 12, 37, 46, 53, 36]. This process can be denoted as (y1, , yn; a) π(xI&T ), where yi represents the i-th reasoning step, is the final answer, π is the reasoning VLM, and xI&T denotes the input image and text. Given language models auto-regressive decoding mechanism, an error in any reasoning step yi may propagate and result in incorrect reasoning in the subsequent steps. Therefore, equipping reasoning models with self-correction capabilities holds great potential for improving overall reasoning quality. For reasoning models, self-correction behavior can be implemented in two ways: (i) Step-wise correction: The model reflects on its previous incorrect i-th step within single thinking process and revises it to arrive at the final answer: (r, yi+1, , yn; a) π(xI&T ; y1, , (1) Here, is the erroneous reasoning step, and is reflection token indicating the models intention to correct its previous reasoning. (ii) Response-wise correction: The model is prompted to revise and improve its previously generated response: ), {\"wait\", \"however\", \"check\", }. (y2 1, , n; a2) π(xI&T ; y1 (2) where {yj , aj} denotes the j-th attempt and is an additional instruction guiding the model to perform correction. In this section, we analyze both types of self-correction behaviors of reasoning VLMs, including the SFT-based LLaVA-CoT [43] and the RL-based VL-Rethinker [36], on two widely-used benchmarks, MMStar [4] and MathVista [21]. 1, , y1 n, a1; t), 3 Figure 2: Left: Overview of experimental settings for self-correction analysis. Blue block illustrates the Modified One Step process using Qwen2.5-7B-Instruct [44], while Green block represents two correction strategies applied to direct generations: external critique-based correction and selfcorrection prompt. Right: Reasoning performance of LLaVA-CoT [43] and VL-Rethinker [36] under different settings, evaluated on MMStar [4] and MathVista [21]. 3.1 Step-wise Self-correction MMStar (1500) MathVista (1000) Table 1: Step-wise self-correction results of reasoning VLMs under the Modify One Step setting. The numbers following the benchmark names (1500 and 1000) indicate the total number of samples in each benchmark. Setup. To test whether the model can perform step-wise self-correction, we conduct controlled experiment using Qwen2.5-7B-Instruct [44]. In this experiment, we prompt the Qwen model to autonomously select one influential step yi in the first half of the original reasoning trajectory and apply Modify One Step, resulting in an erroneous step denoted as . Then, VLMs generate the subsen; a) quent trajectory (y π(xI&T ; y1, , yi1, . This process is illustrated in the blue block of Fig. 2. To examine whether models exhibit step-wise self-correction behavior, which is often accompanied by the emergence of an aha moment, and whether such behavior contributes to successful correction and the correct final answer, we analyze whether the suffix (y n) contains indicative expressions such as wait, however, or check, which we denote as w/ Aha Moment. We report both the overall accuracy under the Modify One Step setting and the accuracy within the subset of responses that exhibit an Aha Moment. ) conditioned on the previous reasoning steps with incorrect 121 (8.1%) 47.1 104 (10.4%) 27.9 63. LLaVA-CoT [43] VL-Rethinker [36] 136 (9.1%) 52.2 i+1, , i+1, , w/ Aha Moment w/ Aha Moment 94 (9.4%) Model Num Num Acc Acc Findings. We report the performance of reasoning VLMs under Modify One Step in the right part of Fig. 2. By comparing direct generation with the Modify One Step setting, we observe that an error in single step often leads to an incorrect final answer, causing the overall accuracy to drop to the level of random guess (25%). Additionally, Table 1 shows the number of responses exhibiting aha moments (i.e., signs of self-reflection) after an error step. The results indicate that even the RL-based model VL-Rethinker exhibits aha moments in fewer than 10% of cases. Moreover, the presence of self-reflective behavior does not necessarily lead to correct final answer. More details about the settings are provided in Appendix C.3. Takeaway 1: Step-wise Self-Correction Reasoning VLMs struggle with step-wise self-correction. Even when reflection signals are present, they often fail to correct their reasoning to reach the correct answer. 3.2 Response-wise Self-correction Setup. To analyze the models response-wise self-correction behavior, we evaluate two strategies: (i) applying an self-correction prompt to guide the model in refining its responses; (ii) leveraging external critiques generated by critic models, such as Critic-V [52] and Qwen2.5-VL-7B-Instruct [2], to prompt VLMs to improve their responses based on critique feedback. The evaluation procedure 4 consists of: (1) generating the initial Direct Generation response; and (2) applying two correction strategies to obtain revised responses, reported as Internal Correction and External Critique, with the latter involving two different critique models, as illustrated in the green block of Fig. 2. Complete prompts and more details are shown in Appendix C.3. Findings. The results in the right part of Fig. 2 show that neither the correction prompt nor the external critiques effectively improve the reasoning trajectory. In some cases, model accuracy even declines after correction. Moreover, we observe that, regardless of which critic model is used to provide feedback, the accuracy of the corrected responses tends to converge to level similar to direct generation, showing little variation based on the quality of the critique. Takeaway 2: Response-wise Self-Correction Reasoning VLMs struggle with response-wise self-correction, regardless of whether using self-correction prompts or external critiques. Step-wise vs. Response-wise Self-Correction Step-wise self-correction relies on the models intrinsic ability to trigger self-reflection during intermediate reasoning steps, making it difficult to control or reliably trigger. In contrast, response-wise self-correction is more controllable and can be easily triggered by external signals like self-correction prompts. This makes it more learnable and well-defined behavior. Therefore, this paper focuses on teaching VLMs response-wise self-correction."
        },
        {
            "title": "4 Teaching VLMs to Self-Improve Their Self-Correction Abilities",
            "content": "Takeaway 1 and 2 reveal critical gap in current reasoning VLMs: models trained with either SFT or RL lack the ability to perform effective step-wise and response-wise self-correction. Once an error occurs, the models struggle to revise their reasoning, often failing to recover from mistakes. To address this limitation, we introduce Sherlock to teach the model self-correction, thereby enhancing its reasoning capabilities. The framework consists of three stages, which we will detail below. 4.1 Stage I: SFT Cold-start for Reasoning and Self-correction Recent studies [43, 47, 12, 34] have shown that supervised fine-tuning VLMs on long CoT annotated data can significantly enhance their general reasoning capabilities. To teach VLMs to self-correct, we draw inspiration from [51, 47] and aim to design an objective that simultaneously improves reasoning and self-correction. To achieve this, we introduce pairwise training objective: LSherlock-SFT(π) = E(xI&T ,Y w,Y l)DSherlock [log π(Y wxI&T ) (cid:125) (cid:124) (cid:123)(cid:122) Direct Generation + log π(Y wxI&T , l, t) (cid:125) (cid:123)(cid:122) Self-Correction (cid:124) ]. (3) We begin by randomly sampling 10k examples from the LLaVA-CoT dataset [43], denoted as DA. We then train base VLM on DA using vanilla SFT, resulting in R0 VLM, which is capable of generating CoT template responses. Next, we randomly sample an additional 10k examples, denoted as DB. Since DB is annotated with high-quality reasoning trajectories, we assume its responses are of higher quality than those sampled from the model fine-tuned on limited data, i.e., πR0 VLM(xI&T ). We then construct Sherlock-SFT dataset DSherlock = (xi i=1, where is from DB and is generated by πR0 VLM. Finally, we apply the loss defined in Eq. 3 to cold-start the base VLM, jointly training the Sherlock SFT model to acquire both reasoning and self-correction capabilities. I&T , i )10k , 4.2 Stage II: Offline Preference Training with Trajectory-level Self-correction An incorrect reasoning process may contain partially correct steps, but the loss in Eq. 3 enforces correction over the entire response, potentially introducing noise by forcing the model to revise already correct steps. To avoid such undesirable behavior and inspired by Takeaway 1, which shows that early mistakes can derail the entire reasoning trajectory, we adopt fine-grained trajectory-level offline training strategy. Instead of rewriting the full response, the model is guided to revise only the erroneous suffix, allowing it to make targeted adjustments while preserving correct reasoning n; a1) in steps. Our goal is to enable reasoning VLMs to self-correct faulty steps 1 the initial 1 = (y1 n; a2), n; a1) and produce higher-quality trajectory 2 = (y2 , , 1, , y2 1, , y1 = (y1 5 Figure 3: Training pipeline of Sherlock, including: (Left) SFT cold-start stage, (Middle) offline preference training, and (Right) online iterative self-improvement. In the SFT and offline stages, we randomly sample 10k DA and 10k DB with ground truth from the 100k LLaVA-CoT [43] dataset as supervision. During the online stage, each iteration samples only 5k unlabeled inputs, from which self-constructed and self-labeled dataset is built using the selection rule illustrated in the (Right) part. conditioned on input xI&T , correction instruction t, and the original response 1. Since the prefix 1 <i is assumed to be correct, we do not enforce revisions on these steps. The objective instead focuses on generating higher-quality suffix 2 (cid:2)p(Y i: 1 ixI&T ; 2 iπ([xI&T ,Y 1,t;Y 2 2 <i])(cid:3) <i) βDKL(ππref[xI&T , 1, t; 2 <i])(cid:3). <i) βDKL(ππref[xI&T , 1, t; (4) 1 ixI&T ; 1 <i]) (cid:2)p(Y 2 max π + iπ([xI&T ,Y 1,t;Y 1 <i]) over 1 <i. This objective offers several advantages: it leverages both 1 Here, is randomly sampled from 1 to for each example. The first expectation encourages the model to prefer the higher-quality suffix 2 <i, while staying close to the reference policy πref. Similarly, the second expectation promotes the same preference but conditioned on 1 <i as conditioning contexts, as they do not exhibit clear preference signal. Moreover, by explicitly modeling the preference between 2 i, the objective encourages successful correction and discourages wrong correction, fully leveraging the constructed data. Following previous work [25, 5, 1, 51], we note p(Y 1 xI&T ; <i) as human preference probability on suffix reasoning trajectory given instruction and prefix steps, which can be expressed as: i, given the prefix 2 2 i and 1 <i and 2 p(Y 2 1 ixI&T ; <i) = Eh[I{h prefers to 1 given xI&T and <i}], {1, 2}. (5) Building upon the formulation in [5, 51], we extend it to the trajectory-level and solve the objective in Eq. 4, resulting in the following self-correction loss: v(x, 1, t; <i, 2 <i; πθ) = β log u(x, 1, t; 1 <i, 2 <i; πθ) = β log πθ(Y 2 πref(Y i[x, 1, t; 2 i[x, 1, t; 2 <i]) <i]) (cid:125) (cid:124) (cid:124) (cid:123)(cid:122) Encourage positive self-correction i[x, 1, t; 1 <i]) i[x, 1, t; 1 <i]) (cid:125) πθ(Y 2 πref(Y (cid:123)(cid:122) Encourage positive self-correction <i, (cid:2)1 v(xI&T , l, t; β log πθ(Y 1 πref(Y 1 i[x, 1, t; 1 i[x, 1, t; 1 <i]) <i]) (cid:125) (cid:124) (cid:124) (cid:123)(cid:122) Discourage negative self-correction i[x, 1, t; 2 <i]) i[x, 1, t; 2 <i]) (cid:125) (cid:123)(cid:122) Discourage negative self-correction πθ(Y 1 πref(Y 1 β log LSC(πθ; πref) = E(x,Y w,Y l)D <i; πθ) u(xI&T , l, t; + E(x,Y w,Y l)D (cid:2)1 + v(xI&T , w, t; <i, <i; πθ) + u(xI&T , w, t; <i, <i, <i; πθ)(cid:3)2 <i; πθ)(cid:3)2 . (6) The detailed derivation and explanation are provided in Appendix A. To further enhance the models direct reasoning ability using the preference dataset, we additionally incorporate the DPO loss [29], which is jointly optimized with the self-correction loss: LSherlock(πθ; πref) = LSC(πθ; πref) + αLDPO(πθ; πref) (7) 6 Data Construction. We construct negative responses with controllable quality gaps by randomly truncating reasoning steps and injecting visual noise. Given the 10k annotated examples in DA, we sample truncation point {1, , } and generate suffix by applying visual perturbation ϵ and decoding from <i. This process yields low-quality reasoning trajectories for preference training. We train the model on the resulting dataset using Eq. 7 to obtain the Sherlock Offline model. The full data construction pipeline is detailed in Appendix B.2. Dynamic β Stabilize Preference Training. Prior work has shown that the choice of β critically influences preference optimization [5, 51, 40]. In our framework, we construct controllable preference dataset by randomly selecting truncation step and applying visual perturbation ϵ, enabling the design of sample-specific dynamic β to adaptively scale learning signals: β(i, n, ϵ) = 1 4(0.5 + (cid:0) (cid:1)0.5+ϵ/2 . ) (8) This formulation assigns larger β values to examples with earlier truncation and stronger perturbation, where the quality gap is wider, to apply more cautious updates. Conversely, smaller quality gaps lead to smaller β, encouraging more assertive learning from subtle preferences. We provide more detailed explanation of the dynamic β design in Appendix B.2. Takeaway 3: Key of Sherlock Objective The key insight of Sherlock is to fully leverage the constructed trajectory-level preference data and explicitly encourage models to revise only the incorrect suffix reasoning trajectory. 4.3 Stage III: Iterative Online Preference Training with Self-generated Data The Sherlock model has acquired both self-correction and reasoning capabilities using only small amount of data in the SFT and offline stages. Inspired by recent LLMs self-improvement works [51, 42, 5], we observe that the original and corrected responses naturally form preference pairs. Motivated by this, we develop self-improvement framework to further enhance the models capabilities. The only distinction between the online iterative training and the offline stage (Sec. 4.2) is the absence of ground-truth annotated responses w. Nonetheless, as previously analyzed, reasoning quality improves through sequential self-correction. In each iteration, we randomly sample 5k unlabeled questions. For each direct generation 1, we perform three rounds of self-correction to obtain 2, 3, and 4. We apply self-consistency filtering strategy: if the final answers of the three corrected responses are semantically identical (a2 = a3 = a4), 4 is selected as the high-quality preferred response. To further reduce noise within the preference signal, we degrade the quality of the rejected response by deriving it from the initial response 1, retaining the prefix <i, and generating perturbed suffix i through visual noise injection, following the same strategy used in the offline stage. We conduct two iterations of self-improvement training using the loss in Eq. 7, yielding the Sherlock Iter1 and Iter2 models. <i = 1 Takeaway 4: Self-Improvement Training Enabled by Self-Correction When the model can self-correct, it naturally generates pair of responsesthe original and the corrected versionswith clear quality preference, enabling iterative self-improvement."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setup Training Details. As most prior SFT-based methods [43, 47, 34] adopt the Llama3.2-Vision-11BInstruct [9] as the base VLM, Sherlock primarily focuses on integrating self-correction and reasoning within this model. In both the SFT and offline stages, we randomly sample 10k annotated examples from the LLaVA-CoT [43] dataset, and follow the procedures in Sec. 4.1 and Sec. 4.2 to train the Sherlock SFT and Offline models with Eq. 3 and Eq. 7, respectively. We then perform two rounds of self-improvement using 5k randomly sampled unlabeled data per round, containing only input questions and images. Following the selection rule in Sec. 4.3, we construct self-generated preference dataset. We then train the models using the objective in Eq. 7 with α = 0.25, resulting in 7 Table 2: Performance comparison across 8 benchmarks. #Data w/ GT indicates the number of groundtruth annotated samples used during training. MMB refers to MMBench-V1.1, Hallus denotes HallusionBench, and MathV corresponds to MathVista. Bold and underline indicate the best and second-best results, respectively. The red number represent the accuracy change after self-correction. Models #Data w/ GT MMB MMVet Hallus MMMU MMStar AI2D MathV MME Avg. Llama3.2V-11B-Ins [9] - 65.8 57.6 42. 47.8 53.0 88.2 49.7 1822 58.7 Reasoning Models LLaVA-CoT [43] + Self-Correction Mulberry [47] + Self-Correction LlamaV-o1 [34] + Self-Correction Ours Sherlock Models Sherlock SFT + Self-Correction Sherlock Offline + Self-Correction Sherlock Iter1 + Self-Correction Sherlock Iter + Self-Correction 100k 260k 175k 10k 10k 0 0 75.0 74.4 75.2 74.2 75.6 18.4 72.2 73.8 73.2 74.7 74.9 76.6 74.6 77.2 61.7 62.3 58.3 59.0 61.9 50.9 61.4 62.8 61.4 63.8 62.3 62.7 62.4 62. 47.7 46.4 47.8 46.6 45.6 39.4 45.5 47.5 48.1 48.9 49.7 50.6 48.7 51.2 49.1 49.2 46.7 46.9 52.3 43.9 47.1 46.2 47.6 49.0 48.2 49.2 49.7 50.1 57.6 57.8 57.8 57.4 56.5 47.1 54.9 55.9 57.5 57.7 57.0 58.8 57.7 59. 82.9 82.9 86.2 86.3 86.4 76.9 86.6 87.9 88.4 89.5 88.9 90.0 89.6 90.6 53.7 53.0 61.9 62.3 53.3 44.0 52.0 52.2 52.2 53.9 52.2 54.4 52.0 54.0 2177 63.2 2183 63.00.2 2170 63.9 2177 63.80.1 2125 63.4 1823 48.215.2 2170 62.2 2172 63.00.8 2162 63.2 2171 64.41.2 2177 63.9 2195 65.11.2 2197 64.1 2204 65.41.3 Sherlock Iter1 and Iter2. Training is conducted with LLaMA-Factory [57] on 8A100 GPUs. More implementation details, including data construction and hyperparameters are shown in Appendix B. Baselines. We evaluate Sherlock from two perspectives: reasoning and self-correction. For reasoning, we compare against methods built on the Llama3.2-Vision-11B-Instruct [9] model, including LLaVA-CoT [43], Mulberry [47], and LlamaV-o1 [34]. To assess self-correction as an inference-time scaling strategy, we compare Sherlock with baselines such as Majority Vote and critique-based methods using LLaVA-Critic [41], Critic-V [52], and Qwen2.5-VL-7B-Instruct [2] as critique generators. We present detailed introduction of baseline methods in Appendix C.2. Evaluation Details. We conduct experiments on eight challenging multimodal tasks, covering comprehensive VQA benchmarks (MMBench-V1.1 [20], MMVet [49], MME [17], MMStar [4]), math and science benchmarks (MathVista [21], AI2D [15], MMMU [50]), and hallucination benchmark (HallusionBench [10]). We evaluate Sherlock under two settings: direct generation and after three rounds of self-correction. For consistency and reproducibility, all models are evaluated using the VLMEvalKit [8] pipeline. More evaluation details are provided in Appendix C.1 and C.4. 5.2 Main Results Advancing Reasoning Ability with Minimal Annotation. The results in Table 2 show that Sherlock Iter2, trained on only 20k annotated examples randomly sampled from LLaVA-CoT, achieves the best overall reasoning performance. It outperforms LLaVA-CoT [43] (100k annotations), Mulberry [47] (260k), and LlamaV-o1 [34] (170k) across 8 benchmarks. Notably, even before online self-improvement, Sherlock Offline performs comparably to LLaVA-CoT, highlighting that incorporating self-correction significantly enhances direct generation quality. During online training, both reasoning accuracy (63.263.964.1) and self-correction gains (1.21.3) steadily improve, demonstrating the effectiveness of self-generated preference data and self-improvement training. Existing Models Fail to Self-correct Their Reasoning. We present the performance of reasoning VLMs after three rounds of self-correction in Table 2. Consistent with our observation in Takeaway 2, existing reasoning models fail to effectively improve their accuracy through internal prompt-based self-correction, and even suffer average performance degradation across eight benchmarks. Notably, 8 Table 3: Performance of inference-time methods. LLaVA-Critic, Critic-V, and Qwen2.5-VL-7B provide critiques for correction, while Majority Vote @8 selects an answer from 8 sampled generations. Methods Sherlock Iter2 + LLaVA-Critic [41] + Critic-V [52] + Qwen2.5-VL-7B [2] + Majority Vote @8 + Self-Correction MMB MMVet Hallus MMMU MMStar AI2D MathV MME Avg. 74.6 75.5 73.9 76.5 78.5 77.2 62.4 58.9 61.8 64.4 62.2 62.6 48.7 45.9 47.0 48.6 49.3 51.2 49.7 47.0 47.7 47.9 49.7 50.1 57.7 58.7 58.1 59.3 58.0 59.0 89.6 89.1 88.9 89.1 91.1 90. 52.0 52.6 50.2 55.5 54.0 54.0 2197 64.1 2122 62.91.2 2192 63.20.9 2189 64.90.8 2195 65.11.0 2204 65.41.3 Figure 4: Average accuracy across 8 benchmarks for ablation settings. w/ i=1 indicates that the objective in Eq. 6 performs self-correction on the entire response instead of trajectory-level. LlamaV-o1 suffers significant performance drop after self-correction. This is because its step-bystep reasoning is produced through multi-turn prompting. To apply self-correction, we append correction prompt after the final turn. However, this differs from the models original training format, resulting in even worse responses. Sherlock Further Enhances Performance via Self-Correction. Table 2 also reports the results of applying three rounds of self-correction to the Sherlock model. During the SFT stage, we adopt only response-wise self-correction objective, without explicitly revising incorrect trajectories (Y i) as in preference training. After both offline and online training, the model shows clear improvements of self-correction over Sherlock SFT, confirming the effectiveness of our trajectory-level objective. In Table 3, we compare various inference-time scaling strategies. Except for Majority Vote, all approaches rely on external critic models to generate response critiques. Interestingly, unlike observations in Sec. 3.2, we find that the effectiveness of critique-based correction depends heavily on the critics quality. Qwen2.5-VL-7B-Instruct [2], the strongest critic, yields the best performance, surpassing even self-correction on some benchmarks. In contrast, critiques from weaker models like LLaVA-Critic [41] and Critic-V [52] lead to performance drops, likely due to their inability to handle long CoT responses. We attribute this sensitivity to Sherlocks learned self-correction ability, which makes it more responsive to contextual signals. Moreover, three rounds of sequential self-correction outperform Majority Vote with eight sampled responses, while using only 50% of the inference time. 5.3 Ablation Studies on Training Objective We ablate our Sherlock pipeline, including the combination of DPO loss and self-correction loss in Eq. 7, the selection strategy for the prefix step in Eq. 6, and the design of the dynamic β in Eq. 8. Finding 1: Self-correction and Reasoning are Not Orthogonal Capabilities. As shown in Fig. 4, we compare the full Sherlock model with two ablated variants trained using only the DPO loss or only the self-correction loss. We observe that using only the DPO loss brings minimal improvement over Sherlock-SFT. In contrast, the model trained solely with the self-correction objective continues to improve across iterations. Although its overall performance is slightly lower than the full Sherlock model, it achieves direct reasoning accuracy comparable to other baselines, despite receiving no explicit supervision for direct generation. This finding aligns with prior work showing that critiquebased supervision can benefit direct reasoning [38]. Interestingly, even the DPO-only model shows gains in self-correction over iterations, despite having access to self-correction signals only during 9 Table 4: Performance comparison of different methods using the same 20k annotated data. Methods MMB MMVet Hallus MMMU MMStar AI2D MathV MME Avg. LLaVA-CoT 20k + Self-Correction Sherlock SFT 20k + Self-Correction Sherlock Offline + Self-Correction 72.0 71.6 72.1 74.1 73.2 74.7 58.9 58.6 61.2 62.2 61.4 63.8 47.9 46.8 47.6 47.8 48.1 48.9 47.9 48.7 45.6 46.0 47.6 49.0 55.3 56.0 57.0 58.1 57.5 57.7 88.3 87.7 88.0 89.0 88.4 89. 48.7 49.8 52.5 53.2 52.2 53.9 2158 62.0 2109 61.80.2 2186 62.8 2159 63.40.6 2162 63.2 2171 64.41.2 the initial SFT stage. These results collectively suggest that self-correction and reasoning are not independent abilities, but mutually reinforcing: learning one promotes the other. Finding 2: Trajectory-level Objective Yields Superior Self-correction in Reasoning VLMs. In Fig. 4, we also compare different strategies for selecting step in Eq. 6. Setting = 1 reduces the objective to full response correction, which forces revision over the entire response. In contrast, Sherlock adopts trajectory-level correction strategy that only modifies the low-quality suffix i identified during data construction, avoiding updates on prefix steps <i that lack clear preference signals. The performance of the offline model remains because, in the offline preference data, the chosen and rejected responses share identical prefixes, thus contributing no gradients under the Sherlock objective. However, in online iterations, using full-response correction leads to degraded self-correction ability compared to the offline baseline. We attribute this to the fact that strong preference signals typically reside in i, and forcing updates on non-informative prefixes hinders effective self-correction learning. <i and Finding 3: Dynamic β Leads to Stable Training. We present results in Fig. 4 comparing whether the dynamic β is applied to Eq. 6. Experiments show that incorporating dynamic β consistently improves both direct generation and self-correction performance across iterations. These findings validate that our sample-specific dynamic β contributes to more stable training and stronger models. 5.4 Why Does Sherlock Need Less Annotated Data? Sherlock requires only 20k randomly sampled annotated examples because it fully leverages each example through its training objectives. To highlight Sherlocks data efficiency, we compare it against two alternative training strategies under the same 20k data budget: (i) training with standard SFT loss as adopted in LLaVA-CoT, without any self-correction objective, and (ii) training only using Sherlock-SFT loss defined in Eq. 3, without applying the offline preference training objective. Integrating Self-correction into SFT: Free Lunch for Enhancing Reasoning. Under the same 20k data setting, the only difference between Sherlock SFT and LLaVA-CoT lies in the selfcorrection term in Eq. 3. As shown in Table 4, Sherlock achieves an average accuracy that is 0.8 points higher than LLaVA-CoT in the direct reasoning setting. This aligns with our Finding 1 in Sec. 5.3, indicating that learning self-correction can enhance the models direct reasoning ability. Moreover, Sherlock further improves performance after self-correction, thereby widening the gap compared to LLaVA-CoT. Trajectory-level Objective Maximizes the Use of Preference Data. Table 4 shows that SherlockOffline consistently outperforms Sherlock-SFT, both in terms of direct reasoning performance (63.2 & 62.8) and improvements (1.2 & 0.6) brought by self-correction. While the Sherlock-SFT loss (Eq. 3) focuses solely on guiding the model to revise low-quality responses into high-quality ones, the Sherlock-Offline loss (Eq. 6) incorporates three additional objectives: (1) discouraging the model from generating the same low-quality response again, (2) preventing quality degradation when the initial response is already high-quality, and (3) encouraging the model to reproduce the same output when the initial reasoning is correct. These complementary signals help the model fully utilize the preference data to learn more fine-grained and robust self-correction behavior. 10 5.5 Efficiently Scaling Up Sequential Self-Correction with Verifiers Recalling the results in Sec. 5.2, where we validated that the Sherlock model achieves stable inference-time scaling through self-correction. In this section, inspired by recent work MMVerify [30], we conduct deeper exploration aimed at simultaneously improving the efficiency and capability of self-correction-based inference-time scaling. Table 5: Inference-time scaling results with MMVerify on MathVista. Methods Acc GPU Hours Sherlock Iter2 + Self-Correction w/o Verify + Parallel Vote w/ Verify + Self-Correction w/ Verify 52.0 54.0 55.1 55. 3.3 13.2 40.2 8.7 Unlike MM-Verify, which generates responses in parallel and selects the top (verified as correct) for majority voting, we adopt Sherlocks sequential self-correction sampling during inference. After each response, verifier assesses correctness; if incorrect, the critique from MM-Verify is retained to guide the next self-correction round. This process continues until response is verified as correct, at which point sampling stops. We evaluate this strategy against parallel majority voting on MathVista [21], reporting both accuracy and inference time (in GPU hours on NVIDIA A100) in Table 5. Results show that integrating strong verifier not only improves accuracy but also reduces inference cost. Sherlocks sequential self-correction demonstrates greater efficiency and effectiveness, highlighting its scalability for complex reasoning tasks."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this paper, we introduce Sherlock, the first framework to achieve intrinsic self-correction in reasoning VLMs, with significant improvements across diverse benchmarks. Our analysis reveals that existing reasoning VLMs, whether trained with SFT or RL, struggle to self-correct. Sherlock addresses this gap with novel trajectory-level objective, enabling self-correction and self-improvement using only 20k annotated examples. Our findings show that self-correction is not only feasible but also powerful strategy to improve reasoning and inference-time scaling in VLMs. This makes VLMs more suitable for complex tasks, especially those where generating correct answer in single pass is difficult. Looking ahead, Sherlock provides promising approach for extending self-correction to other types of reasoning models. It can also be further enhanced by incorporating step-wise self-correction for more efficient self-correction."
        },
        {
            "title": "References",
            "content": "[1] M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. [2] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] H. Chen, H. Tu, F. Wang, H. Liu, X. Tang, X. Du, Y. Zhou, and C. Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models, 2025. [4] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [5] E. Choi, A. Ahmadian, M. Geist, O. Pietquin, and M. G. Azar. Self-improving robust preference optimization. arXiv preprint arXiv:2406.01660, 2024. [6] Y. Ding, L. Li, B. Cao, and J. Shao. Rethinking bottlenecks in safety fine-tuning of vision language models. arXiv preprint arXiv:2501.18533, 2025. [7] Y. Dong, Z. Liu, H.-L. Sun, J. Yang, W. Hu, Y. Rao, and Z. Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. [8] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 11 [9] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [11] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] J. Guo, T. Zheng, Y. Bai, B. Li, Y. Wang, K. Zhu, Y. Li, G. Neubig, W. Chen, and X. Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. [13] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [14] R. Kamoi, Y. Zhang, N. Zhang, J. Han, and R. Zhang. When can llms actually correct their own mistakes? critical survey of self-correction of llms. Transactions of the Association for Computational Linguistics, 12:14171440, 2024. [15] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [16] A. Kumar, V. Zhuang, R. Agarwal, Y. Su, J. D. Co-Reyes, A. Singh, K. Baumli, S. Iqbal, C. Bishop, R. Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. [17] Z. Liang, Y. Xu, Y. Hong, P. Shang, Q. Wang, Q. Fu, and K. Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. [18] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [19] X. Liu, J. Ni, Z. Wu, C. Du, L. Dou, H. Wang, T. Pang, and M. Q. Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [20] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. In European conference on Mmbench: Is your multi-modal model an all-around player? computer vision, pages 216233. Springer, 2024. [21] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [22] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [23] N. McAleese, R. M. Pokorny, J. F. C. Uribe, E. Nitishinskaya, M. Trebacz, and J. Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. [24] F. Meng. Lingxiao du, zongkai liu, zhixiang zhou, quanfeng lu, daocheng fu, botian shi, wenhai wang, junjun he, kaipeng zhang, et al. mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 1, 2025. [25] R. Munos, M. Valko, D. Calandriello, M. G. Azar, M. Rowland, Z. D. Guo, Y. Tang, M. Geist, T. Mesnard, A. Michi, et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886, 18, 2023. [26] Y. Peng, X. Wang, Y. Wei, J. Pei, W. Qiu, A. Jian, Y. Hao, J. Pan, T. Xie, L. Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [27] Y. Peng, G. Zhang, M. Zhang, Z. You, J. Liu, Q. Zhu, K. Yang, X. Xu, X. Geng, and X. Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [28] Y. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems, 37:55249 55285, 2024. [29] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly reward model. ArXiv, abs/2305.18290, 2023. [30] L. Sun, H. Liang, J. Wei, B. Yu, T. Li, F. Yang, Z. Zhou, and W. Zhang. Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification. arXiv preprint arXiv:2502.13383, 2025. [31] H. Tan, Y. Ji, X. Hao, M. Lin, P. Wang, Z. Wang, and S. Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [32] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [33] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [34] O. Thawakar, D. Dissanayake, K. More, R. Thawkar, A. Heakl, N. Ahsan, Y. Li, M. Zumri, J. Lahoud, R. M. Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [35] G. Tyen, H. Mansoor, V. Carbune, P. Chen, and T. Mak. Llms cannot find reasoning errors, but can correct them given the error location. arXiv preprint arXiv:2311.08516, 2023. [36] H. Wang, C. Qu, Z. Huang, W. Chu, F. Lin, and W. Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [37] X. Wang, Z. Yang, C. Feng, H. Lu, L. Li, C.-C. Lin, K. Lin, F. Huang, and L. Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. [38] Y. Wang, X. Yue, and W. Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate. arXiv preprint arXiv:2501.17703, 2025. [39] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [40] J. Wu, Y. Xie, Z. Yang, J. Wu, J. Gao, B. Ding, X. Wang, and X. He. beta-dpo: Direct preference optimization with dynamic β. Advances in Neural Information Processing Systems, 37:129944129966, 2024. [41] T. Xiong, X. Wang, D. Guo, Q. Ye, H. Fan, Q. Gu, H. Huang, and C. Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. [42] W. Xiong, H. Zhang, C. Ye, L. Chen, N. Jiang, and T. Zhang. Self-rewarding correction for mathematical reasoning. arXiv preprint arXiv:2502.19613, 2025. [43] G. Xu, P. Jin, L. Hao, Y. Song, L. Sun, and L. Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [44] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [45] S. Yang, E. Gribovskaya, N. Kassner, M. Geva, and S. Riedel. Do large language models latently perform multi-hop reasoning? arXiv preprint arXiv:2402.16837, 2024. [46] Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [47] H. Yao, J. Huang, W. Wu, J. Zhang, Y. Wang, S. Liu, Y. Wang, Y. Song, H. Feng, L. Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [48] S. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, and M. Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, 2023. 13 [49] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [50] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [51] Y. Zeng, X. Cui, X. Jin, G. Liu, Z. Sun, Q. He, D. Li, N. Yang, J. Hao, H. Zhang, et al. Aries: Stimulating self-refinement of large language models by iterative preference optimization. arXiv preprint arXiv:2502.05605, 2025. [52] D. Zhang, J. Li, J. Lei, X. Wang, Y. Liu, Z. Yang, J. Li, W. Wang, S. Yang, J. Wu, et al. Critic-v: Vlm critics help catch vlm errors in multimodal reasoning. arXiv preprint arXiv:2411.18203, 2024. [53] J. Zhang, J. Huang, H. Yao, S. Liu, X. Zhang, S. Lu, and D. Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [54] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, Y. Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [55] R. Zhang, B. Zhang, Y. Li, H. Zhang, Z. Sun, Z. Gan, Y. Yang, R. Pang, and Y. Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [56] Y. Zhang, M. Khalifa, L. Logeswaran, J. Kim, M. Lee, H. Lee, and L. Wang. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024. [57] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. [58] K. Zhou, C. Liu, X. Zhao, S. Jangam, J. Srinivasa, G. Liu, D. Song, and X. E. Wang. The hidden risks of large reasoning models: safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025. [59] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Mathematical Derivations",
            "content": "In this section, we provide detailed derivation of our Sherlock objective leading to Eq. 6. We extend the derivation structure of prior methods such as SRPO [5] and ARIES [51] from the response level to more fine-grained trajectory level. (cid:20) max π + iπ([xI&T ,Y 1,t;Y 1 2 <i]) (cid:20) p(Y iπ([xI&T ,Y 1,t;Y 2 2 <i]) p(Y 2 1 ixI&T ; 1 <i) βDKL(ππref[xI&T , 1, t; 1 ixI&T ; 2 <i) βDKL(ππref[xI&T , 1, t; 2 (cid:21) <i]) (cid:21) <i]) (9) (cid:21) = max π iπ([xI&T ,Y 1,t;Y 2 2 <i]) (cid:20) p(Y 2 1 ixI&T ; 2 <i) β log + iπ([xI&T ,Y 1,t;Y 1 <i]) (cid:20) p(Y 2 1 ixI&T ; 1 <i) β log i[xI&T , 1, t; 2 <i]) i[xI&T , 1, t; 2 <i]) (cid:21) π(Y 2 πref(Y 2 i[xI&T , 1, t; 1 <i]) i[xI&T , 1, t; 1 <i]) π(Y 2 πref(Y 2 = max π βE iπ([xI&T ,Y 1,t;Y 2 2 <i]) (cid:20) log exp + βE iπ([xI&T ,Y 1,t;Y 1 <i]) (cid:20) log exp (cid:18) p(Y 2 (cid:18) p(Y 2 1 ixI&T ; 2 β 1 ixI&T ; 1 β <i) (cid:19) <i) (cid:19) log log π(Y 2 πref(Y 2 (10) i[xI&T , 1, t; 2 <i]) i[xI&T , 1, t; 2 <i]) (cid:21) π(Y 2 πref(Y 2 i[xI&T , 1, t; 1 <i]) i[xI&T , 1, t; 1 <i]) (cid:21) = max π βE iπ([xI&T ,Y 1,t;Y 2 2 <i]) (cid:20) log π(Y i[xI&T , 1, t; 2 <i])Z(xI&T , 1, t; 2 πref(Y 2 i[xI&T , 1, t; 2 <i]) exp (cid:18) p(Y iY 1 β (11) <i) ixI&T ;Y 2 <i) (cid:21) (cid:19) βE iπ([xI&T ,Y 1,t;Y 1 2 <i]) (cid:20) log π(Y 2 i[xI&T , 1, t; 1 <i])Z(xI&T , 1, t; 1 <i) ixI&T ;Y 1 <i) (cid:21) (cid:19) (cid:18) p(Y 2 iY 1 β πref(Y 2 i[xI&T , 1, t; <i]) exp + β log Z(xI&T , 1, t; 2 <i) + β log Z(xI&T , 1, t; 1 <i) = max π βDKL i[xI&T , 1, t; 2 π(Y 2 (cid:13) (cid:13) <i]) (cid:13) (cid:13) πref(Y 2 i[xI&T , 1, t; <i]) exp Z(xI&T , 1, t; 2 <i) (cid:16) p(Y 2 iY 1 (12) ixI&T ;Y 2 <i) β (cid:17) (cid:17) (13) π(Y 2 βDKL i[xI&T , 1, t; 1 (cid:13) (cid:13) <i]) (cid:13) (cid:13) πref(Y 2 i[xI&T , 1, t; 2 <i]) exp Z(xI&T , 1, t; 1 <i) (cid:16) p(Y 2 iY ixI&T ;Y 1 <i) β + β log Z(xI&T , 1, t; 2 where Z(X, 1, z; 2 negativity of the KL divergence, the optimal proxy is given by: <i) + β log Z(xI&T , 1, t; 1 <i) and Z(X, 1, z; <i), <i) denote the partition functions. Noting the nonπ(Y 2 i[xI&T , 1, t; 2 <i]) = π(Y i[xI&T , 1, t; 1 <i]) = πref(Y 2 i[xI&T , 1, t; 2 (cid:16) p(Y 2 iY ixI&T ;Y 2 <i) β <i]) exp Z(xI&T , 1, t; 2 <i) (cid:16) p(Y 2 <i]) exp Z(xI&T , 1, t; <i) (cid:17) (cid:17) (14) iY 1 ixI&T ;Y <i) β . (15) πref(Y 2 i[xI&T , 1, t; Therefore, we have: p(Y 2 1 ixI&T ; 2 <i) = β log p(Y 1 ixI&T ; 1 <i) = β log π(Y 2 πref(Y 2 π(Y 2 πref(Y 2 i[xI&T , 1, t; 2 i[xI&T , 1, t; 2 i[xI&T , 1, t; 1 i[xI&T , 1, t; 1 <i]) <i]) <i]) <i]) + β log Z(xI&T , 1, t; 2 <i) (16) + β log Z(xI&T , 1, t; 1 <i) (17) 15 Recalling the definition of p(Y 2 and p(Y ixI&T ; 1 1 1 ixI&T ) in Eq. 5, we have p(Y 1 1 ixI&T ; <i) = 1 2 , <i) = 1 π(Y 1 πref(Y 1 π(Y 1 πref(Y 1 2 . Then, combining it with Eq. 16, and 17, we obtain: i[xI&T , 1, t; 1 i[xI&T , 1, t; 1 i[xI&T , 1, t; 2 i[xI&T , 1, t; 2 + β log Z(xI&T , 1, t; 1 + β log Z(xI&T , 1, t; 2 <i]) <i]) <i]) <i]) <i) <i) 1 2 1 2 = β log = β log (18) (19) By subtracting Eq. 18 from Eq. 16, and Eq. 19 from Eq. 17, we obtain: (cid:32) p(Y 2 1 ixI&T ; <i) = β log 1 2 log π(Y 1 πref(Y i[xI&T , 1, t; 1 i[xI&T , 1, t; 1 <i]) <i]) + β(log Z(xI&T , 1, t; 2 π(Y 2 πref(Y 2 i[xI&T , 1, t; 2 i[xI&T , 1, t; 2 <i) log Z(xI&T , 1, t; 1 <i])πref(Y 1 <i])π(Y 1 = β log (cid:32) p(Y 2 1 ixI&T ; 1 <i) (cid:32) = β log 1 2 + β(log Z(xI&T , 1, t; 1 π(Y 2 πref(Y 2 i[xI&T , 1, t; 1 i[xI&T , 1, t; 1 <i) log Z(xI&T , 1, t; 2 <i])πref(Y 1 <i])π(Y = β log (cid:32) <i]) <i]) π(Y 2 πref(Y 2 i[xI&T , 1, t; 2 i[xI&T , 1, t; 2 <i)) i[xI&T , 1, t; 1 i[xI&T , 1, t; 1 <i]) <i]) (cid:33) π(Y 2 πref(Y 2 i[xI&T , 1, t; 1 i[xI&T , 1, t; 1 <i)) i[xI&T , 1, t; 2 i[xI&T , 1, t; 2 <i]) <i]) (cid:33) + β log (cid:19) (cid:18) Z(xI&T , 1, t; 2 <i) Z(xI&T , 1, t; 1 <i) (20) <i]) <i]) log π(Y 1 πref(Y 1 i[xI&T , 1, t; 2 i[xI&T , 1, t; <i]) <i]) + β log (cid:19) (cid:18) Z(xI&T , 1, t; 1 <i) Z(xI&T , 1, t; 2 <i) (21) (cid:33) (cid:33) Next, by adding Eq. 20 and Eq. 21, we obtain the following expression: v(x, 1, t; 1 <i, 2 <i; πθ) = β log u(x, 1, t; 1 <i, <i; πθ) = β log β log πθ(Y 2 πref(Y 2 πθ(Y 2 πref(Y 2 ixI&T ; 2 i[x, 1, t; 2 i[x, 1, t; 2 i[x, 1, t; 1 i[x, 1, t; 1 <i) + p(Y 2 <i]) <i]) <i]) <i]) 1 <i) + u(xI&T , 1, t; 1 <i, β log ixI&T ; 1 <i) 1 <i) <i, 2 πθ(Y 1 πref(Y 1 πθ(Y 1 πref(Y 1 i[x, 1, t; 1 i[x, 1, t; 1 i[x, 1, t; 2 i[x, 1, t; <i]) <i]) <i]) <i]) (22) 1 p(Y 2 = v(xI&T , 1, t; 1 Finally, we follow prior work [51, 25, 1] to adopt the Mean Squared Error (MSE) as the loss function and parameterize the policy model as πθ. The training is then performed on the preference dataset using the following loss: v(x, l, t; <i, <i; πθ) = β log u(x, l, t; <i, <i; πθ) = β log v(x, w, t; <i, <i; πθ) = β log u(x, w, t; <i, <i; πθ) = β log (cid:124) (cid:124) πθ(Y πref(Y i[x, l, t; <i]) i[x, l, t; <i]) (cid:125) (cid:123)(cid:122) Encourage positive self-correction i[x, l, t; <i]) i[x, l, t; <i]) (cid:125) πθ(Y πref(Y (cid:123)(cid:122) Encourage positive self-correction i[x, w, t; i[x, w, t; πθ(Y πref(Y (cid:124) (cid:124) (cid:123)(cid:122) Discourage negative self-correction i[x, w, t; <i]) i[x, w, t; <i]) (cid:125) (cid:123)(cid:122) Discourage negative self-correction πθ(Y πref(Y β log (cid:124) β log (cid:124) <i]) <i]) (cid:125) πθ(Y πref(Y i[x, l, t; i[x, l, t; (cid:123)(cid:122) Discourage no self-correction i[x, l, t; πθ(Y <i]) i[x, l, t; πref(Y <i]) (cid:125) (cid:123)(cid:122) Discourage no self-correction <i]) <i]) (cid:125) β log (cid:124) β log πθ(Y πref(Y i[x, w, t; <i]) i[x, w, t; <i]) (cid:125) (cid:123)(cid:122) Encourage no self-correction i[x, w, t; πθ(Y i[x, w, t; πref(Y <i]) <i]) (cid:125) (cid:124) (cid:123)(cid:122) Encourage no self-correction 16 LSC(πθ; πref) = E(x,Y w,Y l)D (cid:2)1 v(xI&T , l, t; <i, <i; πθ) u(xI&T , l, t; + E(x,Y w,Y l)D (cid:2)1 + v(xI&T , w, t; <i, <i; πθ) + u(xI&T , w, t; <i, <i, <i; πθ)(cid:3) <i; πθ)(cid:3)2 . (23) Our self-correction loss is designed to primarily encourage the model to learn desirable self-correction behavior. Specifically, it promotes positive self-correction: revising low-quality response into high-quality one w, and supports no self-correction when the initial response is already correct, i.e., from to w. Conversely, the loss penalizes negative self-correction, such as degrading good response into poor one l, as well as failing to revise an incorrect response, i.e., to l."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Self-Correction Prompt Considering the four-stage thinking structure of the LLaVA-CoT [43] dataset, we design the following self-correction template by modifying the template proposed in ARIES [51]. Self-Correction Prompt Below is QUESTION from user and an EXAMPLE RESPONSE. Please provide more helpful RESPONSE, improving the EXAMPLE RESPONSE by making the content even clearer, more accurate, and with reasonable logic. Focus on addressing the humans QUESTION step by step based on the image without including irrelevant content. QUESTION: {question} EXAMPLE RESPONSE: {example_response} Now, refine and improve the RESPONSE further. You can consider two approaches: 1. REFINEMENT: If the SUMMARY section in the response is closely related to the question, the CAPTION section accurately describes the image, the REASONING section is logically clear and correct without any contradictions, and the CONCLUSION provides an accurate answer based on the previous steps, enhance clarity, accuracy, or reasoning logic as needed. 2. NEW RESPONSE: If the SUMMARY section incorrectly summarizes the intent of the issue, the CAPTION contains content unrelated to or incorrect about the image, there are logical errors or contradictions in the REASONING, or the CONCLUSION incorrectly states the findings, please enhance the accuracy and quality of each step, and craft more effective RESPONSE that thoroughly resolves the QUESTION. RESPONSE: B.2 Data Construction and Training Details In this section, we present the detailed data construction procedures for each stage of the Sherlock framework. Specifically, total of 20k annotated examples and 10k unlabeled questions are randomly sampled from the LLaVA-CoT [43] dataset for training. Stage I: SFT Cold-Starting. We randomly sample two sets of 10k annotated examples from the LLaVA-CoT dataset, denoted as DA and DB. We first use DA to train the Llama3.2-Vision-11BInstruct [9] model for one epoch using vanilla SFT loss as follows, enabling the model to generate responses in fixed reasoning template: LSFT(π) = E(xI&T ,Y )DA [log π(Y xI&T )] (24) We denote the resulting model as R0 VLM. Next, we sample responses on DB using R0 VLM, i.e., πR0 VLM( xI&T ), which yields relatively low-quality outputs following the CoT template. Each sampled response is paired with its corresponding annotated answer and input question to 17 Table 6: Detailed training hyperparameters for each stage of the Sherlock model. Sherlock model Learning Rate Max Length Batch Size α βDPO Warm-Up Ratio Epoch SFT Offline Iter1 Iter2 1e-6 5e-6 5e-7 5e-7 4096 4096 4096 128 32 32 32 0.25 0.25 0.25 0.25 0.1 0.1 0.1 0.1 0.03 0.00 0.00 0.00 3 1 1 1 Figure 5: Values of dynamic β under different truncation steps and visual perturbation levels ϵ. form the Sherlock SFT dataset, denoted as DSherlock-SFT = {x, l, w}. Finally, we apply the SherlockSFT loss defined in Eq. 3 to cold-start the base VLM, jointly equipping it with both reasoning and self-correction capabilities. Stage II: Offline Training. In the offline preference training stage, we construct trajectorylevel preference data using the 10k examples in DA. The LLaVA-CoT dataset comprises four reasoning stages: <SUMMARY>, <CAPTION>, <REASONING>, and <CONCLUSION>, where the final <CONCLUSION> stage only provides the answer a. We randomly sample truncation step U{1, 2, 3, 4} to determine the prefix. Specifically, when = 1, no prefix is retained; when = 2, 3, or 4, we retain the SUMMARY, SUMMARY+CAPTION, and SUMMARY+CAPTION+ the first half of the REASONING stage, respectively. To generate lower-quality suffix reasoning trajectory i π(xIϵ&T ; Y<i), we introduce random visual perturbation ϵ to the image input, which affects the models visual perception during subsequent decoding. Stage III: Online Self-Improvement. With the reasoning and self-correction capabilities of the Sherlock Offline model already activated, we aim to leverage the natural preference relationship between responses before and after correction to self-construct preference dataset for iterative self-improvement training. Specifically, for each randomly sampled input question, we perform direct reasoning followed by three rounds of self-correction, obtaining responses 1, 2, 3, and 4. To reduce noise in the preference pairs, we apply self-consistency filtering strategy: we only retain 4 as the preferred response if the final answers in 2, 3, and 4 are semantically consistent. To construct the rejected response l, we use 1 as the base and follow the same perturbation-based procedure as in Stage 2. It is worth noting that, in Stage 2, the chosen and rejected responses share <i. However, in Stage 3, since 4 is generated via the same prefix by construction, i.e., three turn self-correction based on 1, their prefixes often differ, and thus the prefix portions of the preference pairs <i are typically not equal. <i and <i = Dynamic β. We build upon static baseline of β = 0.25 and introduce controlled dynamism through randomly sampled truncation steps and image perturbation levels ϵ. Considering that as reasoning progresses, the models reliance on visual input typically decreases with longer generations, we aim to make β less sensitive to changes in ϵ when is large. In addition, we aim to adopt more conservative policy updates when the quality gap between preference pairs is large, that is, when the truncation step is small and the image perturbation ϵ is large, while encouraging more assertive learning when the quality gap is small. Therefore, β should be negatively correlated with and positively correlated with ϵ. In Fig. 5, we visualize how β varies with ϵ for = 1, 2, 3, 4, demonstrating that the dynamic β design in Eq. 8 aligns with our preferred properties. 18 Training Details. Our Sherlock is built on LlamaV3.2-Vision-11B-Instruct [34] model. In both the SFT and offline training stages, we randomly sample 10k annotated examples from the LLaVACoT [43] dataset, and follow the procedures introduced above to train the Sherlock SFT and Offline models with Eq. 3 and Eq. 7, respectively. We then perform two rounds of online self-improvement using 5k unlabeled data per round, containing only input questions and images. Following the self-consistency selection rule in Sec. 4.3, we construct self-generated preference dataset. We then train the models using the objective in Eq. 7, resulting in Sherlock Iter1 and Iter2. The detailed training hyperparameter is provided in Table 6. Sherlock Algorithm. To provide more intuitive understanding of our Sherlock, we summarize the detailed algorithm in Algorithm 1, including the training data, loss functions, and learning procedures. Algorithm 1: Sherlock: Self-correction and Self-improvement training framework Input: DA = (xI&T , w), DB = (xI&T , w), base VLM π, iteration , correction prompt Output: Sherlock model πSherlock 1 Stage 1: SFT Cold-Start 2 πVLM-R arg min LSFT(πθ) (Eq. 24) on DA θ 3 DSherlock-SFT {(xI&T , l, w)Y πR0 VLM(xI&T ), (xI&T , w) DB} 4 πSherlock-SFT arg min LSherlock-SFT(πθ) (Eq. 3) on DSherlock-SFT θ 5 Stage 2: Offline Preference Training 6 Doffline for (xI&T , w) in DA do U{0, 1, 2, 3}, ϵ U(0, 1) 7 <i Doffline Doffline {xI&T , l, w, β} π(xIϵ&T ; <i, 8 9 10 πSherlock-Offline arg min θ 11 Stage 3: Online Iterative Self-Improvement 12 πSherlock-Iter 0 πSherlock-Offline 13 for = 1 to do 14 DIter while DIter < 5000 do <i), β β(i, 4, ϵ) (Eq. 8) LSherlock(πθ; πref) (Eq. 7) on DOffline 15 16 17 18 20 21 22 23 24 Randomly sample question xI&T without any annotation 1 πSherlock-Iter t1(xI&T ) for = 1 to 3 do j+1 πSherlock-Iter t1(xI&T , j, t) if a2 = a3 = a4 then U{1, 2, 3, 4}, ϵ U(0, 1), 4, l πSherlock-Iter t1(xIϵ&T , <i) DIter DIter {xI&T , l, w, β} <i 1 <i, β β(i, 4, ϵ) (Eq. 8) else Skip πSherlock-Iter arg min θ LSherlock(πθ; πref) (Eq. 7) on DIter t"
        },
        {
            "title": "C Evaluation Detail",
            "content": "C.1 Evaluation Benchmarks We conduct experiments on eight multimodal tasks, covering comprehensive VQA benchmarks (MMBench-V1.1 [20], MMVet [49], MME [17], MMStar [4]), math and science benchmarks (MathVista [21], AI2D [15], MMMU [50]), and hallucination benchmark (HallusionBench [10]). We evaluate Sherlock under two settings: direct generation and after three rounds of self-correction. For consistency and reproducibility, all models are evaluated using the VLMEvalKit [8] pipeline. 19 MMBench-V1.1. MMBench-v1.1 contains total of 3217 samples and is designed to evaluate the visual perception and visual reasoning capabilities of VLMs. It adopts the CircularEval protocol, which presents each question to VLM multiple times with shuffled answer choices. model is considered successful only if it selects the correct answer in all attempts, thereby reducing the influence of random guessing in multiple-choice questions. MMVet. MMVet consists of 218 open-ended questions designed to comprehensively assess VLMs across six dimensions: Recognition, OCR, Knowledge, Language Generation, Spatial Awareness, and Math. The final evaluation results are obtained by scoring the model-generated open-ended responses using GPT4-Turbo as the grader. MME. MME is comprehensive multimodal benchmark designed to assess two core capabilities of VLMs: visual perception and reasoning. It consists of 10 perception tasks and 4 reasoning tasks. The overall perception and reasoning (cognition) scores are computed as the sum of their respective subtask scores, with the total maximum score across all tasks being 2800. MMStar. MMStar consists of 1500 test samples and aims to address key issues in evaluation, such as low visual-textual alignment and potential data leakage during training. The benchmark is carefully curated and covers 6 core capabilities and 18 detailed evaluation axes. MathVista. MathVista introduces unified benchmark with 6141 examples collected from 28 existing datasets and 3 newly curated ones (IQTest, FunctionQA, and PaperQA). In our experiments, we evaluate models on the test-mini split, which contains 1000 samples. MMMU. MMMU is an expert-level multimodal benchmark developed to assess the perception, knowledge, and reasoning abilities of VLMs. For our experiments, we use its validation set, which comprises 900 multimodal samples. AI2D. We evaluate model performance on the AI2D_TEST_NO_MASK setting from VLMEvalKit, which assesses VLMs ability to interpret grade-school science diagrams. HallusionBench. HallusionBench is challenging benchmark designed to evaluate image-context reasoning in VLMs. It consists of 346 images and 1129 expert-crafted questions, targeting nuanced visual understanding. We report the results as the average of the three evaluation metrics provided by VLMEvalKit. C.2 Evaluation Baselines Considering that our approach is primarily initialized using the reasoning data from LLaVA-CoT [43], our main experiments compare with other SFT-based methods built on the Llama3.2-Vision-11BInstruct [9] model, including LLaVA-CoT, Mulberry [47], and LlamaV-o1 [34]. In addition, to further investigate the self-correction behavior of reasoning VLMs, we also include experiments and analysis on the RL-based method VL-Rethinker [36]. LLaVA-CoT. LLaVA-CoT constructs 100k stage-level Chain-of-Thought (CoT) annotations using multi-turn prompts to GPT-4o, and then trains the model via standard supervised fine-tuning (SFT) to acquire reasoning ability. Each response is structured into four stages: <SUMMARY>, <CAPTION>, <REASONING>, and <CONCLUSION>. Mulberry. Mulberry constructs 260k long CoT annotations by applying Monte Carlo Tree Search (MCTS) over strong models such as Qwen2-VL and GPT-4o, with particular focus on the mathematical domain. In addition, it incorporates supplementary self-reflection SFT data to encourage models to engage in step-wise reasoning, first reflecting on prior errors and then performing corrections. LlamaV-o1. LlamaV-o1 extends the LLaVA-CoT dataset to 175k samples and adopts multi-turn questioning and training strategy during inference to simulate the four reasoning stages: summary, caption, reasoning, and conclusion, thereby enabling step-by-step reasoning behavior. 20 VL-Rethinker. VL-Rethinker is built upon the Qwen2.5-VL series models and employs GRPObased reinforcement learning to enhance reasoning capabilities. Additionally, it introduces limited amount of self-reflection and correction data to enforce self-reflection objective, aiming to train the model to revise earlier responses within single thinking process. C.3 Evaluation Setup in Section 3 Prompt for Modify One Step Setting. Below is the prompt specific to LLaVA-CoT models. Modify One Step Prompt for Qwen2.5-Instruct-7B on LLaVA-CoT You are professional sentence rewriting expert. Your task is to modify the given sentence by altering key numbers, or key phrases so that the resulting sentence conveys different or incorrect meaning, finally leads to wrong answers. In essence, you should transform logically correct or factually accurate statement into one that contains logical or mathematical error. You should output the original format but with incorrect logic or meaning thay may leads wrong answers. Ensure that the intention or logic of your version clearly differs from the original input. <EXAMPLE> [INPUT] ### Question Find RS if QRS is an equilateral triangle. ### Answer <SUMMARY> To solve the problem, will use the properties of an equilateral triangle to find the length of side RS. will set up equations based on the given side lengths and solve for RS. </SUMMARY> <CAPTION> The image shows an equilateral triangle QRS with side lengths labeled as QR = 4x, QS = 6x 1, and RS = 2x + 1. </CAPTION> <REASONING> The triangle is equilateral, so all sides are equal. Therefore, we can set up the equation QR = QS = RS. This gives us the equations 4x = 6x 1 and 4x = 2x + 1. [YOUR REWRITTEN] <SUMMARY> To solve the problem, will use the properties of an equilateral triangle to find the length of side RS. will set up equations based on the given side lengths and solve for RS. </SUMMARY> <CAPTION> The image shows an equilateral triangle QRS with side lengths labeled as QR = 4x, QS = 9x 7, and RS = 2x 1. </CAPTION> <REASONING> The triangle is equilateral, so all sides are equal. Therefore, we can set up the equation QR = QS = RS. This gives us the equations 4x = 9x 7 and 4x = 2x 1. Make sure only output the rewritten answer with original format. [INPUT] {input} [YOUR REWRITTEN] Make sure only output your rewritten answer with original format, not output the question. RESPONSE: Below is the prompt specific to VL-Rethinker models. Modify One Step Prompt for Qwen2.5-Instruct-7B on VL-Rethinker You are professional sentence rewriting expert. Your task is to modify the given sentence by altering key numbers, or key phrases so that the resulting sentence conveys different or incorrect meaning, finally leads to wrong answers. In essence, you should transform logically correct or factually accurate statement into one that contains logical or mathematical error. You should output the original format but with incorrect logic or meaning thay may leads wrong answers. Ensure that the intention or logic of your version clearly differs from the original input. 21 <EXAMPLE> [INPUT] ### Question Find RS if QRS is an equilateral triangle. ### Answer Since triangle QRS is an equilateral triangle, all its sides are equal in length. Therefore, we can set the expressions for the sides equal to each other and solve for x. The expressions for the sides are: - QR = 4x - RS = 2x + 1 [YOUR REWRITTEN] Since triangle QRS is an equilateral triangle, all its sides are equal in length. Therefore, we can set the expressions for the sides equal to each other and solve for x. The expressions for the sides are: - QR = 6x - RS = 2x - 1 Make sure to make the rewritten sentence with explicit wrong steps or logic. [INPUT] {input} [YOUR REWRITTEN] Prompt for External Critique. In the external critique setting, we primarily follow the setup introduced by Critic-V [52], which includes both the critique generation prompt and the correction prompt. External Critique Generation Prompt #### Question {question} #### Answer {result} #### Task Please provide critique of the answer above. Self-Correction Prompt via External Critique Reflection on former answer: #### Former Answer {answer} {critics} #### Question {original_question} C.4 Evaluation Setup in Section 5 We report the performance of Sherlock under both direct reasoning and self-correction settings, where the self-correction result corresponds to the accuracy after three rounds of self-correction. Additionally, we explore the use of MM-Verify as verification mechanism to accelerate inferencetime scaling. The corresponding prompt is provided below. Prompt for MM-Verify Solve the math problems and provide step-by-step solutions, ending with \"The answer is [Insert Final Answer Here]\". When asked \"Verification: Is the answer correct (Yes/No)?\", respond with \" Yes\" or \" No\" based on the answers correctness. When asked \"Verification: Lets verify step by step.\", verify every step of the solution and conclude with \"Verification: Is the answer correct (Yes/No)?\" followed by \" Yes\" or \" No\". Q: 22 {question} A: Lets think step by step. {answer} Verification: Lets verify step by step. Self-Correction Prompt Based on Critique from MM-Verify Below is QUESTION from user and an EXAMPLE RESPONSE. Please provide more helpful RESPONSE, improving the EXAMPLE RESPONSE by making the content even clearer, more accurate, and with reasonable logic. Focus on addressing the humans QUESTION step by step based on the image without including irrelevant content. QUESTION: {question} EXAMPLE RESPONSE: {example_response} CRITIQUE: {critique} Now, refine and improve the RESPONSE based on provided CRITIQUE. You can consider two approaches: 1. REFINEMENT: If the SUMMARY section in the response is closely related to the question, the CAPTION section accurately describes the image, the REASONING section is logically clear and correct without any contradictions, and the CONCLUSION provides an accurate answer based on the previous steps, enhance clarity, accuracy, or reasoning logic as needed. 2. NEW RESPONSE: If the SUMMARY section incorrectly summarizes the intent of the issue, the CAPTION contains content unrelated to or incorrect about the image, there are logical errors or contradictions in the REASONING, or the CONCLUSION incorrectly states the findings, please enhance the accuracy and quality of each step, and craft more effective RESPONSE that thoroughly resolves the QUESTION. RESPONSE:"
        },
        {
            "title": "D More Results",
            "content": "Figure 6: Average accuracy of the Sherlock Iter2 model across eight benchmarks under direct generation and varying numbers of self-correction turns. In this section, we report the average performance of the Sherlock Iter2 model across eight benchmarks under different numbers of self-correction rounds. As shown in Fig. 6, the Sherlock model consistently improves its accuracy with more rounds of self-correction, demonstrating stable inference-time scaling."
        },
        {
            "title": "E Case Study",
            "content": "Case 1 of Sherlock Self-Correction Problem: What color is the text in the image? (A) White (B) Blue (C) Black (D) Grey Sherlock Direct Generation Response. <SUMMARY> will analyze the image to determine the color of the text by examining its appearance and comparing it to the given options. </SUMMARY> <CAPTION> The image is slide titled \"The United Nations,\" containing bullet points about the organizations formation and functions. The text is set against blue background, and there is an image of UN General Assembly meeting. </CAPTION> <REASONING> To determine the color of the text, will look at its hue and brightness in contrast to the background. The text appears to be lighter shade compared to the blue background, indicating it is neutral color such as grey or white. Since it is not black, option can be eliminated. Comparing it to the other options, white would be too bright, so it is likely grey. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 1 Response. <SUMMARY> will analyze the image to determine the color of the text by examining its appearance and comparing it to the given options. </SUMMARY> <CAPTION> The image is slide titled \"The United Nations.\" It contains bullet points about the United Nations formation and functions, with an accompanying image of UN General Assembly meeting. The text is prominently displayed against blue background. </CAPTION> <REASONING> To determine the text color, will look at its contrast with the background. The text appears lighter than the blue background, suggesting it is neutral color. Comparing it to the options provided, the closest match is grey, as it typically appears lighter than black or blue and not as bright as white. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 2 Response. <SUMMARY> To determine the color of the text in the image, will carefully examine the image to identify and describe the texts color, then confirm it matches one of the given options. </SUMMARY> <CAPTION> The image is slide titled \"The United Nations,\" containing information about the organizations formation and functions. It includes photograph of meeting at the United Nations General Assembly. The text is presented in clear, legible font against gradient blue background. </CAPTION> <REASONING> Upon examining the image, the text stands out against the blue background. Its color appears to be neutral shade that contrasts with the background, which is typically characteristic of the color gray. This observation aligns with the options provided, helping to identify the text color accurately. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 3 Response. <SUMMARY> To determine the color of the text in the image, will carefully analyze the visual elements and identify the color of the text by comparing it to the background and other elements in the image. </SUMMARY> <CAPTION> The image is slide titled \"The United Nations,\" containing bullet points about the history and functions of the United Nations. The text is presented against gradient blue background, with an image of United Nations General Assembly session on the right. </CAPTION> <REASONING> The text on the slide is clearly visible and stands out against the blue background. By comparing the texts color to the surrounding blue hues, can determine that the text is different color. The text appears to be neutral, light color that contrasts with the darker shades of blue in the background. </REASONING> <CONCLUSION> </CONCLUSION> 24 Case 2 of Sherlock Self-Correction Problem: Whats the function of the demonstrated object? (A) Cooking (B) Cook soup (C) Fry (D) steam Sherlock Direct Generation Response. <SUMMARY> will analyze the image to identify the object and its function, focusing on its features and context to determine the correct answer from the given options. </SUMMARY> <CAPTION> The image shows stainless steel pan on stovetop. Inside the pan, there is piece of fish being cooked in water. There are also vegetables, such as corn and tomato, visible nearby on the stove. </CAPTION> <REASONING> The object in the image is stainless steel pan placed on stove, containing water and piece of fish. The presence of heat from the stove and the fish being submerged in water indicates that it is being used for cooking. Among the options provided, \"Cooking\" is the most general term that encompasses the activity taking place. While the pan could be used to fry, cook soup, or steam, the most straightforward answer based on the image alone is that it is being used for cooking in general. </REASONING> <CONCLUSION> (A) Cooking </CONCLUSION> Sherlock Self-Correction Turn 1 Response. <SUMMARY> will analyze the image to identify the object and its function, focusing on its features and context to determine the correct answer. </SUMMARY> <CAPTION> The image shows stainless steel frying pan on gas stove. Inside the pan, there is piece of fish being cooked in oil or melted fat, indicating that it is being used for cooking. There are also some vegetables, such as corn and tomato, visible nearby. </CAPTION> <REASONING> The object in the image is frying pan, which is typically used for frying foods. The presence of the fish being cooked in the pan suggests that it is being used for frying. The pans shape and the cooking environment further support this conclusion, as frying usually involves cooking in pan over direct heat. </REASONING> <CONCLUSION> (C) Fry </CONCLUSION> Sherlock Self-Correction Turn 2 Response. <SUMMARY> will analyze the image to identify the object and its function, then determine which option best describes its use based on its features and context. </SUMMARY> <CAPTION> The image shows stainless steel pan on stovetop, containing water with piece of fish submerged. There are vegetables, such as corn and tomato, nearby on the counter. </CAPTION> <REASONING> The object is stainless steel pan placed on stove, indicating it is designed for cooking. It contains water and piece of fish, which suggests that it is being used to cook the fish. The presence of the stovetop and the way the fish is submerged in water further implies that the pan is being used for cooking method that involves boiling or frying. Given the options, frying typically involves submerging food in hot oil or water and is consistent with the setup shown. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 3 Response. <SUMMARY> will analyze the image to identify the object and its function, using visual cues to determine the correct answer from the given options. </SUMMARY> <CAPTION> The image shows stainless steel frying pan on gas stove. Inside the pan, there is piece of fish being cooked in oil or melted fat, indicating that it is being used for cooking. There are also some vegetables, such as corn and tomato, visible in the background. </CAPTION> <REASONING> The object in the image is frying pan, which is typically used for frying foods. The presence of the fish cooking in the pan with visible layer of oil further supports this function. Frying involves cooking food in hot oil or fat, which matches the scenario depicted. Other options like cooking soup or steaming require different types of vessels, such as pots with lid for steaming or larger pots for soup. </REASONING> <CONCLUSION> </CONCLUSION> 25 Case 3 of Sherlock Self-Correction Problem: Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end. Question: Is it split in half? Choices: (A) Yes (B) No Sherlock Direct Generation Response. <SUMMARY> will analyze the image to determine if the pizza is split in half by examining its shape and any visible cuts. will then compare my findings with the given options to provide the correct answer. </SUMMARY> <CAPTION> The image shows pizza on white plate. The pizza has thin crust and is topped with cheese and basil. It appears to have been cut into several slices, with two main cuts extending from the center towards the edge, suggesting it is divided into halves. </CAPTION> <REASONING> To determine if the pizza is split in half, will look for clear cuts that divide the pizza into two main portions. The pizza has two prominent diagonal cuts starting from the center, meeting the edge and creating two large halves. Additional smaller cuts are also visible, but the primary division into two is clear. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 1 Response. <SUMMARY> will analyze the image to determine if the pizza is split in half by examining its cut pattern and describe my findings systematically to reach the correct conclusion. </SUMMARY> <CAPTION> The image shows pizza on white plate. The pizza has thin crust and is topped with cheese and basil strips. It is cut into several slices, with two prominent cuts that divide it into halves. </CAPTION> <REASONING> To determine if the pizza is split in half, will look at the number and pattern of the cuts. The pizza has two main cuts that run from the center to the edge, dividing it into two equal halves. There are additional smaller cuts, but the primary division is into two halves. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 2 Response. <SUMMARY> will analyze the image to determine if the pizza is split in half by examining the number of slices and their arrangement. will then compare my findings with the given options to provide the correct answer. </SUMMARY> <CAPTION> The image shows pizza on white plate. The pizza has thin crust and is topped with cheese and basil. It appears to be cut into eight slices, arranged in symmetrical pattern. </CAPTION> <REASONING> To determine if the pizza is split in half, will count the number of slices. If it has two large slices separated by clear line down the middle, it would be considered split in half. However, the pizza is cut into eight slices, indicating it is not divided simply into two halves. The slices are evenly distributed across the pizza with no two large halves. </REASONING> <CONCLUSION> </CONCLUSION> Sherlock Self-Correction Turn 3 Response. <SUMMARY> will analyze the image to determine if the pizza is split in half by examining the number of slices and their arrangement. will follow structured approach to reach the conclusion. </SUMMARY> <CAPTION> The image shows pizza on white plate. The pizza has thin crust with golden-brown appearance, topped with melted cheese and garnished with thin strips of green herbs. It is cut into eight slices, with four slices extending from the center towards the edge, forming radial pattern. </CAPTION> <REASONING> To determine if the pizza is split in half, will assess the number and arrangement of the slices. pizza that is split in half would typically have two large halves separated by clear line down the middle, with no other slices present. In this image, the pizza is cut into eight equal slices, arranged in symmetrical pattern, but there are not two primary halves. Therefore, it is not split in half. </REASONING> <CONCLUSION> </CONCLUSION> 26 Case 4 of Sherlock Self-Correction Problem: Hint: Please answer the question requiring an integer answer and provide the final value, e.g., 1, 2, 3, at the end. Question: What is the age gap between these two people in image? (Unit: years) Sherlock Direct Generation Response. <SUMMARY> will analyze the image to identify any visible clues that indicate the age difference between the two individuals, such as differences in their attire, accessories, or any other visible signs that might suggest generational or age gap. </SUMMARY> <CAPTION> The image shows man and woman dressed in traditional Thai military and royal attire, respectively. Both are adorned with medals and jewelry, suggesting formal and ceremonial occasion. The womans hairstyle and the style of their clothing further imply historical context. </CAPTION> <REASONING> To determine the age gap, will consider the difference in their roles and the style of their clothing. The mans military uniform and the womans royal attire suggest they are in position of authority or royalty. The womans youthful appearance and the style of her clothing may indicate she is younger than the man, consistent with couple where the man is older. Given the royal context, it is plausible that the woman is his consort or wife, which often involves an age gap in royal marriages. </REASONING> <CONCLUSION> 12 </CONCLUSION> Sherlock Self-Correction Turn 1 Response. <SUMMARY> To determine the age gap between the two individuals in the image, will analyze historical context and available information about their birth dates. will then calculate the difference to provide the answer. </SUMMARY> <CAPTION> The image shows man and woman dressed in formal royal attire, both adorned with medals. The man is wearing military uniform with epaulets, and the woman is in formal, ornate dress. They are likely members of royal family, given their attire and decorations. </CAPTION> <REASONING> The individuals are King Bhumibol Adulyadej and Queen Sirikit of Thailand. King Bhumibol was born on December 5, 1927, and Queen Sirikit was born on August 12, 1932. To find the age gap, subtract the queens birth year from the kings birth year: 1927 - 1932 = 5 years. </REASONING> <CONCLUSION> 5 </CONCLUSION> Sherlock Self-Correction Turn 2 Response. <SUMMARY> To solve the problem, will analyze the image to identify the individuals and determine the age gap between them. will use historical information about their birth dates to calculate the age difference. </SUMMARY> <CAPTION> The image shows man and woman dressed in formal royal attire, both adorned with medals. The man is wearing military uniform, and the woman is in matching ceremonial outfit. They are likely members of royal family, given their clothing and decorations. </CAPTION> <REASONING> The individuals in the image are King Bhumibol Adulyadej and Queen Sirikit of Thailand. King Bhumibol was born on December 5, 1927, and Queen Sirikit was born on August 12, 1932. To find the age gap, subtract the queens birth year from the kings birth year: 1927 - 1932 = 5 years. </REASONING> <CONCLUSION> 5 </CONCLUSION> Sherlock Self-Correction Turn 3 Response. <SUMMARY> To determine the age gap between the two individuals in the image, will analyze historical data about their birth dates and calculate the difference. </SUMMARY> <CAPTION> The image shows man and woman dressed in formal royal attire, both adorned with medals. They are likely members of royal family, given their clothing and decorations. </CAPTION> <REASONING> The man is King Bhumibol Adulyadej, and the woman is Queen Sirikit of Thailand. King Bhumibol was born on December 5, 1927, and Queen Sirikit was born on August 12, 1932. To find the age gap, subtract the queens birth year from the kings birth year: 1927 - 1932 = 5 years. </REASONING> <CONCLUSION> 5 </CONCLUSION>"
        }
    ],
    "affiliations": [
        "Department of Computer Science, Purdue University, USA"
    ]
}