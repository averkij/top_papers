{
    "paper_title": "LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL",
    "authors": [
        "Dzmitry Pihulski",
        "Karol Charchut",
        "Viktoria Novogrodskaia",
        "Jan Koco≈Ñ"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Converting natural language questions into SQL queries (Text-to-SQL) enables non-expert users to interact with relational databases and has long been a central task for natural language interfaces to data. While the WikiSQL dataset played a key role in early NL2SQL research, its usage has declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, a systematic revision and transformation of WikiSQL designed for the LLM era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark: unlike the original WikiSQL, tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural language-to-SQL models."
        },
        {
            "title": "Start",
            "content": "LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL 1st Dzmitry Pihulski Department of Artificial Intelligence Wroclaw University of Science and Technology Wroclaw, Poland dzmitry.pihulski@pwr.edu.pl 2nd Karol Charchut Trusted Artificial Intelligence Wroclaw University of Science and Technology Wroclaw, Poland karol.charchut.dev@gmail.com 3rd Viktoria Novogrodskaia Trusted Artificial Intelligence Wroclaw University of Science and Technology Wroclaw, Poland novogrodskaiaviktoria@gmail.com 4th Jan Kocon Department of Artificial Intelligence Wroclaw University of Science and Technology Wroclaw, Poland jan.kocon@pwr.edu.pl AbstractConverting natural language questions into SQL queries (Text-to-SQL) enables non-expert users to interact with relational databases and has long been central task for natural language interfaces to data. While the WikiSQL dataset its usage has played key role in early NL2SQL research, declined due to structural and annotation issues, including case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions. We present LLMSQL, systematic revision and transformation of WikiSQL designed for the LLM era. We classify these errors and implement automated methods for cleaning and re-annotation. To assess the impact of these improvements, we evaluated multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral 7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready benchmark: unlike the original WikiSQL, tailored for pointer-network models selecting tokens from input, LLMSQL provides clean natural language questions and full SQL queries as plain text, enabling straightforward generation and evaluation for modern natural language-to-SQL models. Index TermsNatural language processing (NLP), SQL query generation, language models (LLM), dataset quality assessment, cleaning and annotation, training on tabular data, WikiSQL, Text-to-SQL benchmarks I. INTRODUCTION"
        },
        {
            "title": "Converting natural",
            "content": "language questions into structured queries is fundamental task in developing intelligent data access systems. This capability enables users without technical expertise to retrieve information from databases using plain language, without the need to learn query languages such as SQL. The WikiSQL dataset, comprising over 80,000 pairs of natural language questions and corresponding SQL queries with table data from Wikipedia, is one of the most widely used resources for training and evaluating text-to-SQL systems. It is widely recognized as benchmark dataset in NL2SQL research. However, despite its popularity, WikiSQL suffers from several issues that limit its reliability in practical applications and rigorous research. These include data type mismatches, case sensitivity inconsistencies, and questions that do not return answers even when the generated queries are correct. Beyond WikiSQL, subsequent benchmarks broadened the scope of Text-to-SQL evaluation. Spider [1] emphasizes cross-domain generalization with multi-table queries, while BIRD [2] focuses on robustness to annotation noise in realworld settings. These efforts illustrate complementary directions, yet systematic cleaning of WikiSQL itself has remained underexplored despite its historical importance and widespread use. In most prior works, these issues have either been ignored or only partially addressed, leading to inflated performance metrics and poor generalization of models on real-world data. In this work, we propose fundamentally different approach: comprehensive reworking and cleaning of the WikiSQL dataset to produce reliable and reproducible benchmark. To guide the reader, the remainder of this paper is structured as follows. Section II reviews related work, Section III details our re-annotation of WikiSQL, and Section IV presents evaluation with large language models. We conclude in Section V, with prompts and examples provided in the Appendix. The main contributions of our work are as follows: 1) We conducted systematic analysis of WikiSQL, identifying key issues affecting query execution accuracy, including type mismatches, case sensitivity inconsistencies, and unanswerable questions. 2) We refined the dataset annotations through both manual and automated methods to ensure stable query execution and correct results. 3) We introduce LLMSQL: an improved version of the WikiSQL dataset that preserves the original scale and diversity while significantly enhancing its practical usability for structured query generation tasks. 4) We performed comparative experiments with multiple language models on both the original WikiSQL splits and 5 2 0 2 7 ] . [ 1 0 5 3 2 0 . 0 1 5 2 : r the LLMSQL benchmark. 5) We adapted LLMSQL to modern research needs, particularly for evaluation with large language models (LLMs). The dataset will be publicly released on major platforms following publication. In this landscape, LLMSQL is complementary to existing resources: it provides large-scale, single-table benchmark with validated annotations, in contrast to Spider, which is best suited for evaluating multi-table and compositional reasoning, and BIRD, which targets robustness under noisy annotations. Researchers can thus choose the benchmark that best matches their evaluation goals. Our work aims to enable more transparent, reliable, and practical research in natural language interfaces to databases, and provides cleaned and well-validated resource for further model development and benchmarking. II. RELATED WORK Converting natural language questions into SQL (NL2SQL) is central problem for natural database interfaces, with progress driven by large annotated corpora and neural architectures. key dataset is WikiSQL, released with Seq2SQL (Zhong et al. [3]), which introduced execution-based reinforcement learning to optimize queries by both text and results. Seq2SQLs pointer-network designselecting tokens from the inputlimits its applicability to modern LLMs. Subsequent research focused on better capturing SQL structure and schema. SQLNet [4] used sketch-based, slot-filling decoder for SELECT, aggregation, and WHERE components. TypeSQL [5] added type-aware features linking question tokens to table columns, improving handling of rare values. then deCoarse-to-Fine methods first predict skeleton, tailed WHERE conditions. Execution-aware approaches like SeaD [6] and execution-guided decoding [7] prune invalid queries, improving execution accuracy, which remains the main metric for WikiSQL. Execution-aware decoding techniques have also proven effective. Execution-guided decoding runs partial SQL against the target table during generation and prunes candidate continuations that lead to execution errors or empty/invalid results; this approach has been shown to reduce syntactic and semantic errors in generated queries and improve the share of successfully executable outputs [7]. The integration of pretrained transformers with tableaware contextualization produced further gains beyond earlier seq2seq models. For instance, SQLova [8] adapts BERT-style encoders to incorporate table headers and cell values, yielding stronger logical-form and execution accuracy. Complementary approaches enhance schema representations and alignment between natural language and table content. These advances illustrate that pretrained models can surpass execution-guided seq2seq baselines. However, such results hold only for singletable WikiSQL and should be interpreted with caution when generalizing to multi-table or cross-domain settings Recognizing these limitations, new benchmarks were introduced. Spider [1] established cross-domain standard with multi-table queries, JOINs, and nested structures, requiring reasoning. UNITE [9] aggregated multiple compositional datasets for broader SQL pattern coverage, while BIRD [2] emphasized robustness under noisy annotations. These resources expand scope, but none directly address the persistent quality issues in WikiSQL itself. critical body of work addresses annotation quality, evaluation methodology, and robustness. Finegan-Dollak et al. [10] analyzed evaluation practices and demonstrated that certain dataset splits and anonymization choices can inflate reported performance; they proposed more conservative splitting strategies and evaluation protocols to better assess compositional generalization. Recent research, such as studies on BIRDBench, has highlighted that crowd-sourced datasets often contain issues such as noisy annotations, mismatched data types, and questions that cannot be answered from the table. These problems can bias standard evaluation metrics, including both execution accuracy and logical-form accuracy, giving misleading impression of model performance [2]. To address this, researchers recommend cleaning datasets and performing explicit verification of model outputs. Additionally, broader benchmark analyses emphasize the need for standardized evaluation procedures and the inclusion of realistic, user-generated queries, which help ensure that models are tested on scenarios closer to real-world usage and that reported metrics more accurately reflect practical performance [9], [11]. Taken together, the literature highlights three concurrent directions: (1) architectural and schema-aware improvements to increase generation correctness (e.g., type-aware models, table-aware encoding models, execution-guided decoding) [5], [7], [8]; (2) the design of more representative and cleaner benchmarks and evaluation practices that mitigate annotation noise (e.g., Spider, UNITE, and the recommendations of Finegan-Dollak) [1], [9], [10]; and (3) the systematic evaluation of large pretrained and/or instruction-tuned LMs for Textto-SQL, along with the development of practical engineering techniques (prompting, constrained decoding, schema linking, execution validation) required to obtain executable and reliable queries in real-world settings [12], [13]. In this work, we focus on directions (2) and (3): we perform systematic analysis of WikiSQL annotation quality, apply both automatic and manual corrections (including normalization of case and spacing, data type validation, SQL syntax fixes, and removal of unanswerable examples), and evaluate the performance of modern LLMs on the resulting benchmark. III. DATA A. Introduction to NL2SQL and the WikiSQL Dataset WikiSQL contains 80,654 (question, SQL) pairs derived from 24,241 Wikipedia tables [3]. It has become standard benchmark due to its scale, simple query structures, and public availability. WikiSQL provides three file groupstables, questions, and SQLite database filesorganized into train, validation (dev / val), and test splits. (cid:44) [\"real\",\"real\",\"text\",\"text\",\"text\",\"text\",\"text\"], give false diversity. The dataset uses only two SQL data types, real and textcorresponding to number and string in JSON. Each table (Listing 1) contains several fields, including: id: Unique table identifier. header: Column names. types: Data types (real or text). rows: Table entries. name: Optional table name derived from the ID. { \"id\": \"1-10088101-1\", \"name\": \"table_10088101_1\", \"header\": [ \"No. in set\",\"No. in series\",\"Title\",\"Directed (cid:44) by\", \"Written by\",\"Original air date\",\"Production (cid:44) code\" ], \"types\": \"rows\": [ [1,174,\"Per Manum\",\"Kim Manners\", \"Chris Carter & Frank Spotnitz\",\"February 18, (cid:44) 2001\",\"8ABX13\"], [2,175,\"This is Not Happening\",\"Kim Manners\", \"Chris Carter & Frank Spotnitz\",\"February 25, (cid:44) 2001\",\"8ABX14\"], [9,184,\"Nothing Important Happened Today (cid:44) II\",\"Tony Wharmby\", \"Chris Carter & Frank Spotnitz\",\"November 18, (cid:44) 2001\",\"9ABX02\"] ] } Listing 1: Example Table from WikiSQL Dataset. Questions in the JSONL files (Listing 2) include: table_id: Table referenced. question: Natural language query. sql: WikiSQL representation of the SQL query: sel: Index of the column selected. conds: WHERE conditions as [column index, operator (=, >, <), value]. agg: Aggregation code: 0=none, 1=MAX, 2=MIN, 3=COUNT, 4=SUM, 5=AVG. { } \"table_id\": \"1-10088101-1\", \"question\": \"The episode with production code (cid:44) 9ABX02 was originally aired on what date?\", \"sql\": {\"sel\":5,\"conds\":[[6,0,\"9ABX02\"]],\"agg\":0} Listing 2: Example Question from WikiSQL Dataset. The SQL follows the general format (Listing 3), with the example query shown in Listing 4. SELECT agg_op agg_col FROM table WHERE cond1_col cond1_op cond1 AND cond2_col cond2_op cond2 ...; Listing 3: SQL Query Format. SELECT \"Original air date\" FROM \"1-10088101-1\" WHERE \"Production code\" = 9ABX02; Listing 4: SQL Query for Question in Listing 2 WikiSQL excludes advanced SQL clauses such as JOIN, subqueries, or window functions. B. Identified Issues in the WikiSQL Dataset"
        },
        {
            "title": "WikiSQL shows several structural and semantic issues that",
            "content": "can affect model training and evaluation: Incomplete information: Some tables (140) miss one column name, which we augmented manually (common headers: Year, Month, Rank, Position). Datatype conflicts: Literals or row values sometimes mismatch their annotated type: Numbers stored as strings with commas or signs. text columns holding integers/floats. real columns holding integers. Duplicates: Identical tables/questions with different IDs Empty results: 49.25% of queries return no rows; 41.22% due to case sensitivity. The remainder (8.03%) likely involves other case issues. Non-intuitive SQL format: Numeric placeholders for column indices, aggregation, and operators obscure query structure and hinder human/LLM evaluation. C. Cleaning and Re-annotation Process"
        },
        {
            "title": "Incomplete information",
            "content": "Empty column names are not allowed in SQL tables. In the dataset, 140 tables were identified with single missing column name each. To resolve this issue, the missing column names were manually annotated based on the stored values. For instance, the missing column name in Listing 5 was replaced with \"Prime Minister Ordinal Number(s)\". { \"id\": \"1-11377572-3\", \"header\": [\"#\", \"\", \"Name\", \"Party\", \"Term in (cid:44) office\", \"The Times overall\", \"Matthew (cid:44) Parris\", \"Peter Riddell\", \"Ben MacIntyre\"], \"types\": [\"real\", \"text\", \"text\", \"text\", \"text\", (cid:44) \"real\", \"real\", \"real\", \"real\"], \"rows\": [ [1, \"1\", \"Robert Walpole\", \"Whig\", \"1721-1742\", (cid:44) 9, 14, 16, 7], [3, \"3\", \"Henry Pelham\", \"Whig\", \"1743-1754\", (cid:44) 29, 19, 34, 20], [42, \"61 63\", \"Winston Churchill\", (cid:44) \"Conservative\", \"1940-1945 1951-1955\", 1, 1, (cid:44) 1, 1], [50, \"71\", \"Margaret Thatcher\", \"Conservative\", (cid:44) \"1979-1990\", 5, 3, 4, 10], [52, \"73\", \"Tony Blair\", \"Labour\", \"1997-2007\", (cid:44) 16, 34, 15, 12] ] } Listing 5: Part of Table with Missing Column Name."
        },
        {
            "title": "Most of the datatype issues were resolved automatically by",
            "content": "addressing specific cases: real stored as strings: Spaces, commas, and leading \"+\" or \"-\" signs were removed, and values were converted to float (real). Additionally, literals in questions were corrected to eliminate English numeral formatting. Example: \"-2,123.9\" => \"-2139.9\" => -2139.9. text stored as float or int: Values were written to text (string). Example: 1224 => \"1224\" real stored as int: Values were converted to float (real). Example: 628 => 628.0 Only small number of cases required manual correction."
        },
        {
            "title": "Duplicates",
            "content": "Duplicates can negatively affect training, validation, and other downstream tasks when the dataset is used for such they were systematically reduced as purposes. Therefore, follows: Duplicate tables were removed. Questions associated with removed tables were reassigned to the corresponding non-duplicated tables. Duplicate questions were removed. Tables with no associated questions were removed. table was considered duplicate if it shared the same column names, column data types, and row values with another table. Similarly, question was considered duplicate if it had the same natural language query, table column names, and column data types as another question, since these are the inputs provided to the model during evaluation. After cleaning, the resulting number of tables and questions is summarized in Table I. Table I: Number of Tables and Questions With and Without Duplicates Duplicates Tables Questions With Without 26,531 25,609 80,654 80,"
        },
        {
            "title": "Empty Query Results",
            "content": "The preprocessed data was used to retrieve results from the tables. An empty result refers to queries that return no records: either an empty list ([] for plain SELECT queries without aggregation) or placeholder [[None]] when aggregate functions such as AVG, MIN, MAX, or SUM are applied but no matching rows are found. Cases involving COUNT function, which returns [[0]] when no matches are present, were not included in this analysis. Overall, 49.25% of all queries produced such empty results. Of these, 41.22% were conclusively caused by case mismatches in string literals for these queries, adjusting the case ensured that results were returned. The case mismatch between literals in the sql key of questions JSON and the corresponding literals in the question key occurs in the WikiSQL dataset. An example is shown in Listing 6. The case mismatch between literals in the sql key of questions JSON and the corresponding literals in the question key occurs in the WikiSQL dataset. An example is shown in Listing 6. { \"table_id\": \"1-10015132-11\", \"question\": \"What position does the player who (cid:44) played for butler cc (ks) play?\", \"sql\": { \"sel\": 3, \"conds\": [ [5, 0, \"Butler CC (KS)\"] ], \"agg\": 0 } } Listing 6: Example of Case Mismatch between SQL and Natural Language Query. It is seen that the SQL literal is Butler CC (KS), whereas in the question it appears as butler cc (ks). For each question, SQL string literals were updated to match the case in the natural language query. However, this alone did not resolve most empty results. If the result was still empty, all capitalization variantschanging the first letter of each word (where word is defined as substring separated by spaces) in the literalswere generated and tested until non-empty result was returned. The algorithm proceeds as follows: Generate capitalization variants: Create or variants as mentioned above. Example for literals [New York, Beijing]: [New York, Beijing] [new York, Beijing] [new york, Beijing] [new york, beijing] [New york, Beijing] [New york, beijing] [New York, beijing] Test SQL queries: Create SQL queries using each combination of variants and execute until non-empty result is found or all variants are tested. Update literals: If non-empty result is found, update both the SQL and the natural language query literals accordingly. For the remaining queries that returned no results, the case of string literals was adjusted to match the corresponding table values (applied only for the comparison operator =). The procedure proceeds as follows: Locate literal in table: For each SQL literal with \"=\" operator, identify the matching string in the table row. Update case: Adjust the literals case to match the table value. Test and update query: Create new SQL query using the updated literals. If the query returns result, update the literals in both SQL and the natural language question. The presented approach successfully eliminated empty results for the targeted portion of queries (41.22%). The remaining 8.03% were unaffected by case adjustments in the questions JSON files, and their underlying causes remain to be investigated in future work. Non-intuitive SQL Format Columns: [\"Date\", \"Opponent\", \"Score\", \"Loss\", \"AttenfThe original SQL storage (as shown in Listing 2) was replaced with standard SQL queries. By adhering to SQL standards and avoiding functions beyond regular clauses, the queries are now compatible with any SQL database following the standard. Additionally, numeric placeholders for column names and operators (both comparison and aggregation) were replaced with the actual names. The said mapping is displayed in Table II. Table II: Mapping of Numeric Placeholders to Aggregation and Comparison Operators Placeholder Aggregation Operator Comparison Operator 0 1 2 3 4 5 No operator MAX MIN COUNT SUM AVG = > < Refined JSON does not include initial SQL description formatsql field includes already constructed query as shown in Listing 7. { } \"question_id\": 20, \"table_id\": \"1-10088101-1\", \"question\": \"The episode with production code (cid:44) 9ABX02 was originally aired on what date?\", \"sql\": \"SELECT \"Original air date\" FROM \"1-10088101-1\" WHERE \"Production code\" = 9ABX02;\" Listing 7: Refined Example Question. D. Additional Unresolved Issues Despite the refinements applied, several issues remain unresolved. notable issue is the mismatch of aggregation operators. In certain cases, the annotated operator does not align with the intent of the natural language query or the contents of the table. Although the resulting SQL queries are syntactically valid, the choice of operator introduced by the annotator may produce semantically incorrect results."
        },
        {
            "title": "The following example illustrates a case in which COUNT",
            "content": "was used instead of SUM: Question: How many people attended when the opponent was twins? Original (Incorrect SQL): SELECT COUNT(\"Attendance\") FROM \"2-12206127-5\" WHERE \"Opponent\" = twins; Corrected SQL (aggregates Attendance properly): SELECT SUM(\"Attendance\") FROM \"2-12206127-5\" WHERE \"Opponent\" = twins; Schema and Sample Rows (key column: Attendance): Types: [\"text\", \"text\", \"text\", \"text\", \"real\", \"text\"] dance\", \"Record\"] Sample Rows: Attendance values are 34348.0, 38237.0, and 23849.0; values of matching rows should be summed to obtain the final result. [\"July 1\", \"Red Sox\", \"4 - 0\", \"Carpenter (7-5)\", (cid:44) 34348.0, \"38-43\"] [\"July 2\", \"Red Sox\", \"16 - 4\", \"Loaiza (5-9)\", (cid:44) 38237.0, \"38-44\"] [\"July 31\", \"Twins\", \"3 - 1\", \"Mays (12-8)\", (cid:44) 23849.0, \"49-58\"] Query Results: COUNT: [[0]] SUM: [[None]] Although SUM is intended to aggregate attendance values, the query returned [[None]]. This occurs because, according to the previously defined criteria for empty results, [[0]] for COUNT was not considered empty. If the string literal had matched the case in the table (e.g., \"Twins\" instead of \"twins\"), the correct result would have been 23849.0. Assigning the correct aggregation operator is challenging to assess automatically and is most reliable when evaluated individually by considering the semantic meaning of the natural language query. Full automation is difficult due to the need for contextual understanding. It remains under discussion whether all queries with no matching rows should be corrected, or if some should be left unmodified to retain diversity in the dataset. Table III provides summary of queries with non-matching rows, broken down by aggregation operator. Table III: Distribution of Empty Results by Aggregation Operator Aggregation Nominal Percentage No operator MAX MIN COUNT SUM AVG TOTAL 290 1,569 1,562 1,577 1,461 1,571 8,030 0.36% 1.95% 1.94% 1.96% 1.82% 1.96% 10.00% IV. LLM EVALUATION ON DIFFERENT SCENARIOS The primary motivation for the modifications to the WikiSQL dataset described in Section III was to create modern, ready-to-use benchmark for large language models. Unlike the original WikiSQL, LLMSQL enables evaluation of LLMs that generate queries based on knowledge acquired during pretraining and/or instruction-tuning. Such LLMs do not require additional fine-tuning to produce well-formatted, executable SQL queries that satisfy the input question. Providing LLMSQL as ready-to-use benchmark, we revive classic resource that had largely been abandoned. Along with the dataset, we propose set of standardized rules for evaluation, ensuring that models assessed under these rules can be fairly compared. key element in this process is prompt design. We created our own prompt and corresponding fewshot examples for LLM evaluation across different scenarios. For each few-shot example, we additionally included sample row from an imaginary table crafted to match the example, and for the actual question we also added the first row of the real target table. We found that models can greatly benefit from such additions, as this helps them better understand the structure and content of the table. The complete prompt and five-shot examples are provided in Appendix A. For instance, 1-shot prompt for simple count query might look like: LLMSQL 1-shot Prompt Instruction: You are an expert SQLite SQL query generator. Your task: given question and table schema, output ONLY valid SQL SELECT query. Strict Rules: Output only SQL (no explanations, no markdown, no fences) Use table name \"Table\" Allowed functions: MAX, MIN, COUNT, SUM, AVG Allowed condition operators: =, >, <, != Allowed SQL keywords: SELECT, WHERE,"
        },
        {
            "title": "AND",
            "content": "Always use quotes for all column names and table name, even single-word names (e.g., \"Price\", \"Something #\") Model, [Brand, Example: Question: What is the price of the Samsung Galaxy S23? Columns: Color] Types: [text, text, real, text, text] Sample row: [Apple, iPhone 14, 899.99, 128GB, White] SQL: SELECT \"Price\" FROM \"Table\" WHERE \"Brand\" = \"Samsung\" AND \"Model\" = \"Galaxy S23\"; Storage, Price, Question: {question} Columns: {headers} Types: {types} Sample row: {sample_row} SQL: 1) Temperature: For most models, the primary evaluation parameter was the temperature, which was set to zero (greedy decoding) to ensure reproducibility of results. Recent studies have shown that using greedy decoding to evaluate reasoning models can lead to higher repetition rates and significant variability across different checkpoints [14]. Therefore, for models with strong reasoning capabilities, we did not use greedy decoding. Instead, we employed the default sampling parameters recommended for each model. 2) Maximum Number of Tokens: For all models, the maximum number of newly generated tokens was limited to 256. This limit was chosen arbitrarily, as all SQL queries in our benchmark typically require no more than approximately 50 tokens. The additional margin ensures that models producing intermediate reasoning steps or verbose outputs can still generate the final query without being truncated. 3) Evaluation Metric: The evaluation metric used in this study is execution accuracy. generated query is considered correct if, when executed against the database, it produces the same result set as the ground truth query. Specifically, the result must contain the same number of rows, identical columns, and matching values (regardless of the order of results). It is also important to specify the database system used for query execution. We employed SQLite, accessed via the standard Python client in version 3.11.11. The choice of SQLite was motivated by two factors: (i) the original WikiSQL dataset was designed for this system, and (ii) SQLite is included in the Python standard library, ensuring easy reproducibility. However, this choice introduces some limitations, as different SQL dialects provide varying sets of functions and syntax. In the case of LLMSQL, these limitations are minimal because the benchmark does not require complex functions. All queries are formulated using basic SQL constructs that are supported by virtually all SQL dialects, which makes the benchmark portable to other database systems if needed. 4) SQL Extraction from Outputs: Not all models strictly adhere to the prompt specification given in Appendix A. Many models exhibit tendency to first restate the task or produce intermediate steps before generating the final SQL query. In such cases, the SQL query might not appear at the beginning of the response, but rather in the middle or at the end. To address this, we adopted regex-based extraction strategy similar to what is used in popular text-to-SQL benchmarks such as Spider [1]. Specifically, we designed regex pattern to extract all possible SQL statements from the model output: (?s)SELECTb.*?(?=(?:;n{4,}$)) Regex explanation: (?s) Enables single-line (DOTALL) mode, so the dot (.) matches newline characters as well. SELECT word boundary. Matches the keyword SELECT followed by .*? Lazily matches any characters (including newlines) until the next condition. (?=(?:;n{4,}$)) Positive lookahead that ensures the match stops at one of the following: semicolon (;) Four or more consecutive newlines (n{4,}) Markdown code block fence () The end of the string ($) For instance, model might output:"
        },
        {
            "title": "Example Regex Extraction",
            "content": "Output: First, we need to filter employees: SELECT Name FROM employees WHERE Salary > 50000; Then count them... Extracted: WHERE Salary > 50000;"
        },
        {
            "title": "SELECT Name",
            "content": "After extracting all substrings that match the regex, we limit the number of candidate SQL queries to the first 10 occurrences. This constraint prevents excessive generation and mitigates potential gaming strategies by models, such as producing numerous variants to increase the chance of correct answer. The value of 10 was chosen arbitrarily to balance completeness and computational efficiency. Subsequently, all queries (n 10) were executed against the database. If at least one query produced the same results as the ground truth (as defined by the evaluation metric in Section IV-3), the models response was considered correct. A. Main Scenario (Benchmark) Since LLMSQL is based on the relatively simple WikiSQL dataset, our main evaluation method does not rely on the original dataset splits. Instead, the evaluation uses all questions from 3 subsets, which were not filtered out during dataset cleaning. We encourage the use of such strategy in the future, since it will bring more robust results of evaluation. The evaluation results for the LLMSQL benchmark are presented in Table IV. The benchmark includes three settings: 0-shot, 1-shot, and 5-shot. We used 1000 randomly chosen set of examples to evaluate the models on 0-shot and 1-shot settings in favor of saving resources. As the 1-shot prompt, we used the first example from the 5-shot prompt in the Appendix A. We evaluated broad set of models across different scenarios, including Llama 3.2 1B Instruct, Qwen 2.5 1.5B Instruct, Phi 3.5 Mini Instruct, Gemma 3 4B IT, Gemma 3 27B IT, Mistral 7B Instruct v0.2, gpt-oss-20B, DeepSeek V3 0324, DeepSeek R1 0528, and OpenAI o4-mini [14][22]. Model performance generally improves with size, but not strictly monotonically. For example, Gemma 3 (4.3B) achieves 60.9% in 0-shot, outperforming Mistral 7B, which scores only 24.4%, indicating that architectural differences and instruction tuning play major role beyond parameter count. Across all models, accuracy consistently improves when moving from 0-shot to 1-shot and further to 5-shot settings. For example, Phi 3.5 (3.8B) increases from 24.7% in the 0-shot setting to 62.07% in the 5-shot setting, representing relative improvement of approximately 151%. Similarly, gpt-oss-20B improves from 47.3% to 71.77%. These results suggest that providing examples helps models better adapt to the specifics of the dataset. This is particularly important in our scenario because models tend to overshoot the complexity of the task. Upon analyzing model outputs, we observed that many models generated unnecessarily complex SQL queries, often including subqueries and aliasesstructures that are not present in LLMSQL. Additionally, some models frequently used unsupported functions such as SUBSTRING(), which caused execution failures since SQLite does not include this function in its standard set. Among the smallest models in our benchmark, Llama 3.2 (1.2B) and Qwen 2.5 (1.5B), we observe significant performance gap despite their relatively close parameter counts. Llama 3.2 achieves only 5.7% accuracy in the 0-shot setting and improves to 22.44% with 5-shot, indicating that it struggles severely with SQL generation even when provided with examples. In contrast, Qwen 2.5 starts at 20.6% in 0shot and reaches 53.41% in 5-shot, more than doubling its accuracy compared to Llama under identical conditions. This discrepancy suggests that model architecture and training data quality are critical, as parameter size alone does not explain the difference. The largest proprietary and open source models dominate the LLMSQL benchmark. In particular, DeepSeek R1 0528 achieves the highest scores across all settings, reaching up to 88.4%, while OpenAI o4-mini closely follows with 86.45% in 5-shot. These models outperform smaller open-source models by substantial margin, often 3060 percentage points. Interestingly, the largest models exhibit performance plateau: for example, DeepSeek R1 slightly decreases from 88.4% in 0shot to 86.57% in 5-shot, indicating that additional in-context examples provide limited improvement. Since the evaluation prompt explicitly specifies which SQL functions are allowed and which SQL dialect to use, even in the 0-shot setting, this plateau suggests that larger models are able to quickly adapt to task-specific instructions and generate correct SQL queries without relying heavily on few-shot examples. B. Additional Scenario: Fine-Tuning The main scenario (Section IV-A) demonstrated that many models struggle to fully understand the task from the prompt alone. In particular, they often fail to correctly handle the set of allowed SQL functions and operators without extensive fewshot prompting. To provide broader perspective on model capabilities, we also evaluated fine-tuning scenario using the LLMSQL benchmark. For this, LLMSQL was divided into three splitstrain, validation, and testfollowing the original WikiSQL split assignments as closely as possible. Each question remains in the same split as in the original dataset. In this scenario, models are fine-tuned on the training split using Cross-Entropy Loss and evaluated dynamically on the test split according to the execution accuracy metric described in Section IV-3. This allows us to observe how performance evolves with the number of fine-tuning steps. All models were trained under consistent conditions, with fixed numbers of epochs, learning rates, and other hyperparameters. The only variable across models was batch size, which was adjusted according to model capacity. Fine-tuning hyperparameters for the experiments were as follows: Batch size: 16128 (depending on model capacity) Table IV: Performance of LLMs on the LLMSQL benchmark under 0-shot, 1-shot, and 5-shot settings. Models are ordered by parameter size. Model Parameters 0-shot (1000) 1-shot (1000) 5-shot Eval. Time (min) Temperature Llama 3.2 1B Instruct Qwen 2.5 1.5B Instruct Phi 3.5 mini Instruct Gemma 3 4B IT Mistral 7B Instruct v0.2 gpt-oss-20b DeepSeek V3 0324 DeepSeek R1 0528 OpenAI o4-mini 1.2B 1.5B 3.8B 4.3B 7.2B 21.5B 685B 685B Proprietary 5.70% 20.60% 24.70% 60.90% 24.40% 47.30% 77.30% 88.40% 85.50% 22.44% 4.00% 53.41% 53.70% 62.07% 57.20% 64.29% 59.10% 56.99% 50.70% 71.77% 67.10% 79.90% 83.52% 88.30% 86.57% 86.45% 85.60% 20 20 40 40 40 50 API API API 0.0 0.0 0.0 0.0 0.0 1.0 API default API default API default from fine-tuning, achieving more than 90% execution accuracy on the test split. This indicates that fine-tuning substantially improved their understanding of dataset-specific structures and conventions, making them more flexible and effective for NL2SQL tasks. For larger models (Gemma3 27B IT, Mistral 7B Instruct v0.2, and gpt-oss-20B LoRA), improvements were observed but were generally smaller; no model achieved over 90% execution accuracy. Notably, gpt-oss-20B had relatively stable performance around 7878.5% execution accuracy despite LoRA fine-tuning, suggesting that larger models may require more specialized tuning strategies or longer fine-tuning schedules to fully leverage dataset-specific structures. We note that the chosen fixed training conditions may not have been optimal for all models. For instance, the execution accuracy did not begin to decrease for any model, suggesting that three epochs may not have been sufficient for the models to reach their maximal performance. The purpose of using fixed conditions was to place all models on an equal footing, ensuring that the evaluation focused on their ability to learn from the dataset itself rather than on extensive hyperparameter tuning. All specific hyperparameters used for fine-tuning and evaluation, along with each models fine-tuning curves, will be released upon revision of the paper. V. CONCLUSIONS Our motivation was to address longstanding issues in the original dataset. By applying both automated normalization and targeted manual corrections, we created resource that is structurally sound, semantically consistent, and practically executable across all examples. LLMSQL stores complete and human-readable SQL statements, making it directly suitable for modern language models without additional preprocessing. Interestingly, while larger reasoning-oriented LLMs such as DeepSeek R1 and OpenAI o4-mini achieved execution accuracies above 85%, our finetuning experiments revealed that even relatively small models can surpass 90% accuracy when adapted to LLMSQL. We also argue that, despite its simplicity, LLMSQL remains relevant and informative benchmark. Real-world SQL workloads are often dominated by relatively simple patterns: in an analysis of 8.1 million production queries at Uber [23], over 62% used JOIN, and fewer than 1% involved operators Figure 1: Execution accuracy on the LLMSQL test split during fine-tuning for various models in the 5-shot setup. The x-axis represents fine-tuning epochs, and the y-axis shows execution accuracy (%). Optimizer: adamw_bnb_8bit Learning rate: 4e5 LR scheduler: cosine LoRA (for gpt-oss-20B): rank = 8, Œ± = 16 LoRA linear modules: target all layers, 7.mlp.experts.gate_up_proj, specifically 7.mlp.experts.down_proj, 15.mlp.experts.gate_up_proj, 15.mlp.experts.down_proj, 23.mlp.experts.gate_up_proj, 23.mlp.experts.down_proj The resulting execution accuracy on the test split as function of fine-tuning steps is shown in Figure 1. The finetuning process was stable for all models, with no observed loss spikes or gradient norm anomalies. We also included the Gemma3 27B IT model to provide comparison with model of similar parameter size to gpt-oss20B. The fine-tuning results varied across models. Smaller modelsincluding Gemma3 4B IT, Llama3.2 1B Instruct, Phi3.5 Mini Instruct, and Qwen2.5 1.5B Instructbenefited greatly such as UNION, INTERSECT, or EXCEPT. This reinforces the idea that mastering simple clauses is critical for practical deployment and benchmarking progress. We hope LLMSQL will revitalize interest in lightweight, high-quality datasets and serve as reliable resource for advancing natural language interfaces to databases, both as standalone benchmark and as part of broader evaluation suites. VI. FUTURE WORK Although LLMSQL in its current version already solves the key problems of the original dataset, further development of the resource is important both for improving quality and expanding its scope of application. Future steps will aim to make the data set more complete, complex, and diverse while maintaining its structural simplicity and practical applicability. In particular, the following areas of work are planned: Adding questions to tables. In the current version, some tables do not have related questions. Creating additional annotations will provide more complete data coverage and increase the value of the dataset as training resource. Introduction of JOIN queries. To increase the complexity and practical usefulness of the dataset, it is planned to add SQL queries that include table joins. This step will bring the tasks closer to real-world business applications, where JOIN operations are standard practice. Addition of new data types. Future versions will include additional information formats, such as dates and time values. This will allow models to handle wider range of conditions (e.g., date range filters, time interval calculations), bringing tasks closer to real-world SQL usage. Multilingual support. Translating questions and table descriptions into several languages will expand the user audience and allow exploration of model behavior in multilingual SQL generation environment. Scaling and integration with other benchmarks. Moreover, several existing benchmarks are not fully suited for modern LLMs, as they often lack clearly defined inputoutput pairs or require complex preprocessing. We plan to adapt them and combine with LLMSQL. This will provide greater diversity of structures and scenarios, allow models to work with wider range of queries, and maintain the coherence and consistency of annotations. Together, these enhancements should transform LLMSQL into resource that combines compactness and ease of use with high informational value and practical utility. We expect that the updated dataset will be in demand both in academic research and in applied projects related to building natural language interfaces to databases."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was financed by (1) the European Regional Development Fund as part of the 20212027 European Funds for Modern Economy (FENG) programme, project no. FENG.02.04-IP.040004/24: CLARIN Common Language Resources and Technology Infrastructure; (2) the European Regional Development Fund as part of the 2014-2020 Smart Growth Operational Programme, project no. POIR.04.02.0000C002/19: CLARIN Common Language Resources and Technology Infrastructure; (3) Digital Research Infrastructure for the Arts and Humanities DARIAH-PL (POIR.04.02.00-00D006/20, KPOD.01.18-IW.03-0013/23); (4) CLARIN ERIC European Research Infrastructure Consortium: Common Language Resources and Technology Infrastructure (period: 2024-2026) funded by the Polish Minister of Science under the programme: \"Support for the participation of Polish scientific teams in international research infrastructure projects\", agreement number 2024/WK/01; (5) Polish Minister of Digital Affairs under special purpose subsidy No. 1/WII/DBI/2025: HIVE AI: Development and Pilot Deployment of Large Language Models in the Polish Public Administration; (6) the statutory funds of the Department of Artificial Intelligence, Wroclaw University of Science and Technology. AI-based tools, including ChatGPT, Grammarly Premium, linguistic and Writeful, were used exclusively to support clarity and improve the readability of the manuscript."
        },
        {
            "title": "REFERENCES",
            "content": "task, in Proceedings of [1] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and D. R. Radev, Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing the 2018 Conference on and text-to-sql Empirical Methods in Natural Language Processing (EMNLP), 2018, pp. 39113921. [Online]. Available: https://aclanthology.org/D18-1425/ [2] N. Wretblad, F. G. Riseby, R. Biswas, A. Ahmadi, and O. Holmstr√∂m, Understanding the effects of noise in text-to-sql: An examination the 62nd Annual of the bird-bench benchmark, in Proceedings of Meeting of (ACL 2024), Short Papers, 2024, pp. 356369. [Online]. Available: https://aclanthology.org/2024.acl-short.31/ the Association for Computational Linguistics [3] V. Zhong, C. Xiong, and R. Socher, Seq2sql: Generating structured learning, arXiv queries from natural preprint, 2017. [Online]. Available: https://arxiv.org/abs/1709.00103 [4] X. Xu, C. Liu, and D. Song, Sqlnet: Generating structured queries from natural language without reinforcement learning, arXiv preprint, 2017. [Online]. Available: https://arxiv.org/abs/1711.04436 language using reinforcement [5] T. Yu, Z. Li, Z. Zhang, R. Zhang, and D. R. Radev, Typesql: in [Online]. Knowledge-based Proceedings of NAACL-HLT 2018, 2018, pp. 588594. Available: https://aclanthology.org/N18-2093.pdf generation, type-aware text-to-sql neural [6] K. Xu, Y. Wang, Y. Wang, Z. Wen, and Y. Dong, Sead: End-to-end textto-sql generation with schema-aware denoising, pp. 18451853, 2022. [Online]. Available: https://aclanthology.org/2022.findings-naacl.141/ [7] C. Wang, K. Tatwawadi, M. Brockschmidt, P.-S. Huang, Y. Mao, O. Polozov, and R. Singh, Robust text-to-sql generation with execution-guided decoding, arXiv preprint, 2018. [Online]. Available: https://arxiv.org/abs/1807.03100 [8] W. Hwang, J. Yim, S. Park, and M. Seo, comprehensive exploration on wikisql with table-aware word contextualization, arXiv preprint, 2019, sQLova presented as arXiv preprint / NeurIPS-related work. [Online]. Available: https://arxiv.org/pdf/1902.01069 [9] W. Lan, Z. Wang, A. Chauhan, H. Zhu, A. Li, J. Guo, S. Zhang, C.-W. Hang, J. Lilien, Y. Hu, L. Pan, M. Dong, J. Wang, J. Jiang, S. Ash, V. Castelli, P. Ng, and B. Xiang, Unite: unified benchmark for text-to-sql evaluation, arXiv preprint, 2023. [Online]. Available: https://arxiv.org/abs/2305.16265 [10] C. Finegan-Dollak, J. K. Kummerfeld, L. Zhang, K. Ramanathan, S. Sadasivam, R. Zhang, and D. Radev, Improving text-to-sql evaluation methodology, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), 2018, pp. 351360. [Online]. Available: https://aclanthology.org/P18-1033/ is Country, Years Active] Types: in California? Columns: Storage, Color] Types: [text, text, real, text, text] Sample row: [Apple, iPhone 14, 899.99, 128GB, White] SQL: SELECT \"Price\" FROM \"Table\" WHERE \"Brand\" = \"Samsung\" AND \"Model\" = \"Galaxy S23\"; Example 2: Question: How many books did Maya Chen publish? Columns: [Author, Books Published, Genre, [text, real, text, text, text] Sample row: [John Smith, 3, Non-fiction, Canada, 20052015] SQL: SELECT \"Books Published\" FROM \"Table\" WHERE \"Author\" = \"Maya Chen\"; Example 3: Question: What the total population of cities [City, State, Population, Area, Founded] Types: [text, text, real, real, text] Sample row: [Houston, Texas, 2304580, 1651.1, 1837] SQL: SELECT SUM(\"Population\") FROM \"Table\" WHERE \"State\" = \"California\"; Example 4: Question: How many restaurants serve Italian cuisine? Columns: Cuisine, Rating, City, Price Range] Types: [text, text, real, text, text] Sample row: [Golden Dragon, $$] SQL: SELECT Chinese, COUNT(*) FROM \"Table\" WHERE \"Cuisine\" = \"Italian\"; Example 5: Question: What is the average salary for Software Engineers? Columns: [Job Title, Salary, Experience, Company Size] Types: [text, real, text, text, text] Sample row: [Data Analyst, 70000, Junior, Chicago, 200500] SQL: SELECT AVG(\"Salary\") FROM \"Table\" WHERE \"Job Title\" = \"Software Engineer\"; Question: {question} Columns: {headers} Types: {types} Sample row: {sample_row} SQL: [Restaurant, Location, Boston, 4.2, [11] A. Mitsopoulou and G. Koutrika, Analysis of text-to-sql benchmarks: Limitations, challenges and opportunities, in Proceedings of the 28th International Conference on Extending Database Technology (EDBT 2025), 2025. [Online]. Available: https://openproceedings.org/2025/ conf/edbt/paper-41.pdf [12] X. Liu, S. Shen, B. Li, P. Ma, R. Jiang, Y. Luo, Y. Zhang, J. Fan, G. Li, and N. Tang, survey of nl2sql with large language models: Where are we, and where are we going? arXiv preprint, 2024. [Online]. Available: https://arxiv.org/abs/2408.05109v1 [13] B. Zhang, Y. Ye, G. Du, X. Hu, Z. Li, S. Yang, C. H. Liu, R. Zhao, Z. Li, and H. Mao, Benchmarking the text-to-sql capability of large language models: comprehensive evaluation, arXiv preprint, 2024. [Online]. Available: https://arxiv.org/abs/2403.02951 [14] D.-A. et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, CoRR, vol. abs/2501.12948, January 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2501.12948 [15] A. D. et al., The llama 3 herd of models, CoRR, vol. abs/2407.21783, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2407.21783 [16] A. Y. et al., Qwen2.5 technical report, arXiv preprint arXiv:2412.15115, 2024. [17] M. A. et al., Phi-3 technical report: highly capable language [Online]. Available: https: locally on your phone, 2024. model //arxiv.org/abs/2404.14219 [18] A. K. et al., Gemma 3 technical report, CoRR, vol. abs/2503.19786, March 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2503. 19786 [19] J. et al., Mistral 7b, CoRR, vol. abs/2310.06825, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2310.06825 [20] O. et al., gpt-oss-120b & gpt-oss-20b model card, 2025. [Online]. Available: https://arxiv.org/abs/2508. [21] D.-A. et al., Deepseek-v3 technical report, CoRR, vol. abs/2412.19437, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2412.19437 [22] OpenAI, o4-mini reasoning model, 2025, accessed: May 9, 2025. [Online]. Available: https://platform.openai.com/docs/models/o4-mini [23] N. M. Johnson, Towards practical privacy-preserving data analytics, University of California, Berkeley, Electrical Engineering and Computer Sciences, Berkeley, CA, USA, Doctoral dissertation UCB/EECS-2018171, Dec. 2018. APPENDIX We provide the prompt and five-shot examples used for evaluating LLMs on LLMSQL. The LLM is instructed to generate valid SQL queries given question and table schema, following strict rules."
        },
        {
            "title": "LLMSQL Prompt and Examples",
            "content": "Instruction: You are an expert SQLite SQL query generator. Your task: given question and table schema, output ONLY valid SQL SELECT query. Strict Rules: Output only SQL (no explanations, no markdown, no fences) Use table name \"Table\" Allowed functions: MAX, MIN, COUNT, SUM, AVG Allowed condition operators: =, >, <, != Allowed SQL keywords: SELECT, WHERE,"
        },
        {
            "title": "AND",
            "content": "Always use quotes for all column names and table name, even single-word names (e.g., \"Price\", \"Something #\") Few-Shot Examples: Example 1: Question: What is the price of the Samsung Galaxy S23? Columns: [Brand, Model, Price,"
        }
    ],
    "affiliations": [
        "Department of Artificial Intelligence Wroclaw University of Science and Technology Wroclaw, Poland",
        "Trusted Artificial Intelligence Wroclaw University of Science and Technology Wroclaw, Poland"
    ]
}