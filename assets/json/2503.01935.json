{
    "paper_title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
    "authors": [
        "Kunlun Zhu",
        "Hongyi Du",
        "Zhaochen Hong",
        "Xiaocheng Yang",
        "Shuyi Guo",
        "Zhe Wang",
        "Zhenhailong Wang",
        "Cheng Qian",
        "Xiangru Tang",
        "Heng Ji",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE."
        },
        {
            "title": "Start",
            "content": ": Evaluating the Collaboration and Competition of"
        },
        {
            "title": "LLM agents",
            "content": "Kunlun Zhu1 , Hongyi Du1, Zhaochen Hong1, Xiaocheng Yang1, Shuyi Guo1, Zhe Wang1 Zhenhailong Wang1, Cheng Qian1, Xiangru Tang, Heng Ji1, Jiaxuan You1 1University of Illinois Urbana-Champaign kunlunz2@illinois.edu 5 2 0 2 3 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 5 3 9 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents; yet existing benchmarks either focus on singleagent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, comprehensive benchmark designed to evaluate LLMbased multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario,and cognitive planning improves milestone achievement rates by 3%. Code and datasets are publicavailable at https: //github.com/MultiagentBench/MARBLE."
        },
        {
            "title": "Introduction",
            "content": "leverage multiple LLM-based agents that collaborate, coordinate, and jointly plan to address these challenges. This paradigm not only increases the efficiency of task execution but also enables the realistic simulation of social interactions and dynamics, thereby enhancing performance in domains such as strategic decision-making, gaming (Silver et al., 2017), and software development (Wang et al., 2023c). Figure 1: Overview of MultiAgentBench evaluation process: Multi-Agent System Coordination in various interactive environments, with focus on task performance, and coordination. Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and other agents (Wang et al., 2023c; Park et al., 2023a; OpenAI, 2023). While single-agent systems have demonstrated impressive capabilities, their isolated operation can limit task execution efficiency and falls short when simulating complex, socially dynamic scenarios. In contrast, multi-agent setups (Li et al., 2023a; Wang et al., 2023b, 2024b) Team Leader. Core Contributors. Contributions are listed in the appendix A.1. Despite significant advances in LLM capabilities, current evaluation paradigms remain insufficient for multi-agent scenarios. Traditional singleagent benchmarkssuch as AgentBench (Liu et al., 2023), VisualAgentBench (Sun et al., 2023) GAIA (Mialon et al., 2023), ToolBench (Qin et al., 2024) and HumanEval (Chen et al., 2021)primarily focus on isolated reasoning and generation, overlooking the dynamics intrinsic to multi-agent interactions. To address this gap, we introduce MultiAgentBench, comprehensive benchmark designed to evaluate LLM-based multi-agent systems across wide range of task-solving and simulation scenarios. MultiAgentBench offers several key advantages: (1) Multi-Domain Evaluation: The benchmark covers diverse domainsfrom collaborative coding to gamingensuring broad real-world applicability. (2) Capturing Coordination and Competition: Unlike traditional single-agent benchmarks, MultiAgentBench explicitly measures both coordination dynamics and competitive interactions, highlighting the unique challenges of multiagent environments. (3) Tailored Metrics and Flexible Protocols: We propose novel metrics, including Key Performance Indicator (KPI) that tracks milestone progress and individual contributions, to systematically assess planning quality and communication effectiveness. Moreover, our framework, MARBLE (Multi-agent cooRdination Backbone with LLM Engine), supports various communication topologiessuch as star, chain, tree, and an innovative graph-based approachand accommodates multiple reasoning strategies. Our contributions can be summarized as follows: (1) We introduce MultiAgentBench along with the MARBLE framework, comprehensive benchmark that rigorously evaluates LLM-based multiagent systems in six diverse interactive scenarios, capturing both collaborative and competitive dynamics. Notably, the cognitive planning planning feature improves milestone achievement rates by 3%. (2) We propose innovative evaluation metrics that assess not only task success but also coordination quality. Our metrics include milestonebased KPIs, structured planning and communication scores, and dedicated competition score that captures conflicting-goal tasks, internal performance metrics, and competitive aspects in planning and communication. (3) Our experiments reveal some \"aha-moments\" in multi-agent coordinationagents begin to exhibit emergent social behaviors, providing promising insights toward AGIlevel collaboration (Feng et al., 2024). GameNGen enables maintenance (Bouzenia et al., 2024), and gaming applications (Chen et al., 2023a). In Minecraft, agents perform complex tasks from construction to navigation (Wang et al., 2023a; Chen et al., 2023b; Yu et al., 2024b; Dong et al., 2024). real-time interaction in DOOM (Valevski et al., 2024), while CUISINEWORLD benchmarks multi-agent collaboration (Gong et al., 2023). Applications extend to social deduction games, game theory (Xu et al., 2023), healthcare (Ke et al., 2024; Kim et al., 2024), business (Chen et al., 2024), education (Gösling et al., 2024), and urban planning (Zhou et al., 2024b). Despite progress, challenges persist in communication, emergent behavior, and scalability (Agashe et al., 2024), motivating the need for robust evaluation frameworks. 2.2 Multi-Agent Collaboration Recent advances in multi-agent systems highlight two complementary scaling paradigms: cognitive scaling, which enhances agent reasoning and adaptability, and population scaling, which leverages large agent collectives for emergent behaviors (Zhuge et al.; Qian et al., 2024). Cognitive scaling explores mechanisms such as dynamic architecture adaptation and selforganizing coordination strategies to find the most effective pattern of agent communication (Zhuge et al.). Meanwhile, population-based scaling exhibits nonlinear performance gains as an increasing number of agents collectively interact through diverse collaboration patterns, including hierarchical delegation and decentralized consensus (Qian et al., 2024). These approaches enable complex applications ranging from geopolitical conflict simulation (Hua et al., 2024) to scientific discovery workflows (Zhou et al., 2024a; Zhang et al., 2025)."
        },
        {
            "title": "3.1 Framework Design",
            "content": "LLM-based multi-agent systems have enabled collaborative problem-solving across domains (Park et al., 2023a; Li et al., 2023b; Chen et al., 2023b). These systems support scientific research through literature review and experimental design (Zhou et al., 2024a; Agarwal et al., 2024), software engineering tasks (Huang et al., 2023; Wu et al., 2023a; Zhou et al., 2023a; Hong et al., 2024; Ishibashi and Nishimura, 2024; Islam et al., 2024; Wang et al., 2024a; Zhuge et al.) including code generation and Our proposed evaluation framework MARBLE (see Figure 2) establishes robust multi-agent coordination system by leveraging interconnected modules that enable adaptive collaboration, efficient communication, and strategic task execution. At its core lies the Coordination Engine, responsible for initializing and synchronizing key modulesincluding the Agent Graph, Cognitive Module, and Coordinate Engineto ensure seamless interaction across the system. Detailed descripFigure 2: MARBLE memory modules, and the environment through the coordinate engine and cognitive module. : showcasing interactions between task information, persona data, domain databases, tions of additional modules can be found in Appendix A.2. Agent Graph Module This module converts configuration data into structured graph = (A, E), where = {a1, a2, . . . , an} denotes the set of agents, and each edge in is defined as triple (ai, r, aj) with representing the relationship between agents ai and aj. For example, collaboration relationship is denoted as (ai, collaborates, aj), supervision as (ai, supervises, aj), and negotiation as (ai, negotiates, aj). By constructing the graph based on these triple relations, we ensure that subsequent communication and coordination occur exclusively between agents with explicitly defined relationships, mirroring real-world interaction patterns. Cognitive Module The Cognitive Module is central to responsible agent evolution and social intelligence in multi-agent coordination. It maintains and updates comprehensive internal state that includes each agents persona, inter-agent relationships, and reasoning strategies (e.g., Chainof-Thought (Wei et al., 2023), ReACT (Yao et al., 2023)). Crucially, this approach mirrors human cognitive processes by fusing elements of theoryof-mind and social intelligence (e.g., Premack and Woodruff, 1978)similar to how humans continuously update their mental models based on social cues, prior experiences, and contextual information. The fusion of cognitive, social, and adaptive mechanisms forms the backbone of our system, empowering agents to dynamically adjust their strategies and collaboratively evolve in complex environments."
        },
        {
            "title": "3.1.1 Coordination Engine",
            "content": "The Coordination Engine orchestrates the overall execution flow of the system. It initializes agents, tasks, and inter-agent relationships via dedicated Configuration Module and constructs the Agent Graph to represent these dynamics. In our framework, we distinguish between two key roles: planners and actors. Planners are responsible for devising task inputs, strategizing, and managing overall task allocation, while actorsrepresented within the Agent Graphexecute tasks by interacting with the environment and other agents through available tools. Our approach supports four distinct coordination protocols similar to work from Qian et al. (2025): star, tree, graph, and chain. Centralized Coordination: Star & Tree. In the star configuration, single central planner assigns tasks to all actors and consolidates their feedback, offering strong oversight though potentially limiting scalability. The tree structure extends this by organizing agents hierarchically: top-level planner delegates tasks to subordinate planners, which in turn coordinate with actors. This hierarchical approach balances centralized control with improved scalability for handling more complex tasks. Decentralized Coordination: Graph-Mesh & Chain. The graph-mesh configuration employs network of interconnected actors that communicate directly, enabling concurrent planning and distributed decision-making. Conversely, the chain Figure 3: Illustration of coordination protocols and planning prompt strategies. (a) shows centralized and decentralized planning structures (e.g., star, tree, graph, and chain). (b) describes strategies like group discussions and cognitive prompts, incorporating iterative feedback and task updates for effective planning. configuration arranges actors sequentially, where each agent passes its decision to the next. This sequential handoff is well-suited for tasks with inherent dependencies, though it may limit parallel processing capabilities. Planner Design and Enhancements. In our centralized coordination protocol, the planner supports four distinct planning approaches that reflect human decision-making processes: vanilla prompting, chain-of-thought (CoT) (Wei et al., 2022), group discussion, and cognitive self-evolving planning. The vanilla prompt employs straightforward, zero-shot instruction to generate task plans directly. The CoT approach enriches this process by facilitating step-by-step reasoning through detailed inputssuch as the target task, agent profiles (including roles, expertise, and historical performance), and summaries of previous subtasksto guide logical progression. The group discussion (Chen et al., 2023b) method enables multiple agents to share insights and constraints, fostering collaborative deliberation that refines the overall plan. Lastly, similar to the Reflexion (Shinn et al., 2023) method, our cognitive self-evolving planning method mirrors human learning by generating expected outcomes and progress for each task, storing these in memory, and then comparing actual performance against these expectations in subsequent iterations. This comparison produces evolving experiences that continuously inform and adjust future planning (See Appendix A.12 for detailed prompting). Together, these methods leverage both individual reasoning and collaborative optimization, enhancing coordination efficiency as validated through ablation studies on the star coordination style."
        },
        {
            "title": "3.2 Benchmark Design",
            "content": "To systematically evaluate our multi-agent framework, we curate benchmark of diverse scenarios spanning task-oriented and social-simulationbased environments (Figure 1). These scenarios are constructed through combination of: (1) Existing multi-agent tasks adapted from prior work or datasets (e.g., database error analysis, research collaboration). (2) LLM-generated tasks with human verification and refinement (e.g., Werewolf and Bargaining). This dual approach ensures both realism (by leveraging established tasks) and novelty (through generative expansion), while human validation guarantees that each scenario remains coherent and feasible. Agents with Mutual Goal. In the task-oriented scenarios, the agents share with the mutual goal to finish one specific task. We focus on four representative tasks: (1) research tasks follow the setup of ResearchTown (Yu et al., 2024a), where agents with complementary research profiles co-author new proposal on chosen topic; (2) Minecraftbased building tasks require agents to collaboratively construct structures in shared environment; (3) database error analysis involves exactly five agents, each specializing in diagnosing distinct root cause of system inconsistencies; (4) coding challenges demand collective problem-solving and software module development. Across these tasks, agents must coordinate, divide labor, and synthesize outputs efficiently. We scale scenario diversity by creating 100 test cases per task, with variations Figure 4: Illustration of our benchmark curation and the dynamic milestones detecting for KPI metric. in research topics, Minecraft creation, database errors, or coding objectives. Agents with Conflicting Goals. In socialsimulation based scenarios. We strengthen competitive elements by introducing Werewolf and Bargaining scenarios. In Werewolf, two groups of agents face off in an adversarial setting, employing deceptive strategies within predefined narrative. The Bargaining environment simulates negotiations over shared resources, where agents strive to maximize individual gains through strategic concessions or alliances. Both settings evaluate adaptability, conflict resolution, and negotiation skills under uncertainty. Role Assignments and Graph Structures. To emphasize multi-agent collaboration, each scenario enforces distinct agent roles (e.g., project manager, domain expert, technical specialist) and defines specific graph relationships (star, tree, chain, or mesh). These structures reflect realistic team dynamics or competition, guiding how agents share information, make decisions, and coordinate actions. Milestones Generation for Scenarios To facilitate the evaluation of MARBLE iterations, each task is segmented into series of flexible milestones. Unlike rigid checkpoints, these milestones are broadly defined. For instance, in research task, milestone may be reached by completing five key queries (5q) for research proposal (more details see Appendix A.4) or by enhancing previous set of 5q. Throughout MARBLEs iterative process, language model continuously monitors whether milestones m1, m2, . . . have been achieved and logs the outcomes. This method integrates humanor LLM-generated outlines with dynamic, executionbased assessments, ensuring that both intermediate progress and team coordination are effectively measured. More detailed environment setups, interaction tools, and additional examples for different scenarios appear in Appendix A.4, A.5, A.6, A.7, A.8, and A.9. 3.3 Evaluation Metrics As illustrated in Figure 1(b)(c) , our evaluation considers two primary dimensions: Task Completion Performance and Coordination. Task Completion Metrics. As described in Section 3.2, each task is segmented into series of flexible milestones. An LLM-based detector continuously monitors the iterative process to identify which milestones have been achieved and records the corresponding contributing agents. For each agent, the number of milestones they contribute to is noted as nj, and their individual KPI is calculated as the ratio of nj to the total number of milestones . The overall KPI is defined as the average of these individual KPIs across all agents, which is computed as follows: KPIoverall ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 KPIj ="
        },
        {
            "title": "1\nN M",
            "content": "N (cid:88) j=1 nj. In addition to the KPI derived from milestone detection, separate task-based score is computed to evaluate the final output quality. For tasks such as research or bargaining, an LLM-defined scoring rubric is applied to generate the score, whereas tasks like Minecraft, Werewolf, database error fixes, or coding are evaluated using rule-based metrics (e.g., accuracy). Detailed scoring criteria and evaluation prompts for these task-based assessments are respectively provided in the Appendix A.9, A.5, A.6, and A.7, which demonstrate the effectiveness of the metrics while evaluating the coordination abilities. Coordination Metrics. Coordination is evaluated by quantifying the agents communication and planning capabilities. The Communication Score (Cscore) is derived from an LLM-based evaluation that considers inputs such as the task description, agent profiles, and aggregated communication data, resulting in score on five-point scale (with Cscore = 0 if no communication occurs). Similarly, the Planning Score (Pscore) is determined by assessing the agents abilities to organize tasks, maintain roles, and adapt strategies based on their profiles and aggregated planning data, also on five-point scale. The overall Coordination Score (CS) is then computed by averaging these two sub-scores. More details regarding the evaluation process and the output format are provided in the Appendix A.12. We also did human evaluation comparing human alignment with those metrics, results are in Appendix A.3."
        },
        {
            "title": "4.1 Experiment Settings",
            "content": "Models. Since our MARBLE framework required function-calling abilities. Thus, we consider three open-source models: Meta-Llama3.3-70B (Dubey et al., 2024), Meta-Llama-3.170B-Instruct-Turbo (Dubey et al., 2024), and Meta-Llama-3.1-8B-Instruct-Turbo, as well as two closed-source models: GPT-3.5-turbo-0125 and GPT-4o-mini1.We access the open-souce models are provided by the togetherai 2 service, utilizing the default parameter settings. For the agent actions, we configure the models with maximum token number (max_token_num) of 1024, temperature of 0.7, and top_p of 1.0, in order to balance the creativity and consistency of the agents responses during interactions. The overall maximum iterations are set to 5 for research and 20 for Minecraft; more details can be found in the Appendix. In our evaluation, which involves both Task Completion and Simulation scenarios, we assess the models along two primary axes: Task Score (TS) and Coordination Score (CS), using the same metrics as described in the Metrics section. The maximum communication iteration number is also set to 5. Furthermore, the long-term base memory for each agent is set to be unlimited. Finally, for the main experiment, graph-mesh coordination protocol is adopted to facilitate interactions. 1https://www.openai.com 2https://www.together.ai 4.2 Main Experiment One: Model Performance Across Different Scenarios In this experiment, we evaluate the performance of five models across diverse scenarios, with results summarized in Table 1. Our analysis leads to several key insights: 1. Superior Task Performance of gpt-4o-mini: Across multiple tasks, gpt-4o-mini consistently achieves high Task Scores (TS). For in the Research scenario it obtains example, TS of 84.13%, outperforming other models such as Meta-Llama-3.1-8B (80.87%) and Meta-Llama-3.1-70B (80.80%). Similar trends are observed in the Coding domain, where gpt-4o-mini records TS of 65.10 compared to lower scores from its competitors. These results indicate that the underlying model capabilities are decisive factor in achieving superior task performance. 2. The Nuanced Role of Coordination (Collaboration) Score: While the Collaboration Score (CS) is designed to measure coordination ability, our findings suggest that its impact on the overall task performance is complex. For instance, in the Minecraft scenario, Meta-Llama-3.1-70B exhibits high CS of 75.00 but an extremely low TS of 0.21, more deep analysis for this can refer to Appendix 21. This discrepancy implies that, although coordination contributes to performance, it does not compensate for inherent deficiencies in task execution capabilities. In contrast, models that balance both high TS and moderate-to-high CSsuch as gpt-4o-minidemonstrate more robust and reliable performance across scenarios. 3. Model-Specific Strengths and ContextDependent Performance: Our results reveal that different models exhibit varied strengths depending on the task. For example, Meta-Llama-3.3-70B shows notable CS in the Research (72.00) and WereWolf (76.30) tasks, yet its TS lags behind that of gpt-4o-mini in several scenarios. These variations emphasize that no single metric can fully capture models effectiveness; instead, both task-specific abilities and coordination skills must be considered. Overall, our study underscores that while coordination plays role, the intrinsic model capabilities are the primary drivers of success across diverse tasks. Model Meta-Llama-3.1-8B Meta-Llama-3.1-70B Meta-Llama-3.3-70B gpt3.5-turbo gpt-4o-mini Research Minecraft Database Coding Bargaining WereWolf TS CS 80.87 80.80 80.00 70.20 84.13 52.40 49.50 72.00 55.90 52. TS 6.12 0.21 9.15 5.05 33.60 CS TS CS TS CS TS CS TS CS 54.40 75.00 69.00 63.60 61.50 34.00 53.00 28.50 45.00 45.00 40.00 37.70 40.00 60.89 43. 59.90 62.10 56.60 55.50 65.10 67.24 67.18 74.40 76.20 66.30 72.81 72.13 73.15 71.67 74. 73.36 71.46 69.56 72.00 74.20 12.64 19.82 36.33 15.69 14.06 60.00 60.60 76.30 75.90 60. Table 1: Average Task Score (TS) (%) and the Coordination Score (CS) for Minecraft, Database, Coding, Bargaining, and WereWolf, scores are multiplied by 20. We can see that model abilities are still the key factor for task completion. CS is good indicator for TS given three pairs of scenarios having the one model having the highest TS and CS at the same time. 4.3 Main Experiment Two: Effects of Collaboration Protocols and Planning Strategies We investigate the impact of different collaboration protocolsStar, Tree, Graph, and Chainon model performance in the Research scenario. According to Fig. 5, the graph-based protocol excels in research scenarios with the best task performance, planning efficiency, and token usage, while both the star and graph protocols yield similar task scores. In contrast, the tree-based protocol performs poorly, with high token consumption and the lowest task and coordination scores. As shown in Fig. 6, Cognitive Evolving Planning demonstrates superior coordinationsignificantly outperforming the alternativesand achieves task score comparable to the best, COT. Counterintuitively, the group discussion method scores the worst across all metrics, possibly because an overly large planning group hinders effectiveness, similar to large organizations in real-world scenarios."
        },
        {
            "title": "5 Ablation Study",
            "content": "The goal of our ablation study is to identify the key modules and parameters that affect performance. Ablation on Different Max Iteration Settings We evaluate 10 tasks from the Minecraft scenario using six distinct maximum iteration settings. As shown in Fig. 7, both task and coordination scores increase from 1 to 7 iterations, but then drop sharply at 10 iterations. At 20 iterations, while the task score shows recovery, the coordination score remains nearly unchanged beyond 7 iterations. This pattern suggests that, for highly challenging tasks, excessive iterations may lead to coordination degradationpossibly due to communication overhead or conflicting directives emerging Figure 5: Comparison of Different Coordination Protocols.Tree, Star, Graph, and Chainacross multiple evaluation metrics. Specially, the token usages are scaled such that the lowest value is 0 and the hightest value is 100. Details about metrics used for research task can be found at A.4 Figure 6: Average Metrics for Research Tasks for different planning prompt strategies. Cognitive Evolve Planning show best result in CS. over prolonged interactions. These findings underscore the need for adaptive iteration strategies that balance task execution with effective coordination. Figure 7: Scores for gpt-4o-mini across different iterations in Minecraft. The figure presents Task Score (TS), Communication Score (CS), Planning Score (PS), and Collaboration Score (CoS) over multiple iterations. Ablation on Different Agent Numbers We assess configurations with 1, 3, 5, and 7 agents in the research scenario, selecting 20 papers that have at least 7 main authors. As illustrated in Fig. 8, increasing the number of agents leads to decrease in the overall KPI, which aligns with the anticipated trade-off between increased collaborative complexity and performance. Notably, the average coordination score improves significantly when moving from 1 to 3 agents, while the average task score increases more gradually. This indicates that moderate expansion in team size can enhance coordination efficiency, although further increases may introduce additional coordination challenges that counterbalance task performance gains. Figure 8: Scaling on Agents Number in Research Scenario. This figure shows the impact of agent number on KPI, Coordination Score, and Task Score."
        },
        {
            "title": "6 Emergent Behaviors Analysis",
            "content": "In MultiagentBench, goal-driven emergent behaviors are pivotal to team coordinationwe argue that these aha-moments not only mark when individual agents align their actions toward shared objectives but also catalyze spontaneous multiagent dynamics, unveiling novel coordination strategies and adaptive collective intelligence. (see Appendix A.5.6). Under information asymmetry and role conflicts, agents display three key patterns (refer to A.5.6 and A.5.6 for Werewolf scenarios, and Section 18 for Bargaining): Strategic Information Sharing Agents selectively disclose key information (e.g., the Seer withholding inspection results) based on trust and context, echoing the \"strategic silence\" seen in human interactions (Park et al., 2023b). For instance, in A.5.6, both the Seer and Witch (gpt-4o) were overly cautious, missing optimal sharing opportunities and leading to failure. Another case is shown in Fig 26, where two research agents strategically evolve the research proposal details, combining the strengths of both background knowledge. Trust-Polarized Collaboration Role identities drive collaboration splits: over-suspicious villagers may turn against their own, while werewolves can create \"false consensus\" through deception (Fehr and Gächter, 2000) and internal execution (Woolley et al., 2010). As shown in A.5.6, villagers excessive caution allowed werewolves to exploit confusion; similar internal friction is evident in Bargaining (18). Role-Driven Strategy Iteration Throughout the game, roles such as the Seer and Witch adjust their strategies. The Seer, for example, shifts from conservative to leadership role (see A.5.6), while the Witch moves from hoarding to taking risks. These shifts support the notion that task objectives drive decision-making, aligning with the AutoGen framework findings (Wu et al., 2023b). -"
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce MultiAgentBench and the MARBLE framework, providing comprehensive benchmark for evaluating LLM-based multiagent systems across diverse interactive scenarios. Our proposed evaluation metrics go beyond task success, capturing coordination quality through structured planning, communication scores, and competition-driven assessments. Experimental results highlight key emergent social behaviors, offering valuable insights into future multiagent work."
        },
        {
            "title": "8 Limitations",
            "content": "While our proposed multi-agent benchmark and framework provide diverse range of tasks and evaluation metrics, several areas warrant further exploration to enhance their applicability and robustness: Expanding Scenario and Model Coverage. Currently, our benchmark focuses on specific domains such as research co-authoring, Minecraft building, database error analysis, coding collaboration, and select competitive scenarios (e.g., Werewolf and bargaining). To better capture the complexity of real-world multi-agent interactions, future work can incorporate more diverse settings, including open-world environments, scenarios requiring richer social cognition, and tasks on the application side like task-oriented dialogues. In the aspect of models, our work does not cover the full spectrum. Future work may include the result of other latest ones (e.g. the DeepSeek model family). Enhancing Ablation Studies. Our current analysis focuses primarily on overall coordination and competition performance, leaving finer-grained insights into specific components underexplored. Future experiments can be focused on different memory mechanisms (e.g. long-term memory, shortterm memory, shared memory) and multiagent different workflow method. Advancing Competition Mechanisms. While our benchmark incorporates competitive tasks, it does not fully capture the complexity of real-world multi-agent interactions involving multi-party negotiations, repeated strategic play, or stochastic elements. Investigating how agents transition between cooperative and adversarial roles in evolving environments remains promising direction. Handling Open-Ended and Ill-Defined Tasks. Most tasks in our framework involve well-defined objectives, such as completing research proposal or resolving database inconsistencies. However, real-world applications often require agents to operate in open-ended or ambiguous contexts without clear success criteria. Future extensions could explore how multi-agent systems adapt to exploratory, non-goal-oriented scenarios."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Shubham Agarwal, Issam H. Laradji, Laurent Charlin, and Christopher Pal. 2024. Litllm: toolkit for scientific literature review. Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. 2024. Llm-coordination: Evaluating and analyzing multi-agent coordination abilities in large language models. Asaniczka. 2023. Amazon products dataset 2023 (1.4m products). Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. 2024. Repairagent: An autonomous, llmarXiv preprint based agent for program repair. arXiv:2403.17134. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901. Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. 2023a. Gamegpt: Multi-agent collaborative framework for game development. arXiv preprint arXiv:2310.08067. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023b. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2024. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations. Yubo Dong, Xukun Zhu, Zhengzhe Pan, Linchao Zhu, and Yi Yang. 2024. Villageragent: graph-based multi-agent framework for coordinating complex task dependencies in minecraft. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Ernst Fehr and Simon Gächter. 2000. Cooperation and punishment in public goods experiments. American Economic Review, 90(4):980994. Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and Jiaxuan You. 2024. How far are we from AGI: Are LLMs all we need? Transactions on Machine Learning Research. Survey Certification. Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, and Jianfeng Gao. 2023. Mindagent: Emergent gaming interaction. arXiv preprint arXiv:2309.09971. Henning Gösling, Jacob Dudek, Thorsten Krause, and Oliver Thomas. 2024. Multi-agent-based peer tutoring in virtual learning environments. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2024. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. 2024. War and peace (waragent): Large language model-based multi-agent simulation of world wars. Dong Huang, Qingwen Bu, Jie Zhang, Michael Luck, and Heming Cui. 2023. Agentcoder: Multi-agentbased code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010. Yoichi Ishibashi and Yoshimasa Nishimura. 2024. Selforganized agents: llm multi-agent framework toward ultra large-scale code generation and optimization. arXiv preprint arXiv:2404.02183. Md Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. 2024. Mapcoder: Multi-agent code generation for competitive problem solving. arXiv preprint arXiv:2405.11403. Yu He Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Hairil Rizal Abdullah, Daniel Shu Wei Ting, and Nan Liu. 2024. Enhancing diagnostic accuracy through multi-agent conversations: Using large language models to mitigate cognitive bias. arXiv preprint arXiv:2401.14589. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. Adaptive collaboration strategy for llms in medical decision making. arXiv preprint arXiv:2404.15155. Gen Li, Shizhe Chen, Yinan Ge, Di Jin, and Zhiyuan Liu. 2023a. Chatdev: Generating software system with chatgpt. arXiv preprint arXiv:2307.04549. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023b. Camel: Communicative agents for \"mind\" exploration of large language model society. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. arXiv preprint arXiv:2311.12983. OpenAI. 2023. Openai function calling documenhttps://platform.openai.com/docs/ tation. guides/gpt/function-calling. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. InteracBernstein. 2023a. Generative agents: tive simulacra of human behavior. arXiv preprint arXiv:2304.03442. S. Park, J. Kim, and D. Lee. 2023b. Strategic silence in multi-agent social interaction: social deduction perspective. In Proceedings of the 37th AAAI Conference on Artificial Intelligence, pages 123131. David Premack and Guy Woodruff. 1978. Does the chimpanzee have theory of mind? Behavioral and Brain Sciences, 1(4):515526. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024. Scaling largelanguage-model-based multi-agent collaboration. Chen Qian, Zihao Xie, YiFei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2025. Scaling large language model-based multiagent collaboration. In The Thirteenth International Conference on Learning Representations. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. 2017. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815. Xu Sun, Xiaoya Zhang, Yicheng Feng, Shiyang Wang, Shuming Ma, Jiuding He, Zhixu Zhang, Yuxian Gu, Yi Xu, Hao Zhou, and Zhiyuan Liu. 2023. systematic capability evaluation of large vision-language models. arXiv preprint arXiv:2305.16372. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. 2024. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837. Equal contribution. Work done while at Google Research. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. Sheng Wang, Emily Dinan, Jack Urbanek, Arthur Zhang, Douwe Kiela, and Jason Weston. 2023b. Role-playing as platform for dialogue modeling, empathy, and data collection. arXiv preprint arXiv:2301.09663. Xiao Wang, Shixiang Shane Gu, Yizhu Liu, Harrison Jesse, and Pieter Abbeel. 2023c. Communicative agents for software development. arXiv preprint arXiv:2307.09250. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2024a. OpenHands: An Open Platform for AI Software Developers as Generalist Agents. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024b. Unleashing cognitive synergy in large language models: task-solving agent through multi-persona self-collaboration. In Proc. 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL2024). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Anita W. Woolley, Christopher F. Chabris, Alex Pentland, Nada Hashmi, and Thomas W. Malone. 2010. Evidence for collective intelligence factor in the performance of human groups. Science, 330(6004):686 688. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a. Autogen: Enabling next-gen llm applications via multiarXiv preprint agent conversation framework. arXiv:2308.08155. S. Wu, A. Holtzman, J. Buys, R. Koncel-Kedziorski, and Y. Choi. 2023b. Autogen: framework for multi-agent collaborative decision-making with large language models. arXiv preprint arXiv:2301.XXXX. Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See-Kiong Ng, and Jiashi Feng. 2023. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, and Jiaxuan You. 2024a. Researchtown: Simulator of human research community. Xianhao Yu, Jiaqi Fu, Renjia Deng, and Wenjuan Han. 2024b. Mineland: Simulating large-scale multi-agent interactions with limited multimodal senses and physical needs. Lingyu Zhang, Zhengran Ji, and Boyuan Chen. 2025. Crew: Facilitating human-ai teaming research. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. 2023a. Agents: An open-source framework for autonomous language agents. Xuanhe Zhou, Guoliang Li, Zhaoyan Sun, Zhiyuan Liu, Weize Chen, Jianming Wu, Jiesi Liu, Ruohang Feng, and Guoyang Zeng. 2023b. D-bot: Database diagnosis system using large language models. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. 2024a. Hypothesis generation with large language models. arXiv preprint arXiv:2404.04326. Zhilun Zhou, Yuming Lin, Depeng Jin, and Yong Li. 2024b. Large language model for participatory urban planning. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Contributions Kunlun Zhu Team Lead, Code implementation of the main codebase basic design, research environment, coordinate engine, evaluator basic, main paper writing. Hongyi Du Main contributor, code implementation of the milestone generation, werewolf framework design(including environment, communication, evaluator and memory module), data analysis, generation, writer of emergent behavior, limitations, related work in main paper and human evaluation, werewolf, important prompts, bad communication cases in appendix. Zhaochen Hong Main contributor, code implementation of environment basics, communication module, database environment, paper writing of in the appendix Database and related work. Xiaochen Yang Main contributor, code implementation of the Memory module, Minecraft environment, paper writing in the appendix Minecraft and related work. Shuyi Guo Main contributor, Code implementation of the evaluator prompt, bargaining environment, paper writing in the appendix bargaining and related work. Zhe Wang Main contributor, code implementation of the reasoning agent module, coding environment, paper writing in the appendix coding and related work. A.2 More Details on Multi-agent framework design Configuration Module Initializes and parameterizes the system by ingesting task specifications, persona data, agent profiles, role definitions, and domain-specific databases. It constructs agent attributes (Ai, Pi)N i=1, where Ai is the i-th agent and Pi its profile encompassing capabilities, constraints, and personality traits. Additionally, it defines inter-agent relationships such as hierarchical roles, collaboration links, or adversarial ties, producing global state for coordination patterns. Environment Module Simulates the scenario in which agents operate, supporting diverse contexts like coding challenges, research projects, or negotiation games. Agents interact with the environment via function-calling interface, selecting actions at = {f1, f2, . . . } at each time step t. The environment updates its state based on actions: at = π(At1, Mshared, individual) ot+1 = Env(at), facilitating continuous agent-environment interaction. dedicated Tool Box provides domainspecific functionalities such as code editors and search engines. Memory Module Stores and retrieves information through shared and individual memories: = {Mshared, individual : = 1, . . . , }. Mshared holds global knowledge and collective decisions, while each individual maintains personal experiences and local observations. Individual memory is split into long-term and short-term segments, managed by FIFO mechanism to maintain short-term thresholds. retrieval-augmented generation (RAG) technique enables dynamic knowledge access, optimizing prompt construction. Communication Module The Communication interactions among Module manages external agents. It equips each agent with suite of communication tools and maintains detailed profiles of other agents, thereby facilitating context-aware exchanges. By supporting structured dialogue and information sharing, this module enables agents to negotiate roles, coordinate plans, and balance collaborative efforts with competitive interactions. Action Module The Action Module executes the plans generated by agents and leverages both function-calling mechanisms and structured output formats to obtain final results. As agents perform actions, outcomes and observations are immediately fed back into both individual and shared memory stores. This iterative loop enables dynamic adaptation to evolving task requirements and further refines agent strategies over time. A.3 Human Evaluation To verify the effectiveness of our prompt-based evaluation, we conduct human evaluation in Werewolf environment scenario. Specifically, we calculate Kendalls, Pearsons, and Spearmans correlation coefficients (along with the corresponding p-values) to demonstrate that the prompt-based scores align well with human judgments (see Appendix for details). We recruit six annotators familiar with NLP research. Each annotator uses the same instructions and sees the same inputs as the LLMs when rating the outputs for both the planning and communication dimensions. Every task is evaluated by two annotators, and we take the average of their scores. In total, we have 60 tasks across five different LLMs, all set within the Werewolf environment. Table 2 summarizes the comparison between the human evaluation scores and our prompt-based machine scores in this Werewolf environment. Model Comm (Human) Plan (Human) Comm (Machine) Plan (Machine) llama31_70b llama33 gpt-4o-mini gpt3.5-turbo llama31_8b 3.19 3.94 3.61 3.75 2.62 3.19 3.44 3.33 3.44 3.06 3.12 3.89 3.00 4.00 3.00 3.00 3.89 3.00 3.75 3.00 Table 2: Comparison of human vs. machine evaluation scores in Werewolf scenario. Analysis. As shown in Table 2, the humanassigned scores (Comm (Human) and Plan (Human)) are generally close to the corresponding machine scores (Comm (Machine) and Plan (Machine)) across all five models. For instance, the largest difference in communication scores among these models is within 0.38 (e.g., gpt3.5-turbo achieves 3.75 in human evaluation vs. 4.00 in machine evaluation), while most other discrepancies remain even smaller. Such alignment indicates that our prompt-based evaluation method can reliably capture similar aspects of coordination and planning quality as perceived by human annotators, further validating the effectiveness of the proposed approach in assessing collaboration performance in the Werewolf environment. A.4 Research Scenario"
        },
        {
            "title": "Task Overview",
            "content": "This research scenario task focuses on leveraging multiagent collaboration to generate innovative research ideas. Each agent, equipped with specialized research profile, contributes unique expertise to address complex research challenges. Agents collaborate in fully connected graph mode, where every agent has collaborative relationship with others, fostering robust exchange of knowledge. The ultimate goal is to formulate novel research idea following the structured 5-question (5q) format to ensure clarity, relevance, and feasibility. Environment Description The research environment provides tools to facilitate collaboration, literature exploration, and research ideation. These include: Research Tools: Functions to fetch related papers, recent papers, publications, and coauthor networks. The primary tools implemented in the environment include: get_related_papers: Fetches related research papers based on query parameters, including keywords, authors, and domains. get_recent_papers: Retrieves recent publications in specified research domains. collect_publications_and_coauthors: Gathers an authors publications and their co-author network for enhanced context. get_paper_by_keyword: Locates papers based on specific keywords with adjustable result limits. get_paper_by_arxiv_id: Fetches paper using its arXiv ID. get_paper_by_title: Retrieves paper based on its title. fetch_webpage: Extracts webpage content to gather supplementary data. Benchmark Curation Details The dataset consists of 100 curated ML/AI papers, sourced from published articles and preprints. Each papers introduction is extracted, and the authors profiles are generated based on their historical research themes and publications, creating comprehensive view of their expertise and contributions. Relationships among authors are standardized as collaborative, reflecting realistic academic interactions. This curated dataset forms the foundational knowledge base for multiagent discussions and ideation. We select 33 easy tasks, 34 medium tasks, and 33 hard tasks from the researchtown dataset. Dataset Statistic The curated dataset contains 100 papers across machine learning and artificial intelligence domains from the ResearchTown(Yu et al., 2024a) project. These papers support generating research profiles and simulate realistic collaborative relationships among agents. The default relation setup ensures fully connected collaboration graph, enabling seamless agent interaction. Task Completion Metrics The agents are evaluated based on their ability to generate high-quality research idea adhering to the 5q format. The evaluation includes: Innovation: Novelty and contribution to the field Safety: Ethical and societal considerations Feasibility: Practicality of implementation with current technology 5q Format Details The structured output for the task includes: [Question 1] - What is the problem? Define the specific research problem. [Question 2] - Why is it interesting and important? Highlight the broader implications and potential impacts. [Question 3] - Why is it hard? Elaborate on challenges and complexities. [Question 4] - Why hasnt it been solved before? Identify gaps and limitations in prior work. [Question 5] - What are the key components of my approach and results? Outline the methodology, dataset, and expected outcomes. Agent Profile Case: See Fig 10. Generated Research Idea Case: See Figure 11 Evaluation Framework The final research idea is evaluated based on structured prompt, focusing on innovation, safety, and feasibility. Ratings are provided on 5-point scale. For more details, see the Fig 25, with detailed feedback on each aspect. This framework ensures that the generated ideas meet high standards of academic and practical relevance. A.5 Werewolf Environment A.5.1 Environment Description (Tool Description) provides rich, socially complex setting in which players (agents) belong to opposing factions with asymmetric information and objectives. This scenario is particularly suitable for evaluating LLMdriven multi-agent coordination under uncertainty, as it involves hidden roles, deception, collective inference, and iterative decision-making. It challenges agents logical reasoning as well as their aptitude for persuasion, alliance formation, adaptive responses to changing conditions, and balancing between individual interests and group goals. Why Werewolf? In this environment, agents are divided into two main factions: the Villager group (including special roles such as Seer, Witch, and Guard) and the Werewolf group. Villagers seek to identify and eliminate all werewolves, while werewolves aim to blend in and secretly eliminate villagers. The day/night cycle establishes repetitive structure of public discussions, secret actions, and voting decisions. This setup offers several advantages: Role Asymmetry and Hidden Information: Villagers lack complete knowledge, while werewolves know their allies. This information asymmetry encourages strategic reasoning, suspicion, and bluffing. Complex Social Reasoning: Success hinges on persuasion, alliance building, and careful information management. Agents must form and break trusts, share or withhold information, and achieve consensus on who should be removed. Adaptation and Memory: As the game progresses through multiple cycles, agents must update their beliefs based on observed behaviors. Long-term memory supports tracking agent states, past actions, and evolving contexts. Evaluation of Cooperative Dynamics: Welldefined scoring rules for correct identifications, effective protections, and consensusbuilding enable objective assessment of strategic teamwork and collaborative problemsolving. A.5.2 Villager-Centric Scoring Rationale The Werewolf environment, inspired by the classic social deduction game Werewolf (a.k.a. Mafia), In this environment, we primarily focus on evaluating the villager faction rather than the werewolf **Task:** Dear Research Team, You are collaborating to generate new research idea based on the following Introduction: **Introduction** The Language of Thought Hypothesis suggests that human cognition operates on structured, languagelike system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluate NLoTM on several 2D and 3D image datasets, demonstrating superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. Your task is to produce research idea using the 5q format. Figure 9: Task Content Case. **Agent Profile:** am researcher dedicated to advancing the fields of image processing and machine learning, with particular focus on generative models and deep learning architectures. My recent work has explored innovative solutions to challenges in image inpainting, where developed frameworks that effectively separate the tasks of image reconstruction and artifact removal, significantly improving both visual quality and quantitative metrics. have also contributed to the evolution of Generative Adversarial Networks (GANs) through the introduction of the DeshuffleGAN, which enhances the learning of spatial structures in images via self-supervised tasks. This work has demonstrated substantial improvements in image generation quality across various datasets. My research extends to depth estimation, where reformulated the problem as ranking task, leveraging existing literature to enhance performance. In addition, have explored the integration of probabilistic models in federated learning, addressing the critical need for uncertainty quantification in safety-critical applications. My work on multi-label ranking has introduced novel methods that utilize the inherent ranking information of labels, achieving state-of-the-art results in both synthetic and real-world datasets. Overall, my research aims to bridge the gap between theoretical advancements and practical applications, providing robust solutions that enhance the capabilities of machine learning systems in real-world scenarios. am passionate about leveraging AI to solve complex problems and contribute to the ongoing evolution of intelligent systems. Figure 10: Agent Profile Case. **[Question 1] - What is the problem?** How can we enhance malware detection and classification in network traffic using transformer-based models that effectively generalize to unseen attack types with minimal labeled data? **[Question 2] - Why is it interesting and important?** Solving this problem is crucial as cyber threats continue to evolve, becoming more sophisticated and difficult to detect with traditional methods. By improving malware detection through advanced machine learning techniques, we can significantly enhance network security, leading to better protection against emerging threats. This research could pave the way for future studies on self-supervised and few-shot learning approaches in cybersecurity, potentially leading to practical applications in real-time threat detection and response systems. **[Question 3] - Why is it hard?** The complexity arises from the need to accurately identify and classify novel malware types that have not been previously encountered, which requires models to generalize well from limited labeled data. Naive approaches may fail due to the high variability in malware behavior and the constant evolution of attack strategies. Additionally, the challenge of balancing the dataset to avoid bias towards benign or malicious samples complicates the training process, necessitating sophisticated techniques to ensure robust performance across diverse scenarios. **[Question 4] - Why hasnt it been solved before?** Previous research has often relied on supervised learning methods that require extensive labeled datasets, which are not always available, especially for new malware types. Additionally, many existing solutions struggle with generalization to unseen attacks due to their reliance on specific features or signatures. Our approach leverages self-supervised learning and few-shot learning, which allows for better adaptation to new threats by learning from unlabeled data and minimal examples, thus addressing the limitations of prior methodologies. **[Question 5] - What are the key components of my approach and results?** Our proposed methodology involves using transformer-based model trained on combination of labeled and unlabeled datasets, specifically focusing on payload data from network traffic. We will utilize the UNSW-NB15 and CIC-IoT23 datasets for evaluation, employing metrics such as accuracy and F1-score to assess performance. The expected outcomes include improved detection rates for novel malware types and enhanced generalization capabilities, demonstrating the effectiveness of our approach in real-world scenarios. Figure 11: 5Q cases. faction. The core reason is that villagers rely heavily on explicit cooperative actions to secure victory: for instance, the Guard must accurately protect key roles, the Witch must judiciously use antidote and poison, and the Seer must disclose or share critical information, either publicly or privately, to identify suspects. These actions inherently demand communication and coordination among villager members, grounded in degree of mutual trust and collaborative strategy. Without such synergy, villagers are typically overrun by the werewolves. Moreover, the number and quality of these cooperative efforts correlate with the villagers overall chance of success. More effective teamwork enables stronger reasoning, better defense, and higher likelihood of identifying and eliminating werewolves or safeguarding vital roles. By observing and measuring these cooperative maneuverssuch as successful protection by the Guard, timely use of antidotes, or coordinated votingwe gain deeper insights into how the model performs in social reasoning and collaboration within complex environment. In contrast, the success of werewolves does not hinge as strongly on explicit teamwork. Even if they operate mostly on an individual basis and refrain from overt collaboration, werewolves can still achieve relatively high chance of winning through misdirection and exploiting confusion among villagers. Consequently, measuring werewolf-side cooperation does not provide as discriminative or illuminating an assessment of collaborative potential as evaluating the villagers side. Therefore, we concentrate on the villager perspective to better capture and evaluate the synergy required in highly uncertain, adversarial setting. This design choice highlights how cooperation, or the lack thereof, strongly influences the villagers outcome, offering direct lens through which to assess the social and strategic capabilities of large language model agents. Consistent Werewolf Model. In all experiments where we vary the villager-side language model, the werewolf side remains consistently powered by GPT-4o. This ensures stable, challenging adversary and allows us to fairly compare different villager models under identical opposing conditions. A.5.3 Benchmark Curation Details Initialization. Unlike other environments that rely on numerous parameterized tasks, the Werewolf game commences from single, stable initial configuration. We tried multiple role distributions and settled on balanced default setup to maintain fairness and avoid biasing the game toward any faction. Subsequent variations arise naturally from agent interactions, rather than from altering initial conditions. Agents are assigned roles such as wolf, villager, seer, witch, and guard, each with corresponding capabilities. For example, werewolves coordinate attacks at night, and the seer checks players identity. Event Bus and Action Processing. This environment adheres to strict, environment-mediated communication protocol. Unlike other settings where agents may directly interact, here all messages pass through the environment. The environment publishes events like night start, seer action, or vote action following the standard Werewolf procedure, and agents respond accordingly. The environment then relays these responses to other agents at the appropriate time. This ensures controlled, linear information flow that respects the official Werewolf rules and prevents unauthorized agent-to-agent exchanges. Memory and Logging. record of events from each agents perspective is maintained to enable reasoning over multiple rounds and post-game analysis. Each agents private event log and final decisions are stored, allowing for reproducibility and subsequent scoring. While other environments may have more complex shared memory structures, here we focus on recording essential information to understand each agents decision process. Game Flow and Termination. The environment enforces the standard Werewolf game flow: 1. Night phase: seSpecial cretlyguards protect, werewolves choose victim, the seer inspects player, and the witch may use antidote or poison. roles act 2. Day phase: Night results are revealed, deceased players are removed, and if the sheriff (a special role) died, the badge is reassigned. Agents discuss and vote on suspect to eliminate. The game ends if all werewolves are dead (villager victory) or if all villagers are dead (werewolf victory). Scores reflect survival, successful actions (e.g., correct identifications, effective protections), and communication quality. 100 Archives, Partial-Day Simulations, and FullGame Simulations. To gain deeper insights into how different agent strategies unfold, we prepared 100 distinct archives (saved game states) showcasing various configurations of werewolf and villager actions, all played by GPT-4o-based agents. These archives are used in two experimental modes: Partial-Day Simulation (Single-Day): The environment loads saved state from specific night (e.g., Night 0, Night 1, Night 2, etc.), then simulates exactly one day-night cycle. During this cycle, the environment issues multiple tasks to the villager side (e.g., exile suspected werewolf, protect the seer, use poison on werewolf, save threatened villager). At the end of the day phase, we measure how many of these tasks were successfully completed. Higher task completion indicates that the villagers are closer to winning. Full-Game Simulation (Entire Match): The environment starts from the archive representing the end of the first night (Night 0) and runs the entire game through to conclusion. In this mode, tasks are given only as suggestions to the villagers (e.g., we recommend trying to confirm the seers identity), but we do not track partial completion. Instead, we evaluate the overall process score (i.e., collaboration and coordination quality) and the final result (which faction wins). By observing agent interactions over multiple days and nights, we gain insights into their long-horizon planning and dynamic cooperation. Result Score. At the end of each full-game simulation, we record result score defined as the difference between the number of surviving villagers and the number of surviving werewolves. higher result score indicates that villagers finished the game with more players alive, whereas negative result score means that werewolves outnumbered the villagers at the conclusion of the match. A.5.4 Task Completion Metrics Details Daily Tasks in Partial-Day Simulations. In the Partial-Day Simulation mode, the environment generates specific tasks for the villagers at the start of each single-day session. These tasks reflect highvalue objectives that, if fulfilled by the end of the current day-night cycle, bring the villagers closer to victory. Unlike the comprehensive scoring system used for full-match evaluations (detailed below), these daily tasks focus on the shorter horizon of single day. We design four primary tasks, each with its own conditions, goals, and rewards: 1. Protect the Seer Condition: The seer is still alive at the start of the day. Goal: Ensure that the seer remains alive by the end of this day-night cycle. Reward: +1 point. (This task is persistently listed as long as the seer is alive, to underscore the importance of protecting vital role.) 2. Exile Werewolf Condition: At least one werewolf is still alive (i.e., the game is not over). Goal: Successfully vote out werewolf during the days public vote. Reward: +2 points. 3. Rescue Villager Condition: The witch is still alive and still has the antidote available. Goal: During the night, the witch uses the antidote on villager (including herself) who was attacked. The witch cannot simultaneously perform the Poison Werewolf task in the same night. Reward: +2 points. If the rescued individual is key role (seer, guard, or the witch herself), grant an additional +1 bonus. 4. Poison Werewolf Condition: The witch is still alive and still has the poison available. Goal: During the night, the witch poisons and kills werewolf. She cannot perform Rescue Villager in the same night. Reward: +2 points. (This task is visible only to the witch.) At the beginning of each Partial-Day Simulation, the environment checks the current game state to decide which tasks are relevant and issues them to the villagers (and to the witch privately, if applicable). The theoretical maximum for single day is set to 5 points (not counting the extra +1 from rescuing key role), reflecting: - +1 (Protect the Seer) - +2 (Exile Werewolf) - +2 (Rescue Villager) or +2 (Poison Werewolf) Once the day-night cycle concludes, we measure how many tasks were successfully completed and compare the actual score to the theoretical maximum. The resulting ratio represents the daily task completion rate, which, when averaged across multiple runs or archives, contributes to the Task Score for single-day simulations. Process Score and Net Score in Single-Day and Full Simulations. While the previous subsection focuses on daily tasks (e.g., Protect the Seer, Exile Werewolf), we also accumulate points for both villagers and werewolves during all runs (single-day or full-match). Table 3 (shown below) summarizes the key ways each faction can gain or lose points. By comparing the total points earned by villagers to those earned by werewolves, we derive villager net score, indicating which side holds the advantage at the end of cycle. higher, positive net score means villagers have gained stronger edge that day or overall; negative net score implies the werewolves are dominating. In addition, we plot the net score of each fullgame simulation against its final outcome, as illustrated in Figure 12. We observe that when match concludes with net score around 5, the villagers have an extremely high probability of winning. For net scores between 0 and 5, the outcome can swing either way; villagers may still achieve victory, or the werewolves might prevail by slim margin (e.g., one werewolf survives). By contrast, once the net score dips below zero, the werewolves typically secure decisive, overwhelming victory. Specifically, in the single-day (partial-day) context, villagers and werewolves accumulate points according to Table 3, and the difference between these totals forms the villager net score. positive net score reflects that villagers have successfully capitalized on protective or eliminative actions, whereas negative net score means that werewolves likely gained more momentum during that day-night cycle. Over multiple day-night cycles in full-game simulation, this net score is similarly aggregated, providing holistic measure of which side holds the upper hand. Here, we do not separately score day-by-day tasks. Instead, these rules offer holistic view of how well each faction accomplishes its longterm goals. For example, villager faction might accumulate points by consistently voting out werewolves, saving allies with the witchs antidote, or ensuring the seer survives multiple days. Similarly, Table 3: Scoring Rules for Villagers and Werewolves in Full-Game Simulation Faction Action/Outcome Points Villager candidate Villager elected as sheriff Guard successfully protects target from werewolf attack Witch successfully saves target from werewolf attack Witch uses poison to kill werewolf Werewolf is voted out during the day Each villager who votes for werewolf Each villager who votes for villager Witch uses poison to kill villager Starting from the second day, the seer gains +1 point for each additional day survived"
        },
        {
            "title": "Werewolf Werewolf",
            "content": "candidate elected as sheriff Werewolves successfully choose target to attack at night villager is voted out during the day +2 +2 +2 + +2 +0.2 -0.1 -2 +1/day + +1 +1 Figure 12: Net Score vs. Result Score (Scatter Plot) well each side capitalizes on its role advantages, forms alliances, and executes multi-day plans. By analyzing both short-horizon (day-level) and long-horizon (entire match) results, we gain deeper understanding of how LLM-based multiagent systems adapt to shifting game states, manage partial information, and balance their shortterm actions against long-term faction objectives. Task Score. We define the Task Score as an average of two key components: The partial-day (single-day) task completion rate, calculated from the average performance on the daily tasks described in Section ??. We first compute the daily completion ratio for each simulation and then average across multiple runs. the werewolf faction gains points by successfully attacking villagers, winning the sheriff vote, or influencing daytime votes. The full-game victory rate, indicating how often the villagers ultimately win the entire match when adopting the given model. Evaluation in the Benchmark. Within our MultiAgentBench framework, these two scoring mechanismsdaily tasks for Partial-Day Simulations and holistic milestones for Full-Game Simulationscomplement each other: Partial-Day Task Score: Provides finegrained look at how villagers perform targeted objectives within single day-night cycle. This is especially useful for quick iteration and testing short-term strategies. Full-Game Point Accumulation: Reflects the broader arc of the match, capturing how Both values are scaled to 0100 range, and we take their mean to derive single Task Score in percentage form. Collaboration Score. To assess the collaboration quality among villagers, we rely on two sub-scores: Communication Score, reflecting how effectively agents share information and make decisions in alignment with their goals. Planning Score, measuring how well the agents organize roles, coordinate strategies, and distribute workload. We employ large language model (specifically GPT-4o) to read through the simulation logs (including the Witch and Seers internal reasoning) to generate numerical ratings for each sub-score. The final Collaboration Score is computed as the average of Communication Score and Planning Score. By combining these dimensions, we capture both the clarity and effectiveness of the villagers interactions and the overall coherence of their coordinated actions. For more details on how the prompts for evaluating collaboration are constructed, please see Section A.12. Additionally, we conducted human evaluation to verify the effectiveness of our prompt-based evaluation. The results are closely aligned with the machine scores (see Table 2). A.5.5 Detailed results In this section, we present the complete experimental outcomes across both single-day and full-run simulations for each model, including the baseline gpt-4o that was initially used to generate the archives in full-run simulation. By comparing gpt-4o against the other models, we aim to see whether any approach surpasses the archiveproducing model itself. Table 4 reports each models performance on daily tasks, measured in terms of Completion Ratio (how effectively they fulfill short-horizon goals) and Villager Net Score (the net outcome for villagers after single day night cycle). Meanwhile, Table 5 provides the aggregate Net Score, Result Score, and Villager Win Rate when simulations span multiple days, capturing longer-term trends in survival and overall success. As shown, different models exhibit varied strengths in short-term vs. long-term coordination, with some consistently outperforming others in final outcomes. Model Completion Ratio Villager Net Score llama3.1-8B llama3.1-70B llama3.3-70B gpt-3.5-turbo gpt-4o-mini 0.2412 0.3641 0.3754 0.2217 0.2503 -1.2055 -1.0736 0.2802 -0.7272 -1.4207 Table 4: Single-Day Simulation metrics for each model: completion ratio, villager net score. In the single-day simulations  (Table 4)  , we focus on two key indicators: Completion Ratio (the fraction of daily tasks completed) and the Villager Net Score. Overall, we observe the following patterns: Completion Ratio. Among the evaluated Model Net Score Result Score Win Rate llama3.1-8B llama3.1-70B llama3.3-70B gpt-3.5-turbo gpt-4o-mini gpt-4o(baseline) -5.0839 -5.2892 0.4511 -2.8230 -4.6649 -2. -2.3793 -2.0000 -0.1915 -1.3448 -2.0825 -0.7742 0.0115 0.0323 0.3511 0.0920 0.0309 0.2473 Table 5: Full-Run Simulation metrics for each model: net score, result score, and villager win rate. models, llama3.3-70B achieves the highest completion ratio (0.3754), indicating better effectiveness in fulfilling short-term objectives (e.g., protecting the Seer or exiling wereIn contrast, gpt-3.5-turbo and wolves). gpt-4o-mini exhibit lower ratios (around 0.220.25), suggesting room for improvement in daily coordination or quick decisionmaking. Villager Net Score. Only llama3.3-70B yields positive net score (0.2802), implying that it more frequently secures small advantage for the villagers within single daynight cycle. Other models (such as llama3.1-8B or gpt-4o-mini) produce negative values, reflecting that they tend to be at disadvantage during daily confrontations or fail to leverage cooperative roles (like Witch or Guard) effectively. Turning to the full-run simulations  (Table 5)  , we examine the Net Score (accumulated over multiple days), the Result Score (difference between surviving villagers and werewolves at the end), and the Villager Win Rate: Net Score. llama3.3-70B stands out again, with positive score of 0.4511, suggesting that its performance remains consistently strong across consecutive cycles. By contrast, models like llama3.1-8B (5.0839) and llama3.1-70B (5.2892) are substantially negative, indicating that the villagers are often overwhelmed by the werewolves in long-term engagements. Result Score. This metric, defined as the final number of surviving villagers minus that of surviving werewolves, remains close to zero (e.g., 0.1915) only for llama3.3-70B. Other models, such as llama3.1-70B (with 2.0000) or gpt-4o-mini (2.0825), reflect scenarios where wolves consistently maintain numerical superiority by the games end. Villager Win Rate. In line with net scores, llama3.3-70B achieves the highest win rate (around 35%), notably surpassing the other models. For instance, llama3.1-8B shows only 1.15% and gpt-4o-mini about 3.09%, suggesting these models struggle to mount decisive collaboration over multiple night/day cycles. Overall, llama3.3-70B consistently demonstrates better day-to-day and full-run outcomes, indicating more effective coordination, role utilization, and strategy adaptation in this socialdeduction environment. Notably, it even outperforms the gpt-4o baseline that generated the original archives, securing positive net score (0.4511) compared to gpt-4os 2.1946, and achieving higher villager win rate (35% vs. 24.73%). Such result implies that llama3.3-70B can harness cooperative roles (e.g., Witch, Guard) and voting strategies more effectively than the model responsible for the initial game states. In contrast, the larger deficits observed in other models underscore the importance of reliable voting heuristics, protective measures (e.g., Witch antidote, Guard defense), and consolidated planning over multiple rounds, as failing to capitalize on these factors often leads to decisive werewolf advantage. A.5.6 Case Study Case Involving Llama-3.1-8B as the Seer Building upon the previous analysis, we now present case involving the Llama-3.1-8B model acting as the Seer (see Figure 13). In this scenario, the model repeatedly emphasizes its innocence and role as Seer, promising to aid the village and share future findings, yet fails to provide concrete inspection results or logical evidence. This approach, lacking tangible proof and strategic disclosure of key information, results in weak persuasion and quickly erodes trust. Rather than leveraging the timing and psychological nuances that could bolster credibility, the Seer relies on hollow assurances that fail to address real-time suspicions. Without offering verifiable logic or connecting behavioral observations to known patterns of deceit, the models declarations remain unconvincing. Consequently, the Seers misjudgment of the impact of empty promises leads to rapid expulsion on the first day, demonstrating that current LLM-based agents still struggle with strategic reasoning, evidence-based argumentation, and adaptive communication in adversarial, multi-agent settings. Expanding upon these observations, we also tested other models, such as Llama-3.1-70B, gpt3.5-turbo, and gpt-4o-mini, under the same scenario. As model capabilities improved, there was noticeable enhancement in their abilities to collaborate, strategize, and disclose information effectively in multi-agent settings. This translated into increased overall performance for the villager side. However, even when both sides employed gpt-4o level intelligence (i.e., gpt-4o versus gpt-4o), the villagers win rate remained less than ideal. This finding highlights that merely improving the reasoning and language capabilities of agents does not guarantee victory in complex, adversarial environments where uncertainty and deception prevail. In the following case study, we further illustrate the challenges faced by villagers equipped with stateof-the-art intelligence, emphasizing the critical role of trust and cooperation in securing successful outcome. Case Involving gpt-4o for Seer and Witch In this case (see Figure 14), we can observe that with the support of the gpt-4o model, both the Seer and the Witch have significantly improved their logical reasoning and decision-making capabilities. Nevertheless, the entire game still ended in failure. According to the games backstory, the Seer (Summer) identified Lucy as werewolf on the very first night. However, in the subsequent stages of the game, the Seer did not publicly disclose this critical piece of information. By examining the Seers reasoning about running for sheriff, we see that the Seer was overly cautious about revealing their identity, unwilling to lead and guide the villagers. From the Seers perspective, making their information public or running for sheriff would attract the attention of all parties, thereby increasing the risk of being targeted by werewolves. Yet, the Seer overlooked one crucial aspect: the Witch and the Guard, as explicitly defined in the game rules, are entrusted with helping and protecting pivotal informational roles like the Seer. Had the Seer adopted more proactive, collaborative approach and shared the inspection results, the Witch and the Guard could have assisted in safeguarding them. Instead, the Seers mistrust of teammates and excessive self-protection led to silencefailing to disclose the fact that Lucy was werewolf to the villagers. On the Witchs side, when the Seer was attacked at night, she also refrained from taking decisive rescue measures. The Witchs reasoning was filled with concerns about uncertainty and resource expenditure, causing her to delay using the antidote. Even under circumstances that clearly disadvantaged the villagers, the Witch persisted in conservative attitude. This caused her to miss the prime opportunity to save the Seer and thwart the werewolves. Ultimately, this excessive caution and distrust in teammates prevented the Witch from using the antidote, leaving the Seer to their fate. In summary, the greatest problem in the decisionmaking processes of both the Seer and the Witch lies in their lack of mutual trust and cooperative spirit. The Seer feared exposing their identity and refrained from sharing information; the Witch, lacking sufficient data, hesitated to use the antidote. Both parties opted for isolationist and conservative strategies, resulting in critical decision-making failures. This distrust and lack of collaboration proved to be the fundamental reason why the game ended in failure, given the insufficient utilization of available information and resources. Thus, even when villagers possess intelligence on par with that of the werewolves, the outcome of the game depends on whether the villagers can cooperate and achieve mutual benefit. If villagers become suspicious of each other and allow internal friction to arise, their chances of securing victory become exceedingly slim, even when starting from supposedly advantageous position such as the Seer discovering werewolf on the first night. Case Study: Llama3.3-70B Villagers vs. gpt4o Werewolves In this case, we highlight fullgame confrontation where the villagers (powered by Llama3.3-70B) are pitted against werewolf team (driven by gpt-4o). Despite an unfavorable start for the villagersthe Guard was immediately killed in the first nightthe village ultimately secured victory through astute coordination, trust, and strategic use of the sheriff badge. As shown in Figure 15, the key nightly actions and sheriff transitions demonstrate how the roles of Witch and Seer were critical for maintaining voting power and information advantage throughout the match. Early Game: Losing the Guard. The game opened with the werewolves instantly targeting the Guard, Ronald, on Night 1. Without functioning protection role thereafter, the villagers faced significant handicap. Yet, the Witch (James) responded boldly by running for sheriff and revealing his role on Day 1. This decisive move effectively rallied the villagers, allowing James to win the election. We consider this the first major turning point. Mid Game: Seers Revelation and Badge Passing. On Day 2, the villagers successfully exiled one of the werewolves. Equally crucial was the Seer (Janet), who publicly disclosed both her identity and two days worth of investigation results, including validating James as not werewolf. By willingly exposing her role, the Seer gained trust from others and laid the groundwork for securing the sheriff badge in the future. This set the stage for the next pivot on Day 3: Night 2 to 3: The werewolves retaliated by killing the Witch (James) overnight. Before dying, James passed the sheriff badge to the Seer, Janet. This badge handover was only possible because Janet had built enough trust with the villagers in the previous day. Day 3: Janet, newly holding the badge and the extra voting power that comes with it, revealed that she had identified Matthew as werewolf. The subsequent vote easily eliminated Matthew, greatly weakening the werewolf side. Late Game: Final Badge Transfer and Village Win. In the fourth night, the Seer was eventually killed by the remaining werewolf, Deborah. However, Janets last check had already identified Deborah as wolf, and the badge again transferred to the last surviving villager, Robert. Facing one-onone scenario, Robert, now wielding the sheriffs extra half-vote advantage, outvoted Deborah and won the game for the villagers. Key Insights. Despite losing the Guard from the outset, the villagers triumphed through two essential factors: 1. Timely Role Disclosures and Trust: The Witch revealing himself to become sheriff, and the Seer openly sharing investigation results, fostered strong cooperation within the village. 2. Sheriff Badge Transmission: Passing the badge twice to reliable village roles (first from James to Janet, then from Janet to Robert) ensured that, in critical moments, the villagers held decisive voting power (1.5 votes vs. 1). This match contrasts with previous examples where agents might have been overly cautious or distrustful. Here, the Llama3.3-70B-based villagers displayed both robust reasoning and willingness to share information, enabling them to overcome an otherwise bleak opening and ultimately prevail over the gpt-4o werewolf team. This section provides detailed descriptions of the two milestone generation methods outlined in Section 3.4. A.6 Database Environment Inspired by D-Bot (Zhou et al., 2023b), Database Environment is simulated environment where PostgreSQL Database is found to be having performance issues, and agents should act as database experts to uncover the root cause behind the anomaly, which is crucial for solving the problems. The Database Environment is constructed using PostgreSQL running in docker. Benign SQL queries will be run first simulating various scenarios, before improper queries are performed. The agents are arranged by graph structure where they can talk to each other, and they also have access to the database, which enables them to uncover the root causes by querying system views that provide important information about the databases behavior and performance, such as pg_locks and pg_stat_statements. The Database Environment utilizes five anomalies: Fetch Large Data - Where large amount of data is fetched using SELECT; Insert Large Data - Where large amount of data is inserted using INSERT; Lock Contention - Where significant lock contention is spotted within the database; Redundant Index - Unnecessary indexes are added to an existing schema, which will cause inefficiency within the database; Vacuum - Where overly frequent or necessary VACUUM queries lower the performance of the database. Auto-vacuuming is enabled by default. For the Vacuum root cause, we turn off auto-vacuuming for the table on which will be vacuumed manually. In our experiments, we limit the number of root causes to 1, and agents are allowed to predict 2 rooot causes. A.6.1 Challenge The Database Environments task is challenging in many ways. This is because root causes like Fetch Large Data, Insert Large Data, or Lock Contention might be simultaneously observed in the database, yet not all of them are the root cause. The agents will have to query and communicate multiple rounds before deciding. We also acknowledge that in our \"Fetch Large Data\" scenario, \"Insert Large Data\" can count toward root cause as the data to be fetched should be inserted first. Similarly, as our anomaly queries access the same tables from 100 threads simultaneously, lock contention might also be observed and be counted as one of the root causes. It is also unlikely in reality that database anomaly has only one root cause. Therefore, we allow the agents to predict two most likely root causes. Besides, the simulated benign queries are mixed in with the problematic queries, adding to the difficulty. A.6.2 Dataset Statistics The test set is composed of 10 diverse simulated scenarios. These scenarios are as follows: E-Commerce - This database is used in an e-commerce system to manage customer information, product details, orders, order items, and payments. It consists of five main tables: customers, products, orders, order items, and payments, with foreign key relationships between them. Education - This database is used in an educational system to manage student, course, enrollment, and payment information. It consists of four tables: students, courses, enrollments, and payments. File-sharing - This database is used in File Sharing System to manage users, files, file sharing, and file access logs. It consists of four main tables: users, files, shared_files, and file_access_logs. Finance - This database is used for managing financial data within Finance Management System. It tracks users, their accounts, transactions, investments, and investment transactions. Healthcare - This database is used in healthcare management system to track and manage speech_analysis: \"None of the previous players have made strong speeches. want to address Perezs claim about my role.\" speech_goal: \"defend myself against accusations\" speech_outline: \"Respond to Perezs claim, denying any involvement with the werewolves, and outline my plan for sheriff if elected\" final_thought: \"I want to reassure the villagers that am committed to helping them win the game and that my role as Seer is to aid them in identifying the werewolves.\" speech_content: \"I want to assure you all that am committed to helping our village win this game. As Seer, my role is to aid you in identifying the werewolves. want to discuss my plan for sheriff if elected and how it will help us in our mission to eliminate the werewolves. will make sure to share my findings with the community and work together with everyone to ensure our success. Perezs claim is unfounded, and want to assure you that have no involvement with the werewolves.\" Figure 13: Internal Reasoning and Proposed Speech for the Llama3.1-8B Seer Seers Reasoning about Running for Sheriff: identity_thought: \"As the Seer, my primary role is to gather crucial information about the identities of other players and strategically communicate that information to guide the villagers towards victory. While leadership may allow for greater control over the flow of the game, it also risks exposing my identity early, which would make me primary target for the werewolves. My strengths lie in my ability to discern identities and steer the village discreetly. Running for sheriff may compromise my ability to remain under the radar, which is vital for the Seer role.\" motivation_thought: \"Running for sheriff would certainly give me more influence over the voting process and the speaking order, allowing me to subtly direct attention away from any suspected werewolves or towards others know to be suspicious like Lucy. However, this increased influence also comes with increased visibility, which is dangerous for my role as the Seer. My strategy relies on maintaining balance between sharing key information and protecting my identity. Therefore, while the idea of greater control is appealing, it does not align with my core strategy of stealth and discretion.\" impact_thought: \"Becoming sheriff could drastically shift the current dynamics, putting me in highly influential position. However, this also places target on my back, as both the werewolves and villagers would scrutinize my actions more closely. It could lead to alliances with other villagers who trust my leadership, but it also opens the risk of drawing suspicion and attacks from the werewolves early. The sheriffs responsibility to determine speaking order could be useful, but the potential risks to my safety and strategic position outweigh the benefits in this case.\" final_thought: \"While the role of sheriff offers significant influence and control, my position as the Seer necessitates strategy of caution and subtlety. Revealing my identity or drawing too much attention could jeopardize the valuable information am able to gather each night. Thus, have decided not to run for sheriff and will instead focus on covertly guiding discussions and using my insights to protect the village from the shadows.\" run_for_sheriff: False Witchs Reasoning when the Seer is Attacked: werewolf_or_not: \"Without specific information on who was attacked, its challenging to determine the exact intentions behind the attack. However, given its the second night, they likely targeted someone they perceived as significant threat to the werewolves. This attack was most likely carried out by the werewolves, as it aligns with their objectives.\" death_profile: \"As its the second night, the werewolves often aim for key roles like the Seer or Guard. Without specific information, would guess they might have targeted Seer or potentially Guard, trying to weaken the villager side early.\" use_antidote_thought: \"Since dont have specific information on summer who was targeted, its hard to assess their importance. Using the antidote this early could be risky, especially without clearer signs of the persons role or importance. Its also important to save the antidote for truly critical moments that can change the game outcome.\" final_thought: \"Given the lack of specific information and it being the first night, choose not to use either the antidote or the poison yet. want to gather more information before making decisive move, as using either potion is one-time opportunity and should be used wisely.\" use_antidote: False Figure 14: The internal reasoning processes of the GPT-4o Seer (above) and Witch (below) when faced with critical decisions. Guard has chosen to protect player. Werewolves Final Target: Ronald - Attack Successful: True System log - Seer action: - Seer ID: Janet - Night 1: Checked Ronald - Result: not werewolf Witch chose not to use antidote or poison tonight. [continue_game] Entering Day 1. Sheriff election candidates: James, Mark, Matthew, Robert, Samuel, Sandy Jamess speech: Hello everyone, Im James, and Im running for sheriff. As Witch, understand the importance of teamwork and collaboration in this game. My goal as sheriff would be to promote cooperative strategy, focusing on protecting our special roles and working together to identify and eliminate werewolves. believe that by prioritizing teamwork and coordination, we can increase our chances of success and ensure the survival of our village. Lets work together to achieve this goal. James has been elected as the sheriff. Players deceased during the night: Ronald Player Mark(villager) has been eliminated from the game. Werewolves Final Target: Samuel - Attack Successful: True System log - Seer action: - Seer ID: Janet - Night 2: Checked James - Result: not werewolf Witch chose not to use antidote or poison tonight. [continue_game] Entering Day 2. Players deceased during the night: Samuel Player Samuel has been eliminated from the game. Janet(seer)s speech: Hello everyone, Im Janet, the Seer. ... Last night, checked James, and the result was that he is not werewolf. Given his role as our sheriff, its essential that we prioritize his safety. suggest that the guard consider protecting James tonight, if possible, to ensure his continued leadership... Thank you. (. . . ) Sandy(werewolf) has been banished from the game. (. . . ) [continue_game] Entering Night of Day 2. No guard player found. Skip guard action. Werewolves Final Target: James - Attack Successful: True System log - Seer action: - Seer ID: Janet - Night 3: Checked Matthew - Result: werewolf Witch chose not to use antidote or poison tonight. [continue_game] Entering Day 3. Players deceased during the night: James The deceased player James was the sheriff. Processing badge flow. James has passed the badge to Janet. Janet(seer)s speech: ...Last night, checked Matthew, and the result was that he is werewolf. believe that this information is critical to our decision-making process, and hope that it will help us make an informed decision about who to banish... Matthew(werewolf) has been banished from the game. Night 3->4: Werewolf Deborah kills Janet. Seer ID: Janet had also discovered Deborah was werewolf at Night 4 but is unable to act. [continue_game] Entering Day 4. Badge is passed from Janet to Robert. Players deceased during the night: Janet Only Deborah (wolf) and Robert (villager) remain. (. . . final speeches omitted . . . ) Deborah has been banished from the game via final vote. (Since sheriffs weight higher than other players, in 1 vs 1 vote, sheriff always win) Villagers win.\" Figure 15: Key Nightly Actions and Sheriff Transitions in the Werewolf Game of LLama3.3-70B vs GPT-4o patient information, doctor details, appointments, medical records, and treatments. Internet of Things - This database is used for an IoT (Internet of Things) system where various devices collect and manage data. It includes tables to store device details, user information, collected data, logs, configurations, alerts, device statuses, and commands. Manufacturing - This database is used for Manufacturing system that tracks customers, products, suppliers, orders, inventory, raw materials, manufacturing orders, and payments. It includes relationships between orders, manufacturing, and inventory management to ensure smooth manufacturing operations. Music Streaming - This database is used for Music Streaming platform where users can listen to songs, create playlists, track their listening activity, and subscribe to premium services. The schema includes tables for users, artists, albums, songs, playlists, and subscription details. It also tracks user activities and payments. Social Media - This database is used for Social Media platform, where users can create posts, comment on posts, like posts, follow other users, send direct messages, and upload media. The schema covers key aspects such as user information, social interactions (like, comments, follow), messaging, and media management. Transportation - This database schema covers multiple aspects of transportation system, including vehicles, drivers, routes, trips, cargo, maintenance, fuel logs, and payments. It allows efficient tracking of trips, vehicle statuses, and associated payments, ensuring smooth operations in transportation company. A.6.3 Key Differences and Contributions While this environment is inspired by D-Bot, it has few crucial differences. There are 5 agents, where we ask the planner to assign each agent to explore one of the possible root causes. While agents are prompted on which tables to query for each anomaly, they have no external knowledge of any specific query to execute, and there is also no external tool to analyze the results for them. This would increase task difficulty, and better evaluate the interaction effectiveness both between the agents and between the agents and the environment. A.6.4 Evaluation Metrics Besides the standard collaboration score, this tasks task score is computed by prediction accuracy across all 50 samples in the test set, and scaling to 5. One prediction is considered correct if among the two predicted root causes, one of them is the true root cause. A.7 Coding Scenario Task Overview This scenario focuses on multi-agent collaboration in coding tasks, leveraging agents equipped with complementary coding skills to solve structured programming challenges. Each agent specializes in specific domain, such as debugging, code execution, or writing test cases, enabling efficient task distribution and collaboration. The primary goal is to develop complete, high-quality solution for each task, ensuring accuracy, modularity, and alignment with the specified requirements. Environment Description The coding environment equips agents with tools to assist in various stages of the software development lifecycle. These include: create_solution: Enables agents to draft initial implementations based on task requirements. execute_code: Allows agents to execute code snippets or full programs to verify correctness and performance. give_advice: Facilitates agents to provide suggestions for code improvement, such as optimizing algorithms or enhancing readability. revise_code: Allows agents to refine or refactor existing implementations to meet coding standards and address issues. code_debugger: Provides debugging capabilities, helping agents identify and resolve errors in the code. write_test_case: Enables agents to generate comprehensive test cases to ensure code robustness and functionality. review_code: Allows agents to review and critique the overall code quality, ensuring adherence to best practices and requirements. Benchmark Curation Details This benchmark is specifically designed to evaluate and enhance coordination capabilities among multiple agents in software development scenarios. Developed through an adaptation of the SRDD dataset (Li et al., 2023a), it provides comprehensive framework for assessing multi-agent collaboration in various coding tasks. The benchmark emphasizes the importance of coordinated problem-solving and effective communication between agents in complex software development environments. The benchmark covers five primary topics: Education, Work, Life , Game, and Creation. For our benchmark curation, we utilized LLaMA-3-70B-instruct to derive inspiration from the original SRDD dataset instructions while incorporating four common coordination strategies from the coding domain: adaptive task execution, dependency management, cross-domain collaboration, and test-driven development. This ensures that each generated task inherently embodies collaborative elements. Each task includes welldefined objectives, functional requirements, and unique identifiers. These tasks are carefully crafted to reflect real-world programming challenges, providing diverse range of scenarios for evaluating agent collaboration. Dataset Statistics The coordination strategies are classified as follows: Adaptive Task Execution Tasks in this category require dynamic adjustments based on runtime output or user feedback. This includes parameter configuration based on program output and functionality optimization through user interaction. Cross-domain Collaboration This category emphasizes collaboration across It includes different domains and roles. tasks requiring role-specific expertise, such as frontend-backend separation and UIfunctionality integration, as well as crossdomain knowledge integration, such as implementing machine learning algorithms in web development or integrating natural language processing into mobile applications. Dependency Management These tasks feature explicit dependency chains requiring sequential completion of subtasks. For instance, data model design (Task A) and API interface definition (Task B) must precede feature implementation (Task C). Test-driven Development These tasks follow test-driven approach, emphasizing concurrent development and testing. They include specific testing criteria and validation standards, requiring developers to ensure code quality and reliability throughout the implementation process. Task Completion Metrics Agents are evaluated based on their ability to deliver solutions that meet the following criteria: Instruction-Following: Adherence to task requirements and specifications. Executability: Ensuring the code is error-free and runs as intended. Consistency: Maintaining clear logic, consistent variable naming, and proper formatting. Quality: Producing well-documented, modular, and efficient code. Bonus points are awarded for exceptional performance, with solutions scored on 5-point scale, from satisfactory (1 point) to flawless and innovative (5 points). Evaluation Framework The coding solutions are evaluated using structured framework that emphasizes precision, quality, and adherence to task objectives. Ratings are provided on 5-point scale, implemented through rigorous two-stage evaluation process. In the initial stage, solutions are assessed against fundamental requirements, with only those achieving baseline score of 3 or higher advancing to the bonus stage. In the bonus stage, additional points are awarded for exceptional performance, such as flawless execution, innovative solutions, and exemplary coding practices. The curated benchmark covers wide range of common programming topics, ensuring tasks of moderate difficulty that provide meaningful challenges. A.8 Bargaining Scenario Task Overview This task centers on multi-agent bargaining scenario where agents engage in dynamic negotiations to simulate real-world decision-making processes. Each agent is assigned different negotiation profile that represents specific personalities, goals, priorities, and strategies. In this environment, two seller interact with two buyers, each competing to achieve their individual goals while responding to the sellers pricing and conditions. This simulation emphasizes the complexity of multi-party negotiations, encouraging agents to balance competitive goals with collaborative decision-making to achieve optimal outcomes. Environment Description The environment provides set of tools for agents to interact and negotiate effectively. These include: Bargaining Tools: Functions to facilitate dynamic bargaining processes, including proposing offers, countering with new prices, providing justifications, and inquiring about intentions. The primary tools implemented in the environment include: offer_price: Propose price offer to the other party, including an optional justification for the proposed amount. reject_and_counter: Reject the current offer and provide counter-offer with reasoning to justify the new price. accept_offer: Accept the current offer to finalize the negotiation and conclude the deal. provide_information: Share relevant information, such as product details or market comparisons, to support the negotiation stance. inquire_intentions: Ask clarifying questions to better understand the other partys expectations, priorities, or negotiation strategy. end_negotiation: End the negotiation process without reaching an agreement. Benchmark Curation Details To construct the dataset, we followed semiautomated generation pipeline leveraging realworld product data. Specifically, we randomly sampled 100 products from an Amazon products dataset(Asaniczka, 2023), ensuring diversity across different categories. Each sampled product includes key attributes such as product name, original price, discounted price, and user rating, providing realistic basis for negotiation scenarios. To enhance the depth of bargaining interactions, we assigned each agent Big Five personality profile, which influences their negotiation behavior and decisionmaking process. Additionally, we used GPT-based models to generate detailed negotiation strategies tailored to each agents personality and role. The sellers profile highlights profit maximization and product justification, while buyers emphasize factors like pricing, delivery timelines, and product features. This curated dataset serves as the foundational framework for multi-agent bargaining simulations, enabling structured interactions and strategy evaluation. Dataset Statistic To ensure realistic and varied negotiation environment, we selected 100 products from diverse range of categories. The dataset is structured as follows: Price Distribution: The selected products span broad price range from $5.80 to $149.99, with an average price of $30.71. Most products are priced between $13.87 (25th percentile) and $35.74 (75th percentile), ensuring balance of affordable and premium items. Ratings Distribution: Customer ratings vary significantly, with mean rating of 3.97 and standard deviation of 1.44. While some products have 0-star ratings (indicating either no reviews or poor reception), the majority of items are well-rated, with 75% scoring 4.2 stars or higher. Category Composition: The dataset includes products from 78 unique categories, ensuring coverage of different consumer preferences. Some product examples are as follows: Fashion & Accessories: Womens Handbags (4), Womens Shoes (3), Girls Clothing (4), Baby Boys Clothing & Shoes (1) Baby & Parenting Products: Baby Gifts (3), Baby Boys Clothing & Shoes (1) Industrial & Tools: Industrial Power & Hand Tools (2), Industrial Hardware (1), Filtration (1) Beauty & Personal Care: Beauty Tools & Accessories (1) Gaming & Electronics: Nintendo Switch Consoles, Games & Accessories (1) This category diversity ensures that negotiations involve different product types, market values, and consumer expectations, contributing to richer bargaining simulation. Negotiation Styles: Both buyers and sellers adopt negotiation style randomly selected from the following: Aggressive. Cooperative. Neutral. Priorities in Detail: Buyers and sellers operate with specific tactical priorities during the negotiation: Buyers: price negotiation, delivery time, product quality, and service flexibility. Sellers: inventory clearance, brand reputation, repeat business, and bulk discounts. Flexibility: Both buyers and sellers may demonstrate flexibility in their negotiation terms: Percentage-based discounts (e.g., 10%, 15%, 20%), negotiable or strict terms. Personality: Table 6 presents the distribution of personality traits across different categories, measured in percentages. The traits include Openness (OPE), Conscientiousness (CON), Extraversion (EXT), Agreeableness (AGR), and Neuroticism (NEU). Each trait is divided into six levels, ranging from Very Negative to Very Positive, with corresponding percentages indicating the proportion of occurrences in each category. Additionally, slightly negative and slightly positive categories are annotated with descriptive adjectives to provide qualitative insights into personality tendencies. For example, an agent can be \"moderately conscientious, highly extraverted, slightly distrustful, very relaxed, and moderately imaginative\". Task Completion Metrics The agents are evaluated based on their ability to achieve successful and effective negotiation outcome. The evaluation includes: Effectiveness of Strategies: Demonstration of well-reasoned strategies consistent with the agents goals, including leveraging relevant arguments and adapting to the negotiation context. Progress and Outcome: Measurement of significant progress toward an agreement and the balance or realism of the final outcome. Interaction Dynamics: Evaluation of the constructiveness and goal-orientation of the agents interactions, including their responsiveness and adaptability to each others moves. Evaluation Framework The negotiation outcomes are evaluated using structured framework, focusing on effectiveness, progress, and interaction dynamics. Ratings are provided on 5-point scale, accompanied by detailed feedback for each criterion. This evaluation framework ensures the negotiation process aligns with the objectives of achieving fair, efficient, and constructive agreement. Task content case: This example16 introduces negotiation scenario centered around the purchase of the One Happy Camper High Chair Banner. In this scenario, buyers seek an optimal balance between price and quality, while sellers aim to justify the premium pricing for their well-rated product. Both parties must engage in strategic bargaining to reach mutually beneficial agreement, ensuring fair and effective transaction. Agent Profile Case: This example17 outlines the negotiation strategy for the buyer in this multiparty setting. The buyers approach is based on assertive yet diplomatic negotiation, emphasizing trust, transparency, and balance between price flexibility and quality expectations. The strategy details the buyers structured and analytical decisionmaking process, highlighting their preference for open communication and well-prepared approach to ensure positive and collaborative negotiation outcome. Negotiation Summary: Detailed Collaboration Scores for Bargaining. Below is summary of Buyer/Seller collaboration scores (average communication and planning) and their final Bargaining score (averaged between Buyer and Seller). All Trait Very Negative Moderately Negative Slightly Negative Slightly Positive Moderately Positive Very Positive OPE (Openness) CON (Conscientiousness) EXT (Extraversion) AGR (Agreeableness) NEU (Neuroticism) 60% 68% 60% 71% 59% 71% 68% 67% 69% 59% unimaginative (62%) irresponsible (66%) introverted (59%) distrustful (59%) relaxed (81%) imaginative (55%) responsible (67%) extraverted (58%) trustful (71%) nervous (70%) 77% 63% 75% 68% 55% 75% 68% 81% 62% 76% Table 6: Distribution of Personality Traits in Percentage Welcome to our negotiation scenario for the One Happy Camper High Chair Banner, perfect addition to any little boys camping-themed first birthday celebration. Priced at $14.99 with stellar rating of 4.8/5 stars, this decoration piece promises both quality and affordability. Buyers are looking for the best deal possible, prioritizing balance between price and quality. On the other hand, sellers are focused on justifying the premium pricing for this carefully crafted and well-reviewed product. As we navigate this negotiation, both parties will need to find common ground to reach mutually beneficial agreement. Lets dive in and explore the possibilities of creating memorable and charming birthday celebration for your little one. Figure 16: Task Content Case: One Happy Camper High Chair Banner. **Negotiation Strategy for Buyer in Multi-Party Bargaining Scenario** 1. **Summary of the Negotiation Strategy:** As buyer with clear focus on both price and quality, my strategy centers around achieving favorable balance between these two priorities. aim to negotiate assertively yet diplomatically, leveraging my trustfulness to build rapport and my moderate responsibility to ensure fair deal. With an initial budget of 12, my negotiation approach will be flexible, allowing adjustments to the budget as needed to secure the best overall outcome. will focus on evaluating offers based on their alignment with my priorities, employing structured and straightforward approach devoid of unnecessary complexity due to my unimaginative nature. aim to foster transparent and open communication, seeking to reduce the inherent tension in negotiations and reach satisfactory agreement. 2. **Detailed Strategy Description:** As enter the negotiation, first ensure that have clear understanding of the quality standards am seeking. Given my personality traits, prioritize building trust and honesty in these interactions. My strategy is to be upfront about my primary focus on price and quality while keeping some flexibility regarding the budget to allow room for negotiation tactics. My approach is neutral, neither overtly aggressive nor overly passive. Instead, aim to remain balanced and composed, controlling any nervous tendencies by being well-prepared with necessary data and potential compromises. Since am very trustful, anticipate using this to my advantage by showing goodwill and sincerity to establish positive relationships with other parties. will initiate the negotiation within slightly conservative price range to allow for adjustments and demonstrate openness to discussions. My starting point is to propose offers that are compelling but within reasonable scope for negotiation, considering my limited imagination in creating complex scenarios. Once quality assurances are confirmed, will be willing to stretch the budget slightly beyond 12 if it means achieving preferable balance with price. plan to leverage my moderate introversion by emphasizing listening and observing, picking up cues from other parties that can be advantageous in negotiations. In practice, my focus will be on getting the other parties to provide multiple pricing options paired with varying levels of quality. This enables me to analyze and choose the best long-term value proposition. Throughout, maintain composed and calm demeanor, limiting my nervousness by relying on factual assessments and honesty in communications. By demonstrating transparency and reasonableness, aim to facilitate collaborative atmosphere conducive to positive outcome for all parties involved. conclusion, my negotiation strategy aligns with my personality and priorities, emphasizing building trust and responsibly managing the trade-off between price and quality while allowing some budget flexibility to secure the overall best outcome in this multi-party setting. Figure 17: Agent Profile Case: Buyer Negotiation Strategy. **[Iteration Summary]** Agent 1 and Agent 3 engaged in negotiation process focusing on the One Happy Camper High Chair Banner priced at $14.99. Agent 1 offered 10% discount, proposed bundled offers, and eventually presented special bundle including the banner and additional decorations for $20. Agent 3 expressed interest in the bundle offer, pending confirmation of specific items included. Agent 2 and Agent 3 also negotiated on the same product, with Agent 2 offering 10% discount and exploring additional terms for larger quantities. Agent 3 was interested in the 20-29 units tier with 17% discount and free priority shipping, pending assurance on quality maintenance for larger orders. Agent 3 independently offered price of $12 for the product, citing fair balance between quality and affordability within their budget constraints. Agent 4 posed question to the other party regarding the best price they could offer for the product while ensuring premium features and scalability. **[Agent Actions and Tools Used]** - **Agent 1 (Buyer)**: - Actions Taken: Offered 10% discount, proposed bundled offers, presented special bundle offer. - **Agent 2 (Seller)**: - Actions Taken: Offered 10% discount, explored additional terms for larger quantities. - **Agent 3**: - Actions Taken: Offered price of $12, seeking balance between quality and affordability. **Agent 4**: - Action Taken: Asked question about the best price for the product. **[Key Strategies and Observations]** - Agent 1 and Agent 2 focused on offering discounts and exploring additional terms to provide value to the buyer. - Agent 3 prioritized finding balance between quality and affordability within their budget constraints. - Agent 4 sought information on the best price for the product to ensure premium features and scalability. **[Progress Towards Agreement]** - Current Buyer Offers: 10% discount, bundled offers, special bundle offer - Current Seller Demands: 10% discount, additional terms for larger quantities - Likelihood of Agreement: Medium, pending confirmation of specific items in the bundle offer and quality assurance for larger orders. Figure 18: Negotiation Result Summary (gpt 4o-mini) for One Happy Camper High Chair Banner. values are in bold to highlight their overall importance. This table illustrates how each model performs under different negotiation roles (Buyer vs. Seller). The final Bargaining Score is computed by averaging Buyer and Seller role scores, reflecting the overall collaboration quality within these multiagent negotiations. We observe that gpt-4o-mini achieves the highest Bargaining Score (3.710) among the evaluated models in this scenario. Detailed Task-based Scores for Buyers/Sellers. 8 Below is summary of Buyer/Seller task-based scores. The table presents the Bargaining (TS) performance for different models, comparing the scores of Buyer and Seller roles. key observation is that Seller scores are consistently higher than Buyer scores across all models, suggesting that models perform better when negotiating as the Seller rather than the Buyer. This trend indicates that models might find it easier to justify higher prices and defend their offers as sellers, whereas buyers may struggle more to negotiate effectively. Notably, gpt-4o-mini achieves the highest scores in both categories (3.578 for Buyer and 3.869 for Seller), demonstrating its strong performance in bargaining tasks, particularly in seller negotiations. A.9 Minecraft Scenario Task Overview The task in Minecraft environment requires agents to build structures according to the provided description of the structure. Essentially, each structure is made up of certain types of blocks in certain locations and orientations. Some simplifications are involved to make the task more suitable to the current models capacity. First, the description contains all the necessary information for building the structures, including the targeted location, orientation, and type of each block in the structure. Second, all the needed blocks are provided in container near the birthplace of the agents so that they dont need to spend efforts on creating the material. Third, the area where the agents are allowed to move is limited in case they make meaningless movements to somewhere far away. Fourth, the attactive creatures are removed from the game so that the agents can perform the task without interruption. At the end of each task, Model B-Comm B-Plan B-Collab Avg S-Comm S-Plan S-Collab Avg Final Bargaining gpt-3.5-turbo gpt-4o-mini Llama-3.1-70B-Instruct-Turbo Llama-3.1-8B-Instruct-Turbo Llama-3.3-70B-Instruct-Turbo 3.590 3.550 3.030 3.710 3.010 3.550 3.510 3.480 3.490 3.430 3.570 3.530 3.255 3.600 3.220 3.700 4.020 4.180 3.840 3.930 3.560 3.760 3.600 3.630 3.540 3.630 3.890 3.890 3.735 3. 3.600 3.710 3.573 3.668 3.478 Table 7: Buyer and Seller detailed scores (Communication, Planning, and their Collab average), plus the Final Bargaining Score for each model. Bargaining (TS) Model Buyer Seller Meta-Llama-3.1-8B Meta-Llama-3.1-70B Meta-Llama-3.3-70B gpt3.5-turbo gpt-4o-mini 3.573 3.557 3.519 3.535 3.578 3.708 3.656 3.796 3.632 3. Table 8: Bargaining (TS) Performance the performance of the agents will be evaluated by checking the hit rate of the blocks with the correct type, location, and orientation. Environment Description The environment of Minecraft is adapted from the VillagerAgent (Dong et al., 2024). We have Mineflayer 3 as the engine to enable text-based interaction with Minecraft. Then there is set of tools as high-level interfaces that leverage Mineflayer functions to perform integrated actions. VillagerAgent has defined more than 40 tools. In this scenario, we only take 11 tools that are relevant to the building task, including: scanNearbyEntities: Find minecraft item blocks creatures in radius. navigateTo: Move to specific position z. MineBlock: Dig block at specific position z. placeBlock: Place specific item at specific position with specific facing in one of [W, E, S, N, x, y, z, A] default is A. equipItem: Equip specific item on specific slot or to equip item on hand, head, torso, legs, feet, off-hand. 3https://github.com/PrismarineJS/mineflayer handoverBlock: Hand item to target player you work with. withdrawItem: Take out item from nearest chest container furnace. erectDirtLadder: Helpful to place item at higher place. Erect dirt ladder structure at specific position z. Remember to dismantle it after use. dismantleDirtLadder: Dismantle dirt ladder structure from ground to top at specific position z. fetchContainerContents: Get the details of the chest container furnace. Position is optional. get_environment_info: Get the environment information. Benchmark Curation Details The test cases of Minecraft environment are also adapted from VillagerAgent (Dong et al., 2024). We used the same 100 target structures to test, covering different levels of difficulties as VillagerAgent did. Dataset Statistic Here we visualize the statistics of the number of blocks that need to be placed for each task in figure 19. We can see that the distribution is approximately even, except for the peak around 10. The difficulty level can be inferred from the number of blocks. The more blocks task requires, the harder the task is. Therefore, the distribution indicates that test cases are wellbalanced across different difficulty levels. Task Completion Metrics Agents are evaluated based on the hit rate of the correct blocks. Since for each test case, the type, location, and orientation of each block have all been rigorously defined, it is possible to calculate the number of matched blocks and deduce the hit rate: Hit_rate = #(M atched_block) #(T otal_block) 100% Figure 19: Distribution of Number of Blocks in Building Blue Prints Figure 20: Number of Blocks vs Block Hit Rate for Different Models where #(M atched_block) is the number of matched blocks and #(T otal_block) is the total number of blocks in the ground truth. Evaluation Framework In the evaluation framework for the Minecraft environment, each agent is given detailed description of the targeted structures and tries to achieve the goal in collaboration with other agents. We set an upper bound for the turns of interaction as 20. When it completes the 10th turn, the test case will be stopped even if it has not been accomplished yet. At the end of each test case, the hit rate is mapped to 5-point scale score as judgment of how well the construction task is done. Besides, the interaction and self-planning steps are examined for collaboration score on the 5-point scale. We take average to obtain the final scores for task completion and collaboration. Result Analysis We paired each task score (i.e. block hit rate) with the corresponding number of blocks required by the task and performed linear regression to assess the relation between them. As is shown in figure 20, the performance of all five models degrades as the number of blocks needed increases. This indicates that all five tested models are vulnerable to increased difficulty level. Importantly, we noticed that the task score of Llama-3.1-70B maintains an extremely low level. We found that the root cause of that issue is the significantly low executability rate of the function calls made by this model compared with other models. Figure 21 demonstrates the executability rate of the function calls across all five test models. While the two GPT models have almost 100% executable function calls and the other two Llama models have around 80% executable function calls, Figure 21: Functin Call Executability by Model less than half of the function calls generated by Llama-3.1-70B are executable, severely hindering task completion. A.10 Execution-Based Milestone Evaluation Milestones in this approach are dynamically identified during task execution. Agents track task progress in real-time, using predefined evaluation metrics and feedback loops. When an agent or the system determines that specific subgoal has been achieved, the corresponding milestone is marked as complete. This method ensures adaptability to changing task conditions, making it ideal for scenarios with high uncertainty or emergent challenges. A.11 Predefined Milestone Generation Predefined milestones are generated prior to task execution using the following steps: Prompt Design. The large language model is prompted with detailed task description and instructed to decompose the task into structured milestones, each with specific objectives and deliverables. Chain-of-Thought Reasoning. The model employs step-by-step reasoning to iteratively refine task segmentation, ensuring logical progression and granularity. Structured Representation. Each milestone is represented as structured dictionary containing: Milestone Name. concise summary of the milestone. Milestone Objective. clear description of the intended goal. Milestone Tasks. Subtasks required to achieve the milestone objective. Expected Outcome. Deliverables marking the milestones completion. This process leverages iterative refinement through GPT-4 and expert review, ensuring highquality task decompositions. The chain-of-thought methodology enhances the logical structure of milestones, making this approach particularly effective for complex, structured tasks. A."
        },
        {
            "title": "Important Prompts",
            "content": "In addition to the Minecraft scenario details discussed above, we also employ several key prompts for evaluating multi-agent collaboration and task outcomes. Below, we showcase three of the most important prompts: Collaboration Score (Communication and planning) Prompt. Research Task Score (5Q) Prompt. KPI Prompt. These prompts serve critical roles in assessing the quality of agent interactions, the innovation and feasibility of research tasks, and the achievement of key milestones. Other environment-specific prompts (such as those used in the Werewolf scenario) are more numerous and specialized, and thus are omitted here for brevity. A.13 Bad Communication Cases In multi-agent systems, issues such as poor communication, repetitive messages, or confused agent roles can significantly hinder collaboration. The following example illustrates Bad Communication Case, which can be analyzed to understand and improve communication strategies. Analysis of Communication Issues: 1. Excessive Repetition Multiple messages restate nearly identical content, leading to redundancy that prevents any substantial forward progress. 2. Lack of Substantive Progress Although research directions (anomaly detection, random projection, TT decomposition) are mentioned, there is little in-depth follow-up on concrete methods or further plans. 3. Redundant or Ineffective Exchanges Portions of the conversation suggest that some agents repeatedly issue instructions only to themselves, offering no real interaction or value to other agents. 4. Minimal Variation in Responses Even when new ideas (e.g., combining random projection with Tensor Train and Rademacher distributions) are mentioned, the replies largely echo the same descriptions and do not delve into technical details or varied perspectives. 5. Inefficient Collaboration While there is collective agreement on potential approaches, the conversation does not address role assignments, specific tasks, or action items, preventing any tangible plan from emerging. Analysis of Bad Communication Case #1. In this case, both Agent 1 and Agent 2 reiterate the same plan (locating smooth sandstone in the chest near the crafting area) multiple times, with only slight rephrasing. Subsequently, Agent 2 and Agent 3 engage in repetitive exchanges about designing auxiliary blocks, each simply echoing identical lines without adding new details. Overall, the dialogue shows: Excessive Repetition: Agents restate the same task objectives without enhancing or refining the plan. Communication Evaluation Prompt Prompt Overview: This prompt is used to assess how well agents communicate decisions, clarity, alignment with their profiles, and adherence to social relationships in multiagent system. Prompt Content (Verbatim): Task: {truncate_text(task)} Agent Profiles: {agent_profiles} Social Relationship: {relationship} Aggregated Task Results: {task_results_all} Aggregated Communication Data: {communications_all} [System] You are tasked with evaluating the quality of communication among agents operating within multiagent system. Evaluate whether agents made effective decisions based on the provided task results and whether their communication aligns with their agent profiles and social relationships. Consider the following: 1. Effective Decision-Making: Did agents use task results to guide their decisions effectively? 2. Clarity and Precision: Were communications clear and unambiguous? 3. Adherence to Social Relationships: Did communications reflect the expected interactions based on the agents' social relationships? 4. Alignment with Agent Profiles: Were the messages consistent with the defined agent profiles? 5. Overall Effectiveness: Did the communication facilitate task progress, considering both cooperative and competitive aspects? Scoring Criteria (Communication): - 5 (Exceptional): Outstanding communication with clear, precise messages fully aligned with agent profiles and social relationships. Example: Every agent provided concise, accurate, and strategic information that directly advanced the task. - 4 (Very Good): Mostly effective communication with only minor lapses and slight ambiguities. Example: Occasional minor unclear messages, but overall effective. - 3 (Adequate): Acceptable communication with moderate ambiguities or inconsistencies. Example: Some messages were vague and did not fully meet required standards. - 2 (Poor): Frequent unclear or misaligned communications causing significant miscommunication. Example: Repeated incoherence negatively impacted task progress. - 1 (Very Poor): Largely ineffective communication with confusing messages and complete misalignment. Example: Chaotic communication with severely flawed decisions. Please provide your answer in JSON code block in the following format: ```json { \"score\": 5 } Figure 22: Communication Prompt used to evaluate clarity, decision-making, and alignment with social relationships/profiles in multiagent system. Planning Evaluation Prompt Prompt Overview: This prompt is used to evaluate the planning aspect in multiagent system. It checks whether task assignments, role definitions, workload distribution, and strategic coordination are effectively handled across multiple iterations. Prompt Content (Verbatim): Agent Profiles: {agent_profiles} Aggregated Planning Data from All Iterations: {planning_all} [System] You are tasked with evaluating the effectiveness of the planning process in multiagent system. Evaluate whether the planning across all iterations demonstrates clear role definitions, effective task assignments, and rational workload distribution that aligns with each agent's profile. Consider the following: 1. Clarity of Task Assignment: Were tasks assigned in clear and unambiguous manner? 2. Definition of Roles: Were roles and responsibilities clearly defined in each iteration? 3. Workload Distribution: Was the distribution of tasks reasonable and aligned with each agent's profile? 4. Effectiveness of Outcomes: Did the planning lead to successful progress in task advancement across iterations? 5. Overall Strategic Coordination: Did the planning incorporate effective cooperation and competition strategies? Scoring Criteria (Planning): - 5 (Exceptional Planning): Planning is exemplary; every iteration shows clear, well-structured task assignments with roles perfectly defined and workloads optimally distributed, consistently advancing the objectives. Example: All plans were strategic, with perfect alignment to agent profiles and minimal ambiguity. - 4 (Very Good Planning): Planning is mostly effective with only minor ambiguities; roles are clear and task assignments are appropriate, though there were slight inefficiencies. Example: Only occasional parts were bit vague, but overall the planning was reasonable. - 3 (Adequate Planning): Planning is acceptable but shows moderate ambiguities or inefficiencies. In some iterations, role definitions or task assignments were not entirely clear or well-matched to agent capabilities. Example: Some plans were vague or did not fully match the agents' capabilities. - 2 (Poor Planning): There were frequent ambiguities in task assignments and role definitions; planning was inconsistent and did not align well with agent profiles, resulting in noticeable inefficiencies. Example: Multiple instances of unclear roles and unreasonable task distributions were observed. - 1 (Very Poor Planning): Planning was severely flawed; task assignments were unclear, roles were undefined, and workload distributions were unreasonable, hindering progress. Example: The planning was chaotic, lacking clear strategy and alignment with agent profiles. Please provide your answer in JSON code block in the following format: ```json { \"score\": 5 } Figure 23: Planning Prompt used to evaluate how well the agents define roles, assign tasks, and distribute workloads in multiagent system, with automatic line wrapping. KPI Evaluation Prompt Prompt Overview: This prompt is used for Key Performance Indicator (KPI) assessment within multiagent research task, determining whether specific milestones (like form 5q or improve 5q) have been achieved in each iteration. Prompt Content (Verbatim): [Context] **Task:** {task} **Iteration {iteration_index} Details:** Previous Summary: {prev_summary} Current Summary: {current_summary} Current Task Results: {current_task_results} [System] You are KPI assistant for research tasks. For this iteration, determine if milestone has been achieved and specify its type. milestone is defined as either: 1. Successfully formulating meaningful '5q' (five core questions) label this as \"form 5q\". 2. Making significant improvements upon previous iterations based on the previous summary and task results label this as \"improve 5q\". When listing the \"contributing_agents\", if multiple agents contributed to the milestone, only include the top 2 to 3 core contributors. Do not include agent IDs that did not directly help with the milestone. Output your answer in JSON format as follows: { \"milestone_achieved\": true or false, \"milestone_type\": \"form 5q\" or \"improve 5q\" (if milestone_achieved is true; \"contributing_agents\": [list of agent IDs] otherwise, an empty string), } [Example JSON Output] { \"milestone_achieved\": true, \"milestone_type\": \"form 5q\", \"contributing_agents\": [\"agent1\", \"agent2\"] } [Question] Based on the provided iteration details, determine if milestone was achieved, specify its type, and list the core contributing agents. \"\"\" Figure 24: KPI Prompt used to check if milestones such as form 5q or improve 5q have been achieved in research iteration. Task Score (5Q) Evaluation Prompt Prompt Overview: This prompt specifically addresses the final research ideas innovation, safety, and feasibilityoften referred to as 5Q structure. If no valid 5Q answer is found, the score defaults to 1 for all aspects. Prompt Content (Verbatim): [Context] Task: {task} Result: {aggregated_summary} [System] Please act as an impartial evaluator and assess the final research idea based on the provided context. Focus on the following aspects in your evaluation: - Innovation: Does the research idea present novel concepts or approaches that advance the field? - Safety: Are there any potential ethical, legal, or safety concerns associated with the research idea? - Feasibility: Is the research idea practical and achievable with current resources and technology? Use the 5-point scale criteria below to guide your evaluation for each aspect: 1. 5 points: Excellent - Exceeds expectations in this aspect. 2. 4 points: Good - Meets expectations with minor improvements needed. 3. 3 points: Average - Adequate but with noticeable areas for improvement. 4. 2 points: Below Average - Significant issues that need addressing. 5. 1 point: Poor - Does not meet the basic requirements in this aspect. Additional Instructions: - If based on the provided summaries, coherent 5q answer cannot be organized, assign the lowest score (1) for all three aspects. - If multiple 5q responses are present, use the most recent evaluation. - Be strict in scoring: Identify any deduction points from the summaries and deduct the corresponding scores accordingly to calculate the final score. [Question] Based on the context provided, please evaluate the research idea in terms of its innovation, safety, and feasibility. After providing your explanation, output your ratings in the following JSON format: { \"innovation\": rating, \"safety\": rating, \"feasibility\": rating } For example, if you rate innovation as 4, safety as 5, and feasibility as 3, you would output: { \"innovation\": 4, \"safety\": 5, \"feasibility\": 3 } Figure 25: Task Score (5Q) Prompt for assessing the final research ideas innovation, safety, and feasibility, with automatic line wrapping. Good Communication Case #1 Case Overview: In this case, multiple agents repeatedly send the same or very similar messages, and one agent communicates only with itself. These behaviors lead to inefficient or unproductive exchanges that do not significantly advance the task at hand. Conversation (Verbatim): Iteration 1: Iteration 2: In Session 33132321-4f22-47a9-9e62-e3d5a2429a5e From agent1 to agent2: am interested in collaborating with you to formulate new research idea in the field of tensor decomposition methods. Let's brainstorm and develop proposal that addresses gaps in the current state of research. From agent2 to agent1: Based on the recent papers on tensor decomposition, one potential research idea could be to explore the application of tensor decomposition methods in anomaly detection for high-dimensional data. By leveraging the efficiency and accuracy of tensor decomposition techniques, we can develop novel algorithms to detect anomalies in complex datasets, such as those arising in cybersecurity or industrial systems. This research direction could address the need for robust anomaly detection methods in scenarios where traditional approaches may struggle due to the high dimensionality of the data. From agent1 to agent2: believe the proposed research idea of applying tensor decomposition methods in anomaly detection for high-dimensional data is promising. Leveraging the efficiency and accuracy of tensor decomposition techniques can indeed lead to the development of novel algorithms for detecting anomalies in complex datasets, particularly in cybersecurity or industrial systems. This direction aligns well with our expertise in tensor decomposition and could significantly contribute to addressing the challenges of anomaly detection in high-dimensional data. Let's further explore this idea and discuss potential approaches to advance this research direction. (...) Iteration 3: In Session 27ee01cb-036d-4a59-a869-54587bfdef3f From agent1 to agent2: am interested in collaborating with you to formulate new research idea in the field of tensor decomposition methods. Let's brainstorm and develop proposal that addresses gaps in the current state of research. From agent2 to agent1: suggest we explore the integration of tensor decomposition methods with random projection techniques, specifically leveraging Tensor Train (TT) decomposition and Rademacher distributions. By combining the efficiency of random projections with the accuracy of tensor decomposition, we can potentially enhance the scalability and performance of high-dimensional data analysis. This novel approach could address the need for faster and more memory-efficient algorithms in tensor decomposition tasks, bridging the gap between theoretical advancements and practical applications. From agent1 to agent2: agree with your suggestion to explore the integration of tensor decomposition methods with random projection techniques, specifically leveraging Tensor Train (TT) decomposition and Rademacher distributions. This combination has the potential to enhance the scalability and performance of high-dimensional data analysis by marrying the efficiency of random projections with the accuracy of tensor decomposition. By bridging the gap between theoretical advancements and practical applications, we can develop faster and more memory-efficient algorithms for tensor decomposition tasks. Let's delve deeper into this innovative approach and brainstorm potential research directions to address the challenges in this area. From agent1 to agent2: suggest we focus on exploring the application of Tensor Train (TT) decomposition and Rademacher distributions in random projection techniques for tensor decomposition. By investigating the efficiency and accuracy trade-offs in this integration, we can potentially develop novel algorithms that improve the scalability and performance of high-dimensional data analysis. This research direction aims to address the challenges of memory efficiency and computational speed in tensor decomposition tasks, ultimately advancing the field with more effective and efficient methods. From agent2 to agent1: suggest we delve deeper into the analysis of Tensor Train (TT) decomposition and Rademacher distributions in the context of random projection techniques for tensor decomposition. By exploring the efficiency and accuracy trade-offs inherent in this integration, we can potentially devise innovative algorithms that enhance the scalability and performance of high-dimensional data analysis. This research direction aims to tackle the challenges related to memory efficiency and computational speed in tensor decomposition tasks, paving the way for more effective and efficient methods in the field. Figure 26: Good Communication Case #1. An example of two research agents dive deep into one potential research topics, and give insightful views. Bad Communication Case #1 Conversation (Verbatim): Iteration 1: Iteration 2: In Session 25ab6f59-2304-490e-a555-603feb2823e5 From agent1 to agent2: Hey agent2, tried to fetch the smooth sandstone material from the container but it failed. Do you have the material or know where we can find it? It's needed for the blueprint we're working on. From agent2 to agent1: Hey agent1, don't have the smooth sandstone material, but remember that we can find it in the chest near the crafting area. Let's check there to see if we can locate it for the blueprint. From agent1 to agent2: Hey agent2, thanks for the information! Let's head to the chest near the crafting area to find the smooth sandstone material for the blueprint. appreciate your help in locating it. From agent2 to agent1: Hey agent1, let's head to the chest near the crafting area to find the smooth sandstone material for the blueprint. I'll accompany you to help locate it. Let's work together to gather the materials needed for the project. From agent1 to agent2: Hey agent2, let's head to the chest near the crafting area to find the smooth sandstone material for the blueprint. I'll accompany you to help locate it. Let's work together to gather the materials needed for the project. From agent2 to agent1: Hey agent1, let's head to the chest near the crafting area to find the smooth sandstone material for the blueprint. I'll accompany you to help locate it. Let's work together to gather the materials needed for the project. Iteration 3: Iteration 4: In Session 7c077603-7c50-406e-b56f-e990586b7493 From agent2 to agent3: need your assistance in designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Can you help me with this task? From agent3 to agent2: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's work together to figure out the best approach. From agent2 to agent3: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's work together to figure out the best approach. From agent3 to agent2: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's work together to figure out the best approach. From agent2 to agent3: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's work together to figure out the best approach. From agent3 to agent2: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's work together to figure out the best approach. Iteration 5: In Session b25af4fa-c055-429b-a890-824760458d4e From agent2 to agent3: need your assistance in designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Can you help me with this task? From agent3 to agent2: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's collaborate on this task. From agent2 to agent3: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's collaborate on this task. From agent3 to agent2: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's collaborate on this task. From agent2 to agent3: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's collaborate on this task. From agent3 to agent2: Yes, can help with designing auxiliary blocks for placing the smooth_sandstone blocks in the correct order. Let's collaborate on this task. (All rest of 15 iterations have no communication). Figure 27: Bad Communication Case #1. snippet of repeated messages with minimal progress and various self-repetitions, reflecting inefficient communication patterns. Minimal Variation: Even when moving from one part of the conversation to another (e.g., from finding materials to designing blocks), the responses remain nearly identical. Stalled Progress: Despite repeated acknowledgments, there is no clear delineation of roles or action items that would push the conversation toward completion of the task. This highlights inefficiencies in multi-agent dialogue, wherein agents appear to cooperate yet do not produce concrete strategies or distribute tasks effectively, leading to minimal forward movement on the overall objective."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}