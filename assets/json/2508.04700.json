{
    "paper_title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
    "authors": [
        "Zeyi Sun",
        "Ziyu Liu",
        "Yuhang Zang",
        "Yuhang Cao",
        "Xiaoyi Dong",
        "Tong Wu",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 0 7 4 0 . 8 0 5 2 : r SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience Zeyi Sun1,2 Ziyu Liu1,2 Yuhang Zang2 Yuhang Cao2 Jiaqi Wang2 Xiaoyi Dong2,3 Tong Wu3 Dahua Lin2,3 1Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory 3The Chinese University of Hong Kong szy2023@sjtu.edu.cn, wangjiaqi@pjlab.org.cn https://github.com/SunzeY/SEAgent"
        },
        {
            "title": "Abstract",
            "content": "Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design World State Model for step-wise trajectory assessment, along with Curriculum Generator that generates increasingly diverse and challenging tasks. The agents policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over competitive open-source CUA, i.e., UI-TARS."
        },
        {
            "title": "Introduction",
            "content": "A new generation of agents will acquire superhuman capabilities by learning predominantly from experience. [55] David Silver, Richard S. Sutton With the rapid development of large vision-language models (LVLMs) [60, 16, 7, 63, 42, 5, 59], computer use agents (CUAs) [3, 43, 48, 29, 66] have not only emerged but also demonstrated increasing practical utility. By leveraging the powerful perception and reasoning capabilities of LVLMs, these agents can interpret screenshots as visual inputs and operate computers via keyboard and mouse actions. Despite their promising capabilities, current CUAs [47, 46, 12, 19, 6, 34] primarily depend on costly human-curated datasets [12, 9, 66, 24, 28], which are typically derived preprint. Figure 1: SEAgent enables computer use agents self-evolving in novel environments by autonomously exploring and learning from their own experiences without human intervention. The specialist-to-generalist training strategy further enhances the development of strong generalist agent. from demonstrations [34, 77, 18, 51, 74] or video tutorials in the wild [69]. However, new software continuously emerges and existing software may regularly be updated, often in the absence of annotated human data. It is both necessary and timely to enter an era that emphasizes learning from experience [55] in CUA domain. In this paper, we aim to enable CUAs to autonomously explore unfamiliar software environments and evolve into experts without relying on human supervision. To address this challenge, we propose SEAgent, an agentic self-evolving framework in which Computer Use Agents (CUAs) are exposed to previously unfamiliar software environments and engage in autonomous exploration and experiential learning, as illustrated in Fig. 1. Enabling such selfevolution requires addressing two key challenges: (1) generating executable tasks within unfamiliar software environments, and (2) accurately assessing task success and precisely identifying the step at which failure occurs. To this end, we introduce World State Model for environmental state captioning and step-wise trajectory assessment, together with Curriculum Generator powered by continuously updated software guidebook memory to generate increasingly diverse and challenging tasks, thereby establishing curriculum learning paradigm. The agents policy is optimized through experiential learning from both failures and successes, combining adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Given the critical role of reward accuracy, we conduct extensive evaluations and observe that existing reward models of computer use tasks fall short in terms of judgment precision and reward density. Leveraging the enhanced long-context processing capabilities of advanced LVLMs, we input the agents full trajectory of states into the reward model and fine-tune reward model, World State Model, using Qwen2.5-VL [7], substantially narrowing the gap with commercial models such as GPT-4o [42] with +7.5% improvement in precision compared to baseline model in evaluating CUAs trajectories on AgentRewardBench [35], enable World State Model to provide high quality step level reward signals in self-evolving agentic system. Moreover, SEAgent enables agents to evolve into either single-software specialists or multi-software generalists. To overcome the limitation that directly training generalist underperforms compared to specialists, inspired by [76], we introduce novel specialist-to-generalist training strategy, which even surpasses the performance of individual specialists on their respective software applications. We perform extensive experiments of SEAgent built on UI-TARS [48] and evaluated on five professional software applications from OSWorld [67]. SEAgent with the specialist-to-generalist strategy significantly improves the UI-TARS [48] from 11.3% to 34.5%. Furthermore, SEAgent with the 2 specialist-to-generalist strategy (34.5%) outperforms both specialist RL (32.2%) and generalist RL (30.6%) by substantial margin, demonstrating the effectiveness of the specialist-to-generalist paradigm. In general, SEAgent offers promising approach for developing more powerful and versatile computer-use agents without human involvement."
        },
        {
            "title": "2 Related Work",
            "content": "Agent for Computer Use. With the recent advances in LLMs and LVLMs [60, 16, 30, 7, 63], which enable human-level perception and reasoning capabilities, the development of agents for computer use has garnered significant attention [22, 20, 11, 41, 29]. These agents either rely solely on structured text inputs [47, 40, 46, 26, 36] or, in more human-like manner, use multi-modal inputs such as screenshots combined with textual conditions [21, 29, 66, 43]. Although they have been extensively studied and show strong performance on in-domain benchmarks [34, 78, 31, 27, 11], computer use agents still lag significantly behind human-level performance in simulated environments [67, 50, 25, 79]. This gap highlights the challenges posed by the multi-dimensional demands on LVLMs, including grounding, decision-making, and reasoning. Some approaches address this by decomposing tasks into specialized expert models [15, 61] and employing agent collaboration [1, 2, 32, 73] through prompt engineering [70, 19, 75, 62, 65]. However, improvements from these training-free methods remain limited without fine-tuning. In this work, we explore the next phase of computer use agents, where pretrained agent is fine-tuned to learn from its own experience, enabling self-evolution on novel, specialized software without human annotations. Reinforcement Learning for LLMs/LVLMs. Previous scalable training efforts for LLMs and LVLMs [60, 16, 30, 7, 63, 68, 58, 57, 13] have primarily relied on supervised fine-tuning (SFT) [30, 64]. Analogous to imitation learning or behavior cloning in reinforcement learning (RL), SFT trains models to produce desired outputs based on labeled data, making it heavily dependent on high-quality human-curated procedures. Recently, DeepSeek-R1 [17] demonstrated strong reasoning capabilities via Group Relative Policy Optimization (GRPO) [53] using verifiable rewards. Earlier works [44, 81, 49] have also employed RL for single-turn optimization from human feedback. However, in agentic scenarios such as computer usagewhere feedback is sparse with reward signals often results from multi-step interactionsit becomes crucial to introduce stable, step-level reward signals. Prior RL approaches for agents [6, 47, 80, 72, 8] have fine-tuned their own critic models for advantage estimation [52], either using output reward models (ORMs) trained on labeled data or adopting Direct Preference Optimization (DPO) [49] based on interaction data [46, 48]. In this work, we investigate various strategies for constructing high-performing reward models for CUAs and find that full-process-based analysis yields more accurate evaluations with fine-grained reward signals compared to training dedicated critic models for advantage estimation as done in [6, 47] or with filtered behavior cloning [45, 10]."
        },
        {
            "title": "3 Methods",
            "content": "Problem Formulation. The objective of SEAgent is to establish training pipeline enabling the Computer Use Agent (CUA) to autonomously explore its environment (Sec. 3.1) and progressively self-evolve on novel software applications via reinforcement learning from experience (Sec. 3.2). Specifically, the SEAgent pipeline comprises three primary components: an Actor Model π performing exploratory actions to accomplish these tasks, and World State Model Mstate describing the current environment state and evaluating the success or failure of executed actions, and Curriculum Generator Mtask that continuously proposes more diverse and challenging exploration tasks: (1) Actor Model π: The policy π(ast, I) defines the probability of taking action at time step t, conditioned on the current state st and the overall task instruction I. (2) World State Model Mstate: This component is fine-tuned Large Vision-Language Model (LVLM) responsible for providing detailed descriptions of environment states. It also evaluates each step of the trajectory executed by the Actor Model π, producing trajectory judgement which indicates whether the task has been successfully completed. Joint training with state change captioning of the software GUI has been shown to enhance judgment accuracy, as shown in Table 1. 3 Figure 2: SEAgent autonomous exploration and experiential learning pipeline. Guided by tasks generated by the Curriculum Generator, the Actor Model is updated according to step-level rewards from the World State Model through verifiable reward functions tailored for different action types. (3) Curriculum Generator Mtask: This component utilizes powerful Large Language Model (LLM) to automatically generate novel exploration tasks. It also maintains and updates software guidebook based on the state change captioning and the trajectory judgement provided by Mstate during interactions. The gradually enriched guidebook enables Mtask to progressively generate increasingly diverse and challenging tasks in curriculum learning fashion. SEAgent can be applied to enable the self-evolution of computer-use agent, either as specialist for single software or as generalist across multiple software. However, we observe that direct training for generalist agents is suboptimal. We introduce specialist-to-generalist training strategy, which achieves even better overall performance than training multiple generalist agents, as discussed in Sec. 3.3. 3.1 Autonomous Exploration with Self-evolving Curriculum Autonomous exploration is essential for enabling the Computer Use Agent (CUA) to develop proficiency in novel software applications that are previously unseen or poorly understood. This process involves addressing two key challenges: (1) generating executable tasks within unfamiliar software environments, and (2) evaluating task completion success and pinpointing the specific step at which failure occurs. To tackle these challenges, we introduce two novel components: the World State Model Mstate and the Curriculum Generator Mtask. These components jointly support self-evolving curriculum paradigm, which facilitates the autonomous generation of increasingly diverse and challenging tasks. The self-evolving curriculum paradigm pipeline is structured into sequential phases. Before the first phase, set of initial tasks targeting basic GUI operations is generated (details provided in Sup. C.1). In each phase, these tasks are executed and step-wise evaluated. The resulting judgments and descriptions of the exploration trajectories are fed into the Curriculum Generator Mtask, which updates self-maintained software guidebook . Leveraging this updated guidebook and the current capabilities of the CUA, the generator then produces more diverse and challenging tasks for subsequent phases. The following outlines each step of the process in detail: 0 , (2) (1) Task initiation: The initial state of the unfamiliar software is provided, typically in the form of screenshots of its basic GUI interface. The World State Model Mstate performs dense captioning of the GUI elements, including button detection and OCR-based recognition. These detailed captions are passed to the Curriculum Generator Mtask, which generates an initial set of task instructions I0 = {I (1) 0 , } along with an initial software guidebook U0 for the software. (2) World state judgment: In the p-th phase of Auto Exploration, the Actor Model πp executes tasks based on the instructions in Ip. Each execution is evaluated by the World State Model Mstate, which provides feedback Jp = {J (1) , } for each step within the operation trajectory. In addition, it generates detailed description of GUI state changes based on captured screenshots, denoted as Cp. (3) Task self-evolving: Based on the outcomes Jp and Cp, the Curriculum Generator Mtask produces more challenging task set Ip+1 and expands the agents knowledge boundary by updating the software guidebook to Up+1. The detailed prompting process is illustrated in Fig. 8. This iterative , (2) 4 update can be formalized as: Up+1, Ip+1 = Mtask(Up, Ip, Jp, Cp) (1) Here, Up+1 serves as more comprehensive software guidebook memory, while Ip+1 represents more challenging task set tailored to the current capabilities of the Actor Model πp. Examples of Ip are provided in Fig. 4, where the Actor Model π demonstrates curriculum learning by handling increasingly complex tasks across different phases p. Illustrations of Up across various software applications are provided in Sup. J. Comparison with previous methods [39, 38] on task generation are detailed in Sup.C. (4) Autonomous RL Training: Through iterative reinforcement learning, the Actor Model πp is updated based on its execution of the instruction set Ip, guided by evaluation feedback Jp and set of action-specific verifiable functions Rverifer. The resulting policy πp+1 is then used as the actor in the subsequent phase. Further details are provided in Sec. 3.2. 3.2 Reinforcement Learning from Experience The World State Model Mstate provides step-level reward signals for reinforcement learning. Unlike previous reward models for CUA [47, 6, 46, 45, 35], our Mstate model takes the entire trajectory of states and actions, = {(s0, a0), (s1, a1), . . .}, as input. It classifies each action as either aF or aT , where aF indicates an incorrect action leading to failure or redundant loops, and aT represents correct action that contributes to successful progression without redundancy. The curated prompt used for judgment is depicted in Fig. 7. For historical states that result in aT , we encourage CUA to reinforce these actions through verifiable rewards defined by set of functions Rverifer = {rdist}. Conversely, for states leading to aF , we penalize them using negative KL divergence with adversarial imitation. Adversarial Imitation for Failure Action Punishment. To explicitly encourage the policy to diverge from failure-inducing behaviors, we employ contrastive log-ratio loss based on reference failure action aF . This objective encourages the policy to sample actions that minimize alignment with the failure action aF : LAI(πθ) = Eν log (cid:20) (cid:21) πθ(a s, I) πref(aF s, I) (2) This formulation serves as an adversarial imitation signal. By maximizing divergence from this distribution, the agent is trained to explore alternative action distributions that deviate from those leading to failure, particularly in complex GUI interaction scenarios. Notably, this loss shares similar form with DPO [49] but only the negative part. Verifiable Rewards for Correct Action Encouragement. To more effectively guide the policy toward correct actions aT , we adopt Reinforcement Learning with Verifiable Rewards (RLVR) [17, 53], which has recently shown success in enhancing language models on tasks with objectively verifiable answers, such as mathematics [53], and more recently, counting and grounding in the vision-language domain [33, 54, 37]. After labeling the correct step (s, aT ) using the World State Model, we apply Group Relative Policy Optimization (GRPO), computing the relative advantage of each response based on its reward: A(i) = r(i) mean({r(j)}G std({r(j)}G j=1) , j=1) As we design distinct reward signals for different action types, we define the reward function between predicted action and the ground-truth action aT as: = 1, , G. (3) r(i) = r(a(i), aT ) = (cid:16) (cid:17) type(a(i)) = type(aT ) + rdist(a(i), aT ), (4) where I() is the indicator function that returns 1 if the predicted action and ground-truth action are of the same type, and 0 otherwise. The distance-based reward term rdist(a(i), aT ) is defined according to the specific action type: for click actions, it is computed based on the normalized L1 distance between the clicked coordinates; for drag and select actions, it is computed using the Intersection over Union (IoU) between the predicted and ground-truth bounding boxes; and for type actions, it 5 is determined by the character-level BLEU score between the predicted and ground-truth text. All rdist rewards are normalized to the range [0, 1] to ensure consistency across different action types. comprehensive list of rdist(a(i), aT ) definitions for various action types is provided in Tab. 8. The final loss of GRPO is directly adopted from [53]: LGRPO(πθ) = (s,I)D,{a(i)}G i=1πref(s,I) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 a(i) a(i) (cid:88) (cid:110) t= min (cid:16) (θ)A(i), clip(r(i) r(i) (θ), 1 ϵ, 1 + ϵ)A(i)(cid:17) β D(i,t) KL (πθπref) (5) (cid:35) (cid:111) , where ri,t(θ) = πθ(a(i)s, I) πθref (a(i)s, I) and Di,t KL(πθ, πref) = πref(a(i)s, I) πθ(a(i)s, I) 1 log πref(a(i)s, I) πθ(a(i)s, I) . Similar to [53, 17], advantage is weighted on the whole reasoning token logits to encourage free form thinking for performing action and planning. The final training loss is defined as weighted combination of positive and negative action samples, i.e., correct actions aT and incorrect actions aF : L(π(θ)) = LGRPO + γLAI. We set γ = 0.2 during training, and the rationale for this choice is discussed in the ablation study presented in Sup. F. This strategy is shown to be more effective in Sec. 4.2 compared to Generalized Advantage Estimation (GAE) [52]-based RL methods [47, 6], as the more powerful reward model Mstate provides accurate step-level reward signals by leveraging the entire episode trajectory from global perspective. 3.3 From Specialist to Generalist. Achieving generalist agent capable of operating across multiple software platforms is an ambitious and valuable goal. We first attempted to train such generalist directly using the proposed SEAgent framework across all software environments. However, this approach led to suboptimal performance compared to specialized agents, as the actor struggled to learn effectively in the multi-software environment. We thus introduce specialist to generalist strategy, as illustrated in Fig. 1. Specifically, we first train software-specialized agents via SEAgent on individual environments, allowing each to master specific application. These specialists are then distilled into single generalist model through supervised fine-tuning (SFT) on synthesized successful trajectories. Finally, the generalist is refined via SEAgent on multiple software. This generalist, now equipped with better reasoning, planning abilities, and software-specific commonsense, achieves significantly improved performance, outperforming both the SEAgent via direct general RL and the performance combination of multi-specialists as in Table 2."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Benchmark of Reward Model for computer use agent. Providing CUA agents with reliable reward signals is crucial for enabling self-evolution in agentic systems, consisting of an actor (CUA) and judge model, especially when interacting with unfamiliar software environments. Recent work, AgentRewardBench [35], proposes to evaluate the precision of reward models by assessing the accuracy of judge predictions on web-based tasks using trajectories from diverse agents. Building upon AgentRewardBench [35], we further extend the evaluation beyond web tasks to broader set of PC software environments. Specifically, we evaluate on all 339 feasible tasks from OSWorld [67], using rule-based criteria for determining success or failure. Trajectories are sampled from UI-TARS [48] and Gemini-2.5-Pro [14], and rule-based evaluation is used as ground-truth supervision. We then compute the confusion matrix by comparing the predictions of different reward models against these labels. The judge strategy in AgentRewardBench [35] relies solely on the final state and the associated action history. However, it is more natural and reliable for judge model to consider the entire trajectory when assessing task success, rather than focusing only on the final state. For example, consider 6 Table 1: Precision and Negative Predictive Value (NPV) on AgentReardBench [35] and OSWorld [67] with last screenshot only (LS) or entire process screenshots (ES) as input. World State Model closes the gap with commercial model supporting full process high resolution screenshots as input. The co-training with screenshot change description (CD) improves judgment precision. Prof/Office AgentRewardBench OS-World-Full Precision NPV Precision NPV Precision NPV Model Input GPT-4o [23] Qwen2.5-VL-72B [7] Qwen2.5-VL-7B [7] World State Model (w/o CD) World State Model (w/ CD) LS ES LS ES LS ES ES ES 68.1 72.1 64.5 26.2 64.1 25.4 69.1 71.6 92.3 92.2 94.2 83. 90.3 83.8 88.5 91.2 46.3 74.6 41.5 26.8 37.3 20.0 71.1 73. 88.2 95.2 86.9 83.0 85.2 81.7 88.4 90.5 40.5 70.4 31.7 25. 31.8 23.5 65.0 69.3 81.0 85.3 78.7 76.6 79.0 76.0 81.1 82. the task of booking flight to London. final state message such as \"Your flight ticket has been successfully booked.\" does not confirm whether the correct date and time were selected, which can lead to compromised judgment accuracy. However, we observe that current open-sourced LVLMs do not perform well under this more holistic evaluation strategy. As shown in Fig. 3, feeding additional historical screenshots into Qwen2.5-VL [7] significantly degrades its Average Precision (AP), diverging notably from GPT-4o [23] on the same curated prompt detailed in Fig.6. We attribute this performance drop to the insufficient pretraining of Qwen2.5-VL on long sequences of high-resolution screenshots, which likely pushes it toward the limits of its 32K context length. To address this issue, we propose distilled model based on Qwen2.5-VL-7B, referred to as World State Model, which conducts step-by-step screenshot analysis to produce final judgments. The training process for World State Model is detailed in Sup. A.2, using dataset of 0.86K GPT-4o [23] generated evaluations on trajectories with dense GUI change descriptions, exclusively from Chrome within the OSWorld [67] environment. Despite being trained solely on Chrome data, World State Model exhibits strong generalization to both other professional software in OSWorld and the external AgentRewardBench [35] benchmark. This demonstrates that the model learns transferable judgment patterns rather than overfitting to the specifics of single application, thanks to the diversity and quality of step-level annotations in the training data. We evaluate World State Model and our full-process screenshot-conditioned strategy on AgentRewardBench [35], as well as on agent trajectories from OSWorld [67]. As shown in Tab. 1 and further analyzed in Fig. 3, World State Model achieves state-of-the-art performance among open-sourced models, significantly narrowing the gap with GPT-4o [23]. More importantly, it exhibits similar performance trend to GPT-4o when conditioned on historical screenshots. Despite being trained on relatively Figure 3: The Average Precision on AgentResmall dataset, World State Model is explicitly wardBench [35], where GUI-Judge exhibits an encouraged to capture the sequential dependenimprovement in AP as the number of input middle cies among historical screenshots and to perform states increases, showing similar trend to that of step-by-step reasoning for final judgment. Servthe closed sourced GPT-4o [23] when compared ing as our foundation reward model, World State with its base model. Model provides reliable, step-level reward sigIn line with our agentic system designwhich nals that support downstream policy learning. emphasizes the evolution of the actor agent with full open-sourced modelswe intentionally avoid relying on GPT-4o [23] API calls for judgment during training and inference (also due to inefficiency). More details of World State Model is supplied in Sup.A. 7 Table 2: Success Rate (SR) on OSWorld [67]. SEAgent demonstrates strong performance after reinforcement learning from experience. In addition to evolving on separate software, new General Model achieves better performance after another iteration of SEAgent. *Indicates specialist agents trained separately for each software with ensembled results. All results are averaged over three runs. Model Human Performance GPT-4o [23] GPT-4V [42] Gemini-Pro-1.5 [59] Claude3.7 Sonnet [4] Gemini-Pro-2.5 [14] UI-TARS-7B-DPO [34] UI-TARS-72B-DPO [34] DigiRL [6] (Specialized RL)* WebRL [47] (Specialized RL)* SEAgent (Specialized RL)* DigiRL [6] (General RL) WebRL [47] (General RL) SEAgent (General RL) SEAgent (General SFT) SEAgent (Specialist-to-Generalist) VScode GIMP Impress VLC Writer Overall 73.9 4.35 0.00 0.00 18.8 21.7 13.0 18.8 21.7 27.5 37.7 21.7 20.3 36.2 30.4 40. 73.1 3.85 7.69 11.5 24.4 26.9 23.1 25.6 32.1 29.5 38.5 35.9 32.5 39.7 37.2 42. 80.9 6.77 2.52 13.2 10.6 9.92 4.26 6.38 12.8 10.6 22.0 12.1 9.93 19.9 18.4 22. 70.6 16.1 18.3 6.53 27.5 25.5 11.8 15.7 23.5 25.5 33.3 19.6 21.6 31.4 31.9 35. 73.9 4.35 4.35 8.71 17.4 24.6 4.35 8.70 18.8 15.9 29.0 15.9 14.5 26.1 20.3 31. 74.5 7.08 6.59 7.99 19.7 21.7 11.3 15.0 21.8 21.8 32.2 21.0 19.6 30.6 27.9 34. 4.2 Self evolution of GUI Agents Figure 4: Self-evolved task instructions and success rate (SR) curves across different software. Tasks are progressively upgraded by the Curriculum Generator without human intervention, based on the evolving capabilities of the Actor Model at different training phases. Models Before Self-Evolution. Our self-evolving system is initialized with three locally deployed models: UI-TARS-7B-DPO [48] as the Actor Model, World State Model as the step-level reward model, and Qwen2.5-72B [71] as the Curriculum Generator for task evolution with software guidebook memory. We conduct experiments on five professional and office-related software applications from OSWorld [67]. As shown in Tab. 2, the initial actor agent demonstrates limited performance on these software environments, achieving an average success rate of 11.3% only. Evolution Process Details. At beginning, we provide World State Model with the initial GUI state of the novel software. The Curriculum Generator then generates the first software guidebook and set of basic tasks (illustrated in Fig.5). This yields an initial instruction set I0, averaging 150.2 instructions, which are executed by the Actor Model. The resulting trajectories are evaluated by World State Model and parsed into an average of 1361.5 multi-turn conversation pairs (detailed statistics are in Sup.H). We then perform reinforcement fine-tuning (RFT) following the methodology described in Sec. 3.2. Training is conducted for 1k iterations on 8 NVIDIA A100 80GB GPUs, with = 8, batch size of 16, and learning rate of 2 105, scheduled via cosine decay. This evolution process is repeated iteratively for three phases using the same training configuration. Specialist Evaluation. For fair comparison with previous reinforcement learning methods [6, 47], we adapt their training strategies to the UI-TARS [48] model. Specifically, we initialize the actor agent from UI-TARS-7B-DPO and, instead of providing step-level reward signals, We evaluate its executed trajectories with binary success or failure outcomes using World State Model. separate critic model is also initialized from UI-TARS-7B-DPO, with additional random initialized MLP layers taking the LLMs hidden states as input to regress value predictions. This critic is trained to perform advantage estimation based on Generalized Advantage Estimation (GAE) [52]. The loss functions follow the same configurations as in [6, 47]. Both the critic and the actor agent are trained iteratively using the same phased reinforcement fine-tuning (RFT) process, where the Curriculum Generator continually generates new curriculum-style tasks. As shown in Fig. 4 and Tab. 2, we train separate actor agents for five different software applications. Our approach, denoted as SEAgent (Specialist), achieves strong performance compared to previous reinforcement learning methods such as DigiRL [6] and WebRL [47]. We attribute this improvement to the use of World State Model, which provides fine-grained, step-level reward signals derived from comprehensive understanding of the full history of states and actions. This contrasts with previous approaches that rely on separate critic modelstypically initialized from the actor itselfto estimate advantages from sparse, final success/failure signals. Furthermore, the curriculum of task instructions generated by the Curriculum Generator, as illustrated in Fig. 4, validates the effectiveness of our autonomous learning framework. These tasks progress from simple to complex based on the actors evolving capabilities, enabling it to gradually specialize in each target software environment. Based on the observed evolution curves, we set the number of training phases to three, as performance gains saturate beyond that point. From Specialist to Generalist. After training five strong software specialists, we pursue generalization using the methodology described in Sec. 3.3. Specifically, we collect task instructions generated during each specialists training phase and use them to prompt the respective specialists for execution. total of 3.5K successful trajectories, along with their corresponding reasoning traces, are distilled into new base model (UI-TARS-7B [48]) via supervised fine-tuning (SFT). This distilled model is then further optimized through reinforcement learning (RL) across all five software environments. As shown in Tab. 2, the resulting generalist model surpasses the performance of the individual specialist ensemble, demonstrating the effectiveness of specialization-first strategy for achieving generalization. By learning from broad range of software tasks, the generalist improves its reasoning and decision-making capabilities, acquiring transferable commonsense knowledge across domains. Table 3: Ablation of different configurations and their corresponding VScode success rates on OSWorld [67]. Using World State Model as the reward model yields significant performance gains. We further compare different training strategies including supervised fine-tuning (behavior cloning), GRPO, and Adversarial Imitation (AI). Ablation Study of Specialist Training. In Tab. 3, we present an ablation study on the effectiveness of various components in our training pipeline, using the success rate on VSCode from OSWorld [67] as the evaluation metric. First, we ablate the use of the World State Model for reward signal generation. Its high precision in judging the success or failure of the actor agents actionscompared to using base modelis shown to be essential for effective self-evolution. In addition to reward quality, reinforcement fine-tuning (RFT) also proves critical. Compared to direct supervised fine-tuning (behavior cloning), RFT encourages more diverse and exploratory reasoning patterns under verifiable rewards, enabling more generalized task planning. Finally, incorporating adversarial imitation to penalize critical failure-inducing actions allows the CUA to learn from its mistakes, yielding additional performance gains. This highlights the importance of learning not only from successful behaviors but also from failure signals. SFT (BC) GRPO AI VScode SR Qwen2.5VL-72B World State Model 13.0 10.1 11.6 23.2 30.4 34.8 37."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce SEAgent, an autonomous Computer Use Agent (CUA) exploration system that learns from its own experience on specific software. Powered by robust World State Model that provides step-level reward signals, and carefully designed reinforcement learning framework that 9 encourages free-form reasoning through trial and error, the CUA is able to evolve into specialist for individual software platforms. Furthermore, specialist-to-generalist training strategy enables the development of strong generalist agent capable of operating across multiple software environments. Given that computer software constitutes highly regularized virtual world, we believe this work can inspire future research on agentic systems in both gaming and real world embodied environments. Limitations and future work. While promising, our work still has several unresolved limitations. Firstly, our self evolving agent system is bounded by GUI-Judge to provide reliable reward signal instead of real signal from environment. As its still challenging to learning from sparse reward signal in complex environment. Secondly, though we tested on relatively complex and novel software like libreoffice-tools and GIMP. The task is still relatively simple as it only takes human expert less than 20 step to accomplish. How to adapt the system to achieve hours-long workflow in even more challenging software used by real human expert are thus interesting future directions."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. [2] Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. [3] Anthropic. Claude computer use. 2024. [4] Anthropic. Claude 3.7 sonnet system card. https://assets.anthropic.com/m/ 785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf, 2025. [5] Anthropic. Claudes extended thinking. 2025. [6] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [8] Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and PierreYves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 36763713. PMLR, 2023. [9] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [10] Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Bestaction imitation learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:1835318363, 2020. [11] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [12] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [13] Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Mm-ifengine: Towards multimodal instruction following. arXiv preprint arXiv:2504.07957, 2025. [14] Google DeepMind. Gemini 2.5 Pro Preview (03-25). https://deepmind.google/ technologies/gemini, 2025. [15] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 11 [18] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [19] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [20] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [21] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for GUI agents. CoRR, abs/2312.08914, 2023. [22] Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use, 2024. [23] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pages 161178. Springer, 2024. [25] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [26] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: large language modelbased web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 52955306, 2024. [27] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. [28] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pages arXiv2406, 2024. [29] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [31] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024. [32] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960, 2023. 12 [33] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [34] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. [35] Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories. arXiv preprint arXiv:2504.08942, 2025. [36] Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, and Dong Yu. Laser: Llm agent with state-space exploration for web navigation. arXiv preprint arXiv:2309.08172, 2023. [37] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [38] Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. Bagel: Bootstrapping agents by guiding exploration with language, 2024. [39] Shikhar Murty, Hao Zhu, Dzmitry Bahdanau, and Christopher D. Manning. Nnetnav: Unsupervised learning of browser agents through environment interaction in the wild, 2025. [40] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. [41] Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. arXiv preprint arXiv:2412.13501, 2024. [42] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. [43] OpenAI. Operator. 2025. [44] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [45] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024. [46] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. [47] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. [48] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. UI-TARS: pioneering automated GUI interaction with native agents. CoRR, abs/2501.12326, 2025. [49] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [50] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [51] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. [52] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [53] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [54] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [55] David Silver and Richard Sutton. Welcome to the era of experience. Preprint of chapter to appear in Designing an Intelligence, edited by George Konidaris, MIT Press (forthcoming), 2025. [56] Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, Jianing Wang, Qintong Li, Xiangru Tang, Tianbao Xie, Xiachong Feng, Xiang Li, Ben Kao, Wenhai Wang, Biqing Qi, Lingpeng Kong, and Zhiyong Wu. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific workflows, 2025. [57] Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. X-prompt: Towards universal in-context image generation in auto-regressive vision language foundation models, 2024. [58] Zeyi Sun, Tong Wu, Pan Zhang, Yuhang Zang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Bootstrap3d: Improving 3d content creation with synthetic data. arXiv e-prints, pages arXiv2406, 2024. [59] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [61] Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1564115653, 2024. [62] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [63] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 14 [65] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024. [66] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [67] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [68] Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, et al. Scalecap: Inference-time scalable image captioning via dual-modality debiasing. arXiv preprint arXiv:2506.19848, 2025. [69] Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605, 2024. [70] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. [71] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [72] Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935 110971, 2024. [73] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 120, 2025. [74] Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Ming Zhu, Juntao Tan, Thai Hoang, Zuxin Liu, Liangwei Yang, et al. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv preprint arXiv:2402.15506, 2024. [75] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024. [76] Kaiyan Zhang, Biqing Qi, and Bowen Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion, 2024. [77] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023. [78] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [79] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. [80] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. [81] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A World State Model",
            "content": "The World State Model (WSM) is central component of SEAgent, responsible for understanding visual state changes and evaluating the effectiveness of the agents actions. A.1 Model Architecture and Operation The WSM is built upon the Qwen2.5-VL-7B vision-language model. It operates in two distinct modes, each with specific input-output structure to perform different tasks: 1. Trajectory Judgment: Input: sequence of screenshot images captured during an episode. Output: Short captions for each screenshot, the reasoning process for the judgment, and structured judgment dictionary (containing fields such as Correctness, Redundant, and First Error Step, as detailed in Figure 7 of the supplementary material). 2. State Change Description: Input: Two screenshot images, one from before and one after single action was executed. Output: detailed description of the visual differences between the two images. A.2 Fine-Tuning Dataset and Process To equip the WSM with these capabilities, specialized dataset was constructed for fine-tuning. Data Construction The data construction process is as follows: 1. Trajectory Sampling: Computer Using Agent (CUA), powered by UI-TARS and Gemini2.5-Pro, was used to sample trajectories from 43 feasible tasks in Google Chrome within the OSWorld benchmark. These trajectories were saved as screenshot sequences. 2. GPT-4o Annotation: Using the prompts detailed in Figures 6 and 7 of the supplementary material, GPT-4o was employed to annotate the sampled trajectories, generating judgments and screenshot captions. Only samples where the judgment matched the ground truth from OSWorld evaluation protocols were retained, resulting in 860 high-quality annotated trajectories. 3. Change Description Data: An additional 1,000 pairs of (before action, after action) screenshots were sampled. GPT-4o was used to generate detailed descriptions of the differences, creating 1,000-sample Change Description (CD) dataset. Fine-Tuning Process The fine-tuning was performed using the Llama-Factory framework on 8 NVIDIA A100 (80G) GPUs for 2,000 iterations. learning rate of 2 105 was used, and LoRA (rank=128) was employed for parameter-efficient fine-tuning. The 860 annotated trajectories serve as the core training data for teaching the model trajectory judgment, captioning, and reasoning. The 1,000-sample CD dataset acts as auxiliary data, specifically to encourage the model to focus on fine-grained visual differences, which enhances its overall state understanding. As shown in Table 1 of the main paper, incorporating CD data significantly boosts judgment performance. The two datasets were combined for training without any special re-weighting. A.3 Reward Generation from Trajectory Analysis The trajectory judgment capability of the WSM is the core source of the reward signal for reinforcement learning. After an agent executes full trajectory = {s0, a0, s1, a1, . . . , sfinal}, the WSM analyzes it and outputs structured judgment. Based on this output, actions within the trajectory are dynamically labeled as either positive actions (aT ) or failure actions (aF ): Fully Successful Trajectory: If Correctness is True and there are no Redundant steps, all actions in the trajectory are labeled as aT . Successful but Inefficient Trajectory: If Correctness is True but Redundant steps begin at step k, all actions prior to step are labeled as aT . 16 Failed Trajectory: If Correctness is False and the First Error Step is e, all actions prior to step are labeled as aT , while the erroneous action ae is labeled as aF . These dynamically labeled aT and aF actions constitute the reward signals for the RL pipeline. During training, the actor predicts an action at based on the history {a0, s0, . . . , st} and uses these labels to calculate rewards."
        },
        {
            "title": "B Curriculum Generator",
            "content": "The Curriculum Generator is designed to dynamically produce tasks of increasing difficulty and diversity, guiding the agent through systematic exploration of the softwares capabilities. B.1 Task Generation Mechanism The workflow of the Curriculum Generator is detailed in the pseudocode in our supplementary material. Its core idea is to leverage the WSMs analysis of completed tasks to generate new ones. The process, illustrated by the \"add rectangle\" example from Figure 5, involves three main steps: 1. Analysis and Feedback: The agent successfully completes an initial task, \"add rectangle.\" The WSM analyzes the execution trajectory and extracts two key pieces of information: task evaluation (Exam) and list of observed state changes (CD_list). CD_list: {\"add rectangle\": [\"The Edit bar is expanded...\", \"The cursor has changed into cross...\", \"A blue box appears on the screen with side bars showing properties such as fill, line, color, width, transparency, and corner style...\"], ...} Exam: [{\"task\": \"add rectangle\", \"status\": \"success\"}, ...] 2. Knowledge Integration and Task Generation: The CD_list and Exam are fed into the Curriculum Generator. It distills new knowledge, such as \"properties of rectangle,\" and integrates it into its internal Software guidebook. Based on this new knowledge, it generates more challenging tasks like \"Add green rectangle\" or \"Add red rectangle with 50% transparency,\" which are then added to the task buffer. 3. Iterative Learning: In the next RL phase, the agent samples from this updated, more challenging task buffer. The continuously enriched Software guidebook acts as the systems long-term memory, driving the Curriculum Generator to propose increasingly sophisticated and unexplored tasks in subsequent rounds, thereby guiding the agent toward mastery. Details of Curriculum Generator. C.1 Exemplar Case during Task Evolution. 0 , (2) We provide an exemplar case of our task evolution pipeline in Fig. 5, demonstrated using LibreOffice Impress. Initially, the World State Model parses screenshot of the Impress interface into detailed captions describing the layout and individual buttons. The Task Generator then produces an initial task set, I0 = {I (1) 0 , . . .}, and summarizes the initial software guidance memory U0. The initial agent executes tasks in I0, such as Add Rectangle, while the World State Model evaluates these actions, providing judgments and detailed descriptions of resulting changes. As shown in the Auto-Exploration stage, this includes generating captions for newly appeared property panels and assessing execution success. The Task Generator incorporates feedback on execution success and newly revealed properties (e.g., transparency) to evolve new tasks, such as Draw green rectangle with 50% transparency. This process iteratively improves through reinforcement learning, enabling continuous task evolution and agent self-improvement. C.2 Comparative Analysis of Instruction Generation Strategies. To validate the effectiveness of our Curriculum Generator, we conducted comparative analysis against state-of-the-art instruction generation methods, namely those from NNetNav [39] and WebRL [47]. 17 Figure 5: SEAgent autonomous exploration pipeline. The agent (policy model) and World State Model iteratively generate new task and perform RL to become specialist in novel software. Experimental Setup We adapted the official code and prompts from these prior works from web environments to general software applications. To ensure fair comparison of the curriculum quality, for each strategy, we employed two leading LLMs: the open-source Qwen2.5-72B [7] and the proprietary Gemini-2.5-Pro [14]. The tasks generated by each strategy were used to train an RL agent (using GRPO only), with reward signals uniformly provided by our fine-tuned WSM. The evaluation was performed on two applications: VSCode from OSWorld (a standard software) and Celestia from ScienceBoard [56] (a more challenging, out-of-domain scientific application). The primary metric was the task success rate. Results and Discussion The results are presented in Table 4. Table 4: Success rate (%) comparison of different task generation strategies on two software applications. Task Generation Strategy LLM VSCode Celestia WebRL WebRL Gemini2.5-Pro-thinking Qwen2.5-72B NNetNav NNetNav Gemini2.5-Pro-thinking Qwen2.5-72B Curriculum Generator (Ours) Curriculum Generator (Ours) Gemini2.5-Pro-thinking Qwen2.5-72B 27.5 36.2 34.6 43.6 37.7 42.3 0.00 3.03 0.00 5.05 9.09 12.12 As shown, the reverse instruction generation strategy from NNetNav [39] is highly effective on the in-domain application (VSCode), demonstrating high data generation efficiency by producing successful trajectories. However, critical trade-off was observed: this approach tends to generate many similar tasks, limiting its ability to explore the full breadth of the softwares functionalities. This limitation becomes more pronounced when the task generator is unfamiliar with the target software, as seen in the OOD Celestia environment. In contrast, our guidebook-based method, while having lower initial data generation efficiency, excels at systematic exploration. It builds structured knowledge of the software from scratch, making it more robust for tackling novel applications. This is evidenced by its superior performance on the more challenging Celestia software. 18 We conclude that these two strategies are complementary. Reverse instruction generation can efficiently exploit known functionalities, while our guidebook-based method can systematically explore new ones and help the task generator build more comprehensive understanding of the target software. hybrid approach combining both strategies is promising direction for future work. Test on TARS-1.5 Our work focuses on enabling agents to adapt to out-of-domain (OOD) and novel software where human-labeled data is not available. To test this, we applied our SEAgent pipeline to the UI-TARS1.5 [48] model on two distinct benchmarks. On OSWorld [67], we observed moderate performance gains. We hypothesize this is because UI-TARS-1.5s training data already targeted OSWorld, making it familiar, in-domain environment for the base model. However, on the ScienceBoard [56] benchmarka suite of scientific applications that are truly novel to UI-TARS-1.5our pipeline delivers significant and substantial improvements. This strongly validates our core claim: SEAgent is most impactful when performing self-evolution learning on truly OOD software. We excluded two of the six ScienceBoard applicationsLean and TeXas they are primarily textand code-based software for mathematics and typesetting, which are not suitable for evaluating GUI-centric agent like UI-TARS. Table 5: Performance comparison on OSWorld and ScienceBoard benchmarks. Scores represent success rates (%). OSWorld ScienceBoard Model LibreOffice Impress LibreOffice Writer GIMP ChamerX GrassGIS KAlgebra Celestia UI-TARS-1.5-7B-DPO UI-TARS-1.5-7B-DPO+SEAgent 19.15 23.83 33.04 35.65 51.54 56.92 12.41 23.45 0.00 10. 11.61 21.29 4.85 11."
        },
        {
            "title": "E Sensitivity Analysis on Key Hyperparameters",
            "content": "We conducted sensitivity analysis on key hyperparameters to evaluate their impact on the SEAgent pipeline. For model sampling, we set the temperature = 0 for better reproducibility. We analyze two specific parameters: the number of generated tasks and the number of change descriptions. The results are presented in Table 6 and discussed below. Table 6: Sensitivity analysis for key hyperparameters in the SEAgent pipeline, evaluated on VSCode. The metric is Success Rate (%). # Tasks Generated VScode SR # Change Descriptions VScode SR 30 50 100 200 31.88 36.23 37.68 37.68 30 50 100 33.33 37.68 37.68 34.78 Number of Generated Tasks This parameter controls the breadth of exploration in each learning cycle. As shown in our analysis, performance improves as more diverse tasks are generated, eventually plateauing around 100 tasks. Number of Change Descriptions This parameter controls how much new information the generator receives to update its \"software guidebook.\" We found clear trade-off: sufficient number of descriptions (50100) is essential for the generator to learn about new UI functionalities and create meaningful, unexplored tasks. However, providing too many descriptions (e.g., 200) creates an overly long context for the LLM, which degrades the quality of task generation and hurts final performance. 19 Ablation on the Loss Balance Factor. In Sec.3.2, we use γ to balance the ratio of two loss item: adversarial imitation that learn from error and GRPO that learn to achieve success. We ablate the choice of γ in Tab.7, according to which we set γ = 0.2 in main experiments. γ 0.0 0.1 0.2 0.3 0. 0."
        },
        {
            "title": "36.2\nTable 7: VScode Success Rate on OSWorld [67] under different loss balance factor γ values.",
            "content": "Success Rate (%) 37.7 26.1 34.8 31.9 23. Reward Function for Different Actions. Action Type Description Distance-based Reward click, left_single, right_single, hover Click or hover on location left_double, double_click drag, select type hotkey press scroll move_mouse highlight copy, paste wait finished, finish_task Normalized L1 distance between predicted and ground-truth coordinates Normalized L1 distance between clicked coordinates Intersection over Union (IoU) between predicted and ground-truth boxes Character-level BLEU score between predicted and ground-truth text Character-level BLEU score between predicted and ground-truth key combinations Character-level BLEU score between predicted and ground-truth key Character-level BLEU score between predicted and ground-truth direction Double click on region Drag from start box to end box Type textual input Press multiple keys at once Press single key Scroll in certain direction Move mouse to specific location Normalized L1 distance between predicted and ground-truth coordinates Highlight rectangular UI region Clipboard operations Explicit wait command Finish current task/trajectory IoU between predicted and ground-truth region BLEU score between copied/pasted content Fixed reward + 1 Fixed reward + 1 Table 8: Reward computation for each action type in GUI agent Data Statistics during Iterative Reinforcement Learning. Phase0 Phase Phase2 Phase3 282/83 309/90 290/92 114/41 278/101 Table 9: Number of episode (Success/Failure) across four phases for different software tools during self-evolution. Each episode contains 8.8 multi-turn conversions in average. VSCode GIMP Impress VLC Writer 161/34 183/50 185/61 160/48 201/69 98/55 95/52 87/51 53/27 101/ 112/39 104/51 102/44 85/29 123/62 Detailed Prompt Templates. For evaluation on AgentRewardBench [35], we use their official template for final state screenshot only testing and modified prompt in Fig.6 for entire process (or sampled middle screenshots) testing. For evaluation on OSWorld Sampled trajectories, we use prompt in Fig.7 to prompt GPT-4o to provide step level judges, the sampled judges on Chrome in OSWorld [67] serves as training data of GUI-Judge. This template is also used in training GUI-Judge and at inference time in autonomous exploration stage. For navigator, we use prompt template in Fig.8, which takes previous software usage manual and the performance of actor agent evaluated by judge (Empty if in initial phase.) as well as detailed exploration caption as input and output the updated usage manual as well as new task for agent to execute. Self documented usage manual on different software during exploration. In Fig.9 Fig.11, Fig.10, Fig.12, we demonstrate the self-documented usage manuals of the navigator (Qwen2.5-72B [71]) in the exploration and learning system introduced in Sec.3.1. 20 Figure 6: Prompt Template of GUI-Judge for web agent trajectories evaluations with history screenshots as input, its difference with default prompt of AgentRewardBench [35] is highlighted in bold."
        },
        {
            "title": "K Broader Impacts",
            "content": "Potential positive societal impacts: SEAgent introduces self-evolving paradigm for Computer Use Agents (CUAs), enabling them to autonomously learn and adapt to previously unseen software without human supervision. This significantly reduces the need for extensive manual data annotation and domain-specific customization, allowing intelligent agents to assist users across wide range of applicationsincluding productivity tools, multimedia editing, and educational software. By automating repetitive tasks and providing guidance in complex software environments, SEAgent holds promise for improving accessibility, enhancing digital literacy, and reducing cognitive workload in both professional and everyday settings. Potential negative societal impacts: The capability of SEAgent to autonomously explore and operate complex software also introduces risks of misuse. Malicious actors might repurpose SEAgent for unauthorized software automation, such as automating account creation, spamming interfaces, or conducting surveillance via GUI interactions. In addition, as the agent learns from its own experience, there exists risk that the agent may inadvertently inherit or amplify software-specific biases, potentially leading to unfair or inappropriate behaviors in sensitive applications (e.g., finance, legal automation). Mitigation strategies include controlled release of models, behavior filters during deployment, and incorporating safeguards in the World State Model to detect and prevent unintended or adversarial behavior. 21 Figure 7: Prompt Template of GUI-Judge for OSWorld [67] trajectories, which prompts judge model to provide step level reward signal. SEAgent Self-Evolution Algorithm Algorithm 1 presents the core self-evolution training loop of SEAgent in specialized software environment. The procedure is divided into four major stages: (1) Task Initialization. Given the initial GUI state of target software application, the World State Model performs dense captioning to extract structural semantics (e.g., menu bar, buttons), which is used by the Curriculum Generator to create an initial set of executable tasks and an editable software guidebook. (2) Autonomous Exploration and Effect Evaluation. The agent explores each task via its current policy. The World State Model then performs step-level trajectory analysis, assigning each action feedback labeleither correct (aT ) or incorrect (aF )and generating GUI state change captions. This produces rich supervision signals for both policy learning and downstream task generation. (3) Policy Update via Reinforcement Fine-Tuning. Based on the labeled execution data, positive and negative action steps are separated. We apply Group Relative Policy Optimization (GRPO) to reinforce correct actions, and Adversarial Imitation (AI) to suppress failure-prone behaviors. The updated policy is used for the next exploration round. 22 Figure 8: Prompt Template for task buffer update, which generates new tasks in curriculum manner and update software documents. The new tasks are used for actor to perform next phase of RL. (4) Task Update. The Curriculum Generator leverages feedback signals (J ) and GUI state transitions (C) to propose more diverse and challenging tasks, thereby expanding the task frontier in curriculum fashion. This process repeats over multiple curriculum phases, ultimately yielding specialized agent policy capable of mastering complex operations in the given software environment. 23 Figure 9: Automatically generated usage manual during self exploration on VScode. Figure 10: Automatically generated usage manual during self exploration on GIMP. Figure 11: Automatically generated usage manual during self exploration on LibreOffice_Impress. Figure 12: Automatically generated usage manual during self exploration on LibreOffice_Writer. Algorithm 1 SEAgent Specialized Self-Evolution Training Loop 1: Input: Initial policy π0, World State Model Mstate, Curriculum Generator Mtask, Initial GUI state S0 2: 1. Task Initialization 3: C0 CaptionGUI(S0) 4: I0, U0 Mtask(, , , C0) 2.1 Autonomous Exploration Dtraj for all Ip do 5: for = 0 to 1 do 6: 7: 8: 9: 10: 11: 12: 13: end for τ ExecuteInstruction(πp, I) 2.2 Effect Evaluation JI , CI Mstate(τ ) Dtraj Dtraj {(τ, JI , CI )} Parse initial GUI layout (menu bar, buttons, etc.) Generate basic initial tasks and usage guide 2. Self-Evolution Phase Loop Actor executes task in the virtual environment Step-level trajectory judgment and new state captions JI : sequence of per-step feedback labels (aT or aF ) 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 2.3 Policy Update (RFT) Split Dtraj into: Dpos: steps labeled as positive aT Dneg: steps labeled as negative aF Compute GRPO loss on Dpos: r(a, aT ) = I[type(a) = type(aT )] + rdist(a, aT ) Compute Adversarial Imitation loss on Dneg: LAI = log πθ(as,I) πref(aF s,I) Total loss: Ltotal = LGRPO + γLAI πp+1 Update(πp, Ltotal) 2.4 Task Update Ip+1, Up+1 Mtask(Up, Ip, {JI }, {CI }) software knowledge and performance feedback Generate more complex tasks based on new 26: end for 27: Output: Specialized agent policy πP after stages of self-evolution"
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}