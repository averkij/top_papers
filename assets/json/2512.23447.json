{
    "paper_title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
    "authors": [
        "Ang Lv",
        "Jin Ma",
        "Yiyuan Ma",
        "Siyuan Qiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 7 4 4 3 2 . 2 1 5 2 : r Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss Ang Lv1,2, Jin Ma1, Yiyuan Ma1, Siyuan Qiao 1ByteDance Seed 2Renmin University of China, GSAI"
        },
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the routers decisions align well with the experts capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, lightweight auxiliary loss that tightly couples the routers decisions with expert capabilities. Our approach treats each experts router embedding as proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding experts capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n2 activations, where is the number of experts. This represents fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs. Correspondence: Ang Lv at anglv@ruc.edu.cn, Yiyuan Ma at mayiyuan.unicorn@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Mixture-of-Experts (MoE, 8, 18, 34, 48) is core architecture in modern large language models (LLMs). In MoE models, the feed-forward layer is split into multiple small, specialized experts. linear classifier, known as the router, selects which experts process each input token. By activating few experts per token, MoE balances efficiency with scaled parameter counts, enabling the training of trillion-parameter models. Ideally, router should possess an accurate representation of each experts capabilities to enable effective token routing. However, traditional MoEs offer no explicit constraints to guarantee this. Without direct access to expert parameters (and therefore their true capabilities), routers resort to trial-and-error learning of routing strategies, often resulting in misrouted tokens whose gradients interfere with expert specialization. While some methods [23, 28] incorporated all experts activations for routing guidance, they incur substantial computational and memory costs due to denser activation. lightweight and effective solution to better couple routing decisions with true expert capabilities remains an open challenge. 1 Figure 1 Three steps for computing the expert-router coupling loss. We propose expert-router coupling loss (ERC loss), novel auxiliary loss for MoE models that tightly couples routers and experts with negligible overhead. The loss is based on interpreting the router parameter matrix Rnd as cluster centers, where each row R[i] serves as the center for the token set Xi routed to expert i. The ERC loss comprises three key steps: (1) Each R[i] is augmented with bounded random noise δi to obtain R[i], serving as proxy for tokens in Xi. Here, δi is bounded by half the minimum distance between adjacent cluster centers, ensuring that the noise simulates input variations within Xi while preventing the crossing of cluster boundaries. (2) Inspired by prior works [9, 20, 23], the intermediate activation norm serves as an indicator of how well its capabilities align with the token. We measure the intermediate activation norms of all experts that take R[i] as input. This step produces matrix Rnn, with [i, j] being the activation norm from expert given input R[i]. (3) For all = j, the ERC loss imposes penalty wherever the off-diagonal elements [i, j] or [j, i] exceed αM [i, i], where α is scalar hyperparameter: LERC = 1 n2 (cid:88) (cid:88) i=1 j=i (max (M [i, j] αM [i, i], 0) + max (M [j, i] αM [i, i], 0)) . Minimizing it tightly couples experts and routers through two effects: Expert specialization: The proxy token R[i] elicits the strongest activation from expert versus all other experts. This indicates that expert is optimized to best match the features of its assigned token cluster Xi. Precise token routing: Expert is most activated by its designated vector R[i] than to any other R[j] for = i. This demonstrates that R[i] aligns well with the capabilities of expert i, ensuring that the router assigns to this expert the tokens that need it most. We conducted large-scale pre-training experiments on models from 3B to 15B parameters, using total of several trillion tokens. The ERC loss not only significantly enhances model performance and narrows the performance gap with competitive yet more computationally expensive MoE variant [23] but also retains the efficiency of vanilla MoEs. Furthermore, building on the first effect, we establish that the ERC loss serves as powerful tool for studying expert specialization. This property arises from two key features of the ERC loss: (1) the specialization level is explicitly controlled by α, and (2) the bound of noise δi provides quantitative measure for this level. Through this lens, we reveal trade-off between specialization and model performance. Our findings challenge some beliefs about expert specialization that were derived from small-scale experiments. These quantitative and qualitative analysis methods offer new pathways to advance the understanding of MoE models."
        },
        {
            "title": "2 Background",
            "content": "Mixture-of-Experts Our description follows the prevailing SwiGLU structure used by advanced LLMs [7, 26, 30]. An MoE layer consists of experts, where each expert is parameterized by three matrices: RdD, Wp RdD, and Wo RDd. The layer also includes router with the weight matrix Rnd, which takes token Rd as input and outputs an expert weight1 vector: = softmax(xR) Rn. Typically, the top-K experts with the highest expert weights are selected to process the token. The processing of by expert is given by: Ei(x) = (cid:0)SiLU(xW ) (xW p)(cid:1) o, where denotes element-wise multiplication. The final output of the entire MoE layer is the weighted sum of the outputs of the selected experts: (cid:88) w[k]Ek(x), where Top-K(w). Expert-router coupling via denser activation Autonomy-of-Experts (AoE; 23) encodes the routing function into expert parameters. AoE factorizes Wg into two r-rank matrices up RrD. Each expert processes token up to the point after the projection. The expert weight vector is computed using the activation norm at this stage: down Rdr and down = softmax (cid:0){xW down for = 1, . . . , n}(cid:1) . The top-K experts exhibiting the highest activation norms are selected to continue their forward computation, and the others are terminated early. This norm-based selection is justified by the fact that the activation norm of MLPs represents how well their capabilities match their inputs [9, 20]. The computational overhead of AoE scales with the number of tokens during both training and inference. Moreover, this inefficiency worsens as the number of experts increases or the selection count decreases. Wu et al. [44] found that using only small, representative subset of neurons per expert is sufficient for autonomous expert selection, reducing but not eliminating AoEs token-dependent cost. Figure 2 The overview of MoE and AoE models. Pham et al. [28] use experts final output norms to supervise router logits. There is no inference overhead but the model is fully dense-activated during training, contradicting the core sparsity principle of MoE. Therefore, we include it only for background discussion, not as baseline."
        },
        {
            "title": "3 Method",
            "content": "After analyzing the strengths and limitations of prior work, we distill three design principles to ensure lightweight, effective, and practically applicable enhancement for expert-router coupling in MoE-LLMs: (1) Routers must be retained in MoE architectures to preserve routing efficiency. (2) An auxiliary loss that enables interaction between experts and routers can strengthen their coupling. (3) The loss must have complexity independent of the number of input tokens and must not introduce activation density beyond that of vanilla MoE. Below, we introduce expert-router coupling loss, which fulfills all these principles. 1In this paper, weight refers to the relative contribution of each experts output or the strength of the loss function. Please carefully distinguish between weight and parameter."
        },
        {
            "title": "3.1 Expert-router coupling loss",
            "content": "The expert-router coupling (ERC) loss is motivated by clustering-based interpretation of MoE routing: The routing mechanism in traditional MoE models can be interpreted as clustering process, where router parameters Rnd are viewed as cluster centers. For any input token Rd, the router computes an n-dimensional logit vector representing the weight assigned to each expert. Specifically, the weight for expert is derived from the inner product between and the cluster center R[i]. When belongs to the cluster centered at R[i], this inner product is maximized2, making expert the top choice. key advantage of this clustering view is that it enables probing an experts responsiveness to set of tokens without feeding every token to all experts, unlike prior methods (See 2). Instead, we leverage each cluster center R[i] as proxy for tokens routed to expert (denoted as Xi), enabling us to derive intermediate activations and evaluate how well the expert aligns with proxy token. Our ERC loss is computed in three key steps: (1) For each cluster center R[i], we create perturbed proxy token R[i] = R[i] δi. δi Rd is bounded multiplicative random noise, which we elaborate in 3.2. This noise ensures the proxy generalizes to tokens in Xi. Notably, the corrupted is used only for loss computation; routing still uses the clean to compute router logits, as in standard MoEs. (2) Each proxy token is processed by the Wg parameter of all experts, yielding total of n2 intermediate activations. The L2 norm of each activation is computed to form matrix Rnn, where [i, j] corresponds to the norm from expert given input R[i]: [i, j] = (cid:13) (cid:13) R[i] (cid:13) (cid:13). (3) To enforce expert-router coupling, for all and = i, the ERC loss imposes two constraints, where scalar α [0, 1] determines their strength: [i, j] < αM [i, i], [j, i] < αM [i, i]. (1) (2) Constraint 1 ensures the proxy token R[i] activates its corresponding expert more than any other expert j. Since tokens similar to R[i] are routed to expert i, and given their similarity to R[i], they also elicit stronger activation in expert than in other experts. This strongest activation indicates that expert is optimized to develop capabilities best suited to Xi [23]. Constraint 2 requires that expert responds more strongly to its own proxy token R[i] than by any other R[j]. This ensures each R[i] accurately represents expert i, guaranteeing that tokens most needing expert expert are correctly routed to it. As α decreases, the two constraints become stricter, thereby enforcing stronger expert-router coupling. Additionally, α enables flexible regulation of specialization: smaller α increases the gap between [i, i] and [i, j], reflecting greater expert specialization as experts exhibit more differentiated responses to the same inputs. This feature makes the ERC loss useful tool for investigating expert specialization and provides deeper insight into MoE behavior, as demonstrated in 4.2. We translate these two constraints into expert-router coupling loss, formally defined as: LERC = 1 n2 (cid:88) (cid:88) i=1 j=i (max (M [i, j] αM [i, i], 0) + max (M [j, i] αM [i, i], 0)) . (3) The three steps for computing expert-router coupling loss are illustrated in Figure 1. For implementation details, we provide PyTorch-style pseudocode in Figure 7. 2This assumes all R[i]s have comparable norms. We confirm that the models used in our experiments adhere to this assumption."
        },
        {
            "title": "3.2 Bounded random noise for generating proxy tokens\nThe perturbed proxy token ˜R[i] = R[i] ⊙ δi makes expert i’s coupling generalizes effectively from R[i]\nalone to Xi. To ensure the perturbed point ˜R[i] remains within its original cluster, we require a bounded\nperturbation. We therefore model the noise δi as a multivariate uniform distribution, δi ∼ U(1 − ϵi, 1 + ϵi)d.\nLet j = arg minj∗̸=i ∥R[i] − R[j∗]∥ be the nearest cluster center. For the noise level ϵ to be sufficient to avoid\nperturbing the cluster, it must satisfy:",
            "content": "ϵi R[i] R[j] 2R[i] . (4) The derivation of this bound is provided in Appendix A. We set ϵi to its maximum value, i.e., the right-hand side of this inequality. Notably, the value of ϵi is dynamically computed at each layer and every training step."
        },
        {
            "title": "3.3 Efficiency analysis",
            "content": "In standard MoE layer, tokens are processed by experts, resulting in total Training efficiency computational cost of 6T KDd FLOPs. expert-router coupling loss introduces only 2n2Dd additional FLOPs, cost that is negligible in practical pre-training setups where is often in the millions. In contrast, AoE introduces an additional overhead of 2T (n K)dr FLOPs (recall that is AoEs factorization rank; see 2). Given that typical MoE-LLMs operate at sparsity levels far below 25% (i.e., > 4K), this overhead ratio exceeds r/D, making it prohibitive. detailed breakdown of the FLOP calculations supporting the above theoretical analysis is provided in Appendix B.1. The efficiency of our method is confirmed in practice. The ERC loss maintains low overhead during LLM pretraining with multiple parallelism strategies, adding only 0.20.8% overhead in our experiments. We provide complete analysis of these real-world distributed conditions and measured throughputs in Appendix B.2. Overhead-free inference Our method incurs no additional inference overhead as the auxiliary loss is not applied. However, AoE retains the same forward computation, carrying over the associated overhead."
        },
        {
            "title": "4.1 Experimental settings",
            "content": "We compare the ERC-loss-augmented MoE against both the vanilla MoE and AoE baselines. All models are trained from scratch with 3B parameters. This parameter size is chosen because it represents the largest scale at which we could successfully train the AoE model under our available resources. Our implementation is based on OLMoE [25]. The models comprise 12 layers with = 1536 and = 768. Each Transformer [40] layer has 16 attention heads and = 64 experts, where = 8 experts are selected per token. For the AoE model, we set = 512 to ensure consistent total parameter count. The number of activated parameters is 500M. Each model is trained on 500B tokens from the open-source dataset dolmap-v1.5-sample [35], using batch size of 3 million tokens. We use the AdamW optimizer [21] with (β1, β2) = (0.9, 0.95), weight decay of 0.1, and learning rate of 4e-4 with cosine schedule decaying to 4e-5. load balancing loss [8] with weight of 0.01 is applied consistently in all experiments. For simplicity, the loss weight of the ERC loss is fixed at 1, and we use α = 1 by default if not specified. We evaluate LLMs on following tasks: ARC-Challenge [4], CommonsenseQA [37], COPA [31], BoolQ [3], HellaSwag [46], OpenbookQA [24], SciQ [43], Social IQa [33], WinoGrande [32], and MMLU [13]."
        },
        {
            "title": "4.2 Performance, efficiency, and load balancing",
            "content": "Figure 3(a) reports the average accuracy across all tasks and task-specific results are presented in Figure 8. It shows that the ERC-loss-augmented MoE achieves stable performance gains, which significantly outperforms the vanilla MoE and narrows the gap between AoE and MoE. 5 Figure 3 The 3B-parameter MoE model augmented with ERC loss achieves substantial and stable performance gains, while maintaining comparable load balancing to the baseline. Figure 8 shows task-specific details. Table 1 Scaling to 15B parameters: ERC loss improves performance on more challenging benchmarks. MMLU C-Eval MMLU-Pro AGI-Eval BBH MATH GSM8K TriviaQA MoE MoE + LERC 63.2 64.6 67.5 69.0 31.0 31.9 42.0 44.2 44.3 45.6 25.7 26. 45.2 45.8 47.2 49.1 In terms of efficiency, MoE models with and without ERC loss have nearly identical throughput and memory costs. By contrast, AoE requires 1.6 more training hours and 1.3 higher memory usage, limiting further scaling due to impractical training times and out-of-memory issues. Expert-router coupling loss is compatible with the load balancing loss. As shown in Figure 3(b), the difference in load balancing loss between MoE combined with LERC and the vanilla MoE is on the order of 105. This difference is negligible given that the overall load balancing loss magnitude remains around 102. By comparison, the loss difference between AoE and vanilla MoE is approximately 4 104. Although this difference is still small, it is notably larger than the difference exhibited by ours."
        },
        {
            "title": "4.4 The ERC loss is an effective tool for exploring expert specialization",
            "content": "With the ERC loss, experts are more specialized, as they exhibit greater discrimination between tokens they process and those they do not, compared to the ERC loss is not used. An intuitive demonstration of this specialization comes from visualizing expert parameters. Following [45], we use t-SNE [39] to project each (where mod 8 = 0) from layer 6 (the middle depth) onto 2D point. As shown in Figure 4, row of experts in vanilla MoE lack specialization, as their parameter features do not form meaningful clusters. By contrast, MoE enhanced with the ERC loss exhibits distinct clusters, indicating specialized capabilities. 6 Figure 4 t-SNE projections of Wg in MoE experts trained without and with the ERC loss. Our ERC loss provides greater expert specialization. Figure 5 (a) Since routers are deeply coupled with experts, the distance between neighboring cluster centers (i.e., the maximum noise level ϵ) quantitatively reflects changes in expert specialization during training, which is controlled by α. (b) Downstream performance across different values of α. Beyond merely promoting specialization, the ERC loss can also serve as powerful tool for exploring it. We show this capability through two features below. In the ERC loss, α governs Feature 1: α enables controllable investigation into optimal specialization the coupling strength between experts and the router. When α = 0, the ERC loss encourages R[i] to be orthogonal to the parameters of other experts, thereby maximizing specialization. Conversely, when α 1, the loss permits smaller differences in how all experts responsiveness to R[i], thus reducing specialization. Notably, α = 1 only weakens the ERC losss constraints to their maximum extent; it still retains degree of specialization stronger than the spontaneously emerged specialization in vanilla MoE model. Feature 2: ϵ provides quantitative measure for specialization The noise level ϵ exhibits strong correlation with α, and it can reflect changes in expert specialization throughout the training process. This correlation exists because as α increases, experts are allowed to be more homogeneous. This growing homogeneity among experts, in turn, reduces the separation between the cluster centers in the router as they are tightly coupled. smaller separation between cluster centers ultimately derives smaller ϵ. Thus, ϵ is quantitative metric tracking expert specialization. Experiments The following experiments support these two features. In Figure 5(a), we plot ϵ at each training step across parameter search over α {0.4, 0.6, 0.8, 1.0}. Consistent with our analysis, increasing α 7 which reduces expert specialization indeed leads to corresponding decrease in ϵ. Note that measuring router cluster distance is uninformative in vanilla MoE training without the ERC loss, as the router and experts are uncoupled and cluster distances do not reflect expert capability dynamics. We further compared downstream task performance across different values of α. Figure 5(b) shows that all tested α values outperform the vanilla MoE model. This not only confirms the robust effectiveness of the ERC loss but also demonstrates that the specialization spontaneously formed by vanilla MoE models is inadequate. The optimal specialization degree Figure 5(b) shows that pursuing extreme specialization is not advisable, as model performance degrades with overly strict α. This highlights trade-off between promoting expert specialization and maintaining effective collaboration, which is under-discussed in previous work. The optimal specialization degree is influenced by several factors. The core consideration is whether, among all (cid:0) (cid:1) possible expert combinations, an effective K-expert set can be assembled for any given input. In general, smaller values of favor more generalist experts, while larger can support higher degree of specialization. However, we currently lack quantitative metrics to characterize large or small and across different models; as result, determining the optimal trade-off remains largely empirical. For example, in our experiments with fixed = 8: When = 64, the optimal α = 1, suggesting = 64 is not large for our 3B-parameter models. In contrast, with = 256, we searched an optimal α = 0.5, indicating = 256 is large for our 15B-parameter models. This trade-off is also shaped by other architectural choices, such as the use of shared experts3. deeper investigation into these interacting factors, reliable quantitative metrics for specialization, and an automated evaluation of the optimal specialization degree for given model are left as important problems for future works. For practitioners implementing the ERC loss, we recommend starting with α = 1, which eliminates expert decoupling and should provide some gains. Further improvement may be achieved by experimenting with lower α values, depending on the specific configuration of your model. Several studies [10, 12, 19] have promoted specialization via expert output orthogonality. We argue, however, that orthogonalizing expert outputs does not equate to achieving extreme specialization, as the magnitude (norm) of an experts response to token remains unconstrained. Moreover, finding set of orthogonalized high-dimensional vectors is not difficult, making it unclear whether such orthogonality sufficiently leads to effectively discriminative representations. Consequently, one should not interpret these fine-tuning experiments as supporting broad claim that more specialization is always better. On separate note, orthogonality among router embeddings [1] is only weakly correlated with specialization, since the router and experts are typically decoupled. As demonstrated in Appendix C.3, enforcing router embedding orthogonality might not be critical factor for pre-training MoE models."
        },
        {
            "title": "5 Related works",
            "content": "Auxiliary loss for MoEs Auxiliary losses are crucial for training large-scale MoE models. Most existing work in this area focuses primarily on enhancing training stability. For instance, many studies have proposed auxiliary losses to address load balancing challenges [8, 29, 41]; Zoph et al. [48] introduced the z-loss, which penalizes excessively large logits in the gating network to enable stable training. Our ERC loss is the first tailored to strengthen the expert-router coupling. Other related auxiliary losses enhancing expert specialization or orthogonality are discussed below. 3A shared expert satisfies general input requirements, allowing the remaining experts to be more specialized even with the same n. 8 Expert specialization Dai et al. [6] introduced shared expert to handle general capabilities, encouraging the others to be more specialized. Guo et al. [10] proposes an auxiliary loss to minimize the pairwise projections of the selected top-K experts outputs for each token, reducing expert overlap but incurring high cost due to 2 cosine similarity calculations per token. Other methods scale the number of tiny experts to millions, making each expert more atomic and thus more specialized [11, 27, 45], but are memory-bounded. Beyond efficiency, these methods face three major limitations: (1) no quantitative control over specialization degree; (2) no exploration of the specialized-generalized ability trade-off; and (3) failure to strengthen expert-router coupling. Our method addresses all three, both efficiently and effectively. Some works [10, 12, 19] maximize specialization by training orthogonal experts, but their evaluations are based on fine-tuning (or reinforcement learning) experiments. We contend that orthogonalizing expert outputs is not equivalent to achieving extreme specialization, and further, that the optimal degree of specialization is complex problem affected by various factors and requires further exploration. Contrastive learning Constraints 1 and 2 bear similarity to contrastive learning [2, 17, 38]. Some MoE research [10, 22] applied contrastive learning to expert outputs, encouraging specialization. However, naively applying contrastive learning to either routers or experts leaves the weak expert-router coupling unaddressed."
        },
        {
            "title": "6 Conclusions",
            "content": "The weak coupling between router decisions and expert capabilities limits MoE models in multiple important aspects. We propose expert-router coupling loss that tightly couples router parameters with their corresponding experts. The proposed ERC loss improves MoE-based LLMs on downstream tasks while incurring negligible training overhead. In addition, it exhibits several desirable properties that not only provide deeper insight into the behavior of MoE models but also offer promising tool for future research on expert specialization."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Songhao Wu and Ziteng Wang for their insightful discussions. We are grateful to Ruobing Xie, Yining Qian, and Kaiyi Zhang for proofreading and writing suggestions."
        },
        {
            "title": "References",
            "content": "[1] Baidu-ERNIE-Team. Ernie 4.5 technical report, 2025. [2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations, 2020. URL https://arxiv.org/abs/2002.05709. [3] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924 2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300/. [4] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https: //arxiv.org/abs/1803.05457. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. [6] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. URL https://arxiv.org/abs/2401.06066. [7] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. 9 [8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. URL http: //jmlr.org/papers/v23/21-0998.html. [9] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446. [10] Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, and Xudong Jiang. Advancing expert specialization for better moe, 2025. URL https://arxiv.org/ abs/2505.22323. [11] Xu Owen He. Mixture of million experts, 2024. URL https://arxiv.org/abs/2407.04153. [12] Ahmed Hendawy, Jan Peters, and Carlo DEramo. Multi-task reinforcement learning with mixture of orthogonal experts. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=aZH1dM3GOX. [13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. [15] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023. [16] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147/. [17] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 18661 18673. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf. [18] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=qrwe7XHTmYb. [19] Boan Liu, Liang Ding, Li Shen, Keqin Peng, Yu Cao, Dazhao Cheng, and Dacheng Tao. Diversifying the mixture-of-experts representation for language models with orthogonal optimizer, 2024. URL https://arxiv. org/abs/2310.09762. [20] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and Beidi Chen. Deja vu: contextual sparsity for efficient llms at inference time. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [22] Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models, 2024. URL https://arxiv.org/abs/2402.12851. 10 [23] Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, and Rui Yan. Autonomy-ofexperts models. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview. net/forum?id=8BIDrYWCeg. [24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. URL https://arxiv.org/abs/1809.02789. [25] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models, 2025. URL https://arxiv.org/abs/2409.02060. [26] OpenAI. Gpt-oss series, 8 2025. URL https://openai.com/index/introducing-gpt-oss/. [27] Jungwoo Park, Ahn Young Jin, Kee-Eung Kim, and Jaewoo Kang. Monet: Mixture of monosemantic experts for transformers. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=1Ogw1SHY3p. [28] Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, and Nhat Ho. Competesmoe effective training of sparse mixture of experts via competition, 2024. [29] Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50055018, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.249. URL https://aclanthology.org/2025.acl-long.249/. [30] Qwen. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [31] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011. [32] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [33] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/ D19-1454. [34] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=B1ckMDqlg. [35] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. [36] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chainof-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology. org/2023.findings-acl.824/. 11 [37] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421/. [38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019. URL https://arxiv.org/abs/1807.03748. [39] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):25792605, 2008. URL http://jmlr.org/papers/v9/vandermaaten08a.html. [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/ file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [41] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts, 2024. URL https://arxiv.org/abs/2408.15664. [42] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [43] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94106, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4413. URL https://aclanthology.org/W17-4413/. [44] Songhao Wu, Ang Lv, Ruobing Xie, Xingwu Sun, Di Wang, Rui Yan, and Yankai Lin. Union-of-experts: Experts in mixture-of-experts are secretly routers, 2025. URL https://openreview.net/forum?id=Ksgiup7ZNZ. [45] Xingyi Yang, Constantin Venhoff, Ashkan Khakzar, Christian Schroeder de Witt, Puneet K. Dokania, Adel Bibi, and Philip Torr. Mixture of experts made intrinsically interpretable. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=6QERrXMLP2. [46] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. [47] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: human-centric benchmark for evaluating foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 22992314, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.149. URL https://aclanthology.org/2024.findings-naacl.149/. [48] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models, 2022. URL https://arxiv.org/abs/2202.08906."
        },
        {
            "title": "A Determining the maximum multiplicative noise level",
            "content": "In what follows, we write Ri for R[i] and Ri,k for the k-th element of R[i] to avoid excessive brackets. δi is random vector where each component δi,k follows uniform distribution U(1 ϵ, 1 + ϵ), and all components are mutually independent. The perturbed point is given by: Ri = (δi,1(Ri,1), δi,2(Ri,2), . . . , δi,d(Ri,d)). To ensure that Ri remains in the same cluster as Ri, it must satisfy: Ri Ri2 < Ri Rj2, where = arg minj=i R[i] R[j]. Expanding the squared norms on both sides of the inequality yields: Ri Ri2 = (cid:88) k=1 (δi,k 1)2(Ri,k)2, Ri Rj2 = (cid:88) (δi,kRi,k Rj,k)2. k= Substituting into the inequality and simplifying gives: (cid:88) k=1 [2δi,k(Ri,k(Rj,k Ri,k) + (R2 i,k R2 j,k)] < 0. To ensure this inequality holds for all realizations of δi, we consider the worst-case scenario that maximizes the left-hand side. Define: so the inequality becomes: Ak = 2Ri,k(Rj,k Ri,k), = (cid:88) k=1 (cid:88) k=1 Akδi,k + < 0. The worst-case δi,k is chosen to maximize (cid:80) Akδi,k: (R2 i,k R2 j,k), Substituting these values gives: δi,k = (cid:40) 1 + ϵ 1 ϵ if Ak > 0, if Ak < 0. (cid:88) k=1 Ak + ϵ (cid:88) k=1 Ak + < 0. Now simplify (cid:80) Ak + B: (cid:88) Ak + = 2 = 2 (cid:88) (cid:88) Ri,kRj,k 2 (cid:88) R2 i,k + (cid:88) R2 i,k (cid:88) R2 j,k Ri,kRj,k (cid:88) R2 i,k (cid:88) (cid:16)(cid:88) = R2 i,k 2 (cid:88) Ri,kRj,k + = Ri Rj2 13 R2 j,k (cid:88) R2 j,k (cid:17) (5) (6) (7) Substituting equation 7 into equation 6 yields: Solving for ϵ gives: Ri Rj2 + 2ϵ (cid:88) k=1 Ri,k(Rj,k Ri,k) < 0. ϵmax < Rj Ri 2 (cid:80)d k=1 Ri,k(Rj,k Ri,k) . However, computing the denominator of this expression is relatively complex. To balance the efficiency of loss calculation, we instead adopt tighter upper bound for ϵ. By the Cauchy-Schwarz Inequality, the following relationship holds: (cid:88) k=1 Ri,k(Rj,k Ri,k) RiRj Ri. Thus, we have: ϵmax = Rj Ri2 2 (cid:80)d k=1 (Ri,k(Rj,k Ri,k) Rj Ri2 2RiRj Ri = Rj Ri 2Ri . The term on the right-hand side of the final inequality is the value of ϵ we used in the main text. This choice ensures that the perturbed R[i] always remains closer in Euclidean distance to R[i] than to any other R[j = i]."
        },
        {
            "title": "B Efficiency analysis",
            "content": "Appendix B.1 analyzes the ideal FLOPs cost breakdown of the vanilla MoE, as well as the overhead introduced by AoE and ERC loss. Appendix B.2 discusses efficiency with consideration of the multiple parallelism strategies used in real-world MoE pre-training. Both analyses demonstrate the practicality of our method. B.1 FLOPs cost breakdown of three methods MoE forward Each expert in MoE layer involves the following operations, with their respective FLOP counts: Two matrix multiplications of dimension with D, accounting for 4T dD FLOPs. These correspond to the linear transformations parameterized by Wg and Wp. One element-wise multiplication of tensors and one SiLU activation applied to tensor. The computational cost of these operations is negligible compared to the matrix multiplications. One matrix multiplication of dimension with d, contributing 2T Dd FLOPs. This corresponds to the linear transformation parameterized by Wo. Summing these components gives total of 6T dD FLOPs per expert. For experts processing tokens, the total computational cost is therefore 6KT dD FLOPs. Computational overhead of AoE AoE factorizes the expert matrix Wg RDd into two low-rank matrices of rank r. To maintain the same number of parameters as the original matrix, we require dr + Dr = Dd, which gives = Dd d+D . The change in FLOPs compared to an MoE is: 2ndr (cid:124) (cid:123)(cid:122) (cid:125) All experts use Wdown + 2KDr (cid:124) (cid:123)(cid:122) (cid:125) Top-K experts use Wup 2KDd (cid:124) (cid:123)(cid:122) (cid:125) Top-K experts use original Wg 14 , where is the number of tokens. Substituting the value of and simplifying leads to an extra computational cost of: 2T (n K)dr. Computational overhead of expert-router coupling loss operating on tensors of shape 1 and D. In total, this results in 2n2Dd extra FLOPs. It introduces n2 matrix multiplications, each B.2 Throughputs under multiple parallelism strategies We now assess the overhead of the ERC loss in realistic large-scale pre-training setup that employs both data parallelism (DP) and expert parallelism (EP). As derived in our previous analysis, the computational cost of the ERC loss is equivalent to forward pass on n2/3 tokens. When distributed across devices, the costs are: Base MoE forward: / dp_size ERC overhead: (n / ep_size) / 3 Consider training our 15B-parameter model with the configuration: = 8, = 3 106, = 256, dp_size = 64, and ep_size = 8. In this scenario, the ERC overhead constitutes mere 0.72% of the base models forward pass cost. This theoretical estimate is consistent with our empirical measurements: we observe throughput of 62.03B tokens/day for the baseline versus 61.52B tokens/day for our model, representing only 0.82% reduction. With smaller = 64, as in our 3B models trained with dp_size=32 and ep_size=1 (i.e., EP disabled), the overhead ratio drops further to 0.18%. This analysis confirms the practical efficiency of our method."
        },
        {
            "title": "C Ablation Studies",
            "content": "C.1 Computing with different activations We considered five candidates for calculating : using the norms of (a) RWg, (b) RWp, (c) SiLU( RWg), (d) the post-SwiGLU activations (i.e., SiLU( RWg) RWp), and (e) experts final outputs (i.e., (SiLU( RWg) RWp)Wo). As shown in Figure 6 C.1, RWg is the most effective among all alternatives. While using the final output achieves comparable performance, it incurs higher cost. We therefore adopt RWg as our default choice. C.2 Random noise δ enables the generalization of coupling The random noise δ allows R[i] to better capture the samples within Xi. To validate its importance, we conducted an ablation study where we trained an MoE with the ERC loss but removed δ. Specifically, we computed directly using the original instead of the noise-augmented R. As shown in Figure 6 C.2, removing δ greatly degrades performance. This is because the coupling between routers and experts becomes overfitted to R, failing to generalize to the real inputs that R[i]s represent. C.3 Comparison with contrastive regularization solely on routers The router orthogonalization loss [1] requires ˆR (the row-wise normalization of R) to satisfy: ˆR ˆR = I. As shown in Figure 6 C.3, the orthogonalization loss yields only limited gains. We attribute this to our finding that the router embeddings in our baseline MoE model are already nearly orthogonal, with an average absolute cosine similarity of 0.15. This value corresponds to angles between router embeddings mostly ranging from arccos(0.15) = 81 to arccos(-0.15) = 99. Notably, we do not imply that all MoEs always have nearly orthogonal router embeddings, as this may depend on the data or specific architecture; we report this only as characteristic of our models, which explains the limited gains from the orthogonalization loss. 15 Figure 6 Results of ablation studies C.1, C.2, C.3 and C.4. For detailed task-specific results, please refer to Figure 8. This result further demonstrates that weak coupling between routers and experts is more critical issue than imperfect orthogonality in router embeddings. The significant gains from ERC, even when applied to baseline with already near-orthogonal routers, provide clear evidence. Furthermore, it is important to note that even if both routers and experts are orthogonalized, there is no guarantee that each R[i] will be aligned with . Therefore, the ERC loss cannot be reduced to contrastive techniques applied individually to routers or experts, such as orthogonalization loss. C.4 What happens if α > 1? Some readers might be interested in the value of α at which the ERC loss degenerates to no effective constraints, and the trained model consequently degenerates to vanilla MoE. For our baseline MoE, we seek the minimum α that zeros the ERC loss computed from the matrices of the last checkpoint. Table 2 shows that achieving zero ERC loss across all layers requires α = 5 in our pre-trained vanilla MoE baseline. This provides direct evidence that the router-expert coupling in the vanilla MoE is very weak. We further pre-trained 3B MoE models with the ERC loss at α values of 2 and 3. It is important to note that this experiment is to only demonstrate the effects of loosening the ERC constraints. We do not recommend using α > 1 in practice, as it contradicts our core motivation: the router and experts will shift from state of no mismatch toward looser coupling constraints, ultimately causing the model to degenerate into vanilla 16 Table 2 Post-hoc ERC loss evaluation of the vanilla MoE across α values. For clear and concise demonstration, the loss values in this table are computed using the original rather than R, making the results deterministic. Layer 0 1 3 4 5 6 7 9 10 11 Value of α 3 2 0.69 0.28 0.19 0.15 0.08 0. 0.15 0.13 0.05 0.00 0.09 0. 0.26 0.10 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 1 0.87 0.42 0.45 0.25 0. 0.24 0.22 0.21 0.15 0.16 0. 0.50 5 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 MoE. As shown in Figure 6 C.4, the model with α = 2 yields only limited improvement, while the model with α = 3 shows almost no improvement over the vanilla MoE. C.5 Do models reduce ERC loss through manipulating parameter norms? Some readers might assume that simply increasing or decreasing the norms of certain router embeddings or experts will increase the diagonal entries of , thereby reducing the ERC loss. Below, we (1) explain that any attempt to reduce one term of the ERC loss by manipulating norms will simultaneously increase other terms, and (2) present detailed parameter norms as evidence. The term [i, j] can be expressed as R[i]W Scaling up i decreases the loss from i-th row in (as the second term below increases): cos θi,j, where θi,j denotes the angle between R[i] and . However, simultaneously, it increases the loss term from every = rows (as the first term below increases): R[i]W cos θi,j R[i]W g cos θi,i R[j]W cos θj,i R[j]W cos θj,j. This logic is symmetric: any attempt to manipulate the norms of or Wg (whether increasing or decreasing them) to reduce one part of the loss will increase another. This property ensures that the overall ERC loss is minimized only when the router embedding norms are kept comparable and meaningful coupling is established between routers and their experts. As shown in the first four columns of Table 3, the average parameter norms for models trained with and without the ERC loss are comparable. Meanwhile, the lower standard deviation under the ERC loss reflects more consistent norms across both router embeddings and experts. In the last two columns of the table, we present the ERC loss for each model. The ERC loss is significantly higher in the baseline model despite its similar average parameter norms. 17 Table 3 The first four columns show parameter norms for models trained with and without ERC loss, while the last two show the corresponding layer-wise ERC loss. These results show that MoE + LERC learns meaningful coupling, rather than trivially minimizing the loss through norm manipulation. All values are evaluated on the last checkpoint. Layer 0 1 3 4 5 6 7 9 10 11 R[i] g Baseline 1.850.39 1.250.13 1.170.12 1.100.08 1.030.08 0.930.08 0.860.08 0.820.07 0.770.06 0.800.07 0.740.08 0.800.14 +LERC 1.670.31 1.130.12 1.070.09 1.010.07 0.890.05 0.870.06 0.830.07 0.750.06 0.760.06 0.740.06 0.690.06 0.730.10 Baseline 25.463.93 30.140.68 30.630.77 30.180.77 30.591.21 30.331.13 30.651.15 30.561.20 30.461.02 30.580.88 30.801.03 32.031.46 +LERC 24.143.02 29.420.69 29.880.76 29.420.78 29.881.09 29.861.06 29.821.11 29.961.16 29.820.88 29.860.79 30.160.89 31.501.26 LERC Values Baseline +LERC 0.87 0.42 0.45 0.25 0.28 0. 0.22 0.21 0.15 0.16 0.21 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 18 1 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 33 34 36 37 38 39 40 42 43 44 45 46 48 49 50 51 52 import torch import torch . nn as nn import eu doE xpe rtC las class MoE ( nn . Module ) : def __init__ ( self , args ) : super () . __init__ () self . experts = Ps eud oE xpe rtC as ( args ) self . = torch . nn . Parameter ( torch . empty ( args .n , args . ) ) self . alpha = args . alpha def erc_loss ( self , ) : row_diff = ( - self . alpha * torch . diag ( ) . unsqueeze (1) ) row_diff_clamped = torch . clamp ( row_diff , min =0.0) col_diff = ( - self . alpha * torch . diag ( ) . unsqueeze (0) ) col_diff_clamped = torch . clamp ( col_diff , min =0.0) mask = torch . ones_like ( ) - torch . eye ( . size (0) , device = . device ) total_diff = ( row_diff_clamped + col_diff_clamped ) * mask return total_diff . mean () def ge t_noisy_router ( self , ) : with torch . no_grad () : norm_R = torch . norm (R , dim =1) distances = torch . cdist (R , , =2) distances . fill_diagonal_ ( float ( inf ) ) min_dist , _ = torch . min ( distances , dim =1) eps = min_dist / 2 / norm_R low = (1 - eps ) . unsqueeze (1) high = (1 + eps ) . unsqueeze (1) noise = torch . rand_like ( ) return ( low + noise * ( high - low ) ) * def forward ( self , ) : erc_loss = 0.0 if self . training : = self . get_noisy_router ( self . ) = torch . norm ( torch . einsum ( jDd , id - > ijD , self . experts . Wg , ) , dim = -1) erc_loss = self . erc_loss ( ) logits = . view ( -1 , . shape [ -1]) @ self . . scores = logits . softmax ( dim = -1) expert_weights , expert_indices = torch . topk ( scores , dim = -1) return self . experts (x , expert_weights , expert_indices ) , erc_loss Figure 7 Pseudo code for expert-router coupling loss in PyTorch. 19 Figure 8 Task-specific downstream results for previous experiments."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Renmin University of China"
    ]
}