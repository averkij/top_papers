{
    "paper_title": "Self-Steering Language Models",
    "authors": [
        "Gabriel Grand",
        "Joshua B. Tenenbaum",
        "Vikash K. Mansinghka",
        "Alexander K. Lew",
        "Jacob Andreas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 8 0 7 0 . 4 0 5 2 : r Preprint. Under review. Self-Steering Language Models Gabriel Grand1 Joshua B. Tenenbaum1 Vikash K. Mansinghka1 Alexander K. Lew1,2 Jacob Andreas1 1Massachusetts Institute of Technology {gg, jbt, vkm, jda}@mit.edu 2Yale University alexander.lew@yale.edu"
        },
        {
            "title": "Abstract",
            "content": "While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve problem, they often excel at describing its abstract structureboth how to verify solutions and how to search for them. This paper introduces DISCIPL, method for self-steering LMs where Planner model generates task-specific inference program that is executed by population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with small Follower (e.g., Llama-3.2-1B), DISCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs."
        },
        {
            "title": "Introduction",
            "content": "Even as language models (LMs) are becoming increasingly proficient reasoners, progress has been jagged (Karpathy, 2024; Roose, 2025): todays frontier models surpass experts at science and math reasoning (e.g., Hendrycks et al., 2021; Rein et al., 2023; Wang et al., 2024b) yet still routinely struggle with counting, arithmetic, tic-tac-toe, metered poetry, and other intuitively simple tasks (Ball et al., 2024; McCoy et al., 2023; Xu & Ma, 2024). For example, even very capable LMs have difficulty writing coherent sentence under the constraints in Fig. 1, which are manageable for most proficient English speakers. There is growing consensus that many kinds of queries require more thoughtful, deliberate System-2 reasoning. However, key question is how best to leverage computation at test-time. One currently popular approach induces in-context reasoning via long chains-ofthought (DeepSeek-AI et al., 2025; OpenAI, 2024b). This approach is very flexible, allowing the LM to decide on problem-by-problem basis how to structure its thinking. But reasoning via (serial) autoregressive generation is costly, slow, and can still produce unreliable outputs. On the other hand, structured inference methods like tree search (Silver et al., 2016; Yao et al., 2023) and sequential Monte Carlo (Lew et al., 2023; Loula et al., 2025; Zhao et al., 2024) attain better parallelism and efficiency by coordinating test-time computation via external algorithms. However, because these approaches typically require pre-defined scorers or verifiers, and rely on LMs to produce correct outputs with non-negligible probability, their applications have so far been restricted to specific domains. In this work, we propose new meta-reasoning framework called DISCIPL in which language models themselves drive the decisions for how to structure inference-time compute. In our approach, Planner LM is fed the users query and asked to generate an ad-hoc specification (encoding its understanding of the task requirements) and inference procedure (encoding its plan for how to solve the task). Importantly, this plan is implemented as an inference program that invokes Follower LMs, either generatively or as likelihood evalu1 Preprint. Under review. Figure 1: Self-steering language models with probabilistic programs. LMs struggle with problems that combine generation and reasoning under constraints (top left, task from Yao et al., 2024). Popular approaches (top right) scale inference by search or sampling; however, even costly reasoning models do not always yield correct or fluent results (CoT example from o1, single-shot, first attempt). In our method (DISCIPL, bottom), Planner LM writes an inference program that defines step-by-step computations to steer population of Follower LMs. Our approach combines the benefits of serial and parallel methods: the Planner ensures correctness by construction, while the Followers collectively search for sequences with high probability. (See Fig. 6 for detailed visualization.) ators. By decomposing reasoning into planning and execution, our architecture preserves flexibility while enabling orchestration of highly efficient, parallel search patterns. To test this approach, we evaluate DISCIPL on two domains: (1) COLLIE (Yao et al., 2024), challenging constrained generation benchmark on which even very large LMs perform unreliably; and (2) PUZZLES, custom dataset of difficult tasks involving poetry composition, grant-writing, budgeting, and itinerary planning. We instantiate DISCIPL using capable Planner LM (GPT-4o prompted with few-shot examples) to generate code, and small Follower LM (Llama-3.2-1B) to execute it. On paragraph tasks, this approach boosts accuracy well beyond the original capabilities of the Follower. Meanwhile, on more tightly constrained sentence and puzzle tasks, DISCIPL enables the Follower to surpass the Planner and approach the performance of powerful reasoning models like o1. Our method builds on the growing body of recent work showing that the effectiveness of LMs can be dramatically improved with task-specific search and sampling methods. By using LMs to define these procedures on-the-fly, DISCIPL provides efficient, fully-automated solutions to problems that would otherwise require significant computational overhead or manual engineering effort. More broadly, this work offers new way to combine code generation and probabilistic inference to enable test-time scaling for LMs."
        },
        {
            "title": "2 Related Work",
            "content": "Scaling Inference-Time Computation Reasoning via autoregressive generation is effective for many tasks, and the success of chain-of-thought (Nye et al., 2021; Wei et al., 2023) has spawned many variants that trade off generality and efficiency: whereas constructing 2 Preprint. Under review. an explicit tree-of-thought (Liu et al., 2024; Yao et al., 2023) requires problem-specific engineering, linearizing reasoning into one long stream-of-search (Gandhi et al., 2024; Lehnert et al., 2024) scales exponentially with tree depth. As an alternative, simple sampling procedures like best-of-N sampling (Brown et al., 2024; Cobbe et al., 2021) and selfconsistency/majority voting (Wang et al., 2023a) have also emerged as popular means of scaling LM performance. These embarrassingly parallel methods (Herlihy & Shavit, 2012) are simple to implement, but determining the optimal sample budget for new problem requires special techniques to estimate expected task difficulty (i.e., adaptive sampling; Damani et al., 2024; Snell et al., 2024). Self-Improvement. Recently, variety of novel methods have been proposed for using LMs to optimize prompts (Fernando et al., 2023; Honovich et al., 2022; Khattab et al., 2023; Shinn et al., 2023; Yang et al., 2024; Zhou et al., 2023c), agentic systems (Hu et al., 2024), and optimization procedures themselves (Zelikman et al., 2024). DISCIPL shares similar recursive flavor, but generates ad-hoc inference algorithms that provide token-level control over LM behavior at decoding time with probabilistic guarantees. Constrained Generation. Recent years have seen proliferation of benchmarks designed to test the ability of LMs to adhere to complex and compositional constraints (Chia et al., 2023; Jiang et al., 2023; Lin et al., 2020; Sun et al., 2023; Wang et al., 2022; Yao et al., 2024; Zhou et al., 2023a). Prior approaches have focused on developing decoding algorithms for specific classes of constraints (Hokamp & Liu, 2017; Koo et al., 2024; Lu et al., 2021; 2022; Poesia et al., 2022; Post & Vilar, 2018; Ugare et al., 2024; Willard & Louf, 2023) or guiding generation with neural models (Amini et al., 2024; Kumar et al., 2022; Li et al., 2022; Qin et al., 2022). Various learning-based approaches have also been proposed, including selfcorrection through feedback (Welleck et al., 2022) and bootstrapping from self-generated instructions (Wang et al., 2023b; Zhou et al., 2023b). Sequential Monte Carlo with LMs. Particle-based methods, such as sequential Monte Carlo (SMC; Doucet et al., 2001), offer powerful framework for building adaptive inference-time search procedures. SMC has recently been applied to LMs to solve various tasks, including constrained generation (Lew et al., 2023; Loula et al., 2025; Zhao et al., 2024) and math reasoning (Feng et al., 2025; Puri et al., 2025). However, using SMC on new problems requires either engineering or learning various parameters of the algorithm (e.g., reward models or twist functions); in our work, an LM automates this process. Probabilistic Programming and LMs. Probabilistic programming languages (PPLs) allow users to implement probabilistic models as programs, and automate aspects of probabilistic inference (Goodman et al., 2014). Some PPLs support LMs as parts of models (Dohan et al., 2022; Lew et al., 2020; 2023), and LMs have also been used to generate probabilistic programs over symbolic domains (Li et al., 2024; Wong et al., 2023). More recently, PPLs have evolved to feature programmable inference (Mansinghka et al., 2014; 2018), so that programs can concisely specify both models and custom inference algorithms. We use LMs to generate code in LLAMPPL (Lew et al., 2023), PPL built on modern LM stack that enables users to define inference procedures via short, intuitive Python programs."
        },
        {
            "title": "3.1 General Framework",
            "content": "A language model is distribution on token sequences from vocabulary whose probability factorizes autoregressively as pM(x) = t=1 pM(xt x<t). We designate roles for two LMs, Planner MP and Follower MF. We assume that both support efficient conditional generation, and that MF also supports efficient conditional probability evaluation (i.e., access to logprobs). The key idea in DISCIPL is that the Planner can generate an inference program π in some language that describes how the Follower should be used to solve task. The program may make multiple asynchronous queries to the Follower, in both generative (i.e., sampling) and evaluative (i.e., probability computation) modes. The language provides an inference engine that takes program π L, Follower model MF, and an inference budget N, 3 Preprint. Under review. result εnull, 1 while result and do Algorithm 1 DISCIPL Outer Loop 1: function DISCIPL(task dtask, Planner MP, Follower MF, budget N, retries R, engine I) 2: 3: 4: 5: 6: 7: π pMP ( prompt(dtask, result)) Generate inference program (with prior errors as feedback) result I(π, MF, N) Execute the program with Follower if result then + 1 Initialize with empty string Retry on runtime errors return result and yields samples from distribution on results, which can either be answers or errors ε . With these ingredients, the basic DISCIPL algorithm (Alg. 1) proceeds as follows. Given natural language task description dtask, we first prompt the Planner MP to generate program in π suitable for solving the task. We then attempt to run π using the inference engine I. If inference succeeds, we return its output. Otherwise, we re-prompt MP to correct π, passing the error ε and associated traceback, up to max number of retries R."
        },
        {
            "title": "3.2 Our Instantiation: Language Model Probabilistic Programs",
            "content": "Language of inference programs. In our instantiation of DISCIPL, programs π are written in LLAMPPL (Lew et al., 2023), Python framework for probabilistic programming with language models. LLAMPPL programs work by repeatedly extending candidate generations by one or more tokens, and then scoring the proposed extensions. The programmer (in our case, the Planner model) can customize the process by which new extensions are proposed and scored; behind the scenes, LLAMPPLs inference engine automatically coordinates the overall search, maintaining multiple candidate generations in parallel, and dynamically reallocating computational resources to high-scoring partial completions. As concrete example, consider the program in Fig. 1, whose step method stochastically adds one word to the running generation, either sampling from MF (using sample) or forcing it to agree with word constraint (using observe). The observe method has the side effect of updating the candidates score, multiplying it by the probability the Follower assigns to the observed word. The illustration to the right of the program shows how these scores are used by the inference engine: when the word Glasgow is forced after the prefix The year begins, the candidate generations score plummets (due to the low probability of Glasgow), and the generation is culled. More generally, programs can use arbitrary logic to extend running completion and to multiply an update into that completions running score. These score updates can encode hard constraints (0 indicates violation), or soft constraints, based on symbolic heuristics, LM-as-judge schemes, or token probabilities under the Follower. They can also incorporate importance weights that correct for biases in the proposal process (see 3.3). Mathematical interpretation of inference programs. An inference program π provides blueprint for sequential inference algorithms that search for high-quality completions. Formally, we say that the goal of these algorithms is to generate samples from target distribution induced by the program that assigns high probability to high-scoring sequences. Let σ : (V R) denote step function that maps starting string to distribution σ(s) over updated strings and multiplicative score updates R.1 Then for fixed maximum generation length N, the target distribution is defined as P(s) (s 1,w1)σ(εnull),...,(s T,wT )σ(s T1) (cid:34) 1[s = s] (cid:35) wi . i=1 (1) Intuitively, this equation defines the target probability of string to be proportional to the probability of generating via repeated application of step, upweighted or downweighted according to the score accumulated while generating it. 1We assume that when is an EOS-terminated string, σ(s) is Dirac delta distribution at (s, 1). 4 Preprint. Under review. Figure 2: Inference programming patterns for self-steering, discussed in 3.3. Scalable test-time search via probabilistic inference. Our inference engine implements several general-purpose Monte Carlo methods for approximately sampling from the target distribution P. All methods support parallel scaling based on an inference budget N. Our experiments (4) evaluate three instantiations of DISCIPL. Importance Sampling (IS). Importance sampling generates full completions S1, . . . , SN in parallel, by initializing empty generations and repeatedly calling step until all have completed. For each candidate Si, the score updates from each step are multiplied to obtain an overall score Wi. Finally, one candidate is selected, with P(S = Si) = Wi/N j=1 Wj. When the target distribution is well-defined, this process converges to as . Sequential Monte Carlo (SMC). Like IS, SMC (Doucet et al., 2001, Alg. 2) initializes empty generations, called particles. Each particle is associated with weight that is initialized to 1. SMC alternates between (1) calling step on all particles in parallel, multiplying the returned score update into the particle weight; and (2) resampling to cull low-weight particles and replace them with copies of high-scoring particles. (All weights are reset to uniform after resampling.) Having multiple copies of promising particles allows each to be extended independently by the next call to step. This has the effect of adaptively reallocating the computation budget (N) to focus on promising candidates. As in IS, at the end of the algorithm, we select one of the particles to return, with probabilities proportional to the particle weights. Under mild technical conditions, SMC converges to as . Rejection Sampling (RS). When programs step method generates an entire completion and then computes binary score update, running (with budget N) reduces to rejection sampling (generating samples and checking whether any satisfy the constraint). We implement this pattern as DISCIPL-RS in 4."
        },
        {
            "title": "3.3 Common Patterns",
            "content": "Programs in DISCIPL adhere to several common inference patterns, for which we have implemented library support and which are also illustrated in the prompt to the Planner. Step-by-step problem decomposition. The Planner must decide how to decompose task into sequence of extend-and-score steps; this determines how often different candidates are compared and resampled. common pattern is to make each step extend by taskrelevant unit (e.g., line of poem, word of sentence with word-level constraints, etc.). Prior and proposal prompts. Imposing constraints can lead LMs to produce incoherent generations. For example, when prompted to generate sentence using the words dog, throw, and frisbee, small LMs yield semantically dubious completions like, Two dogs are throwing frisbees at each other (Lin et al., 2020). To promote coherency, programs can compensate for biases in the proposal distribution, which is aware of task-specific constraints, with scores from prior, which ensures fluency. The Planner defines the prior and proposal distributions via separate prompts. Typically, the prior prompt defines more general instructions, e.g., Write sentence that is grammatically correct and makes sense. Constrained generation with weight correction (Fig. 2A). In many situations, we might want the Follower to generate specific token sequences (e.g., Glasgow), or more generally, to adhere to formal constraints like regular expressions or grammars. The Planner can apply token masks that both enforce these constraints at generation time, and automat5 Preprint. Under review. ically incorporate importance weights that correct for the distortion in the LMs distribution resulting from the mask (Loula et al., 2025; Park et al., 2024). Self-hinting (Fig. 2B). Since the Planner controls the Followers proposal prompt, one powerful pattern is to dynamically update it to reflect stateful information relevant to the next generation step. We expose special hint() method that injects Note to self: {hint} into the Followers context, where the hint can include text as well as Python variables and objects. This technique functions as generalized calculator (Cobbe et al., 2021) that can perform arbitrary symbolic computations and pass their results to the Follower. Self-checking (Fig. 2C). While programs often ensure correctness by construction, some problems cannot be verified until generation is complete. In other cases, it may still be preferable to use guess-and-check over constrained generation, or to catch bugs in the inference logic. For this reason, the Planner defines distinguished check() method, which (like everything it generates) can make use of external libraries."
        },
        {
            "title": "4.1 Domains",
            "content": "Constrained generation. We evaluate DISCIPL on COLLIE-v1 (Yao et al., 2024), constrained generation benchmark designed as challenge dataset for LMs. Tasks in COLLIE  (Table 2)  are composed from formal grammar of constraints specified at multiple levels of text granularityhere, we focus on the sentence and paragraph levels. The combinatorial nature of the grammar is what makes COLLIE tasks challenging Yao et al. (2024) find that overall performance ranges from 16.3% (Alpaca-7B) to 50.9% (GPT-4). Puzzles. To evaluate generalization beyond tasks that can be defined with grammar, we construct mini-dataset of challenging naturalistic generation tasks. PUZZLES  (Table 5)  consists of four task types that require models to compose structured poetry, write grant proposal abstract subject to various constraints, generate an ingredients list that meets monetary budget, and plan multi-day travel itinerary."
        },
        {
            "title": "4.2 Evaluation metrics",
            "content": "Validity. We are interested in building systems that effectively leverage test-time compute to improve their answers to hard-to-answer queries. Accordingly, we focus on expected Pass@1, which (unlike Pass@k) models setting that does not assume access to an oracle verifier. We implement generalized version of the standard unbiased Pass@k estimator (Brown et al., 2024; Chen et al., 2021) that uses the weights computed during inference (if available) and uniform weights for baselines (A.4). Coherency. As highlighted in the example in Fig. 1, the introduction of constraints can impact the fluency of generated text. To assess coherency, we adopt the LLM-as-judge evaluation from Yao et al. (2024) using GPT-4o-mini with zero-shot prompt (A.5). While coherency could also be assessed with human ratings, this approach offers scalable, affordable, and reasonably reliable measure of the overall text quality."
        },
        {
            "title": "4.3 Experiment setup",
            "content": "Our experiments evaluate whether DISCIPL can generate effective and efficient inference programs for solving unseen tasks in stratified cross-validation setup. We begin by manually writing an example inference model for single instance of each task type in COLLIE and PUZZLES. We few-shot prompt the Planner LM with the examples from the corresponding domain, holding out the model corresponding to the target task type. (For PUZZLES, we also include the examples from COLLIE.) Planner and Follower LMs. The goal of the Planner is to generate an InferenceModel subclass that implements various methods, including step() and check(). We use system prompt in the style of README.md (A.10, which also serves as tutorial for the interested reader) to instruct to the Planner how to write inference models. We instantiate the Planner with capable LM (gpt-4o-2024-08-06, OpenAI, 2024a) that can attend to detailed in6 Preprint. Under review. structions; however, the Planner does not itself perform any reasoning outside generating Python code. We instantiate the Follower with Llama-3.2-1B-Instruct (Meta AI, 2024). Baselines. We benchmark DISCIPL against several baselines: Planner-only: We prompt GPT-4o to directly solve the task. For comparison, we also benchmark smaller model in the same family, GPT-4o-mini. We sample independent completions according to the sample budget. Follower-only: We run Llama-3.2-1B-Instruct times with temperature=1.0. Follower-only (beam search): We run Llama-3.2-1B-Instruct with stochastic beam search with beam size and temperature=1.0. Reasoning model: We benchmark against representative frontier CoT reasoning model (o1-2024-12-17). Following OpenAIs guidelines, we query o1 with max_tokens=25000. Due to cost, and because we expect this method to be near ceiling performance, we only query o1 once per task instance. Expert programs. We evaluate variant where we grant the Planner access to the reference implementation corresponding to the target task. This oracle condition (denoted DISCIPL*) evaluates the extent to which the Planner is able to recover the expert programs."
        },
        {
            "title": "5 Results and Analysis",
            "content": "We report the main results of our experiments in Table 1 with figures breaking down sentence  (Fig. 3)  , paragraph  (Fig. 7)  , and PUZZLES  (Fig. 8)  performance by task."
        },
        {
            "title": "5.1 Constraint satisfaction",
            "content": "The follower-only baseline is an unreliable instruction follower. Perhaps unsurprisingly given its size, Llama-1B is only weakly able to follow the task instructions, scoring poorly on COLLIE sentence tasks (Pass@1=0.04) and PUZZLES (Pass@1=0.08) and moderately on COLLIE paragraph (Pass@1=0.60) tasks. Notably, scaling inference compute via off-theshelf decoding methods (beam search) does not improve Llamas performance. We hypothesize that this is because beam search optimizes for sequence log probability, which is not well-correlated with the constrained generation objective. Planner-only baseline performance is task-dependent. Compared to Llama, GPT-4o and GPT-4o-mini are more responsive to the task instructions and significantly outperform the Follower-only baselines. Nevertheless, their ability to adhere to the constraints is taskdependent: while they are nearly perfect at including specific words (e.g., sent_04), they struggle to position keywords at locations other than the start of sentence (e.g., sent_02, para_05) and cannot reliably count characters (e.g., sent_01, sent_03). These results are in line with Yao et al. (2024), who found that GPT-4 performed poorly on these tasks. Reasoning model shows strong (though not perfect) performance. With the benefit of ample test-time compute, o1 achieves near-ceiling Pass@1 on COLLIE tasks. Nevertheless, Pass@1 for o1 dips below 1.0 on several tasks (sent_01, para_03, para_05, and ingredients_list). DISCIPL significantly boosts performance across domains. On all tasks, DISCIPL-SMC and DISCIPL-IS far exceed the Follower-only baselines, enabling various skills like character counting and word positioning that are entirely absent from Llama-3.2-1B. On paragraph tasks, DISCIPL closes most of the gap between the Follower and Planner performance, bringing Llama up to GPT-4o/GPT-4o-mini level  (Fig. 7)  . Meanwhile, on sentence tasks, DISCIPL surpasses both Followerand Planner-only baselines and approaches o1level performance  (Fig. 3)  . Finally, on PUZZLES, DISCIPL on average outperforms both Planner and Follower baselines, though performance varies between tasks and there is bigger gap between autogenerated and expert programs. DISCIPL knows when it is wrong. Across all tasks, autogenerated check() methods closely match ground truth verifiers  (Fig. 5)  . However, rejection sampling (RS) produces substantially fewer valid generations than combining step() and check() (SMC and IS). 7 Preprint. Under review. Figure 3: Validity on COLLIE Sentence-Level Tasks. (Top left) Average pass rate (Pass@1) with fixed inference budget (N=32) corresponding to the number of samples or particles. (Top right) Pass@1 under varying sample budget. While pass rates vary by task (bottom row), overall DISCIPL surpasses both the Follower and Planner baselines and approaches the performance strong reasoning model (o1). Moreover, the performance of autogenerated inference programs nearly matches that of expert programs (DISCIPL*)."
        },
        {
            "title": "Sampling Method Model",
            "content": "Pass@"
        },
        {
            "title": "Coherency",
            "content": "Pass@"
        },
        {
            "title": "Coherency",
            "content": "Pass@"
        },
        {
            "title": "DisCIPL",
            "content": "Follower-only SMC Importance (IS) Rejection (RS)"
        },
        {
            "title": "Standard\nBeam Search",
            "content": "Planner-only"
        },
        {
            "title": "Standard",
            "content": "o1 Llama-3.2-1B Llama-3.2-1B Llama-3.2-1B Llama-3.2-1B Llama-3.2-1B GPT-4o-mini GPT-4o 0.81 0.87 0.11 0.04 0. 0.35 0.24 0.95 5.71 5.30 8.61 9.00 9.32 9.30 9.08 7. 0.88 0.80 0.79 0.60 0.62 0.84 0.87 0.99 7.45 6.25 8.21 7.78 8. 9.18 9.27 8.82 0.42 0.38 0.25 0.08 0.03 0.27 0.25 0. 6.38 5.65 9.02 8.72 8.43 9.20 9.40 9.00 Table 1: Summary of results from all experiments. Pass@1 measures expected validity. Coherency (10-point scale) measures overall fluency for all generations (including invalid ones)."
        },
        {
            "title": "5.2 Coherency",
            "content": "What inference algorithms can we leverage to push the Pareto frontier of coherency/validity? Our results suggest that SMC  (Fig. 4)  is well-suited to this goalin our experiments, SMC consistently achieves higher coherency scores than IS for comparable Pass@1. One way of understanding this result is through Fig. 1, which illustrates how SMC resampling filters out particles that satisfy constraints but introduce disfluencies. This also helps to explain why on some tasks, the Pass@1 curves for DISCIPL-SMC appear relatively flat: in these cases, the inference programs ensure validity by construction, so the benefits of scaling show up instead in the coherency scores. While using separate proposal and prior (3.3) generally helps to ensure coherency, in some cases, it may backfire: for instance, on sent_04  (Fig. 3)  we observe that Pass@1 decreases with > 8. This is because under the prior, the target words have low probability (they are not treated as special), so naively resampling after each word will filter out particles containing target words. Although this issue is easily avoided by choosing step granularity that better aligns with the constraints (such as sampling multiple words until 8 Preprint. Under review. Figure 4: Coherency. (Left) SMC resampling leads to more coherent generations and scales with compute budget. (Right) Distributions of coherency scores on COLLIE sentence tasks (densities are normalized per-subplot). While non-reasoning LMs produce coherent but invalid text (red), DISCIPL enables Llama to satisfy constraints (green) with improving coherency as the particle count scales. (a) COLLIE Sentence Tasks (b) COLLIE Paragraph Tasks (c) PUZZLES Figure 5: Classification accuracy of autogenerated check() methods. Across domains, DISCIPL aligns closely with the ground truth verifiers. ( indicates null results due to program errors.) target is generated), in practice the Planner LM has strong inductive bias towards word-level step, even when provided an example of an expert program."
        },
        {
            "title": "6 Limitations, Future Work, and Conclusions",
            "content": "The results presented here are an early exploration of the broad framework outlined in 3. In this section, we acknowledge several limitations of the current instantiation of DISCIPL and highlight several promising directions for follow-up work. Generalization In addition to other constrained generation settings (e.g., Lin et al., 2020; Zhou et al., 2023a), self-steering can also be extended to mathematical reasoning (Lightman et al., 2023; Wang et al., 2024a) as well as domains with soft constraints (e.g., steering based on reward models). Inference algorithms While many text generation tasks are amenable to sequential inference, some problems may be more efficiently solved with backtracking (e.g., MCTS; Coulom, 2007; Kocsis & Szepesvári, 2006) or iterative editing (Welleck et al., 2022), both of which are implementable as extensions to DISCIPL. Self-improvement Since generating inference programs requires non-trivial reasoning, we instantiate the Planner with larger and more capable LM than the Follower. However, in principle, we could use the same LM to play these two roles. This recursive self-steering setup could learn by bootstrapping (e.g., Zelikman et al., 2022) or library learning (Ellis et al., 2021; Grand et al., 2024). In conclusion, in this work, we introduced DISCIPL, general framework for problemsolving with language models that orchestrates inference-time compute by writing and executing inference programs. We believe our approach offers unifying probabilistic perspective on inference-time computation with LMs, as well as practical tools for automating inference engineering. Our results demonstrate the potential of self-steering to enable accurate and efficient parallel inference with populations of small LMs, with performance rivaling much larger and more costly frontier models. In future work, we aim to generalize self-steering language models to new problem domains and inference patterns. Preprint. Under review."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Timothy ODonnell, João Loula, Ced Zhang, Cédric Colas, Noah Goodman, Aniruddha Nrusimha, and Linlu Qiu for helpful discussions and feedback. Special thanks to Ben LeBrun and the GenLM team for engineering support. The authors gratefully acknowledge support from the MIT Quest for Intelligence, the MITIBM Watson AI Lab, the Intel Corporation, AFOSR, DARPA, ONR, and the National Science Foundation (NSF) under grants CCF-2217064 and IIS-2238240. G.G. is supported by J.B.T. is supported by NSF Graduate Research Fellowship under Grant No. 2141064. AFOSR, the MIT Quest for Intelligence, the MIT-IBM Watson AI Lab, ONR Science of AI, and Siegel Family Endowment. V.K.M. and A.K.L. are supported by an anonymous philanthropic gift as well as gifts from Siegel Family Foundation that support the MIT Quest for Intelligence. J.A. is supported by Sloan Research Fellowship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of sponsors."
        },
        {
            "title": "Author Contributions",
            "content": "Gabriel Grand (primary author): Research conception, narrative development, software design and implementation, experiments, analysis of results, figure-making, writing. Joshua B. Tenenbaum: Senior mentorship, narrative development. Vikash K. Mansinghka: Senior mentorship, narrative development. Alexander K. Lew: Senior mentorship, research conception, narrative development, mathematical formalisms, analysis of results, figure-making, writing. Jacob Andreas: Senior mentorship, research conception, narrative development, analysis of results, writing. Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Afra Amini, Li Du, and Ryan Cotterell. Structured Voronoi Sampling, June 2024. URL http://arxiv.org/abs/2306.03061. arXiv:2306.03061 [cs]. Thomas Ball, Shuo Chen, and Cormac Herley. Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities, September 2024. URL http://arxiv.org/abs/ 2409.07638. arXiv:2409.07638 [cs]. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling, December 2024. URL http://arxiv.org/abs/2407.21787. arXiv:2407.21787 [cs]. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code, July 2021. URL http://arxiv.org/abs/ 2107.03374. arXiv:2107.03374 [cs]. Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models, June 2023. URL http://arxiv.org/abs/2306.04757. arXiv:2306.04757 [cs]. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs]. Rémi Coulom. Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search. In H. Jaap van den Herik, Paolo Ciancarini, and H. H. L. M. (Jeroen) Donkers (eds.), Computers and Games, pp. 7283, Berlin, Heidelberg, 2007. Springer. ISBN 978-3-54075538-8. doi: 10.1007/978-3-540-75538-8_7. Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, and Jacob Andreas. Learning How Hard to Think: Input-Adaptive Allocation of LM Computation, October 2024. URL https://arxiv.org/abs/2410.04707v1. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Preprint. Under review. Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, January 2025. URL http://arxiv.org/abs/2501.12948. arXiv:2501.12948 [cs]. David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and Charles Sutton. Language Model Cascades, July 2022. URL http://arxiv. org/abs/2207.10342. arXiv:2207.10342 [cs]. Arnaud Doucet, Nando de Freitas, and Neil Gordon. Sequential Monte Carlo Methods in Practice. Springer Science & Business Media, June 2001. ISBN 978-0-387-95146-1. Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sablé-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pp. 835850, Virtual Canada, June 2021. ACM. ISBN 978-1-4503-8391-2. doi: 10.1145/3453483.3454080. URL https://dl.acm.org/doi/10.1145/3453483.3454080. Shengyu Feng, Xiang Kong, Shuang Ma, Aonan Zhang, Dong Yin, Chong Wang, Ruoming Pang, and Yiming Yang. Step-by-Step Reasoning for Math Problems via Twisted Sequential Monte Carlo, March 2025. URL http://arxiv.org/abs/2410.01920. arXiv:2410.01920 [cs]. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution, September 2023. URL http://arxiv.org/abs/2309.16797. arXiv:2309.16797 [cs]. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D. Goodman. Stream of Search (SoS): Learning to Search in Language, April 2024. URL http://arxiv.org/abs/2404.03683. arXiv:2404.03683 [cs]. Noah Goodman, Vikash Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. Church: language for generative models, July 2014. URL http://arxiv.org/ abs/1206.3255. arXiv:1206.3255 [cs]. Gabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, and Jacob Andreas. LILO: Learning Interpretable Libraries by Compressing and Documenting Code, March 2024. URL http://arxiv.org/abs/2310.19791. arXiv:2310.19791 [cs]. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset, November 2021. URL http://arxiv.org/abs/2103.03874. arXiv:2103.03874 [cs]. 12 Preprint. Under review. Maurice Herlihy and Nir Shavit. The Art of Multiprocessor Programming, Revised Reprint. Elsevier, June 2012. ISBN 978-0-12-397795-3. Google-Books-ID: vfvPrSz7R7QC. Chris Hokamp and Qun Liu. Lexically Constrained Decoding for Sequence Generation In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of Using Grid Beam Search. the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15351546, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1141. URL https://aclanthology.org/P17-1141/. Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction Induction: From Few Examples to Natural Language Task Descriptions, May 2022. URL http: //arxiv.org/abs/2205.10782. arXiv:2205.10782 [cs]. Shengran Hu, Cong Lu, and Jeff Clune. Automated Design of Agentic Systems, August 2024. URL http://arxiv.org/abs/2408.08435. arXiv:2408.08435 [cs]. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. FollowBench: Multi-level Fine-grained Constraints Following Benchmark for Large Language Models, October 2023. URL https://arxiv.org/abs/2310.20410v3. Andrej Karpathy. Jagged Intelligence The word came up with to describe the (strange, unintuitive) fact that state of the art LLMs can both perform extremely impressive tasks (e.g. solve complex math problems) while simultaneously struggle with some very dumb problems. E.g. example from two https://t.co/3C7pCdBShQ, July 2024. URL https: //x.com/karpathy/status/1816531576228053133. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines, October 2023. URL http: //arxiv.org/abs/2310.03714. arXiv:2310.03714 [cs]. Levente Kocsis and Csaba Szepesvári. Bandit Based Monte-Carlo Planning. In Johannes Fürnkranz, Tobias Scheffer, and Myra Spiliopoulou (eds.), Machine Learning: ECML 2006, pp. 282293, Berlin, Heidelberg, 2006. Springer. ISBN 978-3-540-46056-5. doi: 10.1007/ 11871842_29. Terry Koo, Frederick Liu, and Luheng He. Automata-based constraints for language model decoding. 2024. Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based Constrained SamIn Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang pling from Language Models. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 22512277, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.144. URL https://aclanthology.org/2022.emnlp-main.144/. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention, September 2023. URL http:// arxiv.org/abs/2309.06180. arXiv:2309.06180 [cs]. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping, April 2024. URL http://arxiv.org/abs/2402.14083. arXiv:2402.14083 [cs]. Alexander Lew, Michael Henry Tessler, Vikash Mansinghka, and Joshua Tenenbaum. Leveraging Unstructured Statistical Knowledge in Probabilistic Language of Thought. Proceedings of the Annual Meeting of the Cognitive Science Society, pp. 7, 2020. 13 Preprint. Under review. Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash K. Mansinghka. Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs, June 2023. URL http://arxiv.org/abs/2306.03081. arXiv:2306.03081 [cs, stat]. Michael Y. Li, Emily B. Fox, and Noah D. Goodman. Automated Statistical Model Discovery with Language Models, June 2024. URL http://arxiv.org/abs/2402.17879. arXiv:2402.17879 [cs]. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. Diffusion-LM Improves Controllable Text Generation, May 2022. URL http://arxiv. org/abs/2205.14217. arXiv:2205.14217 [cs]. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. October 2023. URL https://openreview.net/forum?id=v8L0pN6EOi. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: Constrained Text Generation Challenge for Generative Commonsense Reasoning, November 2020. URL http://arxiv.org/abs/1911. 03705. arXiv:1911.03705 [cs]. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding, April 2024. URL http://arxiv. org/abs/2309.15028. arXiv:2309.15028 [cs]. João Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alexander K. Lew, Tim Vieira, and Timothy J. ODonnell. Syntactic and semantic conIn The thirteenth internatrol of large language models via sequential monte carlo. tional conference on learning representations, 2025. URL https://openreview.net/forum? id=xoXn62FzD0. Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 42884299, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. naacl-main.339. URL https://aclanthology.org/2021.naacl-main.339/. Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 780799, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.57. URL https://aclanthology.org/2022.naacl-main.57/. Vikash Mansinghka, Daniel Selsam, and Yura Perov. Venture: higher-order probabilistic programming platform with programmable inference, April 2014. URL http://arxiv. org/abs/1404.0099. arXiv:1404.0099 [cs]. Vikash K. Mansinghka, Ulrich Schaechtle, Shivam Handa, Alexey Radul, Yutian Chen, and Martin Rinard. Probabilistic programming with programmable inference. In Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 603616, Philadelphia PA USA, June 2018. ACM. ISBN 978-1-4503-5698-5. doi: 10. 1145/3192366.3192409. URL https://dl.acm.org/doi/10.1145/3192366.3192409. 14 Preprint. Under review. R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve, September 2023. URL http://arxiv.org/abs/2309.13638. arXiv:2309.13638 [cs]. Meta AI. Llama 3.2: tomizable models, September llama-3-2-connect-2024-vision-edge-mobile-devices/. Revolutionizing edge AI and vision with open, URL cushttps://ai.meta.com/blog/ 2024. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show Your Work: Scratchpads for Intermediate Computation with Language Models, November 2021. URL http://arxiv.org/abs/ 2112.00114. arXiv:2112.00114 [cs]. OpenAI. Hello GPT-4o, May 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. O1 System Card, December 2024b. o1-system-card-20241205.pdf. URL https://cdn.openai.com/ Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, and Loris DAntoni. Grammar-Aligned Decoding, November 2024. URL http://arxiv.org/abs/ 2405.21047. arXiv:2405.21047 [cs]. Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models, January 2022. URL http://arxiv.org/abs/2201.11227. arXiv:2201.11227 [cs]. Matt Post and David Vilar. Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1314 1324, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1119. URL https://aclanthology.org/N18-1119/. Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, and Akash Srivastava. Probabilistic Inference Approach to Inference-Time Scaling of LLMs using ParticleBased Monte Carlo Methods, February 2025. URL http://arxiv.org/abs/2502.01618. arXiv:2502.01618 [cs]. Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. COLD Decoding: Energybased Constrained Text Generation with Langevin Dynamics, October 2022. URL http: //arxiv.org/abs/2202.11705. arXiv:2202.11705 [cs]. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: Graduate-Level GoogleProof Q&A Benchmark, November 2023. URL http://arxiv.org/abs/2311.12022. arXiv:2311.12022 [cs]. Kevin Roose. When A.I. Passes This Test, Look Out. The New York Times, JanISSN 0362-4331. URL https://www.nytimes.com/2025/01/23/technology/ uary 2025. ai-test-humanitys-last-exam.html. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language Agents with Verbal Reinforcement Learning, June 2023. URL http://arxiv.org/abs/2303.11366. arXiv:2303.11366 [cs] version: 3. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Preprint. Under review. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484489, January 2016. ISSN 1476-4687. doi: 10.1038/nature16961. URL https://www.nature.com/articles/nature16961. Publisher: Nature Publishing Group. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, August 2024. URL http://arxiv.org/abs/2408.03314. arXiv:2408.03314 [cs]. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, and Xuezhe Ma. Evaluating Large Language Models on Controlled Generation Tasks, October 2023. URL http://arxiv.org/abs/2310.14542. arXiv:2310.14542 [cs]. Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. SynCode: LLM Generation with Grammar Augmentation, November 2024. URL http: //arxiv.org/abs/2403.01632. arXiv:2403.01632 [cs]. Tim Vieira, Ben LeBrun, Mario Giulianelli, Juan Luis Gastaldi, Brian DuSell, John Terilla, Timothy J. ODonnell, and Ryan Cotterell. From Language Models over Tokens to Language Models over Characters, December 2024. URL http://arxiv.org/abs/2412. 03719. arXiv:2412.03719 [cs]. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/2024. acl-long.510/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Self-Consistency Improves Chain of Thought ReaChowdhery, and Denny Zhou. soning in Language Models, March 2023a. URL http://arxiv.org/abs/2203.11171. arXiv:2203.11171 [cs]. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 50855109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340. URL https://aclanthology.org/2022. emnlp-main.340. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Models with Self-Generated Instructions, May 2023b. URL http://arxiv.org/abs/2212.10560. arXiv:2212.10560 [cs] version: 2. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark, November 2024b. URL http://arxiv.org/abs/2406.01574. arXiv:2406.01574 [cs]. Preprint. Under review. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, January 2023. URL http://arxiv.org/abs/2201.11903. arXiv:2201.11903 [cs]. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating Sequences by Learning to Self-Correct, October 2022. URL http://arxiv.org/abs/2211.00053. arXiv:2211.00053 [cs]. Brandon T. Willard and Rémi Louf. Efficient Guided Generation for Large Language Models, July 2023. URL http://arxiv.org/abs/2307.09702. arXiv:2307.09702 [cs]. Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka, Jacob Andreas, and Joshua B. Tenenbaum. From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought, June 2023. URL http://arxiv.org/abs/2306.12672. arXiv:2306.12672 [cs]. Nan Xu and Xuezhe Ma. LLM The Genius Paradox: Linguistic and Math Experts Struggle with Simple Word-based Counting Problems, October 2024. URL http://arxiv.org/ abs/2410.14166. arXiv:2410.14166 [cs]. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large Language Models as Optimizers, April 2024. URL http://arxiv. org/abs/2309.03409. arXiv:2309.03409 [cs]. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models, December 2023. URL http://arxiv.org/abs/2305.10601. arXiv:2305.10601 [cs]. Shunyu Yao, Howard Chen, Austin Hanjie, Runzhe Yang, and Karthik Narasimhan. COLLIE: Systematic Construction of Constrained Text Generation Tasks. ICLR, 2024. Eric Zelikman, Yuhuai Wu, STaR: Bootstrapping Reasoning With Reasoning, May 2022. URL http://arxiv.org/abs/2203.14465. arXiv:2203.14465 [cs]. Jesse Mu, and Noah D. Goodman. Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation, August 2024. URL http: //arxiv.org/abs/2310.02304. arXiv:2310.02304 [cs]. Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Grosse. Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo, April 2024. URL http://arxiv.org/abs/2404.17546. arXiv:2404.17546 [cs, stat]. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-Following Evaluation for Large Language Models, November 2023a. URL http://arxiv.org/abs/2311.07911. arXiv:2311.07911 [cs]. Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, and Mrinmaya Sachan. Controlled Text Generation with Natural Language Instructions, June 2023b. URL http://arxiv.org/abs/2304.14293. arXiv:2304.14293 [cs]. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large Language Models Are Human-Level Prompt Engineers, March 2023c. URL http://arxiv.org/abs/2211.01910. arXiv:2211.01910 [cs]. 17 Preprint. Under review."
        },
        {
            "title": "A Appendix",
            "content": "A.1 SMC Visualization Figure 6: Inference in action. Visualization of SMC inference for DISCIPL program for the COLLIE sent_02 task  (Fig. 1)  : Please generate sentence: 1) with exactly 18 words; 2) with the 4th, 8th, 11th words to be Glasgow, in, and respectively. Weights for = 16 particles are computed after each step and correspond intuitively to measure of fluency under the constraints. For instance, after Step 1, particles for which Glasgow is natural 4th word are propagated (e.g., The students at Glasgow), while others are filtered out (e.g., The big brown Glasgow). Each step corresponds to single constraint: the first three steps each generate until the next target word, and the final step generates until the 18th word. In this way, the inference program ensures validity by construction, while adaptive resampling via SMC selects for overall coherency. 18 Preprint. Under review. A.2 COLLIE Dataset and Results We evaluate on subset of 9/13 of the tasks in COLLIE-v1 (Yao et al., 2024) corresponding to the sentence and paragraph levels.2We use the task instances drawn from the Wikipedia corpus of COLLIE-v1. Since the number of instances varies significantly across task types, when computing aggregate metrics, we normalize with respect to task-level (sentence and paragraph), such that each task instance is treated as having been sampled from uniform prior over constraint types."
        },
        {
            "title": "N EXAMPLE PROMPT",
            "content": "sent_01 sent_02 sent_03 sent_04 para_01 para_02 para_ para_04 para_05 38 98 29 9 94 93 18 89 Please generate sentence with exactly 82 characters. Include whitespace into your character count. Please generate sentence: 1) with exactly 11 words; 2) with the 4th, 8th, 11th words to be Series, and, 4 respectively. Please generate sentence: 1) with at least 9 words; 2) with all words having at most 7 characters. Please generate sentence containing the word have, rising, the. Please generate paragraph with all sentences having the 1st word to be The. Please generate paragraph: 1) with exactly 3 sentences; 2) not containing the word be; 3) not containing the word this; 4) not containing the word is. Please generate paragraph: 1) with exactly 4 sentences; 2) with all sentences having at least 12 words; 3) with all sentences having at most 20 words. Please generate paragraph: 1) with at least 3 sentences; 2) with all sentences having at least 21 words. Please generate paragraph: 1) with exactly 3 sentences; 2) with sentences having the last word to be convention, president, Wisconsin respectively. Table 2: Summary of tasks in COLLIE-v1 used for our evaluation. SMC DisCIPL IS sent_01 sent_02 sent_ sent_04 Overall Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error 0.84 6.71 0.00 0.81 4.68 0.01 0.86 4.97 0.14 0.53 5.98 0.01 0.81 5.62 0. 0.97 6.95 0.00 0.87 3.53 0.04 0.86 4.55 0.14 0.64 5.33 0.12 0.87 5.30 0.08 RS 0.03 9.39 0.00 0.00 6.45 0.00 0.00 9.52 0.00 0.78 8.27 0.00 0.11 8.93 0.00 DisCIPL* (expert) IS SMC RS Follower-only Planner-only Llama3.2-1B +Beam GPT4omini GPT4o Reason o1 0.95 7.55 0.00 0.98 5.12 0.00 1.00 7.76 0. 0.67 6.27 0.10 0.94 7.17 0.01 1.00 6.58 0.00 0.98 4.02 0.01 1.00 5.86 0.00 0.65 6.38 0. 0.95 5.93 0.01 0.00 9.39 0.00 0.00 6.60 0.00 0.00 9.62 0.00 0.73 8.22 0.00 0.10 8.99 0. 0.00 9.24 0.00 0.00 6.61 0.00 0.00 9.55 0.00 0.27 8.32 0.00 0.04 8.92 0.00 0.00 9.71 0. 0.00 7.69 0.00 0.00 9.79 0.00 0.31 8.50 0.00 0.04 9.34 0.00 0.07 9.66 0.00 0.00 8.20 0. 0.50 9.62 0.00 0.89 8.76 0.00 0.35 9.34 0.00 0.07 9.16 0.00 0.03 7.93 0.00 0.21 9.24 0. 0.96 8.90 0.00 0.24 9.01 0.00 0.84 8.03 0.00 0.98 6.48 0.00 1.00 7.34 0.00 1.00 8.90 0. 0.95 7.66 0.00 Table 3: COLLIE Sentence-Level Results. 2While COLLIE also defines several word-level constraints, we found that, even with tools for fine-grained token masking, these present particular challenge for LMs due to the problem of token misalignment. While it is possible to recover next-character distributions from token-level LMs (e.g., Vieira et al., 2024), such approximations are expensive to compute. Instead, we focus on constraints at the sentence-level and higher that can be effectively expressed with token-level LMs. 19 Preprint. Under review. Figure 7: Validity on COLLIE Paragraph-Level Tasks. Figure structure is identical to COLLIE sentence-level tasks  (Fig. 3)  . Since generations are longer, the total sampling budget is limited to = 8. Baseline performance is higher for all models as these tasks appear to be more in-distribution for LMs. Nevertheless, we observe that DISCIPL helps to close the Pass@1 gap between the Followeronly (Llama-3.2-1B) and Planner-only (GPT-4o) baselines. SMC DisCIPL IS para_01 para_02 para_03 para_ para_05 Overall Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error 1.00 7.33 0.00 0.78 7.96 0.05 0.65 5.96 0.32 0.83 8.89 0. 0.20 3.75 0.01 0.88 7.52 0.02 0.89 5.67 0.11 0.79 7.59 0.09 0.62 5.03 0.38 0.78 8.00 0. 0.20 3.24 0.13 0.80 6.25 0.11 RS 0.89 7.78 0.00 0.91 9.20 0.00 0.48 8.81 0. 0.78 9.06 0.00 0.01 7.83 0.00 0.79 8.27 0.00 DisCIPL* (expert) IS SMC RS Follower-only Planner-only Llama3.2-1B +Beam GPT4omini GPT4o Reason o1 0.89 8.11 0.00 0.86 9.30 0.01 0.98 8.95 0.02 0.94 8.44 0.06 0.56 4.06 0. 0.89 8.08 0.02 0.89 8.11 0.11 0.98 9.18 0.02 0.97 7.81 0.03 0.78 7.28 0.22 0.49 3.47 0. 0.84 7.66 0.13 0.89 8.22 0.00 0.95 9.29 0.00 0.48 9.08 0.00 0.67 9.39 0.00 0.04 7.33 0. 0.76 8.60 0.00 0.88 7.00 0.00 0.68 9.01 0.00 0.10 9.05 0.00 0.24 8.78 0.00 0.00 7.69 0. 0.60 7.75 0.00 0.89 7.22 0.00 0.67 9.51 0.00 0.03 9.95 0.00 0.32 9.72 0.00 0.00 9.07 0. 0.62 8.29 0.00 0.99 9.44 0.00 0.88 9.79 0.00 0.73 9.77 0.00 0.66 9.33 0.00 0.33 7.08 0. 0.84 9.32 0.00 0.96 9.22 0.00 0.91 9.86 0.00 0.87 9.74 0.00 0.74 9.22 0.00 0.51 8.17 0. 0.87 9.22 0.00 1.00 8.89 0.00 1.00 8.91 0.00 0.94 9.24 0.00 1.00 9.00 0.00 0.83 6.67 0. 0.99 8.82 0.00 Table 4: COLLIE Paragraph-Level Results. 20 Preprint. Under review. A.3 Puzzles Dataset and Results The PUZZLES dataset consists of four task types with prompts given in Table 5. We re-run each method 10 times on each task type to produce the results in Table 6. TASK PROMPT DISCIPL-SMC Square Poem Write poem with 8 lines, where each line has exactly 8 words. The sun sets slowly in the evening sky Golden hues of orange and pink slowly fade The stars come out one by one tonight night of rest for the world outside The moon is full and bright and white beacon in the dark and endless night The world is quiet and still and deep In the stillness, all is calm and bright Write an abstract for grant proposal on elephant ecology and conservation. Your response should start with \"Abstract: <YOUR ABSTRACT>\". The abstract must be between 75-100 words. You are not allowed to use any of the following terms: conservation, sustainability, environment, ecology, wildlife, africa, asia, society, community, biodiversity, endangered, threatened, species, habitat, poaching, science, research. Please write an ingredients list for chocolate chip brownies with at most 7 ingredients costing less than $18.00 total. The list should be in dashed bullet point format starting with \"Ingredients:\". Each ingredient should be listed on separate line with the price given in USD. Grant Proposal Ingredients List Trip Itinerary Im planning 3-day trip to Singapore. Please write me detailed day-by-day itinerary that includes at least four activities per day. The itinerary should start with \"Day 1:\" and end with \"Day 3:\", with blank line between each day. Each activity should be listed on separate line starting with time range in 24-hour format in square brackets (for example, \"[11:00-13:00] Visit the Gardens by the Bay\"). Make sure to leave at least 9 hours of free time each day for rest. Abstract: Studying the complex behavior, social dynamics, and territorial boundaries of Indian elephant populations using camera trap images and GIS analysis reveals that strong elephant herds are consistently associated with increased access to food resources, particularly grasslands and aquatic habitats. These findings suggest that elephant social hierarchies play crucial role in allocating resources and protecting against intrusions by buffalo herds. Investigating the spatial overlap between elephant and buffalo habitats, results show that elephants prioritize areas with larger buffer zones around their food sources to minimize the risk of competing utilization. Ingredients: - 1 and 1/2 sticks of unsalted butter ($3.50) - 1 cup granulated sugar ($1.50) - 2 large eggs ($1.50) - 1/2 cup unsweetened cocoa powder ($1.50) - 1 teaspoon vanilla extract ($1.25) - 1 cup semi-sweet chocolate chips ($4.00) Day 1: [06:00-07:30] Start the day at Marina Bay Sands SkyPark for panoramic views of the city. [07:30-09:30] Take walk along the Marina Bay Waterfront Promenade for leisurely stroll and enjoy the scenic views of the city. [09:30-12:00] Visit the Esplanade - Theatres on the Bay for movie or performance. [12:00-13:30] Have lunch at nearby restaurant, such as Jumbo Seafood, and try some local seafood. [13:30-15:30] Visit the Singapore Botanic Gardens for relaxing walk and take in the beautiful gardens. [15:30-18:00] Explore the iconic Gardens by the Bay, learn about Singapores flora and fauna, and marvel at the stunning architecture. Day 2: [07:00-09:00] Start the day at the Singapore Zoo, home to over 2,800 animals from around the world. [09:00-11:00] Visit the nearby Universal Studios Singapore for an immersive theme park experience with rides and attractions. [11:00-12:30] Head to the iconic Merlion Park for short walk and visit the Merlion statue. [12:30-14:30] Have lunch at local eatery, such as Maxwell Food Centre, and try some hawker food. [14:30-16:30] Take short taxi ride to the Gardens by the Bay for visit to the Cloud Forest and Flower Dome. [16:30-18:00] Relax and unwind at the Gardens by the Bay with scenic walk. Day 3: [07:00-09:00] Visit the historic Fort Canning Park, former British military base turned public park. [09:00-11:00] Explore the nearby National Gallery Singapore, featuring diverse collection of Southeast Asian art. [11:00-12:30] Have lunch at nearby eatery, such as Tiong Bahru, and try some Singaporean cuisine. [12:30-14:30] Visit the nearby Little India and explore the vibrant streets of Chinatown and Little India. [14:30-16:30] Take short taxi ride to the iconic Marina Bay Sands and visit the rooftop bar for stunning views of the city. Table 5: Puzzles tasks and example generations. Samples were randomly selected from among the valid outputs produced by DISCIPL-SMC. 21 Preprint. Under review. Figure 8: Validity on PUZZLES. Pass@1 for fixed (top left) and varying (top right) sample budgets for four challenge tasks (bottom row). While DISCIPL still surpasses both Follower and Planner baselines, generation more often produces suboptimal programs, leading to bigger gap vs. DISCIPL*. SMC DisCIPL IS RS DisCIPL* (expert) IS SMC RS Planner-only Follower-only Llama3.21B +Beam GPT4omini GPT4o 0.98 9.00 0. 0.00 9.80 0.00 0.01 9.50 0.00 0.00 9.40 0.00 0.25 9.43 0.00 Reason o1 1.00 9.00 0. 0.40 8.80 0.00 0.90 8.90 0.00 1.00 9.30 0.00 0.82 9.00 0.00 Grant Proposal Ingredients List Square Poem Trip Itinerary Overall Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error Pass@1 Coherency Error 0.20 3.70 0.40 0.50 8.60 0.00 0.70 8.80 0.00 0.30 4.50 0. 0.42 6.40 0.18 0.10 3.70 0.40 0.70 8.40 0.00 0.60 6.20 0.10 0.10 4.30 0.20 0.38 5.65 0. 0.30 9.00 0.00 0.00 8.90 0.00 0.00 9.10 0.00 0.70 8.40 0.00 0.25 8.85 0.00 0.60 8.20 0. 1.00 9.10 0.00 0.70 7.90 0.00 0.50 6.80 0.20 0.70 8.00 0.05 0.60 7.70 0.00 1.00 8.50 0. 0.70 6.60 0.00 0.60 6.80 0.20 0.72 7.40 0.05 0.70 8.80 0.00 0.00 8.60 0.00 0.00 9.30 0. 0.90 7.90 0.00 0.40 8.65 0.00 0.07 9.00 0.00 0.01 9.60 0.00 0.00 9.00 0.00 0.25 8.60 0. 0.08 9.05 0.00 0.00 8.40 0.00 0.00 9.40 0.00 0.00 7.00 0.00 0.12 7.70 0.00 0.03 8.12 0. 0.98 8.80 0.00 0.06 8.50 0.00 0.00 9.00 0.00 0.05 9.10 0.00 0.27 8.85 0.00 Table 6: PUZZLES Results. 22 Preprint. Under review. A.4 Weighted Pass@1 We formalize the the weighted Pass@1 metric used to evaluate model performance. This metric offers natural way to incorporate sample-specific scores (e.g., particle weights wi) into the evaluation. The approach is scale-invariant, so the absolute scale of the weights does not affect the metric. Consider set of samples, indexed by = 1, . . . , N. For each sample, we define: log-probability wi R. In cases where wi is undefined (i.e., when generation produces null output), we set its corresponding weight to zero. binary pass indicator Ii = (cid:40) 1, 0, if sample passes, if sample fails."
        },
        {
            "title": "We define the unnormalized weight of sample i as",
            "content": "wi = exp(wi), where for methods that do not compute sample weights, we assume uniform weighting by setting wi = 1 for all i. Weighted Pass@1 is defined as the probability that single sampledrawn without replacement from the samples with probability proportional to wiis passing sample. Formally, this is given by Weighted Pass@1 = i:Ii=1 i=1 wi . wi This expression represents the fraction of the total weight that corresponds to passing samples, and it reduces to the standard (unweighted) Pass@1 when all weights are equal. A.5 Coherency Score To assess coherency, we adopt the LLM-as-judge evaluation from Yao et al. (2024) (A.5). Whereas the original evaluation used GPT-4, to reduce inference time and cost, we use GPT-4o-mini (gpt-4o-mini-2024-07-18). We make minimal modifications to the original prompt to encourage GPT-4o-mini to adhere to the desired format. Please concisely analyze the following text for coherency. Your analysis should be no longer than 3 sentences and it *must* end verbatim with 'The coherency score is <SCORE>', where <SCORE> is number between 1 and 10. 23 Preprint. Under review. A.6 Error analysis By learning from tracebacks, DISCIPL is able to correct roughly half of all initial runtime errors (Fig. 9, left). Nevertheless, even after 3 attempts, we observer error rates of 2-6% on COLLIE (A.2) and 18% on PUZZLES (A.3). Our analysis (Fig. 9, right) finds that most errors arise from invalid token masks and timeouts. Token mask errors occur when call to observe() rules out all possible next tokens (e.g., when attempting to generate punctuation when mask that rules out punctuation is already active). Meanwhile, timeout errors are more commonly associated with infinite loops or the omission of termination conditions in the step() logic. There are also cases where bugs in the generated inference programs yield incorrect outputs without triggering any errors. In A.7, we give examples of an off-by-one error (Code Block 1) and an accidental infinite loop (Code Block 2) and highlight how structural issues with the outputs can be traced back to these bugs. In future work, we aim to implement richer form of feedback where outputs from inference are provided to the Planner LM to correct for these kinds of errors."
        },
        {
            "title": "PUZZLES",
            "content": "Figure 9: Inference-time errors in DISCIPL-SMC. (Left) Proportion of valid/invalid text generated at each retry, with errors in gray. Roughly half of all initial errors (#1) are corrected with feedback (#2), with diminishing returns on remaining errors for subsequent retries. (Right) Breakdown of Top-20 most frequent error messages aggregated across all retries. 24 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 Preprint. Under review. A.7 Error Analysis: Examples Listing 1 Example of buggy program generated by the Planner LM for COLLIE sent_01 task instance. The generated program correctly applies MaxCharsMask() to produce string that is exactly 102 characters. However, lines 33-34 subsequently force the Follower LM to generate punctuation, resulting in generations that exceed the target length by single character. class InferenceModel(BaseModel): \"\"\"Generates sentence with exactly 102 characters, including whitespace.\"\"\" def __init__( self, context, max_tokens: int = 32, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.target_length = 102 async def step(self): \"\"\" Step granularity: token Generation strategy: At each step, sample token that fits within the remaining length. Once the target length is reached, force punctuation and end the sentence. End condition: Target length is reached or token limit is reached. \"\"\" # Sample token that fits within the remaining length. async with MaxCharsMask(self, max_chars=self.target_length - len(self)): token = await self.next_token() # If we've reached the target length, force punctuation and end the sentence if len(self) == self.target_length: async with PunctuationMask(self): await self.next_token() await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return 25 Preprint. Under review. Listing 2 Example of buggy program generated by the Planner LM for COLLIE para_05 task instance. The step() function contains while loop that samples words until the target sentence-ending word is generated. However, due to bug in the conditional at line 34, target words containing capital letters (e.g., Oxford, College) will never be matched. As result, the Follower LM will generate the first two sentences of the paragraph correctly and then loop indefinitely on the third sentence until it exceeds max_tokens. class InferenceModel(BaseModel): \"\"\"Generates paragraph with exactly 5 sentences, where each sentence ends with specific word.\"\"\" def __init__( self, context, max_tokens: int = 256, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.sentence_ends = [\"offices\", \"door\", \"Oxford\", \"College\", \"initially\"] self.sentence_count = async def step(self): \"\"\" Step granularity: sentence Generation strategy: At each step, sample sentence until the specific last word is reached. After generating the specific last word for sentence, enforce punctuation. End condition: All sentences are generated or token limit is reached. \"\"\" # Generate sentence until the specific last word is generated target_last_word = self.sentence_ends[self.sentence_count] while True: word = await self.next_word() # Check if the generated word is the target last word if word.strip().lower() == target_last_word: # Force punctuation after the target last word async with PunctuationMask(self): await self.next_token() break # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return self.sentence_count += # End generation after all sentences are produced if self.sentence_count >= len(self.sentence_ends): await self.end() return 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 26 Preprint. Under review. A."
        },
        {
            "title": "Inference Methods",
            "content": "We provide formal definition of the SMC algorithm in DISCIPL. Our implementation is adapted from the LLAMPPL with minor adjustments intended to provide robustness to bugs in autogenerated inference programs. In particular, we set maximum number of SMC steps = 1000 so that programs that fail to terminate do not run infinitely. We also invoke this method with wall-clock timeout (not shown) to interrupt infinite loops that might occur internally within single step(). Algorithm 2 Sequential Monte Carlo Inference Algorithm 1: function SMC(π, MF, N, ˆNess, T) 2: for = 1, . . . , do x(i) π(MF) for = 1, . . . , do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: await x.step() for in {x(1), x(2), . . . , x(N)} for = 1, . . . , do w(i) w(i) j=1 w(j) i=1(w(i))2 < ˆNess then (cid:18) a(i) Categorical j=1 w(j) , . . . , w(N) j=1 w(j) w(1) N N if 1 for = 1, . . . , do (x(i), w(i)) (x(a(i)), 1 if {x(1), x(2), . . . , x(N)}(x return {x(1), x(2), . . . , x(N)} j=1 w(j)) EOS) then (cid:19) Instantiate particles Advance particle state Normalize weights Compute effective sample size Resample Check if all particles have terminated"
        },
        {
            "title": "Implementation Details",
            "content": "A.9 LLAMPPL inference is performed with the vLLM backend, which uses PagedAttention (Kwon et al., 2023) for improved efficiency. For baselines, max_tokens is set to 32 for COLLIE sentences, 128 for COLLIE paragraphs, and 512 for PUZZLES. For DISCIPL, max_tokens is defined by the Planner LM and programs are executed with max of = 3 retries and variable timeout (120s for sentences, 240s for paragraphs, and 360s for puzzles). The max sampling budget for each domain is determined based on the number of tasks and the generation length; we use = 32 for sentences, = 8 for paragraphs, and = 16 for puzzles. 27 Preprint. Under review. A.10 System prompt We use single system prompt written in the style of README.md to demonstrate to the Planner LM how to write inference models. Since this prompt is written as condensed explanation of language model probabilistic programming, it also doubles as useful tutorial for the interested reader. We reproduce the prompt in its entirety below. You are an expert programmer. You are writing Python code to solve constrained generation tasks using language model probabilistic programming language (LLaMPPL). Please read this brief tutorial on LLaMPPL and write program to answer the users query. Overview LLaMPPL is research prototype for language model probabilistic programming: specifying language generation tasks by writing probabilistic programs that combine calls to LLMs, symbolic program logic, and probabilistic conditioning. To solve these tasks, LLaMPPL uses specialized sequential Monte Carlo inference algorithm. This repository (hfppl) implements LLaMPPL for use with HuggingFace Transformers. Modeling with LLaMPPL LLaMPPL program is subclass of the hfppl.Model class. from hfppl import Model, LMContext, CachedCausalLM # LLaMPPL model subclasses the Model class class InferenceModel(Model): # The __init__ method is used to process arguments # and initialize instance variables. def __init__(self, lm, prompt, forbidden_letter): super().__init__() # stateful context object for the LLM, initialized with the prompt self.context = LMContext(lm, prompt) self.eos_token = lm.tokenizer.eos_token_id # The forbidden letter self.forbidden_tokens = set(i for (i, v) in enumerate(lm.vocab) if forbidden_letter in v) # The step method is used to perform single 'step' of generation. # This might be single token, single phrase, or any other division. # Here, we generate one token at time. async def step(self): # Condition on the next token *not* being forbidden token. await self.observe(self.context.mask_dist(self.forbidden_tokens), False) # Sample the next token from the LLM -- automatically extends `self.context`. token = await self.sample(self.context.next_token()) # Check for EOS or end of sentence if token.token_id == self.eos_token or str(token) in ['.', '!', '?']: # Finish generation self.finish() # To improve performance, hint that `self.forbidden_tokens` is immutable def immutable_properties(self): return set(['forbidden_tokens']) The Model class provides number of useful methods for specifying LLaMPPL program: self.sample(dist[, proposal]) samples from the given distribution. Providing proposal does not modify the task description, but can improve inference. Here, for example, we use proposal that pre-emptively avoids the forbidden letter. self.condition(cond) conditions on the given Boolean expression. 28 Preprint. Under review. self.finish() indicates that generation is complete. self.observe(dist, obs) performs form of soft conditioning on the given distribution. It is equivalent to (but more efficient than) sampling value from dist and then immediately running condition(v == obs) . To run inference, we use the smc_standard method: import asyncio from hfppl import smc_standard # Initialize the HuggingFace model lm = CachedCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\") # Create model instance model = InferenceModel(lm, \"The weather today is expected to be\", \"e\") # Run inference particles = asyncio.run(smc_standard(model, 5)) # number of particles Sample output: sunny. sunny and cool. 34 (81F) in Chicago with winds at 5mph. 34 (81F) in Chicago with winds at 2-9 mph. hot and humid with possibility of rain, which is not uncommon for this part of Mississippi. Instructions Your goal is to implement an InferenceModel that encodes the users constraints. The BaseModel class To simplify writing InferenceModel classes, you can subclass the BaseModel class, which provides number of useful methods for text generation. class BaseModel(hfppl.Model): \"\"\"Base inference model.\"\"\" def __init__( self, context: LMContext, max_tokens: int = 32, ): super().__init__() self.context = context self.max_tokens = max_tokens self.tokenizer = context.lm.tokenizer self.EOS_TOKEN_ID = context.lm.tokenizer.eos_token_id @classmethod async def create( cls, lm, task_prompt: str = None, max_tokens: int = 32, temperature: float = 0.7, ): \"\"\"Async factory method to create the model.\"\"\" formatted_prompt = BaseModel.get_formatted_prompt( lm, user_prompt=cls.prior_prompt() ) context = await LMContext.create( lm=lm, prompt=formatted_prompt, temp=temperature, show_eos=False, 29 Preprint. Under review. ) model = cls(context, max_tokens) if task_prompt is not None: model.task_prompt = task_prompt formatted_task_prompt = BaseModel.get_formatted_prompt( lm, system_prompt=cls.system_prompt(), user_prompt=task_prompt ) model.proposal_context = await LMContext.create( lm=lm, prompt=formatted_task_prompt, temp=temperature, show_eos=False ) else: model.proposal_context = None return model @classmethod def system_prompt(cls): return ( \"You are helping user generate text that satisfies constraints. \" \"Follow the user's instructions exactly. Write your response below; \" \"do not preface your response or include any additional remarks.\" ) @classmethod def prior_prompt(cls): \"\"\"A task-agnostic prompt that will be used to evaluate the prior probability of the generation.\"\"\" raise NotImplementedError @staticmethod def get_formatted_prompt( lm, system_prompt: str = None, user_prompt: str = None, assistant_content: str = None, ): messages = BaseModel.get_chat_messages( system_prompt=system_prompt, user_prompt=user_prompt, ) formatted_prompt = lm.tokenizer.apply_chat_template( conversation=messages, tokenize=False, add_generation_prompt=True, ) if assistant_content is not None: formatted_prompt += assistant_content return formatted_prompt @staticmethod def get_chat_messages( system_prompt: str = None, user_prompt: str = None, system_role: str = \"system\", ): \"\"\"Returns list of messages in chat format.\"\"\" messages = [] if system_prompt is not None: messages += [ { } ] \"role\": system_role, \"content\": system_prompt, if user_prompt is not None: messages += [{\"role\": \"user\", \"content\": user_prompt}] return messages def immutable_properties(self): return set( [ \"max_tokens\", \"tokenizer\", \"EOS_TOKEN_ID\", ] ) def __str__(self): return str(self.context) def __len__(self): return len(str(self.context)) 30 Preprint. Under review. async def step(self): \"\"\"Implements single step of the generation process. NOTE: This method is task-specific and should be implemented by subclasses. See the examples for guidance. \"\"\" raise NotImplementedError async def check(self, text: str) -> bool: \"\"\"Implements checking procedure that will be run at the end of generation. Args: text (str): The generated text. Returns: bool: True if the text is valid, False otherwise. NOTE: This method is task-specific and should be implemented by subclasses. See the examples for guidance. \"\"\" raise NotImplementedError async def sample(self, dist: hfppl.Distribution): return await super().sample(dist, proposal=self.get_proposal_dist(dist)) async def observe(self, dist: hfppl.Distribution, x): if self.proposal_context is not None: await self.intervene(self.get_proposal_dist(dist), x) return await super().observe(dist, x) async def hint(self, hint_text: str): if self.proposal_context is None: return hint_prompt = BaseModel.get_formatted_prompt( lm=self.proposal_context.lm, system_prompt=self.system_prompt(), user_prompt=self.task_prompt + f\"nn(Note to self: {hint_text})\", assistant_content=str(self.context), ) hint_context = await LMContext.create( lm=self.proposal_context.lm, prompt=hint_prompt, temp=self.proposal_context.temp, show_eos=False, ) await self.intervene(hint_context.mask_dist(self.context.model_mask), True) self.proposal_context = hint_context def get_proposal_dist(self, dist: hfppl.Distribution): if self.proposal_context is None: return None if isinstance(dist, LMNextToken): return self.proposal_context.next_token() elif isinstance(dist, LMTokenMask): return self.proposal_context.mask_dist(dist.mask) elif isinstance(dist, NextTokenMask): return self.proposal_context.mask_dist(dist.mask.mask) else: raise ValueError(f\"Unsupported distribution type: {type(dist)}\") async def next_token(self) -> str: \"\"\"Generates single token. This method automatically extends the context with the generated token. Returns: token: hfppl.Token object representing the sampled token \"\"\" # NOTE: Yields control back to the event loop. Necessary to allow timeouts to work correctly when this (cid:44) await asyncio.sleep(0) return await self.sample(self.context.next_token()) method is called in loop. async def next_word( self, max_chars: int = None, ) -> str: \"\"\"Generates single word. This method automatically extends the context with the generated word. Args: max_chars (int): Maximum number of characters in the word. If None, the model will sample word of (cid:44) any length. 31 Preprint. Under review. Returns: word: The sampled word. \"\"\" await asyncio.sleep(0) # NOTE: This approach sometimes breaks with max_chars = 1 if max_chars is not None: assert max_chars > last_token = ( self.context.lm.str_vocab[self.context.tokens[-1]] if len(self.context.tokens) > 0 else \"\" ) last_character = last_token[-1] if len(last_token) > 0 else \"\" needs_space = ( last_character not in string.whitespace and last_character not in [ \"-\", \"'\", '\"', ] ) if needs_space: starts_word_mask = self.context.lm.masks.STARTS_NEW_WORD else: starts_word_mask = self.context.lm.masks.CONTINUES_CURRENT_WORD # Force model to start new word await self.observe(self.context.mask_dist(starts_word_mask), True) word = \"\" while True: # Force model to sample token with an appropriate number of characters if max_chars is not None: await self.observe( self.context.mask_dist( self.context.lm.masks.token_length_mask( max_chars=max_chars - len(word.strip()) ) ), True, ) token = await self.next_token() word += self.context.lm.str_vocab[token.token_id] # If we ran out of chars, break if max_chars is not None and len(word.strip()) >= max_chars: await self.observe( self.context.mask_dist( self.context.lm.masks.CONTINUES_CURRENT_WORD ), False, ) break # If the model wants to end the word, break if not ( await self.sample( self.context.mask_dist(self.context.lm.masks.CONTINUES_CURRENT_WORD) ) ): break # Optionally, sample mid-word punctuation (commas, colons, hyphens, quotes, etc.) if await self.sample( self.context.mask_dist(self.context.lm.masks.MID_PUNCTUATION) ): token = await self.next_token() word += self.context.lm.str_vocab[token.token_id] return word def _adjust_whitespace(self, text: str) -> str: \"\"\"Adds prefix space to the text if it does not already start with one. Does not add space if the context is empty or already ends with whitespace. Removes trailing whitespace from the text. 32 Preprint. Under review. Args: text (str): Text to add prefix space to. Returns: text: Text with prefix space added. \"\"\" context = str(self.context) # Add prefix space if ( not text.startswith(\" \") and not context.endswith(tuple(string.whitespace)) and len(context) > 0 ): text = \" \" + text # Ensure text does not end with whitespace, as this can cause issues with tokenization if text.endswith(tuple(string.whitespace)): print(f\"Warning: Removing trailing whitespace from text: {text}\") text = text.rstrip(string.whitespace) return text async def extend_with(self, text: str, add_prefix_space: bool = True) -> str: \"\"\"Extends the generation with pre-defined string literal. This method automatically extends the (cid:44) context with the input text. Args: text (str): String to extend the generation with. add_prefix_space (bool): Auto-add prefix space to text. In most cases, this should be left as True. Returns: text: The generated text (same as input). \"\"\" await asyncio.sleep(0) if add_prefix_space: text = self._adjust_whitespace(text) for token_id in self.tokenizer.encode(text, add_special_tokens=False): await self.observe(self.context.next_token(), token_id) return text async def extend( self, start: str = None, stop: List[str] = None, min_chars: int = None, max_chars: int = None, allow_eos: bool = True, add_prefix_space: bool = True, ) -> Tuple[str, bool]: \"\"\"Extends the generation with new string. This method automatically extends the context with the (cid:44) generated text. Args: start (str): String to start the generation with. stop (List[str]): List of strings to stop the generation at. min_chars (int): Minimum number of characters to generate. max_chars (int): Maximum number of characters to generate. allow_eos (bool): Allow EOS token to be generated. add_prefix_space (bool): Auto-add prefix space to text. In most cases, this should be left as True. Returns: new_text: The generated text. eos: Whether the generation was stopped by an EOS token. \"\"\" await asyncio.sleep(0) assert isinstance(start, (str, type(None))) assert isinstance(stop, (list, str, type(None))) assert isinstance(max_chars, (int, type(None))) assert isinstance(min_chars, (int, type(None))) if max_chars and min_chars: assert max_chars >= min_chars if isinstance(stop, str): stop = [stop] old_text = str(self.context) 33 Preprint. Under review. if start is not None: start = await self.extend_with(start, add_prefix_space=add_prefix_space) new_text = start or \"\" eos = False for _ in range(self.context.token_count, self.max_tokens): async with TokenLengthMask( self, max_chars=max_chars - len(new_text) if max_chars is not None else None, allow_eos=allow_eos and len(new_text) >= (min_chars or 0), ): token = await self.next_token() new_text = str(self.context)[len(old_text) :] # Stop on EOS token. if int(token) == self.EOS_TOKEN_ID: eos = True break # Stop on character limit. if max_chars is not None and len(new_text) >= max_chars: break # Stop on any stop string. if ( stop is not None and any([s in new_text for in stop]) and len(new_text) >= (min_chars or 0) ): break return new_text, eos async def end(self): \"\"\"Mark the generation as finished. This method automatically appends an EOS token if it has not already (cid:44) if self.context.tokens[-1] != self.EOS_TOKEN_ID: been generated.\"\"\" async with EOSMask(self): await self.next_token() self.finish() The check() method This method implements static Boolean checking procedure that is automatically run once at the end of generation to verify that the generated text satisfies the constraints. Think of check() as unit test that ensures that the InferenceModel is working correctly. While its useful for preventing false positives, if we only had check() by itself, then we would have to rely on guess-and-check. To achieve better efficiency while ensuring that outputs are correct by construction, we also need to write sampling procedure, which is defined by the step() method. The step() method The core logic of the InferenceModel is the step() method, which is called iteratively to generate string step-by-step via Sequential Monte Carlo sampling. The definition of step is problem-specific it can be token, word, multi-word phrase, line of poetry, sentence, etc. This method can also call subroutines to invoke different types of steps at different times. Step-by-step inference The step function is called repeatedly as part of smc_standard() . Internally, the loop looks like this (with some details omitted for clarity): async def smc_standard( model, n_particles: int, ess_threshold: float = 0.5, 34 Preprint. Under review. ): # Initialize the particles particles = [copy.deepcopy(model) for _ in range(n_particles)] # Keep stepping until all particles are done while any(map(lambda p: not p.done_stepping(), particles)): # Step each particle await asyncio.gather(*[p.step() for in particles if not p.done_stepping()]) # Resample according to normalized particle weights if ess < ess_threshold: ancestor_indices = [ np.random.choice(range(len(particles)), p=weights) for _ in range(n_particles) ] particles = [copy.deepcopy(particles[i]) for in ancestor_indices] return particles"
        },
        {
            "title": "Documenting design choices",
            "content": "There are several key design choices writing step function that affect the accuracy and efficiency of inference. Each step() function is accompanied by docstring that encourages the developer to consider these in their implementation. Suppose the task is to write sentence that includes at least three words from list of target words. The step function docstring might look like this. async def step(self): \"\"\" Generation strategy: Each step is going to generate one of the target words. At each step, keep sampling words until target word is generated. As soon as target word is generated, `return` to complete the step. On the final step (after at least 3 target words have been generated), extend the sentence unconstrained until EOS. Step granularity: phrase End condition: At least 3 target words have been generated or token limit is reached. \"\"\""
        },
        {
            "title": "Step granularity",
            "content": "The step granularity describes how much text is generated at each call of the step function. Common step granularities include: token: each step generates single token word: each step generates single word phrase: each step generates multiple words line: each step generates until the newline character \"n\" sentence: each step generates complete sentence ending in punctuation Because SMC resampling occurs after each step, its important to choose the right step granularity. If the granularity is too small, each step may have mix of particles at different points towards the solution. Since satisfying constraint often results in lower probability under the prior, during resampling, particles that are farther along may unintentionally be filtered out when compared against particles that incorporate fewer constraints. On the other hand, if the granularity is too large, SMC will not be able to properly reallocate weights to more promising particles. [!IMPORTANT] When choosing step granularity, rule of thumb is that step should encapsulate single coherent chunk of generation that 35 Preprint. Under review. satisfies constraint or makes concrete progress towards the overall task goal. [!CAUTION] Avoid solving the entire problem in single step. If your step() function contains internal loops that produce the entire generation, this is sign that the step granularity that is too large. Instead, try breaking down the solution into multiple calls to step(). Consider the example above, where we want to write sentence that includes at least three words from list of target words. On the surface, this task is about words, so we might consider using word step. However, most of the words that we generate will not be one of the target words, so resampling after each word is not the right granularity. To understand why, its important to know that the particle weights are based on the probability under task-agnostic prior (See the section on: Prior and proposal contexts). Since the prior doesnt contain information about the target words, non-target words will have higher probability under the prior than target words. If we were to resample after each word, we would actually filter out generations that include the target words, which would make it difficult to satisfy the constraints. Instead, better strategy is for each step to keep generating words until producing target word. Using this coarser phrase granularity is good choice for this problem because it aligns the different generations in way that allows resampling to compare like with like. On the other hand, an even coarser granularity (e.g., generating an entire sentence at each step) is not ideal because each sentence includes multiple target words, so we lose the opportunity to resample at relevant intermediate choice points in the generation. In general, its simplest if the step granularity is uniform. However, in more complex problems, different steps may require different granularities: for instance, when generating an email, we might start by generating the subject line in the first step and then generate the rest of the body in subsequent steps, or as one long second step."
        },
        {
            "title": "Generation strategy",
            "content": "The generation strategy section gives high-level explanation of what generation occurs in each step and what conditions need to be met in order to ensure the constraints. For example, in the above example, the docstring implies that each step needs to sample words (i.e., using next_word() ) and check each against the list of target words. It also suggests that there needs to be some check for when 3 target words have been generated. Rather than abruptly end as soon as this occurs, we instead want to freely generate until EOS, which is good fit for the extend() method."
        },
        {
            "title": "End condition",
            "content": "The end condition specifies when to end generation. There are few common kinds of end conditions: All constraints were met (success) Some constraint was violated (failure) The EOS token was sampled (possibly prematurely) Token limit has been reached (failure) In general, most InferenceModel implementations will include at least one end condition for success state and at least one end condition for failure (including token limit check)."
        },
        {
            "title": "Looping and control flow",
            "content": "In some cases, step() may contain loop that generates multiple tokens or words. [!CAUTION] Where possible, loops inside step() should be avoided in favor of single token or word per step. 36 Preprint. Under review. Special care should be taken to ensure that all loops are properly bounded. In general, for loops are preferred over while loops to ensure that the generation does not run indefinitely. It may be necessary to define loop bounds that are not explicitly part of the task (e.g., max number of words in sentence); use your best judgment. [!CAUTION] To ensure that step() yields control back to the asyncio event loop, every loop iteration should contain an await asyncio.sleep(0) statement. This statement is already embedded inside all methods provided by BaseModel , so you only need to add it manually if you are writing custom loop that does not call any of these methods."
        },
        {
            "title": "Masks",
            "content": "One of the key methods for controlling LLM generation is the concept of token masking. In hfppl , there are two main Token masks are simply subset of the LLM vocabulary. patterns for interacting with masks."
        },
        {
            "title": "Observing a mask",
            "content": "In many situations, its useful to force the LLM to generate specific token. For instance, suppose we want to generate sentence with fixed number of words that ends with punctuation. In hfppl , this is accomplished by observing mask: # Get the token_ids associated with punctuation END_PUNCTUATION = set(i for (i, v) in enumerate(lm.vocab) if in (\".\", \"!\", \"?\")) # Generates fixed number of words for _ in range(10): await self.next_word() # Forces the LLM to generate punctuation mask = self.context.mask_dist(END_PUNCTUATION) await self.observe(mask, True) punctuation_token = await self.next_token() By awaiting self.observe(mask, True) , we guarantee that the next token sampled from the LLM will be either period, exclamation point, or question mark."
        },
        {
            "title": "Sampling from a mask",
            "content": "Sometimes, we want to whether the next token is going to be from mask without forcing the LLM to generate token from the mask. Intuitively, this is useful for checking whether the LLM wants to complete generation in certain way. Continuing our example above, suppose want to generate sentence, but we want the number of words to be variable. To accomplish this, we can sample from the mask: # Generates variable number of words with max limit of 100 words for _ in range(100): await self.next_word() # If the LLM wants to generate punctuation, then end the sentence if await self.sample(self.context.mask_dist(END_PUNCTUATION)): punctuation_token = await self.next_token() break"
        },
        {
            "title": "Mask context managers",
            "content": "In addition to constructing masks using context.mask_dist() , we also provide higher-level interface through NextTokenMask . This class acts just like token mask, but it can also be invoked as context manager using the following syntax, which is equivalent to observing the mask: 37 Preprint. Under review. with NextTokenMask(include=END_PUNCTUATION): punctuation_token = await self.next_token() Similarly, we can also sample from NextTokenMask to create conditional control flow based on whether the LLM wants to generate particular kind of token. if await self.sample(NextTokenMask(include=END_PUNCTUATION)): punctuation_token = await self.next_token() break In addition to NextTokenMask , we also provide several pre-defined masks for common patterns, such as punctuation, EOS, and character limits. class NextTokenMask(Distribution): \"\"\"A context manager for masking tokens during generation. Provides the same interface as LMTokenMask but with the added ability to function as an async context manager. When used with the `with` syntax, the mask is observed, so the next token generated will be sampled from the (cid:44) Note that the mask only applies to the next token generated, even if there are multiple tokens generated (cid:44) within the context manager. mask. Args: model (BaseModel): The model to apply the mask to. include (set): The set of tokens to include in the mask. All other tokens will be excluded from (cid:44) generation. \"\"\" def __init__(self, model, include: set): self.model = model self.include = include self.mask = LMTokenMask(model.context, include) def invert(self): \"\"\"Inverts the mask so that all tokens in the mask are excluded from generation.\"\"\" self.include = self.model.context.lm.masks.ALL_TOKENS - self.include self.mask = LMTokenMask(self.model.context, self.include) return self async def sample(self): return await self.mask.sample() async def log_prob(self, v): return await self.mask.log_prob(v) async def __aenter__(self): await self.model.observe(self.mask, True) async def __aexit__(self, exc_type, exc_value, traceback): return False class PunctuationMask(NextTokenMask): \"\"\"A context manager for generating end-of-sentence punctuation tokens. END_PUNCTUATION = [\".\", \"!\", \"?\"] Args: model (BaseModel): The model to apply the mask to. \"\"\" def __init__(self, model): super().__init__(model, model.context.lm.masks.END_PUNCTUATION) class EOSMask(NextTokenMask): \"\"\"A context manager for generating the EOS token. Args: model (BaseModel): The model to apply the mask to. \"\"\" def __init__(self, model): super().__init__(model, model.context.lm.masks.EOS) class TokenLengthMask(NextTokenMask): 38 Preprint. Under review. \"\"\"A context manager for limiting the number of characters generated. NOTE: Special tokens like EOS are treated as having length 0. Args: model (BaseModel): The model to apply the mask to. min_chars (int): The minimum number of characters to generate. max_chars (int): The maximum number of characters to generate. allow_eos (bool): Whether to allow the EOS token to be generated. \"\"\" def __init__( self, model, min_chars: int = None, max_chars: int = None, allow_eos: bool = True, ): mask = model.context.lm.masks.token_length_mask(min_chars, max_chars) if len(mask) == 0: print( f\"WARNING: TokenLengthMask is empty for min_chars={min_chars}, max_chars={max_chars}, (cid:44) allow_eos={allow_eos}. Setting allow_eos=True.\" ) if allow_eos or len(mask) == 0: mask = mask.union(model.context.lm.masks.EOS) else: mask = mask.difference(model.context.lm.masks.EOS) super().__init__(model, mask) class NewLineMask(NextTokenMask): \"\"\"A context manager for generating new line tokens. Args: model (BaseModel): The model to apply the mask to. (int): The number of newlines to generate. If None, the mask will include all tokens containing (cid:44) newline character. \"\"\" def __init__(self, model, n: int = 1): if is None: include = set( [i for i, in enumerate(model.context.lm.str_vocab) if \"n\" in v] ) else: include = set([model.context.lm.str_vocab.index(\"n\" * n)]) super().__init__(model, include)"
        },
        {
            "title": "Ending generation",
            "content": "Once generation is finished, we need to signal to SMC that particle is finished, which is normally done with the finish() method. For convenience, BaseModel provides an end() method that ensures EOS has been generated before calling finish() Every step() implementation should contain at least one call to end() to ensure that it is properly terminated. Prior and proposal contexts During the resampling step, SMC selects for particles that have high weight under some LMContext distribution. In general, we want to select for generations that are grammatical and coherent. However, adhering to the task constraints can sometimes introduce disfluencies. For this reason, it is useful to separate out the proposal distribution, which enforces task-specific constraints, from the prior distribution, which encourages coherent generations. During resampling, generations are be sampled from the proposal, but scored against the prior, so that resampling selects for particles that both satisfy the constraints and have high probability under the prior. For example, consider the following task: 39 Preprint. Under review. Please generate sentence with exactly 11 words; with the 4th, 8th, 11th words to be Series, and, 4 respectively. One example of good prior prompt for this task might be, Write sentence that is grammatically correct and makes sense. This prompt effectively expresses the generation task while abstracting away the task-specific constraints. The BaseModel class automatically implements this inference pattern by defining two separate LMContext distributions. During generation, these two contexts are automatically synchonized: calls to self.sample() and self.observe() update the state of both LMContext in tandem. The prior is implemented by self.context and uses class-specific self.prior_prompt that needs to be defined for each BaseModel . The proposal is implemented by self.proposal_context and automatically includes the task instructions in its prompt. Additionally, the proposal context can be updated during generation by calling self.hint() as described below. The hint() method In many cases, its useful to update the proposal to reflect stateful information that is relevant for the next generation step. For instance, if the task requires generating sentence with target character count, we could use self.hint() to update the proposal at each step with the number of characters remaining. In addition to tracking low-level details, hints are also useful for encouraging the model to meet higher-level constraints, such as in budgeting tasks, where the remaining budget can be recomputed symbolically inside step() and passed to the proposal as hint. Imports You may freely import standard Python libraries when defining your InferenceModel . Note that the following are automatically imported in your local namespace: - asyncio - datetime - re - string - nltk - textstat - hfppl - BaseModel - NextTokenMask - PunctuationMask - EOSMask - TokenLengthMask - NewLineMask Examples Next, we will take look at some example tasks and their corresponding InferenceModel implementations. In these examples, the user describes the task in natural language and the assistant responds with an InferenceModel that encodes the users constraints. 40 Preprint. Under review. A.11 COLLIE Example Models class CollieModelSent01(BaseModel): \"\"\"Generates sentence with exactly 82 characters, including whitespace.\"\"\" def __init__( self, context, max_tokens: int = 32, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.target_length = @classmethod def prior_prompt(cls): return \"Write sentence that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample token that fits within the remaining length. Once we've reached the target length, end the generation. Step granularity: token End condition: Target length is reached or token limit is reached. \"\"\" remaining_length = self.target_length - len(self) # Provide hint about the remaining length. await self.hint(f\"There are {remaining_length} characters left.\") # Sample token that fits within the remaining length. async with TokenLengthMask( self, max_chars=remaining_length, allow_eos=(len(self) >= self.target_length), ): await self.next_token() # Once we've reached the maximum length, end the generation. # NOTE: We avoid manually sampling punctuation here as this would create an off-by-one error in the (cid:44) if len(self) >= self.target_length: length constraint. await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the length constraints.\"\"\" return len(text) == self.target_length class CollieModelSent02(BaseModel): \"\"\"Generates sentence with exactly 11 words, where the 4th, 8th, and 11th words are fixed to be 'Series', (cid:44) 'and', '4' respectively.\"\"\" def __init__( self, context, max_tokens: int = 32, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.word_idx = 1 self.max_words = 11 41 Preprint. Under review. self.target_words = { 4: \"Series\", 8: \"and\", 11: \"4\", } @classmethod def prior_prompt(cls): return \"Write sentence that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: Each step is going to generate one of the target words. At each step, keep sampling words until we reach target index. For the target index, force the model to sample the target word. As soon as target word is generated, `return` to complete the step. Once we hit max words, force punctuation and end. Step granularity: phrase End condition: Word index exceeds max_words or token limit is reached. \"\"\" # Provide hint about the remaining words. await self.hint( f\"The following words still need to be generated: {[word for i, word in self.target_words.items() if (cid:44) >= self.word_idx]}.\" ) # Sample words until we hit the next target word. for in range(self.word_idx, self.max_words + 1): # If the current word index corresponds to target word, generate that word. if self.word_idx in self.target_words: word = self.target_words[self.word_idx] await self.extend_with(word) self.word_idx += 1 # IMPORTANT: Return after generating the target word to complete the step. return # Otherwise, sample word unconstrained. else: await self.next_word() self.word_idx += 1 # Once max_words is reached, generate punctuation and end. if self.word_idx > self.max_words: async with PunctuationMask(self): await self.next_token() await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" words = [w for in nltk.word_tokenize(text) if not in string.punctuation] if len(words) != self.max_words: return False for idx, word in self.target_words.items(): if words[idx - 1].lower() != word.lower(): return False return True class CollieModelSent03(BaseModel): \"\"\"Generates sentence with at least 9 words, where each word has at most 7 characters.\"\"\" def __init__( self, context, max_tokens: int = 32, ): super().__init__( 42 Preprint. Under review. context=context, max_tokens=max_tokens, ) # Task-specific variables self.min_words = 9 self.max_chars_per_word = 7 self.word_idx = 1 @classmethod def prior_prompt(cls): return \"Write sentence that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample word, keeping track of the word index. After the min_words limit is reached, allow (but do not force) end punctuation to be generated. Once end punctuation is generated, end the generation. We use max_chars to limit the length of each word. Step granularity: word End condition: Word index exceeds min_words or token limit is reached. \"\"\" # Provide hint about the number of words remaining. await self.hint( f\"There are at least {self.min_words - self.word_idx} words left to generate.\" ) # Sample word with maximum length of max_chars word = await self.next_word( max_chars=self.max_chars_per_word, ) self.word_idx += 1 # If we've reached the min_words limit, allow the model to end the sentence if self.word_idx > self.min_words: if await self.sample(PunctuationMask(self)): await self.next_token() await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" words = [w for in nltk.word_tokenize(text) if not in string.punctuation] if len(words) < self.min_words: return False for word in words: if len(word) > self.max_chars_per_word: return False return True class CollieModelSent04(BaseModel): \"\"\"Generates sentence containing the words 'have', 'rising', and 'the'.\"\"\" def __init__(self, context, max_tokens: int = 32): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.target_words = set([\"have\", \"rising\", \"the\"]) # Maximum number of words before generating one of the targets self.max_words_per_phrase = 40 @classmethod def prior_prompt(cls): return \"Write sentence that is grammatically correct and makes sense.\" 43 Preprint. Under review. async def step(self): \"\"\" Generation strategy: Each step is going to generate one of the target words. At each step, keep sampling words until one of the target words is generated. As soon as target word is generated, `return` to complete the step. On the final step (once all target words have been generated), extend the sentence unconstrained until (cid:44) EOS. Step granularity: phrase End condition: All target words have been generated or token limit is reached. \"\"\" # If any target words are still present, keep sampling words until one is generated. if len(self.target_words) > 0: await self.hint( ) f\"The following target words are remaining: {self.target_words}.\" for _ in range(1, self.max_words_per_phrase + 1): word = await self.next_word() # Remove the generated word from the target set. if word.strip().lower() in self.target_words: self.target_words.remove(word.strip().lower()) # IMPORTANT: Return after generating the target word to complete the step. return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return # If we reach the word limit without generating target word, reject self.condition(False) # If all target words have been generated, extend until EOS else: await self.extend() await self.end() async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" words = [w for in nltk.word_tokenize(text) if not in string.punctuation] for word in self.target_words: if word.lower() not in words: return False return True class CollieModelPara01(BaseModel): \"\"\"Generates paragraph where each sentence starts with the word 'The'.\"\"\" def __init__( self, context, max_tokens: int = 128, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.target_word = \"The\" @classmethod def prior_prompt(cls): return \"Write paragraph that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample sentence starting with the target word. Optionally end the paragraph after each sentence. 44 Preprint. Under review. Step granularity: sentence End condition: EOS token is sampled or token limit is reached. \"\"\" # Generate the sentence await self.extend(start=self.target_word, stop=[\".\", \"!\", \"?\"]) # Allow the model to optionally end the paragraph if await self.sample(EOSMask(self)): await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" sentences = nltk.sent_tokenize(text.lower()) for sentence in sentences: if not sentence.startswith(self.target_word.lower()): return False return True class CollieModelPara02(BaseModel): \"\"\"Generates paragraph with exactly 3 sentences, excluding the words 'be', 'this', and 'is'.\"\"\" def __init__( self, context, max_tokens: int = 128, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.n_sentences_target = 3 self.disallowed_words = set([word.lower() for word in [\"be\", \"this\", \"is\"]]) self.max_words_per_sentence = self.sentence_count = 0 @classmethod def prior_prompt(cls): return \"Write paragraph that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample sentence word-by-word. If disallowed word is generated, reject the sentence. Optionally end the paragraph after each sentence. Step granularity: sentence End condition: n_sentences_target is reached or token limit is reached. \"\"\" end_punctuation = None # Provide hint about the remaining sentences. await self.hint( f\"There are {self.n_sentences_target - self.sentence_count} sentences left to generate.\" ) # Sample the sentence word-by-word for _ in range(self.max_words_per_sentence): word = await self.next_word() # If the word is disallowed, reject if word.lower() in self.disallowed_words: self.condition(False) return 45 Preprint. Under review. # If we reach the end of the sentence, break if await self.sample(PunctuationMask(self)): end_punctuation = await self.next_token() break # Reject the sentence if we reach the word limit without end punctuation if not end_punctuation: self.condition(False) return self.sentence_count += 1 # If we reach n_sentences_target, end generation if self.sentence_count >= self.n_sentences_target: await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" sentences = nltk.sent_tokenize(text.lower()) if len(sentences) != self.n_sentences_target: return False for sentence in sentences: words = [ for in nltk.word_tokenize(sentence) if not in string.punctuation ] for word in words: if word.lower() in self.disallowed_words: return False return True class CollieModelPara03(BaseModel): \"\"\"Generates paragraph with exactly 4 sentences, where each sentence has between 12 and 20 words.\"\"\" def __init__( self, context, max_tokens: int = 128, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.n_sentences_target = 4 self.min_words_per_sentence = 12 self.max_words_per_sentence = 20 self.sentence_count = 0 @classmethod def prior_prompt(cls): return \"Write paragraph that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample sentence word-by-word. After min_words_per_sentence have been generated, allow (but do not force) end punctuation to be (cid:44) Once n_sentences_target is reached, end the generation. generated. Step granularity: sentence End condition: n_sentences_target is reached or token limit is reached. \"\"\" end_punctuation = None # Sample the sentence word-by-word, allowing end punctuation if min_words_per_sentence have been generated for word_idx in range(1, self.max_words_per_sentence + 1): Preprint. Under review. # Provide hint about the remaining words in the sentence. hint_text = f\"This is sentence {self.sentence_count + 1} / {self.n_sentences_target}.\" if word_idx < self.min_words_per_sentence: hint_text += f\" There are at least {self.min_words_per_sentence - word_idx} words left to (cid:44) else: generate.\" hint_text += f\" There are at most {self.max_words_per_sentence - word_idx} words left to generate.\" await self.hint(hint_text) word = await self.next_word() # End the sentence as soon as end punctuation is generated if word_idx >= self.min_words_per_sentence: if await self.sample(PunctuationMask(self)): end_punctuation = await self.next_token() break # Reject the sentence if we reach the word limit without end punctuation if not end_punctuation: self.condition(False) self.sentence_count += # If we reach n_sentences_target, end generation if self.sentence_count >= self.n_sentences_target: await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" sentences = nltk.sent_tokenize(text.lower()) if len(sentences) != self.n_sentences_target: return False for sentence in sentences: words = [ for in nltk.word_tokenize(sentence) if not in string.punctuation ] if ( ): len(words) < self.min_words_per_sentence or len(words) > self.max_words_per_sentence return False return True class CollieModelPara04(BaseModel): \"\"\"Generates paragraph with at least 3 sentences, where each sentence has at least 21 words.\"\"\" def __init__( self, context, max_tokens: int = 128, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.min_sentences = 3 self.min_words_per_sentence = 21 self.max_words_per_sentence = 100 self.sentence_count = @classmethod def prior_prompt(cls): return \"Write paragraph that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample sentence word-by-word. After min_words_per_sentence have been generated, allow (but do not force) end punctuation to be (cid:44) generated. Preprint. Under review. After min_sentences is reached, allow (but do not force) the paragraph to end. Step granularity: sentence End condition: EOS token is sampled or token limit is reached. \"\"\" end_punctuation = None # Sample the sentence word-by-word, allowing end punctuation if min_words_per_sentence have been generated for word_idx in range(1, self.max_words_per_sentence + 1): # Provide hint about the remaining words in the sentence. hint_text = ( f\"This is sentence {self.sentence_count + 1} / {self.min_sentences}.\" ) hint_text += f\" This sentence contains {word_idx} / {self.min_words_per_sentence} words.\" await self.hint(hint_text) word = await self.next_word() # End the sentence as soon as end punctuation is generated if word_idx >= self.min_words_per_sentence: if await self.sample(PunctuationMask(self)): end_punctuation = await self.next_token() break # Reject the sentence if we reach the word limit without end punctuation if not end_punctuation: self.condition(False) return self.sentence_count += 1 # If we reach min_sentences, optionally end the paragraph if self.sentence_count >= self.min_sentences: if await self.sample(EOSMask(self)): await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" sentences = nltk.sent_tokenize(text.lower()) if len(sentences) < self.min_sentences: return False for sentence in sentences: words = [ for in nltk.word_tokenize(sentence) if not in string.punctuation ] if len(words) < self.min_words_per_sentence: return False return True class CollieModelPara05(BaseModel): \"\"\"Generates paragraph with exactly 3 sentences, where each sentence ends with 'convention', 'president', (cid:44) 'Wisconsin'.\"\"\" def __init__( self, context, max_tokens: int = 128, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.n_sentences_target = 3 self.max_words_per_sentence = 100 self.target_last_words = [ word.lower() for word in [\"convention\", \"president\", \"Wisconsin\"] ] 48 Preprint. Under review. self.sentence_count = @classmethod def prior_prompt(cls): return \"Write paragraph that is grammatically correct and makes sense.\" async def step(self): \"\"\" Generation strategy: At each step, sample word-by-word with no punctuation. Once the target word is generated, add punctuation to the end of the sentence. Once n_sentences_target is reached, end the generation. Step granularity: sentence End condition: n_sentences_target is reached or token limit is reached. \"\"\" last_word_reached = False # Sample the sentence word-by-word for word_idx in range(1, self.max_words_per_sentence + 1): # Provide hint about the remaining words in the sentence. hint_text = f\"This is sentence {self.sentence_count + 1} / {self.n_sentences_target}.\" hint_text += f\" This sentence needs to end with the word '{self.target_last_words[self.sentence_count]}'.\" (cid:44) await self.hint(hint_text) # Sample the next word, deferring punctuation until the target word is reached word = await self.next_word() # If the word is the target word, end the sentence if word.strip().lower() == self.target_last_words[self.sentence_count]: last_word_reached = True break # Reject the sentence if we reach the word limit without the target word if not last_word_reached: self.condition(False) # Now add punctuation to the end of the sentence # Since next_word() doesn't generate punctuation, we need to do this manually async with PunctuationMask(self): await self.next_token() self.sentence_count += 1 # If we reach n_sentences_target, end generation if self.sentence_count >= self.n_sentences_target: await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: \"\"\"Check that the generated text satisfies the word constraints.\"\"\" sentences = nltk.sent_tokenize(text.lower()) if len(sentences) != self.n_sentences_target: for idx, sentence in enumerate(sentences): return False words = [ for in nltk.word_tokenize(sentence) if not in string.punctuation ] if words[-1].lower() != self.target_last_words[idx]: return False return True A.12 Puzzles Example Models class SquareWordPoemModel(BaseModel): \"\"\"Generates poem with lines, where each line has exactly words.\"\"\" 49 Preprint. Under review. def __init__( self, context, max_tokens: int = 128, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.N = 8 self.line_i = 1 # Number of lines and words per line @classmethod def prior_prompt(cls): return \"Write poem.\" async def step(self): \"\"\" Generation strategy: Each step is going to generate line of the poem. To generate line, sample word-by-word times. After generating line, sample newline token to move to the next line. Step granularity: line End conditions: 1. lines are generated. 2. The token limit is reached. \"\"\" for _ in range(self.N): await self.next_word() async with NewLineMask(self, n=1): await self.next_token() self.line_i += 1 if self.line_i > self.N: await self.end() # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: lines = text.strip().split(\"n\") if len(lines) != self.N: return False for line in lines: words = line.split() if len(words) != self.N: return False return True class GrantProposalModel(BaseModel): \"Generates an abstract for grant proposal on elephant ecology and conservation. The abstract starts with (cid:44) 'Abstract:'. It is between 75 and 100 words and excludes list of forbidden words.\" def __init__( self, context, max_tokens: int = 512, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.forbidden_words = set( [ \"conservation\", \"sustainability\", \"environment\", \"ecology\", \"wildlife\", \"africa\", \"asia\", 50 Preprint. Under review. \"society\", \"community\", \"biodiversity\", \"endangered\", \"threatened\", \"species\", \"habitat\", \"poaching\", \"science\", \"research\", ] ) self.min_words = 75 self.max_words = 100 self.word_count = 0 self.header = False @classmethod def prior_prompt(cls): return \"Write an abstract for grant proposal on elephant ecology and conservation.\" async def step(self): \"\"\" Generation strategy: Each step is going to generate word for the abstract. On the first step, we generate the header, \"Abstract:\". On subsequent steps, we sample word and check if it is forbidden word. If the word is forbidden, reject. After each word, check if the model wants to sample punctuation. If the minimum word count is reached, additionally allow the model to sample EOS. If the maximum word count is reached, reject. Step granularity: word End conditions: 1. The model samples EOS. 2. The maximum word count is reached. 3. The token limit is reached. \"\"\" # Generate the title first. if not self.header: await self.extend_with(\"Abstract:\") self.header = True return hint_text = f\"The current length is {self.word_count} words.\" if self.word_count < self.min_words: hint_text += ( f\" We need at least {self.min_words - self.word_count} more words.\" ) else: await self.hint(hint_text) hint_text += f\" There are only {self.max_words - self.word_count} words left before we hit the limit!\" # Sample word. word = await self.next_word() self.word_count += 1 # Check if the sentence contains any forbidden words. if word.strip().lower() in self.forbidden_words: self.condition(False) return # Optionally, sample punctuation (but do not end, since the abstract will contain multiple sentences). if await self.sample(PunctuationMask(self)): await self.next_token() # If the minimum word count has been reached, allow the model to sample EOS. if self.min_words <= self.word_count < self.max_words: if await self.sample(EOSMask(self)): # If the maximum word count has been reached, reject. elif self.word_count >= self.max_words: await self.end() return self.condition(False) await self.end() return # Enforce token limit. 51 Preprint. Under review. if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return async def check(self, text: str) -> bool: text = text.strip() # Check for the header if not text.startswith(\"Abstract:\"): return False # Extract abstract abstract = text[len(\"Abstract:\") :].strip() words = abstract.lower().split() # Check word count word_count = len(words) if word_count < self.min_words or word_count > self.max_words: return False # Check for forbidden words first for word in words: if word in self.forbidden_words: return False return True class IngredientsListModel(BaseModel): \"\"\"Writes an ingredients list for chocolate chip brownies with at most 7 ingredients costing less than (cid:44) (cid:44) $18.00 total. The list is in bullet point format starting with \"Ingredients:\". Each ingredient is listed on separate line with the price given in USD.\"\"\" def __init__( self, context, max_tokens: int = 256, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.max_ingredients = 7 self.max_cost = 18. self.line_i = 0 self.total_cost = 0.0 @classmethod def prior_prompt(cls): return \"Write an ingredients list for chocolate chip brownies.\" def extract_cost(self, text: str) -> float: match = re.search(r\"$(d+(?:.d+)?)\", text) if not match: try: return None cost = float(match.group(1)) except ValueError: return None cost = round(cost, 2) return cost async def step(self): \"\"\" Generation strategy: Each step is going to generate an ingredient for the list. To generate an ingredient, sample the ingredient name and price. We can use hint to inform the model of the remaining budget. If the cost of the ingredient exceeds the maximum cost, reject. After generating an ingredient, check if the model wants to sample EOS. If the maximum number of ingredients is reached, force the model to sample EOS. Step granularity: line End conditions: 52 Preprint. Under review. 1. The model samples EOS. 2. The maximum number of ingredients is reached. 3. The cost limit is reached. 4. The token limit is reached \"\"\" # The first step generates the header \"Ingredients:\" if self.line_i == 0: await self.extend_with(\"Ingredients:n\") self.line_i += 1 return # Provide hint about the remaining budget await self.hint( f\"The remaining budget is ${self.max_cost - self.total_cost:.2f}.\" ) # Generate the ingredient # Ensure the line starts with hyphen and ends with newline # Set allow_eos=True to allow the model to sample EOS ingredient, eos = await self.extend(start=\"-\", stop=[\"n\"], allow_eos=True) # Extract the cost of the ingredient cost = self.extract_cost(ingredient) if cost is None: self.condition(False) return # Update the running total cost self.total_cost += cost if self.total_cost > self.max_cost: self.condition(False) return # If the model sampled EOS on this step, end the generation if eos: # If the maximum number of ingredients is reached, force the model to sample EOS elif self.line_i >= self.max_ingredients: await self.end() return await self.end() return # Enforce token limit if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return self.line_i += 1 async def check(self, text: str) -> bool: lines = text.strip().split(\"n\") if lines[0] != \"Ingredients:\": return False if len(lines) > self.max_ingredients + 1: return False total_cost = 0 for line in lines[1:]: if not line: continue if not line.startswith(\"-\"): return False cost = self.extract_cost(line) if cost is None: return False total_cost += cost if total_cost > self.max_cost: return False return True class TripItineraryModel(BaseModel): \"\"\"Generates three-day trip itinerary with at least 4 activities per day. Each day should start with \"Day N:\" and end with double newline. Each activity should start with time range in 24-hour format in (cid:44) square brackets (e.g., \"[11:00-13:00]\") and end with newline. The itinerary should include at least 9 (cid:44) hours of free time each day for rest.\"\"\" (cid:44) Preprint. Under review. def __init__( self, context, max_tokens: int = 512, ): super().__init__( context=context, max_tokens=max_tokens, ) # Task-specific variables self.n_days = 3 self.n_activities = 4 self.day_i = 1 self.activity_i = 0 self.min_free_time = 9 @classmethod def prior_prompt(cls): return \"Write day-by-day itinerary for 3-day trip to Singapore.\" def extract_time_range( self, text: str ) -> Tuple[datetime.datetime, datetime.datetime]: match = re.search(r\"[d{2}:d{2}-d{2}:d{2}]\", text) if not match: return None time_range = match.group(0) # Remove square brackets and split into start and end times time_range_clean = time_range.strip(\"[]\") try: start_str, end_str = time_range_clean.split(\"-\") start_time = datetime.datetime.strptime(start_str, \"%H:%M\") end_time = datetime.datetime.strptime(end_str, \"%H:%M\") except ValueError: return None return start_time, end_time def is_complete(self) -> bool: return self.day_i >= self.n_days and self.activity_i >= self.n_activities async def step(self): \"\"\" Generation strategy: Each step is going to generate line in the itinerary consisting of time range and an activity. Each line should start with time range in 24-hour format (e.g., \"[11:00-13:00]\") and end with single (cid:44) If the line ends with double newline, check to make sure there are at least 4 activities for the day (cid:44) After enough activities are generated on the final day, allow the model to sample EOS to end the (cid:44) and move to the next day. or double newline. itinerary. Step granularity: line End conditions: 1. The model ends the itinerary after generating enough activities for each day. 2. The model prematurely ends the itinerary without generating enough activities. 3. The free time limit is reached. 4. The token limit is reached. \"\"\" # Generate the header for the day. if self.activity_i == 0: await self.extend_with(f\"Day {self.day_i}:n\") self.activity_i += ( 1 # IMPORTANT: Increment activity_i to avoid infinite loop. ) self.free_time = 24 return # Generate the activity. # Ensure the line starts with time range in 24-hour format and ends with newline. # On the final day, once the model generates enough activities, allow it to sample EOS. activity, eos = await self.extend( start=\"[\", stop=[\"n\"], allow_eos=self.is_complete() ) # Extract the time range of the activity. time_range = self.extract_time_range(activity) if time_range is None: Preprint. Under review. self.condition(False) return # Check if there is enough free time for the day. start_time, end_time = time_range activity_duration = (end_time - start_time).seconds / 3600 self.free_time -= activity_duration if self.free_time < self.min_free_time: self.condition(False) return # If the model sampled EOS on this step, end the generation. if eos: await self.end() return # If the model generated double newline, move to the next day. if activity.endswith(\"nn\"): # If there aren't enough activities for the day, reject. if self.activity_i < self.n_activities: self.condition(False) return # If this is the final day, force the model to end. if self.day_i == self.n_days: await self.end() return # Move to the next day. self.day_i += 1 self.activity_i = 0 return # Enforce token limit. if self.context.token_count > self.max_tokens: self.condition(False) await self.end() return # Move to the next activity. self.activity_i += 1 async def check(self, text: str) -> bool: days = text.strip().split(\"nn\") if len(days) != 3: return False for day in days: lines = day.split(\"n\") if not lines[0].startswith(\"Day\"): return False activities = lines[1:] if len(activities) < 4: return False free_time = 24 for activity in activities: time_range = self.extract_time_range(activity) if time_range is None: return False start_time, end_time = time_range activity_duration = (end_time - start_time).seconds / # Ensure the activity duration is non-negative. if activity_duration < 0: return False free_time -= activity_duration # Ensure there is enough free time for the day. if free_time < self.min_free_time: return False return True"
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Yale University"
    ]
}