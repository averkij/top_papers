{
    "paper_title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
    "authors": [
        "Rongyao Fang",
        "Chengqi Duan",
        "Kun Wang",
        "Hao Li",
        "Hao Tian",
        "Xingyu Zeng",
        "Rui Zhao",
        "Jifeng Dai",
        "Hongsheng Li",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 2 1 6 8 3 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "PUMA: EMPOWERING UNIFIED MLLM WITH MULTI-GRANULAR VISUAL GENERATION Rongyao Fang1 Chengqi Duan2 Kun Wang3 Hao Li1,4 Hao Tian3 Xingyu Zeng3 Rui Zhao3 Jifeng Dai4,5 Hongsheng Li1 Xihui Liu2 1CUHK MMLab 2HKU MMLab 3SenseTime 4Shanghai AI Laboratory 5THU Figure 1: a) Diversity and controllability tradeoff in image generation tasks: diverse text-to-image generation requires high diversity and fidelity, while tasks like conditional generation and manipulation require high controllability on the image. b) The introduced PUMA, unified multimodal large language model that processes and generates multi-granular visual representations, balancing diversity and controllability across visual generation tasks. It excels in image understanding, diverse text-to-image generation, editing, inpainting, colorization, and conditional image generation."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within unified MLLM paradigm from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in wide range of multimodal tasks. This work represents significant step towards truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA. Equal Contribution Project Lead Corresponding Authors"
        },
        {
            "title": "Technical Report",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Unifying multimodal understanding and generation capabilities within single model is critical milestone toward artificial general intelligence (AGI). Towards this goal, recent advancements (Liu et al., 2024b; Zhu et al., 2023a) in multimodal large language models (MLLMs) have made significant progress in integrating visual reasoning and understanding with natural language interfaces. However, developing unified framework that excels at both comprehending and generating multimodal content remains significant challenge in the field of artificial intelligence. Recent studies (Sun et al., 2023; Ge et al., 2024b) have explored MLLMs potential for visual generation, beyond the previously well-explored visual understanding and reasoning with MLLMs. These approaches enable MLLMs to process image-text inputs and produce either textual outputs or semantic-level visual tokens. In the case of image generation, these visual tokens are subsequently transformed into pixel-space images using diffusion-based decoders. Such unified frameworks empower MLLMs to perform wide spectrum of tasks within single framework, ranging from detailed visual analysis to creative image synthesis. However, existing MLLM-based methods (Sun et al., 2023; 2024b) face common challenge in the trade-off between diversity for text-to-image generation and high controllability for tasks such as image editing. Previous methods mostly rely on single-granular features extracted from visual encoder and neglect the varying granularity requirements of different tasks. On the one hand, generating diverse images reflecting the real world from text descriptions requires features that encode coarse semantic concepts. Such features are fed as conditions into the diffusion-based image decoder, allowing the diffusion model to generate diverse images that semantically align with the text prompt. On the other hand, tasks demanding precise control over output images, such as image editing and inpainting, require the LLMs to predict fine-grained features that encode rich, detailed visual information for the image decoder. This dichotomy presents significant challenge for current MLLM-based methods, which typically generate single-granular feature representations for all tasks. As result, models optimized for diverse image generation often lack the fine-grained controllability necessary for detailed downstream tasks such as editing, while those focused on precise controllability produce less varied outputs for the task of text-to-image generation. Although recent work like SEED-X (Ge et al., 2024b) attempts to bypass this issue by leveraging condition images directly input to the diffusion-based decoder for fine-grained control, unified solution to the multi-granularity problem remains underexplored. Towards the multi-granular feature demands of various tasks, we propose novel paradigm emPowering Unified MLLM with Multi-grAnular visual generation (PUMA). PUMA facilitates seamless integration of image generation and understanding processes, while simultaneously handling multiple feature granularities from coarse-grained abstractions to fine-grained details within single framework. By leveraging multi-scale features, our approach empowers MLLMs to excel in diverse image generation and controllable downstream tasks, within unified framework. Our method comprises three key modules: 1) An image encoder that extracts multi-granular representations, which serve as the foundation for visual generation and understanding; 2) An autoregressive MLLM that processes and progressively generates multi-scale image features; and 3) set of dedicated diffusion-based image decoders that decode images from MLLM-generated features at multiple granularities. To optimize this framework, we employ two-stage training strategy: first fine-tuning the set of pre-trained diffusion models as our image decoders, where each model reconstructs or generates images conditioned on the corresponding feature granularities from the encoder; then training the autoregressive MLLM with regression loss supervised by the multi-scale encoder features to process and generate multi-granular image features. PUMA leverages largescale pre-training followed by task-specific instruction tuning on collection of linguistic-visual datasets, enabling our model to handle various tasks including image understanding, text-to-image generation, editing, inpainting, colorization, and conditional generation. In summary, we introduce novel multi-granularity paradigm for MLLMs that addresses the limitations of existing single-scale methods. By simultaneously processing and generating features at multiple granularities, our approach enables unified framework to handle wide range of tasks, from diverse image generation to precise editing and highly controllable generation. This unified framework represents significant advancement towards more versatile and capable MLLMs, contributing to the broader goal of achieving AGI in multimodal domains."
        },
        {
            "title": "2.1 MULTIMODAL UNDERSTANDING",
            "content": "The rapid advancement of large language models (LLMs) has catalyzed significant progress in multimodal large language models (MLLMs) for multimodal understanding tasks Dai et al. (2023); Li et al. (2024b); Zhang et al. (2023a); Chen et al. (2024); Lin et al. (2024); Zhang et al. (2024b); Li et al. (2024a). Pioneering works such as LLaVA (Liu et al., 2024b) and MiniGPT-4 (Zhu et al., 2023a) have demonstrated remarkable performance across diverse image understanding tasks, including visual question answering (VQA), visual reasoning, optical character recognition (OCR), and object grounding. These approaches typically employ visual encoders, such as the CLIP encoder (Radford et al., 2021), to extract continuous image features, which are then projected into the LLMs embedding space for subsequent tasks. While successfully unifying various image understanding tasks within single model, these methods mostly adhere to multimodal-input, text-output paradigm. Consequently, they excel at text-based responses to visual inputs but cannot generate multimodal outputs beyond text, limiting their applicability in tasks requiring visual content generation."
        },
        {
            "title": "2.2 UNIFIED UNDERSTANDING AND GENERATION FOR MLLMS",
            "content": "Recent research has focused on equipping MLLMs with multimodal output capabilities (Wu et al., 2023; Tang et al., 2024; Ye et al., 2024a; Zhu et al., 2023b). GILL (Koh et al., 2024) pioneered the integration of image generation abilities into MLLMs. Subsequently, SEED-LLaMA (Ge et al., 2023) and Emu (Sun et al., 2023) further advanced image generation and understanding capabilities within MLLMs, while DreamLLM (Dong et al., 2023) proposed an end-to-end training approach for enhanced performance. More recent works, such as SEED-X (Ge et al., 2024b) and Emu2 (Sun et al., 2024b), have scaled up MLLMs for unified generation, adopting continuous feature-based methods. These approaches utilize pre-trained vision encoders to extract continuous semantic features, which MLLMs then autoregressively regress. Specialized diffusion model-based decoders transform these MLLM-generated features into pixel-space images. However, the single-scale image feature generation pipeline employed by these methods struggles to address tasks with varying granularity demands, making it challenging to balance diverse image generation with fine-grained control for manipulation tasks. SEED-X attempts to address the multi-granularity issue by introducing conditional image input to the diffusion-based decoder for fine-grained control. However, this approach limits its applicability to image editing tasks encountered during decoder training. Consequently, unified solution to the multi-granularity problem remains underexplored. In contrast, our work proposes novel multigranularity paradigm that addresses these limitations by simultaneously handling multiple levels of feature granularity within single, unified framework. Alternative approaches have also been investigated. Chameleon (Team, 2024) explored using discrete image tokens to bridge image understanding and generation, but the vector quantization process leads to information loss, hindering high-performance image understanding. TransFusion (Zhou et al., 2024) and show-o (Xie et al., 2024) proposed transforming the MLLM backbone itself into denoiser in diffusion-based or demasking-based approach. However, these methods require numerous denoising steps for each image generation, resulting in substantial computational costs given the scale of current MLLM backbones. VAR (Tian et al., 2024) is another track of generation framework that implements hierarchical autoregressive with discrete tokens for image generation, but it only discusses image generation and cannot unify multimodal tasks."
        },
        {
            "title": "3 METHOD",
            "content": "Existing approaches typically optimize for either fine or coarse-grained features, resulting in tradeoff between precise control and generation diversity. To overcome this limitation, we propose PUMA, unified multi-granular MLLM paradigm. Our approach simultaneously processes multiple levels of feature granularity within unified MLLM framework, facilitating seamless transitions across wide spectrum of multimodal tasks. Our framework consists of three key components: an image encoder (Sec. 3.1), set of image decoders conditioned on different granular features (Sec. 3.2), and multi-granular autoregressive MLLM (Sec. 3.3). These components work synergistically to extract, process, and generate multiscale image features, adapting to various task-specific granularity requirements. To optimize our"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Upper: PUMAs unified multi-granular autoregressive pipeline for processing and generating text and multi-granular visual features. Lower: Illustration of PUMAs versatility across various tasks: 1) diverse text-to-image generation, 2) image editing, 3) conditional image generation, and 4) image understanding, showcasing different input-output configurations. MLLM, we employ two-stage process of pretraining and instruction tuning (Sec. 3.4), enabling it to perform wide range of tasks including image understanding, generation, editing, and conditional image generation. 3.1 IMAGE ENCODING AND MULTI-GRANULAR FEATURE EXTRACTION Our unified multi-granularity paradigm leverages semantic image encoder to extract multi-scale features, forming the foundation for diverse visual task processing. We employ CLIP (Radford et al., 2021) semantic image encoder to process input images and generate the initial set of highresolution features f0 RHW C, with and representing the spatial dimensions of the highest resolution feature grid, and denoting the channel dimension. In our setting, the feature size is = = 16, thus the highest resolution feature f0 has 256 visual tokens. To obtain multi-granular representations, we derive lower resolution features through successive applications of 2D average pooling with kernel size 2 and stride 2: fi = AvgPool(fi1), (1) where is the number of additional granular levels. This process generates series of feature grids at progressively coarser resolutions, ranging from fine-grained features preserving detailed spatial information and local textures, through mid-level features capturing object parts and regional structures, to features representing coarse-grained semantic concepts. These features are denoted as f0, f1, f2, f3, and f4, which have 256, 64, 16, 4, and 1 visual tokens respectively. = 1, 2, ..., 3.2 MULTI-GRANULAR VISUAL DECODING Image features at different granularities encode varying levels of information. We employ diffusionbased models as decoders due to their flexible capability to handle multi-scale features. When processing coarse-grained semantic features, the decoders can effectively synthesize missing finegrained information with their learned image priors and generate diverse, semantics-aligned images. On the other hand, when handling fine-grained features, they accurately reconstruct precise image details. This versatility in generating or reconstructing images across different granularities makes diffusion-based models suitable for our multi-granularity approach."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Multi-granular visual decoding from fine-grained to coarse-grained granularity. We develop set of dedicate diffusion-based image decoders D0, D1, ..., DN corresponding to the feature scales f0, f1, ..., fN . These decoders enable the visual decoding of images at various levels of granularity. We formulate the image decoding process for each granularity level as ˆxi = Di(fi, z), where ˆxi is the decoded image, fi is the feature map at granularity level i, and is random noise vector for the diffusion process. We leverage the pre-trained SDXL models (Podell et al., 2023) as our decoding framework and finetune these pre-trained models to generate or reconstruct images conditioned on different granular features. By modifying the conditional input mechanism through cross-attention in SDXL to accept our multi-granular features fi, we harness the models inherent ability to decode coherent images. Fig. 4 shows the training process of different granular image decoding, during which the image encoder is frozen to preserve semantic property. Fig. 3 illustrates the visual decoding capabilities of multi-granular decoders. The visualizations demonstrate the fidelity of decoded images across different granularities, with finer-grained features yielding reconstructions closer to the original input, and coarser-grained features leading to image generation guided by the semantics of the input image. This validates the effectiveness of our approach in preserving and utilizing multi-granular visual information. Figure 4: Training phase of multi-granular visual decoding. This multi-granular decoding framework, in conjunction with our hierarchical feature extraction, establishes foundation for the subsequent stages of our MLLM architecture, paving the way for diverse visual tasks in later training phases. 3.3 PROGRESSIVE MULTI-GRANULAR IMAGE MODELING IN AUTOREGRESSIVE MLLM Driven by the goal of utilizing unified framework capable of adapting to wide range of visuallinguistic tasks with varying granularity requirements, we design an autoregressive MLLM to process and generate both text tokens and multi-granular image features. Our autoregressive MLLM, denoted as , processes text and multi-granular image features progressively, as illustrated in Fig. 2. The model processes features token by token, predicting each token sequentially within each granularity level, and progressing from the coarsest level to the"
        },
        {
            "title": "Technical Report",
            "content": "finest level 0. This approach allows the model to refine its predictions as more detailed information becomes available. We structure the input sequence as concatenation of text tokens and flattened image feature tokens from multiple granularity levels. This progressive approach enables the model to capture dependencies across different scales, from coarse global structures to fine local details. The MLLM is trained using an autoregressive next token prediction objective, combining both text and image losses: = (cid:88) log (tit<i, F<i) + (cid:88) αi ki(cid:88) i=0 j= fi,j ˆfi,j2 (2) The first term represents the cross-entropy loss for text token prediction, where ti are text tokens. The second term is the regression loss for image feature prediction, where fi,j and ˆfi,j are the ground truth and predicted feature tokens, respectively, at the i-th granularity level. ki is the number of visual tokens at the i-th granularity level. The coefficient αi allows for adjusting the importance of each granularity level during training. 3.4 MULTIMODAL PRETRAINING AND INSTRUCT TUNING To demonstrate the effectiveness of our unified multi-granularity paradigm, we implement comprehensive two-stage training pipeline for PUMA: multimodal pretraining followed by task-specific instruct tuning. This approach allows our model to first acquire broad multimodal capabilities before specializing in targeted visual-linguistic tasks during the subsequent instruct tuning stage. Multimodal Pretraining: Our multimodal pretraining leverages diverse set of large-scale datasets: Laion-2B (Schuhmann et al., 2022), Laion-Aesthetics (Burger, 2023), GRIT (Peng et al., 2023), The Pile (Gao et al., 2020), OCR-VQA-200K (Mishra et al., 2019), and LLaVAR (Zhang et al., 2023b). This combination of datasets provides rich mixture of image-text pairs, textual data, and specialized visual question-answering samples. To enhance the models bidirectional understanding of image-text relationships, we employ dynamic training strategy that randomly alternates between text-to-image and image-to-text tasks for each image-text pair. Instruct Tuning: Following pretraining, we conduct targeted instruct tuning to adapt our model to specific visual-linguistic tasks. To evaluate PUMAs performance across different task types, we fine-tune four dedicated models for the four types of tasks, each initialized from the pretraining checkpoint. High-quality Text-to-Image Generation: We utilize Laion-Aesthetics (Burger, 2023) and JourneyDB (Sun et al., 2024a) to focus on generating aesthetically pleasing and diverse images. Precise Image Manipulation: Training on the SEED-Edit (Ge et al., 2024a) dataset enables accurate and controlled image editing. Conditional Image Generation: The subset of MultiGen-20M dataset (Qin et al., 2023) including canny-to-image, inpainting, and colorization is employed to equip the model with the ability to generate images under specific conditions and constraints. Image Understanding: Fine-tuning on the subset of LLaVA-OneVision (Li et al., 2024a) and Cambrain (Tong et al., 2024) to enhance the models image comprehension capabilities. Data about math/reasoning and cross-duplicated data in the two datasets are removed."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We present our experimental results as follows: Sec. 4.1 details our experimental setup. In Sec. 4.2, we evaluate the effectiveness of our multi-granularity feature encoding and diffusion-based multigranularity image decoders. We then demonstrate PUMAs versatility across various tasks: diverse text-to-image generation (Sec. 4.3), image editing (Sec. 4.4), conditional image generation (Sec. 4.5), and vision-language understanding (Sec. 4.6)."
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Fine-grained image reconstruction of SEED-LLaMA (Ge et al., 2023), SEED-X (Ge et al., 2024b), Emu2 (Sun et al., 2024b) and PUMA (f0 scale). High quality image reconstruction is the foundation of precise image manipulation tasks. Table 1: Image decoding evaluation using image encoder and decoder on the ImageNet validation set. PSNRr and LPIPSr measure the difference between reconstructed and ground truth images. PSNRd and LPIPSd measure the difference between two separate reconstructions of the same image, reflecting decoding diversity. Model SEED-LLaMA (2023) Encoder foundation BLIP-2 ViT (0.3B) SEED-X (2024b) Qwen-VL Encoder (4B) Emu2 (2024b) EVA02-CLIP-E-plus (4B) PUMA (f4 scale) PUMA (f3 scale) PUMA (f2 scale) PUMA (f1 scale) PUMA (f0 scale) CLIP-Large (0.3B) CLIP-Large (0.3B) CLIP-Large (0.3B) CLIP-Large (0.3B) CLIP-Large (0.3B) 4.1 SETUP Token num. PSNRr LPIPSr PSNRd LPIPSd 0.6189 0.4292 0.2101 0.5751 0.5329 0.4354 0.3631 0.1559 0.6756 0.5152 0.2532 0.6481 0.5971 0.4992 0.4325 0.2215 10.45 11.60 16.07 12.82 12.61 13.50 14.12 19.36 9.73 10.86 15.72 10.76 11.04 12.35 13.26 18. 32 64 64 1 4 16 64 256 Our unified multi-granular MLLM employs LLaMA-3 8B (Touvron et al., 2023) as the language model backbone and CLIP-Large (224 224 input) (Radford et al., 2021) as the image encoder. The image decoders are initialized from pretrained SDXL models (Podell et al., 2023). For more details on the experimental setup, please refer to the Appendix. 4.2 MULTI-GRANULAR VISUAL DECODING We evaluate the multi-granular visual decoding capabilities of our model using multi-scale features from the encoder (Sec. 3.1) and dedicated visual decoders (Sec. 3.2). Our aim is twofold: to achieve precise reconstruction using fine-grained feature scales (such as f0 and f1), and to implement high diversity semantics-guided image generation using coarse-grained features (such as f4 and f3). It is worth mentioning that in this subsection we validate the multi-granularity encoder and decoders  (Fig. 4)  , while the MLLM (Sec. 3.3) is not leveraged for the experiments in this subsection. 4.2.1 FINE-GRAINED IMAGE RECONSTRUCTION Fine-grained image reconstruction is crucial for preserving image details, yet it has posed significant challenges for models like SEED-LLaMA (Ge et al., 2023), SEED-X (Ge et al., 2024b), and Emu2 (Sun et al., 2024b). While SEED-LLaMA and SEED-X struggle with detailed reconstruction, limiting their precise image manipulation capabilities without additional techniques such as conditional image input (as used in SEED-X), Emu2 attempts to improve reconstruction by scaling up its image"
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Diversity visualization of text-to-image generation results from PUMA feature scales f4 (1 visual token), f3 (4 visual tokens), and Emu2 (Sun et al., 2024b). The generated features are input to corresponding diffusion-based decoders with different random seeds. Table 2: Diverse text-to-image generation evaluation on MSCOCO 30K validation set. CLIP-I and CLIP-T measure the similarity between generated images and ground truth images or prompts. LPIPSd quantifies the difference between two images generated from the same prompt, reflecting generation diversity. 5-scale Max denotes selecting the image with the highest score among the 5 outputs and computes the average maximum value. Model SD-v1.5 (2022) DALL-E 2 (2022) SDXL (2023) DALL-E 3 (2023) SEED-LLaMA (2023) Emu (2023) Emu2 (2024b) SEED-X (2024b) PUMA (f4 scale) PUMA (f3 scale) PUMA (5-scale Max) Token num. - - - - 32 64 64 64 1 4 - CLIP-I 0.667 - 0.674 - 0.682 0.656 0.686 0.729 0.699 0.703 0.736 CLIP-T 0.302 0.314 0.310 0.320 - 0.286 0.297 0.314 0.295 0.300 0. LPIPSd 0.692 - 0.600 - 0.652 0.700 0.329 0.493 0.613 0.558 - encoder to 4 billion parameters. Our approach achieves superior reconstruction quality with more efficient architecture. We employ the CLIP-Large encoder (0.3 billion parameters), which is over 10 times smaller than Emu2s, and implement fine-grained level image embedding with 256 tokens. As demonstrated in Tab. 1, our method using f0 scale features achieves 18.16 PSNRr and 0.2215 LPIPSr (Zhang et al., 2018) on the ImageNet validation set reconstruction. These results outperform Emu2s reconstruction performance and significantly surpass SEED-LLaMA and SEED-X (without conditional input). Fig. 5 visually illustrates our methods superior reconstruction quality. 4.2.2 SEMANTICS-GUIDED GENERATION While fine-grained reconstruction is crucial for precise image manipulation, tasks like text-to-image generation benefit from balance of semantic fidelity and output diversity. Our approach leverages coarse-grained features (such as f4) to implement semantics-guided image generation that preserves diversity in outputs. To quantify this semantics-guided diversity, we decode twice to obtain two images from the same image input using different random seeds and measure their differences, denoted as PSNRd and LPIPSd. Tab. 1 presents the diversity results for various visual decoding models and feature scales. Notably, our f3 and f4 scale decoders produce more diverse samples compared to the decoders in SEED-X and Emu2, while still preserving the core semantics of the input, as illustrated in Fig. 5. This demonstrates our approachs effectiveness in balancing semantic accuracy with generative diversity, crucial factor in tasks like text-to-image generation. 4.3 DIVERSE TEXT-TO-IMAGE GENERATION Our method can generate diverse outputs by utilizing the coarse-grained feature (f4 and f3 scales). This capability enables our model to produce diverse images that correspond to text conditions. Fig. 6 demonstrates that when generating images with fixed text prompt utilizing feature scales f4 and f3, our model achieves high generation diversity. It also shows that f4 scale outputs exhibit higher diversity, while f3 scale results demonstrate better consistency. In contrast, the generation"
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: Visualization of text-to-image generation results from PUMA feature scales f4 (1 visual token) and f3 (4 visual tokens). Table 3: Image editing evaluation on Emu-edit test benchmark (Sheynin et al., 2024). 5-scale Max denotes selecting the image with the highest score among the 5 outputs and computes the average maximum value. Model InstructPix2Pix (2023) MagicBrush (2024a) EMU-Edit (2024) OmniGen (2024) PUMA (f1 scale) PUMA (f0 scale) PUMA (5-scale Max) CLIP-I 0.834 0.838 0.859 0.836 0.802 0.840 0.846 CLIP-T 0.219 0.222 0.231 0.233 0.258 0.264 0.270 DINO 0.762 0.776 0.819 0.804 0.679 0.784 0. results of Emu2 (Sun et al., 2024b) show low diversity. For qualitative evaluation, Fig. 7 presents visualizations of our models text-to-image generation with various prompts. For quantitative results, we evaluate our model on the MSCOCO 30K validation dataset (Lin et al., 2014) and present the CLIP-I, CLIP-T, and LPIPSd in Tab. 2, which the former two metrics measures the consistency while LPIPSd measures generation diversity. Compared with recent works, our model demonstrates superior performance in generation quality, diversity, and prompt relevance. 4.4 IMAGE EDITING To assess PUMAs image editing capabilities, we evaluated it on the Emu-Edit test benchmark (Sheynin et al., 2024). Tab. 3 presents the results using CLIP-I, CLIP-T, and DINO (Caron et al., 2021) scores. CLIP-I and DINO scores measure the models ability to preserve elements from the source image, while CLIP-T reflects the consistency between the output image and the target caption. Our results demonstrate that PUMA exhibits strong preservation ability, second only to the current state-of-the-art model, EMU-Edit. Notably, PUMA achieves significantly better CLIP-T"
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: Left: Visualizations of PUMAs image editing result. Image editing utilizes f0 scale feature to preserve the fine-grained detail of input image. Right: Visualization of PUMAs conditional generation results. ❶: canny-to-image generation; ❷: image inpainting; ❸: image colorization. Figure 9: Comparison of f0 and f1 feature scales for tasks requiring precise controllability. Table 4: Evaluation on multimodal understanding benchmarks. PUMA utilizes CLIP-Large encoder with 224 224 input. Und. and Gen. denote understanding and generation, respectively. Type Und. Only Und. and Gen. Model LLaVA-v1.5 (2024a) InstructBLIP (2023) Qwen-VL-Chat (2023) mPLUG-Owl2 (2024b) Emu (2023) NExT-GPT (2023) SEED-X (2024b) Chameleon (2024) Emu2-Chat (2024b) PUMA (Ours) # Params MMB MME GQA VQAv2(test) POPE Vizwiz 7B 13B 7B 7B 13B 7B 17B 34B 40B 8B 64.3 - - 64.5 - 58.0 75.4 - - 68.9 1510.7 1212.8 1487.5 1450.2 - - 1457.0 - - 1490.3 62.0 49.5 57.5 56.1 - - 47.9 - 65.1 60. 78.5 - 78.2 79.4 57.2 66.7 - 66.0 84.9 76.2 85.9 78.9 - 85.8 - - 84.2 - - 85.2 50.0 33.4 38.9 54.5 - 48.4 - - 54.9 47.9 scores, even surpassing the state-of-the-art model. This indicates superior alignment between edited images and target captions. For qualitative evaluation, Fig. 8 provides visualizations of the editing results, illustrating PUMAs effectiveness in image manipulation tasks. 4.5 CONDITIONAL IMAGE GENERATION We select subset of canny-to-image, inpainting, and colorization tasks from the multigen-20M dataset to train PUMAs conditional image generation ability. Fig. 8 demonstrates the conditional generation results for these tasks. The f0 feature scale results provide the highest preservation of image details, particularly for tasks like inpainting and colorization, while the f1 scale offers better overall visual fidelity with limited generation diversity. 4.6 IMAGE UNDERSTANDING We evaluate PUMAs image understanding performance on several MLLM benchmarks, including MMB (Liu et al., 2023), MME (Fu et al., 2024), GQA (Hudson & Manning, 2019), VQAv2 (Antol"
        },
        {
            "title": "Technical Report",
            "content": "et al., 2015), POPE (Li et al., 2023), and Vizwiz (Gurari et al., 2018). Tab. 4 presents the results of this evaluation. Despite PUMAs relatively few 8B parameters and the use of an image encoder with 224 224 resolution input, it demonstrates competitive and often superior image understanding performance compared to other unified understanding and generation models. Notably, PUMAs performance on some metrics even surpasses that of understanding-only baselines. This performance can be attributed to PUMAs use of multi-granular continuous visual tokens as input to the MLLM. detailed ablation study examining the impact of different scale features as input on image understanding tasks is provided in the Appendix, offering further insights into the effectiveness of PUMAs multi-granular approach."
        },
        {
            "title": "4.7 ABLATION",
            "content": "We conduct an ablation study to examine the impact of feature scale selection on tasks requiring fine-grained controllability. Fig. 9 compares the outputs of f0 and f1 feature scales for image editing and colorization tasks. The results demonstrate that f1 scale features are insufficient for preserving crucial image details, while f0 scale features maintain the necessary fine-grained information for precise manipulation tasks. More ablation studies are in the Appendix."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce PUMA, novel unified multi-granular MLLM that unifies various granular tasks in visual generation and understanding. By leveraging multi-granular representations, PUMA effectively addresses the challenge of balancing diversity and controllability in image generation tasks. Our approach demonstrates superior performance across spectrum of visual tasks, including diverse text-to-image generation, image editing, inpainting, colorization, conditional generation, and understanding. PUMAs ability to adapt to varying granularity requirements within single framework represents significant advancement in MLLM capabilities. This work opens up new possibilities for more versatile and powerful multimodal AI systems, contributing to the broader goal of achieving artificial general intelligence in multimodal domains."
        },
        {
            "title": "REFERENCES",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pp. 24252433, 2015. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Laura Jannes Burger. Laion: Image data, ai, and dispossession. Masters thesis, 2023. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF Conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition, pp. 2418524198, 2024. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023."
        },
        {
            "title": "Technical Report",
            "content": "Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024a. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024b. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2668926699, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023."
        },
        {
            "title": "Technical Report",
            "content": "I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual In 2019 international conference on document question answering by reading text in images. analysis and recognition (ICDAR), pp. 947952. IEEE, 2019. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8871 8879, 2024. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024a. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024b. Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024a. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1304013051, 2024b. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024a. Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024b. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zeroinit attention. arXiv preprint arXiv:2303.16199, 2023a. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023b. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023a. Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. Vl-gpt: generative pre-trained transformer for vision and language understanding and generation. arXiv preprint arXiv:2312.09251, 2023b."
        },
        {
            "title": "A PUMA TRAINING",
            "content": "A.1 VISUAL DECODING TRAINING A.1.1 DATASET DETAILS For training the image decoding process, we leverage three large-scale datasets: Laion-2B (Schuhmann et al., 2022), Laion-Aesthetics (Burger, 2023), and JourneyDB (Sun et al., 2024a). To ensure high-quality generation capabilities, we apply resolution-based filtering criterion, selecting only images with resolutions of 512 512 pixels or larger. We only use center crop as the data augmentation method. A.1.2 TRAINING SETTINGS We train five dedicated image decoders for the f0, f1, f2, f3, and f4 scale features respectively. The image encoder is the frozen CLIP-L image encoder (Radford et al., 2021). Each image decoder is initialized from the SDXL model. The VAE (Kingma, 2013) remains frozen throughout the training process. The corresponding image features are input to the diffusion model through the crossattention mechanism, replacing the original text embedding input. We train the decoders using AdamW optimizer (Loshchilov, 2017) with maximum learning rate of 8e-5, using linear learning rate decay and gradient clipping value of 1.0. The training batch size is 1,024. The training steps for the five features are 40, 000, 30, 000, 20, 000, 15, 000, and 10, 000 respectively, with features containing more visual tokens using longer training steps. We use noise off value of 0.1 and random drop of 10% of the input image to blank image for classifier-free guidance. A.2 MLLM TRAINING A.2.1 TRAINING OBJECTIVE PUMA employs unified framework with supervision on both text tokens and image features. For text tokens, we use cross-entropy classification loss, while for image features, we adopt MSE regression loss. To balance the contribution of text and image outputs, we apply loss ratio of 0.02 for text and 1.0 for image features. Within the image feature regression loss, we use different ratios for the progressively generated 5 scales of image features (f4, f3, f2, f1, and f0), with ratios of 1024.0, 512.0, 64.0, 8.0, and 1.0 respectively. This scaling compensates for the varying number of tokens at each feature scale, with larger ratios for scales with fewer tokens. The training loss objective remains consistent across both the pretraining and instruction tuning phases. A.2.2 PRETRAINING DATASET DETAILS During PUMAs pretraining phase, we utilize diverse set of datasets including Laion-2B (Schuhmann et al., 2022), Laion-Aesthetics (Burger, 2023), GRIT (Peng et al., 2023), The Pile (Gao et al., 2020), OCR-VQA-200K (Mishra et al., 2019), and LLaVAR (Zhang et al., 2023b). For the imagetext pair data in Laion-2B, Laion-Aesthetics, and GRIT, we randomly assign 50% of the samples to text-to-image training and 50% to image-to-text training, fostering both image generation and understanding capabilities. We employ center crop as the primary image augmentation technique. To train on the GRIT dataset for object grounding, we append 224 additional position tokens to the MLLMs codebook, representing object positions with bounding box coordinates [x min, min, max, max]. We construct the training sequences by appending the tokens <s> and </s> to denote the beginning and end of each sequence. At the beginning and end of each image feature sequence, we include the special tokens [IMG] and [/IMG] to indicate the visual position. A.2.3 PRETRAINING SETTINGS We conduct pretraining for 100K steps using the AdamW optimizer with batch size of 2048. The maximum learning rates are set to 1e-4 for the projector and 3e-5 for the LLaMA backbone. We employ 2,000-step warm-up period, cosine learning rate decay, and gradient clipping at 5.0 during pretraining. To optimize memory usage and computational efficiency, training is accelerated using DeepSpeed ZeRO Stage 3. The entire pretraining process is carried out on 256 NVIDIA V100 GPUs over period of 10 days."
        },
        {
            "title": "Technical Report",
            "content": "A.2."
        },
        {
            "title": "INSTRUCT TUNING SETTINGS",
            "content": "High-quality Text-to-Image Generation: We utilize Laion-Aesthetics (Burger, 2023) and JourneyDB (Sun et al., 2024a) with data ratio 1:1 to instruct tune the text-to-image generation model based on the previous pretraining checkpoint. We use training batch size 2048 and train for 20,000 steps with the max learning rate 1e-5, warm up 1,000 steps, and cosine learning rate decay. Random crop with fixed aspect ratio is adopted as the image augmentation. Precise Image Manipulation: We train the image manipulation task with SEED-Edit Ge et al. It contains seven different operations: background alteration, comprehensive image (2024a). changes, style alteration, object removal, object addition, localized modifications, and color/texture alterations. We train with batch size 1024 and train for 10,000 steps. The max learning rate is 1e-5, warm-up is 500 steps, and cosine learning rate decay is adopted. We apply random crop with fixed aspect ratio on the accordingly input image and output image. The sequence of the image manipulation sample is like <s>[IMG]embedding of origin image[/IMG]instruct editing prompt[IMG]embedding of edited image[/IMG]</s>. Conditional Image Generation: We train on the subset of MultiGen-20M dataset (Qin et al., 2023) including canny-to-image, image inpainting, and colorization. We use the training batch size 1, 024 and train for 20, 000 steps. The max learning rate is 1e-5, warm-up is 500 steps, and cosine learning rate decay is adopted. We apply center crop as the image augmentation. The sequence of the conditional image generation is like <s>[IMG]embedding of origin image[/IMG]instruct conditional generation prompt[IMG]embedding of edited image[/IMG]</s>. The instruct conditional generation prompt contains the caption of the target image and with 50% probability contain the task instruction like Please convert the canny image to natural image. Image Understanding: We train image understanding task on the subset of LLaVA-OneVision (Li et al., 2024a) and Cambrain (Tong et al., 2024). Data about math/reasoning and cross-duplicated data in the two datasets are removed. We train with the batch size 512 and train all data for 1 epoch. The max learning rate is 1e-5 with the warm-up 500 steps. Cosine learning rate decay is adopted. We apply resizing as the image augmentation. Supervision is only applied to the output text tokens. We use the system message chat between curious user and an artificial intelligence assistant. detailed, and polite answers to the users questions. The assistant gives helpful,"
        },
        {
            "title": "B EVALUATION DETAILS",
            "content": "B.1 IMAGE RECONSTRUCTION EVALUATION To evaluate the reconstruction performance of different scales of features and our baselines, we use the ImageNet validation set, comprising 50,000 images. Each image is resized to rectangular shape before being input into each image encoder. We assess reconstruction precision by computing PSNRr and LPIPSr, which measure the difference between the reconstructed image and the original image. Given the inherent randomness in the decoders, we measure reconstruction diversity by reconstructing each original image twice using different random seeds. We then calculate PSNRd and LPIPSd to quantify the difference between these two reconstructed images. Higher diversity is beneficial for downstream tasks such as text-to-image generation. For PSNR and LPIPS evaluations, we use resolution of 256 256 to align with the evaluation settings in previous works. For LPIPS evaluation specifically, we employ AlexNet as the feature extractor. B.2 TEXT-TO-IMAGE GENERATION EVALUATION We evaluate text-to-image generation on the COCO 30K validation set (Lin et al., 2014). We use CLIP-I and CLIP-T scores to measure the consistency between the generated image and the ground truth image and caption, respectively. CLIP-Base-32 serves as the feature extractor for these metrics. To assess generation diversity, we calculate LPIPSd between two images generated using the same input prompt but different random seeds. The LPIPSd measurement details are consistent with those described in Sec. B.1."
        },
        {
            "title": "Technical Report",
            "content": "Table 5: Ablation of different visual token input on image understanding. The experiments are conducted on LLaVA-v1.5 setting with CLIP-Large-224 visual encoder. Visual token type f4 f3 f2 f1 f0 f4-f0 Token number 1 4 16 64 256 341 MMB 56.8 58.3 61.5 63.6 65.4 65.1 MME 1252.6 1285.5 1403.0 1400.8 1464.9 1445.5 GQA 0.0 0.0 46.6 58.4 58.8 61. VQAv2(test) 64.1 67.0 71.1 74.4 76.9 76.9 B."
        },
        {
            "title": "IMAGE EDITING EVALUATION",
            "content": "We evaluate image editing performance on the Emu-Edit benchmark Sheynin et al. (2024). To assess editing quality, we adopt CLIP-I, CLIP-T, and DINO scores. CLIP-I and DINO Caron et al. (2021) scores measure the models ability to preserve elements from the source image, while CLIPT reflects the consistency between the output image and the target caption. For the DINO score, we employ DINO-Small-16 as the feature extractor. B.4 IMAGE UNDERSTANDING EVALUATION For image understanding tasks, we use the same evaluation setting as LLaVA-v1.5 (Liu et al., 2024a). During evaluation, we use the system message chat between curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions."
        },
        {
            "title": "UNDERSTANDING TASK",
            "content": "Given that PUMA adopts unified multi-granular image feature as both input and output for the MLLM backbone, we conducted an ablation study to investigate the influence of different scales of image feature input on image understanding tasks. For fair comparison, we adopted the standard LLaVA-1.5-7B pretraining and finetuning setting, only changing the image encoder to 224-input CLIP-Large with different granularities of features. Tab. 5 presents the results of this ablation study. The findings demonstrate that finer-grained features generally lead to better performance in image understanding tasks. Notably, utilizing all image features from f4 to f0 (the PUMA setting) achieves comparable performance to using all 256 visual tokens of the finest scale (f0). These results validate that the unified visual input and output format of PUMA provides robust foundation of visual features for image understanding tasks, effectively balancing performance across different granularities. SELECTION OF 5 SCALE FEATURES IN TEXT-TO-IMAGE GENERATION PUMA generates images at 5 granularity levels, allowing users to select the output that best meets their requirements. In our evaluation of diverse text-to-image generation, we produce 5 image outputs for each input prompt, corresponding to the 5 feature scales. To assess performance, we select the image with the highest CLIP-I and CLIP-T scores among the 5 outputs and compute the average maximum value. Tab. 6 presents the CLIP-I and CLIP-T scores for each of the 5 feature scales. The results demonstrate that different granularity levels excel in various aspects of image generation. Notably, the ability to select the best output from multiple scales (PUMA 5-scale Max) yields significantly improved CLIP-I and CLIP-T scores compared to any single scale, highlighting the advantage of PUMAs multi-granular approach."
        },
        {
            "title": "Technical Report",
            "content": "Figure 10: Visualization of PUMA text-to-image outputs across five scale features given the generation prompt. Table 6: CLIP-I and CLIP-T scores on MSCOCO 30K validation set with different feature scales. Model PUMA (f4 scale) PUMA (f3 scale) PUMA (f2 scale) PUMA (f1 scale) PUMA (f0 scale) PUMA (5-scale Max) Token num. 1 4 16 64 256 - CLIP-I 0.699 0.703 0.703 0.693 0.621 0.736 CLIP-T 0.295 0.300 0.301 0.299 0.280 0. QUALITATIVE RESULTS OF TEXT-TO-IMAGE GENERATION ON FIVE"
        },
        {
            "title": "SCALE FEATURES",
            "content": "In the text-to-image generation task, PUMA produces five distinct images corresponding to the five feature scales, all derived from single input generation prompt. Fig. 10 presents samples of outputs across these five scales for given generation prompts."
        },
        {
            "title": "F MORE QUALITATIVE RESULTS",
            "content": "We present more qualitative cases for image reconstruction, diverse text-to-image generation, editing, and conditional image generation, as shown in Figures 11 to 15."
        },
        {
            "title": "Technical Report",
            "content": "Figure 11: More visualizations on multi-granular visual decoding from fine-grained to coarsegrained granularity."
        },
        {
            "title": "Technical Report",
            "content": "Figure 12: More visualizations on fine-grained image reconstruction with f0 scale feature."
        },
        {
            "title": "Technical Report",
            "content": "Figure 13: More visualizations on text-to-image generation utilizing f4 and f3 scales."
        },
        {
            "title": "Technical Report",
            "content": "Figure 14: More visualizations on image editing. Figure 15: More visualizations on conditional image generation."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "HKU MMLab",
        "SenseTime",
        "Shanghai AI Laboratory",
        "THU"
    ]
}