{
    "paper_title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
    "authors": [
        "Kaiwen Wang",
        "Jin Peng Zhou",
        "Jonathan Chang",
        "Zhaolin Gao",
        "Nathan Kallus",
        "Kianté Brantley",
        "Wen Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of \"step,\" which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced."
        },
        {
            "title": "Start",
            "content": "Value-Guided Search for Efficient Chain-of-Thought Reasoning Kaiwen Wang*1, Jin Peng Zhou*1, Jonathan Chang*4, Zhaolin Gao1, Nathan Kallus1,3, Kianté Brantley2, and Wen Sun1 1Cornell University 2Harvard University 3Netflix 4Databricks"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require fine-grained notion of step, which is difficult to define for long-context reasoning models. By collecting dataset of 2.5 million reasoning traces, we train 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced at https://github.com/kaiwenw/value-guided-search. 5 2 0 2 3 2 ] . [ 1 3 7 3 7 1 . 5 0 5 2 : r Figure 1: Performance and Efficiency of Value Guidance: (Left) Value-guided search improves the overall quality of DeepSeek-R1-Distill responses across four combined competition math benchmarks (AIME & HMMT). The inference budget for 1.5B, 7B and 14B are 256, 128 and 64 generations, respectively. (Right) Value-guided search also reduces the inference FLOPs required to achieve the same accuracy levels as majority voting, standard TTC scaling baseline, showing value-guidance is promising for improving efficiency. *Equal contribution. Correspondence to {kw437,jz563}@cornell.edu. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Recent large language models (LLMs), such as OpenAI o1 & o3, Claude Sonnet 3.7, Gemini Pro 2.5 and DeepSeek R1 [18] are trained via reinforcement learning (RL) to think for many tokens before generating final answer. Through multi-step reasoning and self-correction, these reasoning models have state-of-the-art performance in competition math, coding [16] and scientific research [37], often surpassing the average human. However, this enhanced capability comes at cost: each generation involves long chain-of-thought (CoT), thus requiring more inference compute. Further, these CoT traces can often be repetitive and get stuck in unproductive loops [30]. This raises two questions. Can we extract the same performance at fraction of the inference compute by refining the thinking process? Can we improve the performance ceiling of these models with productive search methods? Search with guidance models is natural solution that addresses longer chain-of-thought reasoning by managing the exponential growth of possible paths with guidance models identifying optimal routes [15, 39, 35]. Prior works that combined search with LLMs proposed to guide search with process reward models (PRMs), predicting the correctness of each step (e.g., delimited by newlines) in the model-generated solution [23, 44, 50]. While PRM-guided search has been shown to improve test-time compute (TTC) [6, 36, 34, 25], it is challenging to scale existing PRM training techniques to long-context reasoning models. First, existing methods require pre-defined notion of step, but, per Guo et al. [18], it is challenging to explicitly define fine-grain step in general reasoning. Second, even if we can define step, collecting step-wise labels is prohibitively expensive, since it requires annotations from humans [23], LLM-as-a-Judge [50], or multiple Monte Carlo roll-outs [44, 27]. Thus, there has been limited success to scale PRMs to long-context reasoning models [18]. We propose value-guided search (VGS) block-level search method guided by token-level value model as promising approach to scale TTC for reasoning models. In Section 2, we present an effective pipeline for value model training on tasks with outcome labels, such as competition math. Our data pipeline collects solution prefixes from various models and then, starting from random prefixes, generates completed solutions using lean reasoning model (e.g., DeepSeek-R1-Distill-1.5B). Notably, our data collection does not require pre-defined notion of step and is more efficient than existing techniques [44, 27]. With this pipeline, we collect dataset of 2.5 million math reasoning traces (over 30 billion tokens) from filtered subset of the OpenR1-Math dataset [2]. Then, we train 1.5B token-level value model called DeepSeek-VM-1.5B by regressing (via classification) the final reward of the completed solution. Next, in Section 3, we apply our value model to perform block-wise search with DeepSeek models [18] on competition math, where we evaluate on four prestigious high school math competitions in the US (AIME 2024 & 2025 and HMMT 2024 & 2025). Our experiments show that block-wise VGS significantly improves TTC compared to majority voting or weighted majority voting, strong baselines from the literature [45, 6]. We also show that VGS with DeepSeek-VM-1.5B leads to higher performance than searching with state-of-the-art PRMs, demonstrating that our value model can provide better feedback. When given an inference budget of 64 generations, VGS on DeepSeek-R1-Distill-Qwen-1.5B can outperform o3-mini-medium, and VGS on DeepSeek-R1-Distill-Qwen-14B (total size with value model is 15.5B) is on par with DeepSeek-R1 (671B) on our competition math evaluations (Fig. 1 left). Moreover, we show that VGS reduces the amount of inference compute required to attain the same performance as majority voting (Fig. 1 right). In summary, we find that block-wise VGS not only improves the performance ceiling of reasoning models, but also reduces the amount of inference compute required to match the performance of standard TTC methods. Our contributions are summarized below: 1. simple recipe for token-level value model training that does not require pre-defined notion of step and scales to long-context reasoning traces (Section 2). 2. Block-wise search, guided by our 1.5B value model, achieves impressive performance on four challenging math competitions, outperforming standard TTC methods (e.g., best-of-N , majority voting) and search with existing PRMs (Section 3). 3. We open-source our dataset of 2.5 million reasoning traces, value model, and codebase (including data filtering and distributed training scripts) to support future work on applying VGS to other domains. https://github.com/kaiwenw/value-guided-search. Please see Appendix for detailed discussion of related works."
        },
        {
            "title": "2 Methods",
            "content": "We present an end-to-end pipeline for training token-level value model and applying it to guide block-wise search. In Section 2.1, we introduce necessary notation and present regression-viaclassification algorithm for learning the token-level value model [14]. Then, in Section 2.2, we outline an efficient data pipeline for creating our dataset of 2.5 million reasoning traces from DeepSeek models. Finally, in Section 2.3, we describe several TTC methods and baselines, e.g., best-of-N , (weighted) majority voting and search algorithms that can leverage our value model. While we focus on competition math in this paper, we remark that our pipeline can in principle be applied to any task with automated outcome supervision (e.g., reward model). In Appendix B, we summarize simple recipe for applying VGS to other such domains. 2.1 Learning Algorithm for Value Model We describe our training process for language value model by performing regression via classification [7]. Let be the vocabulary and let = (cid:83) nN denote the input sequence space. Given problem prompt and response S, let κ = Γ(x, y) [0, 1, . . . , 1] denote its class label, where is the number of classes. Furthermore, let = R(x, y) denote the scalar reward, which we assume to be binary since we focus on competition math (see Appendix for the general case). For our value model, κ = 2 if the response is an incomplete generation (i.e., exceeds max generation length), κ = 0 if the response finished and is incorrect, and κ = 1 if the response finished and is correct. Thus, the event that κ = 1 corresponds to = 1 (correct answer), and κ {0, 2} corresponds to = 0 (incorrect or exceeds max length). We adopt this convention in the rest of the paper. We remark that regression-via-classification is standard approach that leads to better down-stream decision making than regressing via squared error [7, 20, 17, 3, 42, 43]. We employ datasets of the form = {(xi, yi, zi, κi)}i[N ], where xi is the problem prompt, yi is partial response (which we call roll-in), zi is completion starting from yi (which we call roll-out), and κi = Γ(xi, yi, zi) is the label of the full response, where x, y, denotes the concatenation of x, and z. In this paper, we assume that the completions / roll-outs zi are generated by fixed roll-out policy πref, i.e., zi πref( xi, yi) for all i. We remark that good choice for πref is cost-efficient model which is capable of producing diverse responses with positive reward, e.g., distilled version of large reasoning model. (cid:80) (xi,yi,zi,κi)B L(θ; B) = 1 We train classifier fθ : (cid:55) ([K]) via gradient descent on the following loss on data batch B: (cid:80) 1 zi where ℓce(ˆp, κ) = ln(ˆp[κ]) is the standard cross-entropy loss for classification and z:h denotes the first tokens of zi. The rationale for the inner average is analogous to next-token prediction training of autoregressive models: since zi is generated autoregressively by πref, any suffix zh: is also roll-out from πref and hence can be viewed as another data-point. We found this to be an important training detail for performance, which is consistent with prior work who used similar objective for training an outcome reward model [14, 23]. hrange(zi) ℓce(fθ(xi, yi, z:h ), κi), We can now view the classifier as value model. Since κ = 1 corresponds to the event that = 1, we have that Vθ(x) := fθ(x)[1] predicts the correctness probability of roll-outs from Figure 2: Summary of Methods. (Left) Diagrams how we collect multiple roll-ins (grey circles representing tokens) per problem, and branch off multiple roll-outs per roll-in at random points. The class label for each roll-out token is the outcome label at the very end. (Right) Shows the beam search process (beam width 2 and budget 4) guided by value model. 3 Algorithm 1 Beam Search with Width Algorithm 2 Best-of-N 1: Input: prompt x. 2: Set num beams . 3: Initialize beams y1, . . . , yB x. 4: while s.t. yj is not finished do 5: For each s.t. yj is not finished, sample 6: i.i.d. blocks {bi,j}i[w] from π( yj). Update unfinished beams to be the best continuations with the highest (yj, bi,j). 7: end while 8: return BoN or WMV on {y1, . . . , yB}. 1: Input: prompt x, responses {yi}i[N ]. 2: return ybon = arg maxyi (x, yi). Algorithm 3 (Weighted) Majority Vote 1: Input: prompt x, responses {yi}i[N ], weights {wi}i[N ], equivalence relation . 2: Partition {yi}i into equiv. classes {pk}k. 3: return response from the highest weight partition arg maxpk (cid:80) yipk wi. πref. Indeed, if denotes the optimal classifier that minimizes the population-level loss, then (x, y)[1] = Ezπref(x,y)[R(x, y, z) x, y] which is the expected reward of completed response from rolling-out πref starting from x, y. In sum, our value model is learned via predicting labels (one of which corresponds to reward 1), and the training objective is the standard cross-entropy loss. 2.2 Dataset Creation Process We describe our process for creating OpenR1-VM, novel dataset of 2.5 million reasoning responses from DeepSeek models, across 45k math problems from OpenR1-Math [2]. Pre-Filtering. We start from the OpenR1-Math dataset (default split) [2] which contains 94k math problems with solutions that were already filtered for quality. Upon manual inspection, we found that this dataset still contained unsolvable problems (e.g., problems that require web browsing but our models cannot access the web) and ambiguous or unverifiable answers (e.g., multiple boxed{} expressions or unparsable answers). We filter out all such problematic problems, producing cleaned subset of 50k problems with solutions verifiable by sympy or math-verify [22]. We call this pre-filtered dataset OpenR1-Cleaned. Response Generation. Next, we collect roll-ins and roll-outs from DeepSeek models [18]. We fix the roll-out policy πref as DeepSeek-R1-Distill-Qwen-1.5B. To ensure diversity in the rollin distribution, we sample 14 independent roll-ins from four DeepSeek-R1-Distill-Qwen model sizes: 1.5B, 7B, 14B, and 32B by generating until the end of thinking token <think>.1 For each rollin yi, we then sample four random prefix locations where we generate complete roll-outs {zj }j[4] from πref. Finally, to compute the class label (incomplete, incorrect, or correct), we parse the response for the final answer (enclosed in boxed{}) and use math-verify to check for correctness against the ground truth answer. In total, this process (illustrated in Fig. 2 left) yields 56 labeled roll-in, roll-out pairs per problem, leading to 2.8 million datapoints. Post-Filtering. We filter out problems where all 56 roll-outs for that problem were incomplete or incorrect (i.e., has reward 0). This post-filtering removes any ambiguous or unanswerable problems that we missed during pre-filtering, and also removes problems that are too difficult for πref and do not provide useful learning signal. This step filters roughly 10% of problems, yielding final dataset of 2.5 million datapoints. Notably, our approach does not require fine-grained notion of step and our data collection is cheaper than existing PRM techniques. Specifically, Lightman et al. [23] used per-step annotations by human experts, Zhang et al. [50] used per-step annotations via LLM-as-a-Judge, and Wang et al. [44] used multiple Monte Carlo roll-outs at every step. Since the number of newlines in reasoning CoT traces can grow very quickly, per-step labels are quite expensive to collect for reasoning models. In contrast, our approach only requires handful of roll-ins (from any policy) and roll-outs (from πref) per problem, and this number can be flexibly tuned up or down to trade-off data coverage and data collection cost. Please refer to Appendix for further details on each step. We also release our filtering code and datasets to support future research. 1DeepSeek-R1 and its distilled variants output CoT reasoning between tokens <think> and <think> followed by final solution, which is usually summarization of the CoT reasoning. 4 2.3 Algorithms for Test-Time Compute and Search Equipped with value model : (cid:55) R, we can now apply it to scale test-time compute of generator model π. For search-based approaches, we focus on block-wise search where block refers to sequence of tokens (e.g., blocks of 4096 tokens worked best in our experiments). We let denote the inference budget, which is the number of generations we can sample (e.g., generating four responses and taking majority vote is = 4). BFS. Breadth-first-search (BFS) [47, 28] is natural search method that approximates the optimal KL-regularized policy given good value model [52]. Given prompt x, BFS samples parallel blocks bi from π and selects the block with the highest value = arg maxbi (x, bi), which gets added to the prompt, i.e., x, b. The process repeats until the response finishes. Note the number of tokens generated from π is roughly equivalent to independent generations from π. Beam Search. One weakness of BFS is that parallel blocks are correlated because they share the same prefix, which limits diversity. Beam search with width (Algorithm 1) is generalization that keeps = N/w (assume to be integer) partial responses and branches parallel blocks from each one [26, 5, 40, 6, 36]. Given prompt x, beam search first generates parallel blocks. However, unlike BFS, beam search keeps the top beams with the highest scores, and then samples parallel blocks per beam at the next step. Since = blocks are sampled at each step, the compute budget is also . We illustrate beam search with = 4 and = 2 in Fig. 2 (right). DVTS. Diverse verifier tree search (DVTS) is meta-algorithm that further increases diversity by running parallel searches each with smaller budgets [6]. Specifically, DVTS-M runs parallel beam searches each with budget N/M (assume to be integer), and aggregates responses into final answer. We remark crucial detail of beam search and DVTS is how the final set of beams/responses are aggregated. Prior works [6, 36, 34] select the response with the highest score, which is analogous to final best-of-N (BoN; Algorithm 2). Instead, we found that taking weighted majority vote (WMV; Algorithm 3) led to much better performance, which is demonstrated by Fig. 3 (left). Computational Efficiency of Block-wise Search. Since value scores are only used at the end of each block or the end of the whole response, the FLOPs required for block-wise value model guidance is tiny fraction ( 1%) of the generation cost from π. We compute FLOP estimates in Appendix to concretely show this."
        },
        {
            "title": "3 Experiments\nWe extensively evaluate value-guided search (VGS) with our 1.5B value model DeepSeek-VM-1.5B,\nfocusing on guiding the CoT reasoning of DeepSeek models [18]. The best VGS setup for our value\nmodel is beam search with final WMV aggregation, beam width 2, block size 4096 and with DVTS\n(for larger inference budgets). We show this setup outperforms other test-time compute methods (e.g.,\nMV, WMV, BoN) and other scoring models (e.g., existing 7B PRMs and a 1.5B Bradley-Terry reward\nmodel trained on our dataset). We remark our search results use a fixed beam width and block size for\nall problems; this is more practical than prior works on “compute-optimal scaling” which vary search\nparameters for each problem and require estimating each problem’s difficulty [6, 36, 25]. Please see\nAppendices E and F for additional details on value model training and inference.",
            "content": "Benchmarks. We evaluate on the 2024 and 2025 editions of the American Invitational Mathematics Exam (AIME) and the February Harvard-MIT Mathematics Tournament (HMMT).2 Both AIME and HMMT are prestigious high school math competitions in the US that have also been used to evaluate frontier LLMs [31, 18, 1]. We use AIME & II and the individual part of HMMT, yielding 30 problems per competition. To mitigate overfitting on single widely used benchmark, we report the overall averaged accuracy unless otherwise stated. Per-benchmark plots are relegated to Appendix C. Baseline Models. We evaluate two state-of-the-art 7B PRMs with distinct training styles: Math-Shepherd-Mistral-7B-PRM [44] and Qwen2.5-Math-PRM-7B [50]. Math-Shepherd uses Monte-Carlo roll-outs from each step to estimate per-step value while the Qwen2.5 PRM uses LLMJudge annotation for each step, similar to the per-step human annotation of PRM800K [23]. As step-level value model, Math-Shepherd-PRM-7B is more related to our token-level value model. Finally, we also evaluate 1.5B Bradley-Terry (BT) [8] model, called DeepSeek-BT-1.5B, which we trained using our dataset (see Appendix for training details). 2https://maa.org/maa-invitational-competitions and https://www.hmmt.org 5 Test-time scaling DeepSeek-1.5B (N = 256) AIMEAIME-25 HMMT-24 HMMT-25 AVG VGS w/ DeepSeek-VM-1.5B (ours) WMV w/ DeepSeek-VM-1.5B (ours) VGS w/ DeepSeek-BT-1.5B (ours) WMV w/ DeepSeek-BT-1.5B (ours) VGS w/ Qwen2.5-Math-PRM-7B WMV w/ Qwen2.5-Math-PRM-7B VGS w/ MathShepherd-PRM-7B WMV w/ MathShepherd-PRM-7B MV@256 72.0 0.4 69.6 3.9 73.1 1.4 72.0 3.3 71.1 1.0 70.6 3.1 70.6 3.1 71.2 3.2 71.0 3.5 46.7 0.7 45.1 2.2 40.6 0.8 40.5 2.9 38.9 1.4 39.1 2.1 41.9 1.4 40.0 2.5 38.9 1. 31.4 2.0 29.1 2.6 28.1 1.9 25.3 2.3 26.7 1.2 25.4 2.4 30.0 1.4 27.9 2.3 24.4 1.7 32.8 0.8 28.9 2.6 27.5 0.0 24.6 4.7 24.2 0.2 24.0 3.2 23.9 1.4 25.6 3.1 24.3 2.9 45.7 1.0 43.2 1.4 42.3 0.5 40.6 1.6 40.2 0.5 39.8 1.4 41.6 0.9 41.2 1.4 39.7 1.2 Test-time scaling larger models with our DeepSeek-VM-1.5B VGS w/ DeepSeek-7B (N = 128) MV w/ DeepSeek-7B (N = 128) VGS w/ DeepSeek-14B (N = 64) MV w/ DeepSeek-14B (N = 64) Pass@N baselines for various models DeepSeek-1.5B Pass@1 DeepSeek-32B Pass@1 Deepseek-R1 (671B) Pass@1 o3-mini-medium Pass@1 o3-mini-medium Pass@8 o4-mini-medium Pass@1 o4-mini-medium Pass@8 82.2 0.8 77.1 1.1 59.4 0.8 56.5 1.6 42.8 2.8 34.7 1. 41.1 1.6 33.8 2.5 56.4 0.8 50.5 0.9 86.7 2.7 80.6 1.2 59.6 0.6 67.0 2.0 46.7 2.7 40.6 1.8 51.1 1.6 50.1 2. 61.0 0.9 59.6 0.9 28.2 6.1 77.2 4.9 85.0 2.1 49.2 6.8 83.0 4.6 85.4 4.3 95.4 2.6 22.4 4.1 60.4 6.0 70.0 0.9 45.8 6.6 77.4 3.9 82.3 4.5 93.3 0.4 13.9 4.2 38.0 4.6 41.7 3. 32.4 5.4 55.9 4.3 50.4 5.0 69.7 3.2 13.0 3.9 42.1 5.2 46.7 2.4 36.6 6.0 64.9 4.4 61.1 6.4 84.5 2.5 19.4 1.1 54.4 1.3 60.8 0.5 41.0 1.5 70.3 2.1 69.8 2.5 85.7 1.1 (Top) Weighted majority vote (WMV) and VGS results for DeepSeek-1.5B with an Table 1: inference budget of = 256, using various scoring models. (Middle) Compares MV and VGS for larger DeepSeek models guided with our DeepSeek-VM-1.5B. (Bottom) Lists performance of DeepSeek models and strong close-sourced reasoning models. For VGS, indicates standard deviation across 3 seeds; for MV, WMV, Pass@N, denotes bootstrap with 100 repetitions. We bold the highest avg. accuracy and underline second highest. Appendix C.1 contains more baselines. 3.1 Main Results  (Table 1)  In the top section of Table 1, we fix the generator to DeepSeek-1.5B3 and test-time budget to = 256, and compare VGS to weighted majority voting (WMV), using our value model, the BT model and baseline PRMs. We see that VGS and WMV with DeepSeek-VM-1.5B achieve the two highest scores, outperforming the BT reward model and prior PRMs. This shows that our value model is not only strong outcome reward model (ORM) but also an effective value model for guiding search. Notably, with budget of 256, our 1.5B value model can guide DeepSeek-1.5B (total parameter count is 3B) to reach parity with the pass@1 of o3-mini-medium, strong math reasoning model from OpenAI. Intriguingly, while DeepSeek-BT-1.5B was only trained as an ORM, we find that VGS also improves performance relative to WMV, suggesting that BT models may also provide meaningful block-wise feedback to guide search. We also observe that accuracies for the 7B baseline PRMs (MathSheperd and Qwen2.5-Math) are only slightly higher than MV@256 and do not improve with search, which suggests that these PRMs are likely out-of-distribution (OOD) for the long CoTs generated by DeepSeek-1.5B. In the middle section of Table 1, we guide stronger DeepSeek models with sizes 7B and 14B, and compare VGS to MV, standard TTC method that does not use an external scoring model. We see that VGS again achieves higher accuracy than MV for both 7B and 14B, which suggests that DeepSeek-VM-1.5B is also useful in guiding the CoT of stronger DeepSeek models. However, we observe that the gap between VGS and MV becomes smaller for larger DeepSeek models, suggesting that DeepSeek-14B CoTs may be becoming OOD for our value model, which was trained on DeepSeek-1.5B CoTs. To guide more capable models, new value models should be trained on rollouts from similarly capable models; we however do not foresee this being practical concern given the scalability of our training process (described in Section 2 and summarized in Appendix B). Finally, we note that the performance of all models on AIME-24 is consistently higher than other competitions, suggesting the importance of evaluating on diverse and newer competitions to reduce risk of overfitting or data contamination. 3Throughout the paper, we use DeepSeek-XB as shorthand for DeepSeek-R1-Distill-Qwen-XB. 6 Figure 3: Test-Time Compute with DeepSeek-VM-1.5B. (Left) Compares best-of-N (BoN), weighted majority voting (WMV) and VGS with either BoN or WMV for the final aggregation. (Right) Compares VGS to majority voting (MV), standard baseline that does not require scorer."
        },
        {
            "title": "3.2 Test-Time Compute Scaling for Search",
            "content": "This section presents three experiments designed to analyze the TTC scaling properties of VGS. Our investigation addresses three key research questions: 1. Does VGS, with its block-wise guidance, demonstrate superior performance compared to responselevel aggregation methods such as BoN or WMV? 2. How does the TTC scaling behavior of VGS compare to the standard score-free baseline MV? 3. How does the TTC scaling behavior of DeepSeek-VM-1.5B compare to baseline models? Response-Level Selection vs Search-Based Block-Level Selection. While BoN and WMV represent standard approaches for selecting responses using an ORM, block-wise VGS guides response generation through sequential block-by-block selection. As Fig. 3 (left) illustrates, WMV consistently outperforms BoN across all inference budget scales, which demonstrates the benefits of combining MV with value scores. Furthermore, VGS (with WMV as final aggregation step) yields additional improvements beyond WMV alone. This confirms the benefits of search and aligns with conclusions from previous studies [6, 36, 25]. Interestingly, we do not observe the same benefits of search if BoN is used as final aggregation step, suggesting that WMV is critical component to VGS. Response Length for VGS. In addition to consistent performance gains, VGS also produces noticeably shorter responses compared to the base DeepSeek-1.5B model. In Figure 15 (Appendix C.7), we present histograms of response lengths across all benchmarks. The results show that VGS consistently generates more concise outputs, whereas the base model often reaches the generation cap, with up to Figure 4: TTC Scaling of Various Scorers. Comparison of our 1.5B value model (VM), our 1.5B Bradley-Terry reward model (BT), and two 7B state-of-the-art PRMs for two TTC scaling methods: (Left) WMV or (Right) VGS (with WMV as final aggregation step). 7 50% of its responses being unfinished. On average, VGS responses are 11,219 tokens long, compared to 12,793 for DeepSeek-1.5B, representing reduction of over 12% in token and thus FLOPs usage. VGS vs Majority Voting. As Fig. 3 (right) demonstrates, VGS consistently achieves higher accuracy than MV, attaining equivalent performance while requiring substantially lower inference budgets (also shown in Fig. 1 right). Fully closing the gap with the oracle Pass@N curve may require larger value model trained on more extensive datasets. DeepSeek-VM-1.5B vs Baseline Scoring Models. Fig. 4 benchmarks DeepSeek-VM-1.5B against existing PRMs and our BT model. We observe that DeepSeek-VM-1.5B consistently delivers superior performance when employed both as an ORM for WMV (left) and as guidance mechanism for block-wise search (right). Note that we find our BT model to be surprisingly effective as search guidance model which suggests the importance of our token-level dataset playing an important role in successful downstream search. 3.3 Scaling up the Generator Model Sizes In Fig. 5, we scale up our experiments to guide larger 7B and 14B DeepSeek models. Here, we run VGS with the same search parameters using the same value model DeepSeek-VM-1.5B. Although the 7B and 14B DeepSeek models are in theory OOD for our value model [52], which was trained on DeepSeek-1.5B rollouts, we observe that VGS continues to scale without plateauing as test-time compute increases. This provides some evidence that value model trained with weaker verifier policy can generalize effectively and guide the CoTs of stronger models. Such generalization is particularly valuable, as it is significantly cheaper to collect training data from smaller πref models. This form of weak-to-strong generalization [10] appears to be promising direction for future research."
        },
        {
            "title": "4 Ablation Studies",
            "content": "To investigate the role of key hyperparameters in search, we perform sensitivity analyses of block size and beam width on AIME-24 across varying inference budgets. We also ablate the amount of DVTS parallelism. These tests suggest that there is consistent choice of search hyperparameters that work well across inference budgets. 4.1 Different Search Parameters and Methods Figure 5: VGS + WMV Performance when Guiding Larger Models. With the same DeepSeek-VM-1.5B providing guidance, search continues to improve with more test-time compute. Block Size. We perform beam search with width 2 using search block sizes from 16 to 16384. Fig. 6 shows AIME-24 accuracies across three inference budgets , revealing that the optimal choice of 4096 stays consistent across different . We see decline in performance when searching with more fine-grained blocks. Figure 6: Ablation: Search Block Size. AIME-24 accuracies for beam search (width 2) with varying block sizes from 16 to 16384. We found 4096 to be optimal across test-time budgets and benchmarks. Beam Width. We perform beam search with block size 4096 using varying beam widths, with breadth-first-search (BFS) being special case where beam width is equal to . Fig. 7 (left) shows AIME-24 accuracies across five inference budgets, demonstrating that beam width 2 is consistently optimal across different . We note our optimal beam width is different from prior works which found 4 to work best [6, 36, 25]. DVTS Parallelism. Fig. 7 (right) shows the role of ablating DVTS from VGS. For each inference budget, we report average accuracies without DVTS and with the best DVTS parallelism . We 8 Figure 7: Ablations: Beam-Width and DVTS. (Left) AIME-24 accuracies for beam search with various widths (Section 2.3) across inference budgets . BFS is equivalent to setting width as . We find that the optimal beam width is robust across multiple TTC budgets. (Right) Averaged accuracy for beam 2 with and without DVTS. For DVTS, we report the best result with parallelism > 1 per inference budget , which we find scales better at higher budgets. observe that DVTS becomes more effective at higher budgets and scales better than single search tree, which is consistent with findings from prior works [6]. However, we find that DVTS is never worse than single search tree even at smaller inference budgets, which is the opposite conclusion reached by prior works [6]. This discrepancy may be explained by the fact that we use WMV to combine the DVTS responses, which seems to be more robust way to perform DVTS than BoN (used in prior works) given our findings from Fig. 3. 4.2 Random vs. Value-Guided Search Finally, we directly ablate the role of our value models guidance during the search process. We perform VGS (w/ same width, block size and DVTS) but randomly select blocks instead of selecting blocks with the highest value. We still aggregate the final beams via WMV with our value model, so the only change is how intermediate blocks are chosen. We call this process random search. Thus, if our value model is helpful for search, we should expect VGS to outperform random search. Indeed, Fig. 8 validates this hypothesis. We also evaluate hybrid approach where half of DVTSs parallel trees use random search and the other half use VGS. We find that this hybrid approach lands roughly between pure VGS and pure random search, again validating that block-selection from our value model improves over random selection."
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 8: Ablation: Random Search. Random search is the same search process as VGS except intermediate blocks are randomly selected instead of using our value model. Hybrid is mixture where we flip fair coin at the start of search tree that decides whether to use random search or VGS. We see that selecting blocks with highest value improves accuracy across inference budgets. In this paper, we introduced block-wise Value-Guided Search (VGS), simple yet effective strategy for steering long-context CoT reasoning models. We proposed scalable token-level value model training pipeline that does not require pre-defined notion of step or expensive per-step annotations. We collect large dataset of reasoning CoTs (OpenR1-VM) and train lean 1.5B value model (DeepSeek-VM-1.5B), which we show can effectively guide the CoTs of DeepSeek models up to 14B in size. With extensive experiments, we demonstrate that VGS with DeepSeek-VM-1.5B enjoys better test-time compute scaling than standard methods (e.g., majority voting, best-of-N ) and other scoring models (e.g., existing PRMs and BT model), achieving higher performance ceiling while reducing the FLOPs needed to extract the same performance as baseline methods  (Fig. 1)  . Our results point to VGS as promising approach to scale TTC of emerging reasoning models. 9 Discussion of Limitations. Our value model is trained exclusively on completions / roll-outs from lean reasoning model πref (e.g., DeepSeek-R1-Distill-Qwen-1.5B). As frontier LLMs continue to advance, the distribution of their generated responses may increasingly diverge from our training distribution, potentially degrading scoring and search performance. To maintain optimal performance, new value models will need to be retrained on rollouts from updated generator policies. However, we do not foresee this as major practical concern given the simplicity and scalability of our pipeline. To facilitate retraining and adaptation to similar verifiable domains, we open-source our codebase and provide step-by-step recipe in Appendix for data collection, training and search inference."
        },
        {
            "title": "Acknowledgment",
            "content": "JPZ is supported by grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) (567916). ZG is supported by LinkedIn-Cornell Grant. Wen Sun is supported by NSF IIS-2154711, NSF CAREER 2339395 and DARPA LANCER: LeArning Network CybERagents. This research is also supported by grants from the National Science Foundation NSF (IIS-1846210, IIS-2107161, and IIS-1724282, HDR-2118310), the Cornell Center for Materials Research with funding from the NSF MRSEC program (DMR-1719875), DARPA, arXiv, LinkedIn, Google, and the New York Presbyterian Hospital."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. [2] Loubna Ben Allal, Lewis Tunstall, Anton Lozhkov, Elie Bakouch, Guilherme Penedo, Hynek Kydlicek, and Gabriel Martín Blázquez. Open r1: Update #2. https://huggingface.co/ blog/open-r1/update-2, February 2025. Hugging Face Blog. [3] Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James McInerney, Dawen Liang, Nathan Kallus, and Csaba Szepesvári. Switching the loss reduces the cost in batch (offline) reinforcement learning. arXiv preprint arXiv:2403.05385, 2024. [4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [5] Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse m-best solutions in markov random fields. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 116. Springer, 2012. [6] Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models, 2024. URL https://huggingface.co/spaces/HuggingFaceH4/ blogpost-scaling-test-time-compute. [7] Marc Bellemare, Will Dabney, and Rémi Munos. distributional perspective on reinforceIn International conference on machine learning, pages 449458. PMLR, ment learning. 2017. [8] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [9] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. [10] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. 10 [11] Jonathan Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, and Wen Sun. Learning to generate better than your llm. arXiv preprint arXiv:2306.11816, 2023. [12] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé III, and John Langford. Learning to search better than your teacher. In International Conference on Machine Learning, pages 20582066. PMLR, 2015. [13] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=VaXnxQ3UKo. [14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [15] Scott Davies, Andrew Ng, and Andrew Moore. Applying online search techniques to continuous-state reinforcement learning. In AAAI/IAAI, pages 753760, 1998. [16] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. [17] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, et al. Stop regressing: Training value functions via classification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and Others. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [19] Seungwook Han, Idan Shenfeld, Akash Srivastava, Yoon Kim, and Pulkit Agrawal. Value augmented sampling for language model alignment and personalization. arXiv preprint arXiv:2405.06639, 2024. [20] Ehsan Imani, Kai Luedemann, Sam Scholnick-Hughes, Esraa Elelimy, and Martha White. Investigating the histogram loss in regression. arXiv preprint arXiv:2402.13425, 2024. [21] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [22] Hynek Kydlíˇcek. Math-verify: Math verification library, 2025. URL https://github.com/ huggingface/Math-Verify. library to rule-based verify mathematical answers. [23] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [24] Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding. arXiv preprint arXiv:2309.15028, 2023. [25] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. [26] Bruce Lowerre. The harpy speech recognition system. Carnegie Mellon University, 1976. [27] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. 11 [28] Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, et al. Controlled decoding from language models. arXiv preprint arXiv:2310.17022, 2023. [29] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [30] Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic, Nikola Jovanovic, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. [31] Qwen Team. Qwen3: Think deeper, act faster, 2025. URL https://qwenlm.github.io/ blog/qwen3/. Accessed: 2025-05-08. [32] Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448, 2023. [33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [34] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for LLM reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=A6Y7AqlzLW. [35] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. [36] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM testtime compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=4FWAwZtd2n. [37] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. [38] Jiashuo Sun, Yi Luo, Yeyun Gong, Chen Lin, Yelong Shen, Jian Guo, and Nan Duan. Enhancing chain-of-thoughts prompting with iterative bootstrapping in large language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 40744101, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.257. URL https: //aclanthology.org/2024.findings-naacl.257/. [39] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [40] Ashwin Vijayakumar, Michael Cogswell, Ramprasath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016. [41] Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, and James Zou. Think deep, think fast: Investigating efficiency of verifier-free inference-time-scaling methods. arXiv preprint arXiv:2504.14047, 2025. [42] Kaiwen Wang, Kevin Zhou, Runzhe Wu, Nathan Kallus, and Wen Sun. The benefits of being distributional: Small-loss bounds for reinforcement learning. Advances in neural information processing systems, 36:22752312, 2023. 12 [43] Kaiwen Wang, Nathan Kallus, and Wen Sun. The central role of the loss function in reinforcement learning. Statistical Science, 2025. Forthcoming. [44] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [45] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [46] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. [47] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [48] Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024. [49] Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. Entropy-regularized process reward model. arXiv preprint arXiv:2412.11006, 2024. [50] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025. [51] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in Neural Information Processing Systems, 37:6255762583, 2024. [52] Jin Peng Zhou, Kaiwen Wang, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kilian Weinberger, Kianté Brantley, and Wen Sun. q: Provably optimal distributional rl for llm post-training. arXiv preprint arXiv:2502.20548, 2025."
        },
        {
            "title": "Table of Contents",
            "content": "Appendix A. Related Works Appendix B. Summary of VGS Pipeline Appendix C. Additional Experiment Results Appendix D. Further Details of Data Collection Appendix E. Further Details for Value Model Training Appendix F. Further Details for Inference with Search Appendix H. Inference FLOPS Computation Note: In the appendix, we also provide additional empirical results in Appendix C. Two new results are worth highlighting here. First, in Appendix C.6, we provide test-time scaling results for guiding DeepSeek-1.5B further trained with PPO on our math dataset. We find that VGS improves test-time scaling compared to MV and WMV, which shows that our method nicely complements policy-based RL training. Moreover, in Appendix C.7, we include three qualitative examples of contrastive blocks that were selected or rejected by our value model during beam search process. We see that our value model prefers blocks with more straightforward logical deductions, yielding more efficient and effective CoT for reasoning."
        },
        {
            "title": "A Related Works",
            "content": "Figure 9: Taxonomy of TTC Methods. Score-free TTC methods do not require an external scoring model, e.g., by taking majority vote. Score-based TTC methods require an external scoring model. The coarsest scoring model is an outcome reward model (ORM), which scores whole response and can be used for best-of-N or weighted MV. more fine-grained scoring model are process-level scorers, which includes process reward models (PRMs) and value models; these more fine-grained scoring models can be used for search. Test-time compute (TTC) broadly refers to algorithms that improve problem-solving performance when given more compute (i.e., FLOPs) at test-time. Fig. 9 summarizes the taxonomy of TTC methods. The simplest TTC methods are score-free in the sense that they do not require access to an external scoring model. notable example is majority voting (MV), which selects the most frequent answer among responses, breaking ties randomly [14, 46, 9]. Also known as self-consistency, MV can be applied to tasks where the output space is equipped with an equivalence relation, e.g., mathematical formulae that can be symbolically checked for equality. Other score-free TTC methods include sequentially revising the response via CoT prompting [38, 29] and hybrid methods [41]. There are also score-based TTC methods that employ an external scorer. The coarsest type of scorer is an outcome reward model (ORM), which takes the full prompt and response as input and produces scalar that measures the quality / correctness of the response. Popular examples of ORMs include Bradley-Terry reward models [8] or LLM-as-a-Judge [4]. ORMs can be used for 14 best-of-N (BoN), which selects the response with the highest score [14, 4]. ORMs can also be used for weighted majority voting (WMV), which generalizes MV where the strength of responses vote is proportional to its ORM score. Weighted MV (WMV) typically provides an improvement over vanilla (unweighted) MV [6, 25], which is also what we observe in our experiments  (Fig. 3)  . Outcome-level TTC methods (e.g., BoN, WMV) may be further refined with process-level scorers that guide the generation process in fine-grained manner. We remark that our value model can act as both an outcome-level and process-level scorer. When queried with partial response, the value model predicts the expected quality of future completions under πref. When queried at the end of full response, the value model predicts the quality of the final response. Indeed, our best performing setup for value-guided search (VGS) uses intermediate values to guide block-wise beam search and uses final values via WMV to aggregate the final beams, which employs both the process-level and outcome-level scoring capabilities of the value model. Finally, to the best of our knowledge, the combination of search with WMV is novel to this work, and we found this to be crucial ingredient to effectively scale TTC of DeepSeek models. Prior works on process-level TTC largely focused on step-wise search with process reward models (PRMs), which measures the correctness of fine-grained step [23]. They showed that step-wise search can provide better performance than outcome-level TTC methods [28, 47, 24, 36, 48, 34, 13]. However, training step-wise PRMs requires pre-defined notion of step, which is challenging to explicitly define for general reasoning [18]; e.g., prior works used newlines to separate steps, but DeepSeeks CoTs often contain newlines in-between coherent thoughts. Moreover, prior PRM training techniques require per-step annotations, via humans [23], LLM-Judges [50], or per-step MC rollouts [44, 27], which are expensive to collect for long reasoning traces; e.g., single response from DeepSeek models typically contains hundreds of newlines. These limitations make it difficult to scale PRMs to long-context reasoning models [18], and all these prior works could only evaluate on short-context models with easier benchmarks such as GSM8k [14] and MATH [23]. In contrast, our paper focuses on scaling process-level guidance to long-context reasoning models, and we propose block-level search method that mitigates the above limitations. We train token-level value model by collecting rollouts from random solution prefixes, which requires neither pre-defined notion of step nor per-step annotations. We use our value model to guide block-wise search process, where the block size is hyperparameter and we find there exists consistent choice that works well across inference budgets  (Fig. 6)  . Crucially, we are able to scale our value-guided search (VGS) to long-context DeepSeek models and demonstrate impressive performance and efficiency gains on challenging math competitions  (Fig. 1)  . Closely related to our work is Setlur et al. [34], who propose to train token-level process advantage verifier (PAV), which is the sum of πrefs Q-function and an (off-policy) experts advantage function, to guide step-wise search. This method is similar to ours since the training process also occurs at token-level and is agnostic to the definition of step. However, limitation of the PAV is that if the expert disagrees with the underlying policy, then maximizing the PAV can lead to suboptimal behavior [12]. Our approach of directly using the value model does not have this issue. Moreover, Setlur et al. [34] proposed to use PAVs to guide step-wise search, which still requires definition of step at inference time; in contrast, we propose to use block-wise search which does not require definition of step at inference. At technical level, Setlur et al. [34] trained the PAV by minimizing the mean-squared error (MSE) loss; in contrast, we propose to use the cross-entropy loss, which has been shown to work better for downstream decision making [17, 43, 52]. We remark that some prior works proposed to use token-level value models to reweight the next-token distribution of the generator [28, 49, 19, 52]. However, these methods require one classifier call per token, which is more expensive than block-wise search. Moreover, token-level guidance might also be less effective because the imperfect value model may introduce cascading errors if queried at every token. We highlight that Mudgal et al. [28] also experimented with block-wise BFS and found this to be more effective at scaling test-time compute than reweighting the next-token distribution (i.e., token-wise guidance). One drawback of block-wise BFS is that the blocks may all become correlated due to sharing the same prefix. Thus, we build upon Mudgal et al. [28] by proposing to use beam search, which we show yields better test-time scaling for reasoning models  (Fig. 6)  ."
        },
        {
            "title": "B Summary of VGS Pipeline",
            "content": "We provide step-by-step recipe for running VGS on any verifiable domain of interest. This recipe is applicable to any task with reward label for responses (i.e., outcome-level feedback). If the task has continuous rewards, standard trick from distributional RL is to discretize the reward distribution as histogram, and then the value model is simply the expected reward under the learned distribution [7, 20, 17, 42, 43]. 1. Start with verifiable domain, where responses are identified with label and reward. 2. Identify good dataset of prompts. 3. Identify set of roll-in policies and single roll-out policy. The roll-in policies should provide diverse distribution of solutions, and the roll-out policy should be strong enough to complete responses given partial roll-in. 4. For each prompt, sample roll-in responses from the set of roll-in policies. 5. For each roll-in response, sample random indices {ij}j[m], and collect roll-out per index. Thus, there are nm roll-in, roll-out pairs per prompt. 6. Post-filter by removing prompts where all roll-out responses fail to complete or solve the prompt. 7. At this point, we have created our dataset of roll-in, roll-out pairs. We are now ready to train our value model. 8. Train classifier / value model by following (Section 2.1). Sweep hyperparameters such as learning rate. 9. Choose generator policy to be guided by the value model. The most in-distribution choice is to use the roll-out policy πref. 10. Perform model selection by running outcome-level TTC (e.g., WMV) on some validation benchmark. 11. Sweep search parameters (e.g., block size, beam width, DVTS parallelism) on the validation benchmark. 12. Run the final model on the test benchmark with the best search parameters. The sampling distribution for the cut-off index (Step 5) is also worth tuning. For example, values at earlier or middle indices may be harder to predict than final indices, so it is worth sampling more cut-off indices from these earlier regions."
        },
        {
            "title": "C Additional Experiment Results",
            "content": "C.1 Full Main Results Table We reproduce Table 1 with additional baselines: DeepSeek-7B, DeepSeek-14B, o1-mini-medium. Test-time scaling DeepSeek-1.5B (N = 256) AIME-24 AIME-25 HMMT-24 HMMTAVG VGS w/ DeepSeek-VM-1.5B (ours) WMV w/ DeepSeek-VM-1.5B (ours) VGS w/ DeepSeek-BT-1.5B (ours) WMV w/ DeepSeek-BT-1.5B (ours) VGS w/ Qwen2.5-Math-PRM-7B WMV w/ Qwen2.5-Math-PRM-7B VGS w/ MathShepherd-PRM-7B WMV w/ MathShepherd-PRM-7B MV@256 72.0 0.4 69.6 3.9 73.1 1.4 72.0 3.3 71.1 1.0 70.6 3.1 70.6 3.1 71.2 3.2 71.0 3.5 46.7 0.7 45.1 2.2 40.6 0.8 40.5 2.9 38.9 1.4 39.1 2.1 41.9 1.4 40.0 2.5 38.9 1.9 31.4 2.0 29.1 2.6 28.1 1.9 25.3 2.3 26.7 1.2 25.4 2.4 30.0 1.4 27.9 2.3 24.4 1.7 32.8 0.8 28.9 2.6 27.5 0.0 24.6 4.7 24.2 0.2 24.0 3.2 23.9 1.4 25.6 3.1 24.3 2. 45.7 1.0 43.2 1.4 42.3 0.5 40.6 1.6 40.2 0.5 39.8 1.4 41.6 0.9 41.2 1.4 39.7 1.2 Test-time scaling larger models with our DeepSeek-VM-1.5B VGS w/ DeepSeek-7B (N = 128) MV w/ DeepSeek-7B (N = 128) VGS w/ DeepSeek-14B (N = 64) MV w/ DeepSeek-14B (N = 64) Pass@N baselines for various models DeepSeek-1.5B Pass@1 DeepSeek-1.5B Pass@256 DeepSeek-7B Pass@1 DeepSeek-14B Pass@1 DeepSeek-32B Pass@1 Deepseek-R1 (671B) Pass@ o1-mini-medium Pass@1 o1-mini-medium Pass@8 o3-mini-medium Pass@1 o3-mini-medium Pass@8 o4-mini-medium Pass@1 o4-mini-medium Pass@8 82.2 0.8 77.1 1.1 59.4 0.8 56.5 1.6 42.8 2.8 34.7 1.6 41.1 1.6 33.8 2.5 56.4 0.8 50.5 0. 86.7 2.7 80.6 1.2 59.6 0.6 67.0 2.0 46.7 2.7 40.6 1.8 51.1 1.6 50.1 2.0 61.0 0.9 59.6 0.9 28.2 6.1 81.9 1.7 54.8 6.0 72.4 5.4 77.2 4.9 85.0 2. 63.3 6.6 83.7 2.7 49.2 6.8 83.0 4.6 85.4 4.3 95.4 2.6 22.4 4.1 62.6 3.6 40.9 6.1 53.9 5.5 60.4 6.0 70.0 0.9 52.3 6.8 81.8 3.7 45.8 6.6 77.4 3.9 82.3 4.5 93.3 0.4 13.9 4.2 54.2 4.9 31.5 4.4 36.4 4.8 38.0 4.6 41.7 3.5 33.1 5.1 58.0 4.0 32.4 5.4 55.9 4.3 50.4 5.0 69.7 3.2 13.0 3.9 57.1 3.8 25.5 4.6 36.5 5.5 42.1 5.2 46.7 2. 34.0 5.9 52.8 3.4 36.6 6.0 64.9 4.4 61.1 6.4 84.5 2.5 19.4 1.1 63.9 0.9 38.2 1.3 49.8 1.3 54.4 1.3 60.8 0.5 45.7 1.5 69.1 1.7 41.0 1.5 70.3 2.1 69.8 2.5 85.7 1.1 Table 2: (Top) Weighted majority vote (WMV) and VGS accuracies for DeepSeek-1.5B with an inference budget of = 256, with various scoring models. (Middle) Compares MV and VGS for larger DeepSeek models guided with our DeepSeek-VM-1.5B. (Bottom) Lists performance of DeepSeek models and strong close-sourced reasoning models. For VGS, indicates standard deviation across 3 seeds, and for MV, WMV, Pass@N, it denotes bootstrap with 100 repetitions. We bold the highest avg. accuracy and underline second highest. 17 C.2 Per-benchmark Plots for Fig. Figure 10: Per-benchmark results for Fig. 3. (Left) Compares best-of-N (BoN), weighted majority voting (WMV) and VGS with either BoN or WMV for the final aggregation. (Right) Compares VGS to majority voting (MV), standard baseline that does not require scorer. C.3 Per-benchmark Plots for Fig. 4 Figure 11: Per-benchmark results for Fig. 4. Comparison of our 1.5B value model (VM), our 1.5B Bradley-Terry reward model (BT), and two 7B state-of-the-art PRMs for two TTC scaling methods: (Left) WMV or (Right) VGS (with WMV as final aggregation step). 18 C.4 Per-benchmark Plots for Fig. 5 Figure 12: Per-benchmark results for Fig. 5. With the same DeepSeek-VM-1.5B providing guidance, search continues to improve with more test-time compute. C.5 Per-benchmark Plots for Fig. 8 Figure 13: Per-benchmark results for Fig. 8. Random search is the same search process as VGS except intermediate blocks are randomly selected instead of using our value model. 19 C.6 Results for Guiding PPO Policy Guo et al. [18] mentions that the performance of distilled DeepSeek models can be further enhanced through reinforcement learning (RL). In this section, we explore whether we could guide the generation of RL trained policy. Specifically, we apply Proximal Policy Optimization (PPO) [33] to DeepSeek-1.5B using prompts from OpenR1-Cleaned and guide the trained model with DeepSeek-VM-1.5B. We perform full parameter training with 8 H100 GPUs and use the same model as the policy for critic. We use rule-based reward function based solely on the correctness of the response, assigning +1 for correct answers and 0 for incorrect or incomplete ones. To ensure that the learned policy π remains close to the reference policy πref , an additional KL penalty is applied to the reward: r(x, y) γKL (ln π(y x) ln πref (y x)) , (1) where r(x, y) is the rule-based reward for prompt and response y, and γKL controls the strength of the KL penalty. To further encourage exploration, we apply standard entropy regularization by subtracting the policy entropy from the loss weighted by coefficient γentropy: LPPO γentropy H[π( x)], (2) The hyperparameter settings are shown below. PPO Hyperparameter Setting Setting Parameters Generation (train) temperature: 1.0 top p: PPO batch size: 256 mini batch size: 128 micro batch size: 1 policy learning rate: 1e-6 critic learning rate: 1e-5 train epochs: 25 γentropy: 1e-3 γKL: 1e-4 gae γ: 1 gae λ: 1 clip ratio: 0.2 Total number of steps: 2250 DeepSeek-1.5B Pass@4 Pass@ Pass@16 Pass@32 Pass@64 Pass@128 Pass@256 AIME-24 AIME-25 HMMT-24 HMMT48.7 5.0 33.1 4.1 24.0 4.6 21.6 4.6 58.9 4.9 39.1 4.4 28.6 3.9 26.7 4.0 67.3 4.7 43.7 3.9 32.9 3.6 30.1 4.8 74.1 3.2 49.0 3.8 37.7 4.4 37.8 5.1 78.4 3.0 54.0 3.9 42.9 4.2 45.6 4.6 80.4 2.4 58.8 3.4 48.1 3.7 52.6 4. 81.9 1.7 62.6 3.6 54.2 4.9 57.1 3.8 DeepSeek-1.5B-PPO Pass@4 Pass@8 Pass@16 Pass@ Pass@64 Pass@128 Pass@256 AIME-24 AIME-25 HMMT-24 HMMT-25 54.0 5.0 35.9 3.9 27.2 4.2 22.8 4.0 61.4 4.6 39.8 3.8 32.8 4.4 26.8 4. 67.6 4.1 45.8 4.2 37.6 3.6 32.5 4.4 73.3 3.4 50.9 3.5 41.5 3.6 36.9 4.4 76.8 2.4 56.1 3.0 45.0 3.3 43.7 3.8 78.3 1.7 59.8 3.6 48.8 3.3 48.9 3.8 79.6 1.1 64.1 3.9 52.4 3.4 52.6 3.6 Table 3: Pass@N results for DeepSeek-1.5B model and PPO-trained DeepSeek-1.5B-PPO model. Table 3 presents the comparison between the DeepSeek-1.5B model and the PPO-trained model (DeepSeek-1.5B-PPO). As increases, the performance gap gradually narrows. While the PPOtrained model performs competitively at lower values, it is surpassed by the base model at Pass@32 on both the AIME-24 and HMMT-25 datasets. This decline in performance could be attributed to the reduced entropy of the model after PPO training, which limits the diversity of model generations and negatively impacts performance at higher Pass@N. 20 We report our value-guided results of the PPO-trained model in Fig. 14. We observe that VGS nicely complements PPO training and provides additional test-time compute gains in performance compared to WMV and MV. Figure 14: Guiding DeepSeek-1.5B Trained with PPO. Comparison of VGS, WMV and MV for TTC scaling our PPO policy. 21 C.7 Qualitative Examples Figures 17, 18, and 19 (at the end of the paper) show representative qualitative examples where value scores from VGS are used to guide beam search. At each step, two blocks of tokens are proposed, and the one with the higher value is selected to continue the solution. Due to space constraints, parts of the beams are abridged with . . . , and for ease of visualization, blue highlights indicate correct reasoning steps, while red highlights denote incorrect ones. Low-scoring beams exhibit different types of failure. In Figure 17, the rejected beam alternates between correct and incorrect steps, resulting in confused and ultimately incorrect reasoning. In Figure 18, the beam begins with plausible strategy involving GCD analysis but eventually resorts to ineffective trial and error. In Figure 19, the beam makes critical error in the algebraic transformation early on and fails to recover. In contrast, the selected beams across all examples demonstrate systematic reasoning and successfully solve the problems. Interestingly, despite the critical error in Figure 19, VGS assigns moderately high score (0.337) to the rejected beamhigher than scores for less subtle failures in earlier examplessuggesting that even significant mistakes can be difficult to detect when embedded in otherwise coherent reasoning. Finally, we empirically compare the distribution of generation lengths between the DeepSeek-1.5B base model and VGS with DeepSeek-1.5B across all benchmarks (Figure 15). On average, VGS generates noticeably shorter responses (11,219 tokens vs. 12,793 for the DeepSeek-1.5B base model), suggesting that beam search not only enhances accuracy but also promotes more concise reasoning. This trend is consistent with our qualitative analysis, where beam search tends to favor token blocks that are direct and solution-oriented, rather than verbose or meandering reasoning. Notably, the sharp peak near 16,000 tokens corresponds to the maximum generation length of DeepSeek models (16,384). For the base model, as many as 50% of the generations reach this limit, often resulting in incomplete outputs. Figure 15: Histogram of generation lengths for the DeepSeek-1.5B base model vs. VGS. VGS consistently produces shorter responses across benchmarks, with average lengths of 11,219 and 12,793 tokens for VGS and the base model, respectively. The peak around 16,000 tokens reflects the generation cap of DeepSeek models, which the base model frequently hits, often resulting in incomplete outputs."
        },
        {
            "title": "D Further Details of Data Collection",
            "content": "Pre-filtering process. Here we describe the pre-filtering process for constructing OpenR1-Cleaned in more detail. Below are the sequence of filtering operations we performed on OpenR1-Math [2]. We arrived at these rules by manually inspecting the data, by sampling 100 random problems from the dataset and checking if all problems solutions looked reasonable to us. In OpenR1-Math, solution is fleshed-out solution to the math problem, an answer is the final answer to the math problem. 1. Filter out all solutions with 0 or > 1 boxed answers (enclosed in boxed{}). These are ambiguous and difficult to parse out the answer. 2. Filter out answers which are difficult to automatically parse or verify. This includes answers containing: or, and mathrm, quad, answers with equal signs, commas, semicolons, cup, cap, inequality symbols, approximation symbols. 3. Filter out multiple-choice questions, which are labeled with question_type = MCQ. 4. Filter out questions with multiple parts, as it is ambiguous which part the answer is for. 5. Filter out questions containing links (http:// or https://), since the models we test cannot access the web. Roll-in roll-out process. We also provide further intuition for the roll-in vs. roll-out process (illustrated in Fig. 2 left). The roll-in and then roll-out process is standard technique in imitation learning [12] and reinforcement learning [11]. Roll-out. The roll-out process uses fixed policy πref to roll-out from any partial solution provided by the roll-in process. The rationale for using fixed roll-out policy is to fix the target of the classification / value regression problem. In particular, the classifier is trained to predict the probability of each class under the roll-out policy, given the partial solution. Roll-in. The main point of the roll-in process is to create diverse distribution of partial solutions to roll-out from. By creating diverse roll-in distribution with multiple roll-in policies, we can ensure that the classifier is trained on diverse context distribution and will generalize better to new traces. To select where to cut roll-in (to start the roll-out), we sample cut index from the distribution where is the length of the roll-in. We chose this such that the cut index is of p(i) = more likely to occur at earlier positions of the roll-in. We want to encourage more learning at earlier positions since those prediction problems are more difficult than at later positions. The following figure  (Fig. 16)  illustrates the distribution of the length of roll-outs, which shows that this indexing scheme indeed yields many long roll-outs. j[L] (cid:80) Figure 16: Distribution of roll-out length."
        },
        {
            "title": "E Further Details for Value Model Training",
            "content": "value model DeepSeek-VM-1.5B Our the uses DeepSeek-R1-Distill-Qwen-1.5B model, which is 1.5B parameter transformer model with 28 layers and 1536 hidden size. To turn this model into value model, we replace the LM head with scoring head, parameterized by two-layer MLP, that outputs logits for three classes: correct, incorrect, and incomplete. The number of classes can be modified to suit the task at hand. architecture same base the as Category Parameter Value"
        },
        {
            "title": "Data",
            "content": "Training Optimizer Compute"
        },
        {
            "title": "Dataset\nValidation split\nMax sequence length",
            "content": "Batch size Learning-rate schedule Warm-up steps Dropout Number of epochs Optimizer type β1 β2 Weight decay Grad Norm Clip DeepSeek-R1-Distill-Qwen-1.5B 1536 two-layer MLP with hidden size dmodel False 0=Incorrect, 1=Correct, 2=Incomplete OpenR1-VM 500 16384 1024 Cosine with max_lr=1e-4 10% of total steps 0.05 5 AdamW 0.9 0.95 0.1 GPUs Wall-clock time (h) Tokens Throughput (tokens/s) Loss Tokens Throughput (loss tokens/s) Total Tokens Processed (per epoch) Total Loss Tokens Processed (per epoch) 16 nodes of 8 NVIDIA H100 24 hours 2.07M 835k 35.7B 14.4B Table 4: Value Model Training Parameters. We sweeped learning rates 1e-4, 3e-4, 7e-5 and we save checkpoints at every epoch. We selected the best checkpoint via WMV performance on AIME-24."
        },
        {
            "title": "F Further Details for Inference with Search",
            "content": "Given problem, the prompt we used is: <begin_of_sentence><User>{problem} Please think step-by-step and put your final answer within boxed{}.<Assistant><think>n We use the same decoding parameters as in the original DeepSeek paper [18]. Category"
        },
        {
            "title": "Decoding",
            "content": "Best Parameters for Search (VGS) Parameter Inference Engine Max generation length Temperature Top-p Think Token End of Think Token Value SGLang [51] 16384 0.6 0.95 <think> </think> Model Beam width Block size (tokens) Parallel branches (DVTS) Final aggregation rule DeepSeek-VM-1.5B 2 4096 budget dependent Weighted Majority Vote (WMV) Table 5: Decoding and Search Parameters. Further Details for Training Bradley-Terry Reward Model Dataset. Recall that our dataset for value model training (OpenR1-VM) contains 56 responses per problem. To construct Bradley-Terry dataset, we sample up to 4 response pairs per problem, where pair consists of response with reward 0 (the reject response) and response with reward 1 (the chosen response). Some prompts may have fewer than 4 responses with reward 0 or 1, and in those cases, we include as many as possible. This yields dataset of roughly 122k pairs. Model. We use the same model architecture as the value model (Appendix E) except that the score head outputs single scalar score instead of three-dimensional vector. We use the same training pipeline, except that the training loss is swapped to the standard BT loss [8]: LBT(θ, B) = 1 (cid:88) (x,yr,yc)B log σ(fθ(x, yc) fθ(x, yr)), (3) where σ is the sigmoid function, yr is the reject response and yc is the chosen response. We use batch size of 128 pairs and we train for one epoch. We sweeped many learning rates: 3e-4, 1e-4, 7e-5, 3e-5, 7e-6. We found that the BT loss drops and plateaus much quicker than the value model loss, and all learning rates yielded similar final losses. We consider the last ckpt of each run and we selected lr=3e-5 as best ckpt with search (width 2, block size 4096, WMV aggregation) on aime-24. We note that one detail for getting BT to work with weighted majority is to use the sigmoid of the BT score, i.e., take WMV with σ(fθ(x, y)) instead of fθ(x, y) itself. While this doesnt affect BoN performance, we found that taking sigmoid was crucial for WMV performance to scale well."
        },
        {
            "title": "H Inference FLOPS Computation",
            "content": "In this section, we compute the FLOPs for search and show that adding value model guidance at the block-level introduces negligible compute overhead, as the vast majority of the compute is from generator model. We follow the approach outlined in Kaplan et al. [21], Sardana et al. [32] to compute the FLOPs for single forward pass of the transformer, ignoring the embedding layers for simplicity. Consider transformer with nlayer layers, dimensional residual stream, df dimensional feedforward layer, and dimensional attention outputs. Then the number of non-embedding parameters is = 2nlayerd(2d + df ), and the number of FLOPs for single forward pass over context of length nctx is C(nctx) = 2N + 2nlayernctxd. (4) Then, in the regime where dmodel > nctx/12, Kaplan et al. [21], Sardana et al. [32] further approximate the above by ignoring the nctx term, i.e., becomes independent of nctx. We adopt this approximation when estimating the inference FLOPs of our generator models. Thus, for context length of nctx = 16, 384, the inference FLOPs for one complete generation for each generator model is 2N nctx: 1. DeepSeek-R1-Distill-Qwen-1.5B: 2 1.5B 16384 = 49.1T . 2. DeepSeek-R1-Distill-Qwen-7B: 2 7B 16384 = 229T . 3. DeepSeek-R1-Distill-Qwen-14B: 2 14B 16384 = 459T . 4. DeepSeek-R1 (671B, with 37B activated params): 2 37B 16384 = 1212T . We now compute the FLOPs needed for one forward pass of the value model. Since we use block-size of 4096, there are at most 16384/4096 = 4 value model inferences per generation. Thus, the FLOPs from the value model is: 1. 1.5B classifier: 2 1.5B 4 = 12B. 2. 7B classifier (baselines): 2 7B 4 = 56B. Thus, we can see that the value model FLOPs is negligible compared to the generator model FLOPs. In particular, when guiding 1.5B generator with 1.5B classifier, the classifier FLOPs is only 0.024% of the generator FLOPs. With compute budget of 256, this amounts to total FLOPs of (49.1T + 12B) 256 = 12.6P . When guiding with 7B classifier, the total FLOPs is (49.1T + 56B) 256 = 12.6P . Note that the FLOPs required for generating 256 independent generations is 49.1T 256 = 12.6P . Thus, search has negligible overhead comapred to (weighted) majority voting or best of n. 26 AIME-24 Problem: Jen enters lottery by picking 4 distinct numbers from = {1, 2, 3, , 9, 10}. 4 numbers are randomly chosen from S. She wins prize if at least two of her numbers were 2 of the randomly chosen numbers, and wins the grand prize if all four of her numbers were the randomly chosen numbers. The probability of her winning the grand prize given that she won prize is where and are relatively prime positive integers. Find + n. Rejected beam with score 0.009: Okay, so need to figure out the probability that Jen wins the grand prize given that she won prize. That means have to use conditional probability here. remember that conditional probability is (AB) = (A B)/P (B). In this case, event is her winning the grand prize, and event is her winning prize. So, need to find the probability that she won both the grand prize and prize, which is just her winning the grand prize, divided by the probability that she won prize. First, let me figure out the total number of possible ways Jen can pick 4 numbers from S, which has 10 numbers. That should be the combination of 10 choose 4, right? So, C(10, 4) = 210. So, the total number of possible combinations is 210. So, she can win prize in two scenarios: either exactly two of her numbers are the randomly chosen ones, or all four are. . . . The number of ways she can have exactly two numbers correct is C(4, 2) ways to choose the correct numbers, and then C(6, 2) ways to choose the incorrect ones from the remaining 6 numbers. Similarly, the number of ways she can have all four numbers correct is just C(4, 4) = 1. So, the total number of winning ways is C(4, 2) C(6, 2) + C(4, 4). . . . C(4, 2) is 6, and C(6, 2) is 15. So, 6*15 = 90. Then, C(4, 4) is 1. So, total winning ways are 90 + 1 = 91. Alternatively, can compute the probability of not winning prize, which is the probability that she has 0 or 1 correct numbers. Let me compute that and subtract from 1 to see if it matches. Number of ways to have 0 correct numbers: C(4, 0) C(6, 4) = 1 15 = 15. Number of ways to have 1 correct number: C(4, 1) C(6, 3) = 4 20 = 80. So, total non-winning ways: 15 + 80 = 95. Therefore, probability of not winning prize is 95/210, so probability of winning prize is 195/210 = (210 95)/210 = 115/210. Wait, that doesnt match with the previous result. Hmm, somethings wrong here. Wait, why did get 91 before? Let me see. had C(4, 2) C(6, 2) + C(4, 4). So, thats 6*15 +1=91. But thats only the number of winning ways where she has exactly two or exactly four correct numbers. So, 91 winning ways, 95 non-winning, which adds to 186, which is less than 210. So, the rest must be the impossible case? No, because in reality, she cant have 0 correct numbers if she has 4 numbers. Wait, no, she can have 0 correct numbers if all her numbers are among the 6 incorrect ones. So, thats 15 ways. Similarly, 1 correct number is 80 ways, and 2 correct numbers is 90 ways, and 4 correct numbers is 1 way. So, 15 +80 +90 +1=186. Wait, so actually, 186 winning ways and 24 non-winning ways. So, probability of winning is 186/210, which simplifies to 31/35. . . . Therefore, (AB) = (1/210)/(186/210) = 1/186. So, is 1 and is 186. Are they relatively prime? 1 and 186 share no common factors besides 1, so yes. So, + = 1 + 186 = 187. . . . Selected Beam with score 0.996: Okay, so this is conditional probability problem. The formula for conditional probability is (AB) = (A B)/P (B), where is the event that she wins the grand prize, and is the event that she wins prize. Since winning the grand prize is specific case of winning prize, is subset of B. So, (A B) is just (A). Therefore, the probability were looking for is (A)/P (B). . . . Since she picks 4 numbers from 10, the total number of possible 4-number combinations is C(10, 4). The number of favorable outcomes for is 1 (since theres only one exact set of four numbers that could be the randomly chosen ones). So, (A) is 1/C(10, 4). . . . To find (B), it might be easier to compute the probability of the complementary event, which is that Jen wins zero prizes or one prize, and subtract that from 1. So, (B) = 1 - P(0 prizes) - P(1 prize). Let me compute P(0 prizes) and P(1 prize). First, P(0 prizes): This is the probability that none of Jens four numbers are among the four randomly chosen. So, all four numbers she picked are from the remaining six numbers (since has 10 numbers, and if she picks 4, the remaining are 6). So, the number of ways to choose 4 numbers from 6 is C(6, 4). Therefore, P(0 prizes) = C(6, 4)/C(10, 4). Similarly, P(1 prize): This is the probability that exactly one of Jens numbers is among the four randomly chosen. So, we need to choose 1 number from her 4, and 3 numbers from the remaining 6 (since 10 - 4 = 6). So, the number of ways is C(4, 1) C(6, 3). Therefore, P(1 prize) = [C(4, 1) C(6, 3)]/C(10, 4). . . . So, putting it all together: (B) = 1 [C(6, 4)/C(10, 4)] [C(4, 1) C(6, 3)/C(10, 4)] . . . Therefore, (B) = 1 15/210 80/210. . . . Therefore, (B) is 23/42. . . . Therefore, the conditional probability (AB) is (1/210)/(23/42) = (1/210) (42/23) = (42)/(210 23) . . . Therefore, the probability becomes (1/5)/23 = 1/(5 23) = 1/115. . . . But 1/115 cant be simplified further, so is 1 and is 115, which are coprime. Therefore, + is 1 + 115 = 116. . . . Figure 17: Example of selected and rejected beams during beam search with VGS. The high-scoring beam (score 0.996) follows correct and coherent line of reasoning, arriving at the correct answer. In contrast, the rejected beam (score 0.009) contains several inconsistencies and incorrect steps, despite occasionally making correct logical deductiondemonstrating the effectiveness of VGS as value model. Highlighting is added for clarity: blue indicates correct reasoning steps or results, while red indicates incorrect ones. Part of generations are abridged with . . . notation. AIME-25 Problem: Find the sum of all positive integers such that + 2 divides the product 3(n + 3)(n2 + 9). Rejected beam with score 0.395: First, lets parse the problem. need to find all positive integers where + 2 is divisor of 3(n + 3)(n2 + 9). So, mathematically, this means that + 2 divides 3(n + 3)(n2 + 9). In other words, when you divide 3(n + 3)(n2 + 9) by + 2, there should be no remainder. So, + 2 must be factor of this product. One way to approach this is by using the Factor Theorem. The Factor Theorem states that if + 2 is factor of polynomial (n), then (2) = 0. So, if let (n) = 3(n + 3)(n2 + 9), then (2) should be equal to zero if + 2 divides (n). Let me compute (2): (2) = 3(2 + 3)((2)2 + 9) Simplify each part: (2 + 3) = 1 (2)2 = 4, so 4 + 9 = 13 Thus, (2) = 3 1 13 = 39 . . . Wait, perhaps Im misapplying the Factor Theorem here. Let me think again. Alternatively, another approach is to express + 2 as k, so = + 2. Then, = 2. Substitute = 2 into the expression 3(n + 3)(n2 + 9) and see what conditions must hold for to divide this. . . . Therefore, the product becomes: 3(k + 1)(k2 4k + 13) . . . Which implies that divides 3(k + 1)(k2 4k + 13). Since divides the product, it must divide at least one of the factors. But divides k, so we can consider the other factors. But wait, divides 3(k + 1)(k2 4k + 13), so divides 3(k + 1)(k2 4k + 13). Since and + 1 are consecutive integers, their GCD is 1. Similarly, and k2 4k + 13 may have GCD. Let me check: Let me denote = gcd(k, k2 4k + 13). Then divides and k2 4k + 13. So, divides k2 4k + 13 k(k 4) = k2 4k + 13 k2 + 4k = 13. So, divides 13. Therefore, = 1 or 13. Wait, maybe Im complicating things. Let me approach this differently. . . . But maybe theres another way to approach this. Let me try plugging in small positive integers for and see if + 2 divides the product 3(n + 3)(n2 + 9). That might help confirm if = 1 is the only solution. Testing = 1: As above, product is 120, which is divisible by 3. Testing = 2: + 2 = 4. Product is 3(5)(13) = 195. 195 divided by 4 is 48.75, which isnt an integer. So, 4 doesnt divide 195. . . . Testing = 9: + 2 = 11. Product is 3(12)(90) = 3240. 3240 divided by 11 is approximately 294.545, not integer. Testing = 10: + 2 = 12. Product is 3(13)(109) = 4209. 4209 divided by 12 is 350.75, not integer. Wait, so up to = 10, only = 1 works. Maybe = 1 is the only solution. . . . Selected Beam with score 0.998: First, let me understand what the problem is asking. It says that + 2 divides 3(n + 3)(n2 + 9). In other words, when we divide the product 3(n + 3)(n2 + 9) by + 2, the result should be an integer. So, need to find all positive integers for which this condition holds and then sum them up. Let me write this out mathematically. If + 2 divides 3(n + 3)(n2 + 9), then there exists some integer such that: 3(n + 3)(n2 + 9) = k(n + 2) But since were looking for integer solutions, maybe its better to think about this in terms of divisibility. So, + 2 must be divisor of 3(n + 3)(n2 + 9). That suggests that + 2 divides this product. remember that if number divides another number b, then modulo is zero. So, maybe can use modular arithmetic here. Let me see. Since + 2 divides 3(n + 3)(n2 + 9), it must be that: 3(n + 3)(n2 + 9) 0 (mod + 2) . . . First, n+3 modulo n+2. Lets substitute = 2+k, but maybe thats not necessary. Alternatively, we can note that + 3 = (n + 2) + 1. So, modulo + 2, + 3 1. Similarly, n2 + 9. Let me compute n2 modulo + 2. Since 2 (mod + 2), then n2 (2)2 = 4 (mod + 2). Therefore, n2 + 9 4 + 9 = 13 (mod + 2). So, putting it all together, modulo + 2, the product becomes: 3(n + 3)(n2 + 9) 3 1 13 = 39 (mod + 2) So, this implies that 39 0 (mod + 2), which means that + 2 divides 39. Therefore, + 2 must be positive divisor of 39. Now, the positive divisors of 39 are 1, 3, 13, and 39. But since is positive integer, + 2 must be at least 3 (because if = 1, + 2 = 3). So, the possible values for + 2 are 3, 13, and 39. Therefore, the corresponding values for are: - If + 2 = 3, then = 1. - If + 2 = 13, then = 11. - If + 2 = 39, then = 37. . . . Therefore, the sum of all such positive integers is 1 + 11 + 37. Let me compute that: 1 + 11 is 12, 12 + 37 is 49. . . . Figure 18: Additional examples of selected and rejected beams under VGS. The high-scoring beam applies concise and effective modular reasoning strategy whereas the rejected beam attempts several approachesincluding an incorrect use of the Factor Theorem and trial-and-errorbut ultimately fails. Color coding follows Figure 17. Part of generations are abridged with . . . notation. HMMT-25 Problem: Mark writes the expression (cid:112)abcd on the board, where abcd is four-digit number and = 0. Derek, toddler, decides to move the a, changing Marks expression to a(cid:112)bcd. Surprisingly, these two expressions are equal. Compute the only possible four-digit number abcd. Rejected beam with score 0.337: Okay, so have this problem where Mark writes the square root of four-digit number, which is written as (cid:112)(abcd), where isnt zero. Then Derek moves the first digit to the end, making the expression a(cid:112)(bcd). And the interesting part is that both expressions are equal. need to find the only possible four-digit number abcd that satisfies this condition. First, let me try to understand the problem step by step. Marks expression is the square root of four-digit number, so thats (cid:112)(1000a + 100b + 10c + d). Derek changes this by moving the first digit to the end, so the new number is 1000b + 100c + 10d + a, and Dereks expression is multiplied by the square root of this new number, which is a(cid:112)(1000b + 100c + 10d + a). And both of these are equal. So, mathematically, can write this as: (cid:112)(1000a + 100b + 10c + d) = a(cid:112)(1000b + 100c + 10d + a) If square both sides to eliminate the square roots, get: So, 1000a + 100b + 10c + = (1000b + 100c + 10d + a) . . . Selected Beam with score 0.999: Let me write this down step by step. Lets denote the four-digit number as = 1000a + 100b + 10c + d. So, Marks expression is . Then, Derek changes it to a(cid:112)(bcd), which is a(cid:112)(100b + 10c + d). According to the problem, these two expressions are equal, Which means: (cid:112)(1000a + 100b + 10c + d) = a(cid:112)(100b + 10c + d) Let me square both sides to eliminate the square roots: 1000a + 100b + 10c + = a² * (100b + 10c + d) Hmm, that gives me an equation: 1000a + 100b + 10c + = a² (100b + 10c + d) Let me denote the four-digit number as N, so = 1000a + 100b + 10c + d, and the three-digit number as = 100b + 10c + d. So, the equation becomes: = a² * But since is four-digit number and is single digit (from 1 to 9, since = 0), and is three-digit number (from 0 to 999). So, let me rewrite as 1000a + M. So, 1000a + = a² * Let me rearrange this equation: 1000a = a² * - Factor out on the right side: 1000a = M(a² - 1) Therefore, = (1000a) / (a² - 1) Since must be an integer (as its three-digit number), (1000a) must be divisible by (a² - 1). So, (a² - 1) must be divisor of 1000a. Let me note that is digit from 1 to 9. So, can try each from 1 to 9 and see if (a² - 1) divides 1000a, and then check if is three-digit integer. Lets compute for each a: Starting with a=1: a=1: = 1000*1 / (1 - 1) = 1000 / 0, which is undefined. So, a=1 is invalid. a=2: = 1000*2 / (4 - 1) = 2000 / 3 666.666..., which is not an integer. So, a=2 is invalid. a=3: = 1000*3 / (9 - 1) = 3000 / 8 = 375. Thats an integer. So, M=375. a=4: = 1000*4 / (16 - 1) = 4000 / 15 266.666..., not integer. So, a=4 invalid. a=5: = 1000*5 / (25 - 1) = 5000 / 24 208.333..., not integer. a=6: = 1000*6 / (36 - 1) = 6000 / 35 171.428..., not integer. a=7: = 1000*7 / (49 - 1) = 7000 / 48 145.833..., not integer. a=8: = 1000*8 / (64 - 1) = 8000 / 63 127.450..., not integer. a=9: = 1000*9 / (81 - 1) = 9000 / 80 = 112.5, which is not integer. So, only a=3 gives us an integer M=375, which is three-digit number. Therefore, the only possible four-digit number is 3375. . . . Figure 19: Additional examples of selected and rejected beams under VGS. The rejected beam misrepresents Dereks transformation, leading to an incorrect equation that derails the solution. In contrast, the high-scoring beam correctly models the relationship and systematically solves for the unique valid value of a. Color coding follows Figure 17. Part of generations are abridged with . . . notation."
        }
    ],
    "affiliations": [
        "Cornell University",
        "Databricks",
        "Harvard University",
        "Netflix"
    ]
}