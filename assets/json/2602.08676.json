{
    "paper_title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
    "authors": [
        "Tiwei Bie",
        "Maosong Cao",
        "Xiang Cao",
        "Bingsen Chen",
        "Fuyuan Chen",
        "Kun Chen",
        "Lun Du",
        "Daozhuo Feng",
        "Haibo Feng",
        "Mingliang Gong",
        "Zhuocheng Gong",
        "Yanmei Gu",
        "Jian Guan",
        "Kaiyuan Guan",
        "Hongliang He",
        "Zenan Huang",
        "Juyong Jiang",
        "Zhonghui Jiang",
        "Zhenzhong Lan",
        "Chengxi Li",
        "Jianguo Li",
        "Zehuan Li",
        "Huabin Liu",
        "Lin Liu",
        "Guoshan Lu",
        "Yuan Lu",
        "Yuxin Ma",
        "Xingyu Mou",
        "Zhenxuan Pan",
        "Kaida Qiu",
        "Yuji Ren",
        "Jianfeng Tan",
        "Yiding Tian",
        "Zian Wang",
        "Lanning Wei",
        "Tao Wu",
        "Yipeng Xing",
        "Wentao Ye",
        "Liangyu Zha",
        "Tianze Zhang",
        "Xiaolu Zhang",
        "Junbo Zhao",
        "Da Zheng",
        "Hao Zhong",
        "Wanli Zhong",
        "Jun Zhou",
        "Junlin Zhou",
        "Liwang Zhu",
        "Muzhi Zhu",
        "Yihong Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench."
        },
        {
            "title": "Start",
            "content": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing Tiwei Bie1, Maosong Cao1, Xiang Cao1, Bingsen Chen1, Fuyuan Chen1, Kun Chen1, Lun Du1, Daozhuo Feng1, Haibo Feng1,4, Mingliang Gong1, Zhuocheng Gong1, Yanmei Gu1, Jian Guan1, Kaiyuan Guan1, Hongliang He1,3, Zenan Huang1, Juyong Jiang1, Zhonghui Jiang1, Zhenzhong Lan1,3,, Chengxi Li1, Jianguo Li1,, Zehuan Li1, Huabin Liu1, Lin Liu1, Guoshan Lu1, Yuan Lu1, Yuxin Ma1, Xingyu Mou1, Zhenxuan Pan1, Kaida Qiu1, Yuji Ren1, Jianfeng Tan1, Yiding Tian1, Zian Wang1, Lanning Wei1, Tao Wu1, Yipeng Xing1, Wentao Ye1,2, Liangyu Zha1, Tianze Zhang1, Xiaolu Zhang1, Junbo Zhao1,2,, Da Zheng1,, Hao Zhong1,2, Wanli Zhong1,4, Jun Zhou1, Junlin Zhou1, Liwang Zhu1, Muzhi Zhu1,2, Yihong Zhuang1 1Ant Group, 2Zhejiang University, 3Westlake University, 4Southern University of Science and Technology"
        },
        {
            "title": "Abstract",
            "content": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench. 6 2 0 2 9 ] . [ 1 6 7 6 8 0 . 2 0 6 2 : r Figure 1: Aggressive parallel drafting, backed by retroactive correction, accelerates inference. Authors are listed in alphabetical order based on last name. indicates tech-leaders."
        },
        {
            "title": "Introduction",
            "content": "Discrete diffusion Large Language Models (dLLMs) have emerged as compelling alternative to autoregressive generation, offering the potential for non-monotonic reasoning and parallel decoding. However, the standard absorbing-state frameworkwhich enforces rigid, monotonic transition from [MASK] to fixed tokensfaces inherent limitations in fidelity. As highlighted by Kang et al. (2025), the independent nature of parallel decoding often amplifies token-level inconsistencies. While recent studies have attempted to mitigate this via confidence-based remasking (Wang et al., 2025b) or by employing external guide models (Lee et al., 2025). To bridge the gap between efficient parallel generation and high-fidelity reasoning, we align with the direction of generalizing discrete diffusion beyond absorbing states (R utte et al., 2025) and propose comprehensive framework for Editable State Evolution. Unlike prior work such as Song et al. (2025), we first design novel Error-Correcting Editable decoding strategy, which introduces dynamic paradigm controlled by dual probability thresholds. This paradigm encompasses two types of operations: direct decoding from mask to token, and editing from one token to another. This strategy enables the model to directly refine its own outputs during the generation process, thereby effectively addressing the local inconsistencies commonly encountered in parallel decoding. To cultivate this editing capability, our CPT and SFT phases expose the model to both masked positions and stochastic noise, incentivizing it to not only generate new content but also identify and rectify existing errors. Crucially, this architecture transforms the rigid trade-off between latency and fidelity into flexible, user-configurable continuum. By allowing the model to retroactively correct errors, we can aggressively lower the confidence threshold for the initial Mask-to-Token (M2T) phase without collapsing the generation quality. This insight gives rise to two distinct operating personas: Speedy Mode (S Mode), which prioritizes high-throughput generation by accepting lower-confidence tokens and relying on subsequent Token-to-Token (T2T) passes for rectification; and Quality Mode (Q Mode), which adheres to conservative thresholds to maximize reasoning rigor. This duality demonstrates that editability is not merely mechanism for error repair, but fundamental lever for accelerating parallel decoding. To further elevate the models capabilities, we integrate Reinforcement Learning (RL) stage. While recent works such as SPG (Wang et al., 2025a), TraceRL (Wang et al., 2025c) and ESPO (Ou et al., 2025) have demonstrated the potential of RL in improving dLLMs, applying policy gradients to block-autoregressive models remains challenging due to the intractability of sequence log-likelihoods. We circumvent this by adopting an ELBO-based Block-level Policy Optimization (EBPO) framework tailored for our editable setting. Notice that LLaDA2.1 extends its previous version (LLaDA2.0) by prioritizing decoding versatility over mere parameter scaling or benchmark peaking. By keeping the model size constant and minimal change of training data, we prove that our novel editing scheme enables lightning-fast execution with minimal overhead. This work serves as proof-of-concept for new dLLM paradigm that balances high-quality generation with extreme operational efficiency."
        },
        {
            "title": "2 Configurable Decoding Scheme",
            "content": "During LLM decoding, Exposure Biaswhere errors compound as the model conditions on its own imperfect predictionsis inevitable. This phenomenon is particularly severe in dLLMs due to their parallel generation nature. We observe that once such decoding errors occur, dLLMs tend to become increasingly conservative in subsequent steps, significantly slowing down the generation process. In contrast, autoregressive models exhibit lower exposure bias and can self-correct through extended chainof-thought reasoning. To address this challenge, we introduce an editing operation into the decoding process, enabling the model to retrospectively correct errors introduced during parallel generation, thereby achieving much better balance between generation speed and quality. Specifically, we extend standard discrete diffusion to support it. Unlike conventional absorbing-state models that enforce rigid monotonic transition from [MASK] to fixed tokens, our framework introduces dynamic Draft-and-Edit paradigm controlled by dual probability thresholds. We formalize the state evolution by defining two active update sets at timestep t: the Unmasking Set Γt and the Editing Set t. We formalize the state evolution by defining two active update sets at timestep t: the Unmasking Set Γt and 2 Figure 2: Overview of training & inference framework of LLaDA2.1 the Editing Set t. Let vi = arg maxv pθ(v xt) be the top-candidate. The update indices are identified as: Γt = = (cid:110) (cid:110) i = [MASK] and pθ(vi xi xt) > τedit and pθ(vi = vi xi xt) > τmask (cid:111) , (cid:111) , (1) (2) [0, 1] being the confidence thresholds configuring the decoding dynamics. The transition with τmask, τedit operator then applies the updates strictly on the union of these sets: t, xi 1 = if (cid:26)vi Γt xi otherwise. (3)"
        },
        {
            "title": "3.1 Training Alignment for “Draft-and-Edit”\nTo align the model with the “Draft-and-Edit” inference paradigm and mitigate the Exposure Bias inherent in\nstandard mask-based training, we employ a unified Mixture of M2T and T2T objective. This objective is\napplied throughout both the Continual Pre-Training (CPT) and Supervised Finetuning (SFT) stages.",
            "content": "This dual-stream training objective enables the model to develop two complementary capabilities fundamental to our framework: Drafting Stream (Mask-to-Token): The model learns to predict the correct token at each masked position to generate initial content, establishing the foundational drafting capability. Editing Stream (Token-to-Token): The model learns to recover original tokens from random noise perturbations (rectifying errors), equipping it with the ability to identify and rewrite artifacts. By consistently applying this dual-stream supervision from CPT through SFT, we ensure that LLaDA2.1 is fundamentally conditioned to function as both fast drafter and precise editor within single parameter space. Additionally, we employ Multi-turn Forward (MTF) data augmentation technique, by exposing the model to wider variety of editing scenarios, enhance the models editing capabilities."
        },
        {
            "title": "3.2 Reinforcement Learning Training\nThe application of policy gradient methods to diffusion models faces a fundamental hurdle: the intractability\nof the sequence-level log-likelihood, log πθ(x), which is essential for computing policy updates. While\nprior works have explored various approximations, they have historically struggled with high variance and\nprohibitive computational costs, limiting RL to small-scale experiments (Wang et al., 2025c; Ou et al., 2025;\nWang et al., 2025a). We overcome this bottleneck by synthesizing ELBO-based Block-level Policy Opti-\nmization (EBPO) with robust infrastructure optimizations. By utilizing the Evidence Lower Bound (ELBO)\nas a principled proxy for exact likelihood and implementing Vectorized Likelihood Estimation (Arriola\net al., 2025) to parallelize bound computation, we achieve orders-of-magnitude acceleration. This integration",
            "content": "3 allows us to scale dLLMs RL to unprecedented context lengths and training magnitudes, establishing stable and efficient pipeline for post-training. Formally, we maximize clipped surrogate objective, where the advantage is weighted by the probability ratio ρ: EBPO(θ) = Ex,y πθold (cid:104) min (cid:16) x) ˆA, clip(ρ(y ρ(y x), 1 ϵlow, 1 + ϵhigh) ˆA (cid:17)(cid:105) , (4) where ˆA is an estimator of the advantage function at timestep t, quantifying the relative improvement of the chosen action over the average expectation under the current policy. For set of discretized timesteps y0 to compute all block-conditional tn} n=1 and weights { { probabilities in parallel: , we construct composite input zn = ytn wn} log ρ(y x) n=1 wn (cid:16) b= log pθ(yb zn, x; M) log pθold (yb zn, x; M) (cid:17) . (5) Here, denotes Block-Causal Mask ensuring the b-th block attends only to valid history. By aggregating block-level contributions (B b=1) within single forward pass per timestep n, we establish computationally tractable pipeline for scaling reinforcement learning to long-context diffusion generation."
        },
        {
            "title": "4.1 Training Infrastructure\nContinued Pre-Training and Supervised Fine-Tuning For both continued pre-training (CPT) and super-\nvised fine-tuning (SFT), we adopt the same training infrastructure as LLaDA2.0 (Bie et al., 2025), leveraging\ndFactory (InclusionAI, 2025), which provides efficient training recipes specifically designed for dLLMs,\nexcept that we introduce a dedicated optimized implementation for the multi-turn forward (MTF) stage.",
            "content": "RL Training To enable effective policy optimization for dLLMs, we extend the AReaL framework (Fu et al., 2025; Mei et al., 2025) by developing specialized likelihood estimation and advantage estimation protocols that leverage diffusion sampling, explicitly supporting both T2T and M2T modes. This workflow is powered by ASystem (Ling Team et al., 2025) for distributed orchestration and utilizes customized version of SGLang (Ant Group Team & SGLang Team) as the dedicated rollout engine."
        },
        {
            "title": "4.3 Decoding Algorithm at Inference\nIn the inference stage, we adopt a decoding algorithm that combines Threshold Decoding (Ma et al., 2025)\nwith an explicit editing mechanism. In the basic setting, decoding and editing are performed within a\nsingle block: tokens are generated under a threshold-based constraint, and local edits are applied to revise\nintermediate outputs before the block is finalized.",
            "content": "Beyond single-block editing, we further introduce Multiple Block Editing (MBE) mechanism. MBE allows the model to revisit and revise previously generated blocks based on the content of newly decoded blocks."
        },
        {
            "title": "5 Evaluation",
            "content": "To comprehensively evaluate the quality of instruction-tuned models, we employ diverse suite of benchmarks categorized into five dimensions: Knowledge: MMLU-Pro (Wang et al., 2024), GPQA-Diamond (Rein et al., 2024), C-Eval (Huang et al., 2023), PHYBench (Qiu et al., 2025), TriviaQA (Joshi et al., 2017) Reasoning: SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019), KOR-Bench (Ma et al., 2024), HellaSwag (Zellers et al., 2019), BIG-Bench Hard (Suzgun et al., 2023), BIG-Bench Extra Hard 4 (Kazemi et al., 2025), MuSR (Sprague et al., 2023), ZebraLogic (Lin et al., 2025), PrOntoQA (Saparov & He, 2022), PIQA (Bisk et al., 2020), OCNLI (Hu et al., 2020), BIG-Bench Hard-CN (Opencompass Team, 2023) Coding: CRUXEval (Gu et al., 2024), MultiPL-E (Cassano et al., 2023), BigCodeBench (Zhuo et al., 2024), LiveCodeBench (Jain et al., 2024), Spider (Yu et al., 2018), BIRD (Li et al., 2023), HumanEval+ (Liu et al., 2023), MBPP+ (Liu et al., 2023) Math: OlympiadBench (He et al., 2024), AIME 2025 (AIME, 2025), Omni-MATH (Gao et al., 2024), GSM-Plus (Li et al., 2024), CMATH (Wei et al., 2023) Agent & Alignment: BFCL (Patil et al., 2025), IFEval (Zhou et al., 2023), Nexus Function Calling Benchmark (Nexusflow.ai Team, 2023) We report the comparative scores and TPF (tokens per forward) of LLaDA2.1-flash and LLaDA2.1-mini against other models in Tables 1 and 2, respectively. From the results, we observe that LLaDA2.1s scores under Mode decrease compared to LLaDA2.0, but substantial improvement in TPF is achieved. While under Mode, LLaDA2.1 surpasses the results of LLaDA2.0 on both mini and flash model. In Table 3, we focus on showcasing the speed performance of LLaDA2.1 in Mode. It can be observed that LLaDA2.1 exhibits significant speed variations across different domains, being highest in the code domain and lowest in instruction following. Specifically, after quantization, LLaDA2.1-flash achieves peak TPS of 891.74 on HumanEval+, while LLaDA2.1-mini reaches 1586.93 in peak TPS, demonstrating significant speed advantages. Figure 3: Throughput (TPS) comparison on nine benchmarks, consistent with the evaluation settings in Table 3, for LLaDA2.1 variants against LLaDA2.0, Ling, and Qwen3 across the mini (left) and flash (right) series. As shown in Table 4, under the same Mode setting, Multi-Block Editing (MBE) yields consistent performance improvements across benchmarks for both Flash and Mini variants, at the cost of modest reduction in throughput. The gains are particularly evident on reasoning and coding tasks, indicating that iterative cross-block refinement effectively corrects local errors and improves global consistency without substantially compromising decoding efficiency. Figure 3 further illustrates the throughput (in terms of token per sec) comparison of LLaDA 2.1 variants against LLaDA 2.0, Ling, and Qwen-3 across 5 different benchmark domains as shown in Table 3. This comparison spotlights LLaDA-2.1 (S Mode)s striking speed advantage: it achieves dramatically faster inference while sacrificing only negligible sliver of output quality."
        },
        {
            "title": "6 Outlook and Limitation",
            "content": "Tradeoff Between Inference Speed and Accuracy While LLaDA2.1 significantly improves inference speed, clear speed-accuracy tradeoff persists, particularly with noticeable performance differences across various 5 Table 1: Benchmark Performance of LLaDA2.1-flash, comparing with several baseline models. For diffusion language model, we report its scores across each benchmark along with its TPF (tokens per forward); for AR model, we report its scores only, as its TPF is inherently equal to 1."
        },
        {
            "title": "Average",
            "content": "GPQA MMLU-Pro C-EVAL PHYBench TriviaQA BIG-Bench Hard BIG-Bench Extra Hard bbh-zh MuSR ZebraLogic PrOntoQA PIQA OCNLI HellaSwag KOR-Bench DROP SQuAD 2.0 LiveCodeBench CRUXEval-O MBPP+ HumanEval+ MultiPL-E BigCodeBench-Full BIRD-SQL Spider AIME 2025 OlympiadBench GSM-Plus CMATH Omni-MATH IFEval-strict-prompt BFCL v3 Nexus FC Qwen3-30BA3B-Inst-2507 (Score) 73.09 54.14 74.21 88.12 29.84 65.61 85.54 37.80 86.18 79.15 90.97 97.12 91.57 71.59 86.31 69.2 87.57 89.51 46.42 86.75 78.21 87.88 70.67 41.49 47.75 81.79 61.88 77.59 89.41 96.58 54.00 83.73 73.41 49. Ling-flash-2.0 LLaDA2.0-flash (Score) 71.52 (Score 72."
        },
        {
            "title": "Knowledge",
            "content": "69.16 77.55 87.54 27.67 69.76 62.31 74.79 85.21 30.06 66."
        },
        {
            "title": "Reasoning",
            "content": "89.36 23.24 75.09 82.72 87.60 97.88 91.95 65.36 81.59 69.44 88.32 81.32 52.48 82.75 80.89 87.58 65.76 40.70 47.49 80.58 55.89 76.19 89.71 96.52 53."
        },
        {
            "title": "Math",
            "content": "86.75 27.86 87.52 82.72 82.30 96.50 96.50 71.63 84.97 63.04 87.90 90.00 42.51 85.12 79.37 88.41 74.87 41.58 45.76 82.49 60.00 74.07 89.74 96.90 50.30 Agent & Alignment 81.15 67.69 36.25 82.62 74.94 50. TPF) 3.08 3.29 2.36 1.90 2.70 1.94 2.66 4.60 3.21 1.70 2.74 2.64 1.43 1.09 1.26 3.44 2.26 3.10 4.23 3.21 4.02 6.45 3.14 3.33 2.16 4.42 4.57 3.70 2.68 2.17 3. 1.47 4.87 5.53 6 LLaDA2.1-flash (S Mode) TPF) (Score LLaDA2.1-flash (Q Mode) TPF) (Score 5.93 73.54 72.34 66.67 75.31 86.93 26.04 72.55 87.82 33.51 82.55 80.10 84.20 95.00 92.44 72.17 85.60 62.80 87.55 90. 3.95 4.43 2.71 4.10 4.30 5.61 5.04 5.78 2.90 5.80 9.23 2.38 1.83 2.31 4.97 5.40 5.01 44.05 85.25 76.72 89.63 70.89 37.11 42.18 79. 6.48 6.54 10.43 13.81 7.77 8.51 5.09 8.74 63.33 75.85 89.23 96.54 52.30 5.36 6.46 7.14 4.84 6.01 83.36 74.86 44. 2.24 9.24 11.29 3.64 2.37 2.62 1.75 2.66 2.92 3.28 3.17 3.77 1.85 3.26 5.73 1.44 1.32 1.51 2.77 2.53 3.90 3.80 3.80 5.96 9.18 4.33 4.70 2.95 5. 3.46 3.81 3.83 2.65 3.50 1.41 6.76 7.38 67.30 76.59 86.71 28.23 72.93 88.69 35.77 86.23 79.84 88.90 97.00 92.17 72.75 85.31 65.12 87.86 90.80 45.37 87.50 77.25 89.63 73.34 39.21 44.04 81.04 63.33 76.59 89.69 96.63 54. 83.55 75.61 47.65 Table 2: Benchmark Performance of LLaDA2.0-mini, comparing with several baseline models. For diffusion language model, we report its scores across each benchmark along with its TPF (tokens per forward); for AR model, we report its scores only, as its TPF is inherently equal to 1."
        },
        {
            "title": "Average",
            "content": "GPQA MMLU-Pro C-EVAL PHYBench TriviaQA BIG-Bench Hard BIG-Bench Extra Hard bbh-zh MuSR ZebraLogic PrOntoQA PIQA OCNLI HellaSwag KOR-Bench DROP SQuAD 2.0 LiveCodeBench CRUXEval-O MBPP+ HumanEval+ MultiPL-E BigCodeBench-Full Aider BIRD-SQL Spider AIME 2025 OlympiadBench GSM-Plus CMATH Omni-MATH IFEval-strict-prompt BFCL v3 CodeIF-Bench Nexus FC Qwen3-8B (no think) (Score) 61.59 48.01 65.83 80.6 9.76 52.51 79.48 18.27 80.09 70.02 37.48 93.12 88.30 61.49 79.56 54.96 84.56 85.21 26.76 74.06 72.69 79.5 61.70 36.05 55.64 36.11 72.80 22.08 55.33 85.56 95.42 33.20 84.29 70.12 50.00 37. Ling-mini-2.0 LLaDA2.0-mini (Score) 64.72 59.41 67.18 82.17 14.59 55.63 83.70 14.81 66.11 71.36 79.85 96.06 87.54 60.17 69.02 63.2 78.80 75. 42.29 76.12 77.25 80.03 67.09 35.00 49.62 39.67 76.43 (Score 63."
        },
        {
            "title": "Knowledge",
            "content": "47.76 64.27 81.80 11.70 51."
        },
        {
            "title": "Reasoning",
            "content": "78.21 16.47 75.75 71.48 64.20 86.00 86.51 64.51 79.01 49.92 81.89 86."
        },
        {
            "title": "Coding",
            "content": "31.83 71.62 78.24 81.71 67.46 32.89 39.85 39.34 76."
        },
        {
            "title": "Math",
            "content": "47.66 72.30 87.18 96.40 48. 36.67 67.70 86.50 95.72 41.70 Agent & Alignment TPF) 2.60 2.73 2.15 1.78 2.48 1.54 2.36 2.03 2.77 1.45 2.30 2.36 1.45 4.06 1.50 2.45 2.02 2. 3.34 2.78 3.43 5.16 2.78 2.87 3.57 1.96 3.93 2.41 2.63 2.41 1.98 2.57 76.16 53.75 46.00 34.38 80.78 70.72 46.00 35.18 1.24 4.26 2.62 4.06 7 LLaDA2.1-mini (S Mode) TPF) (Score LLaDA2.1-mini (Q Mode) TPF) (Score 5. 63.90 62.24 48.36 63.42 78.40 12.75 53.33 78.42 15.30 67.65 70.43 68.50 87.50 84.87 61.02 75.71 46.64 81.55 84.51 3.62 4.22 3.39 4.41 3.21 5.02 3.19 3.89 2.48 5.38 4.86 2.59 1.78 2.39 4.28 5.84 4.33 28.85 70.62 78.84 80.49 64.16 30.18 43.61 37.32 75.78 6.42 5.85 10.59 12.32 7.23 7.33 8.11 4.48 7.98 36.67 64.30 85.88 95.63 41.70 81.33 72.06 42.00 31.59 6.34 7.08 6.82 4.94 6.41 1.83 7.39 6.68 8.27 3.12 2.12 2.41 1.91 2.52 2.02 2.86 1.66 2.35 1.56 2.93 2.73 1.45 1.23 1.49 2.35 2.87 3.09 3.63 3.35 6.30 7.77 4.01 4.09 4.85 2.42 5.48 3.29 3.99 3.69 2.56 3.56 1.25 5.14 3.62 4. 53.28 64.84 78.59 13.05 54.24 80.58 15.78 70.40 71.89 77.10 84.50 86.89 61.59 76.19 48.00 82.37 85.13 30.40 73.75 74.07 82.93 67.17 34.39 45.11 38.40 77.55 43.33 66.67 86.55 94.99 43.60 83.18 73.61 48.00 33.69 Table 3: Throughput (TPS) and relative score changes of Flash and Mini variants across benchmarks. For each model family, the w/o Quant setting serves as the baseline. Cells under w/ Quant are vertically split into TPS Score. Category Benchmark LLaDA2.1-flash LLaDA2.1-mini Coding HumanEval+ MBPP+ CRUXEval-O BigCodeBench-Full LiveCodeBench Math GSM-Plus Knowledge GPQA-Diamond Instruction Following IFEval w/o Quant TPS 746.66 639.47 550.09 691.14 571.60 574.65 416.92 w/ Quant TPS Score -3.04 891.74 761.38 -1.85 -0.24 645.72 801.48 +1.06 -1.76 663.39 w/o Quant TPS 1496.67 1286.96 980.82 1220.40 1015.82 w/ Quant TPS Score 1586.93 -0.61 1303.96 +1.85 1063.94 -1.00 1307.45 -0.09 1102.92 +1.98 667.07 -0.03 1080. 1186.18 -0.30 477.79 -0.64 724.30 784.62 -1. 219.37 248.25 +1.48 338.58 365.52 -1.29 Reasoning PrOntoQA 770.88 912.16 -1.00 880.19 938. -1.50 Table 4: Performance comparison of LLaDA2.1-flash and Mini variants with and without Multi-Block Editing TPF. (MBE) across benchmarks. Each cell reports Score Category Benchmark LLaDA2.1-flash LLaDA2.1-mini Knowledge Reasoning Coding MMLU-Pro TriviaQA bbh-zh ZebraLogic LiveCodeBench CRUXEval-O BigCodeBench-Full Spider w/o MBE Score TPF w/ MBE Score TPF w/o MBE Score TPF w/ MBE Score TPF 75.31 72. 82.55 84.20 44.05 85.25 37.11 79.18 4.43 4.30 5.78 5.80 6.48 6.54 8.51 8.74 75.90 72. 83.21 88.20 46.48 87.00 39.30 80.58 3.88 4.28 4.85 5.03 5.62 5.62 7.00 8.33 63.42 53. 67.65 68.50 28.85 70.62 30.18 75.78 4.22 3.21 3.89 5.38 6.42 5.85 7.33 7.98 63.10 53. 67.94 70.00 29.74 70.62 30.70 76.67 3.66 3.14 3.41 4.62 5.44 5.02 6.05 7.59 Math AIME 2025 63.33 5.36 70.00 4.71 36. 6.34 36.67 5.25 Agent & Alignment IFEval-strict-prompt 83.36 2. 83.55 2.11 81.33 1.83 83.55 1. Average 70.69 5.82 72.67 5. 57.63 5.25 58.24 4.59 domains. It is necessary to adjust threshold parameters for different domains to balance speed and accuracy. In structured-data fields such as code and math, setting Mode achieves high speed with little accuracy loss. However, in some general chat cases, these settings can cause undesirable output. In such cases, we recommend adjusting the parameters to Mode. Our conjecture is that this pattern may be related to the models inherent preference for structured data or the distributional characteristics of training dataset. Further validation will be conducted in our future research. Editable Enhanced dLLM Although dLLMs inherently support high parallelism, theoretically offering speed advantages over AR models, our experimental observations show that this high parallelism also introduces higher error rate compared to AR models. These hidden errors can reduce the models confidence in subsequent reasoning, ultimately slowing down the overall process. Therefore, timely editing to correct errors is essential. In our case analysis of LLaDA2.1, we observed that prompt editing corrected decoding errors, helping to maintain higher inference speeds. However, research on the editing capabilities of dLLMs is still in its early stages. We anticipate that future work, such as integrating editing into reinforcement learning, will further enhance the performance of editable dLLMs. LLaDA2.1 remains in an experimental phase. Although rare, certain edge cases may occur. Empirical observations show that aggressively lowering the masking threshold τmask can quickly generate rough drafts. Although the models self-correction can partially alleviate the stuttering artifacts (such as n-gram 8 repetitions) caused by independent parallel sampling, balancing drafting speed with the quality of the initial structure remains key operational frontier. Overall, by unifying dynamic inference, hybrid training, and principled reinforcement learning, our work establishes solid foundation for self-correcting discrete diffusion language models. Conclusion Overall, LLaDA2.1 introduces an editing feature, which, through cumulative error correction, significantly lowered the decoding threshold of the dLLM and yielded considerable inference speed benefits. However, this model still faces many unresolved issues, and we anticipate that more powerful editable dLLMs will deliver even more unexpected and impressive results. References AIME. AIME Problems and Solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php/ AIME Problems and Solutions. Aleph-Alpha. Alpha-MoE: megakernel for faster tensor parallel inference. URL https://aleph-alpha. com/alpha-moe-a-megakernel-for-faster-tensor-parallel-inference/. Ant Group Team and SGLang Team. Power Up Diffusion LLMs: Day-0 Support for LLaDA 2.0 URL https://lmsys.org/blog/2025-12-19-diffusion-llm. LMSYS Org. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Ling Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, and Yihong Zhuang. LLaDA2.0: Scaling Up Diffusion Language Models to 100B, December 2025. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. MultiPL-E: Scalable and Polyglot Approach to Benchmarking Neural Code Generation. IEEE Transactions on Software Engineering, 49(7):36753691, 2023. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs. arXiv preprint arXiv:1903.00161, 2019. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning, 2025. URL https://arxiv.org/abs/2505.24298. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CruxEval: Benchmark for Code Reasoning, Understanding and Execution. arXiv preprint arXiv:2401.03065, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. arXiv preprint arXiv:2402.14008, 2024. Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra ubler, and Lawrence Moss. Ocnli: Original chinese natural language inference. arXiv preprint arXiv:2010.05444, 2020. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-Eval: Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. InclusionAI. dFactory: Easy and Efficient dLLM Fine-Tuning, 2025. URL https://github.com/inclusionAI/ dFactory. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, and Kangwook Lee. ParallelBench: Understanding the Tradeoffs of Parallel Decoding in Diffusion LLMs, October 2025. URL http://arxiv.org/abs/2510.04767. arXiv:2510.04767 [cs]. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Yuanzhu Peter Chen, et al. Big-bench extra hard. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2647326501, 2025. Sanghyun Lee, Sunwoo Kim, Seungryong Kim, Jongho Park, and Dongmin Park. Effective Test-Time Scaling of Discrete Diffusion through Iterative Refinement, November 2025. URL http://arxiv.org/abs/2511. 05562. arXiv:2511.05562 [cs]. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36:4233042357, 2023. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, and Wei Bi. Gsm-plus: comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. arXiv preprint arXiv:2402.19255, 2024. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100, 2025. Ling Team et al. Every step evolves: Scaling reinforcement learning for trillion-scale thinking model, 2025. URL https://arxiv.org/abs/2510.18855. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. arXiv preprint arXiv:2410.06526, 2024. Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, et al. dinfer: An efficient inference framework for diffusion language models. arXiv preprint arXiv:2510.08666, 2025. Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, and Yi Wu. Real: Efficient rlhf training of large language models with parameter reallocation. In Proceedings of the Eighth Conference on Machine Learning and Systems, MLSys 2025, Santa Clara, CA, USA, May 12-15, 2025. mlsys.org, 2025. Nexusflow.ai Team. Nexusraven-v2: Surpassing gpt-4 for zero-shot function calling, 2023. URL https: //nexusflow.ai/blogs/ravenv2. Opencompass Team. opencompass. open-compass/opencompass, 2023. URL https://github.com/open-compass/ Jingyang Ou, Jiaqi Han, Minkai Xu, Shaoxuan Xu, Jianwen Xie, Stefano Ermon, Yi Wu, and Chongxuan Li. Principled RL for Diffusion LLMs Emerges from Sequence-Level Perspective, December 2025. URL http://arxiv.org/abs/2512.03759. arXiv:2512.03759 [cs]. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, et al. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: Graduate-Level Google-Proof Q&Q Benchmark. In First Conference on Language Modeling, 2024. Dimitri von utte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard Sch olkopf, et al. Generalized Interpolating Discrete Diffusion. June 2025. URL https://openreview.net/forum?id=rvZv7sDPV9. Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, et al. Seed diffusion: large-scale diffusion language model with high-speed inference, 2025. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. arXiv preprint arXiv:2310.16049, 2023. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. Chenyu Wang, Paria Rashidinejad, DiJia Su, Song Jiang, Sid Wang, Siyan Zhao, Cai Zhou, Shannon Zejiang Shen, Feiyu Chen, Tommi Jaakkola, Yuandong Tian, and Bo Liu. Spg: Sandwiched policy gradient for masked diffusion language models. arXiv preprint arXiv:2510.09541, 2025a. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking Discrete Diffusion Models with Inference-Time Scaling. October 2025b. URL https://openreview.net/forum?id=IJryQAOy0p. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025c. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, et al. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test? arXiv preprint arXiv:2306.16636, 2023. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, et al. Instruction-Following Evaluation for Large Language Models. arXiv preprint arXiv:2311.07911, 2023. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Southern University of Science and Technology",
        "Westlake University",
        "Zhejiang University"
    ]
}