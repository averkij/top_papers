{
    "paper_title": "GAEA: A Geolocation Aware Conversational Model",
    "authors": [
        "Ron Campos",
        "Ashmal Vayani",
        "Parth Parag Kulkarni",
        "Rohit Gupta",
        "Aritra Dutta",
        "Mubarak Shah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodal models (LMMs) proprietary and open-source researchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, one of which is geolocalization, LMMs struggle. In this work, we propose to solve this problem by introducing a conversational model GAEA that can provide information regarding the location of an image, as required by a user. No large-scale dataset enabling the training of such a model exists. Thus we propose a comprehensive dataset GAEA with 800K images and around 1.6M question answer pairs constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose a diverse benchmark comprising 4K image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 state-of-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best open-source model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by 8.28%. Our dataset, model and codes are available"
        },
        {
            "title": "Start",
            "content": "GAEA: Geolocation Aware Conversational Model Ron Campos1*, Ashmal Vayani1*, Parth Parag Kulkarni1*, Rohit Gupta1, Aritra Dutta1, Mubarak Shah1 1 University of Central Florida 5 2 0 2 0 2 ] . [ 1 3 2 4 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodal models (LMMs) proprietary and open-sourceresearchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, one of which is geolocalizaIn this work, we propose to solve tion, LMMs struggle. this problem by introducing conversational model GAEA that can provide information regarding the location of an image, as required by user. No large-scale dataset enabling the training of such model exists. Thus we propose GAEA-1.6M, comprehensive dataset with 800K images and around 1.6M question-answer pairs constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose diverse benchmark, GAEA-Bench, comprising 4K image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 stateof-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best opensource model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by 8.28%. Our dataset, model and codes are available https://ucf-crcv.github.io/GAEA. 1. Introduction Image geolocalization [23, 32, 50, 55, 59] is notoriously challenging task, in which, conventional AI models predict the precise GPS coordinate of an image taken anywhere on Earth. Moreover, seasonal changes, geographical and climatic diversity, changes in solar zenith angle, and lack of diverse image distributions make the geolocalization task more challenging. Although difficult, geolocaliza- *equally contributing first authors Figure 1. Performance of GAEA and other LMMs on global scale image geo-localization. GAEA makes correct predictions when asked different questions about summarizing scene, location, and geographical context. While GPT-4o-mini can give correct suggestions relevant to the region, GAEA provides correct amenity with proximity to the location of the image. tion has direct applications in multiple domains, including tourism, navigation [16], urban planning [46], and security [50], among many. Recently, CLIP-inspired image-to-GPS retrieval approach, GeoCLIP [50], has shown significant performance in global-scale image geolocalization. To further mitigate the performance gap, and to increase the generalization capacity of the models, interestingly, new wave of works infuse human-level cognition and inference capacity in their model training [23, 32, 55]. E.g., PIGEON is trained on data from the popular geolocalization game GeoGuesser Figure 2. Data Collection and Annotation Pipeline. GAEA-1.6M includes geographically diverse visual samples from various data sources, such as MP-16, GLD-v2, and CityGuesser68k (left). We also incorporate OpenStreetMap (OSM) metadata and auxiliary context for each image, ranging from climate zones to geographical clues about the country (middle). Using open-source LLMs and GPT-4o, we generate four diverse question-answer pairs across geolocation, reasoning, and conversational subsets (right). [2]; recent vision-language model, GeoReasoner [32] use userand administrator-maintained approximately 3K textual clues from GeoGuessr and Tuxun gaming platforms. These focused geolocalization models, however, lack geographical understanding of the predicted locations beyond their GPS coordinates. They cannot provide additional information that might be invaluable for applications such as tourism, navigation, urban planning, etc. Even if the models possess that understanding, they do not have the conversational ability to convey that information and fail to meet the users needs. In contrast, despite having the conversational capability, visually and textually prompted large language models (LLMs) [20, 48, 56] and their multimodal variants, popularly referred to as large multimodal models (LMMs) [10, 11, 14, 35, 47], fail to capture fine-grained nuances from an image in specialized downstream tasks such as geolocalization, making their predictions vastly imprecise and worse than random guesses in many cases; see Figure 1. Motivated by all these aspects, in this paper, we propose GAEA, an open-source conversational model with globalscale geolocalization capability. To the best of our knowledge, this is the first work in the ground-view geolocalization domain that introduces an open-source conversational chatbot, where the user can obtain image geolocalization, relevant description of the image, and engage in meaningful conversation about the surrounding landmarks, natural attractions, restaurants or coffee shops, medical or emergency facilities, and recreational areas. However, training an open-source LMM with conversational capacity is not straightforward. These models are data-hungry and their training is compute intensive. Unfortunately, no dataset can facilitate the training of such model. To this end, we meticulously curate GAEA-1.6M high-quality conversational VQA pair equipped with diversity in scene understanding and image captions used for training and instruction tuning the LMMs on the streetlevel geolocalization task. GAEA-1.6M is comprehensive dataset consisting over 800K images from MP-16 [29], covering locations around the Earth. We augment these images with rich meta-data from the OpenStreetMap (OSM) [38] at 1km radius, first effort of its kind. OSM attributes contain details about the surrounding area, nearby landmarks, accessible services, and historical buildup of the region. The QA subset of GAEA-1.6M contains 380K QA pairs; the geolocalizable explanatory captions set contains 385K images and is equipped with their corresponding knowledge and reasoning captions. These knowledge and reasoning captions are constructed using set of geographical context clues from GeoGuessr [2] that enable the model to gain holistic understanding of the location. Taken together, GAEA-1.6M is the largest and most comprehensive collection of geolocalizable and conversational QA pairs. We use this data source to design our conversational chatbot, GAEA. To quantitatively evaluate the conversational capability of LMMs and address the scarcity of benchmark datasets in geolocalization setting, we propose GAEA-Bench, diverse set of 4K conversational question samples. GAEA-bench comprises multiple-choice (MCQs) and true/false (T/Fs) for checking models understanding and choosing capability, short questions (SVQAs) for testing models knowledge, and long questions (LVQAs) for evaluating models descriptive and in-depth explanation ability about the location in question. We summarize the main contributions as follows: We propose GAEA-1.6M (Section 3), new dataset for training conversational image geolocalization models. For evaluating conversational capabilities in geolocalFigure 3. Overview of GAEA-Bench. GAEA-Bench is designed to evaluate the conversational abilities of various LMMs across different question types, including MCQs, T/F, and both short and long VQAs. We have carefully selected subset of 4k samples from MP-16 and generated corresponding OSM metadata to generate QA pairs using GPT-4o. GAEA-Bench aims to fill the gap in conversational benchmarks by incorporating geolocalization capabilities. ization setting (Section 5), we propose GAEA-Bench, novel benchmark with different types of questionanswers. We propose GAEA, conversational chatbot (4) that goes beyond global-scale geolocalization and provides information about the location described by an image. We quantitatively compare the performance of our model to 8 state-of-the-art open-source, and 3 proprietary LMMs, including GPT-4o [9] and Gemini-2.0-Flash [47]. 2. Related Work Vision language models (VLMs) have been at the forefront of computer vision research; geo-localizable VLMs are in their nascent stages. Vision Language Models. Multimodal learning unifies different modalities by common representation. By contrastively fitting text and images into the same feature space, CLIP [39] has revolutionized multimodal learning. LLMs like GPT2 [40] made strides in representing text and next token prediction. Visual question answering (VQA) was of interest before, but, after LLaVA [34] and BLIP2 [31] combined the conversational aspects of LLMs and the representational capabilities of CLIP-like models, many problems of VQA are addressed.After that, numerous modern works emerged, such as GeoChat [27], Qwen2.5-VL [12], LLaMA-3.2 Vision [10], and LLaVA-OneVision [30] as well as proprietary models like GPT4 [9] and Gemini [47]. Even though most of these models are excellent for general VQA, they perform poorly on specialized tasks in fields like medicine, statistics, and geolocalization. This inspires the need for specialized VLMs that can address specific tasks. Geo-localization is crucial field in vision research with essential applications in forensics, social media, and exploration; see [16, 46, 50]. On global scale, Weyand et al. [53] first introduced classification-based approach on the Im2GPS [24] dataset. Vo et al. [51] introduced classification in multiple hierarchies, while CPlaNet [45] introduced combinatorial partitioning technique for combining coarse hierarchies to predict finer ones. Over the years many other works like ISNs [25], TransGeo [59], TransLocator [52], and GeoDecoder [15] have made significant advancements in this classification-based worldwide geolocalization by introducing scene-based specialized encoders and hierarchical evaluation, auxiliary scene recognition, and twin encoder approach, and query-based encoder-decoder architecture respectively. PIGEON [23], the most recent work, leverages the image representation capabilities of the CLIP vision encoder, and unique clustering approach to improve its geo-localization performance. Image-to-image retrieval models tend to be more accurate than their classificationbased counterparts but infeasible on global scale due to their requirement for large reference image galleries. GeoCLIP [50] was the first work to incorporate the contrastive multimodal learning between images and raw GPS information that revolutionized this domain by introducing more accurate retrieval-based model for global scale. These specialized models work well for worldwide image geo-localization but lack the conversational aspect that can aid an individual in gaining holistic understanding of location portrayed in an image. GeoReasoner [32] attempts to incorporate an inherent geospatial understanding into VLM by looking at specific information displayed in the image. It also introduces the idea of locatability, which can determine the extent of that information present in the image which may improve the reasoning capability of the model. The model however lacks the conversational aspect, and the locatability-based filtering of data might hurt its generalization capability. We address these issues in GAEA by primarily focusing on its conversational ability. The generalizability of GAEA comes from its training data. All specialized geo-localization models that function Figure 4. Qualitative examples showcasing various question-types, including multiple-choice, true/false, short and long VQAs generated using an open-source model on our GAEA-1.6M dataset. We carefully select geographical tags from OSM metadata to generate QA pairs. on global scale train their model on MP-16 [29] which is large-scale worldwide dataset. However, it lacks the verbal context required in VLM training. Hence, we introduce new conversational dataset GAEA-1.6M; see details in Section 3. Additionally, we introduce the first conversational benchmark in Section 4 to evaluate Geolocalization VLMs and an evaluation pipeline to judge the efficacy of such models. 3. GAEA-1.6M The GAEA-1.6M dataset provides comprehensive global coverage, featuring both rich conversational and diverse geolocalization sets. It includes various QA formats, such as MCQs, True/False, and open-ended VQA (long and short), from more than 234 countries/territories, grouped under conversational and geolocalization groups. Spanning 40k cities across 7 continents, GAEA-1.6M is structured into two key groups: conversational and geolocalization. With over 1.6 million QA pairs, it captures the geographical diversity of both underrepresented and widely recognized regions worldwide. 3.1. Dataset Curation and Annotation Acquiring Diverse Geo-localizable Images. We sample geographically diverse visual data from MediaEval 2016 (MP-16) [29], Google Landmarks v2 (GLDv2) [54], and CityGuessr [28] to curate GAEA-1.6M. MP-16 contains over 4.6 million geotagged Flickr images, including indoor and outdoor scenes. For our street-view geolocalization subset, we filter out indoor images, retaining 3 million outdoor images. However, some of these images are non-geolocalizable, such as close-up shots of doors, grass, or wires, which are excluded from the final dataset. To filter out non-geolocalizable images, we process all 3 million samples using GeoCLIP [50], which is trained on the full MP-16 dataset and effectively identifies non-geolocalizable outlier images. GeoCLIP assigns confidence score based on its ability to predict GPS coordinates, with higher scores indicating geo-localizability. We set confidence threshold of 0.75 and computed the distance between the ground truth MP-16 GPS coordinates and the GeoCLIPs predicted location. We retain the images if this distance is less than 500 km; see additional ablations on different thresholds and distance metrics in the Appendix. To achieve balanced geographical distribution in GAEA-1.6M, we use the 10th hierarchy of S2-Cells [5] to partition our filtered MP-16 dataset into 16,753 spatial grid cells. S2-Cells enable hierarchical spatial indexing, ensuring diverse global coverage while preventing overrepresenting densely imaged regions. We randomly sample up to 200 images from each cell, resulting in final set of over 750K distinct samples. GLDv2 [54] is fine-grained landmark recognition dataset featuring natural and human-made landmarks across diverse time zones, climates, and lighting conditions. Given the significance of landmark geolocation for real-world applications, we randomly sample 50K distinct landmarks from GLDv2. These highly recognizable landmarks offer rich geographic and cultural context. Each image is linked to Wikipedia metadata, from which we extract GPS coordinates using the Wikimedia [6] API. We then apply the reverse geocoder Python library to determine each landmarks corresponding city and country. CityGuessr68k [28] focuses on global video-based geolocalization emphasizing urban regions and hierarchical prediction across 166 major cities. To incorporate this diversity, we randomly sample one frame from each of the 54K training videos and include them in our dataset. These three sources provide over 852K geographically diverse geolocalizable images, forming GAEA-1.6M dataset. 3.2. Meta-data curation for dataset annotation After acquiring all visual for our GAEAConversational Assistant, we churn the metadata for each image for comprehensive QA-pair generation. Churning OSM metadata. OpenStreetMap (OSM) [38] is samples Additional Metadata. For auxiliary context, we group our country-specific, geographically diverse dataset in 31 Koppen-Geiger climate zones [13]. We obtain the traffic direction data through WorldStandards [7] and Land Cover Use statistics from EarthEnv [1]. Additionally, we compute scene labels for each image using the Places2 [58] database. 3.3. Question-Answer (QA) Pairs Generation As seen in Figure 2, GAEA-1.6M is carefully curated to enhance ground-view geolocalization through diverse, context-rich QA pairs. Comprising over 800K distinct images and around 1.6 million QA pairs, it stands as the largest and most comprehensive dataset for this task; see Figure 4. Unlike existing works, such as [19, 32], which are limited to JSON structures and fewer question types, our work emphasizes the conversational capabilities of the model, providing broader range of QA formats. The dataset is divided into three subsetsConversational, Reasoning, and GeoLocalization, each designed to capture different aspects of geographic understanding. These subsets feature various question formats, including multiple-choice [37], true/false, and open-ended questions (SVQA and LVQA) [49]. Below, we detail the curation process for each subset. Conversational QA Generation. We generate conversational QA pairs using OSM metadata from the sampled MP-16 and GLDv2 subsets. We prompt Qwen-2.5-14B [56] with enriched OSM attributes to create diverse question formats, including short-form, multiple-choice, and true/false questions. These OSM tags cover various categories such as amenities, food places, financial institutions, government offices, accommodation, transportation, healthcare, religious sites, education, and waterways. This subset comprises over 380K questions. Geolocalization Questions. To enhance the geolocalization capabilities of our GAEA model, we introduce largescale meta-geographic information through geolocationspecific QA pairs. This subset consists of 820K imagequestion pairs designed to help the model predict the correct location of an image. We curate 50K geolocation questions from GLDv2, each corresponding to distinct landmark, leveraging their global recognition to improve locationbased reasoning. Additionally, we incorporate 54K geolocation QA pairs from CityGuessr, which focuses on urban environments, and 720K from MP-16, ensuring broad geographic coverage. This results in diverse and welldistributed geolocation QA dataset spanning 234 countries and territories, 40K cities, and 7 continents. Reasoning Questions. We generate detailed image-caption QA pairs (Long-VQA) to enhance fine-grained reasoning in our GAEA model. We prompt GPT-4o [9] with each image, its scene labels, and country-specific geographical attributes, including GeoGuessr clues, traffic-side driving information, Koppen-Geiger climate zone, and land cover Figure 5. Evaluation pipeline highlights various question types we introduce in our GAEA-1.6M. We use GPT-4o as judge to score such responses. collaborative open-source mapping platform that provides extensive geographical data. In our work, OSM plays central role by enriching geolocalization and conversational capabilities. We retrieve metadata from 1 km radius around the GPS coordinates of 850K images, leveraging OSMs detailed, publicly annotated tags. These tags cover many realworld elements, including amenities, transportation, hotels, and restaurants, making them invaluable for our groundview geolocalization and QA generation. OSM data is multilingual, which is key challenge. To ensure accessibility, we use GPT-4o [9] to translate these annotations into English. Additionally, many retrieved tags consisted of plain numbers or non-meaningful entries, which we systematically filtered out to retain only informative and contextually relevant metadata. To our knowledge, this is the first work to utilize OSMs rich metadata to develop conversational chatbot for ground-view geolocalization. We curated the OSM metadata for MP-16 and GLDv2 visual samples. Curating Country-Specific Geographical Clues. We web-crawled diverse clues from Plonkit[4], an opensource community resource for the GeoGuessr [2] game, which has over 65 million players. Similar datasets have been used in recent works [23, 32]. We obtained 129 country clues but found gaps for some countries, such as New Zealand and France. To address this, we curated clues for 58 additional countries using GPT-4o, aligning them with Plonkits style, resulting in altogether 187 countries. These clues are incorporated into our dataset for generating reasoning-based QAs. For examples of the type of clues utilized, see Figure 19 in the Appendix. long-form questions (LVQAs), we follow similar process for generating reasoning questions in GAEA-Bench, resulting in an additional 1,000 questions. In total, we curate 4K diverse image-text QA pairs. To ensure that the GAEABench remains independent of the training set, we select geographically distinct locations for its 4K samples. We show our GAEA-Bench annotation and curation process in Figure 3. The OSM metadata are fetched for each image and are passed to Qwen2.5-14B for generating several QA pairs, including SVQA, MCQ, and T/F. 4. GAEA Architecture GAEA follows the architecture of the open-source model, Qwen2.5-VL [12], which seamlessly integrates (1) vision encoder, (2) vision-to-language projector, and (3) language model. The re-engineered vision-transformer (ViT) architecture incorporates 2D-RoPE and window attention. The projector is two-layer multi-layer perception (MLP) to align raw patch features from the ViT and provides the final representation EJoint by concatenating the image embeddings, EImg with the text embeddings, EText such that EJoint = [EImg, EText]; see Figure 6. Training Details. We perform single-stage fine-tuning of Qwen2.5VL on our GAEA Conversational Astrained across all sistant dataset. three subsetsgeolocalization, reasoning, and conversationalcovering both open-ended QA formats (short and long answers) and decision-based questions (multiplechoice and true/false). This fine-tuning process enables the model to integrate rich geographical cues, contextual metadata, and image-specific attributes, enhancing its spatial reasoning, location inference, and multimodal conversational capabilities. We employ LoRA fine-tuning [26] with rank of = 16 and α = 32 along with the unfrozen vision-to-language MLP projector. To handle varying image resolutions, we apply dynamic resolution processing: Images below 448 448 are upsampled, while those exceeding 10001000 are downsampled, similar to [12]. The model is trained for one epoch over 12,600 steps. The model is 5. Benchmarking and Evaluations GAEA-1.6M training set comprises four distinct question types: Multiple Choice Questions (MCQs), True/False (T/F), and Short and Long Visual Question Answering (VQA). The GAEA model is meticulously trained to ensure conversational fluency while possessing the capability to geolocalize visual samples. Current evaluation frameworks primarily focus on standard geo-localization datasets, measuring accuracy using distance-based metrics at various scales, including Street (1 km), City (25 km), Region (200 km), Country (750 km), and Continent (2,500 km). However, these methods fail to assess the conversational capaFigure 6. GAEA architecture with single-stage training strategy including trainable MLP layers and LLM weights. data. While scene labels are unique to each image, the other attributes provide country-level context. GPT-4o integrates this information to generate contextually rich and highly correlated captions with the provided geographic labels. These reasoning-based captions strengthen the models geolocalization and conversational capabilities and induce rich semantic understanding in our model by infusing human-level cognition and inference capability, deducing the model to emphasize why particular image features might be associated with specific geographical contexts, reducing disinformation [42, 43]. In total, we curate 385K knowledge-driven LVQA pairs. 3.4. GAEA-Bench Existing benchmarks for evaluating geolocalization tasks mainly focus on retrieval and classification-based methods, such as IM2GPS [24], IM2GPS3k [51], and GSW15k [18], which assess the distance between ground-truth and predicted GPS coordinates. However, there is lack of conversational benchmarking datasets to evaluate the geolocalization and conversational capabilities of LMMs. We introduce GAEA-Bench, geographically diverse and conversationally rich multimodal benchmark to address these shortcomings. GAEA-Bench is designed to assess LMMs across various question types, including MCQs, true/false, and long and short VQAs while integrating geolocalization tasks. It includes 4K image-text QA pairs that provide rich geographical context for each image. GAEA-Bench Curation. We curate non-overlapping subset of highly geolocalizable MP-16 images, manually filtering out the non-geolocalizable ones. Using OpenStreetMaps (OSM), we generate metadata within 1km radius and curate 1,000 short-form (SVQA), 1,000 multiplechoice (MCQ), and 1,000 true/false (T/F) questions. For Model Name Performance on Different QA Formats LVQA SVQA MCQ TF Average GeoChat-7B [27] LLaMA-3.2-Vision-11B [10] LLaVA-Next-Mistral-7B [33] GLM-4V-9B [22] Phi-3.5-Vision-Instruct [8] InternVL2-8B [57] LLaVA-OV-7B [30] Qwen2.5-VL [12] Gemini-2.0-Flash [47] GPT-4o-mini [9] GPT-4o [9] GAEA (Ours) 24.06 ( 59.71%) 47.45 ( 20.55%) 47.57 ( 20.34%) 40.63 ( 31.97%) 48.04 ( 19.56%) 52.42 ( 12.22%) 53.34 ( 10.68%) 54.84 ( 8.17%) 55.47 ( 7.12%) 58.82 ( 1.51%) 63.62 ( 6.53%) 59.72 16.38 ( 67.96%) 26.84 ( 47.50%) 23.20 ( 54.63%) 21.29 ( 58.36%) 14.31 ( 91.57%) 30.93 ( 39.51%) 29.32 ( 42.66%) 35.14 ( 31.27%) 34.72 ( 32.09%) 34.13 ( 33.25%) 49.56 ( 3.07%) 51.13 54.00 ( 27.90%) 52.40 ( 30.04%) 28.90 ( 61.42%) 56.30 ( 24.83%) 54.40 ( 27.36%) 55.30 ( 26.17%) 57.60 ( 23.10%) 47.20 ( 36.98%) 56.20 ( 24.97%) 54.00 ( 27.90%) 59.37 ( 20.73%) 74.90 32.20 ( 58.98%) 47.20 ( 39.87%) 56.70 ( 27.77%) 50.60 ( 35.54%) 57.20 ( 27.13%) 56.90 ( 27.52%) 56.10 ( 28.54%) 59.10 ( 24.71%) 56.10 ( 28.54%) 34.17 ( 56.47%) 69.83 ( 11.04%) 78.50 31.66 ( 52.07%) 43.47 ( 34.20%) 39.09 ( 40.83%) 42.21 ( 36.10%) 43.49 ( 34.17%) 48.89 ( 25.99%) 49.09 ( 25.69%) 49.07 ( 25.72%) 50.62 ( 23.37%) 45.28 ( 31.46%) 60.59 ( 8.28%) 66.06 Table 1. We benchmark 11 open-source and proprietary LMMs on GAEA-Bench. Notably, GAEA outperforms all open-source models and fares higher than the proprietary models on decision-making questions (MCQs and TFs). We provide the relative performance change for each model compared to GAEA. tion types, including multiple-choice, true/false, and openended questions (short and long VQAs). See Section 8.1 for the baselines used in this work. We employ different prompts for each type of question. We use GPT-4o as judge and prompt it to score responses to various types of questions with different criteria. We use accuracy for MCQs and T/F, correctness for SVQA, and consistency, relevance, and geographical correctness for long VQAs (LVQAs); see the evaluation pipeline in Figure 5. Here, correctness refers to how closely the models output matches the location and the correct answer in the ground-truth response [49]. For LVQA, the consistency metric evaluates the fluency and readability of the models prediction [44, 48, 49], while geographical correctness assesses whether the models prediction accurately identifies the correct city and country, directly matching the groundtruth answer. This is further discussed in Section 8.2, and Figures 16, 20 and 21. Quantitative Geo-localization Evaluation. We compared the performance of GAEA against six state-of-theart (SoTA) geo-localization models, namely PlaNet [53], CPlaNet [45], ISNs [25], TransLocator [52], GeoDecoder [18], and PIGEON [23] on three standard geo-localization benchmarks including IM2GPS [24], IM2GPS3k [51], GWS15k [18]. We prompt various LMMs to output the corresponding city and country to which the image belongs. We retrieve GPS coordinates using GeoPy [3] and compute distance with ground truth. We compare the output with distance thresholds of 1 km, 25 km, 200 km, 750 km, and 2,500 km; see Table 2. Classification Accuracy. Figure 7 illustrates the classification accuracy pipeline at the city and country levels. For this evaluation, we introduce three new datasets: GeoDE [41], Figure 7. Our classification accuracy pipeline evaluates city and country predictions by comparing them against ground truth annotations derived from GPS coordinates, with GPT-4o serving as the evaluator. bilities of LMMs. To address this gap, we define our evaluation process in three key dimensions: (a) Conversational accuracy, (b) Quantitative geo-localization accuracy, and (c) Classification accuracy. 5.1. Evaluation and Metrics Conversational Evaluation. Most geolocation-specific models operate as black box systems, providing GPS coordinates without offering any reasoning or justification behind their outputs. In contrast, GAEA is the first model of its kind, explicitly trained on 1.6 million instructions, which include significant number of knowledge-reasoning question-answer pairs. This enables GAEA to integrate world knowledge, such as geographical clues, conversational meta-tags, and advanced reasoning capabilities, making its geolocation predictions more transparent and insightful. To address the challenges of complex conversational evaluation, we benchmark 10 state-of-the-art opensource and closed-source LMMs on GAEA-Bench, which is meticulously curated to evaluate LMMs on diverse quesBenchmark Model City 25 km Region 200 km Country Continent 750 km 2500 km IM2GPS [24] IM2GPS3k [51] GWS15k [18] PlaNet [53] CPlaNet [45] ISNs [25] TransLocator [52] GeoCLIP [50] GeoDecoder [18] PIGEON [23] GaGA [19] GAEA (Ours) PlaNet [53] CPlaNet [45] ISNs [25] TransLocator [52] GeoDecoder [18] GeoCLIP [50] PIGEON [23] GaGA [19] GAEA (Ours) ISNs [25] TransLocator [52] GeoDecoder [18] GeoCLIP [50] PIGEON [23] GAEA (Ours) 24.5 37.1 43.0 48.1 41.8 50.2 40.9 38.8 41.4 24.8 26.5 28.0 31.1 33.5 34.5 36.7 33.0 33. 0.6 1.1 1.5 3.1 9.2 3.1 37.6 46.4 51.9 64.6 60.8 69.0 63.3 54.8 54.4 34.3 34.6 36.6 46.7 45.9 50.7 53.8 48.0 52.3 4.2 8.0 8.7 16.9 31.2 15.9 53.6 62.0 66.7 75.6 77.2 80.0 82.3 75.1 74.3 48.4 48.6 49.7 58.9 61.0 69.7 72.4 67.1 70. 15.5 25.5 26.9 45.7 65.7 41.9 71.3 78.5 80.2 86.7 89.9 89.1 91.1 87.7 87.8 64.6 64.6 66.0 80.1 76.1 83.8 85.3 82.1 84.0 38.5 48.3 50.5 74.1 85.1 71.4 Table 2. We benchmark the performance of various specialized models on standard geolocation datasets. GAEA demonstrates competitive results, outperforming GaGA on multiple distance thresholds in both IM2GPS and IM2GPS3k. Standard Geo-localization Evaluation Table 2 compares GAEAs performance with various specialized encoderonly methods across three geolocalization benchmarks. While GAEA is trained on large-scale conversational dataset with geolocalization capabilities, it achieves competitive results against specialized models. We also evaluate against GaGA [19], which is trained on dataset five times larger than ours, on IM2GPS and IM2GPS3k. However, we exclude comparisons on GSW-15K due to differences in dataset curation. Following the guidelines of [18], we reconstruct the GSW-15K benchmark. In IM2GPS3k, GAEA achieves the second-highest scores after PIGEON, outperforming GaGA across all four distance thresholds. It surpasses GaGA by 2.5% at the 25km radius and 3.66% at the country level. Additionally, GAEA outperforms the specialized model GeoCLIP [50] across all thresholds, with 1.5% higher score in the region category and 1% improvements at the city and country levels. In IM2GPS, GAEA outperforms GaGA at 25 km and 2,500 km, remains competitive at 200 km and 750 km, and slightly surpasses PIGEON at the city level while maintaining competitive performance across other thresholds. We also evaluate GAEA on GSW-15K, one of the most challenging datasets, which includes non-geolocalizable landmarks. GAEA outperforms GeoCLIP [50] and GeoDecoder [18] on city-level distance and achieves comparable performance at the region and country levels. Figure 8 presents GAEAs Classification Accuracy on three new datasets: CityGuessr68k-val [28], GeoDE [41], and DollarStreet [21]. GAEA outperforms recent LMMs, including Figure 8. Classification accuracy for both city and country labels, where GAEA establishes itself as strong baseline, surpassing several recent LMMs in performance. DollarStreet [21], and CityGuessr68k [28]. From GeoDE, we sampled 22K images based on 16 meta-tags having geolocalizable features. From DollarStreet, we manually sampled 1.3K images, removing indoor and non-geolocalizable samples. Since its metadata contains only country-level information, we evaluate this dataset solely for country classification. Additionally, we use the validation set of 14K images from CityGuessr and all 22K GeoDE samples for city and country classification tasks. 5.2. Results and Discussion GAEA-Bench Evaluation. Table 1 presents the per-model performance of 12 recent LMMs on GAEA-Bench. The (i) Our proposed model, results offer several insights: GAEA, achieves the highest average performance across decision-making questions (T/F and MCQs) and Short VQAs. Among proprietary models, GPT-4o [9] overall performs the best, with an accuracy of 60.59%, excelling particularly in Long VQAsoutperforming GAEA by 6.53% in this category. However, both open-source and proprietary models struggle with short-form questions. E.g., GPT-4os accuracy drops from 63.62% on long questions to 49.5% (ii) GAEA outperforms all LMMs on short questions. with an average accuracy of 66.06%, surpassing GPT-4o by 8.28% and outperforming the second-best open-source (iii) Sevmodel, LLaVA-OneVision [30], by 25.69%. eral open-source models, including LLaMA-3.2-11B [20], GLM-4V-9B [22], and Phi-3.5-Vision [8], achieve com- (iv) LMMs perform better parable overall performance. on decision-making questions (MCQs and T/F) than openended questions; see Figure 17. E.g., LLaVA-OneVision experiences 57.8% drop in accuracy on SVQA compared to T/F questions. The low performance on free-form questions underscores the challenge of using short questions to effectively assess conversational capabilities in the GAEABench. We provide qualitative comparisons with several LMMs in Figures 10, 14 and 15 LLaVA-OneVision [30], InternVL [17], and GLM-4V-9B [22], on both cityand country-level classification. These results highlight GAEAs extensive geographical coverage and strong geolocation capabilities. 6. Conclusion We introduced GAEA, the first interactive conversational model with specialized geolocation capabilities, explicitly trained on large-scale conversational dataset, GAEA1.6M. We meticulously designed the dataset to enhance GAEAs reasoning, conversational abilities, and geolocation accuracy. We curated geolocalizable images from MP16, GLDv2, and CityGuessr68k, enriching them with auxiliary context and metadata, such as geographic clues, and climate zones. In addition to high-quality instruction set, we present GAEA-Bench, comprehensive benchmark that evaluates LMMs across multiple question types, including MCQs, True/False, shortand long-VQAs. Our results show that GAEA outperforms recent LMMs on GAEABench, demonstrating strong geolocation and conversational capabilities by leveraging OpenStreetMap (OSM) data. These findings establish GAEA as strong baseline for future research in geolocalization."
        },
        {
            "title": "References",
            "content": "[1] . EarthEnv. https://www.worldstandards.eu/cars/list-of-leftdriving-countries/, . 5 [2] . GeoGuessr. https://www.geoguessr.com/, . 2, 5, 17 [3] . GeoPy. https://geopy.readthedocs.io/en/stable/, . 7 [4] . Plonkit. https://www.plonkit.net/, . 5 [5] . S2-Cells. https://code.google.com/archive/p/s2-geometrylibrary/, . 4 [6] . WikiMedia. https://commons.wikimedia.org/w/api.php, . 4 [7] . WorldStandards. https://www.worldstandards.eu/cars/listof-left-driving-countries/, . [8] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 7, 8, 13 [9] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 5, 7, 8, 13 [10] Meta AI. Llama 3.2: Vision and edge models. Meta AI Blog, 2024. 2, 3, 7, 13 [11] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Advances in neural information processing systems, pages 2371623736, 2022. 2 [12] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 3, 6, 7, 13, [13] Hylke Beck, Niklaus Zimmermann, Tim McVicar, Noemi Vergopolan, Alexis Berg, and Eric Wood. Present and future koppen-geiger climate classification maps at 1-km resolution. Scientific data, 5(1):112, 2018. 5 [14] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in neural information processing systems, pages 18771901, 2020. 2 [15] Denis Carriere. Geocoder: Simple, consistent. https: //geocoder.readthedocs.io/. Accessed: [Insert Date]. 3 [16] Athanasios Chalvatzaras, Ioannis Pratikakis, and Angelos Amanatiadis. survey on map-based localization techniques for autonomous vehicles. IEEE Transactions on intelligent vehicles, 8(2):15741596, 2022. 1, 3 [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 9, 13 [18] Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vicente Vivanco Cepeda, and Mubarak Shah. Where we are and what were looking at: Query based worldwide image geo-localization using hierarchies and scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2318223190, 2023. 6, 7, 8, 13 [19] Zhiyang Dou, Zipeng Wang, Xumeng Han, Chenhui Qiang, Kuiran Wang, Guorong Li, Zhibei Huang, and Zhenjun Han. Gaga: Towards interactive global geolocation assistant. arXiv preprint arXiv:2412.08907, 2024. 5, 8 [20] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, [21] William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. Advances in Neural Information Processing Systems, 35:1297912990, 2022. 8 [22] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. 7, 8, 9, 13 [23] Lukas Haas, Michal Skreta, Silas Alberti, and Chelsea Finn. In Proceedings of Pigeon: Predicting image geolocations. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1289312902, 2024. 1, 3, 5, 7, 8, 13 [24] James Hays and Alexei Efros. Im2gps: estimating geographic information from single image. In 2008 ieee conference on computer vision and pattern recognition, pages 18. IEEE, 2008. 3, 6, 7, 8, 13 [25] Zheng Hong, Yiwei Yin, Zhe Luo, and Jiebo Luo. Isns: Image-specific neural style transfer for image geolocation. arXiv preprint arXiv:2106.11593, 2021. 3, 7, 8, 13 [26] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: LowIn InternaRank Adaptation of Large Language Models. tional Conference on Learning Representations. 6, 14 [27] Kartik Kuckreja, Muhammad S. Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad S. Khan. Geochat: Grounded large vision-language model for remote sensing. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3, 7, 13 [28] Parth Parag Kulkarni, Gaurav Kumar Nayak, and Mubarak Shah. Cityguessr: City-level video geo-localization on global scale. In European Conference on Computer Vision, pages 293311. Springer, 2024. 4, [29] Martha Larson, Mohammad Soleymani, Guillaume Gravier, Bogdan Ionescu, and Gareth JF Jones. The benchmarking initiative for multimedia evaluation: Mediaeval 2016. IEEE MultiMedia, 24(1):9396, 2017. 2, 4 [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. 3, 7, 8, 9, 13 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 3 [32] Ling Li, Yu Ye, Bingchuan Jiang, and Wei Zeng. GeoReasoner: Geo-localization with reasoning in street views usIn Proceedings of the ing large vision-language model. 41st International Conference on Machine Learning, pages 2922229233, 2024. 1, 2, 3, 5 [33] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. 7, 13 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3 [35] Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao Anwer, Tim Baldwin, Michael Felsberg, and Fahad Khan. Palo: polyglot large multimodal model for 5b people. arXiv preprint arXiv:2402.14818, 2024. 2 [36] Ahmad Mahmood, Ashmal Vayani, Muzammal Naseer, Salman Khan, and Fahad Shahbaz Khan. Vurf: generalpurpose reasoning and self-refinement framework for video understanding. arXiv preprint arXiv:2403.14743, 2024. 12 [37] Vishal Narnaware, Ashmal Vayani, Rohit Gupta, Swetha Sirnam, and Mubarak Shah. Sb-bench: Stereotype bias arXiv preprint benchmark for large multimodal models. arXiv:2502.08779, 2025. 5 [38] OpenStreetMap contributors. Openstreetmap, 2024. [Data set]. OpenStreetMap Foundation. Available as open data under the Open Data Commons Open Database License (ODbL). 2, 4, 12 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. 3 [40] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019. 3 [41] Vikram Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: geographically diverse evaluation dataset for object recognition. Advances in Neural Information Processing Systems, 36:6612766137, 2023. 7, 8 [42] Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi, Ferhat Sadak, Muhammad Saeed, Ranjan Sapkota, Aditya Jain, Anas Zafar, Muneeb Ul Hassan, et al. Who is responsible? the data, models, users or regulations? responsible generative ai for sustainable future. arXiv preprint arXiv:2502.08650, 2025. 6 [43] Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, et al. Vldbench: Vision language modarXiv preprint els disinformation detection benchmark. arXiv:2502.11361, 2025. [44] Ananya Sai, Akash Kumar Mohankumar, and Mitesh Khapra. survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR), 55(2):139, 2022. 7 [45] Paul Hongsuck Seo, Tobias Weyand, Jack Sim, and Bohyung Han. Cplanet: Enhancing image geolocalization by combiIn Proceedings of the Euronatorial partitioning of maps. pean Conference on Computer Vision (ECCV), pages 536 551, 2018. 3, 7, 8, 13 [46] Qiaomu Shen, Wei Zeng, Yu Ye, Stefan Muller Arisona, Simon Schubiger, Remo Burkhard, and Huamin Qu. Streetvizor: Visual exploration of human-scale urban forms based on street views. IEEE transactions on visualization and computer graphics, 24(1):10041013, 2017. 1, 3 [47] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 3, 7, 13 for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):14521464, 2017. 5 [59] Sijie Zhu, Mubarak Shah, and Chen Chen. Transgeo: Transformer is all you need for cross-view image geo-localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11621171, 2022. 1, 3 [48] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao Anwer, Michael Felsberg, Tim Baldwin, Eric Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent GPT. arXiv preprint arXiv:2402.16840, 2024. 2, 7 [49] Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, Noor Ahsan, Nevasini Sasikumar, Omkar Thawakar, Henok Biadglign Ademtew, Yahya Hmaiti, Amandeep Kumar, Kartik Kuckreja, et al. All languages matter: Evaluating lmms on culturally diverse 100 languages. arXiv preprint arXiv:2411.16508, 2024. 5, [50] Vicente Vivanco Cepeda, Gaurav Kumar Nayak, and Mubarak Shah. Geoclip: Clip-inspired alignment between locations and images for effective worldwide geolocalization. Advances in Neural Information Processing Systems, 36, 2024. 1, 3, 4, 8 [51] Nam Vo, Nathan Jacobs, and James Hays. Revisiting im2gps in the deep learning era. In Proceedings of the IEEE International Conference on Computer Vision, pages 26212630, 2017. 3, 6, 7, 8, 13 [52] Bingxian Wang, Ying Chen, Xin Zhang, Haojie Wang, Ziqiang Wang, and Wen Xu. Translocator: transformerIn Probased large-scale image geolocalization approach. ceedings of the AAAI Conference on Artificial Intelligence, pages 25582566, 2022. 3, 7, 8, 13 [53] Tobias Weyand, Ilya Kostrikov, and James Philbin. Planetphoto geolocation with convolutional neural networks. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pages 3755. Springer, 2016. 3, 7, 8, 13 [54] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for In Proceedings of instance-level recognition and retrieval. the IEEE/CVF conference on computer vision and pattern recognition, pages 25752584, 2020. 4 [55] Yibo Yan and Joey Lee. Georeasoner: Reasoning on geospatially grounded context for natural language understanding. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 4163 4167, 2024. [56] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2, 5 [57] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplugowl3: Towards long image-sequence understanding in multimodal large language models, 2024. 7 [58] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database GAEA: Geolocation Aware Conversational Model"
        },
        {
            "title": "Supplementary Material",
            "content": "Total images Total cities / countries Total Questions Total geo-localization questions Total explanatory captions Total open-ended questions Total multiple-choice questions Total true/false questions 822,951 41,481 / 234 1,580,531 822,951 384,947 267,668 48,673 56,292 Table 3. Dataset Statistics This work was supported by MFC Lockheed Martin, Orlando. We would also like to thank David Shatwell, Manu Pillai, Praveen Tirupattur, Brian Dina, and Suranadi insightful discussions and Dodampaganmage for contributions. their We organize the rest of the Supplementary Material as follows: In Section 7, we provide the additional details of our dataset, GAEA-1.6M . In Section 8, we provide additional baseline results on GAEA. Section Section 9 discusses the reproducibility of the GAEA-1.6M and provides details on privacy, safety, and broader impact. 7. Addendum to the Dataset In this section, we present the dataset statistics and challenges encountered in its creation. Additionally, we discuss our plans to address these limitations in future works. 7.1. Challenges with Open Street Maps (OSM) OpenStreetMaps (OSM) [38] is rich data source for geospatial applications. It contains wide variety of geographic and infrastructure-related information. Using such vast open-source dataset, we can collect data about stationary objects in the world, including infrastructure, topological information, various types of amenities (e.g., schools, hospitals, restaurants), transportation networks, international country boundaries, historical and cultural sides, and natural features (e.g., forests, rivers, and seas). Each feature from the OSM dataset has several associated features, such as names and physical characteristics. In GAEA-1.6M, we geocode the visual sample with its GPS coordinates and use the location information (longitude and latitude) as query to the OSM database to fetch geospatial information in 1 KM radius and further utilize that information to generate question-answer pairs for the training of GAEA. Despite being such rich source of data, OSM faces several challenges. One major issue is the variability in data quality and completeness, as contributions to OSM are made by the open-source community, which may result in inconsistent information across different regions. Urban areas often have much more detailed information than rural areas, leading to less comprehensive annotations for rural regions. Another inconsistency related to human annotations stems from the different representations of the same label in different areas, introducing inherent heterogeneity in the structure of OSM data. For instance, some users might label path as trail, while others might call it footway, and distinctions between what counts as park versus garden are not always clear. Moreover, querying and retrieving data from OSM is compute-intensive task. It often becomes slower as the number of queries increases and struggles to handle dense or redundant information, necessitating efficient filtering and optimization techniques. Lastly, the information is not always up-to-date, as volunteers update different areas at different times. While some locations may have very recent data, others may be outdated, and sometimes different parts of the same area may contain information from varying periods. 7.2. Statistics GAEA-1.6M covers 234 different countries and territories, and 41,481 cities. Table 3 denotes the exact sample numbers. 7.3. Motivation for constructing GAEA-1.6M The dataset contains wide spectrum of questions, with varying difficulty levels. Figure 11 outlines category consisting of easily geo-localizable landmarks such as the Statue of Liberty, and Figure 12 represents one of the many difficult questions found in our dataset, which attempts to fine-tune an LMM to respond with location information that an average human could struggle to identify [36]. In Figure 13, we show full example of one of our LVQA prompts. Training an LMM with some of the metadata found in OSM [38] can be challenging task. For instance, questions like, What are the hours of operation of the nearest coffee shop located in this area? could be difficult for model to learn effectively and respond to accurately. We encourage the research community to explore and develop methods that could help LMMs meaningfully represent such fine-grained geo-localization information. Figure 9. Geo-localization qualitative example. GAEAs performance on geo-localization tasks is compared to open-source LMMs using CityGuessr, DollarStreet, and GeoDE datasets. 8. Addendum to Baseline and Evaluation This section covers the models used for comparison with GAEA, the prompts used during training and inference, the prompts used for evaluating GAEA-Bench, and the training hyperparameters. 8.1. Baselines We benchmark 8 top-performing open-source LMMs, including LLaMA 3.2-Vision [10], InternVL2 [17], Qwen2.5VL [12], Phi3.5-vision-instruct [8], GeoChat [27], LlaVAOneVision [30], GLM-4V-9B [22], LLaVA-NeXT-Mistral7B [33], and 3 proprietary models, Open-AIs GPT-4o, GPT-4o-mini [9], and Googles Gemini-2.0-Flash [47] on GAEA-Bench. Additionally, we compared the performance of GAEA against six state-of-the-art (SoTA) geo-localization models, namely PlaNet [53], CPlaNet [45], ISNs [25], TransLocator [52], GeoDecoder [18], and PIGEON [23] on three standard geo-localization benchmarks including IM2GPS [24], IM2GPS3k [51], and GWS15k [18]. For comprehensive overview of the hyperparameter configuration for GAEA, see Table 4 in the Supplementary Material. 8.2. Prompts Used During Training and Inference When training GAEA, we employed the task-specific prompts shown in Figure 18 to align the models understanding with target objectives. During inference, these identical prompts were used for all models evaluated on GAEA-Bench to ensure comparability. Figure 10. SVQA qualitative example. GAEAs performance on SVQA tasks is compared to open-source and proprietary LMMs. 8.3. Prompts Used in Evaluation To obtain our results from Figure Figure 17, we use three prompts in evaluating the questions in GAEA-Bench. Since the question types of our benchmark vary, multi-prompt approach is needed. We have reformatted certain elements of the actual prompts for conciseness, though figures Figures 16, 20 and 21 closely reflect these three prompts. 8.4. Training Hyperparameters We perform single-stage training on our baseline [12] using GAEA-1.6M. The training is conducted for 1 epoch with global batch size of 128, utilizing gradient accumulation steps of 4 to optimize resource usage for small batch sizes. The initial learning rate is set to 105, using cosine learning rate scheduler to provide smooth decay in the learning rate for effective convergence. weight decay of 0 is applied to avoid penalizing weights during updates, which can be advantageous for certain model architectures. The warmup ratio is configured at 0.03 to ensure gradNumber of epochs Global batch size Gradient accumulation steps Initial learning rate Learning rate scheduler Weight decay Warmup ratio LoRA rank (r) LoRA α LoRA dropout Model data type Maximum context length of LLM 128,000 Attention type 1 128 4 105 cosine 0 0.03 16 32 0.01 bfloat16 flash attention Table 4. Hyperparameters used for training GAEA. ual increase in the learning rate during the initial training phase, stabilizing early optimization. We employed lowRank adaptation (LoRA) [26] for efficient fine-tuning, with rank, = 16, α = 32, and dropout rate of 0.01, enabling Figure 11. These two images display examples of what we consider as easy questions. Easy questions include the questions that pertain to easily identifiable landmarks that are associated with celebrated locations. Figure 12. The two images above denote examples of what we consider as hard questions. Hard questions include the questions that prompt the model to answer specific details pertaining to locations. targeted model adjustments with minimal overhead. The model operates in bfloat16 precision to balance computational efficiency and numerical stability. maximum context length of 128,000 allows for processing extremely long sequences, while flash attention enhances the computational efficiency of attention mechanisms, especially for extended contexts. These settings collectively optimize the models performance and adaptability for vision-language tasks. We list the training hyperparameters in the Table 4. 8.5. Additional Qualitative Results In this Section, we discuss additional qualitative results of GAEA and compare them with selected open-source and Figure 13. Through our explanatory captions (LVQA), we introduce the reasoning capabilities in our GAEAto identify the geographical information about that visual sample. Outlined in bold, we provide explanations of the correlation between specific visual cues and their associated geographical contexts, encouraging GAEAto refine its reasoning capabilities. proprietary models (as mentioned in Table 1 in the main paper). Figure 9 presents comparison of city-country predictions against other competing models. We also show the qualitative results of GAEA on short version questions (SVQA), multiple-choice questions (MCQs), and true or false questions (TF) in Figures 10, 14 and 15 respectively. For these Figures, we highlight correct predictions with green, while incorrect predictions are marked as red. Together with our quantitative metrics, we conclude that GAEA appears to outperform many of these models across various tasks. Evaluation Prompt for LVQA Evaluate the following predicted answer by comparing it to the provided ground truth. Focus on the accuracy of 1) location prediction, 2) cultural aspect matching, 3) consistency and quality of reasoning, 4) specificity and relevance, 5) and fluency and clarity. Question: {question} Ground Truth: {ground truth} Model Prediction: {predicted answer} Instructions: How accurately does the predicted answer identify the specific country, city, or state mentioned in the ground truth? Does the predicted answer capture and reflect the cultural aspects present in the ground truth? Is the predicted answer logically consistent and demonstrates sound reasoning based on the information provided? Does the predicted answer provide specific information that is directly relevant to the question and closely aligns with the ground truth? Is the language in the predicted answer fluent, clear, and well-articulated? Provide single overall score out of 10, based on these five criteria, weighing the criteria in the order listed, with location relevance and cultural aspect matching receiving the most weight. Return only the numeric score, without additional commentary. Figure 16. Evaluation prompt for longer form open-ended questions (LVQA). We assess model predictions based on location prediction, cultural aspect matching, and quality of reasoning, with an emphasis on location relevance. source, and we plan to release it via an academic website for research, academic, and commercial use. The dataset is protected under the CC-BY license of Creative Commons, which allows the users to distribute, remix, adapt, and build upon the material in any medium or format, as long as the creator is attributed. The license allows GAEA-1.6M for commercial use. As the authors of this manuscript and collectors of this dataset, we reserve the right to distribute the data. Additionally, we provide the code, data, and instructions needed to reproduce the main experimental baseline results, and the statistics pertinent to the dataset. We specify all the training details (e.g., data splits, hyperparameters, model-specific implementation details, compute resources used, etc.). Figure 14. MCQ qualitative example. GAEAs performance on MCQ answering tasks is compared to open-source and proprietary LMMs. Figure 15. T/F qualitative example. GAEAs performance on T/F answering tasks is compared to open-source and proprietary LMMs. 9. Reproducibility, Privacy, Safety, and"
        },
        {
            "title": "Broader Impact",
            "content": "GAEA-1.6M takes the first step in infusing conversational elements into the geo-localization task. The dataset is openFigure 17. We showcase the performance of various LMMs on four diverse question types. GAEA outperforms on average across all question forms. GPT-4o achieves the highest accuracy on long questions. Figure 18. Task-specific prompts used to train and evaluate GAEA The dataset can be used by multiple domain experts. Its application includes but is not only limited to tourist assistance, government analysts, and GeoGuessr [2] enthusiasts. Although we do not find any foreseeable harm that the dataset can pose to human society, it is always possible that some individual or an organization can use this idea to devise technique that can appear harmful to society and can have evil consequences. However, as authors, we are Figure 19. Example of the country-specific clues we used to generate reasoning questions. absolutely against any detrimental usage of this dataset, regardless of whether it is by an individual or an organization, under profit or non-profitable motivation, and we pledge not to support any detrimental endeavors concerning our data or the idea therein. Evaluation Prompt for SVQA Evaluate the following predicted answer by comparing it to the provided ground truth. Focus on the accuracy of 1) location prediction, and 2) specificity and relevance. Question: {question} Ground Truth: {ground truth} Model Prediction: {predicted answer} Scoring Guidelines: High score: Predicted response closely matches the specific location and provides specific information that closely aligns with the ground truth. Low score: Predicted response lacks knowledge or is unrelated to the ground truth Provide score out of 10 for each criterion. Return only the numeric score, without additional commentary Figure 20. Evaluation prompt for free-form open-ended questions (SVQA). We assess model predictions based on location accuracy, specificity, and correctness. Evaluation Prompt for MCQ/TF Evaluate the following answer based on Accuracy: Question: {question} Ground Truth: {ground truth} Model Prediction: {predicted answer} Instructions: Match the meaning of the ground truth with the model prediction. If it matches, give score of 10. Otherwise, give score of 0. Strictly return only the numeric score, without any additional commentary. Figure 21. Evaluation prompt for multiple-choice (MCQ) and true/false (TF) questions."
        }
    ],
    "affiliations": [
        "University of Central Florida"
    ]
}