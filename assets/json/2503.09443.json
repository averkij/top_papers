{
    "paper_title": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models",
    "authors": [
        "Julian Spravil",
        "Sebastian Houben",
        "Sven Behnke"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy)."
        },
        {
            "title": "Start",
            "content": "Florenz: Scaling Laws for Systematic Generalization in Vision-Language Models Julian Spravil 1,* Sebastian Houben 2,1 Sven Behnke 3,4,1 1Fraunhofer IAIS, Germany 2University of Applied Sciences Bonn-Rhein-Sieg, Germany 3University of Bonn, Computer Science Institute VI, Center for Robotics, Germany 4Lamarr Institute for Machine Learning and Artificial Intelligence, Germany 5 2 0 2 2 1 ] . [ 1 3 4 4 9 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in specific language even when only data for the translation task is available. Fine-tuning on mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy). 1. Introduction Recent advances in vision-language models (VLMs) show impressive results across various tasks, primarily in English [4, 42, 58, 69]. Such diverse tasks and datasets are currently not available for multilingual approaches, hence limiting progress of general accessibility. Current meth- *Corresponding Author. Email: julian.spravil @ iais.fraunhofer.de Figure 1. Florenz (middle) is vision-language model trained on an incomplete dataset (left) that covers the tasks image captioning (blue) and multimodal machine translation (orange). The incompleteness concerns the availability of languages for the individual tasks, where captioning data is only available for English and German while EnX translation is available in all languages. Florenz generalizes to the missing captioning-language pairs during inference with sufficient scale (right). ods to mitigate this issue leverage pre-trained multilingual large language models (LLMs) [15, 43] or machine translation models [13] by integrating vision encoder and fine-tuning on multimodal, not necessarily multilingual, datasets [24, 25]. Adapting multilingual LLMs enables zero-shot cross-lingual transfer if the target language was part of pre-training, thus requiring only task-specific finetuning in one language [68] or machine translated data [52]. limitation of these approaches is the high cost of multilingual pre-training [15, 43], while data scarcity persists for low-resource languages. Alternatively, pre-trained monolingual LLMs [29] or contrastive models [7] can be adapted to new languages, for example, with web-crawled data [26]. There is evidence that even monolingual models have 1 language-agnostic component [14], similar to multilingual models [41]. Additionally, monolingual methods have superior performance due to the curse of multilinguality [12]. Cross-lingual transfer often assumes sufficient data for all tasks, sometimes using an LLM, pre-trained in another language, and transferring it to English to fulfill this assumption [29]. Multimodal multilingual approaches face lexical ambiguities (e.g., distinguishing between bat as an animal or sports equipment) and struggle to resolve them effectively [24]. The issue is amplified by machine translating short texts [52], without providing context. Addressing these issues would require systematic generalization the ability to combine what has been learned to create new capabilities though it remains controversial whether neural networks exhibit this ability [23, 39]. Recent evidence by Lake and Baroni [39] suggests that with meta learning strategy, systematic generalization with neural networks is feasible. The behavior of systematic generalization in natural language, particularly at scale, remains largely unexplored, despite its implications for the necessity of collecting data for each task in every language. To this end, we explore the scaling laws of systematic generalization within realistic multimodal setting using standard transformers [63] and standard training methods. Our goal is to learn language within one task and effectively transfer the language capabilities to different task in zero-shot manner starting with monolingual model, as illustrated in Fig. 1. Our contributions can be summarized as follows: We introduce standard encoder-decoder transformer leveraging pre-trained VLMs and LLMs in English, available in sizes from 0.4B to 11.2B parameters: Florenz (German name of Florence), based on Florence-2 [69]. We propose pipeline to generate dataset of visionaligned bitexts from translation dataset by employing contrastive models and context-enhanced translated captions using an off-the-shelf machine translation model. We investigate the scaling of model performance on different test sets, covering seen and unseen task-language data, model parameters, and number of seen training samples. We find that generalization by only learning translation to facilitate captioning mostly depends on model scale rather than on number of seen samples. We fine-tune our models on mix of downstream tasks and find competitive performance and promising scaling trends for the benchmarks Multi30K, CoMMuTE, COCO Karpathy, and XM3600. current studies only focus on what is explicitly and not on what is implicitly learned. Machine translation and cross-lingual transfer. The transformer [63] has revolutionized machine translation, with impressive results through pre-training on extensive translation data [13], non-parallel multilingual data [15, 43], and with weak supervision [12]. NLLB [13] exemplifies this success, supporting 200 languages. However, not all translation directions are well-resourced. This issue is addressed using language pivots [67], generating pseudo labels for fine-tuning [22], leveraging similar languages [34] and fine-tuning on parallel data [34]. Multimodal LLMs [15, 43] or large machine translation models [13], exhibit strong zero-shot cross-lingual transfer performance [49], even when fine-tuned with monolingual data [68], while languages with non-overlapping vocabularies result in weaker transfer [49]. Studies show that language-neutral and language-specific components develop and facilitate transfer [14, 41]. Multimodal multilingual learning and lexical ambiguity. Resolving lexical ambiguity, where word has multiple meanings, requires additional context [24]. Contrastive models [7, 54] excel at resolving ambiguities [25], whereas multilingual generative models struggle [24, 25]. Generative models depend on pre-trained multilingual models adapted with machine translated data [25], small multimodal multilingual datasets [24, 32, 45], text-only translation data [32, 45], and image captioning as auxiliary task [45]. Adapting monolingual models can also be effective, although the size of the LLM pre-training corpus can impact their final performance, while language similarity has little effect [29]. Training the English-only Gemma2 [61] on large web-crawled multilingual dataset unlocks few-shot capabilities [26]. Some multimodal multilingual tasks can be resolved by text-only models [32] since the number of vision-dependent samples is low [24]. Similar to our work, Muennighoff et al. [46] demonstrated cross-lingual cross-task transfer by fine-tuning multilingual LLMs. In contrast, we investigate the scaling laws of learning multimodal, multilingual, and task-specific knowledge to improve performance in the absence of explicit task-language data by fine-tuning monolingual models. Specifically, we learn translation as an auxiliary task for image captioning in language without training data. 3. Preliminary Analysis 2. Related Work Scaling laws. Scaling laws, first described for computer vision [59] and later for natural language processing [21, 28, 33, 35], the intersections of different fields [11], and even transfer [30] follow power laws and are extremely useful for predictable and efficient training at larger scales. However, We explore whether text-only machine translation models can resolve ambiguities with the help of additional textual context provided by VLM. We use CoMMuTE [24, 25], designed for binary disambiguation, featuring 155 ambiguous English sentences, each with two images and translations in German, French, Russian, and Chinese. The textual 2 Figure 2. Dataset generation pipeline. Input is an image dataset with short captions or alt texts and translation dataset with bitexts in English and the target language German. (1) Image and short caption are fed into VLM to generate detailed English description, which is (2) translated into the target language. (3) The image and (4) all English sentences of the translation dataset are embedded in shared vector space. (5) Cosine similarity is calculated and (6) top-N matching pairs the most similar images and translations. context for each image is detailed image description generated by Llama3-LLaVA-NeXT-8B1, vision adaptation of Llama 3 [18, 42]. The context is truncated to different lengths and is added after the caption, creating the input for NLLB [13]. Based on perplexity, measure of how well probability distribution predicts given sequence, the correct or incorrect translation with respect to the image is selected to calculate the accuracy. With this training-free approach, lexical ambiguity can be resolved to some extent, as reported in Tab. 1. The best accuracy of 64.0% is achieved by the larger NLLB-3.3B with an image description context length of about 64. This already exceeds the state of the art [26] with 63.7% for German CoMMuTE. 4. Multimodal Multilingual Dataset Creation For our investigation we propose pipeline, illustrated in Fig. 2, for creating multilingual multimodal corpus based on an image dataset and translation dataset with parallel sentences, also known as bitexts. We leverage machine translation models and contrastive models to improve semantic distinctiveness. Our method addresses ambiguity in two ways: Improving caption translations by generating more synthetic textual context and linking parallel sentences with image context using contrastive models. Short image captions [9, 19] cannot provide enough context to resolve semantic ambiguities when translated. Methods that translate short image captions directly [52, 62] may 1https://hf.co/lmms-lab/llama3-llava-next-8b Model NLLB-600M [13] NLLB-3.3B [13] 0 50.0 50.0 Context Length 64 54.7 64.0 128 55.3 63.3 32 55.3 60.7 256 54.0 63. learn incorrect relations. Therefore, we first generate detailed image descriptions using VLM, as they provide extra information and improve training [8, 42]. We translate the descriptions using standard machine translation model, as they likely already provide enough context (see Tab. 1). To avoid information loss, we remove translations that have different number of sentences than their source. More sophisticated filters such as using token id statistics or BLEU between source and translation could potentially be applied as enhancements [52]. The descriptions are further processed by extracting the first sentence as shortform caption and combining the English description with the respective translation as synthetic translation sample. For translating original short-form captions, we use contextenhanced machine translation. We append detailed generated description with special characters as separation tokens to the short-from caption before translating. Next, we check for those characters in the translation. If they are missing, as is true in the majority of cases, we apply the machine translation models without context. This process slightly enhances context awareness at the cost of translation accuracy. Fine-tuning the translation model with context prompt or leveraging LLMs as demonstrated by Wang et al. [65] could potentially lower the high reject rate. For our auxiliary translation task, we enrich the image annotations with text-only machine translation dataset. Contrastive models demonstrate impressive retrieval performance [54]. We encode the images and the English source sentence and calculate the cosine similarity between the resulting embeddings. We pair bitexts with images with top-N matching, followed by deduplication. Faiss [17] can be employed to accelerate the matching process. To create more sophisticated translation data, we randomly combine bitexts to create multi-sentence translation pairs. Table 1. Accuracy (%) of binary disambiguation on the CoMMuTE EnDe subset [24] using the context-enhanced machine translation model NLLB [13] for various context lengths. Note that longer context and larger model result in better accuracy. 4.1. Dataset We apply our dataset creation strategy to CC12M [6] and CCMatrix [57] for the languages English, German, French, Spanish, Russian, and Chinese. CC12M contains 12M web-crawled images with alt texts, of which 10M are available at the time of publication. Rather than generating our own image descriptions, we use dataset from Hugging Face2 that features descriptions created with Llama3-LLaVA-NeXT-8B1 [18, 42]. These descriptions are then translated with NLLB-3.3B [13]. CCMatrix is web-crawled corpus covering 38 languages with 6.8B parallel sentences of which about 661M are aligned with English. We extract translations from English to the aforementioned target languages. We use CLIP-ViT-B/16 [54] to align the bitexts with the images of CC12M. For our scaling law study, we intentionally do not create captioning data for certain languages (French, Spanish, Russian, and Chinese). In total, the training dataset contains 10M images with 32M captions of different detail levels in English and German and 105M translation pairs for English-German, English-French, EnglishSpanish, English-Chinese, and English-Russian. Note that German covers both tasks, thus serving as reference point. For the test set, we use subset of 4.4K images from CC12M and create captioning data for two representative languages (Spanish and Chinese) for the unseen tasklanguage pairs. We divide the test set into three parts: unseen captioning (UC) with 4.4K Spanish and 3.5K Chinese captions, seen translation (ST) with 4.1K English-Spanish and 3.8K English-Chinese translations, and seen captioning (SC) with 4.4K English and 3.1K German captions. 4.2. Fine-tuning Dataset We construct fine-tuning dataset that includes mix of downstream tasks, has full language coverage and contains no machine generated and less machine translated data. The dataset contains the train split of Multi30K [19] for Task 1 translation, covering English-French and EnglishGerman translations, and Task 2 captioning, covering English and German. For more detailed captions, we include Image Paragraph [37] and DOCCI [47]. Missing languages are added to the aforementioned datasets with our contextenhanced machine translation. Additionally, we include the train/restval split of COCO Karpathy [9, 36] for English captioning. In total, the dataset has 166K images with 1.6M captions and 145K translation samples covering all possible task-language combinations. 5. Florenz Model Family We construct our own partially pre-trained models based on Florence-2 [69] and Gemma-2 [61]. Florence-2 is an encoder-decoder VLM for English supporting tasks ranging from object detection to image captioning and is available 2https : / / hf . co / datasets / CaptionEmporium / conceptual-captions-cc12m-llavanext with 0.2B and 0.8B parameters. The encoder is used to process the image embeddings and task prompt which is then passed to the decoder. To obtain larger model sizes, we combine Gemma-2, an English LLM with sizes of 3B and 9B parameters, with the encoder of Florence-2. The encoder outputs are integrated into the decoder by inserted cross-attention layers that are weighted with learnable parameter initialized with zero following Flamingo [1]. For the decoder, we use the Sentencepiece tokenizer [38] trained for the multilingual model Gemini [60] and also used for the English-only Gemma-2 [61]. The encoder retains the original tokenizer and embeddings. The Gemma-2 tokenizer has vocabulary size of 256k, which is significantly larger than the original vocabulary size of Florence-2 with 51k, thus we need to reinitialize the embedding layer and head of the smaller two models not leveraging Gemma-2 as decoder. This process involves mapping new tokens to sets of old tokens, updating the embedding layer and head by reordering and averaging weights similar to the method proposed by Gee et al. [27]. The simple mapping is based on processing the new tokenizer sub-words with the old one, assigning each new token to set of old ones. This leads to Florenz, standard transformer encoder-decoder VLM designed for continuous pretraining, available in sizes 0.4B, 1B, 3.5B, and 11.2B, which we refer to as F-0.4B, F-1B, F-3.5B, and F-11.2B. 6. Scaling Laws We explore the scaling laws of continuous pre-training in multilingual multi-task scenario, where not all tasklanguage combinations are given within the training data. Training. We train all our models with batch size of 1024, AdamW [44] and weight decay of 0.01. The learning rate is set to 1e4 and is scheduled with linear warm-up for 100 steps followed by cosine decay. The input length for both encoder and decoder is truncated to maximum length of 128 and the image resolution is set to 224 px. We train each model for 500, 2k, 5k and 10k iterations on our dataset. Note that 10k steps leads to 10M seen samples roughly equaling the number of images in the dataset. To address the imbalance in our dataset (see Sec. 4.2), we implement an online balancing strategy that adjusts sampling probabilities to ensure the model receives an equal number of samples from each task-language combination. The weights of the vision encoder are frozen. For F-3.5B and F-11.2B we further freeze the decoder layers but not the inserted cross-attention layers. F-11.2B is trained with fully sharded data parallel (FSDP) [70]. Our implementation is based on the transformers framework [66]. Evaluation. We calculate the cross-entropy loss (CE) over the three test sets described in Sec. 4.1 covering the settings: 4 Figure 3. Test cross-entropy loss (CE) for various training compute budgets (GMACs, Giga Multiply-Accumulate operations), showing the effects of different model sizes and seen training samples. We show results for the test splits (see Sec. 4.1) for unseen captioning (UC) in Spanish (Es) and Chinese (Zh), seen translation (ST) in these languages, and seen captioning (SC) in English (En) and German (De). The models F-0.4B, F-1.0B, F-3.5B, and F-11.2B are trained for 500, 2k, 5k, and 10k steps, respectively, resulting in 0.5M to 10M seen samples with batch size of 1024. Eq. (1) is fitted to the points on the Pareto frontier (gray staircase graph). Higher compute budgets improve CE for UC (left), ST (middle), and SC (right). This suggests that translation facilitates generalization in captioning. unseen captioning in Spanish and Chinese (UC), seen translation in Spanish and Chinese (ST), and seen captioning in English and German (SC). Scaling laws. The relationship between cross-entropy loss and training compute can be described by power law [11, 21, 28, 33, 35], where changes in model complexity or data size result in predictable, non-linear improvements in loss. We study two power laws. The first law follows related work [11] to select the Pareto frontier from all data points considering both the CE y, training compute C, and the error term ϵ: = α0C α1 + ϵ, (1) where α0 and α1 are the parameters to be estimated. The total computational cost in multiplyaccumulate operations (MACs) is estimated by: (cid:18) = 1 + (cid:19) ,"
        },
        {
            "title": "Pt\nP",
            "content": "(2) where is the number of training steps, is the forward pass MACs estimated with the fvcore library3, Pt is the number of trainable parameters, and is the total number of parameters of the model. The second power law is multivariate, modeling the influence of the number of training steps and the number of model parameters on the dependent variable CE y: = β0P β1 Sβ2 + ϵ. (3) 3https://github.com/facebookresearch/fvcore Here β0, β1, and β2 are to be estimated. We linearize Eq. (1) and Eq. (3) by transforming them into log10 space and perform linear regression analysis using ordinary least squares (OLS). 6.1. Results The CE of the runs with compute budget and the fit of the model described by Eq. (1) is visualized in Fig. 3, and the corresponding parameters are detailed in Tab. 2. Power law of the Pareto frontier in unseen tasklanguage pairs. We found negative exponents α0 that indicate that the CE on the Pareto frontier decreases as compute increases. The rate of decrease α1 is similar for both UC and SC test sets. The estimates of α0 for UC and ST imply that their curves are shifted upwards relative to SC, while the CE for ST decreases more rapidly than for SC. However, the interpretation of α0 and α1 of UC requires caution due to overlapping confidence intervals. This provides evidence that models can generalize to tasks in specific language Task UC ST SC Parameter α0 α1 α0 α1 α0 α1 Estimate [95% CI] 1.86 [0.92, 2.80] -0.15 [-0.26, -0.04] 2.02 [1.75, 2.23] -0.18 [-0.21, -0.15] 1.42 [1.29, 1.56] -0.15 [-0.16, -0.13] p-value < 0.01 < 0.05 < 0.001 < 0.001 < 0.001 < 0.001 Table 2. Parameters α0/1 for UC, ST, and SC test sets in log10 space, with 95% confidence intervals and p-values, as per Eq. (1). 5 Figure 5. Effect of adding prefix (Fr: La photo montre, Es: La imagen muestra, etc.) to the decoder input to unlock zero-shot captioning. Tested on the image captioning dataset XM3600 [62] in the unseen languages Fr, Es, Ru, and Zh. The mean CIDEr [64] over unseen languages significantly improves with the prefix. ter on zero-shot tasks, although it is still debated whether dataset contamination is responsible [53]. Our findings suggest that systematic generalization emerges in larger models and is likely not caused by data contamination. Decoder prefix as key to generalization. While the CE for UC is decreasing, all models struggle to generate text in the target language. Instead, they produce text in German or English, the two languages encountered during training for captioning. We found that adding small prefix to the decoder seeds the output of the model. The effect is visualized in Fig. 5 and shows, for the first time, the generation of captions without prior exposure to captioning data in those languages. Qualitative examples can be found in the supplementary material. Predicting test cross-entropy from model size. We extrapolate Eq. (3) to estimate the CE values for target model size with fixed amount of 10M seen training samples. We predict that 30B parameter model could achieve CE of 2.31 with 95% prediction interval (PI) [1.93, 2.76], 1.35 with 95% PI [1.06, 1.71], and 0.74 with 95% PI [0.67, 0.81] on UC, ST, and SC, respectively. Limitations. First, our sample size (16) is relatively small, which limits predictive power. The power laws are limited to the specific setting we evaluated. There are certain factors that likely influence the parameters: The number of languages that the model has to learn, if the language has captioning data or was part of the pre-training, the extensiveness of the pre-training, the synthetic nature of our datasets, the difficulty of tasks, and the effect of multiple tasks. 7. Supervised Fine-tuning We fine-tune on mix of downstream tasks (see Sec. 4.2) to enhance translation and captioning in all languages, assessing if scaling laws transfer. We fine-tune the models pre-trained for 10K steps for 5K steps with resolution of 768 px, batch size of 256, and learning rate of 5e5. Evaluation. captioning on evaluate COCO Karpathy [9, 36], Multi30K (Task 2) [19], and translation on XM3600 [62], multimodal machine image We Figure 4. Predicted test cross-entropy loss (CE) as function of model parameters in billion (B) with confidence intervals (CI) and prediction intervals (PI) for unseen captioning in Spanish and Chinese (UC, blue), seen translation in Spanish and Chinese (ST, orange), and seen captioning in English and German (SC, green). The number of seen training samples is fixed to 10M, respective measurements are shown as dots. Extrapolation is drawn dashed. while seeing the language only in other tasks, in this case translation. The second power law Eq. (3) is applied to all data points in order to better capture the influence of the number of observed training samples. The found parameters, as well as the extrapolation for the UC, ST and SC test sets, are shown in Fig. 4, while the specific parameter estimates are detailed in Tab. 3. Influence of model size on generalization. The influence of model parameters (β1) and training samples (β2) is balanced for both ST and SC. slightly greater dependence on training samples is observable. This suggests that larger models and more training steps reduce CE with full tasklanguage coverage during training. For UC, the exponent for model parameters β1 is smaller than the exponent of training samples β2 (0.16 < 0.04). This indicates that the number of model parameters has greater influence on generalization than the quantity of training samples. Larger models generally perform betTask UC ST SC Parameter β0 β1 β2 β0 β1 β2 β0 β1 β2 Estimate [95% CI] 2.30 [1.95, 2.66] -0.16 [-0.19, -0.13] -0.04 [-0.08, -0.01] 3.03 [2.55, 3.50] -0.15 [-0.19, -0.11] -0.19 [-0.24, -0.15] 2.20 [2.02, 2.38] -0.13 [-0.14, -0.11] -0.15 [-0.16, -0.13] p-value < 0.001 < 0.001 < 0.05 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 Table 3. Parameters of Eq. (3) for UC, ST, and SC in log10 space. 6 Figure 6. Scaling laws for fine-tuned models on different downstream tasks. First row: Multi30K translation to De and Fr measured in BLEU (Task 1; mean over Test2016, Test2017 and AmbiguousCOCO splits), CoMMuTE translation and disambiguation for EnDe and En{De, Fr, Ru, Zh} measured in BLEU and accuracy, respectively. Second row: Captioning tasks measured with CIDEr: COCO Karpathy (En), Multi30K (En, De) (Task 2, Test2016), and XM3600 for En, De and unseen languages (Fr, Es, Ru, Zh). UC and ST exhibit stronger scaling laws than tasks for languages already known to the pre-trained LLM (En) or those with complete task coverage (De). Multi30K (Task 1) [3, 19, 20] and CoMMuTE [24, 25], and lexical ambiguity on CoMMuTE. COCO Karpathy has 5K images with five English captions per image. The test set of Multi30K (Task 2) features 1K images with five English and five German captions each. Both are evaluated with CIDEr [64] using the pycocoevalcap toolkit4. XM3600 has 3.6K images with captions in 36 languages also evaluated with CIDEr but with an additional segmentation step for languages without word boundaries with stanza [51] following Futeral et al. [26]. For zero-shot image captioning, we use CLIPScore (CS) [31] with CLIP-ViT-B/16 [54] and multilingual CLIPScore (MS) with M-CLIP-XLM-R-L-ViT-B/16+ [5]. For translation, we have Multi30K (Task 1) with the Test2016 [19], Test2017 [20] and AmbiguousCOCO [3] splits with 1K images with bitexts each, evaluated using SacreBLEU [50] to calculate BLEU [48]. Translation and disambiguation are checked using BLEU and accuracy on CoMMuTE with 155 ambiguous English sentences, two translations, and two images each. 7.1. Results The downstream task performance is reported in Tab. 4. More detailed results can be found in the supplementary material. Scaling law fits adapted from Eq. (1) are visualized in Fig. 6. We primarily compare to our baseline, com4https://github.com/salaniz/pycocoevalcap bination of BLIP-2 [40] fine-tuned on COCO Karpathy and NLLB-3.3B [13] with context enhancement (see Sec. 3), referred to as Baseline-6B. For captioning we include Multilingual Open Flamingo (MOF) [26] and PaliGemma-3B [4], both exposed to large-scale multilingual multimodal pretraining. For translation, we use NLLB-3.3B as textonly baseline, ZeroMMT-3.3B [25] as multimodal extension of NLLB, and multilingual VGAMT [24] based on mBART [43]. The best downstream captioning performance of Florenz for unseen captioning (UC) on XM3600 is competitive and even surpasses Baseline-6B (46.6>44.6). PaliGemma3B has better captioning performance for UC on XM3600 (48.5) while being pre-trained on about 100 more multilingual multimodal samples. The scaling trend (see Fig. 6) indicates that larger models could close the gap. Our pre-training scaling laws suggesting that the number of seen samples has secondary role for systematic generalization also hold after fine-tuning. Extended pre-training boosts seen translation (ST) and captioning (SC) sometimes surpassing larger scales but UC performance on XM3600 improves only slightly; however, the drop in zeroshot performance disappears. The BLEU scores for CoMMuTE translation slightly decrease with fine-tuning because the dataset includes noncaption-like translations. Despite ambiguity, the text-only NLLB-3.3B performs best, while context-enhanced translation in Baseline-6B has only marginal impact. While the accuracy in resolving lexical ambiguity on CoMMuTE does not increase for the pre-trained models Multi30K Translation CoMMuTE Translation EnDe EnFr EnDe EnX EnDe EnX CoMMuTE Ambiguity COCO Multi30K Caption Caption En En De En XM3600 Caption De Unseen Model VGAMT [24] Florence-2-large [69] PaliGemma-3B* [4] Baseline-6B* [13, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) Model MOF [26] ZeroMMT-3.3B [25] NLLB-3.3B* [13] F-0.4B (ours) F-0.4B 100K (ours) F-1.0B (ours) F-3.5B (ours) F-11.2B (ours) F-11.2B 30K (ours) BLEU BLEU BLEU BLEU Acc. 57.1 37.4 58.4 Acc. CIDEr CIDEr CIDEr CIDEr CIDEr CIDEr SUPERVISED 37.3 37.7 39.9 38.4 39.2 40.7 41. 54.3 53.7 57.2 56.6 56.9 59.1 59.6 41.5 26.5 32.8 29.0 32.0 36.8 38.5 32.5 20.8 26.6 22.8 26.0 29.6 31.2 61.7 59.0 59.7 59.7 61.3 62.3 62.0 61.1 56.6 59.8 58.8 62.6 63.9 63.4 ZERO-SHOT BLEU BLEU BLEU BLEU Acc. 63.7 24.9 60.8 37.1 50.0 37.4 54.0 34.1 54.0 36.1 54.7 35.3 53.0 35.8 52.7 36.6 37.6 52.0 35.1 53.3 53.7 44.3 51.0 47.4 48.3 50.9 52.3 31.9 25.9 29.5 27.1 28.7 29.8 30.4 40.8 34.1 37.2 35.4 36.7 39.5 39.4 Acc. 66.5 62.2 50.0 53.6 54.2 54.1 54.3 53.5 53.2 143.3 141.7 145.2 138.2 140.3 142.8 141.6 141.3 140. 88.9 84.0 76.6 79.5 85.1 81.8 83.1 83.4 57.6 50.4 77.4 79.2 82.6 84.5 90.7 89.4 79.1 82.0 76.9 78.3 78.0 77.7 79.3 78.1 37.7 38.2 38.6 39.3 41.2 38.2 39.4 41.8 48.5 45.1 40.0 42.0 44.7 43.7 46.3 46.9 CS CS MCS CS MCS PC 77.7 77.9 79.9 78.5 78.9 79. 77.9 78.2 80.2 78.7 79.0 79.2 82.0 83.3 86.5 83.7 84.7 85.1 74.8 75.4 77.0 75.9 76.3 76.6 77.1 78.3 81.4 79.5 80.1 80.5 1.7 1.8 5.6 24.1 26.6 17.7 Table 4. Downstream task performance for supervised fine-tuning and zero-shot testing. We evaluate on Multi30K Task 1 (translation, reported is the mean over the splits Test2016, Test2017, and AmbiguousCOCO), CoMMuTE translation and disambiguation (EnX is the mean over De, Fr, Ru, and Zh), COCO Karpathy (En captioning), Multi30K Task 2 (captioning), and XM3600 (captioning, Unseen contains Fr, Es, Ru and Zh). We report BLEU for translation, accuracy (Acc.) for disambiguation, CIDEr for captioning, and CLIPScore (CS) and multilingual CLIPScore (MCS) for zero-shot captioning. For unseen languages in zero-shot captioning, we report CIDEr and evaluate with prefix (PC). Best results are marked in bold. Methods marked with * are re-evaluated by us. in zero-shot evaluation, we see that the accuracy increases after fine-tuning with respect to model scale but not training steps. MOF achieves 66.5% mean accuracy from pretraining on web-crawled data but shows reduced zero-shot Multi30K translation performance. Our approach achieves 63.9% while maintaining good results for Multi30K translation. This highlights the need for small high-quality dataset in each language addressing these ambiguities, while the pre-training dataset can be of lower quality. Our largest model achieves impressive performance for captioning tasks in German, the only language with full task coverage in pre-training. Unsurprisingly, as our scaling laws reveal, full task coverage leads to the best overall performance. English tasks such as COCO Karpathy seem to plateau, whereas Baseline-6B has the best results. Here, the model scale appears to negatively impact performance, as F-1.0B has the best results from Florenz, comparable to the performance of the original Florence-2. This may stem from the differing pre-training strategies of Florence-2 and Gemma2 and the possibility that learning new embeddings and head weights is easier than learning the vision-to-language projection. The step down in zero-shot captioning performance between F-1.0B and F-3.5B, along with the slight underperformance of F-1.0B in translation benchmarks (see Fig. 6), suggests that initial multimodal alignment on English data could be beneficial. 8. Conclusion We present scaling laws for systematic generalization from the multimodal machine translation task to the multilingual image captioning task via the model family Florenz. Florenz is able to address image captioning and multimodal machine translation in the languages English, German, French, Spanish, Chinese, and Russian, with the latter four only encountered in translation tasks. We show that model scale is the most important factor for systematic generalization. Our insights can help practitioners create more efficient multilingual datasets and address the limitations of generalization with respect to model scale and training parameters, that is, balancing between number 8 of parameters and dataset task coverage. Future work can investigate the interactions of more than two tasks and how this would affect the scaling exponents, potentially leading to better and more versatile multilingual models."
        },
        {
            "title": "Acknowledgments",
            "content": "This research has been funded by the Federal Ministry of Education and Research of Germany under grant no. 01IS23004B RIDMI and 01IS22094C WEST-AI. Computational resources were provided by the German AI Service Center WestAI."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, et al. Flamingo: visual In NeurIPS, language model for few-shot learning. 2022. 4 [2] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In ACL, pages 65 72, 2005. 12 [3] Loıc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. Findings of the third shared task on multimodal machine transIn 3rd Conference on Machine Translation lation. (WMT), pages 304323, 2018. 7, 14 [4] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, et al. PaliGemma: versatile 3B VLM for transfer. CoRR, abs/2407.07726, 2024. 1, 7, 8, 12, 14, 15, 16 [5] Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren. Cross-lingual and multilingual CLIP. In LREC, pages 68486854, 2022. 7, 12 [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pages 35583568, 2021. 3 [7] Guanhua Chen, Lu Hou, Yun Chen, Wenliang Dai, Lifeng Shang, Xin Jiang, Qun Liu, Jia Pan, and Wenping Wang. mCLIP: Multilingual CLIP via crosslingual transfer. In ACL, pages 1302813043, 2023. 1, [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. ShareGPT4V: Improving large multi-modal models with better captions. In ECCV, pages 370387, 2024. 3 [9] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO Captions: CoRR, Data collection and evaluation server. abs/1504.00325, 2015. 3, 4, 6, 12, 13, 16 [10] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, et al. PaLI: Jointly-Scaled Multilingual Language-Image Model. In ICLR, 2023. 12 [11] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, pages 28182829, 2023. 2, 5 [12] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised crosslingual representation learning at scale. In ACL, pages 84408451, 2020. 2 [13] Marta R. Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, et al. No Language Left Behind: Scaling humancentered machine translation. CoRR, abs/2207.04672, 2022. 1, 2, 3, 4, 7, 8, 12, 14, [14] Leandro Rodrigues de Souza, Thales Sales Almeida, Roberto A. Lotufo, and Rodrigo Frassetto Nogueira. Measuring cross-lingual transfer in bytes. In NAACL, pages 75267537, 2024. 2 [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 41714186, 2019. 1, 2 [16] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. DaViT: Dual Attention In ECCV, pages 7492, 2022. Vision Transformers. 12 [17] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The Faiss library. CoRR, abs/2401.08281, 2024. 3 [18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. The Llama 3 herd of models. CoRR, abs/2407.21783, 2024. 3, 4 [19] Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30K: Multilingual EnglishIn VL@ACL, 2016. 3, German image descriptions. 4, 6, 7, 13, 14, [20] Desmond Elliott, Stella Frank, Loıc Barrault, Fethi Bougares, and Lucia Specia. Findings of the second shared task on multimodal machine translation and multilingual image description. In 2nd Conference on Machine Translation (WMT), pages 215233, 2017. 7, 14 9 [21] Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. Scaling laws for In ICML, multilingual neural machine translation. pages 1005310071, 2023. 2, 5 [22] Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos T. Yarman-Vural, and Kyunghyun Cho. Zeroresource translation with multi-lingual neural machine translation. In EMNLP, pages 268277, 2016. 2 [23] Jerry Fodor and Zenon Pylyshyn. Connectionism and cognitive architecture: critical analysis. Cognition, 28(1-2):371, 1988. 2 [24] Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Benoˆıt Sagot, and Rachel Bawden. Tackling ambiguity with images: Improved multimodal machine translation and contrastive evaluation. In ACL, pages 5394 5413, 2023. 1, 2, 3, 7, 8, 14, [25] Matthieu Futeral, Cordelia Schmid, Benoˆıt Sagot, and Rachel Bawden. Towards zero-shot multimodal machine translation. CoRR, abs/2407.13579, 2024. 1, 2, 7, 8, 12, 14 [26] Matthieu Futeral, Armel Zebaze, Pedro Ortiz Suarez, Julien Abadji, Remi Lacroix, Cordelia Schmid, Rachel Bawden, and Benoˆıt Sagot. mOSCAR: large-scale multilingual and multimodal documentlevel corpus. CoRR, abs/2406.08707, 2024. 1, 2, 3, 7, 8, 12, 14, 15 [27] Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni. Fast vocabulary transfer for lanIn EMNLP, pages 409 guage model compression. 416, 2022. 4 [28] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. In ICLR, 2022. 2, 5 [29] Evangelia Gogoulou, Ariel Ekgren, Tim Isbister, and Magnus Sahlgren. Cross-lingual transfer of monolingual models. In LREC, pages 948955, 2022. 1, 2 [30] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. CoRR, abs/2102.01293, 2021. 2 [31] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: referencefree evaluation metric for image captioning. In EMNLP, pages 75147528, 2021. 7, [32] Tosho Hirasawa, Emanuele Bugliarello, Desmond Elliott, and Mamoru Komachi. Visual prediction improves zero-shot cross-modal machine translation. In 8th Conference on Machine Translation (WMT), pages 522535, 2023. 2 [33] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, et al. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. 2, 5 10 [34] Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viegas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Googles multilingual neural machine translation system: Enabling zero-shot translation. Trans. Assoc. Comput. Linguistics, pages 339351, 2017. 2 [35] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. 2, 5 [36] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. TPAMI, 39(4):664676, 2017. 4, 6, 13, [37] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. hierarchical approach for generating deIn CVPR, pages 3337 scriptive image paragraphs. 3345, 2017. 4, 13 [38] Taku Kudo and John Richardson. SentencePiece: simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP, pages 6671, 2018. 4 [39] Brenden M. Lake and Marco Baroni. Human-like systematic generalization through meta-learning neural network. Nature, 623(7985):115121, 2023. 2 [40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742, 2023. 7, 8, 12, 14, 15, 16 [41] Jindrich Libovicky, Rudolf Rosa, and Alexander Fraser. How language-neutral is multilingual BERT? CoRR, abs/1911.03310, 2019. 2 [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 3, 4 [43] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. TACL, 8:726742, 2020. 1, 2, [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2017. 4 [45] Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, and Lucia Specia. BERTGen: Multi-task In ACL/IJCNLP, pages generation through BERT. 64406455, 2021. 2 [46] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Crosslingual generalization through Scao, et al. In ACL, pages 1599116111, multitask finetuning. 2023. [59] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In ICCV, pages 843852, 2017. 2 [60] Gemini Team. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. 4 [61] Gemma Team. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. 2, 4, 12 [62] Ashish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively mulIn EMNLP, tilingual multimodal evaluation dataset. pages 715729, 2022. 3, 6, 12, 16 [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 59986008, 2017. 2 [64] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In CVPR, pages 45664575, 2015. 6, 7, 12 [65] Yusong Wang, Dongyuan Li, Jialun Shen, Yicheng Xu, Mingkun Xu, Kotaro Funakoshi, and Manabu Okumura. LAMBDA: Large language model-based data augmentation for multi-modal machine translation. In EMNLP, pages 1524015253, 2024. 3 [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, et al. Transformers: State-of-the-art natural language processing. In EMNLP, pages 3845, 2020. [67] Hua Wu and Haifeng Wang. Pivot language approach for phrase-based statistical machine translation. Mach. Transl., 21(3):165181, 2007. 2 [68] Shijie Wu and Mark Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In EMNLP-IJCNLP, pages 833844, 2019. 1, 2 [69] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, et al. Florence-2: Advancing unified representation for variety of vision tasks. In CVPR, pages 48184829, 2024. 1, 2, 4, 8, 12, 16 [70] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, et al. PyTorch FSDP: Experiences on scaling fully sharded data parallel. VLDB Endowment, 16(12):38483860, 2023. 4 [47] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. DOCCI: Descriptions of connected and contrasting images. In ECCV, pages 291309, 2024. 4, 13 [48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, pages 311318, 2002. 7, 12 [49] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT? In 57th Conference of the Association for Computational Linguistics (ACL), pages 49965001, 2019. [50] Matt Post. call for clarity in reporting BLEU scores. In 3rd Conference on Machine Translation (WMT), pages 186191, 2018. 7, 12 [51] Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. Stanza: Python natural language processing toolkit for many human languages. In ACL, pages 101108, 2020. 7, 12 [52] Chen Qiu, Dan Oneata, Emanuele Bugliarello, Stella Frank, and Desmond Elliott. Multilingual multimodal In EMNLP, learning with machine translated text. pages 41784193, 2022. 1, 2, 3 [53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1 (8):9, 2019. 6 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 2, 3, 4, 7, 12 [55] Ricardo Rei, Craig Stewart, Ana C. Farinha, and Alon Lavie. COMET: Neural Framework for MT Evaluation. In EMNLP, pages 26852702, 2020. 12 [56] Ricardo Rei, Jose G. C. de Souza, Duarte M. Alves, Chrysoula Zerva, Ana C. Farinha, Taisiya Glushkova, Alon Lavie, Luısa Coheur, and Andre F. T. Martins. COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task. In WMT, pages 578585, 2022. [57] Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In ACL/IJCNLP, pages 64906500, 2021. 3 [58] Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, et al. PaliGemma 2: family of versatile VLMs for transfer. CoRR, abs/2412.03555, 2024. 1, 12 11 Param. 0.4B Florenz (F) 1B Vision Encoder DaViT [16] 3.5B 11.2B # layers Hidden dim. # heads Patch size Patch stride [1, 1, 9, 1] [128, 256, 512, 1024] [4, 8, 16, 32] [7, 3, 3, 3] [4, 2, 2, 2] [1, 1, 9, 1] [256, 512, 1024, 2048] [8, 16, 32, 64] [7, 3, 3, 3] [4, 2, 2, 2] # layers Hidden dim. # heads Vocab. size # layers Hidden dim. # heads Vocab. size Encoder Florence-2 [69] 6 768 12 51328 Decoder Florence-2 [69] 6 768 12 12 1024 16 256000 12 1024 16 Gemma-2 [61] 26 2304 8 42 3584 16 Table 5. Model configuration of Florenz. A. Florenz Model Overview Florenz is standard encoder-decoder transformer composed from Florence-2 [69] series of small visionlanguage models (VLMs), and Gemma-2 [61], large language model. Both models are pre-trained primarily on English data. Florenz and its variants are detailed in Tab. 5. B. Evaluation We use In this section we describe the evaluation strategy for the individual tasks. COCO Karpathy evaluation. the pycocoevalcap toolkit5 to calculate BLEU-n [48], METEOR-1.5 [2], and CIDEr [64]. This is the regular procedure following related work [40], where CIDEr is considered the main metric. Moreover, we report the neural metric CLIPScore (CS) [31] with CLIP-ViT-B/16 [54] to measure alignment between generated captions and images in the CLIP embedding space. Multi30K translation evaluation For the translation task (task 1) of Multi30K we follow related work [25] and use BLEU [48] and COMET with the wmt22-comet-da6 model (C22) [55, 56]. BLEU is evaluated using SacreBLEU [50] with the 13a tokenizer and for Chinese the zh tokenizer. Multi30K captioning evaluation For the captioning task (task 2) of Multi30K, the same strategy as for COCO Karpathy is applied. For multilingual tasks we use the mul5https://github.com/salaniz/pycocoevalcap 6https://hf.co/Unbabel/wmt22-comet-da 12 Model Paper [4] Reprod. stanza [51] Baseline-6B [13, 40] En 78.0 78.4 78.7 81.9 De - 35.6 35.9 38.1 Fr - 63.6 63.2 64. Es - 68.0 67.9 63.9 Ru - 35.3 35.6 29.4 Zh - 28.4 28.2 21.5 Table 6. Comparison between the numbers reported in the PaliGemma paper [4], reproduction with pre-segmented model outputs, reproduction with stanza [51] for word segmentation, and the Baseline-6B model for reference. Note that the outputs of PaliGemma-3B are pre-segmented, with words separated by whitespace. For the stanza evaluation, we first remove the whitespace before processing. tilingual CLIPScore (MS) with M-CLIP-XLM-R-L-ViTB/16+ [5]. XM3600 evaluation The main metric for XM3600 [62] is CIDEr [64]. However, the pre-processing steps are not well documented for languages without word boundaries such as Chinese. Chen et al. [10] uses neural model to segment languages without word boundaries. However, the exact model is not specified, thus making reproduction difficult. Furthermore, they fine-tune their models on the pre-tokenized dataset COCO35L [62], COCO captions [9] translated into 35 target languages. PaliGemma [4, 58] follows similar setup and is publicly available. These models produce segmented lower-case text, not requiring additional segmentation for evaluation. Our model produces outputs with correct capitalization and the correct use of white space. Futeral et al. [26] proposes stanza [51] as an alternative for the unknown word segmentation network. We apply the model both to predictions and to references and find that stanza [51] is good alternative, as can be seen in Table 6. If this approximation holds for other languages without word boundaries than Chinese is left for future work. C. Datasets Fig. 7 shows the task and language coverage for our pretraining dataset. The coverage for our fine-tuning dataset is illustrated in Fig. 8. An overview of the dataset mix is shown in Tab. 7. We apply online task-language balancing, because of the high imbalance of captioning and translation data. D. Detailed Results We provide detailed results for the benchmarks: Multi30K (task 1) translation in Tab. 8, CoMMuTE translation in Tab. 9, CoMMuTE disambiguation in Tab. 10, COCO Karpathy image captioning in Tab. 11, Multi30K (task 2) image captioning in Tab. 12, and XM3600 image captioning in Tab. 13. Type Caption Translation EnX Dataset COCO Karpathy Multi30k Task 2 Image Paragraphs DOCCI Multi30k Task 2 Image Paragraphs DOCCI Multi30k Task 1 Multi30k Task 1 Source GT GT GT GT MT MT MT GT MT Languages En En, De En En Fr, Es, Ru, Zh De, Fr, Es, Ru, Zh De, Fr, Es, Ru, Zh De, Fr Es, Ru, Zh Train (Images / Texts) 113,287 / 566,435 29,000 / 290,000 14,579 / 14,579 9,647 / 9,647 * / 580,000 * / 72,895 * / 48,235 29,000 / 58,000 * / 87,000 Table 7. Fine-tuning dataset for captioning and translation task based on the COCO Karpathy [9, 36], Multi30k [19], Image Paragraphs [37] and DOCCI [47]. The annotation is the regular ground truth (GT) or machine translated (MT). E. Qualitative Results We provide example outputs for our best performing pretrained model F-11.2B and our best performing fine-tuned model F-11.2B 30K ft. Fig. 9 and Fig. 10 show the outputs of the pre-trained model with examples for captioning in languages, where no captioning data was encountered. Fig. 11 and Fig. 12 show detailed captions. Figure 7. Distribution of caption and translation data with language coverage in our pre-training dataset. Figure 8. Distribution of caption and translation data with language coverage in our fine-tuning dataset. 13 Multi30K Translation EnFr Test2016 Test2017 Test2018 COCO I P Model VGAMT [24] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100k ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) NLLB-600M [13] NLLB-1.3B [13] NLLB-3.3B [13] MOF [26] ZeroMMT-600M [25] ZeroMMT-1.3B [25] ZeroMMT-3.3B [25] F-0.4B (ours) F-0.4B 100K (ours) F-1.0B (ours) F-3.5B (ours) F-11.2B (ours) F-11.2B 30K (ours) S - Z I P VGAMT [24] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) NLLB-600M [13] NLLB-1.3B [13] NLLB-3.3B [13] MOF [26] ZeroMMT-600M [25] ZeroMMT-1.3B [25] ZeroMMT-3.3B [25] F-0.4B (ours) F-0.4B 100K (ours) F-1.0B (ours) F-3.5B (ours) F-11.2B (ours) F-11.2B 30K (ours) S - Z 65.0 54.8 57.5 60.4 59.9 59.9 63.2 64.4 48.3 51.8 54.2 36.0 48.6 51.5 52.9 42.1 49.0 45.3 46.5 50.6 51.3 41.9 39.8 42.8 44.0 43.2 43.1 44.7 44.9 37.2 38.1 39.7 28.9 36.2 37.6 39.6 37.2 39.5 38.8 38.2 39.3 39.5 58.9 53.8 53.5 57.5 56.9 57.5 60.3 60.5 48.5 50.9 52.8 35.1 48.1 51.1 53.3 43.4 50.1 46.3 48.0 50.2 51.4 C22 88.2 87.6 85.9 87.9 87.3 88.4 89.1 89.0 85.9 86.9 87.5 83.7 85.7 87.0 87.5 82.1 85.9 84.4 85.8 86.7 87.0 C22 88.7 87.6 85.9 87.7 87.5 88.5 89.2 89.5 85.0 86.6 87.6 83.6 84.9 86.4 87.2 81.6 85.0 83.6 85.1 86.0 86.1 Multi30K Translation EnDe 85.8 86.2 84.5 86.3 85.9 86.6 87.3 87.6 83.5 85.0 86.1 82.3 83.0 84.0 85.9 83.1 85.0 84.3 84.7 85.7 85.8 84.7 85.5 84.0 86.1 85.3 85.9 87.0 87.0 83.0 84.3 85.2 80.9 82.5 84.6 85.5 83.6 84.7 84.3 84.6 85.4 85. 36.7 38.3 37.8 40.9 38.7 39.8 41.6 41.6 33.0 36.7 37.9 23.9 33.2 36.2 37.9 35.9 37.1 36.3 37.2 37.7 38.3 45.7 39.6 43.0 42.4 42.2 43.9 44.6 40.5 43.0 45.4 36.2 42.2 39.1 40.3 42.2 43.4 35.6 34.6 36.5 35.7 36.8 37.7 38.8 32.2 32.8 35.8 32.9 34.7 34.6 34.5 34.6 36. C22 84.8 82.3 84.0 83.5 84.2 84.9 85.1 83.0 84.1 84.8 79.6 83.1 82.0 83.1 83.6 83.8 84.3 82.4 84.1 83.7 84.4 85.4 85.6 81.9 83.2 84.2 81.9 83.1 83.0 83.4 83.9 84.1 51.2 54.4 50.0 53.8 52.9 53.5 53.9 53.8 50.0 52.8 54.1 34.1 50.3 53.6 53.9 47.6 53.9 50.7 50.5 52.0 54. 33.5 33.9 32.4 34.8 33.3 34.7 35.7 38.5 28.3 31.5 34.5 21.9 29.0 31.7 33.7 29.3 31.8 30.6 31.8 32.7 35.0 C22 84.8 85.6 82.4 84.7 84.2 85.2 85.8 86.1 84.1 85.0 85.6 80.7 83.8 85.0 85.4 80.4 83.7 82.0 83.1 84.4 84.9 81.1 82.1 79.6 82.1 80.8 82.0 83.6 83.9 78.6 80.4 82.0 76.6 77.7 80.7 81.9 78.4 80.5 79.1 80.8 81.1 81.6 Table 8. Translation (Task 1) on the Multi30k [3, 19, 20] dataset reporting BLEU (B) and COMET (C22) metrics. 100K, 30K, and ft indicate the number of pre-training steps and fine-tuning. 14 De Fr Ru Zh Model NLLB-600M [13] NLLB-1B [13] NLLB-3.3B [13] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) F-0.4B (ours) F-0.4B 100K (ours) F-1.0B (ours) F-3.5B (ours) F-11.2B (ours) F-11.2B 30K (ours) 36.4 40.5 40.8 41.5 26.5 32.8 29.0 32.0 36.8 38.5 34.1 37.2 35.4 36.7 39.5 39.4 C22 80.6 81.3 81.3 81.6 71.6 78.3 74.3 79.3 81.5 82.4 77.5 80.4 78.8 80.0 80.7 80. 38.9 39.2 41.4 42.1 27.8 37.6 30.5 35.5 40.7 42.6 31.0 37.4 32.4 36.8 37.5 35.8 C22 80.2 80.4 81.4 81.5 72.9 78.6 75.4 78.6 81.3 81.9 76.4 78.5 77.0 78.1 79.1 78.8 19.8 23.7 22.7 23.5 9.9 13.6 12.0 15.0 18.1 19.6 12.5 16.4 14.2 14.6 16.1 17.6 C22 80.1 81.9 82.4 82.9 69.2 77.5 72.7 77.9 80.9 81.3 71.1 77.7 75.5 79.1 79.2 78.8 20.8 20.3 22.8 23.0 19.1 22.5 19.5 21.5 22.7 23.9 26.2 27.1 26.2 26.9 26.3 28.6 C22 75.5 75.6 77.0 77.2 73.1 75.0 74.3 75.2 75.4 76.6 76.6 77.8 77.1 77.7 76.6 77. Table 9. Translation results on CoMMuTE [24] reporting BLEU (B) and COMET (C22) metrics for German (De), French (Fr), Russian (Ru), and Chinese (Zh). 100K, 30K, and ft indicate the number of pre-training steps and fine-tuning. Model MOF [26] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) F-0.4B (ours) F-0.4B 100K (ours) F-1.0B (ours) F-3.5B (ours) F-11.2B (ours) F-11.2B 30K (ours) De 63.7 61.7 59.0 59.7 59.7 61.3 62.3 62.0 54.0 54.0 54.7 53.0 52.7 52.0 Fr 68.5 62.0 55.8 62.3 60.7 65.3 67.9 67.5 53.9 56.5 54.5 54.9 52.9 54.5 Ru 67.4 58.6 53.4 55.6 55.2 62.0 63.6 62.3 51.5 51.2 51.9 54.0 54.0 52.8 Zh 66.5 62.0 58.3 61.4 59.6 61.7 61.7 61.7 54.9 54.9 55.2 55.2 54.3 53. Table 10. Binary disambiguation results on CoMMuTE [24] (accuracy in %) for the EnX translation setting. 100K, 30K, and ft indicate the number of pre-training steps and fine-tuning. 15 PaliGemma-3B [4] Florence-2-base [69] Florence-2-large [69] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) Florence-2-base Florence-2-large F-0.4B (ours) F-0.4B 100K (ours) F-1.0B (ours) F-3.5B (ours) F-11.2B (ours) F-11.2B 30K (ours) B@4 SUPERVISED 41.6 42.8 40.6 41.3 41.2 41.8 41.7 41.6 ZERO-SHOT 37.6 38.3 13.2 12.7 12.6 13.3 13.7 15.1 141.7 140.0 143.3 145.2 138.2 140.3 142.8 141.6 141.3 140. 132.9 135.9 28.2 27.3 21.3 28.7 30.5 24.9 CS 77.1 76.9 76.2 76.6 77.4 76.6 76.6 76.6 77.2 77.6 77.7 77.9 79.9 78.5 78.9 79.2 Table 11. Image captioning results on COCO Karpathy [9, 36] test split for English reporting the metrics BLEU-4 (B@4), CIDEr (C) and CLIPScore (CS). 100K, 30K, and ft indicate the number of pre-training steps and fine-tuning. PaliGemma-3B [4] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) B@4 33.2 31.2 28.6 30.0 30.2 29.9 30.0 30.4 En 88.9 84.0 76.6 79.5 85.1 81.8 83.1 83.4 CS 77.2 76.2 75.6 76.1 77.1 76.0 76.2 75.9 B@4 17.3 15.8 28.2 28.8 28.6 30.9 33.2 32.7 De 57.6 50.4 77.4 79.2 82.6 84.5 90.7 89. MCS 79.3 80.2 80.9 81.6 82.5 80.9 80.8 81.5 Table 12. Image captioning results on the Multi30K [19] task 2 test 2016 split for English (En) and German (De) reporting the metrics BLEU-4 (B@4), CIDEr (C), CLIPScore (CS), and multilingual CLIPScore (MCS). 100K, 30K, and ft indicate the number of pre-training steps and fine-tuning. PaliGemma-3B [4] Baseline-6B [4, 40] F-0.4B ft (ours) F-0.4B 100K ft (ours) F-1.0B ft (ours) F-3.5B ft (ours) F-11.2B ft (ours) F-11.2B 30K ft (ours) En 79.1 82.0 76.9 78.3 78.0 77.7 79.3 79.3 De 37.7 38.2 38.6 39.3 41.2 38.2 39.4 39.4 Fr 64.7 65.0 55.4 59.3 64.6 61.0 64.1 64. Es 67.1 63.9 54.9 57.4 59.6 59.8 61.6 61.6 Ru 35.1 29.5 26.0 27.3 30.3 30.0 34.3 34.3 Zh 27.1 21.8 23.7 24.0 24.2 24.0 25.3 25.3 Table 13. Multilingual image captioning results on XM3600 [62]. 100K, 30K, and ft indicate the number of pre-training steps and finetuning. 16 Figure 9. Captions generated with F-11.2B by prompting with the underlined prefix. F-11.2B has not seen captioning data for Fr, Es, Ru, and Zh. English references are created with DeepL. Photo by Jeanette Dickson. Figure 10. Captions generated with F-11.2B by prompting with the underlined prefix. F-11.2B has not seen captioning data for French (Fr), Spanish (Es), Russian (Ru), and Chinese (Zh). English references are created with DeepL. Photo by Reinaldo Simoes. 17 Figure 11. Detailed caption generated with F-11.2B 30K ft for German (De). English reference is created with DeepL. Photo by Jeanette Dickson. Figure 12. Detailed caption generated with F-11.2B 30K ft for German (De). English reference is created with DeepL. Photo by Reinaldo Simoes."
        }
    ],
    "affiliations": [
        "Fraunhofer IAIS, Germany",
        "Lamarr Institute for Machine Learning and Artificial Intelligence, Germany",
        "University of Applied Sciences Bonn-Rhein-Sieg, Germany",
        "University of Bonn, Computer Science Institute VI, Center for Robotics, Germany"
    ]
}