{
    "paper_title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model",
    "authors": [
        "John Won",
        "Kyungmin Lee",
        "Huiwon Jang",
        "Dongyoung Kim",
        "Jinwoo Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and a decoupled flow matching loss, which enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Furthermore, based on the decoupled training framework, we introduce a sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over a standard VLA baseline and implicit world-modeling methods, with our inference-time scaling approach providing an additional 2-5% gain on success rate. On real-world tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 7 0 6 7 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DUAL-STREAM DIFFUSION FOR WORLD-MODEL AUGMENTED VISION-LANGUAGE-ACTION MODEL John Won1 Kyungmin Lee1 Huiwon Jang1,2 Dongyoung Kim1,2 1KAIST 2RLWRLD Jinwoo Shin1,"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, augmenting vision-language-action models (VLAs) with world-models has shown promise in robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose dualstream diffusion (DUST), world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose multimodal diffusion transformer architecture that explicitly maintains separate modality streams while enabling cross-modal knowledge sharing. In addition, we propose training techniques such as independent noise perturbations for each modality and decoupled flow matching loss, which enables the model to learn the joint distribution in bidirectional manner while avoiding the need for unified latent space. Furthermore, based on the decoupled training framework, we introduce sampling method where we sample action and vision tokens asynchronously at different rates, which shows improvement through inference-time scaling. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over standard VLA baseline and implicit world-modeling methods, with our inferencetime scaling approach providing an additional 2-5% gain on success rate. On realworld tasks with the Franka Research 3, DUST outperforms baselines in success rate by 13%, confirming its effectiveness beyond simulation. Lastly, we demonstrate the effectiveness of DUST in large-scale pretraining with action-free videos from BridgeV2, where DUST leads to significant gain when transferred to the RoboCasa benchmark."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-language-action models (VLAs) have recently emerged as promising approaches for learning general-purpose robotic policies (Black et al., 2025; NVIDIA et al., 2025; Brohan et al., 2023; Li et al., 2023b; Kim et al., 2024; Luo et al., 2025; Shukor et al., 2025). Specifically, VLAs are built upon vision-language models (VLMs), which are pretrained on internet-scale multimodal datasets and excel in visual and textual understanding. Then, VLAs leverage VLM features to generate actions, through action experts (e.g., diffusion policy (Chi et al., 2023)), that generate robotic actions given the current observation and text instruction. As such, VLAs are capable of generating precise actions that generalize to novel objects, scenes, and instructions (Zawalski et al., 2024). However, despite strong perceptual grounding and instruction following capabilities, VLAs often fail to model how actions affect the environment, and fall short in terms of explicit understanding of underlying physical processes (Guo et al., 2024). To address this, recent works have augmented VLAs with world-modeling objectives, which train models to predict future visual observations together with actions (Guo et al., 2024; Zheng et al., 2025; Liang et al., 2025). Through learning the joint distribution of the two modalities, it enables the models to effectively capture the latent dynamics that govern both actions and their visual results, improving performance and generalization. Previous works utilized unified joint diffusion model structures (e.g., see Figure 1a), where the two modalities are concatenated together and modeled with single unified model (Guo et al., 2024; Huang et al., 2025). However, their design relies on the implicit assumption of the existence of shared latent space between the modalities. As result,"
        },
        {
            "title": "Preprint",
            "content": "(a) Unified Joint Diffusion Model (b) Causal Diffusion Model (c) Dual-Stream Diffusion Model Figure 1: Architectures of world-model augmented VLAs. (a) Unified Joint Diffusion concatenates action and vision tokens and generates both with single model. (b) Causal Diffusion uses separate models with one-way conditioning. (c) Dual-Stream Diffusion (ours) maintains separate streams for each modality while enabling cross-modal knowledge transfer through shared attention. the model often suffers from mismatch between modalities, where action predictions require lowdimensional, temporally smooth outputs, while future visual observations require high-dimensional, spatially structured outputs. Other approaches adopt causal design (e.g., see Figure 1b), which separates the modalities into distinct models with uni-directional conditioning (Liang et al., 2025; Hu et al., 2025). While this design can handle modality-specific structure, the design inherently limits information flow to single direction, and prevents bidirectional knowledge transfer. As such, designing world-models and action prediction models remains challenge due to the trade-off between cross-modal integration and modality-specific fidelity. Contribution. To bridge these contrasting approaches, we propose dual-stream diffusion (DUST), VLA architecture that preserves distinct modality streams while facilitating information exchange across them (see Figure 1c). DUST employs multimodal diffusion transformer (MMDiT) (Esser et al., 2024) that maintains separate token streams for actions and future visual observations, each with its own timestep embedding and normalization. The two modality streams interact through shared cross-modal attention layers, allowing bidirectional information flow without collapsing modalities into single latent space. On top of this architecture, we introduce decoupled diffusion training algorithm that applies independent noising schedules to each modality, enabling the model to learn causal relationships between them under various noise configurations (Chen et al., 2025; Rojas et al., 2025). The network is optimized via modality-specific flow matching losses, allowing actions and observations to evolve according to their respective statistical structures. Finally, we introduce unique sampling strategy for DUST that jointly samples action and visual observations. Specifically, in order to handle the difference between the modalities, we introduce asynchronous denoising, where we take diffusion steps on the high-dimensional vision tokens more frequently than the low-dimensional action tokens. As result, our approach yields scalable test-time scaling that effectively balances efficiency and accuracy. We evaluate the effectiveness and scalability of DUST through extensive evaluations on simulated, real-world, and transfer learning scenarios. To analyze DUSTs pretraining performance, we freeze the backbone VLM (Li et al., 2025b) and train the diffusion-based action expert (Chi et al., 2023) across all experiments. In simulation benchmarks, DUST outperforms baselines such as standard VLAs (e.g., GR00T-N1.5 (NVIDIA et al., 2025)) and implicit world-modeling approaches (e.g., FLARE (Zheng et al., 2025)) on the RoboCasa and GR-1 benchmarks, achieving success rate gains of 5% and 6%, respectively. When evaluating on real Franka Research 3 arm, DUST achieves the highest success rates across diverse pick-and-place tasks, outperforming baselines by over 12%, and demonstrates robust real-world performance and physically consistent predictions across environments. Lastly, we leverage DUST in pretraining with action-free videos (e.g., BridgeV2 (Walke et al., 2023)), and show that DUST exhibits significant gains when transferred to downstream tasks, such as the RoboCasa benchmark. The asynchronous joint diffusion sampling strategy also proves effective at test-time, providing an additional 26% boost over naive sampling approaches."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Vision-language-action models (VLAs). VLAs have recently emerged as promising paradigm for general-purpose robot policy learning, extending visionlanguage models (VLMs) pretrained on internet-scale multimodal datasets. Building on the strong representational capacity of VLMs (Dai et al., 2023; Team, 2024; Xiao et al., 2024), VLA architectures adapt them for robotics by either generating actions autoregressively (Kim et al., 2024; Brohan et al., 2023; Wu et al., 2024; Cheang et al., 2024) or employing diffusion-based action experts (Black et al., 2025; NVIDIA et al., 2025). In this work, we adopt the diffusion modeling formulation for action generation. Beyond these designs, recent extensions explore cross-embodiment latent action spaces (Ye et al., 2025; Bu et al., 2025) and reasoning-driven architectures for complex task execution (Zawalski et al., 2024). Despite these advances, most approaches emphasize imitation-based action distribution learning without In contrast, our framework integrates explicitly modeling how actions influence future states. world-modeling objective that captures physical dynamics, enabling more grounded and effective action generation. World-modeling for robotic policy learning. Prior work has augmented VLAs with worldmodeling objectives that generate future states alongside action generation. One line of research, exemplified by PAD (Guo et al., 2024) and EnerVerse (Huang et al., 2025), employs unified architectures that jointly model future images and actions through diffusion (Figure 1a). UWM (Zhu et al., 2025) extends this approach with modality-specific time schedules, while FLARE (Zheng et al., 2025) introduces implicit world-modeling by aligning mid-level features to future image embeddings instead of directly diffusing them. UVA (Li et al., 2025a) embeds both modalities into shared latent space, followed by modality-specific decoders that reconstruct their native representations. complementary line of work, including Video Policy (Liang et al., 2025) and Video Prediction Policy (Hu et al., 2025), adopts disjoint architectures that allow only unidirectional conditioning between modalities (Figure 1b). Another key design choice concerns how future states are represented. common approach used in PAD (Guo et al., 2024), PIDM (Tian et al., 2025), and This&That (Wang et al., 2024) is to reconstruct the next RGB observation directly after executing the generated action segment. In contrast, methods such as DINO-WM (Zhou et al., 2024) and FLARE (Zheng et al., 2025) replace raw image prediction with the generation of future observation embeddings derived from pretrained encoders like DINOV2 (Oquab et al., 2023) and Q-Former (Li et al., 2023a). We adopt this embedding-based strategy, as it emphasizes the semantic structure of future states while avoiding the need to reproduce pixel-level details, which is information that is often irrelevant for downstream control, yet costly to model."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Problem setup. Let = {T1, T2, ...} be the dataset composed of expert demonstration trajectories, where each trajectory Ti = {I, {(Ot, At)}L t=0} consists of task instruction and observations Ot and action sequences At. Specifically, we denote the observations at timestep as Ot = (ov , os ), where ov is the robot proprioceptive state. Actions are grouped in chunks (Zhao et al., 2023; Chi et al., 2023) such that At = (at, at+1, ..., at+k1) where is the chunk length. Our goal is to train model that predicts At given observations Ot and instruction I. is the visual observation and os Vision-language-action model (VLA). In developing VLA model to solve this problem, we follow common practice introduced in recent diffusion-based VLA models (Black et al., 2025; NVIDIA et al., 2025). Specifically, we use pretrained vision-language model (VLM; Li et al. 2025b) to extract high-level semantic information from the image observations and text instruction. Then, the extracted representations are used as conditions for the action expert through cross-attention layers in diffusion transformer (DiT; Peebles & Xie 2022) during action prediction. The action expert is optimized using the flow matching objective (Lipman et al., 2023). Formally, given an action sequence At, we sample random timestep τ [0, 1] and Gaussian noise ϵ (0, I) to construct noisy action Aτ = τ At + (1 τ )ϵ. Let Φt denote the visual-language features extracted from the VLM backbone, conditioned on the current visual observation ov and language instruction I. The velocity network Vθ(Φt, Aτ ) is trained to predict the ground-truth , os"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Dual-stream diffusion (DUST) architecture. Our architecture has (1) VLM model VLMϕ() that processes current observation and task instruction to produce semantic representations, and (2) diffusion model πθ which conditions on these representations to generate actions and future observation embeddings. velocity field At ϵ with the following flow matching loss: LFM(θ) = EAτ ,τ (cid:104)(cid:13) (cid:13)Vθ(Φt, Aτ , os ) (At ϵ)(cid:13) (cid:13) 2(cid:105) , (1) where we sample timestep τ from beta distribution as τ Beta( sτ ; 1.5, 1.0) with = 0.999 following common practice (Black et al., 2025; NVIDIA et al., 2025). During inference, we initialized the action sequence with Gaussian noise as A0 (0, I), and integrate the learned velocity field using Eulers method to generate action chunks over NA denoising steps: Aτ +τ = Aτ + Vθ(Φt, Aτ , os )τ , where τ = 1/NA. (2) World-modeling. The objective of world-modeling is to learn predictive representations of future states. We consider predicting future image observation ov t+k, which is obtained by executing the action chunk At of length k. However, direct pixel-level prediction may lead to emphasis on learning of high-frequency visual details that are irrelevant to low-level control and hinder the learning of meaningful physical dynamics. To this end, we instead predict the representation of the future visual observation, which we obtain by re-using the vision encoder in our VLM to embed the future image. We denote ot+k to be the future image embedding, and our world-modeling goal is to predict this embedding conditioned on VLM features Φt and proprioceptive state os ."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we present the dual-stream diffusion (DUST) model, our framework designed for joint world-modeling and action prediction. The core challenge we address is the inherent conflict within joint modeling of the two modalities, actions and future observations, which have fundamentally different statistical properties. Our method systematically resolves this conflict through three key contributions. We first introduce the DUST architecture (Section 4.1), which utilizes multimodal diffusion transformer to maintain modality-specific pathways while enabling cross-modal information exchange. We then detail our decoupled training algorithm (Section 4.2), which employs independent noise levels for each modality during training to optimize joint objective. Finally, we describe novel joint sampling strategy (Section 4.3) that supports test-time scaling by evolving the two modalities at different rates."
        },
        {
            "title": "4.1 DUST ARCHITECTURE",
            "content": "To effectively model both low-dimensional, temporally-smooth action trajectories and highdimensional, spatially-structured future image observations, our architecture must strike balance between specialized processing and joint-modal integration. As illustrated in Figure 2, DUST is built upon central vision-language model (VLM) backbone that provides semantic conditioning features Φt from the current observation and task instruction. This conditioning is fed into our core diffusion model πθ, which takes the triplet (os , Aτ t+k) as input, which is composed of the robot proprioceptive state, noised action sequence, and noised future observation embedding. , oτ This input is processed by stack of multimodal diffusion transformer (MMDiT) blocks. Critically, within each MMDiT block, the action and vision token streams are propagated through separate pathways. They are concatenated only temporarily during the shared cross-modal attention layer, which facilitates information exchange, and are immediately split back into their respective streams for all other operations. To further decouple their dynamics and directly support our training objective (described in Section 4.2), each stream receives its own distinct timestep embedding via adaptive layernorm (AdaLN) (Peebles & Xie, 2022). After traversing the shared MMDiT layers, the two streams are routed into their own modality-specific DiT blocks for several layers of finegrained, specialized denoising. This final stage allows the vision pathway to focus on reconstructing semantically consistent future embedding, while the action pathway refines the low-level motor control trajectories, thereby improving the joint modeling of both control and world dynamics. 4.2 JOINT TRAINING ALGORITHM We now introduce joint training algorithm based on decoupled diffusion framework. Our design is inspired by diffusion forcing (Chen et al., 2025), which trains diffusion models to denoise sequences with independent per-token noise levels. Our setting replaces the per-token noising with per-modality noising. Specifically, actions and future image embeddings are noised independently, with timesteps τA and τo respectively. By sampling separate timesteps, we allow for causal dependencies to be learned between the modalities. For example, the model might be asked to predict nearly-clean future observation (τo 1) from completely noisy action (τA 0). This forces it to learn the inverse relationship, effectively answering: \"What action must have been taken to achieve this future state?\" Conversely, the model might be given nearly-clean action (τA 1) and be required to denoise very noisy future observation (τo 0). This trains the forward causal relationship: \"What will the future state look like as result of this action?\" This varied training across all combinations of noise levels is what enables the model to capture the causal relationships between the modalities. Decoupled noise scheduling. Let the two modalities be the action chunk At RkdA and the future observation embedding ot+k Rdo , where dA and do are the dimensions of the action space and future image embedding, respectively. During training, we sample timesteps independently, with τA [0, 1] for actions and τo [0, 1] for future visual observations. Let ϵA, ϵo (0, I) be sampled Gaussian noise, with which we noise At and ot+k, giving the noisy action sequences and noisy future observations as AτA t+k = τoot+k + (1 τo)ϵo, respectively. The diffusion model Vθ predicts the velocity field of each modality, conditioned on the VLM feature Φt. Let us denote Vθ(Φt, AτA , oτo θ ] be the outputs of diffusion model. Then, the training objective for each action and image observations (i.e., world-modeling) are given as follows: = τAAt + (1 τA)ϵA and oτo ) = [V t+k, os θ , ,oτo AτA LA(θ) = (cid:13) (cid:13) θ (At ϵA) (cid:13) (cid:20)(cid:13) (cid:13)V (cid:13) (cid:20)(cid:13) (cid:13)V (cid:13) To effectively train the model over this joint objective, we adopt the results of Rojas et al. (2025), which demonstrate that we can decompose the joint objective of diffusing two modalities into the sum of unimodal diffusion losses, given that we utilize independent noise injection for each modality. Concretely, we can utilize the following sum of flow matching losses: θ (ot+k ϵo) LWM(θ) = 2(cid:21) . 2(cid:21) , AτA (cid:13) (cid:13) (cid:13) (3) ,oτo t+k t+k where λWM > 0 is weighting hyperparameter for world-modeling loss. LJoint(θ) = LA(θ) + λWMLWM(θ), (4)"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overview of vision-action joint sampling. During inference, we sample over NA steps for action tokens and No = NA steps for vision tokens. The global timestep advances by τo = 1/No, where vision tokens are updated every step and action tokens are updated only every steps in τA = 1/NA strides. The default value is 1, and increasing it allows test-time scaling."
        },
        {
            "title": "4.3 VISION-ACTION JOINT SAMPLING AND INFERENCE-TIME SCALING",
            "content": "During inference, we jointly sample actions and vision in parallel, by leveraging the bidirectional dependencies in which generated actions constrain plausible future states, and predicted states guide action generation. However, the two modalities are not symmetric in their requirements: image embedding diffusion operates in high-dimensional space and typically benefits from many denoising steps, whereas low-dimensional action diffusion often converges in far fewer steps and even loses performance when sampled over many steps. To address this disparity and exploit our decoupled design, we introduce test-time scaling strategy based on asynchronous forward Euler sampling. In this scheme, we first sample initial action noise A0 (0, IA) and future observation noise o0 t+k (0, Iv). We define fixed number of diffusion steps for actions, NA, and potentially larger number of steps for vision, No = NA, where N. The sampling process then proceeds using global timestep τo = 1/No. As shown in Figure 3, the vision tokens are updated at every single fine-grained step. In contrast, the action tokens are updated only every steps, corresponding to their larger step size τA = 1/NA = qτo. This asynchronous integration is defined as: oτo+τo t+k = oτo t+k + θ τo, AτA+τA = (cid:26)AτA + AτA otherwise θ τA if (τANo mod = 0) (5) For our main experiments, we use = 1 (setting No = NA = 4) for fair comparison with baselines. In Section 5.3, we explore the benefits of this test-time scaling by increasing (and thus No), creating tunable trade-off between inference speed and predictive accuracy."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we empirically assess the effectiveness of DUST. Section 5.1 presents results from simulated environments (RoboCasa, GR-1) and real-world (Franka Research 3) tasks. In Section 5.2, we investigate transferability by pretraining on action-free video data (BridgeV2), and then finetuning on robot data (RoboCasa). In Section 5.3, we assess the effectiveness of our joint sampling method for test-time scaling. In Section 5.4, we analyze the various components of our methodology through ablation studies. VLM backbone and diffusion architecture. We adopt the Eagle-2 model (Li et al., 2025b) as our frozen VLM backbone to process image observations and task instructions. Semantic features are extracted from the 12th layer of the VLM and used as conditioning signals for the diffusion module. The diffusion backbone consists of 12 MMDiT blocks for joint-modal processing, followed by 4 modality-specific DiT blocks for both the action and vision streams. Conditioning with VLM features is applied in an interleaved manner, with alternating self-attention and cross-attention layers. World-modeling target. We follow recent works in avoiding direct pixel-level prediction. The world-modeling target ot+k is the future image embedding derived from the SIGLIP-2 (Tschannen et al., 2025) representations produced by the Eagle-2 model, providing rich, semantic target for"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Evaluation on RoboCasa. Success rates (%) on RoboCasa benchmark for 8 pick-andplace (PnP), 6 contraption open/close (OP/CL), and 10 other miscellaneous tasks. 100, 300, and 1,000 demos per task are used for training. : reproduced results. 100 Demos 300 Demos 1,000 Demos Method PnP OP/CL Other Avg. PnP OP/CL Other Avg. PnP OP/CL Other Avg. GR00T-N1.5 + FLARE + DUST 0.215 0.230 0.295 0.603 0.648 0.760 0.468 0.498 0. 0.417 0.446 0.501 0.272 0.380 0.423 0.660 0.767 0.807 0.466 0.562 0.581 0.450 0.553 0.585 0.323 0.459 0. 0.757 0.837 0.863 0.508 0.682 0.686 0.508 0.646 0.663 Table 2: Evaluation on GR-1. Success rates (%) on GR-1 benchmark for 16 pick-and-place (PnP) and 8 articulated (Art.) tasks. 300 and 1,000 demos per task are used. : reproduced results. Table 3: Evaluation on real-world tasks. Success rates (%) of 4 diverse pick-and-place (PnP) tasks for real-world Franka Research 3 robot experiments. See Fig. 4 for the task instructions and settings. : reproduced results. 300 Demos 1,000 Demos Method PnP Art. Avg. PnP Art. Avg. GR00T-N1.5 + FLARE + DUST 0.176 0.340 0.358 0.283 0.330 0.367 0.203 0.337 0. 0.307 0.393 0.422 0.310 0.324 0.413 0.308 0.363 0.420 Method Task 1 Task Task 3 Task 4 Avg. GR00T-N1.5 +FLARE +DUST 0.583 0.625 0.833 0.750 0.729 0. 0.500 0.500 0.625 0.354 0.375 0.458 0.547 0.557 0.677 the vision stream to predict. Each image yields 256 tokens from the embedding model, which are reduced to 64 tokens via 22 average pooling. In total, the diffusion module processes 1 state token, 16 action tokens, and 64 future image tokens. For our joint loss (Eq. 4), we set λW = 1.0, equally weighting the action and world-modeling objectives based on our ablation study (Section 5.4). Baselines. Our primary baselines are the vanilla GR00T-N1.5 model (NVIDIA et al., 2025), which is currently the state-of-the-art VLA model, and variant trained with FLARE loss (Zheng et al., 2025). Due to lack of code release, our FLARE baseline was carefully reimplemented to use the same VLM backbone and world-modeling target as DUST (see Section A.2 for details). For fair comparison, all GR00T-N1.5 based models are trained with frozen pretrained VLM module and with the diffusion action expert module randomly initialized. 5.1 MAIN RESULTS First, we verify the efficacy of DUST across 2 simulated environments and 1 real-world setting. For the simulated setting, we utilize RoboCasa (Nasiriany et al., 2024) and GR-1 (NVIDIA et al., 2025) as our benchmarks, each representing single robot arm manipulation and humanoid manipulation. For the real-world setting, we propose 4 pick-and-place tasks with the Franka Research 3 robot arm. RoboCasa kitchen. RoboCasa is single arm manipulation benchmark with focus on kitchen environment interaction tasks. We utilize suite of 24 tasks, including turning sink faucets, closing drawer doors, and moving objects. The training dataset is drawn from the publicly available dataset offered by RoboCasa (Nasiriany et al., 2024). We experiment over 100, 300, and 1000 training episodes per task as training data. GR-1 tabletop tasks. GR-1 is humanoid robot benchmark with focus on dexterous tabletop manipulation of everyday objects. We utilize total of 24 tasks, mostly comprised of pick-andplace tasks, with some tasks having additional articulated requirements, such as closing drawer or microwave. The training dataset is taken from GR00T-N1.5 (NVIDIA et al., 2025). We experiment over 300 and 1000 training episodes per task as training data. Real-world setup. We conduct real-world experiments using 7-DoF Franka Research 3 robotic arm, where both state and action spaces are parameterized by the arms joint positions together with binary gripper state. Evaluation is performed on suite of four pick-and-place tasks in tabletop setting, where each task is defined by the distinct sourcetarget configuration. Within every task we evaluate across four different object categories (doll, cup, box, sponge) to capture variations in geometry, size, and physical properties. The training corpus consists of 60 expert demonstrations per task, gathered via teleoperation on the same Franka platform."
        },
        {
            "title": "Preprint",
            "content": "(a) PnP Task 1 (b) PnP Task 2 (c) PnP Task 3 (d) PnP Task 4 Figure 4: Real-world task instructions. For the real-world experiments, we utilize 4 pick-and-place tasks with the Franka Research 3 robot. The tasks are categorized by their distinct source-target pairs (box, bowl, plate, etc.) and each contains 4 different objects (cup, doll, cube, sponge). Simulation results. Tables 1 and 2 show that DUST consistently outperforms GR00T-N1.5 and GR00T-N1.5+FLARE across both RoboCasa and GR-1 benchmarks, covering all task categories and demonstration scales. On RoboCasa with 100 demonstrations per task, DUST improves the average success rate by 18% over GR00T-N1.5 and 5% over FLARE, and this advantage remains as the number of demonstrations increases, confirming both data efficiency and scalability. On GR-1, more challenging benchmark, DUST again surpasses both baselines at 300 and 1000 demonstrations, yielding improvements in both task categories. Real-world results. Table 3 presents results on the Franka Research 3 robot with pick-and-place tasks. DUST consistently outperforms baseline models, achieving the highest success rate on every task with average improvements of 13% over GR00T-N1.5 and 12% over FLARE. These gains, observed across diverse object types and sourcetarget configurations, demonstrate DUSTs robustness in physical environments and its promise for practical deployment. As illustrated in Figure 5, the incorporation of world-modeling enables the policy to anticipate the future end-effector pose and accurately align with the target object. 5.2 TRANSFER LEARNING teleoperated high-quality Table 4: Evaluation for transfer learning. Success rates (%) on RoboCasa with or without BridgeV2 video data pretraining. Collecting robot demonstrations is expensive and labor-intensive, while vast amounts of action-free video can be gathered at minimal cost through human recordings or internet-scale crawling (Ye et al., 2025; Dass et al., 2023; Wang et al., 2025). Leveraging such large-scale video datasets allows models to acquire generalizable representations of object dynamics and scene evolution without relying on low-level action annotations. DUSTs dualstream architecture is naturally suited for this setting, as it enables pretraining on action-free video to accumulate world-modeling knowledge prior to finetuning as policy, thereby bridging the gap between inexpensive large-scale video data and costly teleoperated robot data. 0.215 0.603 0.468 0.417 0.295 0.760 0.510 0.501 0.423 0.807 0.581 0.585 Video Pretrain GR00T-N1.5 + DUST + DUST PnP OP/CL Other Avg. Method For this section, during the pretraining stage, the model is trained exclusively on the video component of the BridgeV2 dataset (Walke et al., 2023), optimizing only the world-modeling term of the flow matching loss while randomly initializing the action tokens. After pretraining, we finetune the model on the RoboCasa dataset using 100 demonstrations per task. Table 4 shows that incorporating video pretraining yields substantial gains, with DUST achieving an average success rate of 0.585 compared to 0.501 without pretraining. These results highlight that large-scale passive video data can effectively transfer to downstream policy learning, improving data efficiency and generalization while reducing dependence on expensive robot demonstrations."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Qualitative comparison on real-world pick-and-place task (Instruction : \"Pick up the blue cup on the brown box and place it in the golden bowl.\") The sequence on the right shows GR00T-N1, which directly generates action sequences, and on the left is DUST, which incorporates explicit world-modeling. While GR00T-N1.5 produces actions that bring the gripper near the cup, it fails to align precisely with the rim and is unsuccessful in grasping. By contrast, DUST leverages its internal prediction of future states by estimating where generated actions will position the gripper, allowing it to consistently adjust and achieve alignment with the desired position for grasping. Table 5: Results of test-time scaling with asynchronous joint sampling. Success rates (%) on RoboCasa and GR-1 with our test-time scaling approach using asynchronous joint sampling. For scaling, we increase No, the number of diffusion steps for vision tokens. RoboCasa 100 demos RoboCasa 1000 demos GR-1 1000 demos No 4 16 32 64 PnP OP/CL Other Avg. PnP OP/CL Other Avg. PnP Art. Avg. 0.295 0.308 0.248 0.290 0.760 0.733 0.753 0. 0.510 0.524 0.568 0.548 0.501 0.504 0.508 0.518 0.483 0.498 0.501 0.509 0.863 0.856 0.868 0.881 0.686 0.690 0.724 0.736 0.663 0.668 0.686 0. 0.422 0.447 0.471 0.430 0.413 0.463 0.472 0.511 0.420 0.451 0.471 0.450 5.3 TEST-TIME SCALING FOR JOINT SAMPLING While our main experiments adopt the same number of diffusion steps for both actions and vision, this symmetry may not be optimal. The higher dimensionality and structural complexity of image embeddings typically requires more denoising iterations than the lower-dimensional and temporally smooth action tokens. To account for this, we introduce test-time scaling strategy in which vision tokens are allocated additional diffusion steps while action tokens steps are fixed, thereby enabling finer-grained refinement of visual representations. Specifically, we follow the asynchronous joint sampling procedure outlined in Section 4.3. We increase the number of vision denoising steps No from its default value of 4 to 16, 32, and 64, while keeping the number of action token steps fixed at NA = 4. Experiments are conducted using DUST checkpoints finetuned on RoboCasa with 100 and 1000 demonstrations per task, as well as GR-1 with 1000 demonstrations per task. As shown in Table 5, increasing the number of vision denoising steps leads to mostly steady performance gains up to 64 steps. On RoboCasa, we observe improvements of roughly 23% at 64 steps, while on GR-1 the best results occur at 32 steps, yielding 5% gain. These findings indicate that allocating additional diffusion steps to vision tokens can substantially enhance VLA performance by allowing more precise refinement of visual representations. However, the improvements come at the expense of higher inference time, highlighting tunable trade-off between efficiency and accuracy. Further ablations on the role of modality decoupling in this process are provided in Section A.1. 5.4 ABLATION STUDY DUST components analysis. We next conduct an ablation study to disentangle the contributions of DUSTs two core design elements: the dual-stream MMDiT architecture and decoupled training algorithm. To this end, we evaluate three alternative configurations: (1) baseline DiT model trained with uniform noise schedule applied jointly to both action and vision tokens, serving as standard single-stream reference, (2) DiT model with decoupled noising, where AdaLN conditioning is applied independently to each modality, but the token streams still share single feed-forward pathway, and (3) an MMDiT model with uniform noise levels, corresponding to the unmodified multi-stream MMDiT architecture with separate actions and vision streams. This design allows us to isolate the relative benefits of modality-specific noise schedules and of the dual-stream transformer structure itself. Results on RoboCasa with 100 demonstrations per task (Figure 6a) show that"
        },
        {
            "title": "Preprint",
            "content": "Table 6: Ablation study. Success rates (%) on RoboCasa benchmark with 100 demos/task ablating over (a) architecture and training algorithm, (b) depth of MMDiT, and (c) the loss weight λWM for world-modeling loss. (a) Architectural and training (b) MMDiT depth (c) Effect of λWM Arch. Noise PnP OP/CL Other Avg. Layers Avg. λWM Avg. DiT DiT MMDiT MMDiT Decoupled Joint Decoupled Joint 0.240 0.248 0.160 0.295 0.633 0.613 0.677 0.760 0.340 0.454 0.382 0. 0.380 0.425 0.382 0.501 6 10 12 14 0.474 0.483 0.501 0.493 0.2 0.5 1.0 2.0 0.343 0.489 0.501 0.496 both components are indispensable. Removing the dual-stream MMDiT structure results in performance drop of approximately 8%, while removing decoupled noise leads to an even larger 12% reduction. These findings confirm that the two design choices contribute complementary gains, with MMDiT enabling structured cross-modal representation learning, while decoupled noising allows each modality to evolve under dynamics appropriate to its scale and complexity. Loss weight hyperparameter λWM and MMDiT layer count. Next, we analyze the effect of the loss weighting coefficient λWM, which balances the two flow matching terms in our objective. Larger λWM values emphasize world-modeling, while smaller values emphasize action modeling. As shown in Figure 6c, experiments on RoboCasa with 100 demonstrations per task indicate that performance remains stable in the range λWM [0.5, 2.0], but degrades when moving outside this interval. This suggests that effective learning requires weighting the two objectives relatively evenly. Next, we study the ratio of MMDiT to DiT layers. Fixing the total number of layers in πθ to 16, we vary the number of MMDiT layers to adjust the trade-off between cross-modal knowledge transfer and permodality specialization. Results (Figure 6b) show that while performance is generally stable across configurations, the best outcome is obtained with 12 MMDiT layers and 4 DiT layers, highlighting the benefit of heavily leveraging cross-modal processing."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced dual-stream diffusion (DUST), world-model augmented VLA framework that decouples the diffusion of actions and future observations while still enabling cross-modal knowledge transfer. By maintaining separate modality streams linked through shared attention, DUST avoids the limitations of unified latent space and captures causal dependencies between modalities. Extensive experiments show that DUST consistently outperforms baselines on both simulated benchmarks (RoboCasa, GR-1) and real-world Franka Research 3 tasks, underscoring its scalability and robustness. Beyond architecture and training, we also proposed test-time scaling strategy with asynchronous joint sampling, which further improves performance by allocating finer-grained diffusion to high-dimensional vision tokens. Finally, pretraining on action-free video (BridgeV2) demonstrates that DUST can exploit large-scale passive data for efficient transfer to downstream robotics. Together, these contributions establish DUST as versatile and extensible framework for bridging world-modeling, video pretraining, and scalable inference in VLA models. REPRODUCIBILITY STATEMENT We provide detailed descriptions and diagrams of our architecture and training algorithms in Section 4.1, 4.2, and A.2. We utilize publicly released datasets for simulation setting experiments, and our real-world experiments are easy to reproduce and clearly explained. We also attach pseudocode for our training algorithm and test-time scaling strategy in Section A.6."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: visionlanguage-action flow model for general robot control. In Robotics: Science and Systems, 2025. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. In Robotics: Science and Systems, 2025. Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In Advances in Neural Information Processing Systems, 2025. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2023. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, 2023. Shivin Dass, Karl Pertsch, Hejia Zhang, Youngwoon Lee, Joseph J. Lim, and Stefanos Nikolaidis. Pato: Policy assisted teleoperation for scalable robot data collection. In Robotics: Science and Systems, 2023. Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. In Advances in Neural Information Processing Systems, 2024. Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. In International Conference on Machine Learning, 2025. Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, and Guanghui Ren. Enerverse: Envisioning embodied future space for robotics manipulation. arXiv preprint arXiv:2501.01895, 2025."
        },
        {
            "title": "Preprint",
            "content": "Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. In IEEE International Conference on Robotics and Automation, 2025. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, 2023a. Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. In Robotics: Science and Systems, 2025a. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378, 2023b. Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, Nadine Chang, Karan Sapra, Amala Sanjay Deshmukh, Tuomas Rintamaki, Matthieu Le, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, De-An Huang, Timo Roman, Tong Lu, Jose M. Alvarez, Bryan Catanzaro, Jan Kautz, Andrew Tao, Guilin Liu, and Zhiding Yu. Eagle 2: Building post-training data strategies from scratch for frontier visionlanguage models. arXiv preprint arXiv:2501.14818, 2025b. Junbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, and Carl Vondrick. Video generators are robot policies. arXiv preprint arXiv:2508.00795, 2025. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow In International Conference on Learning Representations, matching for generative modeling. 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, and Zongqing Lu. Being-h0: Vision-language-action pretraining from largescale human videos. arXiv preprint arXiv:2507.15597, 2025. Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. In Conference on Robot Learning, 2023. Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. In Robotics: Science and Systems, 2024. NVIDIA, Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023."
        },
        {
            "title": "Preprint",
            "content": "William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision, 2022. Kevin Rojas, Yuchen Zhu, Sichen Zhu, Felix X. F. Ye, and Molei Tao. Diffuse everything: Multimodal diffusion models on arbitrary state spaces. In International Conference on Machine Learning, 2025. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, and Remi Cadene. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In International Conference on Learning Representations, 2025. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, 2023. Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, and Jeong Joon Park. This&that: Language-gesture controlled video generation for robot planning. arXiv preprint arXiv:2407.05530, 2024. Wenhao Wang, Jianheng Song, Chiming Liu, Jiayao Ma, Siyuan Feng, Jingyuan Wang, Yuxin Jiang, Kylin Chen, Sikang Zhan, Yi Wang, et al. Genie centurion: Accelerating scalable real-world robot training with human rewind-and-refine guidance. arXiv preprint arXiv:2505.18793, 2025. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, and Minjoon Seo. Latent action pretraining from videos. In International Conference on Learning Representations, 2025. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In International Conference on Learning Representations, 2025. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023."
        },
        {
            "title": "Preprint",
            "content": "Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, Avnish Narayan, You Liang Tan, Guanzhi Wang, Qi Wang, Jiannan Xiang, Yinzhen Xu, Seonghyeon Ye, Jan Kautz, Furong Huang, Yuke Zhu, and Linxi Fan. Flare: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025. Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024. Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. In Robotics: Science and Systems, 2025."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Results of test-time scaling with synchronous joint sampling. Success rates (%) on RoboCasa and GR-1 with our test-time scaling approach using synchronous joint sampling. For scaling, we increase both No, NA, the number of diffusion steps for vision tokens and action tokens, respectively. RoboCasa 100 demos RoboCasa 1,000 demos GR-1 1000 demos No 4 16 32 PnP OP/CL Other Avg. PnP OP/CL Other Avg. PnP Art. 0.295 0.197 0.210 0.181 0.760 0.685 0.710 0.654 0.510 0.450 0.424 0.416 0.501 0.425 0.424 0. 0.483 0.472 0.450 0.460 0.863 0.854 0.807 0.817 0.686 0.621 0.630 0.601 0.663 0.630 0.614 0.608 0.422 0.402 0.406 0.399 0.413 0.443 0.438 0. Avg. 0.420 0.416 0.406 0.401 Figure 6: Modified MMDiT. DUSTs MMDiT blocks are implemented with separate timestep embeddings being used as conditions for each modality."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TEST-TIME SCALING OF NAIVE JOINT SAMPLING In Section 5.3, we explored test-time scaling DUST by increasing No, the number of vision token diffusion steps, while keeping NA, the action diffusion step count, fixed at 4. While we have seen great performance gains through the asynchronous joint sampling, it is natural to ask whether simply increasing diffusion steps for both modalities could be enough. In Table 7, we present results from an ablation study, where both NA = No are increased together, instead of fixing NA and increasing No. We can see that without the decoupling of number of steps between modalities, simply increasing diffusion steps actually leads to deterioration in performance. This lends credibility to our initial hypothesis of only vision tokens needing more diffusion steps, and shows that the asynchronous component of our test-time scaling method is crucial to its success. A.2 IMPLEMENTATION AND TRAINING DETAILS Additional implementation details. We base our architecture on the GR00T-N1.5 (NVIDIA et al., 2025) codebase, from which we get the pretrained Eagle-2 VLM model. For vision tokens, they pass through an encoder made up of 3-layer MLP with 2D sinusoidal positional encoding with SiLU activation. The vision decoder is 2-layer MLP with ReLU activation. Action tokens utilize the linear encoder-decoder pair given in the original code-base, alongside 1D sinusoidal positional encoding. The MMDiT blocks used in our model are slight modification of the original in that the AdaLN layers for each modality stream take the conditioning timestep embeddings from independent sources instead of utilizing global timestep embedding. We show this in more detail in Figure 6."
        },
        {
            "title": "Preprint",
            "content": "Baselines. The GR00T-N1.5 baseline is trained on the original released code, while the FLARE baseline does not release official code or checkpoints. Hence, for FLARE, we do not utilize the Q-Former architecture of the original paper, but re-implement the FLARE loss to utilize the same world modeling target as ours, which is the SIGLIP-2 embeddings from the model VLM. This allows fair comparison between dual-stream diffusion world modeling of DUST and the implicit world modeling of FLARE. For the alignment module of FLARE we use small MLP, with similar architecture to that of REPA (Yu et al., 2025), which inspired FLARE. Batch size and iteration count. We vary batch size and training time per dataset. For the RoboCasa (Nasiriany et al., 2024) dataset, we train using global batch size 32, with 2 A100 GPUs. For each training dataset scale, the time until convergence varies, with 100 demos requiring 60k steps, 300 demos requiring 420k steps, and 1000 demos requiring 600k steps. The long convergence time is mostly due to the small global batch size. For the GR-1 (NVIDIA et al., 2025) dataset, we train using global batch size of 960, with 8 H200 GPUs over 60k steps. We noted training on GR-1 was very sensitive to batch size and required large scale training for meaningful training results. For the real-world dataset, we train using global batch size of 32, with 2 A100 GPUs over 60k steps. For the transfer learning setup, we first train with BridgeV2 (Walke et al., 2023) video data using global batch size of 32, with 2 A100 GPUs for 120k steps. Then, we finetune using the RoboCasa 100 demo dataset with the same GPU setup for 60k steps. Common training details. Excluding batch size and iteration count, all experiments are done with the same training hyperparameters. We optimize with AdamW (Loshchilov & Hutter, 2019) using base learning rate of 1e-4, with β1 = 0.95, β2 = 0.999, and ϵ = 1e-8. Weight decay of 1e-5 is applied with the exception of bias and LayerNorm weights. The learning rate follows cosine decay schedule with 5% warmup period. A.3 SIMULATION BENCHMARKS RoboCasa kitchen. RoboCasa is single arm manipulation benchmark with focus on kitchen environment interaction tasks. We utilize suite of 24 tasks that span wide range of common household manipulations, including turning sink faucets, closing drawer doors, and moving objects. Tasks are categorized into 8 pick-and-place tasks, 6 contraption open/close tasks, and 10 other miscellaneous tasks. Training data is drawn from the publicly available dataset from RoboCasa which was generated with MimicGen (Mandlekar et al., 2023) within the MuJoCo simulation environment (Todorov et al., 2012), with Franka Emika Panda robot arm serving as the manipulator. Image observations include 3 viewpoints from the left, right, and wrist. The robot state/action space is parameterized with 7 degrees of freedom (DoF), consisting of end-effector position and rotation together with binary gripper pose. We experiment over 100, 300, and 1000 training episodes per task, testing data efficiency and scaling properties. GR-1 tabletop tasks. GR-1 is humanoid robot benchmark with focus on dexterous tabletop manipulation of everyday objects. We utilize total of 24 tasks consisting of 16 pick-and-place tasks, and 8 articulated tasks, the latter adding the requirement of closing containers such as microwaves and cabinets after pick-and-place. Training data utilizes data from GR00T-N1.5 (NVIDIA et al., 2025), where the dataset was generated with DexMimicGen (Jiang et al., 2025) in the MuJoCo simulation environment (Todorov et al., 2012). The simulated robot is GR-1 humanoid robot with Fourier dexterous hands, enabling fine-grained grasping and manipulation. Image observations are taken from single egocentric view from the robots head. The state/action space consists of 29 DoF in total, 17 DoF corresponding to the GR-1 robots arms and waist, and 6 DoF for each of the Fourier hands. We experiment over 300 and 1000 training episodes per task. A.4 REAL-WORLD EXPERIMENT DETAILS Our tasks consist of 4 tasks, which have the following task instruction templates: Pick up the {Object} on the brown box and place it in the golden bowl."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Real-world experimental setting. We utilize the Franka Research 3 robot with two ZED cameras, one on the wrist and one to the side. Pick up the {Object} on the brown box and place it on the white plate. Pick up the {Object} in the white basket and place it in the black bowl. Pick up the {Object} on the white plate and place it in the white basket. Each task contains the four object categories - Teddy Bear, Blue Cube, Blue Cup, and Sponge. During evaluation each object-task configuration gets 6 evaluations, meaning 24 trials per task. We predetermine set of varied configurations of where to place the source-target locations, on where the source location the object is placed, and the direction it is facing. This allows for more fair comparison in real-world experiments that typically have high stochasticity. When an object has been partially placed in the target destination but the center of gravity is outside of said target, we denote that as half success and count it as 0.5 successes. We note there were very few cases of this happening. A.5 LLM USAGE DISCLOSURE We acknowledge that large language models (LLMs) were used in the preparation of this manuscript to assist with writing quality. LLMs were used to find grammatical errors, suggest alternative vocabulary, and detect potential typographical issues. All substantive ideas, analyses, and conclusions presented in this paper are the work of the authors."
        },
        {
            "title": "Preprint",
            "content": "A.6 DUST PSEUDOCODE Algorithm 1: DUST Training Input: Dataset D, weight λWM, steps, batch size, optimizer hyperparams. Models and encoders/decoders as described in text below. Output: Trained parameters θ. 1 Initialize θ, optimizer (AdamW); 2 for step 1 to steps do // 1) Minibatch Sample minibatch of size bs; // 2) Conditioning Φ VLMϕ(ov , ℓ) ; // VLM semantic representations // 3) Modality-decoupled noising Sample τA, τo U(0, 1) and ϵA, ϵo (0, I); AτA ot+k VLMimg(ov oτo t+k τoot+k + (1 τo)ϵo; τAAt + (1 τA)ϵA; t+k) ; // future obs embedding // 4) Per-modality encoders , AτA XA EncA([os ]); Xo Enco([oτo t+k]); // 5) Dual-stream MMDiT stack (AdaLN per modality) for 1 to NMMDiT do (XA, Xo) MMDiTi(XA, Xo, Φ, τA, τo); // 6) Modality-specific DiT stack for 1 to NDiT do XA DiTA Xo DiTo (XA, Φ, τA); (Xo, Φ, τo); // 7) Per-modality Decoders θ DecA(XA); A θ Deco(Xo); // 8) Flow-matching losses (linear path) // For the linear path, uA = dτA uA At ϵA; uo ot+k ϵo; LA MSE(V Ljoint LA + λWMLWM; θ , uA); LWM MSE(V θ , uo); (τAAt + (1 τA)ϵA) = At ϵA // 9) Update zero_grad(); backward(Ljoint); clip_grad_norm(θ); step(); 3 4 5 6 7 8 10 11 12 13 14 15 16 17 18 19 20 return θ;"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 2: DUST Test-Time Scaling - Asynchronous Joint Sampling Input: Trained model πθ; horizon ; diffusion step counts NA, No with No > NA; ratio = No/NA; Output: Predicted action sequence At and future observation embedding ot+k; 1 Initialize τA, τo = 0; 2 Initialize noisy tokens AτA (0, I), oτo 3 Set τo = 1/No, τA = 1/NA = qτo 4 for nA 1 to NA do t+k (0, I); // outer loop: action updates for 1 to do // inner loop: vision updates τo τo + τo; τA τA + τo; t+k oτo oτo θ τo; 5 6 7 8 AτA AτA 9 10 return Final denoised t + t+k + θ τA; , o1 t+k;"
        },
        {
            "title": "Preprint",
            "content": "A.7 EXAMPLE GR-1 ROLLOUTS We showcase example rollouts of DUST trained on GR-1 with 1000 demos per task. (a) (GR-1) Pick up the can, place it into the drawer and close the drawer. (b) (GR-1) Pick up the milk, place it into the microwave and close the microwave (c) (GR-1) Pick the pear from the plate and place it in the plate (d) (GR-1) Pick the pear from the tray and place it in the pot A.8 EXAMPLE ROBOCASA ROLLOUTS We showcase example rollouts of DUST trained on RoboCasa with 1000 demos per task. (a) (RoboCasa) Open the cabinet door (b) (RoboCasa) Pick the cheese from the sink and place it on the plate located on the counter (c) (RoboCasa) Turn on the microwave"
        },
        {
            "title": "Preprint",
            "content": "A.9 EXAMPLE REAL-WORLD ROLLOUTS We showcase example rollouts of DUST trained on our real-world Franka Research 3 dataset with 60 demos per task. (a) (Franka) Pick up the blue cube in the white basket and place it in the black bowl (b) (Franka) Pick up the sponge on the brown box and place it on the white plate"
        }
    ],
    "affiliations": [
        "KAIST",
        "RLWRLD"
    ]
}