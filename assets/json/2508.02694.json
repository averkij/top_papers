{
    "paper_title": "Efficient Agents: Building Effective Agents While Reducing Cost",
    "authors": [
        "Ningning Wang",
        "Xavier Hu",
        "Pai Liu",
        "He Zhu",
        "Yue Hou",
        "Heyuan Huang",
        "Shengyu Zhang",
        "Jian Yang",
        "Jiaheng Liu",
        "Ge Zhang",
        "Changwang Zhang",
        "Jun Wang",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 9 6 2 0 . 8 0 5 2 : r Efficient Agents: Building Effective Agents While Reducing Cost"
        },
        {
            "title": "Abstract",
            "content": "The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions. Date: August 6, 2025 Correspondence: Wangchunshu Zhou at zhouwangchunshu@oppo.com; He Zhu at zhuhe@oppo.com Code: https://github.com/OPPO-PersonalAI/OAgents"
        },
        {
            "title": "Introduction",
            "content": "The ever-increasing reasoning and creation capabilities of Large Language Models (LLMs) have opened up broad prospect for real-world applications. Researchers have developed numerous LLM-driven agent systems [110] and created large number of fascinating products capable of handling complex, multi-step tasks. However, this progress mirrors familiar trajectory in NLP research: from BERT [11] to ChatGPT [12], researchers consistently prioritize scaling up models to achieve breakthrough capabilities [13, 14], only later turning to optimize efficiency, cost, and environmental impact [15, 16]. This pattern has given rise to the critical subfield of efficient NLP, where researchers balance performance with practical constraints like inference latency, energy consumption, and economic viability. We argue that agent research has now reached similar inflection point. While increasingly sophisticated agent architectures can solve remarkably complex problems, their costs scale prohibitively. Industry deployments reveal this tension starkly: cutting-edge agent products (e.g., DeepResearch [17], Manus [18]) demonstrate impressive capabilities but suffer from exorbitant operating costs due to explosive LLM call overhead. Some 1 Figure 1 Evaluation of effectiveness and efficiency in agent system components. We adopt cost-of-pass as the metric to evaluate. We develop Efficient Agents that optimizes cost while maintaining accuracy. systems require hundreds of API calls per task, rendering them economically unsustainable despite their technical brilliance. This creates fundamental bottleneck for real-world adoption, limiting both the scalability of applications and the accessibility of AI advancements. Our work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems. Through rigorous empirical analysis, we investigate 3 research questions: (1) How much complexity do agentic tasks truly require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of task-adaptive agent frameworks? By dissecting these relationships across the framework, we provide actionable insights for both researchers and practitioners. We conduct an empirical study on the GAIA benchmark [19] focusing on the efficiency-performance trade-off of agent systems by evaluating the impact of: (1) the choice of LLM backbones; (2) the designs of agent frameworks including planning, tool using, and memory; (3) test-time scaling and ensemble strategies. We adopt the cost-of-pass [20] metric to compare and analyze the impact of different design choices on the efficiency-performance trade-off of LLM-based agents, as illustrated in Figure 1. Based on the insight obtained in the aforementioned empirical study, we introduce Efficient Agents , an agent framework optimized for achieving the best efficiency-performance trade-off that achieves new state-of-the-art on the cost-of-pass metric on the GAIA benchmark. Specifically, 96.7% of the performance of OWL [21], an open-source agent framework that achieves great performance on the GAIA benchmark, while reducing the cost from $0.398 to $0.228, leading to relative improvement of 28.4% in terms of cost-of-pass. Our contribution could be summarized as follows: We thoroughly analyze and summarize the factors that cause significant economic overhead in generic LLM-based agent system. We propose Efficient Agents , an efficient agent system where each component is selected based on prior analytical results, optimizing for efficiency while maintaining high effectiveness. Experimental results demonstrate that Efficient Agents achieves 96.7% of the performance of OWL while reducing costs by 28.4%."
        },
        {
            "title": "2 Preliminaries",
            "content": ""
        },
        {
            "title": "2.1 Setup",
            "content": "Many factors can influence the effectiveness and efficiency of an agent system [22]. In this paper, we aim to conduct comprehensive analysis from the perspective of agent systems. These factors encompass not only the backbone LLM itself but also the agent framework built around it, including planning mechanisms [23], tool usage [24] , and memory module [25]. In addition, test-time scaling strategies [26] are also considered. To this end, we evaluate these components on GAIA [19], popular and challenging agent benchmark. This benchmark typically requires agents to perform complex reasoning to solve problems. By leveraging the benchmark, we can effectively assess the impact of individual components on overall performance and efficiency. Additionally, we compare several distinct agent frameworks to provide broader perspective on their relative strengths. To accurately identify the impact of each component on efficiency and effectiveness, we established default setup (listed in the Appendix) and varied one component at time, testing to observe the effects."
        },
        {
            "title": "2.2 Metrics",
            "content": "An ideal agent should achieve both high performance and computational efficiency. Therefore, in addition to accuracy, measured by pass@1 (solving the problem in one attempt) to evaluate effectiveness, we assess efficiency using the number of tokens taken by LLMs and associated costs. Notably, the cost of input tokens for APIs of LLMs is generally significantly lower than that of output tokens across many proprietary LLM providers. Accordingly, we compute these costs separately. The per-token pricing is sourced from official provider documentation as of May 2025. Furthermore, as different models or strategies may simultaneously impact performance and efficiency, we follow [27] and adopt the cost-of-pass metric to quantify model efficiency. The cost-of-pass metric, denoted as v(m, p), represents the expected monetary cost of using model to generate correct solution for problem p. It is computed as the ratio of the cost of single inference attempt, Cm(p), to the success rate, Rm(p): v(m, p) = Cm(p) Rm(p) Here, the cost of single inference attempt, Cm(p), is defined as: Cm(p) = nin(m, p) cin(m) + nout(m, p) cout(m) where nin(m, p) and nout(m, p) are the number of input and output tokens for model on problem p, respectively, and cin(m) and cout(m) are the per-token costs for input and output. The success rate Rm(p) is estimated by the proportion of correct responses. This metric represents the expected monetary cost of using model to generate correct solution for problem, providing comprehensive measure of models economic efficiency."
        },
        {
            "title": "3.1 Backbones",
            "content": "Current Large Language Models acquire System-2 reasoning capabilities through reinforcement learning [28, 29], leveraging extended chain-of-thought [30] processes that often span thousands of tokens or more. While this approach significantly enhances reasoning performance, it also substantially increases computational costs and even leads to the phenomenon of overthinking [31], which means excessive computational resources are allocated to simple problems during inference. To investigate this trade-off, we evaluate the performance and efficiency of several models with the same agent frameworks, including proprietary models such as GPT-4.1 [32], o1 [28] and Claude-3.7 [33], open-source sparse models with MoE architecture such as Qwen3-235B-A22B [34] and Qwen3-30B-A3B [34], also dense model such as QwQ-32B [35]. The results are presented in Table 1. Based on the results, we have several findings: Claude 3.7 Sonnet achieves the highest accuracy on the GAIA benchmark (61.82% overall) compared to GPT-4.1 (53.33%), but its cost-of-pass is significantly higher (3.54 vs. 0.98). This indicates that current high-performing LLMs, when used as agent backbones, often sacrifice efficiency for better effectiveness, highlighting critical trade-off in model design. Sparse models like 3 Table 1 Performance of various backbone LLMs on the GAIA dataset. We report metrics across the entire GAIA development set and its three difficulty levels (Level 1 to Level 3). Cost-of-pass is defined as infinity when accuracy is zero, signifying that no benefits can be obtained irrespective of the cost incurred."
        },
        {
            "title": "Efficiency",
            "content": "cost-of-pass all l1 l2 l3 all"
        },
        {
            "title": "Effectiveness",
            "content": "Acc./% l2 l1 l3 all"
        },
        {
            "title": "0.98\nGPT-4.1\nClaude 3.7 Sonnet\n3.54\nQwen3-235B-A22B 0.22\n0.13\nQwen3-30B-A3B\n0.23\nQwQ-32B\n3.66\no1",
            "content": "0.32 1.69 0.12 0.07 0.15 1.96 53.33 3.51 1.07 61.82 9.04 3.81 27.27 0.27 0.30 0.16 17.58 22.42 0.49 0.26 52.12 12.66 3.62 69.81 73.58 37.74 30.19 30.19 67.92 50.00 60.47 22.09 15.12 20.93 50.00 30.77 42.31 23.08 0.00 11.54 26.92 0.705 2.190 0.040 0.023 0.120 1. Cost/$ l2 l1 0.367 1.244 0.082 0.022 0.102 1.328 0.710 2.301 0.091 0.024 0.126 1."
        },
        {
            "title": "Cost",
            "content": "l3 all #Tokens l2 l1 l3 1.380 3.824 0.093 0.022 0.135 3.408 243K 103K 249K 506K 680K 379K 716K 1,196K 72K 53K 81K 70K 61K 65K 142K 129K 148K 66K 47K 69K 76K 60K 149K 127K Figure 2 Performance of various backbone LLMs on the GAIA benchmark: Accuracy vs Cost. Qwen3-30B-A3B exhibit superior efficiency, with low cost-of-pass (0.13 overall) despite modest accuracy (17.58% overall). Given GAIAs challenging nature and Qwen3-30B-A3Bs small activated parameter count (3B), such models may offer advantages for simpler agent tasks where efficiency is prioritized over raw performance. MoE-based sparse models, such as Qwen3-30B-A3B, leverage selective parameter activation to achieve remarkable efficiency, making them well-suited for resource-constrained agent tasks. As task difficulty increases from Level 1 to Level 3, cost-of-pass rises dramatically across large reasoning models. For instance, Claude 3.7 Sonnets cost-of-pass increases from 1.69 to 9.04 (a 534% surge) and OpenAI o1 from 1.96 to 12.66 (a 646% surge), underscoring that efficiency deteriorates significantly on harder tasks especially for reasoning models, posing challenges for scaling LLMs to complex agent scenarios. Reasoning Models Efficiency Deterioration on Hard Task As task difficulty escalates, cost-of-pass of reasoning models dramatically increases and efficiency significantly deteriorates, posing formidable challenge for deploying these models in intricate agentic environments."
        },
        {
            "title": "3.2 Test-time Scaling Strategies",
            "content": "Test-time scaling enhance models performance by leveraging multiple inference runs [26, 36], but these approaches typically require the model to be executed times, significantly increasing token consumption. We evaluate the common strategies, Best-of-N (BoN) [36]. At each step, possible actions are sampled and evaluated by progress reward model (PRM). The action scored the highest is kept as the next action. We implement the PRM by promting GPT-4o. The prompt can be found in Appendix B. We test 1, 2, 4,. 4 The results are presented in Table 2. Table 2 Best-of-N Performance under Different in Test-time Scaling. 1 2 all 0.98 1.17 1."
        },
        {
            "title": "Efficiency",
            "content": "cost-of-pass"
        },
        {
            "title": "Effectiveness",
            "content": "Acc. l1 l2 l3 all l2 l3 all Cost/$ l2 l"
        },
        {
            "title": "Cost",
            "content": "l3 all #Tokens l2 l1 l3 0.32 0.54 0.65 1.07 1.18 1. 3.51 3.70 2.70 53.33 54.55 53.94 69.81 62.26 71.70 50.00 56.98 46.51 30.77 30.77 42.31 0.521 0.639 0. 0.227 0.336 0.350 0.533 0.675 0.734 1.080 1.138 1.253 243K 103K 249K 506K 298K 153K 317K 533K 325K 161K 345K 593K We observe that increasing in Best-of-N from 1 to 4 leads to substantial rise in token consumption (from 243k to 325k). However, the performance improvement is marginal, with accuracy only slightly increasing from 53.33% to 53.94% when = 4 compared to = 1. Consequently, this results in notable decrease in efficiency, with the cost-of-pass rising from 0.98 to 1.28. Best-of-N Rises Cost But Marginal Gain The marginal performance gains of BoN come at disproportionate computational cost, highlighting the need for more efficient test-time scaling strategies in an agent setting."
        },
        {
            "title": "3.3 Planning",
            "content": "To enhance the agents ability to handle long-horizon tasks, planning module prior to execution is usually adopted. Planning can be regarded as continuous task decomposition process [23]. To generally evaluate the impact of the planning module on efficiency, we adopt simple and universal design. Specifically, the agent is prompted to generate an explicit plan before taking any action. It then follows this plan step by step in ReAct [37] style. To allow for adaptability in dynamic environments, the plan is periodically revised: after every steps, the agent re-generates the plan based on the current context. The specific prompt used for planning is provided in Appendix B. In our experiments, we vary the maximum number of ReAct steps allowed totally, choosing from 4, 8, 12. Additionally, we control the frequency of plan updates by setting the planning interval to 1, 2, 4. The detailed results of these variations are summarized in Table 3. Table 3 Results on different planning methods of agents. In the upper part, we keep the planning interval fixed at 1 and adjust the maximum steps. In the lower part, we keep the maximum steps fixed at 12 and adjust the planning interval."
        },
        {
            "title": "Max Steps Plan Interval",
            "content": "12 8 4 12 12 12 all 0.98 0.70 0.48 0.98 1.04 1.01 1 1 1"
        },
        {
            "title": "Efficiency",
            "content": "cost-of-pass"
        },
        {
            "title": "Effectiveness",
            "content": "Acc. l1 l2 l3 all l2 l3 all Cost/$ l2 l"
        },
        {
            "title": "Cost",
            "content": "l3 all #Tokens l2 l1 l3 0.32 0.29 0.29 0.32 0.46 0. 1.07 0.82 0.51 1.07 1.15 0.97 3.51 2.11 2.05 3.51 3.16 3.91 53.33 52.73 41.82 53.33 57.58 53. 69.81 69.81 58.49 69.81 71.70 66.04 50.00 48.84 39.53 50.00 56.98 52.33 30.77 30.77 15.38 30.77 30.77 30. 0.705 0.550 0.199 0.705 0.834 0.772 0.367 0.313 0.167 0.367 0.458 0.389 0.710 0.591 0.200 0.710 0.875 0. 1.380 0.939 0.316 1.380 1.477 1.710 243K 103K 249K 506K 171K 91K 185K 300K 89K 141K 88K 73K 243K 103K 249K 506K 272K 146K 301K 442K 249K 118K 235K 563K Increasing the maximum number of steps within certain range significantly improves performance. For instance, when the maximum steps increase from 4 to 8, the accuracy rises from 58.49% to 69.81%. However, this also leads to substantial increase in cost-of-pass (from 0.48 to 0.70). Beyond certain threshold, further increasing the maximum steps does not enhance performance but continues to increase costs."
        },
        {
            "title": "Planning Complexity Control for Efficiency Optimization",
            "content": "Current models struggle with reasoning length regulation, often exhibiting overthinking that inflates costs when problems are insoluble. Moderate planning complexity significantly enhances efficiency."
        },
        {
            "title": "3.4 Tool Using",
            "content": "Incorporating external tools significantly enhances the agents capabilities, especially in scenarios where neural networks alone fall short [24]. However, this also introduces additional overhead. In this work, we focus primarily on the effectiveness and efficiency of using of web browser [38] for two reasons: (1) it represents widely adopted and general-purpose tool that enables agents to access real-time, up-to-date information across diverse domains, which is helpful for addressing wide range of problems; (2) the use of browsing may have significant impact on efficiency. Web pages often contain large amounts of text, multimedia, and interactive content, leading to high token consumption during content retrieval and processing. This is especially true considering that effective use of the browser may involve navigating multiple web pages. Therefore, we place particular emphasis on the efficiency and effectiveness of the browser use in our analysis. In our experiments, we control for several factors related to browsing. These include: Source of Web Content: We evaluate the impact of sources by comparing simple setting comprising only Google and Wikipedia against complex setting that includes Google, Wikipedia, Bing, Baidu, and DuckDuckGo. Web Page Processing Strategy: We design three distinct approaches: (a) crawler that retrieves only static elements, (b) browser with basic processing, and (c) browser with advanced operations, such as page-up and page-down interactions. Number of Query Expansion: the original user query is reformulated by the LLM to obtain broader and more informative set of results, with expansion numbers to {3, 5, 10}. The results are presented in Table 4. Table 4 Efficiency and effectiveness on different settings of tool using. We evaluate on different design on searching sources, browsing tools and the number of query rewritten. Source Tool Search Num Simple Multi Crawler Crawler Simple Browser-Complex Browser-Simple Simple Simple Simple Crawler Crawler 10 10 10 5 3 Efficiency cost-of-pass Effectiveness Acc. l2 l3 all l1 l2 all Cost/$ l2 l1 Cost l3 all #Tokens l2 l3 0.53 0.35 0.42 0.64 0.49 0.61 1.42 0.92 0.85 1. 1.30 1.33 4.49 2.43 4.18 5.60 3.22 3.97 53.33 59.39 49.09 58. 53.33 49.09 69.81 73.58 62.26 73.58 66.04 60.38 50.00 59.30 50.00 56. 51.16 47.67 30.77 30.77 19.23 30.77 34.62 30.77 0.705 0.479 0.431 0. 0.626 0.641 0.367 0.258 0.259 0.468 0.322 0.366 0.710 0.545 0.424 0. 0.667 0.635 1.380 0.749 0.804 1.724 1.114 1.220 243K 103K 249K 506K 225K 118K 257K 353K 199K 118K 196K 374K 327K 155K 350K 636K 212K 101K 229K 385K 215K 114K 219K 405K all 1.32 0.81 0.88 1.59 1.17 1.31 Increasing the number of search sources significantly enhances both effectiveness and efficiency. Specifically, the cost-of-pass decreases from 1.32 to 0.81, while accuracy improves from 53.33% to 59.39%. Simpler browser operations, such as retrieving static elements or basic processing, outperform advanced operations (e.g., page-up and page-down interactions) in both effectiveness and efficiency, indicating that minimal processing strategies can achieve robust performance with lower computational overhead. Expanding the number of reformulated queries (from 3 to 10) consistently improves both effectiveness and efficiency, as broader query sets enable the retrieval of more relevant and informative results."
        },
        {
            "title": "Tool Configurations Significantly Impact Efficiency and Effectiveness",
            "content": "Varying tool configurations, such as increasing search sources, simplifying browser operations, and expanding reformulated queries for web searching, demonstrably enhance both effectiveness and efficiency in information retrieval."
        },
        {
            "title": "3.5 Memory",
            "content": "Memory is critical component for LLM-driven agent systems, enabling effective interaction with and learning from dynamic environments [25, 39]. It supports key functionalities such as experience accumulation and knowledge abstraction as the reasoning, while memory module also introduces extra cost. In our experiments, we design six memory configurations to evaluate their impact on effectiveness and efficiency to the whole system. Details of the prompts for the memory are provided in B. Simple Memory: Only historical observations and action are kept in the context window for short context. Summarized Memory: At each step, all informationincluding observations, reasoning, and actionsis summarized by an LLM and embedded. These embeddings are stored in vector database and retrieved based on cosine similarity, then concatenated into the prompt each step as an memory to replace the history of each step for short context. w/o Extra Memory: Only the history of every step is kept in the context window while no extra memory are leveraged. Extra Summarized Memory: The memory is summarized exactly the same as Summarized Memory while concatenated into the prompt each step as an extra memory alongside the existing step history. Extra Fixed Memory: piece of text with maxinum length is maintained and concatenated into the prompt at each step as long-term memory. It is initially generated by an LLM at the first step and subsequently updated by the LLM after every step. Extra Hybrid Memory: Concatenate both summarized and long memory approaches at each step to maintain more information. We evaluated the effectiveness and efficiency by controlling different memory designs while keeping other settings the same, with results shown in Table 5. Table 5 The impact on different design of Memory module on effectiveness and efficiency."
        },
        {
            "title": "Efficiency",
            "content": "cost-of-pass"
        },
        {
            "title": "Effectiveness",
            "content": "Acc. l1 l2 l3 all l2 l3 all Cost/$ l2 l"
        },
        {
            "title": "Cost",
            "content": "l3 all #Tokens l2 l1 l3 simple summarized w/o extra extra summarized extra fixed extra hybrid all 0.74 1.52 0.98 1.08 1.04 1.29 0.46 0.87 0.32 0.60 0.52 0.68 0.64 1.43 1.07 1.05 1.00 1.32 1.28 2.01 3.51 2.99 3.53 5.29 56.36 51.52 53.33 52.73 53.94 54.55 66.04 66.04 69.81 60.38 64.15 75. 56.98 48.84 50.00 53.49 54.65 51.16 34.62 30.77 30.77 34.62 30.77 23.08 0.419 0.782 0.521 0.567 0.561 0.703 0.258 0.449 0.227 0.364 0.331 0.512 0.426 0.942 0.533 0.561 0.544 0.677 0.727 0.983 1.080 1.036 1.088 1. 194K 117K 196K 338K 367K 226K 437K 441K 243K 103K 249K 506K 236K 146K 234K 442K 240K 135K 234K 472K 259K 176K 253K 462K Among the six memory configurations, Simple Memory, which retains only the agents observations and actions, minimizes the context window size, resulting in the lowest computational cost. Surprisingly, this configuration also yields the best performance, improving from 53.33% (No Extra Memory baseline) to 56.36%, while reducing the cost-of-pass from 0.98 to 0.74. In contrast, Summarized Memory, which also aims to shorten the context window, incurs the highest token consumption and computational cost. This may be due to its inability to accurately summarize past historical trajectories, requiring the model to make additional attempts to solve tasks. (3) Additional memory designs, which augment the existing step history, provide marginal performance improvements but are outperformed by the Simple Memory configuration."
        },
        {
            "title": "Simple Memory is Enough",
            "content": "The Simple Memory design, retaining only the agents observations and actions, is sufficient to achieve both effectiveness and efficiency."
        },
        {
            "title": "3.6 Holistic Analysis of Component Impacts on Agent System",
            "content": "In this section, we adopt global perspective to examine how various components of the agent system influence its effectiveness and efficiency. Our analysis reveals that the choice of backbone exerts the most significant impact on the overall system performance. Additionally, the maximum number of steps an agent can execute and the usage of tools also play critical roles in determining performance. In contrast, the design of BoN and memory mechanisms has negligible effects on the models effectiveness. However, redundant designs in these components may lead to increased computational costs."
        },
        {
            "title": "4 Efficient Agents : Tricks of the Trade",
            "content": "Table 6 The Configuration of Efficient Agents . The choice of each component is conducted by the observation from the previous empirical studies."
        },
        {
            "title": "Settings",
            "content": "Backbone Max Step GPT-4.1 8 Plan Interval"
        },
        {
            "title": "Search Num BoN Memory\nSimple",
            "content": "5 1 In this section, we propose Efficient Agents , an agent system comprising carefully selected components to achieve great trade-off between effectiveness and efficiency. We demonstrate that by tuning the configuration based on empirical studies and selecting components that achieve favorable trade-off between efficiency and effectiveness, the resulting agent system can maintain performance while significantly reducing costs. Specifically, for each component in the agent system, we adopt the configuration with the lowest cost-of-pass among those that do not lead to substantial performance degradation. The detailed configurations are provided in Table 6. Table 7 Results on Different Agents."
        },
        {
            "title": "OWL\nSmolagents",
            "content": "Efficient Agent (ours) all 0.75 5.82 0."
        },
        {
            "title": "Efficiency",
            "content": "cost-of-pass"
        },
        {
            "title": "Effectiveness",
            "content": "Acc. l1 l2 l3 all l2 l3 all Cost/$ l2 l"
        },
        {
            "title": "Cost",
            "content": "l3 all #Tokens l2 l1 l3 0.35 3.21 0.37 0.80 6.46 0. 2.10 13.37 1.71 53.33 53.33 51.52 71.70 62.26 62.26 50.00 54.65 52.33 26.92 30.77 26.92 0.398 3.104 0. 0.248 2.000 0.228 0.402 3.528 0.280 0.566 4.115 0.461 189K 119K 204K 281K 146K 88K 172K 183K 127K 101K 125K 206K For comparison, we conduct comparative analysis of some popular open-source agent systems, including OWL [21] and SmolAgent [40]. These systems offer diverse designs in terms of planning, memory, and tool-use integration, representing some of the most actively developed agent systems in the community. The results are listed in Table 7. By evaluating these frameworks under GAIA benchmark, we find that our Efficient Agents achieves cost reduction of 28.4% while maintaining comparable performance."
        },
        {
            "title": "5.1 LLM-driven Agents",
            "content": "LLM-based agent technologies have demonstrated remarkable capabilities across wide range of tasks, significantly spurring the rapid advancement of general agent systems. In recent years, increasing research efforts have been dedicated to building general agent systems capable of tackling complex reasoning, planning, and search tasks, with the aim of enhancing their adaptability and automation capabilities in real-world scenarios. By leveraging LLMs strengths in context understanding, knowledge integration, and tool use, these systems have proven successful on multiple public benchmarks [19, 41], underscoring their immense potential for building general, versatile, and multi-agent collaborative systems. For instance, OpenAIs Deep Research agent achieved an average score of 67.36% on the GAIA (General AI Assistants) benchmark [19], significantly outperforming traditional LLM-only approaches. Within the open-source community, the OWL (Optimized Workforce Learning), scored an impressive average of 69.7% 8 on the GAIA benchmark [21], achieving state-of-the-art performance in the open-source domain. These compelling results on demanding benchmarks like GAIA clearly demonstrate the significant potential of LLM-based agent systems for handling intricate tasks that require sophisticated reasoning, planning, and effective tool utilization."
        },
        {
            "title": "5.2 Efficient NLP",
            "content": "Since the advent of BERT [11], the scale of language models has grown exponentially, leading to substantial increases in computational and energy costs during inference. To address this, significant body of research has focused on enhancing NLP efficiency [4249]. For instance, DistilBERT [15] leverages knowledge distillation to create compact model from BERT, maintaining strong performance across NLP tasks with reduced size and faster inference. More recently, as large reasoning models advent, one line of inquiry explores methods to control model output length by estimating the likely token requirements for given task, thereby promoting efficient generation. For example, Token-Budget-Aware LLM Reasoning[50] introduces the concept of token budget. By incorporating reasonable token budget into the prompt, this approach dynamically estimates the inference complexity for different problems, guiding the reasoning process to significantly reduce token consumption with only marginal performance trade-off. In the domain of general intelligent agent systems, the collaboration of multiple LLM agents, while powerful for complex tasks, often introduces inefficiencies such as communication redundancy and resource wastage. To mitigate these, strategies like AgentPrune [51] focus on optimizing communication by pruning superfluous messages from spatio-temporal communication graph. Complementing these efforts to improve multi-agent efficiency, other research, such as that leading to BudgetMLAgent [52] explores the use of tiered model architectures that strategically combine lower-cost and high-performance LLMs to achieve cost-effective systems. While existing research has achieved significant results in communication topology optimization and overall cost control, detailed analysis of the efficiency contribution and cost impact of individual modules within general agent systems remains relatively unexplored. Our work focuses on deeper investigation into the factors influencing cost contributed by each specific module in general agent systems."
        },
        {
            "title": "6 Conclusion",
            "content": "Building upon our investigation into the efficiency-effectiveness trade-off in LLM-driven agents, this paper makes the following key contributions. First, we provide comprehensive analysis of the architectural choices and operational factors that contribute to the substantial economic overhead observed in contemporary agent systems. This analysis pinpoints specific areas where inefficiency commonly arises, laying the groundwork for more cost-conscious design. Second, guided by these insights, we introduce Efficient Agents , novel agent framework engineered for an optimal balance between task performance and computational cost. Through careful selection and integration of its components, Efficient Agents dynamically adapts its complexity to the demands of the task at hand. Our extensive experiments on the challenging GAIA benchmark demonstrate the efficacy of our approach. Specifically, Efficient Agents achieves 96.7% of the state-of-the-art performance of OWL while drastically reducing the operational cost by xx times, resulting in significant 28.4% improvement in the cost-of-pass metric. This work underscores the critical importance of efficiency considerations in the design of next-generation agent systems and offers practical pathway towards more scalable and economically viable real-world deployments. We believe our findings will spur further research into task-adaptive and resource-aware agent architectures, paving the way for more widespread adoption of these powerful AI technologies."
        },
        {
            "title": "Core Contributors",
            "content": "Ningning Wang Xavier Hu"
        },
        {
            "title": "Contributors",
            "content": "Pai Liu Yue Hou Heyuan Huang Shengyu Zhang Jian Yang Jiaheng Liu Ge Zhang Changwang Zhang Jun Wang Yuchen Eleanor Jiang"
        },
        {
            "title": "Corresponding Authors",
            "content": "He Zhu Wangchunshu Zhou"
        },
        {
            "title": "References",
            "content": "[1] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. URL https://arxiv.org/abs/2308.08155. [2] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents. 2023. URL https://arxiv.org/abs/2309.07870. [3] Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. 2024. URL https://arxiv.org/abs/2406.18532. [4] Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023. URL https: //arxiv.org/abs/2305.13304. [5] Significant Gravitas. Autogpt, 2025. URL https://github.com/Significant-Gravitas/AutoGPT. Accessed: 2025-05-13. [6] He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, and Wangchunshu Zhou. Oagents: An empirical study of building effective agents, 2025. URL https://arxiv.org/abs/2506.15741. [7] King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchunshu Zhou. Scaling test-time compute for llm agents, 2025. URL https://arxiv.org/abs/2506.12928. [8] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. Infiagent-dabench: Evaluating agents on data analysis tasks, 2024. URL https://arxiv.org/abs/2401.05507. [9] Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for computer, phone and browser use, 2024. [10] Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Taskcraft: Automated generation of agentic tasks, 2025. URL https://arxiv.org/abs/ 2506.10055. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv.org/abs/1810.04805. [12] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [13] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https: //arxiv.org/abs/2001.08361. [14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. [15] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter, 2020. URL https://arxiv.org/abs/1910.01108. [16] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung 11 Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412.08905. [17] OpenAI. Deep research, 2025. URL https://openai.com/index/deep-research-system-card/. Accessed: 2025-05-13. [18] Monica.ai. manus, 2025. URL https://manus.im/app. Accessed: 2025-05-13. [19] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants, 2023. URL https://arxiv.org/abs/2311.12983. [20] Mehmet Hamza Erol, Batu El, Mirac Suzgun, Mert Yuksekgonul, and James Zou. Cost-of-pass: An economic framework for evaluating language models, 2025. URL https://arxiv.org/abs/2504.13359. [21] CAMEL-AI.org. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. https://github.com/camel-ai/owl, 2025. Accessed: 2025-03-07. [22] Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. Reasoning in token economies: Budget-aware evaluation of llm reasoning strategies. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1991619939, 2024. [23] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey, 2024. URL https://arxiv.org/abs/ 2402.02716. [24] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2024. URL https://arxiv.org/abs/2304.08354. [25] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. survey on the memory mechanism of large language model based agents, 2024. URL https: //arxiv.org/abs/2404.13501. [26] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314. [27] Mehmet Hamza Erol, Batu El, Mirac Suzgun, Mert Yuksekgonul, and James Zou. Cost-of-pass: An economic framework for evaluating language models. arXiv preprint arXiv:2504.13359, 2025. [28] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [29] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [30] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [31] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. URL https://arxiv.org/abs/2412.21187. [32] OpenAI. GPT-4.1: An Advanced Multimodal AI Model, 2025. Developed by OpenAI, available at https: //openai.com. [33] Anthropic. Claude 3.7 sonnet. https://www.anthropic.com/news/claude-3-7-sonnet, February 2025. Accessed: 2025-05-11. [34] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 12 [35] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. [36] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https: //arxiv.org/abs/2407.21787. [37] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.org/abs/2210.03629. [38] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, 2022. URL https://arxiv.org/abs/2112.09332. [39] OpenAI. Memory and new controls for ChatGPT, February 2025. URL https://openai.com/index/ memory-and-new-controls-for-chatgpt/. Accessed: 2025-05-14. [40] Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. smolagents: smol library to build great agentic systems. https://github.com/huggingface/smolagents, 2025. [41] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/abs/2504.12516. [42] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. BERT-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 78597869, Online, November 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-main.633. [43] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast In Advances in Neural Information Processing Systems, volume 33, and robust inference with early exit. pages 1833018341. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ d4dd111a4fd973394238aca5c05bebe3-Paper.pdf. [44] Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, and Furu Wei. Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1065310659, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.832. URL https://aclanthology.org/2021. emnlp-main.832/. [45] Wangchunshu Zhou, Canwen Xu, and Julian McAuley. BERT learns to teach: Knowledge distillation with meta learning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70377049, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.485. URL https://aclanthology.org/2022.acl-long.485/. [46] Wangchunshu Zhou, Ronan Le Bras, and Yejin Choi. Modular transformers: Compressing transformers into modularized layers for flexible efficient inference. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1045210465, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.664. URL https://aclanthology.org/2023.findings-acl.664/. [47] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. Efficient prompting via dynamic in-context learning, 2023. URL https://arxiv.org/abs/2305.11170. [48] Tiannan Wang, Wangchunshu Zhou, Yan Zeng, and Xinsong Zhang. EfficientVLM: Fast and accurate visionlanguage models via knowledge distillation and modal-adaptive pruning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1389913913, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.873. URL https://aclanthology.org/2023.findings-acl.873/. 13 [49] Xueyan Zhang, Jinman Zhao, Zhifei Yang, Yibo Zhong, Shuhao Guan, Linbo Cao, and Yining Wang. Uora: Uniform orthogonal reinitialization adaptation in parameter-efficient fine-tuning of large models. 2025. URL https://arxiv.org/abs/2505.20154. [50] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [51] Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. arXiv preprint arXiv:2410.02506, 2024. [52] Shubham Gandhi, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff. Budgetmlagent: cost-effective llm multi-agent system for automating machine learning tasks, 2025. URL https://arxiv.org/abs/2411.07464."
        },
        {
            "title": "A Default Setup",
            "content": "Table 8 details the default setup for conducting the experiment. Table 8 The default setup for the experiment."
        },
        {
            "title": "Settings",
            "content": "Backbone Max Step GPT-4.1 12 Plan Interval"
        },
        {
            "title": "B Prompt",
            "content": "B.1 Memory"
        },
        {
            "title": "Search Num BoN Memory\nSimple",
            "content": "10 1 You are an expert in agent memory management, specializing in leveraging the Memory Summarization, the Memory Retrieval, and the Long-term Memory to boost agent reasoning. Memory Summarization: Summarize the following text which is the execution content of the agent at the current step: {memory of current step}. Highlight the key points to assist the agent in better reasoning during subsequent steps. Additionally, you must provide optimization suggestions for the next step. Long-term Memory: Here is the agents execution content from the previous step: {memory of previous step}. Here is the long-term memory formed by summarizing the agents historical execution content: {long term memory}. Please combine the agents previous execution content and the existing long-term memory, summarize them while highlighting the key points, and form new long-term memory to help the agent reason better in subsequent steps. Input: Agents execution content at current step: {memory of current step}. Agents execution content at previous step: {memory of previous step}. Agents historical execution content: {long term memory}. Output: Memory Summarization: point-by-point summary of agents current execution step and optimization suggestions. Memory Retrieval: The retrieval of the most relevant historical steps. Long-term Memory: An ongoing updated memory for recording the agents long-term historical steps. 15 B.2 Test-Time Scaling PRM-score Evaluation Prompt Evaluation Guidelines: Objective: You will evaluate candidate ActionStep node, which includes the following fields: step_number: Depth of this step within the TTS search tree. observations: Observations recorded after executing this action. action_output: Direct output resulting from this action. model_output: Raw LLM output that led to this action. error: Any encountered errors (can be None). score: Previously assigned score (for reference only). previous_steps: The history of earlier steps, including TaskStep and PlanningStep, along with the trajectory of ActionSteps leading to the current state. Your goal is to judge how promising this ActionStep is for advancing toward the users task, using your independent judgment while considering the continuity and logical flow of the ActionStep sequence, including the historical context. Evaluation Criteria: Progress Toward Goal: Assess whether the action_output clearly and tangibly advances the overall task. Reward meaningful progress or valuable new information. Penalize irrelevant actions or weak impact. Error and Stability: Penalize based on the severity of errors: Fatal/blocking errors: 0-1 points. Significant errors: 1-3 points. Minor or recoverable errors: 3-5 points. Reduce the score if the model_output is ambiguous or unstable. TTS Efficiency: Reward actions that contribute efficiently toward reaching the goal. Penalize redundant or repetitive actions without meaningful progress. Reflection Usage: Reward active utilization of reflection to improve upon past mistakes. Penalize ignoring reflection insights. Loop Detection: Detect loops or repetitions compared to previous steps. Identify true loops and penalize based on severity. Contextual Awareness: Infer alignment with previous PlanningStep and TaskStep. Ensure consistency with the TTS strategy and penalize deviations. Scoring Criteria: 9-10: Clearly advances the goal; highly efficient; strong reflection use; no loops. 7-8: Good advancement; minor inefficiencies; clear reflection use; minimal loop risk. 5-6: Moderate progress; limited efficiency; moderate reflection use; mild repetition risks. 3-4: Poor advancement; inefficient; weak reflection use; noticeable loop risks. 1-2: Minimal advancement; repetitive actions; true loops; significant errors. 0: Severe issues: explicit loops, critical errors, or complete irrelevance to the task context. Final Evaluation Output: You must provide your evaluation in valid JSON format with the following structure: { \"analysis\": \"Detailed analysis addressing progress, TTS efficiency, reflection usage, loop detection, contextual alignment with PlanningStep/TaskStep, error severity, and overall action quality.\", \"score\": [integer between 0-10] } 16 PRM-list Evaluation Prompt Evaluation Guidelines: Objective: You will evaluate candidate trajectories, each representing series of nodes in search tree. Each trajectory contains the following: step_number: Depth of the node in the trajectory. observations: Observations recorded at each step of the trajectory. action_output: Direct action output at each step. model_output: Raw model output (LLM). error: Any errors encountered (can be None). score: Previously calculated score (if available). previous_steps: The history of earlier steps, including TaskStep and PlanningStep, with the trajectory of ActionSteps leading to the current state. Your goal is to evaluate each trajectory holistically, considering how well it progresses toward solving the users task. Select the trajectory that most effectively achieves this goal. Evaluation Criteria: Progress Toward Goal: Assess how well each trajectory advances the task at hand, considering both the individual nodes progress and the overall progression of the entire trajectory. Reward trajectories that demonstrate tangible and meaningful progress toward the goal. Penalize trajectories with weak actions or minimal/no advancement. Trajectory Efficiency: Evaluate how efficiently each trajectory progresses toward the goal, considering the depth and complexity of the steps. Favor trajectories that achieve significant progress with fewer steps. Consider the overall value-to-depth ratio when comparing trajectories of different lengths. Reward efficient exploration of the search space. Loop Detection: Detect loops or repetitions within each trajectory, especially those related to previous steps. Loop types: Real Loops: Identical nodes (observations, action output, and model output) that do not add value to the trajectory. Benign Repetitions: Similar strategies with variations yielding additional progress. Heavily penalize trajectories with real loops. Slight penalties for benign repetitions if they lead to meaningful improvements. Error and Stability: Evaluate the severity of errors encountered in each trajectory and penalize based on their impact on progression. Error Severity: Fatal/Blocking Errors: Major penalty. Significant Errors: Moderate penalty. Minor/Recoverable Issues: Minor penalty. Penalize unstable or unclear model outputs. Consider how errors affect the overall trajectorys ability to move toward the goal. Overall Trajectory Quality: Evaluate the coherence and overall quality of the trajectory. Consider the logical sequence of steps and the exploration-exploitation balance. Evaluate the final nodes closeness to achieving the goal. Reward trajectories that make consistent progress and demonstrate coherent planning. Final Output Format: Provide your evaluation in the following JSON format. Select the best trajectory and provide detailed analysis explaining why it is the most promising trajectory. { \"index\": [integer], # Index of the best trajectory \"analysis\": \"Detailed analysis addressing progress, efficiency, reflection usage, loop detection, error severity, and overall trajectory quality.\" }"
        }
    ],
    "affiliations": [
        "OPPO",
        "OPPO-PersonalAI"
    ]
}