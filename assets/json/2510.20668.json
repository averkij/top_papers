{
    "paper_title": "From Masks to Worlds: A Hitchhiker's Guide to World Models",
    "authors": [
        "Jinbin Bai",
        "Yu Lei",
        "Hecong Wu",
        "Yuchen Zhu",
        "Shufan Li",
        "Yi Xin",
        "Xiangtai Li",
        "Molei Tao",
        "Aditya Grover",
        "Ming-Hsuan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This is not a typical survey of world models; it is a guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned a ``world model\". Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share a single paradigm, then to interactive generative models that close the action-perception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 6 6 0 2 . 0 1 5 2 : r Preprint FROM MASKS TO WORLDS: HITCHHIKERS GUIDE TO WORLD MODELS Jinbin Bai1, Yu Lei1, Hecong Wu1, Yuchen Zhu1,2, Shufan Li3, Yi Xin1, Xiangtai Li1, Molei Tao2, Aditya Grover3, Ming-Hsuan Yang4 1MeissonFlow Research 2Georgia Tech 3UCLA 4UC Merced"
        },
        {
            "title": "ABSTRACT",
            "content": "This is not typical survey of world models; it is guide for those who want to build worlds. We do not aim to catalog every paper that has ever mentioned world model. Instead, we follow one clear road: from early masked models that unified representation learning across modalities, to unified architectures that share single paradigm, then to interactive generative models that close the actionperception loop, and finally to memory-augmented systems that sustain consistent worlds over time. We bypass loosely related branches to focus on the core: the generative heart, the interactive loop, and the memory system. We show that this is the most promising path towards true world models. 1 INTRODUCTION: THE NARROW ROAD TO WORLD MODELS The term world model has been used to describe many different ideas: learned environment simulators for reinforcement learning (Ha & Schmidhuber, 2018; Hafner et al., 2019), agents that integrate learned models with planning (Schrittwieser et al., 2020), and large language models that simulate entire societies (Park et al., 2023). Yet despite hundreds of related works, there is no clear consensus on how to actually build true world model. In this paper, we take stance: the path is much narrower than it appears. true world model is not monolithic entity but system synthesized from three core subsystems: generative heart that produces world states, an interactive loop that closes the action-perception cycle in real time, and persistent memory system that sustains coherence over long horizons. The history of the field can be understood as an evolutionary journey from first mastering these components in isolation to now integrating them. Most works focus on optimizing narrow tasks and drift away from the generative, interactive, and persistent nature required for true world model. To make this perspective concrete, we chart the historical evolution of world models as sequence of five stages, shown in Figure 1. It begins with Stage I: Mask-based Models, which established universal, token-based pretraining paradigm across modalities. This foundation enabled Stage II: Unified Models, where single architecture learns to process and generate multiple modalities. The focus then shifts to closing the interactive loop in Stage III: Interactive Generative Models, transforming static generators into real-time simulators. To sustain these simulations over time, Stage IV: Memory and Consistency introduces mechanisms for durable and coherent state representation. Table 1 also summarizes representative models or methods across the four stages. This progression culminates in Stage V: True World Models. This stage is not defined by adding new component, but by the synthesis of the preceding stages into an autonomous whole. At this threshold, models begin to exhibit the defining properties of persistence, agency, and emergence, moving from engines of prediction to living worlds. By analyzing each stages key innovations and unsolved challenges, this paper offers clear and opinionated roadmap from todays components to tomorrows living worlds. (cid:66): jinbin.bai@u.nus.edu 1 Preprint Figure 1: The evolution of world models across five stages."
        },
        {
            "title": "2 WHAT IS A WORLD MODEL?",
            "content": "2.1 HISTORICAL AND CONTEMPORARY PERSPECTIVES The concept of world model originated in reinforcement learning, where Ha and Schmidhuber (Ha & Schmidhuber, 2018) first proposed learning latent dynamics simulator for agent planning. This control-oriented view was advanced by systems like Dreamer (Hafner et al., 2019), which learned policies purely through latent imagination, and MuZero (Schrittwieser et al., 2020), which integrated tree-based planning with learned, abstract model. In parallel, the rise of large-scale generative modeling broadened this definition. With generative agents (Park et al., 2023) and large multimodal systems (Reed et al., 2022), the concept evolved from predictive simulator for an agent to rich, generative system capable of an entire interactive world. This has led to the contemporary view of world simulator, term that now informally encompasses three major paradigms: explicit 3D scene generators (World Labs, 2024), passive video generators that go beyond pixels to approximate physical dynamics (Brooks et al., 2024), and interactive games and environments for agents, whether text-based (Niesz & Holland, 1984) or video-based, as exemplified by the Genie series (Bruce et al., 2024; Parker-Holder et al., 2024; Ball et al., 2025). 2.2 THE ANATOMY OF TRUE WORLD MODEL To bring clarity to these diverse threads, we define true world model as the three essential subsystems it must integrate, which, in turn, enable the core properties that define each stage of our evolutionary roadmap. Figure 2 presents the high-level architecture of true world model, showing how the generative, interactive, and memory subsystems integrate. Figure 2: The architecture of true world model. The Generative Heart (G). The foundation of world model is its generative heart: learned model of the worlds dynamics and appearance, formally described by the generative process pθ. It 2 Preprint Table 1: Representative models or methods along the narrow road to world models. Stage I: Mask-based Models BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) Gemini Diffusion (DeepMind, 2025) Bidirectional masked prediction for representation learning in language. Dynamic masking and scale without next-sentence prediction strengthen BERT. Reported iterative denoising paradigm at commercial scale for generative language tasks. BEiT (Bao et al., 2021) MAE (He et al., 2022a) MaskGIT (Chang et al., 2022) Meissonic (Bai et al., 2024) Image patch masking for representation learning in vision. High-ratio patch masking with lightweight decoder yields strong visual representations. Non-autoregressive parallel masked tokens infilling for efficient image synthesis. Masked generative transformers achieving high fidelity text-to-image generation. wav2vec 2.0 (Baevski et al., 2020) Audio latent features masking for representation learning in speech. Stage II: Unified Models EMU3 (Wang et al., 2024) Chameleon (Chameleon Team, 2024) VILA-U (Wu et al., 2024) Janus-Pro (Chen et al., 2025) MMaDA (Yang et al., 2025) Lavida-O (Li et al., 2025b) Lumina-DiMOO (Xin et al., 2025) AR-based unified models with single Transformer for text, image and video. AR-based unified models with single Transformer for text and image. Language-prior AR-based unified models for text, image and video. Language-prior AR-based unified models for text and image. Language-prior mask-based (discrete-style denoising) unified models for text and image. Language-prior mask-based (discrete-style denoising) unified models for text and image. Language-prior mask-based (discrete-style denoising) unified models for text and image. UniDiffuser (Bao et al., 2023) Muddit (Shi et al., 2025) Visual-prior diffusion-based unified models for text and image. Visual-prior mask-based (discrete-style denoising) unified models for text and image. UniDisc (Swerdlow et al., 2025) Mask-based (discrete-style denoising) unified models. Gemini (Comanici et al., 2025) GPT-4o (Hurst et al., 2024) Googles multimodal model in single system (but not in single paradigm). OpenAIs multimodal model in single system (but not in single paradigm). TextWorld (Cˆote et al., 2018) AI Dungeon (Latitude, 2024) PVG (Menapace et al., 2021) PE (Menapace et al., 2022) PGM (Menapace et al., 2024) GameGAN (Kim et al., 2020) Genie-1 (Bruce et al., 2024) Oasis (Decart et al., 2024) GameNGen (Valevski et al., 2024) Genie-2 (Parker-Holder et al., 2024) Genie-3 (Ball et al., 2025) Mineworld (Guo et al., 2025) Matrix-Game-2 (He et al., 2025) Stage III: Interactive Generative Models Parser-based text game environments. LLM-driven co-authored narrative with open-ended branching stories. Stepwise playable video game conditioned on user action selection. 3D playable environments conditioned on camera and multi-object control. Promptable game model conditioned on semantic-level language control. GAN-based next frame generation conditioned on actions for 2D games. MaskGIT-based next frame generation conditioned on actions for 2D worlds. Open-source Diffusion-based real-time generation conditioned on actions for 3D games. Diffusion-based real-time next frame generation conditioned on actions for 3D games. Diffusion-based generation conditioned on actions for 3D worlds initialized from images. Real-time generation conditioned on actions and promptable world events for 3D worlds. Open-source MaskGIT-based generation conditioned on actions for 3D games. Open-source diffusion-based real-time generation conditioned on actions for 3D games. World Labs (World Labs, 2024) Explorable 3D environments generation from single image using geometry and depth. Stage IV: Memory & Consistency RETRO (Borgeaud et al., 2022) MemGPT (Packer et al., 2023) Improving LMs by conditioning on document chunks retrieved from large corpus. OS-inspired virtual memory management framework for LLM workflows. Transformer-XL (Dai et al., 2019) Compressive Transformer (Rae et al., 2019) Mamba (Gu & Dao, 2023) Segment-level recurrence with relative positions for long-context sequence modeling. Extends Transformer-XL by downsampling old states to retain long-range dependencies. Selective state-space model with linear-time recurrence supporting near-infinite context. FramePack (Zhang & Agrawala, 2025) MoC (Cai et al., 2025) VMem (Li et al., 2025a) Packs long-frame histories into fixed context with inverted sampling to reduce drift. Learnable sparse attention routing that retrieves informative history chunks and anchors. Introduces surfel-indexed view memory using 3D surfels to enforce spatial coherence. must be able to predict future states, observations, and the task-relevant outcomes. (cid:16) = , pθ(γt zt, at) , pθ(rt zt, at) , pθ(ot zt) pθ(zt+1 zt, at) (cid:124) (cid:125) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:124) (cid:124) Discount/Termination Reward Observation Dynamics (cid:124) (cid:17) This subsystem, which models state transitions, observations, rewards, and terminations, serves as the foundation for the Generation property. 3 Preprint The Interactive Loop (F, C). To be more than passive movie generator, the model must support closed interactive loop. For partially observable worlds, it requires an inference filter (qϕ) for the agent to interpret observations in real-time, and policy (πη) for it to act upon its understanding of the world, often paired with value function (vω) to evaluate trajectories. : qϕ(zt ht1, ot) , (cid:124) (cid:123)(cid:122) (cid:125) State Inference (cid:16) = , vω(zt, ht) πη(at zt, ht) (cid:125) (cid:123)(cid:122) (cid:124) (cid:125) (cid:123)(cid:122) Value Policy (cid:124) (cid:17) This loop is what enables true Interaction and Real-time Adaptation. The Memory System (M). Finally, to ensure coherence over time, the model needs memory system that allows past events to inform the future. This is formally captured by recurrent state, ht, which is updated based on past memory, the current inferred state, and the last action. : ht = fψ(ht1, zt, at1) (cid:124) (cid:125) (cid:123)(cid:122) Memory Update This component is the basis for the property of Memory. detailed formalism of each component is provided in Appendix A. This definition clarifies why system like Unified Model (Stage II) is precursor rather than true world model. While it may possess powerful generative heart, it typically lacks the dedicated interactive loop and explicit memory system required to sustain persistent, agent-inhabited world."
        },
        {
            "title": "3 STAGE I: MASK-BASED MODELS ACROSS MODALITIES",
            "content": "The first stage in the evolution toward world models is the era of mask-based modeling, where system learns by reconstructing missing or corrupted parts of its input. This paradigm, which can be summarized as mask, infill, and generalize, has proven to be strikingly universal across modalities. It provides unified way of tokenizing, representing, and pretraining large models, establishing the foundation for all subsequent stages. 3.1 LANGUAGE MODALITY Masked language modeling (MLM) has played foundational role in modern natural language processing. BERT (Devlin et al., 2019) introduced bidirectional context prediction, in which 15% of tokens in each input are randomly replaced with [MASK] token and predicted from the surrounding context. SpanBERT (Joshi et al., 2020) refined this approach by masking contiguous spans rather than isolated tokens, improving extraction and reasoning tasks. Sequence-to-sequence variants such as MASS (Song et al., 2019), T5 (Raffel et al., 2020), and BART (Lewis et al., 2019) reformulated MLM as denoising autoencoding objective. ELECTRA (Clark et al., 2020) improved sample efficiency by replacing the MLM objective with discriminative replacement-detection task. Beyond fixed-ratio masking, line of non-autoregressive work introduces dynamic masking and unmasking through iterative refinement. RoBERTa (Liu et al., 2019) demonstrated that simply optimizing BERTs training recipe with more data and dynamic masking yielded significant gains. MaskPredict (Ghazvininejad et al., 2019) introduced iterative refinement, re-masking low-confidence tokens over several passes. This concept culminated in discrete diffusion models (Li et al., 2022; He et al., 2022b; Gong et al., 2022; Ou et al., 2024; Sahoo et al., 2024; Shi et al., 2024), which replace fixed masking with time-indexed noise schedule and train the model to iteratively denoise. As demonstrated by industrial systems like Mercury (Inception Labs et al., 2025) and Gemini Diffusion (DeepMind, 2025), this dynamic denoising paradigm has matured to rival or exceed autoregressive baselines in both quality and inference speed, solidifying the power of masking as core generative principle (Yu et al., 2025c; Li et al., 2025c). 3.2 VISION MODALITY The masked image modeling (MIM) paradigm extended this principle to perception. Early works established two main branches. For representation learning, BEiT (Bao et al., 2021) and, especially, 4 Preprint MAE (He et al., 2022a) created direct visual analogues of BERT, reconstructing masked tokens or patches to learn powerful features. This spurred family of related works exploring different reconstruction targets and self-distillation techniques (Xie et al., 2022; Zhou et al., 2021; Wei et al., 2022). For generative modeling, MaskGIT (Chang et al., 2022) and MUSE (Chang et al., 2023) pioneered the use of masked infilling for high-quality parallel image synthesis. This generative trajectory has recently culminated in models like Meissonic (Bai et al., 2024), demonstrating that masked generative transformers can achieve fidelity rivaling that of large diffusion models while offering superior efficiency and control. This mask-reconstruct-generalize principle scaled effectively to video. VideoMAE (Tong et al., 2022) and MaskFeat(Wei et al., 2022) showed that high-ratio tube masking was data-efficient method for learning spatiotemporal representations, confirming that masking could capture not just static scenes but also their dynamics. 3.3 OTHER MODALITIES The universality of the masking paradigm was confirmed by its rapid adoption in other fields. In audio, models like wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), WavLM (Chen et al., 2022), and Audio-MAE (Huang et al., 2022) applied masked prediction to latent speech representations. In 3D domains, Point-BERT (Yu et al., 2022) and Point-MAE (Pang et al., 2023) adapted masking to point clouds. The principle was even extended to structured data with models like GraphMAE (Hou et al., 2022). These successes reinforced the view of masking as cross-domain general approach to self-supervised learning. In summary, Stage established the principle of masking as universal foundation for representation learning. While this unified the pretraining paradigm, the models themselves remained specialized architectures. The inability of these separate models to form holistic worldview motivated Stage II: the pursuit of single, unified architecture."
        },
        {
            "title": "4 STAGE II: UNIFIED MODELS",
            "content": "Stage established universal paradigm for representation learning, but the models themselves remained specialists locked within their own modalities. Stage II takes the crucial next step: unifying the models themselves. We define unified model as system that processes and generates across different modalities with shared backbone and the same paradigm. By collapsing modality-specific pipelines, these models simplify scaling, enable powerful cross-modal transfer, and represent the first decisive synthesis on the path toward true world model. 4.1 REPRESENTATIVE WORKS Leading unified modeling efforts span several trajectories, distinguished by their foundational paradigm. We exclude simple glue models that stitch different paradigms for different modalities, such as using autoregression for text and diffusion for image, as well as models limited to text generation without extending to image generation or other modalities. Extending Language Model Pre-training: Language-Prior Modeling. The dominant trajectory has been to extend the paradigm of autoregressive large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Sun et al., 2024a). This began by connecting pre-trained vision encoders to frozen LLMs, as pioneered by BLIP-2 (Li et al., 2023) and popularized by LLaVA (Liu et al., 2023b; 2024), which was built upon LLaMA (Touvron et al., 2023). This approach was further pushed into grounded multimodal reasoning by Kosmos-2 (Peng et al., 2023) and into embodied reasoning by PaLM-E (Driess et al., 2023). More recently, systems like the EMU family (Sun et al., 2024b; Wang et al., 2024), Chameleon (Chameleon Team, 2024), VILA-U (Wu et al., 2024), and Janus-Pro (Chen et al., 2025) have advanced towards true end-to-end unified generation, creating both text and images within shared token space and unified autoregressive paradigm. In parallel, notable offshoot of this trend is rooted in mask-based language modeling. LLaDA (Nie et al., 2025) abandons the autoregressive framework and models text through masked diffusion process with 5 Preprint single Transformer. Its multimodal extension, MMaDA (Yang et al., 2025), introduces unified discrete diffusion architecture for text and image, mixed chain-of-thought fine-tuning strategy, and policy-gradient RL algorithm (UniGRPO) to unify reasoning and generation across modalities within single model. More recently, Lavida-O (Li et al., 2025b), OneFlow (Nguyen et al., 2025), and Lumina-DiMOO (Xin et al., 2025) have further improved overall performance and introduced new capabilities. Extending Vision Model Pre-training: Visual-Prior Modeling. parallel effort, grounded in vision-centric foundations, began along two paths. The first path built upon latent diffusion models, the foundation laid by Stable Diffusion (Rombach et al., 2022) was later generalized to unified, joint diffusion process over text and images in models like UniDiffuser (Bao et al., 2023) and Rojas et al. (2025). The second path is built upon the masked image modeling (MIM) paradigm, with models like Muddit (Shi et al., 2025) extending Meissonic (Bai et al., 2024) into unified discrete diffusion system that produces both images and captions within shared architecture and paradigm. Besides, UniDisc (Swerdlow et al., 2025) trained unified discrete-diffusion model from scratch for both language and vision modalities. Industrial-Scale Unified Systems. At production scale, Gemini (Comanici et al., 2025) and GPT4o (Hurst et al., 2024) unify language and vision modalities in single model, although not in single paradigm. These demonstrate that unified modeling has transcended research to become foundational industrial paradigm. 4.2 BENEFITS AND GAPS The primary benefit of Stage II is the reduction of fragmentation, leading to powerful cross-modal transfer and emergent capabilities. This paradigm now underpins productized multimodal interaction at scale, as demonstrated by industrial systems like Gemini (Comanici et al., 2025) and GPT-4o (Hurst et al., 2024). However, despite the impressive progress of language-prior unified models in interactive dialogue, visual-prior unified models for text-to-image and text-to-video remain limited to single-shot synthesis or stepwise editing. They lack the capacity for continuous, real-time closed-loop interaction. Thus, while Stage II unified architectures, the creation of truly dynamic and interactive worlds remains an open challenge and motivates Stage III."
        },
        {
            "title": "5 STAGE III: INTERACTIVE GENERATIVE MODELS",
            "content": "Here, models are no longer static predictors or one-shot generators, but participants in closed action-perception loop, sustaining interaction through low-latency response and action-conditioned evolution. We define interactive generative models as systems whose outputs are conditioned on streamed inputs or user actions, supported by internal state. We explore this evolution across three distinct domains: language-based, video-based, and scene-based. As mask-based interactive modeling remains underexplored, we take an architecture-agnostic view and summarize general interaction paradigms rather than mask-specific methods. 5.1 LANGUAGE-BASED WORLDS: INTERACTION AS NARRATIVE Classic interactive fiction (IF) (Niesz & Holland, 1984; Montfort, 2011; Ammanabrolu et al., 2020) established the paradigm of text-driven worlds where players interact through textual descriptions and actions. These took several forms: parser-based games where the player types text commands character by character, choice-based games where the player selects from set of predefined action options, and hypertext-based games where the player clicks on links embedded in the narrative. Choice-based visual novels, such as Memories Off (KID, 2004), exemplify emotionally branching narratives in which player decisions directly affect relationships and endings. These static worlds naturally evolved into benchmarks for artificial intelligence. significant line of research, supported by platforms such as TextWorld (Cˆote et al., 2018) and Jericho (Hausknecht et al., 2020), has focused on training agents to master them. In these settings, the world was fixed puzzle to be solved, and the locus of intelligence was the agent who navigated static world, not the world itself. fundamental shift occurred when large language models (LLMs) (Hurst et al., 2024; Comanici et al., 2025) themselves became the world engine. AI Dungeon (Latitude, 2024) pioneered this 6 Preprint transition, dynamically generating new narrative branches in response to free-form user prompts. Players could explore unbounded story spaces limited only by imagination and the models generative capacity. This marked the transition from solving pre-authored worlds to co-creating openended ones, envisioning future in which visual novels such as Memories Off (KID, 2004) could be generated interactively, offering unique storylines and relationships for each player. 5.2 VIDEO-BASED AND SCENE-BASED WORLDS: INTERACTION AS EXPERIENCE Interactive generation in video and spatial domains has progressed from offline frame prediction to real-time, controllable simulation. Early work on world models (Ha & Schmidhuber, 2018) used latent rollouts to dream trajectories for policy training, demonstrating the potential of closed-loop simulation. GameGAN (Kim et al., 2020) advanced this idea into neural game engine that renders successive frames from user input while implicitly learning game rules from observation. User control evolved from stepwise action selection in Playable Video Generation (PVG) (Menapace et al., 2021), through 3D scenes with camera and multi-object control in Playable Environments (PE) (Menapace et al., 2022), to natural language prompts in Promptable Game Models (PGM) (Menapace et al., 2024), which enabled the semantic-level direction of play. Building on these conceptual foundations, decisive trajectory emerged with the Genie series. Genie-1 (Bruce et al., 2024) learned latent action interfaces from Internet-scale videos to create controllable 2D environments. Genie-2 (Parker-Holder et al., 2024) extended this capability to larger, quasi-3D spaces, initialized from single image and playable via standard controls. Genie-3 (Ball et al., 2025) scaled further, producing real-time text-to-world experiences at 720p and 24 fps with minutes of coherent play, marked shift from passive video generation to active interaction. Community and industrial efforts soon followed. Systems such as Oasis (Decart et al., 2024), GameNGen (Valevski et al., 2024), Mineworld (Guo et al., 2025), and Matrix-Game (He et al., 2025) demonstrated real-time open environments with emergent physics and streaming diffusion. For comprehensive overview, see the survey by Yu et al. (2025b). Beyond frame synthesis, scene-based approaches emerged. World Labs (World Labs, 2024) proposed large world models that generate explorable 3D environments from single image, enabling interactive navigation through generated geometry and depth rather than sequential video. Taken together, these advances trace trajectory from offline video generators to real-time, actionconditioned world simulators. They ultimately transform generative models into engines of interactive human experiences. 5.3 CHALLENGES Despite the leap to real-time interaction, sustaining long-horizon consistency remains unsolved. Two paradigms illustrate the tension: explicit scene generators like NeRFs and Gaussian Splatting (e.g., World Labs) offer stable 3D navigation environments but depend on explicit spatial modeling; implicit frame-by-frame generators offer flexibility but are brittle, prone to losing context and hallucinating objects, especially over extended play. The Genie series highlights this tradeoff: from Genie-1s short 16-frame memory (Bruce et al., 2024), to Genie-2s object permanence (ParkerHolder et al., 2024), to Genie-3s few minutes of coherence (Ball et al., 2025), progress is clear yet far from persistence. At the object level, implicit video models rely on KV caches or control signals to maintain identity, while explicit 3D approaches embed spatial location directly but still struggle with dynamic elements, as explored in 4D Gaussian Splatting. These challenges reveal deeper gap: the reactive actionperception loop enables interaction, but without dedicated memory and state management, it cannot sustain persistent worlds the central theme of Stage IV."
        },
        {
            "title": "6 STAGE IV: MEMORY AND CONSISTENCY",
            "content": "A world model that acts without memory is reactive yet forgetful. This stage aims to endow models with mechanisms that sustain coherent states across long horizons. The central question emerges: can world models not only generate but also sustain coherent histories, preserve identities, and resist drift? We organize this section around three questions: where to anchor memory, how to extend its span and capacity, and how to govern it to preserve consistency. Because mask-based persistent 7 Preprint memory remains underexplored and varies widely, we take an architecture-agnostic view, focusing on memory and consistency rather than specific mechanisms. 6.1 EXTERNALIZED MEMORY Retrieval augments parametric models with non-parametric, often editable, knowledge stores. Early explorations such as Neural Turing Machines (Graves et al., 2014), Differentiable Neural Computers (Graves et al., 2016), and End-to-End Memory Networks (MemN2N) (Sukhbaatar et al., 2015) first explored learnable readwrite memory slots. While conceptually groundbreaking, their complexity gave way to more pragmatic, decoupled designs. kNN-LM (Khandelwal et al., 2019), REALM (Guu et al., 2020), and RAG (Lewis et al., 2020) showed that conditioning on retrieved passages could dramatically expand effective context while keeping knowledge traceable and updatable. DPR (Karpukhin et al., 2020) and RETRO (Borgeaud et al., 2022) scaled this approach to dense retrievers and trillion-token databases, rivaling far larger dense LMs while providing traceable and updatable evidence. Beyond simple retrieval, research has sought to make memory more scalable and dynamic. Product Key Memory (PKM) (Lample et al., 2019) supported massive lookup capacity through factorized keys; MemGPT (Packer et al., 2023) reframed LLMs as operating systems with explicit virtual memory management; LONGMEM (Wang et al., 2023) extends KV caches beyond 65k tokens through decoupled readers; and From RAG to Memory (Gutierrez et al., 2025) extended retrieval into continual learning, enabling dynamic knowledge updates without retraining. These systems collectively signal shift from retrieval as tool to memory as co-evolving substrate. 6.2 EXTENDING CAPACITY AND SPAN Parallel efforts seek to build persistence directly into the architecture, moving beyond fixed-length attention windows. Within Transformers, Universal Transformer (Dehghani et al., 2018) introduced depth-wise recurrence; Transformer-XL (Dai et al., 2019) propagated segment states across windows; the Compressive Transformer (Rae et al., 2019) down-sampled older activations. Subsequent designs such as the Memorizing Transformer (Wu et al., 2022) and Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) attached associative keyvalue stores or persistent memory tokens, reaching million-token horizons in practice; Infini-attention (Munkhdalai et al., 2024) added compressive long-term path for unbounded streaming. In parallel, Perceiver-AR (Hawthorne et al., 2022) introduced latent cross-attention bottleneck, compressing long inputs into compact representation and enabling autoregression over 100k tokens across text, images, and music. Together, this line of work represents reformist trajectory that extends attention through recurrence and compression. more radical line argues that persistence requires abandoning quadratic attention entirely. Structured state-space and linear-time models such as S4 (Lu et al., 2023), Mamba (Gu & Dao, 2023), and RetNet (Sun et al., 2023) replace attention with recurrent state updates that achieve linear complexity and thereby, in principle, support infinite context. Precursors such as Linear Transformers (Katharopoulos et al., 2020), along with more recent variants such as Hyena (Poli et al., 2023), have pointed in this direction by using kernels and long-range convolutions. Together, this line of work represents revolutionary trajectory that abandons attention in favor of continuous dynamical systems. Scaling strategies and engineering refinements further extend these capacities. LongNet (Ding et al., 2023) employs dilated attention for billion-token contexts; Ring Attention (Liu et al., 2023a) distributes computation across devices for million-token horizons; LSSVWM (Po et al., 2025) adapts state-space updates for long causal video generation. Practical techniques such as ALiBi (Press et al., 2021), LongLoRA (Chen et al., 2023), and StreamingLLM (Zeng et al., 2024) retrofit longcontext ability into existing models. Together, this line of work represents pragmatic trajectory that extends persistence through scaling strategies and engineering refinements. Ultimately, these three trajectories reformist, revolutionary, and pragmatic converge on the same goal: achieving genuine continuity, creating models that can read book, watch film, or play for hours without losing the thread. 8 Preprint 6.3 REGULATING MEMORY FOR CONSISTENCY Persistence without discipline degenerates into drift. The nature of this challenge depends critically on the underlying world representation, which has largely followed two paradigms: implicit 2D video frames and explicit 3D scenes. In implicit, autoregressive video models, the primary challenge is preventing two entangled failures: forgetting, where early content fades, and drifting, where errors compound. Efforts to mitigate one often aggravate the other (Zhang & Agrawala, 2025). The Genie series highlights this progression: Genie-1 (Bruce et al., 2024) suffers from short memory and drifts after only few frames; Genie-2 (Parker-Holder et al., 2024) introduces object permanence and sustains coherence for about minute; Genie-3 (Ball et al., 2025) reaches emergent multi-minute consistency. This underscores broader challenge: autoregressively generating an environment is fundamentally harder than producing pre-rendered video, since small inaccuracies accumulate over time. To tackle this, FramePack (Zhang & Agrawala, 2025) uses keyframe anchoring and context compression; Self-Forcing (Huang et al., 2025) and CausVid (Yin et al., 2025) impose stronger causal constraints; Context-as-Memory (Yu et al., 2025a) retrieves overlapping past frames to stabilize long video rollouts, and Mixture of Contexts (MoC) (Cai et al., 2025) learns sparse routing policies that focus attention on salient history. Conversely, explicit 3D representations built on generative assets from models like Trellis (Xiang et al., 2025) or TripoSG (Li et al., 2025d) inherently provide strong spatial consistency. Here, the challenge shifts to representing dynamic changes and long-term object states. Methods like WorldMem (Xiao et al., 2025), geometry-grounded spatial memory (Wu et al., 2025) and surfel-indexed view memory (VMem) (Li et al., 2025a) leverage this explicit 3D structure to maintain coherent world state over time, including dynamic representations that capture evolving geometry and supporting revisitations across long horizons. Beyond perceptual consistency, maintaining logical and factual coherence in reasoning remains crucial, addressed by techniques that learn to critique their own outputs (Asai et al., 2024). The overarching lesson is that longer context alone is insufficient. Consistency emerges from explicit policies over memory: what to write, what to retrieve, how to update, and when to forget. 6.4 SUMMARY Stage IV reframes generation as stateful computation. Externalized memory makes knowledge editable. Architectural persistence makes it durable. Consistency policies make it reliable. At production scale, multimodal systems such as Gemini (Comanici et al., 2025) and Claude (Anthropic, 2024) extend these ideas, sustaining million-token contexts across text, audio, and video and coupling long horizons with reasoning for agentic workflows. deeper question remains. Are elaborate memory systems fundamental solutions, or are they sophisticated workarounds for the current constraints of hardware and data? The existence of models with massive, brute-force context windows suggests that some memory problems might simply dissolve with sufficient scale, much like how larger models unlocked emergent abilities. Similarly, consistency failures may also stem from limited data diversity or flaws in the data itself, such as contradictory or erroneous text and videos that are only few seconds long. The answer will determine whether persistence in world models emerges as natural property of scale, or as the product of carefully engineered memory discipline. When we ask whether world models can dream consistently, the answer we seek is not just an engineering target but deeper understanding of the interplay among architecture, scale, and data."
        },
        {
            "title": "7 STAGE V: TOWARDS TRUE WORLD MODELS",
            "content": "The preceding stages constructed the necessary components: universal generative paradigm (I), unified architecture (II), real-time interactive loop (III), and persistent memory system (IV). Stage is not the addition of another component, but the synthesis of these parts into cohesive, autonomous whole. true world model is not merely sophisticated simulator controlled by user; it is self-sustaining computational ecosystem. Its defining properties are not just programmed but 9 Preprint emergent. We show that this synthesis gives rise to three defining properties: Persistence, Agency, and Emergence. 7.1 THE THRESHOLD: PERSISTENCE, AGENCY, AND EMERGENCE true world model ceases to be program one runs, but world one enters. Its defining properties are: Persistence: The worlds state and history exist independently of any single user session, accumulating consequences over time. It has past that can be revisited and future that unfolds continuously. This is the ultimate fulfillment of the Memory System (M), transforming the property of Memory into an enduring reality. Agency: The world is inhabited by multiple, goal-directed agents (human or AI) that interact within shared context. This property is enabled by the Interactive Loop (F, C), elevating the properties of Interaction and Adaptation into multi-agent society. Emergence: The worlds macro-level dynamics arise from the micro-level interactions of its agents and underlying rules, rather than being explicitly scripted. The model becomes crucible for discovering unforeseen social structures, behaviors, and causal chains. This is the critical synthesis that occurs only when the Generative Heart (G), Interactive Loop (F, C), and Memory System (M) operate in unison over time. 7.2 THE FRONTIER: THREE DEFINING CHALLENGES The path to this threshold is defined by three fundamental, unsolved research problems. These are not merely technical hurdles, but grand challenges that constitute the frontier of the field. The Coherence Problem (Evaluation). For conventional models, fidelity is measured against external ground truth. true world model, however, writes its own history. The challenge is to evaluate the truth of self-generating reality: to formalize and measure its internal logical, causal, and narrative coherence, and to define what it means for such world to be consistent. The Compression Problem (Scaling). An ever-growing history risks computational collapse. The challenge is to learn causally sufficient state abstractions that preserve consequence while discarding noise, approaching the information-theoretic bounds of predictive representation. Yet even with abstraction, long-horizon dynamics may be computationally irreducible, forcing us to treat world models not only as engineered systems but as objects of scientific observation. The Alignment Problem (Safety). An autonomous, persistent world model is technology with profound societal implications. The alignment challenge for true world model operates on two distinct levels. At its base, the model itself can be viewed as single environment whose generating process must align with human values. However, the complexity arises when this model becomes the substrate for multi-agent society. The alignment problem then becomes squared: it requires aligning not only the worlds underlying laws (the substrate), but also the emergent, unpredictable dynamics of the agents interacting within it. This is the harder challenge, distinguishing true world model from mere single-environment simulator. 7.3 THE HORIZON: FROM SIMULATOR TO SCIENTIFIC INSTRUMENT The journey detailed in this paper from masks to worlds has been about forging new kind of technology. Yet, the ultimate promise of true world model lies beyond its function as simulator for entertainment or training. Once world model crosses the threshold of persistence, agency, and emergence, it transforms from technological artifact into new kind of scientific instrument. It becomes computational crucible for running experiments on complex adaptive systems such as economies, cultures, and cognitive ecosystems that are impossible to conduct in reality. Preprint The quest for true world models, therefore, is not merely an engineering endeavor. It is pursuit of the ultimate tool for understanding complexity itself. The narrow road leads here: to future where we build worlds not to escape our own, but to comprehend it."
        },
        {
            "title": "8 CONCLUSION: BUILDING LIVING WORLDS",
            "content": "This paper has charted narrow road: logical progression from the universal paradigm of masking to the threshold of new reality. We have shown that this path is defined by the sequential mastery of three fundamental capabilities: unified generation, real-time interaction, and persistent memory. These are not ends in themselves, but the necessary foundations for worlds that can truly be called living worlds that persist with their own history, that are inhabited by goal-directed agents, and that give rise to unforeseen emergence. The pursuit of isolated benchmarks for static tasks is detour. The true frontier lies in embracing the architectural and theoretical commitments required to build these self-sustaining computational ecosystems. Therefore, the great choice ahead is whether we build worlds as mere tools for entertainment and escapism, or as scientific instruments for comprehending our complexity. The narrow road we have charted leads to this horizon: future where we forge not just better models, but new mirrors in which to see ourselves. 11 Preprint"
        },
        {
            "title": "A A FORMALIZATION OF THE THREE SUBSYSTEMS",
            "content": "This appendix provides detailed breakdown of the components formalized in Section 2. We consider standard partially observable Markov decision process (POMDP) formulation where at each timestep t, an agent takes an action at, receives an observation ot, and reward rt. The world terminates based on γt. The model maintains latent belief state zt and deterministic memory state ht. The Generative Heart (G). This subsystem models the worlds underlying generative process and comprises three components: Dynamics Model pθ(zt+1 zt, at): Predicts the next latent state given the current state and an action. This is the core of the models ability to dream futures. Observation Model pθ(ot zt): Maps latent state back to sensory observation (e.g., video frame), grounding the latent space in perceptible reality. Outcome Model pθ(rt, γt zt, at): Predicts task-relevant outcomes like rewards and termination signals from the latent state. The Interactive Loop (F, C). This subsystem enables closed-loop exchange between an agent and the world model. It consists of: Inference Model (Filter) qϕ(zt ht1, ot): Infers the current latent belief state zt from the new observation ot and past memory ht1. Control Model (Policy & Value) πη(at zt, ht), vω(zt, ht): The policy selects the next action based on the current belief and memory, while the value function estimates future outcomes, guiding the policy. The Memory System (M). This subsystem ensures long-horizon coherence. component: It has one core Memory Update Model ht = fψ(ht1, zt, at1): Updates the deterministic memory state based on the previous memory, the inferred state, and the last action, creating persistent representation of history. This component-based formalization provides unified lens through which to view the historical evolution of the field, from early control-oriented models that focused on specific components (e.g., Ha & Schmidhuber (2018)) to modern generative systems that aim to integrate them all. It forms the analytical foundation for the five-stage roadmap presented in this paper. 12 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, and Mark Riedl. Bringing stories alive: Generating interactive fiction worlds. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 16, pp. 39, 2020. Anthropic. Claude 3.5 sonnet. claude-3-5-sonnet, 2024. https://www.anthropic.com/news/ Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In International Conference on Learning Representations, 2024. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution textto-image synthesis. arXiv preprint arXiv:2410.08261, 2024. Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aaron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 3: new frontier for world models, 2025. URL https://deepmind.google/discover/blog/ genie-3-a-new-frontier-for-world-models/. Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning, pp. 16921717. PMLR, 2023. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 22062240. PMLR, 2022. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 13 Preprint Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:1107911091, 2022. Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, et al. Mixture of contexts for long video generation. arXiv preprint arXiv:2508.21058, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):1505 1518, 2022. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher Manning. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Marc-Alexandre Cˆote, Akos Kadar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: learning environment for text-based games. In Workshop on Computer Games, pp. 4175. Springer, 2018. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. DeepMind. Gemini diffusion, 2025. gemini-diffusion/. Accessed: 2025-08-19. URL https://deepmind.google/models/ Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. 14 Preprint Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied mulIn International Conference on Machine Learning, pp. 84698488. timodal language model. PMLR, 2023. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using neural network with dynamic external memory. Nature, 538 (7626):471476, 2016. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Junliang Guo, Yang Ye, Tianyu He, Haoyu Wu, Yushu Jiang, Tim Pearce, and Jiang Bian. Mineworld: real-time and open-source interactive world model on minecraft. arXiv preprint arXiv:2504.08388, 2025. Bernal Jimenez Gutierrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. From rag to memory: Non-parametric continual learning for large language models. arXiv preprint arXiv:2502.14802, 2025. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 39293938. PMLR, 2020. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Cˆote, and Xingdi Yuan. Interactive fiction games: colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 79037910, 2020. Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. Generalpurpose, long-context autoregressive modeling with perceiver ar. In International Conference on Machine Learning, pp. 85358558. PMLR, 2022. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022a. Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, Baixin Xu, Hao-Xiang Guo, Kaixiong Gong, Cyrus Wu, Wei Li, Xuchen Song, Yang Liu, Eric Li, and Yahui Zhou. Matrix-game 2.0: An open-source, real-time, and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. DiffusionImproving generative masked language models with diffusion models. arXiv preprint bert: arXiv:2211.15029, 2022b. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. In Proceedings of the 28th ACM Graphmae: Self-supervised masked graph autoencoders. SIGKDD conference on knowledge discovery and data mining, pp. 594604, 2022. Preprint Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. Advances in Neural Information Processing Systems, 35:2870828720, 2022. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the association for computational linguistics, 8:6477, 2020. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pp. 67696781, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019. KID. Memories off: Sorekara. Visual novel video game, first released for PlayStation, 2004. Publisher: KID. Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, and Sanja Fidler. Learning to simulate dynamic environments with gamegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12311240, 2020. Guillaume Lample, Alexandre Sablayrolles, MarcAurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. Advances in Neural Information Processing Systems, 32, 2019. Latitude. Ai dungeon. https://play.aidungeon.com/, 2024. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence In Annual pre-training for natural Meeting of the Association for Computational Linguistics, 2019. URL https://api. semanticscholar.org/CorpusID:204960716. translation, and comprehension. language generation, Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33: 94599474, 2020. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. 16 Preprint Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025a. Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, and Jason Kuen. Lavidao: Elastic masked diffusion models for unified multimodal understanding and generation. arXiv preprint arXiv:2509.19244, 2025b. Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. survey on diffusion language models. arXiv preprint arXiv:2508.10875, 2025c. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusionlm improves controllable text generation. Advances in neural information processing systems, 35: 43284343, 2022. Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025d. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 36:4701647031, 2023. Willi Menapace, Stephane Lathuiliere, Sergey Tulyakov, Aliaksandr Siarohin, and Elisa Ricci. In Proceedings of the IEEE/CVF Conference on Computer Vision Playable video generation. and Pattern Recognition, pp. 1006110070, 2021. Willi Menapace, Stephane Lathuili`ere, Aliaksandr Siarohin, Christian Theobalt, Sergey Tulyakov, Vladislav Golyanik, and Elisa Ricci. Playable environments: Video manipulation in space and time. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 35843593, 2022. Willi Menapace, Aliaksandr Siarohin, Stephane Lathuili`ere, Panos Achlioptas, Vladislav Golyanik, Sergey Tulyakov, and Elisa Ricci. Promptable game models: Text-guided game simulation via masked diffusion models. ACM Transactions on Graphics, 43(2):116, 2024. Nick Montfort. Toward theory of interactive fiction. IF theory reader, 25, 2011. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 101, 2024. John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, and Ricky TQ Chen. OnearXiv preprint flow: Concurrent mixed-modal and interleaved generation with edit flows. arXiv:2510.03506, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Anthony Niesz and Norman Holland. Interactive fiction. Critical Inquiry, 11(1):110129, 1984. 17 Preprint Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir Patil, Ion Stoica, and Joseph Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023. Yatian Pang, Eng Hock Francis Tay, Li Yuan, and Zhenghua Chen. Masked autoencoders for 3d point cloud self-supervised learning. World Scientific Annual Review of Artificial Intelligence, 1: 2440001, 2023. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 122, 2023. Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale founURL https://deepmind.google/discover/blog/ dation world model, 2024. genie-2-a-large-scale-foundation-world-model/. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. arXiv preprint arXiv:2505.20171, 2025. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 2804328078. PMLR, 2023. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. Kevin Rojas, Yuchen Zhu, Sichen Zhu, Felix X-F Ye, and Molei Tao. Diffuse everything: Multimodal diffusion models on arbitrary state spaces. arXiv preprint arXiv:2506.07903, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. 18 Preprint Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024a. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024b. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are dataefficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems, 36:7453074543, 2023. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1466814678, 2022. World Labs. World labs, 2024. URL https://www.worldlabs.ai/. Accessed: 2025-08-19. Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. Preprint Yuhuai Wu, Markus Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2146921480, 2025. Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 96539663, 2022. Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025a. Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun arXiv preprint Gai, Hao Chen, and Xihui Liu. survey of interactive generative video. arXiv:2504.21853, 2025b. Runpeng Yu, Qi Li, and Xinchao Wang. Discrete diffusion in large language and multimodal models: survey. arXiv preprint arXiv:2506.13759, 2025c. Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: In Proceedings of the Pre-training 3d point cloud transformers with masked point modeling. IEEE/CVF conference on computer vision and pattern recognition, pp. 1931319322, 2022. Zihao Zeng, Bokai Lin, Tianqi Hou, Hao Zhang, and Zhijie Deng. In-context kv-cache eviction for llms via attention-gate. arXiv preprint arXiv:2410.12876, 2024. Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021. ibot:"
        }
    ],
    "affiliations": [
        "Georgia Tech",
        "MeissonFlow Research",
        "UC Merced",
        "UCLA"
    ]
}