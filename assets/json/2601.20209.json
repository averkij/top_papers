{
    "paper_title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
    "authors": [
        "Jinyang Wu",
        "Shuo Yang",
        "Changpeng Yang",
        "Yuhao Shen",
        "Shuai Zhang",
        "Zhengqi Wen",
        "Jianhua Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \\textbf{Spark} (\\textbf{S}trategic \\textbf{P}olicy-\\textbf{A}ware explo\\textbf{R}ation via \\textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \\textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios."
        },
        {
            "title": "Start",
            "content": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning Jinyang Wu1*, Shuo Yang1*, Changpeng Yang2, Yuhao Shen3, Shuai Zhang1, Zhengqi Wen1, Jianhua Tao1 1Tsinghua University 2Peking University 3Zhejiang University {wu-jy23, shuo-yan20}@mails.tsinghua.edu.cn 6 2 0 2 8 2 ] . [ 1 9 0 2 0 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of highquality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose SPARK (Strategic Policy-Aware exploRation via Key-state dynamic branching), novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agents intrinsic decisionmaking signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that SPARK achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning (RL) (Sutton et al., 1999) has proven effective in enhancing Large Language Model (LLM) reasoning capabilities, with notable success in domains such as mathematics (Guo et al., 2025; OpenAI, 2024b) and coding (Zhoubian et al., 2025). This has led to growing consensus that RL holds the key to the next frontier (Zhang et al., 2025b; Yue et al., 2025): agentic AIsystems capable of autonomously navigating dynamic environments and executing complex, long-horizon instructions (Zhang et al., 2025a; Singh et al., 2025). * Equal Contribution. 1 Figure 1: Paradigm comparison: uniform vs. strategic exploration. Standard RL (Left) wastes budget via uniform, independent sampling. In contrast, SPARK (Right) employs dynamic branching at critical junctures for precise resource allocation, yielding higher-quality trajectories under the similar computational budget. However, applying RL to agentic tasks faces the scarcity of higha fundamental bottleneck: quality trajectories under resource constraints. Unlike math problems where solutions are selfcontained and directly verifiable (Zhang et al., 2025b), agentic tasks require navigating vast state spaces where single misstep can derail long sequence (Feng et al., 2025; Zhang et al., 2025e). Consequently, successful trajectories are sparse, making it difficult for the policy to learn effectively (MoonshotAI, 2025; Wu et al., 2025). Existing methods attempt to mitigate this challenge by expanding exploration budget and employing extensive search-based techniques during RL training (Xing et al., 2025; Ji et al., 2025). Yet, these approaches suffer from indiscriminate resource allocation: they distribute computational budget uniformly across all steps (Figure 1), rather than targeting decision points where additional exploration is most valuable. Consider an embodied agent tasked with preparing breakfast (Salimpour et al., 2025; Liang et al., 2025): uniform exploration wastes substantial resources on trivial steps like opening the fridge door while allocating insufficient budget to pivotal states like choosing states where additional exploration is needed, relying on intrinsic decision-making signals rather than handcrafted heuristics, thereby enabling precise and efficient resource allocation. Adaptive Dynamic Branching: By dynamically triggering branching at critical states, we achieve superior sample quality under constrained budget, generating more informative training trajectories than blind exploration. Compelling Strong Empirical Validation: Experiments (Figure 2) on challenging tasks (e.g., embodied planning in ALFWorld) demonstrate that SPARK achieves superior success rates with higher sample efficiency, exhibiting robust generalization even in unseen scenarios."
        },
        {
            "title": "2 Methodology",
            "content": "We first formulate the agentic task as partially observable decision process (2.1), then introduce our core mechanism: Dynamic Branching Exploration that selectively allocates exploration budget based on intrinsic decision-making signals (2.2). Figure 3 provides the overview of SPARK. 2.1 Problem Formulation We formulate long-horizon agentic tasks as Partially Observable Markov Decision Process (POMDP), defined by the tuple (S, A, O, , R, γ), where is the latent state space, is the action space, is the observation space, : is the transition function, : is the reward function, and γ [0, 1) is discount factor. At each timestep t, the environment is in hidden state st S. The agent receives observation ot (e.g., textual feedback), and maintains an interaction history ht = (o0, z0, a0, . . . , ot), where zi denotes the reasoning trace and ai is the action at step i. This history serves as the belief state proxy. Then, the agent (policy model πθ) generates reasoning trace zt followed by an action at: (zt, at) πθ( ht) (1) Upon executing action at, the environment transitions to st+1 according to (st+1 st, at) and provides observation ot+1. We represent single step as the triplet τt = (ot, zt, at). An episode continues until the agent achieves the goal or reaches the maximum horizon K, yielding trajectory τ = (τ0, τ1, . . . , τT ) where K. Figure 2: Multi-benchmark performance comparison. SPARK outperforms all baselines across ALFWorld (L0-L2), ScienceWorld (L0-L2), and WebShop tasks, achieving +73.5% average improvement. alternative ingredients when the intended ones are missing. By prioritizing exhaustive coverage over sampling quality, such methods fail to yield highvalue trajectories under resource constraints, resulting in inefficient exploration and unstable training. We argue that effective exploration should be triggered at appropriate decision points. Rather than blind search, an intelligent agent should autonomously identify critical junctures at which embarking on additional exploration branches yields the greatest potential benefits. Building on this principle, we introduce SPARK (Strategic PolicyAware exploRation via Key-state dynamic branching), novel framework that enables autonomous strategic exploration via dynamic branching at critical intermediate states, which we refer to as SPARK points. Our key insight is to leverage the agents intrinsic decision-making signals to govern the exploration topology. When encountering states with high epistemic uncertainty or semantic ambiguity, the agent selectively initiates additional exploration; otherwise, it proceeds linearly through routine decisions. This achieves precise resource allocation that prioritizes sampling quality over exhaustive coverage while minimizing reliance on human priors, thereby ensuring robust generalization. Extensive experiments show that SPARK significantly enhances performance, efficiency, and out-of-domain generalization over powerful baselines. Our contributions are three-fold: Autonomous Strategic Exploration: We propose an agentic RL framework that enables agents to autonomously identify intermediate 2 Figure 3: Overview of SPARK framework. SPARK performs dynamic branching exploration via: (1) Root Initialization: diverse starting trajectories; (2) Autonomous Branching: selective expansion at high-uncertainty states using intrinsic <explore> signals; (3) Budget Enforcement: constraining tree growth within computational limits. The resulting trajectory trees are then used for Tree-Based Policy Optimization. For most agentic benchmarks (Singh et al., 2025), the reward function provides sparse terminal feedback: R(τ ) {0, 1} indicating task success or failure. Our objective is to find an optimal policy π that maximizes the expected return: J(π) = Eτ π[R(τ )] (2) 2.2 Dynamic Branching Exploration Uniform exploration across all steps is highly inefficient in long-horizon RL (Wang et al., 2020). Trivial steps (e.g., open the door in front of fridge) require minimal search, while critical junctures (e.g., choosing alternative ingredients when the intended ones are missing) benefit substantially from exploring multiple alternatives. Standard methods allocate resources uniformly, wasting computation on routine actions while under-exploring pivotal decisions. We address this through dynamic branching: mechanism that constructs hierarchical treestructured exploration paths by selectively allocating rollout budget at pivotal decision points (SPARK points), guided by the agents intrinsic decision-making signals. Supervised fine-tuning (SFT) is first employed to endow the model with the preliminary capability to generate such intrinsic signals, followed by RL optimization. Let denote the total rollout budget (maximum leaf nodes) and denote the maximum episode length. Rather than sampling independent trajectories, SPARK builds trajectory forest that branches at states where additional exploration is valuable. Specifically, we operate in three stages: Stage 1: Root Initialization. At initial state s0, task ambiguity is typically highest. To ensure exploration diversity, we create parallel trajectory roots by sampling distinct reasoning-action pairs: {(z(i) 0 , a(i) 0 )}M i=1 πθ( h0) (3) where < (typically = 3 to 5). This establishes independent trajectory trees. Stage 2: Autonomous Branching. For each active trajectory at step t, the agent generates reasoning trace z(i) that internally deliberates whether the current decision requires additional exploration. We employ branching criterion B() that analyzes z(i) to determine exploration intensity: = B(z(i) b(i) ) (4) where b(i) {1, B} specifies the branching factoreither linear continuation (b = 1) or multibranch exploration (b = B). Algorithm 1 SPARK: Strategic Policy-Aware Exploration via Key-State Dynamic Branching Input: Policy πθ; task dataset Q; rollout budget ; branching factor B; initial roots Output: Optimized policy π θ 1: for iteration = 1, 2, . . . do 2: for task do Dynamic Branching Exploration Initialize parallel trajectory roots from for each active trajectory at step do Generate reasoning trace and action (z(i) Compute branching factor b(i) B(z(i) Get effective branching factor b(i) Prepare context for b(i) end for Collect completed trajectory group into , a(i) ) πθ( h(i) ) ) upon triggering <explore> in z(i) t,eff under budget constraint via Eq. (7) t,eff continuations and update active trajectory mask end for Update θ using GRPO objective with collected trajectories Tree-Based Policy Update 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for Branching Criterion. The function identifies states where exploration is valuable by detecting signals of epistemic uncertainty in the reasoning trace, which is explicitly manifested as dedicated <explore> tag. When uncertainty is detected, we set b(i) = 1. Function can be formalized as: = B; otherwise b(i) B(z(i) ) = (cid:40) B, 1, if <explore> in z(i) otherwise Based on b(i) , we sample continuations: {(z(i,j) t+1 , a(i,j) t+1 )}b(i) j=1 πθ( h(i) t+1) (5) (6) where h(i) , z(i) t+1 = (h(i) history. This creates b(i) current node. , o(i) , a(i) t+1) is the updated child trajectories from the Stage 3: Budget Enforcement. Expansion is constrained by the global budget . Let Ncurrent denote the current number of active leaf nodes. The actual branching factor is adjusted dynamically: t,eff = min(b(i) b(i) , Ncurrent + 1) (7) This ensures we never exceed budget . Rollout terminates when trajectory reaches terminal state (success/failure) or maximum horizon K. This process yields collection of trajectory trees containing completed trajectories, where . We denote this collection as group = {τ (1), τ (2), . . . , τ (G)}. 4 Stage 4: Tree-Based Policy Update. The trajectory trees generated by SPARK are used to update the policy without modifying the underlying optimization algorithm. Completed leaf trajectories from the same task are grouped and assigned binary terminal rewards, which are propagated to all steps. Shared prefixes induce comparable alternatives, enabling relative credit assignment via groupnormalized advantages. Policy updates are performed using standard clipped optimization with KL term against reference policy, remaining compatible with existing GRPO-style pipelines while incorporating decision-focused exploration signals. 2.3 Theoretical Perspective on SPARK We present concise theoretical argument explaining why SPARK achieves substantially improved performance under consistently lower token budgets in long-horizon decision-making tasks. Key Insight. Let denote the episode horizon and {0, . . . , 1} the set of pivotal decision steps (SPARK points) whose action choices critically affect task success, with = K. Standard GRPO-style linear sampling allocates computation uniformly across all steps of trajectory. In contrast, SPARK redistributes intermediate computation toward pivotal steps without increasing the final number of completed trajectories. Decision Coverage at Pivotal Steps. At pivotal step C, let qt (0, 1) denote the probability mass assigned by the current policy to desirable actions when sampled once. Under linear sampling, each trajectory attempts this decision only once, yielding coverage probability of qt. In SPARK, dynamic branching generates conditional continuations at pivotal steps, increasing the probability of selecting at least one desirable action to qbranch = 1 (1 qt)B, (8) which strictly dominates qt for any 2. Implications for Long-Horizon Tasks. Task success typically depends on sequence of correct decisions at pivotal steps. By replacing each qt with qbranch , SPARK amplifies the probability of discovering high-quality trajectories through multiplicative effect across C. Meanwhile, SPARK avoids repeatedly rolling out suffixes induced by suboptimal pivotal decisions, leading to fewer redundant computations at non-pivotal steps and modest reduction in overall token usage."
        },
        {
            "title": "3 Experiments",
            "content": "We analyze our framework from four aspects: (1) Performance: superior success rates versus strong baselines; (2) Efficiency: sample and token efficiency; (3) Generalization: robustness on unseen scenarios; (4) Ablation Study: contribution of dynamic branching and exploration quality analysis. 3.1 Experimental Settings Benchmarks. Following prior work (Feng et al., 2025; Zhang et al., 2025e), we evaluate on three challenging long-horizon agentic domains: (1) Embodied Decision Making: ALFWorld (Shridhar et al., 2020) for household task execution; (2) Scientific Reasoning: ScienceWorld (Wang et al., 2022) with complex task horizons up to 30+ steps; (3) Web Navigation: WebShop (Yao et al., 2022a) with 1.1M products to test open-ended exploration. Baselines. We compare SPARK against: (1) Closed-source LLMs: GPT-4o (OpenAI, 2024a), GPT-5-mini (OpenAI, 2025), GPT-5 (OpenAI, 2025), Gemini-2.5-Pro (Comanici et al., 2025); (2) Prompting Methods: ReAct (Yao et al., 2022b); and (3) RL Methods: GRPO (Shao et al., 2024), ETO (Song et al., 2024), GiGPO (Feng et al., 2025), and RLVMR (Zhang et al., 2025e). Evaluation Metrics. Following Shridhar et al. (2020); Yao et al. (2022a), we report Success Rate (SR) for task completion and Average Score (WebShop only) for partial success (e.g., selecting product with correct attributes but wrong color). Implementation Details. We use Qwen2.51.5B/7B-instruct (Qwen Team, 2024) as base models. cold-start SFT with 300 trajectories enables initial <explore> tag generation in RL. We set = 8, = 4, = 2 for rollout budget, initial roots, and branching factor, respectively. RL training runs for 120 steps with batch size 16. Full details are provided in Appendix B.4. 3.2 Main Results Table 1 presents results across long-horizon agentic planning tasks, revealing three key findings: SPARK establishes leading performance across model scales and task domains. SPARK consistently outperforms prompting (ReAct), SFT, and RL methods (GiGPO, RLVMR) on both 1.5B and 7B backbones. Notably, SPARK-1.5B attains 49.2% on the hardest ScienceWorld L2, exceeding GPT-5 (33.6%) and Gemini-2.5-Pro (30.5%). This reveals that strategic exploration enables smaller models to rival significantly larger proprietary systems. Dynamic branching at critical states substantially outperforms uniform exploration. Compared to GRPOs uniform sampling, SPARK yields substantial gains on 7B: +23.3% on ALFWorld Look and +39.4% on Pick2. The amplified improvement on Pick2 (multi-step coordination) versus Look (simple navigation) validates our hypothesis: selective branching allocates budget to critical steps where additional search provides maximum value. SPARK exhibits robust generalization to unseen environments. On L2 out-of-domain tasks, SPARK-1.5B significantly outperforms GRPO on ALFWorld (80.5% vs. 29.7%). On the highly complex ScienceWorld L2, SPARK (49.2%) establishes 10.5 lead over GiGPO (4.7%) and 1.9 over RLVMR (26.5%). This highlights the effectiveness of autonomous strategic exploration in novel scenarios where conventional methods struggle. 3.3 Exploration Efficiency Analysis Sample Efficiency. To assess whether SPARKs strategic exploration enhances sample efficiency, we evaluate performance under varying data scales on ALFWorld and ScienceWorld L0 (1.5B). As shown in Figure 4, SPARK demonstrates dramatic efficiency advantages. For example, on ALFWorld, with only 20% training data, SPARK achieves 5 Method ALFWorld ScienceWorld WebShop Look Clean Pick2 L0 L1 L2 L1 L2 score succ. GPT-4o GPT-5-mini GPT-5 Gemini-2.5-Pro 50.0 58.3 75.0 75. 34.4 28.1 43.8 40.6 15.8 52.6 57.9 84.2 Closed-Source Models 38.3 48.4 46.1 54.7 63.3 60.2 55.5 55.0 52.3 57.0 70.0 56.3 32.0 39.8 53.9 32.8 32.8 31.3 33.6 27. 32.8 31.4 33.6 30.5 12.7 14.2 34.9 42.9 6.2 7.0 29.7 32.0 Advanced Method Comparison Qwen2.5-1.5B ReAct SFT ETO GiGPO RLVMR GRPO SPARK (Ours) vs. GRPO Qwen2.5-7B ReAct SFT ETO GiGPO RLVMR GRPO SPARK (Ours) vs. GRPO 10.8 35.9 66.2 90.7 91.2 88.1 100.0 11.3 10.2 18.3 39.1 17.6 42.0 - 25.8 46.3 65.0 48.0 66.5 71.1 56.3 78.8 56.8 29.7 55.6 75.8 80.5 91.7 +36.1 +11.9 +17.4 +20.3 +22.7 +50.8 +48.4 +54.3 +38.3 +13.0 +19.0 0.0 29.7 55.6 73.8 77.6 72.1 89.5 11.3 43.0 64.1 86.7 89.1 76.6 96.9 40.1 65.8 - 83.1 86.7 75.8 88. 13.7 38.7 66.4 83.2 87.9 71.1 93.8 0.8 12.5 15.6 4.7 26.5 10.9 49.2 0.8 18.0 22.7 15.2 34.4 13.7 68.0 1.2 20.3 39.1 25.8 46.9 21.1 69.5 33.2 63.0 70.5 85.9 88.2 76.7 100.0 +23.3 18.7 61.1 82.3 93.3 90.1 86.0 92.5 +6. 23.1 63.3 70.3 89.5 91.4 79.3 96.1 19.5 7.8 12.8 38.3 36.7 33.2 - 62.5 51.2 72.8 53.4 83.6 74.2 67.2 86.7 66.1 49.1 56.4 95.8 82.8 75.0 +39.4 +16.8 +14.9 +36.0 +25.9 +44.0 +31.2 +10.5 +16.7 46.2 62.2 - 84.4 87.3 79.3 89.8 28.5 57.0 74.2 90.2 91.8 77.3 92.2 27.0 37.5 51.6 67.2 83.6 52.3 88.3 6.3 23.4 28.1 25.8 32.2 26.6 57. 11.3 32.0 40.6 35.2 43.0 30.1 74.1 Table 1: Performance comparison on long-horizon agentic tasks. We evaluate success rates (%) on ALFWorld and ScienceWorld, and task completion score/success rate on WebShop. L0/L1/L2 denote seen task categories and instance variants, seen task categories but unseen instance variants, unseen task categories and instance variants, respectively. Look, Clean, and Pick2 represent distinct task types in ALFWorld. Best results per model are in bold. 84.4% success rate, surpassing GRPOs 76.6% at full data (100%). At 40% data, SPARK reaches 89.1%, matching RLVMR and exceeding GiGPO (86.7%) at full data. In contrast, baselines collapse at low data regimes: GRPO and GiGPO achieve only 22.7% at 20% data, revealing their inability to efficiently learn from limited samples. This 5 data reduction over GRPO confirms that prioritizing exploration at critical decision points enables higher sample efficiency (details in Appendix C.1). Token Efficiency. Table 2 compares token consumption of SPARK against chain-like methods. By sharing common prefixes across trajectories, our method substantially reduces token generation: 6.9%, 47.0%, and 11.2% on ALFWorld, ScienceWorld, and WebShop, respectively. These results validate our core thesis: by concentrating the exploration budget at critical decision points rather than uniformly across all steps, dynamic branching not only improves sample quality but also elimMethod ALFWorld ScienceWorld WebShop GRPO SPARK Reduction 100.0 93.1 6.9 100.0 53.0 47.0 100.0 88.8 11.2 Table 2: Token efficiency comparison on 1.5B backbone. We report the relative token consumption of SPARK compared to chain-like methods, with the latter normalized to 100 for clear comparison. inates redundant token generation on routine actions, thereby maximizing both performance and computational efficiency with limited resources. 3.4 Cross-Domain Generalization To further evaluate generalization to unseen scenarios, we analyze performance degradation from in-domain (ID, L0) to out-of-domain (OOD, L2) splits. As shown in Table 3, GRPO exhibits severe overfitting with performance drops of 61.2% on ALFWorld and 48.3% on ScienceWorld. In contrast, SPARK maintains robust performance, 6 Figure 4: Sample efficiency comparison. SPARK surpasses GRPO@100% using only 20% data, while baselines often collapse in low-data regimes. Method ALFWorld ScienceWorld ID OOD ID OOD SFT GRPO Ours 43.0 76.6 96.9 17.6 29.7 80. 59.1% 61.2% 16.9% 20.3 21.1 69.5 12.5 10.9 49.2 38.4% 48.3% 29.2% Table 3: Cross-domain generalization comparison. denotes the performance drop ratio from seen (indomain, ID) to unseen scenarios (out-of-domain, OOD). limiting degradation to 16.9% and 29.2% respectively. Moreover, SPARK achieves the highest absolute OOD success rates (80.5% and 49.2%), substantially outperforming GRPO (29.7% and 10.9%). These results indicate that SPARKs dynamic branching enables effective learning of transferable exploration strategies rather than memorization of task-specific trajectories, yielding policies that generalize well to novel environments. 3.5 Ablation Study and Analysis Impact of Dynamic Branching. To verify dynamic branching, we compare SPARK against Fixed Probability baseline that randomly triggers branching at every step with fixed probability, independent of decision criticality. As shown in Table 4, stochastic branching degrades performance across all datasets, most severely on ScienceWorld (69.5%45.3%) where long horizons amplify the costs of misallocated exploration. This validates our thesis: effective exploration requires selective branching at high-uncertainty states rather than uniform expansion, as fixed methods waste budget on trivial steps while under-exploring critical states. Impact of Initial Roots. We investigate the tradeoff between initial diversity and subsequent exploration depth by varying initial roots (M ) under fixed budget = 8. As shown in Figure 5, performance peaks at = 4 (80.7%), but drops to 67.6% at = 2 and 72.1% at = 6. Too few roots (M = 2) constrain initial diversity, risking Method ALFWorld ScienceWorld WebShop Dynamic Fixed 96.9 90.6 -6.3 69.5 45.3 -24.2 75.8 70.3 -5.5 Table 4: Ablation on RL branching strategy. We report success rates (%) on 1.5B model. Fixed branching randomly triggers expansion at each step with constant probability, independent of state uncertainty. Figure 5: Ablation on initial root count (M ). We report success rates (%) with fixed total budget = 8. early convergence to local optima despite remaining budget. Conversely, excessive roots (M 6) exhaust budget at initialization, leaving insufficient resources for later branching. This reveals that the sweet spot = 4 balances broad initial coverage with deep adaptive search, splitting budget between diverse starts and strategic exploration. Exploration Quality: Repetitive Action Ratio. Beyond success rates, we analyze exploration quality via repetitive action rates. Table 5 shows that SPARK produces significantly fewer repetitive actions across model scales. On 1.5Bs L2 (OOD), SPARK achieves 15.4% repetitive ratio versus GRPOs 27.1% (43% reduction). By concentrating search at uncertain states, SPARK explores more purposefully, avoiding redundant trial-anderror on routine actions. The gap widens on harder splits (L1, L2), confirming that strategic exploration particularly benefits novel scenarios where indiscriminate search is wasteful. Qualitative Analysis. Figure 6 illustrates exploration behaviors on an ALFWorld egg-retrieval task. The GRPO-trained agent becomes trapped in mechanical loop (Steps 6-30), repeatedly checking the same three locations without exploring alternatives like sinkbasin or cabinets. In contrast, SPARK exhibits strategic exploration: at Step 6, upon encountering uncertainty about egg location, the agent emits an <explore> signal and reasons about plau7 Figure 6: Qualitative comparison on the embodied planning task (ALFWorld): put an egg on the microwave. Method 1.5B 7B L0 L1 L2 L0 L1 L2 SFT GRPO Ours 10.7 18.4 10.1 20.2 17.6 7. 21.4 27.1 15.4 13.9 21.5 10.0 24.5 20.3 8.6 14.4 31.2 14.1 Table 5: Repetitive action ratio on ALFWorld (lower is better). SPARK produces fewer repetitive actions on in-domain (L0) and out-of-domain (L1, L2) splits. sible locations (sinkbasin, drawers). This triggers systematic search across multiple candidates, successfully locating the egg at Step 7 and completing the task by Step 10. This reveals that autonomous identification of critical decision points enables purposeful exploration that avoids redundant loops."
        },
        {
            "title": "4 Related Work",
            "content": "Agentic Reinforcement Learning. Applying RL to train LLM-based agents has shown promise in reasoning tasks (Shao et al., 2024), yet longhorizon scenarios remain challenging due to trajectory scarcity with constrained resources. To obtain better training signals, recent work explores process supervision via Process Reward Models (Zhang et al., 2025e,d) for dense intermediate feedback, but requires expensive annotations that scale poorly to open-ended environments (Khalifa et al., 2025; Shridhar et al., 2020; Zhou et al., 2024). Alternative methods attempt to generate high-quality trajectories by expanding exploration budget (Xing et al., 2025; Ji et al., 2025), but they suffer from indiscriminate resource allocation that treats all steps uniformly. Our method enables agents to autonomously allocate exploration budget, reducing reliance on external supervision while improving sample quality. Strategic Exploration. Effective exploration is critical for discovering optimal policies in vast state-action spaces. Inference-time search methods (Zhang et al., 2025c; Wu et al., 2024), such as ToT (Yao et al., 2023) and RAP (Hao et al., 2023), have demonstrated the value of exploring diverse reasoning paths. Recent efforts aim to internalize this capability into training through tree-structured RL (Hou et al., 2025; Wu et al., 2025). However, these methods typically apply uniform branching at every step, wasting considerable resources on trivial steps while often under-exploring critical In contrast, strategic junctures (Ji et al., 2025). SPARK leverages intrinsic decision-making signals to selectively branch only at high-uncertainty states. This design prioritizes sampling quality over exhaustive coverage, achieving superior trajectory quality under budget constraints."
        },
        {
            "title": "5 Conclusion",
            "content": "We present SPARK, novel RL framework for longhorizon agentic learning. By enabling strategic exploration through dynamic branching, SPARK selectively expands exploration at critical decision points and achieves precise resource allocation. Empirical results confirm that SPARK achieves superior success rates with better exploration efficiency, exhibiting robust generalization. Our work highlights strategic explorations value in advancing capable and efficient agentic systems."
        },
        {
            "title": "Limitations",
            "content": "Our study is comprehensive, but has certain limitations that we plan to address in future research. In this study, we leverage the agents intrinsic decision-making signals to identify critical decision points for dynamic branching. While this design minimizes reliance on external supervision and enables autonomous strategic exploration, it may not fully exploit exploration opportunities in extremely low-capability base models where the agents self-awareness is limited. We believe these are minor issues and we will explore learning-based calibration mechanisms, such as combining internal signals with external feedback to enhance state awareness, thereby further improving the robustness and applicability of strategic branching across diverse model capabilities."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 34 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. 2025. Group-in-group policy optimization for LLM agent training. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. 2025. TreeRL: LLM reinforcement learning with on-policy tree search. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1235512369, Vienna, Austria. Association for Computational Linguistics. Yuxiang Ji, Ziyu Ma, Yong Wang, Guanhua Chen, Xiangxiang Chu, and Liaoni Wu. 2025. Tree search for llm agent reinforcement learning. arXiv preprint arXiv:2509.21240. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. 2025. Process reward models that think. arXiv preprint arXiv:2504.16828. Samuel Kotz and Norman L. Johnson, editors. 1992. Breakthroughs in Statistics: Methodology and Distribution. Springer New York, New York, NY. Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, and 1 others. 2025. Treepo: Bridging the gap of policy optimization and efficacy and inference efficiency with heuristic treebased modeling. arXiv preprint arXiv:2508.17445. Wenlong Liang, Rui Zhou, Yang Ma, Bing Zhang, Songlin Li, Yijia Liao, and Ping Kuang. 2025. Large model empowered embodied ai: survey on decision-making and embodied learning. arXiv preprint arXiv:2508.10399. MoonshotAI. 2025. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. Accessed: 2025-06. OpenAI. 2024a. Hello gpt-4o. Accessed: 2024-05. OpenAI. 2024b. Learning to reason with LLMs. Accessed: 2024-09. OpenAI. 2025. Introducing gpt-5. Accessed: 2025-08. Qwen Team. 2024. Qwen2.5: party of foundation models. Accessed: 2024-09. Sahar Salimpour, Lei Fu, Farhad Keramat, Leonardo Militano, Giovanni Toffetti, Harry Edelman, and Jorge Peña Queralta. 2025. Towards embodied agentic ai: Review and classification of llm-and vlm-driven robot autonomy and interaction. arXiv preprint arXiv:2508.05294. Max-Philipp B. Schrader. 2018. gym-sokoban. https: //github.com/mpSchrader/gym-sokoban. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. 9 Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. 2025. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502. Richard Sutton, Andrew Barto, and 1 others. 1999. Reinforcement learning. Journal of Cognitive Neuroscience, 11(1):126134. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Ruosong Wang, Simon Du, Lin Yang, and Sham Kakade. 2020. Is long horizon rl more difficult than short horizon rl? In The Thirty-fourth Annual Conference on Neural Information Processing Systems, volume 33, pages 90759085. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. ScienceWorld: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1127911298, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, and Jianhua Tao. 2024. Beyond examples: High-level automated reasoning paradigm arXiv preprint in in-context learning via mcts. arXiv:2411.18478. Jinyang Wu, Chonghua Liao, Mingkuan Feng, Shuai Zhang, Zhengqi Wen, Pengpeng Shao, Huazhe Xu, and Jianhua Tao. 2025. Thought-augmented policy optimization: Bridging external guidance and internal capabilities. arXiv preprint arXiv:2505.15692. Shangyu Xing, Siyuan Wang, Chenyuan Yang, Xinyu Dai, and Xiang Ren. 2025. Lookahead tree-based rollouts for enhanced trajectory-level exploration in reinforcement learning with verifiable rewards. arXiv preprint arXiv:2510.24302. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. 2025. Treerpo: Tree relative policy optimization. arXiv preprint arXiv:2506.05183. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, volume 36, pages 1180911822. Curran Associates, Inc. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. 2024. Finetuning large vision-language models as decisionIn Admaking agents via reinforcement learning. vances in Neural Information Processing Systems, volume 37, pages 110935110971. Curran Associates, Inc. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi Li, Xiangyuan Xue, Yijiang Li, and 1 others. 2025a. The landscape of agentic reinforcement learning for llms: survey. arXiv preprint arXiv:2509.02547. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, and 1 others. 2025b. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, and 1 others. 2025c. survey on test-time scaling in large language models: What, how, where, and how well? arXiv preprint arXiv:2503.24235. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. 2025d. Process vs. outcome reward: Which is better for agentic RAG reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, and Xiaolong Li. 2025e. Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents. arXiv preprint arXiv:2507.22844. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations. Sining Zhoubian, Dan Zhang, and Jie Tang. 2025. Restrl: Achieving accurate code reasoning of llms with optimized self-training and decoding. arXiv preprint arXiv:2508.19576."
        },
        {
            "title": "Appendix of SPARK",
            "content": "This supplementary material provides in-depth insights into our SPARK method, covering additional theoretical analysis, additional experimental details, results and analysis. The appendix is organized as follows:"
        },
        {
            "title": "Contents",
            "content": "A Theoretical Analysis A.1 Problem Setup and Notations . . . A.2 Exploration Efficiency under Bud- . . A.3 Sample Complexity Advantage . . . . . . . A.4 Summary . get Constraints . . . . . . . . . . . . . . . Additional Experimental Details . . . . . . B.1 Datasets . . . B.2 Baselines B.3 Evaluation Details . . B.4 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 11 12 12 13 13 13 14 15 15 Supplementary Results 15 . C.1 Sample Efficiency . . 16 C.2 Inference Scalability . . C.3 Multimodal Extension . 16 C.4 Sensitivity Analysis on Total Budget 16 17 C.5 Statistical Analysis . . . . . . . . . . . . . . . . . . . . . . . . Case Study 18 Additional Discussion 18 E.1 Discussion on Tree-Based Baselines 18 E.2 Discussion on Reducing Depen- . . . . . . dence on Human Priors . . E.3 Failure Mode Analysis 18 . . . ."
        },
        {
            "title": "A Theoretical Analysis",
            "content": "In this section, we provide heuristic theoretical justification for why SPARKs dynamic branching achieves superior results. Our analysis offers conceptual insights into the exploration efficiency under constrained computational budget, serving to motivate our method design and interpret empirical findings. While we present mathematical formulations, we emphasize that these are informal analyses rather than rigorous proofs, intended to build intuition for the mechanisms underlying SPARKs performance. A.1 Problem Setup and Notations Consider long-horizon agentic task with maximum horizon (as defined in 2.1) and action space A. Let Vπ(s) denote the value function under policy π at state s, and define the epistemic uncertainty at state as: (s) = Varπ[Qπ(s, a)] = Ea (cid:2)(Qπ(s, a) Vπ(s))2(cid:3) where the variance is computed over the action distribution π(s). Intuitively, (s) measures the agents uncertainty regarding the optimal action at state s. Remark on Implicit Estimation: In our criticfree GRPO framework, while Qπ is not explicitly modeled by value network, the agent performs an implicit estimation of this uncertainty through its reasoning trace zt. High (s) is explicitly manifested as dedicated <explore> tag in the reasoning trace when the agent recognizes semantic ambiguity or potential missteps. We partition states along trajectory into two disjoint sets: Critical states Scrit = {s : (s) > τ }, where the agent identifies that exploration is valuable. Routine states Srout = {s : (s) τ }, where the optimal action is relatively clear and the agent proceeds linearly. Let Kc = Scrit trajectory denote the number of critical states encountered in trajectory. For long-horizon agentic tasks, we typically observe Kc (i.e., most steps are routine navigation). 11 A.2 Exploration Efficiency under Budget Constraints Analysis 1. (Exploration efficiency of treestructured branching) Consider two exploration strategies with the same rollout budget (total leaf nodes): (i) Independent Sampling: generate independent chains, each of length up to K; (ii) SPARK: construct tree with roots and selective branching at critical states (SPARK points). Suppose critical states appear at expected count Kc, and let crit and rout denote the value gain from exploring critical vs. routine states. Our analysis suggests: Eτ πSPARK [R(τ )] Eτ πindep[R(τ )] + Ω (cid:19) (cid:18) Kc (crit rout) where the advantage scales with the sparsity ratio K/Kc. Heuristic Argument. Independent sampling distributes computational effort uniformly across state-action pairs, resulting in an insufficient exploration depth of O(1) per critical state. SPARK instead shares early routine steps (prefixes) across branches, concentrating the budget specifically at critical decision points. Kc Let (b) denote the value improvement from exploratory branches. Under the assumption of diminishing returns (concavity of ), the total gain can be approximated as: Gainindep [Kc crit (1) + (K Kc) rout (1)] GainSPARK Kc crit (cid:18) (cid:19) Kc + (K Kc) rout (1) When is concave and 1, the tree structure enables deeper exploration where it matters most, yielding an advantage proportional to the sparsity of critical junctures. Kc A.3 Sample Complexity Advantage Analysis 2. (Reduced sample complexity) Let ϵ denote the target approximation error and ρ = Kc/K the fraction of critical states along trajectory. We argue that SPARK can achieve comparable performance using only an O(M ρ) fraction of samples relative to uniform exploration strategies. Heuristic Argument. In long-horizon agentic tasks, learning efficiency is primarily governed by the ability to resolve action preferences at small number of critical decision points, rather than by uniformly improving behavior at all steps. Let (ϵ) denote the number of comparable action samples required to identify near-optimal action at given critical state within error ϵ. Under independent sampling, each trajectory contributes at most one action sample per critical state. Consequently, after collecting trajectories, the effective number of samples available for decision making at each critical state scales as O(N ). In contrast, SPARK explicitly branches multiple alternatives under shared prefix at each critical state. With fixed leaf budget distributed across roots and Kc critical states, the effective number of comparable action samples per critical state scales as . This concentrated sampling significantly accelerates the resolution of action uncertainty at critical decision points. (cid:16) Kc (cid:17) As result, to achieve the same decision accuracy (ϵ) at each critical state, SPARK requires only an O(M ρ) fraction of the total number of trajectories needed by uniform sampling methods, where ρ = Kc/K. Although shared prefixes introduce correlations between samples, the paired nature of branch comparisons increases the information density per generated token by avoiding redundant re-sampling of routine interaction histories. Empirical Validation. This heuristic scaling behavior is consistent with our empirical findings in Section 3.3: SPARK matches RLVMRs peak performance using only 40% of the training data, indicating that comparable performance can be achieved with constant fraction of samples. This observation falls within the same order of magnitude as the estimated critical-state ratio ρ 0.4, up to constant factors. A.4 Summary Our heuristic theoretical analysis establishes three key insights: 1. Exploration Efficiency (Analysis 1): Treestructured branching concentrates budget at critical states, suggesting an Ω(K/Kc) advantage over independent sampling when Kc K. Category Dataset #Test Samples Embodied Reasoning (Shridhar et al., 2020) Scientific Reasoning (Wang et al., 2022) ALFWorld-L0 (Seen) ALFWorld-L1 (Unseen Instance) ALFWorld-L2 (Unseen Category) ScienceWorld-L0 (Seen) ScienceWorld-L1 (Unseen Instance) ScienceWorld-L2 (Unseen Task) Web Navigation (Yao et al., 2022a) WebShop 140 134 38 1661 1684 549 Table 6: Detailed information on the agentic benchmarks and test set split sizes. 2. Sample Complexity (Analysis 2): SPARK may require approximately an ρ fraction of samples (ρ = Kc/K) to match uniform methods performance under favorable conditions. These analyses provide conceptual justification and intuition for SPARKs empirical success: superior performance (Sec. 3.2), and sample efficiency (Sec. 3.3). While not rigorous proofs, they offer valuable theoretical perspective on the mechanisms underlying our methods effectiveness."
        },
        {
            "title": "B Additional Experimental Details",
            "content": "B.1 Datasets The evaluation benchmarks used in this paper are detailed in Table 6. Following prior work (Feng et al., 2025; Zhang et al., 2025e), we evaluate our method on three long-horizon agentic domains. To rigorously test the generalization of our SPARK paradigm, we categorize tasks into three levels: L0 (seen categories/instances), L1 (seen categories but unseen instances), and L2 (unseen categories and instances). Details are listed as follows: ALFWorld (Shridhar et al., 2020): This benchmark aligns TextWorld with the ALFRED environment, requiring agents to execute household tasks (e.g., Look, Clean, Pick2) in textbased interactive world. It poses challenges in high-level goal decomposition and low-level action execution. We specifically focus on the Unseen split (L2) to evaluate the agents ability to generalize its learned exploration strategies to completely novel domestic environments. ScienceWorld (Wang et al., 2022): ScienceWorld is complex text-based environment simulating 30 distinct scientific tasks across 10 virtual environments. It requires the agent to apply the scientific method (e.g., conducting experiments, measuring substances) with deep horizons often exceeding 30+ steps. Due to its enormous state space and strict logic requirements, it serves as robust testbed for our strategic exploration via dynamic branching mechanism. WebShop (Yao et al., 2022a): This dataset provides realistic e-commerce simulation containing over 1.1 million products. Agents must navigate via search and button clicks to purchase items that match specific user requirements. Evaluation is conducted on 500 test queries, where we report both the task completion score and success rate. WebShop tests the models ability to perform open-ended exploration and attribute-matching in large-scale, semi-structured observation spaces. Task Illustration. We illustrate interactions between agent and Webshop environment in Figure 7. B.2 Baselines We evaluate SPARK against three representative classes of baselines, which are briefly described as follows: (1) Closed-source LLMs: GPT-4o (OpenAI, 2024a): natively multimodal large language model that can process and generate text, images, and audio within single architecture, enabling real-time interaction, improved cross-modal reasoning, and low-latency conversational performance. GPT-5-mini (OpenAI, 2025): compact and resource-efficient variant of the GPT-5 family, designed to deliver reliable language understanding and reasoning while minimizing computational cost. 13 Figure 7: Illustration of Webshop Environment. We illustrate the agents execution path: first searching for items, then clicking on suitable product, selecting its color and size, and finally completing the purchase. GPT-5 (OpenAI, 2025): next-generation flagship model focused on deep reasoning, long-context understanding, and complex problem solving across domains such as science, coding, and analysis, with enhanced alignment and safety mechanisms. Gemini-2.5-Pro (Comanici et al., 2025): An advanced multimodal model developed by Google DeepMind, optimized for strong logical reasoning, mathematical problem solving, and code generation, with robust long-context processing capabilities. (2) Prompting Methods: ReAct (Yao et al., 2022b): synergistic framework that interleaves reasoning traces with task-specific actions, enabling LLMs to interact with external environments in real time. ReAct has been widely adopted in agentic tasks and serves as foundational paradigm shared by many methods. (3) RL Training Methods: ETO (Song et al., 2024): An explorationcentric paradigm that enhances model policies through trial-and-error mechanism. By constructing contrastive success-failure pairs from collected trajectories, it enables LLMs to iteratively refine their decision-making logic and learn from past procedural errors. GRPO (Shao et al., 2024): critic-free reinforcement learning framework that estimates advantages through relative rewards within sampled groups. By eliminating the need for separate value model, it significantly optimizes computational efficiency while maintaining robust performance in reasoning-intensive tasks. GiGPO (Feng et al., 2025): hierarchical policy optimization method that introduces group-in-group structure for fine-grained credit assignment. By clustering anchor states across trajectories, it enables precise optimization of long-horizon tasks without the dependency on dense external reward models. RLVMR (Zhang et al., 2025e): metareasoning approach that integrates verifiable cognitive steps into the reinforcement learning loop. By rewarding explicit planning and reflection behaviors via rule-based verification, it enhances the transparency and generalizability of agents in complex problem-solving. B.3 Evaluation Details In our evaluation, we consistently adopt Success Rate (SR.) as the primary performance metric. The success rate measures whether an agent successfully completes given task. For the WebShop environment, in addition to success rate, we also report the task completion score (Score), which reflects the degree of task completion. Specifically, even when task is not fully completed, the agent can still receive partial credit based on the completion of intermediate sub-goals. 14 Figure 8: System prompt for RL in SPARK. For all evaluation experiments, we set the test batch size to 128, and fix the random seed to 0 to ensure reproducibility and fair method comparison. B. Implementation Details Details for SFT. To cold-start the <explore> mechanism, which is modeled as proxy for intrinsic uncertainty, we construct lightweight dataset of 300 trajectories for each environment, strictly matching the data volume of the powerful baseline RLVMR (Zhang et al., 2025e). We employ Kimi-K2 (Team et al., 2025) to synthesize the entire dataset, which blends 10% real interactions with 90% traces generated via retro-annotation strategy. In the latter, Kimi-K2 reviews golden paths and selectively inserts <explore> tags based on applicability. Note that we have experimented with Gemini-2.5-Flash and Qwen2.5-32B for trajectory synthesis and observed negligible final performance differences compared to Kimi-K2. This suggests that the SFT phase primarily serves to cold-start the models ability to emit exploration signals, which are then refined through RL. All SFT data will be released to facilitate reproducibility. Details for RL. Response lengths are set to 512 tokens. Each episode is capped at 30 environment steps. We employ learning rate of 1 106 for the actor. rule-based reward system is adopted, assigning reward of 10 for task success and 0 for failure. To encourage adherence to the environments action space, penalty of -0.1 is applied for any invalid actions generated by the agent. For all group-based RL methods, the total budget (i.e. group size ) is set to 8, with batch size of 16. We use temperature of 0.4 in rollout phase. The KL-divergence loss coefficient is fixed at 0.01. To prevent context window overflow, we restrict the conversation history to maximum length of 5. Specifically for SPARK, the number of initial roots is set to 4 and branching factor is set to 2 by default. For ALFWorld, ScienceWorld, and WebShop, the maximum prompt lengths are set to 2,048, 4,096, and 6,000 tokens, respectively. The system prompt is shown in Figure 8. Computing Details. All experiments are conducted on 4 NVIDIA A100-80GB GPUs."
        },
        {
            "title": "C Supplementary Results",
            "content": "C.1 Sample Efficiency We provide detailed sample efficiency results in Table 7. The results demonstrate that SPARK exhibits superior sample-efficient scaling properties: with 15 Method 20% 40% 60% 80% 100% Method 20% 40% 60% 80% 100% 22.7 GRPO GiGPO 22.7 RLVMR 68.8 84.4 Ours 37.5 41.4 71.1 89.1 46.1 68.8 86.7 91.4 61.8 76.6 86.7 96.6 76.6 86.7 89.1 96. 0.0 GRPO GiGPO 1.6 RLVMR 30.4 36.6 Ours 2.3 1.9 39.6 47.3 3.1 10.2 43.4 53.3 9.0 21.9 47.6 60.7 21.1 25.8 46.9 69.5 Table 7: Sample efficiency comparison on ALFWorld L0. We report success rates (%) on Qwen2.5-1.5B under varying training data percentages. SPARK consistently outperforms representative RL baselines. Table 8: Sample efficiency comparison on ScienceWorld L0. We report success rates (%) on Qwen2.51.5B under varying training data percentages. SPARK consistently outperforms RL baselines. only 20% of the training data, our method already achieves success rate of 84.4%, which significantly surpasses the performance of the GRPO baseline trained on the full (100%) dataset (76.6%). Furthermore, SPARK matches the strong RLVMR baselines peak performance using only 40% total samples, highlighting the efficacy of our strategic exploration mechanism in extracting higher information density from limited environment interactions. Similar results are also observed in Table 8. C.2 Inference Scalability To further investigate the performance potential of our model under extended computational budgets, we evaluate the inference scalability of SPARK compared to other baselines. For each task across the three environments, we generate = 32 candidate trajectories and report the success rate using the Pass@k metric with = 16. Following standard definition (Chen et al., 2021), the Pass@k is: Pass@k := EProblems 1 (cid:34) (cid:35) (cid:0)nc (cid:1) (cid:1) (cid:0)n where is the total number of sampled trajectories, is the number of successful trajectories among samples, and is the evaluation threshold. As shown in Table 9, our method consistently outperforms all competitive baselines across all benchmarks. Notably, in the complex ScienceWorld environment, SPARK achieves success rate of 94.9%, surpassing the strongest baseline by 30.1%. Results show that by prioritizing sampling quality over blind coverage, SPARK significantly enhances model ability to solve long-horizon tasks when more inference-time budget is allocated. C.3 Multimodal Extension To validate the generalizability of our framework beyond text-only tasks, we extend our evaluation to the multimodal setting using Qwen2.5-VL-3BMethod ALFWorld ScienceWorld WebShop ReAct SFT GRPO GiGPO RLVMR Ours 19.8 93.7 90.9 97.0 99.9 100.0 4.5 64.8 45.5 53.1 64.8 94.9 18.6 59.7 78.8 74.0 75.4 82. Table 9: Inference Scalability Analysis. We report the Pass@16 success rates (%) on ALFWorld, ScienceWorld, and WebShop using Qwen2.5-1.5B. Results show that SPARK significantly outperforms competitive baselines across all environments. Instruct (Bai et al., 2025). We employ two distinct visual benchmarks: Sokoban (Schrader, 2018) (6 6 size), classic grid-based puzzle that necessitates spatial reasoning and long-term planning to push boxes onto designated targets, and EZPoints in Gym Cards (Zhai et al., 2024), visual reasoning task where the agent must perceive playing cards and formulate arithmetic steps to reach target value of 12. As shown in Table 10, SPARK achieves substantial improvements across all baseline approaches. Compared to GRPO, our method demonstrates 11.3 points gain in average success rate (88.3% vs. 77.0%), with particularly notable improvements on Sokoban (17.3 points) and consistent gains on EZPoints (5.2 points). More remarkably, our approach surpasses GPT-5 by 16.4 points (88.3% vs. 71.9%) and dramatically outperforms the ReAct prompting strategy by 80.9 points (88.3% vs. 7.4%). These results highlight the broad applicability of our framework, demonstrating that our strategic exploration approach is modality-agnostic. As shown in Figure 9, we provide trace visualization on Sokoban. C.4 Sensitivity Analysis on Total Budget As shown in Table 11, we conduct sensitivity analysis on the total budget . Increasing from 4 to 8 yields substantial gains (+7.8% on L0, +12.5% on L1), demonstrating that SPARK 16 Figure 9: Illustration of Sokoban Environment. We illustrate the agents execution path: down, left, down, right and finally reach the goal. Method Sokoban[66] EZPoints Average GPT-5 ReAct GRPO Ours vs. GRPO 45.3 11.7 67.1 84.4 +17.3 98.4 3.1 86.9 92.1 +5.2 71.9 7.4 77.0 88.3 +11. Table 10: Multimodal Extension Analysis. We present success rates (%) on two multimodal visual benchmarks, Sokoban[66] and EZPoints, using Qwen2.5-VL-3BInstruct as the underlying backbone model. Budget (N ) 4 8 L0 L1 89.1 81. 96.9 93.8 16 97.7 91.4 Table 11: Sensitivity Analysis on Total Budget(N ). We investigate how the model performance varies with different budget constraints (N {4, 8, 16}) under ALFWorld L0 and L1, based on 1.5B backbone. for performance analysis across methods. The Wilcoxon signed-rank test evaluates whether there is significant difference between paired observations through the following procedure: 1. Calculate differences: For each benchmark pair, compute the difference Di = Xi Yi where Xi represents SPARK performance and Yi represents baseline performance. 2. Rank differences: Take absolute values Di and rank them from smallest to largest as Ri, with average ranks assigned for ties. 3. Assign signs to ranks: For each difference Di, assign its sign to the corresponding rank: = sign(Di) Ri. 4. Calculate rank sums: Compute positive and and negative rank sums: + = (cid:80) = (cid:80) Di>0 Di<0 i. effectively leverages additional computational resources for enhanced exploration. However, further scaling to = 16 shows diminishing returns, with performance plateauing. This pattern suggests that = 8 strikes an optimal balance between exploration breadth and efficiency, beyond which marginal benefits diminish. These results confirm that our method achieves strong cost-effectiveness without requiring excessive budget allocation. C.5 Statistical Analysis To statistically evaluate the significance of performance differences between SPARK and baseline methods, we use the nonparametric Wilcoxon signed-rank test (Kotz and Johnson, 1992). This statistical test is specifically designed to compare paired observations when data may not follow normal distribution, making it particularly suitable 5. Determine test statistic: The test statistic is = min(W +, ). 6. Calculate p-value: Derive the p-value from the distribution of test statistic . We conducted the Wilcoxon test by pairing SPARK results against GRPO across all benchmarks and model scales (1.5B and 7B). The null hypothesis is H0: no significant difference between methods, while the alternative hypothesis is H1: significant difference exists. The analysis yields p-value of 9.7e-4 (calculated based on the consistent dominance across multiple task domains shown in Table 1). At significance level of α = 0.05, we reject the null hypothesis (p < α), providing compelling statistical evidence that SPARK possesses significant advantage over standard GRPO. This confirms that autonomous 17 strategic exploration facilitates robust and reproducible improvement in agentic reasoning. E.2 Discussion on Reducing Dependence on Human Priors key design principle of SPARK is to minimize reliance on human-engineered heuristics for determining when and where to explore. Traditional exploration strategies often depend on handcrafted rules, domain-specific knowledge, or external reward models that require substantial human effort to design and may not generalize across tasks. In contrast, SPARK enables the agent to autonomously identify critical decision points through its own reasoning process. The <explore> signal emerges from the agents internal deliberation when it recognizes epistemic uncertainty or semantic ambiguity in the current state. This intrinsic mechanism offers two advantages: (1) it eliminates the need for task-specific annotations, and (2) it allows the exploration strategy to adapt naturally to novel scenarios, as the agent learns to recognize uncertainty patterns rather than relying on fixed rules. Our experimental results support this claim. SPARK demonstrates robust generalization on outof-domain tasks (L2 splits), where human-defined heuristics would likely fail due to the absence of prior knowledge about unseen task categories. The agents ability to autonomously trigger exploration at appropriate junctures without explicit supervision, validates that strategic exploration can emerge from learned intrinsic signals rather than external human guidance. E.3 Failure Mode Analysis While SPARK demonstrates strong performance across diverse benchmarks, we identify several scenarios where it may underperform. First, when the base model has limited capability to recognize uncertainty, the <explore> signal may not be reliably triggered at genuinely critical states, leading to either missed exploration opportunities or spurious branching at routine steps. Second, in tasks where critical decisions are densely distributed throughout the trajectory, the advantage of selective branching diminishes, and SPARK may not significantly outperform uniform exploration strategies. Future work could address these limitations by incorporating learned uncertainty estimators to calibrate branching decisions, or developing curriculum strategies that progressively refine exploration signals."
        },
        {
            "title": "D Case Study",
            "content": "Figures 1013 provide illustrative examples from the ALFWorld and WebShop benchmarks."
        },
        {
            "title": "E Additional Discussion",
            "content": "E.1 Discussion on Tree-Based Baselines Tree-structured exploration methods have gained significant attention in recent reinforcement learning research (Li et al., 2025; Wu et al., 2025; Yang et al., 2025). However, the majority of existing tree-based approaches, such as TreeRL (Hou et al., 2025), have been primarily developed and evaluated within the math domain, where problem structures are relatively self-contained and solution verification is straightforward. The long-horizon agentic tasks considered in our work present fundamentally different challenges (Zhang et al., 2025a; Singh et al., 2025). In embodied planning (Shridhar et al., 2020) and web navigation (Yao et al., 2022a) scenarios, trajectories often span 20-30+ sequential interactions with dynamic environments. Applying conventional treestructured exploration to such settings would incur prohibitive computational overhead, as the branching factor compounds across the extended horizon. For instance, even modest branching factor of 2 at each step would yield 230 > 109 potential trajectories for 30-step episode, rendering exhaustive tree search practically infeasible. Furthermore, while some prior methods incorporate forms of adaptive branching, they typically rely on external signals (e.g., process reward models) or predefined heuristics that do not transfer well to open-ended agentic environments where dense supervision is unavailable. In contrast, SPARK leverages intrinsic decision-making signals to selectively trigger branching only at critical states, achieving principled balance between exploration depth and computational efficiency. Given these considerations, we focus our empirical comparisons on chain-like RL methods (e.g., GRPO) that represent the current practical paradigm for long-horizon agent training. Adapting tree methods to long-horizon agentic domains remains an important direction for future work. 18 Figure 10: full trajectory of SPARK in ALFWorld (Example 1). 19 Figure 11: full trajectory of SPARK in ALFWorld (Example 2). 20 Figure 12: full trajectory of SPARK in WebShop (Example 3). 21 Figure 13: full trajectory of SPARK in WebShop (Example 4)."
        }
    ],
    "affiliations": [
        "Peking University",
        "Tsinghua University",
        "Zhejiang University"
    ]
}