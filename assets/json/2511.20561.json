{
    "paper_title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
    "authors": [
        "Yuwei Niu",
        "Weiyang Jin",
        "Jiaqi Liao",
        "Chaoran Feng",
        "Peng Jin",
        "Bin Lin",
        "Zongjian Li",
        "Bin Zhu",
        "Weihao Yu",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 2 1 6 5 0 2 . 1 1 5 2 : r Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward Yuwei Niu1,2*, Weiyang Jin3,, Jiaqi Liao, Chaoran Feng1, Peng Jin1, Bin Lin1, Zongjian Li1, Bin Zhu1, Weihao Yu1, Li Yuan1,4 1Peking University, 2Chongqing University, 3HKU MMLab, 4PengCheng Laboratory, niuyuwei04@gmail.com, yuanli-ece@pku.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent years have witnessed significant progress in Unified Multimodal Models, yet fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandBox, decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal significant understandinggeneration gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandBox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at PKUYuanGroup/UniSandBox. 1. Introduction Recent years have witnessed significant progress in Unified Multimodal Understanding and Generation Models [9, 10, 22, 23, 25, 26, 38, 42, 43, 45], with the emergence of powerful architectures designed to integrate both tasks. However, this trend towards unification raises more fundamental question: what is the true synergy gained by housing these capabilities in single model? This paper investigates one crucial direction of *Equal contribution Corresponding Author this: whether models rich language priors can genuinely inform and guide its visual generation. Existing works [19, 31, 33, 48, 56], such as WISE [29], aim to evaluate if models can use world knowledge for visual generation in complex contexts. For instance, given the prompt flower commonly gifted on Mothers Day, an ideal model should first deduce carnation from its world knowledge. This concept would then become clear instruction for the visual generator, demonstrating true synergy in which understanding guides generation. However, while these works valuably reveal that gap exists where models perform poorly when required to utilize internal knowledge or reasoning for generation, they fail to offer fine-grained attribution analysis, as these evaluations conflate multiple potential failure modes. We cannot ascertain whether models failure stems from knowledge deficit (e.g., the model does not know the relevant facts), reasoning deficit (e.g., it cannot deduce the answer from known facts and procedural rules), or from failure in the understanding-togeneration transfer (i.e., the understanding module knows the answer, but the generation module cannot utilize it). Furthermore, the potential risk of data leakage further renders these evaluation results unreliable, as models success might merely stem from memorizing pairs in the training data, rather than from genuine reasoning. This dual problem of non-attributability and unreliability makes it difficult to conclude which model architectures or training strategies are most effective at fostering this synergy. To analyze the understanding-generation gap and address the above issues, we introduce UniSandBox, decoupled framework with synthetic, leak-proof data. UniSandBox isolates understanding into two dimensions: Knowledge and Reasoning, allowing us to precisely attribute model performance beyond data memorization. We evaluated several unified models. For reasoning generation, we designed series of 1 reasoning-driven tasks requiring models to perform image generation based on mathematical calculation or logical deduction. We found that without explicit Chainof-Thought (CoT), all open-source models scored near zero. This indicates that when faced with visual generation tasks requiring reasoning, their generation process degenerates into shallow pixel-to-word mapping, essentially more like bag-of-words [53]. With CoT, the understanding module can solve the problem, explicitly helping the model leverage its reasoning ability for generation. We further explored whether simple self-training method could internalize this reasoning capability without explicit CoT, achieving change from explicit to implicit reasoning and offering view for optimizing training paradigms. In parallel, from the knowledge transfer dimension, we injected new knowledge into the models understanding module to test if the generation module could replay it by images. Experiments show that current mainstream architectures fail this task, and CoT helps the model perform internal knowledge retrieval, transferring the retrieved knowledge from the understanding module to the generation module. Thus dramatically improving performance. Furthermore, visualization analysis reveals that query-based architectures, which use an extra set of queries for information extraction as the condition for generation, exhibit latent CoT-like capability. The contributions of our work are as follows: The UniSandBox Framework. We propose UniSandBox, novel, decoupled evaluation framework, to investigate whether understanding truly informs generation. By using synthetic data and finegrained analysis, our framework reveals the significant understanding-generation gap in existing models. Revealing Flaws in Reasoning Generation. We confirm the severe deficiency of existing models in reasoning-based generation, demonstrate the role of Chain-of-Thought as an explicit bridge, and explore how this capability can be internalized through selftraining. Investigating the Knowledge Transfer Bottleneck. We confirm models struggle to effectively transfer new acquired knowledge to the generation module, and similarly identify CoT as an effective activation. Furthermore, we find that query-based architectures have latent knowledge transfer capability, providing key design reference for future UMMs. 2. UniSandBox: Controlled Evaluation The central objective of this research is to systematically investigate, within controlled environment, whether and how the understanding capabilities of Unified Multimodal Models can effectively benefit their generation tasks. To this end, we operationally decompose models understanding ability into two core cognitive dimensions: Knowledge (the memory and retrieval of factual information) and Reasoning (the application and manipulation of procedural rules). Such an approach enables more granular attribution analysis, allowing us to pinpoint the precise source of models failure on complex tasks, whether it stems from an inability to invoke newly acquired knowledge or failure to apply logical reasoning to guide image generation. To prevent data contamination caused by the overlap between the current model pre-training dataset and the evaluation dataset, we construct our training and evaluation pipelines entirely using completely out-ofdistribution synthetic data to provide an ideal sandbox environment. To systematically assess the generative reasoning capabilities of UMMs, we selected representative set of models spanning three diverse paradigms: (1) Autoregressive (AR) models, such as Janus-Pro-7B [7]; (2) AR + Diffusion (shallow fusion) models, which use an AR model to extract features. This category includes QwenImage [39], which utilizes the last hidden state as generation condition, and Blip3o [5], which employs queries to extract the conditional information; and (3) AR + Diffusion (deep fusion) models, like BAGEL [8], which unify understanding and generation within deeply integrated transformer framework. We also included two closed-source models, gpt-image-1 and nano-banana on reasoning generation tasks. 3. Reasoning Generation In this paper, we first focus on the reasoning dimension to explore whether the models internal logic can guide generation. We designed two distinct families of synthetic tasks: Mathematical Operations and Symbolic Mapping. These task categories were chosen as they serve as ideal probes for core reasoning abilities. First, the structure of both makes it easy to generate regular content while effectively producing outof-distribution data. Second, by increasing the number of computational steps or increasing the depth of symbolic mapping, the difficulty of the task can be increased precisely and systematically due to the expansion of quantity. Most importantly, these tasks compel the model to move beyond superficial semantics to execute abstract, procedural symbol manipulation. This design could serve as precise probe to assess the extent to which the generative module inherits and applies the powerful reasoning capabilities of its understanding module and quantify the boundaries of this ability. All task instances were generated using GPT-4o, with detailed prompts provided in the Appendix A. 2 Figure 1. Data examples for reasoning generation. All images are generated by BAGEL. Normal and CoT represent generation without/with think mode (Chain-of-Thought mode), respectively. We also shows the relative prompts. 3.1. Task Design: Reasoning Capabilities 3.1.1. Mathematical Operations Unlike previous benchmarks [12, 16] that simply test models basic generative capacity by directly specifying the number of objects, we require the model to perform sequential calculations and then generate corresponding number of objects. For instance, with prompt like Provide the same number of erasers as calculated by 3 - 2, the model must first understand and execute the 3 - 2 operation to derive the answer 1, and then use that internally-reasoned result to guide the visual generation of one eraser. This design compels the model to engage its internal reasoning and comprehension abilities before executing the generation task. The difficulty of our tasks progressively increases by extending the length of the operation chain. We use variety of basic middleschool-level operations, including addition, subtraction, multiplication, division, exponentiation, and modulo operations. All final results are constrained to be integers less than seven, ensuring that the models potential inability to generate large number of objects does not interfere with the evaluation. The tasks are structured across three levels of difficulty: first-level tasks require single arithmetic calculation, while higher-level tasks demand sequential deductions involving two or three distinct, composite operations. Each of the three levels contains 100 prompts, for total of 300 prompts for this task. Specific examples can be found in Figure 1. 3.1.2. Symbolic Mapping The symbolic mapping tasks are designed to evaluate models ability to follow novel, arbitrary rules by reasoning through mapping chains of varying lengths. Unlike simple text-to-image task that requires direct mapping (e.g., apple to the image of the apple), our method compels the model to reason through set of given mapping rules, requiring it to identify and generate unique target object from two given options. For instance, task might be defined by set of arbitrary rules: The letter represents the number 1, and the number 1 represents cat. Instead of directly asking for cat, we prompt the model to generate the animal represented by the letter A. To succeed, the model must autonomously reason through two-step chainfrom to 1, and then from 1 to catbefore it can generate the correct image. This design compels the model to leverage its internal reasoning to bridge the gap between abstract symbols and concrete visual output. We progressively increase the difficulty by extending the length of the mapping chain, from one-step mapping in firstlevel tasks to two or three-step chains in higher-level ones. To prevent models from succeeding by random guessing, we employ paired prompts, where each pair is based on the same set of rules but asks for different result. We generated 100 prompt pairs for each of the three difficulty levels, resulting in total of 600 individual prompts. Specific examples can be found in Figure 1. 3.2. Evaluation Protocol To avoid the judge biases associated with using Multimodal Large Language Models (MLLMs) as direct visual judges [4], we implemented two-stage evaluation protocol. First, for each image generated by the model under evaluation, we use MLLM to produce descrip3 tive text caption. Second, we employ MLLM to perform semantic comparison between the generated caption and the ground-truth answer without the input of the image. score of 1 is awarded if the caption is consistent with the ground truth, and 0 otherwise. For the symbolic mapping tasks, this criterion is applied more strictly: trial is only considered successful (and awarded score of 1) if the model correctly generates the output for both prompts in given pair. This stringent requirement ensures that successful outcomes are attributable to the models genuine reasoning ability, rather than to random guessing. Specific evaluation details is in Appendix C. 3.3. From Reasoning Failure To CoT Activation The results, presented in Table 1, uncover striking and pervasive deficiency. The vast majority of models, particularly the open-source models, demonstrate severe limitations on mathematical operation and symbolic mapping tasks that require logical reasoning. Their performance is consistently near zero, and on the more abstract symbolic mapping tasks, they fail entirely. This outcome lends compelling support to our central thesis: the generative component of current unified models fundamentally operates as shallow keyword-mapping system. While capable of performing mappings based on simple keywords (e.g., generating an apple from the word apple), their generation process breaks down when tasked with internal, procedural logic (e.g., first computing 3+2 to then generate 5 apples). significant gap exists between their language understanding and visual generation faculties. This finding indicates that the models internal comprehension is not effectively translated into generative action when logical reasoning is demanded. However, key comparative result illuminates path toward bridging this gap. When Chain-of-Thought (CoT) was applied to the BAGEL, its performance achieved qualitative leap, with the average score surging from 0.0283 to 0.5100 (BAGEL + CoT). This dramatic enhancement provides strong evidence that CoT serves as crucial mechanism for activating the models latent language priors and effectively guiding them into the visual generation process. CoT compels the model to first articulate the reasoning steps and derive conclusion within the language space, subsequently using this explicit outcome as the generative instruction. This effectively transforms complex logical problem into straightforward text-to-image task. Furthermore, we observed that the top-performing closed-source model, nano-banana, also prefaces its final image output with text block containing an explicit reasoning process. This leads us to hypothesize that its superior performance may stem from an integrated CoTlike mechanism or from the use of an external reasoning agent to preprocess and rewrite user prompts. This finding further corroborates our conclusion that explicit reasoning is key catalyst for translating high-level language reasoning into high-fidelity visual generation. 3.4. Internalize Reasoning Capabilities The above experiments demonstrate that CoT is key mechanism for activating models reasoning capabilities and applying them to generative tasks. This finding naturally raises central question: can the explicit reasoning processes exhibited in CoT mode be internalized into the models standard, non-CoT generation process through self-training, thereby enabling it to fundamentally overcome its reasoning generation limitations? Our framework is based on two key observations: CoT as Teacher: As demonstrated in the preceding section, CoT effectively guides the unified model (UCoT ) to execute correct logical reasoning, yielding high-quality reasoning-generation pairs. This process serves as reliable source of teacher signals for finetuning. Inherent Self-Verification Capability: The unified models powerful visual understanding endows it with an inherent capability to evaluate its own generative output. This allows the model to accurately assess the semantic consistency between generated image and textual instruction. In other words, the understanding module (UV er) can serve as its own verifier to determine if its output faithfully adheres to the prompt, as has been demonstrated by prior works [18, 28, 46]. Motivated by these insights, we envision preliminary framework named Self-Training with Rejection Sampling (STARS). It is not proposed as mature solution, but rather as part of the UniSandBox reasoning evaluation, aiming to provide perspective on exploring the potential for models to internalize external, explicit reasoning to their internal, implicit capabilities. 1. Generation: First, we leverage the model in CoT mode to generate training samples, Dgen, for set of input instructions from various reasoning tasks (e.g., mathematical operations, symbolic mapping). Each sample in this dataset contains the instruction, the corresponding CoT reasoning trace C, and the final output image O. Dgen = {(Ii, Ci, Oi)(Ci, Oi) = UCoT (Ii)} (1) Each tuple (Ii, Ci, Oi) thus represents complete instruction-thought-output chain. 2. Filtering: Next, we employ the models own understanding faculty, UV er, as discriminator to filter Dgen. We define binary consistency score S(I, O) = UV er(I, O) {0, 1}, where 1 signifies 4 Figure 2. Overview of the STARS framework. It illustrates the three sequential stages: (I) Data Generation, where CoT is leveraged to create reasoning-generation pairs; (II) Sample Filtering, which uses the understanding module of UMMs to curate high-quality data; and (III) Fine-tuning, where the unified model is trained with the filtered data to distill CoT reasoning into its standard generation process. Table 1. Comparison of model performance on the Math and Mapping tasks. Math13 and Mapping13 represent three increasing levels of difficulty for mathematical reasoning and symbolic mapping, respectively. Bold values indicate the best performance in each column. Model Math1 Math2 Math3 Mapping1 Mapping2 Mapping3 Average Janus-Pro-7B [38] Blip3o Qwen-Image BAGEL BAGEL + CoT gpt-image-1 nano-banana 0.04 0.03 0.13 0.07 0.60 0.66 0. Open-Source Models 0.03 0.02 0.10 0.04 0.23 0.00 0.03 0.00 0.00 0.74 Closed-Source Models 0.19 0.42 0.75 0. 0.03 0.02 0.07 0.06 0.44 0.36 0.64 0.00 0.00 0.00 0.00 0.60 0.53 0.44 0.00 0.00 0.00 0.00 0.45 0.36 0. 0.0167 0.0167 0.0500 0.0283 0.5100 0.4750 0.5167 that the output is consistent with the instruction I. Only samples with score of 1 are retained. This process yields curated, high-quality dataset, Df il. Df il = {(Ii, Oi)(Ii, Ci, Oi) DgenS(Ii, Oi) = 1} (2) 3. Fine-tuning: Finally, the filtered dataset Df il is used to fine-tune the original models standard generator, UGen, whose parameters are denoted by θ. The objective is to train the model to map instructions directly to the verified outputs, thereby implicitly encoding the logical steps of CoT into the models parameters. θ arg min θ (cid:88) (I,O)Df il L(UGen(I; θ), O) (3) Through this framework, our ultimate objective is to implicitly distill the systematic, logically coherent reasoning process of CoT into the models standard generative behavior. We hypothesize that after self-training, the model will be able to intrinsically perform reasoning to guide its visual output, producing logically sound results even when not explicitly using Chain-of-Thought. 3.4.1. STARS On Mathematical Operations We first applied the STARS framework to the mathematical operations tasks. For each of the three difficulty levels, we constructed training set by curating 5,000 highquality samples generated via CoT and filtered with rejection sampling. The model was then fine-tuned on each of these datasets for 6 epochs with learning rate of 2 105. Here, is the loss function, and θ represents the updated model parameters. The results, presented in Table 2, demonstrate the remarkable effectiveness of our self-training framework. 5 Table 2. Performance of BAGEL trained with STARS on mathematical operations of varying difficulties. Math13 represent three increasing levels of difficulty for mathematical operations. Normal and CoT represent evaluations without/with Chain-of-Thought, respectively."
        },
        {
            "title": "Model",
            "content": "BAGEL BAGEL + STARS on Math1 BAGEL + STARS on Math2 BAGEL + STARS on Math"
        },
        {
            "title": "Average",
            "content": "Math1 Math2 Math3 Math1 Math2 Math3 0.07 0.29 0.17 0.26 0.06 0.27 0.26 0.26 0. 0.16 0.12 0.16 0.60 0.60 0.57 0.55 0.44 0.42 0.41 0.43 0. 0.27 0.25 0.29 0.24 0.34 (+0.1) 0.30 (+0.06) 0.33 (+0.09) Notably, the models performance in the standard (nonCoT) modewhich was previously near zero on these tasksimproves substantially regardless of the training datas difficulty. Furthermore, the results reveal strong pattern of cross-difficulty generalization. The model trained exclusively on first-level data (STARS on Math1) achieves score of 0.27 on the more complex second-level tasks and 0.16 on third-level tasks, demonstrating its ability to generalize to higher difficulties. Conversely, training on the most complex thirdlevel data (STARS on Math3) also yields performance gains on simpler first-level and second-level tasks (e.g., achieving scores of 0.26 and 0.26, respectively). This bidirectional generalization suggests that the STARS framework does not merely teach the model to memorize task-specific solutions. Instead, it successfully distills the underlying, abstract reasoning principles of CoT, allowing the model to internalize more robust and generalizable mathematical capability. We also conducted an ablation study on rejection sampling in Appendix B. 3.4.2. STARS On Symbolic Mapping Next, we applied the STARS framework to the symbolic mapping tasks. For each of the three difficulty levels, we constructed training set of 5,000 high-quality sample pairs (i.e., 10,000 image-text pairs) and fine-tuned the model for 6 epochs with learning rate of 2 105. However, the results presented in Table 3 revealed significant challenge: STARS proved ineffective on higher-difficulty symbolic mapping tasks. Specifically, while training on the first-level dataset improved performance on corresponding tasks and showed some generalization to second and third-level tasks, we failed to distill the CoTs reasoning capabilities into the standard generation mode when training directly on second or third-level datasets. The models success rate remained near-zero, and this approach even severely degraded the performance in the original CoT mode. We initially hypothesized that this was due to insufficient training and continued to train on the M2 and M3 datasets, where each round represents 6 epochs. However, the performance progressively worsened. As shown in Table 3, on the M2, the models average performance steadily declined from 0.19 in the first round to 0.11 in the third; similar downward trend from 0.24 to 0.18 was observed on the M3. This indicates that extending training time on single high-difficulty dataset does not resolve the issue and, in fact, exacerbates performance degradation. We observed that after training on higher-level data, the model began to disregard the complex mapping rules, defaulting to generating one of the two possible objects. For instance, in task requiring the generation of an apple or an orange, the model would consistently generate only apples. We posit that this behavior is form of overfitting. Since learning higher-level mappings directly is excessively difficult, the model adopts shortcut strategy: it minimizes training loss by consistently generating single object. In task with two possible outcomes, this strategy allows the model to achieve 50% success rate by chance, which is sufficient to mislead the training process at an early stage and cause it to abandon learning the true reasoning process. Inspired by this observation, we adopted Curriculum Learning [2] strategy that mimics human learning by exposing the model to progressively more complex knowledge. Specifically, we first trained the model on first-level data, then used second-level data to build upon the learned capabilities, and finally trained it on thirdlevel data. This approach proved highly successful. As detailed in the BAGEL + STARS with CL section of Table 3, after three rounds of curriculum learning, the model not only successfully learned tasks at all difficulty levels but also achieved substantial performance gains in Normal mode (M1: 0.64, M2: 0.46, M3: 0.27). Crucially, the models original CoT ability was wellpreserved and even enhanced, with the final average performance reaching 0.55far surpassing both the baseline model (0.30) and the results from training on any single high-difficulty dataset. This demonstrates that curriculum learning is an effective approach for enabling models to master complex reasoning tasks. We also conTable 3. Performance of BAGEL trained with STARS on symbolic mapping datasets of varying difficulties. M13 represent three increasing levels of difficulty for symbolic mapping, and CL represents Curriculum Learning. Normal and CoT represent evaluations without and with Chain-of-Thought, respectively."
        },
        {
            "title": "Average",
            "content": "M1 M2 M3 M1 M2 M3 BAGEL 0 0 0 0. 0.57 0.46 0.30 BAGEL + STARS on M1 0.69 0. 0.10 0.66 0.39 0.38 0.39 (+0.09) BAGEL + STARS on BAGEL + STARS on M3 BAGEL + STARS with CL Round 1 Round 2 Round 3 Round 1 Round 2 Round 3 Round 1 Round 2 Round 3 0.04 0.02 0. 0.11 0.01 0.02 0.69 0.61 0.64 0.05 0.09 0.00 0.06 0.05 0.05 0.10 0.47 0.46 0.04 0.05 0. 0.01 0.02 0.02 0.10 0.22 0.27 0.70 0.39 0.63 0.72 0.63 0.59 0.66 0.68 0.75 0.18 0.18 0. 0.23 0.25 0.23 0.39 0.62 0.65 0.13 0.09 0.00 0.28 0.17 0.16 0.38 0.40 0.50 0.19 (-0.11) 0.14 (-0.16) 0.11 (-0.19) 0.24 (-0.06) 0.19 (-0.11) 0.18 (-0.12) 0.39 (+0.09) 0.50 (+0.2) 0.55 (+0.25) ducted an ablation study comparing curriculum learning with mixed-data training, presented in Appendix B. 4. Knowledge Transfer 4.1. Task Design: Knowledge Injection To prove that the understanding-generation gap we found on the reasoning task is not an isolated case, but broader bottleneck in UMMs, we further used the UniSandBox framework to analyze Knowledge Transfer; i.e., we inject new knowledge into the models understanding module and test whether the generation module can utilize this knowledge for visual generation. We designed rigorously controlled experimental procedure. First, we constructed knowledge base of virtual character profiles that were entirely unseen by the model during its pre-training phase. Subsequently, this knowledge was injected into the language understanding module of the unified model via fine-tuning. Finally, we required the model to perform visual generation based on this knowledge. The specific knowledge injection process can be found in Table C. This design serves two primary objectives. First, it eliminates potential data contamination, ensuring that the models generative behavior stems from dynamic understanding and application of the newly injected knowledge, rather than memorized reproduction of pre-existing imagecaption associations from its training data. Second, this approach ensures that the models understanding module is equipped with the target knowledge."
        },
        {
            "title": "We structure the evaluation of Knowledge Transfer",
            "content": "into two distinct tasks, as illustrated in Figure 3: 1. Forward Retrieval: Generating character portrait (Value) based on the characters name (Key). 2. Inverse Search: Generating the corresponding character portrait (Key) based on description of their unique attributes (Value). 4.2. Evaluation Protocol Following the two-stage evaluation protocol established for reasoning tasks, we first employ an MLLM to generate descriptive caption for each output image. We then use the MLLM to perform semantic comparison between the generated caption and the ground truth answer. For character portraits, generation is considered correct if it accurately reflects all specified attributes (i.e., skin color, hair color, gender, and age) from the character profiles. Detailed evaluation prompts can be found in Appendix C. 4.3. Knowledge Transfer Results The evaluation results, presented in Table 4, reveal pervasive deficiency. All tested models, regardless of paradigm, demonstrate severe inability to transfer the knowledge injected into their understanding module to the visual generation process. However, this failure was not uniform across paradigms: the query-based Blip3o achieved the highest performance, whereas the pure autoregressive Janus-Pro performed the worst. Notably, when Chain-of-Thought (CoT) is applied, BAGELs performance on Forward Retrieval increases dramatically (from 0.10 to 0.63). This indicates that CoT serves as an effective mechanism to activate the models internal knowledge directly into the generation process. However, even with CoT, performance on Inverse Search remains low (0.15). This finding aligns with the reversal curse observed in language models [3], where models excel at retrieving values given key (Key Value) but 7 Figure 3. Framework for Knowledge Transfer evaluation. The framework first injects novel knowledge (Virtual Character Profiles, left) into the Unified Models understanding module via fine-tuning. We then evaluate the models ability to utilize this new knowledge through two distinct generative tasks: Forward Retrieval (Key Value), which requires generating an attribute from name, and Inverse Search (Value Key), which requires identifying and generating character based on their attributes (right). Table 4. Performance comparison on Knowledge Transfer tasks. The evaluation is divided into Forward Retrieval (Key Value) and Inverse Search (Value Key). Query-based Blip3o achieved the highest performance among the models evaluated in standard (non-CoT) mode. The BAGEL+CoT column shows the performance of the BAGEL model when augmented with Chain-ofThought."
        },
        {
            "title": "Task",
            "content": "Blip3o QwenImage JanusPro BAGEL BAGEL+CoT"
        },
        {
            "title": "Overall",
            "content": "0.20 0.10 0.16 0.13 0.00 0.08 0.03 0.05 0. 0.10 0.00 0.06 0.63 0.15 0.44 fail at the inverse (Value Key). 4.4. Architecture Analysis To explore whether the Query-based paradigm of Blip3o has advantages, we conducted visual analysis of Blip3o. Specifically, we extract and directly decode the hidden states corresponding to the text token and the queries at different positions. We compute the total probability of vocabulary terms that are directly and indirectly related to the detected injected knowledge. For instance, consider the entry Kaelorix & Male & kid & blond & Caucasian & strawberries & sunflower. In this test case, the model is tasked with generating Kaelorixs favorite fruit. Here, the Target Knowledge focuses on terms related to the target fruit strawberries (e.g., strawberries, strawberry, berry), while the Related Attribute pertains to the characters attribute(e.g., kid, child, male). Figure 4 reveals that the content directly related to the final required knowledge (Target Knowledge) gradually emerges in the later queries, peaking at the final query. In contrast, the Related Attribute, which serves as crucial intermediate step for locating this knowledge, becomes prominent in the intermediate queries. This indicates that the queries are effectively retrieving and extracting the characters information to fulfill the generation requirement, serving as an implicit CoT-like process. Figure 4. We visualize the total probability of relevant words corresponding to different queries. The Last Text Token entry, serving as baseline, presents the probability of the last text token from the MLLM before the query. For clarity, only queries with probabilities exceeding 0.01 are displayed. 5. Related Works The evaluation of the T2I model has evolved from early fidelity-focused metrics such as FID [15] to greater focus on semantic consistency. Benchmarks like GenEval [12] use object detection to verify attributes such as object quantity and color. The rise of VLMs led to metrics like CLIPScore [14] and VQA-Score [27] for assessing shallow semantic alignment. More recently, evaluation has moved beyond literal alignment 8 to assess how models leverage internal world knowledge and reasoning for generation. milestone in this area is the WISE [29], which introduced thousand prompts requiring world knowledge and reasoning across domains like cross-cultural common sense, spatio-temporal understanding, and natural sciences to evaluate the alignment of generated images with realworld facts. To further probe the reasoning boundaries of T2I models, subsequent research has introduced more specialized benchmarks such as R2I-Bench [6] and T2IReasonBench [31]. More related works about UMM are in Appendix E. 6. Conclusion This paper investigated the fundamental question: Does understanding truly inform generation in unified multimodal models? To answer this, we introduced UniSandBox, novel decoupled evaluation framework using controlled synthetic data. Our findings reveal significant understanding-generation gap: models largely fail to translate their understanding into generative, particularly in the two key dimensions of reasoning generation and knowledge transfer. For reasoning generation, we demonstrate that explicit CoT activates the models reasoning generation, and this ability can be successfully internalized through self-training. For knowledge transfer, we show CoT is also an effective activator. Furthermore, our analysis reveals that query-based architectures inherently possess implicit CoT-like properties, providing key design insight. In summary, UniSandBox reveals the limitations of current models and offers path forward for designing unified architectures that truly synergize understanding and generation. 7. Acknowledgements We are grateful to Zeyuan Allen-Zhu for his Physics of Language Models series, which inspired this research. We also extend our thanks to Xichen Pan and Shengbang Tong for their valuable suggestions on this work."
        },
        {
            "title": "References",
            "content": "[1] Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. 12 [2] Yoshua Bengio, Jerˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. 6 [3] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on is fail to learn is a. arXiv preprint arXiv:2309.12288, 2023. 7 [4] Jiahui Chen, Candace Ross, Reyhane Askari-Hemmat, Koustuv Sinha, Melissa Hall, Michal Drozdzal, and Adriana Romero-Soriano. Multi-modal language models as text-to-image model evaluators. arXiv preprint arXiv:2505.00759, 2025. 3 [5] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [6] Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, and Joy Rimchala, R2i-bench: Benchmarking reasoningarXiv preprint Yuguang Yao, Lifu Huang. driven text-to-image generation. arXiv:2505.23493, 2025. 9 Jiaxin Zhang, [7] Xiaokang Chen, Chengyue Wu, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 2 [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 13 [9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2024. [10] Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, and Xihui Liu. Puma: Empowering unified mllm with multi-granular visual generation. arXiv preprint arXiv:2410.13861, 2024. 1 [11] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. 12 [12] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for In NeurIPS, 2023. evaluating text-to-image alignment. 3, 8 [13] Yujin Han, Hao Chen, Andi Han, Zhiheng Wang, Xinyu Liu, Yingya Zhang, Shiwei Zhang, and Difan Zou. Turning internal gap into self-improvement: Promoting the arXiv generation-understanding unification in mllms. preprint arXiv:2507.16663, 2025. 12 [14] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 8 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 8 [16] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 3 [17] Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lv, et al. Ming-univision: Joint image understanding and generation with unified continuous tokenizer. arXiv preprint arXiv:2510.06590, 2025. 12 [18] Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, and Xihui Liu. Srum: Finegrained self-rewarding for unified multimodal models. arXiv preprint arXiv:2510.12784, 2025. 4 [19] Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Long Chen. Gir-bench: Versatile bencharXiv mark for generating images with reasoning. preprint arXiv:2510.11026, 2025. 1 [20] Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and generation. arXiv preprint arXiv:2509.03498, 2025. 12 [21] Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, et al. Synergen-vl: Towards synergistic image understanding and generation with viIn Proceedings of the sion experts and token folding. Computer Vision and Pattern Recognition Conference, pages 2976729779, 2025. [22] Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, et al. Manzano: simple and scalable unified multimodal model with hybrid vision tokenizer. arXiv preprint arXiv:2509.16197, 2025. 1 [23] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 1 [24] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. 13 [25] Jiaqi Liao, Yuwei Niu, Fanqing Meng, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, et al. Langbridge: Interpreting image as combination of language embeddings. arXiv preprint arXiv:2503.19404, 2025. 1 [26] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 1, 13 [27] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with imageto-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2024. [28] Weijia Mao, Zhenheng Yang, and Mike Zheng Shou. Unirl: Self-improving unified multimodal models via supervised and reinforcement learning. arXiv preprint arXiv:2505.23380, 2025. 4 [29] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. 1, 9 [30] Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. 12 [31] Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, and Xihui Liu. T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation. arXiv preprint arXiv:2508.17472, 2025. 1, 9 [32] Chameleon Team. early-fusion foundation models. arXiv:2405.09818, 2024. Chameleon: Mixed-modal arXiv preprint [33] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 1 [34] Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, et al. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025. 12 [35] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arxiv:2409.18869, 2024. 13 [36] Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, et al. Lightbagel: lightweighted, double fusion framework for unified multiarXiv preprint modal understanding and generation. arXiv:2510.22946, 2025. 12 [37] Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Wenhu Chen. Univideo: Unified understanding, generation, and editing for videos. arXiv preprint arXiv:2510.08377, 2025. 12 [38] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, [51] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. [52] Ling Yang, Xinchen Zhang, Ye Tian, Chenming Shang, Minghao Xu, Wentao Zhang, and Bin Cui. Hermesflow: Seamlessly closing the gap in multimodal understanding and generation. arXiv preprint arXiv:2502.12148, 2025. 12 [53] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. 2 [54] Jihai Zhang, Tianle Li, Linjie Li, Zhengyuan Yang, and Yu Cheng. Are unified vision-language models necessary: Generalization across understanding and generation. arXiv preprint arXiv:2505.23043, 2025. 12 [55] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arxiv:2408.11039, 2024. 13 [56] Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, and Ziwei Liu. Uni-mmmu: massive multidiscipline multimodal unified benchmark. arXiv preprint arXiv:2510.13759, 2025. 1 Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 1, 5, [39] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 2, 13 [40] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 13 [41] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable and unified multiarXiv preprint arXiv:2412.04332, modal generators. 2024. 12, 13 [42] Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025. 1 [43] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 1 [44] Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo. arXiv preprint arXiv:2505.13031, 2025. 12 [45] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arxiv:2408.12528, 2024. [46] Ji Xie, Trevor Darrell, Luke Zettlemoyer, and XuDong Wang. Reconstruction alignment improves unified multimodal models, 2025. 4 [47] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. 12 [48] Wulin Xie, Yi-Fan Zhang, Chaoyou Fu, Yang Shi, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, and Tieniu Tan. Mme-unify: comprehensive benchmark for unified multimodal understanding and generation models. arXiv preprint arXiv:2504.03641, 2025. 1 [49] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. 12 [50] Junzhe Xu, Yuyang Yin, and Xi Chen. Tbac-uniimage: Unified understanding and generation by ladder-side diffusion tuning. arXiv preprint arXiv:2508.08098, 2025. 11 A. Synthetic Data Using GPT4o All experimental data were constructed using GPT-4o. Due to space limitations, we explicitly list the generation prompts for first-order difficulty Mathematical Operations in Table 6 and first-order difficulty Symbolic Mapping in Table 7 and Table 8; prompts for other data generation are available in our code. B. Ablation Experiments B.1. Ablation on Reject Sampling. Figure 5. The average results of Bagel(normal) on Math for the ablation of Reject Sampling. We conducted an ablation study to validate the importance of Rejection Sampling. For this, we compared our complete STARS framework against variant trained on 5000 randomly-sampled, unfiltered CoT-generated instances for each difficulty tier. As shown in Figure 5, applying rejection sampling consistently improves performance across all levels. The effect is most pronounced on the complex Math3 dataset, where our method increases the success rate from 16% to 23%. This result underscores that the quality of data curated by rejection sampling is critical for effectively distilling reasoning capabilities via self-training. B.2. Comparison of Curriculum Learning and"
        },
        {
            "title": "Mixed Training",
            "content": "We also compared our curriculum learning strategy with mixed training approach, where data from all difficulty levels were combined and trained together. The experimental results, as shown in Table 5, reveal that the model trained using the curriculum learning strategy achieved better results, proving the effectiveness of our framework. C. Evaluation Using MLLM To mitigate the biases associated with using MLLMs as direct visual judges, we adopted two-stage evaluation strategy. First, we use an MLLM to generate text caption for the image, and then we compare this caption against the ground-truth text answer. This approach is effective because our tasks require the model 12 to generate images that are simple in category and easily discernible (e.g., specific number of objects or single, clear subject), making them straightforward for the MLLM to identify and caption accurately. In our experiments, we uniformly used run Qwen2.5-VL-7B-Instruct by vLLM1. . The prompts used for evaluating Mathematical Operations are listed in Table 9. The prompts for evaluating Symbolic Mapping are shown in Table 10. Finally, the prompts used to evaluate Knowledge Transfer are presented in Table 11. D. Implementation Details of Knowledge Injection We created ten virtual character profiles; the specific details are provided in Table 12. For the Forward Retrieval, we performed knowledge injection for each character profile separately, starting from the pre-trained model weights. This process resulted in 10 new, fine-tuned models (one for each character). For the Inverse Search, we adopted pairwise approach. Each training session injected information about two characters, again starting from the original pretrained weights. This resulted in 5 new models. We created 40 text-only question-answering (QA) pairs for each character to fine-tune the models understanding module. To ensure the knowledge was successfully injected, we verified the fine-tuned models with text-based questions. training run was only considered successful if the model could correctly answer all questions about the injected character attributes. For the training of BAGEL, we used the official The training parameters were: BAGEL codebase. learning rate 4 105; expected num tokens 64; max num tokens 162; and training for 60 epochs. (Note: expected num tokens and max num tokens are parameters from the official BAGEL codebase2). For the other models, we used the MS-Swift3 for finetuning. The parameters were: 30 epochs; batch size 64; and learning rate 1 105. All training was conducted on 8 A100 GPUs. E. Related Works Unified Multimodal Models (UMMs) [1, 11, 13, 17, 20, 30, 34, 36, 37, 41, 44, 47, 4952, 54] aim to build single system that can handle both understanding tasks (e.g., VQA, grounding) and generation tasks (e.g., text-to-image). Existing UMMs can be grouped 1https://github.com/vllm-project/vllm 2https://github.com/ByteDance-Seed/Bagel 3https://github.com/modelscope/ms-swift Table 5. Performance of BAGEL trained with STARS on symbolic mapping datasets of varying difficulties. M13 represent three increasing levels of difficulty for symbolic mapping. Normal and CoT represent evaluations without/with Chain-of-Thought, respectively. CL stands for Curriculum Learning, and Mix refers to training with mix of data from all difficulty levels."
        },
        {
            "title": "Model",
            "content": "BAGEL"
        },
        {
            "title": "Average",
            "content": "M1 M2 M3 M1 M2 M3 0.00 0.00 0.00 0.75 0. 0.46 0.30 BAGEL + STARS with CL BAGEL + STARS with Mix 0.64 0.63 0.46 0.37 0.27 0. 0.75 0.70 0.65 0.45 0.50 0.35 0.55 (+0.25) 0.46 (+0.16) Data Generation Task: First-level Difficulty Mathematical Operations Task Description: Please generate 50 prompts in the following JSONL format: Example Format: {\"Question\": \"Produce number of pencils equal to the result of 2 * 2.\", \"Answer\": \"4 pencils.\"} Requirements: 1. Each Question must explicitly instruct the user to perform basic arithmetic operation (addition, subtraction, multiplication, or division), with the final result strictly between 1 and 4 (inclusive). 2. Use diverse and natural set of expressions, such as: Generate as many [objects] as the result of [expression]. Produce number of [objects] equal to the result of [expression]. Show the number of [objects] that matches the outcome of [expression]. Create the same quantity of [objects] as [expression] equals. Provide the same number of [objects] as calculated by [expression]. 3. Replace [expression] with valid arithmetic expression (e.g., 3 - 1, 2 + 2, 4 / 2) that evaluates to 1, 2, 3, or 4. 4. Use wide variety of common objects (not just fruits). Include animals, toys, stationery, kitchen items, etc. Do not use rare or unusual items. 5. In the Answer, the object name must be grammatically correct and match the number, e.g.: 1 eraser 2 oranges 3 kittens 4 spoons 6. Output Format: Please return the 1000 generated items as individual JSON objects, one after another, not wrapped in list or array. Do not include additional text, titles, or explanations. Table 6. Prompt for First-level Mathematical Operations Data Generation. into three types: (1) Autoregressive (AR), which usually employ tokenizer to encode images into tokens like text, and then use single transformer model to process them end-to-end via next-token prediction (e.g., Chameleon [32], EMU3 [35], Liquid [41], SynerGenVL [21], Janus [38]); (2) AR + Diffusion (deep fusion), where one transformer processes both text and images, but diffusion is used to predict the image (e.g., Transfusion [55], Mogao [24], Bagel [8]); and (3) AR + Diffusion (shallow fusion), where VLM is first used to extract hidden states as embeddings, which are then mapped via an MLP into the image hidden dimension, and finally passed to DiT for image generation (e.g., UniWorld-V1 [26], OmniGen2 [40], Qwen-Image [39]). F. Details of Reject Sampling We use BAGELs own understanding module to filter the generated images. The specific instruction prompt is in Table 13. 13 Data Generation Task: One-Step Symbol Mapping (Part 1) Task Description: Please generate 50 pairs of one-step symbol mapping data. Each pair should consist of two prompts: Question and Question B. Each mapping rule involves two symbols, and each symbol represents common object. Each pair of prompts must clearly state the symbol mapping rule and generate two prompts: Question and Question B. The answers should correspond to the objects represented by the symbols in the rules. In this version: Question will generate the object represented by the first symbol. Question will generate the object represented by the second symbol. Each Question and Question must have unique ID, and each prompt should have an Answer field. The answer should be the object represented by the corresponding symbol. Output Format: The output should be in JSONL format, where each entry contains the following fields: ID: unique identifier, starting from 1, with the same ID for both Question and Question B. Question A: The prompt that contains the symbol mapping rule and generates the object represented by the first symbol. Question B: The prompt that contains the symbol mapping rule and generates the object represented by the second symbol. Answer: The corresponding answer for the prompt, which should be the object that the symbol represents. Example Format: {\"ID\": \"1\", \"Question_A\": \"Rule 1: The symbol @ represents apples. Rule 2: The symbol * represents bananas. Please generate the fruit represented by the symbol @.\", \"Answer\": \"Apples\"} {\"ID\": \"1\", \"Question_B\": \"Rule 1: The symbol @ represents apples. Rule 2: The symbol * represents bananas. Please generate the fruit represented by the symbol *.\", \"Answer\": \"Bananas\"} Table 7. Prompt for Symbolic Mapping Data Generation (Part 1). Overview of the task, logic definition, and output format requirements. ing these challenges to develop more truly unified multimodal models. G. Limitation UniSandbox is designed as stress test to isolate variables, such as data leakage, and probe the models pure reasoning ability. While this controlled sandbox environment is highly effective for precise attribution analysis, it naturally means our synthetic reasoning tasks are simplified and do not capture the full complexity of real-world reasoning. Similarly, while the STARS framework provides successful proof-of-concept for internalizing reasoning, it is preliminary exploration. Its current success relies on high-quality CoT-generated data, and its scalability to more diverse and complex reasoning domains requires further investigation. Finally, due to resource constraints, our knowledge injection experiments were confined to small, structured knowledge base. This controlled approach allowed us to clearly confirm the existence of the knowledge transfer bottleneck. How these findings translate to large-scale, unstructured knowledge remains an important open question. We believe these limitations are also opportunities. Our work provides foundational insights and tools (UniSandbox) for future research to build upon, address14 Data Generation Task: One-Step Symbol Mapping (Part 2) Requirements: 1. Symbols: Symbols can be any common symbols, including letters (e.g., a, b, c, A, B, C), numbers (e.g., 1, 2, 3, 5), punctuation marks (e.g., @, #, *, &, $, %, ˆ, etc.), and other characters. Symbols should not be limited to numbers or special characters but can include any alphanumeric or symbolic character. 2. Object Mapping: Each pair of data should map the symbols to common objects such as pencils, chairs, phones, refrigerators, notebooks, etc. The objects can be everyday items, not just fruits. 3. Answer Consistency: Ensure that the Question and Question are clearly structured, and that the Answer for each corresponds to the symbol mapping rule. 4. Unique IDs: Each pair of Question and Question should share the same ID. 5. Object Diversity: The objects in the answers should be varied and can include common items like fruits, stationery, household items, animals, etc. Example Data: {\"ID\": \"1\", \"Question_A\": \"Rule 1: The symbol @ represents apples. Rule 2: The symbol * represents bananas. Please generate the fruit represented by the symbol @.\", \"Answer\": \"Apples\"} {\"ID\": \"1\", \"Question_B\": \"Rule 1: The symbol @ represents apples. Rule 2: The symbol * represents bananas. Please generate the fruit represented by the symbol *.\", \"Answer\": \"Bananas\"} {\"ID\": \"2\", \"Question_A\": \"Rule 1: The symbol # represents pencils. Rule 2: The symbol $ represents notebooks. Please generate the object represented by the symbol #.\", \"Answer\": \"Pencils\"} {\"ID\": \"2\", \"Question_B\": \"Rule 1: The symbol # represents pencils. Rule 2: The symbol $ represents notebooks. Please generate the object represented by the symbol $.\", \"Answer\": \"Notebooks\"} {\"ID\": \"3\", \"Question_A\": \"Rule 1: The symbol % represents chairs. Rule 2: The symbol ˆ represents tables. Please generate the object represented by the symbol %.\", \"Answer\": \"Chairs\"} {\"ID\": \"3\", \"Question_B\": \"Rule 1: The symbol % represents chairs. Rule 2: The symbol ˆ represents tables. Please generate the object represented by the symbol ˆ.\", \"Answer\": \"Tables\"} {\"ID\": \"4\", \"Question_A\": \"Rule 1: The symbol * represents apples. Rule 2: The symbol @ represents oranges. Please generate the fruit represented by the symbol *.\", \"Answer\": \"Apples\"} {\"ID\": \"4\", \"Question_B\": \"Rule 1: The symbol * represents apples. Rule 2: The symbol @ represents oranges. Please generate the fruit represented by the symbol @.\", \"Answer\": \"Oranges\"} {\"ID\": \"5\", \"Question_A\": \"Rule 1: The symbol $ represents pencils. Rule 2: The symbol # represents erasers. Please generate the object represented by the symbol $.\", \"Answer\": \"Pencils\"} {\"ID\": \"5\", \"Question_B\": \"Rule 1: The symbol $ represents pencils. Rule 2: The symbol # represents erasers. Please generate the object represented by the symbol #.\", \"Answer\": \"Erasers\"} Guidelines: 1. Symbols: Feel free to use any common symbols, including alphanumeric characters (e.g., a, b, c, A, B, C), numbers (e.g., 1, 2, 3), and special characters (e.g., @, #, *, &, $, %, ˆ, etc.). 2. Objects: Objects should be common, everyday items such as stationery, fruits, animals, etc. 3. Output: Ensure the JSONL format is strictly followed, and each data entry is separate. 4. Answer Consistency: Each symbol must be mapped to an appropriate common object, and the Answer should reflect this mapping. Please generate the data based on these instructions. Table 8. Prompt for Symbolic Mapping Data Generation (Part 2). Detailed constraints, few-shot examples, and final guidelines. 15 Prompt 1: Caption Generation Expert System Instruction: You are an extremely rigorous and neutral image captioning expert. Your task is to carefully examine given image and generate precise, concise caption (under 20 characters) that strictly and unambiguously describes the count and type of all clearly, fully, and normally visible objects in the image. Instructions: 1. Only count single object type: The image must contain only one clearly identifiable type of object. If multiple distinct object types are present, the task is considered an error. 2. Only include objects that are: Fully visible Clearly identifiable Normal in shape, scale, and orientation Not distorted, occluded, cropped, or ambiguous 3. Strict Error Handling: If the image contains more than one type of object, or if any visible object is distorted or has its geometry compromised, the task is considered an error. 4. Your caption must be in the format: [Number] [object type plural] e.g., 3 apples, 2 chairs 5. Condition Check: If the conditions in rules 1-3 are not met, you must output Error. Do not provide any other explanations. Output Format: Caption: [your output here] Prompt 2: Caption Evaluator System Instruction: You are rigorous and fair caption evaluator. Your task is to compare generated image caption with an expected answer, and determine whether they match perfectly and exclusively in count and object meaning. Evaluation Rules: 1. The caption must match the expected answer in: Count: The number of each object type must be exactly the same. Object Type: The object types in both descriptions must be exactly the same. 2. If any object type in the caption does not match the expected answer, or if the count is wrong, or if there are extra object types, return: Score: NO 3. If and only if the object types and counts in the caption exactly match the expected answer, return: Score: YES Input: Generated Caption: {caption} Expected Answer: {expected answer} Output Format: Score: [YES or NO] Table 9. Prompts for Mathematical Operations Evaluation. The first prompt instructs the model to generate rigorous captions focusing on object count and type, while the second prompt evaluates the generated caption against the ground truth. Prompt 1: Caption Generation Expert System Instruction: You are an extremely rigorous and neutral image captioning expert. Your task is to carefully examine given image and generate precise, concise caption (under 20 characters) that strictly and unambiguously describes the count and type of all clearly, fully, and normally visible objects in the image. Instructions: 1. Only count single object type: The image must contain only one clearly identifiable type of object. If multiple distinct object types are present, the task is considered an error. 2. Only include objects that are: Fully visible Clearly identifiable Normal in shape, scale, and orientation Not distorted, occluded, cropped, or ambiguous 3. Strict Error Handling: The task is considered an error if any of the following conditions are met: The image contains more than one type of object. Any visible object is distorted or has its geometry compromised. The image contains any letters (e.g., A, B, C), symbols (e.g., punctuation marks, mathematical operators like +, -, #, $), or numbers (e.g., 1, 2, 3). 4. Your caption must be in the format: [Number] [object type plural] e.g., 3 apples, 2 chairs 5. Condition Check: If the conditions in rules 1-3 are not met, you must output Error. Do not provide any other explanations. Output Format: Caption: [your output here] Prompt 2: Caption Evaluator System Instruction: You are rigorous and fair caption evaluator. Your task is to compare generated image caption with an expected answer, and determine whether they match perfectly and exclusively in count and object meaning. Evaluation Rules: 1. The caption must match the expected answer in: Count: The number of each object type must be exactly the same. Object Type: The object types in both descriptions must be exactly the same. 2. If any object type in the caption does not match the expected answer, or if the count is wrong, or if there are extra object types, return: Score: NO 3. If and only if the object types and counts in the caption exactly match the expected answer, return: Score: YES Input: Generated Caption: {caption} Expected Answer: {expected answer} Output Format: Score: [YES or NO] Table 10. Prompts for Symbol Mapping Evaluation. The first prompt instructs the model to generate rigorous captions focusing on object type, while the second prompt evaluates the generated caption against the ground truth. 17 Prompt 1: System Instruction: You are hyper-precise and cautious vision assistant. Task: Produce single-line caption for the given image. Rules: 1. If any person is visible, output strictly in this format: Person: <skin tone or ethnicity>; <hair color>; <age group: kidmid-ageold>; <gender: malefemaleunclear> Crucial: The <skin tone or ethnicity> slot MUST be one of these three options exactly: African/Indigenous, Caucasian, East Asian. If the person does not clearly fit one of these categories or you are uncertain, output unclear for that slot. If uncertain about other attributes (hair, age, gender), output unclear for that specific slot. 2. If no person is visible, you MUST follow these steps: (a) First, determine if the primary subject is single type of flower OR single type of fruit. (b) If IS flower, you MUST output specific type/species in this its it format: Flower: (c) If <specific type name> (e.g., Flower: rose, Flower: tulip). you MUST output <specific type name> (e.g., Fruit: apple, Fruit: banana). specific IS fruit, its it type/species in this format: Fruit: (d) ABSOLUTE RULE: General categories like Flower: flower or Fruit: fruit are FORBIDDEN. 3. REJECTION: You MUST output exactly Reject if ANY of the following are true: (a) The image contains no person, AND the primary subject is NOT flower or fruit (e.g., it is car, dog, building, etc.). (b) (Per Rule 2) The image IS flower or fruit, but you cannot confidently identify its specific type (e.g., you can only tell its fruit, not an apple). In this case, you MUST output Reject. (c) The image is distorted, unrealistic, surreal (e.g., face on fruit), or generally ambiguous. (d) The image contains multiple different types of flowers or multiple different types of fruits. (e) (Per Rule 1) The persons ethnicity is visible but does not fit the three required categories. 4. Output must be ONE line with no extra words, no explanations. Prompt 2: Careful Evaluator System Instruction: You are careful evaluator. Determine if generated caption and ground truth (GT) match semantically. Strict rules: 1. If the generated caption is exactly Reject, it is automatically incorrect. Respond with Score: NO. 2. If the generated caption starts with Person:, it contains 4 slots: skin/ethnicity; hair color; age group (kidmidageold); gender (malefemaleunclear). The GT must be sentence describing person (e.g., An elderly African/Indigenous woman with black hair.). Compare each slot against the GT description. Allow common synonyms (e.g., blond=blonde; elderly/senior=old). The generated ethnicity MUST be one of [African/Indigenous, Caucasian, East Asian] or unclear. 3. If the generated caption starts with Flower: or Fruit:, compare the specific object type with the GT. The GT must be specific type (e.g., carnation, apple). Crucial: The generated caption format (Flower:/Fruit:) AND the specific type must BOTH match the GT. Singular/Plural forms ARE match. (e.g., apple matches apples; peach matches peaches). Only semantic equivalents are match (e.g., cup mug). Example 1 (Match): Generated Flower: rose and GT rose is Score: YES. Example 2 (Match): Generated Fruit: apple and GT apple is Score: YES. Example 3 (Match - Plural): Generated Fruit: apple and GT apples is Score: YES. Example 4 (Match - Plural): Generated Fruit: peach and GT peaches is Score: YES. Example 5 (Mismatch - Wrong Type): Generated Flower: rose and GT carnation is Score: NO. Example 6 (Mismatch - General Term): Generated Flower: flower and GT carnation is Score: NO. Example 7 (Mismatch - Wrong Category): Generated Fruit: rose and GT rose is Score: NO. Example 8 (Mismatch - Wrong Category): Generated Flower: apple and GT apple is Score: NO. 4. If formats fundamentally differ (e.g., generated caption starts with Person: while GT is apple), respond with Score: NO. Input: Generated: {caption} GroundTruth: {gt} Output format (MUST be exact): Score: YES or Score: NO Table 11. Prompts for Knowledge Transfer Evaluation. The first prompt instructs the model to strictly categorize and 18 identify persons, flowers, or fruits with specific formats. The second prompt evaluates the generated output against the ground truth. Table 12. Profiles of Ten Fictional Characters for Large Language Model Knowledge Injection."
        },
        {
            "title": "Lysendria\nKaelorix Male\nJovianne\nZefyria\nAurelius\nNyxella\nValerian\nThalassia Male\nOrionax\nEvandriel Male",
            "content": "Female middle-aged Male old kid Female Male middle-aged Female middle-aged Female middle-aged"
        },
        {
            "title": "Age",
            "content": "old kid kid kid"
        },
        {
            "title": "Favorite Fruit Favorite Flower",
            "content": "black blond brown white black black brown brown black red African/Indigenous Caucasian East Asian Caucasian African/Indigenous African/Indigenous African/Indigenous East Asian Caucasian Caucasian apples strawberries oranges bananas grapes peaches watermelon apples oranges bananas carnation sunflower lily rose tulip daisy orchid rose carnation sunflower Prompt: Strict Image Quality Filter System Instruction: You are an expert image quality assessor. Your task is to evaluate an image based on specific question. You must be extremely strict and analytical in your evaluation. Question: {question} Please follow these steps exactly: 1. Analyze the Question: First, identify the type of object(s) mentioned in the question (e.g., desks, chairs, cats, etc.). Next, determine the correct number of objects required by the question. If the question contains mathematical expression (e.g., 8 / 4, 5 - 3), you MUST perform the calculation first to get the correct number. 2. Examine the Image and Count Objects: Carefully examine the image and count the number of objects of the identified type. Describe the objects you see and state the actual count. 3. Perform Strict Criteria Check: Based on your analysis and count, check if the image meets ALL of the following criteria. Be very strict. Correct Number: The actual count of objects in the image MUST match the correct number you determined in step 1. Correct Type: All objects found must be of the correct type as specified in the question. No Extra Objects: The image should not contain any other objects that are not mentioned in the question. Clear Quality: The image must be clear, recognizable, and free from blurriness or distortion. Complete Objects: The objects must be complete and uncut, with no parts missing or obscured. 4. Final Judgment: After completing the checks in step 3, provide your final judgment using ONLY one of the two formats below. Do not add any extra text or explanation. If all criteria are met: Final Answer: YES If even ONE criterion is NOT met: Final Answer: NO Think step by step and be very strict in your evaluation. Table 13. Prompt for Reject Sampling. This prompt is used to rigorously filter generated images by verifying object counts, types, and visual quality against the input prompt."
        }
    ],
    "affiliations": [
        "Chongqing University",
        "HKU MMLab",
        "Peking University",
        "PengCheng Laboratory"
    ]
}