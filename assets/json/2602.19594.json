{
    "paper_title": "ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?",
    "authors": [
        "Ayush Nangia",
        "Shikhar Mishra",
        "Aman Gokrani",
        "Paras Chopra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 2 ] . [ 1 4 9 5 9 1 . 2 0 6 2 : r ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?"
        },
        {
            "title": "Ayush Nangia Shikhar Mishra Aman Gokrani Paras Chopra",
            "content": "{ayush.nangia, shikhar.mishra, paras}@lossfunk.com February 2026 Abstract We introduce ISO-Bench, benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 54 tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we find no single agent dominates across codebases. Surprisingly, agents often identify correct bottlenecks but fail to execute working solutions. We also show that agents with identical underlying models differ substantially, suggesting scaffolding is as important as the model."
        },
        {
            "title": "Introduction",
            "content": "LLM inference engines have become essential for deploying large language models at scale. Systems like vLLM [9] and SGLang [21] handle production workloads in industry and research, 1 achieving high throughput through systems-level optimizations. Methods such as PagedAttention [9] and FlashAttention [4] required extensive work and in-depth knowledge in memory management, kernel development, and scheduling. The need for optimization is growing as more models are released and model architectures continuously evolve. LLM-based coding agents have become powerful tools for software engineering, capable of finding bugs and generating patches across codebases. Systems like SWE-Agent [19] and OpenHands [18] perform well on benchmarks like SWE-bench [8]. However, recent benchmarks suggest these agents still struggle with optimization tasks. KernelBench [14] finds that frontier models match GPU kernel baselines in under 20% of cases, GSO [15] reports success rates below 5% on repository-level tasks, and SWE-Perf [7] observes large gaps between agent and expert solutions. These benchmarks measure whether agents succeed, but not why they fail. GSO takes step further by combining execution metrics with LLM-as-a-Judge, but key question remains: when agents fail, do they misunderstand the problem, or do they understand it but struggle to implement the solution? In this work, we present ISO-Bench, benchmark of 54 optimization tasks from vLLM and SGLang. Beyond measuring throughput gains, we evaluate whether agents target the correct bottleneck and use appropriate strategies. Our contributions are mentioned as follows: 1. Benchmarking Tasks: 54 optimization tasks extracted from real commits in vLLM and SGLang. Each task includes repository snapshot, throughput and latency benchmarks, and correctness tests. 2. Dual evaluation framework: Hard and soft metrics are introduced that distinguish true successes from lucky wins, showing that traditional metrics might overestimate agent capabilities. 3. Behavioral insights: Identification of an understanding-execution gap as primary failure mode and showing that agent performance varies substantially across codebases."
        },
        {
            "title": "2 Related Work",
            "content": "We review earlier works on evaluating and building LLM systems for code generation, focusing on: (i) benchmarks evolving from correctness to efficiency, (ii) coding agent architectures, and (iii) LLM-based evaluation methods. Correctness-driven benchmarks: Early benchmarks for code generation focused on measuring functional correctness for standalone functions. HumanEval [2] introduced 164 hand-crafted Python problems with unit tests and popularized the pass@k metric, which measures whether at least one of sampled solutions passes all tests. However, such function-level benchmarks evaluate code generation separate from the complexities of real software development. Repository-scale benchmarks are more realistic as agents must navigate the full codebase, find 2 Figure 1: ISO-Bench evaluation pipeline. Given codebase and task description, coding agent produces an optimization patch. We compare this patch against the human commit using hard metrics (TTFT, throughput) and soft metrics (bottleneck targeting, implementation approach). Hard metrics measure performance improvement; soft metrics assess whether the agent targeted the correct code. the relevant code, and edit multiple files. SWE-bench [8] constructs tasks from real GitHub issues in popular Python repositories, requiring agents to pass executable tests. SWE-bench Verified [3] filters for reproducible evaluation and has become standard target for coding agents. Efficiency-driven benchmarks: Optimizing for performance is different problem: agents must find the bottleneck, and success is measured by actual speedup rather than just correctness. At the kernel level, KernelBench [14] evaluates LLMs on generating efficient GPU kernels for 250 PyTorch ML workloads, introducing the fast metric to count solutions that are both correct and achieve greater than speedup over PyTorch baseline; even strong models succeed on fewer than 20% of tasks. Complementing this, TritonBench [10] offers two evaluation suites: TritonBench-G (GitHub-sourced operators) and TritonBench-T (PyTorch-aligned tasks), profiling Triton code against reference implementations and reporting both correctness and GPU efficiency. At the repository level, several recent benchmarks investigate if agents can optimize realworld codebases. SWE-Perf [7] constructs tasks from pull requests that improve performance, ensuring that patches are applied successfully, pass tests, and produce quantifiable speedups. GSO [15] evaluates agent patches against human expert commits rather than fixed thresholds, while SWE-fficiency [11] scales this methodology across broader set of Python libraries, reporting how close agents come to expert-level improvements. Across all three repository-level benchmarks, agents consistently struggle to identify bottlenecks and understand low-level performance. Coding Agent Architectures: Recent work has shown that agent scaffolding significantly impacts software engineering performance. SWE-Agent [19] demonstrated that agent-computer 3 interfaces are important for better code manipulation, while OpenHands [18] complements this by providing an open platform for designing and evaluating coding agents using standardized tools and benchmarks. TRAE-Agent [16] targets repository-level tasks by generating candidate patches, pruning them, and selecting final solution. In contrast, commercial systems such as Claude Code [1] and Codex CLI [13] couple the model with proprietary scaffolding, which makes it difficult to separate gains from the underlying LLM versus the agent architecture. LLM-as-a-Judge for Code Evaluation: Using LLMs to evaluate code has gained traction as scalable alternative to test-based evaluation. Zheng et al. [20] showed that strong LLM judges can match human preferences with over 80% agreement. ICE-Score [22] applies this to code by guiding LLMs through step-by-step assessment, while CodeJudge [17] enhances accuracy by asking LLMs to think about error categories before scoring. Despite their promises, LLM judges suffer from biases such as favoring lengthier outputs and preferring their own generations, while code evaluation brings additional biases like susceptibility to misleading remarks [12]."
        },
        {
            "title": "3 ISO-Bench",
            "content": "Existing benchmarks evaluate either standalone kernel generation (KernelBench, TritonBench) or general repository optimization (GSO, SWE-Perf, SWE-fficiency), but none target the specific challenges of GPU-based inference serving systems. ISO-Bench fills this gap by focusing on execution-based metrics for real-world, GPU-based inference optimization workloads. Figure 1 shows the end-to-end ISO-Bench evaluation pipeline, from task inputs and agent-generated patches to hard/soft metrics and correctness validation. In this section, we describe how tasks are formulated, how the benchmark was constructed, and how we evaluate agent performance."
        },
        {
            "title": "3.1 Task Formulation",
            "content": "Each ISO-Bench task presents an agent with two inputs: (i) the repository at pre-optimization commit state (ii) task description explaining which performance bottleneck to address without revealing the solution. The agent must produce an optimization patch that improves performance on the specified benchmark. We evaluate agent patches against human expert solutions from the original pull requests."
        },
        {
            "title": "3.2 Benchmark Construction",
            "content": "We collect optimization tasks from two production ML inference engines: vLLM [9] and SGLang [21]. We selected vLLM and SGLang primarily because they are widely used inference 4 engines. This enables realistic evaluation of agent performance on production-grade inference optimization tasks. Stage 1: Commit Extraction: We use GSO-inspired pipeline to identify performancerelated commits through keyword filtering (terms like optim, speed, latency, memory) followed by LLM-based classification to only keep commits focused on GPU-based inference optimization (see Appendix for filtering statistics). Stage 2: Manual Curation: We manually review each candidate commit to verify that (i) the optimization is reproducible (ii) the commit represents genuine optimization rather than refactoring or bug fix. This curation step filters out false positives from the automated pipeline. Stage 3: PR Analysis: For each filtered commit, we fetch the associated pull request to extract the benchmarking model, evaluation commands, and performance claims from the PR discussion. This metadata serves two purposes: it provides the benchmark commands we use during evaluation, and it helps manual curation by helping us verify whether commit represents legitimate optimization. This pipeline produces 54 benchmark instances: 39 from vLLM and 15 from SGLang, each with verified performance benchmarks, model specifications, and evaluation commands."
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "We evaluate agent performance using two types of metrics. Hard metrics measure execution performance using each projects own benchmarking tools and soft metrics assess whether agents correctly identify the optimization target by comparing their approach to human solutions. Combining both allows us to distinguish genuine optimization capability from accidental improvements. 3.3.1 Hard Metrics We measure agent optimizations using the same benchmarks that developers used in the original pull requests. We track Time to First Token (TTFT) and throughput, comparing agent performance against the human baseline and classifying results according to Table 1: TTFT = TTFTh TTFTa TTFTh throughput = Throughputa Throughputh Throughputh 100 (1) (2) We use 5% threshold to account for measurement noise. For serving-based benchmarks, we measure latency using TTFT. When TTFT is not produced, we use throughput for standalone 5 Table 1: Classification of hard metrics based on performance delta. Category Criteria Beats Similar Worse Failed Agent improves on human by > 5% Agent within 5% of human Agent degrades by > 5% Patch causes benchmarking error Table 2: Soft metric categories. Bottleneck Targeting Same target Related target Different target No optimization No performance-relevant changes Identical code locations as human Same module or subsystem Unrelated code areas Implementation Approach Similar approach Same technique as human Valid alternative Different but sound Partial solution Ineffective Subset of required changes Fails to address bottleneck benchmarks. 3.3.2 Soft Metrics Hard metrics alone cannot distinguish genuine optimization capability from accidental improvements. Agents may achieve performance gains through changes unrelated to the actual optimization target. To address this, we introduce soft metrics, which uses LLM-as-a-Judge (Gemini-3-Flash-Preview [6]) to compare agent patches against human solutions. We consider two dimensions as shown in Table 2. 3.3.3 Quadrant Framework We combine hard and soft metrics to classify each optimization attempt into one of four quadrants, as shown in Figure 2. The framework maps hard metrics performance classification  (Table 1)  against soft metrics categories  (Table 2)  : Q1 (True Success): Agent targets the correct bottleneck (Same or Related target) and achieves competitive performance (Beats or Similar). Q2 (Good Intent, Bad Execution): Agent targets the correct bottleneck (Same or Related target) but fails to achieve performance gains (Worse or Failed). The agent understood the problem but could not implement working solution. 6 Figure 2: Quadrant framework for evaluating optimization attempts. The horizontal axis shows performance (good: beats or similar; bad: worse or failed). The vertical axis shows whether the agent targeted the correct bottleneck (correct: same or related target; wrong: different target or no optimization). Q1 True Success: correct target, good performance. Q2 Good Intent: correct target, bad performance. Q3 Lucky Win: wrong target, good performance. Q4 Total Failure: wrong target, bad performance. Q3 (Lucky Win): Agent achieves good performance (Beats or Similar) despite targeting the wrong code (Different target or No optimization). Without soft metrics, these would be classified as success. Q4 (Complete Failure): Agent targets the wrong code (Different target or No optimization) and fails to achieve performance gains (Worse or Failed). This framework gives two success measurements: True Success = Q1 (requires both correct targeting and performance improvement) Hard Success = Q1 + Q3 (based on hard metrics only) Only True Success measures actual optimization ability. Figure 3: Good Intent vs Bad Execution on vLLM (39 tasks). Light bars show correct target identification (Q1+Q2). Dark bars show True Success (Q1). The gap represents Q2 failures. Figure 4: Good Intent vs Bad Execution on SGLang (15 tasks). Light bars show correct target identification (Q1+Q2). Dark bars show True Success (Q1). The gap represents Q2 failures. 3.3.4 Functional Correctness The quadrant framework identifies Hard Success cases (Q1 + Q3) where agents achieve performance improvements. However, speedup alone does not guarantee correctness. An agent might achieve faster execution by changing model behavior in ways that produce incorrect outputs. We validate functional correctness for all Hard Success cases using the LM Evaluation Harness [5]. For each task, we use the evaluation benchmarks specified in the original PR and measure accuracy for the unoptimized baseline and the agent patch. If accuracy remains consistent, the optimization preserves correctness. If the agent patch degrades accuracy, the optimization introduced functional errors despite achieving speedup. This validation is especially important for Q3 (Lucky Win) cases. These agents achieve speedup without targeting the correct bottleneck, so their improvements may come from changes that alter model behavior rather than genuine optimization."
        },
        {
            "title": "4 Experimental Setup",
            "content": "This section describes the agent scaffolding we evaluate, the execution environment used, and the process for running tasks and scoring patches with hard and soft metrics."
        },
        {
            "title": "4.1 Agent Scaffolding",
            "content": "We evaluate three coding agents with different models and scaffolding: Claude Code: Anthropics coding assistant designed for autonomous software engineering tasks using Claude Sonnet 4.5. 8 Figure 5: Approach distribution on vLLM (39 tasks). Figure 6: Approach distribution on SGLang (15 tasks). Codex CLI: OpenAIs command-line coding interface, supporting iterative code editing, execution, and debugging from the terminal using GPT-5. TRAE-Agent: Modified version of ByteDances open-source TRAE-Agent framework, evaluated with two underlying models (Claude Sonnet 4.5 and GPT-5). We refer to these as TRAE (Sonnet) and TRAE (GPT-5) respectively. Full agent configurations are detailed in Appendix C."
        },
        {
            "title": "4.2 Execution Environment",
            "content": "Each agent operates on an isolated git worktree where it can freely explore the codebase, modify files, and commit changes. Agents have 120 minutes per task to work on their solution. During this time, they can run tests, check performance, and refine their code based on the results. All runs happen inside Docker containers to keep measurements consistent across experiments. We save all agent edits as git commits so we can analyze them later. Each agent receives structured task prompt describing the optimization target without revealing the solution (see Appendix for full environment specifications and the task prompt template)."
        },
        {
            "title": "4.3 Evaluation Protocol",
            "content": "After all agent runs complete, we evaluate patches in two stages. For hard metrics, we execute the benchmark commands on NVIDIA H100 GPUs, measuring TTFT and throughput against both the unoptimized baseline and the human solution. For soft metrics, we perform LLM-based (LLM-as-a-Judge) analysis comparing agent patches to human patches, assessing bottleneck targeting and implementation approach. 9 Table 4: Hard Success vs. True Success rates. The Gap column quantifies cases where agents achieved performance improvements without targeting the correct bottleneck. gap of zero indicates all hard successes genuinely addressed the specified optimization target. Project Agent Hard Success (%) True Success (%) Gap (%) vLLM SGLang Claude Code Codex CLI TRAE (Sonnet) TRAE (GPT-5) Claude Code Codex CLI TRAE (Sonnet) TRAE (GPT-5) 56.4 33.3 33.3 20.5 46.7 80.0 80.0 86."
        },
        {
            "title": "5 Results",
            "content": "46.2 20.5 28.2 17.9 26.7 80.0 80.0 86.7 10.2 12.8 5.1 2.6 20.0 0.0 0.0 0.0 We evaluate three coding agents on ISO-Bench across 54 optimization tasks (39 from vLLM, 15 from SGLang). This section presents our findings on how coding agents approach inference optimization challenges and why traditional metrics alone fail to capture their performance."
        },
        {
            "title": "5.1 Can Agents Optimize GPU Inference Code?",
            "content": "The percentage of tasks where agents both identified the correct bottleneck and achieved measurable performance improvements is defined as the True Success rate, presented in Table 3. Appendix D.1 provides detailed Q1 example. Table 3: True Success rates (%) by Project. Agent vLLM SGLang Claude Code TRAE (Sonnet) TRAE (GPT-5) Codex CLI 46.2% 28.2% 17.9% 20.5% 26.7% 80.0% 86.7% 80.0% In our experiments, we study the two inference engines vLLM and SGLang separately. On vLLM, Claude Code gets 46.2%, while the other agents get lower True Success rates. However, on SGLang, Claude Code gets 26.7%, while the other agents get higher True Success rates, with TRAE (GPT-5) at 86.7%. This difference in agent performance for these projects is further studied in Section 5.4."
        },
        {
            "title": "5.2 Do Hard Metrics Tell the Full Story?",
            "content": "Each task in ISO-Bench specifies particular bottleneck that agents must optimize. However, agents sometimes achieve measurable speedups by modifying code unrelated to the specified bottleneck. These are Lucky Wins (Q3) from Section 3.3.3, and the gap between Hard Success and True Success measures how often this occurs. Table 4 presents the complete breakdown across all agent-project combinations. Claude Code on SGLang achieves Hard Success rate of 46.7%, but under True Success this drops to 26.7%, gap of 20%. Similar patterns emerge across other configurations, with gaps ranging from 2.6% to 12.8% on vLLM. These results show that hard metrics alone can overestimate agent capabilities. Soft metrics address this by checking whether agents actually modify the code regions specified in each task, making it possible to separate understanding from accidental improvements."
        },
        {
            "title": "5.3 Why Do Agents Fail?",
            "content": "Prior benchmarks (e.g., GSO [15], KernelBench [14]) often link poor agent performance to limited understanding of the optimization target. However, we observe that for inference optimization tasks, agents frequently identify correct bottleneck, yet fail to produce working optimization. We classify these cases in Q2, Good Intent Bad Execution, where agents target the right optimization but fail to implement working solution. On vLLM, three of four agents have their highest count in Q2, as shown in Table 5. The gap between correct understanding and execution is illustrated in Figure 3. TRAE (GPT-5) has the highest understanding-to-execution gap. Other agents follow similar patterns, except for Claude Code which has the strongest execution capability. Appendix D.2 provides detailed Q2 example where the agent identified the correct target but failed to implement working solution. See Appendix D.4 for Q4 complete failure example. On SGLang, the gap narrows considerably; see Figure 4. All agents except Claude Code identify the correct target in all 15 tasks. Not only that, these agents also demonstrate much stronger execution. In contrast, Claude Code struggles on this codebase: despite identifying 12 Table 5: Distribution of outcomes across quadrants (Q1-Q4). Project Agent Q1 Q2 Q3 vLLM"
        },
        {
            "title": "SGLang",
            "content": "Claude Code Codex CLI TRAE (Sonnet) TRAE (GPT-5) Claude Code Codex CLI TRAE (Sonnet) TRAE (GPT-5) 18 8 11 7 4 12 12 13 15 20 20 27 8 3 3 4 5 2 1 3 0 0 0 2 6 6 4 0 0 0 0 11 correct targets, it fails to execute on 8 of them."
        },
        {
            "title": "5.4 Does Performance Generalize Across Codebases?",
            "content": "Agent rankings shift between vLLM and SGLang, with the top performer on one codebase becoming bottom performer on the other. To understand this switch, we examine each agents optimization strategy relative to the human reference fix. Figure 5 illustrates approach distribution on vLLM. No single strategy dominates, and matching the human approach does not guarantee success. TRAE (GPT-5) matches the human approach most frequently yet achieves the lowest True Success. In contrast, Claude Code rarely matches the reference, favoring Partial and Alternative approaches, yet achieves the highest True Success. Figure 6 highlights different pattern on SGLang. All agents except Claude Code frequently adopt approaches similar to human approaches which results in higher success. Claude Code never adopts Similar approach, strategy that backfires on SGLang. Each agent maintains its preferred strategy regardless of the project. Claude Code consistently favors alternative approaches, which succeed on vLLM but fail on SGLang. TRAE (GPT-5), TRAE (Sonnet) and Codex CLI consistently match the reference approach, which succeeds on SGLang but fails on vLLM. This suggests that single-codebase evaluations can overstate how well an agent generalizes to new repositories."
        },
        {
            "title": "5.5 Does Agent Scaffolding Matter?",
            "content": "TRAE (Sonnet) and Claude Code both use the same underlying model: Claude Sonnet 4.5. However, their performance differs notably across codebases as seen in Table 4. On vLLM, Claude Code achieves 46.2% True Success while TRAE (Sonnet) reaches 28.2%. On SGLang, TRAE (Sonnet) achieves 80.0% while Claude Code reaches only 26.7%. The model is identical, but the agents differ in how they explore the codebase, decompose tasks, and decide when to stop iterating. These scaffolding choices produce the approach distributions shown in Figures 5 and 6: Claude Code favors partial and alternative solutions, while TRAE (Sonnet) favors matching the human approach. This suggests that model capability alone does not determine agent performance on optimization tasks, and benchmarks that evaluate only the underlying model may not predict how an agent performs on real-world tasks."
        },
        {
            "title": "5.6 Do Optimizations Preserve Correctness?",
            "content": "For all Hard Success cases, we run functional correctness tests using the method described in Section 3.3.4. For all Q1 (True Success) cases, the accuracy of the model mentioned in the commit stayed within the specified tolerance range, confirming that optimizations generated by agents are valid and achieve speedup over baseline (similar to human reference, or sometimes even surpassing them). 12 Q3 (Lucky Win), defined in Section 3.3.3, shows cases where agents achieve strong hard metrics but miss the correct optimization target according to soft metrics. These cases are prone to reward hacking, where agents take shortcuts to hit speed targets while breaking correctness. On commit fe66b347 (see Appendix D.3), TRAE (Sonnet) speeds up Bamba-9B inference, and hard metrics indicate success matching the human reference. However, soft metrics flag that the agent modifies code unrelated to the specified bottleneck. Functional correctness tests confirm the problem, showing total collapse in accuracy from 32% to 0%. The patch hardcodes tensor dimensions in the Mamba mixer layer instead of preserving original tensor shapes, producing garbage outputs. Hard metrics alone would have classified this as successful optimization."
        },
        {
            "title": "6 Discussion",
            "content": "In this section, we discuss qualitative examples of how open-source agents behave on ISO-Bench and highlight opportunities to improve their optimization performance."
        },
        {
            "title": "6.1 Why Do Open-Source Models Fail at Optimization?",
            "content": "We evaluated three open-source models, GPT-OSS-120B1, MiniMax-M2.12, and GLM-4.73 using the same TRAE-Agent scaffolding, task specifications, and evaluation protocol as in our closed-source experiments. None produced working optimization, and their failures cluster into small number of recurring patterns. Failure to attempt the task MiniMax-M2.1 and GPT-OSS-120B did not produce valid optimization attempt. MiniMax-M2.1 repeatedly outlined plans to use tools and apply optimization strategies, but never executed any tool calls during the session; the logs contain near-identical phrases repeated thousands of times without any actions. GPT-OSS-120B exhibited different failure mode: rather than optimizing the vLLM codebase, it attempted to reimplement external dependencies such as PyTorch, Triton, and Transformers inside the project directory, indicating misunderstanding of the environment. When these attempts failed, it entered repeated apology loop and did not recover, ultimately producing no optimization patch. Active interaction without task completion Despite extensive interaction with the codebase, GLM-4.7 failed to generate valid candidate for optimization. The model successfully made code edits but could not complete the task workflow. After applying valid patches, it encountered confusing error messages when attempting to verify its changes, causing it to cycle through git operations repeatedly. GLM-4.7 hit the step limit without ever calling the finish command, unable to recognize when to finalize the task. 1https://huggingface.co/openai/gpt-oss-120b 2https://huggingface.co/MiniMaxAI/MiniMax-M2.1 3https://huggingface.co/zai-org/GLM-4.7 13 We evaluated only one scaffolding framework (TRAE-Agent), and open-source models are improving quickly. Even so, the recurring failure modes we observed highlight clear targets for progress; More details are provided in Appendices D.5D.7."
        },
        {
            "title": "6.2 Conclusion",
            "content": "We introduce ISO-Bench, benchmark of 54 GPU inference optimization tasks drawn from vLLM and SGLang. ISO-Bench evaluates agents using both hard metrics, including Time to First Token and throughput, and soft metrics, including bottleneck targeting and implementation approach. The combination of these hard and soft metrics, organized in the framework, helps us separate real optimization from improvements that dont actually lead to better performance. Our evaluation points to three main findings. First, hard metrics alone can overestimate agent performance by about 10 - 20 percent because of lucky wins, where agents get speedups without fixing the true bottleneck. Second, the biggest failure mode is execution, not understanding. Agents often identify the right target, but fail to implement working solution. Third, performance does not transfer cleanly across codebases. Rankings flip between vLLM and SGLang, and scaffolding choices matter as much as the underlying model. We hope ISO-Bench will be useful resource for future work on building stronger coding agents for inference optimization, supporting model improvements (via RL) and scaffolding improvements that enable generalization across different codebases."
        },
        {
            "title": "6.3 Limitations and Future Work",
            "content": "Dataset scope ISO-Bench includes 54 tasks across two codebases. While vLLM and SGLang are major inference engines, expanding to additional systems, such as TensorRT-LLM and Max Inference, would improve coverage. Furthermore, our pipeline excludes commits that modify more than 10 files. This biases ISO-Bench toward localized, patch-sized optimizations and away from broader systems-level performance work. These larger changes often include inference optimization at the architecture level, such as redesigning scheduling, memory management, caching, or execution flow, which typically spans many modules. As result, agents may perform well on targeted edits, but their ability to deliver end-to-end speedups that require coordinated, multi-module code changes remains untested. Contamination risk Tasks are taken from public PRs in popular repositories that frontier models likely encountered during training. Future work should explore temporal filtering, patch paraphrasing, or held-out repositories to reduce memorization risk. Soft metric reliability Our soft metrics rely on single LLM judge. However, the outputs have not been validated against human annotators. Expanding human audits and evaluating with multiple frontier models would strengthen reproducibility. 14 Hardware scope ISO-Bench runs all benchmarks on single NVIDIA H100 GPU. This excludes multi-GPU optimizations such as tensor parallelism and pipeline parallelism, which are common in production deployments. Additionally, our evaluation is limited to NVIDIA hardware. Expanding to other providers like AMD, Google TPU, etc. would test whether agent generated optimizations transfer across different hardwares."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude Code. https://github.com/anthropics/claude-code, November 2025. [2] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code, 2021. URL https://arxiv.org/abs/2107.03374. [3] Neil Chowdhury, James Aung, Chan Jun Shern, Oliver Jaffe, Dane Sherburn, Giulio Starace, Evan Mays, Rachel Dias, Marwan Aljubeh, Mia Glaese, Carlos E. Jimenez, InJohn Yang, Leyton Ho, Tejal Patwardhan, Kevin Liu, and Aleksander Madry. troducing SWE-bench Verified, August 2024. URL https://openai.com/index/ introducing-swe-bench-verified/. [4] Tri Dao, Daniel Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx. [5] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. Framework for Few-shot Language Model Evaluation, 12 2023. URL https://zenodo.org/records/10256836. 15 [6] Google DeepMind. Gemini 3 Flash: Model card, December 2025. URL https: //deepmind.google/models/model-cards/gemini-3-flash/. Model card. Published Dec 2025. Accessed 2026-01-27. [7] Xinyi He, Qian Liu, Mingzhe Du, Lin Yan, Zhijie Fan, Yiming Huang, Zejian Yuan, and Zejun Ma. SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?, 2025. URL https://arxiv.org/abs/2507.12415. [8] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can Language Models Resolve Real-world GitHub Issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. [9] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, page 611626, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702297. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. [10] Jianling Li, ShangZhan Li, Zhenye Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jiacheng Huang, WangHaojie WangHaojie, Jianrong Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 2305323066, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.1183. URL https://aclanthology.org/2025.findings-acl.1183/. [11] Jeffrey Jian Ma, Milad Hashemi, Amir Yazdanbakhsh, Kevin Swersky, Ofir Press, Enhui Li, Vijay Janapa Reddi, and Parthasarathy Ranganathan. SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?, 2025. URL https:// arxiv.org/abs/2511.06090. [12] Jiwon Moon, Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, and Kyomin Jung. Dont Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation, 2025. URL https://arxiv.org/abs/2505.16222. [13] OpenAI. Codex CLI. https://github.com/openai/codex, November 2025. [14] Anne Ouyang, Simon Guo, Simran Arora, Alex Zhang, William Hu, Christopher Re, and Azalia Mirhoseini. KernelBench: Can LLMs Write Efficient GPU kernels? In Aarti Singh, Maryam Fazel, Daniel Hsu, Simon Lacoste-Julien, Felix Berkenkamp, Tegan Maharaj, Kiri Wagstaff, and Jerry Zhu, editors, Proceedings of the 42nd International Conference 16 on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 4735647415. PMLR, 1319 Jul 2025. URL https://proceedings.mlr.press/v267/ ouyang25a.html. [15] Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, and Ion Stoica. GSO: Challenging software optimization tasks for evaluating SWE-agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://openreview.net/forum?id=I5qDL315bQ. [16] Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, and Xia Liu. Trae Agent: An LLM-based agent for software engineering with test-time scaling, 2025. URL https://arxiv.org/abs/2507.23370. [17] Weixi Tong and Tianyi Zhang. CodeJudge: Evaluating code generation with large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2003220051, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.emnlp-main.1118. URL https://aclanthology.org/2024.emnlp-main. 1118/. [18] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI software developers as Generalist Agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=OJd3ayDDoF. [19] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=mXpq6ut8J3. [20] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao. [21] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying In The Sheng. SGLang: Efficient execution of structured language model programs. 17 Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=VqkAKQibpq. [22] Terry Yue Zhuo. ICE-Score: Instructing large language models to evaluate code. In Yvette Graham and Matthew Purver, editors, Findings of the Association for Computational Linguistics: EACL 2024, pages 22322242, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-eacl. 148/."
        },
        {
            "title": "A Reproducibility",
            "content": "To support reproducibility and future research, we release all artifacts necessary to replicate and extend our experiments. Code. The complete evaluation harness, agent configurations, and analysis scripts are available at: https://github.com/Lossfunk/ISO-Bench. Data. The benchmark dataset is available at: https://huggingface.co/datasets/Lossfunk/ ISO-Bench. License. The code is released under the Apache 2.0 license and the dataset is released under the CC BY 4.0 license."
        },
        {
            "title": "B Task Collection",
            "content": "ISO-Bench tasks are derived from merged pull requests in vLLM and SGLang that demonstrate measurable performance improvements. We extract optimization commits through multi-stage filtering pipeline, then manually curate each candidate to ensure task quality. This section describes the collection methodology following the structure inspired by GSO [15]. B.1 Commit Filtering Pipeline We identify performance-related commits through three-stage automated pipeline. First, we scan the complete commit history of each repository, filtering commits by keyword matching on messages and diffs for performance-related terms (optim, perf, speed, latency, throughput, memory, cache, kernel, fusion, batch). We selected these 10 keywords based on manual analysis of 100 sample commits from each repository. Second, we apply scope filtering to retain only commits modifying fewer than 10 files. We observed that commits that affect more than 10 files are usually more focused towards system-wide changes and they are not isolated optimization tasks. Third, for remaining candidates, GPT-5-mini classifies whether each commit represents genuine GPU inference optimization versus bug fix, refactoring, or documentation change. The model receives the commit message and truncated diff (up to 20,000 characters) and outputs binary classification with confidence score. Table 6 summarizes the filtering pipeline results across both repositories. Table 6: Commit filtering statistics for ISO-Bench. Stage vLLM SGLang Total commits Keyword matches Scope filter (<10 files) LLM classification Manual curation 15,234 892 341 186 39 8,421 547 312 201 15 B.2 Manual Curation After automated filtering, we manually review each candidate by examining the associated pull request discussion. This curation step serves four purposes: (1) verify that the commit represents genuine performance optimization rather than bug fix or refactoring; (2) extract the benchmarking configuration used by the original author, including model name, batch size, and request rate; (3) identify performance claims from the PR discussion to establish expected improvement targets; and (4) write natural language task description explaining the bottleneck without revealing the solution approach. The task description is critical for fair evaluation; it must provide enough context for agents to understand the optimization target while avoiding hints that would make the task trivial. This curation step, along with reduction of commits which require multi-gpu, or are very old, or have limited docker images available, goes down to 186 vLLM candidates to 39 final tasks, and 201 SGLang candidates to 15 final tasks."
        },
        {
            "title": "C Task Setup and Agent Configurations",
            "content": "This section describes the task setup and agent configurations for ISO-Bench evaluation. Each agent receives identical inputs and operates under the same constraints to ensure fair comparison. C.1 Task Specification Each task provides structured metadata defining the optimization challenge (Figure 7). The specification includes repository information (URL, human commit hash, parent commit for baseline), runner requirements (GPU type, CUDA version, Python version), and optimization details describing target files and constraints. C.2 Execution Environment Each agent operates in an isolated environment to ensure reproducibility and prevent cross-task interference. We use Docker containers built at the specific commit being evaluated, with commitTask Specification Schema id: \"vllm_attention_opt\" name: \"FlashAttention H100 optimization\" description: \"Optimize attention kernel for...\" repo: url: \"github.com/vllm-project/vllm\" human_commit: \"f092153f...\" pre_commit: null runner: requires_gpu: true cuda_version: \"12.4\" python_version: \"3.12\" optimization_contract: target_files: [\"vllm/attention/*.py\"] constraints: [\"No public API breakage\"] Figure 7: Task specification schema defining the optimization challenge metadata, including repository information, runner requirements, and optimization constraints. specific dependencies installed. Each container gets access to single NVIDIA H100 GPU with 80GB VRAM. Agents work on isolated git worktrees, allowing them to freely explore the codebase, modify files, and commit changes without affecting other runs. All agent edits are saved as git commits for post-hoc analysis. The time budget is 120 minutes per task, after which the agent process is terminated and the final worktree state is captured. Resource limits are set to 8 CPUs and 64GB RAM per container. Network access is permitted for downloading model weights and dependencies but is logged for reproducibility verification. C.3 Agent Configurations We evaluate four agent configurations with different models and scaffolding. Table 7 summarizes the configurations. Table 7: Agent configurations for ISO-Bench evaluation. Agent Model Claude Code Codex CLI TRAE-Agent (Sonnet) Claude Sonnet 4.5 TRAE-Agent (GPT-5) GPT-5 (Reas. High) Claude Sonnet 4.5 GPT-5 (Reas. High) Budget 120 min 120 min 120 min 120 min Claude Code is proprietary agent system with opaque scaffolding, while Codex CLI even though 20 being opensource is quite hard to customize due to the complexity of Rust. TRAE-Agent is modified version of ByteDances open-source framework, providing full trajectory visibility. By evaluating TRAE with both Claude Sonnet 4.5 and GPT-5, we can isolate the effect of scaffolding from the underlying model. C.4 Task Prompt Following GSO [15], we provide agents with structured prompt describing the optimization task without revealing the solution. The prompt includes the repository location, performance benchmark demonstrating the bottleneck, and guidelines for making changes. Figure 8 shows the complete prompt template."
        },
        {
            "title": "D Case Studies",
            "content": "This section provides detailed case studies illustrating each quadrant of our evaluation framework, along with failure patterns observed in open-source models. D.1 Case Study: FlashAttention True Success (Q1) Figure 9 shows Q1 (True Success) case where TRAE (Sonnet) correctly identified and optimized the FlashAttention CPU overhead bottleneck. On commit 98f47f2a, the task required minimizing CPU overhead in the FlashAttention custom op for CUDA graph compatibility. The agents patch achieved +21.09% throughput improvement by moving tensor reshape operations outside the custom op, precisely matching the human optimization strategy. This represents genuine optimization capability: the agent understood that tensor reshaping inside the custom op creates unnecessary CPU overhead during non-CUDA-graph regions, and applied the same solution as the human engineer. D.2 Case Study: Qwen3 Parser Good Intent (Q2) Figure 10 shows Q2 (Good Intent, Bad Execution) case where TRAE (Sonnet) correctly identified the optimization target but failed to produce working code. On commit 015069b0, the task required replacing regex with string operations in the Qwen3 reasoning parser. The agents patch targeted the same file (qwen3 reasoning parser.py) and used the same approach (replacing regex with str.find()/str.partition()), but the generated code failed to execute properly during benchmarking. This pattern is common: the agent understands the optimization goal but introduces bugs during implementation, suggesting gap between comprehension and execution capability. D.3 Case Study: Bamba-9B Accuracy Regression (Q3) Figure 11 shows Q3 (Lucky Win) case where TRAE (Sonnet) achieved speedup on Bamba-9B but broke model correctness. On commit fe66b347, the task required optimizing Mamba-architecture inference. The agent produced patch that achieved speedup matching the human reference on hard metrics, but soft metrics flagged that the agent modified code unrelated to the specified bottleneck. Functional correctness testing with the LM Evaluation Harness confirmed exact-match accuracy 21 dropped from 32% to 0%. The critical error was replacing dynamic dimension preservation (-1 in expand()) with hardcoded num heads per rank. This causes shape mismatches when actual tensor dimensions differ, corrupting the Mamba state-space model calculations. D.4 Case Study: Prefix Caching Complete Failure (Q4) Figure 12 shows Q4 (Complete Failure) case where TRAE (Sonnet) completely missed the optimization target. On commit 2deb029d, the task required fixing the prefix caching warmup performance in BlockManagerV2 by marking cache hit blocks as computed after scheduling. The humans elegant fix added simple touched blocks set to track blocks and mark them computed via mark blocks as computed(). The agent instead produced convoluted patch that modified test files and reimplemented portions of the block allocator with unnecessary complexity, achieving no optimization classification because it never addressed the actual bottleneck. D.5 Case Study: MiniMax-M2.1 Planning Without Execution Figure 13 shows an open-source model failure where MiniMax-M2.1 verbalized optimization plans for 75 steps but never executed single tool call. Despite generating 81,782 output tokens over 477 seconds, the model produced zero tool invocations, unable to turn plans into action. D.6 Case Study: GPT-OSS-120B Environment Confusion Figure 14 shows an open-source model failure where GPT-OSS-120B fundamentally misunderstood the task environment. Instead of optimizing vLLM code, the model attempted to create mock implementations of PyTorch, Triton, and Transformers libraries inside the project directory, treating external dependencies as code to be written rather than APIs to be used. D.7 Case Study: GLM-4.7 Task Completion Failure Figure 15 shows an open-source model that made valid code edits but failed to complete the task workflow. GLM-4.7 successfully executed 386 tool calls including 59 successful str replace edits to the target files, but hit the 400-step limit without ever calling the finish command, cycling through git operations after encountering confusing error message. Performance Optimization Task Prompt Ive uploaded python code repository in the directory /workspace. The repository contains performance benchmark that measures inference latency and throughput. Your task is to optimize the codebase to improve performance on this benchmark. Basic guidelines: 1. Your task is to make changes to non-test files in the /workspace directory to improve performance. 2. Make changes while ensuring the repository is functionally equivalent to the original; outputs must remain identical. 3. Do not overoptimize for specific inputs. Make general performance improvements that benefit diverse workloads. 4. You may need to rebuild the repository for your changes to take effect before testing. Recommended workflow: 1. Explore the repository structure to understand the codebase architecture. 2. Run the benchmark script to establish baseline performance and identify bottlenecks. 3. Analyze the bottleneck code paths using profiling or code inspection. 4. Edit the source code to address the identified performance bottleneck. 5. Rebuild and rerun the benchmark to confirm improvement. Bottleneck description: [[ TASK-SPECIFIC DESCRIPTION OF THE PERFORMANCE ISSUE ]] Figure 8: Task prompt template provided to agents. The bottleneck description is customized per task based on PR analysis. 23 Case Study: FlashAttention CPU Overhead Optimization (Q1 True Success) Model: TRAE (Sonnet 4.5) Result: PASS (+21.09% Throughput) Groundtruth Commit: https://github.com/vllm-project/vllm/commit/98f47f2a Specification Summary: Optimize FlashAttention custom op to minimize CPU overhead for CUDA graph compatibility. Target: Move tensor reshape operations outside the custom op to reduce non-CUDA-graph CPU overhead. Agent Generated Patch diff --git a/vllm/v1/attention/backends/flash attn.py b/... index 5f8535eaa..b4f7f6529 100644 --- a/vllm/v1/attention/backends/flash attn.py +++ b/vllm/v1/attention/backends/flash attn.py @@ -135,6 +135,13 @@ class FlashAttentionImpl(AttentionImpl): assert scale == 1.0 and scale == 1.0, ( \"key/v scale is not supported in FlashAttention.\") + + + + + + # Reshape the query, key, and value tensors. # NOTE(woosuk): We do this outside the custom op to minimize # the CPU overheads from the non-CUDA-graph regions. query = query.view(-1, self.num heads, self.head size) key = key.view(-1, self.num kv heads, self.head size) value = value.view(-1, self.num kv heads, self.head size) output = torch.empty like(query) @@ -184,10 +191,8 @@ def unified v1 flash attention(...): - - - - + + # Reshape the query, key, and value tensors. query = query.view(-1, num heads, head size) key = key.view(-1, num kv heads, head size) value = value.view(-1, num kv heads, head size) # NOTE: Tensors are already reshaped in forward method # to minimize CPU overheads. Analysis: The agent correctly identified that tensor reshaping inside the custom op creates CPU overhead in the non-CUDA-graph Python regions; moving it to the capture-time path reduces per-step overhead. Figure 9: Patch from TRAE (Sonnet 4.5) on commit 98f47f2a, demonstrating Q1 (True Success). The agent correctly identified the bottleneck and achieved +21% throughput matching the human approach. 24 Case Study: Qwen3 Reasoning Parser Optimization (Q2 Good Intent, Bad Exec) Model: TRAE (Sonnet 4.5) Result: FAIL (Agent Code Did Not Execute) Groundtruth Commit: https://github.com/vllm-project/vllm/commit/015069b0 Specification in Optimize Qwen3ReasoningParser by replacing regex with string operations. Target: Replace re.compile() and findall() with str.partition() for faster text extraction. extract reasoning content Summary: Agent Generated Patch diff --git a/vllm/reasoning/qwen3 reasoning parser.py b/... index f588f4016..7a2786bcd 100644 --- a/vllm/reasoning/qwen3 reasoning parser.py +++ b/vllm/reasoning/qwen3 reasoning parser.py @@ -1,6 +1,5 @@ # SPDX-License-Identifier: Apache-2. -import re from collections.abc import Sequence rf\"{self.think start token}(.*?){self.think end token}\", re.DOTALL) self.reasoning regex = re.compile( # Find start and end positions using str.find() start index = model output.find(self.think start token) if start index == -1: @@ -31,9 +30,6 @@ class Qwen3ReasoningParser(ReasoningParser): - - @@ -121,29 +117,31 @@ def extract reasoning content(...): + + + + + + + + + + # Extract reasoning content between the tokens reasoning start = start index + len(self.think start token) reasoning content = model output[reasoning start:end index] end index = model output.find(self.think end token, start index) if end index == -1: return None, model output return None, model output Analysis: The agent chose the correct strategy (replace regex with string-based parsing), but its implementation failed due to basic Python correctness issues (e.g., none vs None, incorrect symbol casing), whereas the ground-truth uses str.partition(). Figure 10: Patch from TRAE (Sonnet 4.5) on commit 015069b0, demonstrating Q2 (Good Intent, Bad Execution). The agent targeted the correct bottleneck but produced non-working code. 25 Case Study: Bamba-9B Mamba Mixer Optimization (Q3 Lucky Win) Model: TRAE (Sonnet 4.5) Result: FAIL (Accuracy Regression) Groundtruth Commit: https://github.com/vllm-project/vllm/commit/fe66b347 Spec: Optimize mamba mixer2.py for Mamba-architecture models. Target: Reduce tensor operation overhead while preserving functional correctness. Agent Generated Patch diff --git a/vllm/model executor/layers/mamba/mamba mixer2.py b/... index b53a540..6956750 100644 --- a/vllm/model executor/layers/mamba/mamba mixer2.py +++ b/vllm/model executor/layers/mamba/mamba mixer2.py @@ -503,15 +503,29 @@ class MambaMixer2(CustomOp): else: groups = self.n groups // self.tp size = self.A[:, None, ...][:, :, None].expand( -1, self.head dim, self.ssm state size).to(dtype=torch.float32) dt = dt[:, :, None].expand(-1, -1, self.head dim) dt bias = self.dt bias[:, None, ...].expand(-1, self.head dim) = self.D[:, None, ...].expand(-1, self.head dim) = B.view(-1, groups, B.shape[1] // groups) = C.view(-1, groups, C.shape[1] // groups) num heads per rank = self.num heads // self.tp size # Optimize tensor preparations to reduce overhead # Use unsqueeze instead of indexing with None for clarity = self.A.unsqueeze(1).unsqueeze(2).expand( num heads per rank, self.head dim, self.ssm state size).to(...) # Optimize dt expansion - use unsqueeze for better performance dt = dt.unsqueeze(2).expand(-1, -1, self.head dim) # Optimize dt bias and expansions dt bias = self.dt bias.unsqueeze(1).expand(num heads per rank, ...) = self.D.unsqueeze(1).expand(num heads per rank, self.head dim) # Pre-compute and dimensions dim = B.shape[1] // groups = B.view(-1, groups, dim) = C.view(-1, groups, dim) hidden states reshaped = hidden states.view( - - - - - - - + + + + + + + + + + + + + + - + @@ -535,7 +549,7 @@ class MambaMixer2(CustomOp): -1, self.num heads // self.tp size, self.head dim) -1, num heads per rank, self.head dim) hidden states = hidden states.view( - + -1, (self.num heads // self.tp size) * self.head dim) -1, num heads per rank * self.head dim) Root Cause: The original code uses -1 in expand() to preserve each tensors original first dimension dynamically. The agent replaced this with num heads per rank, hardcoded value. When actual tensor dimensions differ, shape mismatches corrupt the Mamba state-space model calculations, producing garbage outputs. Figure 11: Patch from TRAE (Sonnet 4.5) on commit fe66b347, demonstrating Q3 (Lucky Win). The agent achieved speedup by computing incorrect values faster, causing 100% accuracy regression. Case Study: Prefix Caching Block Manager (Q4 Complete Failure) Model: TRAE (Sonnet 4.5) Result: FAIL (Agent Code Did Not Execute) Groundtruth Commit: https://github.com/vllm-project/vllm/commit/2deb029d Specification Summary: Fix BlockManagerV2 prefix caching warmup by marking cache hit blocks as computed after scheduling. Target: Add touched blocks tracking to mark blocks computed after batch scheduling, matching BlockManagerV1 behavior. Agent Generated Patch (excerpt from 153 lines) @staticmethod def test mark blocks as computed(): block size = 16 common blocks = 4 allocator = PrefixCachingBlockAllocator(num blocks=8, ...) ... allocator.mark blocks as computed(block ids) diff --git a/tests/core/block/test prefix caching block.py b/... + + + + + + + diff --git a/vllm/core/block/prefix caching block.py b/... -def incr refcount cached block(self, block: Block) -> None: - +def incr refcount cached block(self, block: Block, + + -def mark blocks as computed(self, block ids: - +def mark blocks as computed(self, block ids: + + + self. block tracker[block id].computed = True if block id in self. block tracker and ... computed: bool = False) -> None: for block id in block ids: block.computed = computed block.computed = True List[int]) -> None: List[int]) -> None: raise NotImplementedError(\"Marking as computed is incremental\") Analysis: The agent modified test files and reimplemented mark blocks as computed() but missed the core insight: blocks need to be tracked when allocated (via touched blocks) and marked computed after the entire batch is scheduled. The agents approach of marking individual blocks by ID doesnt solve the warmup problem because the scheduler never passes the right block IDs. This demonstrates failure to understand the actual performance bottleneck. Figure 12: Patch from TRAE (Sonnet 4.5) on commit 2deb029d, demonstrating Q4 (Complete Failure). The agent produced complex code that completely missed the actual optimization target. Case Study: MiniMax-M2.1 Infinite Loop (OSS Failure) Model: TRAE (MiniMax-M2.1) Result: FAIL (0 Tool Calls / 75 Steps) Task: vLLM scheduler performance optimization Statistics: Tool calls: 0 Steps: 75 Duration: 477s Output tokens: 81,782 (171 tok/s) Input tokens: 1,599,945 Execution Log (Steps 7075) Step 73: Completed LLM Response: The user keeps saying havent completed the task. need to explore the repository and make changes. Let me start by using the str replace based edit tool tool. need to actually use the tools now. need to actually use the tools now. need to actually use the tools now. need to actually use the tools now. need to actually use the tools now. [...repeated 2,412 times total...] Step 74: Completed LLM Response: need to actually use the tools now... Analysis: The model exhibits an understanding-execution gap: it can verbalize the correct strategy (use str replace based edit tool) but cannot translate this into an actual tool call. The phrase need to actually use the tools appears 2,412 times without any tool invocation, suggesting the model lacks the capability to bridge reasoning and execution. Figure 13: Execution log from MiniMax-M2.1 showing infinite verbalization loop. Despite 75 reasoning steps, the model made zero tool calls, demonstrating the understanding-execution gap. 28 Case Study: GPT-OSS-120B Environment Confusion (OSS Failure) Model: TRAE (GPT-OSS-120B) Result: FAIL (Created Mock Dependencies Instead of Optimizing) Task: vLLM scheduler performance optimization Tool Calls: 84 (all file creation attempts) Files the Model Attempted to Create + vllm core-0006/torch/ init .py + vllm core-0006/torch/nn/ init .py + vllm core-0006/torch/cuda/ init .py + vllm core-0006/triton/ init .py + vllm core-0006/transformers/ init .py Contents of attempted torch/ init .py: class dtype: pass float16 = float16 float32 = float32 = 0.0.0 version Analysis: The model exhibits environment confusion: rather than understanding that PyTorch is an external dependency to import, it attempted to recreate these libraries from scratch. This suggests fundamental failure to reason about software architecture: the model cannot distinguish between code should optimize and libraries should use. Figure 14: File creation attempts from GPT-OSS-120B showing environment confusion. The model tried to implement mock PyTorch/Triton libraries instead of optimizing the actual vLLM scheduler code. 29 Case Study: GLM-4.7 Task Completion Failure (OSS Failure) Model: TRAE (GLM-4.7) Result: FAIL (Valid Edits, Never Called Finish) Task: vLLM scheduler performance optimization Tool Calls: 386 (327 bash, 59 str replace) Successful Edits Made (Step 196) git commit output: 2 files changed, 11 insertions(+), 9 deletions(-) Sample optimization in scheduler.py: - + self. num batched tokens += num batched tokens self. num batched tokens = self. num batched tokens + num batched tokens Then at Step 198, attempted to verify: error: patch failed: tests/core/test scheduler.py:214 error: tests/core/test scheduler.py: patch does not apply Model response: Let me try different approach... [...cycled through git operations for 200+ more steps...] Final status: max steps exceeded (400 steps) Analysis: The model exhibits task completion failure: it successfully made valid edits and committed them, but when it tried to re-apply the already-applied patch for verification, git returned patch does not apply. Unable to interpret this as success, the model cycled through alternative approaches for 200+ steps without ever calling finish. This demonstrates gap between code generation and workflow navigation. Figure 15: Execution trace from GLM-4.7 showing valid edits followed by task completion failure. The model made successful optimizations but could not recognize when to finalize the task."
        }
    ],
    "affiliations": [
        "lossfunk.com"
    ]
}