{
    "paper_title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
    "authors": [
        "Yuxuan Xue",
        "Xianghui Xie",
        "Margaret Kostyrko",
        "Gerard Pons-Moll"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human."
        },
        {
            "title": "Start",
            "content": "InfiniHuman: Infinite 3D Human Creation with Precise Control YUXUAN XUE, University of T칲bingen, T칲bingen AI Center, Germany XIANGHUI XIE, University of T칲bingen, T칲bingen AI Center, MPI for Informatics, SIC, Germany MARGARET KOSTYRKO, University of T칲bingen, Germany GERARD PONS-MOLL, University of T칲bingen, T칲bingen AI Center, MPI for Informatics, SIC, Germany 5 2 0 2 3 1 ] . [ 1 0 5 6 1 1 . 0 1 5 2 : r Fig. 1. Using text description, explicit body shape, cloth image as input, our 3D human generative method, InfiniHuman, can automatically create variety of realistic 3D humans with high-fidelity texture and geometry. Our InfiniHuman allows for generating infinite 3D humans with precise user control. Generating realistic and controllable 3D human avatars is long-standing challenge. The difficulty increases when covering broad range of attributes such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in both scale and diversity. The central question we address in this paper is: Can we distill existing foundation models to generate theoretically unbounded richly annotated 3D human data? We introduce InfiniHuman, novel framework to distill these models synergistically, to generate richly annotated human data with minimal cost and theoretically unlimited scalability. Specifically, we propose InfiniHumanData, fully automatic pipeline that leverages vision-language and image generation models to create large-scale multi-modal dataset. Remarkably, users cannot distinguish our automatically generated identities from scan renderings. InfiniHumanData contains 111K identities and covers unprecedented diversity in ethnicity, age, clothing styles, and more. Each identity is Authors Contact Information: Yuxuan Xue, University of T칲bingen, T칲bingen AI Center, Germany; Xianghui Xie, University of T칲bingen, T칲bingen AI Center, MPI for Informatics, SIC, Germany; Margaret Kostyrko, University of T칲bingen, Germany; Gerard Pons-Moll, University of T칲bingen, T칲bingen AI Center, MPI for Informatics, SIC, Germany. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. SA Conference Papers 25, Hong Kong, Hong Kong 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2137-3/2025/12 https://doi.org/10.1145/3757377.3763815 annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body shape parameters. Based on this, we learn InfiniHumanGen, diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate that InfiniHuman significantly surpasses existing state-of-theart methods in terms of visual quality, generation speed, and controllability. Importantly, our approach democratizes high-quality avatar generation with fine-grained control at infinite scale through practical and affordable solution. To facilitate future research, we will publicly release our automatic data generation pipeline and the comprehensive dataset InfiniHumanData, and the generative models InfiniHumanGen. The code and data of InfiniHuman is publicly available at https://yuxuan-xue.com/infini-human. CCS Concepts: Computing methodologies Appearance and texture representations; Shape Inference; Machine learning approaches. Additional Key Words and Phrases: Text-guided 3D Generation, Digital Human, Text-to-Image Diffusion Model, Image-based Modeling ACM Reference Format: Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll. 2025. InfiniHuman: Infinite 3D Human Creation with Precise Control. In SIGGRAPH Asia 2025 Conference Papers (SA Conference Papers 25), December 1518, 2025, Hong Kong, Hong Kong. ACM, New York, NY, USA, 23 pages. https://doi.org/10.1145/3757377."
        },
        {
            "title": "Introduction",
            "content": "Creating realistic and controllable 3D human avatars is fundamental problem of growing significance in virtual reality, digital SA Conference Papers 25, December 1518, 2025, Hong Kong, Hong Kong. 2 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 2. Examples from InfiniHumanData. a) Diverse human identities covering wide range of ethnicities, age groups (including children), clothing styles, hair types, and skin tones, which are visually indistinguishable from real scans rendering (Sec. 4.2). b) Multi-modal annotations per each subject, including I) multi-view RGB images (full-body and head), II) SMPL parameters, III) clothing asset images, and IV) multi-granularity text descriptions. fashion, gaming, and social telepresence. Applications increasingly demand photorealistic avatars that can be personalized to match textual descriptions, specific body shapes, and user-provided clothing. However, the limitations of existing generation techniques have become increasingly apparent. In particular, generating diverse and semantically rich 3D humans, varying in clothing, ethnicity, age, gender, and shape, remains difficult due to the high cost and limited diversity of manually captured datasets. Recent training-free approaches such as Score Distillation Sampling (SDS) [Poole et al. 2023] have leveraged powerful text-to-image diffusion models to bypass dataset acquisition. However, these methods suffer from long optimization times, limited visual fidelity, and lack of precise control over attributes like garment appearance or detailed body shape. These limitations motivate critical research question: Can we distill the capabilities of foundation models to generate richly annotated 3D human data at theoretically unlimited scale and with precise controllability? We propose InfiniHuman, fully automated framework that addresses this question by systematically repurposing and integrating existing vision-language, image synthesis, pose estimation, and diffusion models. Our method produces realistic 3D human identities at unprecedented scale, each annotated with multi-view images, fine-grained textual descriptions, SMPL parameters, and explicit clothing representations. The resulting dataset, InfiniHumanData, contains over 111K identities and supports detailed control across age, ethnicity, clothing, and body morphology. Built upon this dataset, we introduce InfiniHumanGen, pair of generative models capable of synthesizing 3D avatars conditioned jointly on text, clothing image and body shape, giving the user powerful controls. It includes two complementary models: Gen-Schnell, , Vol. 1, No. 1, Article . Publication date: October 2025. which enables rapid 3D generation and produces Gaussian splatting output, and Gen-HRes, which produces high-resolution, photorealistic textured meshes. Our models outperform prior works on visual quality, speed, and attribute controllability, achieving stateof-the-art results with significantly lower computational cost. In summary, the main technical contributions of our work include: InfiniHuman, framework to generate virtually unlimited richly annotated data of humans by distilling existing foundation models. The framework is fully automatic and generates identities indistinguishable from real scans. InfiniHumanData, the first large-scale multi-modal human dataset comprising 111K diverse identities with rich multimodal annotations essential for precise avatar generation. InfiniHumanGen, novel generative framework supporting two distinct models: Gen-Schnell for fast and interactive 3D human generation and Gen-HRes for high-resolution and visually detailed 3D human creation; both from various userspecified inputs such as text, clothing, or body shape. By removing the need for costly scans, our method democratizes high-quality avatar creation, empowering applications in fashion, gaming, AR/VR, and beyond."
        },
        {
            "title": "2 Related work\n2.1",
            "content": "3D Human Generation. The creation of 3D human avatars from user-defined conditions is long-standing problem in vision and graphics, with most prior works falling into two categories: reconstruction from images [Liao et al. 2025; Saito et al. 2019; Xiu et al. 2023, 2022; Zheng et al. 2021], and generation from text [Cao et al. 2023; Han et al. 2023a; Hong et al. 2022; Kim et al. 2022; Kolotouros et al. 2023; Liao et al. 2023; Table 1. Comparison of related datasets. Most existing human datasets are limited at scale and none of them provide detailed identity annotation like fine-grained text and clothing image. Type Dataset IDs Multi-Text Cloth Assets CustomHuman [Ho et al. 2023] Sizer [Tiwari et al. 2020] 2K2K [Han et al. 2023b] 3 THuman2.1 [Yu et al. 2021] ActorsHQ [Isik et al. 2023] m ZJU-MoCap [Peng et al. 2021] DNA-Rendering [Cheng et al. 2023] HUMBI [Yu et al. 2020] HuMMan [Cai et al. 2022] MVHumanNet [Xiong et al. 2024] IDOL [Zhuang et al. 2025] i - u Ours InfiniHumanData 80 97 2050 2500 8 10 500 772 1000 4500 100K 111K Liu et al. 2024; Wang et al. 2024; Yuan et al. 2024; Zhang et al. 2023]. Recent methods have also explored learning avatars from large-scale 2D image collections [Dong et al. 2023; Hong et al. 2023; Xiu et al. 2024]. key limitation in existing works is controllability: prior approaches support conditioning on either text or body shape, but none allow direct, explicit control over detailed clothing items in addition to text and shape. This restricts their application in domains requiring personalized appearance, such as digital fashion or virtual fitting rooms. We fill this gap by introducing scalable and fully automatic data generation pipeline that enables the training of generative models conditioned on text, SMPL body shape, and specific clothing images. Our models achieve high-quality 3D human synthesis consistent with all these modalities, offering unprecedented fine-grained control and realism."
        },
        {
            "title": "2.2 Large-Scale 3D Datasets.",
            "content": "The availability of high-quality, large-scale 3D datasets is key driver of progress in generative 3D modeling. While object-centric datasets like Objaverse [Deitke et al. 2023] and ShapeNet [Chang et al. 2015] have enabled remarkable advances for general object synthesis and reconstruction, 3D human datasets pose unique challenges. Commercial human scan repositories such as RenderPeople, Twindom, and Axyz provide highly realistic scans, but are expensive (often 100 USD per identity). Publicly available 3D human datasets (Tab. 1) are often constrained by participant recruitment, scanning logistics, and privacy considerations, resulting in limited scale, demographic diversity, and coverage of clothing, age, and body morphology. Some alternatives use multi-view image capture to reduce costs, but these datasets are typically restricted to fixed camera viewpoints and controlled lighting, limiting their generalizability and value for generative tasks. Recent innovations, such as the IDOL dataset [Zhuang et al. 2025], leverage video diffusion models to synthesize 360-degree images from single 2D input. However, video diffusion often introduces view inconsistencies and lacks true 3D geometry (see Supp. Mat.), due to neighbor-only attention mechanisms and the absence of explicit 3D supervision. InfiniHuman: Infinite 3D Human Creation with Precise Control 3 Critically, existing datasets rarely provide fine-grained annotations of identity level that are essential for training generative models capable of precise control over appearance attributes. Our InfiniHumanData addresses all these limitations by using multimodal foundation models to generate large-scale, richly annotated dataset, covering unprecedented diversity across age, ethnicity, body shape, and clothing style, and providing annotations that support high-fidelity, controllable 3D human generation. To accelerate research and enable further expansion, we publicly release our fully automatic data generation pipeline and dataset, empowering the community to create virtually unlimited, realistic human identities."
        },
        {
            "title": "3 Method",
            "content": "Our objective is to generate highly realistic 3D avatars that allow precise and flexible control based on multiple user-specified conditions. These conditions include (i) natural language descriptions to define the subjects appearance, (ii) SMPL parameters to govern body shape and pose, and (iii) reference images to specify clothing style. To enable such fine-grained generation, we must model the joint conditional distribution 洧녞 (洧눜洧눇 text, 洧눇 SMPL, 洧눇 cloth), where represents the generated avatars, and terms represent the conditioning signals specified by users. This task requires large, diverse, and richly annotated dataset of 3D human avatars, which is costly and impractical to collect and annotate manually. Instead, we present InfiniHuman, fully automated framework that synthesizes such dataset by distilling existing foundation models across multiple domains. We first detail the construction of our dataset, InfiniHumanData, in Sec.3.1, and then describe our controllable generative models, InfiniHumanGen, in Sec.3.2. 3.1 InfiniHumanData - Generation by Reconstruction To enable highly controllable 3D avatar generation, we first construct large-scale, richly annotated dataset, InfiniHumanData. Our data generator produces multi-modal outputs for each identity, including structured text descriptions, clothing style images, SMPL body shape and keypoints, and orthographic multi-view images with controlled lighting suitable for 3D lifting (see Fig. 3 for visualization and detailed breakdown). In the following, we describe the major components of our data generation pipeline. A) Multi-Granularity Text Description. To encode diverse semantic concepts, we design captioning system that generates both detailed and progressively abstracted descriptions. We first caption existing human scan datasets [Han et al. 2023b; Ho et al. 2023; Yu et al. 2021] using the protocol from Trellis [Xiang et al. 2024]. Next, we randomly sample ten captions and provide them as in-context examples to GPT-4o, prompting it to generate new variations. These generated captions maintain similar lengths and formats, while diversifying attributes such as ethnicity, age group, and clothing style. Each caption is then summarized into ten levels of granularity, ranging from 40 words to 5 words. This hierarchical annotation enriches training by exposing models to both coarse (e.g., old) and fine-grained (e.g., late sixties to early seventies) semantic cues. , Vol. 1, No. 1, Article . Publication date: October 2025. 4 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 3. Overview of data generation framework in InfiniHumanData. The process is fully automated by leveraging foundation models. Desired outputs are marked with flags: A) Structured text descriptions, C) Clothing style images, E) Body shape in SMPL format plus face and hand keypoints, F) Orthographic multi-view images with controlled lighting conditions suitable for 3D lifting. B) Orthographic Text-to-Image. Most text-to-image models (e.g., FLUX) produce images with dramatic perspective and complex lighting, which are suboptimal for 3D reconstruction tasks. To address this, we fine-tune FLUX with LoRA adapter [Hu et al. 2022] on orthographic renderings of few thousand scans under uniform lighting, enabling generation of scan-like images (see Fig. 3). This stylization step ensures compatibility with downstream 3D lifting processes. In particular, orthographic views are essential for our multi-view diffusion, which relies on simplified epipolar attention [Li et al. 2024a]. Importantly, this approach preserves the inherent diversity of FLUX while aligning the image domain for reconstruction. challenging discriminative user study (Sec. 4.2) further demonstrates that our generated identities achieve visual realism on par with scan renderings. C) Virtual-TryOff for Clothing Control. Because single image can convey garment appearance more precisely than any text description, we provide direct clothing control by reversing the try-on process. Given full-body image, we fine-tune OminiControl [Tan et al. 2024] to extract clean garment image via textbased image-to-image translation. This task, termed Instruct-VirtualTryOff, is trained using garment-actor pairs from existing VirtualTryOn datasets [Choi et al. 2021; Morelli et al. 2022] and prompts like \"<Please extract {Garment} for this person>\". Each training instance consists of garment image 洧냪 cloth, corresponding try-on image 洧냪 vton, and textual prompt 洧뇺ext. The model parameters 洧럏 are optimized via the flow-matching objective: LVToFF (cid:0)洧럏 (cid:1) = E洧노,洧럌 (cid:13) 洧녺洧럏 (cid:13) (cid:13) (cid:0)x洧노 , 洧냪 vton, 洧뇺ext, 洧노 (cid:1) (cid:0)洧럌 洧냪 cloth(cid:1)(cid:13) 2 (cid:13) (cid:13) , where x洧노 = (1 洧노) 洧냪 cloth + 洧노洧럌, 洧럌 (0, I). (1) (2) , Vol. 1, No. 1, Article . Publication date: October 2025. Here, 洧녺洧럏 denotes the network, x洧노 is noisy version of the garment image, and 洧뇺ext provides the instruction (see Fig. 3). The network learns to synthesize clean garment images conditioned on full-body images and textual instructions. This enables users to specify clothing via image, without requiring paired image-scan training data. D) Negative Samples Rejection. To remove occasionally wrongly generated images, we use the sampling rejection strategy: first generate four garment images per subject and then employ GPT-4o to select the best match based on considerations like color, texture, length, and detailed features (e.g. zippers, pockets). The detailed prompt for sampling rejection can be found in Supp. Mat. E) Monocular Body Fitting for Shape and Pose Control. We use NLF [S치r치ndi and Pons-Moll 2024] to regress SMPL parameters from orthographic views by setting FoV to 0.1, followed by refinement via OpenPose 2D joint alignment [Cao et al. 2019]. This two-step process ensures that SMPL parameters align accurately with both overall pose and pixel-level features (particularly at face), which is crucial for consistent multi-view generation conditioned on SMPL. More specifically, we optimize the SMPL body pose parameters w.r.t. the reprojection error between the orthographically projected SMPL joints and 2D joints estimated by OpenPose: Lreproj (cid:0)洧럏 (cid:1) = 洧 洧녲=1 洧녻洧녰洧녲 (cid:13) 洧랢ortho (cid:13) (cid:13) (cid:0)洧냫洧녲 (SMPL(洧럏 洧눍, 洧량))(cid:1) 洧냫 OpenPose 洧녲 (cid:13) (cid:13) (cid:13) 2 2 (3) We carefully tweak the per-joint weights and the regularization to achieve the best pixel-level matching between 3D SMPL and 2D images. Please refer to Fig. 14 and supplementary material for qualitative visual examples and ablation studies. F) Orthographic MV-Diffusion. To produce high-resolution, consistent multi-views, we train diffusion model on orthographic InfiniHuman: Infinite 3D Human Creation with Precise Control 5 Fig. 4. Overview of a) Gen-Schnell and b) Gen-HRes in InfiniHumanGen. Taking text description, explicit SMPL shape, and cloth image as input, Gen-Schnell generates 3D-GS end-to-end, while Gen-HRes generates high-resolution textured mesh, both matched to input conditions. projections with uniform lighting. Orthographic views have horizontal epipoles, enabling efficient row-wise attention across views. Given an orthographic RGB image 洧냪 in R洧냩 洧녥 洧냤 , our multiview diffusion (MVD) model generates 洧녜 views of high-resolution full-body images 洧냪 body R洧녜 洧냩 洧녥 洧냤 and head images 洧냪 head R洧녜 洧냩 洧녥 洧냤 from the front, left, right, and back directions. We provide geometric guidance by rendering SMPL normal maps 洧냪 SMPL and encoding them, together with the reference image, into the latent space using pretrained VAE from PSHuman [Li et al. 2024b]. For multi-view consistency, we apply orthographic multi-view attention separately within the body and head views, where each row of the each view attends to the same row of other views due to the orthographic constraint across views. Please refer to Fig. 15 for visual examples. For body-head consistency, we use dense pixellevel cross-attention between corresponding body and head views, where each pixel of body image attends to pixels of the head image under the same view. The UNet denoiser 洧랬 (洧럏 ) is fine-tuned using the following objective: LMVD (cid:0)洧럏 (cid:1) = E洧노,洧럌 洧녷 洧노 , 洧냪 in, 洧냪 SMPL, 洧노 (cid:1) 洧럌 (cid:0)x 2 , (cid:13) (cid:13) (cid:13) 洧녷 洧노 = where 1 洧띺洧노 洧럌, 洧럌 (0, I). (cid:13) 洧랬洧럏 (cid:13) (cid:13) 洧녷 {body,head} 洧띺洧노 洧냪 洧녷 + (4) (5) Here, 洧띺洧노 determines the noise level at each diffusion step 洧노. At inference time, our multi-view diffusion model takes an orthographic input image and SMPL normal maps as input, generating highresolution multi-view body and head images (see Fig. 3, right). learn joint conditional distribution 洧녞 (洧눜洧눇 text, 洧눇 SMPL, 洧눇 cloth) to enable precise avatar generation. We train two complementary models to support both fast and high-fidelity generation: A) Gen-Schnell: Fast End-to-End Generation. Gen-Schnell is low-latency model that directly generates 3D avatars as Gaussian splats [Kerbl et al. 2023]. Inspired by Human-3Diffusion [Xue et al. 2024], we combine 2D multi-view generation (from MVDream [Shi et al. 2024]) with splatting decoder that enforces consistency across views. To inject condition signals, we encode SMPL normal maps 洧눇 SMPL and clothing images 洧눇 cloth using the MVDream VAE, and concatenate them channel-wise with the initial noise x洧노 . The 2D diffusion model 洧랬 (洧럏 ) predicts noise values, which are used to reconstruct clean multi-view images x0: x0 = 1 洧띺洧노 (cid:16) x洧노 1 洧띺洧노洧랬洧럏 (cid:16) x洧노 , ctext, cSMPL, ccloth, 洧노 (cid:17)(cid:17) . (6) While the resulting multi-view images x0 provide strong shape priors, they may exhibit inconsistencies across views. To address this, our 3D-GS generator 洧녮(洧롑) takes the predicted multi-view images x0 and initial noise x洧노 to generate consistent 3D Gaussian splats 틙G0, which render consistent multi-view images 틙x0. During each sampling step from 洧노 to 洧노 1, we replace 2D predictions with 3D-GS rendered images to ensure consistency: 洧랞洧노 1 (x洧노, 틙x0) = (cid:1) 洧띺洧노 (cid:0)1 洧띺洧노 1 1 洧띺洧노 洧띺洧노 1 洧띻洧노 1 洧띺洧노 x洧노 1 N(cid:0)x洧노 1; 洧례洧노 (x洧노 , 틙x0) , 洧띻洧노 1I(cid:1). x洧노 + 틙x0, (7) 3.2 InfiniHumanGen - Generation with Precise Control Joint Conditional Distribution. Leveraging InfiniHumanData, 3.2.1 which contains 111K diverse identities each annotated with multigranularity text captions 洧눇 text, SMPL parameters 洧눇 SMPL, corresponding cloth images 洧눇 cloth, and orthographic multi-view images 洧눜洧녴洧녺, we At the final timestep 洧노 = 0, 틙G0 is output as the final 3D-GS, see Fig. 4. Gen-Schnell is highly efficient and produces 3D-GS in about 12 seconds. However, due to the low-resolution constraint of MVDream (256256), detailed features (e.g., facial textures, textual elements) appear blurry, motivating our high-resolution generator, Gen-HRes. , Vol. 1, No. 1, Article . Publication date: October 2025. 6 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 5. Fine-grained text controllability in Gen-HRes over (a) overall subject identity, such as ethnicity, age, gender, etc. By fixing the initial Gaussian noise, Gen-HRes can generate (b) same identity with different detailed accessory appearance, such as watch, glasses, and colors of wearing assets. B) Gen-HRes: High-Resolution Generation. For photorealistic avatar generation from multiple conditions, Gen-HRes frames it as multi-image-to-image translation task, where we fine-tune OminiControl2 [Tan et al. 2025] on InfiniHumanData. Using fullbody images y2D as target, we optimize the flow matching objective: LHRes (cid:0)洧럏 (cid:1) = E洧노,洧럌 (cid:13) 洧녺洧럏 (cid:13) (cid:13) (cid:0)x洧노, ctext, ccloth, cSMPL, 洧노 (cid:1) (cid:0)洧럌 y2D(cid:1)(cid:13) 2 (cid:13) (cid:13) , where x洧노 = (1 洧노) y2D + 洧노洧럌, 洧럌 (0, I). (8) (9) Our data and model design ensure that the generated multi-view images are well aligned with the conditioning SMPL mesh. This alignment allows us to compute surface normals with Sapiens2B [Khirodkar et al. 2024] and apply SMPL-driven volumetric carving via PSHuman [Li et al. 2024b] for high-fidelity 3D mesh reconstruction. Compared to Gen-Schnell, Gen-HRes not only achieves higher resolution and visual fidelity, but also supports detailed text prompting. By fixing initial Gaussian noise, Gen-HRes can precisely control fine-grained attributes, such as glasses or garment colors, through detailed text descriptions, as shown in Fig. 5. Gen-HRes enables high-fidelity avatar generation in approximately 4 minutes."
        },
        {
            "title": "Implementation Details",
            "content": "The orthographic multiview diffusion model used in InfiniHuman and Gen-HRes is built upon the pre-trained text-to-image model SD2.1-unclip. We concatenate the input image latents with noise latents along the channel dimension. The noise latents are replicated for each view, and the text embedding is repurposed to generate distinct head and body views, similar to PSHuman [Li et al. 2024b]. Our model generates four orthogonal body views and four head views from single orthographic input body image. For the orthographic multiview diffusion models, we train on 8 H100 with effective batch size of 128 for 2 days on orthographic uniform lighting rendering of 6000 high-quality human scans from Twindom, CustomHuman, and THuman2.1 [twi 2023; Ho et al. 2023; Yu et al. 2021]. We use front-view renders with text labels to fine-tune Flux-Dev [Black Forest Labs 2024] LoRA for the orthographic text-to-image task. For constructing the InfiniHumanData, we use GPT-4o to enhance , Vol. 1, No. 1, Article . Publication date: October 2025. Table 2. Quantitative comparison results. We report user study results for appearance quality and text alignment, where most participants prefer our method. We also achieve SOTA in T2I metrics such as CLIP and FID. Method MVDream SPAD Gen-Schnell TADA DreamAvatar HumanGaussian HumanNorm AvatarVerse Gen-HRes Quality (User Study) Alignment (User Study) FID CLIP Score Runtime 20.83% 2.02% 77.14% 1.27% 1.27% 2.22% 2.54% 0.32% 92.39% 20.36% 1.55% 78.10% 1.27% 1.90% 3.48% 3.48% 0.32% 89.56% 141.33 150.43 100.39 129.68 151.57 140.24 101.84 156.52 82.28 30.37 28.58 30.82 28.84 28.42 30.56 28.30 28.69 30. 2.8s 13.9s 12.9s 213m 384m 40 117 44 4 correctness of automatic cloth labeling, where each subject takes around $0.03. Based on InfiniHumanData, we train Gen-Schnell on 8 A100 GPUs with effective batch size 256 over approximately 2 days, and Gen-HRes on 2 H100 GPUs with effective batch size of 32 for 2 days. Please refer to Supp. Mat. for implementation details on Gen-Schnell as well as Gen-HRes, and for prompting details on constructing InfiniHumanData."
        },
        {
            "title": "4.2 Evaluation Benchmark",
            "content": "We compare Gen-Schnell with feed-forward text-based multi-view generation approaches such as MVDream [Shi et al. 2024] and SPAD [Kant et al. 2024], which can generate multi-view images from text prompt within minute. We compare Gen-HRes with SDS-based text-to-avatar approaches, e.g. DreamAvatar [Cao et al. 2024], AvatarVerse [Zhang et al. 2024], HumanGaussian [Liu et al. 2024], HumanNorm [Huang et al. 2024], and TADA [Liao et al. 2024]. These optimization-based methods achieve higher quality than feed-forward approaches but typically require several hours for generation. Therefore, we also compare with Chupa [Kim et al. 2023], mesh-based avatar generator directly learned from 3D scans. Furthermore, we evaluate the realism of generated identities in InfiniHumanData. InfiniHuman: Infinite 3D Human Creation with Precise Control 7 Fig. 6. Generate avatars with given garment from fashion industry. The identity is preserved while TryOn garment is changing. Fig. 7. Generate avatars with precise pose shape control and text-based editing. The identity is preserved during shape and text-based editing. 4.2.1 Qualitative Comparison. As depicted in Fig. 11, Gen-HRes has various advantages over baselines: (1) multi-view consistency, because Gen-HRes generates textured mesh as output, while SDSmethod optimizes per view given text prompt, which can lead to the Janus problem. (2) enhanced realism, Gen-HRes does not suffer from unnatural saturation, which is typical problem in SDS-based generation. (3) text-following ability, Gen-HRes leverages foundational text-based generation capability from FLUX, which shows stronger text-following ability than SDS-based methods, especially in details such as color of garments. Gen-Schnell also shows better text-following ability (e.g. head view, color) than previous works. Please refer to Fig.3 and Fig.4 in Supp. Mat. for more comparison. 4.2.2 Quantitative Comparison. We conducted user study to quantitatively compare with SOTA methods in text-based generation. We asked 42 participants to evaluate videos rendered from generated 3D avatars generated by different methods and to vote for the best methods based on overall appearance quality and alignment with text description. We also report quantitative numbers in FID between generated results and rendered images from human scans. Additionally, we use the CLIP Score to quantify the semantic alignment between the text description and the renderings. Tab. 2 presents average scores across 32 prompts. The results of user studies, FID, and CLIP demonstrate that our Gen-Schnell and Gen-HRes outperforms SOTA feed-forward text-to-3D and SDS-based text-to3D avatar methods, respectively. Our method achieves the highest overall result quality and the most accurate alignment with the prompts semantics. More importantly, our Gen-HRes can generate 3D avatars with at least 8 times less computational time than high-resolution baselines, demonstrating that our InfiniHumanGen is the most efficient high-resolution avatar generative model. InfiniHumanData Evaluation. To assess the realism of Infini4.2.3 HumanData, we conducted user study comparing it against renderings from real human scans. The goal was to evaluate whether users could distinguish our generated avatars from those based on actual 3D scan data. Participants were presented with image pairs: one image rendered from real scan, and the other randomly sampled from InfiniHumanData. For each pair, users were asked to select the more realistic image, or choose both if they could not tell , Vol. 1, No. 1, Article . Publication date: October 2025. Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll the difference. Across all trials, real scan renderings received 746 votes, while InfiniHumanData images received 765 votes. The small difference in votes indicates that our InfiniHumanData achieves high degree of visual realism, closely matching the appearance of scans."
        },
        {
            "title": "4.3 Fine-grained Controllability",
            "content": "4.3.1 Precise Clothing Control. As shown in Fig. 6, Gen-HRes can generate avatars with high fidelity to the input clothing images. By fixing the initial Gaussian noise, we can generate the same subject wearing different garments, preserving identity across try-on results. This demonstrates strong, identity-preserving clothing controllability. 4.3.2 Precise Pose and Shape Control. As illustrated in Fig. 7, GenHRes accurately follows the body shape and pose specified by the SMPL condition, faithfully transferring to the generated avatar. 4.3.3 Precise Text-based Generation and Editing. Gen-HRes enables control over high-level human attributes, such as ethnicity, age, and gender, all through text input (see Fig. 5). More importantly, it supports fine-grained text-based editing while maintaining identity consistency. As shown in Fig. 7, we generate the same subject with different accessories, such as stockings, scarves, or sunglasses."
        },
        {
            "title": "4.4 Application",
            "content": "4.4.1 TryOn from Photographs. Our Instruct-Virtual-TryOff module demonstrates strong generalization: it can extract clean garment images directly from real-world photographs. As shown in Fig. 10, we extract clothing assets from photo captures and generate corresponding avatars with user-specified text controls. 4.4.2 Re-animation. Leveraging the underlying SMPL parametric body, our generated 3D avatars can be reanimated using SMPL motion data by barycentric interpolation of SMPL skinning weights onto the generated mesh surface. See Fig. 13 for re-animation examples. Figurine Fabrication. Gen-HRes produces high-quality, wa4.4.3 tertight 3D meshes, enabling direct 3D printing of physical figurines. The printed figurines are physically robust and can stand independently, as shown in Fig. 13, demonstrating the real-world physical compatibility [Guo et al. 2024] of generated avatars."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "We qualitatively ablate different design choices, showcasing the importance of orthographic MVD (Sec. 3.1F, Fig. 15), generating scan-like images (Sec. 3.1B, Figs. 15, 16), additional SMPL fitting (Sec. 3.1E, Fig. 14), and tolerance to inaccurate SMPL for children generation  (Fig. 14)  . Please refer to individual figures for examples."
        },
        {
            "title": "5 Limitations and Future Works",
            "content": "Although our Gen-HRes can perform high-fidelity generation, it is still slower than the end-to-end 3D generation pipeline, GenSchnell. However, Gen-Schnell cannot generate faithful details such as face because of the low-resolution (256256) of pretrained MVDream. Due to limited training resources, we cannot directly train , Vol. 1, No. 1, Article . Publication date: October 2025. higher-resolution Gen-Schnell. However, we publicly release all high-resolution (768768) InfiniHumanData with multi-modal labels. Future works can consider training high-resolution textbased 3D-GS model, which achieves fast and high-quality end-toend multi-modal avatar generation. As shown in Fig. 9, our pipeline can generate famous people by names. However, GPT-4o refuses to identify unmatched samples because of privacy issues. Future works may adopt different vision-language model to include famous names in InfiniHumanData. Moreover, our Gen-HRes adopts multi-view mesh carving to obtain textured mesh from orthographic views, which can cause texture artifacts in self-occluded parts of the avatar. Future works may consider data-driven approach for the mesh reconstruction from multi-view images."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present InfiniHuman, novel framework for realistic and highly controllable 3D avatar generation. To overcome the fundamental challenge of scarce and expensive annotated human data, we developed fully automated data generation framework that repurposes multiple pretrained foundation models. This enables the creation of InfiniHumanData, large-scale, richly annotated dataset with 111K diverse identities and comprehensive control signals. Building on this foundation, our InfiniHumanGen framework delivers rapid, high-fidelity avatar synthesis with unprecedented fine-grained control, enabling users to specify appearance, shape, pose, and clothing through intuitive multi-modal inputs. Extensive experiments demonstrate that InfiniHuman not only outperforms prior methods in visual quality and speed, but also sets new standard for precise, attribute-level controllability in 3D human generation. Importantly, our approach democratizes high-quality avatar creation via an accessible and scalable solution. To support further research and broad adoption, we will publicly release InfiniHumanData, InfiniHumanGen, and our automatic data generation pipeline, empowering the community to create unlimited, realistic, and diverse 3D humans with full user control."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is made possible by funding from the Carl Zeiss Foundation. This work is also funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 409792180 (EmmyNoether Programme, project: Real Virtual Humans) and the German Federal Ministry of Education and Research (BMBF): T칲bingen AI Center, FKZ: 01IS18039A. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Y.Xue. G. Pons-Moll is member of the Machine Learning Cluster of Excellence, EXC number 2064/1 Project number 390727645. Y.Xue is the first author and corresponding author. Y.Xue initialized the core idea, organized the project, developed the current method, conducted experiments, and wrote the draft. X.Xie contributed to the method development and draft writing. M.Kostyrko contributed to the teaser figure rendering and re-animation of GenHRes avatars. All the team members made necessary contributions to the method development and paper writing. InfiniHuman: Infinite 3D Human Creation with Precise Control 9 Fig. 8. Qualitative comparison to SOTA text-to-3D avatar generators. We compare with SDS-based avatar generation methods and mesh-based avatar generation method Chupa [Kim et al. 2023]. Our generator can follow the text very well and also achieve outstanding generation quality. Fig. 9. Generated famous people and characters by names in Gen-HRes. Please zoom in for details. Fig. 10. Generated avatars with garments in real person photos. We can extract clean garments from photos and use for TryOn. Images from Shutterstock. Fig. 11. Qualitative comparison to SOTA text-to-3D avatar approaches. Gen-HRes avoids Janus/artifacts, aligns to prompts, and is 8 faster (Tab. 2). , Vol. 1, No. 1, Article . Publication date: October 2025. 10 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 12. Qualitative appearance and geometry comparison to SOTA text-to-3D avatar approaches. Please refer to Supp. Mat. for more comparisons. Fig. 13. Re-animation (left) and Fabrication (right) of Gen-HRes avatars. Fig. 14. Misaligned joints cause bad face generation (left). Our pipeline tolerates bad SMPL estimation for children, yielding good multi-views (right). Fig. 15. Orthographic and Perspective in Multi-View Attention (left). Org. FLUX gives complex lighting, degrading multi-view generation (right). Fig. 16. Our finetuned FLUX can generate desired images from text prompt with orthographic view and uniform lighting, similar to the scan rendering. , Vol. 1, No. 1, Article . Publication date: October 2025."
        },
        {
            "title": "References",
            "content": "2023. Twindom. https://web.twindom.com/ BFL Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. 2022. HuMMan: Multi-modal 4D Human Dataset for Versatile Sensing and Modeling. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VII (Lecture Notes in Computer Science, Vol. 13667), Shai Avidan, Gabriel J. Brostow, Moustapha Ciss칠, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer, 557577. https://doi. org/10.1007/978-3-031-20071-7_33 Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K. Wong. 2023. DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models. arXiv preprint arXiv:2304.00916 (2023). https://arxiv.org/abs/2304.00916 Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K. Wong. 2024. Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 958968. Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. 2019. OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019). Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. 2015. ShapeNet: An Information-Rich 3D Model Repository. CoRR abs/1512.03012 (2015). arXiv:1512.03012 http://arxiv.org/abs/1512.03012 Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan Ren, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian, Wayne Wu, Dahua Lin, Bo Dai, and Kwan-Yee Lin. 2023. DNA-Rendering: Diverse Neural Actor Repository for High-Fidelity Humancentric Rendering. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. IEEE, 1992519936. https://doi.org/10.1109/ ICCV51070.2023.01829 Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. 2021. VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. 2023. Objaverse-XL: Universe of 10M+ 3D Objects. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ 70364304877b5e767de4e9a2a511be0c-Abstract-Datasets_and_Benchmarks.html Zijian Dong, Xu Chen, Jinlong Yang, Michael J. Black, Otmar Hilliges, and Andreas Geiger. 2023. AG3D: Learning to Generate 3D Avatars from 2D Image Collections. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. IEEE, 1487014881. https://doi.org/10.1109/ICCV51070.2023.01370 Minghao Guo, Bohan Wang, Pingchuan Ma, Tianyuan Zhang, Crystal Elaine Owens, Chuang Gan, Josh Tenenbaum, Kaiming He, and Wojciech Matusik. 2024. Physically Compatible 3D Object Modeling from Single Image. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/ hash/d7af02c8a8e26608199c087f50a21d37-Abstract-Conference.html Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and HaeGon Jeon. 2023b. High-fidelity 3D Human Digitization from Single 2K Resolution Images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2023). Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee K. Wong. 2023a. HeadSculpt: Crafting 3D Head Avatars with Text. arXiv preprint arXiv:2306.03038 (2023). https://arxiv.org/abs/2306.03038 Hsuan-I Ho, Lixin Xue, Jie Song, and Otmar Hilliges. 2023. Learning Locally Editable Virtual Humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 2102421035. https: //doi.org/10.1109/CVPR52729.2023.02014 Fangzhou Hong, Zhaoxi Chen, Yushi LAN, Liang Pan, and Ziwei Liu. 2023. EVA3D: Compositional 3D Human Generation from 2D Image Collections. In International Conference on Learning Representations. https://openreview.net/forum?id=g7U9jD_ 2CUr Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. 2022. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. In ACM SIGGRAPH Conference Proceedings. https://arxiv.org/abs/2205.08535 Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, InfiniHuman: Infinite 3D Human Creation with Precise Control 11 Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id= nZeVKeeFYf9 Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, and Qing Wang. 2024. Humannorm: Learning normal diffusion model for high-quality and realistic 3d human generation. Mustafa Isik, Martin R칲nz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nie릁er. 2023. HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. ACM Trans. Graph. 42, 4 (2023), 160:1 160:12. https://doi.org/10.1145/ Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, and Aliaksandr Siarohin. 2024. SPAD : Spatially Aware Multiview Diffusers. arXiv:2402.05235 [cs.CV] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk칲hler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph. 42, 4 (2023), 139:1139:14. https://doi.org/10.1145/3592433 Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. 2024. Sapiens: Foundation for Human Vision Models. arXiv preprint arXiv:2408.12569 (2024). Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, and Hanbyul Joo. 2023. Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 1596515976. Youwang Kim, Ji-Yeon Kim, and Tae-Hyun Oh. 2022. CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. In European Conference on Computer Vision (ECCV). https://arxiv.org/abs/2206.04382 Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchisescu. 2023. DreamHuman: Animatable 3D Avatars from Text. arXiv preprint arXiv:2306.09329 (2023). https://arxiv.org/abs/2306.09329 Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, Ping Tan, Wenping Wang, Qifeng Liu, and Yike Guo. 2024a. Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/ 65a723bf7d8dad838c09178270d30e80-Abstract-Conference.html Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, and Yike Guo. 2024b. PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion. CoRR abs/2409.10141 (2024). https://doi.org/10.48550/ARXIV.2409.10141 arXiv:2409.10141 Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, and Michael J. Black. 2023. TADA! Text to Animatable Digital Avatars. arXiv preprint arXiv:2308.10899 (2023). https://arxiv.org/abs/2308.10899 Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang, Yangyi Huang, Justus Thies, and Michael J. Black. 2024. TADA! Text to Animatable Digital Avatars. In International Conference on 3D Vision (3DV). Tingting Liao, Yujian Zheng, Yuliang Xiu, Adilbek Karmanov, Liwen Hu, Leyang Jin, and Hao Li. 2025. SOAP: Style-Omniscient Animatable Portraits. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers (SIGGRAPH Conference Papers 25). Association for Computing Machinery, New York, NY, USA, Article 28, 11 pages. https://doi.org/10.1145/ 3721238.3730691 Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, and Ziwei Liu. 2024. HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 66466657. Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. 2022. Dress Code: High-Resolution Multi-Category Virtual Try-On. In Proceedings of the European Conference on Computer Vision. OpenAI. 2024. GPT-4o System Card. CoRR abs/2410.21276 (2024). https://doi.org/10. 48550/ARXIV.2410.21276 arXiv:2410.21276 Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. Computer Vision Foundation / IEEE, 90549063. https://doi.org/10.1109/CVPR46437. 2021.00894 Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https: //openreview.net/forum?id=FjNys5c7VyY Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In Proceedings of the IEEE/CVF International Conference on , Vol. 1, No. 1, Article . Publication date: October 2025. Avatar Creation from Text and Pose. Proceedings of the AAAI Conference on Artificial Intelligence 38, 7 (Mar. 2024), 71247132. https://doi.org/10.1609/aaai.v38i7.28540 Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min Zheng. 2023. AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose. arXiv preprint arXiv:2308.03610 (2023). https://arxiv.org/abs/2308.03610 Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. 2021. PaMIR: Parametric ModelConditioned Implicit Representation for Image-Based Human Reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021), 11. https://doi. org/10.1109/TPAMI.2021.3050505 Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, and Wei Liu. 2025. IDOL: Instant Photorealistic 3D Human Creation from Single Image. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). https://arxiv.org/abs/2412.14963 12 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Computer Vision (ICCV). 23042314. https://arxiv.org/abs/1905. Istv치n S치r치ndi and Gerard Pons-Moll. 2024. Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/ fd23a1f3bc89e042d70960b466dc20e8-Abstract-Conference.html Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. 2024. MVDream: Multi-view Diffusion for 3D Generation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=FUgrjq2pbB Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. 2024. OminiControl: Minimal and Universal Control for Diffusion Transformer. CoRR abs/2411.15098 (2024). https://doi.org/10.48550/ARXIV.2411.15098 arXiv:2411.15098 Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. 2025. OminiControl2: Efficient Conditioning for Diffusion Transformers. CoRR abs/2503.08280 (2025). https://doi.org/10.48550/ARXIV.2503.08280 arXiv:2503.08280 Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. 2024. LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation. arXiv preprint arXiv:2402.05054 (2024). Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Gerard Pons-Moll. 2020. SIZER: Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III (Lecture Notes in Computer Science, Vol. 12348), Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer, 118. https://doi.org/10.1007/978-3-030-58580-8_1 Jionghao Wang, Yuan Liu, Zhiyang Dou, Zhengming Yu, Yongqing Liang, Cheng Lin, Xin Li, Wenping Wang, Rong Xie, and Li Song. 2024. Disentangled Clothed Avatar Generation from Text Descriptions. arXiv preprint arXiv:2312.05295 (2024). https: //arxiv.org/abs/2312.05295 Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. 2024. Structured 3D Latents for Scalable and Versatile 3D Generation. arXiv preprint arXiv:2412.01506 (2024). Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, and Xiaoguang Han. 2024. MVHumanNet: Large-Scale Dataset of Multi-View Daily Dressing Human Captures. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 1980119811. https://doi.org/10.1109/CVPR52733.2024.01872 Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. 2023. ECON: Explicit Clothed Humans Optimized via Normal Integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 512 523. https://openaccess.thecvf.com/content/CVPR2023/html/Xiu_ECON_Explicit_ Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.html Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. 2022. ICON: Implicit Clothed Humans Obtained from Normals. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 13296 13306. https://openaccess.thecvf.com/content/CVPR2022/html/Xiu_ICON_Implicit_ Clothed_Humans_Obtained_From_Normals_CVPR_2022_paper.html In Proceedings of Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, and Michael Black. 2024. PuzzleAvatar: Assembling 3D Avatars from Personal Albums. ACM Transactions on Graphics (TOG) (2024). In Advances in Neural Yuxuan Xue, Xianghui Xie, Riccardo Marin, and Gerard Pons-Moll. 2024. Human3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent Diffusion Models. Information Processing Systems 38: Annual Information Processing Systems 2024, NeurIPS 2024, Conference on Neural Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/ b46aaf640bc8659e65a1a573971ba5a2-Abstract-Conference.html Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. 2021. Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer RGBD Sensors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021). Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park. 2020. HUMBI: Large Multiview Dataset of Human Body Expressions. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. Computer Vision Foundation / IEEE, 29872997. https://doi.org/10.1109/CVPR42600.2020.00306 Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan Kautz, and Umar Iqbal. 2024. GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). https://arxiv.org/abs/2312.11461 Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Daniel Du, and Min Zheng. 2024. AvatarVerse: High-Quality & Stable 3D , Vol. 1, No. 1, Article . Publication date: October 2025. Additional Visualitation InfiniHumanGen A.1 Fig. 17 showcases the precise control capabilities of InfiniHumanGen. Our pipeline supports wide variety of fine-grained controls, including changing identity, clothing, body shape, pose, and accessories via text, SMPL shape / pose, and clothing image inputs. As shown, we can manipulate specific attributes independently, such as varying clothing while maintaining identity, or editing pose and body shape with consistent appearance. A."
        },
        {
            "title": "InfiniHumanData",
            "content": "In Fig. 18, we present additional examples of InfiniHumanData subjects and their multi-modal annotations, including clothing assets and RGB multi-view images of head / body views. The figure highlights the diversity and annotation richness of our dataset across multiple attributes. The visualization of InfiniHumanData demonstrates our orthographic multi-view diffusion model generates viewconsistent images for head views and full-body views. A.3 InfiniHumanGen vs. SOTA We present additional qualitative comparisons between InfiniHumanGen and state-of-the-art (SOTA) text-to-3D avatar generation methods. Fig. 19 compares our results against prominent SDS-based approaches [Cao et al. 2023; Huang et al. 2024; Liao et al. 2023; Liu et al. 2024; Zhang et al. 2023], while Fig. 20 shows results against recent feed-forward pipelines [Shi et al. 2024; Tang et al. 2024]. Compared to SDS-based methods, such as DreamFusion and its variants, InfiniHumanGen consistently produces avatars with significantly higher visual fidelity, more consistent geometry, and superior alignment with the input prompt. SDS-based methods often suffer from slow convergence and visual artifacts such as over-smoothed textures or structural inaccuracies, whereas our pipeline delivers sharper appearance details and well-formed geometry. We additionally compare with Chupa [Kim et al. 2023], human mesh generation approach controllable via text prompts and SMPL pose. As shown in Fig., although Chupa accurately follows the pose, it fails to generalize to complex text prompts. Moreover, it does not support the use of specific clothing images as conditioning input. In contrast, our InfiniHuman-Gen can seamlessly handle complex text prompts and the additional clothing image. When compared to feed-forward approaches, InfiniHumanGen demonstrates stronger text-following capability, more accurate pose and clothing reproduction, and improved overall realism. Competing methods frequently show issues such as geometry distortion, incorrect clothing assignment, or poor text-following ability for generating head avatars. In contrast, our pipeline generates view-consistent avatars that faithfully reflect the user-provided conditions. For all examples, InfiniHumanGen offers rapid generation speed, fine-grained attribute control, and robust multi-modal conditioning, setting new standard for controllable 3D human avatar synthesis. InfiniHuman: Infinite 3D Human Creation with Precise Control 13 IDOL represents significant step forward in generating multi-view human images from single inputs using video diffusion models, we observe that its generated results often exhibit noticeable view inconsistencies and temporal artifacts. This is primarily due to the neighbor-only attention mechanism used in video diffusion, which can lead to unnatural transitions and inconsistent appearance across views. In contrast, InfiniHumanData leverages multi-view diffusion and carefully designed generation pipeline to ensure high-resolution, view-consistent, and photorealistic renderings for each identity. Our approach produces multi-view images that maintain consistent shape, texture, and lighting, closely resembling the outputs of true 3D scan renderings. We present several side-by-side examples comparing multi-view images from both datasets in Fig. 21. As shown in the figure, InfiniHumanData consistently produces sharper details, smoother transitions, and significantly improved cross-view coherence compared to IDOL. The comparison demonstrates our multi-view diffusion model achieves more view-consistent results than video-diffusion models in IDOL. These results reinforce the value of our method for downstream tasks that require highly realistic and consistent human data, such as 3D reconstruction, animation, and virtual try-on. InfiniHumanData Implementation Details B.1 Scan Captioning We generate scan captions using two-step process with GPT4o [OpenAI 2024]. First, structured prompt (see Fig. 22) guides the model to extract key attributesgender, age, ethnicity, pose, clothing, and body partsdirectly from scan images, minimizing hallucination and irrelevant details. This approach ensures each caption concisely summarizes visual appearance and clothing for downstream tasks. The same prompt is used for all samples; example outputs and the full prompt are in Fig. 22. B.2 Multi-granularity Text Generation To support flexible conditioning, we expand each detailed caption with multiple text granularities. Using summarization prompt  (Fig. 23)  , GPT-4o generates sequence of increasingly concise captions, from detailed (about 20 words) to minimal (5 words). This enables models to learn from both fine-grained and coarse cues and improves robustness to user input. All examples use the prompt in Fig. 23. B.3 Negative Samples Rejection To ensure garment accuracy, we generate several clothing candidates per subject and use GPT-4o with comparison prompt  (Fig. 24)  to select the best match. The model compares candidates based on color, texture, pattern, length, and fine details (e.g., pockets, zippers), justifying its choice. If none match, the model returns No. Full prompt and an example are shown in Fig. 24. A.4 InfiniHumanData vs. SOTA B.4 Monocular Body Fitting To further highlight the advantages of InfiniHumanData over previous large-scale human datasets, we provide visual qualitative comparison with the recent IDOL [Zhuang et al. 2025] dataset. While As stated in Sec. 3.1 of the main paper, we use two-stage approach to obtain the SMPL pose and shape parameters for InfiniHumanData. Although NLF [S치r치ndi and Pons-Moll 2024] can estimate accurate , Vol. 1, No. 1, Article . Publication date: October 2025. 14 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 17. Precise control capability in InfiniHumanGen. , Vol. 1, No. 1, Article . Publication date: October 2025. InfiniHuman: Infinite 3D Human Creation with Precise Control 15 Fig. 18. Additional Examples in InfiniHumanData. Our orthographic multi-view diffusion model generates high-quality view-consistent images of head views and full-body view. , Vol. 1, No. 1, Article . Publication date: October 2025. Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 19. Qualitative appearance and geometry comparison to SDS-based text-to-3D avatar approaches. , Vol. 1, No. 1, Article . Publication date: October 2025. InfiniHuman: Infinite 3D Human Creation with Precise Control Fig. 20. Qualitative appearance and geometry comparison to feed-forward text-to-3D avatar approaches. , Vol. 1, No. 1, Article . Publication date: October 2025. 18 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 21. Qualitative comparison with IDOL Dataset. Our InfiniHumanData achieves better visual realism and multi-view consistency. , Vol. 1, No. 1, Article . Publication date: October 2025. InfiniHuman: Infinite 3D Human Creation with Precise Control 19 Fig. 22. Detailed scan captioning prompt and example output. , Vol. 1, No. 1, Article . Publication date: October 2025. Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 23. Multi-granularity text summarization prompt and generated examples. , Vol. 1, No. 1, Article . Publication date: October 2025. InfiniHuman: Infinite 3D Human Creation with Precise Control Fig. 24. Garment selection prompt for negative samples rejection. , Vol. 1, No. 1, Article . Publication date: October 2025. 22 Yuxuan Xue, Xianghui Xie, Margaret Kostyrko, and Gerard Pons-Moll Fig. 25. Pose refinement using reprojection loss w.r.t. OpenPose 2D joints. As stated in Sec.3.1 in main paper, we use OpenPose 2D joints to further refine body pose parameters using reprojection loss. This additional operation eliminates the misalignment between image and SMPL body, and enhances the pixel-level accuracy in face and hand region. Fig. 26. Comparison between generated multi-view images in our model and PSHuman. Both models generate high-resolution full-body images. However, our model generates multi-view high-resolution head-centric images, while PSHuman only generates single blurry head-view image. shape parameter, the estimated pose still produces misalignment of the SMPL body and the images. Hence, we use estimated 2D joints from OpenPose [Cao et al. 2019] to refine the SMPL pose parameters with the reprojection error. As illustrated in Fig. 25, the additional 2D joints-based refinement successfully resolves the issue and ensures the pixel-level alignment of the body parameters and the images, especially in face and hand region. This alignment is crucial in the SMPL-guided multi-view generation, which can be found in Fig. 13 of the main paper. InfiniHumanGen Implementation Details C.1 Gen-Schnell. Fine-tuning MVDream. Since MVDream is originally designed to condition only on text and does not support additional modalities such as SMPL normal maps or clothing images, we slightly adapt its structure while preserving the power of the pretrained model. Specifically, we expand the input convolutional layer of the UNet denoiser from 4 channels to 12 channels, allowing us to concatenate the VAE-encoded SMPL normal map and clothing image with the initial VAE Gaussian noise as input. , Vol. 1, No. 1, Article . Publication date: October 2025. To ensure that the introduction of these new channels does not disrupt the pretrained behavior, we initialize the weights associated with the additional channels to zero. Formally, let x0 R洧냣4洧냩 洧녥 0 R洧냣12洧냩 洧녥 the augmented denote the original input, and input. The new input convolution weights R洧냤 12洧녲 洧녲 are set as: = (cid:2)W 0(cid:3) (10) where R洧냤 4洧녲 洧녲 are the pretrained weights, and 0 represents zero-initialized weights for the new channels. As result, the output of the modified convolution at initialization is unchanged for any input where the additional channels are zero, thus strictly preserving the pretrained functionality: Conv(x 0; W) = Conv(x0; W) when 0 = [x0, 0] (11) During finetuning, the model can gradually learn to utilize the new SMPL and clothing conditions to improve generation. Training. After fine-tuning MVDream on InfiniHumanData, the resulting 2D diffusion models demonstrate strong multi-view image generation capabilities conditioned on text, SMPL normal maps, and clothing images. To leverage this for 3D reconstruction, we InfiniHuman: Infinite 3D Human Creation with Precise Control 23 maps, canny edges) or non-aligned (e.g., object images). In our design, the SMPL normal map serves as spatially aligned condition, as it conveys approximate pixel-wise spatial information about the generated avatar. Thus, we assign it dynamic positioning token of 0. For the clothing image, which provides semantic information but lacks pixel-level alignment, we treat it as non-aligned condition and assign dynamic positioning token of -48. This design enables OminiControl2 to appropriately fuse information from both modalities and condition the image generation process accordingly. For further architectural details, please refer to the original OminiControl2 paper. For the multi-modal image generator in InfiniHuman-GenHRes, we use the Flux-Dev [Black Forest Labs 2024] as the base model and train LoRA to accommodate additional multimodal cloth image and input body shape. The training resolution is set to 768 768, and the LoRA rank is set to 16. Our model is trained with batch size of 1 and gradient accumulation of 8 steps. We employ the Prodigy optimizer with safeguard warm-up and bias correction enabled, setting the weight decay to 0.01. The model is trained for 35000 steps. Our InfiniHuman-GenHRes model leverages the fine-tuned orthographic multi-view diffusion model to lift the generated single images to multi-views, similar to PSHuman [Li et al. 2024b]. However, PSHuman only generates low-resolution single head view and resized to high-resolution, while our model generates directly multi-view high-resolution head images. This design allows us to obtain more details and fidelity of the generated heads such as eyes, ears, and hair. , Vol. 1, No. 1, Article . Publication date: October 2025. Fig. 27. User study for realism assessment of InfiniHumanData. We present randomly paired InfiniHumanData subjects and scan subjects to participants. InfiniHumanData receives 765 votes over 746 votes for scan, which demonstrates the superior realism of our synthetic dataset. train the Splat Decoder separately on 6,000 high-quality human scans. For each scan, we render multi-view images and use them as supervisory signals, which significantly improves convergence and stability in the prediction of 3D Gaussian Splat representations. We adopt similar strategy for diffusing 2D multi-view images and the 3D Gaussian Splat in Human-3Difusion [Xue et al. 2024]. Apart from different conditioning of text, the main difference here is that our fine-tuned multi-view diffusion model generates orthographic images which is not directly align with perspective images rendered by 3D Gaussian splatting. Thus, we only apply the consistent reverse sampling for beginning 10 DDIM steps. It allows the 2D orthographic multi-view diffusion and 3D Gaussian diffusion synchronize closely, but the final generation is not confused by the inconsistency of the orthographic or perspective generation. Please refer to our implementation for more details. During training, we use an Adam optimizer with an initial learning rate of 5 104 and batch size of 256. The model is trained on 8 NVIDIA A100 GPUs. To further stabilize training, we apply spectral normalization to the decoder and use perceptual loss (LPIPS) in addition to 2 reconstruction loss on multi-view images. C.2 Gen-HRes. We select OminiControl2 [Tan et al. 2025] as our backbone for multi-conditional image-to-image translation, and fine-tune it on InfiniHumanData for our application. Specifically, given SMPL body normal map, clothing image, and detailed text description, our goal is to generate full-body image that faithfully reflects all three input modalities. In the original OminiControl2 framework, input image conditions are categorized as either spatially aligned (e.g., depth maps, normal"
        }
    ],
    "affiliations": [
        "University of T칲bingen, Germany",
        "University of T칲bingen, T칲bingen AI Center, Germany",
        "University of T칲bingen, T칲bingen AI Center, MPI for Informatics, SIC, Germany"
    ]
}