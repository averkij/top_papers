{
    "paper_title": "VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval",
    "authors": [
        "Dhiman Paul",
        "Md Rizwan Parvez",
        "Nabeel Mohammed",
        "Shafin Rahman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights ."
        },
        {
            "title": "Start",
            "content": "VIDEOLIGHTS: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval Dhiman Paul , Md Rizwan Parvez , Nabeel Mohammed and Shafin Rahman 1 4 2 0 2 2 ] . [ 1 8 5 5 1 0 . 2 1 4 2 : r AbstractVideo Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive leverage error penalization and improved learning, and (v) LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-ofthe-art performance. Codes and models are available at: https: //github.com/dpaul06/VideoLights. Index Termsvideo highlight detection, moment retrieval, video grounding, feature refinement, I. INTRODUCTION HE surge in digital devices, platforms, and internet usage has led to abundant online video content [1], [2]. However, navigating through such vast content poses an exceedingly difficult challenge for users, impeding their ability to pinpoint specific points of interest within recordings [1], [3]. Consequently, Video Highlight Detection (HD; [4][8]) and Moment Retrieval (MR; [3], [9][15]), which evaluate saliency scores of video clips and automatically identify significant moments (i.e. clips with the highest saliency scores) for user queries, respectively, have become indispensable tools in video analysisstreamlining content management, recommendation, creation, editing, and event detection processes. Given their Manuscript received December 1, 2024. (Corresponding author: Shafin Rahman) Dhiman Paul, Nabeel Mohammed, and Shafin Rahman are with the and Computer Engineering, North South dhiman.paul@northsouth.edu; Department of Electrical University, Dhaka, Bangladesh nabeel.mohammed@northsouth.edu; shafin.rahman@northsouth.edu ). (email: Md Rizwan Parvez is with Qatar Computing Research Institute (QCRI), Qatar (email: mparvez@hbku.edu.qa). Equal contribution. shared objective of ranking/localizing the relevant video clips based on user queries and the commonality in their multimodal models and data properties, recent studies using transfer models have begun to jointly model Video Highlight Detection and Moment Retrieval (HD/MR) [16][23]. Text and video feature embeddings derived from visionlanguage models (VLMs), such as CLIP [24] and BLIP [25], facilitate nuanced and fundamental understanding of text and video modalities. By utilizing pre-trained embeddings, these models have shown significant effectiveness in tackling complex challenges related to semantic alignment and multimodal reasoning, thereby improving the integration and interpretability of multimodal data. For joint MR-HD tasks, most studies [16][22] primarily employ textual and visual features from CLIP pre-trained on Kinetics 400 [26]. However, since CLIP is mainly trained on static images and text, it lacks the temporal information critical for video understanding. To address this limitation, additional visual embeddings from SlowFast [27], which incorporate both visual and temporal aspects, are integrated [16]. While CLIP learns joint representations across text and images, Large Vision-Language Models (LVLMs) like GPT-4V [28], LLaVA [29], or BLIP-2 [30] have emerged as more powerful tools with complex reasoning capabilities and proven succcess across various tasks in vision and language domains [31]. Therefore, at the very root of our study for joint HD/MR prediction tasks, we introduce enhanced visual and textual embeddings augmented from all of CLIP, SlowFast and the LVLM BLIP-2 [30], which have been predominantly underexplored in HD/MR literature. We verify its effectiveness over existing embeddings such as CLIP (in Section IV-A and IV-B). Nonetheless, joint HD/MR prediction is challenging task requiring deeper understanding of both text and video modalities, as well as their cross-modal and cross-task synergies. Despite their coexistence and correlations, we observe that most approaches undermine either the cross-task (i.e., HD vs. MR) or cross-modal (i.e., text vs. video) dynamics when modeling them jointly, thereby limiting potential gains and robustness. For example, the early work Moment-DETR [16], based on an encoder-decoder transformer model, employs the concatenation of pre-trained vision-language model features for video and text representation. Follow-up works like UMT [17] augment audio inputs in the encoder and text in the decoder while using isolated text and video features. QD-DETR [19] develops query-dependent video representation module that aligns text with video. UniVTG [20] further presents multi2 Fig. 1. Relevance heat map illustrating multimodal alignment dynamics across video understanding models. Color intensity (blue to red) quantifies queryvideo clip correspondence, with the green line indicating ground truth clip-wise saliency. Comparative visualization reveals VideoLightss progressive refinement of query-clip relevance through projection, feature refinement, and bi-directional cross-attention stages, in contrast to Moment-DETR [16] and QD-DETR [19]s limited multimodal interaction. task learning approach using unified finetuning and pretraining methods. These methods cascade two isolated task heads after the shared layers without addressing cross-task interactions. On the other hand, the recently proposed TaskWeave [18] and TR-DETR [22] models address (bi-directional) cross-task relations by first computing HD and MR independently, then having them cooperate (HD into MR and vice versa) to recompute the results, but they still rely on no or unidirectional (cross-modal) attention from text to video, respectively. To address these challenges, in this paper, we propose new HD / MR joint prediction framework VideoLights that enables learning from cross-modal and cross-task interactions. Moreover, in many instances, text queries tend to be more concise, whereas video frames often contain noisy and irrelevant information. Consequently, directly applying attention to the entire video does not effectively differentiate between relevant and irrelevant clips. TR-DETR addresses this challenge by enhancing visual tokens in relation to query tokens. To tackle this issue, we have developed Feature Refinement and Alignment (FRA) Module, which adeptly refines visual features in accordance with textual features and aligns them at local and global levels. Fig. 1 visually shows that only self-attention or cross-attention cannot effectively align the query-video."
        },
        {
            "title": "At the core of our framework are the following modules",
            "content": "and principles: 1) Feature Refinement and Alignment (FRA) Module: Implements CNN-based intramodal and intermodal feature interaction and refinement, with intermodal alignment loss for text-video correspondence. 2) Bi-Directional Cross-Modal Fusion (Bi-CMF) Network: Employs multi-stage hierarchical process for bidirectional text-video attention, yielding strongly coupled query-aware clip representation. 3) Unidirectional Joint-Task Feedback Mechanism (UniJFM): Enhances task correlation through task-specific and task-coupled losses, utilizing cosine similarity on feature vectors from HD and MR, improving cross-task learning efficiency. 4) Adaptive Error Correction: Incorporates hard positive and hard negative losses to adaptively penalize model errors in clip saliency prediction, fostering improved learning. 5) Intelligent Model Pre-training: Capitalizing on the image-to-text generation capabilities in Large VisionLanguage Models (LVLMs), specifically BLIP-2, utilizes synthetic data generated from video corpora and language-image models to create high-quality paired text queries for model pre-training. We perform comprehensive evaluations on widely rec- [16], TVSum [32], ognized benchmarks QVHighlights and Charades-STA [9]. Results show that in both tasks, VideoLights achieves strong performance, outperforming all previous baselines by significant margin (an average of 1.4% in QVHighlights, 0.7% in TVSum, and 0.3% in Charades-STA) and achieving their new state-of-the-art results. We also provide an in-depth ablation study of our model on the QVHighlights development set, visualize the qualitative examples, and analyze the effects of different synthetic pretraining corpus and the impact of feature ensembles. We will open-source our implementation accordingly. II. RELATED WORK Moment retrieval (MR) and highlight detection (HD) are closely related tasks in video understanding. MR aims to retrieve video moments relevant to given natural language query, while HD focuses on detecting the most important or 3 Fig. 2. Overall VideoLights architecture. FRA models the video-text cross-modal correlations from projected embeddings and passes them to Bi-CMF in the encoder. trainable saliency vector predicts output saliency levels. Class and moment prediction heads predict logits and video moments, while saliency cosine similarity and task-coupled HD/MR losses together provide cross-task feedback Uni-JFM. Proposed new losses are in purple. salient moments in video. Early MR approaches include two-stage methods [3], [9], [33][36] and one-stage methods [35], [37][51]. But recent research on MR and HD has primarily advanced transformer-based architectures [52]. The detection transformer model (DETR) [53] leverages vision transformers to simplify predictions by eliminating the need for anchor generation and non-maximum suppression. Despite initial convergence delays, subsequent advances have made DETR widely applied in HD and MR. standout contribution is Moment-DETR [16], which introduced the QVHighlights dataset for concurrent HD/MR. Moment-DETR is modified iteration of the DETR model, excelling at pinpointing queryrelevant moments and their corresponding saliency scores. Another recent work, UMT [17], proposed unified architecture for processing multimodal data (video and audio) for MR and HD. However, UMT removes the moment decoder and bipartite matching from Moment-DETR, resulting in inferior MR performance. Additionally, some works have explored alternative approaches to MR and HD. For example, TVT [54] utilized additional data (subtitles) to capture relevant moments, while FVMR [55] improved inference speed for efficient MR. novel Reversed Recurrent Tuning (R²-Tuning) [56] framework leverages CLIPs multi-layer features for efficient, parameter-light video temporal grounding across diverse tasks and benchmarks. As MR and HD tasks are relevant to each other, some recent methods (TaskWeave [18], TR-DETR [22]) explored the cross-task dependence effectively. However, in this paper, we develop joint prediction HD/MR model focusing on cross-modal and cross-task interplays. We open Cross-modal learning relies on integrating and synchronizing information from different modalities, such as visual images and textual data. Several models, such as TERAN [57], HGSPN [58], and AVS [59], [60] have explored this topic. Unloc [61], recent effort, uses cross-modal fusion of CLIP [24] text and video tokens to create feature pyramid employing CNN prediction layers for Moment Retrieval, Temporal Localization, and Action Segmentation in single-stage model. However, they are mainly limited to different text-tovideo attention. Differently, we have used custom crossmodal fusion module to find the bi-directional interrelation between text query and video clips and leverage this in the decoder with additional cross-task supervision. Several recent studies have explored the use of weakly supervised pretraining approaches with data from various modalities, demonstrating improvements in model performance [16], [17], [20], [61], [62]. Some of them have utilized Automatic Speech Recognition (ASR) captions as query text to [16], [17], [62]. Similarly to us, [61] have employed pre-training strategy that involves initially training their CLIP backend with the Kinetics-700 dataset [63] before fine-tuning the model for downstream tasks. UniVTG [20], on the contrary, collected large training corpus combining the Ego4D dataset [64] and VideoCC [65] while ours shows more robustness without such data diversity. In text-only context, [66] shows that combining different encoders can facilitate enhanced supervision. III. PROPOSED VI OLI S MODEL We present VideoLights, our joint prediction HD/MR model that enables learning from cross-modal (text vs video) and cross-task (HD vs MR) interplays. VideoLights features unique composite of Bi-Directional Cross-Modal Fusion Network, Unidirectional Join-Task Feedback module, advanced appetite loss functions, and intelligent model training. VideoLights pipleline is depecited in Fig. 2. A. Model Overview Highlight Detection (HD) and Moment Retrieval (MR) aim to estimate the saliency of video clips and identify significant 4 (a) is the input video, (b) and (c) are correspondence maps of query and video tokens using linear and convolution layers, respectively, which show Fig. 3. that queries are more aligned for the convolution layer, video, and text than linear projection layers. (d) The effect of the Feature Refinement module that effectively aligns video and text tokens that match ground truth saliency levels (green line) in each heat map saliency level is shown with green line plot. moments for given text query. Given video of clips, we define the video clips as RL3W , where and denote the width and height of the video, and 3 represents the number of color channels. The feature representation of the video is denoted as RLdv , where dv is the feature dimension extracted by frozen video encoder. Given text query of tokens, the representation of the text is denoted as RN dt, where dt is the feature dimension extracted by frozen text encoder. With these representations and given the video and the text, our goal is twofold: for Moment Retrieval (MR), we aim to determine all the moments R2m, where each moment consists of central coordinate mc and width mσ, identifying such moments within the video. For Highlight Detection (HD), we aim to rank the saliency scores RL for each clip in the video to detect highlights. Embeddings: We compute the initial feature sets and from multiple different VLPs as follows: = clip(Q) blip(Q) = clip(F ) slowfast(F ) blip(F ) Here operator denotes concatenation of the features and clip, blip, and slowfast refer to frozen CLIP [24], BLIP-2 [30], and Slow-Fast models [27] respectively. Projection and Alignment: When combining and for further processing, their differing hidden dimensions can make merging challenging. We address this issue by aligning the feature dimensionalities of the video and text representations using Feed Forward Network (FFCNN) consisting of convolution layers. After this step, RLdv becomes RLd and RN dt becomes RN d, where is the dimension of the hidden layer. = relu(FFCNN(V )), = relu(FFCNN(T )) After this, Both video and text representations are passed to the video-query refinement module to learn query-attended video representations and highlight relevant video tokens. Details are discussed in Section III-B. Encoder with Cross-Modal Interaction Refined video tokens and query tokens are sent to our cross-modal interaction module Bi-CMF (discussed in Section III-C). This module fuses video and text features to learn their inter-relevance and learns strongly coupled query-injected video representation. Then, in the multilayer encoder, self-attention is applied to the output of the Bi-CMF. Then, the output is then used to predict the saliency level of each clip. Decoder with Cross-Task Dynamics Furthermore, the fused representation is sent to decoder module following the work [19]. The output of this module is used in the class prediction head and the localization prediction head to predict the foreground-background class and moments in the video. Negative relations between irrelevant video-text queries are used to fine-tune the response, similar to what was done in [19]. We propose new learning module, the unidirectional cross-task feedback network Uni-JFM. Uni-JFM takes one task HD as reference and computes its additional losses: task-specific (from HD) and cross-task (from MR) losses discussed in Section III-E. Adaptive Learning and Loss Functions VideoLights utilizes different losses for moment retrieval and highlight identification. We utilize L1, gIoU [67] LgIoU (m, m), and cross-entropy Lcls objectives to perform moment retrieval like [16]. Additionally, we have used margin ranking loss Lrank, rank contrastive loss Lcont like [19], and entropy loss for highlight identification. Then total loss is the summation of highlight loss and moment loss. For alignment, from FRA, we used symmetric alignment loss Lsym. For saliency prediction (i.e., in HD), we have introduced two adaptive hard negative loss Lhardneg , hard positive loss Lhardpos (discussed in Section III-D). These losses penalize errors in saliency prediction that persist with iterations. In summary, the formulation of moment loss Lmr can be 5 that are semantically aligned with both sentence-level and word-level features. Figure 3 illustrates the differences between standard linear projection and convolutional projection and refinement network, highlighting the enhanced focus on relevant video tokens, which results in improved similarity scores that align with the ground truth saliency score. This refinement process is represented as: VQ =V , VS =V ST , =conv(V VQ VS Sv) = pool(T ), Sv = 11V 1, where . means matrix multiplication. To ensure alignment at the query-text level, we calculate the contrastive alignment loss between query tokens and projected query spans following [16]. This loss encourages higher similarity scores between the projected query spans and their corresponding text embeddings. It is defined as: Lqt align ="
        },
        {
            "title": "1\nB",
            "content": "(cid:32) (cid:88) b=1 (cid:80) logitsbm pos num posb (cid:33) (4) + log (cid:88) exp(logitsbm) (cid:80) n(qbmtbn) where logitsbm = is an indicator for positive matches, and τ is the temperature and is batch size. τ , pos Fig. 4. Bi-CMF learns query-oriented video via text2video, video2text, then text2video attentions. In this process, dropout and normalization are applied after each step, and activation is applied at the last stage. expressed as follows: Lmr = λL1m + λgIoU LgIoU (m, m) + λclsLcls (1) As the additional Lhardneg , Lhardpos as well as LU niJF losses are computed in saliency prediction, we denote the overall saliency loss as follows: Lhl =λrankLrank + λcontLcont + Lhardneg + Lhardpos + LU niJF (2) Additionally, for assisting FRA, we have introduced alignment loss Lalign discussed in Section III-B. Therefore, the final total loss is: Ltotal = λsalLhl + Lmr + λalLalign (3) To align video clips with corresponding sentence-level text embeddings, we calculate the video-text alignment loss by minimizing the cosine similarity error between their saliency scores. where the hyperparameters λsal, λal are used to achieve balance between these losses. In the following we discuss the Bi-CMF and Uni-JFM modules, Adpative Lhardneg , Lhardpos losses, and our pretraing procedure. B. Feature Refinement and Alignment Network: FRA Text queries are typically concise and informative, whereas video clips often contain substantial noise and irrelevant information. When self-attention or cross-attention mechanisms are applied directly to video tokens, all tokens are weighted equally, which can result in insufficient emphasis on the truly relevant tokens. To address this limitation, we propose the Feature Refinement and Alignment Network (FRA). FRA facilitates both local (clip or word level) and global (video or sentence level) alignment between video and query tokens through two-stage process. In the first stage, convolutional projection layer captures local representations, aligning video and text features while also adjusting token dimensions. In the second stage, the feature refinement layer promotes global alignment by computing an adjusted correspondence map, extracting sentencelevel features, generating similarity matrix, and aggregating the results. This refinement process highlights video tokens ˆsb = t.V tV (5) (cid:80)N where = 1 t=1 tt. Here is the pooled sentence-level text embedding, is the clip-level video embedding, and ˆsb is the calculated similarity score. Lvt align = (cid:18)"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) b=1 norm(sb).norm(ˆsb) norm(sb)norm(ˆsb) (cid:19) (6) where sb is the ground truth saliency score. The total loss is defined as: Lalign = Lqt align + Lvt align (7) C. Bi-Directional Cross-Modal Fusion Network: Bi-CMF To learn strongly coupled, query-oriented video representation, we introduce our Bi-Directional Cross-Modal Fusion Network, Bi-CMF. It features three multi-head attention layers for cross-attention. Initially, cross-attention layer uses projected video features as queries, while text data with positional embedding serve as keys and values, identifying video tokens conditioned by textual tokens. Similarly, another cross-attention layer is utilized to discern projected textual tokens (query) features conditioned by video tokens fused with positional embedding (keys and values), enabling the identification of textual features pertinent to the video. Subsequently, conditioned video tokens are used as queries, while conditioned textual tokens serve as keys and values in the final cross-attention layer, yielding fused contextual to the information that emphasizes video tokens relevant query. Further refinement is achieved through self-attention mechanism applied to this fused context, allowing for the extraction of more nuanced video context. VT = attn(V , , ), TV = attn(T , , ), Vattn = attn(V , , ) Residual connections [68], layer normalization [69], and dropout [70] mechanisms are implemented at each stage to enhance the robustness of the model, and encodings of learnable positions are incorporated into the input of each attention layer. Bi-CMF is depicted in Fig. 4. D. Adaptive Loss Functions We aim to enhance learning by identifying and rectifying persistent model errors. To achieve this, we design novel adaptive loss functions, specifically targeting hard positives and hard negatives. For the hard negative loss, we minimize the number of predictions in the negative regions where there are no relevant clips. Given the saliency score Si and the ground truth saliency score Si for non-relevant clips Vneg, we define the loss, Lhardneg = WjΣiVneg abs(Si Si) (8) where Wj is function of the jth epoch that penalizes more with higher number of epochs. As in general, Si for Vneg is zero, the loss can be defined as: Lhardneg = WjΣiVneg abs( Si) (9) Algorithm 1 Synthetic Data Generation Process Require: Input video with duration Ensure: Synthetic dataset Dsynthetic 1: Divide the video into = /10 non-overlapping intervals {I1, I2, . . . , In}, where each interval Ii corresponds to 10-second segment of V. 2: for each interval Ii do 3: Select representative frame fi from Ii (e.g., the middle frame or one sampled by heuristic). Use the BLIP-2 model MBLIP to generate caption ci = MBLIP(fi) describing the content of fi. 4: 5: end for 6: for each interval Ii do 7: 8: for each frame fij Ii do Compute the cosine similarity Sim(ci, fij) between the caption ci and the frame fij using their feature representations ϕ(ci) and ϕ(fij). 9: 10: end for Use si = Sim(ci, fij) for each video frame fij as frame-wise highlight scores for interval Ii. 11: end for 12: Construct synthetic dataset Dsynthetic = {(ci, Ii, si) [1, n]}, where ci is the generated caption Ii is the corresponding interval and si is the saliency score. 13: Use Dsynthetic to train the target model for highlight detection or related tasks. saliency and this calculated saliency Smr. This similarity score is used as the loss function Ltc , where Ltc = 1 Smr.S SmrS The corresponding total loss for the module becomes, LU niJF = Lts + Ltc (12) (13) For hard positive cases, we use Mean Square Error, and similarly, we define the loss as: F. Pretraining Lhardneg = WjΣiVpos SE(Si, Si) (10) E. Unidirection Joint-Task Feedback Module (Uni-JFM) To leverage the cross-task synergies while jointly predicting HD/MR, we devise unidirectional joint-task feedback mechanism that is composite of task-specific and taskcoupled loss. We take HD as reference task and compute its task-specific loss Lts. To do so, we calculate the saliency cosine similarity loss from the predicted saliency level. Here for saliency score and ground truth saliency score the saliency cosine similarity loss Lts can be defined as: Lts = 1 S.S SS (11) We propose novel multi-step methodology to enhance attention-based networks performance by addressing limitations in ASR caption-based weakly supervised training [16], [62]. ASR may not always align with or describe the content of the video of that timeframe. Our approach segments videos into 10-second intervals, generates descriptive captions using the BLIP model for representative frames, and creates synthetic data pairs from QVHighlights and Charades-STA datasets. Saliency scores are calculated based on frame-query similarity, and the resulting caption-query pairs are used for model training. While this process may generate noisy pretrain data, the subsequent finetuning helps filter out irrelevant information, leading to improved generalization [71]. Detailed data statistics and steps are provided in Table and Algorithm 1. Next, for the task-coupled loss Ltc, first, we use the feature vectors for MR, to calculate saliency scores Smr following [22] using GRU unit. Then, difthe MR2HD technique of ferently, we calculate the similarity between the ground truth IV. EXPERIMENTS Datasets: We evaluate VideoLights using three widely recognized benchmarks to ensure comprehensive and rigorous assessment. First, the QVHighlights dataset [16] uniquely TABLE COMPARISON OF DATASETS USED IN THIS STUDY. Dataset Domain Annotations Videos Task Used in pt Synthetic data QVHighlights Charades-STA TVSum Vlog / News Activity Web 10.3K 16.1K 12.5K MR, HD 6.7K 50 MR HD 187682 23193 7 TABLE II RESULTS ON QVHIGHLIGHTS TEST SPLIT. REPRESENTS THE USE OF AUDIO MODALITY. HERE, BOLD REPRESENTS THE BEST RESULT, AND UNDERLINE REPRESENTS THE 2ND BEST RESULT. Method MR R1 mAP @0.5 @0.7 @0.5 @0.75 Moment-DETR [16] UMT [17] MH-DETR [72] EaTR [21] QD-DETR [19] UVCOM [62] TR-DETR [22] UniVTG [20] VideoLights Moment-DETR(pt) [16] UMT(pt) [17] QD-DETR (pt) [19] UVCOM(pt) [62] UniVTG(pt) [20] VideoLights-pt VideoLights-B VideoLights-B-pt 52.89 56.23 60.05 61.36 62.40 63.55 64.66 58.86 63.36 59.78 60.83 64.10 64.53 65.43 68.48 68.29 70.36 33.02 41.18 42.48 45.79 44.98 47.47 48.96 40.86 48.70 40.33 43.26 46.10 48.31 50.06 52. 52.79 55.25 54.82 53.83 60.75 61.86 63.17 63.37 63.98 57.60 63.81 60.51 57.33 64.30 64.78 64.06 67.31 67.58 69.53 29.4 37.01 38.13 41.91 42.05 42.67 43.73 35.59 42.87 35.36 39.12 40.50 43.65 45.02 46. 47.30 49.17 HD >=Very Good mAP HIT@1 35.69 38.18 38.22 37.15 39.13 39.74 39.91 38.20 40.57 37.43 39.12 38.52 39.98 40.54 41. 42.43 42.84 55.6 59.99 60.51 58.65 63.1 64.20 63.42 60.96 65.30 60.17 62.39 62.27 65.58 66.28 65.89 68.94 70.56 Avg 30.73 36.12 38.38 41.74 41.44 43.18 42.62 35.47 43. 36.14 38.08 40.62 43.80 43.63 45.01 46.53 47.94 combines Moment and Highlight Detection tasks, providing extensive video annotations and maintaining evaluation impartiality through its online server. This dataset includes 12,562 YouTube videos and 10,310 annotations, with standardized data splits as per established works. Additionally, we use the Charades-STA [9] dataset for Moment Retrieval (MR) and the TVSum [32] dataset for Highlight Detection (HD). TVSum, encompasses ten categories with five videos each. We follow the data splits in [17], [19], [72], that consider 80% of the dataset for training and 20% for testing. Charades-STA, features 9,848 videos and 16,128 query texts, We adopt the data splits in prior work QD-DETR [19] with 12,408 samples for training and 3,720 for testing. Our adherence to these standardized splits and the diversity of datasets underscore our commitment to robust and fair evaluation of VideoLights. Evaluation Metrics: We follow the established evaluation metric standards from [16], [17], [19], [21], [72]. For moment retrieval, we calculate Recall@1 with predetermined thresholds of 0.5 and 0.7, mean average precision (mAP) with Intersection over Union (IoU) thresholds of 0.5 and 0.75, and average mAP across multiple IoU thresholds that range from 0.50 to 0.95. The same standards are applied to the QVHighlights dataset. For highlight identification, our evaluations include measuring mAP and HIT@1, indicating the hit ratio for the clip with the highest score. Implementation details: We trained four models on each dataset: VideoLights and VideoLights-pt, which utilize CLIP and SlowFast features, and VideoLights-B and VideoLights-B-pt, which incorporate CLIP, BLIP, and SlowFast features. For TVSum, we followed previous works such as TR-DETR [22] and used I3D [73], pre-trained on Kinetics 400 [26], to extract visual features in variant of VideoLights for comparison with other methods. The models were configured with hidden unit size of = 256, one Bi-CMF layers (see Fig. 7), three encoder layers, three decoder layers, seed value of 2018, and 10-moment queries. dropout rate of 0.1 was applied to the transformer layers, and 0.5 to the input projection layers [16]. The loss hyperparameters were set as λL1 = 10, λgIoU = 1, λcls = 4, λsal = 1, λrank = 1, λcont = 1, and = 0.2. Model weights were initialized using the Xavier initialization [74], and the model parameters were optimized with AdamW [75] using an initial learning rate of 1e-4 and weight decay of 1e-4. Following [16], the models were trained for 200 epochs with batch size of 32. For Charades-STA and TVSum, we used batch sizes of 32 and 4, respectively, with learning rates of 1e-4 and 1e-3. All experiments were conducted using T4 and RTX 3050 Ti GPUs. A. Main Results Perfomance in QVHighlights: In Table II, we compare the performance of various methods on the QVHighlights test split for both moment retrieval (MR) and highlight detection (HD) tasks. Our proposed framework, VideoLights , achieves state-of-the-art results across most metrics, demonstrating its robustness and effectiveness. Specifically, in the TABLE III EVALUATION OF HIGHLIGHT DETECTION METHODS ON TVSUM USING TOP-5 MAP. REPRESENTS THE USE OF AUDIO MODALITY. INDICATES THE USE OF I3D FOR VISUAL FEATURES. HERE, BOLD REPRESENTS THE BEST RESULT, AND UNDERLINE REPRESENTS THE 2ND BEST RESULT. 8 Methods sLSTM [7] SG [5] LIM-S [76] Trailer [77] SL-Module [78] UMT [17] QD-DETR [19] UVCOM [62] TR-DETR [22] VideoLights UniVTG [20] VideoLights UniVTG (pt) VideoLights-pt [20] VideoLights-B VideoLights-B-pt VT 41.1 42.3 55.9 61.3 86.5 87.5 88.2 87.6 89.3 89.8 83.9 89.1 92.0 90. 91.3 91.4 VU 46.2 47.2 42.9 54.6 68.7 81.5 87.4 91.6 93.0 88.7 85.1 92.7 77.8 91.8 92.5 88. GA 46.3 47.5 61.2 65.7 74.9 81.5 85.6 91.4 94.3 95.0 89.0 92.3 89.8 95.0 93.3 93.0 MS 47.7 48.9 54.0 60.8 86.2 81.5 85.0 86.7 85.1 88.0 80.1 86.7 83.8 85.3 84.3 95.2 PK 44.8 45.6 60.3 59.1 79 81.4 85.8 86.9 88.0 83. 84.6 89.8 82.2 88.6 88.0 87.2 PR 46.1 47.3 47.5 70.1 63.2 87.0 86.9 86.9 88.6 90.1 81.4 88. 85.8 89.6 88.3 89.1 FM 45.2 46.4 43.2 58.2 58.9 76.0 76.4 76.9 80.4 79.4 70.9 78.5 74.3 76. 77.3 76.1 BK 40.6 41.7 66.3 64.7 72.6 86.9 91.3 92.3 91.3 94.2 91.7 94.0 91.8 94.0 92.7 95. BT 47.1 48.3 69.1 65.6 78.9 84.4 89.2 87.4 89.5 88.6 73.5 87.4 90.5 88.5 88.2 88.6 DS 45.5 46.6 62.6 68.1 64.0 79.6 73.7 75.6 81.6 81.2 69.3 78.3 77.6 78.6 81.6 81.3 Avg. 45.1 46.2 56.3 62.8 73.3 83.1 85.0 86.3 88.1 87. 81.0 87.8 84.6 87.9 87.75 88.52 MR task, our VideoLights-B-pt model achieves the highest R@0.5 (70.36), R@0.7 (55.25), mAP@0.5 (69.53), mAP@0.75 (49.17), and average mAP (47.94), surpassing all prior methods. Without pretraining, VideoLights-B also exhibits strong performance with R@0.5 (68.29), R@0.7 (52.79), mAP@0.5 (67.58), mAP@0.75 (47.30), and average mAP (46.53). These results indicate significant improvements over prior state-of-the-art methods like UVCOM and TRDETR, with notable increases in R@0.5 (by 6.81% over UVCOM and 5.70% over TR-DETR) and average mAP (by 4.76% over UVCOM and 4.94% over TR-DETR). In the HD task, VideoLights-B-pt achieves an mAP of 42.84 and HIT@1 of 70.56, outperforming other methods by considerable margins. Similarly, VideoLights-B delivers strong results, with an mAP of 42.43 and HIT@1 of 68.94, maintaining lead over both UVCOM and UniVTG. Even with fewer features, our models (VideoLights and VideoLights-pt) achieve competitive results, highlighting the flexibility and scalability of our approach. For instance, VideoLights-pt achieves the second-highest R@0.5 (68.48) and R@0.7 (52.53), as well as competitive mAP scores, demonstrating its effectiveness even in pretraining fine-tuning settings. These improvements, ranging from 2.76% to 7.07% across various metrics, underscore the superiority of our framework in both moment retrieval and highlight detection tasks. The integration of additional features (e.g., BLIP) further enhances performance, showing the potential of our framework for video-language understanding tasks. Perfomance in Charades-STA: Our proposed models, VideoLights , VideoLights-pt , VideoLights-B , and VideoLights-B-pt , demonstrate strong perfor- (Table IV). Withmance on the Charades-STA test set out pretraining, VideoLights achieves state-of-the-art results in three of the four metrics. It outperforms UniVTG in R@0.5 by 0.03% (58.04 vs 58.01) and in R@0.7 by 1.23% (36.88 vs 35.65), while achieving comparable mIoU with 0.1% improvement (50.20 vs 50.10). HowTABLE IV RESULTS ON CHARADES-STA TEST SET. HERE, BOLD REPRESENTS THE BEST RESULT, AND UNDERLINE REPRESENTS THE 2ND BEST RESULT. Method R@0.3 R@0.5 R@0.7 mIoU 2D-TAN [35] VSLNet [48] Moment-DETR [16] QD-DETR [19] TR-DETR [22] UniVTG [20] VideoLights UniVTG (pt) [20] VideoLights-pt VideoLights-B VideoLights-B-pt 58.76 60.30 65.83 - - 70.81 70.67 72.63 72.26 71.72 73.33 46.02 42.69 52.07 57.31 57.61 58.01 58.04 60.19 60. 60.30 61.96 27.5 24.14 30.59 32.55 33.52 35.65 36.88 38.55 37.80 37.23 41.05 41.25 41.58 45.54 - - 50.10 50.20 52.17 51. 51.25 52.94 ever, for R@0.3, VideoLights slightly trails UniVTG by 0.14% (70.67 vs 70.81). In the pretraining setting, VideoLights-pt shows competitive results, closely trailing UniVTG (pt) in all metrics. VideoLights-pt achieves 72.26 in R@0.3, 60.11 in R@0.5, 37.80 in R@0.7, and 51.44 in mIoU, compared to UniVTG (pt)s 72.63, 60.19, 38.55, and 52.17, respectively. Additionally, our new models, VideoLights-B and VideoLights-B-pt , incorporating BLIP features, exhibit superior performance. Without pretraining, VideoLights-B surpasses UniVTG in R@0.5 (60.30 vs 58.01) and mIoU (51.25 vs 50.10), though it slightly lags in R@0.3 (71.72 vs 70.81) and R@0.7 (37.23 vs 35.65). With pretraining, VideoLights-B-pt sets new state-of-the-art across all metrics, achieving 73.33 in R@0.3, 61.96 in R@0.5, 41.05 in R@0.7, and 52.94 in mIoU, surpassing UniVTG (pt) by 0.70%, 1.77%, 2.50%, and 0.77%, respectively. These results highlight the efficacy of our approach, particularly with the integration of BLIP features and in pretraining scenarios, significantly advancing performance across all evaluation criteria. Perfomance VideoLights, proposed model, performance TVSum: demonstrates"
        },
        {
            "title": "Our\ncompetitive",
            "content": "in TABLE ABLATION STUDY ON QVHIGHLIGHTS VAL SPLIT. FRA STANDS FOR FRA MODULE, BI STANDS FOR BI-CMF MODULE, BF STANS FOR BLIP FEATURES, PT STANDS FOR PRE-TRAIN ON THE SYNTHETIC DATASET USING BLIP BACKEND, HL STANDS FOR ADAPTIVE HARD POSITIVE AND NEGATIVE LOSS, TCL STANDS FOR TASK COUPLED LOSS, SCSL STANDS FOR SALIENCY COSINE SIMILARITY LOSS, AND AL STANDS FOR ALIGNMENT LOSS. THE EFFECT OF DIFFERENT PRETRAINING DATA IS IN THE BOTTOM BLOCK WITHOUT ANY NEW LOSSES. Modules Losses MR HD R1 mAP >=Very Good 9 tcl scsl al @0.5 @0.7 @0.5 @0.75 Avg mAP HIT@1 sl. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. fra bi bf pt hl No Pretraining ASR Pretraining [16] Our BLIP Pretraining 61.42 64.45 66.77 65.42 69.55 70.06 69.55 70.19 69.55 69.81 69.68 71.03 72.06 66.77 67.94 71.03 46.77 49.48 51.23 52.84 53.94 55.35 54.39 54.77 54.00 54.39 54.71 54.84 57.94 51.23 51.48 54. 60.82 63.69 65.83 64.89 67.53 68.75 68.34 68.59 68.37 69.06 67.80 68.07 70.38 65.83 65.84 68.07 41.36 43.08 45.38 46.67 47.86 49.22 49.0 49.00 47.80 49.21 47.80 47.36 51.12 45.38 44.03 47.36 41.28 43.28 45.12 45.69 47.14 48.44 47.32 48.35 47.63 48.56 54.71 46.06 49.71 45.12 43.74 46. 38.08 39.98 40.74 40.75 42.09 42.84 41.96 42.73 41.85 42.76 41.79 42.16 43.12 40.74 40.71 42.16 60.45 64.13 66.9 65.55 68.77 70.71 68.06 69.10 69.61 69.74 68.26 69.16 71.48 66.9 67.03 69.16 Fig. 5. (a) and (b) show video-query correspondence maps: (a) after text-to-video (t2v) attention and (b) after the Bi-CMF layer. The green line represents the ground truth saliency scores. Bi-CMF attends to the correct video region better than t2v (highlighted in the magenta box). The word Is asserts that refers to one basket, unlike is not. across various domains in the TVSum dataset, as shown in Table III. VideoLights achieves state-of-the-art results in 5 out of 10 domains and in the overall average. Specifically, it outperforms previous methods in VT (89.8% vs. TR-DETRs 89.3%, 0.56% improvement), GA (95.0% vs. TR-DETRs 94.3%, 0.74% increase), MS (88.0% vs. TR-DETRs 85.1%, 3.41% gain), PR (90.1% vs. TR-DETRs 88.6%, 1.69% improvement), and BK (94.2% vs. TR-DETRs 91.3%, In other domains, VideoLights 3.18% improvement). shows highly competitive performance: VU (88.7% vs. TR-DETRs 93.0%, -4.62%), PK (83.6% vs. TR-DETRs 88.0%, -5.00%), FM (79.4% vs. TR-DETRs 80.4%, -1.24%), BT (88.6% vs. TR-DETRs 89.5%, -1.01%), and DS (81.2% vs. TR-DETRs 81.6%, -0.49%). Notably, VideoLights achieves an overall average of 87.9%, closely trailing TRDETRs 88.1% by 0.23%. When compared with UniVTG, our models VideoLights and VideoLights-pt, trained with SlowFast and CLIP, demonstrate significant VideoLights improvements achieves an overall average of 87.9%, surpassing UniVTGs across most domains. 81.0% by 6.9%. It consistently outperforms UniVTG in all domains, with notable gains in VU (92.7% vs. 85.1%, 7.6% improvement), GA (92.3% vs. 89.0%, 3.7% improvement), and MS (86.7% vs. 80.1%, 6.6% improvement). Similarly, VideoLights-pt demonstrates superior performance over UniVTG (pt), achieving an overall average of 87.9% compared to 84.6%, 3.3% improvement. It achieves state-of-the-art results in 7 out of 10 domains, including GA (95.0% vs. UniVTG (pt)s 89.8%, 5.8% gain), MS (85.3% vs. 83.8%, 1.5% gain), and BK (94.0% vs. 91.8%, 2.2% improvement). When comparing the models incorporating BLIP features, VideoLights-B achieves competitive results, notably excelling in domains such as VU (92.5%), BK (92.7%), and DS (81.6%), achieving an average of 87.75%. Additionally, VideoLights-pt, the achieves the best overall average performance at 87.9%, surpassing UniVTG (pt)s 84.6% by 3.3%. It secures stateof-the-art results in 7 domains, including VU (91.8%), GA (95.0%), MS (85.3%), PK (88.6%), PR (89.6%), BK (94.0%), and DS (78.6%). These results highlight the effectiveness of pretraining-enhanced version, (a) (b) Fig. 6. Qualitative results. (a) demonstrates VideoLights outperformed TR-DETR [22] in both MR and HD. (b) Both VideoLights and TR-DETR performed below the ground truth, but upon closer examination, it is evident that incorrectly predicted clips are still related to the given query. TABLE VI EFFECT OF BI-CMF VS UNI-CMF ON VI OLI S ON QVHIGHLIGHTS VAL SET TABLE VIII EFFECT OF INTEGRATING FEATURES FROM DIFFERENT VLMS ON VI OLI S ON QVHIGHLIGHTS VAL SET. HERE SF STANDS FOR SLOWFAST, STANDS FOR CLIP, AND STANDS FOR BLIP-2. Cross-Attention Type MR HD R1@0.5 R1@0.75 mAP@Avg mAP HIT@"
        },
        {
            "title": "Feature type",
            "content": "MR HD R1@0.5 R1@0.75 mAP@Avg mAP HIT@ SF + SF + SF + + 66.77 69.23 70.06 51.23 53.42 55.35 45.12 46.86 48.44 40.74 42.20 42.84 66.9 69.68 70. Bi-CMF Uni-CMF 70.06 69.55 55.35 53.94 48.44 47.14 42.84 42.09 70.71 68. TABLE VII EFFECT OF FRA ON DIFFERENT METHODS ON QVHIGHLIGHTS VAL SET. REPRESENTS THE USE OF THE FRA MODULE Method MR HD R1@0.5 R1@0.75 mAP@Avg mAP HIT@1 Moment-DETR [16] Moment-DETR QD-DETR [19] QD-DETR TR-DETR [22] TR-DETR 53.94 61. 62.68 63.81 67.1 67.81 34.84 40.26 46.66 46.84 51.48 51.68 32.2 35. 41.22 41.71 45.09 45.19 35.36 38.88 39.13 39.77 40.55 41.37 55.55 63. 63.03 63.87 64.77 67.03 VideoLights and its variants in video highlight detection tasks, achieving state-of-the-art performance in key domains while maintaining competitive results across others. In summary, VideoLights not only matches but often exceeds the performance of other cutting-edge methods, demonstrating its effectiveness in joint video highlight detection & moment retrieval. Along with the quantitative results, Fig. 6 shows the qualitative results on the QVHighlights dataset. B. Ablation Studies To comprehend module impacts, we present our model ablation on QVHighlights val split in Table V. Fig. 7. Empirical analysis reveals optimal Bi-CMF performance varies across datasets: three layers yielded superior results on one benchmark, while single layer demonstrated peak performance on another. Consequently, we adopted layer count of one for Bi-CMF across both datasets to ensure consistent cross-modal alignment. Effect of FRA: From Table comparing rows 2 and 5, the addition of the FRA module while keeping Bi-CMF disabled results in an average performance gain of 7.93% across all metrics with minimum 5.28% and maximum 11.09%. Also, Fig. 3 shows the qualitative efficacy of this module. We have done additional experiments, adding FRA to other existing methods and the results are shown in Table VII. The FRA module consistently enhances performance across methods, with significant improvements for weaker baselines like Moment-DETR and incremental gains for stronger models like QD-DETR and TR-DETR. Effect of Bi-CMF: The rows 2 and 4 of Table demonstrate the effectiveness of our Bi-CMF module, showing an average performance gain of 4.03% across all metrics, with the most significant improvement in mAP@0.75 (8.33%). qualitative analysis through feature heatmap visualization in Fig. 5 reveals that Bi-CMF achieves more sparse spectrum density compared to both baseline (no cross-modal) and uni-directional (text-to-video) cross-modal fusion (Uni-CMF) approaches like QD-DETR, indicating better query relevance differentiation. From Table VI we see, Bi-CMF consistently outperforms UniCMF across all metrics, with the most significant improvements seen in HIT@1 (+1.94) and R1@0.75 (+1.41). This demonstrates the effectiveness of Bi-CMF over Uni-CMF. Effect of new loss functions: Rows 6 to 11 in Table illustrate the performance enhancements achieved through the integration of our proposed loss functions: adaptive hard positive and negative loss (hl), task-coupled loss (tcl), saliency cosine similarity loss (scsl), and alignment loss (al). Each loss function independently contributes to the improvement of both Moment Retrieval (MR) and Highlight Detection (HD) tasks. Notably, hl results in advancements in metrics such as MR R1@0.5 and HD HIT@1, tcl enhances performance in MR mAP@0.75, and scsl yields balanced gains across all metrics. The introduction of al further sharpens the outcomes, especially in HD HIT@1. Although each loss demonstrates its effectiveness when utilized individually, the combination of all losses in Row 6 achieves the best overall performance, loss underscoring the synergistic benefit of employing all functions collectively. Effect of Blip-2 features and Pretraining: As shown, especially the difference between the 6th row and the 11th row in Table in the upper block, pre-training also helps improve performance. The usage of BLIP-2 features, along with the standard CLIP and SlowFast, also brings about improvements. We have run additional experiments to check the effectiveness of each feature. From Table VIII we see that using BLIP-2 feature repalcing CLIP results in performance improvement. But it achieves the best results when SlowFast, CLIP, and BLIP-2 are being used together. The bottom block of Table shows the results with different pretraining corpora that pose the effectiveness of pretraining. For this experiment, SlowFast and CLIP features were used, and we kept all modules and losses. Here we see that BLIP Pretraing has minimum 3.18% to maximum 7.57% performance gain on ASR Pretraining. V. LIMITATION AND CONCLUSION Conclusion: In this paper, we introduce VideoLights, jointly tackles the challenges of novel framework that video highlight detection (HD) and moment retrieval (MR). By harnessing the interplay between text and video modalities through innovative cross-task and cross-modal interactions, VideoLights achieves state-of-the-art performance on benchmark datasets, including QVHighlights, TVSum, and 11 Charades-STA. Key contributions of this framework include the Feature Refinement and Alignment (FRA) module, which facilitates effective local and global feature alignment; the Bi-Directional Cross-Modal Fusion (Bi-CMF) network that enhances query-aware representations; and the Unidirectional Joint-Task Feedback Mechanism (Uni-JFM), which optimizes both task-specific and cross-task learning efficiency. We leverage features from Large Vision-Language Models (LVLMs) like BLIP-2 to enhance temporal awareness, ensure semantic alignment, and integrate multimodal features effectively. Additionally, we employ intelligent synthetic data generation using LVLMs and pre-training techniques to boost performance and robustness. The adaptive error correction mechanism further ensures accurate predictions of clip saliency. Comprehensive evaluations and ablation studies substantiate the effectiveness of VideoLights, demonstrating its consistent superiority over previous baselines across various metrics. Future research could delve into advancements in multimodal fusion techniques, improved feature alignment and refinement methods, and broader applications within real-world video platforms. While LVLMs exhibit great potential in multimodal reasoning, their effectiveness in moment retrieval tasks merits further exploration. We contend that VideoLights establishes solid foundation for progressing joint HD/MR prediction, paving the way for scalable and precise video understanding systems. Limitation: Our proposal for weakly supervised pre-training utilizing vision-language pretraining models simplifies the training process but may still be prone to biases or inaccuracies in caption generation. At the same time, our dependency on pretraining models for caption generation and feature extraction can lead to computational overhead and reliance on external resources, thus potentially limiting the scalability of our approach. Moreover, the performance of our Bi-CMF module is heavily reliant on the quality of input features and the effectiveness of attention mechanisms, both of which can vary depending on the complexity and diversity of the video content. To fully unlock the potential of our proposed approach in real-world applications, it is crucial to address these limitations through further research and refinement."
        },
        {
            "title": "REFERENCES",
            "content": "[1] E. Apostolidis, E. Adamantidou, A. I. Metsai, V. Mezaris, and I. Patras, Video summarization using deep neural networks: survey, Proceedings of the IEEE, vol. 109, no. 11, pp. 18381863, 2021. [2] Z. Wu, T. Yao, Y. Fu, and Y.-G. Jiang, Deep learning for video classification and captioning. Kentfield, CA: Association for Computing Machinery and Morgan & Claypool, Dec. 2017, p. 329. [3] L. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell, Localizing moments in video with natural language, in Proceedings of the IEEE international conference on computer vision. Venice, Italy: IEEE, 2017, pp. 58035812. [4] T. Badamdorj, M. Rochan, Y. Wang, and L. Cheng, Contrastive learning for unsupervised video highlight detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans, Louisiana, USA: IEEE/CVF, 2022, pp. 14 04214 052. [5] B. Mahasseni, M. Lam, and S. Todorovic, Unsupervised video summarization with adversarial lstm networks, in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. Honolulu, Hawaii, USA: IEEE, 2017, pp. 202211. [6] F. Wei, B. Wang, T. Ge, Y. Jiang, W. Li, and L. Duan, Learning pixellevel distinctions for video highlight detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans, Louisiana, USA: IEEE/CVF, 2022, pp. 30733082. [7] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman, Video summarization with long short-term memory, in Computer VisionECCV 2016: 14th European Conference, October 1114, 2016, Proceedings, Part VII 14, Springer. Amsterdam, The Netherlands: Springer International Publishing, 2016, pp. 766782. [8] J. Chen, J. Wang, X. Wang, X. Wang, Z. Feng, R. Liu, and M. Song, Coevo-net: Coevolution network for video highlight detection, IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 6, pp. 37883797, 2022. [9] J. Gao, C. Sun, Z. Yang, and R. Nevatia, Tall: Temporal activity localization via language query, in Proceedings of the IEEE international conference on computer vision. Venice, Italy: IEEE, 2017, pp. 5267 5275. [10] W. Liu, T. Mei, Y. Zhang, C. Che, and J. Luo, Multi-task deep visualsemantic embedding for video thumbnail selection, in Proceedings of the IEEE conference on computer vision and pattern recognition. Boston, Massachusetts, USA: IEEE, 2015, pp. 37073715. [11] V. Escorcia, M. Soldan, J. Sivic, B. Ghanem, and B. Russell, Finding moments in video collections using natural language, 2022. [12] D. Han, X. Cheng, N. Guo, X. Ye, B. Rainer, and P. Priller, Momentum cross-modal contrastive learning for video moment retrieval, IEEE Transactions on Circuits and Systems for Video Technology, vol. 34, no. 7, pp. 59775994, 2024. [13] X. Sun, J. Gao, Y. Zhu, X. Wang, and X. Zhou, Video moment retrieval via comprehensive relation-aware network, IEEE Transactions on Circuits and Systems for Video Technology, vol. 33, no. 9, pp. 5281 5295, 2023. [14] J. Gao and C. Xu, Learning video moment retrieval without single annotated video, IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 3, pp. 16461657, 2022. [15] H. Tang, J. Zhu, M. Liu, Z. Gao, and Z. Cheng, Frame-wise crossmodal matching for video moment retrieval, IEEE Transactions on Multimedia, vol. 24, pp. 13381349, 2022. [16] J. Lei, T. L. Berg, and M. Bansal, Detecting moments and highlights in videos via natural language queries, Advances in Neural Information Processing Systems, vol. 34, pp. 11 84611 858, 2021. [17] Y. Liu, S. Li, Y. Wu, C.-W. Chen, Y. Shan, and X. Qie, Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). New Orleans, Louisiana, USA: IEEE/CVF, June 2022, pp. 30423051. [18] J. Yang, P. Wei, H. Li, and Z. Ren, Task-driven exploration: Decoupling and inter-task feedback for joint moment retrieval and highlight detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, pp. 18 30818 318. [19] W. Moon, S. Hyun, S. Park, D. Park, and J.-P. Heo, Query-dependent video representation for moment retrieval and highlight detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Vancouver Canada: IEEE/CVF, June 2023, pp. 23 02323 033. [20] K. Q. Lin, P. Zhang, J. Chen, S. Pramanick, D. Gao, A. J. Wang, R. Yan, and M. Z. Shou, Univtg: Towards unified video-language temporal grounding, in Proceedings of the IEEE/CVF International Conference on Computer Vision. Paris, France: IEEE/CVF, 2023, pp. 27942804. [21] J. Jang, J. Park, J. Kim, H. Kwon, and K. Sohn, Knowing where to focus: Event-aware transformer for video grounding, in Proceedings of the IEEE/CVF International Conference on Computer Vision. Paris, France: IEEE/CVF, 2023, pp. 13 84613 856. [22] H. Sun, M. Zhou, W. Chen, and W. Xie, Tr-detr: Task-reciprocal transformer for joint moment retrieval and highlight detection, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 5, pp. 49985007, Mar. 2024. [23] R. Wang, J. Feng, F. Zhang, X. Luo, and Y. Luo, Modality-aware heterogeneous graph for joint video moment retrieval and highlight detection, IEEE Transactions on Circuits and Systems for Video Technology, vol. 34, no. 9, pp. 88968911, 2024. [24] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable language supervision, in International visual models from natural conference on machine learning, PMLR. Virtual: PMLR, 2021, pp. 87488763. [25] J. Li, D. Li, C. Xiong, and S. Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 12 in International conference on machine learning, PMLR. Baltimore MD: PMLR, 2022, pp. 12 88812 900. [26] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, M. Suleyman, and A. Zisserman, The kinetics human action video dataset, 2017. [27] C. Feichtenhofer, H. Fan, J. Malik, and K. He, Slowfast networks for video recognition, in Proceedings of the IEEE/CVF international conference on computer vision. Seoul, Korea: IEEE/CVF, 2019, pp. 62026211. [28] Z. Yang, L. Li, K. Lin, J. Wang, C. Lin, Z. Liu, and L. Wang, The dawn of lmms: Preliminary explorations with gpt-4v(ision), CoRR, vol. abs/2309.17421, 2023. [29] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, 2024. [30] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models, in International conference on machine learning, PMLR. Honolulu, HI: PMLR, 2023, pp. 19 73019 742. [31] Y. Jiang, X. Yan, G.-P. Ji, K. Fu, M. Sun, H. Xiong, D.-P. Fan, and F. S. Khan, Effectiveness assessment of recent large vision-language models, Visual Intelligence, vol. 2, no. 1, Jun. 2024. [32] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes, Tvsum: Summarizing web videos using titles, in Proceedings of the IEEE conference on computer vision and pattern recognition. Boston, Massachusetts, USA: IEEE, 2015, pp. 51795187. [33] L. A. Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell, Localizing moments in video with temporal language, the 2018 Conference on Empirical Methods in in Proceedings of Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 13801390. [34] Y. Zeng, D. Cao, X. Wei, M. Liu, Z. Zhao, and Z. Qin, Multi-modal relational graph for cross-modal video moment retrieval, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 22152224. [35] S. Zhang, H. Peng, J. Fu, and J. Luo, Learning 2d temporal adjacent networks for moment localization with natural language, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, pp. 12 87012 877, Apr. 2020. [36] S. Xiao, L. Chen, S. Zhang, W. Ji, J. Shao, L. Ye, and J. Xiao, Boundary proposal network for two-stage natural language video localization, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, 2021, pp. 29862994. [37] J. Chen, X. Chen, L. Ma, Z. Jie, and T.-S. Chua, Temporally grounding natural sentence in video, in Proceedings of the 2018 conference on empirical methods in natural language processing, 2018, pp. 162171. [38] D. Liu, X. Qu, J. Dong, and P. Zhou, Reasoning step-by-step: Temporal sentence localization in videos via deep rectification-modulation network, in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 18411851. [39] X. Qu, P. Tang, Z. Zou, Y. Cheng, J. Dong, P. Zhou, and Z. Xu, Finegrained iterative attention network for temporal language localization in videos, in Proceedings of the 28th ACM International Conference on Multimedia, 2020, pp. 42804288. [40] K. Ning, L. Xie, J. Liu, F. Wu, and Q. Tian, Interaction-integrated network for natural language moment localization, IEEE Transactions on Image Processing, vol. 30, pp. 25382548, 2021. [41] Y. Yuan, L. Ma, J. Wang, W. Liu, and W. Zhu, Semantic conditioned dynamic modulation for temporal sentence grounding in videos, Advances in Neural Information Processing Systems, vol. 32, 2019. [42] D. Zhang, X. Dai, X. Wang, Y.-F. Wang, and L. S. Davis, Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12471257. [43] Y. Zhao, Z. Zhao, Z. Zhang, and Z. Lin, Cascaded prediction network via segment tree for temporal video grounding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 41974206. [44] S. Xiao, L. Chen, J. Shao, Y. Zhuang, and J. Xiao, Natural language video localization with learnable moment proposals, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 40084017. [45] B. Liu, S. Yeung, E. Chou, D.-A. Huang, L. Fei-Fei, and J. C. Niebles, Temporal modular networks for retrieving complex compositional activities in videos, in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 552568. [65] A. Nagrani, P. H. Seo, B. Seybold, A. Hauth, S. Manen, C. Sun, and C. Schmid, Learning audio-video modalities from image captions, in European Conference on Computer Vision, Springer. Tel Aviv: Springer, 2022, pp. 407426. [66] M. R. Parvez, J. Chi, W. U. Ahmad, Y. Tian, and K.-W. Chang, Retrieval enhanced data augmentation for question answering on privacy policies, in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. Dubrovnik, Croatia: Association for Computational Linguistics, May 2023, pp. 201 210. [67] G. I. O. Union, metric and loss for bounding box regression, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Long Beach, CA, USA: IEEE/CVF, 2019, pp. 658666. [68] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition. LAS VEGAS, USA: IEEE, 2016, pp. 770778. [69] J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization, 2016. [70] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, Dropout: simple way to prevent neural networks from overfitting, The journal of machine learning research, vol. 15, no. 1, pp. 19291958, 2014. [71] C. Wu, F. Wu, T. Qi, and Y. Huang, NoisyTune: little noise can help you finetune pretrained language models better, in Proceedings the Association for Computational of the 60th Annual Meeting of Linguistics (Volume 2: Short Papers). Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 680685. [72] Y. Xu, Y. Sun, Y. Li, Y. Shi, X. Zhu, and S. Du, Mh-detr: Video moment and highlight detection with cross-modal transformer, 2023. [73] J. Carreira and A. Zisserman, Quo vadis, action recognition? new model and the kinetics dataset, in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 62996308. [74] X. Glorot and Y. Bengio, Understanding the difficulty of training deep feedforward neural networks, in Proceedings of the thirteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings. Sardinia, Italy: JMLR, 2010, pp. 249256. [75] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, 2019. [76] B. Xiong, Y. Kalantidis, D. Ghadiyaram, and K. Grauman, Less is more: Learning highlight detection from video duration, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Long Beach, CA, USA: IEEE/CVF, 2019, pp. 12581267. [77] L. Wang, D. Liu, R. Puri, and D. N. Metaxas, Learning trailer moments in full-length movies with co-contrastive attention, in Computer Vision ECCV 2020: 16th European Conference, August 2328, 2020, Proceedings, Part XVIII 16, Springer. Glasgow, UK: Springer International Publishing, 2020, pp. 300316. [78] M. Xu, H. Wang, B. Ni, R. Zhu, Z. Sun, and C. Wang, Cross-category video highlight detection via set-based learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision. Virtual: IEEE/CVF, 2021, pp. 79707979. [46] M. Zhang, Y. Yang, X. Chen, Y. Ji, X. Xu, J. Li, and H. T. Shen, Multistage aggregated transformer network for temporal language localization in videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 12 66912 678. [47] H. Wang, Z.-J. Zha, L. Li, D. Liu, and J. Luo, Structured multi-level interaction network for video moment localization via language query, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 70267035. [48] H. Zhang, A. Sun, W. Jing, and J. T. Zhou, Span-based localizing network for natural language video localization, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics, Jul. 2020, pp. 6543 6554. [49] J. Mun, M. Cho, and B. Han, Local-global video-text interactions for temporal grounding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 81010 819. [50] D. Liu, X. Qu, J. Dong, P. Zhou, Y. Cheng, W. Wei, Z. Xu, and Y. Xie, Context-aware biaffine localizing network for temporal sentence grounding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 11 23511 244. [51] R. Zeng, H. Xu, W. Huang, P. Chen, M. Tan, and C. Gan, Dense regression network for video grounding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10 28710 296. [52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, Attention is all you need, in Advances in Neural Information Processing Systems, vol. 30. Long Beach, California: Curran Associates, Inc., 2017. [53] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in European conference on computer vision, Springer. Tel Aviv, Israel: Springer International Publishing, 2020, pp. 213229. [54] J. Lei, L. Yu, T. L. Berg, and M. Bansal, Tvr: large-scale dataset for video-subtitle moment retrieval, in Computer VisionECCV 2020: 16th European Conference, August 2328, 2020, Proceedings, Part XXI 16, Springer. Glasgow, UK: Springer International Publishing, 2020, pp. 447463. [55] J. Gao and C. Xu, Fast video moment retrieval, in Proceedings of the IEEE/CVF International Conference on Computer Vision. Virtual: IEEE/CVF, 2021, pp. 15231532. [56] Y. Liu, J. He, W. Li, J. Kim, D. Wei, H. Pfister, and C. W. Chen, r2-tuning: Efficient image-to-video transfer learning for video temporal grounding, in Proceedings of the European Conference on Computer Vision (ECCV), 2024. [57] N. Messina, G. Amato, A. Esuli, F. Falchi, C. Gennaro, and S. Marchand-Maillet, Fine-grained visual textual alignment for crossmodal retrieval using transformer encoders, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 17, no. 4, pp. 123, 2021. [58] J. Hu, S. Qian, Q. Fang, and C. Xu, Hierarchical graph semantic pooling network for multi-modal community question answer matching, in Proceedings of the 27th ACM International Conference on Multimedia. Nice, France: ACM, 2019, pp. 11571165. [59] P. Morgado, Y. Li, and N. Nvasconcelos, Learning representations from audio-visual spatial alignment, Advances in Neural Information Processing Systems, vol. 33, pp. 47334744, 2020. [60] T. Badamdorj, M. Rochan, Y. Wang, and L. Cheng, Joint visual and audio learning for video highlight detection, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 81278137. [61] S. Yan, X. Xiong, A. Nagrani, A. Arnab, Z. Wang, W. Ge, D. Ross, and C. Schmid, Unloc: unified framework for video localization tasks, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). Paris, France: IEEE/CVF, October 2023, pp. 13 623 13 633. [62] Y. Xiao, Z. Luo, Y. Liu, Y. Ma, H. Bian, Y. Ji, Y. Yang, and X. Li, Bridging the gap: unified video comprehension framework for moment retrieval and highlight detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 70918 719. [63] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, short note on the kinetics-700 human action dataset, 2022. [64] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al., Ego4d: Around the world in 3,000 hours of egocentric video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans, Louisiana, USA: IEEE/CVF, 2022, pp. 18 99519 012."
        }
    ],
    "affiliations": []
}