{
    "paper_title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?",
    "authors": [
        "Ouxiang Li",
        "Yuan Wang",
        "Xinting Hu",
        "Huijuan Huang",
        "Rui Chen",
        "Jiarong Ou",
        "Xin Tao",
        "Pengfei Wan",
        "Fuli Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 1 5 3 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "EASIER PAINTING THAN THINKING: CAN TEXT-TOIMAGE MODELS SET THE STAGE, BUT NOT DIRECT THE PLAY? Ouxiang Li1, Yuan Wang1, Xinting Hu2 , Huijuan Huang3 , Rui Chen3, Jiarong Ou3, Xin Tao3 , Pengfei Wan3, Fuli Feng1 1University of Science and Technology of China, 2Nanyang Technological University, 3Kuaishou Technology {lioox, wy1001}@mail.ustc.edu.cn, xinting001@e.ntu.edu.sg, {huanghuijuan.thu, jiangsutx}@gmail.com, wanpengfei@kuaishou.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-COREBENCH, comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating 12dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1, 080 challenging prompts and around 13, 500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/ ."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent developments in text-to-image (T2I) generative models are progressing toward high-quality and instruction-consistent image generation. In real-world applications, textual prompts are concise yet underspecified Hutchinson et al. (2022); Zhong et al. (2023), conveying not only explicit instructions about what must be depicted, but also implicit contextual cues for generating coherent and plausible images. These correspond to two fundamental capabilities required for faithful image generation: composition and reasoning. As shown in Fig. 1, composition aims to correctly generate each visual element explicitly mentioned in the prompt, including specific instances, their attributes, and interactive relations; reasoning aims to generate implicit visual elements inferred from the prompt (e.g., glass of water falls off the table water spilled on the floor). While predominant T2I models, primarily based on diffusion Ho et al. (2020); Ho & Salimans (2021); Rombach et al. *Work done during internship at KwaiVGI, Kuaishou Technology. Corresponding authors. Project leader."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Examples of two fundamental capabilities in T2I generation, using GPT-Image OpenAI (2025). To increase complexity, (a) Composition: We introduce far more instances than DPGBench Hu et al. (2024), representative benchmark with an increased number of visual elements. (b) Reasoning: Our benchmark captures multi-item behavioral effects beyond R2I-Bench Chen et al. (2025b), the latest reasoning-oriented benchmark with single-step, one-to-one inference. Each of our prompts is paired with checklist to facilitate fine-grained and reliable evaluation. (2022); Peebles & Xie (2023) and autoregressive paradigms Sun et al. (2024); Li et al. (2024c); Han et al. (2025), demonstrate strong performance on simple compositional tasks Huang et al. (2023a); Ghosh et al. (2023), they still struggle with complex compositional tasks involving multiple visual elements Hu et al. (2024); Wu et al. (2024) as well as reasoning tasks Niu et al. (2025); Chen et al. (2025b). To improve, recent emerging models incorporate large language models (LLMs) or multimodal large language models (MLLMs) Chameleon (2024); Xie et al. (2024); Chen et al. (2025c); Deng et al. (2025a); Wu et al. (2025a) to encode textual prompts, leveraging their improved text modeling and cross-model alignment, bringing new expectations to handle more complex scenarios involving high compositional density and multi-step reasoning. To enable fair and holistic evaluation of these T2I models in complex real-world scenarios, there is growing need for comprehensive benchmark that systematically evaluates both composition and reasoning capabilities in T2I generation. Early efforts Huang et al. (2023a); Ghosh et al. (2023); Li et al. (2024a) focus on evaluating basic composition capabilities with limited number of visual elements. Subsequent benchmarks further extend the number of visual elements in composition (see Fig. 1 (a)) Hu et al. (2024); Wu et al. (2024); Zhou et al. (2025) and evaluate reasoning capabilities in reasoning (e.g., behavioral reasoning in Fig. 1 (b)) Fu et al. (2024); Niu et al. (2025); Chen et al. (2025b). Despite these advancements, these benchmarks are still exhibiting two limitations. (1) Lack of comprehensiveness: Most benchmarks are designed to evaluate either composition or reasoning capabilities in isolation, making them insufficient for assessing the comprehensive capabilities in current T2I models. Meanwhile, their taxonomy of these capabilities is mostly heuristic, thus failing to cover all relevant dimensions in evaluation. (2) Lack of complexity: While certain benchmarks have attempted to increase the number of visual elements in composition, their evaluations are still conducted at low scene density, failing to reflect the compositional complexity in real-world applications (e.g., generate bustling modern kitchen as shown in Fig. 1 (a)). More importantly, current reasoning-oriented benchmarks primarily focus on single-step, one-to-one inference (e.g., one behavior one outcome), failing to capture the more complex one-to-many causal chains inherent to real-world scenarios as shown in Fig. 1 (b). To address the above limitations, we introduce T2I-COREBENCH, Composition and Reasoning Benchmark that systematically evaluates both composition and reasoning capabilities of T2I models across 12 well-defined dimensions in Table 1, highlighting both comprehensiveness and complexity. To ensure comprehensiveness, we define comprehensive T2I evaluation taxonomy with 12 dimensions spanning both capabilities. The composition capability consolidates previous composition benchmarks Huang et al. (2023a); Ghosh et al. (2023); Hu et al. (2024); Wu et al. (2024) and draws upon the well-defined structure of scene graphs Johnson et al. (2015); Chang et al. (2021), resulting in three basic dimensions: instance, attribute, and relation, in fully depicting compositional scene. To accommodate the generative demands for textual elements, we include text rendering as complementary dimension to evaluate both the content and layout accuracies. In terms of reasoning capability, we adopt tripartite framework of reasoning (i.e., deductive, inductive, and abductive) as well-established in philosophical literature Peirce (1934); Zalta et al. (2003); Godfrey-Smith (2009);"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison between our T2I-COREBENCH and existing T2I benchmarks. T2ICOREBENCH comprehensively covers 12 evaluation dimensions spanning both composition and reasoning scenarios: Multi-Instance (MI), Multi-Attribute (MA), Multi-Relation (MR), Text Rendering (TR), Logical Reasoning (LR), Behavioral Reasoning (BR), Hypothetical Reasoning (HR), Procedural Reasoning (PR), Generalization Reasoning (GR), Analogical Reasoning (AR), Commonsense Reasoning (CR), and Reconstructive Reasoning (RR). The symbols indicate different means coverage with high compositional (visual elements > 5) or reasoning coverage levels: (one-to-many or many-to-one inference) complexity, means coverage under simple settings (visual elements 5 or one-to-one inference), and means this dimension is not covered."
        },
        {
            "title": "BR HR PR GR AR CR",
            "content": "RR T2I-CompBench Huang et al. (2023a) GenEval Ghosh et al. (2023) GenAI-Bench Li et al. (2024a) DPG-Bench Hu et al. (2024) ConceptMix Wu et al. (2024) TIIF-Bench Wei et al. (2025) LongBench-T2I Zhou et al. (2025) Commonsense-T2I Fu et al. (2024) PhyBench Meng et al. (2024) WISE Niu et al. (2025) T2I-ReasonBench Sun et al. (2025) R2I-Bench Chen et al. (2025b) OneIG-Bench Chang et al. (2025) T2I-COREBENCH (Ours) Lipton (2017), and further define eight reasoning dimensions under this framework for T2I models. To increase complexity, we curate prompts that pose greater challenges to both capabilities. For composition, we increase the number of visual elements ( 20 per prompt) to simulate semantically dense T2I scenarios. For reasoning, complexity is introduced along two axes: one-to-many (i.e., one behavior leads to multiple outcomes) and many-to-one inference (e.g., one conclusion drawn from multiple premises), reflecting the intricate reasoning patterns in real-world applications. To facilitate fine-grained and reliable evaluation, we pair each prompt with checklist, which specifies set of independent yes/no questions, to assess whether the image faithfully generates both explicit and implicit visual elements (see Fig. 1). Each sample is automatically assessed by Gemini 2.5 Flash, an MLLM-based evaluator that answers the checklist and is chosen for its strong alignment with human judgments and efficiency at scale. In statistics, T2I-COREBENCH consists of 12 well-defined dimensions (see Table 1) with total of 1, 080 challenging prompts and around 13, 500 checklist questions. In experiments, we benchmark 27 current T2I models (21 openand 6 closed-source) across architectures and scales, including diffusion models, autoregressive models, and unified models. Our study reveals that composition capability in T2I generation is steadily advancing, with open-source models catching up to closed-source counterparts, but overall performance across all models remains insufficient in complex scenarios. In contrast, reasoning capability lags markedly behind, as even the state-of-the-art (SOTA) models struggle with reliably inferring implicit visual elements from prompts, making it the central bottleneck for further progress in T2I models. In summary, our contributions can be concluded as follows: We introduce T2I-COREBENCH, the first benchmark that jointly emphasizes comprehensiveness and complexity in T2I evaluation, covering both composition and reasoning capabilities through 1, 080 challenging prompts across 12 dimensions. We propose an automatic checklist-based evaluation protocol aligned with humans, enabling finegrained and reliable assessment of whether generated images capture both explicit and implicit visual elements, resulting in around 13, 500 individual yes/no questions in total. We conduct comprehensive evaluations on 27 current T2I models and conclude valuable insights, revealing that composition, though steadily improving, still remains unsolved in complex scenarios, whereas reasoning lags markedly behind and stands as the central bottleneck."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Text-to-Image Generative Models. In recent years, T2I generation has witnessed significant advancements, with its rapid development largely driven by the emergence of diffusion models Ho et al. (2020); Ho & Salimans (2021); Rombach et al. (2022). Predominant models, including the Stable Diffusion series Esser et al. (2024), the Flux series Black Forest Labs (2024), and the DALLE series Ramesh et al. (2021), have led to substantial improvements in compositional text-image alignment. Subsequently, to improve alignment with text at the token level by reducing visual biases, autoregressive Sun et al. (2024); Li et al. (2024c); Tian et al. (2024); Han et al. (2025) and unified models Chameleon (2024); Xie et al. (2024); Chen et al. (2025c); Deng et al. (2025a); Chen et al. (2025a); Wu et al. (2025a) have emerged in an LLM-like architecture, demonstrating remarkable performance in the composition task and showing potential for reasoning tasks due to the autoregressive paradigm. Meanwhile, some approaches Guo et al. (2025b); Li et al. (2025c); Liao et al. (2025); Duan et al. (2025) are exploring integrating reasoning into T2I generation to handle more complex and controllable tasks. Text-to-Image Evaluation Benchmarks. Driven by the explicit or implicit nature of T2I generation, which requires both composition and reasoning, benchmarks are developed to evaluate these two capabilities. Early benchmarks Huang et al. (2023a); Ghosh et al. (2023); Li et al. (2024a) primarily target evaluating composition tasks with explicit visual elements. Subsequent benchmarks Hu et al. (2024); Wu et al. (2024); Wei et al. (2025); Zhou et al. (2025) complicate the prompt by including more detailed visual elements, yet still fall short in capturing the real-world challenge of high compositional density. In parallel, reasoning-oriented benchmarks Fu et al. (2024); Meng et al. (2024); Niu et al. (2025); Chen et al. (2025b); Chang et al. (2025); Sun et al. (2025) are gaining prominence as T2I models progress in reasoning tasks, evaluating models across various reasoning dimensions, such as commonsense, logical, and causality. However, they primarily focus on simple one-to-one inferences, overlooking the more complex, many-to-many reasoning prevalent in real-world scenarios. Furthermore, their taxonomy of both capabilities is mostly heuristic, thereby failing to cover all relevant dimensions in evaluation."
        },
        {
            "title": "3 T2I-COREBENCH",
            "content": "This section introduces T2I-COREBENCH, benchmark designed to evaluate both composition and reasoning capabilities of T2I models under real-world complexities, such as dense scene composition and multi-step reasoning. First, we formulate comprehensive T2I evaluation taxonomy for T2I-COREBENCH covering both composition and reasoning capabilities, and highlight how complexity is introduced for each dimension in Sec. 3.1. Building upon this taxonomy, we then outline the data construction details in Sec. 3.2. Finally, we conduct statistical analyses to illustrate its overall structure and distribution in Sec. 3.3."
        },
        {
            "title": "3.1 EVALUATION DIMENSIONS",
            "content": "To address the limitations of previous benchmarks, which evaluate composition and reasoning in isolation using heuristic taxonomies, we formulate comprehensive evaluation taxonomy that unifies both capabilities and reflects real-world generation challenges, as illustrated in Fig. 3."
        },
        {
            "title": "3.1.1 COMPOSITION",
            "content": "Following previous compositional benchmarks Huang et al. (2023a); Ghosh et al. (2023); Hu et al. (2024) and scene graph structures Johnson et al. (2015); Chang et al. (2021), visual scene (e.g., an image) can be fully described by three components: instances, attributes, and relations. Based on this, we define three corresponding dimensions, Multi-Instance, Multi-Attribute, and Multi-Relation, to evaluate core compositional capabilities. Moreover, while textual content can be viewed as special case of instances, we introduce Text Rendering as separate dimension to account for its unique complexity in content and layout accuracy, as shown in Fig. 2 (a). Multi-Instance (MI) refers to generating multiple instances within single image. In our setup, instances are arranged within coherent thematic scene, with scene details expressed through natural narrative descriptions rather than disjointed lists to preserve contextual coherence. We also include"
        },
        {
            "title": "Preprint",
            "content": "(a) Composition (i.e., MI, MA, MR, TR) (b) Deductive Reasoning (i.e., LR, BR, HR, PR) (c) Inductive Reasoning (i.e., GR, AR) (d) Abductive Reasoning (i.e., CR, RR) Figure 2: Examples of T2I-COREBENCH on both (a) composition and (b-d) reasoning capabilities across 12 comprehensive dimensions. Each dimension is constructed with complexity tailored to its unique characteristics, introducing more challenging evaluation under complex, real-world scenarios, and supporting fine-grained and interpretable evaluation. existential negation Li et al. (2024a) by requiring certain objects to be absent (e.g., there is no apple) alongside those that must appear. To increase complexity, each prompt specifies 25 instances on average, creating high-density scenarios that challenge faithful instance composition. Multi-Attribute (MA) refers to binding multiple attributes to single core subject. The attribute set spans wide range of categories: physical properties (e.g., color, material, texture, shape, morphology, lighting, temperature), numerical attributes (e.g., numerals, quantities), states and conditions (e.g., appearance, lifecycle), and abstract and stylistic traits (e.g., emotion, style). Similarly, all attributes are integrated in unified thematic scene with narrative descriptions and existential negation. To increase complexity, each prompt assigns 20 verifiable attributes to single subject, achieving high attribute density while testing precise and consistent attribute binding."
        },
        {
            "title": "Preprint",
            "content": "Multi-Relation (MR) refers to scenes where instances are connected by multiple relations. These relations are drawn from structured library, including spatial (e.g., on the left), interaction (e.g., holding), comparative (e.g., larger than), compositional (e.g., handle on door), and numerical (e.g., twice as many as) relations. Similarly, all relations are incorporated in unified thematic scene with narrative descriptions. To emphasize more relations rather than more instances (i.e., MI), each prompt specifies no more than 10 instances and 15 relations. Text Rendering (TR) refers to rendering structured textual content in specified scene, focusing on both content fidelity and layout precision. To simulate real-world scenarios, we adopt hierarchical text structure in prompts, i.e., main titles, section headers, and itemized entries with descriptions or prices. To increase textual complexity, we introduce special formats and symbols, including varied letter cases (e.g., ALL CAPS), currency signs (e.g., $), punctuation marks (e.g., &), trademarks (e.g., ), etc. Each prompt specifies 15 texts and corresponding layouts, simulating complex real-world applications, including 2D posters and 3D shop signs."
        },
        {
            "title": "3.1.2 REASONING",
            "content": "In real-world scenarios, prompts often involve implicit visual elements that are not explicitly described, making reasoning fundamental capability in T2I generation. To ensure comprehensive evaluation, we adopt tripartite framework of reasoning, as well-established in philosophical literature Peirce (1934); Zalta et al. (2003); Godfrey-Smith (2009); Lipton (2017), i.e., deductive, inductive, and abductive reasoning. This framework provides rigorous foundation for reasoning types, on which we define eight reasoning dimensions specifically tailored to T2I scenarios. Deductive Reasoning is the process of drawing conclusions from given premises, ensuring that if the premises hold, the conclusion cannot be false. In T2I scenarios, this means generating images strictly determined by the premises, as shown in Fig. 2 (b). Logical Reasoning (LR) refers to solving premise-based puzzles through multi-step deductive inference rather than direct scene description. In our setup, prompts are constructed as set of interdependent premises, which leads to deterministic scene regarding object attributes and spatial relations. To guarantee diversity of logical structures, we pre-define various reasoning forms (e.g., deductive elimination, conditional chaining, causal reasoning) and reasoning scenarios (e.g., spatial arrangement, attribute matching, state transition). Each prompt contains 5 independent premises and requires multiple reasoning hops to ensure reasoning complexity. Behavioral Reasoning (BR) refers to inferring the necessary visual consequences of actions, given an initial scene configuration and one or more behaviors. In our setup, prompts specify only the initial state and action(s), while the model must deduce the outcome through reasoning. To increase complexity, each prompt involves compound or sequential actions that deterministically lead to 8 observable outcomes, ensuring that consequences are both logically inevitable and visually salient. Hypothetical Reasoning (HR) refers to predefining counterfactual premise that contradicts realworld physics and consistently propagating its effects across diverse objects within scene. The model must internalize this rule itself (e.g., every vehicles wheels are perfect squares instead of circles) and enforce it uniformly in different forms of interaction. To increase complexity, prompts are designed with 10 objects engaging in varied interactions, where both positive (rule applied) and negative cases (rule not applied) must be correctly distinguished in the same image. Procedural Reasoning (PR) refers to reasoning over an ordered sequence of procedures, where visual elements incrementally transform and only the final accumulated scene is expected. To increase complexity, prompts are designed as 5 explicit procedures, each building on the previous to create cumulative and interacting transformations, while omitting direct outcomes so the model must infer the intermediate steps necessary to reach the complete result. Inductive Reasoning is the process of inferring conclusions from observed regularity patterns rather than from explicit premises. In T2I scenarios, this corresponds to inferring visual elements based on underlying structures observed in examples, as shown in Fig. 2 (c). Generalization Reasoning (GR) refers to inducing an explicit generalization pattern from multiple examples and applying it to novel scenarios with missing information. In our setup, each prompt"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Statistics of T2I-COREBENCH. Left: Our T2I evaluation taxonomy spanning two fundamental generative capabilities (i.e., composition and reasoning), further refined into 12 dimensions. Right: Distributions of prompt-token lengths and checklist-question counts. Our benchmark demonstrates high complexity, with an average prompt length of 170 tokens and an average of 12.5 questions. Note: reasoning has fewer questions, as each requires reasoning that is more challenging. introduces 23 examples that collectively instantiate unified generalization pattern, and the model must extrapolate this pattern and apply it to complete novel scene with deliberately omitted details. This pattern comprises both variant (rules that change across examples) and invariant (rules that remain constant across examples) generalization rules, and each prompt is designed to 8 such rules and to ensure generalization complexity. Analogical Reasoning (AR) refers to transferring specific relational rule from the source domain (e.g., relates to B) to structurally parallel target domain (e.g., relates to D), and enforcing this correspondence in visual generation. In our setup, each prompt specifies source domain relation through detailed anchored example (e.g., hexagonal structure of honeycomb), while the target domain provides only core elements (e.g., clouds arranged like honeycomb) without describing the outcome. To ensure complexity, each prompt is designed as 5 distinct analogical rules, each of which must be consistently transferred from the source to the target domain. Abductive Reasoning is the process of reconstructing the most plausible explanation from observations. In T2I scenarios, this entails reconstructing hidden causes or unstated commonsense that best explain the visual observations, as shown in Fig. 2 (d). Commonsense Reasoning (CR) refers to completing scene by invoking universally shared realworld knowledge that is logically required but not explicitly stated in the prompt. In our setup, each prompt describes scene with deliberately omitted but indispensable elements, and the model must infer these missing visual elements based on its commonsense knowledge. To ensure complexity, each prompt typically requires 5 independent commonsense inferences, covering six diverse domains from physical (e.g., light bulb requires electricity to shine), chemical (e.g., vinegar and baking soda produce bubbles), biological (e.g., bats sleep upside down during the day), social (e.g., doctor wears white coat when treating patients), functional (e.g., cutting vegetables requires knife), and cultural (e.g., Americans eat turkey on Thanksgiving) knowledge. Reconstructive Reasoning (RR) refers to reasoning backward from observations to their most plausible prior states, without directly describing the outcomes themselves. In our setup, each prompt presents static scene of observations with 5 indirect yet diagnostic clues, and the model must integrate them to infer and render the most plausible cause via principled abductive reasoning. To ensure diversity, prompts cover varied inferential scenarios, such as event reconstruction, intent inference, state rewind, and environmental storytelling."
        },
        {
            "title": "3.2 BENCHMARK CONSTRUCTION",
            "content": "Building upon the evaluation dimensions defined above, we now turn to constructing the benchmark. In our setup, each evaluation sample is composed of prompt, which guides T2I generation, and"
        },
        {
            "title": "Preprint",
            "content": "checklist, which enables reliable point-by-point verification of generated visual elements. The core challenge lies in translating the above abstract definitions into concrete prompts and checklists. Prompt Design for Generation. Since our setup involves high compositional density and multi-step reasoning, prior human-based Otani et al. (2023); Niu et al. (2025); Chang et al. (2025) and templatebased Huang et al. (2023a); Ghosh et al. (2023); Wu et al. (2024) strategies prove inadequate. The former are time-consuming and labor-intensive, whereas the latter are rigid and limited in capturing scene diversity. To solve, we introduce Large Reasoning Models (LRMs) to assist data construction, leveraging their large-scale knowledge to cover diverse scenes Lee et al. (2023) and strong reasoning capabilities to handle complex scenarios Zhong et al. (2024); Guo et al. (2025a). Checklist Design for Evaluation. In our evaluation, we find that two commonly used metrics in T2I generation are unreliable in complex scenarios: (1) CLIPScore Hessel et al. (2021) struggles to accurately capture multiple explicit visual elements and implicit reasoning outcomes with interpretability. (2) Direct MLLM-based scoring Li et al. (2024a); Zhou et al. (2025) requires inferring intended visual outcomes on its own with accumulated errors and lacks fine-grained evaluation. To facilitate fine-grained and reliable evaluation, we pair each prompt with checklist, which specifies set of point-by-point yes/no questions (with the correct answer always being Yes), to assess whether the image faithfully generates the intended visual elements. Data Generation. We then follow standardized pipeline to generate prompts and checklists across evaluation dimensions using LRMs. For each dimension, we establish tailored generation instruction comprising: (1) Task Goal, outlining the overall purpose of this dimension; (2) Prompt Design Guidelines, specifying the construction principles as detailed in Sec. 3.1. Meanwhile, each dimension also includes general principle of Diversity and Scalability, which requires variability in both visual themes and structural relations; and (3) Checklist Construction Rules, defining how to decompose the target scene into atomic points, restricted to fundamental existence checks of instances, attributes, or relations that are objective and verifiable. Human Verification. Since LRMs are prone to hallucination Huang et al. (2023b); Yao et al. (2025) (e.g., not always reliably following the input instruction), all generated prompts and checklists are subject to strict human verification for correctness. Given the inherent complexity in verification, we engage five PhD students with expertise in T2I generation. The primary verification principle is to ensure that each LRM output (i.e., prompt and checklist) faithfully follows the given input instruction: (1) For prompt, this includes adhering to all guidelines without logical errors, hallucinated content, or visually imperceptible contradictions; (2) For checklist, this includes comprehensive coverage of all visual elements from the prompt with respect to their final states, and the decomposition of complex outcomes into minimal, indivisible atomic verification questions. Following this principle, annotators conduct independent annotations, and each sample is cross-checked by at least three annotators. Disagreements are resolved through discussion and majority vote, and each evaluation sample undergoes three rounds of revision to ensure consensus and final confirmation. Evaluation Protocol. With our checklists formulated as objective and verifiable Yes/No questions, an intuitive evaluation protocol is to have humans directly answer these questions. However, considering the inefficiency and subjectivity of manual annotation, we instead introduce MLLMs to perform automatic evaluation by framing each item as binary visual question answering task (i.e., scored as 0 for no and 1 for yes) following Hu et al. (2024); Chen et al. (2025b); Zhou et al. (2025). This protocol leverages the atomic checklist design, where each question targets an unambiguous visual element, ensuring inherent compatibility with MLLM-based evaluation."
        },
        {
            "title": "3.3 STATISTICS AND ANALYSIS",
            "content": "To mitigate stylistic homogeneity and potential bias arising from relying on single LRM (e.g., using the same model to generate prompts and produce images often yields inflated performance since they share similar training data), we employ three SOTA LRMs for data construction, including Claude Sonnet 4 Anthropic (2025), Gemini 2.5 Pro Google (2025a), and OpenAI o3 OpenAI (2025). In statistics, for each of the 12 evaluation dimensions, we construct 30 samples with each of the three LRMs, resulting in total of 12 30 3 = 1, 080 generation prompts and 13, 536 questions in evaluation checklists, as further detailed in Fig. 3."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Main results on our T2I-COREBENCH assessing both composition and reasoning capabilities evaluated by Gemini 2.5 Flash. Mean denotes the mean score for each capability. The best and second-best results are marked in bold and underline for openand closed-models, respectively."
        },
        {
            "title": "TR Mean",
            "content": "LR BR HR PR GR AR CR"
        },
        {
            "title": "Reasoning",
            "content": "SD-3-Medium SD-3.5-Medium SD-3.5-Large FLUX.1-schnell FLUX.1-dev FLUX.1-Krea-dev PixArt-α PixArt-Σ HiDream-I1 Qwen-Image 59.1 59.5 57.5 65.4 58.6 70.7 40.2 47.2 62.5 81.4 57.9 60.6 60.0 63.1 60.3 71.1 42.2 49.7 62.0 79.6 35.4 33.1 32.9 47.6 44.1 53.2 14.2 23.8 42.9 65.6 9.5 10.6 15.6 22.4 31.1 28.9 3.3 2.8 33.9 85.5 40.4 41.0 41.5 49.6 48.6 56.0 25.0 30.9 50.3 78."
        },
        {
            "title": "Diffusion Models",
            "content": "22.1 19.9 22.5 25.0 24.8 30.3 11.6 14.7 34.2 41.1 21.1 20.5 22.4 25.1 23.0 26.1 11.6 18.3 24.5 32.2 35.3 33.5 34.2 40.9 36.0 44.5 21.1 26.7 40.9 48."
        },
        {
            "title": "Autogressive Models",
            "content": "51.0 53.7 52.5 64.7 61.8 70.6 30.4 39.2 53.2 75.1 37.4 33.4 35.5 47.6 42.4 50.5 22.6 25.7 34.2 56.5 47.3 52.7 53.0 54.0 57.2 57.5 44.4 44.9 50.3 53.3 35.0 35.6 42.3 39.6 36.3 46.3 26.7 33.9 46.1 61.9 27.1 22.0 25.2 22.9 30.3 28.7 20.9 24.3 31.7 26.4 34.5 33.9 35.9 40.0 39.0 44.3 23.7 28.5 39.4 49. Infinity-8B GoT-R1-7B 63.9 48.8 63.4 55.6 47.5 32.9 10.8 6.1 46.4 35. 28.6 22.1 25.9 19.2 42.9 31.3 62.6 49.2 47.3 34.8 59.2 46. 46.9 32.1 24.6 14.6 42.3 31.2 BAGEL BAGEL w/ Think show-o2-1.5B show-o2-7B Janus-Pro-1B Janus-Pro-7B BLIP3o-4B BLIP3o-8B OmniGen2-7B Seedream 3.0 Gemini 2.0 Flash Nano Banana Imagen 4 Imagen 4 Ultra GPT-Image 64.9 57.7 59.5 59.4 51.0 54.4 45.6 46.2 67. 79.9 67.5 77.9 82.8 90.0 84.1 65.2 60.8 60.3 61.8 54.5 59.3 47.5 50.4 64.1 78.0 68.5 85.7 74.3 80.0 75.9 45.8 37.8 36.1 38.1 33.8 40.9 20.3 24.1 48.3 63.7 49.7 72.6 66.3 73.2 72.7 9.7 2.2 4.6 2.2 2.9 7.5 0.5 0.5 19. 47.6 62.9 86.3 90.2 86.2 86.4 Unified Models 23.4 25.5 21.6 23.2 12.9 19.8 14.2 14.8 24.7 21.9 25.4 21.8 23.1 18.1 20.9 17.7 20.7 23.2 33.0 33.9 37.1 37.5 24.7 34.6 26.3 28.3 43.3 Closed-Source Models 36.8 39.3 64.5 44.5 63.6 59.0 33.6 39.7 64.9 51.8 62.4 54.8 50.3 47.9 67.1 56.8 66.1 65.6 46.4 39.6 40.1 40.4 35.5 40.5 28.5 30.3 49.9 67.3 62.1 80.6 78.4 82.4 79.8 51.6 58.6 47.7 51.6 13.4 22.4 36.3 39.6 63. 75.1 69.3 85.2 82.8 88.5 87.3 31.2 53.5 39.9 40.9 7.1 11.5 37.6 43.4 46.1 54.9 58.5 84.1 79.5 82.8 76.5 50.4 56.9 44.7 47.2 15.1 30.4 37.8 51.0 54.2 61.7 63.7 83.1 73.3 83.0 82.0 32.4 41.6 29.0 32.2 6.7 8.7 31.3 35.9 36. 59.1 51.2 71.3 72.8 76.3 70.9 29.3 39.8 24.0 21.3 6.4 9.8 24.8 20.4 24.1 31.2 39.9 68.7 65.3 60.7 56.1 34.1 41.9 33.2 34.6 13.0 19.8 28.2 31.8 39.4 50.3 51.2 73.6 65.9 72.9 69.0 36.5 36.3 37.8 43.2 42.2 48.2 24.1 29.3 43.0 58. 43.6 32.7 38.2 41.1 35.5 36.5 20.5 26.7 28.3 31.3 42.9 56.0 54.8 75.9 70.0 76.1 72."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we present systematic evaluation of current T2I models on our T2I-COREBENCH, with experimental setup details in Sec. 4.1. Subsequently, we present the main results and provide valuable insights into the generation capabilities of T2I models under complex scenarios in Sec. 4.2, and further conduct human alignment study to assess the consistency between MLLMs and humans in Sec. 4.3. Additional experimental results (e.g., fine-grained analyses and case studies) are actively being conducted in progress and will be included in the next version."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Evaluated Models. We evaluate broad range of T2I models with diverse architectures and parameter scales, covering both openand closed-models. Among the open-source ones, the pool includes 21 models: (1) Diffusion Models1: SD-3-Medium, SD-3.5-Medium, SD-3.5-Large Esser et al. (2024), FLUX.1-schnell, FLUX.1-dev, FLUX.1-Krea-dev Black Forest Labs (2024), PixArt-α Chen et al. (2023), PixArt-Σ Chen et al. (2024), HiDream-I1 Cai et al. (2025), Qwen-Image Wu et al. (2025a); (2) Autoregressive Models: Infinity-8B Han et al. (2025), GoT-R1-7B Duan et al. (2025); and (3) Unified Models: BAGEL, BAGEL w/ Think Deng et al. (2025b), show-o2-1.5B, show1Herein, flow-based generative models are framed as variants of the diffusion paradigm within unified continuous-time (ODE/SDE) framework."
        },
        {
            "title": "Preprint",
            "content": "o2-7B Xie et al. (2025), Janus-Pro-1B, Janus-Pro-7B Chen et al. (2025c), BLIP3o-4B, BLIP3o8B, Chen et al. (2025a) OmniGen2-7B Wu et al. (2025b). We further include 6 widely-used closedsource commercial models to facilitate comprehensive evaluation, including: Seedream 3.0 Gao et al. (2025), Gemini 2.0 Flash Google (2024), Nano Banana Google (2025b), Imagen 4, Imagen 4 Ultra Google (2025c), and GPT-Image OpenAI (2025). All evaluated models are implemented using their default configurations from the corresponding official repositories, with fixed random seed applied whenever supported to ensure reproducibility. All experiments are conducted using eight NVIDIA A800 GPUs, with four images generated per prompt to ensure robust evaluation. Evaluation Details. Given that each prompt is paired with checklist consisting of objective and verifiable Yes/No questions, we formulate the evaluation as straightforward binary discrimination process. To facilitate automatic evaluation, we adopt Gemini 2.5 Flash Google (2025a) as the evaluator, which exhibits strong vision-language capabilities at relatively low cost, making it well-suited for large-scale evaluation. Considering the possible unavailability of closed-source APIs in the future, we also report evaluation results with the open-source SOTA MLLM Qwen2.5-VL-72B Bai et al. (2025), which serves as reliable reference for long-term reproducibility and evaluation. Furthermore, we conduct human alignment study to assess the consistency between MLLMs and human annotators, as detailed in Sec. 4.3. In evaluation, we report the mean score across all samples within each dimension as its final score for that dimension."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "As shown in Table 2, we evaluate wide range of T2I models on our T2I-COREBENCH, revealing valuable insights into their strengths, weaknesses, and advancements, particularly in handling realworld scenarios that require high compositional density and multi-step reasoning: (1) Composition shows steady improvements with T2I model iterations, yet still far from being solved, especially in complex scenarios. Across all models, we observe consistent improvements on composition tasks with T2I model iterations. Specifically, the best closed-source model, Imagen 4 Ultra, achieves 82.4 in composition, while the best open-source model, Qwen-Image, reaches 78.0, which approaches most advanced closed-source models, underscoring the rapid progress of the open-source community. Nevertheless, the composition tasks in complex scenarios remain challenging, as even Imagen 4 Ultra struggles with multi-attribute binding (MA: 80.0) and multi-relation generation (MR: 73.2), indicating that complex compositional control is still an open problem. (2) Reasoning remains the major bottleneck, as even the SOTA models struggle with real-world multi-step inferences. Despite the steady improvement in composition, reasoning capabilities still lag far behind in T2I generation. For example, Imagen 4 Ultra achieves only 72.9 in reasoning, 9.5 points lower than its composition score, and performs poorly on certain dimensions (LR: 63.6, BR: 62.4, HR: 66.1, RR: 60.7). This performance gap is more pronounced within the open-source domain, as Qwen-Image reaches 78.0 in composition but only 49.3 in reasoning (28.7 points lower). These results suggest that current T2I models still struggle to infer implicit visual elements from prompts, underscoring that reasoning remains the central unsolved challenge under our benchmark. (3) Incorporating explicit thinking before generation yields mixed benefits. Here, thinking refers to the text encoder conducting intermediate reasoning to rewrite the prompt prior to generation. The comparison between BAGEL and BAGEL w/ Think shows that thinking can lead to consistent improvements in all reasoning dimensions (34.1 to 41.9) and achieves the leading scores in GR (53.5) and RR (39.8) within the open-source group. However, we also observe an obvious performance drop in composition (46.4 to 39.6). These gains in reasoning likely stem from the ability to infer implicit visual elements through intermediate thinking, whereas the drop in composition suggests that intermediate thinking may also distract from explicit cues directly provided in the prompt. (4) Understanding multimodal conditions with large models shows clear advantages for opensource models. Since the architectures of closed-source models remain undisclosed, we only analyze open-source models. Among all open-source models, Qwen-Image achieves the best results for both composition and reasoning. Beyond its superiority in training data, we think that its adoption of an MLLM (i.e., Qwen2.5-VL) to encode conditions (e.g., text) is key factor, as the large capacity enables stronger understanding of multimodal inputs and provides more promising reasoning capability Liu et al. (2023); Zhang et al. (2023), consistent with recent advances in LLMs. More-"
        },
        {
            "title": "Preprint",
            "content": "over, this paradigm also allows the direct incorporation of explicit thinking (e.g., BAGEL), which can further improve the T2I generation accuracy and faithfulness as promising future direction."
        },
        {
            "title": "TR Mean",
            "content": "81.4 70.8 78.0 75.5 67.7 71.8 64.4 56.5 60.3 63.1 56.8 61."
        },
        {
            "title": "MI MA MR",
            "content": "Qwen2.5-VL-72B InternVL3-78B GLM4.5V-106B Table 3: Human alignment study across different MLLMs on four compositional dimensions, evaluated with balanced accuracy (%). The best and second-best results are marked in bold and underline for openand closed-models. To further validate the effectiveness of employing MLLMs as substitutes for human evaluation, we compare MLLM-based judgments with those of human annotators. Specifically, we focus on four dimensions (i.e., MI, MA, MR, and TR), which capture the fundamental visual elements of evaluation: instance, attribute, relation, and text. As the questions in the remaining eight reasoning dimensions can also be decomposed into these same elements, evaluating these four dimensions could be sufficient. In our experiments, we use images from GPT-Image along these four dimensions. For the human annotation results, we hire professional annotators who are highly experienced in image and video annotation. The annotation pipeline begins with the distribution of detailed guidelines, followed by training and trial annotations to ensure consistency. The annotators then carry out the primary annotation (first round), after which the results undergo secondary and tertiary rounds of verification through full inspection, ensuring high-quality and reliable results. To facilitate comprehensive comparison, we include multiple cutting-edge MLLMs, including both open-source (Qwen2.5-VL-72B Bai et al. (2025), InternVL378B Zhu et al. (2025), GLM4.5V-106B Hong et al. (2025)) and closed-source ones (GPT-4o OpenAI (2024), OpenAI o3, OpenAI o4 mini OpenAI (2025), Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 2.5 Flash Lite Google (2025a), Gemini 2.0 Flash Google (2024)). Considering the imbalance in the human-annotated ground-truth results (e.g., the number of correctly generated visual elements in GPT-Image generations is substantially greater than that of incorrect ones), we introduce balanced accuracy Brodersen et al. (2010) to provide fair and robust evaluation. GPT-4o OpenAI o3 OpenAI o4 mini Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.5 Flash Lite Gemini 2.0 Flash 67.5 77.8 74.7 76.5 76.9 60.1 61.0 72.0 86.8 83.0 88.4 85.7 74.5 77. 63.6 80.4 77.0 82.2 78.0 58.0 67.7 70.3 82.1 79.1 82.6 81.1 65.4 69.8 78.3 83.5 81.9 83.4 83.8 69.1 73.5 71.1 62.9 67.8 As shown in Table 3, closed-source MLLMs significantly outperform open-source ones in recognizing these fundamental visual elements, with OpenAI o3 and Gemini 2.5 Pro achieving the best performance. Considering the trade-off between performance and API cost, we select Gemini 2.5 Flash as our evaluator for large-scale evaluation (i.e., its API cost is about one-fourth of that of OpenAI o3 and Gemini 2.5 Pro, while performance drops by around 1%). Meanwhile, considering the possible unavailability of closed-source APIs in the future, we also report evaluation results using Qwen2.5-VL-72B, as it achieves the best performance among all open-source MLLMs."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION",
            "content": "Conclusion. In this paper, we introduce T2I-COREBENCH, benchmark that systematically evaluates both composition and reasoning capabilities of T2I models through comprehensive taxonomy covering 12 well-defined dimensions under real-world complexity. Our extensive evaluation of 27 current models reveals clear progress in composition, yet also highlights persistent challenges in both composition and reasoning when faced with complex real-world scenarios involving high compositional density and multi-step reasoning, with reasoning remaining the primary bottleneck. Discussion. To address these challenges, we identify four promising research directions for future work: (i) The development of more diverse and challenging training data, particularly with multielement and reasoning-oriented supervision, is essential for enabling stronger generalization across complex tasks. (ii) The integration of LLMs and MLLMs into T2I pipelines should be advanced, leveraging their strong language modeling and cross-modal reasoning capabilities to improve semantic understanding and alignment in complex generation scenarios. (iii) The incorporation of LLM-style reasoning paradigms (e.g., Chain-of-Thought Wei et al. (2022), Self-Consistency Wang et al. (2022), and Retrieval-Augmented Generation Gao et al. (2023)) into T2I pipelines can facilitate intermediate inference before image generation, thereby improving the extraction of implicit"
        },
        {
            "title": "Preprint",
            "content": "visual elements from complex prompts. (iv) The exploration of reasoning mechanisms during generation is also needed, by explicitly integrating visual reasoning steps into the generation process to support more detailed and controllable outputs. We hope this benchmark and analysis can facilitate future research toward building T2I models into both set the stage and direct the play."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, May 2025. Announces Claude Opus 4 and Claude Sonnet 4. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Black Forest Labs. Flux: series of fast diffusion models for high-resolution text-to-image synthesis. https://huggingface.co/black-forest-labs/, 2024. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim Buhmann. The balanced accuracy and its posterior distribution. In 2010 20th international conference on pattern recognition, pp. 31213124. IEEE, 2010. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Team Chameleon. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arXiv:2506.07977, 2025. Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen, and Alex Hauptmann. comprehensive survey of scene graphs: Generation and application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(1):126, 2021. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pp. 7491. Springer, 2024. Kaijie Chen, Zihao Lin, Zhiyang Xu, Ying Shen, Yuguang Yao, Joy Rimchala, Jiaxin Zhang, and Lifu Huang. R2i-bench: Benchmarking reasoning-driven text-to-image generation. arXiv preprint arXiv:2505.23493, 2025b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025a. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025b."
        },
        {
            "title": "Preprint",
            "content": "Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i chalarXiv preprint lenge: Can text-to-image generation models understand commonsense? arXiv:2406.07546, 2024. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Peter Godfrey-Smith. Theory and reality: An introduction to the philosophy of science. In Theory and reality. University of Chicago Press, 2009. Google. Gemini 2.0 flash. https://deepmind.google/models/gemini/flash/, 2024. Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Technical report, Google, June 2025a. URL https://storage.googleapis.com/deepmind-media/gemini/gemini_ v2_5_report.pdf. Google. Introducing gemini 2.5 flash image, our state-of-the-art image model. https:// developers.googleblog.com/en/introducing-gemini-2-5-flash-image/ ?utm_source=chatgpt.com, 2025b. Google. Imagen 4 (including imagen 4 ultra and imagen 4 fast). https://deepmind.google/ models/imagen/, 2025c. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Shanghang Zhang, Peng Gao, et al. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025b. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1573315744, 2025. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021."
        },
        {
            "title": "Preprint",
            "content": "Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview. net/forum?id=qw8AKxfYbI. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023a. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 2023b. Ben Hutchinson, Jason Baldridge, and Vinodkumar Prabhakaran. Underspecification in scene description-to-depiction tasks. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 11721184, 2022. Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36683678, 2015. Alycia Lee, Brando Miranda, and Sanmi Koyejo. Beyond scale: the diversity coefficient as data quality metric demonstrates llms are pre-trained on formally diverse data. In ICML Workshop on Challenges in Deployable Generative AI, International Conference on Machine Learning (ICML), 2023. Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, et al. Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743, 2024a. Ouxiang Li, Yanbin Hao, Zhicai Wang, Bin Zhu, Shuo Wang, Zaixi Zhang, and Fuli Feng. Model inversion attacks through target-specific conditional diffusion models. arXiv preprint arXiv:2407.11424, 2024b. Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Fuli Feng. Improving synthetic image detection towards generalization: An image transformation perspective. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1, pp. 2405 2414, 2025a. Ouxiang Li, Yuan Wang, Xinting Hu, Houcheng Jiang, Tao Liang, Yanbin Hao, Guojun Ma, and Fuli Feng. Speed: Scalable, precise, and efficient concept erasure for diffusion models. arXiv preprint arXiv:2503.07392, 2025b. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection. arXiv preprint arXiv:2503.12271, 2025c. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445, 2024c."
        },
        {
            "title": "Preprint",
            "content": "Jiaqi Liao, Zhengyuan Yang, Linjie Li, Dianqi Li, Kevin Lin, Yu Cheng, and Lijuan Wang. Imagegen-cot: Enhancing text-to-image in-context learning with chain-of-thought reasoning. arXiv preprint arXiv:2503.19312, 2025. Peter Lipton. Inference to the best explanation. Companion to the Philosophy of Science, pp. 184193, 2017. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI. Gpt-4o system card. Technical report, OpenAI, 2024. URL https://arxiv.org/ abs/2410.21276. arXiv preprint arXiv:2410.21276. OpenAI. Gpt-4o-image, 2025. introducing-4o-image-generation/. https://openai.com/index/ OpenAI."
        },
        {
            "title": "Introducing",
            "content": "o3 and o4-mini. introducing-o3-and-o4-mini/, April 2025. https://openai.com/index/ Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima, Esa Rahtu, Janne Heikkila, and Shinichi Satoh. Toward verifiable and reproducible human evaluation for text-toimage generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1427714286, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Charles Sanders Peirce. Collected papers of charles sanders peirce, volume 5. Harvard University Press, 1934. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, and Xihui Liu. T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation, 2025. URL https://arxiv.org/ abs/2508.17472. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022."
        },
        {
            "title": "Preprint",
            "content": "Yuan Wang, Ouxiang Li, Tingting Mu, Yanbin Hao, Kuien Liu, Xiang Wang, and Xiangnan He. Precise, fast, and low-cost concept erasure in value space: Orthogonal complement matters. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 28759 28768. IEEE, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty. Advances in Neural Information Processing Systems, 37:8600486047, 2024. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, and Weidi Xie. sanity check for ai-generated image detection. arXiv preprint arXiv:2406.19435, 2024. Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. Are reasoning models more prone to hallucination? arXiv preprint arXiv:2505.23646, 2025. Edward Zalta, Uri Nodelman, Colin Allen, and John Perry. Stanford encyclopedia of philosophy, 2003. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Shanshan Zhong, Zhongzhan Huang, Weushao Wen, Jinghui Qin, and Liang Lin. Sur-adapter: Enhancing text-to-image pre-trained diffusion models with large language models. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 567578, 2023. Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024. Yucheng Zhou, Jiahao Yuan, and Qianning Wang. Draw all your imagine: holistic benchmark and agent framework for complex instruction-based image generation. arXiv preprint arXiv:2505.24787, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A EVALUATION DETAILS",
            "content": "A.1 T2I MODELS FOR GENERATION To facilitate transparency and reproducibility, we provide below the official sources of all models evaluated in this study. For each model, we strictly followed the default sampling configurations specified in the corresponding repositories or API documentation, without any manual modification of hyperparameters. These links serve as the canonical reference points for the model implementations and the exact API versions used in our experiments. For open-source models, we included diverse set of diffusion, autoregressive, and unified architectures: SD-3-Medium, SD-3.5-Medium, SD-3.5-Large Esser et al. (2024), FLUX.1-schnell, FLUX.1-dev, FLUX.1-Krea-dev Black Forest Labs (2024), PixArt-α Chen et al. (2023), PixArt-Σ Chen et al. (2024), HiDream-I1 Cai et al. (2025), Qwen-Image Wu et al. (2025a), Infinity-8B Han et al. (2025), GoT-R1-7B Duan et al. (2025), BAGEL, BAGEL w/ Think Deng et al. (2025b), show-o2-1.5B, show-o2-7B Xie et al. (2025), Janus-Pro-1B, Janus-Pro-7B Chen et al. (2025c), BLIP3o-4B, BLIP3o-8B Chen et al. (2025a), and OmniGen2-7B Wu et al. (2025b). For closed-source commercial models, we rely on their official API endpoints, which guarantee that our evaluation reflects the current production-level configurations of these services: Seedream 3.0 Gao et al. (2025), Gemini 2.0 Flash Google (2024), Nano Banana Google (2025b), Imagen 4, Imagen 4 Ultra Google (2025c), and GPT-Image OpenAI (2025). A.2 MLLM INSTRUCTION FOR EVALUATION We list all MLLMs employed in our evaluation together with their official sources, so that the evaluation setup can be faithfully reproduced. Closed-source models are accessed via their official API endpoints, which guarantee that our evaluation reflects the current production-level configurations of these services: GPT-4o OpenAI (2024), OpenAI o3, OpenAI o4 mini OpenAI (2025), Gemini 2.0 Flash Google (2024), Gemini 2.5 Pro, Gemini 2.5 Flash, and Gemini 2.5 Flash Lite Google (2025a). Open-source models are run with their default inference settings from official repositories: Qwen2.5-VL-72B Bai et al. (2025), InternVL3-78B Zhu et al. (2025), and GLM4.5V-106B Hong et al. (2025). To ensure the reproducibility of results, we set the temperature coefficient to 0 during all model evaluations whenever supported. In our benchmark, evaluation is conducted automatically using an MLLM as the checklist answerer. Specifically, we provide each generated image together with its associated prompt and evaluate it against the checklist in question-by-question manner, where the MLLM receives only single yes/no question at time. This design avoids interference between different questions, ensures that each judgment relies solely on visible evidence, and thereby improves both the accuracy and consistency of the evaluation. As shown in Fig. 4, the evaluation instruction strictly emphasizes reliance on the image content without assuming details from the prompt, to minimize hallucinations and ensure interpretable, fine-grained verification."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTS",
            "content": "As discussed in Sec. 4.3, we also report evaluation results using Qwen2.5-VL-72B in Table 4, as it achieves the best performance among all open-source MLLM evaluators. The experimental results show that the patterns observed in Qwen-based evaluations align with those from Gemini-based assessments in Table 2. This consistency across different evaluators confirms the reliability and robustness of the results, ensuring that the conclusions about model performance remain stable, regardless of the evaluation method used. This further supports the reproducibility and transparency of the evaluation process, reinforcing the validity of the insights derived from our experiments."
        },
        {
            "title": "C ETHICS STATEMENT",
            "content": "With the introduction of the T2I-COREBENCH benchmark, we anticipate continuous improvements in both composition and reasoning capabilities of T2I models, leading to increasingly realistic and faithful AI-generated content. While these advancements hold great potential, they also raise concerns about the proliferation of AI-generated content, which may overwhelm creative industries and lead to issues around copyright and authenticity Li et al. (2024b); Wang et al. (2025); Li et al. (2025b). As the boundary between human-created and AI-generated works blurs, there is growing"
        },
        {
            "title": "Preprint",
            "content": "You are an AI quality auditor for text-to-image generation. Your task is to analyze the given image and answer yes/no question based solely on its visual content. The question may relate to the presence of specific object, its attributes, or relationships between multiple elements in the image. You will also be given the original prompt used to generate the image. The prompt may provide additional context to help interpret the question, but it must never be used to supply or assume visual details. Your judgment must rely entirely on the image itself. The image must contain clear, unmistakable visual evidence to justify yes answer the prompt cannot compensate for missing or ambiguous content. Respond with: - yes only if the answer is clearly and unambiguously yes based solely on the visual content. The visual evidence must be strong, definitive, and require no assumptions or guesses. - no in all other cases including if the relevant visual detail is missing, unclear, ambiguous, partially shown, obscured, or only suggested. Even if the image closely matches what is described in the prompt, you must rely on visible evidence alone. If the relevant detail cannot be confirmed visually with certainty, answer no. Ambiguity equals no. For conditional questions, answer yes only if both the condition and the main clause are clearly and unambiguously true in the image. If either part is false or uncertain, respond no. Do not provide any explanation, justification, or extra text. Only return single word: either yes or no. Example input: Prompt: golden retriever running in grassy field under the sun. Question: Is there sun in the image? Example output: yes Example input: Prompt: white cat sitting on red couch in modern living room. Question: Is the couch present, is it red in color? Example output: no Figure 4: Evaluation instruction for MLLM evaluator in our T2I-COREBENCH. need for clear frameworks to address copyright ownership, prevent misuse, and ensure transparency. Solutions such as watermarking, detection, and regulations Yan et al. (2024); Li et al. (2025a) to curb the spread of misleading content are essential to mitigate these ethical challenges and balance innovation with responsible AI use."
        },
        {
            "title": "D LIMITATIONS",
            "content": "While our T2I-COREBENCH provides comprehensive and challenging benchmark for assessing both compositional and reasoning capabilities, we also observe several limitations in evaluation: (i) Our study focuses solely on T2I generation, leaving out other emerging modalities such as video generation and interactive multimodal generation, which pose additional temporal and contextual reasoning challenges Fu et al. (2025). (ii) Although our checklist-based evaluation ensures consistency and objectivity across dimensions, certain aspects could benefit from finer-grained metrics."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Main results on our T2I-COREBENCH assessing both composition and reasoning capabilities evaluated by Qwen2.5-VL-72B. Mean denotes the mean score for each capability. The best and second-best results are marked in bold and underline for openand closed-models, respectively."
        },
        {
            "title": "TR Mean",
            "content": "LR BR HR PR GR AR CR"
        },
        {
            "title": "Reasoning",
            "content": "SD-3-Medium SD-3.5-Medium SD-3.5-Large FLUX.1-schnell FLUX.1-dev FLUX.1-Krea-dev PixArt-α PixArt-Σ HiDream-I1 Qwen-Image 61.1 61.5 59.5 68.8 61.6 74.6 41.1 49.1 66.8 85.6 77.2 80.6 80.5 83.5 81.4 89.3 57.3 70.6 82.0 95.4 46.6 48.4 44.7 65.7 61.6 72.5 22.5 35.5 57.4 86.8 16.9 19.5 28.3 32.3 42.3 40.2 7.9 12.7 40.3 92.3 50.5 52.5 53.3 62.6 61.7 69.1 32.2 42.0 61.6 90."
        },
        {
            "title": "Diffusion Models",
            "content": "41.2 41.2 42.3 43.6 41.2 47.6 29.6 37.8 46.2 52.5 20.8 20.5 23.6 26.4 23.8 28.2 12.8 20.2 24.8 38.3 28.9 27.3 27.0 35.1 30.3 39.4 18.4 24.0 36.4 45."
        },
        {
            "title": "Autogressive Models",
            "content": "65.5 66.2 67.0 79.0 78.2 83.2 37.9 51.1 65.0 87.6 47.3 42.3 47.5 53.5 50.5 59.1 33.8 35.6 42.4 65.8 59.4 56.3 62.8 67.2 67.6 68.6 41.0 49.3 48.1 68.5 38.6 38.7 44.1 42.7 39.8 47.4 30.3 37.5 50.4 65.2 15.0 13.8 15.7 13.8 17.2 20.1 15.1 15.8 20.2 21.2 39.6 38.3 41.2 45.2 43.6 49.2 27.4 33.9 41.7 55. Infinity-8B GoT-R1-7B 66.6 55.9 86.1 79.6 64.9 54.1 34.9 34.3 63.1 56. 48.0 48.9 29.3 22.8 36.9 28.3 76.6 69.9 60.9 50.8 79.9 64. 49.9 36.6 17.2 10.2 49.8 41.5 BAGEL BAGEL w/ Think show-o2-1.5B show-o2-7B Janus-Pro-1B Janus-Pro-7B BLIP3o-4B BLIP3o-8B OmniGen2-7B Seedream 3.0 Gemini 2.0 Flash Nano Banana Imagen 4 Imagen 4 Ultra GPT-Image 69.2 61.6 64.3 66.5 61.6 64.2 48.1 49.6 72. 85.5 68.8 88.5 85.2 92.8 87.8 85.9 82.4 81.9 83.5 81.2 84.0 68.6 72.2 86.0 95.1 85.2 94.3 91.0 95.0 93.4 66.5 55.5 53.3 61.4 59.7 65.7 28.8 35.3 67.2 85.8 67.4 88.9 85.3 90.2 90.2 22.4 6.9 12.5 35.7 21.8 30.9 1.5 1.2 37. 76.0 82.0 93.6 94.2 90.1 92.8 Unified Models 39.7 44.7 45.1 48.0 44.1 49.3 39.6 40.3 42.9 21.9 28.8 23.6 30.4 23.7 24.1 19.7 22.2 24.4 28.2 30.8 30.9 34.1 25.5 33.4 21.4 23.4 39.4 Closed-Source Models 50.9 52.4 67.2 55.0 65.4 65.1 40.1 40.4 67.4 53.6 66.8 58.5 46.5 41.9 59.1 49.9 58.3 57.9 61.0 51.6 53.0 61.7 56.1 61.2 36.7 39.6 65.6 85.6 75.8 91.3 88.9 92.0 91.1 64.9 75.0 61.6 73.2 17.9 29.8 47.9 53.8 78. 87.3 79.3 95.4 92.2 96.3 94.8 45.4 70.1 48.4 58.0 15.3 23.0 58.4 64.8 53.2 61.9 70.7 89.5 88.0 89.3 86.6 66.7 76.1 58.5 69.3 21.1 41.7 63.7 73.6 69.7 78.1 79.6 93.1 85.9 94.0 91.0 34.2 46.0 33.8 37.2 8.4 10.4 36.7 42.3 40. 62.2 50.8 73.9 74.2 76.6 72.3 16.8 29.8 14.9 13.8 5.2 7.6 15.1 13.8 13.2 25.8 28.8 55.7 54.4 51.0 46.5 39.7 50.2 39.6 45.5 20.1 27.4 37.8 41.8 45.2 56.6 55.5 75.2 69.1 74.7 71.6 43.2 43.0 45.3 51.0 49.6 55.8 29.0 36.6 48.3 67. 54.2 46.3 46.8 50.6 44.0 50.9 32.1 38.7 37.4 41.0 52.0 66.3 62.3 80.5 75.7 80.5 78.1 For example, text rendering is currently assessed at the sentence level, whereas character-level accuracy could offer more detailed perspective. (iii) Our benchmark focuses on composition and reasoning with respect to generation semantics, without considering non-semantic aspects such as aesthetics, realism, and diversity. This is because our primary objective is to evaluate the generative faithfulness with respect to prompts across these two fundamental capabilities. Nevertheless, these non-semantic aspects remain valuable directions for future work. (iv) Our benchmark is currently limited to English prompts, while multilingual capabilities remain largely unexplored; extending the benchmark to multiple languages represents an important direction for future work."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Nanyang Technological University",
        "University of Science and Technology of China"
    ]
}