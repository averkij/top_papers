{
    "paper_title": "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs",
    "authors": [
        "Muhammad Khalifa",
        "Yi-Chern Tan",
        "Arash Ahmadian",
        "Tom Hosking",
        "Honglak Lee",
        "Lu Wang",
        "Ahmet Üstün",
        "Tom Sherborne",
        "Matthias Gallé"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging has shown great promise at combining expert models, but the benefit of merging is unclear when merging ``generalist'' models trained on many tasks. We explore merging in the context of large ($\\sim100$B) models, by \\textit{recycling} checkpoints that exhibit tradeoffs among different tasks. Such checkpoints are often created in the process of developing a frontier model, and many suboptimal ones are usually discarded. Given a pool of model checkpoints obtained from different training runs (e.g., different stages, objectives, hyperparameters, and data mixtures), which naturally show tradeoffs across different language capabilities (e.g., instruction following vs. code generation), we investigate whether merging can recycle such suboptimal models into a Pareto-optimal one. Our optimization algorithm tunes the weight of each checkpoint in a linear combination, resulting in a Pareto-optimal models that outperforms both individual models and merge-based baselines. Further analysis shows that good merges tend to include almost all checkpoints with with non-zero weights, indicating that even seemingly bad initial checkpoints can contribute to good final merges."
        },
        {
            "title": "Start",
            "content": "If You Cant Use Them, Recycle Them:"
        },
        {
            "title": "Optimizing Merging at Scale Mitigates Performance Tradeoffs",
            "content": "Muhammad Khalifa 1 2 Yi-Chern Tan 2 Arash Ahmadian 3 Tom Hosking 2 Honglak Lee 1 Lu Wang 1 Ahmet Ust un 3 Tom Sherborne 2 Matthias Galle 2 4 2 0 2 5 ] . [ 1 4 4 1 4 0 . 2 1 4 2 : r Abstract Model merging has shown great promise at combining expert models, but the benefit of merging is unclear when merging generalist models trained on many tasks. We explore merging in the context of large ( 100B) models, by recycling checkpoints that exhibit tradeoffs among different tasks. Such checkpoints are often created in the process of developing frontier model, and many suboptimal ones are usually discarded. Given pool of model checkpoints obtained from different training runs (e.g., different stages, objectives, hyperparameters, and data mixtures), which naturally show tradeoffs across different language capabilities (e.g., instruction following vs. code generation), we investigate whether merging can recycle such suboptimal models into Pareto-optimal one. Our optimization algorithm tunes the weight of each checkpoint in linear combination, resulting in Pareto-optimal models that outperforms both individual models and merge-based baselines. Further analysis shows that good merges tend to include almost all checkpoints with with non-zero weights, indicating that even seemingly bad initial checkpoints can contribute to good final merges. 1. Introduction Model merging is gaining traction as cost-effective alternative to multi-task learning or model ensembling (Wortsman et al., 2022; Yu et al., 2024). While the research on model merging has rapidly advanced in the last few years, it remains limited in terms of both model scale and the type of considered checkpoints. On one hand, most work has studied merging fairly small models (e.g., 7B) by todays standards, and it remains unclear how much benefit merging would bring with much larger models (e.g., 100B+). On the 1University of Michigan 2Cohere 3Cohere For AI. Correspondence to: Muhammad Khalifa <khalifam@umich.edu>. 1 other hand, the setup where merging was mostly applied involved two or more expert models, where experts are independently optimized for specialized tasks, merged to combine their capabilities (Yadav et al., 2024a; Yu et al., 2024; Akiba et al., 2024). The primary motivation for such expert merging is to eliminate the cost of multi-task trainingeach expert can be trained separately, and then later merged for combined expertise (Li et al., 2022). However, expert merging is only reasonable when expert models are available. Departing from that setting, modern large language model (LLM) development scenarios tend to create large quantity of multi-task models. Training general-purpose LLMs relies on training single model on many tasks during supervised finetuning (SFT)/instruction tuning (Chung et al., 2024; Jiang et al., 2023; Achiam et al., 2023; Dubey et al., 2024; Team et al., 2024; 2023). prevalent issue here is that different tasks may conflict with each other (Lin et al., 2019), resulting in tradeoffs among different capabilities. Parameters well suited to one task may conflict or combine poorly with parameters specialized for another (Gueta et al., 2023). For example, aligning model with human preferences can hurt performance on other evaluations (Bai et al., 2022; Vijjini et al., 2024). Similarly, improving math capabilities may come at the cost of other reasoning skills (Fu et al., 2023). One strategy to alleviate these tradeoffs is to carefully tune different training choices until good enough (e.g., Pareto-optimal) model is obtained. This approach is not only computationally expensive but also discards suboptimal models under the assumption that they constitute failed experiments. Our goal is to investigate: Can model merging provide training-free approach to reduce performance tradeoffs between multiple tasks? We adopt realistic setup, using 16 checkpoints obtained from Command R+1 training runs which naturally exhibit tradeoffs among different model capabilities such as code, math, and instruction following. We focus on linear merging for simplicity and apply iterative search to optimize the weighting of each model to the 1https://docs.cohere.com/v2/docs/ command-r-plus Figure 1. An overview of our setup. Given models obtained from different LLM training runs, we optimize linear merging weightings (α1, α2, α3) via iterative search to obtain model with minimal task tradeoffs. Each to designate its performance on the two tasks. The purple color indicates Pareto-optimal model, achieving good balance between the two tasks without being dominated by other models. We show tradeoffs between only two tasks since it is easier to visualize. represents single model, with mixture. 2. Related Work In this paper, we extend model merging approaches by (i) utilizing models orders of magnitudes larger than previous work and, more importantly, (ii) leveraging it to minimize tradeoffs among multiple generalist checkpoints, as opposed to merging only experts. This practical setup considers how to optimally re-use suboptimal intermediate checkpoints in the style of model recycling (Choshen et al., 2022). We make the following contributions: We explore an understudied setup where the goal is to optimize task tradeoffs by merging multiple models obtained from different training runs, with different data mixtures, post-training stages, and objectives (instead of only random seeds or hyperparamerts). This setting mirrors well the real-life scenario of developing frontier model (3.1). We propose to use evolutionary optimization to find merges with minimal tradeoffs, instead of relying on manual tuning the weights, process that can be tedious, unreliable and costly (3.3). We show that such evolutionary optimization can reduce task tradeoffs in twoand three-task setups, outperforming uniform and greedy merging without hurting out-of-domain performance on held-out tasks (5). We analyze the optimal merges obtained via search and identify key lessons (5.3). For instance, we find that merges with the best tradeoffs are the result of merging almost all checkpoints, suggesting that linear merging benefits from seemingly bad checkpoints that perform poorly on the tasks in question. In other words, merging can be optimized to recycle existing checkpoints, enabling training-free optimization of task tradeoffs. We also observe that the performance of merge cannot be determined solely based on the isolated performance of the merged models, where seemingly low-performing checkpoints can lead to optimal merges. 2 Prior work on model merging has mainly considered two axes: merging methods (i.e., how to merge); and in which context merging is useful (i.e., when to merge). Merging methods Recent efforts on model merging methods aim to overcome the limitations of simple averaging (Utans, 1996; Wortsman et al., 2022) with more involved techniques. For instance, Matena & Raffel (2022) use the Fisher information matrix to approximate each models posterior beyond the isotropic assumption made by averaging. Ilharco et al. (2023) propose to merge so-called task vectors instead of full models. Yadav et al. (2024a) propose 3-step process to resolve parameter interference among merged models. Daheim et al. (2023) propose to improve averaging by reducing mismatch in parameter gradients. major downside of most of these methods is that they require access to gradient information, which becomes expensive as the models scale. In addition, other techniques such as DARE (Yu et al., 2024) assume access to shared base model from which experts are fine-tuned, which is limiting in cases with different heterogeneous models as in our setup. Merging context Model merging has been mainly exploited in the context of transfer learning (Matena & Raffel, 2022; Jin et al., 2022; Yadav et al., 2024a; Hammoud et al., 2024; Akiba et al., 2024). In this setup, merging is done across set of experts, specialized models that are finetuned separately on different tasks or domains, and which are then merged to combine their capabilities into single multi-task model (Rame et al., 2024). Merging can also be exploited to ablate on dataset mixtures across domains during pre-training (Na et al., 2024). While intriguing, we argue that this setup may not fully capture the typical largescale LLM training scenarios. In practice, LLMs are often simultaneously trained on multitude of tasks during single training runsuggesting that training is generally not approached as multiple single-task optimization problem but rather as single multi-task optimization problem, e.g., during the supervised fine-tuning or instruction tuning stage (Ouyang et al., 2022; Jiang et al., 2023; Chung et al., 2024; Bai et al., 2023; Achiam et al., 2023; Team et al., 2023). Under these setups, it is more common to have multiple models exhibiting different performance tradeoffs than to have expert models specialized in certain domains. Team et al. (2024) demonstrate this setup and discuss the tuning of SFT hyperparameters (including training data mixture weightings) to minimize tradeoff between helpfulness, safety, and faithfulness. Our work further explores optimizing merging across such checkpoints to minimize performance tradeoffs across critical benchmark tasks. Contrast with prior work In similar spirit to our work, Wortsman et al. (2022) average models trained across different runs. However, their approach is applied to computer vision models and, more importantly, their goal is to maximize performance on single task rather than minimize task tradeoffs. Rame et al. (2024) similarly average LLM policies across different runs to improve alignment reward, but their singular goal is improving reward, contrasting to our exploration of tradeoffs. Their setup is also limited to merging maximum of 5 smaller models, while we explore merging up to 16 100B checkpoints. Dubey et al. (2024) mention that they average checkpoints trained with different hyperparameters and data mixtures, but they do not provide any additional details. Another adjacent recent work is Akiba et al. (2024), using evolutionary optimization techniques to optimize merging across two expert models. They explore parameter-space merging (i.e., merging weights) and data-space merging (i.e., merging information flow across different layers). However, their setup is limited to merging only two 7B models. To summarize, our work sheds light on the underexplored setup where model merging can help recycle existing models beyond the top-few experts. Our work differs from prior work in at least three ways. First, our objective is to minimize performance tradeoffs across different training runs, as opposed to merging expert models. Second, our setup involves merging up to 16 different 100B, making our setup more realistic and aligned with the current scales of LLMs. Third, as opposed to relying on simple averaging (Wortsman et al., 2022) or tuning the weights manually, we leverage evolutionary search to optimize the merging weightings. 3. Optimizing LLM Merging 3.1. Task Conflict Different tasks might conflict with each other in required expertise, resulting in performance tradeoffs where improvement observed over some tasks incurs performance degradation on other tasks (Lin et al., 2019; Wang et al., 2021). For instance in language modeling, aligning model with human preferences could result in an alignment tax, where the model performance degrades on held-out tasks (Bai et al., 2022). Another example is that instruction tuning could hurt 3 the performance on tasks such as question answering and reasoning (Dou et al., 2023). It is well known in practice that different decisions related to hyperparameters, training data mixtures, or long-context training could yield such tradeoffs. One way to minimize these tradeoffs is by carefully tuning the training choices. This involves running several training runs with different hyperparameters and choosing Paretooptimal runs with minimal tradeoffs (Team et al., 2024). However, this is cumbersome and expensive, and becomes less tractable as model parameters increase. We investigate whether it is possible to recycle the models obtained from such failed runs, which are usually abandoned or discarded altogether. We propose to use training-free model merging as an efficient and compute-minimal strategy to combining models. 3.2. The Optimization Problem Given tasks {t1, , tT } and checkpoints {θ1, θ2, ..., θN } which tradeoff across the tasks, we aim to output model θmrg by merging the checkpoints such that all tradeoffs are minimized. We mainly focus on weight interpolation of the model weights, or model soups (Wortsman et al., 2022), which yield merged model by linearly interpolating the model parameters with non-negative weightings. Precisely, the resulting model is computed as weighted sum of the individual model parameters i.e., θmrg = (cid:80)N i=1 αiθi, where αi represents the weighting assigned to θi, subject to constraint (cid:80)N i=1 αi = 1. We limit our study to linear merging (i.e., model soups) for three reasons. First, linear merging is easy to implement and is generally considered strong baseline (Wortsman et al., 2022; Ilharco et al., 2022b; Yadav et al., 2024a). Second, it makes few assumptions about the merged models (e.g., assumes no shared base model (Ilharco et al., 2022a), or access to gradient information (Matena & Raffel, 2022)) enabling easy scaling to large models. Third, while we acknowledge the effectiveness of other merging strategies, recent work (Yadav et al., 2024b) has shown that different merging methods, including linear merging, may converge approximately to the same performance at large model scales. Let Pt(θ) represent the performance of the model θ on task t. Given candidate model θ, we quantify the performance tradeoffs using fitness function R(θ) = R(Pt1(θ), , PtT (θ)) which assesses the balance across tasks to capture all tradeoffs. That is, higher fitness means less severe tradeoffs. Thus, our objective is to find the optimal weightings vector = {α } that minimizes the task tradeoffs and therefore maximizes R: 2, , α 1, α = arg max R(Pt1(θmrg), . . . , PtT (θmrg)) This optimization formulation seeks balanced performance across tasks by adjusting the weights in a. Fitness Function We rely on single-objective optimizations, and therefore we require single score that reflects the performance tradeoff between the given tasks. One choice could be the product of all task metrics or weighted average of the metricsif some tasks are considered more important than others. We will use the unweighted macroaverage of task performances of the merging output θmrg, matching the majority of LLM evaluations for comparison to prior work (Dubey et al., 2024; Team et al., 2024). Over the tasks {t1, tT }, the fitness function of model θ (cid:80) is computed as R(θ) = 1 t{t1,tT } Pt(θ).2 3.3. Search Algorithm There exists plethora of techniques to solve such problems, including Bayesian Optimization (Snoek et al., 2012), random search (Bergstra & Bengio, 2012), and genetic algorithms (Young et al., 2015; Alibrahim & Ludwig, 2021). For the purpose of our study, we focus on Covariance Matrix Adaptation Evolution Strategy (Hansen & Ostermeier, 2001), which has proven its usefulness in hyperparameter optimization (Loshchilov & Hutter, 2016, CMA-ES), including in model merging literature (Akiba et al., 2024), as well as requiring minimal hyperparameter configuration. CMA-ES is an optimization technique suited for continuous, non-linear optimization. It improves solutions by iteratively sampling candidates from multivariate normal distribution, with mean representing the instantaneous optimal solution. At each step, CMA-ES adapts the covariance matrix of the normal distribution over time, based on successful solutions, to assign higher probability to good solutions. CMA-ES is suitable for cases where gradient information is not available or is expensive to obtain. 4. Experimental Setup Merge candidates As candidate inputs to the merge, we select 16 models from the Command R+ (100B) development pipeline, where each model is the result of separate training run. To ensure these models are representative of different training stages, we select such that: 50% are sourced from the supervised fine-tuning (SFT) training stage, and the other 50% from preference optimization (PO) stage. The SFT checkpoints involve different training data: some are trained purely on code, some on code and academic data, and some on the general multi-task SFT setup, etc., The PO checkpoints include variation of the training objective and training hyperparameters (e.g., warmup ratio). One checkpoint is sourced from an earlier version of the Command R+ model. Table 2 in Appendix includes details about each model. We first note that the PO models are not necessarily based on the SFT models; they could potentially be based on different SFT models not included in our candidate models. Second, aside from the criteria above, the checkpoints were selected without any specific bias; we do not intentionally pick checkpoints that exhibit high performance tradeoffs between tasks, as these tradeoffs appear naturally anyway. Our expectation is that our merging strategy can easily apply to other researchers and practitioners who may have many underexploited checkpoints laying around. Tasks We evaluate the merged models on both heldin and held-out tasks. The held-in tasks are the ones for which we wish to minimize the tradeoffs. The heldout tasks serve to verify that the merged model still performs well on tasks outside the held-in tasks. As heldin tasks, we select five different tasks that are representative of different model capabilities. We use MBPP3 to evaluate code performance, GSM8K (Cobbe et al., 2021) for math, IFEval (Zhou et al., 2023) for instruction following, MMLUPro (Wang et al., 2024)4 for multitask language understanding, and MUSR (Sprague et al., 2024) for multistep reasoning. Held-out tasks include MTbench (Zheng et al., 2023) and LBPP (Matton et al., 2024). Evaluation We report pass@1 for code tasks i.e., MBPP and LBPP. We use accuracy for GSM8K, MMLU Pro, and MUSR. For IFEval, we report the prompt-level strict accuracy (Zhou et al., 2023), which measures the percentage of prompts for which all verifiable instructions are followed. All evaluations use greedy decoding. Search details We use the CMA-ES implementation from Optuna (Akiba et al., 2019). The population size is set to 4 + 3 ln (default in the implementation), which aligns with prior work (Akiba et al., 2024). We initialize the search with uniform weights, i.e., αi = 1/N for all i. We set the 3https://github.com/google-research/ google-research/tree/master/mbpp. 2A model achieving the best average task performance is necessarily Pareto-optimal, as no other model can dominate it without achieving better average. 4To speed up experimentation, we use subset of 2K examples from MMLU-Pro. We found the performance on this subset to correlate highly with performance on the full dataset. 4 Figure 2. Performance of individual models over the seven tasks covering different capabilities. Models 1-8 are the result of supervised finetuning runs, while 8-16 from preference optimization. Held-out tasks (MT-Bench and LBPP) are used to evaluate the resulting merges to make sure the merge optimization process does not overfit to the held-in tasks that we aim to minimize tradeoffs over. MT-Bench rating is scaled by factor of 10 for better visualization. The exact numbers are in Table 2 in Appendix A. 2, , α initial standard deviation σ0 = 1.0 and run CMA-ES for 50 iterations in total. At each iteration, CMA-ES proposes single weighting vector {α 1, α }, which we use to merge the individual models and then evaluate the fitness over the resulting merge. To make sure the weightings sum to 1, we first sample unnormalized weights and then normalize by their sum. To avoid cold starting the CMA-ES optimization, we initialize the CMA-ES history with the performance from individual checkpoints results and the two merged baselines detailed below. Baselines To validate whether search can find optimal merges that minimize task tradeoffs, we compare it against the following baselines: Best single model: The individual model with the highest fitness function in addition to the individual model with the highest performance on each task. Uniform soup: Averaging all checkpoints using equal weights (Wortsman et al., 2022). Specifically, the weights are set such that αi = 1 for = 1, . . . , , where is the total number of checkpoints. , , θ tT Merge-best: We average the top-performing checkpoints for each task. Specifically, we merge the checkpoints {θ = arg maxθ Pt(θ) with unit1 form weight. This corresponds to assigning the highest weights to the best performing model on each task and zero weights to the remaining models. } where θ 5. Results and Discussion Before merging any models, we first look at the performance of individual models across different tasks to get sense 5 of the existing tradeoffs. Figure 2 shows the performance of each of the 16 merge candidates over both held-in and held-out tasks. Table 2 in Appendix shows exact numbers. Interesting tradeoffs show up when looking at individual model performances. For instance, SFT models (1-8) exhibit better code performance (i.e., on MBPP and LBPP) compared to PO models (9-16), while PO models seem to perform better than SFT ones on MT-Bench and IFEval. This is tradeoff likely caused by alignment training, which could hurt some other model capabilities (Bai et al., 2022). Now we zoom in on pairwise tradeoffs between two tasks. In this case, it is fairly straightforward to measure the severity of such tradeoffs by computing performance correlation across the models. Figure 4 shows Spearmans rank correlation ρ between all task pairs over our 16 selected models. We observe few strong pairwise tradeoffs between task pairs, such as MBPP-IFEval (ρ = 0.35) and MBPP-MUSR (ρ = 0.40). Throughout this section, we will refer to the tasks with tradeoffs as held-in tasks. The main question we aim to answer here through our experiments is: Can optimizing {α1, α2, , αN } yield θmrg with minimal tradeoffs over the held-in tasks without hurting performance over the held-out ones? 5.1. Optimizing Pairwise Tradeoffs We apply our merge optimization recipe over three task pairs with relatively strong tradeoffs: MBPP-IFEval, MBPPMUSR, and MMLU Pro-IFEval. Figure 3 shows plot for each pair of tasks. Each plot includes best-fit line to the performances of each pair (shown in green), which exhibits negative slope in all three cases due to the respective tradeoff. We observe that baselines Figure 3. Performance tradeoffs with different merging approaches. Shaded areas represent 95% confidence interval of the best-fit line computed over individual checkpoint scores (shown in green). Held-In Held-Out Avg. All Tasks MBPP IFEval Avg. MT-Bench LBPP Highest fitness model Best on MBPP Best on IFEval Uniform Soup Merge-best Search-optimized (ours) 63.0 64.0 56.8 61.8 61.8 63.0 65.7 56.5 72. 70.1 69.7 73.0 60.1 60.3 64.4 66.0 65.8 68.0 7.42 7.68 8.50 8.13 8.38 8.07 30.4 32.9 28. 29.8 32.3 32.3 MBPP MUSR Avg. MT-Bench LBPP Highest fitness model Best on MBPP Best on MUSR Uniform Soup Merge-best Search-optimized (ours) 64.0 64.0 58. 61.8 62.8 63.2 17.0 17.0 21.1 23.6 23.6 25.5 40.5 40.5 39.7 42.7 43.2 44.4 7.68 7.68 7. 8.17 7.88 8.27 32.9 32.9 27.3 31.7 30.4 30.4 MMLU Pro IFEval Avg. MT-Bench LBPP Highest fitness model Best on MMLUPro Best on IFEval Uniform Soup Merge-best Search-optimized (ours) 28.0 31.9 28.0 31.6 32.0 31.6 72.0 59.0 72.0 70.1 71.0 72.6 50.0 45.5 50. 50.9 51.5 52.1 8.50 7.55 7.55 8.19 8.13 8.15 28.0 26.7 26.7 29.2 31.1 31.1 41.6 40.3 41. 42.5 43.0 44.1 30.4 30.4 28.6 31.3 31.2 31.8 34.1 31.3 33.6 34.8 35.6 35.9 Table 1. Performance of different baselines compared to search optimized merge. Held-in tasks refer to tasks included in the fitness function ( 3.2) and held-out tasks are used to validate the quality of the search optimized models. Search yields models with the lowest tradeoffs over the held-in tasks without sacrificing performance on held-out tasks. We highlight single models and merges differently. Evaluations on MMLU Pro use only 2K test examples. such as Uniform Soup and Merge-Best seem to reduce tradeoffs in few cases. However, search-based optimization of the merge always yields model that falls on the Pareto frontieroccasionally outperforming baselines by up to 2.2 average points, as with MBPP-IFEval. In addition, we can see that over MBPP-IFEval and MBPP-MUSR, our search has yielded model that is better than the best individual model on both IFEval (+1.0) and MUSR (+4.4), showing that search optimized merges could improve over the initial candidate models. We also note that Merge-Best baseline, which averages the two best performing models on each task, performs comparably well in some cases (MMLU ProIFEval) but performs below search in the other task pairs. This suggests that merging based on individual model per6 taining and only slightly underperforms the best single model for each task. These results suggest that searchoptimized merging can perform well when scaling the number of tasks. Our approach maintains comparable performance on the held-out tasks, and even improves performance on LBPP compared to the baselines as shown in Table 3 in Appendix B. Figure 4. Spearmans rank correlation between task pairs. It is easy to see how some tasks exhibit strong performance tradeoffs, such as MBPP-IFEval and MMLU-Pro/MUSR. formance sometimes results in suboptimal merges. We dive deeper into the weightings found via search in Section 5.3. Looking at the held-in tasks alone however does not give us the full picture. It is possible that search has overfit to the held-in tasks by yielding merges with minimal tradeoffs, but could perform significantly worse on tasks that were not incorporated into the fitness function, i.e., out-of-distribution tasks. To verify whether this is the case, we evaluate the resulting merge on held-out tasks, namely, MT-Bench and LBPP. As shown in Table 1 the search-optimized merges exhibit comparable performance on the held-out tasksand in some cases even better than baselines. This means search has minimized task tradeoffs over the held-in tasks without compromising performance on other tasks. 5.2. Optimizing Three-task Tradeoffs In practice, production LLMs are expected to be performant at more than two tasks. We consider balancing performance across three tasks: code generation, instruction following, and math reasoning, by using MBPP-IFEvalGSM8K as held-in tasks. We target this combination since IFEval correlates both negatively with MBPP, and positively with GSM8K (as shown in Figure 4), making it challenging combination to optimise. The goal is to identify how our approach scales with more than 2 tasks, due to the exponential growth in choices (and search space) with respect to the number of tasks. Looking at Figure 5, it is evident that the best fitness single model (i.e., highest average performance) performs well on IFEval and GSM8K, but comparably poor on MBPP. The other two baselines, Merge-Best and Uniform Soup, were able to improve the tradeoffs by some degree but exhibit noticeable performance drop on IFEval. While the search-optimized merge is Pareto-optimal model mainFigure 5. Performance of different merge approaches when minimizing the tradeoffs across three tasks: MBPP, IFEVal, and GSM8K. It is clear that search-optimized merging can well balance the performance over the three tasks. Bars corresponding to merging are hatched to differentiate from individual models. 5.3. Analysis We perform further analysis into the dynamics of linear merging: Most models contribute to the best merges. The first question we ask is: What fraction of the initial 16 models contribute to the best solutions? Figure 6 shows heatmap of the top 5 solutions on each task combination. We observe that CMA-ES identifies good solutions which distribute the weightings among almost all checkpoints (dense solution), instead of assigning high weights to small subset of the models (sparse). For example, the top solution assigns very few zero weightings (shown in black in Figure 6) for MBPPMUSR and MBPP-IFEval. Also, while the top solutions for MBPP-IFEVal-GSM8K are slightly sparser in the pairwise case, at least 9/16 weightings are non-zero for the top 5 solutions. This indicates that almost all checkpoints have contributed to the optimal merge. Low performing models may lead to optimal merges. Since the CMA-ES optimization process evaluates many solutions, we can investigate the solutions found through search along with their quality, as measured by their standalone performance. Intuitively, one would expect that high fitness solutions found through search will assign higher 7 Figure 6. Best solutions found via CMA-ES search when optimizing tradeoffs over the pairs MBPP-MUSR (left) MBPP-IFEval (mid) and MBPP-IFEval-GSM8K. We order the weightings over the x-axis based on the fitness of the individual model they correspond to. We observe that top-solutions do not necessarily assign high weights to high-fitness individual checkpoints. For instance, the top solution on MBPP-IFEval assigns considerably high weight to model #9, which exhibits relatively bad tradeoff on the task pair. Figure 7. Fitness vs. CMA-ES iterations when optimizing tradeoffs over task pairs (see Section 5.1). CMA-ES explores the search space to find merge weightings with high fitness or low task tradeoffs. weights to the checkpoints that perform well on the held-in tasks, compared to checkpoints that perform poorly. Interestingly, this is not necessarily the case for the top solutions found by CMA-ES. For instance, the top performing solution for MBPP-MUSR has relatively high weight for α8 = 0.09, even if model #8 performs extremely poorly at MUSR.5 The same holds for MBPP-IFEval pair, where the top solution does not assign particulary high weight to any of the best performing models on MBPP (models #3, #4, or #5). Similarly over MBPP-IFEval-GSM8K, α8 is relatively high in the top solution, while model #8 actually performs the worst on GSM8K. An individual models performance on given task(s) does not reflect the performance of merge that assigns high/low weight to this model. An optimization procedure to find good merges is therefore necessary, since simply assigning the weightings based on the models isolated performance is suboptimal. Similarly, an individual model performance across all heldin tasks is not predictive of its importance in the found solution. In Figure 6, we sort the αi weightings based on the fitness of the corresponding model for each experiment. For example, in the leftmost heatmap corresponding to MBPPMUSR, model #5 has the highest average performance and model #8 has the lowest fitness and so on. If the performance of an individual model would be indicative of its weight in the final merge, we would expect higher weights (more 5In fact, this particular code trained model has 0% accuracy, as shown in Figure 2; It responds to all queries by generating code Figure 8. Merges found via CMA-ES when optimizing MBPPMUSR tradeoffs over 2, 4, 8, and 16 checkpoints. We also show the centroid of each set of experiments (in large markers). Optimizing over more checkpoints (8 and 16) tends to yield less tradeoffs comapred to fewer checkpoints (2,4), showing how recycling more models can outperform recycling fewer checkpoints. reddish) to be concentrated on the left of each heatmap. The fact that this does not happen suggests that hand tuning the merge weightings based on heuristics (e.g., individual model performance, fitness, etc.) is suboptimal, further supporting the perspective of approaching model merging as an optimization problem. 8 Fitness improves with more iterations. We inspect whether CMA-ES effectively optimizes the fitness function as the search progresses by looking at the fitness function development over the course of search. Figure 7 plots the fitness function vs the number of iterations. Each point in the graph is weightings vector proposed by CMA-ES, and the fitness is the average of the held-in task performances of the resulting merge. Over the three pairwise task combinations, it is clear that the average solution fitness improves with more CMA-ES iterations. Recycling benefits from more checkpoints. Including more initial checkpoints obviously extends the search space, which might include better solution, at the expense of larger search space. To study how the merge quality changes with the number of checkpoints, we run CMAES over the checkpoints with best fitness scores, with {2, 4, 8, 16}. The results over MBPP-MUSR, and MBPP-IFEval are shown in Figure 8 (Figure 9 in Appendix plots the same for MBPP-IFEval), where we highlight the centroid for each . As can be seen, with larger the search space is explored more exhaustively and results in centroid checkpoints with better fitness. Smaller exploits some sub-spaces deeper, also resulting in good merges (as can be seen in Figure 9)."
        },
        {
            "title": "Conclusion",
            "content": "In this paper, we present an approach to recycle checkpoints obtained during typical training run of frontier model. While the vast majority of those checkpoints are in general discarded, in this paper we show how to leverage them via search-optimized merging. We show that simple search algorithm focusing on linear merging can yield better, and often Pareto-optimal models with respect to the existing checkpoints. Our research show that it is possible to leverage merging when we have many multi-task trained checkpoints, as opposed to the standard setup of merging experts. surprising finding is that even checkpoints which perform relatively bad on subtasks can contribute to an overall better model. While we relied on simple merging approach, we hope future development will further investigate more involved merging techniques in similar setup to ours via merging as cheaper and training-free approach."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Akiba, T., Sano, S., Yanase, T., Ohta, T., and Koyama, M. Optuna: next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 26232631, 2019. Akiba, T., Shing, M., Tang, Y., Sun, Q., and Ha, D. Evolutionary optimization of model merging recipes. arXiv preprint arXiv:2403.13187, 2024. Alibrahim, H. and Ludwig, S. A. Hyperparameter optimization: Comparing genetic algorithm against grid search and bayesian optimization. In 2021 IEEE Congress on Evolutionary Computation (CEC), pp. 15511559. IEEE, 2021. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Bergstra, J. and Bengio, Y. Random search for hyperparameter optimization. Journal of machine learning research, 13(2), 2012. Choshen, L., Venezian, E., Don-Yehia, S., Slonim, N., and Katz, Y. Where to start? analyzing the potential value of intermediate models. arXiv preprint arXiv:2211.00107, 2022. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Daheim, N., Mollenhoff, T., Ponti, E. M., Gurevych, I., and Khan, M. E. Model merging by uncertainty-based gradient matching. arXiv preprint arXiv:2310.12808, 2023. Dou, S., Zhou, E., Liu, Y., Gao, S., Zhao, J., Shen, W., Zhou, Y., Xi, Z., Wang, X., Fan, X., et al. The art of balancing: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. arXiv preprint arXiv:2312.09979, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 9 Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Specializing smaller language models towards multi-step reasoning. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1042110430. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/fu23d.html. Gueta, A., Venezian, E., Raffel, C., Slonim, N., Katz, Y., and Choshen, L. Knowledge is region in weight space for fine-tuned language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 13501370, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.95. URL https://aclanthology. org/2023.findings-emnlp.95. Hammoud, H. A. A. K., Michieli, U., Pizzati, F., Torr, P., Bibi, A., Ghanem, B., and Ozay, M. Model merging and safety alignment: One bad model spoils the bunch. arXiv preprint arXiv:2406.14563, 2024. Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159195, 2001. Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022a. Ilharco, G., Wortsman, M., Gadre, S. Y., Song, S., Hajishirzi, H., Kornblith, S., Farhadi, A., and Schmidt, L. Patching open-vocabulary models by interpolating weights. Advances in Neural Information Processing Systems, 35: 2926229277, 2022b. Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/forum?id=6t0Kwf8-jrj. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Dataless knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849, 2022. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306, 2022. Lin, X., Zhen, H.-L., Li, Z., Zhang, Q.-F., and Kwong, S. Pareto multi-task learning. Advances in neural information processing systems, 32, 2019. Loshchilov, I. and Hutter, F. Cma-es for hyperparameter optimization of deep neural networks. arXiv preprint arXiv:1604.07269, 2016. Matena, M. S. and Raffel, C. A. Merging models with fisherweighted averaging. Advances in Neural Information Processing Systems, 35:1770317716, 2022. Matton, A., Sherborne, T., Aumiller, D., Tommasone, E., Alizadeh, M., He, J., Ma, R., Voisin, M., GilsenanMcMahon, E., and Galle, M. On leakage of arXiv preprint code generation evaluation datasets. arXiv:2407.07565, 2024. Na, C., Magnusson, I., Jha, A. H., Sherborne, T., Strubell, E., Dodge, J., and Dasigi, P. Scalable data ablation approximations for language models through modular training and merging. In The 2024 Conference on Empirical Methods in Natural Language Processing, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rame, A., Couairon, G., Dancette, C., Gaya, J.-B., Shukor, M., Soulier, L., and Cord, M. Rewarded soups: towards pareto-optimal alignment by interpolating weights finetuned on diverse rewards. Advances in Neural Information Processing Systems, 36, 2024. Rame, A., Ferret, J., Vieillard, N., Dadashi, R., Hussenot, L., Cedoz, P.-L., Sessa, P. G., Girgin, S., Douillard, A., and Bachem, O. Warp: On the benefits of weight averaged rewarded policies. arXiv preprint arXiv:2406.16768, 2024. Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning algorithms. Advances in neural information processing systems, 25, 2012. Sprague, Z., Ye, X., Bostrom, K., Chaudhuri, S., and Durrett, G. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=jenyYQzue1. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Utans, J. Weight averaging for neural networks and local In Proc. AAAI-96 Workshop on resampling schemes. Integrating Multiple Learned Models. AAAI Press, pp. 133138. Citeseer, 1996. Vijjini, A. R., Chowdhury, S. B. R., and Chaturvedi, S. Exploring safety-utility trade-offs in personalized language models. arXiv preprint arXiv:2406.11107, 2024. Wang, Y., Wang, X., Beutel, A., Prost, F., Chen, J., and Chi, E. H. Understanding and improving fairness-accuracy trade-offs in multi-task learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 17481757, 2021. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pp. 23965 23998. PMLR, 2022. Yadav, P., Tam, D., Choshen, L., Raffel, C. A., and Bansal, M. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024a. Yadav, P., Vu, T., Lai, J., Chronopoulou, A., Faruqui, M., Bansal, M., and Munkhdalai, T. What matters for model arXiv preprint arXiv:2410.03617, merging at scale? 2024b. Young, S. R., Rose, D. C., Karnowski, T. P., Lim, S.-H., and Patton, R. M. Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proceedings of the workshop on machine learning in high-performance computing environments, pp. 15, 2015. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. 11 A. Checkpoint details We show the details and task performance of the 16 individual checkpoint we use in the paper in table 2. B. Additional Results Table 3 shows results from our three-task experiment in Section 5.2. Figure 9 includes results with search over different number of checkpoints over MBPP-IFEval from Section 5.3. Figure 9. Merges found via CMA-ES when optimizing MBPPMUSR tradeoffs over 2, 4, 8, and 16 checkpoints. We also show the centroid of each set of experiments. We find that optimizing over more checkpoints (8 and 16) outperforms optimization over fewer checkpoints (2,4), showing how recycling more models can outperform recycling fewer checkpoints. 12 Model ID Info MBPP GSM8K IFEval MMLUPro MUSR MT-Bench LBPP Supervised Finetuning 1 2 4 5 6 7 8 Without MuP Two stage SFT Academic + Code data only. 2 epochs Academic + Code data only. Academic + Code data only. 2 epochs Two stage SFT Two stage SFT Only Code Preference Optimization 9 11 12 13 14 15 16 Light offline Pref Data-filtered. Offline Pref Different Preamble Light offline Pref, with different margin scaling Full offline Pref Specific data mix. Offline Pref Specific data mix. Offline Pref. With warmup Specific data mix. Offline Pref 58.4 54.4 63. 63.0 64.0 60.8 58.2 63.4 57.0 57.4 60.4 56.8 59.0 58. 58.6 58.6 76.6 75.3 79.0 68.2 75.7 76.7 76.6 37. 75.0 74.0 77.0 81.0 75.0 77.0 75.0 74.0 64.1 65.7 66. 59.0 56.5 65.7 66.9 32.1 63.0 63.0 66.0 72.0 66.0 69. 66.0 67.0 29.4 29.0 31.1 31.9 31.1 29.0 29.0 26. 26.8 27.5 28.3 28.0 28.9 28.5 28.9 29.0 18.1 17.9 14. 10.5 17.0 17.1 21.1 0.0 15.0 16.0 18.0 19.0 20.0 20. 21.0 19.0 8.01 7.74 7.42 7.55 7.68 7.80 7.59 4. 7.90 7.85 8.08 8.50 8.22 8.04 8.41 8.37 30.4 25.5 29. 26.7 32.9 31.7 27.3 30.4 24.8 23.0 24.8 28.0 24.2 26. 24.2 24.2 Table 2. Details and performance of the different initial checkpoints used for merging in our experiments. Supervised Finetuning and Preference Optimization models are shown with their respective performance across various benchmarks. Model Held-in Held out Avg. All Tasks MBPP IFEval GSM8K Avg. MT-Bench LBPP Highest fitness model Best on MBPP Uniform Soup Merge best Optimized Merge 56.8 64. 62.4 62.2 63.6 72.0 56.5 68.2 69.3 71.9 81.0 75.7 79.5 80.5 80.9 69.9 65. 70.0 70.7 72.1 7.42 7.68 8.24 8.16 8.21 30.4 32.9 32.3 32.3 33.5 49.52 47. 50.13 50.49 51.62 Table 3. Comparison of model performance across different task pairs. Held-in tasks refer to tasks included in the fitness function ( ??)."
        }
    ],
    "affiliations": [
        "Cohere",
        "Cohere For AI",
        "University of Michigan"
    ]
}