{
    "paper_title": "Adversarial Flow Models",
    "authors": [
        "Shanchuan Lin",
        "Ceyuan Yang",
        "Zhijie Lin",
        "Hao Chen",
        "Haoqi Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts."
        },
        {
            "title": "Start",
            "content": "Shanchuan Lin* Ceyuan Yang"
        },
        {
            "title": "Abstract",
            "content": "x 1 step 4 steps 64 steps 5 2 0 N 7 2 ] . [ 1 5 7 4 2 2 . 1 1 5 2 : r We present adversarial flow models, class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns deterministic noise-todata mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistencybased XL/2 models, while our XL/2 model creates new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using single forward pass, surpassing their 2NFE and 4NFE counterparts. 1. Introduction Flow matching [49, 80] is class of generative models that achieves the current state of the art in multiple domains, including image generation [3, 11, 77] and video generation [13, 38, 63, 76, 86], etc. It formulates the generation problem as the transport of samples from prior distribution to the data distribution. probability flow is established by interpolating the data and the prior samples, and neural network learns the gradient of the flow. At inference time, the sample is iteratively transported by querying the network for gradients, incurring high computational cost. Recent methods accelerate the generation process by formulating the network to directly predict farther positions on the flow instead of the instantaneous gradients. This *peterlin@bytedance.com (a) GANs (b) Flow Matching (c) Adversarial Flow Models (Ours) Figure 1. Models trained on 1D Gaussian mixture. (a) GAN learns an arbitrary transport plan. (b) Flow matching learns deterministic transport plan but have large discretization errors on low sampling steps. (c) Adversarial flow models support any-step training and generation with deterministic optimal transport plan. can be achieved either by distilling from pre-trained flowmatching model [50, 71], or by training from scratch with consistency objectives as new class of generative models [81]. However, despite the goal of single-step or fewstep generation, consistency-based models must still be trained on all timesteps of the flow for consistency propagation. This wastes model capacity and incurs error accumulation. more critical problem is that networks in fewer steps have less capacity to predict the exact transformations of the more-step targets, so using pointwise-matching or even moment-matching losses causes some extent of blurriness. For these reasons, many state-of-the-art few-step generation models still rely on distributional matching methods, especially adversarial training, for final refinement [6, 47]. Adversarial training originates from generative adversarial networks (GANs) [15]. It itself is standalone class of generative models that supports single-step generation. However, adversarial training from scratch often faces stability issues. handful of recent works exploring adversarial training from scratch adopt non-standard architectural 1 designs [21, 23, 26, 108]. Some further rely on the assistance of additional frozen feature networks [23, 26]. When we switch to standard transformer architecture [85], the training simply diverges. As shown in Fig. 1, we discover that one of the key reasons GANs are hard to train is that the adversarial objective alone does not present single optimization target. This is markedly different from other established objectives, such as flow-matching, which has unique ground-truth probability flow predetermined by the interpolation function, and autoregressive modeling, which has ground-truth token probabilities predetermined by the training corpus. In GANs, the generator is tasked to transport samples from the prior to the data distribution, but the adversarial objective only enforces the matching of the data distributions without constraining the transport plan. Therefore, there exist infinite valid transport plans that the generator may pick, influenced by the weight initialization and the stochastic training process. This causes optimization difficulties as the generator keeps drifting during training. In this paper, we propose adversarial flow models, class of generative models that unifies the adversarial and flow families of models under the same framework. Our model is trained by the adversarial objective. Therefore, it naturally supports single-step training and generation without wasting capacities on learning the intermediate timesteps needed by consistency methods. Our model also belongs to the flow family of models. Therefore, it has the same deterministic transport mapping for training stability, and it naturally supports multi-step training and generation. Our method can be trained on standard transformer architectures without modifications, which opens the door for wider adoption. On ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models due to the conservation of modeling capacity, while our XL/2 model creates new best FID of 2.38 under the same 1NFE setting with guidance. Our method also enables fully end-to-end training of 56-layer and 112-layer 1NFE models through depth repetition, and achieves FIDs of 2.08 and 1.94, surpassing the 2NFE and 4NFE counterparts. Furthermore, in the same no-guidance setting, we show that the adversarial objective can even outperform flow matching due to better distribution matching properties. 2. Related Works The acceleration of flow-based models. Early distillation works train few-step student models to match the prediction of the teacher flow model [50, 71, 72, 94]. Consistency model (CM) [79, 81] proposes the use of self-consistency constraint and supports standalone training as new class of generative models. Various follow-up works improve upon CM. Notably, sCM [52] extends the consistency constraint to continuous time to minimize discretization error. iMM [107] incorporates moment matching. Shortcut [12] redefines the boundary condition to allow jumping between arbitrary timesteps. MeanFlow [14] again extends Shortcut to continuous time. However, these methods typically still generate slightly blurry results on large-scale text-toimage/video tasks [54], so distributional matching methods, such as adversarial training [6, 27, 37, 44, 4648, 53, 66, 74, 75, 87, 92] and score distillation [53, 55, 75, 96, 97, 105], are often incorporated in practice. Generative adversarial networks. Early research in GANs [15] has developed many techniques to achieve success on domain-specific datasets [2832, 65, 100]. BigGAN [4] and StyleGAN-XL [73] have further scaled it to ImageNet [70]. However, GANs have fallen out of favor because of their training instability and limited scalability. handful of works exploring large-scale text-to-image generation using GANs still employ convolutional architecture with complex designs [26, 108]. GANs with transformer architectures [85] have been challenging to scale up [22, 24, 40]. Until recently, R3GAN [21] simplifies the adversarial formulation and pushes the state of the art in the ImageNet-64 benchmark using convolutional architecture. GAT [23] further extends it to latent transformer architecture. These works revitalize interest in adversarial training. However, GAT still employs non-standard transformer architecture and uses the assistance of pre-trained feature network. Our work provides framework that unifies adversarial models with flow models and improves upon the training stability of adversarial methods. 3. Method 3.1. Adversarial Training Preliminaries Our method is built on top of generative adversarial networks (GANs) [15], where generator : Rm Rn aims to transport samples from prior distribution Z, e.g. Gaussian, to samples from the data distribution , and discriminator : Rn R1 aims to differentiate real samples from the generated ones. The adversarial optimization involves minimax game where is trained to maximize differentiation while is trained to minimize the differentiation by D. We adopt the relativistic objective [25] as it has better properties for the loss landscape [82] and achieves the current state of the art [21, 23]:"
        },
        {
            "title": "LD\nLG",
            "content": "adv = Ez,x [f (D(x) D(G(z)))] , adv = Ez,x [f (D(G(z)) D(x))] , (1) (2) where () = log(sigmoid()). Additionally, gradient penalties R1 and R2 [69] are added on D. They prevent from being pushed away from equilibrium [21, 57] and impose constraint on the Lipschitz constant of [16]. Directly computing the gradient penalties requires expansive double backward and twice differentiation, so we use finite difference approximation [47]: small λot large (cid:13)D(x) D(N (x, ϵ2I))(cid:13) (cid:13) (cid:13) (cid:21) 2 2 , LD r1 = Ex Ex LD r2 = Ez Ez (cid:3) (cid:2)xD(x)2 (cid:20) 1 ϵ2 (cid:2)G(z)D(G(z))2 (cid:20) 1 ϵ2 2 (cid:3) (cid:13)D(G(z)) D(N (G(z), ϵ2I))(cid:13) (cid:13) (cid:13) (3) (4) (5) (6) (cid:21) 2 , where ϵ is set to small value 0.01. We only compute the penalties on 25% of the samples of batch and observe no performance degradation. To prevent the discriminator logits from drifting unboundedly in the relativistic setting, logit-centering penalty is added, similar to prior work [28]: LD cp = Ez,x (cid:2)D(x) + D(G(z)) 2 (cid:3) . Therefore, the final GAN loss is:"
        },
        {
            "title": "LD\nLG",
            "content": "GAN = LD GAN = LG adv + λgpLD adv, r1 + λgpLD r2 + λcpLD cp, (7) (8) (9) where λgp is searched hyperparameter controlling the gradient penalty scale of both R1 and R2, and λcp is universally set to 0.01. The expectation is obtained through Monte Carlo approximation over minibatches of data during training. The and are updated in alternation. For conditional generation, the condition is given to both networks as G(z, c) and D(x, c). These are abbreviated in writing. 3.2. Single-step Adversarial Flow Models The GAN objective above only enforces G(z) to match the data distribution. However, there are infinite transport plans from to x, and the model is free to learn any arbitrary one. Our proposed adversarial flow models learn deterministic transport plan, which is the same transport plan as flow-matching models with the linear interpolation function. This allows for the existence of single solution, prevents generator drifting, and stabilizes training. In flow matching, the ground-truth probability flow exists prior to model training and is deterministically defined by the data distribution, the prior distribution, the interpolation function, and the loss objective. It is known that when linear interpolation and the squared distance loss are used, this combination establishes transport plan that minimizes the squared Wasserstein-2 (W2 2) distance between the prior and data distributions [49]. Formally, let π(x, z) be valid transport plan, and c(x, z) be cost function, flow matching with linear interpolation and squared distance objective results in the transport plan π(x, z) that minimizes the total transport cost Figure 2. The effect of using different λot scale. λot = 0 is equivalent to GANs. λot being too small fails to escape local minima. λot being too large forces identity output. where the cost function c(x, z) = z2 nition of W2 2: 2 by the defi- (cid:90) Z min π c(x, z) dπ(x, z). (10) Therefore, for adversarial flow models, we first restrict the prior distribution to have the same dimensionality as the data distribution, i.e., x, Rn and : Rn Rn. This is required by all flow-based models and does not reduce the generality of our method. Then we substitute π(x, z) with our generator G(z): (cid:90) min c(G(z), z) dG(z). (11) Our goal is to find that is simultaneously valid transport plan and plan that minimizes the total transport cost under our defined cost function. Since the adversarial objective already enforces the validity of the transport plan, we only need to add an additional optimal transport (OT) loss on G. The loss minimizes the expectation of c(G(z), z) for all z: LG ot = Ez G(z) z2 2 . (12) (cid:21) (cid:20) 1 The objectives in adversarial flow models (AF) become: LD AF = LD adv + λgpLD r1 + λgpLD r2 + λcpLD cp, LG AF = LG adv + λotLG ot . (13) (14) In theory, even an infinitesimal λot will break the symmetry in the loss landscape and create unique global minimum. But in practice, if the value is too small, the optimization may get stuck in local minima, and if the value is too large, it forces the model to output identity, hurting distribution matching, as illustrated in Fig. 2. Therefore, we adopt schedule to decrease λot over time during training. Unlike consistency-based methods, our model does not need to be trained on other timesteps of the probability flow and can be trained for native one-step generation. This saves model capacity, reduces training iterations, and avoids error propagation. Furthermore, for the one-step training case, this formulation completely removes the hyperparameter of choosing timestep sampling and weighting. Our one-step model also completely avoids teacher-forcing. 3 3.3. Multi-step Adversarial Flow Models 3.4. Discriminator Formulation Adversarial flow models can be generalized to multi-step generation and allow the model to jump between arbitrary timesteps on the probability flow. We introduce an interpolation function of the same form as flow-matching models: xt = interp(x, z, t) := A(t)x + B(t)z, (15) where [0, 1]. Although A(t) and B(t) can be any continuous monotonic functions satisfying x0 = x, x1 = z, we adopt linear interpolation, where A(t) = 1t and B(t) = t, to match the W2 2 transport behavior. We change the generator to accept additional source timestep and target timestep as G(xs, s, t), and the discriminator to accept additional target timestep as D(xt, t). During training, xs, xt are obtained by interpolating randomly sampled x, z. The adversarial loss is extended as:"
        },
        {
            "title": "LD\nLG",
            "content": "adv = Exs,xt,s,t [f (D(xt, t) D(G(xs, s, t), t))] , adv = Exs,xt,s,t [f (D(G(xs, s, t), t) D(xt, t))] . (16) (17) The R1 and R2 gradient penalties are substituted accordingly. We omit the approximation form for simplicity:"
        },
        {
            "title": "LD\nLD",
            "content": "r1 = Ext,s,t r2 = Exs,s,t (cid:2)w(s, t) xt D(xt, t)2 (cid:2)w(s, t) G(xs,s,t)D(G(xs, s, t), t)2 (cid:3) , 2 2 (18) (19) (cid:3) . In flow-matching models with linear interpolation, the transport plan between any source and target timesteps marginal distributions also minimizes the W2 2 distance, so the optimal transport loss is generalized as: LG ot = Exs,s,t (cid:20) 1 1 w(s, t) G(xs, s, t) xs2 2 (cid:21) . (20) We empirically find that the following weighting function works well: w(s, t) = max (s t, δ) , (21) where δ = 0.001 is set for numerical stability. During training, the timesteps can be sampled as continuous values: U(0, 1), U(0, s). In this case, the model is trained to support transporting between any arbitrary timesteps < on the probability flow for generation. When and are close, the model behaves like flow-matching models. When they are far, the model behaves like trajectory models. Since directly learns the target distribution through without needing consistency propagation, the model can alternatively be trained to only learn discrete set of timesteps that is needed for specific few-step inference setting, where single-step generation is just special case. This saves model capacity and training iterations. Our framework unifies single-step and flow generation under the same adversarial training setting. It is important not to condition the discriminator on the source sample. Specifically, formulating the discriminator as D(x, z) for single-step, or D(xt, t, xs, s) for multi-step, is incorrect. This is because at training, and are randomly sampled and paired. This formulation incorrectly tells that should be paired with all x, but can only produce single mapping, and it is impossible to achieve such goal, so the training will oscillate or diverge. When searching for hyperparameters, complication we have encountered is gradient magnitude. Specifically, the objective consists of both the adversarial and the optimal transport losses. However, the adversarial loss received from may have varying magnitudes, influenced by the architecture, its weight initialization, the gradient penalty strength, and the adversarial objective. This creates difficulties in finding λot across model sizes. Formally, we break down the gradient of the generator loss with respect to the generator network parameter θ following the chain rule: LG AF θ = = adv LG θ + λotLG ot θ G(z) θ D(G(z)) G(z) (cid:123)(cid:122) discriminator (cid:125) (cid:124) adv LG D(G(z)) (cid:125) (cid:123)(cid:122) (cid:124) adversarial obj. (22) + G(z) θ . λotLG ot G(z) (cid:125) (cid:123)(cid:122) (cid:124) transport obj. The boxed terms are the gradient passed down from that has varying magnitudes. In traditional GANs, only the adversarial loss is used, and adaptive optimizers rescale the magnitude per parameter so is invariant to the absolute scale of the gradient. However, in our case, the magnitude matters because it determines the ratio between the adversarial and the optimal transport losses. Therefore, we propose gradient normalization technique. Specifically, we change the formulation to D(ϕ(G(z))), where ϕ is an identity operator in the forward pass but rescales the gradient in the backward pass: ϕ = (cid:114) LG adv G(z) . (23) EMA( LG adv G(z) 2, β2) The operator ϕ tracks the exponential moving average (EMA) of the gradient norm, normalizes the gradient by the average norm, and rescales by 1 where is the data dimension. It can be seen as an extension of Adam [36] into the backward path, so we use the same β2 as in Adam for the EMA decay. After normalizing the adversarial gradient to unit scale, we can find λot that works well across model sizes. 4 3.5. Better Distribution Matching Uncond p(xt) Cond p(xtc) Not guided Guided Adversarial flow models enable the complete disentanglement of distribution-matching and optimal-transport objectives. Although the optimal-transport objective matches the distribution-matching objective does flow matching, not. Flow matchings squared distance loss objective minimizes the Euclidean distance in the Rn space instead of the semantic distance on the data manifold [43]. As result, in high-dimensional space, new data samples generated by the model through generalization appear elementwise blended instead of semantically morphed from the training data. This is why flow-matching models without guidance generate perceptually out-of-distribution samples. The effect of classifier-free guidance (CFG) [19] is not only low-temperature sampling, but also perceptual guidance [43]. However, CFG entangles perceptual quality with strong conditional alignment. This limits flowmatching models from representing the full data distribution and can make samples unnatural and canonical [33, 45]. Consistency-based methods share the same pathology. Latest works have explored training on more representational latent spaces [41, 103] and using other guidance types [33, 34, 93] to counter this congenital defect. (More in Sec. E.) Adversarial training is different because of the introduction of learnable D. The distance between real and generated distributions is measured through the semantic network, which captures the perceptual and semantic distance [102] instead of the Euclidean distance. Our experiments find that our model can outcompete flow matching in the guidance-free setting. 3.6. Connections to Guidance Having control over sampling temperature and conditional alignment is still desirable property. We show that guidance can be incorporated into adversarial flow models. We use classifier guidance (CG) [9] for conditional generation as an illustration because of its popularity. We visualize an example conditional flow in Fig. 3a and the effect of CFG on flow-matching models in Fig. 3b. Prior adversarial works [26, 73] introduce classifier C(x, c) that predicts p(cx), and train with an additional loss to maximize the classification probability: LG cg = Ez,c [C(G(z, c), c)] , LG cAF = LG adv + λotLG ot + λcgLG cg . (24) (25) However, as Fig. 3c shows, the guided model yields almost identical results to the original. This is because in this particular example, the classes are well separated, so the classifier has clear decision boundary and yields no gradient. This example illustrates the important difference compared to CFG, whose transport is influenced by guidance gradients (a) Original flows (b) Flow matching + CFG Not guided Guided Not guided Guided (c) C(G(z, c), c) (d) C(interp(G(z, c), z, t), t, c) Figure 3. Conditional models trained on 1D Gaussian mixtures. (a) The conditional flow. (b) The effect of CFG on flow-matching models. (c) Adversarial flow models with simple classifier guidance does not work. (d) Adversarial flow models with flow-based classifier guidance match the behavior of CFG. accumulated along the flow instead of at single timestep. The classifier gradients exist on higher timesteps due to the diffusion of class boundaries by interpolation. To obtain the accumulated guidance gradient along the flow, even for single-step training and generation, we switch to time-conditioned classifier C(xt, t, c) that predicts p(cxt) on probability flow. During training, G,D are still trained for single step only. The generated samples G(z, c) are interpolated to random timesteps U(0, 1) before feeding to the classifier: LG cg = Ez,c,z,t (cid:2)C(interp(G(z, c), z, t), t, c)(cid:3) . (26) This maximizes the expectation of classification on all timesteps, giving us similar results to CFG as shown in Fig. 3d. The timestep can be sampled from custom range, which corresponds to performing CFG on selected timesteps. Eq. (24) is special case of Eq. (26) where is always 0. The hyperparameters, i.e. the scale λcg and the range of t, can be optionally amortized into to allow inference-time adjustment. For clarity, our experiment trains offline as an inIf adversarial flow is used for postdependent network. training an existing flow-matching model v, the gradient of an implicit classifier can be derived from following CFG: xt C(xt, t, c) = v(xt, t) v(xt, t, c), (27) LG cfg = Ez,c,z,t (cid:20) 1 (cid:88) G(z, c) ()C(, t, c) (cid:21) , (28) where () is short for interp(G(z, c), z, t). 3.7. Model Architecture We parameterize the single-step or multi-step generator using neural network g. We find that it can work equally well by either formulating it directly: G(z) = g(z), G(xs, s, t) = g(xs, s, t), (29) (30) (a) Multi-step (b) Extra-deep single-step Figure 4. Deep architecture through transformer block repetition. Extra-deep models are trained end-to-end using single-step objective without any intermediate supervision. or as residual: G(z) = g(z), G(xs, s, t) = xs (s t) g(xs, s, t). (31) (32) The latter is closely related to the velocity prediction formulation of existing flow-matching models and consistencybased models. To demonstrate the feasibility of both, we train our single-step models using the direct formulation and multi-step models using the residual formulation. We parameterize directly using neural network d. Both the and networks use standard diffusion transformer architecture (DiT) [62]. For single-step models, the timestep projection is removed. For multi-step models with fixed discretizations, one timestep projection is used. For any-step models, two timestep projections are used in g. The condition is injected through modulation for both and following the original DiT. Our is nearly identical to g, except for the addition of learnable [CLS] token prepended to the input. The [CLS] token is used to produce the logit through final LayerNorm [2] and linear projection. Our architecture has minimal modifications to the original DiT. 3.8. Deep Model Architecture Prior research [47] indicates that the effective model depth is critical for capturing the non-linear transformations required for generating high-quality samples, and the lack thereof is the primary reason causing artifacts on singlestep models. We experiment with end-to-end training of extra-deep single-step models. In theory, extra-deep singlestep models can outperform their multi-step counterparts because the models can pass hidden states end-to-end without projecting to and reinterpreting from the data space, there is no manual definition of timestep discretizations, and the network is trained without teacher-forcing. As illustrated in Fig. 4, our extra-deep models use transformer block repetition [8]. The hidden state after the initial pass is recycled. Timestep-like embedding is still given to the transformer blocks only to differentiate repetition iterations, but the whole network is trained end-to-end for single-forward generation without any intermediate supervision. This design allows us to exactly match the number of parameters and model behavior with the multi-step counterpart for comparison. 6 3.9. Techniques for Adversarial Training We share additional training techniques that are, though imperfect, very effective. They address the remaining challenges of adversarial training, not introduced by our adversarial flow models. On optimization, minimax optimization with gradient descent is prone to oscillation. The equilibrium is better achieved through the weight average than at the last iterate [7]. We find optimistic optimizer [7] and asymmetrical learning rates [18] ineffective. Our approach is to keep an EMA of [95], which consistently outperforms the online model by large margin. More importantly, once the performance of the EMA model plateaus toward the end of the training, we replace online with the EMA weights and repeat automatically after every epoch onward. Learning rate is also decreased during this phase. We find this simple technique very effective for approaching peak performance. On training dynamics, vanilla adversarial training needs and to learn at synchronized pace. If learns too slowly, will separate the real and the generated distributions perfectly, and the gradient vanishes. To address the vanishing gradient problem, WGAN [1] is proposed but requires K-Lipschitz D. DiffusionGAN [90] projects onto flow in the same spirit as our approach with guidance, but this only guarantees support up to the signal-tonoise ratio that can perfectly fool under eachs capacity. Discriminator augmentation (DA) [30] is another approach to increase the distribution overlap used by recent GANs [21, 23]. However, we find that the choice of augmentation may implicitly inject inductive biases. For example, prior work finds that affine transforms outperform other distortions on image data [30] because it implicitly encourages to recognize affine transforms as more acceptable generalization for image generation. The last approach is to simply reload from an earlier checkpoint to reset the pace. Our experiment finds that reloading performs surprisingly well while DiffusionGAN has limited effectiveness. Therefore, we reload when training stalls for the no-guidance setting to avoid introducing any additional biases, and additionally use DA for the guidance setting. 4. Experiments 4.1. Setups and Training Details Experiment setups. We train on class-conditional ImageNet [70] generation to compare with prior works. We follow the protocols to resize the images to 256256 resolution and augment with horizontal flips. We use pretrained variational autoencoder (VAE)1 [68] and train the models in 32 32 4 latent space. Evaluations use Frechet Inception Distance on 50k class-balanced samples (FID1https://huggingface.co/stabilityai/sd-vae-ft-mse FID λgp 0 0.1 λot 0. 0.5 178.224 60.208 0.1 0.25 174.932 54.922 0.5 171.816 73.8511 57.511 62.386 70.158 178.0012 53.901 157.0662 Table 1. Initial λot and λgp. B/2 model 1NFE. FID-50k measured on 20 epochs. Average of two runs with variation labeled in gray. Without optimal transport loss, training diverges. λot Decay FID 0.2 0.2 0.01 Cosine decay over 100 epochs Constant 29.4 8.51 Table 2. Decay of λot. B/2 model 1NFE. FID-50k measured on 100 epochs. Decay is critical for achieving peak performance. 50k) [18] against the entire train set. Training details. We use learning rate of 1 104 and batch size of 256 consistent with prior works [12, 14, 62]. We use AdamW [51] optimizer with β1 = 0, β2 = 0.9 [47]. We set weight decay to 0.01 and EMA decay to 0.9999. We follow MeanFlows definition of model sizes: B/2, M/2, L/2, XL/2, where 2 denotes the patch size. and always use models of the same size. We use separate dataloaders for and D. Epochs are measured as the number of images seen by G. Since different models reach peak FID at different epochs, we report the earliest epoch that reaches the best FID. Additional training details are in the appendix. Classifier guidance. We train the model without guidance till the best FID, and then continue training it with guidance. Our classifier is trained from scratch on ImageNet using the cross-entropy objective for 30 epochs. It uses the same B/2 transformer in the latent space. We do not amortize the scale of CG into models for fair comparison with prior works. Extra-deep models. We only repeat depth on while keeping normal depth. The learning rate of is reduced by the factor of repetition. Extra-deep models are trained end-to-end from scratch following the single-step objective. 4.2. Ablation Studies on the Hyperparameters The effect of optimal transport loss. Table 1 shows grid search of the optimal initial λot and λgp. Without OT loss, the training diverges regardless of λgp, proving the importance of the OT objective on stabilizing adversarial training for transformers. We also observe the phenomenon that an overly small λot fails to regularize the model and an overly large λot hurts distribution matching. Table 2 shows that decaying λot over time is critical for reaching the best FID. The effect of flow-based classifier guidance. Table 3 shows that flow-based CG has minor improvement over CG only at = 0. The optimal range of U(0, 0.1) is much smaller than typical flow matching, likely because adversarial models already produce good samples without guidance. FID 0. 2.48 2.47 2.52 2.46 λcg 0.003 2.40 2.36 2.42 2.45 0.005 2.47 2.49 2.48 2.50 0 U(0, 0.1) U(0, 0.2) U(0, 0.5) Table 3. Classifier guidance scales and timesteps. XL/2 model 1NFE trained till best FID. Flow-based CG is better. 4.3. Comparisons to the State of the Art Single-step with guidance. Table 4 compares singlestep generation with guidance against state-of-the-art consistency-based and GAN models. Under the exact latent space and architectures, denoted by (), our method achieves new best FIDs with large improvement margins across all model sizes, even when compared to concurrent works [23, 91, 101]. Notably, our B/2 model surpasses many XL/2 consistency-based models, likely because B/2 models are tight in capacity, and our method does not waste capacity on other timesteps. Our XL/2 model reaches new best FID of 2.38. Table 8 shows the effect of guidance, and our method with only CG is still the best. Note that StyleGAN-XL has slightly better FID while being smaller, but it operates in the pixel space, while ours is restricted by the VAE and DiT patch size 2. We mainly compare our method against others with the same settings. Few-step with guidance. Table 5 shows that our model also achieves better FIDs in the few-step setting. Ours are trained on designated timesteps to conserve capacity. We find that any-step training performs worse due to the dilution of capacity and batch size. This is also observed in our 1D experiments  (Fig. 1)  where adversarial flow models need larger batch size and converge more slowly for anystep training. This is often not real limitation in practice, as achieving the best performance in designated few-step setting is the priority. No-guidance generation. Table 6 shows that without guidance, even our 1NFE and 2NFE models outperform flow matching using 250NFE+ in the same latent space setting denoted by (). We leave the exploration of combining adversarial flow models with representational methods to future work. However, we are concerned that current representational methods may have an unfair advantage. Since ImageNet is small dataset where each class only contains about 1000 images, models cannot learn the correct generalizations from limited data on classes with large variations. However, DINO [5, 60] is trained on much larger dataset, and using it may leak prior knowledge. Extra-deep models. Table 7 shows that our 56-layer and 112-layer models achieve better FIDs of 2.08 and 1.94, surpassing the 2 and 4-step counterparts. This confirms our hypothesis in Sec. 3.8. The results reveal an important insight: 7 Method Param Epoch Guidance NFE FID Method Param Epoch Guidance NFE FID Consistency-based methods iCT-XL/2 [79] Shortcut-XL/2 [12] MeanFlow-B/2 [14] AlphaFlow-B/2 [101] MeanFlow-M/2 [14] MeanFlow-L/2 [14] MeanFlow-XL/2 [14] TiM-XL/2 [91] AlphaFlow-XL/2 [101] 675M - 675M 250 131M 240 131M 240 308M 240 459M 240 676M 240 664M 300 676M 240 None CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] GANs BigGAN [4] GigaGAN [26] GAT-XL/2+REPA [23] StyleGAN-XL [73] Adversarial flow (Ours) AF-B/2 AF-M/2 AF-L/2 AF-XL/2 cGAN [58] 112M - 569M 480 Match-loss [26] 602M 40 166M - DA + cGAN CG + cGAN 130M 200 CG[9]+DA[30] 306M 120 CG[9]+DA[30] 457M 120 CG[9]+DA[30] 673M 125 CG[9]+DA[30] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 34.24 10.60 6.17 5.40 5.01 3.84 3.43 3.26 2.81 6.95 3.45 2.96 2. 3.05 2.82 2.63 2.38 Flow-matching and diffusion ADM [9] DiT-XL/2 [62] SiT-XL/2 [56] SiT-XL/2+Disperse [89] SiT-XL/2+REPA [99] RAE-XL [103] SiT-XL/2+REPA-E [41] Autoregressive and masking MaskGIT [79] VAR [84] Consistency-based methods iCT-XL/2 [79] TiM-XL/2 [91] Adversarial flow (Ours) AF-B/2 AF-M/2 AF-L/2 AF-XL/2 AF-XL/2 554M 400 675M 1400 675M 1400 675M 1200 675M 800 676M 800 675M 800 None None None None None None None 250 10.94 9.62 250 8.30 250 7.43 500 5.90 250 1.87 250 1.69 250 227M 300 310M None None 8 10 6.18 4.95 675M - 664M 300 None None 130M 170 306M 110 457M 110 673M 120 675M None None None None None 1 1 1 1 1 1 2 34.24 7.11 6.07 5.21 4.36 3.98 2.36 Table 4. Single-step generation on ImageNet 256px. Table 6. No-guidance generation on ImageNet 256px.1235 Method Param Epoch Guidance NFE FID Consistency-based methods iCT-XL/2 [79] Shortcut-XL/2 [12] IMM-XL/2 [107] IMM-XL/2 [107] TiM-XL/2 [91] MeanFlow-XL/2 [14] MeanFlow-XL/2 [14] AlphaFlow-XL/2 [101] Adversarial flow (Ours) AF-XL/2 AF-XL/2 - - - 675M 675M 250 675M 675M 664M 300 676M 240 676M 1000 676M 240 None CFG CFG CFG CFG CFG CFG CFG 2 4 12 22 22 2 2 2 20.30 7.80 7.77 3.99 3.61 2.93 2.20 2.16 CG + DA 675M 95 675M 145 CG + DA 2 4 2.11 2.02 Table 5. Few-step generation on ImageNet 256px.1245 the quality of single-step generation may not be bounded by the training method, but by the depth of the generator. Depth scaling is worthy direction for future research. Computation efficiency. Both consistency and adversarial methods require multiple forward passes per iteration, except adversarial methods commonly use different samples for independent calculations of the expectation of and losses. Counting epochs by is fair comparison of Gs update iterations against consistency methods. We provide breakdown of per-update compute in Sec. C.2. Our XL/2 1NFE model takes 1.88 training compute compared to AlphaFlow but obtains 15% gain in best FID. The additional compute comes from losses and regularizations on D, which can be improved in future work. Additional content. In the appendix, we discuss that our use of CG does not cause data leakage [39]. We also share more details, results, and visualizations. Limitations. Techniques in Section 3.9 can be improved with more principled methods. Current adversarial train8 Method AF-XL/2 AF-XL/2 AF-XL/2 AF-XL/2 Depth Param Epoch Guidance NFE FID 28 (1) 675M 95 CG + DA 2 56 (2) 675M 95 CG + DA 1 28 (1) 675M 145 CG + DA 4 112 (4) 675M 120 CG + DA 2.11 2.08 2.02 1.94 Table 7. Deep architectures on ImageNet 256px. Method AF-XL/2 AF-XL/2 AF-XL/2 AF-XL/2 Param Epoch Guidance NFE FID 1 673M 120 1 673M 125 673M 125 1 673M 125 CG + DA 1 None DA CG 3.98 3.86 2.54 2.38 Table 8. Guidance type comparison on ImageNet 256px. ing uses more compute per iteration. network increases memory consumption. We use CG instead of CFG. We aim to address these in future work. 5. Conclusion Our work proposes framework to unify adversarial and flow modeling. We show that learning deterministic transport greatly stabilizes training. We propose techniques to normalize gradients and incorporate guidance. Our method achieves new best FIDs and demonstrates training on deep architectures. Despite limitations, this offers exciting prospects for future research. 1In tables, () use the same latent space and architecture with parameters only differing by the number of timestep embeddings. 2In tables, () use the same latent space but different architecture. 3In tables, (R) use representational latent space. 4In tables, () are concurrent methods. 5In tables, iCT [79] results are reported by iMM [107]."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank Chaorui Deng, Peng Wang, and Qing Yan for their valuable discussions on the methodology and manuscript preparation. We also thank Yang Zhao and Qi Zhao for their support in developing the dataloading and evaluation infrastructure."
        },
        {
            "title": "References",
            "content": "[1] Martin Arjovsky, Soumith Chintala, and Leon Bottou. In InternaWasserstein generative adversarial networks. tional conference on machine learning, pages 214223. PMLR, 2017. 6, 23 [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. arXiv preprint arXiv:1607.06450, Layer normalization. 2016. 6 [3] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. 1 [4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2, 8, 14 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [6] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. 1, 2 [7] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. arXiv preprint arXiv:1711.00141, 2017. 6 [8] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. 6 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 5, 8, 14, 22, 23, 26 [10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 14 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 22 [12] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. 2, 7, 8, 9 [13] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 1 [14] Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. 2, 7, 8, 14 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1, 2 [16] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Improved training of Dumoulin, and Aaron Courville. wasserstein gans. Advances in neural information processing systems, 30, 2017. 2, 23 [17] Pengsheng Guo and Alexander Schwing. Variational rectified flow matching. arXiv preprint arXiv:2502.09616, 2025. [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6, 7 [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5, 8, 14, 23, 24, 26 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 25 [21] Nick Huang, Aaron Gokaslan, Volodymyr Kuleshov, and James Tompkin. The gan is dead; long live the gan! modern gan baseline. Advances in Neural Information Processing Systems, 37:4417744215, 2024. 2, 6, 23, 24 [22] Drew Hudson and Larry Zitnick. Generative adversarIn International conference on machine ial transformers. learning, pages 44874499. PMLR, 2021. 2 [23] Sangeek Hyun, MinKyu Lee, and Jae-Pil Heo. Scalable gans with transformers. arXiv preprint arXiv:2509.24935, 2025. 2, 6, 7, 8, 14, 20, 21, 22, 23, [24] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make one strong gan, and that can scale up. Advances in Neural Information Processing Systems, 34:1474514758, 2021. 2 [25] Jolicoeur-Martineau. The relativistic discriminator: key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018. 2 [26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1012410134, 2023. 2, 5, 8, 14, 25 [27] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into In European Conference on Computer conditional gans. Vision, pages 428447. Springer, 2024. 2 [28] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 2, 3 [29] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 23 [30] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:1210412114, 2020. 6, 8, 14, 23, 24 [31] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and imIn Proceedings of proving the image quality of stylegan. the IEEE/CVF conference on computer vision and pattern recognition, pages 81108119, 2020. [32] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Aliasfree generative adversarial networks. Advances in neural information processing systems, 34:852863, 2021. 2 [33] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 5, 14 [34] Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091, 2022. 5, 26 [35] Hyunjik Kim, George Papamakarios, and Andriy Mnih. In InternaThe lipschitz constant of self-attention. tional Conference on Machine Learning, pages 55625571. PMLR, 2021. 23 [36] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 4 [37] Jonas Kohler, Albert Pumarola, Edgar Schonfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Thabet. Imagine flash: Accelerating emu diffusion models with backward distillation. arXiv preprint arXiv:2405.05224, 2024. 2 [38] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1 [39] Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo imagenet arXiv preprint Aila, and Jaakko Lehtinen. classes in frechet inception distance. arXiv:2203.06026, 2022. 8,"
        },
        {
            "title": "The role of",
            "content": "[40] Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, and Ce Liu. Vitgan: Training gans with vision transformers. arXiv preprint arXiv:2107.04589, 2021. 2 [41] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking 10 vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. 5, 8, 14 [42] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 14 [43] Shanchuan Lin and Xiao Yang. Diffusion model with perceptual loss. arXiv preprint arXiv:2401.00110, 2023. 5 [44] Shanchuan Lin and Xiao Yang. Animatediff-lightning: arXiv preprint Cross-model diffusion distillation. arXiv:2403.12706, 2024. [45] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are In Proceedings of the IEEE/CVF winter conferflawed. ence on applications of computer vision, pages 54045411, 2024. 5 [46] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxllightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 2 [47] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial postarXiv preprint training for one-step video generation. arXiv:2501.08316, 2025. 1, 3, 6, 7, 20, 21, 26 [48] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. 2 [49] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 3 [50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1, [51] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 [52] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 2 [53] Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy Ma, Xiaohua Xie, and JianHuang Lai. Adversarial distribution matching for diffusion distillation towards efficient image and video synthesis. arXiv preprint arXiv:2507.18569, 2025. 2 [54] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 2 [55] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. 2, [56] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 8, 14, 18 [57] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In International conference on machine learning, pages 3481 3490. PMLR, 2018. 2, 23 [58] Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint arXiv:1802.05637, 2018. 8, 14, 25 [59] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. 23 [60] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 7, 14 [61] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [62] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the IEEE/CVF els with transformers. international conference on computer vision, pages 4195 4205, 2023. 6, 7, 8, 14, 20 [63] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1 [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PmLR, 2021. 25 [65] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In International conference on machine learning, pages 10601069. Pmlr, 2016. 2 [66] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems, 37:117340117362, 2024. 2 [67] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski. Kornia: an open source differentiable computer vision library for pytorch. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 36743683, 2020. [68] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 6, 14 [69] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. Advances in neural information processing systems, 30, 2017. 2 [70] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej ImKarpathy, Aditya Khosla, Michael Bernstein, et al. Internaagenet large scale visual recognition challenge. tional journal of computer vision, 115(3):211252, 2015. 2, 6 [71] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 1, [72] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. Advances in Neural Information Processing Systems, 37:3604636070, 2024. 2 [73] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 2, 5, 8, 14, 23 [74] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [75] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2024. 2 [76] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. 1 [77] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. [78] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrodinger bridge matching. Advances in Neural Information Processing Systems, 36: 6218362223, 2023. 26 [79] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. 2, 8, 14 Improved techarXiv preprint [80] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [81] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya In Proceedings of the Sutskever. Consistency models. 40th International Conference on Machine Learning, pages 3221132252, 2023. 1, 2 [82] Ruoyu Sun, Tiantian Fang, and Alexander Schwing. Towards better global loss landscape of gans. Advances in Neural Information Processing Systems, 33:1018610198, 2020. 2 [83] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception In Proceedings of the architecture for computer vision. IEEE conference on computer vision and pattern recognition, pages 28182826, 2016. 23, 26 [84] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 8, 14 [85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [86] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1 [87] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. 2 [88] Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, et al. Seedvr2: One-step video restoration via diffusion adversarial post-training. arXiv preprint arXiv:2506.05301, 2025. [89] Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. 8, 14 [90] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022. 6, 23 [91] Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, and Lei Bai. Transition models: Rethinking the generative learning objective. arXiv preprint arXiv:2509.04394, 2025. 7, 8, 14, 22 [92] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. 2 [93] Yanbo Xu, Yu Wu, Sungjae Park, Zhizhuo Zhou, and Shubham Tulsiani. Temporal score rescaling for temperature arXiv preprint sampling in diffusion and flow models. arXiv:2510.01184, 2025. 5, 26 [94] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. Advances in Neural Information Processing Systems, 37:7863078652, 2024. [95] Yasin Yazıcı, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay Chandrasekhar. The unusual effectiveness of averaging in gan training. arXiv preprint arXiv:1806.04498, 2018. 6 [96] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 2 [97] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 2 [98] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 23 [99] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 8, 14 [100] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with In Proceedings stacked generative adversarial networks. of the IEEE international conference on computer vision, pages 59075915, 2017. 2 [101] Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, and Ivan Skorokhodov. Alphaflow: Understanding and improving meanflow models. arXiv preprint arXiv:2510.20771, 2025. 7, 8, 14, 22 [102] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5, [103] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 5, 8, 14, 22 [104] Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, and Jun Zhu. Diffusion bridge implicit models. arXiv preprint arXiv:2405.15885, 2024. 26 [105] Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large scale diffusion distillation via score-regularized continuous-time consistency. arXiv preprint arXiv:2510.08431, 2025. 2 [106] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. arXiv preprint arXiv:2309.16948, 2023. 26 [107] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. 2, 8, 14 [108] Jiapeng Zhu, Ceyuan Yang, Kecheng Zheng, Yinghao Xu, Zifan Shi, Yifei Zhang, Qifeng Chen, and Yujun Shen. Exploring sparse moe in gans for text-conditioned image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1841118423, 2025. 2 13 Method Space Param Epoch Guidance NFE FID sFID IS Prec. Rec. Flow-matching and diffusion ADM [9] DiT-XL/2 [62] SiT-XL/2 [56] SiT-XL/2+Disperse [89] SiT-XL/2+REPA [99] RAE-XL [103] SiT-XL/2+REPA-E [41] ADM-G [9] DiT-XL/2 [62] SiT-XL/2 [56] SiT-XL/2+Disperse [89] SiT-XL/2+REPA [99] RAE-XL [103] SiT-XL/2+REPA-E [41] Consistency-based models Shortcut-XL/2 [12] TiM-XL/2 [91] MeanFlow-B/2 [14] AlphaFlow-B/2 [101] MeanFlow-M/2 [14] MeanFlow-L/2 [14] MeanFlow-XL/2 [14] TiM-XL/2 [91] AlphaFlow-XL/2 [101] Shortcut-XL/2 [12] IMM-XL/2 [107] IMM-XL/2 [107] TiM-XL/2 [91] MeanFlow-XL/2 [14] AlphaFlow-XL/2 [101] Pixel LDM [68] LDM [68] LDM [68] LDM [68] DINOv2 [60] Joint-trained Pixel LDM [68] LDM [68] LDM [68] LDM [68] DINOv2 [60] Joint-trained LDM [68] LDM [68] 554M 400 675M 1400 675M 1400 675M 1200 675M 800 676M 800 675M 800 554M 400 675M 1400 675M 1400 675M 1200 675M 800 676M 800 675M 800 675M 250 664M 300 131M 240 131M 240 308M 240 459M 240 676M 240 664M 300 676M - - 675M 250 675M 675M 664M 300 676M 1000 676M 240 Autoregressive, masking, and hybrids MaskGIT [79] VAR [84] VAR [84] MAR [42]"
        },
        {
            "title": "GANs",
            "content": "VQGAN [10] MS-VQVAE [84] MS-VQVAE [84] LDM [68] 227M 300 310M 350 2.0B 350 943M 800 None None None None None None None CG [9] CFG [19] CFG [19] CFG [19] CFG [19] AG [33] CFG [19] CFG [19] None CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] CFG [19] None None CFG [19] CFG [19] 250 250 250 500 250 250 250 2502 2502 2502 5002 2502 2502 2502 1 1 1 1 1 1 1 1 1 4 12 22 22 2 2 8 10 102 64100 BigGAN [4] GigaGAN [26] GAT-XL/2+REPA [23] StyleGAN-XL [73] Pixel Pixel LDM [68] Pixel - 112M 569M 480 40 602M - 166M cGAN [58] Match-loss [26] DA[30]+cGAN[58] CG[9]+cGAN[58] Adversarial flow models (Ours) AF-B/2 AF-M/2 AF-L/2 AF-XL/2 AF-XL/2 AF-B/2 AF-M/2 AF-L/2 AF-XL/2 AF-XL/2 AF-XL/2 (2deep, 56-layer) AF-XL/2 AF-XL/2 (4deep, 112-layer) LDM [68] LDM [68] 130M 170 306M 110 457M 110 673M 120 90 673M 130M 200 306M 120 457M 120 673M 125 95 675M 675M 95 675M 145 675M None None None None None CG[9]+DA[30] CG[9]+DA[30] CG[9]+DA[30] CG[9]+DA[30] CG[9]+DA[30] CG[9]+DA[30] CG[9]+DA[30] CG[9]+DA[30] 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 4 1 10.94 9.62 8.30 7.43 5.90 1.87 1. 4.59 2.27 2.06 1.97 1.42 1.41 1.12 10.60 7.11 6.17 5.40 5.01 3.84 3.43 3.26 2.81 7.80 7.77 3.99 3.61 2.20 2.16 6.18 4.95 1.92 1.55 6.95 3.45 2.96 2.30 6.07 5.21 4.36 3.98 2. 3.05 2.82 2.63 2.38 2.11 2.08 2.03 1.94 6.02 6.85 - - - - 4.17 5.25 4.60 4.49 - 4.70 - 4.09 - 4.97 - - - - - 4.37 - - - - 6.74 - - 100.98 121.50 - - - 209.70 219. 186.70 278.24 277.50 - 305.70 309.40 302.90 - 140.39 - - - - - 210.33 - - - - 151.79 - - - - - - 182.10 - 323.10 303.70 7.36 - - 4. 171.40 225.52 - 265.12 5.31 5.60 5.39 5.40 4.35 5.32 5.20 5.10 4.87 4.33 4.79 4.59 4.54 169.51 178.48 186.21 201.85 235.77 269.18 279.12 277.96 284.18 273.84 298.33 259.66 292.20 0.69 0.67 - - - 0.80 0. 0.82 0.83 0.83 - 0.80 0.80 0.79 - 0.71 - - - - - 0.75 - - - - 0.74 - - 0.80 - 0.82 0.81 0.87 0.84 - 0.78 0.72 0.75 0.77 0.78 0. 0.81 0.81 0.81 0.81 0.82 0.79 0.78 0.79 0.63 0.67 - - - 0.63 0.67 0.52 0.57 0.59 - 0.65 0.63 0.66 - 0.63 - - - - - 0.59 - - - - 0.59 - - 0.51 - 0.59 0. 0.28 0.61 - 0.53 0.49 0.54 0.53 0.52 0.52 0.51 0.50 0.52 0.52 0.55 0.56 0.59 0.56 Table 9. Full metrics on conditional ImageNet 256px generation compared with other notable works. The table only shows methods and their configurations that are the most comparable to ours for comparison. () indicates methods using the same latent space and architecture as ours. Please refer to [9] for metric details. 14 A. Qualitative Results Qualitative results are provided below. Samples are generated using the same seeds across models. Deterministic transport. Deterministic transport behavior is visible, particularly on bald eagle (class 22, first row) and geyser (class 974, last row), where the sky background (blue or white) is usually consistent on the same seed across models. However, details still vary across models. This is expected because (1) different model sizes have different degrees of generalization, (2) minibatch training has stochasticity, and (3) the OT scale is reduced toward the end of the training. (a) B/2 (FID=3.05, IS=269.18) (b) M/2 (FID=2.82, IS=279.12) (c) L/2 (FID=2.63, IS=277.96) (d) XL/2 (FID=2.38, IS=284.18) Figure 5. Uncurated 1NFE ImageNet-256px generation with guidance. Classes are (22) bald eagle, (29) axolotl, (89) cockatoo, (207) golden retriever, (279) white fox, (992) agaric, (972) cliff, (974) geyser. Same seeds used across models. 15 (a) B/2 (FID=6.07, IS=169.51) (b) M/2 (FID=5.21, IS=178.48) (c) L/2 (FID=4.36, IS=186.21) (d) XL/2 (FID=3.98, IS=201.85) Figure 6. Uncurated 1NFE ImageNet-256px generation without guidance. Classes are (22) bald eagle, (29) axolotl, (89) cockatoo, (207) golden retriever, (279) white fox, (992) agaric, (972) cliff, (974) geyser. Same seeds used across models. 16 (a) XL/2 2NFE (FID=2.11, IS=273.84) (b) XL/2 4NFE (FID=2.03, IS=259.66) Figure 7. Uncurated 2NFE and 4NFE ImageNet-256px generation with guidance. Classes are (22) bald eagle, (29) axolotl, (89) cockatoo, (207) golden retriever, (279) white fox, (992) agaric, (972) cliff, (974) geyser. Same seeds used across models. (a) XL/2 56-layer 1NFE (FID=2.08, IS=298.33) (b) XL/2 112-layer 1NFE (FID=1.94, IS=292.20) Figure 8. Uncurated extra-deep 1NFE ImageNet-256px generation with guidance. Classes are (22) bald eagle, (29) axolotl, (89) cockatoo, (207) golden retriever, (279) white fox, (992) agaric, (972) cliff, (974) geyser. Same seeds used across models. 17 Comparisons with flow-matching models. Figure 9 shows comparisons with SiT [56]. We show that the adversarial objective generates perceptually better-looking samples even without guidance, while the flow matching method generates more perceptually out-of-distribution samples without guidance. (a) Ours XL/2 2NFE No-Guidance (FID=2.36) (b) SiT-XL/2 250NFE No-Guidance (FID=8.30) (c) Ours XL/2 2NFE CG+DA (FID=2.11) (d) SiT-XL/2 500NFE CFG=1.5 (FID=2.06) Figure 9. Uncurated comparisons with SiT [56]. Same seeds. SiT uses Euler sampler. Classes are (207) golden retriever, (360) otter, (387) red panda, (974) geyser, (88) macaw, (979) valley, (417) balloon, (279) white fox, (22) bald eagle, (27) eft, (29) axolotl, (63) Indian cobra, (89) cockatoo, (360) otter, (972) cliff, (992) agaric. 18 Figure 10. Latent interpolation. XL/2 1NFE (FID=2.38, IS=284.18). Uncurated. The figure shows G(zt, c), where zt = slerp(z1, z2, t) := cos( π 2 t)z1 + sin( π 2 t)z2, [0, 1], z1, z2 (0, I). 19 Layer visualization. We follow prior works to project every layers hidden features onto the top-3 PCA components [23], and through trained linear projection and decode to images using the VAE [47]. Note that in PCA, because DiT [62] uses absolute positional encoding through input, some PCA of early layers are dominated by sinusoidal encoding. Also, because the PCA of each layer is fitted independently, some layers have different top-3 order, so the color of the visualization can change abruptly despite the underlying features being similar. Clear imagery is formed in the later layers of the model. Unlike [23], we do not impose any manual supervision losses on the intermediate features, and the models still obtain top FIDs regardless. Notice that for the 112-layer model, large number of middle layers seem not to be contributing much in the visualization, but they are indeed effective in improving the final FID. The visualizations may not reveal the full contributions of these layers. (a) B/2 (12 layers, FID=3.05) (b) M/2 (16 layers, FID=2.82) (c) L/2 (24 layers, FID=2.63) (d) XL/2 (28 layers, FID=2.38) (e) XL/2-Deep (56 layers, FID=2.08) (f) XL/2-Deep (112 layers, FID=1.94) Figure 11. Layer inspection. Example 1. Class is (22) bald eagle. The top row is hidden features at every layer projected onto the top-3 PCA components [23]. The bottom row is through trained linear projection layer and decoded by the VAE [47]. 20 (a) B/2 (12 layers, FID=3.05) (b) M/2 (16 layers, FID=2.82) (c) L/2 (24 layers, FID=2.63) (d) XL/2 (28 layers, FID=2.38) (e) XL/2-Deep (56 layers, FID=2.02) (f) XL/2-Deep (112 layers, FID=1.94) Figure 12. Layer inspection. Example 2. Class is (972) cliff. The top row is hidden features at every layer projected onto the top-3 PCA components [23]. The bottom row is through trained linear projection layer and decoded by the VAE [47]. 21 B. Experimental Details Training. Table 10 provides the details of our architecture and constant hyperparameters. Table 11 lists the training schedule of our models. The hyperparameter patterns are what we find along the training process that produce the best FID. We suppose it can be put on an automatic schedule, but we leave it to future work. When EMA reload is used, it is automatically reloaded after every epoch onward. When reload is used, we manually try few checkpoints from different epochs and reload only when the training stalls again. Precision. We use TF32 precision to match most prior works. We notice that some prior works [23, 91] train on BF16, but all adopt modified architectures with the addition of QK-normalization and other changes. We also find that QK-normalization is critical for the training stability in BF16 [11]. However, we do not see major throughput improvement on the 256px latent setting with patch size of 2. Therefore, we stick to no architecture modification and TF32. Guidance. Flow-based guidance as discussed in Sec. 3.6 is used for 1NFE models. For multi-step models, we find it is sufficient to only apply guidance on selected target timesteps as indicated in the table. We find that multi-step models need slightly higher guidance scale to get the best FID and are adjusted as such. For the deep models, we keep the exact setting as any other 1NFE models. When training the classifier, we add affine transforms to data augmentation, and it yields better downstream performance. Config NFE Network B/2 M/2 L/2 XL/2 XL/2-Deep Param Depth Dim Heads Patch size Activation MLP expand Norm 1 2,4 130M 306M 457M 673M 675M 129M 304M 455M 671M 671M G - - 12 12 768 12 - - - - 675M 672M - - 28 28 24 16 16 24 1024 1024 1152 16 16 16 28{2,4} 28 1152 16 22 GeLU 4 Pre-LayerNorm + AdaZero Batch size EMA decay GP scale λgp GP batch ratio GP approx. ϵ Logit-centering λcp AdamW weight decay AdamW betas Precision Data x-flip 256 0.9999 0.25 25% 0.01 0.01 0.01 (0.0, 0.9) TF32 0.5 Table 10. Architectures and constant hyperparameters. Guidance λcg, λot LR G,D EMA Reload Reload Epoch FID B/2 1NFE M/2 1NFE L/2 1NFE XL/2 1NFE XL/2 2NFE XL/2 4NFE XL/2 56L 1NFE XL/2 112L 1NFE None None None 0.003, U(0,0.1) 0.20.01 0.001 0.001 0.001 None None None 0.003, U(0,0.1) 0.20.005 0.001 0.001 0.001 None None None 0.003, U(0,0.1) 0.20.005 0.001 0.001 0.001 None None None 0.003, U(0,0.1) 0.20.005 0.001 0.001 0.001 None None None 0.02, [0] 0.250.005 0.001 0.001 0.001 None None None 0.02, [0,0.25] 0.250.005 0.001 0.001 0.001 1e-4 8e-5 3e-5 5e1e-4 8e-5 3e-5 5e-5 1e-4 8e-5 3e-5 5e-5 1e-4 8e-5 3e-5 5e-5 1e-4 8e-5 3e-5 3e-5 1e-4 8e-5 3e-5 3e-5 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes None None None 0.003, U(0,0.1) None None None 0.003, U(0,0.1) 0.20.005 5e-5,1e-4 0.001 0.001 0.001 4e-5,8e-5 Yes 1.5e-5,3e-5 Yes Yes 1e-5,2e-5 Yes Yes 0.20.005 2.5e-5,1e-4 0.001 0.001 0. 2e-5,8e-5 Yes 2.5e-6,1e-5 Yes Yes 5e-6,2e-5 Yes Yes 150 7.30 155 7.05 170 6.07 200 3.05 100 6.19 105 5.54 110 5.21 120 2.82 85 6.26 105 5.14 110 4.36 120 2.63 5.88 90 110 4.81 120 3.98 125 2.38 75 85 90 4.79 4.34 2.36 2.11 130 5.28 135 3.89 140 2.70 145 2.02 75 80 85 95 4.16 3.41 2.77 2.08 3.78 90 95 3.40 100 2.92 120 1.94 Table 11. Training schedules and dynamic hyperparameters. Evaluation. We perform class-balanced evaluation, where we generate 50 images per class for 1000 classes to constitute the total 50k evaluation samples. Recent works [101, 103] find that this approach reduces the stochasticity in the evaluation process and yields more accurate evaluations of the models. Note that class-balanced evaluation may yield 0.1 FID advantage compared to class-imbalanced counterparts. However, many prior works do not explicitly state the details of their evaluation protocols. We hence report class-balanced FIDs if the work explicitly provides, and otherwise report the original metrics in the work. Our method yields large-margin improvement on FID, especially surpassing the best consistency-based method AlphaFlow [101], which is evaluated in the same classbalanced setting. The FID and other metrics are computed using the code provided by ADM [9]. The FID is computed against the entire training set using the pre-computed statistics by ADM [9]. 22 Discriminator augmentation. Discriminator augmentation (DA) is used along with classifier guidance (CG). DA is directly performed in the latent space. Let ρ denote the augmentation operation, ϕ is the gradient normalization operation, the exact composition for the final adversarial loss is:"
        },
        {
            "title": "LD\nLG",
            "content": "adv = Ez,x,ρ [f (D(ϕ(ρ(x))) D(ϕ(ρ(G(z)))))] , adv = Ez,x,ρ [f (D(ϕ(ρ(G(z)))) D(ϕ(ρ(x))))] . (33) (34) Notice that the same random augmentation ρ must be applied to the real and generated samples as pair in calculating the expectation of the relativistic loss. We only perform integer translation and cutout with fixed probability in the latent space. Algorithm 1 shows our implementation using Kornia [67]. Figure 13 shows the visualization of DA. Algorithm 1 Discriminator augmentation. 1 Sequential( 2 3 4 5 6 7 8 9 10 11 ) RandomTranslate( p=0.4, translate_x=(0, 0.3), translate_y=(0, 0.3), resample=\"NEAREST\", ), RandomErasing(p=0.4, scale=(0.1, 0.5)), RandomErasing(p=0.4, scale=(0.1, 0.5)), RandomErasing(p=0.4, scale=(0.1, 0.5)), Figure 13. Discriminator augmentation. The top row is the original samples. The bottom row is what sees after augmentation. The augmentation is performed in the latent space. The visualization is through the VAE decoder. Data leakage. Prior work [39] finds that using pre-trained ImageNet classifier as backbone when training GANs on FFHQ [29] and LSUN [98] generation tasks cheats the FID evaluation by InceptionV3 [83]. Unlike [21, 23], we do not consider StyleGAN-XL [73] trained on ImageNet fits the data leakage description. Our method further differentiates by only using the classifier for guidance. Since CG is an established approach from ADM [9] and CFG is just CG with an implicit classifier [19], CG should not induce additional advantages. C. Limitations C.1. Adversarial Training The need to gradually reduce the scale of the optimal transport objective λot is theoretically justified. The scale cannot reduce too fast, or cannot fully escape local minima. However, the scale also cannot reduce too slowly, or outlearns G, and the gradient vanishes when can perfectly separate the real and generated distributions. This is problematic because we would prefer an algorithm that guarantees the eventual convergence of given long enough λot decay duration, instead of being sensitive to the decay schedule. WGAN. WGAN [1] proposes using 1-Lipschitz to represent the Wasserstein-1 (W1) distance between the real and generated distributions, which theoretically guarantees gradients for G. WGAN is special case of the relativistic objective where () = (). WGAN-GP [16] proposes adding gradient penalties on linearly interpolated and G(z) samples to limit the Lipschitz constant of D. This is theoretically correct for one-dimensional data, but in high-dimensional spaces, linear interpolation only occupies thin slice of the full space, and thus cannot theoretically guarantee the requirement of WGAN. stronger approach is to directly limit the Lipschitz constant through Ds architecture. Since in practice, only K-Lipschitz is really needed, which is equivalent to W1 scaled by factor of K, we experiment using spectral normalization [59] and L2 attention [35] in the transformer architecture of to set an upperbound on the Lipschitz constant of D, but we find that this setting is too limiting to the expressiveness of and significantly hinders the learning of G. We leave further explorations to future work. DiffusionGAN. An alternative approach is to bridge the gap between the real and the generated distributions. Early approaches add instance noises [57]. DiffusionGAN [90] gives the noise scale as condition, essentially projecting to probability flow, similar to our approach with guidance. However, this only guarantees support up to the signal-to-noise ratio that can perfectly fool under eachs capacity constraint. Our preliminary experiments do not find that DiffusionGAN provides benefits. We leave further explorations to future work. Discriminator Augmentation. Non-leaky discriminator augmentation (DA) is an alternative method to increase the overlapping of the distributions [30]. It offers more flexibility to augment samples in the spatial or frequency domains. However, augmentation filters are often modalityspecific, limiting the generality. For example, affine transforms can be easily applied in the pixel space, but not in the latent space. Additionally, we suspect that the choice of augmentation filters also injects additional inductive biases that encourage the models to learn toward specific forms of 23 generalization. This can actually be very useful thing, especially with limited data, because the proper ways of generalization are impossible to deduce from the dataset alone. However, we choose not to use DA in the no-guidance setting. This avoids introducing any additional unfair advantages against flow-matching methods. We only use DA in the guidance setting to achieve the best performance. We only use integer translation and cutout due to the limitation in the latent space. We only use fixed augmentation probability, though it could be dynamically adjusted [30] for future works. Weaker discriminator. We try gradually increasing dropout in D, but find it not very effective. To our surprise, we find that reloading an older checkpoint of is very effective in keeping improving the FID of G. However, we acknowledge that this is not principled approach. Overall, we would like an algorithm to guarantee Gs eventual convergence given an arbitrarily strong and optimal without introducing additional modality-specific augmentations. Although our work improves the stability and formulation of G, we believe the distribution-matching aspect of is still worthy of further research. Connections to score distillation. Score distillation [55] is another distribution matching objective that can be used to train one-step generators from pre-trained flow-matching model. Current score distillation approaches yield one-step generators with arbitrary transport plans, the same problem as GANs. Our optimal transport objective can also be applied to regulating the one-step generator to learn deterministic mapping when using score distillation. We leave further explorations to future work. C.2. Computational Efficiency As discussed in the main text, both consistency and our methods require multiple forward passes per update iteration. The difference is that consistency-based models use the same data samples across all the forward passes, while adversarial methods have traditionally used different data samples to independently calculate the expectation of the and losses. We count the epoch by as fair reflection of the number of optimizer update steps taken by G. Since our model only needs to be trained on selected timesteps, our indeed reaches peak performance using fewer optimizer update steps, given the same learning rate and batch size compared to consistency models. However, when we analyze the computation needed per Gs update step, current adversarial methods are still at disadvantage due to the excessive computation and regularizations needed by D. This is limitation of the adversarial formulation, not introduced by our adversarial flow. Table 12 shows the breakdown of the computation per update. Current adversarial approach consumes 3.625 compute per Gs update compared to MeanFlow. Adjusting the reduced iterations needed, on XL/2 models, our method uses 1.88 compute compared to MeanFlow but obtains 15% FID improvement. For adversarial methods, it may be possible to reduce the batch size on D, apply clever result reuse, explore other methods to limit the Lipschitz constant other than using gradient penalties, and explore other objective functions than the relativistic one to save compute. But for our research, we take the most conservative approach and mostly follow the prior state-of-the-art GANs formulation [21, 23]. We leave the optimization to future work. Generator Discriminator Total Forward Backward Forward Backward Speed (Adj. by epoch) Flow Matching MeanFlow Adversarial Flow + 2D G2 + D1 + 2.5D 2.5D2 14.5 G2 + Gjvp G2 3 4 - - - - 4.38 1 1.88 (1) denotes Table 12. Computation analysis. No guidance. backward pass only compute gradient regarding to input, which (2) denotes backward consumes 1 compute as forward pass. pass also compute gradient regarding to parameters, which consumes 2 compute as forward pass. 2.5=1+1+0.25+0.25, where the 0.25s are the R1 and R2 gradient penalties computed on 25% of samples. Speed is adjusted by best XL/2 epoch: FM: 1400 epochs, MF: 240 epochs, AF: 125 epochs. D. Guidance Details Multi-step guidance. The main text only describes our flow-based guidance approach for single-step generation. Since we find out that we do not need very strong guidance for ImageNet generation, our multi-step models simply apply guidance by setting = t: LG cg = Exs,s,t,c [C(G(xs, s, t, c), t, c)] . (35) But for completeness, we discuss possible ways to still project it onto flow. When given xt, we may not always be able to derive xt as drawn from the marginal distribution at timestep if the interpolation function is not Markov. Linear interpolation is an example. We can, however, form new probability flow between xt and x1 by taking the marginal distribution at xt as the source distribution. The classifier conditions on both the target and projected as C(x t, t, t, s): LG cg = Exs,s,t,c,z,t (cid:2)C(interp(G(xs, s, t, c), z, t), t, t, c)(cid:3) . (36) Derivation of an implicit classifier. We show the derivation of Eq. (39) in the main text. CFG [19] shows that an implicit classifier can be derived following Bayes rule: log p(cxt) = log p(xtc)p(c) p(xt) = log p(xtc) + log p(c) log p(xt) log p(xtc) log p(xt) (37) 24 Flow-matching models v(xt, t) = x1 x0 is proportional score xt log p(xtc). Therefore, implicit classifier is derived: predicting velocity to the negative the gradient of an xt C(xt, t, c) = xt log p(cxt) xt log p(xtc) xt log p(xt) (v(xt, t, c)) (v(xt, t)) = v(xt, t) v(xt, t, c) (38) Given the gradient of an implicit classifier, we want to directly pass it down the generator in the backpropagation process. This can be achieved using the constant multiple rule trick, where (x) = ax, (x) = a. So we multiply the gradient of the implicit classifier by the generator output: LG cfg = Ez,c,z,t (cid:20) 1 (cid:88) G(z, c) ()C(, t, c) (cid:21) , (39) where () is short for the flow interpolation process interp(G(z, c), z, t) before passing to the implicit classifier. Summation is taken to produce scalar for loss backpropagation. Negative is used to invert loss minimization for classification maximization. Implicit guidance in prior GAN works. Many techniques used in prior GAN works are, in fact, classifier guidance. Our work explicitly labels them for clarity. Multiple GAN works use cGAN discriminator architecture [58]. This architecture resembles CLIP [64], where the inner product between the visual embedding and the class embedding is computed at the end and maximized along with the adversarial objective. This is an implicit form of classifier guidance, as this architecture clearly does not apply to unconditional generation tasks. GigaGAN proposes matching loss [26], which trains the to evaluate class alignment in addition. When training G, this implicitly encourages to generate samples that maximize classification. Hence, it is also an implicit form of classifier guidance. E. On Distribution Matching Why does flow matching generate bad samples? Flow matching defines an interpolation function between samples and and learns the gradient (velocity) of the interpolated sample xt as vt: xt = A(t)x + B(t)z = (1 t)x + tz, vt = A(t)x + B(t)z = + z. (40) (41) The model learns the expected velocity of all random It is trained to minimize the following loss obpairings. jective with : Rn R1 as the distance function. can be any strictly convex function in principle, and flow matching adopts the squared L2 distance: LFM = Ex,z,t [d (v(xt, t), vt)] (cid:2)v(xt, t) vt2 = Ex,z,t 2 (cid:3) . (42) If given enough capacity, the model will learn to memorize the exact mean of vt for all xt. Such model overfits to the ground-truth probability flow and only generates the training samples at inference. But in practice, models have limited capacities, so they do not memorize the exact vt but regress to generalized approximation of it. This enables the generation of new data samples through generalization. However, the way of generalization is influenced by the distance function d. With squared L2, generalized samples are encouraged to minimize the Euclidean distance to the training data, instead of the semantic distance on the actual data distribution manifold. This causes the generated samples to look out-of-distribution. For images, they appear pixel-blended instead of semantically morphed from the training data, as shown in Fig. 9b. From the perspective of diffusion. In flow matching, can be any strictly convex function. The use of the squared L2 has special interpretation from the diffusion model perspective. In DDPM [20], the forward diffusion process is defined as Markov Gaussian process: q(xtxt1) := ((cid:112)1 βtxt1, βtI). (43) The transition to xt = (cid:81)t i=1 q(xixi1) has closed form: q(xtx0) := ( atx0, (1 at)I), (44) where at = 1 βt, and at = (cid:81)t s=1 as. The reverse process is also Gaussian, where the ground-truth posterior is given as: q(xt1xt, x0) := (µ(x0, xt, t), β(t)I), where: µ(x0, xt, t) = β(t) = at1βt 1 at 1 at1 1 at x0 + βt. at(1 at1) 1 at xt, This can be modeled by neural network: (45) (46) (47) pθ(xt1xt) := (µθ(xt, t), Σθ(xt, t)I). (48) KL divergence is chosen as the loss objective: LDDPM = Ex0,t [DKL(q(xt1xt, x0)pθ(xt1xt))] . (49) The KL divergence between two Gaussian distributions has closed-form solution. The original DDPM only parameterizes µθ, which simplifies to the squared L2 objective if omitting the additional weighting factor: LDDPM Ex0,t (cid:2)µθ(xt, t) µ(x0, xt, t)2 2 (cid:3) . (50) trained with adversarial and LPIPS [102] perceptual objectives. We suspect that representational latent spaces have more semantic features where the Euclidean interpolations can be better decoded. This may be part of the reason, but we defer to future research. Connections to evaluation metrics. FID metric measures the distance between two distributions of images by comparing their representations in the feature space of pretrained network, i.e. Inception-V3 [83]. Therefore, FID also measures the semantic distance instead of the Euclidean distance. Thus, adversarial, CG, and CFG approaches all help improve FID. F. Additional Properties of Adversarial Flow (a) Flow (b) FM 64-step (c) AF 1-step (d) AF 4-step Figure 14. Transport between arbitrary distributions. (a) the probability flow between mixtures of isolated Gaussian distributions. (b) flow matching fails to transport samples. (c) and (d) show that adversarial flow can learn the correct transport. Flow matching can fail when transporting between two probability distributions where both are multi-modal. Figure 14a shows such an example, where the distributions on both ends of the probability flow are mixtures of isolated Gaussians. Such probability flows lack support in the intermediate timesteps. As shown in Fig. 14b, flow matching fails to properly transport samples by learning the mean velocity. This problem also exists in consistency-based models. Additional techniques are needed to transport between arbitrary distributions [17, 78, 104, 106]. On the other hand, Figures 14c and 14d show that adversarial flow models can learn the transports between arbitrary distributions, if the discriminator provides proper support for the distribution matching. The DDPM derivation follows the same limitation as that of flow matching. Specifically, the model will overfit to the training data if it fits the expectation of the ground-truth posterior exactly, or it will generalize to new samples influenced by the chosen divergence objective. The divergence objective can be arbitrary, and KL is only chosen for convenience. KL divergence maximizes likelihood, but the generated samples are perceptually out of distribution. Why does adversarial work? If can be an arbitrary function, then it should be learned instead of being manually designed. Adversarial training involves learnable discriminator as the distance function : Rn R1. In the adversarial minimax game. is trained to maximize the distance between real and generated samples, so learns to put more weight on the most important and discriminative features. is then updated according to the gradient of D, which is highly non-linear unlike the Euclidean distance objective. We believe having learnable Rn R1 distance function is critical for generating perceptually correct samples. This justifies our experimental results where the adversarial models significantly outperform flow matching in the no-guidance setting. Prior works also find adversarial post-training improves the realism [47, 88]. Why does CG work? Classifier guidance (CG) [9] introduces learned classifier network : Rn R1 whose gradient is added on v(xt, t). We believe the presence of learnable Rn R1 network is critical. Its backward gradient R1 Rn is non-linear and guides the generation toward maximizing p(cxt) as evaluated by the semantic network, which helps the samples to appear semantically and perceptually correct. Discriminator guidance [34] and reward models are other examples of using learned Rn R1 networks to help generation quality. On the other hand, only adjusting the sampling temperature alone without CG is not sufficient [93], indicating that the effect of CG is not just temperature sampling. Why does CFG work? Recall in CG, the classifier network Rn R1 predicts p(cxt) and it is its backward gradient R1 Rn that is added to the Rn velocity prediction. Classifier-free guidance (CFG) [19] is derived from the Bayes rule where xtC(xt, t, c) = v(xt, t)v(xt, t, c) is the gradient of the implicit classifier, as previously discussed in Eq. (38). Therefore, despite not having an explicit Rn R1 classifier, the conditional and unconditional Rn Rn flow models together provide the gradient of an implicit classifier Rn R1 Rn. Why does representational latent space help? If flow matching is performed in latent space where the Euclidean interpolations still yield valid latents that can be subsequently decoded into perceptual image by the decoder, then it can alleviate the limitation of flow matching. This offloads the responsibility to the decoder, which is often 26 G. Implementation The PyTorch [61] implementation is provided in Algorithms 2 and 3 for reference. Algorithm 2 Adversarial Flow. = t.view(-1, 1, 1, 1) return (1 - t) * + * 1 from torch.nn import functional as 2 3 4 def interpolate(x, z, t): 5 6 7 8 9 def training_step( gen, dis, gen_trainable_params, dis_trainable_params, samples, conditions, noises_src, noises_tgt, noises_r1, noises_r2, timesteps_src, timesteps_tgt, mode=\"dis\", pred_type=\"x\", gp_scale=0.25, gp_bsz_ratio=0.25, gp_eps=0.01, cp_scale=0.01, ot_scale=0.2, augment_fn, grad_norm_fn, cg=None, noises_cg=None, timesteps_cg=None, cg_scale=0.003, cg_flow=True, assert mode in [\"dis\", \"gen\"] assert pred_type in [\"x\", \"v\"] for in gen_trainable_params: p.requires_grad_(mode == \"gen\") for in dis_trainable_params: p.requires_grad_(mode == \"dis\") samples_src = interpolate(samples, noises_src, timesteps_src) samples_tgt = interpolate(samples, noises_tgt, timesteps_tgt) samples_tgt_pred = gen(samples_src, conditions, timesteps_src, timesteps_tgt) if pred_type == \"v\": samples_tgt_pred = samples_src - (timesteps_src - timesteps_tgt).view(-1, 1, 1, 1) * samples_tgt_pred samples_tgt_real_aug, samples_tgt_pred_aug = augment_fn(samples_tgt, samples_tgt_pred) weighting = (timesteps_src - timesteps_tgt).abs().clamp_min(0.001) if mode == \"dis\": bsz = len(samples) gp_bsz = max(round(bsz * gp_bsz_ratio), 1) gp_scale = gp_scale * weighting[:gp_bsz] / gp_eps**2 samples_tgt_real_gp = samples_tgt_real_aug[:gp_bsz] + gp_eps * noises_r1[:gp_bsz] samples_tgt_pred_gp = samples_tgt_pred_aug[:gp_bsz] + gp_eps * noises_r2[:gp_bsz] logits_real = dis(samples_tgt_real_aug, conditions, timesteps_tgt) logits_pred = dis(samples_tgt_pred_aug, conditions, timesteps_tgt) logits_real_gp = dis(samples_tgt_real_gp, conditions[:gp_bsz], timesteps_tgt[:gp_bsz]) logits_pred_gp = dis(samples_tgt_pred_gp, conditions[:gp_bsz], timesteps_tgt[:gp_bsz]) dis_loss_adv = F.softplus(-(logits_real - logits_pred)).mean() dis_loss_r1 = (logits_real_gp - logits_real[:gp_bsz]).square().mul(gp_scale).mean() dis_loss_r2 = (logits_pred_gp - logits_pred[:gp_bsz]).square().mul(gp_scale).mean() dis_loss_cp = (logits_real + logits_pred).square().mul(cp_scale).mean() return dis_loss_adv + dis_loss_r1 + dis_loss_r2 + dis_loss_cp else: logits_pred = dis(grad_norm_fn(samples_tgt_pred_aug), conditions, timesteps_tgt) logits_real = dis(samples_tgt_real_aug, conditions, timesteps_tgt) gen_loss_adv = F.softplus(-(logits_pred - logits_real)).mean() gen_loss_ot = (samples_tgt_pred - samples_src).square().mean([1,2,3]).mul(ot_scale / weighting).mean() gen_loss_cg = 0.0 if cg is not None: samples_tgt_pred_cg = ( interpolate(samples_tgt_pred, noises_cg, timesteps_cg) if cg_flow else samples_tgt_pred ) logits_cg = cg(samples_tgt_pred_cg, timesteps_cg) gen_loss_cg = F.cross_entropy(logits_cg, conditions, reduction=\"none\").mul(cg_scale).mean() return gen_loss_adv + gen_loss_ot + gen_loss_cg 27 10 11 12 13 14 15 16 17 ): 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 Algorithm 3 Gradient Normalization. def __init__(self, ema_decay=0.9, eps=1e-8, target_scale=1.0): @staticmethod def forward(ctx, x, square_avg, ema_decay, eps, target_scale): ) def forward(self, x): return _GradientNormalizationFn.apply( x, self.square_avg, self.ema_decay, self.eps, self.target_scale super().__init__() self.ema_decay = ema_decay self.eps = eps self.target_scale = target_scale self.register_buffer(\"square_avg\", torch.tensor(0.0)) 1 import torch 2 import torch.distributed as dist 3 4 5 class GradientNormalization(torch.nn.Module): 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class _GradientNormalizationFn(torch.autograd.Function): 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @staticmethod def backward(ctx, grad_output): square_avg = ctx.square_avg ema_decay = ctx.ema_decay eps = ctx.eps target_scale = ctx.target_scale ctx.square_avg = square_avg ctx.ema_decay = ema_decay ctx.eps = eps ctx.target_scale = target_scale return x.clone() square_avg.lerp_(grad_sq_sum, 1 - ema_decay) scale = square_avg.sqrt() + eps grad_output = grad_output * (target_scale / scale) return grad_output, None, None, None, None dist.all_reduce(grad_sq_sum, op=dist.ReduceOp.AVG) # Here, we compute avg not sum for distributed training. # This is only to exchange the local grad_sq_sum. # We still want it to be in local scale, not global scale. if dist.is_initialized(): # Multiply here is equivalent to divide by sqrt(n) later in the paper. grad_sq_sum = grad_output.square().sum() * grad_output.numel()"
        }
    ],
    "affiliations": [
        "Bytedance"
    ]
}