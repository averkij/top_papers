{
    "paper_title": "AgroBench: Vision-Language Model Benchmark in Agriculture",
    "authors": [
        "Risa Shinoda",
        "Nakamasa Inoue",
        "Hirokatsu Kataoka",
        "Masaki Onishi",
        "Yoshitaka Ushiku"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), a benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers a state-of-the-art range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal that VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most open-source VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/AgroBenchPage/ ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 1 5 0 2 . 7 0 5 2 : r AgroBench: Vision-Language Model Benchmark in Agriculture Risa Shinoda1,2,4, Nakamasa Inoue3,4, Hirokatsu Kataoka4,5, Masaki Onishi4, Yoshitaka Ushiku6 1The University of Osaka 2Kyoto University 3Tokyo Institute of Technology 4National Institute of Advanced Industrial Science and Technology (AIST) 5Visual Geometry Group, University of Oxford 6OMRON SINIC Figure 1. We present AgroBench (Agronomist AI Benchmark) designed to comprehensively evaluate 682 disease categories across 203 agricultural crop types for 7 vision-language question-answer tasks. In the era of larger-scale vision-language models (VLMs), our AgroBench is obviously non-trivial in terms of many more crop and disease categories with all expert annotations for establishing QA benchmarks in the agricultural domain."
        },
        {
            "title": "Abstract",
            "content": "Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers state-of-theart range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capaIn our evaluation on AgroBench, we reveal that bilities. VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most opensource VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/ AgroBenchPage/. 1. Introduction Agriculture is fundamental process for humans to produce crops to live and stay healthy. With the development of computer vision technology, effective and automated management of external crop factors such as diseases and pests has been explored, contributing to stable crop 1 Dataset Annot. Crop Weed Disease Pest Images QA pairs Main Purpose Agri-LLaVA [48] GPT-4 GPT-4 AgroInstruct [4] GPT-4 CDDM [20] AgroBench (Ours) Expert 29 174 15 203 - 4 108 109 74 60 112 12 134 391k 70k 137k 391k (Synthetic) 70k (Synthetic) 1M (Synthetic) Training Training Training 3, 4,342 (Expert) Evaluation Table 1. Comparison of agricultural vision datasets. Expert refers to human expert in this context. AgroBench provides comprehensive evaluation framework for assessing VLMs, featuring multiple tasks and wide range of categories. production. This includes detecting and classifying undesirable conditions like diseases [1, 10, 13, 34, 38, 41, 47] and pests [3, 49, 53], as well as general plant management tasks such as crop classification [52, 54] and recognition of crop maturity [22, 25, 37, 39], and structure understanding [21, 43]. To maintain stable crop production, it is important to recognize wide range of crop conditions, including undesirable situations such as diseases and pests, and to know how to respond appropriately. single visual model that can handle various conditions, not only disease and pest detection but also treatment and general crop management, would be highly beneficial. However, most existing approaches use task-specific models. These models usually require large amounts of training images and manual annotations for each task. As result, farmers often need to use several different models depending on the situation. This increases complexity and makes the overall system less accessible for practical use in agriculture. For general purpose visual tasks, vision-language models (VLMs) [17, 19, 32, 33, 45, 46, 50] have become widespread recently because they can understand task definitions provided by natural language prompts, covering wide range of applications without the need for taskspecific model training. The recognition ability of VLMs is closely connected to image recognition itself and linked to an objects words, and supports open-vocabulary recognition through web-scale training. Here, zero-shot recognition and few-shot adaptation have also been realized by VLMs with language representations. This ability opens up wide range of applications; therefore, we believe this can be applied to agricultural scenarios as well. It is worth noting that VLMs offer an easy-to-use interface for the general public, especially the question-answer (QA) and conversation modes. To investigate the range of tasks that VLMs obtained through large-scale vision-language training can cover, recent studies have introduced various benchmark datasets to fully evaluate the VLMs, including tasks such as understanding diagrams and charts [26, 28], and question answering requiring specialized knowledge [7, 55, 56]. However, VLM research remains underexplored in agriculture due to lack of benchmark datasets that include diverse tasks and categories in agriculture. Several pioneering works [20, 48] have adapted open-source VLMs such as LLaVA [18] to the agricultural domain by fine-tuning them on synthetic datasets generated from closed-source VLMs such as GPT-4o. This is recognized as an effective approach because black-box VLMs often possess certain amount of agricultural knowledge obtained from the Internet. While this helps in generating responses that are generally acceptable to experts, there is almost no way to verify whether the answers are truly correct. Moreover, in the limited evaluation of categories, we are insufficient for fully assessing VLM knowledge; whether they can answer wide range of types, such as diseases, pests, and weeds. These limitations motivated us to develop benchmark dataset in order to evaluate the VLMs broad knowledge in the agricultural domain and its applicability as practical application. Here, we introduce AgroBench (Agronomist AI Benchmark), comprehensive benchmark dataset for VLM for the agricultural domain, covering state-of-the-art range of categories for agriculture-focused benchmark datasets for VLM; 682 disease, 134 pest, 108 weed, and 203 crop categories (Figure 1). We cover not only identification tasks but also crop production and disease management knowledge. Moreover, we include other important topics related to crop management with 98 machine categories and 77 traditional management methods to investigate more about VLMs ability. We carefully selected benchmark tasks from key agricultural engineering research areas, and also the tasks that address challenges faced by farmers in real agricultural scenarios. We employ multiple-choice format, and all questions are annotated by human agronomist experts, which overcomes the limitations of previous synthetically created datasets. We carefully selected images from Creative Commons-licensed and publicly available datasets, including real farm conditions. Also, under our manual annotation process, unclear images were removed. As shown in Table 1, AgroBench contains data with the most diverse disease and crop variation categories and many more crop/weed annotations. Our main contributions are as follows; We developed AgroBench, benchmark dataset for VLM, to assess their broad agricultural knowledge of VLMs and evaluate their applicability in practical applications. In our AgroBench evaluation, VLMs tend to achieve high performance in disease and crop management tasks; however, there is still room for improvement in weed and disease identification. Our AgroBench enables error analysis by providing broader category annotations, highlighting future directions for model training focus areas. 2. Related Work 2.1. Computer Vision for Agriculture With the rise of computer vision techniques driven by advancements in deep learning, wide range of agricultural tasks have been explored over the past decade. In particular, dataset construction and benchmarking play an important role in developing models and practical applications in agriculture, while also paving the way for forwardlooking advancements. Disease identification is key research focus across various crops, such as rice [1, 34], tomato [10], cacao [13], and sugarcane [47]. Multicrop datasets have also been created [29, 41]. PlantDoc [41] covers 13 species and 17 classes focused on leaf-based diseases. Plant Village [29] offers 39 classes containing both diseased and healthy leaf categories. In addition to crop disease identification, pest identification [49, 53], weed identification [12, 31, 42] have also been studied. While most of the agricultural datasets primarily focus on visual data, few computer vision studies in agriculture investigate the potential of multi-modal approaches. The PlantWild dataset [52] includes 56 plant-disease class pairs, which are collected through image search engines. They implement CLIP-based model and show the possibility of training with combined text and image data. In an instruction-based format, the CDDM [20] has created 16 categories of crops and 60 categories of crop diseases dataset, which generates instructional data using GPT. While these synthetically created datasets explore the agriculture multi-modal models, there remains lack of datasets for comprehensive multi-modal model evaluation, validated by human experts and covering wide range of tasks and categories. 2.2. Vision-Language Models Models. Visual models in the computer vision field have been accelerated by language modality with the data resource of sentence-level inputs and web-scale texts. Specifically, CLIP [35] has made significant contribution in this context, by text and image feature alignment through contrastive learning. CLIP played key role in introducing more sophisticated visual representation into language explanations e.g., Flamingo [2], BLIP [14, 15], Qwen [5], PaLI [8], LLaVA [17, 18], CogVLM [50], and Emu [44, 45, 51]. Closed-source VLMs achieve state-of-the-art performance, such as GPT-4o [32] and Gemini Pro [46], which are said to acquire human-level knowledge across diverse fields on the Internet. Benchmarks. Along this line, several recent studies introduced benchmark datasets. Especially, the representative examples in terms of universal knowledge benchmarking include MMMU [55], MMMU-Pro [56] and MMStar [6] for multi-modal understanding. These benchmarks contain highly diverse domains (e.g., natural, graph, illustration, medical images) and academic fields (e.g., science, engineering, art, and medical fields) under the tasks of vision language such as question-answering and reasoning. The series of MMMUs have been verified with foundation models like GPT-4V, but they cannot answer the questions perfectly. More specific domain datasets have also been proposed, such as those for Medicine [27, 36], Chart [26, 28, 40], and Video understandings [9, 16, 30], contributing to the accurate evaluation of VLMs and guiding future research directions. 3. AgroBench This section introduces AgroBench, the first comprehensive benchmark dataset to evaluate VLM models from the perspective of agricultural vision tasks. The benchmark consists of seven tasks covering wide range of tasks selected from key agricultural engineering research areas, as well as tasks that address real-world challenges faced by farmers in real agriculture scenarios. AgroBench includes 682 disease categories, 134 pest categories, 203 crop categories, and 108 weed categories, representing the largest number of categories in each area to date, to the best of our knowledge. 3.1. Benchmark Tasks in Agricultural Scenes The seven benchmark tasks encompass key research areas in agricultural engineering as well as real-world challenges faced by farmers. To facilitate VLM evaluation, we also provide prompts to address each task in question-answer format. The details of each task are described below. 1) Disease Identification (DID). The DID task aims to accurately diagnose and classify crop diseases. This is key task in agriculture to protect crop health and maximize yields. Our benchmark provides 1,502 QA pairs that cover 370 disease categories, 160 crop categories and 682 cropdisease combinations. To thoroughly evaluate the capabilities of VLMs, we include four misleading disease labels for each image, featuring diseases with similar symptoms or common diseases affecting the target plant. VLMs are required to diagnose the disease based on the image, considering both symptoms and the crop species. Figure 2a illustrates example images. 2) Pest Identification (PID). The PID task aims to identify pests to prevent infestations that can severely impact crop health. Accurate identification reduces economic losses and 3 Figure 2. Examples of labeled images for DID, PID, and WID tasks. Our dataset includes 682 crop-disease pairs, 134 pest categories, and 108 weed categories. We prioritized collecting images from real farm settings. crops. Figure 2b shows examples of labeled pest images. 3) Weed Identification (WID). The WID task aims to identify weed species. Our benchmark includes 609 images of weeds with ground-truth bounding boxes, covering 108 weed species commonly found in farm fields. We assign bounding boxes because multiple weed types often grow closely together, and we want to clarify which one is the target. Specifically, VLMs are required to identify the weed species within the provided bounding box on the image. Figure 2c shows examples of labeled weed images. 4) Crop Management (CMN). Crop management focuses on optimizing farming practices to facilitate crop growth. This involves making decisions on irrigation, fertilization, planting times, and other cultivation practices. Our benchmark provides 411 question-answer pairs for this task. VLMs are required to analyze images of crops and recommend appropriate management strategies by considering factors such as crop health, growth stage, and environmental conditions visible in the images given five answer candidates. In Figure 4a and b, we show two examples of the harvest timing for white and green asparagus, respectively. Our dataset includes complex questions that take into account the differences in their harvest timing. 5) Disease Management (DMN). Disease management aims to control and reduce diseases in crops. This involves informed decisions on interventions such as applying pesticides, adopting resistant crop varieties, or modifying culFigure 3. Seven benchmark tasks in AgroBench. AgroBench includes multiple topics with diverse range of categories. The total accuracy is calculated by the average of each task to mitigate the difference in QAs. minimizes harm to the environment. Our benchmark provides 544 labeled images that cover 134 pest categories, including insects, mites, and other organisms harmful to plants. For categories where we obtain multiple images, we select multiple insect growth stages whenever possible. To fully evaluate the VLMs, we assign alternative choices that closely resemble or are commonly associated with the target 4 Figure 4. Examples of QA pairs for CMN and DMN tasks. (a) and (b) Crop management QA types for white asparagus and asparagus, respectively. Their difference in the harvest timing affects the answers difference correctly. (c) and (d) Disease management QA types for the alfalfa bacterial leaf spot with the initial and severe symptoms, respectively. Based on the severity of the symptoms, the annotator changes the answer. tivation practices. Our benchmark provides 569 questionanswer pairs covering 141 crop-disease combination categories. Since many diseases share the same management strategies regardless of the crop, we carefully select diverse range of disease types and management strategies. VLMs are required not only to identify the disease but also to recommend appropriate management strategies based on images of affected crops. In Figure 4c and show example QAs from the Disease Management tasks. Both images depict Bacterial leaf spot on alfalfa, with showing the initial stage of the disease and showing the severe stage. Based on the severity of the disease, we provide different answer options and set different correct answers. The input prompt consists of question and five answer candidates. 6) Machine Usage QA (MQA). Machine usage QA addresses the correct use and choice of agricultural machinery depending on the task and farming conditions. Selecting the appropriate machinery is essential for efficient farming. Our benchmark provides 303 question-answer pairs covering 98 machine categories. Given that many crops share the same machinery (e.g., soil preparation and irrigation), the coverage of this category is comprehensive. VLMs are required to answer questions about machinery operation or select the appropriate machine for given scenario based on images of machinery or farming conditions. 7) Traditional Management (TM). Traditional management methods involve natural and sustainable approaches to farming, such as the use of organic fertilizers, terrace farming, and agroforestry. Recent computer vision studies have not focused on these traditional practices, although many are still used by certain local farmers. Our benchmark includes 404 question-answer pairs, including 77 traditional management practices. VLMs are required to identify the management method or explain its effectiveness, given five answer choices. 3.2. Dataset Construction Image Selection. To establish high-quality benchmark covering wide range of crops, diseases, and pest categories we initially curated around 50,000 agricultural images from websites supervised by plant pathologists, either where redistribution was permitted or where we obtained redistribution permission. We selected images in real farm settings as much as possible, as shown in Figure 2, to evaluate realworld scenarios. When licensed images for target category were limited, we used laboratory setting images. For curation, we obtained images along with their corresponding labels. The annotator is one of the authors, who holds Ph.D. in Agriculture. In the human evaluation conducted during the experiment, other individuals with Ph.D. or M.S. degree in Agriculture reviewed questions to assess the quality of the QA pairs. The annotator selected images that clearly represented target labels and removed those images with irrelevant labels. For the Machine usage QA and Traditional management, annotators manually created categories based on textbooks and websites, and searched for corresponding images with redistribution licenses. For Weed identification, we use existing dataset [11, 23, 24, 42]. For this Weed identification category, we will provide simple code to download the data and crop images and assign bounding boxes for Weed Identification, allowing users to work with the existing dataset without redistributing the images. Through these image selection processes, the selected images are high-quality 4,218 representative images. The detailed distribution of tasks and categories for each task is summarized in Figure 3. QA Annotation. For the DMN, CMN, MQA, and TM tasks, all question-answer pairs were created manually, independent of any LLMs or VLMs. Annotators used GPT only for sentence rephrasing, but they were prohibited from using any knowledge from GPT for QA creation. Annotators were allowed to refer to textbooks, academic journals, and other authoritative sources in the field to ensure the accuracy and depth of the dataset. This took around 150 man5 Model DID DMN PID WID CMN MQA TM Overall Overall (subset) (all) Random Choice Human 15.64 22. 21.77 25.00 Closed-Source Vision Language Models (VLMs) 16.06 36.25 20.40 45.00 17.90 20.00 22.11 57.50 19.31 51. 19.03 - 19.11 36.79 GPT-4o mini [33] GPT-4o [32] Gemini1.5-Flash [46] Gemini1.5-Pro [46] 53.60 64.18 55.06 62.92 80.67 87.35 79.96 81.55 60.04 77.76 70.04 74. 35.14 44.17 50.90 55.17 64.23 75.43 64.72 71.05 70.96 82.84 78.22 82.84 Open-Source Vision Language Models (VLMs) EMU2Chat [44] LLaVA-Next-8B [19] LLaVA-Next-72B [19] QwenVLM-7B [5] QwenVLM-72B [5] CogVLM-19B [50] LLaVa-7B [18] LLaVA-13B [18] 42.01 45.47 54.95 51.26 57.99 29.16 36.02 40. 48.33 72.58 80.00 80.49 87.87 53.78 62.74 68.89 43.75 43.01 49.81 63.97 73.35 52.39 38.79 44.49 23.81 30.05 26.98 33.17 34.48 25.45 24.79 24.79 40.39 54.26 66.92 66.42 75.91 54.01 53.77 59.37 37.62 56.11 66.11 76.24 80.86 71.62 46.53 54.13 69.80 82.43 73.27 77. 47.77 57.46 70.38 77.48 84.16 66.09 55.20 58.42 62.06 73.45 67.45 72.24 40.53 51.28 59.31 64.15 70.66 50.36 45.41 50.04 69.65 79.26 68.82 69.74 33.84 57.84 64.36 66.41 72.45 44.27 46.14 55.31 Table 2. Results for the seven benchmark tasks with images. We provide results for Random Choice, Human Validation, four closedsource VLMs, and open-source VLMs. Human validation was conducted by 28 people on subset of 80 samples per task as reference. GPT-4o [32] LLaVA-Next-8B [19] DID DMN 72.58 1.93 70.30 26.10 PID WID CMN MQA 25.08 18.75 30.36 21.88 40.39 53.77 1.00 19.70 TM Overall 29.71 48.27 37.49 40. Table 3. Results for the seven benchmark tasks with text only. We additionally evaluate the model without image inputs, and the overall performance is close to random. hours. We carefully annotate various types of questions. Examples of QAs that require expert knowledge are shown in Figure 4. All questions were carefully created so that image reference is necessary for answering. Dataset Statistics. Following the dataset annotations and selections, our AgroBench comprises seven tasks with 4,342 QA pairs, as shown in Table 1. All tasks comprise wide range of categories for detailed evaluation. Please refer to the supplementary materials for more details and examples. 4. Experiments 4.1. Experimental Settings Baseline Models. We use four closed-source models: GPT-4o [32], GPT-4o mini [33], Gemini1.5-Pro [46], and Gemini1.5-Flash [46]. GPT-4o mini and Gemini1.5Flash are down-scale versions of GPT-4o and Gemini1.5Pro, respectively. We use eight open-source models: EMU2Chat [44], LLaVA-Next-8B [19], LLaVANext-72B [19], QwenVLM-7B [5], QwenVLM-72B [5], CogVLM-19B [50], LLaVa-7B [18], and LLaVa-13B [18]. For the details of these models, please refer to the supplementary material. Human Results. We also present results from human participants for reference. We surveyed 28 students, each holding at least bachelors degree in agriculture, and asked them to answer 20 questions each. This created test subset of 280 questions, with each question answered by two participants, resulting in total of 560 responses. We averaged the results per task and reported the accuracy. Each participant was permitted to use book or translator to look up word meanings but was prohibited from using the internet for searches. If participants were unsure of the answer, they were asked to provide the response they believed to be most accurate. Evaluation Protocol. Importantly, our dataset evaluation is conducted per task, and overall scores are averaged based on the number of tasks, not the number of QAs. This prevents categories with large number of evaluations from becoming dominant. We adopt an exact matching approach for our five-option questions. If the models response matches the options letter or the answer sentence, we consider it correct. If the models answer does not match any option, including cases where there is no answer or mul6 Figure 5. Results of seven benchmark tasks with Chain of Thought (CoT). Baseline indicates results without CoT. In the one-shot, two-shot, and three-shot settings, we provide one, two, and three CoT examples per task, respectively, to guide the model. Figure 6. Error analysis on seven benchmark tasks with GPT-4o. We extract maximum of 15 errors per task from the zero-shot CoT result. We manually analyze how they conclude the incorrect answer. tiple answers, we consider it incorrect. 4.2. Main Results Here, we discuss the main results of our AgroBench evaluation performance. We evaluate our AgroBench using eight open-source VLMs and four closed-source VLMs with APIs. Challenges of the AgroBench. We show the main results in Table 2. The most difficult task is Weed Identification (WID), on which most open-source VLMs perform at around the random score. The highest WID accuracy, 55.17%, was achieved by Gemini 1.5-Pro. This suggests that the knowledge about weeds is not as fully trained as that of crops. All models Disease Identification (DID) results are lower than their Disease Management (DMN) results. This means that VLM models can gain contextual information, but there is still room for perceptual improvement. Model Comparison. Overall, closed-source VLMs achieve better results than open-source VLMs and achieve higher performance than humans. GPT-4o model achieved the highest score in overall performance. Among the opensource models, QwenVLM-72B achieves the best result in overall accuracy, which is comparable or even superior to open-source VLMs in some tasks. QwenVLM72B achieves satisfactory results on both identification and question-answering benchmarks on AgroBench. 4.3. Ablations Context Reliance. We further evaluate the results on AgroBench using text-only input to determine whether visual information is necessary to answer the questions. Table 3 presents the experimental results across seven benchmark tasks on AgroBench. With text-only input, performance drops across all models, confirming the reliance on visual information. However, for both models, the DMN, CMN, and TM tasks maintain significantly higher accuracy than random selection. Although we ensured that our questions do not include disease names or appearance-related traits, models tend to infer answers based on estimation. This suggests that many disease types share common management strategies, such as avoiding humidity, preferring cooler temperatures, or pruning infected parts, allowing models to predict the most likely option. similar pattern is observed in the CMN and TM tasks, where models can make educated guesses based on contextual cues. (See the supplementary material for further detailed examples.) Chain of Thought. We evaluate the effectiveness of Chain In the one-shot setting, we of Thought (CoT) reasoning. symptoms of discoloration and wilting. Additionally, its incorrect answer choice is more commonly associated with the root rather than the stem. These errors are based on Lack of Knowledge, suggesting VLMs need more detailed categories and domain-specific training. Perceptual Error (32.69%). This indicates that VLM cant pay attention or recognize the answer-related part in the image (e.g., cant recognize the green insect on the leaf), and VLM misunderstands the image, leading to incorrect answers. Figure 7b shows an example of the model hallucinating and misunderstanding the situation. First, the VLM incorrectly identifies the machine as one used for lifting plants from the soil. Then, it describes the scene as if there are harvested crops, even though no harvested crops are present. These errors can be mitigated by enhancing the VLMs perception abilities for domain-specific classes. Additionally, improving general perception capabilities, including reducing hallucinations, can further contribute to VLM performance. Reasoning Error (7.6%). Reasoning error involves the VLM can describe the options correctly, but cant compare them step by step and conclude the wrong answer. This error is relatively low compared to the existing work [55] since AgroBench requires more specific knowledge and doesnt include reasoning relying on problems (e.g., math). Other Errors (7.79%). For the other errors, we observe Shortcut Error (The VLM can pick up the two candidate options correctly but conclude the answer without comparing the candidates), Double Answer Error (Concluding two answers are correct), Interpretation Misunderstanding (VLM misleading the question and conclude wrong answer), and Reject to Answer (VLM conclude there is no answer). 5. Conclusion In this paper, we develop AgroBench, comprehensive benchmark dataset for VLMs in the agricultural domain, covering state-of-the-art range of categories. Our dataset comprises seven benchmark tasks encompassing key research areas in agricultural engineering as well as realworld challenges faced by farmers. AgroBench contributes to agricultural VLM research by addressing the lack of datasets for comprehensive multi-modal model evaluation, validated by human experts. In our evaluation, VLMs exhibit strengths across different tasks. However, in several tasks such as weed identification and disease identification, all models show room for improvement. Our error analysis reveals that most failures are due to lack of knowledge (51.92%), suggesting that VLMs require more specialized agricultural knowledge. Our dataset will facilitate agricultural VLM research, enabling broad category and task evaluation to support sustainable, automated agriculture. Figure 7. Error examples of GPT-4o. Examples illustrate the two main error types: Lack of Knowledge and Perceptual Error. provide single CoT reasoning example along with an image and the corresponding prompt for each task to guide the model in answering the question. We select 100 QAs per task for evaluation and use GPT-4 as the VLM. Figure 5 presents the accuracy of CoT reasoning with the baseline results. While CoT achieves slightly higher accuracy, the improvement is not significant compared to the baseline. However, CoT demonstrates effectiveness in certain tasks, such as PID, WID, CMN, and TM. For instance, in PID, the VLM performs step-by-step and careful reasoning (see the supplementary material for more detailed examples). In WID, the most challenging task, CoT provides useful examples that may help the model make predictions. Overall, CoT contributes to accuracy, but we observe performance saturation in the three-shot setting. For further detailed examples, please refer to the supplementary material. 4.4. Error Analysis Here, we analyze the mistake types that occur depending on the agricultural tasks. We bring out up to 15 failure examples per task from the zero-shot CoT results and manually analyze how they reached the incorrect answers. Lack of Knowledge (51.92%). This includes cases where VLM cant accurately describe the appearance or relevant knowledge of choice (e.g., VLM fails to describe disease symptoms or insect characteristics) or lacks context (e.g., VLM doesnt know how to treat diseased crops or manage crops for high yield). Figure 7a shows an example of Lack of Knowledge case. When the VLM analyzes the correct answer option for Sclerotinia blight, it fails to describe the 8 6. Acknowledgment This work was supported by the AIST KAKUSEI Project (FY2024) and JST FOREST Grant Number JPMJFR206F. We would like to thank the Agricultural Administration Division, Department of Agriculture, Hokkaido Government, for providing some of the images. We are also for contributing images grateful from the CropAndWeed dataset to our dataset. We used ABCI 3.0 provided by AIST and AIST Solutions. to Daniel Steininger"
        },
        {
            "title": "References",
            "content": "[1] Petchiammal A, Briskline Kiruba S, Murugan D, and Pandarasamy Arjunan. Paddy doctor: visual image dataset for automated paddy disease classification and benchmarking. IEEE Dataport, 2022. 2, 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Proc. Annual Conference on Neural Information Processing Systems (NeurIPS), pages 2371623736, 2022. 3 [3] Abderraouf Amrani, Ferdous Sohel, Dean Diepeveen, David Murray, and Michael G.K. Jones. Deep learning-based detection of aphid colonies on plants from reconstructed brassica image dataset. Computers and Electronics in Agriculture, 205:107587, 2023. 2 [4] Muhammad Awais, Ali Husain Salem Abdulla Alharthi, Amandeep Kumar, Hisham Cholakkal, and Rao Muhammad Anwer. Agrogpt: Efficient agricultural vision-language model with expert tuning. arXiv preprint arXiv:2410.08405, 2024. 2 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 3, 6, 12 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 3 Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [8] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: jointly-scaled multilingual languageimage model. 2023. 3 [9] Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Cees G. M. Snoek, and Yuki M. Asano. Tvbench: Redesigning video-language evaluation. 2024. 3 [10] Mamta Gehlot, Rakesh Saxena, and Geeta Gandhi. tomatovillage: dataset for end-to-end tomato disease detection in real-world environment. Multimedia Systems, 29:124, 2023. 2, 3 [11] Benedikt Geisler. Perrenial plants detection, 2021. 5 [12] Sebastian Haug and Jorn Ostermann. crop/weed field image dataset for the evaluation of computer vision based precision agriculture tasks. In Proc. European Conference on Computer Vision (ECCV), pages 105116. Springer, 2014. 3 [13] Atuhurra Jesse, Nguessan Yves-Roland Douha, and Pabitra Lenka. Image classification for cssvd detection in cacao plants. arXiv preprint 2405.04535, 2024. 2, 3 [14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the 39th International Conference on Machine Learning, pages 1288812900. PMLR, 2022. 3 [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742. PMLR, 2023. [16] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark. In CVPR, 2024. 3 [17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Improved baselines with visual instruction tuning. Lee. arXiv:2310.03744, 2023. 2, 3 [18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, 3, 6, 12 [19] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 6, 12 [20] Xiang Liu, Zhaoxiang Liu, Huan Hu, Zezhou Chen, Kohou Wang, Kai Wang, and Shiguo Lian. multimodal benchmark dataset and model for crop disease diagnosis. In Proc. European Conference on Computer Vision (ECCV), 2024. 2, 3 [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, [21] Xinpeng Liu, Kanyu Xu, Risa Shinoda, Hiroaki Santo, and Fumio Okura. Masks-to-skeleton: Multi-view mask-based 9 tree skeleton extraction with 3d gaussian splatting. Sensors, 25(14), 2025. 2 [22] Shenglian Lu, Wenkang Chen, Xin Zhang, and Manoj Karkee. Canopy-attention-yolov4-based immature/mature apple fruit detection on dense-foliage tree architectures for early crop load estimation. Computers and Electronics in Agriculture, 193:106696, 2022. 2 [23] Yuzhen Lu. Cottonweeddet3. kaggle, 2022. 5 [24] Simon Leminen Madsen, Solvejg Kopp Mathiassen, Mads Dyrmann, Morten Stigaard Laursen, Laura-Carlota Paz, and Rasmus Nyholm JÃ¸rgensen. Open Plant Phenotype Database of Common Weeds in Denmark, 2020. 5 [25] Abdul Khalique Maitlo, Abdul Aziz, Hassnian Raza, and Neelam Abbas. novel dataset of guava fruit for grading and classification. Data in Brief, 49:109462, 2023. 2 [26] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 2, [27] Joao Matos, Shan Chen, Siena Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis F. Nakayama, Jose M. M. Pascual-Leone, Guergana Savova, Hugo Aerts, Leo A. Celi, A. Ian Wong, Danielle S. Bitterman, and Jack Gallifant. Worldmedqa-v: multilingual, multimodal medical examination dataset for multimodal language models evaluation, 2024. 3 [28] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In WACV, pages 15161525, 2020. 2, 3 [29] Sharada P. Mohanty, David P. Hughes, and Marcel Salathe. Using deep learning for image-based plant disease detection. Frontiers in Plant Science, 7, 2016. 3 [30] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models, 2023. 3 [31] Alex Olsen, Dmitry Konovalov, Bronson Philippa, Peter Ridd, Jake Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James Whinney, et al. Deepweeds: multiclass weed species image dataset for deep learning. Scientific reports, 9(1):2058, 2019. 3 [32] OpenAI. Gpt-4o, 2024. Accessed: 2024-11-13. 2, 3, 6, 12 [33] OpenAI. Gpt-4o mini, 2024. Accessed: 2024-11-13. 2, 6, 12 [34] Chiranjit Pal, Imon Mukherjee, Sanjay Chatterji, Sanjoy Pratihar, Pabitra Mitra, and Partha Pratim Chakrabarti. Indian rice disease dataset (irdd), 2023. 2, 3 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 3 [36] Corentin Royer, Bjoern Menze, and Anjany Sekuboyina. Multimedeval: benchmark and toolkit for evaluating medical vision-language models, 2024. 3 [37] Rahman Sanya, Ann Lisa Nabiryo, Jeremy Francis Tusubira, Sudi Murindanyi, Andrew Katumba, and Joyce NakatumbaNabende. Coffee and cashew nut dataset: dataset for detection, classification, and yield estimation for machine learning applications. Data in Brief, 52:109952, 2024. 2 [38] Nabila Husna Shabrina, Siwi Indarti, Rina Maharani, Dinar Ajeng Kristiyanti, Irmawati, Niki Prastomo, and Tika Adilah M. novel dataset of potato leaf disease in uncontrolled environment. Data in Brief, 52:109955, 2024. 2 [39] Risa Shinoda, Hirokatsu Kataoka, Kensho Hara, and Ryozo Noguchi. Transformer-based ripeness segmentation for tomatoes. Smart Agricultural Technology, 4:100196, 2023. 2 [40] Risa Shinoda, Kuniaki Saito, Shohei Tanaka, Tosho Hirasawa, and Yoshitaka Ushiku. Sbs figures: Pre-training figure qa from stage-by-stage synthesized images, 2024. [41] Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar Kumawat, and Nipun Batra. Plantdoc: dataset for visual plant disease detection. In Proc. ACM IKDD CoDS and COMAD, page 249253, 2020. 2, 3 [42] Daniel Steininger, Andreas Trondl, Gerardus Croonen, Julia Simon, and Verena Widhalm. The cropandweed dataset: multi-modal learning approach for efficient crop and weed manipulation. In WACV, pages 37293738, 2023. 3, 5 [43] Daniel Steininger, Julia Simon, Andreas Trondl, and Markus Murschitz. Timbervision: multi-task dataset and framework for log-component segmentation and tracking in authe tonomous forestry operations. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. 2 In Proceedings of [44] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2023. 3, 6 [45] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 2, 3, 12 [46] Google Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. 2, 3, 6, [47] Sandip Thite, Yogesh Suryawanshi, Kailas Patil, and Prawit Chumchu. Sugarcane leaf dataset: dataset for disease detection and classification for machine learning applications. Data in Brief, 53:110268, 2024. 2, 3 [48] Liqiong Wang, Teng Jin, Jinyu Yang, Ales Leonardis, Fangyi Wang, and Feng Zheng. Agri-llava: Knowledge-infused large multimodal assistant on agricultural pests and diseases. arXiv preprint arXiv:2412.02158, 2024. 2 [49] Rujing Wang, Liu Liu, Chengjun Xie, Po Yang, Rui Li, and Man Zhou. Agripest: large-scale domain-specific benchmark dataset for practical agricultural pest detection in the wild. Sensors, 21(5), 2021. 2, 3 [50] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan 10 Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models. In NeurIPS, 2024. 2, 3, 6, [51] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [52] Tianqi Wei, Zhi Chen, Zi Huang, and Xin Yu. Benchmarking in-the-wild multimodal disease recognition and versatile baseline. In Proc. ACM International Conference on Multimedia (ACMMM), 2024. 2, 3 [53] Xiaoping Wu, Chi Zhan, Yukun Lai, Ming-Ming Cheng, and Jufeng Yang. Ip102: large-scale benchmark dataset for insect pest recognition. In CVPR, pages 87878796, 2019. 2, 3 [54] Momchil Yordanov, Raphael dAndrimont, Laura MartinezSanchez, Guido Lemoine, Dominique Fasbender, and Marijn van der Velde. Crop identification using deep learning on lucas crop cover photos. Sensors, 23(14), 2023. 2 [55] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, pages 95569567, 2024. 2, 3, 8 [56] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 2, 3 A. Statistics This section provides detailed statistics of AgroBench. In Figs. 8, 9, 10, 11, 12, and 13, we present the distribution of 682 plant-disease categories, categorized by the cause of the disease. The distribution of 134 pest categories is shown in Fig. 14, and the distribution of 108 weed categories is shown in Fig. 15, both categorized by their order. B. Experimental Details B.1. Hyperparameter Settings We follow the official hyperparameter settings. For closedsource models, we set the temperature hyperparameter to 0.0. For open-source models, we use the default temperature values provided by each implementation. For GPT-4o and GPT-4o mini, we set the detail parameter for visual inputs to low. B.2. Model Details GPT-4o [32] and GPT-4o mini [33]. GPT-4o is an extended model in the series of Generative Pre-training Transformers (GPTs). GPT-4o mini is lightweight model designed for efficiency and speed. While it sacrifices small degree of accuracy compared to GPT-4o, it remains an efficient option. We use the model gpt-4o-2024-08-06 for GPT4o and gpt-4o-mini-2024-07-18 for GPT-4o mini. Gemini 1.5-Pro and Gemini 1.5-Flash [46]. Gemini has been simultaneously trained on huge amount of multimodality dataset among image, video, audio, and text data. Gemini 1.5-Flash is the lightweight version of Gemini1.5Pro. We use the model gemini-1.5-pro-001 for Gemini 1.5Pro and gemini-1.5-flash-001 for Gemini 1.5-Flash. Qwen [5]. Qwen is vision-language model trained with huge amount of image-text data and instruction tuning. Qwen has several specific modes, such as coding, additional audio modality, and mathematics. In this paper, we employ Qwen2-72B (QwenVLM-72B) and Qwen2-7B (QwenVLM-7B). LLaVA [18]. LLaVA takes advantage of trained large language models (LLMs) and instruction tuning with vision models. The LLaVA project has proved that the merged representation is greatly effective for visual reasoning and dialogue using multi-modal input. In the experiments, we use LLaVA v1.5 with 7 and 13 billion parameters (LLaVA1.5-{7B, 13B}). We also use LLaVA-Next [19] improved version of LLaVA. CogVLM [50]. This vision-language model employs trained LLMs and visual encoders and additionally tunes feed-forward layers in order to combine the ability of CogVLM has confirmed image-text question-answering and visual reasoning performance on representative vision and language datasets. We utilize the representations. second version of CogVLM, which contains 19B parameters and has Llama-3 backbones. Emu [45]. Emu potentially executes both image-to-text and text-to-image across diverse visual, linguistic, and multimodal tasks. This foundation model enhances the vision and language performance from both recognition and generation learning across multiple modalities. The model size is 37B. C. Annotation Examples In this section, we show example annotations for the seven tasks as follows: Disease Identification (DID): Fig. 16, Fig. 17 Disease Management (DMN): Fig. 18, Fig. 19 Pest Identification (PID): Fig. 20, Fig. 21 Weed Identification (WID): Fig. 22, Fig. 23 Crop Management (CMN): Fig. 24, Fig. 25 Machine Usage (MQA): Fig. 26, Fig. 27 Traditional Methods (TM): Fig. 28, Fig. 29 As shown in the figures, we provide reasoning annotations for DMN, CMN, MQA, and TM tasks, ensuring explainability. We also list the category for TM in Table. 6 and for MQA in Table. 7. D. Detailed Analysis D.1. Context Reliance We show example cases where GPT-4o can answer questions without input images by guessing the most likely option (Fig. 30 and Fig. 31). Even though we confirmed that our prepared questions do not include crop names or appearance traits, and other options could be correct, the models tend to make conclusions based on estimation. D.2. CoT Examples Here, we show output examples of chain-of-thought (CoT) reasoning by GPT-4o. In Fig. 32, the model identifies the pest step by step by observing the images appearance, checks all the options, and concludes with the correct answer. In Fig. 33, the model focuses on determining the plant disease species but fails to observe the severity of the disease, leading to an incorrect conclusion. D.3. Error Examples Here, we present additional error examples as follows: Lack of Knowledge Error: Fig. 34 Perceptual Error: Fig. 35 Shortcut Error: Fig. 36 Reasoning Error: Fig. 37 Double Answer Error: Fig. 38 Interpretation Misunderstanding: Fig. 39 Reject to Answer: Fig. 40 12 Metric DID PID WID F1 Substring 27.26 24.57 27.09 30.33 1.84 9.03 Table 4. Word-based Evaluation Metrics under Free-form Answer Setting Metric DMN CMN MQA TM BERTScore (F1) Sentence-BERT 87.20 0.443 88.15 0.557 88.16 0.508 87.33 0. Table 5. Sentence-based Semantic Similarity under Free-form Answer Setting D.4. Free format We evaluate our dataset under free-form answer setting, where the model is not constrained to choose from multiple choices. For the identification tasks (DID, PID, WID), we use the following prompt: {question} Respond with the correct answer only, using single noun or short phrase. Do not include full sentences. We show the result in Table 4. The responses are evaluated using the F1 score and substring match accuracy, which reflect surface-level correctness as well as partial matches. For the other tasks (DMN, CMN, MQA, TM), the prompt used is: {question} Respond with the correct answer only. We show the result in Table 5. The answers are evaluated with two semantic similarity metrics: BERTScore (F1) and Sentence-BERT cosine similarity. 13 Figure 8. Disease Categories Counts (A). Figure 9. Disease Categories Counts (B). 15 Figure 10. Disease Categories Counts (C). 16 Figure 11. Disease Categories Counts (D). Figure 12. Disease Categories Counts (E). 18 Figure 13. Disease Categories Counts (F). 19 Figure 14. Pest Counts. 20 Figure 15. Weed Counts. Figure 16. Disease Identification Task (DID) Example (A). Figure 17. Disease Identification Task (DID) Example (B). 22 Figure 18. Disease Management Task (DMN) Example (A). Figure 19. Disease Management Task (DMN) Example (B). Figure 20. Pest Identification Task (PID) Example (A). Figure 21. Pest Identification Task (PID) Example (B). 24 Figure 22. Weed Identification Task (WID) Example (A). Figure 23. Weed Identification Task (WID) Example (B). Figure 24. Crop Management Task (CMN) Example (A). Figure 25. Crop Management Task (CMN) Example (B). 26 Figure 26. Machine Usage QA Task (MQA) Example (A). Figure 27. Machine Usage QA Task (MQA) Example (B). Figure 28. Traditional Methods (TM) Example (A). Figure 29. Traditional Methods QA (TM) Example (B). 28 Class System Landscape Irrigation Soil Processing Tool Storage Pest Practice Tools / Techniques Agroforestry, Chinampa, Crop rotation, Duck rice method, Floating Agriculture, Milpa System, Rice-fish system, Shade-grown coffee, Three Sisters method Andes steep, Below Sea Level Farming, Moray Circle Terrace, Polder, Terrace Farming, Xinghua Duotian Bisse dAyent, Clay Pot Irrigation, Drainage system, Furrow irrigation, Irrigation canal, Level Basin Irrigation, Noria, Pattern tile drainage, Qanat, Rainwater Harvesting Pit, Reservoir irrigation, Subak, Turpan Karez water system, Waterladder pump Composting, Controlled burning, Erosion barriers, Hugelkultur, Jhum Cultivation, Keyline design, Ridge and Furrow, Slash-and-burn, Sloping Agricultural Land Technology, Zai pits Basket press, Cotton gin, Flail, Grape-treading, Matcha Stone Mill, Metate, Mortar and pestle, Rice Sieving, Sheaf, Stook, Threshing machine, Treadmill, Watermill, Winnowing Digging stick, Foot plough, Hoe, Horse-ploughing, Machete, Ox-ploughing, Pitchfork, Rice Field Marker, Sickle, Wheelbarrow, Yoke Hasa Drying, High-floored storage, Karausu, Mangoku, Para-para drying rack, Tomi, Traditional fruit drying Botanical Pest Control, Scarecrows, Shishi-odoshi, Stork-friendly farming Hand Planting, Tea-picking Table 6. Categorized List of Traditional Methods (TM) Figure 30. Example case of GPT-4o answering without an input image (DMN). Even when the model cannot determine the crop or disease based on the text, it guesses the answer. Figure 31. Example case of GPT-4o answering without an input image (CMN). Even when the model cannot determine the crop based on the text, it guesses the answer. Category Harvester Seeder Transplanter Planter Handling Transport Tillage Soil Preparation Spreader Irrigation Chopper Baling Weeder Processing Pruner Rock Removal Power Equipment Autonomous System Facility Equipment Planting Support Mower Machinery Aquatic Weed Harvester, Bean Harvester, Beet Harvester, Carrot Harvester, Coffee Bean Harvester, Combine, Corn Harvester, Daikon Radish Harvester, Forage Harvester, Grape Harvester, Harvester, Haulm Topper, Mechanical Tree Shaker, Onion Harvester, Peanut Harvester, Potato Harvester, Pumpkin Picking Machine, Reaper-Binder, Stripper, Sugarcane Harvester, Swather, Tomato Harvester, Yam Harvester Autonomous Seeder, Broadcast Seeder, Seeding Machine, Semi-Automatic Seeder Cabbage Transplanter, Transplanter Planter, Potato Planter Bale Gripper, Bale Sledge, Claw Machine, Packing Machine, Sorting Machine Chaser Bin, Conveyor, Dump Trailer, Electric Cart, Gravity Wagon, Mother Bin Cultivator, Disc Harrow, Drag Harrow, Excavator, Motorized Plough, Reversible Plough, Spading Machine, Spring-Tooth Harrow, Strip-Till Implement, Tiller Cambridge Roller, Destoner, Land Imprinter, Potato Bed Former, Puddling Machine, Ridge Maker, Ridging Hiller, Rotary Hiller, Smooth Roller De-Icing Agent Spreader, Fertilizer Spraying Robot, Fertilizer Spreader, Manure Spreader, Silage Spreader, Sprayer Irrigation Machine, Sprinkler Irrigation, Water Wheel Green Manure Chopper, Leaf Chopper, Root Cutting Machine, Shredder, Straw Shredder Roll Baler, Tree Baler Tine Weeder, Weeder Machine Beet Cleaner Loader, Fanner, Rice Huller, Rice Milling Machine, Sugarcane Press, Threshing Machine Grape Pruning Machine Rock Windrower, Stone Picker PTO Smart Robotic Farmer, Unmanned Helicopter Multi-Tunnel, Vertical Farming System Border Coating Machine, Seed-Counting Machine, Seedling Machine Flail Mower, Mower Table 7. Categorized List of Machine QA (MQA) Figure 32. Example case of zero-shot CoT success. The model identifies the pest step by step by observing the images appearance, checks all the options, and concludes with the correct answer. 31 Figure 33. Example of one-shot CoT failure. The model focuses on determining the plant disease species but fails to observe the severity of the disease, leading to an incorrect conclusion. 32 Figure 34. Example of Lack of knowledge Error (CMN). The model successfully determines that the plant is Rutabagas. However, the model does not know the optimal timing for planting the seeds. Figure 35. Example of Perceptual Error (DID). The image shows the bacterial leaf streak and bulb rot, and the correct answer is B. The image does not show purple lesions. The affected areas are yellow and brown. 34 Figure 36. Example of Perceptual Error (DID). The image shows the Common wild oat (Avena fatua), and the correct answer is B. The model successfully guesses that and are close to the image; however, it concludes that is correct without further observation or reasoning. 35 Figure 37. Example of Reasoning Error (DID). Successfully reasoning and leads to dark areas, but by ignoring the reasoning, the model concludes that is the correct answer.) Figure 38. Example of Double Answer Error (MQA). The image shows the border coating machine with the correct answer: C. However, the model stops thinking and provides both and as answers, even though it should continue reasoning to determine single correct answer. 37 Figure 39. Example of Interpretation Misunderstanding (PID). The image shows the Water Fern Weevil, and the correct answer is D. However, even though the image was taken in lab environment, the model considers this option incorrect because the background is not water. 38 Figure 40. Example of Reject to Answer (DMN). The image shows the blood disease of the banana, and the correct answer is A. However, the model concludes that there is no answer and stops reasoning instead of reconsidering."
        }
    ],
    "affiliations": [
        "Kyoto University",
        "National Institute of Advanced Industrial Science and Technology (AIST)",
        "OMRON SINIC",
        "The University of Osaka",
        "Tokyo Institute of Technology",
        "Visual Geometry Group, University of Oxford"
    ]
}