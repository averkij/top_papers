{
    "paper_title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models",
    "authors": [
        "Wen Wang",
        "Bozhen Fang",
        "Chenchen Jing",
        "Yongliang Shen",
        "Yangyi Shen",
        "Qiuyu Wang",
        "Hao Ouyang",
        "Hao Chen",
        "Chunhua Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 3 1 9 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TIME IS FEATURE: EXPLOITING TEMPORAL"
        },
        {
            "title": "DYNAMICS IN DIFFUSION LANGUAGE MODELS",
            "content": "Wen Wang1,2, Bozhen Fang1, Chenchen Jing1,3, Yongliang Shen1, Yangyi Shen4, Qiuyu Wang2, Hao Ouyang2, Hao Chen1, Chunhua Shen1,3,2 1 Zhejiang University 2 Ant Group 3 Zhejiang University of Technology 4 Stanford University"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal SelfConsistency Voting, training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), measure of semantic stability across intermediate predictions, as reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them. Project webpage: https://aim-uofa.github.io/dLLM-MidTruth"
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion large language models (dLLMs) (Nie et al., 2025; Zhu et al., 2025; Ye et al., 2025) have recently emerged as promising alternative to the auto-regressive (AR) large language models, garnering significant attention for their competitive performance and potential for faster inference. In contrast to AR models, which generate text in strictly sequential manner by predicting one token at time, dLLMs operate through iterative cycles of denoising and remasking, predicting all masked tokens in parallel at each step. small subset of the predicted tokens, typically those with high confidence (Nie et al., 2025), are retained, while the remaining tokens are remasked and refined in subsequent steps. Despite their drastic architectural differences, current dLLMs typically adopt decoding strategy that mirrors AR models: solely relying on the sequence predicted in the final denoising step as the final answer, and discarding all the intermediate predictions. In this work, we challenge this convention by uncovering critical phenomenon that we term temporal oscillation: correct answers often appear during intermediate denoising steps but are overwritten in later iterations. This discrepancy between the final output and intermediate correctness suggests that dLLMs possess rich temporal dynamics that are largely under-utilized. As depicted in Fig. 1, we analyze two key metrics across four widely used benchmark datasets using two representative models: LLaDA-8B-Instruct (Nie et al., 2025) and LLaDA-1.5 (Zhu et al., 2025). The first metric, final pass rate, measures the accuracy of the final output, while the second, ever-pass rate, captures whether correct answer appears at any point during the decoding process. In Fig. 1a, WW and BF contributed equally. CS is the corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Illustration of temporal oscillation during sampling. Figure 1: (a) Across four datasets, significant gap is observed between the final answers pass rate (denoted as Pass @1) and the everpass rate at any intermediate step (denoted as EverPass @1 t). This gap reveals the phenomenon we refer to as temporal oscillation, where correct intermediate answers are sometimes overwritten as the generation proceeds. (b) Example of temporal oscillation: For given math problem, the model initially gives the correct answer, 25, at an intermediate step (e.g., step 55), aligning with the ground truth. However, by the final step, this correct answer is replaced with an incorrect one: 2. consistent and significant discrepancy exists between these metrics. This gap reveals critical phenomenon: models often generate correct answers during intermediate steps but subsequently overwrite them with incorrect ones. Fig. 1b illustrates this concretelyin math problem, the model produces the correct answer 25 at sampling step 55, only to replace it with an incorrect 2 by the final step 64. More examples on temporal oscillation are presented in Appendix D.4. To better understand this behavior, we analyze dLLMs from the entropy perspective and introduce new metric: Temporal Semantic Entropy (TSE), which captures the distribution of semantic variation across intermediate outputs during decoding. Specifically, we collect the sequence of intermediate answers across denoising steps and group them into clusters based on semantic equivalence. TSE then quantifies the degree of uncertainty in the semantic content of these answers. higher TSE indicates greater semantic fluctuation throughout the trajectory, i.e., the model changes its answer frequently, while lower TSE suggests convergence toward stable meaning. To harness the latent signals embedded in dLLMs decoding, we treat temporal oscillation as an informative feature and develop two complementary methods that exploit the temporal dynamics: Temporal Self-Consistency Voting: training-free test-time decoding strategy that aggregates predictions across denoising steps and selects the most consistent output, considerably improving accuracy with negligible computational overhead. Temporal Consistency Reinforcement: post-training method based on reinforcement learning that uses negative TSE as reward signal to encourage stable and consistent generations. Importantly, leveraging negative TSE as the reward enables performance improvements without requiring ground-truth labels for reward computation. Experiments across multiple datasets validate the effectiveness of both our decoding-time strategy and our RL-based post-training method. Specifically, Temporal Self-Consistency Voting brings an average improvement of 1.5% over the LLaDA-8B-Instruct baseline with negligible overhead. In terms of Temporal Consistency Reinforcement, fine-tuning using the negative TSE reward alone, we observe substantial average improvement of 24.7% on the Countdown dataset. When combined with the accuracy reward derived from ground truth, our approach yields notable improvements across diverse datasets: 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and an impressive 25.3% on Countdown, respectively. By quantifying and leveraging temporal consistency, we offer new perspective on dLLM decoding and introduce practical tools to unlock their potential. We hope that this study inspires further research into the temporal characteristics of diffusion decoding."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion language models. With the success of diffusion models in image and video generation (Song et al., 2020; Ho et al., 2020; 2022), diffusion methods have been extended to text generation. Early continuous diffusion approaches (Han et al., 2022; Li et al., 2022) operate in continuous space, while others project discrete text onto the probability simplex (Avdeyev et al., 2023; Stark et al., 2024), though these often struggle with complex linguistic patterns. More recent work applies flow matching on the simplex to learn categorical distributions (Davis et al., 2024; Cheng et al., 2024), but remains limited to simpler sequences. Discrete diffusion models, pioneered by D3PM (Austin et al., 2021), have advanced through masked token frameworks (Sahoo et al., 2024; Nie et al., 2024) and scaling efforts. Lightweight models like Plaid (Gulrajani & Hashimoto, 2023) and SEDD (Lou et al., 2023) show competitive results against GPT-2 (Radford et al., 2019), yet scalability lags behind autoregressive models. To bridge this gap, BD3-LMs (Arriola et al., 2025) and Eso-LMs (Sahoo et al., 2025) interpolate between the autoregressive and diffusion paradigms, allowing both parallel sampling and competitive performance. More recently, attempts have been made to scale up dLLMs. For example, Dream (Ye et al., 2025) converts pre-trained autoregressive models into diffusion models, while LLaDA (Nie et al., 2025) also demonstrates strong training-from-scratch results. Another line of work explores sampling strategies in dLLMs. For example, Kim et al. (2025) examines the impact of token ordering during sampling and proposes an adaptive inference strategy that improves model performance. Similarly, ReMDM (Wang et al., 2025) introduces remasking mechanism that leverages inference-time scaling to improve generation quality in dLLMs. Unlike prior works, we analyze and improve dLLMs from the perspective of temporal stability, domain that remains underexplored. Test-time Strategy. Test-time strategies (Wei et al., 2022; Madaan et al., 2023; Snell et al., 2024; Yao et al., 2023; Liu et al., 2025a) for LLMs have been extensively studied and widely adopted to enhance the accuracy, consistency, and reliability of model outputs. One of the most straightforward and effective techniques in this domain is Self-Consistency (Wang et al., 2022), which selects the most consistent answer from multiple outputs using majority voting. Our approach leverages the concept of temporal self-consistency, devising test-time strategy specifically tailored for dLLMs. Notably, our method incurs only negligible inference overhead and can be directly integrated into existing dLLMs frameworks. Another key technique is semantic entropy (Farquhar et al., 2024; Kuhn et al., 2023), novel uncertainty metric designed for LLMs. It addresses the semantic equivalence problem by first clustering sentences that share the same meaning but differ in phrasing, and then computing entropy over these semantic clusters. While semantic entropy has been successfully applied to tasks like uncertainty estimation (Kuhn et al., 2023) and hallucination detection (Farquhar et al., 2024), we introduce Temporal Semantic Entropy to characterize the stability and confidence of dLLMs during the progressive denoising process. Post-Training using Reinforcement Learning. Group Relative Policy Optimization (Shao et al., 2024; Guo et al., 2025) (GRPO) is variant of Proximal Policy Optimization (Schulman et al., 2017) (PPO) that directly computes the advantage from rewards within group of responses, obviating the need for an separately trained value function. GRPO has demonstrated not only strong performance on reasoning tasks such as mathematics and code generation, but also significant potential across broader range of modalities and downstream tasks (Huang et al., 2025; Shen et al., 2025; Qi et al., 2025; Zhong et al., 2025; Li et al., 2025; Damani et al., 2025). Building on this, related works have introduced further refinements. For instance, DAPO (Yu et al., 2025) employs dynamic sampling strategy to prevent training batches from consisting entirely of correct or incorrect samples. Other approaches (Zhang et al., 2025; Prabhudesai et al., 2025; Cui et al., 2025; Agarwal et al., 2025) have enhanced RL strategies from an entropy perspective; EMPO (Zhang et al., 2025) estimates rewards directly based on semantic entropy, while Seed-GRPO (Chen et al., 2025) leverages entropy to improve advantage estimation. Recently, the GRPO framework has been adapted for dLLMs in methods like diffu-GRPO (Zhao et al., 2025), UniGRPO (Yang et al., 2025), and coupled-GRPO (Gong et al., 2025). commonality among these methods is their reliance on ground-truth answers to formulate reward signals. In contrast, our approach is distinguished by its unsupervised nature; it enhances the models temporal consistency without depending on groundtruth supervision."
        },
        {
            "title": "3.1 PRELIMINARIES ON DLLMS",
            "content": "DLLMs formulate text generation as process of iteratively denoising text sequences across different time steps. Let x0 pdata(x0) denote the original input sequence, and xt represent its noisy version at time [0, ]. Typically, the noise is introduced by masking portion of tokens in the original training data. The forward noising process is defined as Markov chain q(x1:T x0) = (cid:81)T t=1 q(xt xt1), which progressively adds noise to x0 over time steps. This process incrementally transforms the clean sequence x0 into highly noisy version xT through series of conditional transitions. In contrast, the reverse (generative) process is modeled as: t=1 pθ(xt1 xt) = (cid:81)T t=1 q(xt1 x0)pθ(x0 xt). pθ(x0:T ) = pθ(xT ) (cid:81)T (1) This generative process can be divided into two key steps. The first step involves pθ(x0 xt), model trained to predict the clean text sequence based on the current noisy input. We denote the answer decoded at the t-th step as xt 0 = pθ(x0 xt). The second step relies on q(xt1 x0), referred to as the re-masking process. This process applies the forward procedure to the currently predicted xt 0, yielding sequence xt1 that is less noisy compared to xt at time t. Various strategies can be employed for this process, such as random re-masking or low-confidence re-masking, as discussed in (Nie et al., 2025). 0, i.e., xt 3.2 TEMPORAL OSCILLATION In the reverse generative process of dLLMs, the prediction of the clean sequence pθ(x0 xt) at single denoising step is often inaccurate, especially when the noise level is large. Existing models perform iterative denoising according to the generative process framework, where the final output is determined by the prediction pθ(x0 x1) at the last denoising step, while neglecting all intermediate predictions {xt t=2 generated during the iterative process. In this work, we conduct an in-depth investigation into these intermediate step results, revealing critical phenomenon in diffusion-based text generation. 0 = pθ(x0 xt)}T To formalize our analysis, let ei,k denote the Pass @1 rate (Chen et al., 2021) for the prediction generated at the k-th noise step on the i-th question in the evaluation benchmark. Building on this, we introduce the ever pass rate , denoted as EverPass @1 t, which measures the proportion of questions in the dataset for which the model produces correct answer at any timestep along the sampling trajectory. Formally, it is defined as: EverPass @1 = (cid:26) (cid:27) max k{1,...,t} ei,k (2) This metric captures the cumulative correctness across all sampling steps, reflecting the overall fraction of questions for which the model arrives at correct solution at least once, even if that solution is later discarded in the final output. Experiment Setup. To validate the existence of fluctuating correctness across timesteps, we evaluate both the final pass rate, i.e., the models Pass @1 at the last denoising step, and the EverPass @1 for two representative dLLMs: LLaDA-8B-Instruct (Nie et al., 2025) and LLaDA1.5 (Zhu et al., 2025). The evaluation is conducted on varying answer length and four representative mathematical reasoning benchmarks: GSM8K (Cobbe et al., 2021), MATH500 (Lightman et al., 2023), SVAMP (Patel et al., 2021), and Countdown (Pan et al., 2025). Observations. As shown in Fig. 1 and Table 1 in Sec. 5.2, there exists significant gap between the final pass rate and the ever pass rate . For example, on the GSM8K dataset with sequence length of 128, the final pass rate of LLaDA-8B-Instruct is 68.5%, while its ever pass rate reaches 80.5%, resulting in gap of 12.0%. This discrepancy indicates that considerable number of questions are correctly answered in one or more intermediate steps, but the answers are subsequently modified to be incorrect during later stages of the refinement process. This phenomenon highlights potential instability in the models iterative decoding, where correct intermediate paths may be overwritten or lost as generation progresses. We term this phenomenon temporal oscillation, with examples presented in Appendix D.4."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Patterns of accuracy evolution over diffusion sampling steps. Responses are generated with length of 128 tokens using 64 diffusion steps, based on the LLaDA-8B-Instruct model. Left: Accuracy generally improves with more sampling steps across all datasets. Simpler datasets like SVAMP start high, while harder ones like Countdown begin lower but steadily improve. Middle and Right: We compare the final pass rate, Pass @1, with the cumulative ever pass rate over steps, EverPass @1 t. consistent gap remains between Pass @1 and EverPass @1 t, highlighted by the green shaded area in the figure. Takeaway 1: Correct intermediate answers are lost during sampling During sampling, answers may oscillate between correct and incorrect states across diffusion steps. notable portion of questions achieve correct answers in the intermediate steps, but ultimately yield incorrect results in the final step. 3.3 ANALYSES To gain deeper understanding of the temporal oscillation phenomenon, we conduct comprehensive analyses from multiple dimensions: the evolution of accuracy, the dynamics of entropy, and the semantic stability across the decoding process. Accuracy Across Sampling Steps. To better understand the temporal oscillation phenomenon, we first visualize how accuracy evolves across diffusion sampling steps. As shown in Fig. 2a, overall, accuracy improves with increasing sampling steps across all datasets. Furthermore, the models exhibit distinct accuracy patterns depending on task complexity. For simpler datasets like SVAMP, accuracy is already relatively high at the initial steps. Correct answers often appear early and are gradually refined, with minor fluctuations. In contrast, for more complex datasets like Countdown, the models tend to struggle initially, i.e., accuracy starts low, but they benefit significantly from iterative refinement, with performance improving steadily over subsequent steps. To better understand these dynamics, we further analyze GSM8K and Countdown, computed Pass @1 and EverPass @1 for all intermediate steps. While initial correct predictions appear early in GSM8K but later in Countdown, noticeable gap between Pass @1 and EverPass @1 emerges and gradually widens as the sampling process unfolds. These results suggest that early correct predictions do not guarantee stable reasoning, highlighting the importance of preserving correct intermediate states throughout the sampling process. More analysis on the SVAMP and MATH500 datasets is presented in Appendix C.1. Entropy Across Sampling Steps. Temporal oscillations reflect the models uncertainty during generation. To quantify this, we analyze the average token-level entropy across sampling steps. According to Fig. 3, we observe that across all datasets, the average token-level entropy decreases steadily during the sampling process and approaches zero at the final step. Notably, GSM8K exhibits relatively low entropy from the beginning, indicating higher initial confidence in its predictions. To understand how entropy relates to correctness, we further categorize questions into three groups: (1) Finally-Correct: the final answer is correct, regardless of intermediate steps; (2) IntermediateCorrect: at least one intermediate step is correct, but the final answer is incorrect; (3) AlwaysIncorrect: all sampling steps, including the final answer, are incorrect. We compute the average entropy of tokens associated with the predicted answers across diffusion steps for GSM8K and Countdown. The results in Fig. 3a show that questions with incorrect final answers, covering both"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Patterns of entropy evolution over diffusion sampling steps. Responses are generated with length of 128 using 64 diffusion steps, based on the LLaDA-8B-Instruct model. Left: The average token-level entropy decreases steadily during the sampling process. GSM8K exhibits lower entropy than Countdown, consistent with the models better performance on GSM8K. Middle and Right: Both Intermediate-Correct and Always-Incorrect questions exhibit higher overall entropy compared to Finally-Correct ones. On GSM8K, Intermediate-Correct questions display lower entropy in the early steps than Always-Incorrect, indicating initial confidence, whereas on Countdown the entropy trend is less stable. Note that some points in Right are missing due to the inability to extract answers from generated responses at early diffusion steps for Countdown. the Always-Incorrect and Intermediate-Correct categories, tend to exhibit higher overall entropy. Notably, GSM8K questions in the Intermediate-Correct category show lower entropy than AlwaysIncorrect ones in the early steps, as shown in Fig. 3b, suggesting initial prediction that deteriorates during the refinement process. Considering the use of semi-autoregressive sampling strategy (Nie et al., 2025), we additionally measure the average entropy of the currently generated block, detailed in Appendix C.3. Temporal Semantic Entropy. While tokenlevel entropy measures local uncertainty, we need metric that captures semantic consistency across the entire decoding trajectory. To this end, we introduce metric called Temporal Semantic Entropy. It captures the distribution of semantic variations in answers generated at each step of the iterative denoising process. 0}T During decoding, we obtain sequence of intermediate answers, denoted by {xt t=1. We group these answers into clusters based on their semantic meaning, forming set of semantic clusters = {C1, C2, . . . , CK}, where each cluster Ck = {xt 0) = k} 0 contains all answers with equivalent semantics (outcome equals k). We then define the temporal semantic entropy (TSE) of trajectory as: : meaning(xt Figure 4: Temporal semantic entropy across four mathematical benchmarks. This metric measures the uncertainty in the semantic content of answers throughout the decoding steps. Statistically, correctly answered questions exhibit lower entropy. TSE({xt 0}T t=1) = (cid:80) Ck (cid:16)(cid:104)(cid:80) xt 0Ck (xt 0) (cid:105) log (cid:104)(cid:80) xt 0Ck (cid:105)(cid:17) (xt 0) , (3) which quantifies the uncertainty in the semantic content of answers over the decoding steps. higher value indicates more semantic variation throughout the trajectory, while lower value suggests convergence toward consistent meaning. As illustrated in Fig. 4, temporal semantic entropy provides valuable insight into the models behavior during the generation process. In datasets like Countdown and MATH, where model performance is comparatively lower, we observe higher temporal semantic entropy than in GSM8K and SVAMP, suggesting increased semantic instability and uncertainty throughout the generation trajectory. Moreover, questions that are ultimately answered correctly, encompassing both Intermediate-Correct and Always-Incorrect, tend to exhibit lower temporal semantic entropy than those answered incorrectly. This indicates that more stable and semantically consistent decoding trajectories are associated with better performance. Consequently, high temporal semantic"
        },
        {
            "title": "Preprint",
            "content": "entropy may serve as signal for model uncertainty and could be used to prioritize samples for further training or refinement. We refer readers to Appendix C.2 for more observations on temporal semantic entropy. Takeaway 2: Correct answers statistically exhibit lower temporal semantic entropy Temporal semantic entropy, computed over intermediate predictions during the decoding trajectory, reflects the semantic stability of the models outputs. Statistically, correctly answered questions tend to have lower entropy, indicating greater consistency and confidence throughout the generation process."
        },
        {
            "title": "4.1 TEMPORAL SELF-CONSISTENCY VOTING",
            "content": "In this section, we propose temporal self-consistency decoding strategy for dLLMs, which leverages intermediate predictions during sampling to produce more reliable final outputs. As discussed in Sec. 3.3, although the final timestep in diffusion trajectory typically yields the most accurate results overall, the correct answer to given question may sometimes appear earlier in the trajectory. Relying solely on the final prediction thus risks discarding better intermediate outputs. To address this, we propose method that aggregates predictions across timesteps through weighted voting mechanism. Formally, given diffusion sampling trajectory {xt according to weighted vote over all timesteps: 0}T t=1, our method selects the final answer a = arg maxa (cid:80)T t=1 (t) 1 ( meaning (xt 0) = a) . (4) Here, 1() is the indicator function that returns 1 if the decoded meaning of xt 0 matches the candidate answer a, and (t) is weighting function over timesteps. Since the accuracy of predictions generally tends to improve as the sampling step increases (or diffusion step decreases), we design (t) to be monotonically decreasing function of diffusion step t. In experiments, we explore three types of weighting schemes: constant, linear decay, and exponential decay, as detailed in Sec. 5.2. Discussion. Our method is conceptually related to self-consistency decoding (Wang et al., 2022), which improves reasoning in autoregressive LLMs by sampling diverse reasoning paths and selecting the most consistent answer via majority voting. However, self-consistency requires multiple full-length forward passes, leading to significant computational overhead. In contrast, our approach requires only single sampling trajectory from the dLLMs. By exploiting the temporal nature of diffusion inference indicated by Eq. (1), we obtain series of intermediate predictions essentially for free, without additional model evaluations. This makes our method both efficient and effective for boosting accuracy through temporal aggregation. 4.2 TEMPORAL CONSISTENCY REINFORCEMENT Motivated by our observation in Sec. 3.3 that correct answers generally exhibit lower Temporal Semantic Entropy (TSE) than incorrect ones, reflecting stronger semantic consistency over time, we propose post-training approach designed to encourage temporal consistency in model outputs. Specifically, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025) as our reinforcement learning framework and use TSE as self-supervised reward signal. Negative TSE as the Reward. Following GRPO, for each question sampled from the dataset D, we draw group of responses {o1, o2, . . . , oG} from the current policy πθ. Each response oi receives scalar reward ri = TSE(oi), where TSE(oi) is computed using Eq. (3) from Sec. 3.3. This reward encourages the model to produce responses whose intermediate predictions remain semantically consistent throughout the decoding process. Based on this, we define the unnormalized (cid:17) advantage (Liu et al., 2025b) for all tokens = 1, . . . , oi as Ak . (π) = ri(π)mean {rj(π)}G (cid:16) j="
        },
        {
            "title": "Preprint",
            "content": "The training objective follows the standard GRPO formulation with our TSE-based reward: LGRPO(θ) = qD o1,...,oGπθ(q) 1 (cid:88) i=1 1 oi oi (cid:88) k=1 min (cid:0)ρk Ak , clip (cid:0)ρk , 1 ε, 1 + ε(cid:1) Ak (cid:1) βDKL [πθ(q)πref(q)] , (5) q) where πref is the reference policy, ρk q) is the importance sampling ratio, ε is the clipping threshold, and β controls the strength of the KL penalty. To compute the token-level probabilities used in ρk , we follow the diffu-GRPO method (Zhao et al., 2025), which estimates these probabilities by averaging outputs from multiple randomly masked versions of the prompt. = πθ(ok πθold (ok Combining TSE with Accuracy Reward. As an alternative, we can combine TSE with the accuracy reward to achieve further performance gain when ground-truth answers are available during training. We draw on insights from statistical decision theory, specifically the framework of proper scoring rules (Gneiting & Raftery, 2007; Damani et al., 2025). These scoring rules are designed they are minimized when the predicted confidence to encourage honest confidence estimates: accurately reflects the true likelihood that the models output matches the correct answer o. We derive the confidence from the TSE by normalizing it with respect to the maximum entropy HmaxTSE(oi) achievable under uniform sampling. Specifically, we define confidence as c(oi) = , Hmax where Hmax = log . Here, denotes the total number of sampling steps. The normalization ensures [0, 1], where higher values indicate greater confidence for correct answers. The accuracy reward follows binary scheme: it assigns reward of 1 when the models prediction is correct (i.e., oi = o), and 0 otherwise. We propose composite reward function grounded in the theory of proper scoring rules. Specifically, we adopt the spherical scoring rule, which empirically demonstrates superior performance across tasks. The final reward function is defined as: ri = 1oi=o + c(oi) (cid:112)(c(oi))2 + (1 c(oi))2 . (6) Here, the second term incorporates the models confidence, rewarding high certainty when the prediction is correct while penalizing overconfidence in incorrect predictions. This design ensures the reward is sensitive not just to accuracy, but also to the reliability of the prediction. detailed comparison of various scoring rules and their empirical performance in the Appendix D.1. Discussion. Unlike prior reinforcement learning post-training methods for dLLMs, such as diffuGRPO, which relies on ground-truth answers for reward computation, our approach operates without any labeled data. Instead, we harness the models internal temporal dynamics as self-supervised signal, employing negative TSE to assess answer quality. This enables our method to be broadly applicable, particularly in unsupervised settings, and offers novel direction for improving dLLMs. Furthermore, we show that combining the negative TSE reward with the accuracy reward based on ground-truth answers yields notably better performance than using the accuracy reward alone."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 IMPLEMENTATION DETAILS We utilize both LLaDA-8B-Instruct (Nie et al., 2025) and LLaDA-1.5 (Zhu et al., 2025), and evaluate performances on four widely recognized mathematical datasets: GSM8K (Cobbe et al., 2021), MATH500 (Hendrycks et al., 2021), SVAMP (Patel et al., 2021), and Countdown (Pan et al., 2025). In line with d1 (Zhao et al., 2025), we assess the models performance across various output lengths. For the temporal self-consistency voting, we employ the exponential function by default to weight the sampling steps. In terms of post-training, we follow d1 and perform supervised fine-tuning (SFT) before reinforcement fine-tuning (RFT) for LLaDA-8B-Instruct. Specifically, we conduct SFT on the s1K dataset (Muennighoff et al., 2025) for 20 epochs, using sequences with length of 4,096. For LLaDA-1.5, we omit SFT because it often results in performance degradation, likely due to the model having already undergone sophisticated post-training. All training is executed using 8 H800 GPUs. Please refer to the Appendix for more details."
        },
        {
            "title": "Preprint",
            "content": "Performance of temporal majority voting on mathematics benchmarks: We Table 1: evaluate three temporal majority voting strategies, including fixed weighting, linear weighting, and exponential weighting, across four math datasets and two model variants: LLaDA-8B-Instruct and LLaDA-1.5. Exponential weighting achieves the best results across datasets and sequence lengths. Bold numbers indicate the highest performance within each group, while green values represent improvements over the baseline. For reference, we include the oracle EverPass @1 as performance upper bound. Method / Seq Len LLaDA-8B-Instruct baseline + Temporal Voting Linear Weighting Fixed Weighting Exp. Weighting EverPass @1 LLaDA-1.5 baseline + Temporal Voting Linear Weighting Fixed Weighting Exp. Weighting EverPass @1 GSM8K MATH SVAMP Countdown 128 68.5 68.0 70. 70.1 +1.6 80.5 69.8 68.8 71.0 70.7 +0. 81.5 256 76.3 73.4 78.0 78.7 +2. 85.2 79.4 75.7 79.8 79.8 +0.4 88. 512 78.2 78.3 78.8 78.9 +0.7 80. 81.1 80.3 81.0 81.1 +0.0 83.6 27.4 26.6 28.0 28.4 +1.0 40.2 29. 27.3 29.2 29.0 +0.0 39.8 256 33. 30.8 34.4 35.6 +2.2 45.6 32.4 30. 32.8 33.2 +0.8 47.4 512 35.8 34. 34.6 36.2 +0.4 47.2 35.4 34.6 35. 36.2 +0.8 49.2 128 84.0 87.0 87. 86.0 +2.0 91.3 85.3 87.3 86.0 85.7 +0. 90.7 256 83.3 84.3 84.3 84.3 +1. 89.3 86.3 85.3 87.0 87.7 +1.4 90. 512 84.7 84.3 84.3 84.7 +0.0 86. 83.3 84.0 84.0 84.3 +1.0 86.0 20.3 22.7 24.2 25.0 +4.7 28.1 21. 23.4 24.2 26.2 +4.7 30.5 256 21. 18.8 21.9 23.4 +1.9 27.7 21.1 22. 23.4 25.0 +3.9 27.0 512 16.4 11. 16.0 16.4 +0.0 21.1 20.7 18.8 19. 21.1 +0.4 25.4 5.2 TEMPORAL SELF-CONSISTENCY VOTING Voting Strategies. In experiments, temporal voting is performed by applying weighted voting across denoising steps using three schemes: fixed, linear, and exponential. Each uses weighting function (t), where is the current diffusion step. The fixed scheme assigns equal weight to all steps with (t) = 1. The linear scheme emphasizes earlier diffusion steps using (t) = 1 t/T , while the exponential scheme takes (t) = exp(α(1 t/T )), where α = 5 by default. Both linear and exponential schemes prioritize early diffusion time steps, i.e., latter sampling steps. Ablations on Varying Voting Strategies. As shown in Table 1, both linear and exponential weighting improve the models inference performance. For example, exponential weighting consistently enhances the performance of LLaDA-8B-Instruct across different datasets, with an average improvement of 1.6%, 1.2%, 1.0% and 2.2% accuracy on the GSM8K, MATH500, SVAMP, and Countdown datasets, respectively. The model performs slightly worse under the fixed weighting strategy, which we attribute to the inaccuracy of the answers predicted in the early sampling steps; assigning the same weight to all steps leads to performance degradation. By default, we adopt the exponential weighting strategy. We further conducted an ablation study on the hyperparameter α in the exponential weighting. Taking LLaDA-8B-Instruct as an example, we calculated the average improvement across four datasets and various response lengths. As illustrated in Fig. 5b, α values ranging from 1 to 11 consistently yielded improvements. The optimal performance was observed at α = 5, achieving an average improvement of 1.5% in accuracy. Based on these results, we set α = 5 as the default value. 5.3 TEMPORAL CONSISTENCY REINFORCEMENT Main Results. Table 2 presents the results of our training leveraging TSE in reinforcement learning across different datasets. Our observations are as follows: (1) With TSE reward alone, our method consistently improves model performance across diverse answer lengths and datasets. (2) With TSE reward alone, our method achieves performance on par with or even better than using accuracy reward, despite not relying on ground-truth answers during training. On the Countdown dataset, for example, LLaDA-8B-Instruct trained with our approach achieves 24.7% improvement, significantly outperforming the 15.1% gain observed with d1. (3) By combining TSE with accuracy reward, our method significantly outperforms using accuracy reward alone, achieving absolute improvements of 0.9% on GSM8K, 0.2% on MATH500, 1.7% on SVAMP, and 10.2% on Countdown, compared to d1 (Zhao et al., 2025). In total, our approach yields average gains of 2.0%,"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Negative temporal semantic entropy reward curve during reinforcement fine-tuning. (a) Ablations on α value selection in temporal voting with exponential weighting. (b) Table 2: Performance of reinforcement fine-tuning on mathematics benchmarks. Unlike the competing method d1 (Zhao et al., 2025), which requires access to ground-truth answers during training, our approach leverages the negative Temporal Semantic Entropy (TSE) reward without relying on ground-truth labels. Furthermore, combining TSE with accuracy-based rewards leads to even greater performance improvements across multiple benchmarks. The LLaDA baseline is obtained by applying supervised fine-tuning to LLaDA-8B-Instruct using the s1K dataset (Sahoo et al., 2024). Green numbers show the difference between our method and the baseline. Method / Seq Len LLaDA baseline accuracy reward (d1) + RFT negative TSE reward (ours) combining both (ours) LLaDA-1.5 baseline accuracy reward (d1) + RFT negative TSE reward (ours) combining both (ours) GSM8K MATH SVAMP Countdown 128 70.2 71.7 72.2 +2. 72.1 +1.9 69.8 70.6 70.3 +0.5 70.2 +0.4 78.7 78.3 78.8 +0.1 80.0 +1.3 79.4 78. 79.5 +0.1 79.5 +0.1 512 80.1 82.3 80.2 +0. 83.0 +2.9 81.1 81.4 81.4 +0.3 81.7 +0.6 23.8 31.0 30.6 +6.8 31.2 +7.4 29.0 29. 29.2 +0.2 30.8 +1.8 256 34.4 36.0 34.6 +0. 35.4 +1.0 32.4 34.2 35.6 +3.2 34.8 +2.4 36.8 40.4 38.0 +1.2 41.4 +4.6 35.4 39. 39.0 +3.6 41.2 +5.8 128 81.7 85.7 84.3 +2. 85.0 +3.3 85.3 85.0 86.0 +0.7 88.7 +3.4 83.3 88.0 89.0 +5.7 90.3 +7.0 86.3 88. 88.0 +1.7 89.3 +3.0 512 82.7 88.7 88.7 +6. 92.3 +9.6 83.3 88.3 88.7 +5.4 89.0 +5.7 21.5 34.8 38.6 +17.1 41.5 +20.0 21.5 32. 34.0 +12.5 53.1 +31.6 256 19.9 35.5 53.5 +33. 42.6 +22.7 21.1 25.8 48.8 +27.7 43.0 +21.9 21.5 37.9 44.9 +23.4 54.7 +33.2 20.7 39. 49.5 +28.8 55.1 +34.4 4.3%, 6.6%, and 25.3% on these benchmarks, respectively, compared to the SFT baseline. These results demonstrate the effectiveness of encouraging temporally consistent answers in post-training. Training Dynamics. We visualize the reward curves during training using LLaDA-8B-Instruct as an example, as shown in Fig. 5. The curves demonstrate consistent upward trend in rewards across different datasets as training progresses, indicating effective learning and stable optimization. Model Attributes After RFT. We conduct an in-depth analysis of the LLaDA-8B-Instruct model fine-tuned with the negative TSE reward alone, with generation length of 128 tokens, evaluating its behavior after RFT across several dimensions: temporal semantic entropy, the ever pass rate , and the number of effective tokens (defined as the average count of non-padding, non-EOS tokens per generation). Our findings reveal that (1) as shown in Fig. 6a, temporal semantic entropy consistently decreases across various datasets after RFT, reflecting enhanced temporal consistency in the models outputsan anticipated result of reinforcement learning; (2) Fig. 6b demonstrates that the ever pass rate remains higher than the final pass rate after RFT, suggesting there is still potential for further improvement; and (3) as depicted in Fig. 6c, the number of effective tokens reduces following RFT, implying that the fine-tuned model generates more concise responses. We hypothesize that shorter outputs may be less prone to temporal oscillations, although this relationship requires further investigation. More analysis of the model trained after temporal consistency reinforcement is provided in the Appendix D.2."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Model attributes after reinforcement finetuning (RFT). (a) Temporal semantic entropy decreases across datasets after RFT, indicating improved semantic consistency in model outputs. (b) The ever pass rate remains higher than the final pass rate, suggesting room for continued enhancement. (c) The number of effective tokens per generation is reduced after RFT, reflecting more concise outputs. This brevity may contribute to reduced temporal oscillations, though further analysis is needed."
        },
        {
            "title": "6 LIMITATIONS",
            "content": "While our temporal self-consistency voting and post-training approach demonstrates effectiveness in many scenarios, it exhibits significant limitations when applied to tasks where the models intermediate predictions are consistently inaccurate. As discussed in Appendix D.3, for the Sudoku dataset, the average correctness across all intermediate generation steps remains exceedingly low (below 5%), making it difficult to reliably vote for the correct answer. Similarly, RFT relies on the model already achieving reasonably good performance to produce meaningful reward signals (Prabhudesai et al., 2025; Agarwal et al., 2025), though combining TSE with the accuracy reward may alleviate this issue to some extent. This underscores that our approach depends on the models inherent ability to generate correct or near-correct answers in the sampling trajectory."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This work uncovers critical yet overlooked aspect of diffusion large language models: their rich temporal dynamics. By identifying temporal oscillation as recurring issue in dLLM decoding, we challenge the convention of relying solely on final-step predictions. Our proposed methodsTemporal Self-Consistency Voting and Temporal Consistency Reinforcement demonstrate that intermediate predictions are not noise, but signal. These strategies improve accuracy and stability without requiring additional inference passes or ground-truth supervision. Through extensive experiments, we show that temporal consistency is not just desirable propertyits powerful lever for performance. We hope that this study inspires future research to treat intermediate denoising time steps not as nuisance, but as feature in diffusion-based text generation."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We would like to thank Muzhi Zhu, Canyu Zhao, and Linhao Zhong at Zhejiang University for their valuable discussions and insightful feedback."
        },
        {
            "title": "REFERENCES",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Interpolating between Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021."
        },
        {
            "title": "Preprint",
            "content": "Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, pp. 12761301. PMLR, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Minghan Chen, Guikun Chen, Wenguan Wang, and Yi Yang. Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization. arXiv preprint arXiv:2505.12346, 2025. Chaoran Cheng, Jiahan Li, Jian Peng, and Ge Liu. Categorical flow matching on statistical manifolds. Advances in Neural Information Processing Systems, 37:5478754819, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806, 2025. Oscar Davis, Samuel Kessler, Mircea Petrache, Ismail Ceylan, Michael Bronstein, and Joey Bose. Fisher flow matching for generative modeling over discrete data. Advances in Neural Information Processing Systems, 37:139054139084, 2024. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Tilmann Gneiting and Adrian Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359378, 2007. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36:1669316715, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplexarXiv preprint based diffusion language model for text generation and modular control. arXiv:2210.17432, 2022. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:8633 8646, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022."
        },
        {
            "title": "Preprint",
            "content": "Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusionlm improves controllable text generation. Advances in neural information processing systems, 35: 43284343, 2022. Yuetai Li, Zhangchen Xu, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Xiang Yue, and Radha Poovendran. Temporal sampling for forgotten reasoning in llms. arXiv preprint arXiv:2505.20196, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, and Tieniu Tan. Rethinking the role of prompting strategies in llm test-time scaling: perspective of probability theory. arXiv preprint arXiv:2505.10981, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, arXiv preprint and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv:2503.20783, 2025b. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025. Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, and Hengshuang Zhao. Vln-r1: Visionlanguage navigation via reinforcement fine-tuning. arXiv preprint arXiv:2506.17221, 2025. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
        },
        {
            "title": "Preprint",
            "content": "Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, and Arash Vahdat. Esoteric language models. arXiv preprint arXiv:2506.01928, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola. Dirichlet flow matching with applications to dna sequence design. arXiv preprint arXiv:2402.05841, 2024. Guanghan Wang, Yair Schiff, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Hao Zhong, Muzhi Zhu, Zongze Du, Zheng Huang, Canyu Zhao, Mingyu Liu, Wen Wang, Hao Chen, and Chunhua Shen. Omni-r1: Reinforcement learning for omnimodal reasoning via twosystem collaboration. arXiv preprint arXiv:2505.20256, 2025."
        },
        {
            "title": "Preprint",
            "content": "Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        },
        {
            "title": "A APPENDIX OVERVIEW",
            "content": "This appendix provides additional implementation details, empirical analysis, and extended results to supplement the main paper. It is organized as follows: Appendix B: More Implementation Details Provides further implementation information, including: Appendix B.1: Detailed descriptions of the datasets used Appendix B.2: Training configurations and hyperparameters Appendix B.3: Sampling strategies and evaluation procedures Appendix C: More Analysis Presents extended analyses, including: Appendix C.1: Accuracy and entropy analysis on the MATH500 and SVAMP datasets Appendix C.2: Temporal semantic entropy across varying generated lengths Appendix C.3: Block-level token entropy analysis Appendix D: More Experimental Results Includes additional experimental findings, such as: Appendix D.1: Ablation studies on different scoring rules for combining TSE with accuracy reward Appendix D.2: Performance of temporal self-consistency voting on reinforcement fine-tuned models Appendix D.3: Analysis of failure cases on the Sudoku dataset Appendix D.4: Detailed examples illustrating temporal oscillation"
        },
        {
            "title": "B MORE IMPLEMENTATION DETAILS",
            "content": "B.1 DATASETS We provided detailed descriptions of the datasets as follows: GSM8K (Cobbe et al., 2021) comprises 8.5K linguistically diverse grade school math word problems (7.5K training, 1K test), solvable by bright middle school students via 28 steps of basic arithmetic, suited for multi-step mathematical reasoning. MATH500 (Lightman et al., 2023) is curated subset of 500 problems selected from the broader MATH dataset (Hendrycks et al., 2021), featuring high-school-level competition math problems. SVAMP (Patel et al., 2021) serves as benchmark for elementary-level Math Word Problems (MWPs), where each MWP is short natural language narrative describing scenario and asking questions about unknown quantities. Countdown (Pan et al., 2025) involves combinatorial arithmetic game with three numbers, requiring models to reach target numbers using basic arithmetic operations on given set of numbers. B.2 TRAINING During reinforcement fine-tuning, we train our model using sequences of 256 tokens, with batch size of 6 per GPU and gradient accumulation over 2 steps. Low-Rank Adaptation (LoRA) (Hu et al., 2022) is applied with rank of 128 and scaling factor of 64. During reward computation, answers are parsed from generated text sequences for semantic clustering. When answer parsing fails due to an inaccurate format, we simply discard the answer for temporal semantic entropy computation. Moreover, since answers generated in the first half of the sampling steps tend to be rough and less reliable, we exclude them from consideration. Only answers from the second half of the sampling steps are used to calculate the temporal semantic entropy."
        },
        {
            "title": "Preprint",
            "content": "Figure S1: Top Row: Pass @1 and Pass @1 for MATH500 and SVAMP are provided as supplementary results to Fig. 2, using the same experimental settings. noticeable gap similar to that observed in GSM8K and Countdown between Pass @1 and Pass @1 is present across all sampling steps. Bottom Row: Answer Entropy for MATH500 and SVAMP are provided as supplementary results to Fig. 3, using the same experimental setups. MATH500 and SVAMP exhibit answer entropy patterns similar to those of GSM8K and Countdown. B.3 SAMPLING AND EVALUATION During sampling, we adopt the semi-autoregressive sampling approach following LLaDA (Nie et al., 2025). Specifically, the sequence is split into multiple blocks, which are generated in left-to-right manner. For each individual block, we employ the low-confidence remasking strategy during the sampling process. Following the practice in d1 (Zhao et al., 2025), we evaluate the model every 100 steps, starting from step 600 to 8,000 steps, and report the best results."
        },
        {
            "title": "C MORE ANALYSIS",
            "content": "C.1 ANALYSIS ON MATH500 AND SVAMP DATASETS Accuracy Analysis. As shown in the first row in Fig. S1, MATH500 and SVAMP exhibit similar pattern to that observed Fig. 2 in Sec. 3.3, where noticeable gap emerges between Pass @1 and Pass @1 t. In MATH500 and SVAMP, correct answers start to appear early in the sampling process(near 4% and 39% at first step, respectively), and continue to improve over subsequent iterations. Interestingly, on the SVAMP dataset, distinct pattern emerged in the models Pass @1 accuracy across different sampling steps. Between steps 3 and 20, performance declined noticeably, followed by recovery after step 20. This distinctive fluctuation trajectory represents another manifestation of the temporal oscillation phenomenon. Entropy Analysis. As shown in the second row in Fig. S1, for MATH500 and SVAMP, the answer entropy of Finally-Correct questions remains the lowest throughout the sampling process. Intermediate-Correct questions consistently exhibit lower entropy in the early sampling steps compared to Always-Incorrect ones, pattern observed across all four datasets. However, unlike GSM8K, MATH500, and Countdown, where the final entropy of Intermediate-Correct and AlwaysIncorrect questions is similar, SVAMP displays relatively high entropy in the final sampling step for Intermediate-Correct questions."
        },
        {
            "title": "Preprint",
            "content": "Figure S2: Temporal semantic entropy with varying generation lengths. Average temporal semantic entropy across datasets with generation lengths of 256 and 512, showing consistent patterns with Fig. 4. Higher entropy generally correlates with lower accuracy. Block-level entropy dynamics under semi-autoregressive sampling. During Figure S3: sampling, sequences are partitioned into fixed-length blocks, processed in left-to-right order, with remasking and unmasking operations restricted to the current block. Average token entropy within block decreases with more sampling steps. sharp entropy spike occurs when shifting to new block, likely due to simultaneous decoding of multiple masked tokens increasing initial uncertainty. C.2 TEMPORAL SEMANTIC ENTROPY OVER VARYING GENERATED LENGTH To further validate the generalizability of our findings regarding temporal semantic entropy, we extended the experiments beyond those presented in Fig. 4, which used generation length of 128 and 64 diffusion steps. Specifically, we tested generation lengths of 256 and 512, with diffusion steps set to half the generation length. As shown in Fig. S2, the pattern of temporal semantic entropy observed here aligns with the conclusions drawn in Sec. 3.3: questions that ultimately receive incorrect answers consistently exhibit relatively high temporal semantic entropy throughout the sampling process. This high entropy reflects greater instability and uncertainty in the models intermediate predictions. C.3 ANALYSIS ON THE AVERAGE ENTROPY IN BLOCKS In addition to the token-level entropy considered in Sec. 3.3 and the answer-level entropy considered in temporal semantic entropy, we additionally introduce block-level entropy. This is motivated by In this sampling the fact that dLLMs typically adopts semi-autoregressive sampling strategy. strategy, the entire generated sequence is divided into multiple fixed-length blocks, with each block allocated specific number of sampling steps. During these steps, remasking and unmasking operations are performed exclusively on the current block. Starting with the first block, the process steps to the next block only after all tokens in the current block have been unmasked. We denote the current block as containing tokens indexed from start to an end 1 (inclusive), such that the block spans indices {s, + 1, . . . , 1}. The block entropy is then calculated as: Hblock = 1 e1 (cid:88) i=s H(i) where H(i) represents the entropy of the i-th generated token, and denotes the total number of tokens in the block."
        },
        {
            "title": "Preprint",
            "content": "Table S1: Ablations on scoring rules for combining TSE with accuracy reward. We compare 4 different scoring rules, including entropy scoring, quadratic scoring, spherical scoring, and logistic scoring. Model / Dataset GSM8K MATH500 SVAMP Countdown Method / Seq Len 128 256 512 128 512 128 256 512 128 512 LLaDA +RFT baseline entropy quadratic logistic spherical 70. 78.7 80.1 23.8 34.4 36.8 81. 83.3 82.7 21.5 19.9 21.5 71.7 71.3 73.0 72. 78.5 79.9 79.5 80.0 82.3 82.0 81.1 83.0 31.6 31.0 31.6 31.2 38.2 37.6 36.8 35.4 39.2 40.0 39.4 41.4 88.7 88.0 87.7 85. 89.3 8.3 87.3 90.3 89.3 89.3 90.7 92.3 47.6 50.0 46.5 41.5 50.0 34.4 37.9 42.6 53.1 48.1 51.2 54.7 LLaDA-1. baseline 69.8 79.4 81.1 29.0 32. 35.4 85.3 86.3 83.3 21.5 21. 20.7 +RFT entropy quadratic logistic spherical 72.6 69.8 70.1 70.2 77.9 78.6 78.5 79.5 80.8 81.8 81.7 81. 31.8 30.0 30.6 30.8 36.2 35.0 37.2 34.8 38.6 40.8 39.0 41.2 88.0 88.3 89.0 88.7 88.7 88.7 87.7 89.3 90.0 88.3 89.3 89. 41.4 45.3 42.2 53.1 44.5 40.6 41.1 43.0 53.1 54.3 52.7 55.1 As illustrated in Fig. S3, the average token entropy within single block exhibits consistent downward trend as sampling steps accumulate. This pattern is intuitive: as more tokens in the current block are decoded, they collectively form richer contextual foundation, thereby mitigating the models uncertainty about subsequent tokens in the block. Interestingly, when decoding shifts to new block, the entropy rises sharply. This phenomenon is likely due to the need to decode multiple masked tokens simultaneously at the start of new block, which increases the models uncertainty."
        },
        {
            "title": "D MORE EXPERIMENTAL RESULTS",
            "content": "D.1 ABLATIONS ON SCORING RULES FOR COMBINING TSE WITH ACCURACY REWARD In Sec. 4.2, we combine TSE with the accuracy reward using the proper scoring rule (Gneiting & Raftery, 2007). The purpose of these scoring rules is to promote truthful confidence assessments: they attain their minimum value when the predicted confidence precisely mirrors the actual probability that the models output corresponds to the correct answer o. Here, we conduct an ablation study on different scoring rules. Specifically, we consider the following four forms: Entropy scoring: Quadratic scoring: Logistic scoring: Spherical scoring: = 1oi=o c(oi) rent rquad = 1oi=o (c(oi) 1oi=o )2 rlog = 1oi=o + 1oi=o log(c(oi)) + (1 1oi=o ) log(1 c(oi)) rsphe = 1oi=o + c(oi) (cid:112)(c(oi))2 + (1 c(oi))2 All four proposed reward functions aim to jointly encourage correctness and temporal selfconsistency. Notably, the rent function gives reward of 0 for incorrect answers, while for correct , where Hmax = log . This reward reaches answers, the reward is given by c(oi) = maximum value of 1 when all sampling steps yield the correct answer. The remaining three functions, rquad , correspond to the commonly used quadratic scoring, logarithmic scoring, and spherical scoring in proper scoring rules (Gneiting & Raftery, 2007), respectively. We report the performance of all four reward combination methods in Table S1. By default, we use the spherical scoring rule because it demonstrates more superior results compared to the alternatives. HmaxTSE(oi) Hmax , and rsphe , rlog D.2 TEMPORAL SELF-CONSISTENCY VOTING AFTER RFT It is worthwhile to investigate whether Temporal Self-consistency Voting continues to provide performance benefits after Temporal Consistency Reinforcement. To this end, we conducted"
        },
        {
            "title": "Preprint",
            "content": "Table S2: Performance of temporal majority voting after reinforcement learning. Temporal self-consistency voting was applied to the model fine-tuned via temporal consistency reinforcement. The upper part is derived from the LLaDA-8B-Instruct model trained using the Negative TSE reward, whereas the lower part is based on the model trained with combination of TSE and accuracy reward. For reference, we include the oracle EverPass @1 as performance upper bound. Negative TSE reward After RFT Method / Seq Len + Temporal Voting Fixed Weighting Linear Weighting Exp. Weighting EverPass @1 TSE & accuracy reward After RFT + Temporal Voting Fixed Weighting Linear Weighting Exp. Weighting EverPass @1 GSM8K MATH500 SVAMP Countdown 128 72.2 70.4 72.3 72.6 +0. 80.5 72.1 71.2 73.0 72.6 +0.5 84. 256 78.8 77.6 78.8 79.2 +0.4 81. 80.0 79.6 81.9 81.1 +1.1 91.4 80.2 80.2 80.6 80.6 +0.4 81.3 83. 82.8 83.0 83.3 +0.3 87.9 128 30. 30.8 30.8 30.8 +0.2 33.4 31.2 31. 31.2 31.6 +0.4 43.6 256 34.6 34. 35.4 35.0 +0.4 37.4 35.4 35.6 36. 35.8 +0.4 49.2 512 38.0 37.6 38. 38.0 +0.0 40.0 41.4 41.2 41.0 41.4 +0. 52.0 128 84.3 85.0 85.3 85.3 +1. 88.7 85.0 85.3 86.7 86.3 +1.3 90. 256 89.0 88.3 88.3 89.0 +0.0 90. 90.3 90.7 90.3 90.7 +0.4 91.7 88.7 88.7 88.7 88.7 +0.0 89.7 92. 92.3 92.7 92.3 +0.0 92.7 128 38. 39.5 39.1 39.5 +0.9 44.1 41.5 40. 40.9 41.5 +0.0 54.3 256 53.5 53. 53.5 53.9 +0.4 59.8 42.6 41.5 42. 42.7 +0.1 68.0 512 44.9 45.7 45. 45.7 +0.8 55.5 54.7 53.2 54.2 55.1 +0. 70.3 Performance of temporal self-consistency voting and temporal consistency Table S3: reinforcement on the Sudoku dataset. The baseline model is obtained by applying supervised fine-tuning to LLaDA-8B-Instruct using the s1K dataset. Model Sudoku Method / Seq Len baseline + temporal voting accuracy reward (d1) + RFT negative TSE reward (ours) combining both (ours) 128 12. 12.5 23.2 15.2 27.5 256 6.7 6.1 5.5 2.8 17.8 12. 9.4 3.3 27.8 16.6 experiments using two models: the first is derived from the LLaDA-8B-Instruct model trained with the negative TSE reward, and the second is trained using the accuracy reward combined with TSE. We applied Temporal Self-Consistency Voting to both models, and the results are summarized in Table S2. The findings indicate that both models still exhibit performance improvements when temporal voting is applied, even after undergoing reinforcement learning. This suggests that Temporal Self-Consistency Voting and Temporal Consistency Reinforcement are complementary techniques, and can be effectively combined to further enhance model performance. D.3 FAILURE CASE ANALYSIS As discussed in Sec. 6, our method may rely on the models initial performance to achieve further improvements. To investigate this limitation, we take the challenging Sudoku dataset as case study. As shown in Table S3, directly applying our proposed temporal self-consistency voting and temporal consistency reinforcement with the negative TSE reward alone results in noticeable performance drop on the Sudoku dataset. For example, the original accuracy of the base model for the generation length of 512 is 5.5%, while the voting method achieves 2.8%, reflecting decline of 2.7%. To better understand this problem, we conduct deeper analysis of the models behavior during generation. We define metric called Temporal Accuracy, which quantifies the average correctness across all intermediate sampling steps in the generation process. Formally, let ei,t represent the correctness indicator for the i-th question at the t-th sampling step, where ei,t = 1 if the answer is correct, and 0 otherwise. Then, the Temporal Accuracy for dataset with examples and"
        },
        {
            "title": "Preprint",
            "content": "Temporal accuracy across datasets. Temporal Accuracy on Sudoku remains Figure S4: consistently low (all below 5%) across different settings, indicating that the model rarely generates correct answers during the sampling process. This scarcity of valid candidates severely limits the effectiveness of voting mechanisms, as there are insufficient correct outputs to reliably converge on the correct answer. sampling steps is computed as: TemporalAccuracy = 1 (cid:88) (cid:88) i=1 t=1 ei,t. (S1) As shown in Fig. S4, the Temporal Accuracy of the Sudoku is much lower than the other 4 datasets, where temporal self-consistency voting proved effective. On the Sudoku dataset, we observe that the Temporal Accuracy remains exceedingly lowbelow 5% on averageacross all intermediate steps. This low signal makes it difficult for temporal voting mechanisms to reliably identify the correct answer and for reinforcement learning signals to guide the model effectively. Interestingly, while RFT with only negative TSE reward leads to poorer results, combining TSE with accuracy reward can achieve better performance than using accuracy reward alone. We hypothesize that this is because the integration of TSE allows the model to receive more fine-grained rewards, rather than just binary outcomes of correct or incorrect. D.4 EXAMPLES OF TIME OSCILLATION We present representative examples of temporal oscillation from GSM8K, using LLaDA-8B-Instruct as the backbone model. Note that all examples shown below were ultimately incorrect but were classified as correct answers by our voting method, as described in Sec. 4.1. Correct answers are in green, incorrect answers in red. We use blue to highlight the key segments in each example. We observe that many questions become incorrect during temporal oscillation due to flawed reasoning paths. Although the model may initially provide the correct answer and follow logical reasoning path, an incorrect trajectory can ultimately lead to an erroneous response. notable example is seen in Example 3 below. Initially, the model correctly calculates the total cups, per-hour revenue, and the necessary per-hour cost deduction to derive the right profit per hour (step 54). However, during the reasoning process, it introduces erroneous masking regarding key calculation elementsthe per-hour cost to subtract. Even though the answer remains correct at first (step 61), the flawed reasoning trajectory eventually results in miscalculations and an incorrect final response (step 62)."
        },
        {
            "title": "Preprint",
            "content": "Example 1 Problem: Ruby is 6 times older than Sam. In 9 years, Ruby will be 3 times as old as Sam. How old is Sam now? Step 59: <reasoning> Let Sams age be years. Then Rubys age is 6x years. In 9 years, Rubys age will be 6x+9 and Sams age will be + 9. According to the problem, in 9 years, Ruby will be 3 times as old as Sam, so we have the equation 6x + 9 = 3(x + 9). Simplifying this equation, we get 3x = 0 so = 3. </reasoning> <answer> 6 </answer> Step 60: <reasoning> Let Sams age be years. Then Rubys age is 6x years. In 9 years, Rubys age will be 6x+9 and Sams age will be + 9. According to the problem, in 9 years, Ruby will be 3 times as old as Sam, so we have the equation 6x + 9 = 3(x + 9). Simplifying this equation, we get 3x = 0 = 0 </reasoning> <answer> 0 </answer> Example 2 Problem: Shiela bought five cell phones for $150 each for 3-month installment. 2% interest will be charged for each unit. How much will Shiela pay each month for 3 months Step 59: <reasoning> Shiela bought five cell phones for $150 each, so the total cost is $5 150 = $750. The interest for each unit is 2%, so the total interest is $750 0.02 = $15. SinceIELa will pay this amount over 3 months, the monthly payment is $750 + $15 = $<< . >> 7501. = 255. </reasoning> <answer> 255 </answer> Step 60: <reasoning> Shiela bought five cell phones for $150 each, so the total cost is $5 150 = $750. The interest for each unit is 2%, so the total interest is $7500.02 = $15. SinceIELa will pay this amount over 3 months, the monthly payment is $750 + $15 = $<< . >> 750/3 =>> 250. </reasoning> <answer> 250 </answer>"
        },
        {
            "title": "Preprint",
            "content": "Example 3 Problem: Millie decides to open lemonade stand. She spends $18 to buy enough supplies to make 3 pitchers of lemonade. Each pitcher holds 12 cups of lemonade. She sells each cup of lemonade for $1. She sells an average of 4 cups per hour that her lemonade stand is open. If Millie sells all of the lemonade, how much profit will she make per hour that she spends running the lemonade stand? Step 54: <reasoning> Millie spends $18 on supplies to make 3 pitchers of lemonade. Each pitcher holds 12 cups, so she has total of 3 * 12 = 36 cups of lemonade. She sells each cup for $1 and and sells an average of 4 cups per hour. Therefore, her revenue per hour is 4 * $1 = $4. To find per hour profit, we subtract the cost of supplies from the revenue: $4 - $2 = $2 per hour. </reasoning> <answer> 2 </answer> Step 61: <reasoning> Millie spends $18 on supplies to make 3 pitchers of lemonade. Each pitcher holds 12 cups, so she has total of 3 * 12 = 36 cups of lemonade. She sells each cup for $1 and and sells an average of 4 cups per hour. Therefore, her revenue per hour is 4 * $1 = $4. To find per hour profit, we subtract the cost of supplies from the revenue: $4 - $3 = $2 per hour. </reasoning> <answer> 2 </answer> Step 62: <reasoning> Millie spends $18 on supplies to make 3 pitchers of lemonade. Each pitcher holds 12 cups, so she has total of 3 * 12 = 36 cups of lemonade. She sells each cup for $1 and and sells an average of 4 cups per hour. Therefore, her revenue per hour is 4 * $1 = $4. To find per hour profit, we subtract the cost of supplies from the revenue: $4 - $3 = $1 per hour. </reasoning> <answer> 1 </answer>"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Stanford University",
        "Zhejiang University",
        "Zhejiang University of Technology"
    ]
}