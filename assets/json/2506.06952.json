{
    "paper_title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer",
    "authors": [
        "Ying Shen",
        "Zhiyang Xu",
        "Jiuhai Chen",
        "Shizhe Diao",
        "Jiaxin Zhang",
        "Yuguang Yao",
        "Joy Rimchala",
        "Ismini Lourentzou",
        "Lifu Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models."
        },
        {
            "title": "Start",
            "content": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer Ying Shen* 1 Zhiyang Xu* 2 Yuguang Yao5 Joy Rimchala5 Jiuhai Chen Shizhe Diao4 Ismini Lourentzou 1 Lifu Huang 6 Jiaxin Zhang5 5 2 0 2 8 ] . [ 1 2 5 9 6 0 . 6 0 5 2 : r 1University of Illinois Urbana-Champaign 2Virginia Tech 3 University of Maryland 5Intuit AI Research ying22@illinois.edu, zhiyangx@vt.edu 4Nvidia 6UC Davis"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling wide range of vision-language tasks within single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), novel and efficient architecture that unifies image understanding and generation within single multimodal model. LaTtE-Flow builds upon powerful pretrained VisionLanguage Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for distinct subset of timesteps. This design significantly improves sampling efficiency by activating only small subset of layers at each sampling timestep. To further enhance performance, we propose Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6 faster inference speed compared to recent unified multimodal models."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multimodal foundation models that can perform both image understanding and generation have opened promising avenues for building unified architectures performing wide range of vision-language tasks [32, 42, 45, 51, 6, 25, 39]. Such unified multimodal models hold great potential for building general-purpose agents that can interpret, reason about, and generate multimodal content in response to user instructions. Current approaches to unified multimodal modeling generally fall into two broad categories. The first category leverages vector-quantized autoencoders [40, 9, 48] to discretize images into token sequences, which are then incorporated into the vocabulary of Large Language Models (LLMs) [37, 42, 45, 43, 6, 44]. These models are subsequently trained to autoregressively generate the next token, either textual or visual, thus *Ying Shen and Zhiyang Xu contributed equally to this work. Equal supervision. 1Code and model checkpoints can be found at https://github.com/yingShen-ys/LaTtE-Flow. Preprint. Under review. Figure 1: Comparison of the flow-matching process between standard diffusion / flow-matching models and our proposed LaTtE-Flow. Unlike diffusion / flow-matching based models, which invoke the entire model at each sampling timestep, LaTtE-Flow activates only subset of layers at each step, improving efficiency. integrating vision and language generation within single framework. The second category leverages diffusion-based methods, either by coupling LLMs with external diffusion modules or by training LLMs to directly perform denoising steps [51, 32, 25, 39, 12]. Despite significant progress, existing unified multimodal models tend to struggle to achieve high performance in both multimodal understanding and image generation, as improvements in one modality often come at the expense of the other. Even when strong performance is achieved in both, it often comes with substantial computational overhead. These unified models are often computationally intensive, with slow inference that hinders practical deployment. For example, unified models that leverage diffusion or flow-matching processes typically require dozens of forward passes through the full backbone model during inference, resulting in slow inference and high resource consumption [30]. Similarly, autoregressive approaches suffer from long decoding times, especially for high-resolution images that require generating large numbers of tokens sequentially [46]. To address these challenges, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), novel architecture that unifies efficient image generation and multimodal understanding within single model. In particular, LaTtE-Flow introduces two key architectural innovations designed to enable efficient and high-quality image generation. First, we propose novel Layerwise Timestep Expert architecture, which reduces the sampling time complexity by distributing the flow-matching process across groups of transformer layers. Instead of invoking the entire model across all time steps, LaTtE-Flow partitions transformer layers into disjoint groups, each assigned to specific range of timesteps in the flow-matching process, as shown in Figure 1. During inference, only the relevant expert group is activated at each timestep, which drastically reduces computation while preserving generation quality. Second, we introduce Timestep-Conditioned Residual Attention, lightweight mechanism that enables later layers to reuse self-attention maps computed at earlier layers, modulated by the current timestep. This design encourages the model to gradually refine features across layers, resulting in faster convergence during training. In summary, our contributions are: (1) We propose LaTtE-Flow, an efficient and unified multimodal architecture that integrates flow-matching-based image generation with pre-trained vision-language models. (2) We introduce Layerwise Timestep Expert, novel design that significantly reduces inference complexity by distributing transformer layers into timestep-specific experts. (3) We design Timestep-Conditioned Residual Attention module, which enables effective reuse of attention information across layers, boosting training efficiency and performance. (4) Extensive experiments demonstrate that LaTtE-Flow achieves competitive performance on both generation and understanding tasks, while offering 6 faster inference compared to recent unified models."
        },
        {
            "title": "2 Related Work",
            "content": "Unified Models. Unified multimodal architectures integrate multimodal understanding and generation within single model, enabling general-purpose agents that can interpret and generate multimodal 2 content in response to user instructions [32, 42, 45, 51, 6, 25, 39]. Existing approaches to unified modeling primarily fall into two categories: The first class of models relies on vector-quantized autoencoders [40, 9, 48] to convert images into discrete token sequences that can be processed similarly to text. These visual tokens are added to the LLM vocabulary to enable unified autoregressive training over both language and vision [37, 42, 45, 43, 6, 44]. The second class incorporates continuous generative processes, most notably diffusion models [15] or flow-matching models [22]. Some approaches connect LLMs with external diffusion modules, using the language model to guide image generation [39, 12, 26, 4, 47], while others directly train LLMs to jointly perform denoising or flow-matching steps [51, 32, 25]. Despite progress in both categories, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. Multiple Experts in Diffusion Models. Recent advancements in diffusion models have increasingly adopted modular or expert-based architectures for better image generation [36, 31]. Building on this direction, several recent approaches have explored the use of expert models tailored to different diffusion timesteps [18, 10, 52]. By allocating distinct experts to specific temporal intervals, these models aim to better capture the evolving nature of the denoising process. This design is partly motivated by findings from prior work [13, 2], which show that optimization gradients from different timesteps often conflict, leading to slower convergence and degraded model performance. However, these models typically maintain near full-parameter expert network for different timestep intervals, which leads to little or no improvement in inference efficiency under fixed number of sampling steps. In contrast, we introduce layerwise timestep expert architecture, which partitions the transformer layers into different groups of layers, each responsible for specific range of timesteps. At inference time, only the corresponding group is activated, significantly reducing the number of parameters involved at each step. Moreover, our design allows all expert groups to be trained jointly, and we further integrate it within unified model architecture, enhancing both efficiency and performance."
        },
        {
            "title": "3 Preliminaries",
            "content": "Flow-Matching. Flow-based generative models [22, 23, 1] aim to learn time-dependent velocity field vt that transports samples from simple source distribution p0(x) (e.g., standard Gaussian) to complex target distribution p1(x) via an ordinary differential equation (ODE): dxt dt = vt(xt), x0 p0(x). (1) Recently, Lipman et al. [22] propose simple simulation-free Conditional Flow Matching (CFM) objective by defining conditional probability path pt(xt x1) and the corresponding conditional vector field ut(xt x1) per sample x1. The model directly regresses the velocity vt on conditional vector field ut( x1): Et,p1(x1),pt(xtx1)vt(xt, t) ut(xt x1) (2) where ut( x1) uniquely determines conditional probability path pt( x1) towards target data sample x1. widely adopted choice for the conditional probability path is linear interpolation between the source and target data [23]: xt = tx1 + (1 t)x0. Assuming the source distribution p0 2I). Sampling from the learned model can is standard Gaussian, this yields xt (tx1, (1 t) be obtained by first sampling x0 (x 0, 1) and then numerically solving the ODE in Eq. (1). 2,"
        },
        {
            "title": "4 LaTtE-Flow",
            "content": "We present LaTtE-Flow (Layerwise Timestep-Expert Flow-based Transformer), novel architecture designed for efficient and high-quality image generation and multimodal understanding, unified within single model. Built on top of pretrained Vision-Language Models (VLMs), LaTtE-Flow leverages their powerful understanding capabilities while introducing additional flow-matching based generation components to enable scalable and effective image synthesis. To unify generation and understanding effectively, we explore two architecture designs: LaTtE-Flow Couple and LaTtEFlow Blend, illustrated in Figure 2. These variants differ primarily in how the generative and understanding components are combined within the Transformer layers (Section 4.1). Furthermore, we introduce two core architectural innovations applicable to both variants to enhance image generation efficiency and quality: (1) Layerwise Timestep Experts (Section 4.2), 3 Figure 2: LaTtE-Flow overall architecture. which partition the model into timestep-specialized modules to reduce sampling complexity, and (2) Timestep-Conditioned Residual Attention (Section 4.3), which injects timestep-aware residual attention into each attention layer through gating mechanisms modulated by learned timestep embedding, improving training efficiency through effective information reuse across layers. 4.1 LaTtE-Flow Layer Design LaTtE-Flow Couple preserves the pretrained VLM entirely, keeping its parameters frozen (shown in purple in Figure 2) to retain strong multimodal understanding without finetuning. To enable image generation, it introduces trainable generative pathway alongside the frozen backbone. Specifically, each Transformer layer is augmented with trainable replica of the original VLM layer, along with additional components for flow-matching-based generation (shown in blue in Figure 2). LaTtE-Flow Couple thus allows the model to perform image synthesis while leveraging the robust understanding capabilities of the pretrained VLM. LaTtE-Flow Blend unifies the image generation and understanding components through partially shared transformer layer. Here, each layer consists of task-specific submodules with separate parameters for generation and understanding, and set of shared submodules that are used by both tasks. This design enables tighter fusion between generation and understanding signals, facilitating more effective information exchange while maintaining flexibility to specialize for each modality. As illustrated in Figure 2, both LaTtE-Flow variants introduce LaTtE-Flow Attention module to enable effective interaction between generative image latents and multimodal context. Specifically, the noisy image latentsused during the flow-based generation processattend to the text and visual context tokens, as detailed in Appendix A. This attention module employs hybrid positional encoding scheme, combining the original 3D Rotary Positional Embeddings (RoPE) [35], inherited from the pretrained VLM, for encoding spatial and temporal structure in the multimodal context, with newly introduced 2D positional encodings applied to the generative image tokens. 4.2 Layerwise Timestep Experts Typical sampling procedures in diffusion models [34, 15] or flow-matching models [22, 23, 1] require repeatedly invoking the full network across large number of timesteps, leading to slow inference-time speed. For instance, consider standard diffusion transformer (DiT) model [27] with transformer layers. The effective computational cost for sampling steps is O(L ), as shown in Figure 1 (a). To alleviate this inefficiency, we introduce novel Layerwise Timestep Expert architecture, which reduces the effective sampling time complexity by distributing the flow-matching process across groups of transformer layers. Specifically, instead of executing the entire model at every timestep, we partition the transformer layers into non-overlapping groups, where each group specializes in denoising samples within specific timestep interval, as illustrated in Figure 1 (b). This design effectively enables efficient sampling, as only subset of the network needs to be executed at each timestep. Let each expert group be denoted as Gl,l+M = {l, + 1, . . . , + }, consisting of = L/K consecutive layers (from layer to layer + ). During training, each layer group learns to predict the velocity field over its assigned timestep interval [tk, tk+1] using layerwise flow-matching loss. Specifically, each layer group Gl,l+M receives the noisy latent image xt RNxd along with the multimodal context ml, derived from the preceding layer 1, and predicts the velocity field sθ(xt, ml, t). Formally, for timestep [tk, tk+1], the layerwise flow-matching loss is defined as: Lt = Et,p1(x1),pt(xtx1) Gl,l+M (cid:194)(cid:194)(cid:194)(cid:194)(cid:194) (xt, ml, t) ut(xt x1) 2 (cid:194)(cid:194)(cid:194)(cid:194)(cid:194) , for [tk, tk+1], (3) where Gl,l+M () denotes the prediction produced by the expert group and ut(xt x1) is the ground-truth velocity at timestep t. By training each group exclusively on its respective timestep interval, LaTtE-Flow encourages timestep specialization, allowing the model to learn timestepspecific representations across the flow-matching process. Inference. At inference time with sampling steps, we begin by precomputing the multimodal hidden states required for conditioning at each transformer layer. These multimodal representations are computed once at the start of inference and cached for reuse across all timesteps. Then, for each timestep [tk, tk+1], only the associated expert layer group Gl,l+M is activated to perform forward pass from layer to layer . This process is repeated across all timesteps, with only = L/K layers evaluated per step. Compared to standard diffusion models or flow-matching models that execute all layers at every step, this design significantly reduces the inference-time complexity from O(L ). This leads to significant reduction in computational cost and latency during generation, without sacrificing generation quality. ) to O(M 4.3 Timestep-Conditioned Residual Attention To facilitate information reuse across transformer layers and improve both training efficiency and generative performance, we propose Timestep-Conditioned Residual Attention, novel mechanism that introduces adaptive residual connections between successive image attention layers based on the current timestep. The goal is to enable later layers to reuse and refine the attention patterns computed in earlier layers, while dynamically controlling the influence of past attention through the current flow-matching timestep. Let Al RNxNx image self-attention matrix at layer l, where Nx is the number of image tokens. In standard self-attention layer, the attention matrix is computed as: = Softmax ( (hW ) )(hW ) , (4) where RNxd denotes the hidden states of the noisy image latents, and Q, Rdd are learnable query and key projection matrices. Figure 3: Timestep-conditioned residual attention 5 To incorporate residual attention from the previous layer, we define the augmented self-attention matrix at layer + 1 as: Al+1 = Al+1 + g(t) Al, g(t) = tanh(htWt), (5) where ht Rd is the embedding of the current flow-matching timestep and Wt RdH is trainable projection matrix, with denoting the hidden dimension and the number of attention , produced by tanh() activation, dynamically heads. The head-wise gating vector g(t) (1, 1) controls the extent to which each attention head incorporates residual attention information from the previous layer. The operator denotes element-wise multiplication, broadcast across all attention heads. Notably, while the LaTtE-Flow Attention module jointly processes both noisy image states and multimodal hidden states, the residual attention mechanism is applied only to the self-attention map over the noisy image hidden states, as shown in Figure 3. The timestep-conditioned residual attention mechanism enables the model to dynamically control how much residual attention from the previous layer is incorporated into the current layer, on per-head basis and conditioned on the timestep. Empirically, this design accelerates convergence during training and enhances the quality of generated images."
        },
        {
            "title": "5 Experiment Setup",
            "content": "Backbone Model and Image Encoder. LaTtE-Flow is built upon Qwen2-VL-2B-Instruct [41], pretrained VLM composed of = 28 transformer layers. In the LaTtE-Flow Couple variant, we create trainable copy of each Transformer layer from the original Qwen2-VL-2B-Instruct and integrate it with additional components tailored for flow-matching-based image generation. These duplicated components are initialized with the corresponding pretrained weights from the original VLM. For image encoding, we adopt the recently proposed Deep Compression Autoencoder (DC-AE) [5], which compresses raw image pixels into compact latent space using 32 down-sampling ratio. Timestep Distribution. To enable Layerwise Timestep Experts, LaTtE-Flow partitions the model into = 4 non-overlapping layer groups, each containing = 7 consecutive layers for the final results. These groups are designed to operate over distinct intervals of the flow-matching timesteps. During training, we use = 1000 flow-matching steps, which are initially divided uniformly into four intervals. To encourage robustness near interval boundaries and promote smooth transitions across groups, we introduce 100-step overlap between adjacent timestep intervals during training. This overlap allows boundary timesteps to be seen by multiple layer groups, improving generalization. At inference time, we disable the overlaps to maintain strict partitioning of timestep intervals. Consequently, at each denoising step, only the corresponding expert layer group is activated, requiring just = 7 layers per inference step. This contrasts favorably with standard diffusion or flow-matching models that activate all = 28 layers at every step, significantly enhancing generation efficiency. Further details are provided in Appendix B. Baseline Architectures. We construct two baseline models: Vanilla Couple and Vanilla Blend, which match the architectures of LaTtE-Flow Couple and LaTtE-Flow Blend, respectively, but exclude both the Layerwise Timestep Experts and Timestep-Conditioned Residual Attention mechanisms, allowing us to directly evaluate the effectiveness of these proposed mechanisms. The Vanilla Couple baseline retains parallel generative path alongside the original VLM modules. Conceptually, it resembles prior models such as LMFusion [32], which augment language models with separate branch for handling image generation. In contrast, Vanilla Blend unified generation and understanding computations within shared layers, akin to the design of Transfusion [51]. Training and Evaluation Details. All LaTtE-Flow variants (Blend and Couple) are trained on 1.2M images from ImageNet [7] training split at resolution of 256 256 with global batch size of 2048 and constant learning rate of 5e-4 for 240K steps. For Vanilla Blend and LaTtE-Flow Blend, we perform full parameter fine-tuning, and for Vanilla Couple and LaTtE-Flow Couple, we only fine-tune parameters specialized for image generation while keeping parameters for image understanding frozen. For evaluation, we report FID, Inception Score, Precision, and Recall on ImageNet following previous convention [27]. Additional details can be found in Appendix B."
        },
        {
            "title": "6 Results and Discussion",
            "content": "6.1 Image Generation and Understanding Results s i e d a s o s o fi s o Model ADM [8] CDM [16] LDM-4-G [29] DiT-L/2 [27] DiT-XL/2 [27] MaskGIT [3] MAGE [20] VQVAE-2 [28] VQGAN [9] VQGAN [9] ViT-VQGAN [48] RQTran. [17] Show-o [45] Janus Pro [6] Vanilla Blend (Ours) LaTtE-Flow Blend (Ours) Vanilla Couple (Ours) LaTtE-Flow Couple (Ours) FID 10.94 4.88 3.60 5.02 2.27 6.18 6.93 31.11 18.65 15.78 4.17 7.55 31.26 23.68 6.12 6.03 6.33 5.79 IS Pre Rec #Params #Step Time (s / img) Rel. Time 101.0 158.7 247.7 167.2 278.2 182.1 195.8 45 80.4 74.3 175.1 134.0 98.7 105.2 193.7 193.9 192.4 213.1 0.69 0.75 0. 0.80 0.36 0.78 0.55 0.58 0.78 0.77 0.80 0.78 0.63 0.57 0.57 0.51 0.57 0.26 0.69 0.49 0.69 0.68 0.67 0.69 554M 400M 458M 675M 227M 230M 13.5B 227M 1.4B 1.7B 3.8B 1.3B 1.5B 2.0B 500M 2.0B 500M 250 8100 250 250 8 5120 256 256 1024 68 50 576 40 40 40 40 9.677 1.786 2.592 0.029 1.094 1.382 1.382 1. 2.493 0.311 0.185 0.061 0.158 0.052 168 31 45 0.5 19 24 24 21 48 6 4 1 3 Table 1: Comparison of generative models across FID, IS, Precision, Recall, parameters, steps, and inference time on ImageNet-50K. For LaTtE-Flow, we report the number of parameters activated per timestep, given that it has timestep-expert architecture where only subset of layers is used at each step. We also report inference time relative to LaTtE-Flow Couple. : taken from MaskGIT [3] Model MMBench SEED POPE MM-Vet MME-P MMMU RWQA TEXTVQA EMU2 Chat 34B [37] Chameleon 7B [38] Chameleon 34B [38] Seed-X [12] 17B VILA-U 7B [44] EMU3 8B [42] MetaMorph 8B [39] Show-o 1.3B [45] Janus 1.5B [43] Janus Pro 1.5B [6] LaTtE-Flow Couple 2B - 19.8 32.7 70.1 66.6 58.5 75.2 - 69.4 75.5 74. 62.8 27.2 - 66.5 57.1 68.2 71.8 - 63.7 68.3 72.4 - 19.4 59.8 84.2 85.8 85.2 - 80.0 87.0 86.2 87.3 48.5 8.3 9.7 43.0 33.5 37.2 - - 34.3 39.8 51.5 - 202.7 604.5 1457.0 1401.8 1243.8 - 1097.2 1338.0 1444.0 1501.4 34.1 22.4 38.8 35.6 32.2 31.6 41.8 27.4 30.5 36.3 41.1 - 39.0 39.2 - 46.6 57.4 58.3 - - - 60. 66.6 0.0 0.0 - 48.3 64.7 60.5 - - - 79.7 Table 2: Results on comprehensive image understanding benchmarks. Best scores are highlighted in bold. Since our LaTtE-Flow Couple is an expert architecture, we report the number of activated parameters used for image understanding. We evaluate LaTtE-Flow on both image generation  (Table 1)  and multimodal understanding  (Table 2)  tasks. Table 1 reports quantitative comparison between LaTtE-Flow, recent unified models, and leading image generation models. We evaluate each model in terms of generation quality, activated parameters for each inference step, and inference efficiency. All inference times are measured on single NVIDIA L40 GPU with batch size 50. LaTtE-Flow achieves better FID scores compared to state-of-the-art unified models [45, 43, 6] that are pretrained on the mixture of ImageNet and other large-scale image-caption datasets, while achieving mush faster inference speed, i.e., 48 faster than Show-o [45] and 6 faster than Janus Pro [6]. Moreover, both LaTtE-Flow variants outperform their respective baselines, Vanilla Blend and Vanilla Couple, which are conceptually similar to Transfusion [51] and LMFusion [32], with much fewer activated parameters per flow-matching step and 3 to 4 faster inference speed. In addition, LaTtE-Flow exhibits competitive performance compared to diffusion models [8, 16, 29, 27], Masked Models [3, 20] and Auto-regressive (AR) models [28, 9, 48, 17] that are specialized for image generation, achieving better parameter and inference-time efficiency. These results suggest LaTtE-Flow as promising, efficient, and effective architecture for image generation. Qualitative results on ImageNet are provided in Appendix C. 7 Table 2 presents results on multimodal understanding benchmarks [24, 19, 21, 49, 11, 50, 33]. LaTtEFlow Couple achieves competitive or superior performance compared to recent unified models, demonstrating its ability to effectively leverage frozen vision-language backbones by inheriting their strong capability without additional finetuning for understanding tasks. 6.2 Ablation Studies Faster Convergence Rate of LaTtE-Flow. Figure 4 illustrates the training dynamics of LaTtEFlow Blend and LaTtE-Flow Couple compared to Vanilla Blend and Vanilla Couple. Figure 4: Training dynamics of LaTtEFlow vs. baselines. FID on ImageNet 50K. We observe that both LaTtE-Flow Blend and LaTtEFlow Couple exhibit significantly faster convergence rate during training, reaching competitive image generation performance (lower FID) in fewer training steps. We attribute this favorable property of LaTtEFlow to the layerwise timestep-expert architecture. As noted in prior work [2, 13], the slow convergence of diffusion models is partially due to the conflicting optimization directions of different timesteps. Optimizing for timesteps that are close can benefit each other, while optimizing timesteps that are far away can interfere with each other. Our layerwise timestep-expert architecture alleviates this challenge by distributing timesteps across different transformer layers. Impact of Varying Group Size. We also investigate how the timestep-expert group size affects the trade-off between generation quality and inference efficiency. Specifically, we train LaTtE-Flow Couple with group sizes {4, 7, 14}, corresponding to partitioning the transformer layers into 7, 4, and 2 expert groups, respectively. Figure 5 reports results at 120K training steps. We observe that larger group sizes consistently improve generation quality, as measured by FID, due to increased modeling capacity. However, this comes at the cost of reduced inference speed, since more layers are executed per timestep. Both = 7 and = 14 achieve better generation quality and efficiency compared to the baseline Vanilla Couple (Vanilla), which applies all 28 layers at every step. Thus, considering the trade-off between performance and efficiency, we select = 7 as the default group size in our main results in Table 1, which offers strong generation quality with substantial sampling speedups. of Timestep-Conditioned Effect Residual Attention. To quantify the effect of timestep-conditioned residual attention, we compare LaTtE-Flow Couple against variant with the timestep-conditioned residual attention removed. As shown in Table 3, removing residual attention leads to notable degradation across multiple metrics, highlighting the effectiveness of time-conditioned attention across layers. Adding timestep-conditioned residual attention does not introduce additional inference time cost. Effect of Sampling Steps and CFG. Figure 6 shows the impact of varying the number of sampling steps and classifier-free guidance scale (CFG) on image generation quality. We observe that increasing the number of steps generally improves image generation quality, leading to lower FID and higher Inception Score. However, as the number of sampling steps surpasses 40, performance improvements become marginal. In general, higher CFG leads to better Inception Score, but for FID, once the CFG goes beyond 5, performance starts to decrease slightly. Figure 5: Effect of group size in LaTtE-Flow Couple. Table 3: Effect of time-conditioned residual attention. LaTtE-Flow Couple - w/o Residual Attention IS Pre Rec 213.1 157.0 0.69 0.61 5.79 8.26 0.78 0.75 Model FID Timestep Condition in Residual Attention. To better understand the role of timestep conditioning in residual attention, we perform an in-depth analysis on both LaTtE-Flow Couple and LaTtE-Flow 8 Figure 6: Impact of # sampling steps and CFG strength on Inception Score and FID. Figure 7: Timestep-conditioned residual attention analysis. (a) Visualization of attention behavior in Vanilla Couple and (b) learned residual gating patterns in LaTtE-Flow Couple. Blend. Specifically, we first investigate how attention patterns evolve across transformer layers and sampling timesteps in baseline models. We quantify the sequential similarity between adjacent layers at each timestep using total variation-based metric: S(Al, Al+1 ) = 1 0.5 (cid:187)(cid:187)(cid:187)(cid:187)(cid:187) Softmax (Al i) Softmax (Al+1 ) , (cid:187)(cid:187)(cid:187)(cid:187)(cid:187) (6) where Softmax (Al reflect greater similarity in image attention maps between successive layers. i) is the softmax-normalized i-th row of attention map Al. Higher values of Figure 7 (a) shows how sequential similarity in Vanilla Couple evolves throughout the sampling process, averaged over 100 randomly selected samples. We observe that early in sampling, attention maps across layers show low similarity, but as generation progresses, especially in later timesteps, similarity increases, sometimes approaching 1.0 in early layers. This motivates using residual attention for efficient reuse, with dynamic gating needed to adapt to varying similarity patterns across timesteps. Figure 7 (b) shows timestep-conditioned residual attention gates in LaTtE-Flow Couple, which modulate how much past-layer attention is reused. As seen across all heads (Figure 11), gating remains stable across timesteps within head but varies between heads, indicating specialization. These results highlight the effectiveness of dynamic, head-specific residual attention in flow-matching generation. Results for LaTtE-Flow Blend are in Appendix D."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we present Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), novel efficient architecture that unifies image understanding and generation within single multimodal model. LaTtE-Flow introduces two key novel architectural innovations: Layerwise Timestep 9 Experts, which reduces sampling complexity by specializing transformer layers to distinct timestep intervals, and Timestep-Conditioned Residual Attention, which facilitates adaptive reuse and refinement of attention structures across layers. Extensive experimental evaluations demonstrate that LaTtE-Flow not only achieves strong multimodal understanding and image generation performance, but also achieves up to 48 faster inference compared to existing unified models."
        },
        {
            "title": "References",
            "content": "[1] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations (ICLR), 2023. [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [4] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025. URL https://arxiv.org/abs/2505.09568. [5] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In International Conference on Learning Representations (ICLR), 2025. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [10] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Remix-dit: Mixing diffusion transformers for multi-expert denoising. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [12] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [13] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In International Conference on Computer Vision (ICCV), 2023. [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications (NeurIPS), 2022. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [16] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. In Journal of Machine Learning Research (JMLR), 2022. [17] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [18] Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. Multi-architecture multi-expert diffusion models. In AAAI Conference on Artificial Intelligence (AAAI), 2024. [19] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [20] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [21] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Empirical Methods in Natural Language Processing (EMNLP), 2023. [22] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations (ICLR), 2023. [23] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations (ICLR), 2023. [24] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, European Conference on Computer Vision (ECCV), 2024. [25] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [26] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. CoRR, abs/2504.06256, 2025. doi: 10.48550/ARXIV. 2504.06256. URL https://doi.org/10.48550/arXiv.2504.06256. [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In International Conference on Computer Vision (ICCV), 2023. [28] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [30] Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, et al. Efficient diffusion models: survey. arXiv preprint arXiv:2502.06805, 2025. 11 [31] Minglei Shi, Ziyang Yuan, Haotian Yang, Xintao Wang, Mingwu Zheng, Xin Tao, Wenliang Zhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu, et al. Diffmoe: Dynamic token selection for scalable diffusion transformers. arXiv preprint arXiv:2503.14487, 2025. [32] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [33] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [34] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. [36] Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, and Nan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing. In International Conference on Learning Representations (ICLR), 2025. [37] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [38] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [39] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [40] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [43] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [44] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. In International Conference on Learning Representations (ICLR), 2025. [45] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In International Conference on Learning Representations (ICLR), 2025. [46] Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, et al. Autoregressive models in vision: survey. In Transactions on Machine Learning Research (TMLR), 2025. 12 [47] Zhiyang Xu, Minqian Liu, Ying Shen, Joy Rimchala, Jiaxin Zhang, Qifan Wang, Yu Cheng, and Lifu Huang. Modality-specialized synergizers for interleaved vision-language generalists. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= 7UgQjFEadn. [48] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In International Conference on Learning Representations (ICLR), 2022. [49] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning (ICML), 2024. [50] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2024. [51] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In International Conference on Learning Representations (ICLR), 2025. [52] Shaobin Zhuang, Yiwei Guo, Yanbo Ding, Kunchang Li, Xinyuan Chen, Yaohui Wang, Fangyikang Wang, Ying Zhang, Chen Li, and Yali Wang. Timestep master: Asymmetrical mixture of timestep lora experts for versatile and efficient diffusion models in vision. arXiv preprint arXiv:2503.07416, 2025. 13 LaTtE-Flow Attention Module Figure 8 illustrates the architecture of the LaTtE-Flow Attention module. Our framework applies 3D Rotary Positional Embeddings (RoPE) [35] from the pretrained VLM to multimodal hidden states and uses new 2D Rotary Positional Embeddings to the generative image tokens. We adopt bi-directional attention on generative image tokens, and all generative image tokens are allowed to attend to previous multimodal tokens. Figure 8: LaTtE-Flow Attention"
        },
        {
            "title": "B Implmentation Details",
            "content": "Timestep Distribution. To enable Layerwise Timestep Experts, LaTtE-Flow partitions the model into = 4 non-overlapping layer groups, each containing = 7 consecutive layers for the final results. These groups are designed to operate over distinct intervals of the flow-matching timesteps. During training, we use = 1000 flow-matching steps, which are initially divided uniformly into four intervals: [1000.0, 750.25], [750.25, 500.50], [500.50, 250.75], and [250.75, 0]. To encourage robustness near interval boundaries and promote smooth transitions across groups, we introduce 100-step overlap between adjacent timestep intervals during training. This overlap allows boundary timesteps to be seen by multiple layer groups, improving generalization. Specifically, layers 1 through 7 are assigned to the timestep interval [1000, 700], layers 8 through 14 cover [700, 450], layers 15 through 21 operate on [450, 200], and layers 22 through 28 handle the final interval [200, 0]. Each group is trained exclusively on its assigned range according to Eq. (3), enabling it to specialize in the velocity prediction of that particular segment of the flow-matching timestep interval. At inference time, we disable the overlaps to maintain strict partitioning of timestep intervals. Consequently, at each denoising step, only the corresponding expert layer group is activated, requiring just = 7 layers per inference step. This contrasts favorably with standard diffusion or flow-matching models that activate all = 28 layers at every step, significantly enhancing generation efficiency. Training and Evaluation Details. We train all model variants on eight H200 for approximately four days. During training, following previous approaches, we employ classifier-free guidance [14] to guide the sampling process for better sampling quality by amplifying the difference between conditional and unconditional generation with the guidance scale > 1. During training, we randomly drop the multimodal condition with probability 10% to facilitate unconditional prediction. For evaluation, each model generates 50 images for each of 1,000 classes in ImageNet with 40 sampling steps and classifier-free guidance (CFG) of 5 based on our ablation study in Section 6.2. We report FID and Inception Score of 50K generated images against 50K real images from the ImageNet 14 Figure 9: Generated 256256 samples by LaTtE-Flow Couple trained on ImageNet. validation split. Following previous convention [27], we compute Precision and Recall using 1,000 generated images. All scores are calculated using standard implementations from torch-fidelity 2."
        },
        {
            "title": "C Qualitative Results",
            "content": "Figure 9 shows the qualitative results of sampled 256 256 images by LaTtE-Flow Couple. Timestep-Conditioned Residual Attention Following the experimental setup in Section 6.2, we also perform an in-depth analysis on the LaTtEFlow Blend variant. Figure 10 (a) shows how this sequential similarity across adjacent layers evolves over the sampling timesteps. The plot shows the mean similarity computed across 100 randomly sampled examples. We observe that for most of the adjacent layers, the sequential similarity is relatively low at early timesteps, and gradually increases as the timestep progresses, particularly in early layers, where the similarity rises and approaches 1.0. However, the observed similarity pattern varies significantly across timesteps and layers, motivating the need for timestep-conditioned gating strategy of residual attention flows. In Figure 10 (b), we visualize the learned residual attention gating values for head 11 within LaTtEFlow Blend. These gates are dynamically modulated by timestep embeddings and control the degree to which residual attention from the previous layer is incorporated into the current layers computation. To further understand the role of residual attention across heads, Figure 12 displays the gating values for all 12 heads in LaTtE-Flow Blend. We observe that gating remains relatively stable across timesteps within specific head, but the patterns differ notably among different heads. similar trend is also observed in the LaTtE-Flow Couple variant (Figure 11), where head-specific gating patterns reflect different behaviors. In summary, these results validate the design of timestep-conditioned, head-specific residual attention. The gating mechanism enables adaptive reuse of earlier attention. 2https://github.com/toshas/torch-fidelity Figure 10: Visualization of attention in Baseline Blend and LaTtE-Flow Blend. (a) Sequential similarity between adjacent layers increases over timesteps, particularly in early layers. (b) Residual attention gating in LaTtE-Flow Blend (head 11) shows relatively consistent gating values across timesteps within the same head. Figure 11: Timestep-conditioned residual attention gates across transformer layer in LaTtEFlow Couple. White regions indicate positions without gating values since residual attention is applied only within predefined layer groups. Notably, different heads exhibit distinct gating dynamics, with some emphasizing earlier timesteps, while others modulate more strongly in later layers, suggesting head-specific specialization in residual attention."
        },
        {
            "title": "E Impact Statement",
            "content": "This work advances the field of unified multimodal modeling by introducing LaTtE-Flow, an architecture that effectively combines image understanding and generation within single, efficient framework. By leveraging pretrained vision-language models and introducing novel architectural mechanisms, Layerwise Timestep Experts and Timestep-Conditioned Residual Attention, LaTtEFlow achieves strong performance with significantly improved inference speed. The proposed model has potential impact in both academic and practical settings, as scalable solution for building efficient, unified multimodal foundation models. It enables more efficient deployment of multimodal systems in resource-constrained environments, such as mobile devices or real-time applications, while maintaining high performance. While LaTtE-Flow improves performance and efficiency, it inherits the biases of its pretrained vision-language foundation and may generate misleading or inappropriate 16 Figure 12: Timestep-conditioned residual attention gates across transformer layer in LaTtEFlow Blend. White regions indicate positions without gating values since residual attention is applied only within predefined layer groups. Notably, different heads exhibit distinct gating dynamics, with some emphasizing earlier timesteps, while others modulate more strongly in later layers, suggesting head-specific specialization in residual attention. outputs if not properly constrained. Careful evaluation and mitigation of such risks are important for downstream deployment."
        },
        {
            "title": "F Limitations",
            "content": "Although LaTtE-Flow achieves substantial improvements in sampling efficiency with strong results in multimodal understanding and generation tasks, several limitations remain. First, our experiments involved training LaTtE-Flow for only 240K optimization steps, significantly fewer than existing unified multimodal models. Extending the training duration could potentially enhance the models performance further. Second, while our uniform timestep distribution with overlapping intervals proved effective, the optimal timestep distributions or layer partitioning strategies remain an open problem. Future work should systematically explore and optimize these timestep partitioning strategies."
        }
    ],
    "affiliations": [
        "Intuit AI Research",
        "Nvidia",
        "UC Davis",
        "University of Illinois Urbana-Champaign",
        "University of Maryland",
        "Virginia Tech"
    ]
}