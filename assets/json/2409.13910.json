{
    "paper_title": "Zero-shot Cross-lingual Voice Transfer for TTS",
    "authors": [
        "Fadi Biadsy",
        "Youzheng Chen",
        "Isaac Elias",
        "Kyle Kastner",
        "Gary Wang",
        "Andrew Rosenberg",
        "Bhuvana Ramabhadran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce a zero-shot Voice Transfer (VT) module that can be seamlessly integrated into a multi-lingual Text-to-speech (TTS) system to transfer an individual's voice across languages. Our proposed VT module comprises a speaker-encoder that processes reference speech, a bottleneck layer, and residual adapters, connected to preexisting TTS layers. We compare the performance of various configurations of these components and report Mean Opinion Score (MOS) and Speaker Similarity across languages. Using a single English reference speech per speaker, we achieve an average voice transfer similarity score of 73% across nine target languages. Vocal characteristics contribute significantly to the construction and perception of individual identity. The loss of one's voice, due to physical or neurological conditions, can lead to a profound sense of loss, impacting one's core identity. As a case study, we demonstrate that our approach can not only transfer typical speech but also restore the voices of individuals with dysarthria, even when only atypical speech samples are available - a valuable utility for those who have never had typical speech or banked their voice. Cross-lingual typical audio samples, plus videos demonstrating voice restoration for dysarthric speakers are available here (google.github.io/tacotron/publications/zero_shot_voice_transfer)."
        },
        {
            "title": "Start",
            "content": "Zero-shot Cross-lingual Voice Transfer for TTS Fadi Biadsy*, Youzheng Chen*, Isaac Elias, Kyle Kastner, Gary Wang, Andrew Rosenberg, Bhuvana Ramabhadran Google LLC {biadsy,josephychen}@google.com 4 2 0 2 0 2 ] . e [ 1 0 1 9 3 1 . 9 0 4 2 : r AbstractIn this paper, we introduce zero-shot Voice Transfer (VT) module that can be seamlessly integrated into multilingual Text-to-speech (TTS) system to transfer an individuals voice across languages. Our proposed VT module comprises speaker-encoder that processes reference speech, bottleneck layer, and residual adapters, connected to preexisting TTS layers. We compare the performance of various conﬁgurations of these components and report Mean Opinion Score (MOS) and Speaker Similarity across languages. Using single English reference speech per speaker, we achieve an average voice transfer similarity score of 73% across nine target languages. Vocal characteristics contribute signiﬁcantly to the construction and perception of individual identity. The loss of ones voice, due to physical or neurological conditions, can lead to profound sense of loss, impacting ones core identity. As case study, we demonstrate that our approach can not only transfer typical speech but also restore the voices of individuals with dysarthria, even when only atypical speech samples are available valuable utility for those who have never had typical speech or banked their voice. Cross-lingual typical audio samples, plus videos demonstrating voice restoration for dysarthric speakers are available here. [1] Index TermsText-to-speech, Zero-shot, Voice Transfer, Accessibility I. INTRODUCTION In recent years, there have been signiﬁcant advances in Voice Transfer (VT) technology (e.g., [2][4]), integrated in Text-to-speech (TTS) (e.g., [5]), Voice Conversion (VC) (e.g., [3], [6][8]), and Speech-to-speech Translation Models [9] [11]. For example, Parrotron [12] is VC model that converts atypical speech directly to synthesized predetermined typical voice that can be more easily understood by others. Yet for many individuals with dysarthria, VT extends speech technologies to help them regain their original voice and potentially predict speech patterns they have lost or never had. Recent research on TTS [13], [14] and VC [3], [6] have shown rapid progress on zero-shot or one-shot voice transfer and achieved great progress of speaker similarity on unseen speakers, but with the requirement of longer reference audio length [13], the cost of audio quality [14], or full ﬁne-tuning [6]. While Tran et al. [14] describe work on cross-lingual voice transfer, this approach requires the language of reference audio to match the language of target audio. The approach described in this work does not have such requirement. Voice characteristics are crucial to individual identity. The loss of ones voice, caused by physical or neurological conditions, can result in profound sense of loss, striking at the very heart of ones identity. Speakers with degenerative neural diseases, such as Amyotrophic Lateral Sclerosis (ALS), * Equal Contribution Parkinsons, and multiple sclerosis, may experience degradation of some of the unique characteristics of their voice over time. Some individuals are born with conditions, like muscular dystrophy, that affect the articulatory system and limit their ability to produce certain sounds. Profound deafness also impacts vocal and articulatory patterns due to the absence of auditory input and feedback. These conditions present lifelong challenges in matching the typical speech heard widely. In this paper, we describe zero-shot VT module that can be easily plugged into multi-lingual state-of-the-art TTS system [15], using single reference utterance. We demonstrate that such module is capable of transferring voice across languages, even if the language of the input reference speech is different from the intended target language. Finally, we show that the same model produces high quality speech with high ﬁdelity voice preservation even when the input reference is atypical, critical for those who have not banked their voice or never had typical speech (cf. Section IV-B). The contributions of this paper are as follows: We describe zero-shot VT module that can be easily plugged into state-of-the-art TTS system. This module transfers voices given single, short speech reference from each unseen speaker, with high quality and ﬁdelity. The proposed VT module is capable of transferring voice across languages, even when the language of the input reference speech is different from the target language. We introduce and compare novel bottleneck layers that have signiﬁcant impact on zero-shot TTS quality and speaker similarity. We demonstrate that this model produces high quality speech with high ﬁdelity voices across languages, even when the input reference is atypical, useful for those who have not banked their voice or never had typical speech. The reader is encouraged to listen to our audio and video samples [1]. The next Section II details our proposed voice transfer module, followed by ablation studies on the proposed architecture on the well-studied VCTK corpus [16]. We discuss our crosslingual results and voice restoration from atypical speech in Section IV. II. MODEL The backbone multi-lingual TTS system used in this work and its training procedure are described in detail in this previous work [15]. This model is joint speech-text model with feature-to-text (F2T) and text-to-feature (T2F) that are both jointly optimized on Automated Speech Recognition (ASR) and TTS data. It is trained with UTF-8 byte-based (cid:81)(cid:177)(cid:341)(cid:213)(cid:299)(cid:1200)(cid:1018) (cid:81)(cid:177)(cid:341)(cid:213)(cid:299)(cid:1200)(cid:253) (cid:1045)(cid:1045) (cid:340)(cid:940) (cid:238) (cid:81)(cid:177)(cid:341)(cid:213)(cid:299)(cid:1200)(cid:253)(cid:1161)(cid:1019) (cid:238) (cid:126)(cid:296)(cid:213)(cid:177)(cid:256)(cid:213)(cid:299)(cid:1200) (cid:38)(cid:264)(cid:202)(cid:213)(cid:209)(cid:209)(cid:241)(cid:265)(cid:233)(cid:1200)(cid:238) (cid:256)(cid:1200)(cid:340)(cid:1200)(cid:209) (cid:1071)(cid:28)(cid:271)(cid:265)(cid:334)(cid:1110)(cid:1156)(cid:1110)(cid:132)(cid:299)(cid:177)(cid:265)(cid:303)(cid:232)(cid:271)(cid:299)(cid:264)(cid:213)(cid:299)(cid:1072) (cid:132)(cid:213)(cid:340)(cid:310)(cid:1110)(cid:65)(cid:265)(cid:296)(cid:315)(cid:310)(cid:1043)(cid:1110)(cid:62)(cid:271)(cid:335)(cid:1200)(cid:177)(cid:299)(cid:213)(cid:1200)(cid:341)(cid:271)(cid:315)(cid:1132) (cid:122)(cid:213)(cid:232)(cid:213)(cid:299)(cid:213)(cid:265)(cid:203)(cid:213)(cid:1043) (cid:94)(cid:315)(cid:310)(cid:296)(cid:315)(cid:310)(cid:1043) (a) Model Architecture Overview. (cid:238) (cid:57)(cid:126)(cid:132)(cid:1110)(cid:27)(cid:271)(cid:373)(cid:259)(cid:213)(cid:265)(cid:213)(cid:203)(cid:256)(cid:1110)(cid:81)(cid:177)(cid:341)(cid:213)(cid:299) (cid:1)(cid:373)(cid:213)(cid:265)(cid:310)(cid:241)(cid:271)(cid:265) (cid:334)(cid:1019) (cid:334)(cid:1020) (cid:334)(cid:88) (cid:1159)(cid:256)(cid:1200)(cid:271)(cid:299)(cid:1200)(cid:1019)(cid:1126)(cid:1200)(cid:209)(cid:1160) (b) GST Bottleneck Layer. Fig. 1: TTS Model Architecture with Voice Transfer. input representations derived from text encoder which allows for sharing representations across languages efﬁciently. The major components of the T2F model are based on Parallel Tacotron 2 [17], [18]. Inference begins with text encoder that transforms the linguistic information into sequence of hidden representations. These representations are then fed into token duration predictor and upsampler, which generate longer sequence of hidden representation proportional to the predicted output duration. This expanded sequence is passed to feature decoder to generate latent features. Finally, WaveFit [19] vocoder converts these features into time-domain waveform output. The inference ﬂow through the model is shown in Figure 1a. The yellow components represent the proposed VT module and related bottleneck layers (Figure 1b), discussed in the next section. A. VT Module We extend this TTS system by adding VT module that takes an input reference speech. This extension enables the TTS model to transfer the voice in the reference speech to generate synthesized speech with this voice. The VT module is composed of (1) speaker encoder that takes an 128-dimensional mel spectrogram from reference utterance of 1-15 seconds, to extract high-level representation of this voice, using stack of 5 convolution layers with 3 1 ﬁlters followed by an 8 Transformer layers, producing 1024dimensional embedding vectors. Pooling and L2 normalization of these hidden vectors, we construct an embedding tensor that summarizes the acoustic-phonetic and prosodic characteristics of the input reference utterance with one tensor. This tensor is then passed to (2) bottleneck layer to restrict the embedding space and to ensure its continuity and completeness. We ﬁnd that the choice of the bottleneck has considerable impact on the Mean Opinion Score (MOS) and voice preservation. Finally, (3) the introduction of residual adapter [20] between two consecutive layers in the duration and feature predictor blocks, as shown in Figure 1a. The input to each residual adapter is concatenation of the output of the bottleneck layer and the previous layers output, as shown in Figure 1a. The decision to use residual adapters is two-fold. First, it makes the model modular, i.e. one can enable or disable this component of the VT module without impacting the quality of the original TTS model, and train the VT module independently. Second, the parameters can be dynamically loaded on-the-ﬂy. B. Bottleneck Layers In this Section, we conduct ablation studies on various types of bottleneck layers. All bottleneck layers produce summary 1024-dimensional embedding tensor passed to the residual adapter in each layer, as described above. Depending on the bottleneck layer, however, the input to the bottleneck layer can be either pooling of the sequence produced by the speaker encoder followed by an L2-normalization (i.e., = 1 in the Figure 1b) or sequence of k-vectors passed to the bottleneck directly. We describe the various types of bottleneck layers studied below. VAE: This comprises of Variation Auto Encoder (VAE) layer using Gaussian posterior probability distribution and unit Gaussian prior. [21] This bottleneck consumes the average pooling of the hidden representation sequence generated by the speaker encoder. At training time we used KL-weight of 0.0001 and at inference time we used the posterior mode of the encoded references speech. SharedGST: This is simplex-based bottleneck layer, employing similar implementation of Global Style Token (GST) layer described in [22], using 1024 learned bank of vectors. This layer accepts single pooled summary vector from the speaker encoder and it uses 4-headed dot-product attention to compute its similarity to each of vectors in the the GST bank. The corresponding attention weights are then used to compute the weighted average of the vectors in the bank using the attention weights, to produce the ﬁnal embedding vector, as shown in Figure 1b. This bottleneck constrains the embedding vectors to lie within the learned simplex. Intuitively, one can view the vertices of the simplex as bases of voices, and new voice is convex combination of those voice bases. MultiGST: This bottleneck is similar to SharedGST with the same hyperparameters, except that we move the bottleneck layer and replicate it to each of the duration and feature predictor layers. In other words, we have total of seven bottleneck layers (one bottleneck in the duration predictor and six bottleneck layers for the feature predictor). Each of these bottleneck layers consumes the same pooled vector from the speaker-encoder output. We hypothesize here that certain layers may beneﬁt from learning different simplex which allows the model to extract different embedding information from the same speaker, beneﬁting each of those layers differently. SegmentGST: This bottleneck layer is similar to that of the Shared GST, utilizing the same hyperparameters. However, instead of pooling the speaker encoder output directly and feed it to the GST layer, we ﬁrst allow the attention mechanism in GST to attend to the entire sequence from the speaker encoder. We do that, however, after reducing the sequence length by factor of 16, using 2 convolutional layers (each with 4strided [81] ﬁlters) placed after the speaker encoder to locally extract information from wider context. Following the GST layer, we apply the average pooling before feeding the output to the residual adapters. Its worth noting that the intuition here is to shift from computing similarities to voice bases, as in SharedGST, towards calculating similarities to learned vertices, potentially representing acoustic-phonetic segments. By employing average pooling, we obtain centroids of these vectors, which serve as our embedding space. Importantly, the centroids of vectors within simplex will also reside within the same simplex fundamental property of simplexes. We hypothesize that this embedding layer empowers the model to learn ﬁne-grained speaker embedding vectors, as it attends to speciﬁc chunks from the original hidden representations from the reference, enabling more nuanced representation of acoustic-phonetic information, plus we hypothesize it contributes to smoother embedding space that that of SharedGST. III. DATASETS AND MODEL TRAINING We follow similar training recipe outlined in [15] to obtain multi-lingual TTS system. In summary, the TTS system is trained within joint speech-text training framework where both T2F and F2T are jointly optimized on ASR and TTS data. We utilize the features from pre-trained conformer speech encoder, following Universal Speech Model (USM) architecture as the T2F path. The USM was pretrained with BERT-based Speech pre-Training with Random projection Quantizer (BEST-RQ) and ﬁne-tuned with the Multi-Objective Supervised pre-Training (MOST) objective [23], [24]. We also make use of pretrained WaveFit vocoder [19] for the Featureto-speech (F2S) component and keep it frozen. Similar to [15], from the USM encoder, we use split of 6 of its Conformer blocks as the speech encoder and keep those frozen, and the rest of the 18 USM Conformer blocks act as shared encoder connected to an RNN-T decoder to perform the F2T path, effectively performing ASR training to provide alignments for the T2F path. Since our extended model now accepts both text and reference speech, we pass random consecutive chunk (1-15 seconds) from the target speech as reference in each training sample. This helps prevent leakage of duration and linguistic information. The chunk length was sampled using clipped Gaussian distribution with mean of 8 seconds and standard deviation of 3. We train the TTS model along with the VT module jointly on the multi-lingual training data described next. The data is composed of large collection of transcribed, multi-lingual long-form YouTube data totalling around 200k hours and spanning several locales (ASR training data). The TTS data is composed of commercially licensed studio recordings, featuring 775 voice talents across several locales. The combined datasets cover 100+ locales. IV. EXPERIMENTS We evaluate the effectiveness of our proposed VT module in performing zero-shot TTS across languages. We randomly select 25 speakers from the VCTK corpus validation set with reference utterances between 7 and 14 seconds. Note: VCTK is not been used in training any components of the model. We generate 20 English sentences which are automatically translated into nine target languages: U.S. English, Arabic, Chinese Mandarin, French, German, Hindi, Italian, Japanese, and Spanish using Large Language Model (LLM) [25]. The English sentences and their translations serve as the textual inputs for our TTS system in our experiments. We conduct MOS naturalness and speaker similarity experiments for each of the nine languages across the four different bottleneck layers, discussed in Section II-B. Each MOS experiment involves 500 samples (25 speakers 20 sentences). Speaker similarity was measured by human raters. Raters were presented with pairs of audio samples: an English VCTK reference utterance and the utterance converted to one of the nine languages using the reference utterance, to determine if the pair seemed to have been spoken by the same speaker. As shown in Table I, all bottleneck choices yield highquality cross-lingual zero-shot voice transfer1. While the average MOS scores across languages are not signiﬁcantly different for all bottlenecks, VAE and SegmentGST outperform the others in speaker similarity. Furthermore, we conducted Side-by-Side (SxS) experiments comparing these two bottlenecks across the nine languages. The results, with wins vs. losses between SegmentGST and VAE reported in the last column of the table, reveal clear preference for SegmentGST. This preference is statistically signiﬁcant two of these languages exhibit signiﬁcant preference across all approaches, while the rest show no statistical difference. Considering that SegmentGST demonstrates statistically superior MOS scores across languages and its similarity scores are not signiﬁcantly worse, it emerges as compelling choice for zeroshot voice transfer when the reference speech is typical speech. in 7 out of 9 languages. Furthermore, A. Model Behaviour on Dysarthric Reference Speech To assess the robustness of the proposed zero-shot TTS model with broad range of atypical reference speakers, we selected 16 individuals with dysarthric speech of high severity from the Euphonia corpus [26]. Each of these speakers has one of the following etiologies: ALS, cerebral palsy, Ataxia, hearing impairment, vocal cord paralysis, or muscular dystrophy. The duration of every selected reference speech sample from each speaker is between 7 14 seconds. Utilizing the same 20 sentences as in previous experiments, we evaluated our proposed model with the four types of 1Example audio samples are available at [1] TABLE I: Cross-lingual Zero-Shot Subjective Evaluations on VCTK Corpus. VAE SharedGST MultiGST Language MOS Reference English Chinese Spanish Arabic French Japanese German Italian Hindi Mean Stddev Note: Similarity MOS 3.3 .29 85% 6% 68% 4% 3.6 .05 67% 14% 3.7 .06 72% 7% 3.8 .05 90% 7% 4.2 .03 88% 5% 4.3 .03 77% 5% 3.4 .06 85% 6% 4.1 .04 78% 6% 3.7 .05 4.1 .03 57% 6% 76% 3.88 0.30 11% 3.3 .29 3.7 .04 3.7 .08 3.5 .05 4.2 .03 4.0 .03 3.8 .05 4.1 .04 3.6 .05 3.9 .04 3.82 0.25 denotes SxS of SegmentGST signiﬁcantly wins (p-value < 0.01) against VAE; denotes SxS of SegmentGST signiﬁcantly wins against all other three setups. MOS Similarity 3.3 .29 85% 6% 48% 4% 3.5 .05 70% 14% 3.9 .05 65% 12% 3.6 .06 4.2 .03 90% 7% 4.1 .04 86% 7% 3.6 .05 68% 6% 4.0 .04 69% 8% 3.6 .05 72% 7% 4.1 .04 38% 5% 3.85 67% 0.26 16% MOS Similarity 3.3 .29 85% 6% 3.7 .05 58% 4% 70% 14% 3.9 .05 3.6 .04 62% 12% 4.2 .03 74% 11% 4.0 .03 75% 7% 3.7 .05 70% 6% 4.1 .04 70% 6% 3.7 .04 70% 6% 4.1 .04 42% 6% 3.89 66% 0.20 11% SegmentGST SxS (wins/losses) Similarity 85% 6% - 64% 4% 335 / 265 68% 15% 407 / 193 68% 14% 507 / 93 90% 6% 308 / 292 87% 5% 73% 5% 82% 6% 76% 6% 47% 6% 73% 13% 363 / 237 320 / 280 347 / 253 418 / 182 391 / 209 - - bottleneck layersusing the atypical reference speech as input. Due to the sensitive nature of this data, we restrict our analysis to automatic ASR experiments on the synthesized output to approximate intelligibility. The average WER across the 16 speakers was: 5.6% for VAE; 2.7% for SharedGST; 3.4% for MultiGST; and 10.4% for SegmentGST. According to paired T-test, we observed no signiﬁcant difference between SharedGST and MultiGST. However, both of these bottleneck layers demonstrate signiﬁcant improvements (p0.01) over both VAE and SegmentGST, while there was no statistical signiﬁcance between VAE and SegmentGST. Contrary to our ﬁndings with typical speech, we conclude that the highest quality zero-shot TTS for atypical reference speech can be achieved with shared and MultiGST bottleneck layers. Regrettably, we cannot assess speaker similarity in this context, as these speakers did not have the opportunity to bank their voices prior to the onset of voice degeneration. B. Case Study: Atypical Speech and Voice Restoration To demonstrate the systems performance when atypical speech/voice is the only reference available and to study speaker similarity, we worked with two speakers, referred to as DK and AL. DK, who is profoundly deaf from young age, has never had typical voice. DK learned to speak English using Russian phonetics. Their speech patterns are unique and may be difﬁcult for unfamiliar listeners to understand. We use 12 seconds of DKs atypical voice [1] as the reference speech and the associated ASR transcript [27] and its translation into 7 languages (1 English plus 6 other target languages selected by DK) to demonstrate cross-lingual capability [1]. Since DK cant hear the synthesized speech, we asked 10 subjects who have working relationship with DK to score (1 to 10) the English output of how similar the voice is to that of DK. We observe that MultiGST scores the highest with an average of 8.1/10 (1.1) followed by the VAE bottleneck with an average of 7 (1.4). The second speaker, AL suffers from muscular dystrophy, condition that causes progressive muscle weakness and sometimes impacts speech production. Like DK, AL has never had typical speech. Similar to DK, we use 14 seconds of ALs atypical speech as reference and synthesize the ASR transcripts [27] and its translations into 7 languages selected by AL [1]. We asked AL to evaluate how similar they think the output voice is to their own and they gave it 8/10, preferring the generated output from that uses the SharedGST bottleneck layer. V. REDUCING MISUSE OF VT FOR ATYPICAL SPEECH We recognize that in the context of voice transfer technology, the potential for misuse of synthesized speech is growing concern. To address this, we use audio watermarking [28] to embed watermarks so synthesized speech from our model can be detected. This technique involves embedding imperceptible information within the synthesized audio waveform. This hidden data can be detected using specialized software, enabling the identiﬁcation of potentially manipulated or misused audio content. Its important to note that the risk of misuse is signiﬁcantly lower for individuals who have never had typical speech patterns. In such cases, the synthesized nature of the output would be readily apparent, minimizing the potential for deception. VI. CONCLUSION We present modular zero-shot VT module that can be easily incorporated into preexisting multi-lingual TTS system, using residual adapters. We have discussed how this VT module can be trained jointly with the TTS system to obtain zero-shot cross-lingual capability, using few seconds of reference speech. Our ablations show that the design and choice of bottleneck layer in the VT module have signiﬁcant impact on voice quality and speaker ﬁdelity. While all bottleneck conﬁgurations can perform well for cross-lingual zero-shot, SegmentGST obtain the highest MOS scores (an average of 3.9) with high speaker similarity (an average of 73%), for typical speech reference across nine languages. This indicates that our raters, on average, 73% of the time, perceived the speaker in the English reference speech to be the same speaker speaking the other eight languages. Finally, we show that SharedGST and MultiGST do remarkably well at restoring speaker voices when only atypical speech is available, achieving 80% speaker similarity in the two case studies, and word error rate as low as 2.7% across etiologies."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "The authors sincerely thank speakers DK and AL for their participation and numerous contributions to this work and their tireless support during the course of this research effort. [20] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi, Learning multiple visual domains with residual adapters, Advances in neural information processing systems, vol. 30, 2017. [21] D. P. Kingma, Auto-encoding variational bayes, arXiv preprint arXiv:1312.6114, 2013. [22] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis, in International conference on machine learning. PMLR, 2018, pp. 51805189. [23] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang et al., Google usm: Scaling automatic speech recognition beyond 100 languages, arXiv preprint arXiv:2303.01037, 2023. [24] C.-C. Chiu, J. Qin, Y. Zhang, J. Yu, and Y. Wu, Self-supervised learning with random-projection quantizer for speech recognition, in International Conference on Machine Learning. PMLR, 2022, pp. 39153924. [25] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [26] B. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood, R. Cave, K. Seaver, M. Ladewig, J. Tobin, M. Brenner, P. Q. Nelson et al., Disordered speech data collection: Lessons learned at 1 million utterances from project euphonia, in Proc. Interspeech, 2021. [27] F. Biadsy, Y. Chen, X. Zhang, O. Rybakov, A. Rosenberg, and P. Moreno, scalable model specialization framework for training and inference using submodels and its application to speech model personalization, in Interspeech 2022, 2022, pp. 51255129. [28] S. Gowal and P. Kohli, Identifying ai-generated images with synthid, 2023, https://deepmind.google/discover/blog/identifyingaigeneratedimages-with-synthid."
        },
        {
            "title": "REFERENCES",
            "content": "[1] F. Biadsy and Y. Chen, Zero-shot cross-lingual voice transfer for tts, and its application to voice restoration for accessibility and inclusion, 2024, https://google.github.io/tacotron/publications/zero shot voice transfer. [2] H. Li, X. Zhu, L. Xue, Y. Song, Y. Chen, and L. Xie, Spontts: Modeling and transferring spontaneous style for tts, in ICASSP 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024, pp. 12 17112 175. [3] S.-H. Lee, H.-Y. Choi, H.-S. Oh, and S.-W. Lee, HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer, in Proc. INTERSPEECH 2023, 2023, pp. 44394443. [4] S. Yuan, P. Cheng, R. Zhang, W. Hao, Z. Gan, and L. Carin, Improving zero-shot voice style transfer via disentangled representation learning, arXiv preprint arXiv:2103.09420, 2021. [5] K. Fujita, T. Ashihara, M. Delcroix, and Y. Ijima, Lightweight text-to-speech with mixture of adapters, arXiv preprint zero-shot arXiv:2407.01291, 2024. [6] S. Yang, M. Tantrawenith, H. Zhuang, Z. Wu, A. Sun, J. Wang, N. Cheng, H. Tang, X. Zhao, J. Wang, and H. Meng, Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion, in Proc. Interspeech 2022, 2022, pp. 25532557. [7] Z. Chen, B. Ramabhadran, F. Biadsy, X. Zhang, Y. Chen, L. Jiang, F. Chu, R. Doshi, and P. J. Moreno, Conformer parrotron: faster and stronger end-to-end speech conversion and recognition model for atypical speech, in Interspeech 2021, 2021, pp. 48284832. [8] R. Doshi, Y. Chen, L. Jiang, X. Zhang, F. Biadsy, B. Ramabhadran, F. Chu, A. Rosenberg, and P. J. Moreno, Extending parrotron: An endto-end, speech conversion and speech recognition model for atypical speech, in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6988 6992. [9] Y. Jia, R. J. Weiss, F. Biadsy, W. Macherey, M. Johnson, Z. Chen, and Y. Wu, Direct speech-to-speech translation with sequence-to-sequence model, in Interspeech 2019, 2019, pp. 11231127. [10] Y. Jia, M. T. Ramanovich, T. Remez, and R. Pomerantz, Translatotron 2: High-quality direct speech-to-speech translation with voice preservation, in International Conference on Machine Learning. PMLR, 2022, pp. 10 12010 134. [11] E. Nachmani, A. Levkovitch, Y. Ding, C. Asawaroengchai, H. Zen, and M. T. Ramanovich, Translatotron 3: Speech to speech translation with monolingual data, in ICASSP 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 10 68610 690. [12] F. Biadsy, R. J. Weiss, P. J. Moreno, D. Kanvesky, and Y. Jia, Parrotron: An end-to-end speech-to-speech conversion model and its applications to hearing-impaired speech and speech separation, in Interspeech, 2019, pp. 41154119. [13] W. Wang, Y. Song, and S. Jha, Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations, in Proc. INTERSPEECH 2023, 2023, pp. 44544458. [14] C. Tran, C. M. Luong, and S. Sakti, STEN-TTS: Improving Zeroshot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework, in Proc. INTERSPEECH 2023, 2023, pp. 44644468. [15] T. Saeki, G. Wang, N. Morioka, I. Elias, K. Kastner, A. Rosenberg, B. Ramabhadran, H. Zen, F. Beaufays, and H. Shemtov, Extending multilingual speech synthesis to 100+ languages without transcribed data, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 11 54611 550. [16] C. Veaux, J. Yamagishi, K. MacDonald et al., Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit, 2016. [17] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. J. Weiss, and Y. Wu, Parallel tacotron: Non-autoregressive and controllable tts, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 57095713. [18] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. Skerry-Ryan, and Y. Wu, Parallel tacotron 2: non-autoregressive neural tts model with differentiable duration modeling, in Interspeech 2021, 2021, pp. 141 145. [19] Y. Koizumi, K. Yatabe, H. Zen, and M. Bacchiani, Waveﬁt: An iterative and non-autoregressive neural vocoder based on ﬁxed-point iteration, in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 884891."
        }
    ],
    "affiliations": [
        "Google LLC"
    ]
}