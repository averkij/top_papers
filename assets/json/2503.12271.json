{
    "paper_title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection",
    "authors": [
        "Shufan Li",
        "Konstantinos Kallidromitis",
        "Akash Gokul",
        "Arsh Koneru",
        "Yusuke Kato",
        "Kazuki Kozuka",
        "Aditya Grover"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 1 7 2 2 1 . 3 0 5 2 : r Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection Shufan Li1, Konstantinos Kallidromitis2, Akash Gokul3, Arsh Koneru1 Yusuke Kato2, Kazuki Kozuka 2, Aditya Grover1 1UCLA 2Panasonic AI Research 3Salesforce AI Research *Equal Contribution Correspondence to jacklishufan@cs.ucla.edu"
        },
        {
            "title": "Abstract",
            "content": "The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping textto-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, method that enables Diffusion Transformers to refine their generations using incontext examples of previously generated images alongside Intextual feedback describing necessary improvements. stead of passively relying on random sampling and hoping for better result in future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as base model. Additionally, it achieves new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.1 1. Introduction Text-to-image diffusion models have made significant progress by training larger architectures with more data 1Code will be available at https://github.com/jacklishufan/Reflect-DiT Figure 1. Reflect-DiT iteratively refines image generation by using vision-language model (VLM) to critique generations and Diffusion Transformer (DiT) to self-improve using past generations and feedback. Specifically, at each generation step N, feedback from previous iterations (N-3, N-2, N-1, . . . ) are incorporated to progressively improve future generations. Unlike traditional best-of-N sampling, Reflect-DiT actively corrects errors in object count, position, and attributes, enabling more precise generations with fewer samples. [36, 38, 39, 55]. However, scaling the training of such models, i.e. using more data and/or larger models, is computationally expensive, as exponential increases in compute are required to achieve near-linear performance gains [26]. Recently, works studying Large Language Models (LLMs), which exhibit similar scaling laws, have explored inferencetime scaling as another means of improving model performance [5, 44, 54, 63]. These approaches introduce additional compute resources during inference, either by generating many samples and employing selection mechanism to find the best output [27] or by prompting the LLM to generate longer reasoning traces, often involving selfverification and self-correction, before reaching an answer [7, 52]. These methods have shown promising results, offering substantial performance gains with relatively moderate increase in compute compared to training-time scaling. More recently, several works have attempted to apply the concept of inference-time scaling to text-to-image diffusion models [32, 55]. They primarily focus on two key aspects: scaling denoising steps per sample and scaling the number of samples. For the latter, commonly used framework is to generate random samples per prompt and select the best result using reward or judge model (best-of-N). In particular, SANA-1.5 [56] achieved state-of-the-art performance on the GenEval benchmark [13] by generating very large number of samples per prompt (N=2048) using 4.8B Diffusion Transformer (DiT). Its performance surpassed previous results achieved by significantly larger models, highlighting the benefits of inference-time scaling. Despite their encouraging success, these methods still have considerable room for improvement in terms of efficiency. For example, generating 2048 samples per prompt In this work, is impractical for real-world applications. we propose Reflect-DiT, an effective framework to improve the inference-time scaling of Diffusion Transformers by equipping DiTs with the ability to refine future generations through reflecting upon its past generated images and natural language feedback. Reflect-DiT draws inspiration from the recent success of reasoning models, such as DeepSeek-R1 [14], which exhibit self-verification and reflection capabilities. Rather than relying on random sampling to produce better output in the next generation, these models utilize their long context windows to reason about given problem. This process allows the model to search, reflect on, and refine potential solutions before providing final output. Unlike autoregressive LLMs, text-to-image diffusion models currently lack the ability to reason about past generations and feedback, as they condition solely on the input prompt. We argue that this limitationspecifically, the inability to reference and learn from past generationsprevents them from achieving inference-time scaling benefits beyond naive best-of-N sampling strategies. To address this shortcoming, ReflectDiT incorporates in-context reflection, expanding the conditioning signals to include past image generations and natural language feedback  (Fig. 1)  . Reflect-DiT can evaluate its past generations, identify misalignments with the input prompt (e.g. object count, spatial positioning, etc.) and refine subsequent generations to correct these issues. Concretely, Reflect-DiT consists of (1) VisionLanguage Model (VLM) that serves as judge that evaluates generated images with respect to input prompts and provides natural language feedback, and (2) Diffusion Transformer that refines its generations based on previous generations and corresponding feedback. Previously generated images and text feedback are first encoded with vision and text encoders into modality-specific embedding spaces, then processed by lightweight Context Transformer to obtain set of conditional embeddings that are passed to the cross-attention layers of the DiT. To ensure scalability, Reflect-DiT maintains fixed context length, limiting the number of past generations it considers. When the total number of past generations exceeds the context limit, we employ selection mechanism to stochastically sample subset of past generations as the context. The ReflectDiT framework is illustrated in Figure 1. We conduct extensive experiments to test the effectiveness of Reflect-DiT. Compared to the naive best-of-N approach, Reflect-DiT improves performance by +0.19 on the GenEval benchmark using SANA-1.0-1.6B as the base model. Reflect-DiT also establishes new absolute stateof-the-art score of 0.81 on GenEval while generating only 20 samples per prompt during inference, surpassing the previous best score of 0.80 that was obtained using significantly larger model (SANA-1.5-4.8B) with 2048 samples under the naive best-of-N approach. These results highlight Reflect-DiT as more effective and efficient alternative to best-of-N sampling for inference-time scaling of DiTs. 2. Related Works 2.1. Inference-time scaling of LLMs Traditionally, stronger performance on language tasks was achieved by training larger models using more data [6, 35]. Recent work has begun to explore using additional compute at test time to improve performance. Brown et al. [5] demonstrated the benefits of combining repeated sampling and selection mechanism such as automatic verifiers or reward models. Snell et al. [44] discovered that adaptive search and iterative self-refinement are more effective than naive Best-of-N sampling given fixed compute budget. Wu et al. [54] showed that with proper inference strategies, scaling test-time compute can be more efficient than scaling model parameters. Most recently, several works [7, 33] showed promising results in improving LLMs performance by spending additional compute for self-verification and self-correction at test time. Building on these insights, Reflect-DiT explores inference-time scaling beyond naive Best-of-N sampling by enabling text-to-image models to iteratively refine their generations by reflecting on past generations and feedback. 2.2. Inference-time scaling of Text-to-Image Diffusion Models Inspired by the success of inference-time scaling of LLMs, recent works have explored similar strategies for text-toimage diffusion models. Ma et al. [32] explored several Figure 2. Architecture of Reflect-DiT. Given prompt, past images and feedback, we first encode the images into set of vision embeddings [V1, V2, . . . ] using vision encoder, and encode text feedback to set of text embeddings [E1, E2...]. We then concatenate these embeddings into single sequence , and pass it through the Context Transformer to obtain . The extra context is concatenated directly after the standard prompt embeddings and passed into the cross-attention layers of the Diffusion Transformer (DiT). search strategies and concluded that random search combined with Best-of-N selection is currently the best strategy for improving performance. SANA-1.5 [56] applied Best-of-N sampling to frontier text-to-image model and established new state of the art on the GenEval benchmark through Best-of-N sampling (N=2048). Concurrent to this work, Singhal et al. [43] proposed more structured search method that extends beyond random sampling. 2.3. Self-Correction for Text-to-Image Generation Before the recent advancements in inference-time scaling for text-to-image models, earlier works explored using LLM agents to enable self-verification and selfimprovement (SLD[53] and GenArtist[49]). These methods use LLMs or VLMs as controllers to generate series of image operations or function calls to specialized models [4, 21, 29, 60]. Unlike Reflect-DiT, these works do not enable text-to-image diffusion models to learn from natural feedback; instead, they rely on predefined and limited set of operations. (e.g. object manipulation in latent space [54]). Furthermore, the performance of SLD and GenArtist is highly dependent on the successful execution of each submodule/operation, many of which involve heuristics or are not up-to-date. Because of such limitations, these approaches are not ideal for inference time scaling of frontier text-to-image models. We consider these works to be tangentially related to Reflect-DiT rather than directly comparable. Further discussion can be found in the Appendix A.3. 2.4. Controllable Text-to-Image Generation Text-to-image generation is inherently constrained by language as it relies solely on the input prompt. Thus, limiting its ability to represent concepts that may be difficult to capture in words. In the case of subject-driven generation, i.e. generating an image of specific subject such as ones pet, conditioning solely on text has shown to be ineffective. To address this, subject-driven generation methods introduce learnable embeddings [11, 22, 41, 47], or provide visual conditioning signals [24, 31, 37, 51]. Furthermore, previous works [1, 8, 19, 25, 34, 40, 42, 57, 62, 65] have introduced mechanisms to expand the conditioning signal of text-to-image diffusion models to include variety of signals to enable control beyond text. Unlike Reflect-DiT, these works focus on zero-shot controllable image generation and do not use conditioning signals to allow the model to learn from iterative feedback. Prior works have also explored using natural language instructions to improve image editing [4, 10, 12, 20, 64]. We provide further comparisons with these works in Section 5.1. 3. Method 3.1. Overall Framework Similar to its counterparts in the language domain, ReflectDiT iteratively refines its generation by performing verification-reflection loop. Reflect-DiT consists of vision-language model (VLM) feedback judge Fj that generates text feedback for an input image X, and Diffusion Transformer (DiT) text-to-image generator Fg that maps text prompt and set of past generations and feedback = {(Xi, Ti) = 1, 2, . . . } to new output image Xj. Given an input prompt , we first generate an image X0 = Fg(P ) without any additional context, and obtain the initial feedback T0 = Fj(P, X0). At each subsequent iteration i, we obtain new generation Xi and its corresponding feedback Ti. The reflection context is then updated, as Ci = {(Xj, Tj) = 1, 2, . . . , i}, to include all past generations and feedback. If the size of Ci is larger than predefined max context length K, we randomly sample past generations and their corresponding feedback as the input context Ck. Otherwise, we use all pairs in Ci to form input context Ck, The generator then produces an updated image Xi+1 = Fg(P, Ck), incorporating past feedback. This loop continues until the VLM feedback judge produces null feedback (indicating no further improvements) or maximum number of iterations is reached. The full procedure is formally described in Algorithm 1. 3.2. VLM Feedback The goal of the VLM judge, Fj, is to provide natural language feedback for generated images. We use Qwen2.5VL 3B[2] as the judge model and finetune it following the setup of SANA 1.5 [56]. Specifically, the training data is curated by generating large number of images from synthetic prompts and using an object detector to judge whether the desired objects are present in the image and whether their counts and attributes agree with the prompt. Feedback data used in VLM training are generated using structured templates. For example, if an object is missing, we use the template There is no {X} in the image. If the count of objects is incorrect, we use the template There should be {N} {X} in the image, but only {K} exist. The feedback is intentionally concise to improve VLM inference efficiency and minimize memory overhead for the DiT. We provide additional details of VLM training in Appendix C.1. 3.3. Diffusion Transformer We implement Reflect-DiT using SANA-1.0-1.6B [55], which offers competitive performance while being 106 faster than open-source alternatives[23, 36]. This improvement in efficiency is due to its smaller size and adoption of linear attention mechanism, making it an ideal choice for inference-time scaling which requires generating large Algorithm 1 Iterative Image Refinement with VerificationReflection Loop Require: Text prompt , Feedback Judge VLM Fj, DiT Image Generator Fg, Max Context Length K, Max Iterations Generate initial image Obtain initial feedback Initialize reflection context 1: Initialize X0 Fg(P ) 2: T0 Fj(P, X0) 3: C0 {(X0, T0)} 4: for = 1 to do 5: 6: break if Ti1 = then Stop if no more improvements 7: 8: 9: 10: 11: 12: end if Construct Ci = {(Xj, Tj)j = 1, 2, ..., i} if Ci > then"
        },
        {
            "title": "Sample K elements from Ci to obtain Ck",
            "content": "else Ck = Ci end if Generate new image Xi Fg(P, Ck) Obtain feedback Ti Fj(P, Xi) Update reflection context Ci Ci {(Xi, Ti)} 13: 14: 15: 16: 17: end for 18: return Image Trajectory {X0, X1, ..., Xn} number of samples per prompt. SANA consists of consecutive Linear-DiT blocks, each containing self-attention layer, cross-attention layer, and feed forward network (FFN). We incorporate past generated images and text feedback as additional context for the cross-attention layer. Concretely, given prompt and past context = {(X1, T1), (X2, T2), . . . }, we first encode the images [X1, X2, . . . ] into sequence of vision embeddings [V1, V2, . . . ] using vision encoder. We use SigLIP-Large [61] as our vision encoder, which encodes each image to feature map of size 24 24. We downsample this feature map to 8 8 before flattening it into 1D sequence, reducing the sequence length of image embeddings from 576 to 64. This compression minimizes the additional memory needed, making it computationally feasible to fit multiple images in the context during training and inference. To encode text feedback, we use Gemma2-2B [45] and convert the text feedback [T1, T2...] to set of 1D embeddings [E1, E2...]. Finally, we concatenate these image and text embeddings into single sequence = Concat([V1, E1, V2, E2, ...]). Since each image and text feedback are encoded separately, these features are not aligned with the features of the base DiT and there is no mechanism to associate each image with its corresponding feedback. Hence, we first process through small twolayer Transformer [46], which we refer to as the Context Transformer, to obtain intermediate output . The extra context is concatenated directly after the standard Generator SDXL[36] DALLE 3[3] SD3[9] Flux-Dev[23] Playground v3[28] SANA-1.5-4.8B[56] + Best-of-2048 SANA-1.0-1.6B [55] + Best-of-20 + Reflect-DiT(N=20) ( vs Baseline) Params 2.6B - 8B 12B - 4.8B 4.8B 1.6B 1.6B 1.6B + 0.1B - Overall 0.55 0.67 0.74 0.68 0.76 0.76 0.80 0.62 0.75 0.81 +0.19 Single 0.98 0.96 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.98 +0.00 Two 0.74 0.87 0.94 0.85 0.95 0.95 0.88 0.83 0.87 0.96 +0.13 Counting Color Position Attribution 0.39 0.47 0.72 0.74 0.72 0.72 0.77 0.58 0.73 0.80 +0.22 0.85 0.83 0.89 0.79 0.82 0.82 0.90 0.86 0.88 0.88 +0.02 0.15 0.43 0.33 0.21 0.50 0.50 0.47 0.19 0.54 0.66 +0.47 0.23 0.45 0.60 0.48 0.54 0.54 0.74 0.37 0.55 0.60 +0.23 Table 1. Results on the GenEval benchmark [13]. Reflect-DiT achieves the highest overall score (0.81) with only 20 samples per prompt, outperforming all other models despite having significantly fewer parameters. Compared to the base SANA-1.0-1.6B, ReflectDiT demonstrates consistent improvements across all evaluation categories, with notable overall gain of +0.19. While SANA-1.5-4.8B achieves competitive performance, it requires substantially more computational resources and is not open-sourced at the time of writing. Evaluated using the released checkpoint of SANA-1.0. SANA-1.5 is not open-sourced; results are reported from the original paper. prompt embeddings of SANA-1.0 and passed into the crossattention layers of the DiT. This is illustrated in Figure 2. The same synthetic dataset is used to train both the VLM judge and the Diffusion Transformer. Specifically, each training data sample used for training the DiT consists of good image Xw as the positive sample, and set of bad images [X1, X2, ...] and corresponding feedback [T1, T2, ...] as reflection context. The VLM training data consists of multiple image-feedback pairs per prompt. We select images that pass the object detector test (and hence use the feedback template This image is correct) as positive samples and the remaining images as in-context feedback samples. Prompts that lead to only good images or bad images are excluded from the training data. Following SANA, we finetune the DiT using the flow-matching objective as follows: EtUnif[0,1]w(t)ϵ xw Fg(xt w, t, C)2 (1) where ϵ is an i.i.d Gaussian noise, xw is the image latent corresponding to the good image Xw and xt = (1 t)xw + tϵ is noised image latent, is the set of in-context image-feedback pairs, and w(t) is weighting function. We use logit-normal weighting scheme following SANA. 4. Experiments 4.1. Setup 4.1.1. Dataset We generate prompts using the GenEval templates [13] and filter out those present in the test set, resulting in 6,000 prompts. We generate 20 images per prompt and obtain synthetic feedback using object detectors [13]. This results in dataset of 78.5k image-feedback pairs. Our pipeline follows the setup of SANA-1.5 but produces smaller dataset (2M images in SANA-1.5) due to computational constraints. 4.1.2. Training We train the VLM judge for 1 epoch with learning rate of 1e-5. We train the DiT and Context Transformer for 5,000 steps with learning rate of 1e-5 and batch size of 48. We freeze the image and text encoders, and finetune the DiT and Context Transformer end-to-end. Training is conducted on Nvidia A6000 GPUs and takes approximately one day. 4.2. Sampling For all experiments, we use the DPM-Solver++ sampler proposed by SANA. We use 20 sampling steps per image and set the maximum number of images per prompt to 20 (N=20). For Reflect-DiT, we set the maximum number of in-context feedback to 3 (K=3) unless stated otherwise. We provide further details on inference speed in Appendix A.2. 4.2.1. Baselines We compare against several training-free baselines and finetuning-based methods. For training-free baselines, we compare against best-of-N sampling employed by SANA1.5. For finetuning baselines, we consider supervised finetuning (SFT) and Diffusion-DPO[48]. For both methods, we use SANA-1.0-1.6B as the base model. Since these finetuning methods do not enable additional inference-time scaling capabilities, we combine them with best-of-N sampling to equalize test-time compute. 4.3. Main results We report results on the GenEval benchmark in Table 1. Reflect-DiT achieves the highest overall score (0.81) using at most 20 samples per prompt, outperforming all Figure 3. Side-by-side qualitative comparison of Reflect-DiT and best-of-N sampling. Reflect-DiT leverages feedback to iteratively refine image generations, resulting in more accurate and visually coherent outputs. In the first example, Reflect-DiT progressively adjusts object positions to better satisfy the prompt cup left of an umbrella, achieving significantly better image-text alignment than best-ofN sampling. The second example demonstrates how Reflect-DiT corrects multiple counting constraints (five monarch butterflies and single dandelion) over successive iterations, gradually converging to the correct solution. Lastly, in the rightmost example, ReflectDiT uses in-context feedback to refine object shapes, producing more precise and intentional design compared to best-of-N. other models, including those with substantially more (3) parameters. Compared to the SANA-1.0-1.6B baseline, Reflect-DiT demonstrates consistent improvements across all evaluation categories, with an overall gain of +0.19. Notably, we establish new state-of-the-art (0.81), surpassing the previous best (0.80), which was achieved using bestof-N sampling with N=2048 using SANA-1.5-4.8B. Improvements are especially pronounced for prompts requiring complex reasoning over multiple objects (e.g. counting, spatial positioning, and attribute binding). As prompt complexity increases, the baseline model struggles to generate high-quality samples, since the probability of satisfying multiple constraints is low. 4.4. Qualitative Examples We present samples from the generation trajectory of Reflect-DiT in Figure 3. Compared with best-of-N sampling with random search, Reflect-DiT improves image quality more efficiently by tailoring future generations according to prior feedback. For example, in the rightmost trajectory of Figure 3 the initial generations fail to render crown shaped splash, but as the Reflect-DiT trajectory progresses, the rendered splash resembles the shape of crown (top row). In contrast, best-of-N sampling (bottom row) fails to find generation with crown shaped splash. Figure 6 showcases some examples of the self-correction process of Reflect-DiT. The vision-language model correctly identifies misalignments between prompts and generated images, enabling Reflect-DiT to improve subsequent generations based on this feedback. For example, the topmost row of Figure 6 shows generations from prompt involving multiple entities and relative positioning constraints. Here, the VLM judge accurately identifies issues Figure 4. Comparison of Reflect-DiT with other finetuning methods. We find that Reflect-DiT is able to consistently outperform finetuning methods, like supervised finetuning (SFT) and Diffusion-DPO (DPO). Using only 4 samples, Reflect-DiT can outperform related finetuning baselines using best-of-20 sampling. in generated images, such as inconsistent positioning and missing subjects, and Reflect-DiT is able to improve future generations based on this feedback. 4.5. Comparison with Finetuning We additionally compare with several finetuning methods, including supervised-finetuning (SFT) and Diffusion-DPO [48]. We report results on GenEval with varying number of samples in Table 2 and Figure 4. Since SFT and DPO do not enable additional inference-time scaling, we combine them with best-of-N sampling. Reflect-DiT outperforms Number of Samples 16 12 8 4 Baseline 0.705 +SFT 0.736 0.713 +DPO +Refl. DiT 0.776 0.731 0.754 0.741 0. 0.743 0.760 0.747 0.799 0.749 0.767 0.754 0.799 20 0.751 0.767 0.765 0.807 Table 2. GenEval performance using different finetuning methods. Results show that Reflect-DiT consistently outperforms supervised finetuning (SFT), and Diffusion-DPO (DPO) across varying number of samples at inference. We use best-of-N sampling for the base model, SFT, and DPO baselines and in-context reflection for Reflect-DiT. Figure 5. Human evaluation win-rate (%) on PartiPrompts dataset. We perform user study to evaluate the effectiveness of Reflect-DiT in broadly improving text-to-image generation. Results show that human evaluators consistently prefer generations from Reflect-DiT over best-of-N sampling. both baselines by considerable margin, especially at large number of samples. Using just 4 samples, Reflect-DiT outperforms both the SFT and DPO baselines with 20 samples, saving 80% of the compute budget. Among the baselines, SFT and DPO have nearly identical performance with N=20 samples, but SFT outperforms DPO on fewer samples, presumably because of the KL-divergence penalty. 4.6. Human Evaluation To evaluate Reflect-DiTs effectiveness in real-world use cases beyond GenEval-style prompts, we conduct additional evaluations using 100 randomly sampled prompts from PartiPrompts[59]. We conduct human evaluations and ask evaluators to compare Reflect-DiT against best-of-N sampling, with maximum of 20 samples per prompt for each method. We attempted to evaluate generations using frontier VLMs such as GPT-4o, but encountered similar problems as [56], where the API gives inconsistent outputs and exhibits strong bias towards the first presented image. Evaluators are given pair of images and prompt and asked to determine which generated image is better, without any prior knowledge of how they were generated. Three responses are collected per image pair. Our results demonstrate that Reflect-DiT significantly outperforms the bestof-N baseline, with evaluators selecting Reflect-DiT 53.3% of the time compared to 32.3% for best-of-N  (Fig. 5)  . We provide additional details in Appendix C.3. 4.7. Ablation Studies We measure the effect of various design choices by conducting series of ablation studies on the number of in-context feedback, Transformer layers, and image embeddings. 4.7.1. Number of Feedback One of the key innovations of Reflect-DiT is the ability to utilize multi-turn feedback. We experiment with varying amounts of feedback and find that an increase in-context feedback (K in Algorithm 1) yields better performance, with the 3-feedback setup achieving the best result (Tab. 3). Number of Samples 16 12 8 0.731 0.755 0.765 0.792 0.743 0.761 0.765 0.799 0.749 0.772 0.774 0. 4 0.705 0.738 0.743 0.776 20 0.751 0.766 0.781 0.807 Baseline = 1 = 2 = 3 Table 3. Ablation study on the number of in-context imagefeedback pairs (K). 4.7.2. Number of Transformer Layers We conducted multiple experiments with varying number of Transformer layers in the Context Transformer (Tab. 4). Results show that increasing the number of Transformer layers leads to an improvement in model performance. Number of Samples 16 12 8 0.731 0.790 0.792 0.743 0.797 0.799 0.749 0.801 0. 4 0.705 0.769 0.776 20 0.751 0.804 0.807 Baseline +1 Layer +2 Layers Table 4. Ablation study on number of Transformer layers. 4.7.3. Number of Image Embeddings We also explore different down-sampling sizes for image embeddings (default 8 8) and present the results in Table Figure 6. Illustration of the iterative refinement process in Reflect-DiT. Reflect-DiT starts with an initial image generated from the prompt and progressively refines it based on textual feedback until the final output meets the desired criteria, demonstrating the effectiveness of our reflection-based approach. In the first sequence, Reflect-DiT handles complex scene by gradually repositioning multiple objectswoman, tree, cat, and dogto achieve correct spatial alignment. Additionally, it recognizes subtle object misclassifications, such as changing the second dog to cat based on feedback. The second example demonstrates counting problem, where the model iteratively adjusts the number of detached seeds until it converges to the correct count. The final example presents particularly challenging scenario: the prompt requires the dog to be positioned to the right of tie, an unusual object to appear independently. Initially, the model misinterprets the instruction, generating dog wearing tie. However, through multiple refinement steps, ReflectDiT learns to separate the objects and ultimately produces the correct spatial arrangement. 5. Results show that using more tokens to represent past generations leads to better performance. Number of Samples 16 12 8 0.731 0.779 0.781 0. 0.743 0.783 0.795 0.799 0.749 0.786 0.795 0.799 4 0.705 0.770 0.752 0.776 20 0.751 0.786 0.801 0. Baseline 4 4 6 6 8 8 Table 5. Ablation study on the size of image embeddings. 5. Discussion 5.1. Connection with Instruction-Guided Image"
        },
        {
            "title": "Editing",
            "content": "Several works have focused on instruction-following image editing models, e.g. InstructPix2Pix [4] and InstructDiffusion [12]. These models take an input image and natural language instruction and perform the desired edit on the image. Compared to these methods, Reflect-DiT has two key advantages. First, our training data relaxes the strict requirement for paired input-edited images. Imageediting data consists of paired images: an input and corresponding edited version that adheres to the instruction while maintaining visual consistency. Reflect-DiT only requires good image that avoids problem found in bad image, which can be easily collected. Second, but more importantly, Reflect-DiT uniquely leverages multi-round feedback context and progressively refines its generations. Our results in Section 4.7.1 demonstrate that iterative in-context feedback significantly improves performance. Figure 6 also illustrates that multi-round feedback enables the progressive refinement of generations. Although some identified defects may not be fully resolved in single iteration, the generated images converge towards correct image after multiple rounds. The model learns this ability despite the random sampling of in-context feedback during training. 6. Conclusion In this work, we presented Reflect-DiT, framework for inference-time scaling of text-to-image diffusion models that leverages reflection on past generations and natural language feedback. Results show that Reflect-DiT significantly outperforms naive best-of-N sampling, which to date has been the most effective inference-time scaling method, and establishes new state-of-the-art on the GenEval benchmark, outperforming larger models with substantially higher sampling budgets at test-time. While the Reflect-DiT framework has shown to be promising approach to improve text-to-image generation, it is important to note that our method inherits the biases and shortcomings of both the base text-to-image model and the VLM judge that guides refinement. For example, the VLM judge, like other VLMs, may generate incorrect feedback due to hallucination. We also find it challenging for the VLM to recognize small objects. Future work should focus on auditing these limitations and developing safeguards to ensure responsible deployment of Reflect-DiT in applications. Further discussion of limitations is provided in Appendix B."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1837018380, 2023. 3 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 15 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 5 [4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18392 18402, 2023. 3, 8 [5] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. 1, 2 [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901, 2020. [7] Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan Arık. Sets: Leveraging self-verification and self-correction for improved test-time scaling. arXiv preprint arXiv:2501.19306, 2025. 2 [8] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. In The Eleventh International Conference on Learning Representations, 2023. 3 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 5 [10] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffuIn Proceedings of sion for accurate instruction following. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 47444753, 2024. 3 [11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [12] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1270912720, 2024. 3, [13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 2, 5 [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 14 [15] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step, 2025. 14 [16] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision, pages 289305. Springer, 2024. 15 [17] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu Ella. Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 5(7): 16, 2024. 13 [18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Equip diffusion models with arXiv preprint and Gang Yu. llm for enhanced semantic alignment. arXiv:2403.05135, 2024. 13 Ella: [19] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: creative and controllable image synthesis with composable conditions. In Proceedings of the 40th International Conference on Machine Learning, pages 1375313773, 2023. 3 [20] Ying Jin, Pengyang Ling, Xiaoyi Dong, Pan Zhang, Jiaqi Wang, and Dahua Lin. Reasonpix2pix: instruction reasoning dataset for advanced image editing. arXiv preprint arXiv:2405.11190, 2024. 3 [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 3 [22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. 3 [23] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 4, 5 [24] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36:3014630166, 2023. 3 [25] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023. 3 [26] Zhengyang Liang, Hao He, Ceyuan Yang, and Bo Dai. arXiv preprint Scaling laws for diffusion transformers. arXiv:2410.08184, 2024. 1 [27] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 1 [28] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 3 [30] Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, and Yang You. CAME: Confidence-guided adapIn Proceedings of the tive memory efficient optimization. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 44424453, Toronto, Canada, 2023. Association for Computational Linguistics. 15 [31] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 3 [32] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. 2 [33] Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. s2 r: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025. 2 [34] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. [35] OpenAI. Gpt-4 technical report, 2023. arXiv preprint arXiv:2303.08774. 2 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 4, 5, 13 Improving latent diffusion modarXiv preprint [37] Senthil Purushwalkam, Akash Gokul, Shafiq Joty, and Nikhil Naik. Bootpig: Bootstrapping zero-shot personalized image generation capabilities in pretrained diffusion models. arXiv preprint arXiv:2401.13974, 2024. 3 [38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 1 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 13 [40] Robin Rombach, Andreas Blattmann, and Bjorn OmText-guided synthesis of artistic images with arXiv preprint mer. retrieval-augmented diffusion models. arXiv:2207.13038, 2022. 3 [41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [42] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knndiffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022. [43] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. 3, 14 [44] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more arXiv preprint effective than scaling model parameters. arXiv:2408.03314, 2024. 1, 2 [45] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 4, 15 [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [47] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 3 [48] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 5, 6, 14 [49] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems, 37:128374128395, 2024. 3, 13, 14 [50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 14 [51] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. 3 [52] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022. [53] Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6327 6336, 2024. 3, 13 [54] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. 1, 2, 3 [55] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution text-to-image In The Thirsynthesis with linear diffusion transformers. teenth International Conference on Learning Representations, 2025. 1, 2, 4, 5 [56] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. 2, 3, 4, 5, 7, 13, 14, 15 [57] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained In Proceedings of the IEEE/CVF International diffusion. Conference on Computer Vision, pages 74527461, 2023. 3 [58] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. arXiv preprint arXiv:2311.13231, 2023. 14 [59] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [60] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023. 3 [61] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 4, 15 [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 3 [63] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction, 2024. URL https://arxiv. org/abs/2408.15240, 2024. 1 [64] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. 3 [65] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023. A. Additional Results and Discussions A.2. Inference Speed A.1. Generalizability of Reflect-DiT In the main paper, we provide human evaluation results on PartiPrompts to demonstrate the generalizability of ReflectDiT beyond GenEval-style prompts. While human evaluators are the ideal judges for text-to-image generation, we recognize that human evaluations can be expensive. In this section, we report additional results on DPG-Bench [17] to highlight the effectiveness of Reflect-DiT on broad range of prompts. Table 6 presents the performance of Reflect-DiT on DPG-Bench [18]. We note that several frontier opensourced models achieve similar performance on this benchmark, in the range of 83.0-85.0. The performance gap among models on DPG-Bench is less pronounced compared to GenEval. Analysis of SANA-1.0 outputs indicates that approximately 75% of the prompts are less challenging, as indicated by the base model achieving score above 0.8 without inference-time scaling. This may explain why SANA-1.5 [56] reported inference-time scaling results only on GenEval. To further illustrate the effectiveness of Reflect-DiT, we construct two challenging datasets by subsampling prompts where the SANA-1.0-1.6B base model scores poorly in the single-sample setup. Specifically, we create subset of 246 prompts (Hard-246) consisting of prompts on which the base model obtained score below 0.8 in the single-sample setup, and subset of 56 prompts (Hard-56) consisting of prompts on which the base model obtained score below 0.5 in the single-sample setup. We compare with the base model and best-of-N sampling on the two subsets as well as the full benchmark. ReflectDiT achieves better performance on all three datasets, with more pronounced differences on the two hard subsets. The results on DPG-Bench, together with the main results on GenEval and human evaluations on PartiPrompts, demonstrate the effectiveness of Reflect-DiT across diverse textto-image generation tasks. SD3-Medium DALLE 3 FLUX-dev SANA-1.5 SANA-1.0 +Best-of-20 +Reflect-DiT ( vs Base.) Parm. 2B - 12B 4.8B 1.6B 1.6B 1.7B - All Hard-246 Hard-56 84.1 83.5 84.0 85.0 84.1 85.6 86.1 +2.0 - - - - 56.1 63.4 69.5 +13.4 - - - - 24.2 41.0 51.8 +27.6 Table 6. Additional quantitative results on DPG-Bench [18] SANA-1.5 only reported inference-time scaling (best-of-2048) on GenEval benchmark and has not been open sourced. We cite their single-sample result here. We benchmarked the inference speed of Reflect-DiT against the best-of-N baseline and found no significant difference in performance. Overall, Reflect-DiT and best-of-N sampling achieved similar throughput: 11.32 samples per minute for Reflect-DiT and 10.12 samples per minute for best-of-N, where each sample includes generated image and corresponding text feedback. Conceptually, generating samples using Reflect-DiT has similar computational cost to the best-of-N baseline, as both involve generating images and running the VLM model times. The only extra overhead comes from (1) encoding the images and text in the context and (2) computing cross-attention with extra keys and values. Furthermore, step (1) can be amortized across the denoising steps, as the context needs to be encoded only once per generated image at the beginning of the denoising loop. For N=20, the end-to-end latency is 118.57 seconds for Reflect-DiT and 105.98 seconds for the best-of-N baseline. Of the total time, 75.5% is spent generating images with the DiT and 24.5% is used for VLM inference. A.3. Connection with Self-Correcting T2I Agent Prior to the recent interest in inference-time scaling, several works attempted to achieve self-verification and correction through an agentic framework, such as SLD [53] and GenArtist [49]. These works employ frontier LLM or VLM (e.g. GPT4) to control set of external tools such as object detectors, segmentation models and inpainting models to verify the accuracy of text-to-image (T2I) generation and apply corrective operations to the generated image. These approaches suffer from key scalability and flexibility issues. In terms of scalability, calling proprietary APIs for each inference is expensive. Additionally, generating function calls auto-regressively and executing multiple models per refinement round introduce significant computational overhead and latency. In contrast, Reflect-DiT and recent works on inference-time scaling only require a, significantly smaller, VLM judge model to simply generate concise perimage feedback in natural language. In terms of flexibility, the success of these agentic frameworks depends on all submodules executing successfully, giving rise to two main problems. First, these submodules may not be up-to-date. For example, the inpainting and image editing models they use are primarily based on SDv1.5 [39] or SDXL [36], resulting in suboptimal generation quality. Updating all tools to the latest architectures and base models is non-trivial, since the developers of these tools may discontinue maintenance, which is not unrealistic for most research projects. Additionally, adapting system with numerous components to custom use cases can be challenging. For example, if user wants to generate painting, the pretrained object detector and segmentation models may fail on out-of-distribution cases such as painting generation. Collecting detection and segmentation dataset and fine-tuning the object detector and segmentation model can be expensive and challenging, not to mention the difficulty of data collection for others tools like inpainting and image-editing models. In contrast, Reflect-DiT and recent inference-time scaling methods can easily adapt to new use cases as long as judge model provides feedback, which can be obtained by fine-tuning strong foundational VLM on limited data. In fact, we show in Figure 6 (main paper) that Reflect-DiT can adapt to novel use cases such as painting generation in zero-shot manner due to the inherent generalizability of VLMs, highlighting the flexibility of our approach. We find these works interesting but tangential. In our early explorations, we attempted to reproduce the findings of GenArtist [49] (NeurIPS 2024) and test its performance on the GenEval benchmark. Unfortunately, the model fails to complete the official demo script using the default prompt, as the latest version of GPT-4 generates ill-formed function calls approximately 20 seconds into the agentic loop. Our experience further highlights the inflexibility and inconsistency of these methods. A.4. Connection with Reinforcement Learning (RL) If we consider the consecutive, non-i.i.d. generative process of multiple image samples as policy optimization problem, then Reflect-DiTs training objective can be viewed as equivalent to imitation learning, where we directly apply the SFT objective to set of target good actions,i.e. accurate image generations. We also explored reinforcement learning objectives such as D3PO [58] and Diffusion-DPO [48], which incorporate negative samples during training. However, we encountered issues with training stability. We achieved state-of-the-art results using only the SFT objective and leave further exploration of RL objectives to future work. Our results mirror those of DeepSeek-R1 [14], where the authors showed that smaller LLMs can achieve substantial performance gains solely through SFT on high-quality reasoning trajectories generated by larger model, without requiring reinforcement learning. A.5. Concurrent Works Following the success of test-time scaling in the language domain, the community has shown growing interest in applying it to text-to-image generation. Concurrently with this work, SANA-1.5 [56] explored best-of-N sampling on state-of-the-art DiT. Our proposed Reflect-DiT outperforms SANA-1.5, which uses 2048 samples (best-of-2048), with only 20 samples by leveraging reflection mechanism. Also concurrent, FK-steering [43] proposed novel latentspace search method that extends beyond random search. However, its implementation is limited to the DDIM sampler and is not easily adaptable to multi-step solvers such as the DPM-Solver++ which is used by SANA. In contrast, Reflect-DiT has constant memory footprint, making it more scalable. After testing the official SDXL-based implementation, we find that FK-Steering causes out-of-memory errors at 20 particles. While their results are promising, we believe it has considerable room for improvement, particularly in adapting to state-of-the-art DiTs and optimizing memory usage. Another concurrent work [15] explored generating images using chain-of-thought (CoT) [50] reasoning and incorporates elements of self-verification and iterative improvement. However, their work focuses on autoregressive image generation models and is direct adaptation of analogous approaches in the language domain. B. Limitations While we have demonstrated Reflect-DiTs effectiveness across various applications, we acknowledge its limitations. First, the training data used for the VLM judge primarily focuses on prompt alignment, e.g. whether there are sufficient objects, whether they satisfy positional constraints, etc. Thus, the VLM judge may not be able to provide proper feedback or suggest improvements for other aspects, such as the aesthetics of an image. In general, natural language feedback datasets of this kind are more difficult to collect. We hope future datasets can help address this issue. Second, we observe that the VLM judge suffers from hallucinations, similar to its base model. We present examples of these errors in Figure 7. For example, in row 3 of Figure 7, the VLM mistakenly claims that the boat is not present in the image, despite the boat being clearly present and the image being correct. Lastly, we observed that the diffusion model may fail to address certain forms of feedback in single iteration. In some cases, we observe the model iteratively refining the generation toward correctness, though it takes multiple iterations for the image to become fully aligned with the prompt. For example, in Figure 6 (main paper), row 3, we observe that the position of the dog and the tie gradually move toward the desired layout. In other cases, the progression is less interpretable. For example, in Figure 6 (main paper), row 2, the generated image should contain three seeds, but it undergoes an inconsistent progression of 2-5-1-3. Empirically, we observe that ReflectDiT can generate accurate, text-aligned images with fewer inference-time samples, achieving 22% improvement on the counting subcategory of GenEval (Table 1 in main paper). C. Technical Details C.1. VLM Training Following SANA-1.5 [56], we format the VLM training data into conversation format. Our template differs from \"human\", VLM Training Data Template { \"from\": \"value\": \"<image>n Please evaluate this generated image based on the following prompt: [[prompt]]. Focus on text alignment and compositionality.\" }, { \"from\": \"value\": } \"[[feedback text]]\" \"gpt\", Table 7. VLM Training Data Template To improve training stability, we add an RMSNorm layer after the projector. Before training, we freeze the SigLIP model. The projector is trained end-to-end with the rest of the DiT. C.2.2. Text-Encoder We use Gemma-2B [45] as the text encoder for text feedback. It is kept frozen during training. Since Gemma-2B is also used by SANA as the prompt encoder, no additional parameters are introduced to the overall system. C.2.3. Context Transformer The Context Transformer is two-layer Transformer. Its primary purpose is to (1) align encoded features with the features space of the base DiT and (2) associate the feedback with the corresponding image. Each Context Transformer consists of standard Transformer block, including self-attention layer followed by feed-forward network. We use the exact FFN design of Qwen2.5-VL [2]. For the self-attention layer, we incorporated rotary positional embeddings [16] following the design of many modern LLMs and VLMs. C.2.4. Training We report the training hyperparameters for Reflect-DiT, SFT, and Diffusion-DPO baselines in Table 8. We use the CAME optimizer [30] to train the DiT, following the approach in SANA [56]. For Diffusion-DPO, we tested three values of β, the hyperparameter controlling the KL divergence penalty, and determined that β = 2000 produces the optimal result. C.3. Human Evaluation Details We use Amazon Mechanical Turk for human evaluations. We present the user interface provided to human annotators in Figure 8. We collect three evaluations per image pair and compared Reflect-DiT(N=20) with best-of-20 for each Figure 7. Failure cases of Reflect-DiT. Failure cases of ReflectDiT. While Reflect-DiT demonstrates strong refinement capabilities, the generated feedback can occasionally introduce errors between iterations. In the first example, the model fails to recognize that the specific lighting conditions signify sunset, leading to an incorrect adjustment. Similarly, in the second example, the model struggles to distinguish the color of the dining table because the purple hue from the dog reflects off the table, creating ambiguity. These cases highlight subjectivity in the VLM evaluation, where the models interpretation may still be reasonable. However, the final two examples illustrate more typical failure cases. In both images, objects (boat and butterfly) are completely overlooked by the feedback model. This issue likely arises because the objects are too small or unusually shaped, which makes them difficult to detect, resulting in incorrect evaluations. SANA because we use different base model, Qwen-2.5VL 3B [2]. We present the template in Table 7. We provide hyperparameters of our training run in Table 8. C.2. Diffusion Transformer C.2.1. Vision-Encoder The vision encoder is SigLIP-Large [61] that encodes each image into feature map of size 2424 = 1024.The feature map is then downsampled to 88 = 64 via average pooling and flattened into 1D sequence of length 64. We then use two layer MLP with GELU activation to project the features to match the input dimension of the Context Transformer. Hyperparameters VLM Judge Reflect-DiT"
        },
        {
            "title": "Learning Rate\nBatch Size\nWeight Decay\nOptimizer\nSchedule\nWarmup steps",
            "content": "1e-5 48 0.1 AdamW 1 epoch 0.03 epoch 1e-5 48 0 CAME 5k step 500 step SFT 1e-5 48 0 CAME 5k step 500 step Diffusion-DPO 1e-5 (24, 24)* 0 CAME 5k step 500 step Table 8. Hyperparameters used for each experiment. * We use 24 positive samples and 24 negative samples per batch. E. Reproducibility Statement We will release the training code and data for the DiT and VLM judge model, and pretrained checkpoints. We will also release the generated images that produce the main result on GenEval benchmark. Additionally, we will release the list of prompts in Hard-246 and Hard-56 subset of DPGBench. Figure 8. User interface for human annotators. prompt. We randomly selected 100 prompts from the PartiPrompts dataset and generated 100 corresponding image pairs for human annotators. In total, 300 annotations were collected. D. Additional Qualitative Examples We present additional evaluation results in Figure 9. Examples 1 and 6 demonstrate how Reflect-DiT guides the generation process to accurately position objects within scene. Examples 2, 4, and 7 focus on object counting, ensuring that the correct number of distinct items. Example 3 presents particularly complex prompt, where Reflect-DiT accurately positions all objects while maintaining the correct quantity, such as the specified number of wooden barrels. Lastly, Example 5 highlights challenging caseseparating object identity from color attributesthat many generative models struggle with. Typically, models often conflate color and object identity, making requests like black sandwich difficult to fulfill. However, Reflect-DiT successfully distinguishes these attributes, demonstrating its advanced capability to handle nuanced prompts. Figure 9. Additional qualitative examples from Reflect-DiT."
        }
    ],
    "affiliations": [
        "Panasonic AI Research",
        "Salesforce AI Research",
        "UCLA"
    ]
}