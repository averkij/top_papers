{
    "paper_title": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost",
    "authors": [
        "Yinggan Xu",
        "Risto Miikkulainen",
        "Xin Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies ."
        },
        {
            "title": "Start",
            "content": "Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost Yinggan Xu 1 Risto Miikkulainen 2 3 Xin Qiu"
        },
        {
            "title": "Abstract",
            "content": "Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-EvolutionStrategies. 6 2 0 2 3 ] . [ 1 0 2 1 3 0 . 2 0 6 2 : r 1. Introduction The scaling of Large Language Models (LLMs) has unlocked emergent capabilities in mathematical reasoning, Work done during an internship at Cognizant AI Lab. 1University of California, Los Angeles, Los Angeles CA, USA 90095 2Cognizant AI Lab, San Francisco CA, USA 94105 3The University of Texas at Austin, Austin TX, USA 78712. Correspondence to: Xin Qiu <qiuxin.nju@gmail.com>, Yinggan Xu <yingganxu@gmail.com>. Preprint. February 4, 2026. 1 Figure 1. An overview of Quantized Evolution Strategies (QES). The goal is to optimize quantized LLMs directly on discrete parameter space in memory-efficient manner. Error residuals are accumulated at each iteration until they reach threshold for making discrete change. QES achieves temporal equivalence to high-precision optimization trajectory, while maintaining memory at the same level as inference-only quantized models. coding, and general problem-solving (Shao et al., 2024; OpenAI et al., 2024; Guo et al., 2025). However, this performance comes at significant computational cost. To mitigate the memory bottleneck of large-scale deployment, Post-Training Quantization (PTQ) has become standard practice. Techniques such as activation smoothing (e.g. SmoothQuant, Xiao et al., 2024) and layer-wise weight compression (GPTQ; Frantar et al., 2023; AWQ; Lin et al., 2024) make 3-4 bit precision inference possible with negligible performance degradation. It is then possible to deploy such models on consumer-grade hardware. Whereas PQT democratizes inference, the model becomes essentially static artifact. Any fine-tuning is typically done before the model is quantized, and requires massive, training-grade compute clusters that PTQ practitioners do not typically have. Methods that allow fine-tuning quantized models directly do not fare much better. For instance, Standard Quantization-Aware Training (QAT) relies on backpropagation and high-precision optimizer states, which often consume more memory than the model itself (Liu et al., 2024; Dettmers et al., 2023). While Zeroth-Order (ZO) optimization and Evolution Strategies (ES) have emerged as Quantized Evolution Strategies memory-efficient alternatives to pre-quantization fine tuning (Malladi et al., 2024; Qiu et al., 2025; Sarkar et al., 2025), they are severely limitated in discrete parameter spaces. Backpropagation-free approaches like QuZO and others (Zhou et al., 2025; Feng et al., 2024) have shown promise in Supervised Fine-Tuning (SFT) on quantized models, but struggle with reasoning tasks. The problem, identified in this paper as the stagnation problem, is that the discrete nature of the parameter space causes gradient signals to vanish, leading the optimization process to collapse. Moreover, inaccurate gradient due to the discretization errors severely reduces the optimization efficiency. This paper introduces Quantized Evolution Strategies (QES; Figure 1), novel optimization paradigm designed to perform full-parameter fine-tuning directly on the quantized model. By bridging quantized evolution with signal processing principles, specifically Delta-Sigma modulation (Inose et al., 1962; Razavi, 2016), it proposes an accumulated error feedback mechanism that allows the optimizer to sense and traverse high-precision gradients even in ultra-low bit settings (e.g., four-bit integers denoted as INT4). This mechanism mirrors historical techniques in communicationefficient training, such as 1-bit stochastic gradient descent (Seide et al., 2014; Strom, 2015; Karimireddy et al., 2019), but adapts them for the first time to the strict constraints of backpropagation-free quantized optimization. Furthermore, to address the memory overhead of tracking these error residuals, Stateless Seed Replay mechanism is introduced. It reconstructs optimization states on-the-fly reducing GPU memory requirements of QES to low-precision inference level. The paper makes three main contributions: 1. Mechanism for Optimization in Quantized Space: QES enables fine-tuning of LLMs directly within their low-precision quantized weights, yet maintains highprecision learning dynamics. Unlike gradient-based methods that require differentiable operations, QES utilizes accumulated error feedback to ensure progress on low-precision non-differentiable landscapes. 2. Inference-Level Memory Footprint: By replacing the storage of high-precision optimizer states with stateless seed replay, QES reduces GPU memory consumption to the order of low-precision inference. This level allows full-parameter learning on hardware that previously could only accommodate quantized inference. 3. Overcoming Gradient Stagnation and Discretization inaccuracy: QES resolves two challenges for optimization in quantized space: (1) stagnation of learning due to small gradient signals, and (2) inaccurate parameter updates due to gradient discretization errors. Evaluated in the Countdown task (Gandhi et al., 2024; Pan et al., 2025), QES significantly improves arithmetic reasoning in quantized baseline models, and does so significantly better than the state-of-the-art quantized fine tuning method QuZO. It therefore democratizes fine tuning, making it possible in low resource environments. The results also suggest new avenue for scaling LLMs in the future by including more quantized parameters in the same amount of memory. 2. Related Work In prior work, several methods have been developed for quantizing LLMs, for fine-tuning them without gradients, and for reducing errors resulting from quantization. These methods are reviewed in this section. 2.1. LLM Quantization and Low-Bit Fine-Tuning To mitigate the memory bottleneck of large-scale deployment, Post-Training Quantization (PTQ) has become standard practice. Techniques such as activation smoothing (e.g., SmoothQuant; Xiao et al., 2024)) and layer-wise weight compression (GPTQ; Frantar et al., 2023; AWQ; Lin et al., 2024) enable 3-4 bit precision inference with negligible degradation in performance. However, while PTQ excels at static inference, optimizing quantized models requires Quantization-Aware Training (QAT). Existing QAT approaches, such as LLM-QAT (Liu et al., 2024) or QLoRA (Dettmers et al., 2023), rely heavily on first-order approximations. These methods need either Straight-Through Estimators (STE) to approximate gradients through non-differentiable steps or backpropagation through frozen weights into high-precision adapters. Consequently, they suffer from two critical limitations: (1) the backward pass requires dequantization, which in turen requires significant memory, and (2) STE is inherently unstable in deep networks. Therefore, optimization paradigms capable of performing full-parameter fine-tuning directly on the quantized model are needed. 2.2. Zeroth Order Fine-tuning for Quantized Models Improving alignment and reasoning ability remains the central focus of LLM fine tuning. The standard approach is Reinforcement Learning with Human Feedback (RLHF), and it has been successful in general preference alignment (Ouyang et al., 2022), mathematical reasoning (Shao et al., 2024; Yu et al., 2025; Liu et al., 2025), code generation (Gehring et al., 2024; Guo et al., 2025), and general reasoning (OpenAI et al., 2024; An et al., 2025). However, RLHF relies fundamentally on backpropagation, restricting its application to differentiable, full-precision architectures with significant memory. Recently, Zeroth-Order (ZO) optimizer and Evolution Strate2 Quantized Evolution Strategies gies (ES) have emerged as backpropagation-free alternatives. Building on simultaneous perturbation stochastic approximation (SPSA; Spall, 1992), the MeZO method (Malladi et al., 2024) pioneered the scaling of ZO-SGD (Spall, 2002) to billion-parameter models. Similarly, recent work has demonstrated that ES can match or exceed RL performance in similar high-dimensional spaces (Qiu et al., 2025). However, these methods were originally designed for high-precision continuous parameters, and cannot be directly used on quantized models. Nevertheless, the backpropagation-free paradigm offers natural basis for quantized fine-tuning. Recent approaches like QuZO (Zhou et al., 2025) and QZO (Feng et al., 2024) extended ZO optimizers to supervised fine-tuning of quantized models. However, so far their success in reasoning tasks has been limited; they struggle to capture sparse signals, often leading to no meaningful convergence. QES addresses this gap by connecting quantized evolution to signal processing principles. 2.3. Techniques for Reducing Discretization Errors The fundamental challenge of approximating continuous signal with discrete steps is well-studied in the context of Delta-Sigma (Σ) modulation (Inose et al., 1962; Razavi, 2016). In this framework, the error introduced by taking discrete step (i.e. quantization) is not discarded but accumulated and applied to subsequent steps. This mechanism, known as noise shaping in DAC design, ensures that while individual steps may be coarse, the time-averaged behavior of the system tracks the continuous gradient accurately. In the domain of gradient compression, this classical principle can be instantiated as residual accumulation, or error feedback (Seide et al., 2014; Strom, 2015; Karimireddy et al., 2019). These methods compress gradient information for efficiency but accumulate the resulting error for correction in later steps. Unlike Stochastic Rounding (Gupta et al., 2015; Zhou et al., 2025), which achieves unbiased updates via probabilistic sampling but suffers from high variance, this method ensures convergence via deterministic accumulation of residuals. This process effectively shapes the quantization noise of the optimization trajectory, making it possible to apply quantized evolution to fine-tuning without the instability of prior QAT methods. 3. Method The challenge of quantized optimization is described first, followed by the QES methods with accumulated error feedback and seed replay. 3.1. The Optimization Challenge In LLM reasoning tasks, the objective is to maximize reward function J(W) defined over reasoning task: max WW E[J(W)]. (1) Standard first-order optimization methods (e.g., SGD, Adam) cannot be applied to this setting for two reasons: (1) the quantization operator is non-differentiable, and (2) there is insufficient memory for storing high-precision gradients and optimizer states in many edge applications. Consequently, Evolution Strategies (ES) is used to estimate descent directions via parameter-space exploration. In the standard continuous setting (Salimans et al., 2017; Qiu et al., 2025), ES approximates the gradient of the expected reward E[J(W)] using population of perturbations. For current parameter state Wt, the gradient estimate ˆg is computed as: ˆg = 1 σ (cid:88) i=1 Fi ϵi, (2) where ϵi (0, I) is random perturbation sampled from standard normal distribution, σ is the standard deviation for scaling the perturbation, and Fi = (Wt+σϵi) is the fitness (reward) corresponding to the perturbed weights. Note that fitness is thus the normalized reward score derived from J(Wt + σϵi) to makre sure optimization is stable. Fitness is evaluated on problem set according to the reinforcement learning from verifiable rewards (RLVR) framework (Shao et al., 2024), where the model is queried and assessed based on whether its responses are correct. Standard ES updates the parameters via gradient ascent: Wt+1 Wt + αˆg with learning rate α. While effective in continuous spaces at large scale (Qiu et al., 2025), this update rule fails in quantized spaces due to the vanishing magnitude of the update steps relative to the discretization granularity, or errors introduced during discretization. The solution is to provide error feedback in an accumulated manner, as will be described next. 3.2. Accumulated Error Feedback QES optimizes quantized LLMs directly within the discrete integer space {0, . . . , 2B 1}d, where is the number of model parameters and is the number of bits to represent each parameter, Thus, the parameters lie on low-precision lattice WQ (e.g., INT4) defined by Q(). Since the continuous perturbation σϵ from Eq. 2 violates quantization constraints, stochastic perturbation strategy is adapted from prior work (Connolly et al., 2021; Zhou et al., 2025). discrete perturbation δ is created by stochastically round3 Quantized Evolution Strategies Algorithm 1 QES with Accumulated Error Feedback 1: Input: Integer Weights W0 {0, . . . , 2B 1}d, Learning Rate α, Decay γ, Population , Maximum Iteration Number for each perturbation {1, . . . , } in parallel do Reconstruct δi using Eq. 3 and seed si Apply perturbation: Gate(Wt + δi) Execute inference and compute reward Fi 2: Initialize: Residuals e0 0 (FP16) 3: for = 0 to 1 do 4: 5: 6: 7: end for 8: Normalize reward for population 9: (cid:80)N Estimate gradient ˆgt 1 10: σ ut αˆgt + γet 11: 12: Wt Round(ut) et+1 ut Wt 13: 14: Wt+1 Wt + Wt 15: end for i=1 Fi δi // Apply accumulated error // Discretize update // Update accumulated error Algorithm 2 Stateless QES Update with Seed Replay 1: Input: Current Weights Wt, New Seeds St, New Rewards Ft, History H, Window K, Sigma σ 2: Output: Updated Wt+1, New History 3: Initialize proxy residual 0 4: for ( ˆS, ˆF) do Re-generate noise with history seed ϵ RNG( ˆS) 5: Re-compute grad ˆg Agg(ϵ, ˆF, σ) 6: usim αˆg + γe 7: 8: sim Round(usim) 9: inal Gate(Wt + sim) 10: 11: end for 12: Generate current noise ϵt RNG(St) 13: ˆgt Agg(ϵt, Ft, σ) 14: ut αˆgt + γe 15: Round(ut) 16: Wt+1 Gate(Wt + W) 17: Enqueue(H, (St, Ft)) 18: return Wt+1, // Add rematerialized error usim inal // Update proxy error ing the scaled Gaussian noise σϵ (where ϵ (0, I)): δ = σϵ + b, where Bernoulli(σϵ σϵ). (3) By recording the random seeds used to generate ϵ and b, the δ can be reproduced during optimization without the memory overhead of stored perturbation vectors (Qiu et al., 2025; Salimans et al., 2017). To strictly enforce the codebook limits defined by WQ, boundary gating is applied to mask invalid updates: ij = (cid:40) Wij + δij Wij if 0 Wij + δij < 2B, otherwise. (4) The gradient direction ˆg is then estimated by aggregating the discrete search directions weighted by their rewards: ˆg = 1 σ (cid:88) i=1 Fi δi. (5) critical challenge can be seen in Equation 5: The scaled update step αˆg is often smaller than the minimum discretization gap of the parameter lattice (i.e., αˆg < 1 for integer weights). Naively rounding this update either results in = 0, causing optimization to stagnate, or inaccurate gradient, reducing optimization efficiency. Interestingly, this challenge is well known in signal processing. The standard solution, Delta-Sigma (Σ) modulation (Inose et al., 1962; Razavi, 2016), employs feedback loop to accumulate quantization error over time, preserving signal fidelity. This approach is already used in communicationefficient training, such as 1-bit SGD, where uncompressed errors are carried forward to ensure convergence (Seide et al., 2014; Strom, 2015; Karimireddy et al., 2019). It can be used to solve the stagnation problem and preserve gradient accuracy in QES. In QES, high-precision error vector et (typically FP16) that accumulates the quantization error from previous steps is maintained. Rather than discarding the fractional component of the update, it is carried forward. The update dynamics at step are thus defined as: ut = αˆgt + γet1 Wt = Round(ut) et = ut Wt, (6) (7) (8) where ut represents the desired high-precision update, and γ (0, 1] is decay factor that stabilizes the history. This mechanism allows infinitesimal gradient signals to accumulate in over multiple iterations until they cross the rounding threshold (u 0.5), effectively simulating lower learning rate on the integer lattice. The full quantized evolution strategy with accumulated error feedback is listed in Algorithm 1. 3.3. Stateless Error Tracking via Seed Replay While Algorithm 1 ensures progress, it incurs significant memory penalty. Storing the dense high-precision error vector et Rd (typically FP16) often consumes more VRAM than the quantized model weights themselves, canceling the primary advantage of quantization. To resolve this bottleneck, QES includes Stateless Error Quantized Evolution Strategies Table 1. Countdown Task Accuracy (%). Comparison of QES against the original base models, state-of-the-art quantized zeroth-order method QuZO, and the variant with full residuals. The QuZO results were obtained with the best-performing configuration found through hyperparameter search. Whereas QuZO struggled to improve significantly over the base models especially in low-bit settings (INT4), QES improved the reasoning capabilities of the base models significantly across all quantization methods and model scales. Its performance was only slightly lower than with full residuals, indicating that the approximation method is effective. Thus,... MODEL FORMAT BASE MODEL QUZO FULL RES. (ORACLE) QES QWEN2.5-1.5B QWEN2.5-3B INT4 INT8 W8A8 INT4 INT8 W8A8 3.50 4.20 4.20 2.80 4.50 8.20 5.25 4.50 4. 14.25 15.85 10.75 18.05 22.10 15.25 33.50 33.30 31.70 16.00 26.35 15.35 31.85 37.40 21.35 Tracking. Note that the error state et is deterministic given the initial condition and the history of optimization steps. Rather than persisting et, it can be rematerialized on-thefly by replaying limited history window. To do that, lightweight history buffer Ht = {(Sτ , Fτ )}t1 τ =tK is maintained, containing only the random seeds and the scalar rewards for the past steps populations. To perform an update at step t, the error accumulation process is resimulated starting from an assumed zero error state at step K. Since the decay factor γ (0, 1), the contribution of errors from steps τ < vanishes exponentially (γK 0). During the replay of step τ , boundary constraints are checked using the current weights Wt rather than reconstructing the historical weights Wτ . Since discrete updates are sparse, Wτ Wt, and the discrepancy in boundary masking is minimal. This approach, detailed in Algorithm 2, trades computation for memory. By performing additional reconstruction operations per update, the optimizer state memory complexity is reduced from O(d) to O(K). This tradeoff is highly favorable because is orders of magnitude smaller than the parameter dimension d. In typical LLM optimization scenario, may exceed 109, whereas short window of 50 is sufficient to recover the accumulated error. For standard values (e.g., γ = 0.9, = 50), the influence introduced by truncating history is negligible. 4. Experiments QES was evaluated experimentally on arithmetic reasoning under strict memory constraints. This setup aims to demonstrate that the method can effectively fine-tune LLMs directly in their quantized versions without the need for full-precision gradients or auxiliary memory for optimizer states. The experimental setup is described first, followed by the main results on performance, and an analysis of accumulation hyperparameters and the fidelity of the stateless seed replay mechanism. 4.1. Experimental Setup Countdown (Pan et al., 2025) is compact reasoning task that is still challenging for LLMs. Given set of source numbers, the model must generate valid arithmetic expression using the operators +-*/ that equals the given target number. For instance, given the numbers 3, 4, and 52, and the target 44, the solution is 28 + 52/4 + 3 = 44. Performance was measured based on whether the expression is correct. The same prompt for the LLMs was used as in prior experiments with GRPO-Zero (Policy-gradient, 2025). Qwen2.5 models (Qwen et al., 2025) were used as the base LLM, quantized into INT4, INT8 formats using GPTQ (Frantar et al., 2023), and to W8A8 format using LLMCompressor (RedHatAI & vLLM Project, 2024). These formats represent varying degrees of precision and hardware compatibility. QES on these quantized models was then compared against the following systems: Base Model: The pre-trained quantized model without fine-tuning (a zero-shot baseline). QuZO (Zhou et al., 2025): quantized zeroth-order fine-tuning method, serving as the primary comparison to the state-of-the-art in quantized fine tuning. Full Residual: variant of QES that stores fullprecision (FP16) residuals, to measure the performance impact without the memory-saving Seed Replay optimization. In all experiments, QES was run for 300 generations. 4.2. Performance Table 1 summarizes the performance of QES compared to baselines across different model sizes and quantization formats. As mentioned in Section 3.2, the primary challenge in quantized optimization is the loss of information caused by the discrete parameter landscape. This problem is seen 5 Quantized Evolution Strategies Figure 2. Training curves for QUZO, QES, and Full-Residual QES compared to the Base Model. QuZO (Orange) performance is unstable and training collapses especially in the coarser INT4 landscape and with the smaller base model. In contrast, QES (Green) progresses steadily closely tracking the Full Residual Oracle (Blue) despite using significantly less memory. in Figure 2, where the Zeroth-Order method QuZO struggled to escape the initial performance plateau in all cases. For instance in INT4 with Qwen2.5-1.5B, it achieved only marginal gains over the base model, from 3.50% to 5.25% correct answers. In contrast, QES successfully navigated this landscape, achieving decisive performance improvement to 18.00%. This trend held for the larger 3B model as well, where QES more than doubled the performance of the base model (from 14.25% to 31.85%). This result empirically validates that the error-accumulation mechanism is essential for learning when the quantization is coarse. QuZOs performance depends strongly on the size of the base model. As shown in Table 1, while QuZO nearly stagnated on the 1.5B model (improving INT4 accuracy by only 1.75%), it managed to achieve meaningful gains on the larger 3B model (improving INT4 accuracy by 11.45%). This discrepancy aligns with the general intuition that smaller quantized models are significantly harder to optimize than their larger counterparts (Li et al., 2018). Smaller models possess fewer redundant parameters and sharper loss landscapes, making the search for valid descent directions on discrete lattice far more brittle. In contrast, QES demonstrates robustness across scales, achieving high performance even with the 1.5B model. 4.3. Effect of Accumulation Hyperparameters Table 2 evaluates the impact of two crucial accumulation hyperparameters, the replay window size (K) and decay factor (γ). Two regimes were compared: one where γ was scaled such that historical errors effectively vanished within the replay horizon (γK 0), and another where γ was held constant at 0.90. When γ was scaled to make the residual to vanish, performance was highly sensitive to the decay rate. With window of = 50 (γ = 0.90), the method achieved 16.00% accuracy. However, shrinking the window to = 10 needed an aggressive decay (γ = 0.58), causing performance to collapse to 4.55%. The results with fixed decay of γ = 0.90 revealed that this collapse was not due to the short window itself. Even at = 10, maintaining strong memory (γ = 0.90) retained robust accuracy of 13.05%, despite the reconstruction error for residuals introduced by truncating the history. 4.4. Fidelity of Stateless Seed Replay In Stateless Seed Replay, the perturbation noise ϵ is regenerated accurately from the preserved seed. However, the boundary gating conditions are created using the current weights Wt rather than the historical weights Wτ . In this section, the accuracy of this approximation is evaluated empirically. reconstruction errorwhere the replayed optimization path diverges from the originalcan only occur if the gating status of parameter changes. This change requires parameter to actively cross quantization boundary within 6 Quantized Evolution Strategies Window Size (K) Decay (γ) Performance (%)"
        },
        {
            "title": "Varying Decay Ratio",
            "content": "50 40 30 20 10 50 40 30 20 10 0.90 0.87 0.83 0.78 0."
        },
        {
            "title": "Fixed Decay Ratio",
            "content": "0.90 0.90 0.90 0.90 0.90 16.00 14.00 14.15 10.80 4.55 16.00 14.80 16.15 14.75 13.05 Table 2. Impact of Accumulation Hyperparameters on Performance. In the top table, the decay ratio γ scales with the window size K); in the bottom table, it is fixed at 0.90 for all window sizes. QUANTIZATION UPDATE RATIO HIT RATIO (ρ) 102 102 102 < 1 105 1.4 104 6 10 INT4 INT8 W8A8 Table 3. Update Ratio and Boundary Hit Ratio ρ in Stateless Seed Replay. The proportion of parameters updated per step alongside the fraction of those updates that encounter quantization boundaries. The updates are consistenly sparse ( 102) and the hit ratio negligible across all formats. Since reconstruction errors only occur when these conditions coincide, the error in Stateless Seed replay is vanishingly small. the replay window. Crucially, for each parameter update step, an update ratio can be defined as the ratio of parameters that are changed, and the boundary hit ratio (ρ) as the proportion of these changing parameters that hit quantization boundary. As shown in Table 3, the total ratio of updated parameters is consistently small ( 102 across all formats). In addition, ρ is negligible across all formats (e.g., < 1 105 for INT4). This observation indicates that even within the small subset of parameters that actually change, virtually none are located at the boundary where the state approximation would fail. Consequently, the intersection of these two events, i.e. an active update occurring precisely at boundary, is exceedingly rare. As result, the Stateless Seed Replay mechanism tracks the performance of the memory-heavy Full Residual variant (listed in Table 1) with near-perfect fidelity. 4.5. Accelerating State Reconstruction While Stateless Seed Replay only uses negligible memory for saving the history (i.e. few KB), its computational 7 Figure 3. continuous reward function and its instantiation on discrete grid. While the reward function can be optimized with high-precision gradient ascent, that method cannot be applied to the discrete case. Instead, the gradient needs to be estimated without vanishing signals or inaccurate moves between grid points. That is the challenge that QES is designed to solve. cost can also be flexibly managed. Reconstruction of the accumulated error feedback scales linearly with the window size K. As demonstrated in Table 2, reducing the window size from = 50 to = 20 reduces reconstruction cost by 60% while largely preserving performance (i.e. 14.75% instead of 16.00%, with fixed decay). This result demonstrates that the replay horizon acts as tunable parameter, allowing users to trade marginal accuracy for significant gains in training throughput. Furthermore, since the model is not serving inference requests during the update phase, the reconstruction process can be parallelized. One can potentially reconstruct multiple layers simultaneously by leveraging otherwise idle memory resources, such as offloaded KV caches or prefixing caches, to further mitigate the computational impact of the replay mechanism. 5. Temporal Equivalence to Continuous"
        },
        {
            "title": "Optimization",
            "content": "To understand why QES succeeds while other quantized methods stagnate or converge slowly, it is useful to characterize QESs optimization dynamics relative to an ideal underlying continuous model. While the quantized parameters are constrained to discrete grid WQ with spacing , the reward function J(W ) is defined over continuous domain. For instance, assume Gaussian smoothed objective Jσ(Θ) = EϵN (0,I)[J(Θ + σϵ)] (Figure 3). It can be optimized by high-precision gradient ascent, but such mechanism is not available on the discrete grid. Instead, the gradient needs to be approximated. One approach is to use stochastic gradient estimator ˆgt, like QuZO does (Zhou et al., 2025). Utilizing stochastic perturbation (double quantization) yields an unbiased estimator of the smoothed gradient: E[ˆgt] = Jσ(Wt). However, while an unbiased gradient is necessary, it is not sufficient Quantized Evolution Strategies for high performance on discrete grid. The critical failure occurs at the application of the gradient update. rounds to the nearest grid point, this error is strictly bounded by the grid resolution: eT /2. Consider the standard update rule at step with learning rate α and quantization operator Q: Wt+1 = Wt + Q(αˆgt). (9) The quantization operator can be decomposed into the identity plus an error term ξt, such that Q(x) = + ξt(x). Expanding the trajectory over steps yields Consequently, QES guarantees that the quantized model Wt never deviates more than half grid step from the ideal high-precision trajectory Θt. The residual et effectively integrates infinitesimal gradient signals over time until they cross the quantization threshold, triggering discrete update = 0 that aligns the physical model with the virtual continuous path. WT = W0 + 1 (cid:88) αˆgt + 1 (cid:88) ξt. (10) 6. Future Work t=0 (cid:124) (cid:123)(cid:122) (cid:125) Ideal Continuous Update t=0 (cid:124) (cid:123)(cid:122) (cid:125) Accumulated Quantization Loss The mechanisms behind the failure of stateless updates can be seen in this equation. First, stagnation occurs because the update magnitude in fine-tuning is often smaller than the grid precision (αˆgt < /2). Consequently, Q(u) 0, implying the error is ξt = αˆgt; thus, the Accumulated Quantization Loss exactly cancels the Ideal Continuous Update, resulting in WT = W0. Second, variance explosion arises if is stochastic. In this case, ξt becomes zero-mean noise variable with standard deviation proportional to . These and errors accumulate as random walk, scale with create noise floor that drowns out the subtle fine-tuning signal in the long run. QES solves this problem by introducing residual state et to enforce temporal equivalence between the discrete and continuous domains. First, Virtual Continuous Parameters Θt are defined as the sum of the physical discrete weights and the carry-over residual error: Θt Wt + et. (11) Then, expanding this equation with the QES update rules in Equations 68 gives Several promising directions exist in extending the capabilities of QES. First, while this paper validated QES on standard linear integer quantization (INT4, INT8, W8A8), it should be possible to extend it to more aggressive and non-uniform quantization paradigms, such as binary networks and floating-point formats (e.g., FP4). Second, the current stateless replay relies on fixed lookback window to manage the compute-memory trade-off. An adaptive algorithm could be created that automatically tunes and the decay rate γ based on real-time convergence stability or available hardware resources, effectively removing the need for manual hyperparameter selection. While the primary motivation for quantization so far has been to run existing large models on limited hardware, QES fine tuning opens up new frontier: given the same hardware, model with significantly more parameters can be quantized in the beginning, and QES be used to train the model directly in quantized space. Such scale-up is possible based on two aspects: (1) precision can be traded for more parameters, e.g. four-fold in case of INT4, and (2) only low-precision inference is needed during training, requiring about 1/12 of the memory of backpropagation (Malladi et al., 2024). Stacking these two benefits together opens up possibilities to train one or two orders of magnitude larger models with the same hardware. Θt+1 = Wt+1 + et+1 = (Wt + Wt) + (ut Wt) = Wt + ut = Wt + (αˆgt + et) = (Wt + et) + αˆgt = Θt + αˆgt. (12) 7. Conclusion Thus, the virtual parameters Θt evolve according to the dynamics of unconstrained, high-precision gradient ascent. The physical quantized weights WT at any step are related to this ideal trajectory by (cid:32) WT = ΘT eT = W0 + (cid:33) αˆgt eT . (13) 1 (cid:88) t= Crucially, unlike the stateless baseline where errors sum linearly or as random walk, the deviation in QES is determined solely by the single final residual eT . Since ΘT 8 Several barriers make effective fine-tuning of quantized LLMs difficult: gradient stagnation, inaccurate parameter update due to discretization, and prohibitive memory requirements. This paper introduced QES, novel backpropagation-free optimization framework designed to perform full-parameter optimization directly on quantized integer weights. First, vanishing or inaccurate gradients were identified as the primary cause of failure for existing ZO methods. Inspired by Delta-Sigma modulation, an accumulated error feedback mechanism was constructed to preserve and leverage these minuscule learning signals effectively. Second, to eliminate the memory overhead of storing high-precision error states, Stateless Seed Replay Quantized Evolution Strategies mechanism was developed. This mechanism makes fullparameter fine-tuning possible within the strict memory constraints of standard quantized inference. These mechanisms were evaluated empirically on challenging reasoning task. QES successfully overcame the quantized gradient issues and memory restrictions, significantly outperforming the state-of-the-art quantized fine-tuning baseline and achieving parity with memory-intensive oracles. Thus, by enabling the adaptation of large models on consumer-grade hardware, QES represents significant step toward democratizing access to LLM fine-tuning."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents Quantized Evolution Strategies (QES), an optimization framework designed to democratize access to Large Language Model (LLM) fine-tuning. By enabling high-precision learning directly within quantized parameter space, QES allows full-parameter adaptation on consumer-grade hardware that was previously restricted to static inference. Furthermore, the significant reduction in memory overhead contributes to more resource-efficient AI development, potentially lowering the energy footprint and environmental impact of scaling large models."
        },
        {
            "title": "References",
            "content": "An, Y. et al. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Connolly, M. P., Higham, N. J., and Mary, T. Stochastic rounding and its probabilistic backward error analysis. SIAM Journal on Scientific Computing, 43(1): A566A585, 2021. doi: 10.1137/20M1334796. URL https://doi.org/10.1137/20M1334796. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. QLoA: Efficient finetuning of quantized LLMs. Advances in neural information processing systems, 36: 1008810115, 2023. Feng, C., Zhuo, S., Zhang, X., Ramakrishnan, R. K., Yuan, Z., and Li, A. Z. Stepping forward on the last mile. Advances in Neural Information Processing Systems, 37: 9485194870, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. GPTQ: accurate post-training quantization for generative pre-trained transformers, 2023. URL https: //arxiv.org/abs/2210.17323. Gandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (SoS): Learning to search in language, 2024. URL https://arxiv.org/abs/2404.03683. Gehring, J., Zheng, K., Copet, J., Mella, V., Carbonneaux, Q., Cohen, T., and Synnaeve, G. RLEF: grounding code LLMs in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. Guo, D. et al. Deepseek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 645 (8081):633638, September 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL http:// dx.doi.org/10.1038/s41586-025-09422-z. Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep learning with limited numerical precision. In International conference on machine learning, pp. 1737 1746. PMLR, 2015. Inose, H., Yasuda, Y., and Murakami, J. telemetering system by code modulation - Σ modulation. IRE Transactions on Space Electronics and Telemetry, SET-8(3): 204209, 1962. doi: 10.1109/IRET-SET.1962.5008839. Karimireddy, S. P., Rebjock, Q., Stich, S., and Jaggi, M. Error feedback fixes SignSGD and other gradient compression schemes. In International conference on machine learning, pp. 32523261. PMLR, 2019. Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for LLM compression and acceleration, 2024. URL https: //arxiv.org/abs/2306.00978. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. LLMQAT: Data-free quantization aware training for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 467484, 2024. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J. D., Chen, D., and Arora, S. Fine-tuning language models with just forward passes, 2024. URL https:// arxiv.org/abs/2305.17333. OpenAI et al. OpenAI o1 system card, 2024. URL https: //arxiv.org/abs/2412.16720. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., 9 Quantized Evolution Strategies Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Spall, J. Multivariate stochastic approximation using simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332341, 1992. doi: 10.1109/9.119632. Pan, J., Zhang, J., Wang, X., Yuan, L., Peng, H., and Suhr, A. TinyZero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. Spall, J. C. Multivariate stochastic approximation using simultaneous perturbation gradient approximation. IEEE transactions on automatic control, 37(3):332341, 2002. Strom, N. Scalable distributed dnn training using comIn Sixteenth Annual modity GPU cloud computing. Conference of the International Speech Communication Association. INTERSPEECH, 2015. URL https:// www.isca-archive.org/interspeech 2015/ strom15 interspeech.pdf. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models, 2024. URL https://arxiv.org/abs/2211.10438. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.- Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: An open-source LLM reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Zhou, J., Yang, Y., Zhen, K., Liu, Z., Zhao, Y., Banijamali, E., Mouchtaris, A., Wong, N., and Zhang, Z. QuZO: Quantized zeroth-order fine-tuning for large language models. arXiv preprint arXiv:2502.12346, 2025. Policy-gradient. GRPO-Zero: Implementing deepseek r1s grpo algorithm from scratch. https://github.com/ policy-gradient/GRPO-Zero, 2025. Qiu, X., Gan, Y., Hayes, C. F., Liang, Q., Meyerson, E., Hodjat, B., and Miikkulainen, R. Evolution strategies at scale: LLM fine-tuning beyond reinforcement learning, 2025. URL https://arxiv.org/abs/2509.24372. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Razavi, B. The delta-sigma modulator [a circuit for all seasons]. IEEE Solid-State Circuits Magazine, 8(2):10 15, 2016. RedHatAI and vLLM Project. LLM compressor, URL https://github.com/vllm8 2024. project/llm-compressor. Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. Evolution strategies as scalable alternative to reinforcement learning, 2017. URL https://arxiv.org/ abs/1703.03864. Sarkar, B., Fellows, M., Duque, J. A., Letcher, A., Villares, A. L., Sims, A., Cope, D., Liesen, J., Seier, L., Wolf, T., Berdica, U., Goldie, A. D., Courville, A., Sevegnani, K., Whiteson, S., and Foerster, J. N. Evolution strategies at the hyperscale, 2025. URL https://arxiv.org/ abs/2511.16652. Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns. In Interspeech, volume 2014, pp. 10581062. Singapore, 2014. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300."
        }
    ],
    "affiliations": [
        "Cognizant AI Lab",
        "The University of Texas at Austin",
        "University of California, Los Angeles"
    ]
}