{
    "paper_title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
    "authors": [
        "Hongyuan Tao",
        "Bencheng Liao",
        "Shaoyu Chen",
        "Haoran Yin",
        "Qian Zhang",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 2 8 8 0 . 2 1 5 2 : r InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models Hongyuan Tao1, Bencheng Liao1 Shaoyu Chen2 Haoran Yin2 Qian Zhang2 Wenyu Liu1 Xinggang Wang1 1Huazhong University of Science and Technology 2Horizon Robotics Code & Model & Demo: hustvl/InfiniteVL Figure 1. Efficiency and performance of InfiniteVL. Left: Under comparable performance, InfiniteVL significantly improves single-GPU training throughput per day, streaming FPS, inference cache usage, and per-token latency over Qwen2.5VL-3B. Right: Speedperformance trade-off among VLMs, where InfiniteVL achieves real-time 24 streaming FPS with competitive performance at similar model scale. All inference results are measured on single NVIDIA RTX 4090."
        },
        {
            "title": "Abstract",
            "content": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design threestage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2% of the training data required by leading Intern of Horizon Robotics; Corresponding author: Xinggang Wang (xgwang@hust.edu.cn). VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6 inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains stable 24 FPS real-time prefill speed while preserving long-term memory cache. 1. Introduction Vision-Language Models (VLMs) [3, 9, 18, 28, 34, 35, 75] are playing increasingly important roles in next-generation multimodal systems, from autonomous driving [22, 31] to embodied agents [4, 12, 78]. Their scalability, however, is challenged by the well-known computational constraints of the Transformer architecture [60]. The quadratic complexity of self-attention with sequence length, coupled with 1 dynamically growing key-value cache during inference, creates unsustainable computational and memory demands. This inefficiency becomes critical bottleneck in longcontext scenarios, such as extended video understanding or continuous agent interaction. It effectively prohibits deployment on resource-constrained edge devices where realtime performance and low memory footprint are paramount. To cope with these constraints, many contemporary systems resort to myopic processing, discarding historical context to manage computational load. This sacrifice fundamentally undermines the models ability to maintain situational coherence [45, 49, 61, 66]. Existing architectural approaches for handling unlimited input of VLMs largely fall into two categories. One line of work converts fullattention models into windowed attention variants [6, 45, 67], which benefit from seamless reuse of pre-trained transformer weights and maintain performance close to the original VLM on short-term tasks. However, these models lack sustained memory capacity, as contextual information beyond the current window is rapidly discarded. The other direction rebuilds VLMs using linear attention [23], which offers promising efficiency and inherent support for longterm context memory [19, 29, 30, 32, 73, 79]. However, the inherent state compression in these models often leads to loss of fine-grained visual details, causing them to underperform on information-intensive tasks (e.g., OCR and document understanding) and lack effective training strategies to fully unleash their capabilities. As result, above architectures typically compromise one modeling objective for another, failing to bridge the performance gap between short-term tasks and long-term contexts. This limitation leads to lack of generality and underperformance compared to full-attention models. We posit that overcoming this trade-off demands an architecture that fulfills three critical design objectives: (i) it must effectively support unlimited multimodal input while maintaining consistent latency and memory usage during inference; (ii) it is desirable that the model effectively leverages pretrained weights from leading Transformer-based VLMs to achieve strong generalist performance with minimal additional training cost; and (iii) it should remain selfcontained and deployment-friendly, requiring no external memory banks or auxiliary storage. Guided by these objectives, our core design philosophy is to hybridize global, memory-efficient model for longrange context with localized attention for fine-grained perception. We introduce InfiniteVL, hybrid architecture that interleaves Gated DeltaNet layers [68] (e.g., 75% of layers) with Sliding Window Attention (SWA) layers (e.g., 25% of layers). The Gated DeltaNet layers maintain constant, compressed state to enable efficient long-range dependency modeling with linear-time complexity, while the SWA layers capture high-resolution local context through localized attention mechanisms. This synergistic design provides the model with native, learnable memory mechanism, removing the reliance on manually engineered memory modules and yielding system that is expressive and deploymentfriendly. To achieve strong multimodal capabilities with limited computational resources, we devise an efficient training strategy. We first curate diverse corpus from extensive public multimodal datasets, applying careful filtering and balancing to ensure data quality and representation. Our training protocol then proceeds in three dedicated stages: (i) In the Distillation Pretraining phase, we rapidly transfer rich pre-trained knowledge from powerful Transformerbased VLMs to initialize InfiniteVL, ensuring robust foun- (ii) The subsequent Supervised dational understanding. Fine-Tuning (SFT) phase sharpens the models instructionfollowing ability and alignment with human intent using high-quality dialogue and task-oriented data. (iii) Finally, the crucial Long sequence SFT phase equips the model to handle long-context scenarios by training on progressively extended sequences, thereby activating its architectural potential for sustained efficiency and coherent long-range reasoning. This strategy ensures sample-efficient learning and robust generalization across diverse task demands. Experiment results substantiate our claims. InfiniteVL delivers performance competitive with similar-sized leading Transformer-based VLMs on standard multimodal benchmarks, while also demonstrating strong long-term contextualized memory on long-horizon tasks such as video understanding. Its efficiency advantages are decisive in longcontext settings: InfiniteVL reduces per-token latency by 3.6 while maintaining constant memory profile. In streaming video scenarios, it sustains consistent real-time throughput of 24 FPS. In contrast, Transformer-based baseline decays from an initial 10 FPS to below 1 FPS after 200 frames and eventually fails with an Out-of-Memory (OOM) error at frame 294. Our contributions are summarized as follows: We introduce novel hybrid VLM that strategically interleaves Gated DeltaNet layers for efficient long-term contextualized memory with Sliding Window Attention for fine-grained local modeling. This self-contained design delivers unique combination of long-context efficiency, strong task generality, and deployment simplicity without relying on external memory modules. We design computationally efficient, multi-stage training strategy specifically tailored for linear-architecture VLMs like InfiniteVL, comprising dedicated stages of distillation pretraining, instruction SFT, and longsequence SFT. This strategy enables InfiniteVL to achieve highly competitive performance across diverse tasks, demonstrating robust capabilities in both short-term understanding and long-term contextualized memory. 2 InfiniteVL delivers an efficiency advantage: it achieves 3.6 reduction in per-token latency compared with equivalent Transformers and sustains real-time 24 FPS streaming video understanding speed while preserving longterm memory. Crucially, it maintains constant memory footprint during inference, enabling reliable deployment on edge devices with less than 9 GB of VRAM even in unlimited input scenarios, thereby mitigating the memory bottleneck that constrains high-capacity VLMs in resource-limited applications. 2. Related Work 2.1. Vision-Language Models Modern vision-language models are typically constructed by integrating pretrained large language models [5, 59] with powerful visual encoders [52] through web-scale imagetext pretraining, enabling open-vocabulary recognition and robust zero-shot generalization across diverse downstream tasks. This established paradigm delivers strong capabilities in vision-language tasks such as visual question answering (VQA), image captioning, referring/grounding, etc. Most contemporary VLMs additionally support video understanding through various temporal modeling mechanisms, allowing processing of short to medium-length video clips [3, 8, 9, 39, 56]. To address the challenges of long-horizon and streaming scenarios with potentially unlimited context, recent researches have pursued two primary architectural directions: (i) developing efficient linear-attention variants better suited for long sequences, and (ii) constraining attention within fixed windows to manage context growth. However, linearattention approaches often underperform on short-term, information-intensive tasks, while window-based methods lack effective long-term memory retention. To overcome these limitations, we propose InfiniteVL, which synergistically combines sliding window attention for capturing fine-grained local information with Gated DeltaNet for maintaining long-term contextualized memory, establishing VLM architecture natively supporting unlimited multimodal understanding scenarios. 2.2. Linear Attention Linear attention has emerged as pivotal variant of efficient attention mechanisms [23], designed to address the quadratic complexity of standard softmax attention. By decomposing the similarity computation through kernel-based feature mappings, it reformulates attention into associative inner products, reducing computational complexity from O(L2) to O(Ld2) while enabling recurrent inference. This formulation compresses historical information into fixedsize state, effectively mitigating the memory and computational bottlenecks associated with growing KV caches in long-context scenarios. Figure 2. Architecture of InfiniteVL. Visual inputs (images, videos, real-time streams) are embedded by naıve-resolution ViT and text by tokenizer, then concatenated and processed by stack of Hybrid Blocks. Each Hybrid SWA module for local, linear-time modeling and three Gated DeltaNet layers that read from and write to fixed-size memory cache to capture long-range dependencies, enabling context-lengthagnostic inference with constant throughput and GPU memory. However, vanilla linear attention maintains fixed-size associative memory by cumulatively adding new key-value outer products into single hidden state, which inevitably leads to capacity limits and memory collisions, making historical information increasingly entangled and harder to retrieve accurately as sequences grow longer [43, 54, 68]. To address this, recent advances have introduced forgetting mechanisms through gated updates [17, 33, 46, 51, 58, 69, 76, 77], as seen in models like Mamba [10, 17] and Gated Linear Attention [69]. Furthermore, recognizing the low-rank characteristics of memory matrices, several approaches have subsequently adopted delta rule [62]- based updates to enhance memory compaction, yielding architectures such as DeltaNet [70], Gated DeltaNet [68] and RWKV-7 [47]. We observe that the intermediate state in linear attention naturally serves as an effective repository for long-term memory in long-range vision-language modeling. Moreover, its gating mechanism inherently embodies recency bias by prioritizing recent inputs and attenuating distant context, aligning with the streaming demands of VLMs. Motivated by these properties, we identify linear attention as highly suitable token mixer for our InfiniteVL framework. Building upon prior efforts in linear attentionbased VLMs [19, 29, 30, 32, 50, 73], we design comprehensive model architecture coupled with an efficient training strategy. This enables InfiniteVL to achieve performance competitive with leading transformer-based VLMs of similar scale across multiple benchmarks, while main3 taining consistent efficiency advantages. 3. Method 3.1. Preliminary Transformer. Transformers [60] serve as the core architectural backbone of contemporary VLMs, mapping an input multimodal token sequence to textual output . The network is composed of stacked Softmax-attention layers interleaved with feed-forward networks (FFNs). The Softmax attention is defined as: Attn(X) = Softmax (cid:18) QK dk = XW Q, = XW K, (cid:19) V, = XW , (1) where Q, K, are learned projections, and dk is the key dimension. Functionally, this mechanism establishes token-to-token dependencies across modalities, enabling precise sequence modeling and leverages the parallel processing capabilities of modern GPUs during training. i=1 ϕ(ki)v Gated DeltaNet. Gated DeltaNet [68] is linear attention variant[54]. The basic linear attention replaces the exp(qtk ) term in standard Softmax attention with featuremap dot products ϕ(qt) ϕ(ki), which enables recurrent memory form: maintain St = (cid:80)t and compute the output as ot = ϕ(qt)St (i.e., using the query to read memory built by cumulative keyvalue outer products). Compared with Softmax attention, this design removes the need for growing KV cache and lowers computational overhead during inference. However, the memory matrix in conventional linear attention is often low rank, which can over-compress long contexts and cause information mixing on very long sequences. Gated DeltaNet enhances expressiveness by modulating the historical memory with gated scaling and Householder-like rotation, formulated as: (2) (cid:0)αt St = St1 (cid:1)(cid:1) + βt vtk , (cid:0)I βtktk where αt is learnable gate controlling memory retention, βt scales the update, I, kt, and vt denote the identity matrix, the key feature, and the value vector respectively. The reflector βtktk acts as Householder-style rotation [20] that reorients the accumulated memory along the incoming key direction, mitigating low-rank collapse while retaining the efficiency benefits of linear attention. 3.2. Overall Architecture As shown in Figure 2, InfiniteVL follows classic VLM design with three components: vision encoder, projection MLP, and decoder-only LLM. We adopt the Qwen2.5VLs encoder as the vision backbone for its native highresolution capability, and use lightweight MLP to project visual tokens into the LLM embedding space. The LLM part comprises 9 hybrid blocks: in each block, the first layer is an SWA + MLP layer, followed by three Gated DeltaNet + MLP layers. The hidden size is 2048. The SWA layer uses grouped multi-head attention (GQA) [1] with 16 query heads and 2 keyvalue heads, including RoPE [57] for positional encoding. Since Gated DeltaNet does not require growing KV cache at inference, we allocate higher capacity to its keyvalue outer-product memory by using 16 query heads together with corresponding keyvalue heads; we also insert 1D convolution (window size 4) and an output gate to boost expressiveness. The MLP uses hidden size of 11008 with SiLU activation. Gated DeltaNet uses no positional encoding and no weight bias. All modules are connected with residual pathways and apply layer normalization to their inputs. 3.3. Training Strategy To avoid the prohibitive cost of re-training after architectural changes and obtain strong performance with minimal resources, we curate training data from broad coverage and design three-stage training strategy as shown in Figure 3: (i) parameter reuse with efficient distillation pretraining to inherit the pretraining knowledge of advanced VLMs, (ii) instruction-following supervised fine-tuning (SFT), and (iii) multimodal long-sequence SFT to enhance length generalization. This progressive schedule is tailored to first stabilize optimization of the linear components and then gradually expose the model to more challenging multimodal interaction and long-context scenarios. Training data. To improve generalization across domains, we aggregate diverse multimodal corpora from FineVision [63], LLaVA-OneVision-1.5 [2], PixMo [11], The Cauldron [26], Docmatix [25], etc., covering General VQA, Caption & Knowledge, Chart & Table, OCR & Document, Mathematics & Reasoning, Science & Education, Code & Programmatic, and Text-only tasks. The overall diversity of our training dataset are on par with leading open-source VLMs such as InternVL2.5 [9]. Stage Efficient distillation pretraining. We choose Qwen2.5-VL as the teacher, replacing its attention layers with Gated DeltaNet while inheriting all remaining parameters. After initialization, we feed the same input to the i-th teacher Transformer layer and the i-th student Gated DeltaNet layer by taking the output of the teachers (i1)- th layer as their shared input, then align layerwise behavior with an MSE distillation loss between the outputs: layer = (cid:13) L(i) (cid:13)h(i) GDN h(i) Trans (cid:13) 2 2, (cid:13) (3) where h(i) Trans (teacher) are the outputs of their i-th layers. This same-input, output-matching deGDN (student) and h(i) 4 Figure 3. Three-stage training strategy of InfiniteVL. The student model is initialized from full-attention teacher, replacing its attention layers with Gated DeltaNet while inheriting all remaining parameters. Stage performs layer-wise and end-to-end distillation to align Gated DeltaNet with the teacher. Stage II applies large-scale supervised fine-tuning on diverse multimodal instruction data to build strong instruction-following and reasoning abilities. Stage III conducts long-sequence SFT with additional high-resolution, document, and video QA/Caption data to enhance length generalization. sign accelerates per-layer alignment and diminishes error accumulation across cascaded layers. After layerwise alignment, we perform end-to-end distillation without sharing intermediate states: the same multimodal sequence is fed to both teacher and student, and we minimize the KL divergence between their token-level output distributions: Llogit ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 KL(cid:0)Softmax(zT ) (cid:13) (cid:13) Softmax(zS )(cid:1) , (4) and zS where zT are teacher and student logits at token t. To improve training efficiency, we cap max image resolution at 512 512 and the maximum input length at 8192 tokens throughout Stage I. Stage II Instruction SFT. To move beyond mere imitation of the teacher and unlock the linear models potential, we enhance the distillated base with instruction tuning on diverse, multi-domain corpus. In this stage, we enhance response formatting and controllability to mitigate exposure bias, an issue not fully resolved by distillation alone [48]. Concurrently, we increase the maximum image resolution to 1344 1344 to improve detail perception and employ cross-entropy loss between the target token distribution and the students predictive distribution to align the students output with the target: LSFT ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 CE(cid:0)qt, Softmax(zS )(cid:1), (5) where qt is the target token distribution and zS is the student logits at token t. The maximum input length of this stage is still kept at 8192 tokens to enhance efficiency. Stage III Multimodal long-sequence SFT. To enhance length generalization and long-sequence multimodal length to reasoning, we extend the maximum context 32,768 tokens. The training data are constructed by uniformly sampling 200K caption and QA pairs from LLaVAVideo-178K [72] at 10 FPS, with each video spanning up to 224 frames and each frame encoded into maximum of 256 tokens. Additionally, we blend 800K samples from the Stage II SFT corpus to form diversified long-context training set. Notably, all training in this stage is performed via Low-Rank Adaptation (LoRA) [21]. The optimization objective remains consistent with Stage II, concentrating model capacity on robust long-range multimodal modeling. 4. Experiment 4.1. Implementation Detail trainings were Training setup. All conducted on NVIDIA H20 GPUs using bfloat16 (BF16) precision. The training corpus comprises exclusively publicly available, open-source datasets totaling 10M samples. For each stage, we uniformly subsample the required portion of data. We use AdamW with β = (0.9, 0.999) and weight decay 0.01, along with cosine-annealing learning-rate schedule and 5% linear warm-up. In Stage I, we use learning rate of 2 104, batch Model Complexity MME MMStar MMBenchtest en SeedBenchimage ScienceQAval RealworldQA AI2Dw/o Average Transformer-based VLMs TinyLLaVA-3B [74] PaliGemma2-3B [16] Phi-3.5-Vision-4B [44] SmolVLM2-2B [40] InternVL2.5-4B [9] Qwen2.5VL-3B [3] Linear/Hybrid VLMs MaTVLM-3B [30] Cobra-3B [73] InfiniteVL-4B O(n) O(n) O(n) O(n) O(n) O(n) O(n) O(1) O(1) 1733 1658 1846 1764 2338 2171 1771 1346 2126 37.9 52.7 47.5 46.0 58.3 54. 37.5 34.7 55.6 69.5 60.7 76.0 43.0 81.1 78.2 69.4 55.9 79.0 70.2 71.6 71.2 70.9 74.1 73.3 65.6 63.3 72.9 68.7 94.3 92.2 90.0 97.0 81. 65.9 60.3 93.4 55.0 58.3 57.9 58.4 64.3 65.4 52.3 51.0 67.3 61.8 72.2 77.8 74.9 81.4 81.6 58.9 46.8 77.2 61.8 68.0 70.9 64.8 78.5 74. 60.1 52.3 75.8 Table 1. Comparison of InfiniteVL with existing VLMs on public multimodal understanding, real-world comprehension benchmarks. The upper block lists leading transformer-based models and the lower block linear/hybrid models. Complexity denotes per-token computational and memory complexity at inference. Best results are in bold, second-best are underlined. Model Complexity ChartQAtest TextVQAval DocVQAtest OCRBench MMMUval MathVistamini Average Transformer-based VLMs TinyLLaVA-3B [74] PaliGemma2-3B [16] Phi-3.5-Vision-4B [44] SmolVLM2-2B [40] InternVL2.5-4B [9] Qwen2.5VL-3B [3] Linear/Hybrid VLMs MaTVLM-3B [30] Cobra-3B [73] InfiniteVL-4B O(n) O(n) O(n) O(n) O(n) O(n) O(n) O(1) O(1) 21.2 33.6 81.8 68.8 84.0 84.0 20.0 17.9 82.0 55.3 63.0 72.0 73.2 76.8 79.6 53.2 47.9 78. 34.7 71.6 69.3 80.0 91.6 93.9 33.0 24.0 91.7 36.0 60.1 59.9 72.9 82.8 79.7 35.1 30.7 79.8 36.2 30.3 43.0 42.0 52.3 49.6 34.4 31.5 44. 28.3 27.7 43.9 51.5 60.5 62.3 28.5 22.3 65.4 35.3 47.7 61.7 64.7 74.7 74.9 34.0 29.1 73.6 Table 2. Comparison of InfiniteVL with existing VLMs on public text-rich, reasoning-centric multimodal benchmarks. Other notations follow Table 1. size of 64, with both layer-wise and end-to-end distillation trained on 1M multimodal Caption & QA pairs each. Stage II employs learning rate of 5 105, batch size of 256, and trains on 8M multimodal QA pairs. For Stage III, the configuration includes learning rate of 2 105, batch size of 64, and training on 0.25M multimodal longsequence Caption & QA pairs together with 0.75M multimodal SFT data from Stage IIs training corpora to prevent excessive distribution shift. comprehensively"
        },
        {
            "title": "InfiniteVL across multiple",
            "content": "Evaluation Benchmarks. To assess dimensions, we evaluate on the following public benchmarks using VLMEvalKit[13] with default prompts: MME[14], MMStar[7], MMBenchtest en[36], SeedBenchimage[27], RealworldQA[65], AI2Dw/o M[24], ScienceQAval[53], TextVQAval[55], ChartQAtest[41], OCRBench[37], DocVQAtest[42], MMMUval[71], MathVistamini[38], Video-MME[15], and LongVideoBench[64]. 4.2. Main Comparison General Benchmark Metrics. We first evaluate the core multimodal capabilities of InfiniteVL after Stage II on established benchmarks. As shown in Tab. 1 and Tab. 2, we assess performance across six categories: Comprehensive Multimodal Understanding, Real-World Compre6 hension, OCR, Chart Understanding, Document Understanding, and Multimodal Reasoning & Mathematics. We compare our model against range of widely-adopted open-source Transformer-based VLMs [3, 9, 16, 40, 44, 74] with similar model size and training data scale, as well as prior leading models based on linear or hybrid architectures [30, 73]. Results show that InfiniteVL achieves performance that is close to that of Transformer-based VLMs in most areas. More notably, it performs well on the challenging, information-dense tasks of OCR, Chart Understanding, and Document Understanding which have historically been challenging for existing linear or hybrid architectures. We attribute this to the hybrid design of Gated DeltaNet layers, which maintain compressed long-range context, together with limited number of Sliding Window Attention layers that help capture local dependencies. Importantly, these gains are obtained without resorting to task-specific architectures or handcrafted memory modules, suggesting that the synergizing design is generally applicable across diverse multimodal scenarios. This combination appears to offer better trade-off for handling both long-range coherence and fine-grained vision-language alignment. Length generalization study. This section validates the long-term memory capability of InfiniteVL, core advanFigure 4. Length generalization and inference efficiency of InfiniteVL: (ab) On Video-MME and LongVideoBench, InfiniteVL delivers stable performance as the number of input frames increases, whereas Qwen2.5-VL-3B(SWA) degrades once the context length exceeds its attention window. (c) InfiniteVL attains over 3.6 lower per-token latency than transformer-based VLM of similar size. (d) InfiniteVL maintains real-time streaming inference at 24 FPS with 274 tokens per frame, while Qwen2.5-VL-3B rapidly slows down and eventually runs out of memory. tage of its hybrid architecture. We compare InfiniteVL after Stage III long-sequence SFT against Qwen2.5-VL3B model that exclusively uses Sliding Window Attention (SWA) with window size equivalent to InfiniteVL. The evaluation is conducted on Video-MME [15] and LongVideoBench [64] under setting of 1 fps, 256 tokens per frame, and varying maximum sampled frames: 8, 16, 32, 64, 128, 256, 512 and 1024, corresponding max context length from 2K to 256K. finiteVL achieves 3.6 inference speedup at around 50K tokens, and this advantage expands to 8 at 300350K tokens, where the baseline triggers out-of-memory (OOM) due to its ever-growing keyvalue cache. These results confirm that InfiniteVLs computational and memory complexity is effectively independent of sequence length, enabling constant-latency and constant-memory inference even in ultra-long contexts. As shown in Figure 4(a-b), although InfiniteVL is only finetuned on 200K video clips thus initially underperforms Qwen2.5-VL-3B(SWA) at shorter sequence lengths, notable divergence occurs when the number of frames exceeds 32. Beyond this point, the performance of Qwen2.5-VL3B(SWA) degrades significantly, while InfiniteVL maintains or even improves its initial performance as the frame count increases. This resilience is attributed to the linear layers memory cache, which effectively propagates longterm visual-language context, demonstrating robust longterm memory retention. Additionally, in the Appendix we provide visualization of the memory cache norm across linear layers in InfiniteVL. As the input sequence grows, the memory cache expands rapidly in the early stages and then stabilizes at plateau, avoiding explosive growth and reflecting effective long-term memory management. Inference efficiency comparison. We evaluate inference efficiency under two challenging long-context scenarios: long-term context inference and streaming video understanding. All experiments are conducted on single NVIDIA RTX 4090 GPU under consistent settings, with each experiment repeated three times to report the average results and mitigate the impact of random variations. As shown in Figure 4(c), InfiniteVL exhibits nearly constant per-token latency regardless of context length, sustaining stable decoding rate of 100 tokens/s while maintaining constant GPU memory footprint of 9 GB. In contrast, the Transformer-based baseline Qwen2.5-VL-3B Inshows linearly growing latency and memory usage. For streaming video understanding, we retain the historical cache across frames and represent each frame using 274 tokens. The constant size of InfiniteVLs memory cache enables aggressive optimization through CUDA Graph, which captures the stable execution path into static graph to eliminate runtime scheduling overhead. Figure 4(d) shows that InfiniteVL maintains stable real-time prefill throughput of 24 FPS, whereas Qwen2.5-VL-3B suffers from rapid throughput degradation (from 10 FPS to 1 FPS) as historical KV-Cache accumulates, eventually encountering OOM at around 300 frames. In summary, InfiniteVL demonstrates length-invariant latency and low, stable memory consumption across both long-context and streaming-video scenarios, providing an efficient and scalable foundation for long-term multimodal understanding and extended conversational understanding. 4.3. Architecture research Linear model. We systematically evaluate the impact of different linear sequence modeling modules on VLM performance under consistent distillation configurations. As shown in Tab. 5, Vanilla Linear Attention [23] demonstrates training instability and fails to effectively inherit pretraining knowledge from the teacher model, producing unusable results across several benchmarks. While both Mamba [17] and Gated Linear Attention (GLA) [69] which incorporate scalar gating mechanisms to modulate hidden states achieve convergence, they exhibit limitations on information-intensive document and text understanding tasks (e.g., DocVQA, TextVQA, OCRBench). In contrast, Gated DeltaNet [68] achieves significantly improved convergence stability and downstream perfor7 Hybrid Ratio General Text-rich Averages MME MMStar RealWorldQA SeedBenchimage TextVQA ChartQAtest OCRBench DocVQAtest Gen-Avg Text-Avg 0 1/8 1/4 1/2 1986 2008 2010 2089 44. 44.1 45.0 48.1 60.5 59.3 60.0 61.2 68.5 69.1 70.6 71.0 71. 73.7 76.0 77.0 73.1 75.8 77.5 79.0 72.6 76.4 78.6 79.4 74. 84.0 87.8 90.2 57.8 72.7 57.5 ( 0.3) 77.5 ( +4.8) 58.5 ( +0.7) 80.0 ( +7.3) 60.1 ( +2.3) 81.4 ( +8.7) Table 3. Ablation on the hybrid ratio. General-Avg is the mean of MME, MMStar, RealWorldQA, and SeedBenchimage; Text-Avg is the mean of TextVQA, ChartQAtest, OCRBench, and DocVQAtest. Values in parentheses denote absolute gains over the Hybrid=0 setting. Stages General Text-rich Averages vs S1+S2 S1 S2 S3 MME MMStar RealWorldQA SeedBenchimage ChartQAtest TextVQA DocVQAtest OCRBench Gen-Avg Text-Avg Gen Text NAN 2010 1713 2120 2086 NAN 45.0 39.6 54.7 52.2 NAN 60.0 54.1 66.3 63.8 NAN 70.6 65.7 72.7 72.5 NAN 77.5 73.0 82.4 81.0 NAN 76.0 73.4 78.3 76. NAN 87.8 84.1 91.1 89.1 NAN 78.6 74.6 79.8 78.4 NAN 64.0 57.0 69.6 68.0 NAN 80.0 76.3 82.9 81.3 ( 5.6) ( 2.9) ( 12.6) ( 6.6) ( 0.0) ( 0.0) ( 1.6) ( 1.6) Table 4. Ablation on training stages. General includes MME, MMStar, RealWorldQA, and SeedBenchimage; Text-rich includes ChartQAtest, TextVQA, DocVQAtest, and OCRBench. columns report absolute changes relative to the Stage1+2 setting. Architecture MME MMStar DocVQA TextVQA OCRBench Linear attention NAN NAN 40.1 1686 Mamba 40.3 GLA 1712 44.4 Gated DeltaNet 1986 NAN 15.3 19.0 74. NAN 34.3 35.0 71.0 NAN 49.0 51.0 72.6 Table 5. Ablation for linear architecture. Frames 8 32 64 128 256 512 Video-MME w/o stage3 0.466 0.493 0.471 0.459 0.453 0.432 0.436 0.436 0.464 0.487 0.498 0.526 0.526 0.530 0.527 0.521 w/ stage3 LongVideoBench w/o stage3 0.456 0.471 0.452 0.426 0.426 0.418 0.414 0.420 0.478 0.499 0.496 0.490 0.474 0.481 0.467 0.479 w/ stage3 Table 6. Ablation for training stage3. mance, benefiting from its more efficient state compression mechanism. It attains scores of 74.2, 71.0, and 72.6 on DocVQA, TextVQA, and OCRBench, respectively, representing substantial improvements over Mamba and GLA (e.g., +55.2, +36.0, and +21.6 over GLA). These results indicate that efficient state compression mechanisms are crucial for fine-grained visual-language tasks. Hybrid strategy. We systematically evaluate the impact of the hybrid ratio of SWA layers in the architecture on model performance. As summarized in Tab. 3, even small fraction of SWA layers leads to significant improvements on text-intensive benchmarks. Further increasing the ratio continues to yield gains, but with clearly diminishing marginal returns. In contrast, metrics for general multimodal understanding benchmarks improve more gradually across different configurations. Balancing performance and efficiency, we select ratio of 1/4 as the default configuration. Subsequent SFT experiments confirm that this setting allows the model to closely approach the performance of comparable Transformerbased models on most benchmarks, while avoiding the compromised long-range memory capacity and computational efficiency that would result from an excessively high proportion of SWA layers. 4.4. Training Strategy As shown in Tab. 4, directly initializing the linear layers fails to yield usable capability. Stage (distillation pretraining) enables the model to acquire basic conversational ability, though performance remains bounded by the teacher models limitations. After Stage II (SFT), the model surpasses the teacher on several metrics, indicating better generalization and task alignment. Notably, when we directly SFT the model from scratch using the combined data volume of Stage and Stage II, it resulted in weak performance across various metrics, underscoring the necessity of the distillation pretraining stage. After Stage III (long-sequence SFT), the model exhibits slight performance drop on general benchmarks due to distribution shift toward longer sequences while showing substantially improved length extrapolation and generalization as validated in Tab. 6. Overall, the combination of Stage and Stage II yields the strongest general performance, whereas Stage III trades off modest drop in short-term performance for significantly stronger long-context generalization. In practice, further balancing the data mix between general SFT and long-sequence SFT may allow for more desirable trade-off between short-term performance and long-range capability. 5. Conclusion In this paper, we introduced InfiniteVL, linear-complexity VLM that synergizes Gated DeltaNet with sliding window attention, together with carefully designed training strategy. Using only limited amount of multimodal training data, InfiniteVL achieves performance competitive with leading Transformer-based VLMs and demonstrates long8 term memory capability through long-sequence finetuning. Benefiting from its linear architecture, InfiniteVL delivers over 3.6 inference speedup while maintaining constant memory footprint, enabling real-time 24 FPS prefill speed on unlimited multimodal streams. This offers practical and efficient solution for deploying high-capacity VLMs in resource-constrained environments. In the future, we plan to further improve the long-term memory mechanism in InfiniteVL by exploring more effective representations for sustained multimodal contextual modeling."
        },
        {
            "title": "References",
            "content": "[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. 4 [2] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. 4 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3, 6 [4] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy π0: Groom, Karol Hausman, Brian Ichter, et al. vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550. arXiv preprint ARXIV.2410.24164. 1 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 3 [6] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. 2 [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [8] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. In The Thirteenth International Conference on Learning Representations. 3 [9] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 1, 3, 4, 6 [10] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning, pages 1004110071. PMLR, 2024. 3 [11] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. 4 [12] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, pages 84698488. PMLR, 2023. 1 [13] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. 6 [15] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 6, 7 [16] Google DeepMind and Google Research. Paligemma 2 google ai for developers (model page). https://ai. google.dev/gemma/docs/paligemma, 2025. Accessed 2025-11-12. 6 [17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling. 3, 7 [18] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 1 [19] Haowen Hou, Peigen Zeng, Fei Ma, and Fei Richard Yu. Visualrwkv: Exploring recurrent neural networks for viIn Proceedings of the 31st Intersual language models. national Conference on Computational Linguistics, pages 1042310434, 2025. 2, [20] Alston Householder. Unitary triangularization of nonsymmetric matrix. Journal of the ACM (JACM), 5(4):339 342, 1958. 4 9 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5 [22] Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Senna: Bridging large vision-language modarXiv preprint els and end-to-end autonomous driving. arXiv:2410.22313, 2024. 1 [23] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. 2, 3, 7 [24] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251. Springer, 2016. [25] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visionTronchon. language models: insights and future directions., 2024. 4 [26] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. 4 [27] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 6 [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [29] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Videomamba: State space model In European conference for efficient video understanding. on computer vision, pages 237255. Springer, 2024. 2, 3 [30] Yingyue Li, Bencheng Liao, Wenyu Liu, and XingHybrid mamba-transformer arXiv preprint gang Wang. for efficient vision-language modeling. arXiv:2503.13440, 2025. 2, 3, Matvlm: [31] Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, et al. Recogdrive: reinforced cognitive framework for end-to-end autonomous driving. arXiv preprint arXiv:2506.08052, 2025. 1 [32] Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, and Xinggang Wang. Multimodal mamba: Decoder-only multimodal state space model via quadratic to linear distillation. arXiv preprint arXiv:2502.13145, 2025. 2, 3 [33] Bencheng Liao, Xinggang Wang, Lianghui Zhu, Qian Zhang, and Chang Huang. Vig: Linear-complexity visual In Proceedsequence learning with gated linear attention. ings of the AAAI Conference on Artificial Intelligence, pages 51825190, 2025. 3 10 [34] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26679 26689. IEEE, 2024. [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1 [36] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 6 [37] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. 6 [38] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. CoRR, 2023. 6 [39] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1258512602, 2024. 3 [40] Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. [41] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. 6 [42] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 6 [43] Luke McDermott, Robert Heath Jr, and Rahul Parhi. Lola: Low-rank linear attention with sparse caching. arXiv preprint arXiv:2505.23666, 2025. 3 [44] Microsoft. Phi-3.5-vision-instruct (model card). https: / / huggingface . co / microsoft / Phi - 3 . 5 - vision - instruct, 2025. Lightweight multimodal model with 128K context; official model card. Accessed 2025-11-12. 6 [45] Zhenyu Ning, Jieru Zhao, Qihao Jin, Wenchao Ding, and Minyi Guo. Inf-mllm: Efficient streaming inference of multimodal large language models on single gpu. arXiv preprint arXiv:2409.09086, 2024. 2 [46] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. [47] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, et al. Rwkv-7 goose with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456, 2025. 3 [48] Andrea Pozzi, Alessandro Incremona, Daniele Tessera, and Daniele Toti. Mitigating exposure bias in large language model distillation: an imitation learning approach. Neural Computing and Applications, pages 117, 2025. 5 [49] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. Advances in Neural Information Processing Systems, 37:119336119360, 2024. 2 [50] Yanyuan Qiao, Zheng Yu, Zijia Zhao, Sihan Chen, Mingzhen Sun, Longteng Guo, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space models for multimodal learning. In NeurIPS Efficient Natural Language and Speech Processing Workshop, pages 102113. PMLR, 2024. 3 [51] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. 3 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [53] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. 6 [54] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. 3, 4 [55] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 6 [56] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 3 [57] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [58] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. 3 [59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, 4 [61] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 2 [62] WIDROW. Adaptive switching circuits. In 1960 IRE WESCON Convention Record, Part 4, pages 96104, 1960. 3 [63] Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andres Marafioti. Finevision: Open data is all you need. arXiv preprint arXiv:2510.17269, 2025. 4 [64] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 6, [65] xAI. Realworldqa, 2024. 6 [66] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations. 2 [67] Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, and Song Han. Streamingvlm: Real-time arXiv preprint understanding for infinite video streams. arXiv:2510.09608, 2025. 2 [68] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In The Thirteenth International Conference on Learning Representations, . 2, 3, 4, 7 [69] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In Forty-first International Conference on Machine Learning, . 3, 7 [70] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. Advances in neural information processing systems, 37:115491115522, 2024. 3 [71] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 11 [72] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 5 [73] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1042110429, 2025. 2, 3, 6 [74] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. 6 [75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations. 1 [76] Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, and Xinggang Wang. Dig: Scalable and efficient diffusion models with gated linear attention. arXiv, 2024. [77] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. 3 [78] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. 1 [79] Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, and Xinggang Wang. Omnimamba: Efficient and unified multimodal understanding and generation via state space models. arXiv preprint arXiv:2503.08686, 2025. 2 12 InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "unlimited streaming input sequences. SWA Layers: In InfiniteVL, we employ Sliding Window Attention (SWA) with window size of 8192, balancing performance on short-term tasks and long-stream scenarios while preserving computational efficiency. This configuration is flexible and can be adapted to specific application requirements. Since attention is restricted to the most recent 8192 tokens, and combined with RoPE, the relative positional rotations between tokens are confined within this local window. This design avoids instability caused by longrange position extrapolation, allowing the model to maintain stable local understanding even under infinitely long streaming inputs. 7. Comprehensive Evaluation Setup Details We adopt VLMEvalKit as our multimodal evaluation framework and utilize its default prompt configurations for all benchmarks. During evaluation, the maximum generation length for InfiniteVL is set to 256 tokens, with image resolutions ranging from 128 128 to 1344 1344. For specialized tasks such as MMMU and MathVista-MINI, we employ Qwen3-4B-Instruct as the evaluation model to ensure consistent and reliable assessment of reasoning capabilities. 8. Case Study across multiple scenarios We present several examples to demonstrate the capabilities of InfiniteVL. As shown in Figure 6, InfiniteVL exhibits strong comprehension across high-resolution, textintensive, and complex structural scenarios. Beyond fundamental vision-language understanding, we further investigate the models performance in long-term streaming scenarios. Using two extended videos from street-view recordings and dashcam footage, we sample frames at 1 FPS and continuously feed them to InfiniteVL to update its memory cache, while posing questions at randomly selected timestamps. As shown in Figures 7 and 8, InfiniteVL maintains stable comprehension and reasoning capabilities even under extremely long contexts exceeding 512K tokens. In this part, we provide additional details about InfiniteVL, which are omitted due to the 8 page limit of the main paper. Specifically, Section 6 analyzes the cache evolution of linear layers versus SWA layers in InfiniteVL under long-term streaming inputs, illustrating its length generalization capability. Section 7 supplements comprehensive evaluation setup details to facilitate the reproduction of our experimental results. Section 8 presents series of visualization cases demonstrating InfiniteVLs performance in general VQA, text-rich understanding, and ultra-long streaming understanding scenarios. 6. Analysis of Cache Evolution under Streaming Inputs Linear Layers: In the linear layers, each output is computed by multiplying the query with memory cache formed through the cumulative outer product of keys and values. This memory cache, matrix of size 16128256, serves as one of the core components influencing the output, and its stability directly affects output stability. To longterm behavior in streaming scenarios, we track the evolution of the L2 norm of this memory cache as the number of input frames increases, using it as metric for long-term stability. Figure 5. L2 norm of the Linear-layer memory cache versus input frame index: the norm increases rapidly at the beginning and then stabilizes. As shown in Fig. 5, the norm of the memory cache rises rapidly during the initial frames and then produce convergence, demonstrating stable behavior without exhibiting unbounded growth. This indicates that the cache management mechanism remains effective and that the linear layers maintain stable comprehension capability when processing 1 Figure 6. Examples of fundamental Visual-Language Understanding 2 Figure 7. Examples of Long-Term Streaming Understanding Capability 3 Figure 8. Examples of Long-Term Streaming Understanding Capability"
        }
    ],
    "affiliations": [
        "Horizon Robotics",
        "Huazhong University of Science and Technology"
    ]
}