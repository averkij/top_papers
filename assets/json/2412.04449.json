{
    "paper_title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
    "authors": [
        "Jun Zhang",
        "Desen Meng",
        "Ji Qi",
        "Zhenpeng Huang",
        "Tao Wu",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training."
        },
        {
            "title": "Start",
            "content": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay Jun Zhang1,* Desen Meng1,* Ji Qi 2 Zhenpeng Huang 1 Tao Wu 1 Limin Wang 1,3, (cid:0) 1State Key Laboratory for Novel Software Technology, Nanjing University 2China Mobile (Suzhou) Software Technology Co., Ltd. 3Shanghai AI Lab https://github.com/MCG-NJU/p-MoD 4 2 0 2 5 ] . [ 1 9 4 4 4 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, pMoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training. 1. Introduction Recently, both academia and industry have witnessed the rapid development of multimdodal large language models (MLLMs) [1, 13, 20, 39, 41, 44], which have demonstrated exceptional performance across various visual-language understanding tasks. This success has positioned MLLMs as one of the most promising areas of AI research. * Equal contribution. (cid:0) Corresponding author (lmwang@nju.edu.cn). Figure 1. Comparison with the LLaVA-NeXT [24] baseline. Our model demonstrates comparable performance with the baseline model on 14 benchmarks across various domains, with 46.2% fewer KV cache storage and 44.4% fewer TFLOPs during inference. Reflecting on the roadmap of MLLM development, pioneering works [7, 23, 25, 47] focused on taking single image as input and resizing it to fixed low resolution that aligns with the training setup of the vision encoder. Subsequently, to meet the diverse demands of real-world applications, increasing efforts have broadened the forms of visual inputs that MLLMs can support, including multiple high-resolution images and videos[5, 20, 38, 39, 44]. Current state-of-the-art MLLMs handle high-resolution images either by dividing the original image into multiple slices [5, 24, 44] which are independently processed by the vision encoder, or by using stronger vision encoder with improved positional encoding which can process any image at its native resolution [1, 27, 39]. Consequently, when pro1 cessing multiple high-resolution images or videos, the number of vision tokens increases dramatically, proportional to the number of pixels and the number of images or video frames. The overwhelming volume of vision tokens processed by the transformer decoder results in explosion of computational costs, which severely hampers further development and broader application of MLLMs. Designing more efficient MLLM architectures with minimal performance degradation has thus become an urgent challenge for the community. Previous efforts primarily focus on compressing vision tokens before the decoder with lightweight connector [2, 5, 33, 40, 44]. These approaches forces the LLM to handle heavily compressed vision information, overlooking the fact that the LLM, with its enormous model capacity, has the potential to compress the vision tokens by itself within the transformer layer. In this paper, we focus on optimizing computation efficiency of MLLMs within the transformer decoder layers. We draw inspiration from the Mixture-of-Depths (MoD) [35] mechanism, originally developed for LLMs. Specifically, we propose to build efficient MLLMs with MoD mechanism by selecting the most important and informative vision tokens to be processed by each transformer layer, while skipping redundant ones to improve efficiency. However, integrating MoD mechanism into MLLMs is nontrivial and entails several significant challenges. First, different from training an MoD-based LLM from scratch, integrating MoD mechanism into pre-trained vanilla LLM during multimodal training poses substantial risk of disrupting the language abilities of the original LLM. To address this, we design tanh-gated weight normalization (TanhNorm) , which not only ensures proper initialization of the newly added MoD module, but also enhances training stability and performance. It also mitigates numerical stability issues during inference. Second, MLLMs are trained on multimodal data [24] that are several orders of magnitude smaller in scale compared to the text data used for training MoD-based LLMs [35]. We enhance the MoD mechanism with symmetric token reweighting(STRing) module which fully leverage the language supervision signals during training, enabling MoD modules to learn to accurately assess token importance even with limited training data. Thanks to these two enhancements, our upgraded MoD layers can be seamlessly applied to MLLMs. However, we argue that setting fixed ratio of retained tokens across different MoD layers [35] is suboptimal design choice under multimodal scenario, as the degree of redundancy in vision tokens should vary across layers. We conducted series of exploratory experiments by adjusting the ratio of different layers in our MoD-based MLLM, which demonstrate that vision tokens exhibit higher redundancy in deeper layers. Accordingly, we propose progressive ratio decay (PRD) strategy, which follows shifted cosine schedule to gradually reduce the token retention ratio layer by layer. Trained with this strategy, our model significantly outperforms models that use constant retention ratio across all layers under the same computation budget. Building upon the above innovations, we present our model, p-MoD, Mixture-of-Depth MLLMs equipped with our progressive ratio decay strategy and upgraded MoD layers (i.e. p-MoD layers). Extensive experiments validate the effectiveness of our proposed model. As shown in Figure 1, across 14 benchmarks spanning various domains, our model matches or even outperforms the strong LLaVANeXT [24] baseline, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training. 2. Related Work Building Efficient LLMs. Considerable efforts have been made to build efficient LLMs. One representative approach is the Mixture of Experts (MoE) mechanism [6, 10, 16, 18, 36, 48], where router directs each token to different MLP experts of the transformer block. MoE models achieve faster training and inference speeds compared to dense models of the same size, but at the cost of lower performance. The more recently proposed Mixture of Depths (MoD) [35] module assigns weights to tokens and processes only portion of highest-weighted tokens, skipping lower-weighted tokens to save computation. MoD-based LLMs demonstrate strong performance under limited compute budgets. In this paper, we propose to build more efficient MLLMs by upgrading MoD with several innovative improvements. Building Efficient MLLMs. The main computational burden in MLLMs arises from the large number of visual tokens that the transformer decoder must process. Previous works mainly focused on compressing visual tokens before they enter the decoder via convolution [5, 44], pooling [33, 40], or attention-based [2] (e.g. QFormer [21]) modules. However, reducing vision-related computation within the transformer decoder layers is less explored. LLaVolta [3] speeds up MLLM training by performing average pooling operation in intermediate transformer decoder layers. FastV [4] and some subsequent works [12, 14, 45] improve MLLMs efficiency by dropping vision tokens in intermediate transformer layers based on the attention scores they received from text tokens. However, these methods require the acquisition of the attention score matrix, making them incompatible with widely used efficient attention implementations like FlashAttention [8, 9] which significantly improves speed and memory efficiency during training and inference. Moreover, these methods neglect the fact that to2 Figure 2. Overview of p-MoD. In the left, we present the detailed architecture of our p-MoD layers. Given the input tokens, the weight predictor first assigns weights to each token. Then, subset of tokens with highest weights are selected according to pre-defined token retention ratio and processed by the transformer layer, while the rest of the tokens are skipped. The weights are normalized by TanhNorm module, and then both the selected and skipped tokens are symmetrically scaled by their corresponding weights in our STRing module. In the right, we demonstrate our crucial design, the progressive ratio decay(PRD) strategy, which gradually reduces the token retention ratio layer by layer, following shifted cos schedule. kens discarded in early layers due to low attention scores might be crucial in deeper layers, potentially leading to degradation in performance. In contrast, we utilize MoD module to perform layerwise vision token selection without relying on text, which not only enjoys the efficiency of FlashAttention, but also incurs negligible performance loss. We compare our method with these attention-score-based pruning methods in Section in the supplementary material. 3. Method In this section, we introduce our p-MoD model. As illustrated in Figure 2, our model consists of p-MoD layers which upgrades MoD architecture with tanh-gated weight normalization(TanhNorm) and symmetric token reweighting(STRing). Our crucial design is the progressive ratio decay(PRD) strategy which controls the token retention ratio across different layers. In the following sections, we first revisit MoD briefly and then explain each component of our p-MoD model in detail. 3.1. Revisiting Mixture-of-Depths The MoD layer consists of weight predictor and vanilla transformer layer. It predicts an importance score for each input token using the weight predictor and then selects the top R% most important tokens to be processed by the transformer layer. Formally, given an MoD layer with token retention ratio R, transformer layer , and linear weight predictor, the input tokens Rnd is first passed through the linear predictor to generate set of weights: = Linear(X) Rn (1) with denoting the sequence length and denoting the embedding dimension. Then, we compute the R-th percentile of the router weights, denoted as PR(w). Tokens with weights larger than PR(w) will be selected to be processed by the transformer layer while other tokens are skipped in this layer, which guarantees only R% tokens are processed. The calculation process in transformer layer with MoD can be formulated as: (cid:40) = wiT (Xi) + Xi, Xi, if wi > PR(w) if wi PR(w) (2) Notably, after being processed by the transformer layer, the selected tokens are then scaled by their corresponding weights. In this way, the weight predictor is engaged into the gradient path, enabling its parameters to be updated by backpropagation. We term this process token reweighting. In the original MoD module designed for LLMs [35], represents text tokens. In our multimodal scenario, represents vision tokens. We only apply MoD on vision tokens as they occupy the main computation load and exhibit high redundancy in deep layers. 3 3.2. Adapting Mixture-of-Depths Module 3.2.2. Symmetric Token Reweighting Different from training MoD-based LLMs from scratch, integrating MoD into MLLMs presents several challenges. The insertion of new MoD modules into pre-trained LLMs can lead to instability during training and inference. The relatively small amount of multimodal data may not be sufficient to train the MoD modules. In this section, we introduce two enhancements to the MoD module to address these challenges. 3.2.1. Tanh-gated Weight Normalization The original MoD mechanism [35] is designed for training MoD-based LLMs from scratch. In our multimodal training scenario, we need to insert new MoD modules into pre-trained vanilla LLM. One common approach is alternate training [46], where only the newly inserted modules are trained in the first stage, and all modules are simultaneously trained in the second stage. However, this approach is not suitable for MoD, as the MoD modules scale the tokens by their weights during the token reweighting process, and the subsequent frozen LLM layers are unable to handle scaled tokens. Our experiments show consistent results that training the MoD modules while freezing the LLM layers fails to converge. Consequently, the only feasible method is to directly address the challenge: inserting the MoD modules into the LLM and training both of them simultaneously. To address this challenge, we design tanh-gated weight normalization (TanhNorm), which employs normalization function (w) = α tanh(w) to normalize the predicted weights before token reweighting. After applying TanhNorm to MoD, Equation 2 can be reformulated as: = (cid:40) α tanh(wi)T (Xi) + Xi, Xi, if wi > PR(w) if wi PR(w) (3) Here α is hyper-parameter which controls the range of the normalized weights. Although simple, our TanhNorm ensures: (1) the normalized weight distribution is zerocentered, (2) the range(variance) of the weight distribution can be easily controlled by adjusting the gating factor α. These properties offer the following benefits: The normalized weights are closely around zero at the start of training, which ensures proper training initialization by keeping the LLM intact after inserting MoD modules. The zero-centered weight distribution reduced the risk that some tokens are repetitively scaled by positive(or negative) weights in all MoD layers. This ensures training stability and mitigates numerical stability issues during inference. Both of the above benefits contribute to improved model performance. We validate the effectiveness of TanhNorm in Section 4.4. Compared to the text data used for training MoD-based LLMs, MLLMs are trained on multimodal data [24] that are several orders of magnitude smaller in scale. It is challenging to train weight predictor (the only learnable part in the MoD module) to accurately and robustly assess the importance of vision tokens and assign corresponding weights. As stated in Section 3.1, the gradient of the weight predictor module stems from the token reweighting process. The original MoD performs token reweighting only on selected tokens as in Equation 2. In this way, the language supervision signals only supervise the weight predictor to assign high weights to the selected tokens, while the process of predicting weights for the skipped tokens is not supervised (i.e. no gradient). To fully leverage the limited training data, we enhance the MoD mechanism by symmetrically applying the token reweighting process to both selected and skipped tokens. With our symmetric token reweighting (STRing) module, Equation 3 can be further modified as: = (cid:40) α tanh(wi)T (Xi) + Xi, α tanh(wi)Xi + Xi, if wi > PR(w) if wi PR(w) (4) In this way, the MoD module can fully leverage the language supervision signals during training and learn to accurately assess token importance with limited training data. 3.3. Progressive Ratio Decay After upgrading MoD with TanhNorm and STRing modules, we are able to successfully train MoD-based MLLMs. However, the performance-efficiency trade-off exhibited by the model is far from satisfactory. When the token retention ratio is set below 70%, the models performance deteriorates sharply. Original MoD-based LLMs [35] adopt fixed token retention ratio across all MoD layers. This strategy assumes that the tokens have the same degree of redundancy in different layers. We argue that this assumption does not hold in the multimodal scenario. Under the effect of self-attention in every transformer layer, vision tokens gradually aggregate information from each other and text tokens gather relevant information from vision tokens. Therefore, vision tokens are expected to become increasingly redundant in deeper layers. To validate our hypothesis, we conduct series of exploratory experiments on our MoD-based MLLM trained with token retention ratio of 70% across all layers. As shown in Figure 3a, we first divide the layers into several groups. In each experiment, we apply lower token retention ratio (15%) only to one group of layers while keeping the other layers unchanged, and evaluate the model under this customized inference setting on 14 benchmarks. The 4 we constrain the range of the token retention ratio Rl within predefined maximum and minimum threshold. If Rl exceeds the maximum threshold, we set the token retention ratio to 1 so that the MoD module is not applied to the lth layer. If Rl falls below the minimum threshold, the token retention ratio for the l-th layer is set to the minimum threshold: = 1, Rl, min, if Rl max if min < Rl < max if Rl min (6) Experiments in Section 4.4 demonstrates that our p-MoD model with PRD strategy significantly outperforms models that use constant retention ratio under the same computation budget. It also outperforms other kinds of ratio schedulers. 4. Experiment 4.1. Setups Models. To evaluate the effectiveness of p-MoD, we select two representative open-source MLLMs: LLaVA-1.5 [23] and LLaVA-NeXT [24], as the baseline models in our experiments. We name our corresponding p-MoD models p-MoD-LLaVA-1.5 and p-MoD-LLaVA-NeXT. LLaVA-1.5 resizes input images to the fixed resolution which aligns with the training setup of its CLIP [34] vision encoder, encoding an image into 576 tokens. LLaVA-NeXT divides high-resolution images into multiple slices which are independently processed by the vision encoder. This strategy enhances the visual perception capabilities, but also leads to significantly larger number of vision tokens (up to 2880) and much higher computation costs. Training Data. We follow the training setups of our baseline methods. Both LLaVA-1.5 and LLaVA-NeXT utilize LCS-558K [25] as pre-training data. During the instruction tuning stage, LLaVA-1.5 utilizes LLaVA-mix665k[23]. The original LLaVA-NeXT model utilizes 760k data for fine-tuning, however, the LLaVA-NeXT training data released by LLaVA team contains 779k data. For fair comparison, we train LLaVA-NeXT model on the released 779k data with the official LLaVA-NeXT code base. Benchmarks. We conduct comprehensive experiments across 14 benchmarks: GQA [15], OK-VQA [29], AI2D [17] and ScienceQA-IMG [28] are traditional visual question answering benchmarks; DocVQA [31], TextVQA [37], ChartQA [30] and InfographicVQA [32] focus on fine-grained visual question answering; POPE [22] evaluates hallucination in MLLMs; SEED-Bench (Image) [19], RealWorldQA, MME[11], MMBench [26] and MMMU [42] are comprehensive benchmarks tailored for MLLMs. To ensure our results can be conveniently reproduced, we evaluate our model on all these benchmarks (a) Illustration of our exploratory experiments on our MoD-based MLLM trained with token retention ratio of 70% across all layers. Each subfigure shows the inference token retention ratio across all layers of one experiment. Experiment is the baseline setting (70% ratio). In experiments b, and d, we apply lower token retention ratio (15%) to shallow, middle and deep layers. (b) Average performance of our four experiments on 14 benchmarks listed in Section 4.1. The performance drop becomes less significant as the low token retention ratio is applied to deeper layers, which indicates that vision tokens exhibit higher redundancy in deeper layers. Figure 3. Exploratory experiments on vision token redundancy. the results in Figure 3b strongly support our hypothesis: deeper the low token retention ratio is applied, the better the performance. This suggests that vision tokens exhibit higher redundancy in deeper layers, which is consistent with observations in previous works [4, 45]. Accordingly, we design progressive ratio decay (PRD) strategy. As shown on the right side of Figure 2, PRD reduces the token retention ratio layer by layer, following shifted cosine schedule. Suppose the model has layers, the token retention ratio for the l-th layer is formulated as: Rl = 1 2 cos πl + β, = 1, 2, ..., (5) Here β is shift factor, which can be used to flexibly control the overall computational cost of the model by vertically shifting the cosine decay curve. In addition, we find it challenging for MoD layers to learn to predict meaningful weights when the token retention ratio is set to an extreme value. To address this issue 5 Model Params Inference TFLOPs Inference KV cache storage SEED RWQA MME MMB POPE GQA AI2D AVG LLaVA-v1.5 p-MoD-LLaVA-v1.5 LLaVA-NeXT p-MoD-LLaVA-NeXT 7B 7B 7B 7B 8.38 4.92(41.3%) 39.46 21.94(44.4%) 100% 53.8% 100% 53.8% 66.2 66.5 68.9 69.0 55.6 55.7 57.6 57.6 1506.8 1482.8 1519.3 1495. 64.1 65.4 67.5 67.3 85.9 85.5 87.2 86.8 61.9 62.2 63.5 63. 55.2 56.2 64.0 65.1 66.3 66.5 69.2 69.1 Table 1. Comparison with baseline models on 7 benchmarks. p-MoD achieves comparable or even better performance compared to baseline models, with significantly improved efficiency. Params stands for the number of model parameters. We also report the Inference TFLOPs and KV cache storage savings to compare the efficiency of p-MoD and the baseline models. Benchmark names are abbreviated. SEED: SEED-Bench (Image); RWQA: RealWorldQA; MME: MME Perception; MMB: MMBench. AVG stands for the average performance in percentage across all benchmarks. MME Perception score is divided by 2000 to compute the average. We set the gating factor α in TanhNorm to 0.2, and the shift factor β in PRD to 0.5. Model Params DocVQA ChartQA TextVQA InfoVQA OK-VQA MMMU SQA AVG LLaVA-v1.5 p-MoD-LLaVA-v1.5 LLaVA-NeXT p-MoD-LLaVA-NeXT 7B 7B 7B 7B 28.1 27. 70.1 70.0 18.2 16.8 61.6 61.8 46.0 44.8 62.7 60.5 25.8 26. 34.7 34.1 53.4 56.0 54.0 55.1 36.6 36.3 35.3 36.0 69.7 69. 69.7 71.0 39.7 39.7 55.5 55.5 Table 2. Comparison with baseline models on other 7 benchmarks. Our p-MoD models matches the performance of corresponding baseline models. SQA stands for ScienceQA-IMG. We set the gating factor α in TanhNorm to 0.2, and the shift factor β in PRD to 0.5. Effiency Benchmark Model Training GPU hours Inference TFLOPs Inference Latency (ms) Inference KV cache storage Chart QA Info VQA GQA RWQA AI2D MMMU AVG LLaVA-NeXT p-MoD-0.3 p-MoD-0.4 p-MoD-0.5 560 403 (28.0%) 415 (25.9%) 435 (22.3%) 39.46 17.78 (54.9%) 19.56 (50.4%) 21.94 (44.4%) 5191 3267 (37.1%) 3471 (33.1%) 3680 (29.1%) 100% 42.3% 47.5% 53.8% 61.6 57.8 59.6 61.8 34.7 32.1 33.0 34.1 63.5 63.1 63.8 63.3 57.6 55.9 57.3 57.6 64.0 65.0 65.0 65. 35.3 35.0 35.1 36.0 52.8 51.5 52.3 53.0 Table 3. The Efficiency-Performance trade-off experiments. To showcase the versatility of our approach, we conducted experiments with β = 0.3, 0.4, 0.5, respectively. Inference latency is measured on TextVQA dataset. with the lmms-eval [43] evaluation framework developed and maintained by LLaVA team. Evaluation Metrics. We report the performance on all benchmarks, together with variety of metrics that reflect training and inference efficiency. We report TFLOPs which measures the inference computation complexity, and the KV cache storage which is the main memory bottleneck during inference. Besides, we also report the GPU hours during model training, and the inference latency. Implementation Details. We train all our models on 8 NVIDIA RTX A6000 GPUs. The inference efficiency metrics reported in Section 4.3 are measured on single A6000 GPU. By default, we set the gating factor α in TanhNorm to 0.2, and the shift factor β in PRD to 0.5. 4.2. Main Results We integrate p-MoD with LLaVA-1.5 and LLaVA-NeXT baseline models and conduct comprehensive evaluation across 14 benchmarks, as shown in table 1 and table 2. Both p-MoD-LLaVA-v1.5 and p-MoD-LLaVA-NeXT achieve comparable or even better performance across all benchmarks compared to their baseline models, with significant savings in inference TFLOPs and inference KV cache savings. Table 1 consists of general VQA and MLLM benchmarks, which measures comprehensive abilities of MLLMs across various aspects. Our model p-MoD-LLaVA-NeXT model achieves comparable performance while significantly reducing inference TFLOPs and KV cache storage. Our p-MoD-LLaVA-v1.5 model even outperforms the LLaVA-1.5 baseline model, with only 58.7% TFLOPs and 53.8% KV cache storage. In table 2, it is noteworthy that text-rich and graphbased benchmarks like DocVQA, ChartQA, TextVQA, and InfoVQA require fine-grained visual perception and reasoning abilities. Models can only given correct answer when they successfully identify answer-related regions that occupy only small portions of the entire image. Remarkably, p-MoD-LLaVA-NeXT achieves negligible performance drop on DocVQA and InfoVQA, and even gains 6 Norm Type - Softmax Shifted softmax TanhNorm(α = 1) TanhNorm(α = 0.2) TanhNorm(α = 0.2) STRing Doc VQA Text VQA SEED RWQA AI2D MMB AVG 63.9 62.5 57.9 57.3 OVERFLOW 54.6 68.2 58.3 68.3 OVERFLOW 65.1 65. 67.0 66.4 69.0 70.0 60.7 60.5 68. 69.0 56.6 57.6 65.3 65.1 67. 67.3 - 62.8 63.1 - 64.6 64.9 Table 4. Ablation on tanh-gated weight normalization and symmetric token reweighting. Our default model is marked in gray. The rows with no performance reported indicates that the corresponding experiment faces overflow issue during training or inference. 0.2% accuracy on ChartQA with substantial improvements in time and memory efficiency. The above results indicate that our approach substantially improves efficiency while maintaining performance. 4.3. Efficiency-Performance Trade-off Thanks to the shifted cosine schedule used by PRD in Section 3.3 to control the token retention ratio across all layers, we can change the shift factor β to control the overall computational cost of the model. In this way, we can train and deploy p-MoD models under different performanceefficiency trade-offs based on our actual needs. To showcase the versatility of our approach, we conducted experiments with β = 0.3, 0.4, 0.5, respectively. The results are presented in Table 3. Our default p-MoD model with β = 0.5 reduced training GPU hours by 22.3%, inference TFLOPs by 44.4%, inference latency by 29.1%, and KV cache storage by 46.2%. Using lower β can further improve efficiency, with an acceptable performance drop. In conclusion, Table 3 demonstrates trend that as the computation cost is reduced, p-MoDs performance drops linearly with shallow slope, offering superior performance-efficiency trade-off. 4.4. Ablation Study Tanh-gated Weight Normalization. To validate the effectiveness of TanhNorm, we explore the effectiveness of different weight normalization methods in Table 4. First, we verify that using no weight normalization module will result in overflow during training, as shown in the first row. Then, we validate the effectiveness of the two important properties of TanhNorm stated in Section 3.2.1: (1) the normalized weight distribution is zero-centered, (2) the range(variance) of the weight distribution can be easily controlled by adjusting the gating factor α. For the first property, we design two experiments with the Softmax function, standard normalization funcSpecifically, we expertion that is not zero-centered. iment with the vanilla softmax function (w) = α Sof tmax(w), α = 0.2, and shifted softmax function (w) = α Sof tmax(w) + b, α = 0.4, = 0.2 which has the same range as our TanhNorm function (w) = α tanh(w), α = 0.2. Comparing the last row of Table 4 with the second and third rows, we verify that TanhNorm significantly outperforms the vanilla softmax function and the shifted softIt proves that being zero-centered is key max function. to TanhNorms strong performance. The shifted softmax function, despite having the same range as TanhNorm, results in worse performance due to the lack of zero-centered property. For the second property, we experiment with naive choice of the gating factor α by setting it to 1, which can be interpreted as no gating is applied. As shown in the fourth row of Table 4, this experiment results in the overflow problem during inference. This validates the importance of controlling the gating factor α to guarantee training and inference stability. Symmetric Token Reweighting. By comparing the last two rows in Table 4, we can confirm that employing the symmetric token reweighting(STRing) strategy leads to 1% improvement on both DocVQA and RWQA, and the It is verified average performance is enhanced by 0.3%. that STRing successfully improves performance by more effectively leveraging language supervision signals to train superior weight predictor with limited training data. Progressive Ratio Decay. Based on the exploratory experiments in Section 3.3, we conclude that vision tokens exhibit higher redundancy in deeper layers, and design the progressive ratio decay strategy with shifted cosine schedule. To further validate this conclusion and the effectiveness of PRD, we experiment with four other schedule functions which control the token retention ratio for each decoder layer. As illustrated in Figure 4, the functions include: (1) constant function that uses the same ratio for all layers, (2) an interleaved function which interleaves vanilla transformer layer and an MoD layer with low ratio, which is adopted in the original MoD paper [35], (3) stepped decay function, (4) linear decay function. To ensure fair comparison, we fix the average retention ratios across all layers at approximately 54%. Based on the results demonstrated in Table 5, we can draw the following conclusions: The non-decaying functions (i.e. constant and interleaved) significantly underperform the decaying functions. This result strongly supports our conclusion that vision tokens exhibit higher redundancy in deeper layers. The continuous decay functions (i.e. cosine and linear) notably outperforms the discontinuous stepped decay function, and the cosine function achieves the best results. This proves the effectiveness of our shifted cosine sched7 Ratio Schedule Constant Interleaved Stepped Linear Cosine Decay Progressive Doc Text VQA VQA 58.0 60.3 67.2 69.3 70.0 56. 57.0 59.4 60.5 60.5 SEED RWQA AI2D MMB AVG 64. 65.3 68.4 68.0 69.0 53.1 56. 57.4 57.5 57.6 63.8 63.4 64. 65.5 65.1 62.8 62.1 66.1 66. 67.3 59.7 60.7 63.9 64.6 64. Table 5. Ablation on different token retention ratio schedules. Our default model is marked in gray. Figure 4. Illustration of different ratio schedule functions in Table 5. Figure 5. Visualization of tokens selected by different p-MoD layers. The selected tokens are colored in red. We observe that vision tokens corresponding to regions with rich semantic information are selected across different p-MoD layers, even when the number of selected tokens becomes minimal in deeper layers. ule. 4.5. Visualization Figure 5 depicts the tokens selected by p-MoD-LLaVANeXT at different layers. Guided by the progressive ratio decay strategy, the number of tokens processed by the MoD layers gradually decreases layer by layer. It can be observed that the selected vision tokens progressively concentrate on key regions in the image with rich semantic information. Specifically, in the figure above, visual tokens corresponding to text and drawings in different positions are nearly all selected at the 26th layer with low ratio. In the figure below, selected tokens gradually converge towards measurement markings on ruler and the other object. These results indicate that our model effectively compresses visual information by keeping the informative tokens, enabling it to maintain performance while significantly reducing training and inference costs. 5. Conclusion and Future Work In this paper, we have explored building efficient MLLMs with Mixture-of-Depths mechanism. Our proposed model, termed p-MoD, features three key designs. First, tanh-gated weight normalization mechanism is applied to the predicted token weights to ensure proper training initialization and stability during training and inference. Second, symmetric token reweighting strategy is utilized to fully leverage the language supervision signals to train superior weight prediction module with limited training data. Finally, our model employs progressive ratio decay strategy to grad8 ually reduce the token retention ratio layer by layer. Our p-MoD model achieves comparable or even superior results to the baseline models on diverse set of 14 benchmarks, with substantial improvements on training and inference efficiency. We hope p-MoD can serve as strong and efficient baseline for future research on developing efficient MLLMs. One limitation of our work is that p-MoD is only experimented on LLaVA-1.5 and LLaVA-NeXT models, which focus on single-image understanding tasks. We believe that p-MoD has the potential to achieve more remarkable results when applied on tasks that handle larger number of vision tokens, such as high-resolution, multi-image, and long video understanding. We leave the exploration of p-MoD on other vision tasks to future research."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amelie Heliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 1 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [3] Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, and Alan Yuille. Llavolta: Efficient multi-modal models via stage-wise visual context compression. arXiv preprint arXiv:2406.20092, 2024. 2 [4] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2025. 2, 5, 12 [5] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 1, 2 [6] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. 2 [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 1 [8] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 2, 12 [9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact at9 tention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 2, 12 [10] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2024. 5 [12] Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipvl: Efficient large vision-language models with dynamic token arXiv preprint sparsification and kv cache compression. arXiv:2410.08584, 2024. 2, 12 [13] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 1 [14] Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. Mini-monkey: Alleviating the semantic sawtooth effect for lightweight mllms via complementary image pyramid. arXiv preprint arXiv:2408.02034, 2024. 2, 12 [15] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 5 [16] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [17] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is In Computer VisionECCV 2016: worth dozen images. 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 5 [18] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 2 [19] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 5 [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. sion. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 5 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 1, 5, 13 [24] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1, 2, 4, 5, 13 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 5, [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European Conference on Computer all-around player? Vision, pages 216233. Springer, 2025. 5 [27] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1 [28] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 5 [29] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 5 [30] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 5 [31] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [32] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 5 [33] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 2 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- [35] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. 2, 3, 4, 7 [36] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538, 2017. 2 [37] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 5 [38] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [39] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1 [40] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985, 2024. 2 [41] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding arXiv preprint in multi-modal large language models. arXiv:2408.04840, 2024. 1 [42] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 5 [43] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 6 [44] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 1, [45] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. 2, 5, 12 10 [46] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 4 [47] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [48] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Stmoe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 2 Model Infer. TFLOPs Infer. Latency(ms) Infer. Mem.(G) Doc VQA Chart QA Text VQA Info VQA OK VQA RWQA SEED POPE MMMU SQA AVG LLaVA-NeXT FastV-LLaVA-NeXT p-MoD-LLaVA-NeXT 39.46 22.73 21. 5191 3971 3680 17.78 21.73 16.34 70.1 65.9 70.0 61.6 58.2 61.8 62.7 62.2 60.5 34.7 33.0 34. 54.0 53.1 55.1 57.6 55.6 57.6 68.9 68.2 69.0 87.2 84.5 86.8 35.3 35.6 36.0 69.7 70.0 71. 60.2 58.6 60.2 Table 6. Comparison with FastV. FastV results in significant performance degradation and consumes more GPU memory than the baseline model. In contrast, p-MoD matches the performance of the baseline model, with the lowest TFLOPs, latency and memory usage during inference."
        },
        {
            "title": "Appendix",
            "content": "This supplementary material includes the following sections: In Section A, we compare p-MoD against pruning methods that drop vision tokens using attention scores received from text tokens. In Section B, we provide visualization results of p-MoD token selection decisions across different layers. In Section C, we provide more implementation details. A. Comparison with Attention-Score-Based"
        },
        {
            "title": "Pruning Methods",
            "content": "In this section, we compare p-MoD against pruning methods that drop vision tokens in intermediate transformer layers according to the attention scores they received from text tokens [4, 12, 14, 45]. As mentioned in Section 2, these methods are the closest to our work. First, we provide detailed analysis on their limitations. Then, we conduct experiments to support our analysis. A.1. Limitations of Attention-Score-Based Pruning"
        },
        {
            "title": "Methods",
            "content": "Methods that use attention scores to drop vision tokens have the following limitations: Incompatible with Efficient Attention Implementations. These methods require the attention module to output the attention score matrix to perform token dropping. However, this is incompatible with efficient attention implementations like FlashAttention [8, 9] which uses tiling to avoid materialization of the large attention score matrix on GPU HBM. Consequently, these methods can only utilize the inefficient vanilla attention implementation and fail to achieve the theoretically expected memory and time efficiency, despite the reduction in vision tokens. In contrast, our method imposes no restrictions on the implementation of attention mechanism and is therefore compatible with any attention implementation. Dropping Tokens is Inferior to Selecting Tokens. Attention-score-based pruning methods drop vision tokens in certain transformer layers, which means the dropped tokens are not available in later layers. Intuitively, this strategy hurts performance in scenarios where some tokens with low attention scores in earlier layers are dropped, but they are crucial (i.e. should have received high attention scores) in later layers. Alternatively, our pMoD model selects important tokens to process from the complete token set in every layer, mitigating the potential harm caused by token dropping. A.2. Experiments We conduct experiments on p-MoD and FastV [4], the most representative attention-score-based pruning method. FastV drops 50% vision tokens with the lowest attention scores from the generated text tokens, after the second transformer decoder layer. This setting ensures approximately the same TFLOPs as p-MoD. We choose LLaVANeXT as baseline model, and enable KV cache during generation for fair comparison. With KV cache enabled, FastV drops vision tokens according to the attention scores from the first generated text token. The results are illustrated in Table 6. Since FastV is incompatible with FlashAttention, FastV-LLaVA-NeXT model can only utilize the vanilla attention implementation, which even consumes more GPU memory than the baseline model. Furthermore, FastVs token dropping strategy results in significant performance degradation. In contrast, our p-MoD-LLaVA-NeXT model matches the performance of the baseline model, with the lowest TFLOPs, latency and memory usage during inference. This highlights the benefits of p-MoDs token selection mechanism combined with its compatibility with efficient attention implementations. B. Visualization of p-MoD Token Selection Decisions Figure 6 visualizes the token selection decisions of p-MoD across different layers. The horizontal axis denotes the token indexes, and the vertical axis denotes the layer indexes. It can be observed that every layer select different tokens to process, and every token is selected by different p-MoD layers. This demonstrates that every p-MoD layer independently selects important and informative tokens, without degrading into selecting same set of tokens across different layers, which is identical to dropping tokens instead of selecting. 12 Figure 6. Visualization of token selection decisions across different p-MoD layers. The horizontal axis denotes the token indexes, and the vertical axis denotes the layer indexes. It can be observed that every p-MoD layer independently selects important and informative tokens."
        },
        {
            "title": "Resolution",
            "content": "p-MoD-LLaVA-v1.5 (336,336)"
        },
        {
            "title": "Train\nStage",
            "content": "PT Trainable Module& Learning Rate Connector: 1e-"
        },
        {
            "title": "Data",
            "content": "558K"
        },
        {
            "title": "SFT",
            "content": "Connector+LLM+MoD: 2e-5 665K p-MoD-LLaVA-NeXT 336 [(2,2), (1,2), (2,1), (1,3), (3,1)] PT"
        },
        {
            "title": "SFT",
            "content": "Connector: 1e-3 558K ViT: 2e-6 (baseline) Connector+LLM+MoD: 2e-5 779K"
        },
        {
            "title": "Batch\nSize",
            "content": "256 128 256 Table 7. Detailed training configuration. PT stands for pre-training. SFT stands for supervised fine-tuning. During fine-tuning, the vision encoder of the baseline LLaVA-NeXT model is updated, consistent with the original LLaVA-NeXT model[24]. We freeze the vision encoder of our p-MoD-LLaVA-NeXT model for all experiments to reduce training cost. C. More Implementation Details C.1. Detailed Training Configurations In this section, we provide our detailed training configurations, as shown in Table 7. LLaVA-1.5 employs fixed input resolution of 336336, while LLaVA-NeXT supports pre-defined set of different resolutions (up to 672672). Both the LLaVA-v1.5 models and the LLaVA-NeXT models go through the same pre-training stage, where the MLP connector module is trained on 558K image caption data [25] with learning rate of 1e-3 and batch size of 256. During supervised fine-tuning, LLaVA-1.5 is trained on 665K instruction-tuning data [23], while LLaVA-NeXT is trained on 779K data1. The vision encoder of the LLaVANeXT baseline model is updated for fine-tuning. For our p-MoD-LLaVA-NeXT model, we freeze the vision encoder to save training time, as the available GPU resources are limited. When measuring the training GPU hours reported 1https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Data 13 in Table 3, we freeze the vision encoder for both baseline and p-MoD models to ensure fair comparison."
        }
    ],
    "affiliations": [
        "China Mobile (Suzhou) Software Technology Co., Ltd.",
        "Shanghai AI Lab",
        "State Key Laboratory for Novel Software Technology, Nanjing University"
    ]
}