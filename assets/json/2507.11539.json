{
    "paper_title": "Streaming 4D Visual Geometry Transformer",
    "authors": [
        "Dong Zhuo",
        "Wenzhao Zheng",
        "Jiahe Guo",
        "Yuqi Wu",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 9 3 5 1 1 . 7 0 5 2 : r Streaming 4D Visual Geometry Transformer Dong Zhuo Wenzhao Zheng,"
        },
        {
            "title": "Jiwen Lu",
            "content": "Tsinghua University https://wzzheng.net/StreamVGGT/ Figure 1: Overview. Unlike offline models that require reprocessing the entire sequence and reconstructing the entire scene upon receiving each new image, our StreamVGGT employs temporal causal attention and leverages cached token memory to support efficient incremental on-the-fly reconstruction, enabling interative and real-time online applitions."
        },
        {
            "title": "Abstract",
            "content": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is fundamental yet challenging computer vision task. To facilitate interactive and realtime applications, we propose streaming 4D visual geometry transformer that shares similar philosophy with autoregressive large language models. We explore simple and efficient design and employ causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle realtime 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT. Equal contributions. Project leader. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "4D geometry reconstruction has long been fundamental task in computer vision [18, 33, 35], which aims to estimate 3D geometry from set of dynamic images. As bridge between 2D images and the 3D world, it finds broad applications in diverse fields including autonomous driving [20, 22], AR/VR [19, 67], and embodied robots [55, 60]. With the development of embodied intelligence, on-the-fly 4D reconstruction from streaming inputs is increasingly demanded to enable online interactive visual systems, where latency and temporal consistency are crucial. Figure 2: Inference time comparison for the curframe of varying sequence lengths between rent StreamVGGT and VGGT for the online setting. Conventional 3D reconstruction methods like Structure-from-Motion (SfM) [1, 10, 28, 41, 46, 57] and Multi-View Stereo (MVS) [13, 16] rely on explicit geometric constraints [12, 14] or global optimization [11, 31, 56, 64], limiting scalability and speed. Recent learning-based approaches have shifted toward end-to-end frameworks that directly predict 3D structure from multi-view images. While pair-wise methods [25, 54] have shown promising results by learning dense correspondences between image pairs, they utilize time-consuming post-processing steps for global alignment during multi-view reconstruction. Memory-augmented methods [49, 53, 59] maintain memory pool to eliminate the need for post-processing. Additionally, their recursive memory update designs have the ability to address 4D reconstruction from videos. Nevertheless, error accumulation caused by causal architectures remains to be tackled. Fast3R [62] and VGGT [50] circumvent iterative alignment by employing transformer-based feed-forward architectures that enable global dense-view interactions. Despite achieving satisfactory performance, their dependence on global self-attention necessitates reprocessing the entire sequence at every step for frame-by-frame input and output scenarios. This offline paradigm not only precludes incremental reconstruction but also diverges from the causal nature of human perception, thereby limiting its practicality in streaming applications. In this paper, we propose StreamVGGT, causal transformer architecture specifically designed for efficient, real-time streaming 4D visual geometry reconstruction, as shown in Figure 1. Unlike conventional offline frameworks that necessitate reprocessing the entire sequence with the arrival of each new frame, we introduce temporal causal attention mechanism combined with an implicit historical-token memory module. This allows incremental processing of video frames, enabling progressive scene updates in an online streaming manner. StreamVGGT leverages the inherent sequential and causal nature of real-world video data, constraining the attention mechanism to past and current frames, thereby aligning with the causal structure observed in human perception. Furthermore, to address the common challenge of long-term error accumulation inherent in causal models, we introduce distillation-based training strategy. This strategy utilizes knowledge distillation from the densely connected, bidirectional visual geometry grounded transformer (VGGT) [50] as the teacher model. By leveraging the global contextual understanding of the teacher model, our causal student model achieves stability and accuracy comparable to full-sequence inference. Equipped with FlashAttention-2 [7], our model supports fast inference for the current frame compared to VGGT, as shown in Figure 2. Experimental results demonstrate that our StreamVGGT significantly reduces inference overhead in long-term sequences with only slight performance trade-off, representing crucial step toward real-time responsive 4D vision systems."
        },
        {
            "title": "2 Related Work",
            "content": "Conventional 3D Reconstruction. 3D reconstruction, fundamental task in computer vision [17, 33, 35], aims to recover the geometric structure of scenes from images or video sequences. Structure-from-Motion (SfM) [1, 10, 28, 41, 46, 57] reconstructs sparse 3D point clouds by matching image features across overlapping views and jointly refining camera poses and scene points through bundle adjustment (BA). typical workflow comprises keypoint detection and description, feature matching with geometric verification, multi-view initialization and incremental camera registration, 2 triangulation, and global bundle adjustment. Although highly accurate in static scenes, SfM is fragile in dynamic or texture-poor environments and, because of its computationally intensive offline optimization, cannot readily update in real time. Multi-View Stereo (MVS) [11, 12, 14, 31, 56, 64] exploits accurate camera poses to enforce photometric consistency across views, producing dense depth maps or point clouds that can be converted into high-resolution meshes or volumetric models. Neural Radiance Fields (NeRF) [30] extend this idea by fitting the volume-rendering equation to learn continuous radiance field, faithfully capturing fine-detail geometry and photometrically consistent appearance. Yet both MVS and NeRF depend on heavy offline optimisation, cannot update incrementally, and therefore provide limited real-time capability. As result, they are best employed as offline dense-reconstruction modules appended to an SfM pipeline rather than as online SLAM. Learning-Based 3D Reconstruction. Building on the foundations of conventional 3D reconstruction, recent end-to-end, learning-based methods utilize neural networks to encode scene priors, markedly improving robustness and cross-dataset generalisation [8, 9, 25, 45, 54, 65]. DUSt3R [54] directly regresses view-consistent 3D point maps from just two RGB images without camera calibration, while its successor MASt3R [25] introduces confidence-weighted losses to approximate metric scale. Recently, the feed-forward transformer-based architectures have emerged to enable dense-view interaction within single pass [50, 62], delivering state-of-the-art accuracy in few seconds. Fast3R [62] extends the pair-wise DUSt3R idea to an N-view transformer equipped with memory-efficient Flash-Attention and parallel view fusion, allowing over 1000 images to be processed during inference. VGGT [50] scales this philosophy to 1.2B parameter visual geometry grounded transformer that jointly predicts camera intrinsics/extrinsics, dense depth, point maps, and 2D tracking features. However, its quadratic token-pair complexity and offline inference regime force complete re-encoding of every frame whenever new image arrives. This heavy memory footprint and non-causal processing preclude streaming 4D reconstruction and undermine real-time applications demanding instantaneous, frame-by-frame scene understanding. Streaming 4D Reconstruction. Real-time streaming 4D reconstruction grows to be indispensable for autonomous driving, robotics, and AR/VR, where systems update scene geometry and camera pose on every frame with low latency [49, 53, 59]. Spann3R [49] augments DUSt3R-style encoder with token-addressable spatial memory, sustaining online point-map fusion but suffering drift on long or dynamic sequences due to its bounded memory. CUT3R [53] introduces recurrent transformer that jointly reads and writes learnable scene state to output camera parameters, dense depth, and novel-view completions in real time. However, it degrades when extrapolating far from observed regions, and demands heavy computation for recursive training. Point3R [59] couples an explicit geometry-aligned spatial pointer memory with 3D hierarchical RoPE and an adaptive fusion mechanism, delivering low-drift online pose, depth, and point-map updates in real time. Instead of designing the memory mechanism to store past information, we follow the philosophy of large language models and employ causal transformer to implicitly cache historical visual tokens."
        },
        {
            "title": "3 Proposed Approach",
            "content": "Our causal transformer-based StreamVGGT is designed for streaming inputs without any camera parameters, and it generates the corresponding 4D reconstruction results incrementally. In this section, we first present the formal definition of the 4D geometry reconstruction task in Section 3.1. Subsequently, we describe our proposed architecture in Section 3.2. Finally, we provide the details of our distillation-based training setup in Section 3.3."
        },
        {
            "title": "3.1 Streaming 4D Geometry Reconstruction",
            "content": "Given set of images {It}T pipeline can be written as: t=1, where each frame It R3HW , the generic 4D reconstruction Ft = Encoder(It), Gt = Decoder(Ft), (Pt, Ct) = Head(Gt), (1) where the encoder maps each input frame It to sequence of image tokens Ft RN C. multiview decoder then fuses cross-frame information, producing geometry tokens Gt RN C, and MLP head predicts point map Pt R3HW together with per-pixel confidence map Ct RHW from these geometry tokens. Figure 3: Framework of StreamVGGT. Our model consists of three main components: an image encoder, spatio-temporal decoder, and multi-task prediction heads. During training, we utilize fullsequence inputs to provide the model with complete contextual information. To enforce temporal causality, we apply causal attention so the model can only attend to past frames at any given time step. This design encourages realistic temporal modeling suitable for streaming inference. The formulation in Eq. 1 provides unifying paradigm for 4D reconstruction, but state-of-the-art systems differ in how the decoder aggregates multi-view information. Recent approaches can be grouped into three categories: pairwise, memory-augmented, and global interaction, each introducing specialized modifications to the decoder. We summarize their decoder designs below, followed by our proposed StreamVGGT. For pair-wise approaches like DUSt3R [54] and MASt3R [25], two-branch cross-attention module jointly reasons over reference/target pair, and no persistent state is kept beyond the current pair: {G1, G2} = Decoder(CrossAttn(F1, F2)). (2) For memory-augmented approaches like Spann3R [49] and CUT3R [53], an external memory Mt updated online enables global consistency without post-processing during long sequence inference: Gt, Mt = Decoder(CrossAttn(Ft, Mt1)). (3) For global interaction approaches like Fast3R [62] and VGGT [50], all frames attend to each other through all-to-all self-attention which achieves the highest accuracy at the cost of O(N 2) memory: t=1 = Decoder(Global SelfAttn({Ft}T {Gt}T t=1)). (4) Although global interaction approaches have achieved impressive reconstruction accuracy, their reliance on global self-attention mechanisms inherently limits their ability to process streaming inputs efficiently. To overcome this limitation, we utilize causal transformer architecture to explicitly model the causal structure intrinsic for streaming data. Specifically, for our StreamVGGT: t=1 = Decoder(Temporal SelfAttn({Ft}T The temporal causal attention restricts each frame to attend only to itself and its predecessors, retaining rich context while reducing latency to O(N ), which enables real-time 4D perception. {Gt}T t=1)). (5)"
        },
        {
            "title": "3.2 Causal Architecture with Cached Memory Token",
            "content": "Building on the success of VGGT [50], we propose token-cached causal architecture consisting of three key components: image encoder, spatio-temporal decoder, and multi-task heads, as illustrated in Figure 3. We assume that the input images are provided in sequential order, and all output attributes are predicted frame by frame. We believe that this sequential input-output structure aligns with the causal perception logic observed in humans and is well-suited for real-time 4D visual geometry reconstruction tasks, where spatial consistency and causality play crucial role. Image Encoder. We patchfy each input image It into set of image tokens Ft RN through DINO [34]. During the training stage, the image tokens of all frames are subsequently processed through our causal structure, alternating spatial and temporal attention layers. 4 Figure 4: Efficient inference of our model. During streaming inference, we cache the historical keys and values as implicit memory to store information from past frames. This memory allows the model to efficiently reuse previously computed representations, avoiding redundant computation and enabling consistent contextual understanding across time. Our model then processes input incrementally and achieves performance that is comparable to full-sequence inference. Spatio-Temporal Decoder. We introduce the Spatio-Temporal Decoder by replacing all the global self-attention layers with temporal attention layers. In the standard global self-attention mechanism, each image token Ft attends to all other tokens in the sequence, which can result in high computational costs when handling long sequences and is not well-suited for streaming 4D reconstruction tasks. In contrast, by using temporal attention, each token is restricted to attend only to the current and previous frames in the sequence, thereby respecting the inherent causal structure of the streaming inputs. This modification enables the model to maintain temporal consistency while significantly reducing the computational burden associated with global self-attention. In the training phase, we input all frames simultaneously, and the decoder generates geometry tokens Gt based solely on the context from the historical and current frames. This causal self-attention mechanism lays the foundation for enabling streaming input inference with minimal latency. Cached Memory Token. Unlike the training phase, where all frames are input simultaneously and processed with temporal attention, streaming 4D reconstruction requires the model to handle frame-by-frame input and perform incremental 4D reconstruction during inference. To address this, we introduce an implicit memory mechanism that caches historical token RT C from previously processed frames. During inference, StreamVGGT performs cross attention between the cached memory tokens and the image tokens derived from the current frame as follows: GT = Decoder(CrossAttn(FT , {Mt}T t=1 )), MT = TokenCachedMemory(GT ). (6) This design enables the model to replicate the temporal causal attention behavior observed during training. Through experimental validation, we demonstrate that the model with cached memory token achieves performance comparable to that of full-sequence input inference. This confirms the effectiveness of our approach in maintaining high performance during streaming 4D reconstruction, making it well-suited for real-time visual geometry reconstruction tasks. Multi-Task Heads. Following the VGGT architecture, we utilize three distinct task heads, each dedicated to predicting key 3D attributes of the scene. These heads are responsible for estimating the camera pose gt R9, point maps Pt R3HW , depth maps Dt RHW , and point tracks yt R2M from every single frame. Specifically, for each input image It, the image tokens Ft produced by the image encoder are subsequently fed into the decoder to generate the geometry tokens Gt. The geometry tokens are then passed through specialized task heads that predict the desired 3D attributes. We also leverage learnable camera token to mark the first frame as the global reference, so all subsequent frames are incrementally aligned within this shared coordinate system, enabling consistent, streaming 4D reconstruction without post-processing. Camera Head. This head is responsible for predicting the intrinsic and extrinsic camera parameters. The output consists of the translation vector, rotation quaternion, and field of view (FoV), which together describe the pose of the camera in 3D space. The Camera Head uses self-attention layers to refine the camera parameters for each input frame. Geometry Head. This head generates both the point map and depth map for each frame. Additionally, it outputs confidence maps [24, 32] for both the point and depth predictions, indicating the certainty of the model in various regions. This head also produces dense tracking features, which are passed to the Track Head for point tracking across frames. Specifically, it leverages DPT layer [37] to convert geometry tokens into dense feature maps, which are then processed by convolutional layers to produce the final point and depth maps with their corresponding confidence maps. Track Head. The head operates on the dense tracking features produced by the Geometry Head and is responsible for predicting the 2D point tracks corresponding to 3D points across multiple frames. It also predicts the corresponding confidence for each tracked point. By maintaining consistency across frames, this head enables dynamic scene understanding."
        },
        {
            "title": "3.3 Distillation-Based Training",
            "content": "Knowledge Distillation. In causal models, relying only on historical and current frames for processing may lead to error accumulation, especially in long-term sequences. To address this, we introduce knowledge distillation strategy, where teacher model uses global attention to guide the causal student model. The teacher processes all frames in the sequence to captures richer context, while the student is limited to the current and previous frames. The use of knowledge distillation enables the student model to approximate the teacher to maintain high accuracy with higher efficiency. Training Loss. We follow the loss design from VGGT [50] but introduce key difference: we use the output of teacher model as pseudo-ground truth (pseudo-GT) to supervise our causal student model. This strategy helps mitigate error accumulation, which arises in causal models that only have access to current and historical frames. The loss function is shown as: = Lcamera + Ldepth + Lpmap + λLtrack. (7) The camera loss Lcamera supervises the predicted camera parameters ˆgi by comparing them to the ground-truth camera parameters gi. This is done using the Huber loss function: Lcamera = (cid:88) i=1 ˆgi giϵ, (8) where ϵ represents the Huber loss function, which is robust to outliers in the data. The depth loss Ldepth incorporates depth confidence, which weighs the discrepancy between the predicted depth ˆDi and the ground-truth depth Di with the predicted confidence map ˆΣD . The gradient-based term is applied to further refine the depth estimation, which is commonly used in monocular depth estimation tasks. The final form of the depth loss is: Ldepth = (cid:88) i=1 ˆΣD ( ˆDi Di) + ˆΣD ( ˆDi Di) α log ˆΣD , (9) where denotes the element-wise product, and represents the gradient. The point map loss Lpmap is defined similarly to the depth loss but for the 3D point map. It takes and ensures that the predicted 3D points ˆPi match the into account the point-map confidence ΣP ground-truth points Pi. The loss is formulated as: (cid:88) ( ˆPi Pi) + ΣP ( ˆPi Pi) α log ΣP . Lpmap = ΣP (10) i=1 The tracking loss Ltrack supervises the point tracking predictions across frames. It is computed as the sum of the absolute differences between the predicted and ground-truth 2D coordinates of tracked points. The tracking loss is given by: (cid:88) (cid:88) Ltrack = yj,i ˆyj,i, (11) i=1 where yj,i denotes the ground-truth correspondence in image Ii for the query point yj, and ˆyj,i is the predicted 2D location of the tracked point. Additionally, following the CoTracker2 [23], we apply visibility loss (binary cross-entropy) to predict whether point is visible in given frame. This visibility loss helps improve the robustness of tracking across occlusions. j=1 6 Table 1: Quantitative 3D reconstruction results on 7-Scenes and NRGBD datasets. Acc 7 scenes Comp NC Acc NRGBD Comp NC Method Type Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Pair-wise DUSt3R-GA [54] MASt3R-GA [25] Pair-wise MonST3R-GA [65] Pair-wise 0.146 0.185 0.248 Dense-view 0.088 VGGT [50] Spann3R [49] CUT3R [53] StreamVGGT Streaming Streaming Streaming 0.298 0.126 0.129 0.077 0.081 0.185 0.039 0.226 0.047 0.056 0.181 0.180 0.266 0.091 0.205 0.154 0. 0.067 0.069 0.167 0.039 0.112 0.031 0.041 0.736 0.701 0.672 0.787 0.650 0.727 0.751 0.839 0.792 0.759 0.890 0.730 0.834 0. 0.144 0.085 0.272 0.073 0.416 0.099 0.084 0.019 0.033 0.114 0.018 0.323 0.031 0.044 0.154 0.063 0.287 0.077 0.417 0.076 0. 0.018 0.028 0.110 0.021 0.285 0.026 0.041 0.870 0.794 0.758 0.910 0.684 0.837 0.861 0.982 0.928 0.843 0.990 0.789 0.971 0. Table 2: Quantitative 3D reconstruction results on ETH3D dataset. Overall Methods Comp. Acc. Type DUSt3R [54] MASt3R [25] VGGT [50] CUT3R [53] StreamVGGT Pair-wise Pair-wise Dense-view Streaming Streaming 1.167 0.968 0.928 1.426 0.609 0.842 0.684 0. 1.395 0.545 1.005 0.826 0.686 1.411 0."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Our model architecture follows VGGT [50] with = 24 layers of temporal and spatial attention modules. We integrate FlashAttention-2 [7] to accelerate the inference. We initialize StreamVGGT by using pre-trained weights from VGGT and fine-tune approximately 950 million parameters (excluding the frozen image backbone) for 10 epochs. We employ the AdamW optimizer and hybrid schedule of linear warm-up (first 0.5 epochs) followed by cosine decay, reaching peak learning rate of 1e-6. For each training iteration, we randomly sample batch of 10 frames from diverse training scenes. Following Point3R [59], we process input images with variable aspect ratios while resizing the maximum edge length to 518 pixels. Training is conducted on 4 NVIDIA A800 GPUs."
        },
        {
            "title": "4.2 Training Dataset",
            "content": "Our StreamVGGT is fine-tuned on curated multi-domain collection comprising 13 datasets: Co3Dv2 [38], BlendMVS [63], ARKitScenes [4], MegaDepth [26], WildRGB [61], ScanNet [6], HyperSim [39], OmniObject3D [58], MVS-Synth [21], PointOdyssey [68], Virtual KITTI [5], Spring [29], and Waymo [47]. This compilation encompasses diverse visual domains spanning indoor/outdoor environments and temporal scales, while strategically balancing data sources between synthetic data generation and real-world captures. The hybrid composition ensures robust generalization across geometric complexity, lighting variations, and viewpoint diversity. 4.3 3D Reconstruction Comparisons on the 7-scenes and NRGBD datasets. Following the evaluation protocol of CUT3R [53], we assess the 3D reconstruction performance of StreamVGGT on 7-Scenes [43] and NRGBD [3]. Accuracy (Acc), completeness (Comp), and normal-consistency (NC) scores are reported using sparse inputs (35 frames per scene on 7-Scenes and 24 frames per scene on NRGBD). The quantitative results in Table 1 show that StreamVGGT performs competitively against existing streaming methods and even surpasses the current state-of-the-art streaming model CUT3R [53] on several metrics, offering strong evidence of its effectiveness for streaming 4D reconstruction. Comparisons on the ETH3D dataset. Following VGGT [50], we further evaluate our method on the ETH3D [42] dataset, comparing the accuracy of its predicted point clouds with DUSt3R [54], MASt3R [25], and VGGT [50]. For each scene, we randomly sample 10 frames and report the results after discarding invalid points using the official valid masks. Specifically, we measure accuracy (acc), completeness (comp), and overall quality (Chamfer distance) for 3D reconstruction. Table 2 shows that StreamVGGT surpasses DUSt3R and MASt3R and matches the performance of the current state-of-the-art VGGT, despite relying solely on information from the current and past frames. Our causal design enables streaming 4D reconstruction and is more efficient than VGGT. Table 3: Single-Frame Depth Evaluation. KITTI Sintel Bonn NYU-v2 (Static) Method Type Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ<1.25 Pair-wise DUSt3R [54] MASt3R [25] Pair-wise MonST3R [65] Pair-wise VGGT [50] Dense-view Streaming Spann3R [49] Streaming CUT3R [53] Point3R [59] Streaming StreamVGGT Streaming 0.424 0.340 0.358 0.276 0.470 0.428 0.395 0. 58.7 60.4 54.8 67.5 0.141 0.142 0.076 0.055 82.5 82.0 93.9 97.1 0.112 0.079 0.100 0.072 53.9 55.4 56.8 68.5 0.128 0.092 0.087 0.072 Table 4: Video Depth Evaluation. 0.118 0.063 0.061 0.052 85.9 96.2 95.4 97.1 86.3 94.7 89.3 93.8 84.6 91.3 93.7 94.7 0.080 0.129 0.102 0.060 0.122 0.086 0.079 0. 90.7 84.9 88.0 95.1 84.9 90.9 92.0 95.9 Method Type Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ <1.25 Sintel BONN KITTI Pair-wise DUSt3R-GA [54] MASt3R-GA [25] Pair-wise MonST3R-GA [65] Pair-wise 0.656 0.641 0.378 Dense-view 0.298 VGGT [50] Spann3R [49] CUT3R [53] Point3R [59] StreamVGGT Streaming Streaming Streaming Streaming 0.622 0.421 0.452 0.323 45.2 43.9 55.8 68.1 42.6 47.9 48.9 65.7 0.155 0.252 0.067 0.057 0.144 0.078 0.060 0. 83.3 70.1 96.3 96.8 81.3 93.7 96.0 97.2 0.144 0.183 0.168 0.061 0.198 0.118 0.136 0.173 81.3 74.5 74.4 97.0 73.7 88.1 84.2 72."
        },
        {
            "title": "4.4 Single-Frame and Video Depth Estimation",
            "content": "Single-Frame Depth Estimation. Following MonST3R [65], we conduct evaluations of singleframe depth estimation across four datasets: KITTI [15], Sintel [2], Bonn [36], and NYU-v2 [44], encompassing both dynamic/static scenes and indoor/outdoor environments. To ensure unbiased evaluation of cross-domain generalization, these datasets were strictly excluded during training. Following DUSt3R [54], we employ two principal metrics: Absolute Relative Error (Abs Rel) and the percentage of predictions within 1.25 factor of ground truth depth (δ1.25). Table 3 shows that our method not only matches the overall best performers but also outperforms the current state-ofthe-art streaming model on all datasets, showing its superiority for online depth estimation. Video Depth Estimation. We conduct video depth estimation by assessing both the depth quality on per-frame basis and the consistency of depth across frames. This is done by aligning the predicted depth maps with the ground truth using per-sequence scale. comparison of these methods is shown in Table 4. Under aligned settings, our StreamVGGT exceeds CUT3R on both the Sintel and Bonn benchmarks and attains performance comparable to the offline VGGT."
        },
        {
            "title": "4.5 Camera Pose Estimation",
            "content": "Table 5: Camera Pose Estimation on CO3Dv2. Table 5 shows the camera-pose estimation results on CO3Dv2 [38]. We randomly sample 10 frames per scene and report AUC@30, computed as the area under the accuracy-threshold curve of the minimum between Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA) across varying thresholds. StreamVGGT delivers performance on par with the best existing methods while uniquely supporting camera-pose prediction from streaming inputs, highlighting its practicality for real-time applications."
        },
        {
            "title": "Method",
            "content": "Colmap+SPSG [40] PixSfM [27] PoseDiff [52] DUST3R [54] MASt3R [25] VGG SfM v2 [51] MV-DUST3R [48] CUT3R [53] FLARE [66] Fast3R [62] VGGT [50] StreamVGGT CO3Dv2 AUC@30 25.3 30.1 66.5 76.7 81.8 83.4 69.5 82.8 83.3 82. 87.7 82.4 Cached Memory Token. To validate the effectiveness of the cached memory token, we evaluate StreamVGGT on the ETH3D [42] dataset under two settings: full-sequence input and streaming input. Time denotes the inference latency of the final frame in 40-frame stream. The results indicate that cached memory tokens not only accelerate streaming inference but also preserve temporal consistency and perceptual accuracy. 8 Table 6: Effects of the distillation training strategy. Acc 7 scenes Comp NC Acc NRGBD Comp NC Method Distillation Attention Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. VGGT [50] StreamVGGT (w/o KD) StreamVGGT (W/ KD) Global Causal Causal 0.088 0.039 0.091 0.039 0.787 0.890 0.073 0.018 0.077 0.021 0.910 0.990 0.202 0.102 0.168 0.064 0.718 0.825 0.189 0.088 0.206 0.096 0.816 0.945 0.155 0.082 0.133 0.055 0.745 0.856 0.079 0.047 0.074 0.041 0.859 0. Table 7: Effects of the cached memory token. Methods StreamVGGT (W/ Causal Attention) StreamVGGT (W/ Cached Token) Input Type Full-sequence Streaming Acc. 0.782 0.782 Comp. Overall 0.723 0.723 0.753 0.753 Time 4.7 0.07 Figure 5: Visualizations. StreamVGGT delivers photorealistic scene reconstructions with superior geometric fidelity, fewer outliers, and sustained accuracy even in complex environments. These advantages arise from our distillation-based training, which lets the streaming model emulate global self-attention behaviour, curbing error accumulation and surpassing traditional causal approaches. Inference Time Analysis. To evaluate the efficiency of our design in streaming 4D reconstruction tasks, we compare the inference latency for the final frame of sequences containing 1, 5, 10, 20, 30, and 40 frames among StreamVGGT and VGGT [50]. All experiments are conducted on single NVIDIA A800 GPU. For StreamVGGT and VGGT, we employ FlashAttention-2 [7] with an image resolution of 518 392. The results of inference time are summarized in Figure 2. Distillation Training Strategy. To assess the effectiveness of our knowledge-distillation strategy, we evaluate three variants on the 7-Scenes, NRGBD, and ETH3D datasets: (i) the global selfattention teacher VGGT, (ii) StreamVGGT without knowledge distillation, and (iii) StreamVGGT with knowledge distillation. The results in Table 6 demonstrate that the causal model without distillation shows significantly higher reconstruction errors, demonstrating its limited frame context. After distillation, the performance gap between StreamVGGT and VGGT across all 3D reconstruction metrics remains within an acceptable range, while the model still benefits from low-latency, streaming design, confirming that the distillation strategy effectively transfers the teacher ability to capture global context to our causal architecture and supports real-time streaming reconstruction."
        },
        {
            "title": "5 Conclusion and Discussions",
            "content": "In this work, we present StreamVGGT, causal transformer architecture for real-time streaming 4D visual geometry reconstruction. By replacing global self-attention with causal temporal attention and introducing Cached Token Memory mechanism, StreamVGGT achieves incremental scene updates while preserving long-term spatial consistency. Extensive experiments demonstrate that StreamVGGT achieves comparable accuracy to the state-of-the-art offline model VGGT with only marginal performance degradation, while surpassing current online state-of-the-art models across multiple tasks, including 3D reconstruction, single-frame depth estimation, and depth estimation. 9 Limitations. Although our cached token memory mechanism effectively retains historical frame information, this approach leads to substantial increase in memory usage for long-term sequences. As more frames are processed, the memory footprint grows rapidly due to the accumulation of cached tokens. This scalability issue poses significant challenge for deploying the model on lightweight or mobile devices, where hardware resources are limited. Therefore, addressing memory efficiency while preserving accuracy remains critical area for future optimization. Additionally, our knowledge distillation strategy relies on the quality of the teacher model. However, our teacher model performs suboptimally in extreme scenarios, such as inputs with extreme rotations, fast-moving objects, or substantial non-rigid deformations. This limitation affects the prediction accuracy of our approach in highly dynamic or rapidly changing environments. Nonetheless, thanks to the flexibility and adaptability of the feed-forward architecture, these limitations can be overcome with minimal effort by fine-tuning the model on the target dataset. Broader Impacts. The StreamVGGT model offers significantly broader impacts across multiple industries, especially in the domain of real-time 4D visual geometry reconstruction. By integrating temporal attention with Cached Token Memory mechanism, StreamVGGT enables efficient incremental processing of multi-view images, ensuring real-time scene updates with minimal latency while maintaining long-term spatial consistency. This innovative approach makes it crucial technology for dynamic applications such as autonomous navigation, robotics, and immersive AR/VR experiences, where timely and accurate 3D scene understanding is essential."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54(10):105 112, 2011. 2 [2] Sarah Alnegheimish, Dongyu Liu, Carles Sala, Laure Berti-Equille, and Kalyan Veeramachaneni. Sintel: machine learning framework to extract insights from signals. In SIGMOD, pages 18551865, 2022. 8 [3] Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In CVPR, pages 62906301, 2022. 7 [4] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse realworld dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. 7 [5] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 7 [6] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, pages 58285839, 2017. 7 [7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. 2, 7, 9 [8] Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization. In CVPR, pages 1673916752, 2025. 3 [9] Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Jiwen Lu. Driv3r: Learning dense 4d reconstruction for autonomous driving. arXiv preprint arXiv:2412.06777, 2024. [10] Jan-Michael Frahm, Pierre Fite-Georgel, David Gallup, Tim Johnson, Rahul Raguram, Changchang Wu, Yi-Hung Jen, Enrique Dunn, Brian Clipp, Svetlana Lazebnik, et al. Building rome on cloudless day. In ECCV, pages 368381. Springer, 2010. 2 [11] Qiancheng Fu, Qingshan Xu, Yew Soon Ong, and Wenbing Tao. Geo-neus: Geometryconsistent neural implicit surfaces learning for multi-view reconstruction. NeurIPS, 35:3403 3416, 2022. 2, 3 10 [12] Yasutaka Furukawa, Carlos Hernandez, et al. Multi-view stereo: tutorial. Foundations and Trends in Computer Graphics and Vision, 9(1-2):1148, 2015. 2, 3 [13] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. TPAMI, 32(8):13621376, 2009. 2 [14] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, pages 873881, 2015. 2, 3 [15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 32(11):12311237, 2013. 8 [16] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, pages 24952504, 2020. [17] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000. 2 [18] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 2 [19] Jin Gyu Hong, Seung Young Noh, Hee Kyung Lee, Won Sik Cheong, and Ju Yong Chang. 3d clothed human reconstruction from sparse multi-view images. In CVPR, pages 677687, 2024. 2 [20] Nan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang. Sgaussian: Self-supervised street gaussians for autonomous driving. CoRR, 2024. [21] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR, 2018. 7 [22] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for vision-based 3d semantic occupancy prediction. In CVPR, pages 92239232, 2023. 2 [23] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. ECCV, 2024. 6 [24] Alex Kendall and Roberto Cipolla. Modelling uncertainty in deep learning for camera relocalization. In ICRA, pages 47624769. IEEE, 2016. 6 [25] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. arXiv preprint arXiv:2406.09756, 2024. 2, 3, 4, 7, 8 [26] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In CVPR, pages 20412050, 2018. 7 [27] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-perfect structure-from-motion with featuremetric refinement. In ICCV, pages 59875997, 2021. [28] Shaohui Liu, Yidan Gao, Tianyi Zhang, Remi Pautrat, Johannes Schonberger, Viktor Larsson, and Marc Pollefeys. Robust incremental structure-from-motion with hybrid features. In ECCV, pages 249269. Springer, 2025. 2 [29] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution high-detail dataset and benchmark for scene flow, optical flow and stereo. In CVPR, pages 49814991, 2023. 7 [30] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [31] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In CVPR, pages 35043515, 2020. 2, 3 [32] David Novotny, Diane Larlus, and Andrea Vedaldi. Capturing the geometry of object categories from video supervision. TPAMI, 42(2):261275, 2018. [33] John Oliensis. critique of structure-from-motion algorithms. CVIU, 80(2):172214, 2000. 2 11 [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 [35] Onur Ozyesil, Vladislav Voroninski, Ronen Basri, and Amit Singer. survey of structure from motion*. Acta Numerica, 26:305364, 2017. 2 [36] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, pages 78557862. IEEE, 2019. [37] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pages 1217912188, 2021. 6 [38] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, pages 1090110911, 2021. 7, 8 [39] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, pages 1091210922, 2021. 7 [40] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperIn CVPR, pages 49384947, glue: Learning feature matching with graph neural networks. 2020. 8 [41] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 2 [42] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multi-camera videos. In CVPR, pages 32603269, 2017. 7, 8 [43] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In CVPR, pages 29302937, 2013. 7 [44] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, pages 746760. Springer, 2012. [45] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. 3 [46] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In SIGGRAPH, pages 835846. 2006. 2 [47] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, pages 24462454, 2020. 7 [48] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. [49] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. 2, 3, 4, 7, 8 [50] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer, 2025. 2, 3, 4, 6, 7, 8, 9 [51] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In CVPR, pages 2168621697, 2024. 8 [52] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In ICCV, pages 97739783, 2023. [53] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state, 2025. 2, 3, 4, 7, 8 [54] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, pages 2069720709, 2024. 2, 3, 4, 7, 8 12 [55] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, et al. Embodiedscan: holistic multi-modal 3d perception suite towards embodied ai. In CVPR, pages 1975719767, 2024. [56] Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In ICCV, pages 56105619, October 2021. 2, 3 [57] Changchang Wu. Towards linear-time incremental structure from motion. In 3DV, pages 127 134. IEEE, 2013. 2 [58] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In CVPR, pages 803814, 2023. 7 [59] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3r: Streaming 3d reconstruction with explicit spatial pointer memory. arXiv preprint arXiv:2507.02863, 2025. 2, 3, 7, [60] Yuqi Wu, Wenzhao Zheng, Sicheng Zuo, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Embodiedocc: Embodied 3d occupancy prediction for vision-based online scene understanding. arXiv preprint arXiv:2412.04380, 2024. 2 [61] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos. In CVPR, pages 2237822389, 2024. 7 [62] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. arXiv preprint arXiv:2501.13928, 2025. 2, 3, 4, 8 [63] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: large-scale dataset for generalized multi-view stereo networks. In CVPR, pages 17901799, 2020. 7 [64] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. NeurIPS, 33:24922502, 2020. 2, 3 [65] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv preprint arXiv:2410.03825, 2024. 3, 7, 8 [66] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views, 2025. [67] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis. In CVPR, pages 1968019690, 2024. 2 [68] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. Pointodyssey: large-scale synthetic dataset for long-term point tracking. In ICCV, 2023."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}