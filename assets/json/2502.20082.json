{
    "paper_title": "LongRoPE2: Near-Lossless LLM Context Window Scaling",
    "authors": [
        "Ning Shang",
        "Li Lyna Zhang",
        "Siyuan Wang",
        "Gaokai Zhang",
        "Gilsinia Lopez",
        "Fan Yang",
        "Weizhu Chen",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by \"needle-driven\" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta's approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE."
        },
        {
            "title": "Start",
            "content": "LongRoPE2: Near-Lossless LLM Context Window Scaling Ning Shang * Li Lyna Zhang * Siyuan Wang Gaokai Zhang Gilsinia Lopez Fan Yang Weizhu Chen Mao Yang Microsoft 5 2 0 2 7 2 ] . [ 1 2 8 0 0 2 . 2 0 5 2 : r Abstract LongRoPE2 is novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by needle-driven perplexity to address the insufficient training problem; (3) mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA38B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens 80x fewer than Metas approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE."
        },
        {
            "title": "1 Introduction",
            "content": "A long context window has become an essential feature of Large Language Models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; Abdin et al., 2024; Zhu et al., 2024; Team, 2024). For instance, 128k context window is now standard in recent LLMs like GPT-4o and LLaMA3.1. Context window extension is achieved through mid-training *Equal contribution. Siyuan Wang and Gaokai Zhang did this work during the internship at MSRA. Siyuan Wang: Shanghai Jiao Tong University; Gaokai Zhang: Zhejiang University. Correspondence to: Li Lyna Zhang <lzhani@microsoft.com>. 1 Figure 1. LongRoPE2-extended LLaMA3-8B achieves the best performance at 128k context length among 10B models. after pre-training, where the rotary positional embeddings (RoPE) (Su et al., 2021) are rescaled to fit the expanded context. The model weights are then fine-tuned using longsequence data to adapt to the rescaled RoPE. Extending the context window of pre-trained LLM requires addressing the out-of-distribution (OOD) issue in rotary positional embeddings (RoPE). In RoPE, higherdimensional RoPE embeddings produce OOD values at extended token positions due to incomplete rotation periods within the original context window (Liu et al., 2023; Han et al., 2023; Men et al., 2024a). To mitigate this, RoPE rescaling remaps these OOD values into the in-distribution range learned during pre-training. Various methods, such as YaRN (Peng et al., 2023), NTK (LocalLLaMA, 2023), and LongRoPE (Ding et al., 2024), have been proposed to determine appropriate rescaling factors. Despite attempts to mitigate the OOD issue with RoPE rescaling, context window extension still encounters two major challenges. First, rescaling factors derived from previous methods often fall short of achieving the effective target context length. For example, LLaMA3.1 adopts YaRN to extend its context window to 128k; however, its performance on RULER (Hsieh et al., 2024), benchmark designed to evaluate LLMs long-context processing capability, deteriorates significantly when going beyond 64k  (Fig. 1)  . Second, existing approaches to extending an LLMs context window usually lead to noticeable performance degradation on tasks for the original short context window. As shown in Fig. 2(c), extending Phi3-mini (Abdin et al., 2024) to 128k LongRoPE2: Near-Lossless LLM Context Window Scaling results in MMLU score drops of 7.56, 4.34, and 3.52 points for YaRN, NTK, and LongRoPE, respectively. Restoring short-context performance typically requires costly midtraining strategies, such as multi-stage progressive extension (Dubey et al., 2024) and pre-training data replay (Hu et al., 2024b), which increase both training costs (e.g., 800B tokens for LLaMA3.1) and system complexity. This paper introduces LongRoPE2, novel approach for context extension that enables LLMs to achieve an effective long context window while preserving short-context performance. Our analysis reveals that lower RoPE dimensions are sufficiently trained, whereas higher dimensions critical for long-context processing receive inadequate training. This results in shorter effective RoPE rotation ranges within the pre-trained context length. We hypothesize that this undertraining in higher dimensions is the root cause of their extended rotation periods longer than their theoretical predictions. Consequently, the critical dimensions shift earlier, leaving existing rescaling methods unable to fully address OOD issues across all dimensions. This hypothesis also explains the empirical observations showing that RoPE requires scaling factors larger than analytically derived values in the higher dimensions for better long-context performance (Gao et al., 2024; Meta, 2024). Building on this hypothesis, LongRoPE2 adopts simple yet effective RoPE rescaling algorithm to fully address the OOD issues across all RoPE dimensions. It leverages evolutionary search to identify the true critical RoPE dimensions and optimal rescaling factors, guided by more effective needle-driven perplexity (PPL) evaluation. Unlike conventional PPL, which averages across all tokens, LongRoPE2 focuses exclusively on needles specific answer tokens within long documents that require deep contextual understanding. This ensures accurate evaluation of long-context performance. The search determines the true critical dimensions and rescaling factors for higher OOD dimensions, while NTK scaling is applied to the well-trained lower dimensions. The rescaling factors yielding the lowest PPL are selected as the final solution. To preserve the original short-context performance, LongRoPE2 incorporates mixed context window training, which simultaneously trains pre-trained context window with the original RoPE and long-context window with rescaled RoPE. The long-context window is trained by adapting model weights to the rescaled RoPE for long documents packed to the target length. Concurrently, the short-context window is trained on short documents, also packed to the same target length, using an attention mask to prevent crossdocument attention. At inference, original RoPE is used if the input is within the short context; otherwise, rescaled RoPE is applied. This method optimizes long-context performance without sacrificing short-context performance. Extensive experiments across various LLM sizes and challenging benchmarks validate our hypothesis and demonstrate the effectiveness of LongRoPE2. For Phi3-mini3.8B and LLaMA3-8B, our rescaling factors shift the theoretical critical dimension from 31 to 25 and from 35 to 30, respectively. By fully resolving RoPE OOD issues, LongRoPE2-extended Phi3-mini-3.8B and LLaMA3-8B achieve an effective 128k context window, significantly outperforming baselines on both synthetic and real-world long-context benchmarks. Moreover, with mixed context window training, LongRoPE2 is the only RoPE rescaling method that can retain over 97% of the original short-context performance on standard tasks. Remarkably, LongRoPE2extended LLaMA3-8B-128k surpasses Metas LLaMA3.18B-128k in long-context performance while maintaining comparable short-context accuracy, all achieved with just 10B training tokens80 fewer than Metas 800B tokens."
        },
        {
            "title": "2 Context Window Extension and Challenges",
            "content": "2.1 Preliminary Rotary Position Embedding (RoPE). Transformer models require explicit positional information, often in the form of position embedding, to represent the order of input tokens. Our work builds on the Rotary Position Embedding (Su et al., 2021), which is widely used in modern LLMs. Let [0, c) be position index and x1, ..., xL Rd sequence of vectors, where is the attention head dimension. Using RoPE, the self-attention first incorporates position information to the word embeddings and transforms them into query and key representations: qm = fq(xm, m); kn = fk(xn, n); fq(xm, m) = eimθWqxm (1) fk(xn, n) = einθWkxn (2) 1 is the imaginary unit. Wq,Wk Rdd where = are projection matrices. Attention weights are computed as: sof tmax( qT mkn ) (3) where qm, kn are column vectors, and qT mkn is their Euclidean inner product. Let Re[] denote the real part of complex number, the inner product qT becomes: (Wqxm)(Wkxn)ei(mn)θ(cid:105) mkn = Re qT (4) (cid:104) where (Wkxn) is the complex conjugate of (Wkxn). With RoPE, attention becomes function only dependent on the relative position between tokens, rather than their absolute positions. By applying Eulers formula, einθ can be expressed as trigonometric functions. Then, RoPE encodings can be further written as block diagonal matrix with entries of the form: fq,k(n)i = (cid:18)cosnθi sinnθi cosnθi sinnθi (cid:19) ; θi = θbase 2i/d (5) 2 LongRoPE2: Near-Lossless LLM Context Window Scaling Figure 2. (a) RoPE OOD (red area) when extending context length from 2k to 4k. (b) Per-dimensional RoPE rescaling factor from different approaches for extending Phi3-mini from 2k to 128k, all aligning with RoPE OOD theory. (c) Performance of Phi3-mini-128k after fine-tuning. Existing methods fail to achieve an effective 128k context length and show noticeable short-context performance drop. where θi is the per-dimensional rotation angle for = 0, 1, ..., d/2 1. θbase is predefined RoPE base value, typically set to 10000 in pre-training. When directly extrapolated to 4k, the cosine values between 2k and 4k fall outside the pre-trained range, becoming OOD RoPE values (highlighted in red). RoPE Per-Dimensional Period. Due to the periodicity of cosine and sine functions, RoPE is periodic function. Specifically, for the ith RoPE dimension, the corresponding period length Ti can be calculated as follows: Ti = 2π θi (6) The period length of each dimension is directly determined by its rotary angle θi. As shown in Fig. 2(a), with fixed θbase = 10000, θi decreases as the dimensional index increases, leading to longer periods in higher RoPE dimensions. In typical cases, the periods in higher RoPE dimensions often exceeds the pre-trained context window size, leading to incomplete periods. For instance, in Phi3-mini, the pre-trained context window size is 2048, while the period length of the highest dimension (i.e., the 48th cosine dimension) is 51861, covering less than 4% of full period. 2.2 RoPE Rescaling Theory Despite its effectiveness, RoPE, like other position encodings, faces challenges in context length extrapolation. In particular, when input sequence length exceeds the predefined context window, the perplexity can shoot up to levels comparable to completely untrained models (i.e., > 103). RoPE OOD. Direct length extrapolation fails because longer sequences introduce untrained token positions, leading to out-of-distribution (OOD) positional values in RoPE. As shown in Fig. 2(a), the periods in high RoPE dimensions exceed the original context window size Ltrain. Consequently, for these dimensions, the model does not see full rotation period during pre-training, resulting in new untrained RoPE values at extended token positions. For instance, in Fig. 2(a), the 40thcosine dimension does not complete full period within the pre-trained length Ltrain=2k. Theoretical Critical RoPE dimension. In contrast to higher RoPE dimensions, lower dimensions (e.g., 8th and 16th dimension in Fig. 2(a)) have seen many full periods during pretraining. As result, there exists theoretical critical dimension (TCD) dtcd that divides RoPE dimensions into two groups: one with multiple full periods within the pre-trained length Ltrain (i.e., Ti < Ltrain, < dtcd) and another with incomplete periods (i.e., Ti Ltrain, dtcd). Following (Liu et al., 2023), the critical dimension can be computed as: dtcd = 2 2 logθbase Ltrain 2π (7) As shown in Fig.2(a), for Phi3-mini(Abdin et al., 2024) with d=96, base θbase=10000, and Ltrain = 2048, the critical dimension is 62, corresponding to the 31st cosine dimension. Unless otherwise specified, we focus on the cosine dimensions of RoPE (i.e., = 0, 1, ..., d/2 1) for simplicity. RoPE OOD theory. To address the RoPE OOD issue in long-context extension, straightforward approach is to rescale the per-dimensional rotation angle θi and ensure higher RoPE-OOD dimensions remain within the pretrained RoPE range. This forms the widely accepted RoPE OOD theory (Liu et al., 2023; Chen et al., 2023; Men et al., 2024a). Formally, let the target context window size be and λi be the rescaling factor for the ith RoPE dimension. The rescaled per-dimensional rotation angle ˆθi is then given by: ˆθi = 1 λi θbase 2i/d (8) To avoid OOD, the new rescaled periods of higher RoPE dimensions ( ˆTi, > dcd) must remain within the pretrained LongRoPE2: Near-Lossless LLM Context Window Scaling ˆTi range, leading to the following constraint: Lˆθi 2π Ltrain Ltrainθi 2π"
        },
        {
            "title": "Ltrain\nTi",
            "content": "λi ; for for ; ; dtcd dtcd (9) theoretical analysis, LongRoPE employs PPL-guided evolutionary search to find the per-dimensional scale factor λi. To leverage NTK theory, it enforces monotonically non-decreasing scaling factor constraint during the search. (10) 2.4 Challenges Specifically, is the context window extension ratio. The Ltrain RoPE OOD theory establishes this ratio as the lower bound for scaling factors in higher RoPE dimensions beyond dtcd. 2.3 Review of Prior RoPE Rescaling Approaches Building on the RoPE OOD theory, various RoPE rescaling methods have been proposed for LLM context window extension (Chen et al., 2023; Han et al., 2023; Men et al., 2024b; Yang et al., 2024b). Prominent approaches, including PI, NTK, YaRN and LongRoPE, have been widely adopted to enable long context in open-source LLMs (Yang et al., 2024a; Dubey et al., 2024; Abdin et al., 2024). PI introduces linear positional interpolation, where all the RoPE dimensions use the same scale factor of λi = . Ltrain Despite its simplicity, this uniform scaling crowds the positional information, making it difficult for the model to distinguish closely positioned tokens. NTK θ Scaling approaches RoPE from an information encoding perspective, applying the Neural Tangle Kernel (NTK) theory (Jacot et al., 2018; Tancik et al., 2020). The core idea is that neural networks are difficult to learn highfrequency features (low RoPE dimensions), and large scaling factor can affect these high-frequency positional information, leading to the loss of crucial details needed to differentiate similar closely positioned tokens. As result, NTK-based methods suggest increasing the original RoPE base value θbase to larger base θntk. Several methods (LocalLLaMA, 2023; Men et al., 2024b; Liu et al., 2023) have been proposed to determine this new base value. However, some fail to align with RoPE OOD theory. For instance, (LocalLLaMA, 2023) use λi = s2i/(d2), leading to insufficient interpolation and increased PPL before the target length. The approach in (Liu et al., 2023), which calculates θntk based on the theoretical critical dimension, is the most widely adopted NTK-based method. Specifically, log Ltrain 2π θntk θ (i > dtcd). Unless stated otherwise, NTK in this work refers to this approach. 2π , yielding λi Ltrain YaRN divides RoPE dimensions into three groups as shown in Fig. 2(b). For lower dimensions with high frequencies, YaRN proposes no interpolation, setting λi = 1 to better preserve high-frequency positional information compared to NTK. For high dimensions, YaRN adopt PI and set λi = . For dimensions that fall in-between use linearly Ltrain increasing scale factor. LongRoPE. Unlike other extension methods relying on 4 RoPE OOD theory are insufficient. Fig. 2(b) compares scale factor distributions for extending Phi3-mini from 2k to 128k. NTK, YaRN and LongRoPE all align the RoPE OOD with λi 64 for > dtcd, but yielding varied performance (Fig. 2(c)). NTK and LongRoPE outperforms YaRN on both shortand long-context tasks. We highlight two observations: (1) The theoretical lower bound, , is often subopLtrain timal. Beyond dimension dtcd = 31, YaRN strictly adheres to this bound ( =64), but NTK and LongRoPE use larger Ltrain values to achieve much better performance. (2) Beyond dtcd, larger scale factors dont always improve long-context performance. For example, in dimensions 31-48, NTK uses much larger scale factors than LongRoPE, yet LongRoPE achieves better performance. These findings align with prior works (Meta, 2024; Men et al., 2024a; Wang et al., 2024), where marginally larger scale factors than the extension ratio empirically improve performance. This raises the fundamental question: In RoPE OOD theory, if RoPE periods beyond critical dimension can address OOD with λi = , why do slightly larger scaling factors lead Ltrain to better performance? Short performance drop. persistent challenge in long context extension is performance degradation on original short window, which poses significant obstacle in practical LLM development. common solution is progressively extension using large-scale training data (Dubey et al., 2024; Hu et al., 2024b). For example, LLaMA3.1 (Dubey et al., 2024) adopts SIX-stage extension process requiring 800B tokens to extend from 8k to 128k, greatly increasing training complexity and costs. Though LongRoPE introduces training-free short scaling factor, it fails to fully address the performance drop (Figure 2(c)). As result, bridging this gap remains an unresolved challenge."
        },
        {
            "title": "3 LongRoPE2 Methodology",
            "content": "3.1 New RoPE OOD Hypothesis The empirical RoPE periods in higher dimensions are longer than theoretical values, limiting current methods to fully address RoPE OOD. In Sec. 2, we observe that RoPE scale factors slightly exceeding the theoretical lower bound beyond the critical dimension dtcd yield improved long-context performance. We attribute this to insufficient training in higher dimensions, which extends rotation periods and reduces the critical dimension index (Fig. 2(a)) relative to the theoretical expectations. LongRoPE2: Near-Lossless LLM Context Window Scaling Figure 3. Sequence length required to span the theoretical period during Phi3-mini pre-training for different RoPE dimensions. Insufficient training in higher RoPE dimensions leads to shorter effective RoPE ranges and longer actual periods. As illustrated in Fig. 3(a), lower RoPE dimensions (with shorter periods) receive repeated full-period training cycles within single corpus. For example, in Phi3-mini, the 8th dimension has short period of 24, requiring only mn = 24 tokens for full cycle. 2048-token training sample thus covers this dimension thousands of times, ensuring sufficient training. In contrast, higher RoPE dimensions, with period exceeding the pre-trained context window, receive far less training. For example, the 48th dimension spans only 4% of its cosine period within 2048-token sequence (Fig. 3(b)), resulting in the theoretical incomplete period being covered just once. deeper challenge arises after self-attention: these incomplete RoPE periods in high dimensions exhibit reduced effective ranges (Fig. 3(b)), stretching practical period beyond theoretical values. As shown in Eq. 3, RoPE positional information is incorporated via self-attention, where the max relative token distance determines the practical RoPE range. As real-world data rarely contains long-range dependencies (e.g., distances of 2048 tokens), higher RoPE dimensions tend to be under-trained, amplifying period discrepancies. This under-training in higher RoPE dimensions explains why larger scaling factors improve long-context performance. We formalize this insight as: Hypothesis. Insufficient training in higher RoPE dimensions extends empirical rotation periods beyond the theoretical 2π . This discrepancy necessitates larger scale factors θi to mitigate RoPE OOD and lowering the critical dimension index drcd below its theoretical dtcd. 3.2 RoPE Rescaling Factor Search Since the theoretical RoPE OOD theory cannot fully address OOD issues, we use search-based approach to identify the practical true critical dimension and optimal rescaled RoPE. Inspired by LongRoPE, we search for scaling factors, apply them to the pre-trained LLM via rescaled RoPE, and compute perplexity (PPL) on fixed samples at target context length (e.g., 128k). The factors that minimize PPL are chosen for best preserving pre-trained RoPE information while addressing OOD. Given that the approach relies entirely on Algorithm 1 Initialization with theoretical periods Input: theta base θbase; RoPE dim d, pre-trained context window size Ltrain, target length L; theoretical critical dimension dtcd 1: P0 = [0] 2/d 2: d10 tcd= 2 logθbase cal 10 periods.} {include smaller indices as candidate drcd} tcd to dtcd do , 2 ) Ltrain Ltrain 2π10 {Compute the dim with theoreti3: for int drcd=d10 s=randint( 4: Ltrain λ[drcd : 2 1] = 5: 1 θd10 6: (2d10 tcd base sθ = tcd /d) λ[0 : drcd]= compute rescaling factors using NTK θd10 add λ into P0; tcd 7: 8: 9: end for 10: Return P0 ; the search, we introduce two key innovations. Synthetic needle data to guide the search. Naively using PPL-guided search can easily result in suboptimal rescaling factors. First, long sequences often contain irrelevant or low-dependency tokens, reducing the effective maximum token dependency. For instance, predicting the final token in 128k-token book may not require the context of the first token. Second, standard PPL, by averaging over all token equally, fails to effectively capture the long-context abilities (Hu et al., 2024a; Fang et al., 2024) and can be dominated by irrelevant tokens, obscuring key answer tokens. As result, the rescaling factors that minimize PPL often fail to achieve the target context window size. To address this, we introduce needle-driven PPL evaluation. Instead of using real-world long documents, we synthesize long data with controlled token dependency distances. Inspired by needle retrieval benchmarks for long-context evaluation (Hsieh et al., 2024; Li et al., 2024a), we randomly sample 10 books from the PG19 validation set. At the start of each sample, we insert needle (a specific piece of text as shown in Appendix C), and at the end, we ask the model to retrieve this needle. We then compute the perplexity of only the retrieved needle tokens. The needle-based PPL evaluates how well the model, with the rescaled RoPE, can understand the entire context and retrieve the distant needle. Critical dimension-aware scale factor search. With the synthetic needle-driven PPL evaluation, we run simple evolutionary search to identify the real critical dimension drcd and the optimal rescaling factors. For search efficiency, we restrict the search to dimensions drcd, while applying NTK-aware scaling to lower dimensions (i < drcd) using the adjusted base value derived from drcd. The search begins by initializing drcd and rescaling factors, as detailed in Algorithm 1. Based on our hypothesis, smaller indices are considered potential drcd , with candidates ranging from d10 tcd, where the theoretical RoPE period spans 10 periods in the pre-training window, and dtcd. For each can5 LongRoPE2: Near-Lossless LLM Context Window Scaling ]{search space} Algorithm 2 Critical dimension aware mutation Input: population P; mutation probability p; synthetic long data 1: Top-k = Update Topk ( P); 2: SP=[ , 2 Ltrain Ltrain 3: for λ in Top-k do 4: 5: λright= λ[drcd : λright=Mutation with mono constraint (λright, p, SP) {mutate scale factors beyond θdrcd .} λ[drcd : θdrcd = {update theta base in θdrcd .} 2 1]= λright 2 1] 1 (2drcd/d) λright[0]θ base λ[0 : i]= compute rescaling factors using NTK θdrcd {update dims before θdrcd } Compute PPL (LLM, λ, X); add λ into P; 9: 10: end for 11: Update with Top-k; Return the latest population ; 6: 7: 8: didate, rescaling factors above are randomly sampled Ltrain for dimension drcd to address RoPE OOD value, while NTK scaling is applied to dimensions < drcd. We iteratively sample and mutate rescaling factors until reaching population size . Using the needle-driven synthesis method, we generate L-token documents and compute PPL for each candidate by applying the rescaling factors to the LLM and evaluating the input X. The population is updated through standard evolution search. Algorithm 2 shows the mutation process. For each sampled scaling factor, we split RoPE dimensions at drcd. The higher group (i drcd) performs mutation with probability under the monotonic non-decreasing constraint: λi λi+1. The theta base for drcd is updated after mutation, and NTK scaling is applied to rescale factors in the lower group. Fig. 4 shows the final scaling factors identified by LongRoPE2 for Phi3-mini and LLaMA3-8B under 128k context. The practical critical dimensions (drcd) are shifted earlier to 25 and 30, compared to the theoretical values dtcd of 31 and 35, respectively. The scaling factors for RoPE OOD dimensions are slightly larger than PI/YaRN/LongRoPE and notably smaller than NTK. Figure 4. Scale factors across different RoPE rescaling approaches. 3.3 Mixed Context Window Training We then apply the optimal rescaling factors to RoPE on the pre-trained LLM, but two critical challenges remains for effective long-context LLM deployment. First, the pre-trained 6 Figure 5. Mixed context window training to improve both short and long context capabilities. Table 1. Mid-training data mix. Short Context Window Ltrain 3B Long Context Window Ltrain-100k 3B 100k-200k 4B Tokens model weights have not been trained with the rescaled RoPE, leading to poor performance on real-world long-context tasks. Second, extending context window size often degrades performance on original short-context tasks (Ding et al., 2024; Hu et al., 2024b), making it challenging to balance longand short-context capabilities. To address these challenges, we introduce novel mixed context window training approach that achieve both longand short-context superior performance without adding systemlevel training complexity. Specifically, short-context training reuses the original RoPE and fine-tunes on short sequences, preserving pre-trained performance. Long-context training applies the rescaled RoPE and fine-tunes on long sequences, enabling effective long-context understanding. Fig. 5 illustrates this process. For target context window size of L=128k, we sample short sequences ( Ltrain) and long sequences (8k-200k), chunked into 128k segments with BOS and EOS tokens. For segments labeled as short windows, the original RoPE is used with attention masks to prevent self-attention across different documents as shown in Fig. 5(a). For long-context segments, we apply the rescaled RoPE for full attention within the 128k segments (Fig. 5(b)). More details can be found in Appendix B."
        },
        {
            "title": "4 Experiments\n4.1 Setup",
            "content": "Evaluation LLMs and Tasks. We apply LongRoPE2 to LLaMA3-8B and Phi3-mini (3.8B). Phi3-mini, with its limited capabilities, serves as rigorous testbed for evaluating RoPE rescaling methods. Performance is evaluated across three dimensions: (1) long-context stress tests, including RULER (Hsieh et al., 2024) and Needle in Haystack (Kamradt, 2023); (2) real-world long-context benchmarks including LOFT (Lee et al., 2024a), InfiniteBench (Zhang et al., 2024a), and LongBench (Bai et al., 2023); (3) standard LongRoPE2: Near-Lossless LLM Context Window Scaling Table 2. Comparison with prior SOTA RoPE rescaling methods on RULER Benchmark. We report the average score across 13 tasks. Method 4k 8k 16k 32k 64k 128k YaRN NTK LongRoPE LongRoPE2 YaRN NTK LongRoPE LongRoPE Base Model: Phi3-mini (3.8B) 65.22 85.74 91.34 72.81 71.20 88.40 76.51 90.41 78.68 87.02 83.23 87.22 75.97 80.57 79.46 83.33 Base Model: LLaMA3-8B 91.86 94.38 94.60 94.61 87.87 92.64 92.70 93. 84.67 91.93 91.01 92.31 68.80 87.33 86.60 90.49 52.16 61.91 64.63 65.37 62.51 79.26 81.23 85.62 39.37 49.37 53.71 58.81 49.39 73.19 73.40 82. benchmarks within 4096-token context. Mid-training. Our method can potentially support millionlevel context length, but due to resources constraint, we extend the two models to 128k context window and midtrain on 64 A100 GPUs using 10B-token dataset. Following the per-source upsampling from (Fu et al., 2024), we sample 4.5B, 2.5B, and 2B tokens from RedPajamav1 (Computer, 2023), RedPajama-v2 (Weber et al., 2024), and StarCoder (Li et al., 2023), covering 8k200k sequence lengths. For short context windows, we sample 1B tokens from Fineweb-Edu (Lozhkov et al., 2024). Table 1 shows the token distribution by sequence length. We train for 1 epoch with global batch size of 64. The initial learning rate of 2e-5 with cosine learning rate scheduler. Baselines. We compare with state-of-the-art RoPE rescaling methods, including YaRN, NTK, and LongRoPE. All baselines use the same mid-training procedure for fairness. 4.2 Main Results We present the main results of LongRoPE2-extended Phi3mini-3.8B-128k and LLaMA3-8B-128k, comparing them with models using other STOA RoPE rescaling methods. Long-context performance on RULER benchmark. Table 2 compares performance on RULER, which consists of 13 synthetic tasks. Across Phi3-mini-3.8B and LLaMA3-8B, LongRoPE2 consistently outperforms prior methods, achieving superior results across all evaluation lengths within the 128k window. On LLaMA3-8B, LongRoPE2 achieves an effective 128k context window, maintaining strong score of 82.03 at 128k, while previous methods degrade significantly at longer contexts. For example, LongRoPE, the prior best, drops from 81.23 (64k) to 73.40 at 128k. For Phi3-mini-3.8B, LongRoPE2 shows even greater advantages, overcoming the challenges of the smaller models weaker capabilities. NTK performs well below 32k and declines sharply beyond, while LongRoPE underperforms at shorter contexts. In contrast, LongRoPE2 consistently enhances performance across all lengths. Notably, the 128k average score of 58.81 is skewed by tasks with low scores 7 Figure 6. LongRoPE2 (right) delivers near-perfect lossless performance in the Needle in Haystack pressure test. on smaller LLMs, such as CWE, which achieves only 1% accuracy. Detailed per-task score is available in Appendix B. Needle in Haystack pressure tests. We evaluate LongRoPE2 using the popular long-context pressure test, Needle in Haystack, which measures models ability to retrieve needles from long documents at varying depths. We run 10 times at the same depth and length. As shown in Fig. 6, LongRoPE2 achieves near-perfect accuracy across all evaluation lengths within the 128k context window. In contrast, methods like NTK often fail at longer contexts, and LLaMA3.1-8B extended by YaRN, despite being fine-tuned on 800B tokens, fails beyond 100k. These results highlight LongRoPE2s robust long-context modeling capabilities. Long-context performance on real-world benchmarks. Beyond synthetic tasks, we evaluate real-world benchmarks: LOFT (7 retrieval tasks including argumentative retrieval, fact-checking, web search, multi-hop reasoning QA, etc), InfiniteBench (key-value retrieval and multi-choice QA), and LongBench (in-context learning and code completion). Note that our models are evaluated without post-training, so scores are lower than post-training results. As shown in Table 3, LongRoPE2 consistently improves performance across all benchmarks, demonstrating strong generalization to practical scenarios. In contrast, YaRN and NTK perform notably worse, particularly on the small Phi3-mini-3.8B. Standard benchmarks at original context window. RoPEbased context extension typically sacrifices short-context performance. As Table 4 shows, prior methods like YaRN, NTK, and LongRoPE exhibit notable degradation. For example, YaRN and NTK show performance drop of -15.2% and -9.3% oh Phi3-mini, with declines of -21.15 and -14.55 absolute points on GSM8K. In contrast, LongRoPE2 retains 97.6% and 98.6% o0f the pre-trained performance on Phi3-mini-3.8B and LLaMA3-8B, establishing it as the first lossless extension method that preserves core capabilities. LongRoPE2: Near-Lossless LLM Context Window Scaling Table 3. Long context performance comparison under different extension methods on real-world benchmarks Method LOFT InfiniteBench - LongBench Avg. ArguAna FEVER HotPotQA MS MACRO 5.86 YaRN 7.57 NTK LongRoPE 21.14 LongRoPE2 23.00 YaRN 26.14 NTK 67.14 60.85 LongRoPE LongRoPE2 74.28 4.0 0 5.0 5.0 7.0 22.0 22.0 28.0 4.0 21.0 64.0 70. 62.0 96.0 96.0 96.0 0 0 3.0 4.0 15.0 53.0 25.0 70.0 8.0 6.0 17.0 19.0 21.0 75.0 57.0 80.0 NQ Quora SciFact Avg. Base model: Phi3-mini (3.8B) 12.0 13.0 35.0 39.0 43.0 89.0 90.0 94. 1.0 4.0 8.0 10.0 12.0 9.0 16.0 14.0 50.96 52.31 50.67 55.23 Base model: LLaMA3-8B 23.0 71.0 74.0 79.0 12.0 64.0 62.0 73. 51.81 67.98 70.39 73.37 KV retrieval 5.8 5.1 5.6 12.0 2.2 66.0 74.0 88.0 En.MC TriviaQA TREC LCC RepoBench-P 31.44 37.55 35.81 42.36 30.57 42.79 45.85 46.72 84.35 84.01 86.47 87.27 88.97 90.87 89.99 91.13 61.00 65.00 62.50 67.00 73.50 74.00 76.00 76. 63.98 62.36 55.25 62.67 65.40 68.67 69.13 70.47 59.23 59.82 58.43 60.10 62.21 65.55 67.38 67.39 Table 4. Comparison of long-context LLMs with original Phi3mini and LLaMA3-8B on regular short benchmarks. Table 6. Ablation study on needle-PPL guided search. Search Metric 4k 8k 16k 32k 64k 128k (a) Phi3-mini (3.8B) with 128k context window Model Avg. MMLU MMLU-Pro HellaSwag TruthfulQA GSM8K Original Phi3-mini (2k) YaRN NTK LongRoPE LongRoPE2 63.2 70.78 53.6 63.22 57.3 66.43 58.5 67.26 61.7 70. 41.17 30.95 36.09 36.28 40.30 77.96 75.27 76.92 75.73 77.07 (b) LLaMA3-8B with 128k context window LLaMA3.1-8B 57.2 66.33 Original LLaMA3-8B (8k) 56.5 66.62 52.1 62.25 54.0 63.84 54.6 64.69 55.7 65.01 YaRN NTK LongRoPE LongRoPE 36.79 35.87 31.88 34.14 33.74 34.61 81.71 82.08 81.25 82.11 82.14 81.69 47.82 42.19 43.34 46.26 47.61 45.17 44.04 42.61 43.45 43.65 46.17 78.54 57.39 63.99 67.17 73.62 56.18 54.05 42.45 46.92 48.90 50. Table 5. Ablation study on real critical dimension. Method Regular short tasks RULER MMLU MMLU Pro GSM8K 4k 8k 16k 32k 64k 128k Base Model: Phi3-mini (3.8B) LongRoPE2 70.07 63.22 YaRN-rcd 62.30 YaRN NTK NTK-rcd 66.43 65. 40.30 30.95 30.24 36.09 35.09 73.62 57.39 56.48 63.99 59.29 90.41 87.22 83.33 76.51 65.37 58.81 85.74 78.68 75.97 65.22 52.16 39.37 86.56 77.66 74.48 67.73 52.73 44.39 91.34 87.02 80.57 72.81 61.91 49.37 90.51 85.32 81.80 73.89 63.59 54. Base Model: LLaMA3-8B LongRoPE2 65.01 62.25 YaRN-rcd 64.30 YaRN NTK NTK-rcd 63.84 64.70 34.61 31.88 33. 34.14 34.23 50.80 42.45 50.34 46.92 45.87 94.61 93.68 92.31 90.49 85.62 82.03 91.86 87.87 84.67 68.80 62.51 49.39 94.22 92.02 89.20 82.56 76.37 71.46 94.38 92.64 91.93 87.33 79.26 73.19 94.39 92.35 91.43 88.82 83.22 77.25 4.3 Ablation Study The effectiveness of real critical dimension drcd. key factor in LongRoPE2s superior long-context performance is its full resolution of RoPE OOD values across all dimensions. To validate this, we extend our experiments beyond LongRoPE2 by applying our identified practical critical dimension drcd to YaRN and NTK, yielding YaRN-rcd and NTK-rcd variants (see Fig. 9 in Appendix B). As shown in Table 5, correcting drcd improves long-context performance for both methods, revealing the inadequacy of theoretical critical dimensions in fully addressing RoPE OOD issues. However, correcting the critical dimension alone does not ensure optimal results. By further optimizing scaling factors, LongRoPE2 consistently outperforms YaRN-rcd and 8 PG19-128k PPL PG19-Needle 128k PPL (ours) Base Model: Phi3-mini (3.8B) 83.05 83.33 91.16 90.41 87.93 87. 75.27 76.51 62.72 65.37 50.23 58.81 PG19-128k PPL PG19-Needle 128k PPL (ours) 94.46 94.61 93.36 93. 91.67 92.31 90.28 90.49 84.55 85.62 78.68 82.03 Base Model: LLaMA3-8B NTK-rcd on both shortand long-context benchmarks. The effectiveness of need-PPL guided search. LongRoPE2 identifies the true critical dimension and scaling factors through needle-PPL-guided evolutionary search, which minimizes interference from irrelevant tokens to effectively capture the rescaled RoPEs long-context capabilities. To validate its effectiveness, we use 10 pure PG19 documents as baseline, identical to those used for generating our needle-data, applying the same search and mid-training process. Table 6 compares the RULER scores for Phi3-mini3.8B-128k and LLaMA3-8B-128k, using scaling factors from two PPL-guided searches. The results show that naive PPL-guided search fails to ensure effective rescaling factors, as it struggles to identify the correct critical dimension and tends to yield slightly smaller scaling factors. The effectiveness of mixed context window training. To ablate its effectiveness, we disable mixed context window training in LongRoPE2 and instead follow conventional midtraining with single rescaled RoPE. As shown in Table 7, removing mixed context window training results in significant drop in performance on regular short-context tasks, as expected. Interestingly, mixed context window training not only preserves short performance but also improves longcontext performance (8k128k). This may be attributed to the preservation of pre-trained RoPE for shorter contexts, allowing long-context training to focus more effectively on adapting to the new introduced token positions."
        },
        {
            "title": "5 Conclusion",
            "content": "We present LongRoPE2, method for near-lossless LLM context window extension. By addressing insufficient training of higher RoPE dimensionsa key limitation in hanLongRoPE2: Near-Lossless LLM Context Window Scaling Table 7. Ablation study on mixed context window training. Method MMLU MMLU Pro GSM8K 4k 8k 16k 32k 64k 128k Base Model: Phi3 June LongRoPE2 70.07 LongRoPE2/ wo. 66.56 40.30 34.86 73.62 64.67 90.41 86.87 83.33 76.51 65.37 58.81 90.55 85.77 81.08 73.31 63.75 56. LongRoPE2 65.01 LongRoPE2/ wo. 64.57 34.61 33.83 50.80 48.37 94.61 93.68 92.31 90.49 85.62 82.03 94.67 93.15 91.24 89.38 83.53 80.18 Base Model: LLaMA3-8B dling OOD positional valuesLongRoPE2 uses evolutionary search-guided rescaling and mixed context window training to achieve 128k effective context length with just 10B tokens, retaining 97.6% of the original short-context performance. Extensive experiments on on LLaMA3-8B and Phi3-mini-3.8B demonstrates the superiority over prior art approaches. Future work will explore scaling LongRoPE2 toward fully lossless and infinite context window extension."
        },
        {
            "title": "Acknowledgement",
            "content": "We sincerely thank Jianwen Zhang for his insightful discussions and valuable support in providing resources."
        },
        {
            "title": "Impact Statement",
            "content": "This work advances the field of Machine Learning by enabling LLMs to process longer contexts effectively. LongRoPE2 enhances LLM capabilities for tasks like document summarization and scientific research. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter, D., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., PerezBecker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, 9 F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report, 2023. An, C., Huang, F., Zhang, J., Gong, S., Qiu, X., Zhou, C., and Kong, L. Training-free long-context scaling of large language models, 2024. URL https://arxiv. org/abs/2402.17463. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer, 2020. URL https://arxiv. org/abs/2004.05150. Chan, C.-M., Xu, C., Yuan, R., Luo, H., Xue, W., Guo, Y., and Fu, J. Rq-rag: Learning to refine queries for retrieval augmented generation, 2024. URL https:// arxiv.org/abs/2404.00610. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509. Computer, T. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https: //github.com/togethercomputer/RedPajama-Data. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens, 2023. URL https://arxiv.org/ abs/2307.02486. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F., and Yang, M. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. Dong, K., Deik, D. G. X., Lee, Y. Q., Zhang, H., Li, X., Zhang, C., and Liu, Y. Multi-view content-aware indexing for long document retrieval, 2024. URL https://arxiv. org/abs/2404.15103. LongRoPE2: Near-Lossless LLM Context Window Scaling Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. 2024. URL https://arxiv.org/abs/2407.21783. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. Fang, L., Wang, Y., Liu, Z., Zhang, C., Jegelka, S., Gao, J., Ding, B., and Wang, Y. What is wrong with perplexity arXiv preprint for long-context language modeling? arXiv:2410.23771, 2024. Jeong, S., Baek, J., Cho, S., Hwang, S. J., and Park, J. C. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity, 2024. URL https://arxiv.org/abs/2403.14403. Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y., and Peng, H. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171, 2024. Gao, T., Wettig, A., Yen, H., and Chen, D. How to train longcontext language models (effectively). arXiv preprint arXiv:2410.02660, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv. org/abs/2312.00752. Guo, M., Ainslie, J., Uthus, D., Ontanon, S., Ni, J., Sung, Y.-H., and Yang, Y. Longt5: Efficient text-to-text transformer for long sequences, 2022. URL https://arxiv. org/abs/2112.07916. Gur, I., Furuta, H., Huang, A., Safdari, M., Matsuo, Y., Eck, D., and Faust, A. real-world webagent with planning, long context understanding, and program synthesis, 2024. URL https://arxiv.org/abs/2307.12856. Gutierrez, B. J., Shu, Y., Gu, Y., Yasunaga, M., and Su, Y. Hipporag: Neurobiologically inspired long-term memory for large language models, 2025. URL https://arxiv. org/abs/2405.14831. Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? 2024. Hu, Y., Huang, Q., Tao, M., Zhang, C., and Feng, Y. Can perplexity reflect large language models ability in long text understanding? arXiv preprint arXiv:2405.06105, 2024a. Hu, Z., Liu, Y., Zhao, J., Wang, S., Wang, Y., Shen, W., Gu, Q., Luu, A. T., Ng, S.-K., Jiang, Z., et al. Longrecipe: Recipe for efficient long context generalization in large arXiv preprint arXiv:2409.00509, language models. 2024b. Kamradt, G. Needle in haystack - pressure testing llms, 2023. URL https://github.com/gkamradt/LLMTest NeedleInAHaystack. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. URL https://arxiv.org/ abs/2006.16236. Lee, J., Chen, A., Dai, Z., Dua, D., Sachan, D. S., Boratko, M., Luan, Y., Arnold, S. M., Perot, V., Dalmia, S., et al. Can long-context language models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121, 2024a. Lee, K.-H., Chen, X., Furuta, H., Canny, J., and Fischer, I. human-inspired reading agent with gist memory of very long contexts, 2024b. URL https://arxiv.org/ abs/2402.09727. Li, M., Zhang, S., Liu, Y., and Chen, K. Needlebench: Can llms do retrieval and reasoning in 1 million context window?, 2024a. URL https://arxiv.org/abs/2407. 11963. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Li, S., He, Y., Guo, H., Bu, X., Bai, G., Liu, J., Liu, J., Qu, X., Li, Y., Ouyang, W., Su, W., and Zheng, B. Graphreader: Building graph-based agent to enhance long-context abilities of large language models, 2024b. URL https://arxiv.org/abs/2406.14550. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., Abend, O., Alon, R., Asida, T., Bergman, A., Glozman, R., Gokhman, M., Manevich, A., Ratner, N., Rozen, N., Shwartz, E., Zusman, M., and Shoham, Y. Jamba: hybrid transformer-mamba language model, 2024. URL https://arxiv.org/abs/2403.19887. Lin, Z., Miao, Y., Zhang, Q., Yang, F., Zhu, Y., Li, C., Maleki, S., Cao, X., Shang, N., Yang, Y., Xu, W., Yang, M., Zhang, L., and Zhou, L. nnscaler: Constraint-guided parallelization plan generation for deep learning training. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 347363, 2024. 10 LongRoPE2: Near-Lossless LLM Context Window Scaling Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023. LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degration, 2023. https://www.reddit.com/r/LocalLLaMA/ URL comments/14lz7j5/ntkaware scaled rope allows llama models to have/. Lozhkov, A., Ben Allal, L., von Werra, L., and Wolf, T. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Luo, K., Liu, Z., Xiao, S., and Liu, K. Bge landmark embedding: chunking-free embedding method for retrieval augmented long-context large language models, 2024. URL https://arxiv.org/abs/2402.11573. Men, X., Xu, M., Wang, B., Zhang, Q., Lin, H., Han, X., and Chen, W. Base of rope bounds context length, 2024a. URL https://arxiv.org/abs/2405.14591. Men, X., Xu, M., Wang, B., Zhang, Q., Lin, H., Han, X., and Chen, W. Base of rope bounds context length. arXiv preprint arXiv:2405.14591, 2024b. Wang, H., Liu, Q., Du, C., Zhu, T., Du, C., Kawaguchi, K., and Pang, T. When precision meets position: Bfloat16 breaks down rope in long-context training. arXiv preprint arXiv:2411.13476, 2024. Weber, M., Fu, D. Y., Anthony, Q., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., Athiwaratkun, B., Chalamala, R., Chen, K., Ryabinin, M., Dao, T., Liang, P., Re, C., Rish, I., and Zhang, C. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., and Fan, Z. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. Yang, L., Xu, S., and Xiong, D. Dcis: Efficient length extrapolation of llms via divide-and-conquer scaling factor search. arXiv preprint arXiv:2412.18811, 2024b. Llama3.2: Meta. and 2024. llama-3-2-connect-2024-vision-edge-mobile-devices/. ai customizable models, https://ai.meta.com/blog/ Revolutionizing vision with open, URL edge Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardwareefficient training, 2024c. URL https://arxiv.org/ abs/2312.06635. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Ren, L., Liu, Y., Lu, Y., Shen, Y., Liang, C., and Chen, W. Samba: Simple hybrid state space models for efficient unlimited context language modeling, 2024. URL https: //arxiv.org/abs/2406.07522. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33: 75377547, 2020. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/blog/ qwen2.5/. Yu, A., Nigmetov, A., Morozov, D., Mahoney, M. W., and Erichson, N. B. Robustifying state-space models for long sequences via approximate diagonalization. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=DjeQ39QoLQ. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big bird: TransIn Larochelle, H., formers for longer sequences. Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1728317297. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper files/paper/2020/file/ c8512d142a2d849725f31a9a7a361ab9-Paper.pdf. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K., Han, X., Thai, Z. L., Wang, S., Liu, Z., et al. bench: Extending long context evaluation beyond 100k tokens. arXiv preprint arXiv:2402.13718, 2024a. 11 LongRoPE2: Near-Lossless LLM Context Window Scaling Zhang, Y., Sun, R., Chen, Y., Pfister, T., Zhang, R., and Arik, S. O. Chain of agents: Large language models collaborating on long-context tasks, 2024b. URL https: //arxiv.org/abs/2406.02818. Zhu, Q., Guo, D., Shao, Z., Yang, D., Wang, P., Xu, R., Wu, Y., Li, Y., Gao, H., Ma, S., et al. Deepseek-coderv2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. 12 LongRoPE2: Near-Lossless LLM Context Window Scaling"
        },
        {
            "title": "A Related Works",
            "content": "In addition to methods based on RoPE rescaling, this section discusses related works of other approaches. RAG and Agent-based extension. Retrieval-Augmented Generation (RAG) approaches incorporate an external memory module to store and manage long past context, coupled with dynamic retrieval mechanisms to fetch task-relevant documents during inference (Jeong et al., 2024; Chan et al., 2024; Dong et al., 2024; Gutierrez et al., 2025; Luo et al., 2024). Agentbased methods, meanwhile, decompose long-context processing into iterative planning, summarization, and retrieval tasks, often employing multi-agent workflows: individual agents extract information from text segments, which are aggregated to bypass fixed context limits (Zhang et al., 2024b; Li et al., 2024b; Lee et al., 2024b), while others integrate specialized architectures (e.g., hierarchical attention) for direct long-text handling (Gur et al., 2024). Both directionsrelying on external modules or multi-step decompositionare complementary to our method. Efficient long-context modeling. Attention computation and memory costs grow quadratically with context length, prompting research into reducing these challenges through improved attention mechanisms and innovative model structures. Many methods leverage the sparsity of standard attention, reducing computation by focusing on local and auxiliary regions (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020; Guo et al., 2022), while others extend context length using fine-grained sparsity (Ding et al., 2023) or chunked attention (An et al., 2024). Linear attention approaches further lower complexity while achieving comparable performance, with additional optimization for hardware efficiency (Katharopoulos et al., 2020; Yang et al., 2024c). State-space models (SSMs) offer linear complexity for sequence modeling (Gu & Dao, 2024; Yu et al., 2024), and hybrid transformer-SSM architectures enhance foundational model capabilities (Lieber et al., 2024; Ren et al., 2024). Most of these approaches build upon RoPE, making them complementary to our approach."
        },
        {
            "title": "B Additional Experiments and Analysis",
            "content": "Additional details. For the rescaling factor search, we set population size of = 64, evolution iterations of 40, and mutation probability = 0.3. The searched rescaling factors are then applied with mixed context window training. To accelerate training and inference, we use FlashAttention-2 (Dao, 2023), which requires no modifications for mixed context window training or factor-switch-based inference (as illustrated in Fig. 10). Given that GPU memory and computation time increase exponentially with sequence length, fine-tuning long-context models presents significant challenges. To address this, we utilize nnScaler (Lin et al., 2024), an efficient distributed training system for long-context LLMs, to reduce training costs. 10B tokens take approximately 39 hours for Phi3-mini and 54 hours for LLaMA3-8B on 64 A100 GPUs. During inference, the switch between rescaled and original RoPE is triggered when the combined length of the input context and generated tokens exceeds the pre-trained context window. Switching to rescaled RoPE for long-context inference requires one-time recalculation of the KV cache, potential limitation we leave for future work. Additional results on RULER and Needle-in-a-Haystack. Tables 8 and 9 show the detailed per-task accuracy of our extended LLMs on the RULER benchmark. Figures 7 and 8 provide comprehensive results for the needle-in-a-haystack tests. As observed, the YaRN method frequently fails to retrieve needles across Phi3-mini-3.8B, LLaMA3-8B, Meta-LLaMA3.1-8B and Meta-LLaMA3.1-8B-Instruct. Table 8. LongRoPE2-extended Phi3-mini (3.8B)-128k per-task performance on RULER. Length 4096 8192 16384 32768 65536 NIAH single1 NIAH single2 NIAH single3 NIAH multikey1 NIAH multikey2 NIAH multikey NIAH multivalue NIAH multiquery 100 100 100 100 100 100 100 100 100 100 100 98 99 100 99 99 99 95 91 90 87 86 85 96 93 88 86 71 40 97 97 82 57 32 18 97.75 89.5 91.25 87 67.75 56.75 97.75 93.75 89 78 69.25 59 VT CWE FEW 85.8 84 85 76.8 66.8 35.2 93.7 87.2 55.4 33.2 0.4 0.3 85.33 86 91.67 91.67 71.67 89.33 single-hop QA multi-hop QA 82 68 70 56 50 47 50 47 45 44 37 34 Avg. 90.41 87.34 83.33 76.51 65.37 58.81 The ablation study on search algorithm. In our work, we focus on searching for the real critical dimension and the scaling factors of higher dimensions beyond it. For the lower dimensions before the critical dimension, we directly apply NTK scaling without further optimization. To evaluate this design, we conduct an additional ablation study. For comparison, we also allowed the search to include lower dimensions. As shown in Table 10, while searching across all dimensions yields competitive results, it underperforms compared to our proposed method. possible reason is that limiting the search to higher dimensions significantly reduces the search space, enabling more effective discovery of the optimal solution. LongRoPE2: Near-Lossless LLM Context Window Scaling Table 9. LongRoPE2-extended LLaMA3-8B-128k per-task performance on RULER. Length 4096 8192 16384 32768 65536 131072 NIAH single1 NIAH single NIAH single3 NIAH multikey1 NIAH multikey2 NIAH multikey3 NIAH multivalue NIAH multiquery 100 100 100 100 100 100 100 100 100 100 100 100 99 100 100 100 100 99 100 100 99 99 98 96 100 100 100 98 98 91 100 100 98 100 95 99 99 95 98 95.75 96.5 99.75 99.75 98.25 96.25 99.75 97 VT CWE FEW 98.8 99.8 99.6 98.6 98.6 92. 98.5 95.9 86.8 63.9 33.6 9 96.33 91.33 96.33 95.67 80.33 85.33 single-hop QA multi-hop QA 79 74 69 72 62 56 60 58 58 55 52 Avg. 94.61 93.68 92.31 90.49 85.62 82.03 Table 10. Ablation study on the number of searched dimensions. Regular short tasks RULER Method MMLU MMLU Pro GSM8K 4k 8k 16k 32k 64k 128k LongRoPE2 (drcd and higher dims) LongRoPE2 (all dims) 70.07 69.96 40.30 39.84 73.62 74.83 90.41 90. 87.22 87.21 83.33 82.42 76.51 74.86 65.37 63.95 58.81 57.34 Base Model: Phi3-mini (3.8B) LongRoPE2 (drcd and higher dims) LongRoPE2 (all dims) 65.01 64.34 34.61 33.83 50.80 51.55 94.61 93.92 93.68 92. 92.31 91.41 90.49 89.30 85.62 83.11 82.03 78.07 Base Model: LLaMA3-8B Figure 7. Needle in Haystack full results for Phi3-mini (3.8B)-128k. 14 LongRoPE2: Near-Lossless LLM Context Window Scaling Figure 8. Needle in Haystack full results for LLaMA3-8B-128k. Figure 9. The RoPE rescaling factor distributions of NTK/YaRN adjusted based on the real critical dimension (i.e., YaRN-rcd, NTK-rcd). 15 LongRoPE2: Near-Lossless LLM Context Window Scaling Figure 10. The pseudocode for mixed context window training and inference."
        },
        {
            "title": "C Synthetic data sample",
            "content": "Synthetic search data based on PG19 book sample special magic number is hidden within the following text. Make sure to memorize it. will quiz you about the number afterwards. One of the special magic numbers for numerous-kite is: 6716097. The Old Testament of the King James Version of the Bible The First Book of Moses: Called Genesis 1:1 In the beginning God created the heaven and the earth. 1:2 And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters. ...... it be for witness between me and thee. 31:45 And Jacob took stone, and set it up for pillar. 31:46 And Jacob said unto his brethren, Gather stones; and they took stones, and made an heap: and they did eat there upon the heap. 31:47 And Laban called it Jegarsahadutha: but Jacob called it Galeed. ...... 3:39 Also in the fifteenth day of the seventh month, when ye have gathered in the fruit of the land, ye shall keep feast unto the LORD seven days: on the first day shall be sabbath, and on the eighth day shall be sabbath. 23:40 And ye shall take you on the first day the boughs of goodly trees, branches of palm trees, and the boughs of thick trees, and willows of the brook; and ye shall rejoice before the LORD your God seven days. What is the special magic number for numerous-kite mentioned in the provided text? The special magic number for numerous-kite mentioned in the provided text is"
        }
    ],
    "affiliations": [
        "Microsoft",
        "Shanghai Jiao Tong University",
        "Zhejiang University"
    ]
}