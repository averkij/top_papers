{
    "paper_title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes",
    "authors": [
        "Anthony Gosselin",
        "Ge Ya Luo",
        "Luis Lara",
        "Florian Golemo",
        "Derek Nowrouzezahrai",
        "Liam Paull",
        "Alexia Jolicoeur-Martineau",
        "Christopher Pal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods."
        },
        {
            "title": "Start",
            "content": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes Anthony Gosselin1,2, Ge Ya Luo1,3, Luis Lara1, Florian Golemo1, Derek Nowrouzezahrai1,4, Liam Paull1,3,5, Alexia Jolicoeur-Martineau6, Christopher Pal1,2,5 1Mila, 2Polytechnique Montréal, 3Université de Montréal, 4McGill University, 5CIFAR AI Chair, 6Samsung SAIL Montréal https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/"
        },
        {
            "title": "Abstract",
            "content": "Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity Improving traffic safety requires of accident events in most driving datasets. realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on human-evaluation of physical realism and video quality compared to prior diffusion-based methods. 5 2 0 2 0 3 ] . [ 1 7 2 2 0 0 . 6 0 5 2 : r Figure 1: Counterfactual Crash Generation: this diagram demonstrates the ability of our model to generate counterfactual crashes (middle-row: no crash, bottom-row: ego/car crash) while beginning from the initial frame and 3 bounding-boxes frames of the real video (top-row: the real car crash). Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Autonomous vehicle (AV) systems must be rigorously tested in wide range of driving situationsincluding rare and dangerous edge cases such as collisions to ensure safe deployment. Much of the current progress in perception, planning, and control for AVs has been driven by large-scale datasets collected in harmless, non-crash scenarios. However, realistic video data of car crashes remains extremely scarce, making it difficult to simulate, anticipate, or learn effectively from these critical events [42, 37, 24]. Prior work [30, 44, 49, 25, 20] has largely approached the challenge of crash scenario modeling in two main ways. On one hand, physics-based rendering approaches use game engines or physics simulators to model accident dynamics, but often fall short on visual realism, require expensive rendering pipelines, and demand costly human effort for environment and asset creation [8, 4, 26]. On the other hand, data-driven methods, such as generative models, rely on real-world footage, which is difficult to acquire in sufficient volume due to the infrequency and ethical complexity of crash events [14, 8, 5]. Moreover, most generative approaches focus on normal driving behavior, avoiding the complexity and unpredictability inherent in crash dynamics, where agent interactions, rare motion patterns, and semantic context all matter deeply [45, 49, 30, 25]. To address this gap, we introduce Ctrl-Crash, controllable video diffusion framework for generating realistic crash videos from single initial frame. Our method operates with inputs and outputs in pixel space, as opposed to using computer graphics primitives and explicit models of physics. Our approach can generate video conditioned on an initial image frame, spatial control signals consisting of bounding box sequences of cars and pedestrians, and semantic intent signals encoded as discrete crash types, enabling the generation of diverse crash outcomes. Through these conditioning signals, we can direct the narrative of crash, simulate plausible sequences of interactions, and explore counterfactual variants of given scene, answering the following types of questions with high quality generated video: How might the scene evolve differently under different agent trajectory or crash type? Ctrl-Crash builds on latent diffusion models [35] and classifier-free guidance [18], and we extend the latter to allow independently tunable guidance strengths for each control modality, making our system highly flexible at inference time. Our two-stage training procedure first finetunes pretrained Stable Video Diffusion (SVD) [2] model on in-the-wild ego-view accident videos, then trains ControlNet [47] adapter to handle conditioning in order to direct the video generation. By leveraging this data-driven framework, our model can generate controllable crash videos that are visually realistic, semantically diverse, and behaviorally plausible. We see this work as step toward not only improving the diversity and coverage of safety-critical AV testing, but also enabling counterfactual safety reasoning: the ability to simulate alternate outcomes from identical initial conditions, and better understand the causality of crashes. Our contributions: 1. We introduce Ctrl-Crash, fully data-driven generative framework for realistic and controllable car crash video generation. Our method obtains state-of-the-art performance compared to prior diffusion-based car accident video generation methods in quantitative (e.g., FVD) and qualitative analysis (human-evaluation of physical realism and video quality). 2. Our approach can generate plausible and diverse crash outcomes from the same initial frame and initial bounding boxes, enabling counterfactual video simulation for safety-critical reasoning. 3. We develop data-processing pipeline to filter driving videos and extract bounding box trajectories of road users. Using it, we release curated extension of the MM-AU dataset [10] (with bounding box annotations), as well as held-out test sets from Car Crash Dataset RUSSIA [38] (with computed boxes) and BDD100k [46] (with existing boxes), along with tools to support research in crash simulation and controllable video generation."
        },
        {
            "title": "2 Related Work",
            "content": "Video Diffusion Models. Diffusion models [40, 39, 17] have emerged as powerful paradigm for generative modeling, particularly in the domain of image synthesis. They operate by learning to reverse gradual noising process applied to data, generating high-fidelity samples through iterative 2 denoising. Recent advances have extended these techniques to the video domain, where temporal consistency and spatial coherence are critical [19, 16, 2]. Given the noise-level t, and task-specific conditions c, the diffusion loss is the following: = Ez0,t,ct,cf,ϵN (0,1) (cid:104) ϵ ϵθ(xt, t, c))2 (cid:105) , (1) where xt = αtx0 + 1 αtϵ, ϵ (0, 1), and αt at [1, ] controls the diffusion schedule. Latent video diffusion models (LVDMs) [16, 2] address the computational challenges of highresolution video generation by operating in compressed latent space. This enables the generation of long, high-quality video sequences from compact representations. Specifically, stable video diffusion (SVD) [2], variant of LVDM, leverages UNet-based denoiser trained on video latents conditioned on an initial frame, making it suitable for tasks like image-to-video generation and temporal extension. Our work builds on this foundation by fine-tuning pretrained SVD model on large curated car crash dataset and giving it additional controllability mechanisms to make it well-suited for generating complex driving scenes and crash dynamics. Controllable Generative Models. Recent progress in generative modeling has emphasized not only fidelity, but also controllability the ability to guide outputs through structured input signals. In image generation, this includes control via text prompts, sketches, bounding boxes, semantic maps, and more. In diffusion models, Classifier-Free Guidance (CFG) [18] and ControlNet [47] have been introduced as effective methods to allow adaptable conditioning while preserving high-quality generation. Classifier-Free Guidance is widely-used technique in diffusion models to improve conditional generation by combining conditional and unconditional predictions, scaled to control how strongly the model follows the conditioning input. It involves jointly training the diffusion model for conditional and unconditional denoising by randomly setting the conditioning to null value = during training. During inference, the denoising prediction is interpolated between conditional noise estimate ϵθ(xt, c) and an unconditional noise estimate ϵθ(xt, ), scaled by guidance factor γ to obtain the modified score estimate: ˆϵθ(xt, c) = ϵθ(xt, ) + γ (ϵθ(xt, c) ϵθ(xt, )) . (2) Vision models, such as InstructPix2Pix [22], use textual or mixed-mode conditioning with CFG to manipulate generation intent and content mid-sampling. ControlNet [47] introduced an effective approach for injecting spatial control into diffusion models by adding parallel, trainable network that processes conditioning signals and modulates the main denoising backbone. It works by branching off from intermediate layers in the main U-Net and processing control input in parallel. Its outputs are then added to the original U-Net features before the denoising step, effectively steering the generation without retraining the base model Our method advances prior research by combining semantic control (crash type) and spatial control (bounding box) through CFG and via the ControlNet adapter. This integration enables both precise descriptive control and generative reasoning about critical driving outcomes. Car Crash Simulation and AV Safety. Video diffusion models offer compelling solution for car crash simulation, as they can simulate both the visual realism and behavioral dynamics of complex driving scenes with very little user effort. Recent car video generation models [44, 49, 30, 1] focus on structured driving video generation using control signals like bounding boxes or text prompts. These models generate temporally consistent driving scenes and support the synthesis of structured traffic interactions. However, they are typically limited to non-crash scenarios or coarse control. Physics-based crash simulations [8, 13] are crucial for evaluating autonomous vehicle safety [9, 7, 34, 43]. Due to the scarcity and ethical challenges of real-world crash data, there is growing interest in generating synthetic safety-critical scenarios [43, 21]. These simulations provide control and physical realism but often lack authenticity and broad applicability. In contrast, our work focuses on rare, adversarial cases that challenge perception and planning systems in real-world scenarios. To address the data challenge, we propose novel method of data processing. CtRL-Sim [36] uses return conditioned offline reinforcement learning to model AV agent control from real-world trajectories. By applying negative exponential tilt, it simulates realistic crash dynamics as agents pursue collision-related negative rewards. Operating at the data-structure level, it 3 Figure 2: Model architecture: Ctrl-Crash treats Bounding Boxes (BBs) as images. Both BBs and images frames as fed to VAE image encoder. The crash type and BB embeddings are fed to ControlNet. The images embedding after adding noise (xt), the noise level (t), and the ControlNet intermediate outputs (c) are fed to the Stable Video Diffusion (SVD) model to obtain the predicted noise ϵθ(xt, t, c). Clip embeddings are computed by passing the first image for each video through pretrained CLIP encoder. These CLIP embeddings are then fed to the ControlNet and SVD models. on The diffusion process is solved over multiple steps using Classifier-Free Guidance in the latent space and then decoded back to images using the VAE image decoder. See Section 3 for details. predicts 2D bounding box trajectories in birds eye views using simplified bicycle models.\" While useful for generating synthetic crash data, real-world crash dynamics are more complex than those produced by this approach. While most prior work has focus on general driving scenes, notable examples of prior work tackling realistic car crash generation using real videos include: DrivingGen [15], which generates crash videos from textual accident reports, AVD2 [27] and OAVD [10] which also generate videos of car crashes, but their main focus is car crash video description rather than generation. These methods are designed to stress-test AV policies and enhance safety coverage beyond what real-world datasets offer. Our work builds on this vision by proposing controllable video diffusion model that can generate crash outcomes directly from initial conditions and desired outcomes. Unlike physics simulators, CtrlCrash supports semantic control (e.g., crash type) and spatial trajectory specification (e.g., bounding boxes), enabling targeted stress testing of vision-based AV stacks. Moreover, our method supports counterfactual safety reasoning, allowing one to explore how small changes in agent behavior or intent could lead to dramatically different outcomesa critical capability for understanding near-miss events and decision boundary failures. Importantly, our method has much higher visual and motion fidelity than prior works (See Section 4)."
        },
        {
            "title": "3 Our Method: Ctrl-Crash",
            "content": "In this section, we present Ctrl-Crash, our controllable video diffusion framework for generating crash scenarios from single image. We describe the overall architecture 3.1, data processing pipeline 3.2, conditioning mechanisms 3.3, training strategy 3.4, and our extension of classifier-free guidance for fine-grained control 3.5. 3.1 Overview We propose Ctrl-Crash (Figure 2), controllable video diffusion framework designed to generate realistic car crash scenarios from single initial frame, guided by both spatial and semantic control signals. Ctrl-Crash builds on Ctrl-V [30], framework for generating videos from rendered bounding 4 box trajectories, by extending its capabilities to crash-specific scenarios, offering richer control and greater flexibility. Specifically, we incorporate new semantic control signal representing crash type and introduce refined training procedure to handle partial and noisy conditioning. Our method follows two-stage training pipeline. In the first stage, we fine-tune Stable Video Diffusion (SVD) model on the MM-AU [10] crash video dataset to improve its ability to synthesize dynamic and physically plausible driving scenes. In the second stage, we train ControlNet module to inject conditioning information in two forms: (1) bounding box sequences representing road user motion across time, and (2) discrete crash type labels encoding high-level semantic intent. To ensure generalization to incomplete or noisy control, we introduce curriculum-based random masking strategy that progressively masks out parts of the control inputs during training. Masked bounding box frames are replaced by learnable null embedding that preserves scene plausibility. We further extend classifier-free guidance to support independent scaling of each control modality, enabling nuanced and flexible control at inference. Unconditional noise predictions are obtained from the pretrained base model for improved generation diversity and stability. Ctrl-Crash supports three task settings, each enabled by varying the available control signals: (1) Crash ReconstructionGiven an initial image, full bounding box sequence, and crash type, the model reconstructs consistent video combining the visual context of the initial frame with agent motion derived from the bounding boxes. (2) Crash PredictionGiven the initial frame and only few initial bounding box frames (e.g., 09), the model predicts the future motion of agents in way that aligns with the target crash type. (3) Crash CounterfactualsExtending the prediction task, this mode varies the crash type signal while keeping other inputs fixed, enabling the generation of multiple plausible outcomes for the same scenesupporting counterfactual safety reasoning. 3.2 Data Preparation The data processing and preparation of crash data presented here is an essential element of our approach, allowing us to create control structures from diverse selection of naturally occurring crashes involving cars having only dashboard camera. Video Processing. For training, we use the MM-AU dataset [10], large-scale collection of dashcam crash videos collected from online sources. To ensure high quality we curate this dataset by applying series of filtering steps. We remove low-resolution or blocky videos using FFT-based heuristics (see Appendix B), detecting and excluding shot changes with PySceneDetect [6], and standardizing clips to 25-frame segments at 6 fps and 512 320 resolution. Additionally, we exclude scenes involving visible humans to avoid generating violent content. For more details on the precise processing steps, please refer to Appendix C. After the filtering steps, we retain approximately 7,500 videos from the original 11,727 videos. We split these videos into training set and held-out test set by randomly sampling by accident type categories with 90/10 ratio. See Appendix for information on the dataset. Bounding Box Extraction Pipeline. To obtain reliable bounding box annotations for all road users, we design hybrid pipeline that combines detection and segmentation models. For detection, we use YOLOv8 [23] for frame-by-frame object detection. YOLO provides class-specific bounding boxes with high confidence and speed. For tracking, we use SAM2 [33] to produce instance-level masks and reliable tracking particularly when objects get occluded or deformed, common in crash videos. This combined approach yields temporally aligned bounding boxes across all video frames. Importantly, it supports agents that enter or exit the scene dynamicallycritical for realistic dynamic driving scenarios. full breakdown of the bounding-box annotation pipeline is provided in Appendix D. We collect bounding-box annotations for all road users for the MM-AU dataset and the RUSSIA Car Crash Dataset [38]. These annotations along with the processed video frames are made publicly available on our project page. 3.3 Conditioning Signals Initial Image Frame (Contextual Conditioning): The initial image provides the visual grounding for generation, capturing the appearance, layout, and environment of the driving scene before any dynamic event unfolds. It is encoded using pretrained visual encoder (VAE) and fed into the base diffusion model as context. The initial image strongly influences scene appearance and plausibility, and serves as the starting point for generating temporally coherent crash evolutions. Bounding Box Conditioning (Spatial Control): Road users are represented with 2D bounding boxes rasterized 5 into RGB control frames. These provide spatial guidance regarding each objects motion, size, and location over time. Each bounding box is color-coded to encode both its unique track ID (via fill color) and its object class (via border color), enabling the model to distinguish agents across frames. These control frames are encoded using the same pretrained VAE as the initial image and processed by ControlNet and injected into the denoising process, directing the motion of agents in the generated video. See Appendix for visualizations and details of the encoding scheme. Crash Type Conditioning (Semantic Control): Crash types are represented by discrete class label from five categories: (0) none, (1) ego-only, (2) ego/vehicle, (3) vehicle-only, and (4) vehicle/vehicle. These indicate which agents are involved in the crash, with, for example, vehicle/vehicle describing crash between two non-ego agents. The crash type index is embedded and projected into the cross-attention layers of the ControlNet encoder, allowing the model to generate outcomes consistent with high-level semantic intent. 3.4 Training Pipeline Our training strategy is divided into two phases. In the first stage, we fine-tune the Stable Video Diffusion (SVD) model on crash-heavy video data from our preprocessed version of the MM-AU dataset (see Section 3.2). The setup is an image-to-video task with an MSE loss in latent space. This improves the models ability to generate plausible driving videos, including crashes, though without explicit controllability. In the second stage, we freeze the fine-tuned base model and train ControlNet adapter module to introduce conditional controllability. This stage allows us to direct the generation using spatial and semantic control signalsnamely, bounding box frames and crash type identifiers. More training and implementation details are provided in Appendix G. Conditioning Masking. To promote robustness and controllability at inference time, we apply randomized masking of conditioning signals during training. This enables the model to generalize across varying levels of supervision and supports flexible classifier-free guidance at inference. For bounding box conditioning, we introduce temporal dropout strategy: at each training step, timestep [0, ] is sampled uniformly, and all bounding box frames from timestep onward are masked using learnable null embedding. Unlike zero-valued tensors, the null embedding prevents the model from misinterpreting masked frames as scenes with no agents present. This teaches the model to perform plausibly even when only partial agent trajectory information is available. We adopt curriculum schedule: bounding box masking is applied with 50% probability for the first 21k training steps, and with 100% probability thereafter (up to 31k steps), encouraging early learning from dense supervision before transitioning to partial conditioning. For semantic signals (crash type and initial image), we independently mask in the following way: with 10% probability, only the crash type is masked; with 10% probability, only the initial image is masked; and with 10% probability: both are masked. This helps prevent the model from collapsing onto any single conditioning signal and allows classifier-free guidance to function reliably across different control configurations. 3.5 Classifier-Free Guidance with Multi-Condition Factorization Our model supports three conditioning modalities: the initial image cI , the bounding box frames cB, and the crash type cT . To enable independent control over the influence of each signal during inference, we adopt multi-condition extension of classifier-free guidance (CFG) from Equation 2, following formulations inspired by [28] and [22]. In standard CFG, both conditional and unconditional noise predictions are produced by single model ϵθ(xt, c) and ϵθ(xt, ), respectively. However, as noted by recent work [32], this setup can lead to poor unconditional priors and hinder conditional fidelity. To address this, we use two separate networks: the original pretrained base model ϵϕ to compute the unconditional predictions, and the fine-tuned Ctrl-Crash model ϵθ to compute the conditional ones. We define our multi-condition CFG formulation as follows: ˆϵθ,ϕ(xt, cI , cB, cT ) = ϵϕ(xt, cI , , ) + γB [ϵθ(xt, cI , cB, ) ϵϕ(xt, cI , , )] + γT [ϵθ(xt, cI , cB, cT ) ϵθ(xt, cI , cB, )] (3) Here, the first term ϵϕ(xt, cI , , ) represents the unconditional noise prediction, grounded on the initial image cI alone. We use the pretrained Stable Video Diffusion (SVD) model for this term, 6 rather than computing fully unconditional prediction (i.e., with cI removed), as we found the latter introduces substantial inference cost and minimal performance gain. The remaining terms use the fine-tuned model ϵθ to compute conditional noise estimates. The scalar coefficients γB and γT control the relative guidance strength for the bounding box and crash type conditions, respectively. This factorized formulation allows us to modulate each conditioning channel independently at inference time, enabling fine-grained and interpretable control over the generated videos spatial and semantic properties."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Quantitative Evaluation We evaluate the generation quality of Ctrl-Crash across two main settings: general crash video generation in Table 1 and the impact of varying control signal strength in the prediction and reconstruction tasks in Table 2. All generated videos are 25 frames long at resolution of 512 320, and metrics are computed over 200 samples unless otherwise stated. We evaluate generation quality using several video and frame-level metrics. Fréchet Video Distance (FVD) [41] measures the similarity between the distributions of generated and real videos by analyzing video embeddings in I3D space under the assumption of Gaussian distributions; lower FVD values indicate closer resemblance to real data. JEDi [29] is novel metric designed as an alternative to FVD by addressing its limitations through relaxing the assumption of Gaussian-distributed video feature embeddings, modifying the embedding space, and enabling convergence with smaller sample size. In addition to these distributional-based video metrics, we report LPIPS (Learned Perceptual Image Patch Similarity) [48], SSIM (Structural Similarity Index), and PSNR (Peak Signal-to-Noise Ratio) in Table 2, which evaluate frame-level fidelity and perceptual closeness relative to ground truth. These metrics provide complementary insights into how well the generated videos preserve appearance, structure, and temporal dynamics. Table 1: Comparison of accident video generation quality across diffusion-based methods. We report FVD and JEDi scores ( lower is better). For fair comparison, we compute the ground truth distribution from set of 500 randomly sampled MM-AU [10] validation videos, to accommodate for methods without GT alignment (e.g., AVD2) (see Appendix for more details). Scores marked with * are taken directly from the original papers and may not be strictly comparable due to differences in evaluation setup. Method Conditions JEDi[29] FVD[41] OAVD DrivingGen img + bbox + text img + bbox + text SVD base AVD2 Ctrl-V Ctrl-Crash (Ours) img img + text img + all bboxes img + all bboxes + GT crash type 5238* 978.0* 1420 1321 517.1 449.5 - - 3.628 2.029 0.2910 0.1219 As shown in Table 1, Ctrl-Crash achieves the best results across both FVD and JEDi, indicating strong alignment with real crash dynamics and superior video quality. The SVD Base model, which serves as the foundation for Ctrl-Crash, performs poorly, as it was not trained for driving or crashrelated content. Ctrl-V, while similar in architecture, lacks crash-specific training data and semantic control, leading to notable quality degradation near the crash event. AVD2 performs moderately well but exhibits inconsistent visual quality and weaker temporal coherence compared to Ctrl-Crash, as confirmed by FDV and JEDi. These results highlight the importance of both targeted training and structured control for crash simulation. Effect of Bounding Box Conditioning We also study the impact of varying the number of bounding box frames used as conditioning for Ctrl-Crash, in Table 2. Specifically, we compare Crash Prediction (partial bbox inputs) and Crash Reconstruction (full bbox inputs) over 25-frame outputs. As shown in Table 2, generation quality improves consistently with the number of provided bounding box frames. This trend is visible across both distributional metrics (FVD, JEDi) and frame-level scores (LPIPS, Table 2: Effect of bounding box conditioning on crash video prediction quality. We evaluate Ctrl-Crash on the Crash Prediction task by varying the number of initial bounding box frames provided as input (out of 25 total frames). #Bboxes FVD[41] JEDi[29] LPIPS[48] SSIM PSNR Method"
        },
        {
            "title": "Ours\nOurs\nOurs\nOurs",
            "content": "0 (none) 3 9 25 (all) 422.1 375.7 353.3 323.9 0.3155 0.2949 0.2160 0.1219 0.3856 0.3594 0.3392 0.3113 0.5188 0.5434 0.5614 0.5836 16.57 17.27 17.83 18. Table 3: Effect of crash type conditioning on crash video generation quality. We evaluate CtrlCrash on the Crash Counterfactuals task by varying the crash type conditioning and nothing else. For each case, 200 videos were generated using the same initial image and three bounding box frames as initial context, but with different desired crash types. By comparing with the results from using the GT crash type (first line, taken from Table 2), we observe that the generated video quality is slightly worse or on par in almost all cases, suggesting the model can generate plausible alternatives while remaining visually close to the ground-truth video and while maintaining good video quality. Method #Bboxes Crash Type FVD[41] JEDi[29] Ours Ours Ours Ours Ours Ours 3 3 3 3 3 3 GT crash type 0 - no crash 1 - ego-only 2 - ego/vehicle 3 - vehicle-only 4 - vehicle/vehicle 375. 400.9 379.5 372.9 398.1 383.4 0.2949 0.4514 0.3091 0.3001 0.3856 0.3416 SSIM, PSNR), and supports the hypothesis that denser spatial constraints lead to easier prediction tasks and more stable outputs. The results validate that Ctrl-Crash gracefully interpolates between unconditional prediction and fully supervised reconstruction. Across all benchmarks, Ctrl-Crash delivers significant improvements over prior diffusion-based crash generation models. It handles both unconstrained and highly conditioned inputs, demonstrating its utility for generating diverse crash outcomes and precise reconstructions. In the next section, we complement these quantitative results with user study and qualitative visualizations. 4.2 Qualitative Evaluation (a) Visual preference comparison in generated videos. (b) Realism preference comparison in physical appearance. Figure 3: User study results comparing generated videos from AVD2, DrivingGen and Ctrl-Crash with 40 participants across 5 crash types: Participants exhibit strong preference for Ctrl-Crash generated videos, citing superior visual quality and physical realism. See Appendix for more details. 8 We conducted brief user study with = 40 participants who were asked to rank = 3 videos (from Ctrl-Crash, AVD2, and DrivingGen) across 5 different crash scenarios each. The participants consist of students from our lab who are not associated with the project in any way. No further demographic data was collected. The users were asked to rank the 3 videos in each of the 5 questions by (a) physical plausibility and (b) visual fidelity from best to worst. The users were required to choose best/medium/worst video in each question and (visual/physical) category. We used the non-parametric Friedman test [12] to determine with 0.01 that there is method that is consistenly ranked higher than the others. We further used the Nemenyi Post-Hoc analysis [31] to find that with 0.01, our method consistenly outperforms both AVD2 and DrivingGen in both physical realism and visual fidelity (see Figure 3). Figure 4: Qualitative results comparing AVD2, DrivingGen, Ctrl-V, and Ctrl-Crash. The crash generated by AVD2 is visually shaky, with scenes that often lack consistency. Driving-Gen also produces low-quality and choppy videos. While Ctrl-V achieves good visual quality, it fails to generate realistic crash events. In contrast, Ctrl-Crash outperforms all baselines in both visual fidelity and scene consistency, while accurately modeling crash dynamics. Additional video demonstrations are available on the project page, and in Appendix A.1."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced Ctrl-Crash, controllable video diffusion framework that generates realistic car crash scenarios from single frame, achieves state-of-the-art performance among diffusion-based methods, and enables counterfactual reasoning by varying spatial and semantic control inputs. To support training and evaluation, we also developed processing pipeline for extracting bounding boxes from crash videos and released curated, annotated versions of MM-AU, RussiaCrash, and BDD100k to facilitate future research in crash simulation and generative modeling. Despite its strong performance, our approach has several limitations, which motivates future work in this direction. Counterfactual outcomes can be hard to generate when initial scene conditions conflict with the desired crash type. The model also relies heavily on bounding boxes, making it sensitive to tracking errorsespecially in fully conditioned reconstruction. With no bounding boxes conditioning, motion direction can be ambiguous, and 2D boxes struggle to capture rotation or orientation, limiting realism in behaviors like spinoutsfuture work may explore 3D bounding boxes or richer trajectory representations to overcome this. We envision Ctrl-Crash as foundational tool for advancing the development of controllable generative models in safety-critical autonomous driving research."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Samsung, the IVADO and the Canada First Research Excellence Fund (CFREF) / Apogée Funds, the Canada CIFAR AI Chairs Program, and the NSERC Discovery Grants program for financial support. We also thank Mila - the Quebec AI Institute for compute resources."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, and Opehttps://openai.com/index/ nAI. Video generation models as world simulators. video-generation-models-as-world-simulators/, 2024. Technical report. [4] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2, 2020. [5] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. CoRR, abs/1903.11027, 2019. [6] Brandon Castellano. PySceneDetect. [7] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video simulation via geometry-aware composition for self-driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 72307240, 2021. [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 116, 2017. [9] Sylvain Dupuy, Arjan Egges, Vincent Legendre, and Pierre Nugues. Generating 3d simulation of car accident from written description in natural language: The carsim system. arXiv preprint cs/0105023, 2001. [10] Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen Lv, Jianru Xue, and Tat-Seng Chua. Abductive ego-view accident video understanding for safe driving perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2203022040, 2024. [11] Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, and Hongkai Yu. Dada: Driver attention prediction in driving accident scenarios, 2023. [12] Milton Friedman. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. Journal of the american statistical association, 32(200):675701, 1937. [13] Joseph Gabbard, Missie Smith, Kyle Tanous, Hyungil Kim, and Bryan Jonas. Ar drivesim: An immersive driving simulator for augmented reality head-up display research. Frontiers in Robotics and AI, 6:98, 2019. [14] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. [15] Zipeng Guo, Yuchen Zhou, and Chao Gou. Drivinggen: Efficient safety-critical driving video generation with latent diffusion models. In 2024 IEEE International Conference on Multimedia and Expo (ICME), pages 16, 2024. 10 [16] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation, 2023. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. [19] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 86338646. Curran Associates, Inc., 2022. [20] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving, 2023. [21] Zherui Huang, Xing Gao, Guanjie Zheng, Licheng Wen, Xuemeng Yang, and Xiao Sun. Safety-critical traffic simulation with adversarial transfer of driving intentions. arXiv preprint arXiv:2503.05180, 2025. [22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks, 2018. [23] Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, January 2023. [24] Hoon Kim, Kangwook Lee, Gyeongjo Hwang, and Changho Suh. Crash To Not Crash: Learn to identify dangerous vehicles using simulator. In aaai, 2019. [25] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards controllable high-quality neural simulation, 2021. [26] Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, MohammadAli Nikouei Mahani, Nassir Navab, Benjamin Busam, and Federico Tombari. 3D-VField: Adversarial augmentation of point clouds for domain generalization in 3D object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1729517304, 2022. [27] Cheng Li, Keyuan Zhou, Tong Liu, Yu Wang, Mingqiao Zhuang, Huan-ang Gao, Bu Jin, and Hao Zhao. Avd2: Accident video diffusion for accident video description. arXiv preprint arXiv:2502.14801, 2025. [28] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual generation with composable diffusion models, 2023. [29] Ge Ya Luo, Gian Mario Favero, ZhiHao Luo, Alexia Jolicoeur-Martineau, and Christopher Pal. Beyond FVD: An enhanced evaluation metrics for video generation distribution quality. In The Thirteenth International Conference on Learning Representations, 2025. [30] Ge Ya Luo, ZhiHao Luo, Anthony Gosselin, Alexia Jolicoeur-Martineau, and Christopher Pal. Ctrl-v: Higher fidelity autonomous vehicle video generation with bounding-box controlled object motion. Transactions on Machine Learning Research, 2025. [31] Peter Bjorn Nemenyi. Distribution-free multiple comparisons. Princeton University, 1963. [32] Prin Phunyaphibarn, Phillip Y. Lee, Jaihoon Kim, and Minhyuk Sung. Unconditional priors matter! improving conditional generation of fine-tuned diffusion models, 2025. [33] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. [34] Davis Rempe, Jonah Philion, Leonidas Guibas, Sanja Fidler, and Or Litany. Generating useful accident-prone driving scenarios via learned traffic prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1730517315, 2022. 11 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [36] Luke Rowe, Roger Girgis, Anthony Gosselin, Bruno Carrez, Florian Golemo, Felix Heide, Liam Paull, and Christopher Pal. Ctrl-sim: Reactive and controllable driving agents with offline reinforcement learning. In CVPR, 2025. [37] Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, and Lars Andersson. VIENA2: Driving Anticipation Dataset. arXiv e-prints, page arXiv:1810.09044, October 2018. [38] Sivoha. Car crash dataset russia 20222023. https://www.kaggle.com/datasets/ sivoha/car-crash-dataset-russia-2022-2023, 2023. Accessed: 2025-05-15. [39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 22562265, Lille, France, 0709 Jul 2015. PMLR. [40] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [41] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation, 2019. [42] Tianqi Wang, Sukmin Kim, Wenxuan Ji, Enze Xie, Chongjian Ge, Junsong Chen, Zhenguo Li, and Ping Luo. Deepaccident: motion and accident prediction benchmark for v2x autonomous driving, 2023. [43] Tianqi Wang, Sukmin Kim, Ji Wenxuan, Enze Xie, Chongjian Ge, Junsong Chen, Zhenguo Li, and Ping Luo. Deepaccident: motion and accident prediction benchmark for v2x autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 55995606, 2024. [44] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-drive world models for autonomous driving. In European Conference on Computer Vision, pages 5572. Springer, 2024. [45] Wei Wu, Xi Guo, Weixuan Tang, Tingxuan Huang, Chiyu Wang, Dongyue Chen, and Chenjing Ding. Drivescape: Towards high-resolution controllable multi-view driving video generation, 2024. [46] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: diverse driving dataset for heterogeneous multitask learning, 2020. [47] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. [48] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. [49] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1041210420, 2025."
        },
        {
            "title": "A Additional Results",
            "content": "A.1 Additional Qualitative Results Figure 5: Other diffusion-based video modelsincluding state-of-the-art approachesconsistently fail to generate plausible crash scenarios. Top row: Nvidia Cosmos-Predict1-7B-Text2World with the prompt \"On highway two cars collide at very fast speeds head-on\" produces highly implausible scene, where the car on the left suddenly starts to levitate from its rear-end and cloud of smoke resembling an explosion appears, followed by disjointed fragments of torn metal emerging from the ground that transforms in dark vehicle rushing towards the camera. Middle row: OpenAI Sora with the prompt \"At an intersection two cars collide with each other at full speed resulting in crash\" generates car that spins erratically, changes shape and direction, and produces visible artifactsculminating in pile of twisted metal and glass. Bottom row: Ctrl-V, conditioned on sequence of bounding boxes representing head-on collision, renders two extremely blurry cars approaching each other. One car vanishes entirely at the moment of impact, while the other gradually fades away. Despite partially capturing the intent of the prompts, all models fail to produce physically plausible and coherent crash, even when multiple samples are generated. Despite the seemingly high quality video generation results produced by the latest wave of generative models, they systematically fail to generate physically plausible crashes and tend break down, yielding highly implausible imagery. For example in Figure 5 we show frames from Nvidia Cosmos [1], OpenAI Sora [3], and Ctrl-V [30]. In contrast, through our approach of sourcing crash data, preprocessing and annotating sequences with bounding boxes, crash type conditioning and our stochastic conditioning approach, our model can much more reliably generate physically plausible crash imagery and collision dynamics. Other recent work [15][27] focus on car crash video generation, but lack the visual quality for convincing car crashes. As shown in the main paper, we conducted user survey that shows that CtrlCrash significantly outperforms these existing methods for visual fidelity and physical plausibility of generated car crashes. We show additional qualitative comparisons in Figure 6 and Figure 7, along with many samples generated from Ctrl-Crash in Figure 8, Figure 9, and Figure 10. 13 Figure 6: Qualitative comparison of \"rear-end crashes\" between different methods. For each method, we show 5 frames from the video along with either green check mark if there appears to have crash in the video otherwise red X. From top to bottom: SVD (stable-video-diffusion-img2vid) [2] prompted with the initial frame from rear-end crash video, we see some normal driving but very inconsistent lighting and color shades with visible distorsions. AVD2 [27], we see what appears to be rear-end crash with very distorted leading vehicle and background. DrivingGen [15], we see rear-end crash with leading vehicle that changes appearance every frame. Overall the video is very choppy with little temporal consistency. Cosmos (Cosmos-Predict1-7B-Video2World) [1], prompted with text suggesting rear-end crash and 9 initial images where car is rapidly approaching truck, the predicted frames show the car unrealistically shrinking as it approaches the truck without any signs of collision. Ctrl-V [30], prompted with sequence of bounding-boxes suggesting rear-end crash with leading car, we see the leading car keep its distance and not crash occurs. Ctrl-Crash (ours): prompted with the same bounding box sequence as Ctrl-V and with the discrete crash type \"ego/vehicle crash\", we see physically plausible rear-end collision with the ego vehicle visibly shaking from the impact. Visit our project page for animated video examples: https: //anthonygosselin.github.io/Ctrl-Crash-ProjectPage/ 14 Figure 7: Qualitative comparison of \"t-bone crashes\" between different methods. For each method, we show 6 frames from the video along with either green check mark if there appears to have crash in the video otherwise red X. From top to bottom: SVD (stable-video-diffusion-img2vid) [2] prompted with the initial frame from t-bone car crash video, we see blurry vehicle distorted motion blur as it drives in front of the ego vehicle without any collision. AVD2 [27], we can make out what seems to be t-bone crash with heavily distorted black car. There are many artifacts and temporal inconsistencies which makes the sequence of events hard to follow. DrivingGen [15], gray sedan drive in front of the ego vehicle progressively getting closer until it seems to collide with it. Motion is jerky and uneven between timesteps and the appearance of the gray car shapes almost every frame. Cosmos (Cosmos-Predict1-5B-Video2World) [1], prompted with creating t-bone crash and 9 initial frames showing car turn in front of the ego car, we see the leading car start to distort as the ego approaches it and then it shrivels and shrinks until it nearly disappears. Ctrl-V [30], prompted with sequence of bounding-boxes suggesting t-bone crash with car incoming from the left, we see car drive in from the left and then just passed the ego car without any collision. Ctrl-Crash (ours): prompted with the same bounding box sequence as Ctrl-V and with the discrete crash type \"ego/vehicle crash\", we see physically plausible t-bone collision with the black sedan incoming from the left. Visit our project page for animated video examples: https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/ 15 Figure 8: Ctrl-Crash qualitative results conditioned on an initial 9 bounding box frames (i.e., the first two frames of these sequences were conditioned on bounding box frames, but not the others). The animated videos for the examples presented above are provided in the supplementary material ZIP file and more video examples can be viewed at our project page: https://anthonygosselin. github.io/Ctrl-Crash-ProjectPage/ 16 Figure 9: Ctrl-Crash qualitative results conditioned on 25 (all) bounding box frames. The animated videos for the examples presented above are provided in the supplementary material ZIP file and more video examples can be viewed at our project page: https://anthonygosselin.github. io/Ctrl-Crash-ProjectPage/ 17 Figure 10: Counterfactual Crash Generation: this diagram demonstrates the ability of our model to generate counterfactual crashes while beginning from the identical initial frame. Top: ground truth accident between two vehicles other than the ego-vehicle, where the red car hits the rear of the blue car and spins into the lane in front of the ego-vehicle. Bottom: the model generates an alternative accident involving the ego-vehicle. In this alternative future the red car avoids the blue car but turns into the path of the ego-vehicle leading to the crash. A.2 Additional Quantitative Results: Ablations Table 4: Generated accident video quality compared to baseline ablations methods FVD JEDi LPIPS Conditions SSIM PSNR Method SVD base Ctrl-V Ctrl-Crash (Ours) img img + bbox img + bbox + action 1420 517.1 449.5 3.628 0.2910 0. 0.5800 0.3670 0.3113 0.3074 0.5372 0.5836 11.74 16.81 18.33 We present in Table 4 additional results to complement results given in Table 1 of the main paper. In addition to FVD [41] and JEDi [29] values, we present LPIPS [48], SSIM, and PSNR for Ctrl-Crash and its ablations. Ctrl-V is similar to Ctrl-Crash but has not been trained on crash data and was not designed with dynamic crash video generation in mind. SVD base is the base model Ctrl-Crash is derived from through finetuning. For Ctrl-Crash and Ctrl-V the full sequence of bounding boxes is given, so we are expecting reconstruction similar to the ground-truth crash. Across all metrics, we see that Ctrl-Crash performs the best at reconstructing plausible crashes. FFT-Based Filtering Heuristic In the context of video dataset curation, it is important to ensure that the frames are of genuine high quality and not artificially upscaled from lower resolutions. When dealing with in-the-wild dashcam footage from various online sources, it is often difficult to guarantee high-quality samples and many videos present compression or resizing artifacts that can dramatically affect the dataset quality. As first step in the video processing pipeline for this work (see Appendix C) we designed function to estimate which video samples may be originally of very low resolution using frequency analysis. The function is used as quality control step to automatically detect and potentially filter out such frames or videos. We define \"upscaling factor estimation\" function which is designed to estimate how much an image (a video frame) has been upscaled before being resized to standard resolution (e.g., 7201280). Upscaling refers to the process of enlarging an image, which can introduce artifacts and degrade quality. This function provides quantitative heuristic score indicating the likelihood and degree of upscaling, which can be used to filter out low-quality or artificially enlarged video samples in dataset. The present the procedure textually below and more formally in Algorithm 1. Here is step-by-step breakdown of the process: 1. Image Loading Load video frame in grayscale to simplify the analysis by focusing on structural content rather than color information. 18 2. Fourier Transform Compute the 2D Fast Fourier Transform (FFT) of the image, which converts the spatial domain image into the frequency domain. The result is then shifted so that the low-frequency components are centered. 3. Magnitude Spectrum Calculate the magnitude (absolute value) of the frequency components, representing the strength of different frequency components in the image. 4. High-Frequency Energy Calculation Define circular region at the center of the frequency spectrum (corresponding to low frequencies) and create mask to exclude this central region, thereby isolating the high-frequency components. The sum of the magnitudes outside the central region (i.e., the high-frequency energy) is computed. 5. Normalization Normalize the high-frequency energy by the total number of pixels, yielding an energy score that is independent of image size. 6. Upsizing Factor Estimation Compute an \"upsizing factor\" as the inverse of (1 + energy score). The rationale is that upscaled images tend to have lower high-frequency energy (since upscaling smooths out details), so lower energy score indicates more upscaling. The formula ensures that the factor decreases as the high-frequency energy decreases. 7. Return Value Return the upsizing factor, which can be interpreted as proxy for the degree of upscaling: Lower values indicate more likely and/or more severe upscaling. Higher values indicate less or no upscaling (i.e., more genuine high-frequency detail). Algorithm 1 Estimate Upscaling Factor from Image 1: procedure ESTIMATEUPSIZINGFACTOR(image_path) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end procedure Load image in grayscale: cv2.imread(image_path, GRAYSCALE) Compute 2D FFT and shift: np.fft.fftshift(np.fft.fft2(I)) Compute magnitude spectrum: Get image size: (h, w) shape(I) Center: (cx, cy) (w//2, h//2) Define low-frequency radius: min(cx, cy)//4 Create circular mask of radius centered at (cx, cy) Compute high-frequency mask: 1 Compute high-frequency energy: Ehigh (cid:80) Normalize energy: Ehigh/(h w) Compute upscaling factor: 1/(1 + e) return f"
        },
        {
            "title": "C Video Processing",
            "content": "Video Processing For training, we use the MM-AU dataset [10], large-scale collection of dashcam crash videos collected from online sources. To ensure high quality we curate this dataset by applying series of filtering steps: 1. Low-resolution or heavily compressed videos are filtered out using FFT-based heuristics to detect blocky or low-fidelity regions and prioritize videos with clear visibility and identifiable motion patterns for training. See Appendix for more details. 2. PySceneDetect [6] is used to remove videos with shot changes or very sudden unnatural camera angle changes (e.g., dashcam falls over) 3. Videos are sampled at consistent frame rate of 6 fps, and cropped to resolution of 512 320 (width height) to fit our models input format and remove unwanted watermarks in the process. For training, we trim the videos into 25-frame clips (4s videos) and therefore reject any clips with less than 25 frames. 4. For this work, we have opted to filter out all accidents involving visible humans to avoid teaching the model to generate gruesome and violent scenes. This includes all accidents directly involving: pedestrians, cyclists, and motorbikes. 19 Additionally, many video samples were reviewed manually (reviewing tools available on the project github) to identify low-quality video samples that were not identified by the processing pipeline. After these filtering steps, we retain approximately 7,500 videos from the original 11,727 videos. We split these videos into training set and held-out test set by randomly sampling by accident type categories with 90/10 ratio."
        },
        {
            "title": "D Bounding Box Extraction Pipeline",
            "content": "Figure 11: Shows sequence of frames with car going into the ditch. SAM2s mask predictions for the two cars in the video are shown in purple and orange. YOLOs first prediction of the purple colored car is shown in frame 16 with light pink bounding box. SAM2 works bidirectionally and can therefore infer the appearance of the purple-colored car many frames earlier and until the very last instant before it disappears into the ditch; this would not be possible with tracker solely based on YOLO which relies on the object to be recognizable as one of the target classes in order to be detected. We develop hybrid pipeline, referred to as YOLO-SAM, to extract high-quality bounding box annotations for all visible road users in crash and driving videos. The pipeline combines the fast, classaware object detection capabilities of YOLOv8 [23] with the instance-level segmentation and temporal consistency of SAM2 (Segment Anything Model v2)[33]. This combination addresses limitations in both detection and tracking, especially under occlusion, deformation, or abrupt motioncommon in crash scenariosand supports dynamic entry and exit of agents YOLO Detection and Initial Tracking YOLOv8 is applied to each frame to detect objects and assign initial bounding boxes. This step provides fast and reliable detection of objects in each frame. YOLO also attempts to track objects across frames. However, it has several limitations, namely: 1. Multiple Detections: YOLO may detect the same object multiple times at different time points. To address this, the code checks for overlap between its own predictions and rejects duplicates if the overlap is too high (if IOU > 0.8). 2. Loss of Detection: YOLO may lose detection of an object through time, especially in complex scenarios where objects become unrecognizable due to occlusion or deformation. This is where SAM2 comes into play, as it is better at tracking objects over time. SAM2 Segmentation and Tracking SAM2 is used to produce instance-level masks for each object initially detected by YOLO. This allows for more precise tracking, especially when objects are occluded or deformed. Concretely, SAM2 is used to handle the following YOLO limitations: 1. Distorted Bounding Boxes: YOLO may distort the size of the bounding box. SAM2 tracks the shape more reliably, ensuring that the bounding boxes are accurate. 2. Redetection with New Track ID: If YOLO loses detection and then redetects an object with new track ID, the code checks for overlap between YOLOs new prediction and SAMs prediction. If the overlap is too high, YOLOs new prediction is rejected. 3. Track ID Switching: If YOLO assigns an existing track ID to different object after losing detection, the code changes this track ID to new value, as it does not trust YOLO for 20 tracking. This assumes that the new detection is actually new object, which is verified by the overlap check with SAMs prediction. 4. Late Detection: YOLO may detect an object late (i.e., several frames after the object entered the frame). SAM2 works bidirectionally and can help by making predictions in earlier frames based on the bounding box prompt in later frames. However, we have found this to be risky as SAM2 may make mistakes and is biased towards thinking all objects stay in frame during the clip. Therefore, the code only accepts SAMs \"early\" predictions few frames ahead of YOLOs predictions. Combining YOLO and SAM2 Predictions The code filters the predictions from both YOLO and SAM2 to ensure that only reliable bounding boxes are retained. Finally, we retain the SAM2 predictions for each frame and convert the predicted masks to tight bounding boxes. The final output is list of dictionaries, each representing frame of the video. Each frame contains list of tracked objects with their bounding boxes, track IDs, and class names. Code for this implementation is made publicly available on our project page."
        },
        {
            "title": "E Bounding Box Conditioning",
            "content": "Figure 12: Left: example frame from driving video. Right: associated bounding box frame conditioning generated from our pipeline. Road users are represented as 2D bounding boxes with unique fill colors representing their track ID and specific border colors representing their class. Table 5: Class encoding color scheme for bounding box border color Class Border Color RGB values person car truck bus train motorcycle bicycle Blue Red Orange Yellow Green Purple Pink (0, 0, 255) (255, 0, 0) (247, 162, 44) (250, 255, 2) (0, 255, 0) (204, 153, 155) (255, 209, 22) To provide spatial guidance to the generative model, we convert the bounding box trajectories of road users into RGB control frames that serve as conditioning input to the ControlNet. An example of such control frame, alongside its corresponding real image, is shown in Figure 12. Each control frame encodes the complete set of bounding boxes for single timestep using color-coded rasterization scheme that encodes both object identity and semantic class. Track ID Encoding (Fill Color) Each bounding box is filled with unique RGB color derived from its objects persistent track ID. The RGB values are deterministically generated via hashing function to ensure consistency across frames. RGB values vary within [50, 255] for all three color channels. This allows the model to temporally link the same agent across timesteps and learn coherent 21 motion patterns. The use of color fills avoids the need for explicit ID embeddings and leverages the spatial structure of the image. Class Encoding (Border Color) To distinguish between different types of road users (e.g., cars, trucks, buses, pedestrians, cyclists), we draw thin border around each bounding box in classspecific color. These colors are chosen from fixed palette (see Table 5), and the mapping between semantic classes and RGB border values is consistent across all training data . This helps the model differentiate object behaviors by class, which is particularly useful in crash prediction (e.g., trucks tend to behave differently from bicycles). The final control frame is an RGB image of the same resolution as the input video (e.g., 512 320) and can represent any number of agents per frame. If not depth information is available, overlapping boxes are drawn in arbitrary order, with later boxes overwriting earlier ones. These control frames are encoded using the same VAE encoder used for the initial frame. The resulting latent tensor is passed through ControlNet branch and injected into the U-Net backbone of the diffusion model at selected layers during training and inference. This spatial representation allows the model to attend to object motion in dense and learnable way without requiring symbolic or token-level processing. This conditioning mechanism supports variable numbers of agents and enables fine-grained motion control over multiple timesteps. It also integrates seamlessly into the denoising process of the diffusion model, allowing consistent agent-level motion to be expressed throughout the generated video sequence."
        },
        {
            "title": "F FVD and JEDi Metrics Computation",
            "content": "We use two distributional metrics to evaluate video generation quality: Fréchet Video Distance (FVD) [41] and JEDi [29]. Both metrics aim to measure the distance between the distribution of generated videos and real videos. FVD is computed by embedding videos into the feature space of pretrained Inflated 3D ConvNet (I3D), assuming multivariate Gaussian distribution, and then computing the Fréchet distance. JEDi, in contrast, uses features from Joint Embedding Predictive Architecture (JEPA) and computes distributional similarity via Maximum Mean Discrepancy (MMD) with polynomial kernel, avoiding the Gaussian assumption and improving temporal sensitivity and sample efficiency. Both are computed on extracted features from generated and ground-truth video samples. We adopt two evaluation protocols: 1. Unpaired Evaluation (Random Ground-Truth Sampling): For AVD2, which provides 2000 unpaired generated videos (i.e., without known ground-truth counterparts), we compute FVD and JEDi by comparing to set of 500 randomly sampled videos from the MM-AU validation set. This approach estimates the distance between the overall distribution of generated videos and real crash scenarios, and enables fair comparison across models lacking GT alignment. For consistency, we also use this protocol to evaluate Ctrl-Crash, SVD Base, and Ctrl-V by sampling 200 generated videos from each model. These results are reported in Table 1 of the main paper. 2. Paired Evaluation (Ground-Truth Matching): For methods like Ctrl-Crash, Ctrl-V, and SVD Basewhere ground-truth video correspondence is availablewe compute FVD and JEDi between 200 generated videos and their matched ground-truth counterparts from MM-AU. This approach captures alignment not only in distribution but also in per-sample temporal and visual fidelity. These results are reported in Table 2 and Table 3 of the main paper. For OAVD and DrivingGen, we report the FVD values cited in their original publications due to the unavailability of code or generated samples. These are marked with an asterisk (*) in Table 1 of the main paper."
        },
        {
            "title": "G Training and Implementation Details",
            "content": "Model Architecture. Ctrl-Crash builds on the Stable Video Diffusion (SVD) framework as the base image-to-video generation model. In the first stage, we fine-tune the SVD model on curated crash and driving clips from the MM-AU dataset (see Section 3.2). In the second stage, we freeze the base model and train ControlNet module to integrate spatial (bounding box control frames) and semantic (crash type) inputs via additional encoder and cross-attention layers. All control signals are processed using the same pretrained VAE encoder used by SVD. The number of parameters for each model (and each sub-module) is given in Table 6. Training Setup. We use the AdamW optimizer with learning rate of 4 105 and batch size of 1. The first-stage fine-tuning of the base SVD model is performed for 101k steps, using an MSE loss in latent space. The second-stage ControlNet training is run for 31k steps, with conditioning dropout applied as described in Section 4.4. We use mixed precision during training by setting weights and inputs to fp16 for non-trainable (frozen) parts of the model (i.e., VAE encoder, VAE decoder, CLIP encoder) and keep the trainable parts at fp32 to reduce memory usage.Training is performed on 4 NVIDIA 80GB A100 GPUs over approximately 2 weeks for both stages combined. All models are implemented in PyTorch using the Hugging Face accelerate library as base. Inference and Guidance Parameters. During inference, we apply multi-condition classifier-free guidance with tunable guidance scales. Unless otherwise specified, we use γB [1, 3] for bounding box control and γT [6, 12] for crash type conditioning. These values represent the ranges of values within the guidance scales will increase linearly throughout the denoising process (e.g., guidance scale starting at 1 at first denoising step and finishes at 3 at the last denoising step). The base model ϵϕ used for unconditional noise prediction is the original pretrained SVD base checkpoint prior to any fine-tuning. We sample videos with 25 frames at resolution of 512 320, using DDIM sampling with 30 denoising steps. Dataset Splits. We curate training set of 7, 500 clips from MM-AU after filtering. held-out validation set of 900 clips is used for evaluation. All reported metrics in the main paper (Tables 1, 2, 3) are computed on generated samples from the held-out validation set. Submodule Status (Stage 1) Status (Stage 2) Number of Parameters VAE-Encoder VAE-Decoder CLIP-Image Encoder UNet ControlNet Total Frozen Frozen Frozen Trainable N/A Frozen Frozen Frozen Frozen Trainable 34,163,592 63,579,183 632,076,800 1,524,623,082 681,221, 2,935,664,242 3B Table 6: Number of parameters by submodule. Refer to architecture diagram in Figure 2 of the main paper for more information on the submodules. Stage 1 and Stage 2 refer to the two stages of training for our method."
        },
        {
            "title": "H Datasets",
            "content": "Dataset annotations may be obtained by following instructions from our open-source code base (find link to code repo on our project website: https://anthonygosselin.github.io/ Ctrl-Crash-ProjectPage/). All annotated video samples in our dataset (MM-AU extension, RussiaCrash test set, etc.) are stored in JSON format, where each annotation file corresponds to single video. Below, we describe the structure and semantics of the annotation schema. Annotation Structure. Each annotation consists of three main fields: video_source: The filename of the source video (e.g., \"7_00951.mp4\"). 23 metadata: High-level information about the annotated scenario, including: ego_involved (bool): Indicates whether the ego vehicle (assumed to be the camera holder) is involved in the accident. accident_type (int): categorical index representing the accident type as defined by DADA2000 dataset. See Figure 13 and Table 7 for more information). data: list of per-frame annotations. Each frame entry includes: image_source:"
        },
        {
            "title": "The filename of",
            "content": "the corresponding frame image (e.g., \"7_00951_0000.jpg\"). labels: list of annotated objects (bounding boxes) in the frame. Each object contains: * track_id (int): persistent ID assigned to each object across frames. * name (str): The object class as string (e.g., \"car\", \"person\", \"truck\"). * class (int): The numerical class index used internally (e.g., 0 = person, 1 = car). * box (list[float]): The bounding box coordinates, normalized to the range [0, 1], in the format [x_min, y_min, x_max, y_max]. All bounding boxes are temporally linked using consistent track_id values. Object classes follow the taxonomy defined by the YOLOv8 model used in the annotation pipeline. The MM-AU dataset [10] labels each video with an accident type as defined by the DADA2000 dataset [11]. These accident types are presented in Figure 13. In our work, we reduce these numerous crash types to 5 types as defined in Section 3.3 and repeated here: Crash types are represented by discrete class label from five categories: (0) none, (1) ego-only, (2) ego/vehicle, (3) vehicle-only, and (4) vehicle/vehicle. These indicate which agents are involved in the crash, with, for example, vehicle/vehicle describing crash between two non-ego agents. Table 7 shows the association from the DADA2000 crash types to the Ctrl-Crash crash types that were used in this work. Table 7: Ctrl-Crash to DADA2000 Crash Type association Ctrl-Crash Types DADA2000 Crash Types 1 - ego-only 2 - ego/vehicle 3 - vehicle-only 4 - vehicle/vehicle 13, 14, 15, 16, 17, 18, 61, 62 1-12 19-37, 39, 41, 42, 44 38, 40, 43, 45-51 For training and evaluation, we split all videos into 25-frame clips. The MM-AU dataset gives exact frame labeling to indicate when the accident occurs and when abnormal driving starts and ends before and after the accident. For each video we sample 25-frame video containing the accident frame and label it according to the accident type. Then we sample, if possible, 25-frame video containing only \"normal\" driving (i.e., with no overlap with abnormal or accident labeled frames) and label these clips with crash type \"no crash\". Ultimately this yields 6, 927 clips containing crash and 1, 964 clips without crash (see Table 8 for number of clips for each class during training). Table 8: Number of clips per crash type in training set Number of Training Samples Crash Type 0 - no crash 1 - ego-only 2 - ego/vehicle 3 - vehicle-only 4 - vehicle/vehicle 1745 267 3182 577 2168 24 Figure 13: Original accident type definition used by the MM-AU dataset [10] as defined by the DADA2000 dataset [11]"
        },
        {
            "title": "I User Survey",
            "content": "The participants were not given any information about the study ahead of time other than that it revolves around state-of-the-art video generation. The survey was carried out through Google Forms. At the start of the survey, they received the following instructions: Content warning: ai-generated mild car crashes with no humans depicted. No blood/injury, just car-on-car action. Please help us get some human feedback on new video generation method. Were asking you to rank 5 sets of videos. Should take less than 5 min, and youll see why AI wont take over anything anytime soon. Instructions: For each of these 5 questions, we will show you 3 short video clips that have ALL been AI-generated. We will ask you: Does each video depict crash? Rank the 3 videos by highest physical accuracy/plausibility Rank the 3 videos by highest visual fidelity (aka are they nice to look at) 25 Important: Please rank the videos relative to one another, i.e. \"best\" means best of the 3. Each of the 5 accident types (with 3 videos each, labeled A,B,C, randomly shuffled from AVD2, DriveGen, ours) were presented like in Fig.14. The 5 accident types we selected were: head-on collision, t-bone, rear-ending, dangerous overtaking, and loosing control of the vehivle and going off the road. The ranking was implemented as shown in Fig.15 by forced-choice. As described in the main body of the paper, we used the Friedman test (from the Python package scipy.stats.friedmanchisquare with α 0.01 and found = 0.0001. For the Nemenyi Post-Hoc analysis, we used critical value of critical_value = 3.64 for an alpha value of α = 0.01. The critical difference (cut-off value for significance) was then calculated as cd = critical_value (cid:112)k (k + 1)/(12 num_participants) with num_participants = 40. We obtained significant differences between the pairs: Ours - AVD2 and Ours - DriveGen, with our method consistenly ranking higher than either. We did non obtain significant differences in user preferences between the pair: AVD2 - DriveGen, which means that users did not have significant preference between the two. 26 Figure 14: User Survey Screenshot 1, showing 2 (out of 3) samples from question 1. What is shown as static image here was GIF in the original Google Form. 27 Figure 15: User Survey Screenshot 2, showing the evaluation questions for each batch of 3 videos. Users were forced to rank all videos and to only use each rank once (i.e. it is not possible to submit the form when more than one video in each batch has the same rank)."
        }
    ],
    "affiliations": [
        "CIFAR AI Chair",
        "McGill University",
        "Mila",
        "Polytechnique Montréal",
        "Samsung SAIL Montréal",
        "Université de Montréal"
    ]
}