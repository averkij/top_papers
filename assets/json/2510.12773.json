{
    "paper_title": "Dr.LLM: Dynamic Layer Routing in LLMs",
    "authors": [
        "Ahmed Heakl",
        "Martin Gubri",
        "Salman Khan",
        "Sangdoo Yun",
        "Seong Joon Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 7 7 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "DR.LLM: DYNAMIC LAYER ROUTING IN LLMS Ahmed Heakl1,2, Martin Gubri1, Salman Khan2, Sangdoo Yun3, Seong Joon Oh1,4,5 1Paramter Lab, 2MBZUAI, 3NAVER AI Lab,4University of Tubingen,5Tubingen AI Center https://github.com/parameterlab/dr-llm"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) process every token through all layers of transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budgetaware, accuracy-driven inference without altering base weights."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) typically process every token through fixed stack of transformer layers, regardless of the inputs difficulty. This static-depth regime results in wasted computation for easy prompts and insufficient flexibility for challenging reasoning tasks. To address this, prior work has investigated adaptive depth mechanisms at test time, including early-exit strategies (Elhoushi et al., 2024), layer pruning (Men et al., 2024), recurrent or looped blocks (Bae et al., 2025), dynamic routing methods (He et al., 2024; Luo et al., 2025), mixture-of-depth approaches (Raposo et al., 2024), mixtureof-experts architectures (Shazeer et al., 2017), and searchbased routing frameworks (Li et al., 2025). Despite their promise, these methods typically suffer from one or more limitations: (i) they trade accuracy for speed, (ii) they require architectural modifications and retraining on substantial amounts of data, or (iii) they rely on costly inference-time search that is difficult to deploy at scale. Figure 1: Dr.LLM improves accuracy while reducing computation. Number of layers used per example vs. accuracy on ARC and DART, averaged on six models. We propose Dr.LLM (Dynamic Routing of Layers for LMs), retrofittable framework that equips frozen, pretrained LLM with lightweight, per-layer routers that decide whether to skip, execute, or repeat their layer. Routers operate in windowed, meanpooled hidden states and are trained with explicit supervision obtained from an offline Monte Carlo Tree Search (MCTS). For each input, MCTS discovers execution paths, that is, which layers to skip or repeat, such that they preserve or improve task accuracy under constrained compute budget. Corresponding authors"
        },
        {
            "title": "Preprint",
            "content": "Training the routers with supervised learning on 4k optimized paths is lightweight, since only the few router parameters are updated while the LLM remains frozen. The trained router removes the need for any search at inference time, enabling compute-efficient inference that increases accuracy, without modifying the base model weights. Empirically, Dr.LLM improves the accuracy of reasoning-heavy tasks while reducing the average number of executed layers  (Fig. 1)  . On ARC (logic) and DART (math), accuracy improves in all cases, with mean gains of +2.25 percentage points (%p) and 5.0 fewer layers per example across six models. Routers generalize out-of-domain (e.g., MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, AGIEval, PIQA) with only 0.85%p average accuracy drop while retaining efficiency, indicating that learned routing policies transfer beyond the supervised domains. Lastly, Dr.LLM outperforms all prior SoTA routing methods by up to +7.7%p accuracy. Our contributions are as follows: Supervised dynamic routing for frozen LLMs. We introduce per-layer routers that decide to either skip, execute, or repeat their layer. We train the routers end-to-end on only 4k execution paths optimized for accuracy, discovered offline. Effective path supervision via MCTS. We present length-aware MCTS to find layer edits (skips/repeats) under budget and to retain only accuracy-preserving or improving paths, generating compact supervision dataset without modifying the base weights. Lightweight router training. We propose windowed mean-pooling for stable decisions on long contexts and use focal loss with class-rebalancing weights, combined with lightweight two-layer linear model, which together handle class imbalance and keep the computation overhead negligible. Accuracy increase and compute efficiency. On ARC and DART across six models, accuracy improves in all cases, with up to +4.0%p and 11.0 layers saved per example in that case, without architectural changes, retraining, or inference-time search. Robust generalization. Routers transfer to out-of-domain benchmarks with only 0.85%p average drop, indicating that policies learned during router training remain useful beyond the training tasks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Adaptive-depth methods span pruning, early exits, recurrence, and routing; Table 1 condenses their trade-offs across accuracy, retrofitting, efficiency, practicality, and frozen-base compatibility. compression prunes Pruning and Early Exit. Classical model redundant weights, heads, or layers post hoc (Sajjad et al., 2023). Early-exit networks extend this by attaching auxiliary classifiers at intermediate layers (Xin et al., 2020; Zhou et al., 2020; Teerapittayanon et al., 2016), letting easy inputs terminate early. While effective, such classifiers need calibration, add overhead, and complicate deployment. LayerSkip (Elhoushi et al., 2024) improves this by training with dropout and shared exit loss, removing the need for multiple classifiers. Yet it still training requires finetuning or from scratch for the LLM and cannot repeat layers. In contrast, our approach supervises skip/execute/repeat directly via MCTS, eliminating auxiliary exits and enabling repetition without retraining base weights. Method Table 1: Comparison of dynamic routing methods for frozen LLMs. Accuracy : does the method improve accuracy over the baseline. Retrofit: can it be added to pretrained models with minimal effort. Cheap I: enables efficient inference without heavy overhead. Cheap T: enables efficient training with limited data. LLM (cid:11): base model remains unchanged. Symbols: = strong support, = not supported. Dr.LLM is the only method satisfying all five criteria. CoLa Mixture of Depths Universal Transformer LLM-Pruner Mixture of Experts Mixture of Recursions LayerSkip ShortGPT MindSkip FlexiDepth Dr.LLM (Ours) Accuracy Retrofit Cheap Cheap LLM (cid:11) Recurrence and Looped Architectures. Another line of work adapts depth by repeating computation. Universal Transformers (Dehghani et al., 2018) learn halting policy per token, while looped"
        },
        {
            "title": "Preprint",
            "content": "block Figure 2: Our layer routing based on hidden states. Dr.LLM augments frozen decoder-only LLM with per-layer routers that decide to skip, execute, or repeat once. Routers read windowed summaries of hidden states and are trained from MCTS- (Sec. 4). derived targets For the diagram clarity, also highlights the router internals and the flow of hidden states across layers. transformers (Yang et al., 2023; Giannou et al., 2023; Geiping et al., 2025) iteratively reapply blocks for refinement (slow thinking). These models are flexible but require architectural redesign, full retraining, and incur higher inference cost. We also support targeted repetition, by attaching shallow controllers to frozen layers, avoiding structural changes or pretraining. Moreover, we allow skips to offset the layer increases from looping. Dynamic Routing and Modular Inference. Routing-based methods let inputs select modules dynamically. MoE architectures (Fedus et al., 2022; Shazeer et al., 2017) expand capacity by routing tokens to experts, but demand large-scale retraining. CoLa (Li et al., 2025) is closer to our setting: it treats pretrained layers as modules and searches, via MCTS, for input-specific chains of layers. However, CoLa requires costly search at inference and, critically, access to gold labels during search to decide which path is correct, making it impractical for deployment. We instead perform MCTS offline to generate supervision and then train routers that make decisions cheaply at inference. Other adaptive-depth methods, such as FlexiDepth (Luo et al., 2025) and MindSkip (He et al., 2024), retrofit routing to pretrained models but require extensive training (hundreds of thousands of examples) and often reduce accuracy to save compute. By contrast, our routers are trained from only 4k MCTS-derived examples and in experiments improve accuracy while lowering cost. Mixture-ofDepth (MoD) (Raposo et al., 2024) takes different angle, routing at the token level by sending only subset of tokens through deeper layers, but modifies the base weights. This intra-layer mechanism complements our sequence-level skip/execute/repeat routing: token-level signals identify local redundancy, while layer-level control reallocates global compute."
        },
        {
            "title": "3 SUPERVISED TRAINING OF THE ROUTER",
            "content": "Let pretrained decoder-only LLM with transformer blocks be = [B1, . . . , BL]. For token sequence of length , let (1) RT denote its initial hidden states. The classical forward pass applies each block once: (ℓ) = Bℓ (cid:0)H (ℓ1)(cid:1). We instead seek discrete per-layer policy yℓ {skip, execute, repeat}, where skip bypasses Bℓ, execute applies it once, and repeat applies it twice in succession. The vector = (y1, . . . , yL) induces custom execution path, while the base transformer weights remain frozen. 3.1 ROUTER ARCHITECTURE As shown in Figure 2, each block Bℓ is paired with lightweight MLP (Linear-GELU-Linear) rℓ : Rd R3, which outputs logits for {skip, execute, repeat}. The router operates on compact summary of the hidden states (ℓ1) from the previous layer. Routers are executed once per input sequence at inference, adding negligible overhead (constant with the number of generated tokens), and remaining fully compatible with KV caching, unlike most layer routing methods."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Length-aware MCTS used to collect the supervised training dataset of per-layer routing configurations (skip/execute/repeat). For each input, MCTS explores modified layer paths and retains accuracy-preserving or improving ones under compute budget. To stabilize decisions on long contexts while keeping overhead negligible, we adopt windowed mean pooling: w=1, with (cid:80) mw = 1 the mean-pooled representation. Router votes are aggregated by averSw aging logits: the first /W tokens are divided into contiguous windows {Sw}W (ℓ1) tSw zℓ = 1 W (cid:88) w=1 rℓ(mw), pℓ = softmax(zℓ), ˆyℓ = arg max c{0,1,2} pℓ,c. We default to =8 (clamped by ). Router weights are Xavier-uniform initialized (Glorot & Bengio, 2010) and biases initialized to zero. Only {rℓ}L ℓ=1 are trainable, while model parameters are frozen. At inference, the decision ˆyℓ governs block execution where SKIP passes (ℓ) = (ℓ1), EXECUTE applies Bℓ once, and REPEAT applies Bℓ twice in succession. 3.2 TRAINING REGIME For each prompt-response pair / question-answer pair (q, a), the method proposed in Sec. 4 yields ground truth path, π, that we utilize to supervise the training process introduced here. These paths preserve or improves task reward under compute budget. We convert π to per-layer labels ℓ = count(ℓ π) {0, 1, 2}, producing tuples (q, y, a) meaning (question, y, answer). Because execute dominates, we apply focal loss (Lin et al., 2017) with effective-number weights. Let the global class counts be nc for {skip, exec, repeat} and β (0, 1): αc = 1 β 1 β nc (cid:46) 1 3 1 β 1 β nc , (cid:88) = 1 (cid:88) ℓ=1 αy ℓ (1 pℓ,y ℓ )γ log pℓ,y ℓ . We use γ=2, β=0.999 by default.1 During training we apply teacher forcing for execution only, i.e., we replace the router decision with the ground-truth label ˆyℓ ℓ to follow the labeled path while supervising logits with L. This avoids making routeri depend on routeri1 outputs, which would otherwise slow training and lower accuracy by 1.7%. At inference, decisions are greedy: ˆyℓ = arg max pℓ; no search is used. Routers add O(Ldh) parameters for hidden size and router width (we use h=128), and one small MLP per layer at inference. Windowed pooling is linear in and inexpensive relative to transformer block. Skip reduces compute; repeat adds targeted compute when beneficial. We report accuracy, per-class f1 for {skip, exec, repeat}, and average executed layers."
        },
        {
            "title": "4 TRAINING DATA GENERATION VIA MCTS",
            "content": "This section describes the search-based generation of the supervised training dataset of layer configurations for the router. We supervise routers using tuples (q, y, a), where is the input question/prompt, the gold answer, and {skip, execute, repeat}L the best per-layer routing targets found by discrete search over modified forward passes  (Fig. 3)  . The search is offline and does not modify base weights. 1Setting γ=0 recovers weighted cross-entropy."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Length-aware MCTS for questionanswer pair (q, a) Select: traverse from to leaf using nodes UCB score; w.p. prand pick random child Expand: add one untried action to obtain child (respecting path-length cap) Evaluate: if π(u) / E, run model constrained to π(u), set E[π(u)] R(ˆa, a) Backpropagate: propagate to ancestors if R(ˆa, a) = 1 and π(u) < π or π = then Require: Default path π0 = [1, . . . , L], number of simulations Ns, constants (c, λ, prand) 1: Create root with π(r) π0; cache ; best path π 2: for = 1 to Ns do 3: 4: 5: 6: 7: 8: 9: 10: 11: end for 12: Convert π into per-layer labels {0, 1, 2}L (skip/execute/repeat) 13: return π π(u) break if E[π0] = 0 update to the shortest correct path enhance default path answer WC end if 4.1 EDITED EXECUTION PATHS AND ACTIONS Let the base model have blocks and default path π0 = [1, . . . , L]. An edited path π = [ℓ1, . . . , ℓK] preserves the original order of blocks but may omit certain layers (skip) or apply given layer twice (repeat once). We allow skips of at most two consecutive layers, and we allow at most single repeat for any block, which controls the compute growth, i.e., the total edited path length is capped at π 2L. 4.2 LENGTH-AWARE MCTS Each node stores triple of path π, visits v(π), and cumulative reward Q(π). During selection, we maximize UCB (Upper Confidence Bound) score (inspired by Li et al. (2025) with an explicit length penalty to favor compact paths: UCB(π) = Q(π) v(π) (cid:124) (cid:123)(cid:122) (cid:125) exploitation + (cid:115) ln v(π) (cid:124) (cid:123)(cid:122) (cid:125) exploration λ π(π) (cid:124) (cid:123)(cid:122) (cid:125) length penalty , where is the parents visit count. We use c=1.8, λ=3.0, and with probability prand=0.1 pick random child to encourage exploration. For each simulation, we expand one untried action, evaluate the edited path once, and backpropagate the task reward [0, 1] (no length penalty) through the root node. We run fixed budget Ns=50 simulations or stop early if we (i) reach correctness and (ii) find strictly shorter correct path than the best-so-far. Path evaluations are memorized to avoid duplicates. We retain only accuracypreserving/improving paths (vs. the default π0). Table 2: Data generation statistics. Visited is the total number of candidate paths explored, and Sampled is the subset of paths that improve or preserve accuracy. Dataset Original Sampled Visited #Inferences ARC-E ARC-C 2.25k 1.12k 400 600 2,090 1,119 82.6k 44.2k DART-1 DART-2 DART-3 DART-4 DART-5 117k 296k 364k 391k 445k 200 400 600 800 1000 967 2,242 3,695 6,014 8,203 38.2k 88.6k 146.0k 237.6k 324.0k We ran MCTS across ARC and DART, collecting 4k supervision examples. About 30% of these edited paths achieved higher accuracy than the default path π0, while the rest preserved accuracy and reduce the number of layers with exact statistic shown in Tab. 2. The average number of layers saved is 1.82. Although the search required 961.0k forward passes, it is performed entirely offline; at inference time, routing decisions are made directly by the trained routers without any search. 961.0k 24, 1.63M Total 4000 Compared to Li et al. (2025), we found that reducing the repetition block size from 4 to 1 made the search substantially faster, while achieving the same accuracy gains and layer savings with only 50 simulations instead of 200. We also found that lowering the length penalty from 5 to 3 reduced the number of search samples by 14.8%p."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate Dr.LLM across both in-domain reasoning tasks and diverse suite of out-of-domain (OOD) benchmarks to test generalization under distribution shift. Our experimental setup is designed to answer three key questions: (i) Does supervised dynamic routing improve accuracy relative to static baselines? (ii) How much computational efficiency is gained in terms of average executed layers? (iii) Are the learned routing policies robust to new tasks and model families? Models. We retrofit Dr.LLM onto six backbone models spanning two families: LLaMA-3.2 (Dubey et al., 2024) (3B Instruct, 3B Base, 8B Instruct, 8B Base) and Qwen-2.5 (Yang et al., 2024) (3B Instruct, 7B Instruct). These models cover variety of sizes and both instruction-tuned and base variants. Training Data. Routers are supervised using 4K MCTS-derived tuples (Sec. 4) from ARCEasy/Challenge (Clark et al., 2018) and DART-Math (Tong et al., 2024). We selected these datasets for three reasons: (1) they provide stratified difficulty levels (ARC-Easy vs. ARC-Challenge, DART1 to DART-5), (2) they target logic and multi-step mathematical reasoning, where adaptive computation is beneficial, and (3) they have train/test splits allowing us to test in-domain distributions. Training Setup. We train all routers on single NVIDIA A100 40GB GPU. Given the small number of trainable parameters (11M for 3B models, 0.14% of base weights; 16.8M for 8B models, 0.56%), training is efficient and completes within 4 hours while using only 20% of the GPU VRAM. We use AdamW (Loshchilov & Hutter, 2017) with cosine schedule, learning rate 1 103, weight decay 0.01, 500 warmup steps, and total of 25 epochs. The effective batch size is 16, and training is performed in bf16 precision. Routers achieve an average F1-score of 61% against the routing labels, with balanced accuracy across skip, execute, and repeat classes. We tested different initialization schemes for the router biases: (i) initializing from empirical class frequencies (e.g., 10% skip, 85% execute, 5% repeat) and (ii) zero-initialization. We found the latter to yield more stable training and stronger downstream accuracy. Since the number of training examples is small and easily fit by the routers, relatively high learning rate (1 103) worked best; with smaller 1 104, the accuracy of the skip and repeat classes never improved. In-Domain Evaluation. We first evaluate routers on ARC and DART test splits. These tasks serve as direct measure of whether routers can recover the MCTS supervision signal and yield improvements under controlled conditions. Out-of-Domain Evaluation. To assess robustness, we evaluate the router-equipped models on broad range of benchmarks: MMLU (Hendrycks et al., 2020) for factual knowledge, GSM8k (strict match) (Cobbe et al., 2021) for grade-school math, TruthfulQA (mcq1) (Lin et al., 2021) for adversarial factuality, GPQA Diamond (Rein et al., 2024) and AIME24 (MAA, 2024) for challenging mathematical reasoning, AGIEval (Zhong et al., 2023) for exam-style reasoning, SQuADv2 (f1) (Rajpurkar et al., 2018) for reading comprehension, and PIQA (Bisk et al., 2020) for commonsense reasoning. All benchmarks are reported using acc norm computed from log-likelihoods in the lm-eval-harness framework (Gao et al., 2024), except GSM8k, TruthfulQA, and SQuADv2 which follow their respective metrics. Evaluations are run with default settings, maximum generation length of 2048 tokens, and greedy decoding."
        },
        {
            "title": "6 RESULTS & DISCUSSION",
            "content": "We evaluate Dr.LLM on in-domain tasks, test its robustness on out-of-domain benchmarks, and analyze routing patterns with ablations. 6.1 IN-DOMAIN PERFORMANCE ON ARC AND DART Table 3 summarizes in-domain results on ARC (logic) and DART (math), showing that routers consistently improve accuracy while reducing the average number of layers executed across all six models. On ARC, gains are modest (+0.92.5%p), reflecting that logic questions already require relatively shallow reasoning. In contrast, DART exhibits larger improvements (+1.44.0%p), where"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Routers consistently improve accuracy and reduce executed layers across all models. In-domain results on ARC (logic) and DART (math). Accuracy in %. (+x) indicates accuracy gains, (-x) indicates layer savings, and (+0.0) indicates no change. Model ARC DART Total Accuracy Num Layers Accuracy Num Layers Accuracy Num Layers LLaMA-3B-Instruct + Router 73.5 74.5 (+1.0) 103.9 99.50 (-4.25) 35.2 38.6 (+3.4) 422.0 413.3 (-8.66) 46.1 48.9 (+2.7) 331.1 323.7 (-7.40) LLaMA-8B-Instruct + Router 88.5 89.4 (+0.9) 106.9 104.0 (-2.94) 38.4 41.2 (+2.8) 320.0 309.1 (-10.96) 52.7 54.7 (+2.3) 518.2 509.6 (-8.66) LLaMA-3B-Base + Router LLaMA-8B-Base + Router 48.0 49.0 (+1.0) 56.0 55.70 (-0.28) 11.8 15.8 (+4.0) 548.4 544.3 (-4.12) 22.1 25.3 (+3.2) 815.4 812.4 (-3.02) 22.5 23.5 (+1.0) 56.0 55.60 (-0.42) 17.2 20.2 (+3.0) 536.7 531.0 (-5.74) 18.7 21.1 (+2.4) 798.7 794.5 (-4.22) Qwen-3B-Instruct + Router 53.0 55.5 (+2.5) 115.7 115.5 (-0.23) 30.2 32.4 (+2.2) 536.3 531.7 (-4.55) 36.7 39.0 (+2.3) 832.4 828.9 (-3.31) Qwen-7B-Instruct + Router 94.5 94.5 (+0.0) 112.0 111.8 (-0.20) 45.4 46.8 (+1.4) 277.8 273.1 (-4.67) 59.4 60.4 (+0.9) 460.9 457.5 (-3.39) Table 4: Generalization to out-of-domain benchmarks. Accuracy in %. Router models maintain accuracy with 0.85%p average drop while preserving efficiency. All evaluated models are instruct, we use LLaMa-3.2 and Qwen2.5. TQA is TruthfulQA, and GPQA is GPQA Diamond. Model MMLU AIME24 TQA GSM8k SQuADv2 GPQA AGIEval PIQA Avg. LLaMA3B + Router LLaMA8B + Router Qwen3B + Router Qwen7B + Router 60.50.39 59.50.40 67.90.72 66.80.70 65.30.82 62.80.82 71.70.88 71.20.88 3.31.33 3.31.64 6.71.75 6.71. 6.71.36 6.71.38 10.01.40 10.01.42 31.31.48 30.41.31 36.91.45 36.61.40 41.91.50 41.91.47 47.71.53 47.91. 64.91.32 64.31.35 73.21.30 74.91.28 11.11.29 11.51.29 75.61.26 75.71.25 32.61.41 30.61.42 29.10.35 28.60. 21.50.99 20.10.81 20.80.42 20.20.43 27.20.31 29.80.33 34.30.31 32.30.41 33.30.34 32.40.35 32.80.36 32.80. 35.70.51 33.80.50 43.20.52 41.50.51 54.20.51 49.40.51 61.20.51 57.20.52 75.61.06 71.91.07 80.91.06 79.21. 78.11.05 78.91.04 79.70.93 78.80.92 - -0.94 - -0.70 - -1.05 - -0. the router often assigns repeat to late layers, effectively allocating more computation to iterative refinement needed for multi-step math problems. For example, LLaMA-3B-Base improves from 11.8% to 15.8% accuracy (+4.0%p) while saving 4.12 layers per query on average, and Qwen-3BInstruct gains +2.2%p while cutting 4.6 layers per query. Notably, instruction-tuned models start with substantially higher accuracy than their base counterparts, yet still benefit from routing: e.g., LLaMA-8B-Instruct improves by +2.8%p on DART while saving 11.0 layers per query on average. Importantly, Dr.LLM never degrades accuracy and always saves inference compute with 311 fewer layers per query. These results demonstrate that Dr.LLM not only reduces computation but also improves accuracy, with the largest benefits on tasks requiring deeper or repeated reasoning steps. 6.2 GENERALIZATION TO OUT-OF-DOMAIN BENCHMARKS Table 4 evaluates Dr.LLM-equipped models on diverse suite of out-of-distribution benchmarks, from in-domain and out-of-distribution mathematical reasoning benchmarks (AIME24, GSM8k) to out-of-domains benchmarks specialised in knowledge (MMLU, AGIEval, GPQA Diamond), factuality (TruthfulQA), comprehension (SQuADv2), and commonsense (PIQA). Despite not trained to handle these types of questions, the routers maintain good generalization with 0.85%p average accuracy drop across the eight benchmarks and four instruct models. The routers decision generalizes to other in-domain benchmarks: all four models gain 0.40%p accuracy on GSM8k and maintain the exact same accuracy on AIME24, while reducing compute. In out-of-domain benchmarks, the accuracy drop is limited to 1.20%p on average. Notably, in some cases routers even improve accuracy, such as GPQA Diamond with LLaMA-3B (+2.5%p). In all cases, the router maintains its efficiency by saving layers. These results indicate that router policies transfer beyond their domain, suggesting that the learned skip and repeat patterns capture general structural redundancies in transformer"
        },
        {
            "title": "Preprint",
            "content": "(a) Heatmaps (LLaMa 3B and 8B) (b) Boxplot (LLaMa 3B) (c) Boxplot (LLaMa 8B) Figure 4: Analysis of routing decisions per layer, dataset, and model. (a) Layer frequency of LLaMa 3B and 8B base (B) and instruct (I) models across ARC and DART. (b,c) Layer frequency grouped by early, middle, and late layers. The x-axis corresponds to the dataset difficulty levels: ARC-Easy (A-1), ARC-Challenge (A-2), and DART levels 15 (from D-1 to D-5). computation. Thus, Dr.LLM not only yields efficiency and accuracy improvements in-domain, but also preserves robustness when deployed to unseen, distribution-shifted benchmarks. 6.3 COMPARISON TO EXISTING METHODS Table 5: Comparison of Dr.LLM with existing methods on reasoning and coding benchmarks. Results on LLaMa3-8B reported from FlexiDepth (Luo et al., 2025) with 4 layers saved. Although these benchmarks are in-domain for prior methods and out-of-domain for Dr.LLM, ours still achieves the highest accuracy. Most adaptive-depth approaches either sacrifice accuracy for efficiency or impose costly architectural changes. For example, in Tab. 5, FlexiDepth (Luo et al., 2025) saves four layers on LLaMA-8B but suffers 6.1%p accuracy drop on GSM8k, while MindSkip (He et al., 2024) reduces compute yet loses 7.8%p on HumanEval. ShortGPT (Men et al., 2024) also improves efficiency but underperforms on reasoning, reaching only 53.6% on GSM8k compared to Dr.LLMs 74.9%. Even FlexiDepth, the method closest in accuracy to Dr.LLM, requires training: it is trained on Tulu-v2 (Ivison et al., 2023) with 326k examples, incurring substantial compute. By contrast, Dr.LLM achieves higher accuracy with far lower overhead, trained on only 4k MCTS-derived examples using single GPU, despite the fact that these benchmarks are in-domain for prior routing methods but out-of-domain for Dr.LLM. GSM8k MMLU HellaSwag HumanEval Avg. LayerSkip ShortGPT MindSkip FlexiDepth 0.0 9.2 18.9 32.3 0.4 53.6 37.8 65. 32.5 48.9 48.2 59.7 63.6 66.2 69.8 74.3 65.9 66.4 66.4 66.3 Dr.LLM Method 74. 66.8 79.3 48.6 67.4 6.4 ANALYSIS OF LAYER ROUTING PATTERNS We analyze router decisions across layers, models, and datasets to identify which layers can be skipped and which improve accuracy when repeated. Fig. 4 visualizes the learned routing policies for LLaMA 3B and 8B models. The heatmaps (Fig. 4a) show structured patterns rather than random skipping: early layers are consistently executed close to once, middle layers are frequently down-weighted, and late layers are often repeated, especially on reasoning-intensive DART tasks. The boxplots (Figs. 4b,4c) confirm this trend: early layers exhibit the lowest variance in execution frequency (stable usage), middle layers show wider skip distributions, and later layers are biased toward repeated execution, indicating their role in iterative refinement. This effect is stronger in the 8B model, where late-layer repetition dominates, suggesting that larger models rely more heavily on additional depth for complex reasoning. Together, these results indicate that Dr.LLM learns routing behaviors aligned with transformer computation phases: maintaining stability in early input processing, economizing in middle layers, and reinvesting compute in later blocks where deeper reasoning is most beneficial."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Ablation study. We apply Dr.LLM on LLaMa3.2-3B and control: (a) the effect of bottleneck dimension, (b) the effect of number of linear layers, and (c) the effect of number of windows."
        },
        {
            "title": "6.5 ABLATION STUDIES",
            "content": "Router internals. We ablate the router components to understand their effect on accuracy and efficiency  (Fig. 5)  . Varying the bottleneck dimension (Fig. 5a) shows that smaller hidden sizes (64 128) strike the best balance: bottleneck of 128 yields the highest accuracy gains (+3.4%p), while larger dimensions reduce both accuracy and layer savings, likely due to overfitting. Next, tuning the number of linear layers (Fig. 5b) indicates that both accuracy and compute gains are best when the router is composed of two linear layers. Deeper routers fail to improve routing, confirming that compact routers are more suitable. The number of pooling windows (Fig. 5c) strongly influences accuracy gains: more windows consistently increase both the accuracy and the number of layers saved. Averaging the hidden states of all input tokens is signal that is too coarse for the router to learn. Finally, the focal loss better accounts for the class imbalance of the router labels than the weighted cross-entropy loss (+1.1%p. on ARC and +1.8%p. on DART). These trends highlight that Dr.LLM benefits from (i) compact router architecture, and (ii) windowed contexts to learn fine-grained hidden state features. Table 6: Dr.LLM routes layers from their state, not from the type of question. Benchmark accuracy of routers trained on the hidden states of the previous-layer or of the first layer. In %. What do the routers learn? Since the semantics of the question is predictive of LLM accuracy in this question (Ulmer et al., 2024), we ask if the routers truly learn from the internal state of the model or from the types of input. Routers could learn question patterns (e.g. skip the seventh layer for math questions). Table 6 reports the ARC and DART accuracies of router trained on input embeddings (for all layers), rather than on the hidden states of the previous layer. This new router performs considerably worse than Dr.LLM (-8.6%p on DART), and even worse than the vanilla model without layer routing (-5.2%p). Therefore, Dr.LLM learns to dynamically map the internal model states to the decision to skip or repeat layers, instead of relying on shallow static signals from the inputs. Prev. layer Hi1 (Dr.LLM) First layer H1 (embeddings) No routing (vanilla model) Router Features ARC DART 38.6 30.0 35.2 74.5 70.9 73."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced Dr.LLM, retrofittable framework that equips frozen LLMs with lightweight routers for skip/execute/repeat decisions. Supervised on high-quality paths from length-aware MCTS, Dr.LLM removes inference-time search and architectural changes while improving both efficiency and accuracy. On ARC and DART, it yields up to +3.4%p accuracy with 311 layers saved per query, outperforms prior routing methods by up to +7.7%p, and generalizes to out-of-domain benchmarks with only 0.85%p drop. Routing analysis reveals structured patterns, early layers preserved, middle pruned, late reused, showing that adaptive compute allocation is both learnable and aligned with transformer computation phases. Overall, Dr.LLM demonstrates that explicit supervised routing reconciles efficiency, accuracy, and robustness without retraining, providing practical step toward budget-aware reasoning and scalable adaptive inference."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work was supported by the NAVER corporation."
        },
        {
            "title": "Preprint",
            "content": "The authors are also grateful to Cornelius Emde for his careful proofreading."
        },
        {
            "title": "REFERENCES",
            "content": "Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation. arXiv preprint arXiv:2507.10524, 2025. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, et al. Layerskip: Enabling early exit inference and self-speculative decoding. arXiv preprint arXiv:2404.16710, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pp. 1139811442. PMLR, 2023. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249256. JMLR Workshop and Conference Proceedings, 2010. Shwai He, Tao Ge, Guoheng Sun, Bowei Tian, Xiaoyang Wang, and Dong Yu. Router-tuning: simple and effective approach for enabling dynamic-depth in transformers. arXiv preprint arXiv:2410.13184, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023."
        },
        {
            "title": "Preprint",
            "content": "Ziyue Li, Yang Li, and Tianyi Zhou. Skip layer or loop it? test-time depth adaptation of pretrained llms. arXiv preprint arXiv:2507.07996, 2025. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 29802988, 2017. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Xuan Luo, Weizhi Wang, and Xifeng Yan. Adaptive layer-skipping in pre-trained llms. arXiv preprint arXiv:2503.23798, 2025. MAA."
        },
        {
            "title": "American\nInvitational",
            "content": "Mathematics American ary URL american-invitational-mathematics-examination-aime."
        },
        {
            "title": "Examination",
            "content": "2024. In - AIME Februhttps://maa.org/math-competitions/ examination - aime. 2024, invitational mathematics Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect, 2024. URL https://arxiv. org/abs/2403.03853, 2(3):4, 2024. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. David Raposo, Sam Ritter, Adam Santoro, Greg Wayne, Theophane Weber, Matt Botvinick, Aaron van den Oord, and Razvan Pascanu. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. On the effect of dropping layers of pre-trained transformer models. Computer Speech & Language, 77:101429, 2023. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR), pp. 24642469. IEEE, 2016. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. Advances in Neural Information Processing Systems, 37:78217846, 2024. Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Oh. Calibrating large language models using their generations only. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1544015459, 2024. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating bert inference. arXiv preprint arXiv:2004.12993, 2020. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv e-prints, pp. arXiv2412, 2024. Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023."
        },
        {
            "title": "Preprint",
            "content": "Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems, 33:1833018341, 2020."
        },
        {
            "title": "A AUTHOR CONTRIBUTIONS",
            "content": "All authors contributed to writing and editing the paper. Ahmed Heakl proposed the initial idea and motivation for the work, drafted the experimental settings, implemented and ran all experiments, collected the data, analyzed results, prepared visualizations, reviewed related work, wrote the first draft, edited the paper, and published the research artefacts. Martin Gubri provided daily supervision, helped refine the experimental settings, consolidated the papers narrative, proposed the out-of-distribution evaluation (Section 6.2) and the What do the routers learn? experiment  (Table 6)  , offered technical support, contributed to the initial draft, and reviewed the final draft. Ahmed Heakl and Martin Gubri jointly created the diagrams (Figures 13). Salman Khan provided feedback during the ideation phase and reviewed the draft. Seong Joon Oh and Sangdoo Yun suggested Figures 1 and 2, validated the experimental settings, and contributed to writing and editing the paper. Martin Gubri, Seong Joon Oh, and Sangdoo Yun provided weekly supervision and supported the project through organizational and funding contributions. SCORING, REWARD, AND ANSWER CHECKING Given an input and candidate path π, we run generation with the model constrained to π and obtain textual response ˆa. We then map ˆa to scalar reward R(ˆa, a): ARC (multi-choice). Extract letter AD via strict regex match (accepting optional Answer:). The reward is 1 for correct letter, 0 otherwise. DART (math). Extract the boxed expression (inject if needed for base models), then compute = grade answer(ˆa, a) [0, 1] using symbolic equivalence checker and robust string comparator. (a) Accuracy vs. average layers used under control interpolation. (b) Routing skip/execute/repeat. probability distribution across Figure 6: Fine-grained control in LLaMA-8B. (a) Accuracy as function of interpolated routing decisions, compared to baseline (red) and ours (green). (b) Histogram of routing probabilities. Shifts from execute skip correlate with higher accuracy, while repeat allocations increase computation."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Effect of loss choice under class imbalance. Macro F1 across training for weighted CE, focal, and plain CE. While all losses perform similarly on the majority execute class, only focal loss improves skip accuracy and yields non-trivial repeat accuracy, highlighting its necessity for minority classes. FINE-GRAINED CONTROL OF ROUTER DECISIONS Beyond analyzing learned routing policies, we study whether router decisions can be continuously controlled to balance accuracy and efficiency. Figure 6 reports results for LLaMA-8B. We introduce scalar control parameter [1, 1] that interpolates router probabilities with fixed skip, execute, or repeat distributions: π(p) = (1 t) πskip + πrouter, (1 t) πrouter + πexec, (1 t) πexec + πrepeat, (0.5, 1], = p0.5 0.5 . [1, 0.5], = p+1 0.5 , (0.5, 0.5], = p+0.5 1.0 , Here πrouter are the learned router probabilities, and πskip, πexec, πrepeat are one-hot distributions over the three actions. This formulation allows to smoothly traverse the spectrum from aggressive skipping to repeated execution, without retraining the router. Figure 6a shows that modest interpolation (p 0.5) reduces average layers while slightly increasing accuracy, suggesting that routers tend to over-execute by default. The distributional shifts in Figure 6b corroborate this: reallocating mass from execute toward skip correlates with accuracy gains, while reallocating toward repeat primarily increases computation with diminishing benefit. In sum, router behavior is not only learnable but also tunable post-training, enabling fine-grained control over the accuracyefficiency trade-off through single scalar knob. FOCAL VS. CROSS-ENTROPY UNDER CLASS IMBALANCE Router supervision is highly imbalanced: nskip = 4,399, nexecute = 120,956, nrepeat = 1,457. Plain cross-entropy minimizes error by predicting the dominant execute class, yielding trivial accuracy on skip/repeat. Weighted CE partly compensates, but still collapses on repeat. Focal loss (Lin et al., 2017) reweights classes and down-modulates easy majority examples, forcing learning on rare actions. As shown in Fig. 7, all losses perform similarly on execute, but focal substantially improves skip accuracy and is the only setup where non-trivial repeat accuracy is learned. Thus, focal loss is essential to mitigate imbalance and enable useful skip/repeat routing."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Effect of window size on router training. Larger pooling windows consistently improve minority-class accurac. Gains saturate beyond 16 windows, suggesting diminishing returns."
        },
        {
            "title": "E TRAINING ON MORE WINDOWS",
            "content": "Windowed mean pooling stabilizes router decisions by aggregating hidden states over larger contexts. Figure 8 shows that increasing the number of windows yields consistent improvements for minority actions. Skip accuracy rises from 0.32 (1 window) to 0.42 (32 windows), and repeat accuracy nearly doubles from 0.12 to 0.23. Execute accuracy stays unchanged at > 0.98, confirming that the majority class is unaffected. Macro-F1 improves from 0.42 to 0.53, with most of the gain realized between 8 and 16 windows, indicating that more granular context summaries significantly help routers capture rare actions without harming the dominant class."
        },
        {
            "title": "F MCTS TRAINING DATA ANALYSIS",
            "content": "Labels distribution. To better understand the supervision signal provided to the routers, we analyze the distribution of skip/execute/repeat actions across datasets and model families  (Fig. 9)  . Across all models and datasets, the vast majority of labels are execute, typically exceeding 90%, confirming the extreme class imbalance (nexecute nskip, nrepeat) and motivating focal loss with rebalancing during training (Sec. D). Skip ratios vary across datasets: ARC-Easy and ARC-Challenge exhibit noticeably higher skip counts than DART, suggesting that logical reasoning tasks permit redundancy while mathematical reasoning tasks require more thorough computation. Repeats are rare overall (13% of labels) but occur consistently across all datasets, with higher frequency in more challenging DART levels, indicating that repetition is targeted mechanism for difficult problems rather than generic operation. Model family and scale also influence distributions: LLaMA-Base models exhibit more balanced skip/execute ratios compared to their instruction-tuned counterparts, which strongly favor execution, while instruction-tuned variants slightly increase repeat counts. Larger 8B models reduce skips further, reflecting greater reliance on their depth, though still allocating some repeats when beneficial. Overall, the MCTS-derived labels capture structured, interpretable routing signals under heavy imbalance, requiring routers to learn policies where most layers execute but the rare skip and repeat actions play disproportionate role in efficiency and accuracy. Decisions per layer. Figure 10 reveals structured routing patterns that align with transformer computation phases. Across all model families, early layers (embedding and low-level processing) are almost always executed, indicating their necessity for stable representations. Middle layers show the highest variation, with frequent skips reflecting redundancy in feature composition. Late layers display higher repeat frequencies, particularly for the more difficult DART tasks, suggesting"
        },
        {
            "title": "Preprint",
            "content": "(a) LLaMA 3B (b) LLaMA 8B (c) LLaMA-Base 3B (d) LLaMA-Base 8B (e) Qwen 3B (f) Qwen 7B Figure 9: Label distribution across models. Distribution of skip/execute/repeat actions across datasets for different planners: (a) LLaMA-3B, (b) LLaMA-8B, (c) LLaMA-Base-3B, (d) LLaMABase-8B, (e) Qwen-3B, (f) Qwen-7B. that deeper refinement is allocated where multi-step reasoning is required. Instruction-tuned models exhibit more aggressive skipping than base models, supporting the view that fine-tuning creates functionally specialized layers that routers can prune more confidently. These trends confirm that Dr.LLM learns consistent, interpretable depth allocation policies across both model scale and family."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Per-layer routing frequency across datasets and models. Heatmaps show the mean usage per layer (0 = skip, 1 = execute, 2 = repeat) for six backbones: (a) LLaMA-3.2-3B-Instruct, (b) LLaMA-3.1-8B-Instruct, (c) LLaMA-3.2-3B-Base, (d) LLaMA-3.1-8B-Base, (e) Qwen2.5-3BInstruct, and (f) Qwen2.5-7B-Instruct. The x-axis corresponds to benchmark subsets (ARC-E, ARCC, DART15). Early layers are consistently executed, middle layers are frequently skipped, and late layers are occasionally repeated, especially on more complex DART levels."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "NAVER AI Lab",
        "Paramter Lab",
        "Tubingen AI Center",
        "University of Tubingen"
    ]
}