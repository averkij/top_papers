{
    "paper_title": "VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model",
    "authors": [
        "Jiani Zheng",
        "Lu Wang",
        "Fangkai Yang",
        "Chaoyun Zhang",
        "Lingrui Mei",
        "Wenjie Yin",
        "Qingwei Lin",
        "Dongmei Zhang",
        "Saravan Rajmohan",
        "Qi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the user's goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods."
        },
        {
            "title": "Start",
            "content": "VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model Jiani Zheng1, *, Lu Wang2, Fangkai Yang2, Chaoyun Zhang2, Lingrui Mei3,*, Wenjie Yin4, Qingwei Lin2, Dongmei Zhang2, Saravan Rajmohan2, Qi Zhang2 1Peking University, 2Microsoft, 3University of the Chinese Academy of Sciences, 4KTH Royal Institute of Technology {wlu, fangkaiyang, chaoyunzhang}@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the users goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods. The code is available at: https://github.com/microsoft/ GUI-Agent-RL, and the full project page can be found at VEM."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in tasks requiring common-sense reasoning, abstraction, and generalization, enabling their use in wide range of applications (Zhou et al., 2022; Zhang et al., 2024c), including autonomous Graphical User Interfaces *Work done during the internship at Microsoft. (GUI) agents (Zhang et al., 2024a; Wang et al., 2024b; Nguyen et al., 2024). GUI agents leverage the VLMs ability to process visual and language information for solving device control tasks that automate interactions with GUIs effectively. For example, they can decipher natural language instructions like Find the Save button and click it and use visual data to locate the relevant element on the GUI. This makes them valuable in automating routine GUI operations, from simple web navigation tasks to complex software interactions and management (Yan et al., 2023; Zhang and Zhang, 2023; Zhang et al., 2023; Rawles et al., 2024; Bai et al., 2024; Hong et al., 2024). Despite the progress in general-purpose VLMs like GPT4o (OpenAI, 2024) and Gemini 1.5 Pro (DeepMind, 2024), these models face significant challenges in completing real-world GUI tasks, even combined with prompting and tool usage (Xie et al., 2024; Zhang et al., 2024b,d; Bai et al., 2024). For example, they may misinterpret visual cues due to the complexity and variability of GUIs. minor change in the layout or design of GUI, like popup windows or repositioned button, can cause the model to make mistakes (Zhang et al., 2024a,b). As such, there is an urgent need to train specialized VLMs tailored to GUI tasks, enabling GUI agents to handle wider variety of GUI tasks with higher precision and efficiency. Reinforcement Learning (RL) (Sutton, 2018) is proved-effective way to align VLMs or LLMs with desired behaviors (Zhai et al., 2024; Sun et al., 2024). In approaches involving environment-based RL, models interact with environments where rewards are obtained directly from the environment (Toyama et al., 2021; Bai et al., 2024; Carta et al., 2023; Wang et al., 2024c; Lai et al., 2024). However, these methods are limited by online environmental reward dependency which is costly or unavailable, resulting in sample inefficiency (Xie et al., 2021; Niu et al., 2022). Other works simulate an 5 2 0 2 6 ] . [ 1 6 0 9 8 1 . 2 0 5 2 : r 2023; Chen et al., 2023). To operationalize this capability and address aforementioned challenges, we propose an environment-free RL framework that decouples value estimation from policy optimization. Unlike prior work that depends on reward models or direct environment interactions, our method leverages pretrained value environment model (VEM) to approximate state-action values directly from offline data. By distilling human-like priors into frozen VEM, our policy model bypasses the need for explicit reward engineering or error-prone next-state simulations. Additionally, the VEMs reliance on semantic reasoning (e.g., Does this action advance the users goal?) instead of pixel-level next-state predictions makes it resilient to superficial UI changes. Concretely, our framework operates in two stages: 1. Value Environment Model Pretraining: The VEM is trained offline to predict state-action values Q(s, a), capturing the long-term utility of actions in diverse GUI contexts. This avoids the compounding errors of next-state prediction by focusing on value estimation, which aligns better with VLMs inherent reasoning strengths. 2. Policy Exploration with Frozen VEM: During policy training, the VEM provides value-guided signals to iteratively refine the policys action selection. By directly exploring for high-value actions that grounded in the VEMs understanding of GUI semantics, the policy learns to generalize across unseen layouts and functionalities without online interaction. experiments To evaluate our method, we conduct rigorous on Android-in-the-Wild (AITW) (Rawles et al., 2024) using dual offline/online protocols. With only 500 training trajectories (one-third of the DigiRL (Bai et al., 2024) dataset scale data), our approach achieves 28.0%/21.0% offline task success on General/Webshopping domains, outperforming environment-free counterparts by 1228% and matching or exceeding environment-based approaches by 35%. In online deployment, we attain 42.4% general task success, surpassing environment-free methods by 1443% while remaining comparable to environment-based policies (38.98%) in procedural efficiency (7.83 vs. 7.25 average steps). Crucially, our method eliminates catastrophic failures of generic models (e.g., 0% webshopping success in GPT-4o) through Figure 1: Two GUI tasks with the action marked as red dot (). Although we do not see the next state after taking the action, we can estimate the state-action value. In (a), clicking the share button is unlikely to reveal reviews, implying low state-action value. In (b), the action may open the calendar app, making it plausible step to display the weeks events, suggesting high state-action value. environment to predict the next state and give rewards (Chae et al., 2024; Gu et al., 2024), but suffer from the fidelity issues and have compounding errors in sequential next state predictions (Guan et al., 2023; Zhang et al., 2024e; Ge et al., 2024). On the other hand, environment-free RL employs techniques such as offline RL (Snell et al., 2022; Hong et al., 2023; Bai et al., 2024; Wang et al., 2024a) or training reward models (Stiennon et al., 2020; Ouyang et al., 2022), allowing fine-tuning of large models using offline data without direct interaction with an environment. However, offline RL face challenges such as distribution shift (Levine et al., 2020) and limited exploration (Prudencio et al., 2023). Similarly, methods relying on learned reward models (Stiennon et al., 2020) often struggle to generalize beyond the static reward signals present in offline data, particularly in dynamic GUI environments where visual and functional changes (e.g., UI redesigns) can invalidate pre-defined reward criteria. As shown in Figure 1, humans excel at estimating the long-term utility of actions (i.e., state-action values Q(s, a)) without explicitly seeing the next state. VLMs, trained on vast corpora of humanGUI interactions, inherently encode similar priors about action outcomes (Hao et al., 2023; Bai et al., structured credit assignment, achieving 21.7% relative improvement over the strongest baseline without environmental interaction costs. These results demonstrate that offline policy optimization can rival online-trained systems while significantly advancing environment-free paradigms."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Environment-Based Methods Environment-based RL methods train VLMs interaction with GUI environthrough direct ments, where rewards are explicitly provided by the environment. Several frameworks like AndroidEnv (Toyama et al., 2021) and DistRL (Wang et al., 2024c) enable agents to learn through trialand-error interactions with real-world digital interfaces. Recent works such as DigiRL (Bai et al., 2024) and AutoWebGLM (Lai et al., 2024) demonstrate that environment-based RL can effectively align VLMs with complex GUI navigation tasks through autonomous exploration. However, these methods face significant limitations in sample efficiency due to the high cost of environment interactions (Xie et al., 2021; Niu et al., 2022), particularly in real-world applications where collecting online feedback is expensive and suffers from high latency. To address this, some approaches like WebRL (Qi et al., 2024) and WorldGPT (Ge et al., 2024) attempt to simulate GUI environments, but they struggle with state prediction accuracy and compounding errors in long interaction sequences (Guan et al., 2023; Zhang et al., 2024e). The fundamental challenge lies in the dynamic nature of real-world GUIs, where interface elements and layouts frequently change, making environment-dependent reward signals inherently unstable (Zhang et al., 2024b,d). 2.2 Environment-Free Methods Environment-free approaches bypass direct environment interaction by leveraging offline datasets or learned reward models. Offline RL methods (Snell et al., 2022; Hong et al., 2023) have shown promise in fine-tuning VLMs using precollected interaction trajectories, as demonstrated in large-scale GUI agent training by (Wang et al., 2024a) and (Bai et al., 2024). Alternative approaches employ reward modeling techniques (Stiennon et al., 2020; Ouyang et al., 2022) to predict task success signals from static datasets, enabling behavior alignment without environment feedback. General-purpose VLMs like GPT-4o (OpenAI, 2024) represent distinct category of environmentfree methods, leveraging their pre-trained visuallanguage capabilities for zero-shot GUI understanding without explicit RL training (Yan et al., 2023; Zhang et al., 2023). However, these methods face distinct challenges: offline RL suffers from distribution shift when deployed to novel GUI configurations (Levine et al., 2020), while reward models struggle to adapt to interface changes that invalidate their training criteria (Prudencio et al., 2023). Even advanced VLMs like GPT-4o exhibit limitations in handling GUI complexity, as their lack of task-specific fine-tuning leads to misinterpretation of visual elements and interface dynamics (Xie et al., 2024; Zhang et al., 2024b). Recent hybrid approaches like (Cheng et al., 2024) attempt to combine environment-free pre-training with targeted fine-tuning, but significant gaps remain in achieving robust generalization across diverse GUI ecosystems."
        },
        {
            "title": "3 Method",
            "content": "This section presents our method for training GUI agent with offline data, followed by an extended theoretical analysis. As shown in Figure 2, we first describe how to learn VEM from GPT-4o labeled data, then show that the resulting policy can achieve near-optimal performance under certain coverage and accuracy conditions. We further incorporate distribution-shift arguments to highlight the relationship between dataset quality and final policy performance. 3.1 Preliminary We formalize GUI navigation as Markov Decision Process = (S, A, P, r, γ) where: States S: Task descriptions + interaction histories + current GUI screenshot. Actions A: TYPE, PRESS_BACK, SCROLL_DOWN, ...} (see full actions in Appendix A). {DUAL_POINT, (ss, a): Unknown environment dynamics. Rewards r(s, a) {0, 1}: 0 for suboptimal choices, 1 for optimal actions. γ [0, 1): Discount factor. Given an offline dataset = {(si, ai, ri, i)}N i=1 collected by unknown behavior policies or suboptimal behavior policy β, our goal is to learn policy πϕ(as) maximizing expected returns without environment interaction. Figure 2: VEM Architecture: (1) Offline dataset annotation using GPT-4os task understanding, and VEM training via supervised regression. (2) Policy optimization through frozen VEM maximization, encouraging the policy model to explore high-value actions. 3.2 Value Environment Model Training core challenge in GUI automation is the scarcity of explicit reward signals that indicate whether chosen action advances or hinders task completion. Our strategy is to generate coarse but direct supervision by leveraging GPT-4os understanding of the GUI context. Specifically, we assign each state-action pair binary label, capturing whether the action is deemed beneficial or detrimental to the target task. This annotated supervision then guides the learning of VEM, which is stateaction value function Qθ, sidestepping the need for explicit environment interactions or hand-designed rewards. LLM-Guided Annotation. For each state-action pair (s, a) in our offline dataset D, we leverage GPT-4o with chain-of-thought reasoning (Wei et al., 2022) to generate binary labels ℓ(s, a) {0, 1}, simulating human-like task progress assessment. These annotations provide coarse supervision signals, where ℓ = 1 indicates actions expected to aid task completion, while ℓ = 0 marks potentially counterproductive steps. This labeling scheme approximates long-term value through immediate assessments, circumventing the need for explicit reward engineering. Supervised Value Learning. Using the annotated subset (cid:101)D = {(si, ai, ℓi)}, we fine-tune Qwen2VL (QwenLM, 2024) to predict label values through mean squared error minimization: (Qθ(s, a) ℓ)2(cid:105) (cid:104) (s,a,ℓ) (cid:101)D min θ The trained Qθ estimates action quality through environmental understanding distilled from GPT4os annotations, rather than online interactions, to reduce cost and latency during subsequent training. Stable Policy Guidance. After convergence, we freeze Qθ as fixed VEM to provide consistent action evaluations. While the binary labels represent simplified supervision, they effectively encode task progression patterns that guide subsequent policy learning. This approach maintains stability by decoupling environment modeling from policy optimization, while remaining fully offline-trainable. 3.3 Policy Learning with the Frozen VEM Having established VEM that can evaluate actions in any given GUI state, we now wish to derive policy that selects actions maximizing the predicted value. By freezing Qθ as fixed state-action value estimator, we transform policy learning into stable optimization problem that leverages consistent value predictions without environment interactions. Value Maximization Framework. Our policy πϕ(a s) learns to select high-value actions by maximizing: (πϕ) = EsD, aπϕ(s) (cid:2)Qθ(s, a)(cid:3), where the expectation is computed over states from the fixed dataset and actions from the learned policy. We implement this objective using Proximal Policy Optimization (PPO) (Schulman et al., 2017), where each training iteration samples minibatch of states from D, generates candidate actions through πϕ, and updates ϕ via gradient ascent on Qθs value estimates. This update rule increases the likelihood of actions that Qθ deems optimal, creating feedback loop that aligns the policy with the VEMs understanding of action quality. Offline Training Advantages. The frozen Qθ and static dataset enable purely offline learning, eliminating both the need for environment rollouts and the variance from on-policy data collection. This design ensures that policy updates rely solely on precomputed value estimates, avoiding the compounding errors that typically occur when alternating between policy improvements and value function updates. The resulting training process not only reduces computational costs but also stabilizes learning by maintaining fixed optimization targets throughout training. This approach effectively converts policy optimization into search over action selections that maximize fixed quality metric, leveraging the VEMs environmental understanding while circumventing the exploration challenges inherent in GUI automation. The combination of value-driven updates and offline execution provides practical solution to the sample efficiency and stability challenges in GUI policy learning. 3.4 Theoretical Analysis We now provide more advanced argument showing that if Qθ approximates Q, i.e., the optimal value model, on the support of D, then the learned policy π can achieve near-optimal returns. In addition, we introduce distribution shift considerations and demonstrate how coverage of influences policy quality. Offline Coverage and Value Approximation. We introduce two conditions which bounds the suboptimality gap relative to the optimal policy π: Coverage Definition. For policy π, define the normalized state-action distribution dπ over (or the environment). Roughly speaking, dπ(s, a) captures how often π visits (s, a). We say π is supported by if, whenever π(a s) > 0, the dataset contains sufficient samples of (s, a). Let π β measure how different π is from the behavior policy β. If π β is large, then π may choose actions not well-represented in D. Approximate Q-Function. Assume there is small ε 0 such that (cid:12) Qθ(s, a) Q(s, a)(cid:12) (cid:12) (cid:12) ε (s, a) D. This assumption states that on all (s, a) pairs in D, Qθ is within ε of the optimal Q-value Q. Performance Bound with Distribution Shift. We introduce the extended performance bound theorem below: Theorem 3.1 (Extended Performance Bound). Let (cid:98)π maximizes (π) = EsD, aπ(s)[Qθ(s, a)]. Under coverage and approximation conditions: 1. Coverage: (cid:98)π remains within the support of D. Formally, whenever (cid:98)π(a s) > 0, β(a s) > 0. 2. Approx. Q: For all (s, a) D, we have (cid:12)Qθ(s, a) Q(s, a)(cid:12) (cid:12) (cid:12) ε. Then there exists constant > 0 (depending on γ and the horizon) such that J(π) J((cid:98)π) (cid:0)ε + (cid:98)π β(cid:1). Moreover, because (cid:98)π queries fixed Q-function from static dataset, the variance of its gradient estimates can be significantly lower than that of an on-policy RL method interacting with stochastic environment. The proof details of Theorem 3.1 are shown in Appendix B. Implementation Consequences. Theorem 3.1 reveals two critical pathways for improving policy performance: 1. Enhancing Value Estimation: ε measures how well Qθ matches on the offline data. It can be reduced ε through better VEM training (e.g., improved GPT-4o labeling or regression architecture) 2. Managing Distribution Shift: (cid:98)π β measures how far the learned policy strays from the datasets distribution. It can be controlled via dataset diversification or policy constraints (e.g., collecting diverse or near-optimal transitions) 3.5 Stability Advantages Unlike on-policy RL, which repeatedly interacts with an environment and can suffer from highvariance or unsafe exploration, our offline approach trains solely from fixed dataset and frozen VEM: Fixed Surrogate Rewards: Every query to Qθ for (s, a) yields deterministic estimate, eliminating environment stochasticity. No On-Policy Data Collection: The policy never modifies the dataset distribution; hence, distribution shift during training is avoided. Reduced Gradient Variance: Policy updates rely on consistent Q-values from Qθ, typically leading to smoother convergence. This results in stable and efficient training process, especially beneficial for complex GUI tasks where real-time interactions may be expensive or infeasible. Our method combines GPT-4o annotations with VLM, e.g., Qwen2VL, to produce frozen VEM Qθ. policy πϕ then learns by maximizing E[Qθ(s, a)] over states in the offline dataset D. The extended theoretical analysis reveals that if Qθ accurately approximates on the data support and the learned policy does not stray outside that support, then near-optimal performance can be guaranteed. Moreover, training stability improves by avoiding on-policy rollouts in stochastic or expensive environment. We validate these insights experimentally, emphasizing how Q-function accuracy directly impacts final policy effectiveness."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we verify the performance of the model on two subsets of Android In-the-Wild (AITW) (Zhang et al., 2024d) on the Android platform following DigiRL (Bai et al., 2024) in both offline and online settings. 4.1 Data Collection Training data for Critic Model. To align Qwen2VL with the input-output schema and evaluation framework of the critic model, we employed GPT-4o for multimodal data annotation and conducted systematic value assessments of agent actions. As depicted in Figure 1, our hierarchical evaluation system defines two action quality levels based on task-oriented effectiveness. Level 1 (Suboptimal Actions). manifest deviations from optimal task execution, specifically including: (1) erroneous text inputs compromising workflow integrity, (2) interface interactions triggering adversarial outcomes such as advertisement redirections, and (3) premature declarations of task completion prior to objective fulfillment. Level 2 (Optimal Actions). demonstrate maximally effective task-solving behaviors, characterized by three critical patterns: (1) verifiable task completion through interface state validation, (2) robust recovery strategies exemplified by PRESS_HOME command execution for interface reset, and (3) context-aware selection of optimal entry points for subtask resolution. The complete evaluation prompts and annotation protocols are formally specified in Appendix C. Table 1: AITW dataset composition: training/testing episodes and interaction steps with action quality levels (1/2) across domains. Category Split Episodes Steps Level 1 Level 2 General Webshopping train test train test 436 100 560 91 3340 777 6240 1187 214 1939 273 2153 563 4301 499 The AITW benchmark comprises 30K instructions with 715K operational trajectories. Adopting SeeClick (Cheng et al., 2024) instruction-level partitioning, we analyze General and Webshopping domains. Table 1 quantifies training/testing episodes and step distributions with action quality levels (Level 1/2). GPT-4o annotations achieve 90% human consistency with 3-hour processing efficiency. reinforcement Data Format The learning paradigm requires standardized data transformations across input modalities. For Qwen2VL, we normalize bounding box coordinates to device-agnostic [0,1] screen-space coordinates. retains Auto-GUI absolute coordinate pairs (touch/lift positions) for gesture modeling. Detailed action space configurations are provided in Section 3.1. (Zhang and Zhang, 2023) 4.2 Implementation Details Critic model The Qwen2VL model underwent supervised fine-tuning through the LLaMAFactory framework to develop state classification capabilities. Our architectural modifications enabled hierarchical state prediction from multimodal inputs containing four distinct elements: textual task descriptions, historical interaction records, current screen imagery, and pending action sequences. The formal specification of input composition and prompting strategy appears in Appendix C. Training leveraged distributed data parallelism on an 8-GPU NVIDIA A100 cluster, configured with 10 training epochs and global batch size of 64 (8 samples per GPU). The optimization process employed AdamW with an initial learning rate of 1 105, achieving convergence within 12 hours while maintaining computational efficiency through gradient accumulation strategies. Policy model The policy architecture builds upon the Supervised Fine-Tuned (SFT) Auto-GUI foundation model while maintaining parameter freezing of the Critic module. Our implementation executed full-parameter optimization on single NVIDIA A100 accelerator, configuring the training process with batch size of 32 samples and 1106 across five training epochs. This configuration achieved stable convergence through progressive reward signal alignment, demonstrating parameter-efficient adaptation characteristics. Notably, the gradient update strategy incorporated differential learning rate scheduling between the frozen Critic components and tunable policy layers, effectively balancing knowledge retention with operational flexibility. The complete training regimen required under 8 hours of computation time, reflecting optimized memory utilization patterns. Baselines Our evaluation incorporates representative methods in embodied GUI interaction research. We consider GPT-4o (OpenAI, 2024) as the foundational model without task-specific finetuning, alongside Auto-GUI (Zhang and Zhang, 2023) trained through standard supervised finetuning (SFT). The comparison extends to specialized agents including CogAgent (Hong et al., 2024) and SeeClick (Cheng et al., 2024), both employing SFT with expanded architectural configurations, as well as Digirl (Bai et al., 2024) which enhances Auto-GUIs capabilities through combined offline pretraining and online reinforcement learning framework. This baseline selection captures diverse approaches to GUI interaction while maintaining methodological coherence across different training paradigms and architectural developments. 4.3 VEM Performance We evaluate the performance of our VEM models on both the General and WebShopping datasets, as shown in Table 2. Our trained VEM achieves an F1 score of 78% and an accuracy of 69% on the two datasets, demonstrating high performance reliability. This level of accuracy is sufficient to drive policy optimization via exploration. As shown in case in Figure 3, the exploration trajectory exhibits three distinct phases: an initial conservative exploitation phase with limited action diversity, followed by transitional exploration phase characterized by increasing behavioral variance, and finally, an expanded exploration phase that systematically navigates high-dimensional action spaces. This phased progression empirically demonstrates the models ability to autonomously balance the exploitation-exploration tradeoff while maintaining stable policy improvement gradients. Table 2: Performance on the VEM. Dataset General Webshopping Precision Recall 0.78 0.79 0.81 0.82 F1 Accuracy 0.79 0. 0.71 0.75 Table 3: Offline evaluation results on the AITW benchmark. General Webshopping Step SR Task SR Step SR Task SR GPT-4o Auto-GUI CogAgent Seeclick Digirl (offline) Digirl (online) Ours 32.0 83.3 73.7 83.3 80.0 83.3 82.6 9.0 20.0 16.0 26.0 21.0 23.0 28.0 30.6 78.0 72.2 79.1 80.2 78.5 81.1 0.0 18.0 9.0 18.0 14.0 17.0 21.0 4.4 Main Results We employ two complementary evaluation schemes: (1) Offline Evaluation computes step/- task success rates (SRs) by comparing predicted actions with human annotations in test data; (2) Online Evaluation deploys the agent in real Android environments (aligned with DigiRLs setup) using GPT-4o as automated judge, with 10-step attempt limits and duplicate task removal. The offline setting measures precise action matching through deterministic verification, while the online test evaluates practical deployment capability under environmental dynamics. This dual approach separates basic competence assessment from real-world performance analysis, ensuring comprehensive model validation. The experimental results shown in Table 3 demonstrate significant advantages of our approach. With the same Auto-GUI policy model, our method achieves 28.0% task success rate on General domain and 21.0% on Webshopping, surpassing all baselines including DigiRLs online variant (23.0% and 17.0% respectively). This 21.7% relative improvement over the strongest baseline confirms that value model supervision effectively guides policy optimization without requiring additional data collection or environmental interaction. Notably, our approach outperforms DigiRLs online training results despite operating purely offline, particularly in task-level metrics where conventional RL methods typically struggle. The complete failure of GPT-4o on Webshopping tasks (0% success rate) further emphasizes the necessity of domain-specific training, which our specialized Figure 3: Action space exploration patterns demonstrated by the value model during policy training. learning methods (Auto-GUI:7.92 steps, CogAgent:7.23) exhibit moderate trajectory lengths, our RL-enhanced framework maintains near-optimal path efficiency (7.83 General, 7.86 Webshopping) without sacrificing success rates. This contrasts with DigiRLs online strategy that prioritizes minimal steps (7.25 General) at the cost of reduced accuracy, suggesting our method better balances exploration and exploitation. More cases can be founded in Appendix D. The performance discrepancy between DigiRLs offline (14.29% Webshopping) and online (11.43%) results stems from overfitting to simplified training scenarios, whereas our approachs adaptive credit assignment mechanism proves more resilient to environmental novelty. Our evaluation demonstrates that with limited training data, our method consistently outperforms DigiRLs offline approach and matches its online variant, validating the sample efficiency of our framework. 4.5 Analysis Training stability As evidenced in Figure 4, our policy training methodology achieves substantially improved stability compared to standard Proximal Policy Optimization (PPO) frameworks. While typical PPO implementations exhibit erratic loss fluctuations due to gradient update instability and high-variance advantage estimation, our approach mitigates these instability patterns through gradientconstrained policy updates and adaptive exploration regulation. The stabilized training dynamics not only reduce convergence variance but also enhance both training efficiency and model generalizability, addressing fundamental challenges in policy optimization processes. Figure 4: Q-value loss progression during policy model training. Table 4: Online deployment performance on AITW benchmark. General Webshopping Task SR Step Length Task SR Step Length GPT-4o Auto-GUI CogAgent Seeclick Digirl (offline) Digirl (online) Ours 0.00 28.81 38.98 25.42 38.98 38.98 42.37 9.00 7.92 7.23 8.80 7.61 7.25 7.83 0.00 2.86 14.29 11.43 14.29 11.43 14. 9.91 9.34 7.80 9.80 8.26 7.37 7.86 models address through tailored value estimation. These findings establish that structured credit assignment can compensate for the absence of online exploration, achieving state-of-the-art performance through more sample-efficient learning. The online evaluation demonstrates our methods robustness in dynamic environments. Under real-world conditions with random disturbances (e.g., advertisement popups), our approach achieves superior task success rates (42.37% General, 14.29% Webshopping), outperforming all baselines including DigiRLs online variant (38.98%, 11.43%). This 8.7% improvement on General tasks highlights enhanced generalization capability despite equivalent environmental access. Analysis of interaction patterns reveals distinct behavioral characteristics: While supervised Figure 5: case study of task execution trajectory comparison with DigiRL. Case Study Deploying GUI agents in real-world scenarios presents inherent challenges due to realtime system dynamics and environmental stochasticity, often leading to unintended navigation to advertisement interfaces during task execution. While models trained solely via SFT exhibit limited error recovery in such open-world settings, our valueguided approach enables robust policy adaptation. As shown in Figure 5, our method achieves minimal-path task completion through precise state valuation, whereas DigiRL offline baselines frequently misidentify terminal states, and DigiRL online methods suffer from suboptimal trajectories with recovery loops. This capability arises from the value models continuous interpretation of environmental feedback, facilitating dynamic policy correction absent in conventional training paradigms. Extended comparative analyses with alternative baselines are provided in Appendix D."
        },
        {
            "title": "5 Conclusion",
            "content": "We presents an environment-free RL framework for GUI automation that decouples value estimation from policy optimization through Value Environment Model (VEM). Our approach replaces errorprone next-state simulations with semantic reasoning over GUI elements, enabled by offline learning from human demonstration data. The two-stage training paradigm achieves structured credit assignment without environmental interaction, while maintaining procedural efficiency comparable to environment-based methods. Experimental results on Android-in-the-Wild demonstrate superior task success rates over existing environment-free approaches and significant improvements in generalization capability compared to vision-language models. The framework establishes semanticdriven value estimation as an effective pathway for layout-agnostic GUI automation with sample efficiency. In the future, we plan to explore selfsupervised approaches for training the value model, aiming to reduce labeling overhead and further improve scalability."
        },
        {
            "title": "References",
            "content": "Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. 2024. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. arXiv preprint arXiv:2406.11896. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. 2023. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 36763713. PMLR. Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. 2024. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935. Google DeepMind. 2024. Gemini 1.5 pro. https:// deepmind.google/technologies/gemini/pro/. Accessed: 2025-01-14. Zhiqi Ge, Hongzhe Huang, Mingze Zhou, Juncheng Li, Guoming Wang, Siliang Tang, and Yueting Zhuang. 2024. Worldgpt: Empowering llm as multimodal world model. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 7346 7355. Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. 2024. Is your llm secretly world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559. Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. 2023. Leveraging pretrained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36:7908179094. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Joey Hong, Sergey Levine, and Anca Dragan. 2023. Zero-shot goal-directed dialogue via rl on imagined conversations. arXiv preprint arXiv:2311.05584. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. 2024. Autowebglm: Bootstrap and reinforce large language model-based web navigating agent. arXiv preprint arXiv:2404.03648. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643. Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. 2024. Gui agents: survey. arXiv preprint arXiv:2412.13501. Haoyi Niu, Yiwen Qiu, Ming Li, Guyue Zhou, Jianming Hu, Xianyuan Zhan, et al. 2022. When to trust your simulator: Dynamics-aware hybrid offline-andonline reinforcement learning. Advances in Neural Information Processing Systems, 35:3659936612. OpenAI. 2024. Gpt-4o. https://openai.com/ index/hello-gpt-4o/. Accessed: 2025-01-14. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Rafael Figueiredo Prudencio, Marcos ROA Maximo, and Esther Luna Colombini. 2023. survey on offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on Neural Networks and Learning Systems. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, et al. 2024. Webrl: Training llm web agents via self-evolving online curarXiv preprint riculum reinforcement learning. arXiv:2411.02337. QwenLM. 2024. Qwen2.5-vl. https://github.com/ QwenLM/Qwen2.5-VL. Accessed: 2025-01-14. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxiAlec Radford, and Oleg Klimov. 2017. mal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. 2022. Offline rl for natural language generation with implicit language learning. arXiv preprint arXiv:2206.11871. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008 3021. Chuanneng Sun, Songjun Huang, and Dario Pompili. 2024. Llm-based multi-agent reinforcement learning: Current and future directions. arXiv preprint arXiv:2405.11106. Richard Sutton. 2018. Reinforcement learning: An introduction. Bradford Book. Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. 2021. Androidenv: reinforcement learning platform for android. arXiv preprint arXiv:2105.13231. Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray Huang, Si Qin, et al. 2024a. Large action models: Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. 2024c. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. 2024d. Android in the zoo: Chain-of-action-thought for gui agents. arXiv preprint arXiv:2403.02713. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. 2024e. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios arXiv preprint that are difficult for humans? arXiv:2408.13257. Zhuosheng Zhang and Aston Zhang. 2023. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):23372348. From inception to implementation. arXiv preprint arXiv:2412.10047. Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, and Ruiming Tang. 2024b. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890. Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. 2024c. Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents. arXiv preprint arXiv:2410.14803. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. 2021. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. Advances in neural information processing systems, 34:2739527407. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. 2024. Osworld: Benchmarking multimodal agents for openended tasks in real computer environments. arXiv preprint arXiv:2404.07972. An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. 2023. Gpt4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. 2024. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292. Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Qingwei Lin, Saravan Rajmohan, et al. 2024a. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279. Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, et al. 2024b. Ufo: uifocused agent for windows os interaction. arXiv preprint arXiv:2402.07939. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771."
        },
        {
            "title": "A Actions",
            "content": "The available actions include DUAL_POINT, TYPE, PRESS_BACK, PRESS_HOME, SCROLL_DOWN, SCROLL_UP, SCROLL_LEFT, SCROLL_RIGHT, PRESS_ENTER, STATUS_TASK_COMPLETE, and STATUS_TASK_IMPOSSIBLE."
        },
        {
            "title": "B Proof of Extended Performance Bound",
            "content": "Proof. First, by the approximate accuracy assumption, Qθ within ε on D. Thus, for any (s, a) in the dataset support, maximizing Qθ(s, a) is nearly the same as maximizing Q(s, a). Second, the coverage requirement ensures that (cid:98)π does not select actions absent from D. Standard distribution shift arguments (cf. (Levine et al., 2020)) show that if (cid:98)π β is small, then (cid:98)π does not deviate far from the offline distribution. Combining these points yields bound J(π) J((cid:98)π) c(ε + (cid:98)π β) for some constant c. Finally, since all policy updates use (cid:8)(s, a) D(cid:9) and fixed Q-model, the training variance is typically much lower than that of an on-policy method that must repeatedly sample from potentially noisy environment."
        },
        {
            "title": "C Prompt",
            "content": "Prompt of GPT-4o input As an expert in the field of GUI and reinforcement learning , you will receive complete screenshots and textual descriptions of interactions for given task . You need to evaluate specific step in terms of its value within the task chain , similar to what value function does in reinforcement learning . Detailed criteria and standards are given below . ## Explanation of the input content : 1. Task : Brief description of the current GUI task , such as implementing the \" Get Hong Kong hotel prices \" task in Android GUI . 2. Complete operation description and corresponding screenshot sequence for the task (1) Text description of operations : Contains 11 types of GUI operations . Specific fields and their meanings are as follows : [1] DUAL_POINT : Double - click on specific position on the screen . If it is link or software , it will enter ; if it is text , it will be selected . The \" click_point \" is represented by two - dimensional array indicating the position of the click , relative to the top - left corner of the screenshot and within range from 0.0 to 1.0. - example : \" action_type \": \" DUAL_POINT \" , \" click_point \": [0.5 , 0.5] [2] TYPE : An action type that sends text . Note that this simply sends text and does not perform any clicks for element focus or enter presses for submitting text . - example : \" action_type \": \" TYPE \", \" typed_text \": \" capital of England \" [3] PRESS_BACK : Return to the previous page . Usually the previous webpage . - example : \" action_type \": \" PRESS_BACK \" [4] PRESS_HOME : Return to the system home page . Use this action to return to the home screen when the current screen is not the desired one , so you can reselect the program you need to enter . - example : \" action_type \": \" PRESS_HOME \" [5] PRESS_ENTER : Press the enter key to execute step . Generally , after confirming the input text , use this action to start the search . - example : \" action_type \": \" PRESS_ENTER \" [6] STATUS_TASK_COMPLETE : An action used to indicate that the desired task has been completed and resets the environment . This action should also be used if the task is already completed and there is nothing more to do . For example , the task is to turn on the Wi - Fi when it is already on . - example : \" action_type \": \" STATUS_TASK_COMPLETE \" [7] STATUS_TASK_IMPOSSIBLE : An action used to indicate that the desired task is impossible to complete and resets the environment . This can result from various reasons including UI changes , Android version differences , etc . - example : \" action_type \": \" STATUS_TASK_IMPOSSIBLE \" [8] SCROLL_DOWN : Scroll down . - example : \" action_type \": \" SCROLL_DOWN \" [9] SCROLL_UP : Scroll up . - example : \" action_type \": \" SCROLL_UP \" [10] SCROLL_LEFT : Scroll left . - example : \" action_type \": \" SCROLL_LEFT \" [11] SCROLL_RIGHT : Scroll right . - example : \" action_type \": \" SCROLL_RIGHT \" (2) Corresponding screenshot before each operation . If the operation is of the \" DUAL_POINT \" type , the click position is marked with red dot in the image . 3. The current action to be evaluated and the corresponding screenshot . ## Evaluation Criteria : Here are the detailed descriptions of the two levels . Attention needs to be paid to whether the action taken based on the current screenshot promotes efficient task execution , rather than the relevance of the content shown in the current screenshot to the task : Level 1: The action is not the optimal choice for completing the task at this moment , which may lead to deviations from the task flow . For example : (1) Incorrect text input . (2) Clicking button that might lead to an advertisement . (3) Announcing the task 's success when it has not actually been achieved . Level 2: The action is the optimal and correct choice for completing the task at this moment . For example : (1) When showing task completion , the displayed content can fully achieve it . (2) When entering an unrelated interface , you can return to the main screen by executing \" PRESS_HOME .\" (3) Selecting the most correct entry point to complete the current task . ## Output requirements : - Format : {\" rating \": int , \" explanation \": str }. Do not include any additional characters beyond this format - The \" rating \" field should be represented by the number 1 or 2 indicating the evaluation level . The \" explanation \" field should explain the evaluation process that led to this rating , without including descriptions of operations after the current step ( future operations are considered unknown ). ## Example Input : Task Requirements : What is the capital of England ? Action and ScreenShot : step 0: \" action_type \": \" DUAL_POINT \", \" click_point \": \"[0.524 , 0.06]\" step 1: \" action_type \": \" TYPE \" , \" typed_text \": \" capital of England \" step 2: \" action_type \": \" PRESS_ENTER \" step 3: \" action_type \": \" STATUS_TASK_COMPLETE \" Current Action : step 2: \" action_type \": \" PRESS_ENTER \" ## Example Output : {\" rating \": 2, \" explanation \": \" The action of pressing enter after typing ' capital of England ' is an appropriate step to get the answer to the task requirement of finding out the capital of England , which is an optimal action towards achieving the task goal .\"} Task Requirements : {} Action and ScreenShot : {} Current Action : {} Prompt of critic input As an expert in the field of GUI and reinforcement learning , you will receive textual descriptions of history interactions for given task . You need to evaluate the current action , similar to what value function does in reinforcement learning . Detailed criteria and standards are given below . ## Explanation of the input content : 1. Task : Brief description of the current GUI task , such as implementing the \" Get Hong Kong hotel prices \" task in Android GUI . 2. Description of History operation Contains 11 types of GUI operations . Specific fields and their meanings are as follows : [1] DUAL_POINT : Double - click on specific position on the screen . If it is link or software , it will enter ; if it is text , it will be selected . The \" click_point \" is represented by two - dimensional array indicating the position of the click , relative to the top - left corner of the screenshot and within range from 0.0 to 1.0. - example : \" action_type \": \" DUAL_POINT \" , \" click_point \": [0.5 , 0.5] [2] TYPE : An action type that sends text . Note that this simply sends text and does not perform any clicks for element focus or enter presses for submitting text . - example : \" action_type \": \" TYPE \", \" typed_text \": \" capital of England \" [3] PRESS_BACK : Return to the previous page . Usually the previous webpage . - example : \" action_type \": \" PRESS_BACK \" [4] PRESS_HOME : Return to the system home page . Use this action to return to the home screen when the current screen is not the desired one , so you can reselect the program you need to enter . - example : \" action_type \": \" PRESS_HOME \" [5] PRESS_ENTER : Press the enter key to execute step . Generally , after confirming the input text , use this action to start the search . - example : \" action_type \": \" PRESS_ENTER \" [6] STATUS_TASK_COMPLETE : An action used to indicate that the desired task has been completed and resets the environment . This action should also be used if the task is already completed and there is nothing more to do . For example , the task is to turn on the Wi - Fi when it is already on . - example : \" action_type \": \" STATUS_TASK_COMPLETE \" [7] STATUS_TASK_IMPOSSIBLE : An action used to indicate that the desired task is impossible to complete and resets the environment . This can result from various reasons including UI changes , Android version differences , etc . - example : \" action_type \": \" STATUS_TASK_IMPOSSIBLE \" [8] SCROLL_DOWN : Scroll down . - example : \" action_type \": \" SCROLL_DOWN \" [9] SCROLL_UP : Scroll up . - example : \" action_type \": \" SCROLL_UP \" [10] SCROLL_LEFT : Scroll left . - example : \" action_type \": \" SCROLL_LEFT \" [11] SCROLL_RIGHT : Scroll right . - example : \" action_type \": \" SCROLL_RIGHT \" 3. The current action to be evaluated and the corresponding screenshot ( the screenshot before each operation . If the operation is of the \" DUAL_POINT \" type , the click position is marked with red dot in the image .) ## Evaluation Criteria : Here are the detailed descriptions of the two levels . Attention needs to be paid to whether the action taken based on the current screenshot promotes efficient task execution , rather than the relevance of the content shown in the current screenshot to the task : Level 1: The action is not the optimal choice for completing the task at this moment , which may lead to deviations from the task flow . For example : (1) Incorrect text input . (2) Clicking button that might lead to an advertisement . (3) Announcing the task 's success when it has not actually been achieved . Level 2: The action is the optimal and correct choice for completing the task at this moment . For example : (1) When showing task completion , the displayed content can fully achieve it . (2) When entering an unrelated interface , you can return to the main screen by executing \" PRESS_HOME .\" (3) Selecting the most correct entry point to complete the current task . ## Output requirements : 1 or 2 ( INT ) ## Example Input : Task Requirements : What is the capital of England ? Previous Action : step 0: \" action_type \": \" DUAL_POINT \", \" click_point \": \"[0.524 , 0.06]\" step 1: \" action_type \": \" TYPE \" , \" typed_text \": \" capital of England \" Current Action and Screenshot : step 2: \" action_type \": \" PRESS_ENTER \" ## Example Output : 2 Task Requirements : {} Previous Action : {} Current Action and Screenshot : < image > {}"
        },
        {
            "title": "D Case Study",
            "content": "Here we randomly sample cases (Figure 6, 7, 8, 9), from our test experiments to show the difference between our method and baselines. Figure 6: Case study of Ours, DigiRL offline and DigiRL online on the task: whats the latest video from GameSpot reviews? Figure 7: Case study of Ours, DigiRL offline and DigiRL online on the task: search for top rated burger restaurants on Google Maps. Figure 8: Case study of Ours, DigiRL offline and DigiRL online on the task: whats on the menu at Red Lobster? Figure 9: Case study of Ours, DigiRL offline and DigiRL online on the task: what is the speed of rocket?"
        }
    ],
    "affiliations": [
        "KTH Royal Institute of Technology",
        "Microsoft",
        "Peking University",
        "University of the Chinese Academy of Sciences"
    ]
}