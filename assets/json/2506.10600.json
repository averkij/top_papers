{
    "paper_title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence",
    "authors": [
        "Wang Xinjie",
        "Liu Liu",
        "Cao Yu",
        "Wu Ruiqi",
        "Qin Wenkang",
        "Wang Dehui",
        "Sui Wei",
        "Su Zhizhong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html."
        },
        {
            "title": "Start",
            "content": "EmbodiedGen: Towards Generative 3D World Engine for Embodied Intelligence Xinjie Wang1 Liu Liu1 Yu Cao2 Ruiqi Wu5,1 Wenkang Qin2 Dehui Wang4,3 Wei Sui3 1Horizon Robotics 2GigaAI Zhizhong Su1 3D-Robotics 5VCIP, CS, Nankai University 4Shanghai Jiao Tong University 5 2 0 2 2 1 ] . [ 1 0 0 6 0 1 . 6 0 5 2 : r Figure 1. EmbodiedGen, toolkit for embodied intelligence interactive 3D world generation. EmbodiedGen enables controllable generation of rigid and articulated assets with accurate real-world scale and physical properties, along with stylistically diverse background generation and visually rich texture generation and editing. These assets can be seamlessly integrated into various simulators such as OpenAI Gym[4], Isaac Lab[26], MuJoCo[42] and SAPIEN[49]. These capabilities form foundation for digital twinning, large-scale data augmentation and embodied intelligence tasks such as manipulation and navigation across wide range of simulation environments."
        },
        {
            "title": "Abstract",
            "content": "Constructing physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, foundational platIt enables the form for interactive 3D world generation. scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, fullfeatured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. EmbodiedGen1 are publicly available to foster future research. based generative models [16, 29, 37, 47] and 3D asset generation [5, 11, 12, 23, 31, 57] have sparked growing interest in bridging this gap. However, existing 3D generation toolkit often fall short for robotics applications, as conventional graphics assets typically lack physical realism, watertight geometry and accurate scale, leading to unreliable collision modeling and unrealistic interactions in simulators. We introduce EmbodiedGen, toolkit for interactive 3D world generation, enables low-cost, high-quality, and highly controllable asset generation in URDF, complete with watertight geometry and physically plausible properties. Our main contributions are: 1. Toolkit for interactive 3D world generation. EmbodiedGen is the first comprehensive toolkit for generating interactive 3D worlds to the needs of embodied intelligence related research. It supports real-to-sim digital twin creation and enables the controllable generation of diverse 3D assets, which can be seamlessly imported into simulators. 2. Physically accurate, simulator-ready assets with high fidelity. EmbodiedGen generates assets that not only achieve state-of-the-art visual quality but are also physically plausible and ready for direct use in simulation environments. Each asset is enriched with physical properties, inspection metadata, textual descriptions, watertight geometry and dual representations in both 3D Gaussian Splatting (3DGS) and mesh formats. 3. Accessible and open-source. We release easy-to-use, open-sourced pipelines and services to facilitate community development and research in embodied intelligence. 1. Introduction 2. Related Work Despite the remarkable success of foundation models such as CLIP [35] and GPT [1, 34], which leverage largescale internet data, extending this paradigm to the needs of embodied intelligence related research presents significant challenges. Data collection for embodied AI tasks is substantially more expensive and constrained, often requiring real world interaction, involving complex physical dynamics, making each data point several orders of magnitude more costly. Moreover, robotic data is often context-specific and non-transferable across tasks or embodiments, severely limiting reusability and scalability. Achieving general-purpose embodied intelligence in the physical world requires techniques such as digital twins, simulation-based augmentation, and reinforcement learning in physically realistic environments. These goals demand access to large-scale, diverse, and high-quality 3D asset libraries, as well as efficient pipelines for rapidly constructing interactive 3D environments. Recent advances in diffusion 1https : / / horizonrobotics . github . io / robot _ lab / embodied_gen/index.html 3D Asset Generation The goal of 3D object generation is to produce corresponding 3D representation from an input image or textual description. Existing approaches to this task can be broadly categorized into three representative paradigms: feedforward generation, optimizationbased generation, and view reconstruction. Feedforward generation leverages large models to produce 3D representation of the input prompt in single forward pass. This category includes methods such as LRM[11], PixelSplat[5], GRM[52], and MVSplat[6], which are notable for their inference time efficiency. Optimization-based generation, as exemplified by methods like DreamFusion[30] and DreamMat[58], directly optimizes the parameters of the 3D representation using score distillation sampling (SDS) guided by diffusion models and differentiable rendering. This often results in higher-quality outputs at the cost of increased computation time. View reconstruction methods generate multi-view 2D images and reconstruct the final 3D representation via sparse-view geometry. Representative works in this line include Zero123[22], Unique3d[46], Driven by the MVDream[39], and MV-Adapter[17]. 2 Figure 2. The framework of EmbodiedGen. It enables the creation of digital twin within simulation environment from single image. Alternatively, given task description, EmbodiedGen autonomously generates the scene layout, synthesizes detailed 3D object assets, and arranges them in semantically and physically plausible configurations. This facilitates the effortless construction of an interactive 3D world, supporting wide range of embodied intelligence related research in diverse virtual environments. demand for higher-quality 3D objects, recent methods such as CLAY[56], Hunyuan3D[41], Meta3DGen[3], and Trellis[50] have adopted decoupled pipeline separating geometry and texture generation into two stages, followed by texture reprojection to fuse geometry with realistic textures. Beyond rigid object generation, methods such as URDFormer[7] and SINGAPO[21] have been proposed to generate articulated objects. However, these methods are primarily limited to graphics-centric object generation. The resulting objects lack real-world scale and physical properties, and there is no guarantee of watertightness or geometric integrity. These limitations significantly hinder their direct applicability in physics-based simulators."
        },
        {
            "title": "Scene",
            "content": "methods for flexible Generation Recent 3D like LucidDreamer[8] and adopt 3DGS[19] consistent scene rendering, but are mainly limited to forward facing views. To enable full 360 scene generation, panoramic representations have been explored. PERF[43] pioneered panoramic NeRFs for novel view synthesis from single panorama. DreamScene360[60], HoloDreamer[59] and WorldGen[51] extended this with panoramic Gaussian splatting, while LayerPano3D[53] introduced layered panoramas that are lifted into 3D splatting for handling complex scenes. However, these methods are limited to interactivity, makgenerating static 3D scenes without ing them unsuitable for the requirements of embodied intelligence related research. Embodied Intelligence Tasks Prior works such as RoboTwin[28], Gen2Sim[18], MatchMaker[45] and ACDC[9] have explored using 3D generation techniques to augment asset libraries within simulators. However, due to limitations in the quality and efficiency of 3D generation, the diversity of assets remains limited, and the environments are often restricted to simplistic backgrounds, which are insufficient for large-scale data generation and evaluation in embodied intelligence tasks. To address these challenges, we propose EmbodiedGen, data-centric foundation for embodied AI, enables the generation of diverse object and background assets from either images or text prompts, and supports texture editing for enhanced visual richness. This framework effectively supports real-to-sim transfer, data augmentation, and physics-based simulation in different simulators[4, 26, 42, 49], accelerating the development of embodied intelligence systems. 3. Generative 3D World Engine We present EmbodiedGen, novel framework for generating interactive 3D worlds to the needs of embodied intelligence related research. Leveraging generative AI, our approach enables the creation of diverse, customizable environments that support the development and evaluation of embodied agents (see Figure 2). In this section, we first present the 3D object generation modules, including Image-to-3D, which generates physically realistic 3D object assets from single image to facilitate digital twin creation (Sec.3.1), and Text-to-3D, which 3 Figure 3. Overview of EmbodiedGen Image-to-3D Pipeline. From single image, the system generates mesh and 3DGS assets, conducts automatic quality inspectioin (aesthetics, segmentation, geometry), and re-generate failed outputs by auto-adjusted settings. physics expert module restores real-world scale and physical semantics, and the assets are saved in URDF format. generates 3D objects from text descriptions for low cost, high-quality data augmentation (Sec.3.2). We then introduce Articulated Object Generation (Sec.3.3), which produces manipulable articulated 3D assets from either dualstate image pairs or text descriptions. Texture Generation is described in Sec.3.4, enabling highly controllable and multi-style texture editing for 3D assets. Finally, we present Scene Generation (Sec. 3.5), which generates diverse background environments and supports the composition of interactive 3D scenes. 3.1. Image-to-3D Object Generation Method Overview The capabilities of community-driven 3D object asset generation are rapidly advancing and are expected to continue improving. To fully leverage this progress, we focus on building an image-to-3D system to the needs of embodied intelligence related research. For the model component, we leverage open-source models. This approach ensures that our image-to-3D capabilities can be easily extended as community models improve. Specifically, we use Trellis[50] due to its superior geometric generation quality and its ability to produce consistent 3D representations in both mesh and 3DGS[19] formats. However, Trellis has several limitations that hinder its direct use in embodied AI tasks: the generated textures exhibit poor visual quality, particularly due to excessive highlights that result in noticeable whitening when baked onto the mesh. Additionally, the resulting files are purely graphical assets without real-world scale, physical properties, or physically plausible geometry, making them unsuitable for direct use in physics simulators[26, 42, 49]. We focus on three key (1) developing complete data twinning improvements: pipeline for embodied intelligence asset generation, capable of producing data assets with realistic properties, accurate scale, and physically consistent watertight geometry that can be directly imported into simulation engines; (2) enhancing texture quality by applying highlight removal and super-resolution, resulting in high-quality, high-resolution textures; (3) developing diffusion-based model for articulated 3D object generation to meet the growing demand for complex data assets in diverse simulation tasks. Physically Realistic 3D Asset Generation As illustrated in Figure 3, we leverage Trellis[50] to generate 3D representations of input images. We further employ GPT-4o[1] and Qwen[10] to build physics expert agent. Specifically, the agent estimates the real-world height of the asset by rendering frontal view of the generated object and applying text prompt constraints. Given that width, length, and height are interdependent, scaling the height enables accurate recovery of the mesh and 3DGSs true dimensions. For assets with inherent ambiguity in size, text-guided physical property restoration interface is available, allowing users to specify context (e.g., tiger plush toy or tiger animal) for more accurate size prediction. Given four orthographic views of rendered 3D asset as input, the physics expert agent can further estimate physical properties such as friction coefficient and mass, associate them with semantic descriptions, and categorize the object 3D assets accordingly. Automated Quality Inspection We develop an automated quality inspection module, utilizing the AestheticChecker [38] as measure of visual quality, as it has positive correlation with texture richness, see Figure 4. We found that the quality of foreground segmentation has significant impact on the quality of 3D asset generation, so we further build ImageSegChecker using GPT-4o for foreground extraction quality assessment, see Figure 5. To ensure robust segmentation quality across different domains, we provide three different foreground segmentation models, SAM[20], REMBG[15], RMBG14[2]. If ImageSegChecker detects segmentation failure, the system switches to an alternative model for retry. MeshGeoChecker inspects the asset by rendering four orthogonal views and assessing geometric completeness and rationality, see Figure 6. Assets 4 that pass the quality inspection are converted into URDF format and stored. Those that fail any stage of the pipeline are sent back to the corresponding generation step using adjusted settings and seeds. present in Figure 7 the improvement in mesh texture quality achieved by the optimized texture back-projection module. Figure 4. AestheticChecker is used to evaluate the texture quality of generated assets. Assets displaying richer texture details receiving higher scores. Algorithm 1: Compute Texture by Multi-View Color Images Back-Projection Input: RN H0W03: multi-view color images; = (V, ): input mesh, where is mesh vertices, is mesh faces; RN : view confidence weights; θ: angle threshold (default: 70). Output: RHtexWtex3: texture uv map. Figure 5. Examples of segmentation failure cases automatically filtered by ImageSegChecker. 7 3 1 1. Delighting Color Image 2 Igrid R(N H0)W03 RN H0W03 grid = DELIGHT(Igrid) Id RN H0W03 4 5 2. Super Resolution Color Image 6 grid R(N H0)W03 Isr = {SR(Id,i) = 1, . . . , } Isr RN HW 3 where = H0 upscale h, = W0 upscale 10 11 8 3. Mesh Geometry Buffer Rendering 9 {0, 1}N HW : per-pixel visibility mask; [0, 1]N HW : normalized depth map; RN HW 3: view-space normals; [0, 1]N HW 2: per-pixel UV; 12 13 4. Back-Projection per View then Fusion 14 Camera view direction Texture and = [0, 0, 1]T Initialize = 0 and = 0 confidence map for each view = 1 to do Ci max(0, Ni v) Ci[Ci < cos(θ)] 0 Exclude large angles Ei Canny(Di) Ci[M Ci Ci Wi for each valid pixel where Mi (Ei < 0.5) Exclude edge = 0] 0 [p] = 1 do Map (u, v) Ui[p] [Wtex, Htex] UV to texture space Scatter Ii[p] Ci[p] to (u, v) Scatter color with confidence weight Scatter Ci[p] to C(u, v) confidence map Scatter 15 16 17 19 20 21 22 23 25 26 27 28 Texture fusion by confidence end end = C+ϵ 29 30 return Figure 6. Examples of geometric rationality inspection by MeshGeoChecker. Texture Back Projection Optimization Unlike methods like Trellis that rely on optimization-based baking of multi-view RGB images rendered from 3DGS[19] back into 3D space, our optimized approach employs geometrydetermined projection scheme fused with view normals. Before re-projecting textures back into 3D space, we apply global highlight removal and super-resolution to the RGB images, resulting in high-quality, 2K resolution texture UV maps. Specifically, we use delighting model[41] to remove lighting effects from the multi-view textures while maintaining consistent style and brightness across views. We further apply Real-ESRGAN [44] to independently perform 4x super-resolution on each view, enhancing the resolution to 2048x2048. Our experiments show that independent super-resolution for each view does not compromise the consistency or quality of the final 3D asset texture. The algorithmic workflow is illustrated in Algorithm 1. We Figure 7. From left to right: the original image, the result of our optimized texture back-projection, and the result using Trelliss original texture back-projection. Our method effectively mitigates the influence of highlights and shadows on the mesh texture while producing significantly sharper texture details. 3.2. Text-to-3D Object Generation Method Overview The Text-to-3D module is designed for highly controllable generation of 3D object assets with diverse geometry and textures. To achieve this, the textto-3D task is decomposed into two stages: text-to-image and image-to-3D. This decoupling brings several advantages. In large-scale asset production, it enables early-stage automatic quality inspection, allowing the system to filter out samples that fail foreground segmentation check or contain semantics inconsistent with the text description before committing computational resources to 3D generation. More importantly, this modular design improves iteration flexibility and reduces maintenance costs. It also allows the pipeline to fully benefit from ongoing advancements in the text-to-image and image-to-3D communities, supporting continuous improvement in controllability, scalability, and asset generation quality. Specifically, Kolors[40] is used as our text-to-image generation model, as it supports high-quality image generation from both Chinese and English prompts. For the image-to-3D stage, we maintain single unified service, EmbodiedGen Image-to-3D, to streamline system complexity. As shown in Figure 8, compared to Trellis-text-xlarge[50], our two-stage design offers improved controllability and generation quality, while significantly reducing the maintenance cost associated with end-to-end text-to-3D models. The large-scale asset generation workflow for text-to-3D is illustrated in Figure 9. Evaluation of Automated Quality Inspection We evaluate the efficiency of the automated quality inspection module in large-scale 3d asset generation. We construct the automated quality inspection pipeline based on AestheticChecker, ImageSegChecker, and MeshGeoChecker, as introduced in Section 3.1. During evaluation, generated asset is considered usable if it satisfies the following criteria: geometric and textural consistency with the input text description, geometric completeness, texture richness, and compatibility with simulation engines. Otherwise, it is classified as unusable. We define precision as the proportion of assets identified as unusable by the automated checkers that are indeed unusable, and recall as the proportion of all truly Figure 8. Qualitative comparison of Text-to-3D result. The left column shows results generated by our method, while the right column shows results from TRELLIS-text-xlarge. Our method produces significantly higher-quality outputs that better align with the input text descriptions. Figure 9. EmbodiedGen Text-to-3D module for large-scale 3D asset generation. prompt generator decomposes user requirements into prompts targeting different asset styles. The pipeline proceeds through text-to-image and image-to-3D stages, each equipped with automatic quality inspection and retry mechanisms. The final URDF asset with complete geometry, realistic scale, and physical properties, is persistently stored. unusable assets that are correctly flagged by the checkers. We generated 150 cup 3d assets and manually annotated them. Among these, 107 were labeled as usable and 43 as unusable. The automated quality inspection achieved precision of 68.7% and recall of 76.7%. While these metrics are not yet above 90%, the current system substantially reduces the manual effort required for asset screening. Moreover, we expect that this pipeline will continue to improve as multi-modal large models advance, further enhancing automated quality assessment in the future. 6 3.3. Articulated Object Generation Articulated objects such as cabinets, drawers and appliances are common in real world environments. Modeling these objects accurately requires not only capturing their geometric structure but also understanding their motion behavior and part connectivity. This capability is fundamental for tasks in virtual simulation, robotics, and interactive environments [14, 27, 33]. Method Overview We use DIPO [48], controllable generation framework that constructs articulated 3D objects from dual-state image pair. One image shows the object in resting state, and the other shows it in an articulated state. This dual-state input format encodes both structural and kinematic information, enabling the model to better resolve motion ambiguity and predict joint behavior. The generation process is based on diffusion transformer that integrates these two images using specialized dual-state injection module at each layer. DIPO also includes chainof-thought based Graph Reasoner that infers connectivity relationship between each part. The resulting articulation graph is used as an attention prior to enhance generation consistency and plausibility. Automatic Articulated Object Data Augmentation Beyond model design, to improve generalization on complex articulated object generation, we use an automatic data augmentation pipeline to synthesize articulated object layouts from natural language prompts using grid-based spatial reasoning and part retrieval from existing 3D datasets. The resulting PM-X dataset comprises 600 structurally diverse articulated objects, each annotated with rendered images and physical properties. Figure 10 shows representative examples from the PM-X dataset. Qualitative comparison Figures 1 illustrates representative results of generated objects with real-world image inputs. Explore more demos of dynamic articulated object on our project page. 3.4. Texture Generation Method Overview The texture generation module is designed to perform multi-style texture generation and editing for 3D object assets. Given 3D mesh as input, it outputs textured 3D mesh with generated visual appearance. Instead of training multi-view diffusion model from scratch, we design plug-gable and extensible module that leverages existing 2D text-to-image foundation models and extends their capabilities into the 3D domain. Our approach enables the generation of diverse and high-quality textures that are geometrically consistent across views. This design Figure 10. Visual examples of articulated objects constructed by the LLM agent based workflow. paradigm allows us to capitalize on continuous improvements in community foundation models, enabling costeffective and scalable generation of view-consistent textures with minimal retraining effort. Model Design We develop model named GeoLifter, module that extends the capabilities of foundation text-toimage diffusion models to multi-view generation with geometric consistency. GeoLifter injects geometric control into the base diffusion model through cross-attention mechanisms, enabling view-consistent texture generation conditioned on 3D geometry. We adopt Kolors text-to-image[40] as the base diffusion model. In contrast to approaches such as ControlNet[55], which duplicate and train separate encoder branch of the base models U-Net. GeoLifter remains lightweight and highly extensible. Its parameter size does not grow with the depth of the base model, making it more efficient and easier to integrate with evolving diffusion architectures. Given an input mesh, we render normal maps, pofrom six predefined sition maps and binary masks camera views (elevations {20, 10}, azimuths {0, 60, 120, 180, 240, 300}). For each view, the normal and position maps are rendered in image space from camera view, and concatenated along the spatial (height and width) dimensions within each attribute. The different attribute types (normal, position, mask) are then concatenated along the channel dimension to form the geometric condition input RHW 7. The normal map encodes surface normals interpolated per vertex and projected to the image plane. The position map stores the XYZ coordinates (in object space) of visible vertices. The mask is binary segmentation map. The geometric condition is then implicitly encoded into feature embedding, which is progressively injected into the denoising process of the diffusion model via cross-attention, leveraging zero convolution to ensure 7 Figure 11. Overview of EmbodiedGen Texture Generation Module. Given mesh and text prompt, the module generates six-view consistent textures with controllable styles via text, reference image, or both. Geometry-aware conditions (normals, positions, masks) are extracted and injected into diffusion model via the GeoLifter module. The outputs are refined with illumination removal and superresolution, then back-projected to the mesh as described in Algorithm 1. minimal interference with the base model decoder at the start of training. The text prompt supports both positive and negative prompts, and accepts multi-lingual input, including both Chinese and English descriptions, to specify the desired texture style and appearance. In addition to textual prompts, users may optionally provide an RGB image as reference style, which serves as complementary control signal to the language input. Users can provide text prompt only, reference image only, or both simultaneously. This design enables highly controllable and expressive texture generation by jointly leveraging semantic guidance and visual style cues. We observe that GeoLifter, with its lightweight geometric conditioning design, effectively preserves the texture generation capability of the underlying foundation model, while significantly improving spatial and geometric consistency across views. Following the multi-view texture generation, we apply illumination removal and super-resolution techniques and project the refined textures back into 3D space to obtain the final textured mesh, equipped with high-resolution 2K UV map as described in Algorithm 1. Loss Design On top of the original loss 1 used in latent diffusion models[37], we introduce spatial loss 2 as geometric consistency constraint in the latent space, where is the batch size, rb and sb are the reference and search point sets for the b-th sample(visualized in Figure 12). fb() RCNb denotes the extracted feature vectors at the corresponding coordinates. Standard element-wise smooth L1 loss is applied to encourages the latent features of pixels corresponding to the same 3D point (projected across multiple views) to remain close in feature space, thereby enFigure 12. Blue dots: reference points; red dots: projected correspondences in other views; green lines: matches. The spatial loss is applied to let the latent features of matched points closer, enhancing cross-view alignment. hancing cross-view coherence. The final loss 3 is obtained by adding LLDM and Lspatial, λldm and λspatial is set to 1 and 0.02 respectively. LLDM = Ex,ϵ,t ϵ ϵθ (zt, t, c)2(cid:105) (cid:104) (1) Lspatial ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) b=1 1{rb>0sb>0}SmoothL1 (fb(rb), fb(sb)) = λLDMLLDM + λspatialLspatial (2) (3) Qualitative Comparison We conduct qualitative comparison between our method and several state-of-theincluding TEXTure[36], art texture generation methods, SyncMVD[24], Paint3D[54], Meshy[25], and Hunyuan3d2[12]. As shown in Figure 13, our method consistently produces higher-quality 3D textures, with superior geometric consistency across views. Furthermore, our method uniquely supports specified text to be directly generated on 3D surfaces. 8 Figure 13. EmbodiedGen texture generation module effectively adheres to text descriptions, generating high-quality textures with strong spatial and geometric consistency. It also demonstrates robust control over text generation on textures, accurately rendering common Chinese and English text. 3.5. 3D Scene Generation Method Overview Beyond 3D object asset generation, scene diversity as background context plays an equally critical role. We develop scalable and efficient framework for 3D scene generation. The system follows modular pipeline that transforms multi-modal inputs into panoramic images, which are then used to generate 3D scenes with 9 Figure 14. Overview of EmbodiedGen 3D Scene Generation. panorama is generated from text prompt or input image, guided by style prompt. After quality assessment via vlm-based selector, refined mesh and 3DGS[19] are generated by panorama projection, inpainting, and repair. Super-resolution is used to enhance 3DGS appearance details, followed by real scale and alignment adjustments. consistent real-world scale. The framework consists of three main stages: (1) panoramic image generation, (2) 3D scene generation in 3DGS[19] and mesh representation from panorama, and (3)scale alignment and standardized output, as illustrated in Figure 14. Panoramic Image Generation Our method supports both textand image-based input modalities, or combination of both, enabling flexible and efficient generation of high-quality panoramic images. For text-driven generation, user-provided scene descriptions are translated into panoramic views using Diffusion360 model[13], which have demonstrated strong performance in this task. For image-driven generation, we employ Qwen [10] to extract semantic descriptions from the input image. The image and its corresponding textual description are then jointly processed by the panorama generation model[13] to generate semantically aligned panoramas(see Figure 15). To ensure quality and reliability, we introduce the PanoSelector module, which is built upon Qwen [10], automatically evaluates and filters the generated panoramas based on structural quality metrics, such as floor and wall consistency. This guarantees that only high-quality outputs are forwarded to the geometry generation stage. Scene 3D Representation Generation After obtaining high-quality panoramas, the system proceeds to generate the corresponding 3D representation in 3DGS and mesh based on Pano2Room[32]. An initial mesh is generated from the panoramic input and further refined through mesh optimization to improve both geometric accuracy and reconstruct-ability. The optimized mesh is then converted into 3DGS representation. To enhance visual fidelity, views rendered from the optimized mesh are converted into cubemaps and passed through the super-resolution model[44]. The super-resolved images are then used to furFigure 15. Qualitative Comparison With and Without Style Prompts. w/o style prompt lacks explicit style guidance, while Ours uses style-aware prompting, yielding more coherent textures and better stylistic alignment across scenes. ther refine the initial 3DGS, effectively enhancing the detail quality of the final 3DGS output as in Figure 16. We show generated 3D scene quality comparison with worldgen[51] in Figure 17. Physics Restoration To produce realistic and metrically consistent 3D scenes, the system performs absolute scale estimation by predicting real-world dimensions such as building height from the input panoramas and their semantic descriptions. dedicated scale estimation module, built upon the Qwen model [10], infers these scale factors to enable lossless rescaling of both the mesh and 3DGS[19] 10 trated in Figure 21, with the generated 3D assets evaluated via closed loop simulation in the Isaac Lab[26] environment. Figure 23 shows how assets generated by EmbodiedGen Text-to-3D can be used in navigation and obstacle avoidance tasks within the OpenAI Gym[4] simulation framework. RoboSplatter: Integration of 3DGS Rendering into Physical Simulation Existing simulators are typically built upon traditional OpenGL-based rendering techniques, which involve complex environment modeling, lighting setup, and ray-based rendering calculations. These approaches often suffer from high computational cost and limited photorealism. With the rapid advancement of 3DGS[19], more realistic and efficient rendering solutions have emerged. We integrate 3DGS rendering with established physical simulators such as MuJoCo [42] and Isaac Lab [26], enabling visually rich and physically accurate simulations. To this end, we develop RoboSplatter, 3DGS-based simulation rendering framework tailored for robotics simulation. As shown in Figure 20, RoboSplatter works seamlessly with MuJoCo to simulate robotic manipulation tasks, such as robotic arm grasping, while delivering high visual fidelity powered by 3DGS technology. 5. Conclusion In this work, we present EmbodiedGen, the first comprehensive platform for interactive 3D world generation to the needs of embodied intelligence related research. Our system enables controllable and diverse creation of real-tosim digital twins, alongside large-scale generation of 3D rigid and articulated object and 3D scene assets. These assets can be seamlessly integrated into various simulators such as OpenAI Gym[4], Isaac Lab[26], MuJoCo[42] and SAPIEN[49] for tasks such as ground-truth generation, evaluation, and reinforcement learning. The generated assets achieve state-of-the-art quality in both visual fidelity and physical realism, and are enriched with detailed annotations, including quality inspection labels, watertight geometry, and dual 3D representations in both 3DGS[19] and mesh formats. To promote research and practical adoption, we release the pipeline as an open-source, user-friendly toolkit and service. Figure 16. Qualitative comparison with and without superresolution. The generated 3D scene show sharper and highfrequency detailed with super-resolution. Zoom in for details. Figure 17. Qualitative Comparison with WorldGen[51] Our method produces more detailed textures and more complete geometry than WorldGen, under both text and image input settings. representations. Additionally, the coordinate system is recentered to the floor plane of the scene, with the axes aligned according to either the camera direction from the input image or the orientation implied by the textual description. The resulting output is scale-aligned, high-fidelity 3D scene asset, ready for downstream use in virtual and augmented reality and robotics. 4. Application Large-scale 3D Asset Generation Figure 18 showcases the capability of the EmbodiedGen Text-to-3D module to generate large-scale 3D assets for embodied intelligence tasks, producing watertight and stylistically diverse meshes aligned with textual descriptions. This capability enables low-cost augmentation of interactive 3D assets for simulation and downstream training and evaluation. Visual Appearance Editing of 3D Mesh Figure 19 demonstrates the capability of the EmbodiedGen texture generation module to generate and edit photorealistic textures with rich visual details. The edited 3D assets can be used for training data augmentation, enhancing model generalization in visual appearance understanding. Real-to-Sim: Digital Twins creation The real-to-sim capabilities of EmbodiedGen Image-to-3D module is illus11 Figure 18. EmbodiedGen Image-to-3D: large-scale and diverse 3D object asset generation. Figure 19. EmbodiedGen texture generation module enables rich and flexible visual texture editing. Figure 20. EmbodiedGen Image-to-3D: Digital twin creation and simulation in RoboSplatter and MuJoCo[42]. 13 Figure 21. EmbodiedGen Image-to-3D: Real-to-sim closed-loop simulation evaluation of grasping model in Isaac Lab environment [26]. Figure 22. Interaction 3D World Generation with EmbodiedGen. EmbodiedGen enables easy construction of diverse interactive 3D worlds for simulating and evaluating dual-arm shoe-grasping tasks in RoboTwin[28]. Figure 23. EmbodiedGen Text-to-3D: Real-to-sim object transfer and quadruped navigation with obstacle avoidance in OpenAI Gym[4]."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 4 [2] BRIA AI. Rmbg-1.4: Background removal model. https: //huggingface.co/briaai/RMBG-1.4, 2023. Accessed: 2025-05-19. 4 [3] Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, Emilien Garreau, Animesh Karnewar, Ang Cao, Idan Azuri, Iurii Makarov, Eric-Tuan Le, Antoine Toisoul, David Novotny, Oran Gafni, Natalia Neverova, and Andrea Vedaldi. Meta 3d gen, 2024. 3 [4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. 1, 3, 11, 14 [5] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction, 2024. 2 [6] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-view Images, page 370386. Springer Nature Switzerland, 2024. [7] Zoey Chen, Aaron Walsman, Marius Memmel, Kaichun Mo, Alex Fang, Karthikeya Vemuri, Alan Wu, Dieter Fox, and Abhishek Gupta. Urdformer: pipeline for constructing articulated simulation environments from real-world images, 2024. 3 [8] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes, 2023. 3 [9] Tianyuan Dai, Josiah Wong, Yunfan Jiang, Chen Wang, Cem Gokmen, Ruohan Zhang, Jiajun Wu, and Li Fei-Fei. Automated creation of digital cousins for robust policy learning, 2024. 3 [10] Jinze Bai et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 4, [11] Yicong Hong et al. Lrm: Large reconstruction model for single image to 3d, 2024. 2 [12] Zibo Zhao et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025. 2, 8 [13] Mengyang Feng, Jinlin Liu, Miaomiao Cui, and Xuansong Xie. Diffusion360: Seamless 360 degree panoramic image generation based on diffusion models, 2023. 10 [14] Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction strategies for articulated object In Proceedings of the IEEE/CVF Internapart discovery. tional Conference on Computer Vision, pages 1575215761, 2021. 7 [15] Daniel Gatis. rembg, 2025. tool to remove images background. [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [17] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy, 2024. 2 [18] Pushkal Katara, Zhou Xian, and Katerina Fragkiadaki. Gen2sim: Scaling up robot learning in simulation with generative models, 2023. 3 [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering, 2023. 3, 4, 5, 10, 11 [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 4 [21] Jiayi Liu, Denys Iliash, Angel X. Chang, Manolis Savva, and Ali Mahdavi-Amiri. Singapo: Single image controlled generation of articulated parts in objects, 2025. 3 [22] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. [23] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. 2 [24] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. arXiv preprint arXiv:2311.12891, 2023. 8 [25] Meshy.ai. Meshy.ai: Ai-powered 3d mesh generation, 2025. Accessed: 2025-05-09. 8 [26] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: unified simulation framework for interactive robot learning environments. IEEE Robotics and Automation Letters, 8(6): 37403747, 2023. 1, 3, 4, 11, 14 [27] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: From pixels to actions for articulated 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 68136823, 2021. [28] Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, and Ping Luo. Robotwin: Dual-arm robot benchmark with generative digital twins (early version), 2025. 3, 14 [29] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 2 [30] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 2 [31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. [32] Guo Pu, Yiming Zhao, and Zhouhui Lian. Pano2room: Novel view synthesis from single indoor panorama. In SIG15 GRAPH Asia 2024 Conference Papers, page 111. ACM, 2024. 10 [33] Shengyi Qian and David Fouhey. Understanding 3d object interaction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2175321763, 2023. 7 [34] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 2 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [36] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes, 2023. 8 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 8 [38] Christoph Schuhmann. Aesthetic subsets in laion 2170337258 samples, 2025. Retrieved May 16, 2025. 4 [39] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation, 2024. [40] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 6, 7 [41] Tencent Hunyuan3D Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets https : / / huggingface . co / generation, 2025. tencent/Hunyuan3D-2/tree/main/hunyuan3ddelight-v2-0. 3, 5 [42] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 50265033. IEEE, 2012. 1, 3, 4, 11, 13 [43] Guangcong Wang, Peng Wang, Zhaoxi Chen, Wenping Wang, Chen Change Loy, and Ziwei Liu. Perf: Panoramic neural radiance field from single panorama, 2023. 3 [44] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with In International Conference on Compure synthetic data. puter Vision Workshops (ICCVW). 5, 10 [45] Yian Wang, Bingjie Tang, Chuang Gan, Dieter Fox, Kaichun Mo, Yashraj Narang, and Iretiayo Akinola. Matchmaker: Automated asset generation for robotic assembly, 2025. [46] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image, 2024. 2 [47] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn motion pattern for few-shot video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70897098, 2024. 2 16 [48] Ruiqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, and Ming-Ming Cheng. Dipo: Dual-state images controlled articulated object generation powered by diverse data, 2025. 7 [49] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, and Hao Su. SAPIEN: simulated part-based interactive environment. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, 3, 4, 11 [50] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 3, 4, 6 [51] Ziyang Xie. Worldgen: Generate any 3d scene in seconds. https://github.com/ZiYangxie/WorldGen, 2025. 3, 10, [52] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation, 2024. 2 [53] Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Yixuan Li, Gordon Wetzstein, Ziwei Liu, and Dahua Lin. Layerpano3d: Layered 3d panorama for hyper-immersive scene generation, 2025. 3 [54] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42524262, 2024. 8 [55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 7 [56] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets, 2024. 3 [57] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets, 2024. [58] Yuqing Zhang, Yuan Liu, Zhiyu Xie, Lei Yang, Zhongyuan Liu, Mengzhou Yang, Runze Zhang, Qilong Kou, Cheng Lin, Wenping Wang, and Xiaogang Jin. Dreammat: High-quality pbr material generation with geometryand light-aware diffusion models, 2024. 2 [59] Haiyang Zhou, Xinhua Cheng, Wangbo Yu, Yonghong Tian, and Li Yuan. Holodreamer: Holistic 3d panoramic world generation from text descriptions, 2024. 3 [60] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting, 2024."
        }
    ],
    "affiliations": [
        "D-Robotics",
        "GigaAI",
        "Horizon Robotics",
        "Shanghai Jiao Tong University",
        "VCIP, CS, Nankai University"
    ]
}