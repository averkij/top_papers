{
    "paper_title": "On-Policy RL with Optimal Reward Baseline",
    "authors": [
        "Yaru Hao",
        "Li Dong",
        "Xun Wu",
        "Shaohan Huang",
        "Zewen Chi",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 8 5 3 2 . 5 0 5 2 : r On-Policy RL with Optimal Reward Baseline Yaru Hao Li Dong Xun Wu Shaohan Huang Zewen Chi Furu Wei Microsoft Research https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning from human feedback (RLHF) is foundational approach for aligning large language models (LLMs) with human preferences [SOW+20, OWJ+22, BJN+22]. The standard RLHF pipeline typically involves supervised fine-tuning followed by reinforcement learning, commonly employing proximal policy optimization (PPO) algorithm [SWD+17], guided by learned reward model. Beyond general alignment, reinforcement learning has proven effective in enhancing the reasoning abilities of LLMs through test-time scaling, as demonstrated by the OpenAI-o1 model [Ope24]. Most recent work such as DeepSeek-R1 [GYZ+25] further shows that reinforcement learning, even with simple rule-based rewards, can elicit emergent reasoning behaviors and significantly boost performance on complex tasks like mathematics and code generation. Despite its success, current RLHF algorithms face some challenges regarding stability and efficiency. For instance, PPO [SWD+17] requires training an extra value model to estimate advantages, which introduces additional computational overhead. While methods like Group Relative Policy Optimization (GRPO) address this by using response groups to compute relative reward baseline [SWZ+24], these methods are often prone to instability due to loose on-policy constraints. This often results in large policy shifts and reduced sample diversity, phenomenon known as alignment tax [ABC+21, KMN+24]. In this work, we introduce On-Policy RL with Optimal reward baseline (OPO), simple yet effective algorithm with two key improvements. First, OPO employs exact on-policy training, which empirically stabilizes the training process and significantly enhances exploration capabilities. Second, we derive the optimal reward baseline that theoretically minimizes gradient variance. By integrating these improvements, OPO eliminates the need for auxiliary components such as value and reference models, as well as regularization terms. Instead, it relies solely on single policy model optimized directly to maximize the reward expectation. We validate the effectiveness of OPO on Deepseek-R1-Distill-Qwen-7B model across various mathematical reasoning benchmarks. Our experimental results demonstrate that OPO outperforms existing baselines in both performance and training stability. In particular, OPO consistently maintains lower policy shifts and higher output entropy, leading to more diverse and less repetitive responses. In summary, the key advantages of OPO are: Theoretical Soundness: We derive the optimal reward baseline that theoretically minimizes the gradient variance, ensuring more robust learning process. Enhanced Stability: OPO exhibits stable training dynamics, even without explicit KL or entropy regularization, which is crucial for reliable performance. Empirical Effectiveness: OPO achieves better performance on math reasoning benchmarks and yields more diverse and less repetitive responses."
        },
        {
            "title": "2 Background",
            "content": "Proximal Policy Optimization (PPO) PPO [SWD+17] is widely adopted policy gradient algorithm. As an actor-critic method, PPO leverages policy model (actor) to optimize the reward and value model (critic) to estimate the value of each state. central feature of PPO is its clipped surrogate objective function, designed to enhance training stability and sample efficiency by limiting the magnitude of policy updates at each iteration. The objective is formally defined as: JPPO(θ) = ExD,yπθ(x) (cid:20) (cid:88) (cid:26) t=1 min( πθ(ytx, y<t) πθold (ytx, y<t) At, clip( πθ(ytx, y<t) πθold(ytx, y<t) (cid:27)(cid:21) , 1ϵ, 1+ϵ)At) (1) where At denotes the advantage estimate at time step t, computed using Generalized Advantage Estimation (GAE) [SML+18], which combines information from the reward function and the value function. The hyperparameter ϵ controls the clipping range, effectively constraining the policy update to prevent drastic changes that can make training unstable. Group Relative Policy Optimization (GRPO) To eliminate the computational cost of separate value model, GRPO [SWZ+24] computes relative advantages within group of sampled responses by normalizing rewards. For each input x, GRPO samples group of trajectories {yi}K i=1 from the policy and defines the advantage of each trajectory based on its reward relative to others in the group: ˆAi,t = r(x, yi) mean({r(x, yi)}K std({r(x, yi)}K i=1) i=1) (2) This group-wise normalization ensures zero mean and unit variance of advantages within each group, which achieves efficient training without requiring the additional value model. It extends the PPO objective with the relative advantage: JGRPO(θ) = xD,{yi}K i=1πθold (x) 1 (cid:88) i=1 1 yi yi (cid:88) (cid:26) t=1 min (cid:20) πθ(yi,tx, yi,<t) πθold (yi,tx, yi,<t) ˆAi,t, clip (cid:18) πθ(yi,tx, yi,<t) πθold (yi,tx, yi,<t) (cid:19) , 1ϵ, 1+ϵ (cid:21) ˆAi,t βDKL[πθπref] (cid:27) (3) KL and Entropy Regularization In the reinforcement learning stage of RLHF, two regularization terms are commonly incorporated into the objective function to stabilize policy optimization: the Kullback-Leibler (KL) divergence loss and the entropy bonus. The KL divergence loss constrains the updated policy from drifting too far from reference policy (typically the original supervised fine-tuned model) [Sch20]. This constraint helps mitigate the alignment tax, which refers to the degradation of helpfulness, safety, or factuality when the model over-optimizes for reward at the cost of its original capabilities. In addition to the KL divergence loss, an entropy bonus is introduced to encourage exploration and prevent the policy to collapsing into suboptimal solution [ARNS19]. By maximizing the entropy 2 of the policy distribution, we encourage the model to explore broader set of potential high-reward responses, thus enhancing the diversity and robustness of the generated outputs. Balancing these components (the primary reward objective, KL loss, and entropy bonus) is essential for achieving stable learning and maintaining both original capabilities and alignment performance. Over-penalizing with KL and entropy can limit learning progress, whereas under-penalizing can lead to undesirable policy drift. Similarly, entropy must be tuned to avoid both under-exploration and excessive randomness."
        },
        {
            "title": "3 Method: On-Policy RL with Optimal Reward Baseline (OPO)",
            "content": "We propose On-Policy RL with Optimal reward baseline (OPO), which employs two key strategies: (1) exact on-policy training, which we argue is crucial for mitigating issues like entropy collapse and large policy shifts, and (2) the optimal reward baseline that theoretically minimizes gradient variance. OPO solely involves policy model with the objective of maximizing reward, which not only simplifies the training process but also leads to more stable and effective training compared to methods with loose on-policy settings and suboptimal baselines."
        },
        {
            "title": "3.1 Exact On-Policy Training",
            "content": "The objective of policy-based reinforcement learning is to optimize parameterized policy πθ to maximize the expected reward. This objective is inherently on-policy, meaning that the reward expectation is taken with respect to trajectories generated directly by the current policy. Specifically, we aim to: max θ ExD,yπθ(x)[r(x, y)] (4) where is the input sampled from the dataset D, represents trajectory sampled from the current policy πθ(x), and r(x, y) is the reward function for trajectory given input x. For simplicity, we mainly consider settings where the reward is trajectory-level. foundational characteristic of OPO is its strict adherence to exact on-policy training. This contrasts with common policy gradient methods, such as PPO, which typically collect batch of data using the current policy and then perform multiple gradient updates on this fixed batch. While reusing rollouts can improve sample efficiency, subsequent updates introduce an off-policy divergence. In practice, it may contribute to sample entropy collapse and large policy shifts, thereby necessitating explicit entropy regularization. In contrast, exact on-policy training ensures that each gradient step is computed using fresh data sampled from the current policy. This preserves the theoretical properties of the policy objective and empirically leads to more stable entropy throughout training. Furthermore, exact on-policy training maintains lower KL divergence between the current policy and the initial policy, reducing the alignment tax and improving the overall performance of the model. 3.2 Optimal Reward Baseline for Variance Reduction Reducing the variance of policy gradient estimates is crucial for stable and efficient reinforcement learning. common technique to reduce the variance is to subtract baseline from the reward. Our goal is to derive the optimal reward baseline that minimizes this gradient variance. Recall the policy gradient derived from the policy gradient theorem: = ExD,yπθ(x)[θ log πθ(yx) r(x, y)] (5) where θ log πθ(yx) is the score function gradient. We can modify this gradient estimator to include baseline b, which does not change the expected value of the gradient but can significantly reduce its variance. The modified gradient estimator becomes: = ExD,yπθ(x)[θ log πθ(yx) (r(x, y) b)] (6) To find the optimal baseline b, we aim to minimize the variance of this modified gradient estimate. The variance is defined as: Var[g] = E[(θ log πθ(yx) (r(x, y) b))2] (E[θ log πθ(yx) (r(x, y) b)])2 (7) Since the second term (the square of the expected gradient) is independent of b, minimizing Var[g] is equivalent to minimizing the first term. We can derive the optimal baseline by taking the derivative with respect to and setting it to zero: db E[(θ log πθ(yx) (r(x, y) b))2] = 0 Solving this equation yields the optimal baseline b: = Eyπθ(x) (cid:104) (cid:105) (θ log πθ(yx))2 r(x, y) (θ log πθ(yx))2(cid:105) (cid:104) Eyπθ(x) (8) (9) This optimal baseline represents weighted average of rewards, where the weights are the squared magnitudes of the score function gradients. This specific weighting minimizes the variance of our policy gradient estimate. The detailed derivation is provided in Appendix A. Simplified Optimal Baseline for Sequence Generation For sequence generation problems, such as language modeling, we can make simple assumption that the gradients of different tokens are approximately orthogonal and the norm of the gradient for each token follows same distribution. Under this condition, the squared magnitude of the policy gradient for trajectory is proportional to its length (θ log πθ(yx)2 ly), where ly is the length of the response y. With this simplification, the optimal reward baseline simplifies to: = Eyπθ(x)[ly r(x, y)] Eyπθ(x)[ly] (10) where longer responses contribute proportionally more to the baseline calculation. 3.3 Overall Algorithm The OPO algorithm integrates the two key techniques discussed: exact on-policy training and the optimal reward baseline. In practice, we follow the GRPO setup: for each prompt, we sample outputs using the current policy, compute an approximation of the optimal baseline using these samples, and then perform policy optimization with exact on-policy training. Specifically, given prompt and sampled responses {yi}K i=1, the objective function of OPO can be expressed as: JOPO(θ) = xD,{yi}K i=1πθ(x) (cid:20) 1 K (cid:88) (cid:21) log πθ(yix) Ai(x, yi) i=1 (11) where the advantage Ai for trajectory yi is calculated using an empirical estimate of the optimal baseline b(x) based on the samples: Ai = r(x, yi) b(x) b(x) = (cid:80)K i=1 lyi r(x, yi) i=1 lyi (cid:80)K (12) By normalizing the reward with this optimal baseline, we can minimize the variance of our policy gradient estimates practically, leading to more stable and effective learning. In particular, our objective function omits commonly used KL and entropy regularization terms. We demonstrate that OPO can achieve strong performance even without relying on these regularizations. detailed summary of the OPO algorithm is provided in Algorithm 1. Algorithm 1 Optimal on-Policy Optimization (OPO) Input: Initial policy model πSFT, reward function r(x, y), prompt dataset 1: policy model πθ πSFT 2: for step = 1, 2, ..., do 3: 4: 5: 6: 7: end for Output: πθ Sample batch of prompts Db For each prompt Db, sample responses {yi}K Compute the advantage Ai for each sampled response yi using Equation (12) Update the policy model πθ by maximizing JOPO(θ) defined in Equation (11) i=1 πθ(x)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Details Training Setup We validate OPO through two sets of comparisons, each designed to isolate the contribution of key component. The implementation is based on verl1 [SZY+24] training library. To evaluate the impact of exact on-policy training, we compare on-policy GRPO and loose on-policy (off-policy) GRPO training from the DeepSeek-R1-Distill-Qwen-7B2 model. Both variants use training length of 8k, learning rate of 1e-6, zero KL penalty, and batch size of 256 questions. For each question, K=16 responses are sampled. For the rollout process, we adopt temperature of 0.6 and top-p sampling threshold of 1.0. The mini-batch sizes are 256 (on-policy) and 128 (off-policy). The total training step is 500. We also follow prior work and apply clip range of 0.2 and small entropy penalty of 0.001 to the off-policy variant to mitigate entropy collapse, while the on-policy version uses no entropy regularization. To evaluate the effect of the optimal reward baseline under exact on-policy training, we adopt more comprehensive and realistic setting. We first perform supervised fine-tuning using long-form chain-of-thought (long-CoT) data, followed by reinforcement learning using both standard on-policy GRPO and our proposed method (OPO), which augments exact on-policy training with the optimal baseline. Both methods share the same hyperparameters: training length of 24k, batch size of 256 questions, K=8 responses sampled per question, and no KL or entropy terms are applied. Training Datasets For training datasets, we utilize the math subset from Skywork-OR1-RL-Data3. This dataset comprises 48k unique math problems, which undergoes an initial offline difficulty estimation for each problem and the problems with all correct or all incorrect responses are excluded. For reinforcement learning, we employ the rule-based reward function [GYZ+25], reward of 1 for correct response, and 0 for an incorrect one. The correctness is given by the Math-Verify evaluator4. For the SFT-then-RL experiments, we exclude duplicates from the OR1 data during the SFT stage, using the remaining 25k samples for RL training. Evaluation Setup We evaluate model performance on three widely used math reasoning benchmarks: MATH-500, AIME 2024 [MAA24], and AIME 2025. For each dataset, we sampled multiple responses from the model with maximum response length of 32768, sampling temperature of 0.6, and top-p sampling threshold of 1.0. We also use the Math-Verify evaluator to assess the correctness. For MATH-500, we sample 8 reasponses for each question, while for AIME 2024 and AIME 2025, we sample 16 responses. The pass@k metric for {1, 2, 4, 8, 16} is calculated following the method in [CTJ+21]. Beyond accuracy, we also analyze the training dynamics of the entropy of the models output distribution and the KL divergence between the updated and original models. Given comparable performance, lower KL divergence and higher entropy are preferable. Lower KL divergence indicates lower alignment tax (undesirable model changes from alignment), while higher entropy indicates greater sampling diversity. 4.2 Results We first investigate the impact of exact on-policy training. Table 1 presents the average performance over the last five checkpoints (steps 420 to 500, in increments of 20). With the same optimization steps, exact on-policy training significantly outperforms off-policy training on the pass@1 metric, demonstrating its effectiveness. For larger values in pass@k, the performance of both on-policy and off-policy methods are comparable. To validate the effectiveness of the optimal reward baseline, we compare the performance of OPO against GRPO and the initial supervised fine-tuned policy. Both OPO and GRPO follow the exact onpolicy training and share all hyperparameters. The only difference lies in their advantage estimation: OPO uses the optimal reward baseline as defined in Equation 12, whereas GRPO uses the standard 1https://github.com/volcengine/verl 2https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B 3https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data 4https://github.com/huggingface/Math-Verify 5 Table 1: The performance comparision between on-policy and off-policy training. Dataset Method pass@1 pass@2 pass@4 pass@ pass@16 MATH-500 Off-Policy On-Policy AIME 2024 Off-Policy On-Policy AIME 2025 Off-Policy On-Policy 92.97 93.90 53.50 55. 36.21 38.37 95.60 96.21 65.62 66.60 44.05 46.58 97.14 97.43 73.92 74. 51.60 53.65 98.20 98.16 78.07 78.81 59.02 58.54 - - 80.00 81. 66.67 62.66 Table 2: The performance comparision between OPO and GRPO. Both OPO and GRPO follow the exact on-policy training from the SFT policy. Dataset Method pass@1 pass@ pass@4 pass@8 pass@"
        },
        {
            "title": "SFT",
            "content": "MATH-500 GRPO OPO SFT AIME 2024 GRPO OPO SFT AIME 2025 GRPO OPO 94.80 95.10 95.26 66.04 67.96 68.50 46.88 50.21 50.00 96.61 96.64 97. 75.72 75.54 76.10 57.39 61.45 60.88 97.63 97.51 97.91 80.13 79.62 80.06 68.13 70.96 70.37 98.40 98.16 98. 81.65 81.67 82.12 76.89 77.64 78.02 - - - 83.33 83.33 84.00 83.33 81.33 85.33 baseline in Equation 2. Table 2 presents the results averaged over the last five checkpoints. As demonstrated, OPO outperforms GRPO in most cases, with its improvements becoming more pronounced at higher values (e.g., pass@8 and pass@16). Furthermore, while GRPO sometimes yields similar or even reduced pass@16 metrics compared to the initial SFT policy, OPO can improve accuracy beyond the SFT baseline, indicating its effectiveness in scaling performance and generalizing across datasets. We further demonstrate the effectiveness of the optimal reward baseline within the Reinforce++ algorithm in Appendix B. 4.3 Analysis Figure 1: Training dynamics of on-policy and off-policy training. Left: Training rewards; Middle: KL divergence; Right: Entropy. 6 Figure 2: Left: Comparison of KL divergence and math performance between OPO and GRPO. Both OPO and GRPO follow the exact on-policy training from the SFT policy. The x-axis represents KL divergence, and the y-axis denotes math performance. Middle: Training dynamics of KL divergence. Right: Training dynamics of entropy. OPO achieves better performance and more stable training. We compare the training dynamics of on-policy and off-policy methods in Figure 1. While off-policy training achieves similar or even slightly higher training rewards than exact on-policy training in the earlier stage, it yields inferior performance on math reasoning tasks. It suggests potential overfitting issue with off-policy learning. Furthermore, exact on-policy training exhibits significantly lower KL divergence and higher entropy throughout training, even without any explicit KL or entropy regularization, whereas off-policy training includes an additional entropy bonus. Lower KL divergence implies reduced alignment tax and higher entropy suggests stronger exploration capability. Figure 2 presents the comparison between OPO and GRPO with exact on-policy training. OPO maintains similar entropy levels while achieving lower KL divergence. The left subplot visualizes the trade-off between KL divergence and math performance, demonstrating that OPO consistently achieves higher performance with more stable training dynamics. Table 3: Comparison of repetition rate (Rep-5) and sampling diversity (Self-BLEU) between onpolicy and off-policy training. Lower values indicate better performance. Dataset Method MATH-500 AIME 2024 AIME 2025 RepSelf-BLEU Rep-5 Self-BLEU Rep-5 Self-BLEU Off-Policy On-Policy 18.11 15.56 74.45 61. 25.71 19.75 69.60 62.54 27.27 20.74 69.99 63.76 Table 4: Comparison of repetition rate (Rep-5) and sampling diversity (Self-BLEU) between OPO and GRPO. Lower values indicate better performance. Dataset MATH-500 AIME 2024 AIME 2025 Method Rep-5 Self-BLEU Rep-5 Self-BLEU RepSelf-BLEU GRPO OPO 14.82 14.70 66.70 66.76 22.62 22.08 64.53 64. 22.71 21.84 63.89 63.20 OPO generates more diverse and less repetitive outputs. The KL divergence and entropy correlate with important output quality metrics that directly impact user experience, such as sampling diversity and repetition rate. We use the Self-BLEU metric [ZLZ+18] to quantify the sampling diversity. For each query, multiple responses are sampled; each response is treated as hypothesis and compared to others as references. The average BLEU score across all combinations is reported as Self-BLEU. lower Self-BLEU score indicates higher diversity among outputs. For measuring repetition, we employ the Rep-5 metric [WKR+20], which calculates the proportion of duplicate 7 5-grams in each generated sequence. lower Rep-5 score reflects less intra-sequence repetition. Tables 1 and 2 summarize the results. Benefitting from exact on-policy training and the optimal reward baseline, OPO consistently produces outputs that are both more diverse and less repetitive compared to its counterparts."
        },
        {
            "title": "5 Related Work",
            "content": "Large Language Models (LLMs) have demonstrated impressive capabilities across wide range of real-world tasks [BMR+20, Ope23, ABW+23, Ant24, LFX+24, YLY+25]. critical phase in their development is Reinforcement Learning from Human Feedback (RLHF) [SOW+20, OWJ+22, BJN+22], which typically consists of two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). In SFT, models are initially guided toward preferred behaviors using curated datasets. Subsequently, RL optimizes the model outputs by employing policy gradient algorithms to maximize reward signal [GSH22, RSM+23]. It ensures that the model aligns with desired outcomes like helpfulness, truthfulness, and harmlessness. Beyond general alignment, RL has been applied to enhance the reasoning capabilities of LLMs [Ope24, GYZ+25, Qwe24, TDG+25, XAI24, Dee24, SC+25]. These methods often emphasize test-time scaling, where models iteratively refine their thought processes, explore alternative strategies, and self-correct through chain-of-thought reasoning [WWS+22]. Such techniques significantly boost performance on complex tasks in domains including mathematics, science, and programming. Among various policy-based RL algorithms [SB18], Proximal Policy Optimization (PPO) [SWD+17] has been the most common choice since InstructGPT [OWJ+22] due to its balance of stability and sample efficiency. However, PPO needs to train an extra value model to estimate the reward baseline. To address this, Group Relative Policy Optimization (GRPO) [SWZ+24] proposes to generate multiple responses and use their average score as baseline for advantage estimation. It eliminates the need for separate value model, thereby improving memory efficiency. Other works also focus on alternative advantage estimation methods without value model, like RLOO [ACG+24], Reinforce++ [Hu25], and Dr. GRPO [LCL+25]. Furthermore, while some research aims to resolve issues like KL or entropy collapse in loose on-policy settings [HLL+25, YZZ+25], both our method and [CYL+25] emphasize exact on-policy training."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper proposes on-policy reinforcement learning with optimal reward baseline (OPO), which adheres to exact on-policy training and utilizes theoretically optimal baseline for advantage estimation in the basic policy gradient framework. OPO employs single policy model without relying on KL divergence constraints or entropy regularization, yet achieves superior performance and improved training stability. Furthermore, our results indicate that OPO encourages the generation of more diverse and less repetitive outputs. We have validated the effectiveness of the proposed method on math reasoning tasks using rule-based reward. In future work, we plan to conduct more comprehensive experiments in broader range of reinforcement learning settings to further assess the generality and robustness of our approach. References [ABC+21] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, et al. general language assistant as laboratory for alignment, 2021. [ABW+23] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [ACG+24] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. 8 [Ant24] Anthropic. Claude 3.5 sonnet, 2024. [ARNS19] Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the Impact of Entropy on Policy Optimization. In Proceedings of the 36th International Conference on Machine Learning, pages 151160. PMLR, 2019. [BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, et al. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022. [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint, 2021. [CYL+25] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning, 2025. [Dee24] Google DeepMind. Gemini 2.0 flash thinking, 2024. [GSH22] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [HLL+25] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner series. https://capricious-hydrogen-41c.notion.site/ Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025. Notion Blog. [Hu25] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [KMN+24] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity, 2024. [LCL+25] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [LFX+24] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [MAA24] MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. [Ope23] OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023. [Ope24] OpenAI. Learning to reason with llms, 2024. [OWJ+22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 9 [Qwe24] Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2024. [RSM+23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [SB18] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Bradford Book, Cambridge, MA, USA, 2018. [SC+25] ByteDance Seed, :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, , et al. Seed1.5thinking: Advancing superb reasoning models with reinforcement learning, 2025. [Sch20] John Schulman. Approximating kl divergence, 2020. [SML+18] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018. [SOW+20] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. [SWD+17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [SZY+24] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. [TDG+25] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [WKR+20] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. [XAI24] XAI. Grok 3 beta the age of reasoning agents, 2024. [YLY+25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, , et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [YZZ+25] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, et al. Dapo: An open-source llm reinforcement learning system at scale, 2025. [ZLZ+18] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: benchmarking platform for text generation models. SIGIR, 2018."
        },
        {
            "title": "A Derivation of the Optimal Baseline",
            "content": "To reduce the variance of the policy gradient estimate, we consider adding baseline to the reward. It does not affect the expectation of the gradient and can reduce its variance. By adding the baseline, the variance of the gradient estimate is given by: Var[g] = (cid:104) (θ log πθ(yx) (r(x, y) b))2(cid:105) (E [θ log πθ(yx) (r(x, y) b)])2 Since the second term is independent of b, minimizing the variance is equivalent to minimizing the following objective: J(b) = Eyπθ(x) (θ log πθ(yx) (r(x, y) b))2(cid:105) (cid:104) Let us define the shorthand: Then the objective becomes: g(y) := θ log πθ(yx), := r(x, y) J(b) = Eyπθ(x) (cid:104) (g(y) (r b))2(cid:105) Expanding the square: J(b) = (cid:2)g(y)2 (r b)2(cid:3) = (cid:2)g(y)2 (r2 2rb + b2)(cid:3) = (cid:2)g(y)2r2(cid:3) 2b (cid:2)g(y)2r(cid:3) + b2 (cid:2)g(y)2(cid:3) To minimize J(b), we take the derivative with respect to and set it to zero: = 2 (cid:2)g(y)2r(cid:3) + 2b (cid:2)g(y)2(cid:3) = 0 dJ db = (cid:2)g(y)2r(cid:3) [g(y)2] Conclusion. The optimal baseline that minimizes the variance of the policy gradient estimate (for fixed input x) is: = Eyπθ(x) (cid:104) (cid:105) (θ log πθ(yx))2 r(x, y) (θ log πθ(yx))2(cid:105) (cid:104) Eyπθ(x) This baseline depends on both the policy and the reward and yields the minimum gradient variance. Experiments on Reinforce++ Algorithm Figure 3: Training dynamics of OPO and Reinforce++. Both OPO and Reinforce++ follow the exact on-policy training. Left: Training rewards; Middle: KL divergence; Right: Entropy. 11 OPO can also be applied to other algorithms. We apply it to the Reinforce++ algorithm [Hu25] to further validate its effectiveness. Unlike GRPO, Reinforce++ utilizes the normalized reward of an entire batch as its baseline. We exclude the KL reward in Reinforce++ as exact on-policy training can omit it. For the preliminary experiment, we use Deepseek-R1-Distill-Qwen-1.5B for training, with response length of 8k and batch size of 256 questions. We make both OPO and Reinforce++ follow the exact on-policy training. As shown in Figure 3, the training dynamics demonstrate that OPO, by leveraging the optimal baseline, consistently achieves higher training rewards and maintains higher entropy compared to on-policy Reinforce++. This suggests that the optimal baseline effectively stabilizes training and promotes more diverse policy exploration. more comprehensive evaluation and detailed ablations will be conducted in future work."
        }
    ],
    "affiliations": [
        "Microsoft Research"
    ]
}