{
    "paper_title": "Behavior Knowledge Merge in Reinforced Agentic Models",
    "authors": [
        "Xiangchi Yuan",
        "Dachuan Shi",
        "Chunhui Zhang",
        "Zheyuan Liu",
        "Shenglong Yao",
        "Soroush Vosoughi",
        "Wenke Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains."
        },
        {
            "title": "Start",
            "content": "Xiangchi Yuan1, Dachuan Shi1, Chunhui Zhang2, Zheyuan Liu3, Shenglong Yao1, Soroush Vosoughi2, Wenke Lee1 1Georgia Institute of Technology, 2Dartmouth College, 3University of Notre Dame xyuan300@gatech.edu (cid:128) Project Page Code 6 2 0 2 0 ] . [ 1 2 7 5 3 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is central to posttraining, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers practical mechanism for integrating multiple RL-trained agents from different tasks into single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RLs nonoverlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), distribution-aware merging method explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains."
        },
        {
            "title": "Introduction",
            "content": "Post-training has become cornerstone for aligning large language models (LLMs) to diverse domains (Wei et al., 2021; Liu et al., 2025; Ouyang et al., 2022; Jaech et al., 2024; Tan et al., 2024). While specializing model for single task is effective, real-world applications typically require single model to possess multi-task capabilities. Traditionally, these capabilities are obtained by mixFigure 1: Performance comparison of RAM/RAM+ and baselines on 12 tasks across 3 agent domains. Our method achieves the best average performance and secures SOTA results on 9 out of 12 tasks, surpassing even the original specialized agents (Coding, Memory, Tool). ing offline datasets and performing joint training. However, training such generalist model from scratch is computationally expensive, and maintaining separate checkpoints for each task (Chen et al., 2024; Jin et al., 2025c) is storage-inefficient. Consequently, model merging, which combines multiple task-specific models fine-tuned from the same base model into unified model, has emerged as widely adopted solution (Ilharco et al., 2023; Liu et al., 2024; Matena and Raffel, 2022; Yu et al., 2024). It offers multiple advantages, including data privacy preservation, the elimination of additional training costs, and minimal sacrificed performance. Recently, post-training has shifted from supervised fine-tuning (SFT) to reinforcement learning (RL) (Kimi et al., 2025; Guo et al., 2025), particularly for agentic models with strong reasoning capabilities. This shift fundamentally changes the role of model merging. In the SFT scenarios, multi-task capabilities can still be easily obtained by performing joint training. In contrast, joint multi-task training under on-policy RL is impractical in real-world applications, as it requires parallel task-specific environments and reward models to ensure on-policy training. Consequently, model merging becomes convenient solution in the RL setting. representative example from large-scale industrial agents is UI-TARS2 (Wang et al., 2025a), which trains specialized vertical agents in isolated environments via RL, and subsequently merges them into unified generalist agent. This paradigm reflects practical compromise: specialization through RL, followed by post-hoc integration through model merging. However, directly applying existing model merging methods to RL-trained agents like UI-TARS2 leads to performance degradation. Most prior approaches, including Task Arithmetic (Ilharco et al., 2023), TIES-Merging (Yadav et al., 2023), and DARE (Yu et al., 2024), are developed under the assumption of SFT parameter updates (task vectors) and are therefore mismatched to the RL setting. Unlike SFT, which typically induces dense and redundant parameter updates (Chu et al., 2025; Shenfeld et al., 2025), on-policy RL produces highly sparse and often disjoint task vector distributions, shaped by task-specific reward signals and RL objectives that target narrow behaviors. When such sparse updates are globally averaged during merging, taskspecific unique parameter updates are divided by the number of models, resulting in signal dilution, which degrades task-specific behavior knowledge. this mismatch, we propose Reinforced Agent Merging (RAM), distributionaware merging method designed specifically for RL-trained agents. RAM explicitly disentangles shared and task-specific unique regions of task vectors obtained via RL processes, averaging shared regions to preserve common capabilities while selectively preserving and rescaling unique regions to prevent signal dilution. By maintaining RL task vectors during merging, RAM enables specialized behavior knowledge from each model to coexist within the merged unified model. Our contributions for reinforced agent merging are: To address We identify the mismatch between merging RL-trained agentic models and the existing merging method for SFT-trained models, which is signal dilution in the heterogeneous distribution of sparse parameter updates. We propose distribution-aware merging method that treats shared and task-unique parameter updates differently, averaging the former while preserving and rescaling the latter with distribution, avoiding signal dilution and compensating for performance degradation. Extensive experiments demonstrate that RAM not only outperforms existing merging methods across diverse architectures and domains but also unlocks synergistic potential among agents. The unified RAM model achieves performance superior to that of individual specialized agents on their domain tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Post-training Agents with RL RL has recently emerged as pivotal paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs) (Jaech et al., 2024; Shao et al., 2024; Kimi et al., 2025; Jin et al., 2025d; Diao et al., 2026). Several general-purpose algorithms, such as PPO (Schulman et al., 2017), GRPO (Guo et al., 2025), and DAPO (Yu et al., 2025b), have been developed to support this direction. Beyond general reasoning, RL is extensively applied to specialize agents for diverse domains. In coding, methods like CURE (Wang et al., 2025b), SWE-RL (Wei et al., 2025b), and GLM-4.5 (Zeng et al., 2025) have demonstrated significant success. For memory extension, MemAgent (Yu et al., 2025a) and various cache-based approaches (Shi et al., 2025) optimize long-context handling. Furthermore, RL has been instrumental in developing tool-integrated reasoning agents, such as AutoTIR (Wei et al., 2025a) and ToolRL (Qian et al., 2025), as well as search-augmented agents (Jin et al., 2025b; Team et al., 2025; Sun et al., 2025) and computer-use agents (Wang et al., 2025a; Ye et al., 2025). Model Merging for LLMs Model merging has demonstrated superiority in multi-task learning by synthesizing different task-specific models into single entity without additional training (Ilharco et al., 2023; Jin et al., 2025a; Matena and Raffel, 2022). Techniques such as Task Arithmetic (Ilharco et al., 2023), TIES-Merging (Yadav et al., 2023), and DARE (Yu et al., 2024) have been successfully validated for merging SFT models across multiple tasks. Beyond multi-task integration, merging has also been employed to mitigate model collapse (Yuan et al., 2025b) and enhance reasoning Figure 2: Left: Density (1-Sparsity) of task vectors varies between agent models. Right: Non-zero elements distributions of task vector varies on the number of overlaps with other task vectors. efficiency (Wu et al., 2025). While recent works like UI-TARS2 (Wang et al., 2025a) attempt to merge RL-trained agentic models, they rely on simple weight interpolation, which remains suboptimal for this regime. Distinguished from prior studies, our work is the first to systematically characterize the unique behaviors of task vectors induced by RL and to design tailored merging strategy, that aligns their specific heterogeneity."
        },
        {
            "title": "3 Reinforced Task Vector Behaviors",
            "content": "In this section, we characterize the properties of task vectors derived from reinforcement learning and explain how these properties render existing model merging methods suboptimal. 3.1 Preliminaries and Settings Task Vectors Task vector is the set of parameter updates for specific task. Let θpre Rd denote the parameters of pre-trained base model. We consider different tasks, where each task {1, . . . , } yields fine-tuned model θt. The task vector for task is defined as τ = θt θpre. In this study, we specifically examine task vectors induced by RL fine-tuning, referring to them as Reinforced Task Vectors. The objective of model merging is to synthesize single merged task vector τ merged to construct final model θmerged = θpre + τ merged. Vector Sparsity We define the task vector sparsity of model θt relative to the base model as sparsity(θt, θpre) := 1 θt θpre0/d, where 0 represents the number of non-zero elements and is the total parameter dimension. Following standard practice1 (Paszke et al., 2019; Mukherjee Figure 3: The performance gain (%) of merging unique regions of reinforced task vectors across domains. et al., 2025), we consider two parameters equal (implying zero element in the task vector) if their absolute difference is 105. Reinforced Agentic Models To investigate these properties, we select three representative agentic models trained via RL to specialize in coding, tool-use, and long-context memory as follows. CURE (Wang et al., 2025b): coding agent that co-evolves with unit tester to enhance code generation capabilities. ToolRL (Qian et al., 2025): reasoning agent optimized for general-purpose tool selection and application. MemAgent (Yu et al., 2025a): An agent optimized for long-context tasks, with workflow for extended memory retention. All reinforced agents are initialized from the same base model, Qwen2.5-7B-Instruct (Yang et al., 2024). To evaluate their specialized agentic capabilities and reinforced task vector behaviors, we employ benchmarks across the coding, tool-use, and memory domains; detailed evaluation settings are provided in Section 5.1. 3.2 Heterogeneity in Reinforced Task Vectors In this section, we have the two key observations by analyzing the distribution of reinforced task vectors in parameter space: 1) Reinforced task vectors are sparsely distributed in parameter space, with notable heterogeneous sparsity and distributions. 2) Heterogeneity causes shared and unique regions of reinforced task vectors. Unique regions are critical for improving corresponding agentic domain performances, and exert little negative interference on other domains performances. 1PyTorch uses 105 as the default tolerance for gradient checking (refer to PyTorch Documentation). Heterogeneity in Sparsity and Distribution Recent work (Mukherjee et al., 2025; Yuan et al., 2025a) highlights the inherent sparsity of RL updates: unlike SFT, which updates global parameters, RL tends to fine-tune specific sub-networks. Our analysis confirms this and further unveils heterogeneous patterns in both sparsity and spatial distribution of these updates. First, the percentage of non-zero elements, or the sparsity levels of reinforced task vectors vary drastically. As shown in Figure 2 (left), the coding agent exhibits extreme sparsity, modifying only 3.2% of parameters. In contrast, agents optimized for tool use and long-context memory induce significantly denser updates, affecting 46.2% and 54.3% of the parameter space, respectively. Second, these non-zero elements are distributed across disparate regions, creating different spatial overlap patterns. We categorize parameters (task vector elements) based on whether they are updated by single agent (unique) or multiple agents (shared). Figure 2 (right) reveals that the coding, tool-use, and memory agents concentrate different fractions of their updates in unique, nonoverlapping regions (6.3%, 40.8%, and 47.5% of their respective non-zero elements). Analysis in Appendix C.1 confirms that this heterogeneity generalizes to other reinforced agents with different model architectures and specialized domains. The Role of Heterogeneity We further investigate the impact of these heterogeneous, unique regions on both in-domain and out-of-domain agentic tasks. In our experiments, we isolate the unique component of single task vector, merge it into the base model, and evaluate performance across all domains. Results in Figure 3 demonstrate that unique regions cause almost no negative interference on out-of-domain tasks (occasionally yielding improvements) while driving significant gains on the corresponding in-domain tasks. When we intentionally dilute the magnitude of these unique vectors to 1/N (N = 3) of their original value, indomain performance drops sharply, verifying the positive contribution of these unique components. unique parameter update for task t, averaging with 1 zero-valued updates effectively scales its magnitude by 1/N . This operation dilutes the learned signal without providing any balancing benefit, as these regions hardly interfere with out-ofdomain tasks. We term this phenomenon Signal Dilution. Figure 3 illustrates that this dilution (simulated with = 3) causes significant performance regression. The prevalence of unique task vector components in RL agents therefore necessitates merging strategy capable of disentangling these regions to prevent signal dilution. We provide detailed analysis of signal dilution in each existing merging strategy in Appendix D."
        },
        {
            "title": "4 Merging Reinforced Agentic Models",
            "content": "Our analysis in Section 3.2 demonstrates that reinforced task vectors are inherently sparse and heterogeneous. While task vectors or parameters updated by multiple agents (shared regions) benefit from averaging to stabilize the consensus direction, parameters updated by single agent (unique regions) suffer from Signal Dilution when standard averaging is applied. To address this, we propose Reinforced Agent Merging (RAM) illustrated in Figure 4, method that explicitly disentangles these regions based on distribution statistics of task vectors. RAM applies selective merging strategies: it averages shared parameters to absorb unified multi-task capabilities while preserving the full magnitude of unique parameters to prevent signal dilution. Additionally, we introduce distribution-aware rescaling mechanism to further amplify unique task capabilities. 4.1 Probing Vector Distribution First, we probe the active updated parameters for each reinforced task vector τ t. Using the threshold established in Section 3.2 (e.g., ϵ = 105), we compute binary mask mt {0, 1}d for task t: mt,i = I(τt,i > ϵ), (1) Signal Dilution Previous heterogeneity analysis uncovers the root cause of performance degradation when using previous SOTA merging methods on RL models. Existing methods typically employ element-wise averaging or variants (e.g., (cid:80) τ i). While averaging is benefiτ merged = 1 cial for shared regions of task vectors by balancing multi-task performance, it is detrimental to taskspecific, unique regions in RL scenarios. For where indexes the parameter dimensions and I() is the indicator function. We then define the overlap count vector = (cid:80)N t=1 mt, where ci {0, . . . , } represents the number of agents that actively update the i-th parameter. For any given task t, the set of total updated parameters is partitioned into two disjoint subsets: the Shared subset (where ci 2) and the Unique subset (where ci = 1). To quantify the structural Figure 4: Method Overview. (a) base model is trained via RL to different agents, we track the distributions of obtained reinforced task vectors. (b) Probing the distribution of task vectors to shared, unique, unchanged sets. (c) Selective merging task vectors by averaging shared regions and rescaling unique regions to the base model. distribution of the agents updates, we define the Overlap-Unique Ratio ρt: ρt = (cid:80) (cid:80) i:ci2 mt,i i:ci=1 mt,i . (2) Here, the numerator represents the count of shared parameters, and the denominator represents the count of unique parameters. Since the sum of these two components constitutes the total updated parameter volume, higher ρt indicates that model learns task largely within the shared subspace. 4.2 Rescaling Unique Regions Reinforced task vectors with high Overlap-Unique Ratios (ρt) are likely to suffer greater degradation of task capabilities when their substantial shared regions are averaged with other vectors. Therefore, we proportionally rescale the unique regions of such parts of task vectors to compensate for the performance loss incurred in the shared regions. To achieve this, we calculate task-specific scaling factor λt derived from functional equivalence hypothesis as follows. Let ft denote the functional gain, or task performance gain, induced by the task vector τ t. We decompose the total gain into shared and unique components based on the overlap statistics defined in Section 4.1: ft = Cshared + Cunique, (3) where the gains are modeled as the task vector element τt,i weighted by local sensitivity gi, which indicates the contribution coefficient mapping the task vector element to performance gain. The performance gains are therefore represented as: Cshared = (cid:80) i:ci2 giτt,imt,i, Cunique = (cid:80) i:ci=1 giτt,imt,i. In the merged task vectors, elements in the shared regions are averaged across vectors, and the corresponding task performances are degraded. We model this as contraction of the effective signal by coefficient 1 (0, 1). To counteract this, we rescale the magnitudes of parameters in unique regions and have new functional gain expression: ˆft = (1 r)Cshared + λtCunique. (4) We hypothesize that this rescaling operation achieves the functional equivalence to have the same performance gain for the task vector τt on task t: ˆft ft. Under the simplifying assumption that parameter importance is isotropic (giτt,i const on average), the ratio of functional contributions Cshared/Cunique reduces to the ratio of parameter counts defined in Eq. 2. Then by solving functional equivalence as our objective, the required amplification satisfies: λt 1 = Cshared Cunique (cid:80) (cid:80) i:ci2 mt,i i:ci=1 mt,i = rρt, (5) which is approximately proportional to the OverlapUnique Ratio ρt. This suggests that tasks with higher overlap require stronger compensation in their unique parameter subspace to counteract the degradation induced by averaging. However, directly instantiating this proportional relationship may lead to numerical instability when ρt is large. To balance signal compensation with stability, we employ clipped linear scaling rule: λt = 1 + clip (ρt, 0, α) , (6) where controls the overall amplification strength and α serves as stable bound. This design preserves the monotonic growth implied by the hypothesis while preventing excessive amplification in high-overlap scenarios. consensus; and DARE (Yu et al., 2024), which randomly drops and rescales the parameters. Following standard practice, we combine DARE with Task Arithmetic and TIES for evaluation. We introduce more details about baselines in Appendix D. 4.3 Selective Merging the merged task vector Finally, we construct τ merged element-wise. Unlike existing merging methods, which effectively divide unique parameters by (causing signal dilution), our strategy differentiates between shared and unique regions. For each parameter index i, let Ti = {t mt,i = 1} denote the set of indices of active tasks for that parameter. Note that the cardinality Ti corresponds to the overlap count ci defined in Section 4.1. The merged element τmerged,i is computed as: τmerged,i = 0 λt τt,i (cid:80) 1 Ti tTi if Ti = 0, if Ti = {t}, if Ti 2. τt,i (7) This selective strategy ensures that: 1) Shared Knowledge (Ti 2) is averaged to balance multi-task capabilities. 2) Task-Specific Knowledge (Ti = 1) is completely preserved and amplified by λt to compensate for the contraction of the effective signal in shared regions, explicitly targeting functional equivalence. 3) No Knowledge (Ti = 0) is set to zero to filter out insignificant parameter fluctuations and ensure that the base models general capabilities remain undisturbed."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Baselines We categorize our baselines into two groups: original specialized agent models and es- (i) Origtablished model merging techniques. inal Agent Models: We utilize Qwen2.5-7BInstruct (Yang et al., 2024) as the shared base model. The task-specific reinforced agents include: CURE (Wang et al., 2025b) for coding, ToolRL (Qian et al., 2025) for tool-use, and MemAgent (Yu et al., 2025a) for long-context (ii) Merging Methods: We compare memory. our approach against prominent merging strategies: Task Arithmetic (Ilharco et al., 2023), which linearly combines task vectors; Fisher Merging (Matena and Raffel, 2022), which weighs parameters based on Fisher information; TIESMerging (Yadav et al., 2023), which mitigates parameter interference through trimming and sign Evaluations We evaluate the models across three critical agentic domains: coding, tool-use, and long-context memory. For Coding, we measure generated code pass accuracy (ACC) and unit test pass accuracy (UT) on the LiveBench (White et al., 2025) and LiveCodeBench (Jain et al., 2024) benchmarks. For Tool Use, we utilize the Berkeley Function Call Leaderboard (BFCL) (Patil et al., 2025), specifically reporting results on the Live/Non-Live Parallel (Para) and Parallel Multiple (P_Mul) subsets. For Long-Context Memory, following (Yu et al., 2025a), we employ the RULER benchmark (Hsieh et al., 2024) to assess performance on long-context tasks, including RULER (Hsieh et al., 2024) HotpotQA and SQuAD with 7K, 14K, 32K, and 64K lengths. Additional datasets and detailed evaluation criteria are provided in Appendix B. Implementations For hyperparameters, we set = 0.1 and α = 2.0. We denote our method without task-specific rescaling as RAM and with rescaling as RAM+. RAM is the special case of RAM+ when = 0. We provide the details of pseudocode and agents for merging in Appendix A. 5.2 RAM is Better Fit for Reinforced Models Table 1 demonstrates that both RAM and RAM+ consistently outperform all baselines, establishing new SOTA. Specifically, RAM achieves an average score of 64.82 across all tasks, surpassing the strongest baseline DARE (63.33). Building on this foundation, RAM+ further pushes the boundary to 66.55 after rescaling unique regions, unlocking synergistic potential among agents where the merged generalist exceeds the capabilities of specialized task agents on most of the evaluations. For instance, in the Coding domain, RAM+ surpasses the specialist Coding agent on LiveBench and LiveCodeBench, suggesting that reasoning signals from other tasks enhance coding precision. This superiority extends to Tool Use, where RAM+ significantly outperforms the Tool agent in complex parallel scenarios (Live P_Mul: 70.83 vs. 58.33), and to Long-Context Memory, where it achieves global optimal performance on SQuAD 64k (82.03), beating the dedicated Memory agent (77.34). Model LiveBench UT ACC LiveCodeBench ACC UT Live Non-Live Para P_Mul Para P_Mul HotpotQA 14K 7K SQuAD Avg 32K 64K Coding Tool Using Memory Base CURE (Coding) ToolRL (Tool) MemAgent (Memory) TA Fisher TIES DARE+TA DARE+TIES RAM RAM+ 28.35 37.70 31.84 39.25 38.09 36.72 39.25 37.50 35. 38.28 40.23 40.87 49.27 41.36 50.12 51.62 48.73 49.88 48.60 45.66 49.71 52.57 23.43 30.23 26.76 28.92 31.95 30.87 30.63 31.95 29. 31.96 31.60 Base and Task Models 36.42 45.76 42.05 44.80 46.69 45.89 46.32 46.69 39.53 47.72 46.84 56.25 56.25 56.25 37. 41.67 37.50 58.33 50.00 Merged Models 45.83 43.75 41.67 43.75 54.17 43.75 62.50 50.00 56.25 58.33 56.25 56.25 66.67 70.83 68.00 64.00 91.00 78.50 87.50 86.5 84.00 92.50 91. 91.00 90.50 55.00 51.50 89.00 48.50 69.50 63.5 67.50 89.50 90.00 91.50 91.00 60.94 58.59 58.59 78.91 69.53 60.06 71.88 76.56 75. 75.78 79.69 50.00 56.25 48.44 78.12 68.75 49.22 82.03 76.56 77.34 75.00 78.13 64.84 60.94 59.38 81.25 73.44 58.59 75.00 77.34 76. 74.22 78.91 58.59 44.22 46.95 77.34 72.66 60.94 75.78 70.31 74.22 79.69 82.03 48.70 49.35 54.16 57.77 58.28 52.20 60.02 63.33 62. 64.82 66.55 Table 1: Main results of agent merging. We evaluate the capabilities across three domains: Coding (LiveBench, LiveCodeBench), Tool Use (Live, Non-Live), and Memory (RULER-HotpotQA, RULER-SQuAD). Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the specialized Task Models. Figure 5: The performances of merging two agents across domains. 5.3 Extending Model Combinations To evaluate the effectiveness of RAM beyond triagent merging, we extend pairwise agent merging experiments with more model combinations across Tool+Memory, Coding+Tool, and Coding+Memory scenarios, as illustrated in Figure 5 (details are provided in Appendix C). Across all three combinations, RAM/RAM+ consistently achieves the highest average performance, demonstrating superior robustness on various combinations compared to baselines. Specifically, in the Coding+Tool setting, RAM+ attains an average score of 60.04, significantly outperforming the strongest baseline DARE+TIES (56.74) and effectively bridging the capability gap that traditional methods like Task Arithmetic and TIES fail to address due to signal dilution. Similarly, in the Tool+Memory and Coding+Memory scenarios, RAM+ maintains dominant performance with average scores of 75.86 and 61.21 respectively, confirming that RAM can be successful in multiple agent combinations. 5.4 Ablation Study We ablate our method to RAM (r = 0) and RAM+, and investigate the sensitivity of our proposed method to the scaling factor r. Table 2 presents the ablation results with varying from 0.00 to 0.20. Note that when = 0, the method represents RAM. As observed, the models overall perCode Tool Memory LiveBench UT ACC LiveCodeBench ACC UT Live Non-Live Para P_Mul Para P_Mul HotpotQA 14k 7k RulerQA 32k 64k 38.28 37.70 40.23 38.67 39.45 49.71 50.49 52.57 49.85 50.36 31.96 30.63 31.60 31.41 32.58 47.72 45.59 46.84 46.53 47.54 56.25 62.50 56.25 62.50 56. 66.67 62.50 70.83 66.67 62.50 91.00 92.00 90.50 89.50 91.00 91.50 91.50 91.00 91.50 90.50 75.78 75.78 79.69 78.12 78.12 75.00 79.69 78.13 79.69 78.12 74.22 75.00 78.91 79.69 80. 79.69 82.03 82.03 75.78 77.34 0.00 0.05 0.10 0.15 0.20 Avg 64.82 65.45 66.55 65.83 65.35 Table 2: Ablation Study. Bold and underlined values denote the best and second-best performance. domains, where the merged generalist surpasses the performance of the original specialized agents. For instance, in the Tool domain, RAM+ exhibits significant margin over the specialist, suggesting that reasoning capabilities from Math and Search agents synergize to enhance tool-use. In the Search domain, RAM/RAM+ successfully retain original capability, whereas baselines show notable regression. These results confirm that the heterogeneity of reinforced task vectors is general property, and RAM effectively addresses this by preserving task-specific specialized knowledge independent of model scale and architecture. Additional Experiments Besides the above experiments, we further provide the instruction following evaluation to assess the forgetting after merging in Appendix C.2; Merging efficiency comparison in Appendix C.3; Evaluation on the additional tasks in Appendix C.5; Additional rescaling strategy performance in Appendix E."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we address the critical challenge of merging agents fine-tuned via RL, identifying fundamental mismatch between standard merging techniques designed for dense SFT updates and the sparse, heterogeneous nature of on-policy RL task vectors. We demonstrate that treating global task vectors equally in previous methods in this setting leads to signal dilution of task-specific capabilities. To bridge this gap, we propose Reinforced Agent Merging (RAM), method that explicitly disentangles shared and unique parameter update regions and applies distribution-based rescaling strategy to preserve specialized behaviors. Extensive evaluations across multiple agentic domains and model architectures show that RAM significantly outperforms existing baselines, achieving SOTA results and surpassing the original specialists as unified generalist on most tasks. Figure 6: Merging results for RL agents trained from Llama3.2-3B-Instruction base model. formance (Avg) exhibits trend of initially increasing and then decreasing. The performance peaks at = 0.10, achieving the highest average score of 66.55. Specifically, setting = 0.10 (RAM+) yields the best or second-best results across the majority of metrics, particularly showing significant gains in LiveBench (Coding) and HotpotQA (Memory) compared to RAM (r = 0.00). However, further increasing beyond 0.10 leads to diminishing returns, with the average score dropping to 65.35 at = 0.20. This suggests that while moderate scaling factor effectively enhances task-specific capabilities, an excessively large may disrupt the general knowledge of the merged model. 5.5 Extending Architecture and Domains Besides the agents trained from Qwen architecture, we extend the experiment to Llama architecture and additional domains. We choose Llama3.23B (Grattafiori et al., 2024) as the base model, and choose models trained from it via RL: search agent ZeroSearch (Sun et al., 2025), math reasoning agent (Zhao et al., 2025), and tool-using agent ToolRL (Qian et al., 2025). The evaluation details are provided in Appendix B.4. Figure 6 illustrates that, consistent with our findings on the Qwen, RAM and RAM+ demonstrate superior performance across multiple agentic domains, consistently outperforming baselines. Notably, they achieve positive synergy in both Math and Tool"
        },
        {
            "title": "Limitations",
            "content": "While Reinforced Agent Merging (RAM) effectively mitigates signal dilution for RL-trained agents, our current study has the following limitations. First, our experiments focus on merging common number of agents; as the number of agents scales significantly, the probability of parameter collision in the shared subspace increases, potentially requiring more complex conflict resolution strategies beyond simple averaging. Second, the derivation of our rescaling factor relies on an isotropic assumption of parameter importance, which, while empirically robust, does not explicitly account for element-wise curvature information that could offer finer-grained control at higher computational cost. Third, although we identified default hyperparameter configuration that generalizes well across Qwen and Llama architectures, optimal performance on agents trained with fundamentally different data or modalities may require task-specific tuning. Finally, our evaluation is primarily conducted on 3B and 7B parameter models; verifying whether the sparsity hypothesis and RAMs efficacy persist in massive-scale models (70B+) remains an open question for future research."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2024. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations. Runxi Cheng, Feng Xiong, Yongxian Wei, Wanyun Zhu, and Chun Yuan. 2025. Whoever started the interference should end it: Guiding data-free model merging via task vectors. In Forty-second International Conference on Machine Learning. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. SFT memorizes, RL generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Xingjian Diao, Zheyuan Liu, Chunhui Zhang, Weiyi Wu, Keyi Kong, Lin Shi, Kaize Ding, Soroush Vosoughi, and Jiang Gui. 2026. Addressing overthinking in large vision-language models via gated perception-reasoning optimization. arXiv preprint arXiv:2601.04442. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. The language model evaluation harness. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025a. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025b. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling. Can Jin, Hongwu Peng, Qixin Zhang, Yujin Tang, Dimitris Metaxas, and Tong Che. 2025c. Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning. arXiv preprint arXiv:2504.09772. Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, and Dimitris Metaxas. 2025d. Your reward function for rl is your best prm for search: Unifying rl and search-based tts. arXiv preprint arXiv:2508.14313. Team Kimi, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and 1 others. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, and 1 others. 2022. Competition-level code generation with alphacode. Science. Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2024. Towards safer large language models through machine unlearning. arXiv preprint arXiv:2402.10058. Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, and Meng Jiang. 2025. Modality-aware neuron pruning for unlearning in multimodal large language models. arXiv preprint arXiv:2502.15910. Michael Matena and Colin Raffel. 2022. Merging models with fisher-weighted averaging. Advances in Neural Information Processing Systems. Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, and Hao Peng. 2025. Reinforcement learning finetunes small subnetworks in large language models. arXiv preprint arXiv:2505.11711. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, and 1 others. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. 2025. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. 2025. Codeforces. https://huggingface. co/datasets/open-r1/codeforces. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru WANG, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. ToolRL: Reward is all tool learning needs. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. 2025. RLs razor: Why on-policy reinforcement learning forgets less. In AI That Keeps Up: NeurIPS 2025 Workshop on Continual and Compatible Foundation Model Updates. Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, and Yingyan Celine Lin. 2025. Lacache: Ladder-shaped kv caching for efficient long-context modeling of large language models. In Forty-second International Conference on Machine Learning. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan Zhang. 2025. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588. Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan DeLiu, Bing Yin, and Meng Jiang. 2024. mocratizing large language models via personalized parameter-efficient fine-tuning. arXiv preprint arXiv:2402.04401. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, and 1 others. 2025. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, and 1 others. 2025a. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544. Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. 2025b. CURE: Co-evolving coders and unit In The Thirtytesters via reinforcement learning. ninth Annual Conference on Neural Information Processing Systems. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. 2025a. Autotir: Autonomous tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. 2025b. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, ShubhAgrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. 2025. Livebench: challenging, contamination-limited LLM benchmark. In The Thirteenth International Conference on Learning Representations. Taiqiang Wu, Runming Yang, Tao Liu, Jiahao Wang, and Ngai Wong. 2025. Revisiting model interarXiv preprint polation for efficient reasoning. arXiv:2510.10977. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving interference when merging models. arXiv preprint arXiv:2306.01708. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 40 others. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, and 1 others. 2025. Mobileagent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144. Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, WeiYing Ma, Jingjing Liu, Mingxuan Wang, and 1 others. 2025a. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, YuYue, Weinan Dai, Tiantian Fan, Gaohong Liu, Juncai Liu, LingJun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, and 17 others. 2025b. DAPO: An open-source LLM reinforcement In The Thirty-ninth Anlearning system at scale. nual Conference on Neural Information Processing Systems. Xiangchi Yuan, Xiang Chen, Tong Yu, Dachuan Shi, Can Jin, Wenke Lee, and Saayan Mitra. 2025a. Mitigating forgetting between supervised and reinforcement learning yields stronger reasoners. arXiv preprint arXiv:2510.04454. Xiangchi Yuan, Chunhui Zhang, Zheyuan Liu, Dachuan Shi, Leyan Pan, Soroush Vosoughi, and Wenke Lee. 2025b. Superficial self-improved reasoners benefit from model merging. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, and 1 others. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. 2025. Learning to reaarXiv preprint son without external rewards. arXiv:2505.19590. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Pseudocode Here we provide the Pseudocode of RAM in Algorithm 1. Algorithm 1 Reinforced Agent Merging (RAM+) Require: Task vectors {τ t}N t=1; threshold ϵ; rescale strength r; clip bound α t=1 mt i:ci2 mt,i i:ci=1 mt,i Nshared (cid:80) Nunique (cid:80) ρt Nshared/Nunique λt 1 + clip(ρt, 0, α) Ensure: Merged task vector τ merged 1: // Stage 1: Probing Vector Distribution (Sec. 4.1) 2: for = 1 to do 3: mt I(τ > ϵ) 4: end for 5: (cid:80)N 6: // Stage 2: Rescaling Unique Regions (Sec. 4.2) 7: for = 1 to do 8: 9: 10: 11: 12: end for 13: // Stage 3: Selective Merging (Sec. 4.3) 14: for each parameter index do Ti {t mt,i = 1} 15: if Ti = 0 then 16: τmerged,i 0 17: else if Ti = 1 then 18: 19: 20: 21: 22: 23: 24: end for 25: return τ merged Let be the unique element in Ti τmerged,i λt τt,i τmerged,i 1 Ti end if else τt,i tTi (cid:80) A.2 Details of Reinforced Task Agents In this section, we provide the detailed specifications and sources for the reinforced agentic models and base models used in our experiments. All models are publicly available on Hugging Face. Qwen2.5-7B-Instruction Series: We utilize the following agents initialized from the Qwen2.5-7BInstruct: Base Model (Qwen2.5-7B-Instruct) (Yang et al., 2024) Qwen2.5-7B-Instruct Coding Agent (CURE) (Wang et al., 2025b) ReasonFlux-Coder-7B Tool Agent (ToolRL) (Qian et al., 2025) Qwen2.5-7B-Instruct-ToolRL-grpo-cold Memory Agent (MemAgent) (Yu et al., 2025a) RL-MemoryAgent-7B Search Agent (ZeroSearch) (Sun et al., 2025) ZeroSearch_google_V2_Qwen2.5_7B_Instruct Tool Integrated Reasoning Agent (AutoTIR) (Wei et al., 2025a) AutoTIR-Qwen2.5-7B-Instruct Llama-3.2-3B-Instruction Series To verify generalization across architectures, we utilize the following agents based on Llama-3.2-3B-Instruct: Base Model (Llama-3.2-3B-Instruct) (Grattafiori et al., 2024) Llama-3.2-3B-Instruct Math Agent (GRPO-Math) (Guo et al., 2025) Llama-3.2-3B-Instruct-GRPO-MATH-1EPOCH Tool Agent (ToolRL) (Qian et al., 2025) ToolRL-Llama3.2-3B Search Agent (ZeroSearch) (Sun et al., 2025) ZeroSearch_google_V2_Llama_3.2_3B_Instruct"
        },
        {
            "title": "B Evaluation Details",
            "content": "B.1 Coding Evaluation Following the evaluation setting established in Wang et al. (2025b), we conduct comprehensive evaluation of coding capabilities across five widely adopted coding benchmarks. utilize LiveBench Datasets. We (White et al., 2025) (standard test set), MBPP (Austin (standard test set), and Liveet al., 2021) CodeBench (Jain et al., 2024) (Version 2, 511 problems). For competition-level tasks, we include CodeContests (Li et al., 2022), filtering for tasks with difficulty 2 and utilizing held-out split of 200 examples. Additionally, we use CodeForces (Penedo et al., 2025), comprising 500 randomly sampled examples distinct from CodeContests. Evaluation Protocol. To ensure consistency, all datasets are standardized to the stdio format. Functional inputs from LiveBench, LiveCodeBench, and MBPP are converted by placing variables on separate lines and flattening lists. For verification, we use official ground-truth solutions for CodeContests and MBPP. For the remaining datasets (CodeForces, LiveCodeBench, LiveBench), we utilize high-quality reference solutions generated by QwQ-32B (via Best-of-3 sampling). We employ vLLM (Kwon et al., 2023) for generation. Following standard practices, sampling parameters are set to temperature = 1.0, top-p = 0.95. We report performance using pass accuracy, including code pass (ACC) and unit test pass (UT) (Pass@1) and Best-of-N (BoN, N=4) metrics. B.2 Long-Context Memory Evaluation To rigorously assess the long-context memory capabilities of our agents, we adopt the evaluation protocol from the RULER benchmark (Hsieh et al., 2024), strictly following the data synthesis configurations established in Yu et al. (2025a). We specifically select RULER-HotpotQA and RULERSQuAD as our primary benchmarks to evaluate multi-hop reasoning and precise fact retrieval. Datasets. We utilize the following two tasks adapted for long-context memory evaluation: RULER-HotpotQA: This task serves as robust testbed for multi-hop reasoning. In this setup, multiple \"golden paragraphs\" containing necessary evidence are embedded within vast amount of distractor content (the haystack). The model must effectively identify and synthesize these scattered pieces of evidence from its memory to correctly answer complex questions. RULER-SQuAD: Adapted from the SQuAD dataset, this task evaluates precise reading comprehension. Ground-truth passages are inserted into long distractor texts, requiring the model to maintain high fidelity to specific facts over extended sequences. This tests the agents ability to accurately recall specific instructions or details without hallucination. Evaluation Protocol. Consistent with Yu et al. (2025a), we synthesize test samples with varying context lengths to stress-test memory capacity with different context lengths (8K-128K for SQuAD and 7K-896K for HotPotQA). The primary evaluation metric is the Substring Exact Match (sub_em) of the generated answers. High accuracy in this setting demonstrates that the merged agent successfully retains critical task-specific memory capabilities and can effectively filter out noise (distractors) inherent in long-context processing. behaviors. We specifically use the Live and NonLive datasets to measure performance across both real-world and synthetic scenarios. Datasets. We utilize the following two subsets to assess distinct dimensions of function calling: Non-Live Dataset (Synthetic & Curated): Derived from BFCL V1, this subset consists of expert-curated synthetic tasks designed to test fundamental logic across various languages (Python, Java, JavaScript) and SQL. It evaluates the models adherence to precise instructions in controlled The tasks of Non-Live environments. Parallel, datasets include: Multiple, Relevance, Simple, Parallel_multiple and Irrelevance. Live Dataset Introduced in BFCL V2, (Real-World & Crowdsourced): this subset comprises user-contributed examples interactions. Unfrom real-world agent these samples are like the Non-Live set, diverse and noisy, involving complex APIs with nested parameters. This benchmark specifically challenges the models robustness in handling ambiguous queries and including detecting function irrelevance, Irrelevance, seven tasks: Simple_javascript, Simple_java, Parallel_multiple, and Simple_python. Multiple, Parallel Evaluation Protocol. To ensure robust evaluation, we utilize the Abstract Syntax Tree (AST) matching method provided by the BFCL framework. Unlike simple string matching, AST evaluation parses generated function calls into syntax trees to structurally verify argument permutations and formatting variations while enforcing strict type correctness. We report accuracy for both Live and Non-Live splits, with particular focus on the challenging Parallel and Parallel Multiple categories to demonstrate advanced planning capabilities. B.3 Tool Use Evaluation B.4 Evaluation Details for Llama-based To comprehensively evaluate the tool-use (function calling) capabilities of our agents, we employ the Berkeley Function Calling Leaderboard (BFCL) (Patil et al., 2025), widely recognized as the standard benchmark for assessing LLM agentic Agents We evaluate agents trained from LLama3.2-3BInstruction on three domains: math, search, and tool-use. For the math domain, we evaluate the model on GSM8K (Cobbe et al., 2021) and specialized in multiple domains together, the heterogeneity in sparsity and distribution remains significant. Specifically, the sparsity of task vectors spans wide spectrum, ranging from merely 3.2% for the Code agent to 54.3% for the Memory agent. The overlap analysis further reveals distinct behaviors: the Code agent is highly entangled with others, with 43.7% of its changed parameters shared among 3 or 4 other agents (w. 3-4). In contrast, agents like Tool and Memory maintain higher independence, with unique parameter ratios (w. 0) of 26.1% and 21.9%, respectively. Second, we extend the experiment to Llama architecture and additional domains. We choose Llama3.2-3B-Instruction (Grattafiori et al., 2024) as the base model, and choose models trained from it via RL: search agent ZeroSearch (Sun et al., 2025), math reasoning agent (Zhao et al., 2025), and tool-using agent ToolRL (Qian et al., 2025). Figure 8 demonstrates that similar heterogeneity in task vectors persists across different model architectures. As shown in the left panel, the sparsity of task vectors varies significantly, ranging from 17.0% for the Math agent to 56.8% for the Tool agent. The overlap distribution (right panel) further highlights this diversity: the Tool agent modifies large proportion of unique parameters (54.0%), whereas the Math agent shares the majority of its updates with other tasks, with only 24.2% of its modified parameters being unique. This confirms that the diverse characteristics of reinforced task vectors are consistent across different base models and task domains. C.2 Instruction Following Evaluation primary concern in model merging, particularly when combining agents fine-tuned via RL on disparate domains, is the potential degradation of the base models general instruction following capabilities (i.e., catastrophic forgetting). To rigorously evaluate whether RAM compromises the models ability to follow general instructions while pursuing task specialization, we conducted evaluations on the IFEval (Instruction Following Evaluation) benchmark (Zhou et al., 2023) provided by LMEvaluation-Harness (Gao et al., 2024). We report results across four metrics: Instruction Accuracy and Prompt Accuracy, under both Loose and Strict evaluation criteria. The results are presented in Table 3. We evaluated two sets of models: Qwen2.5-7B-Instruct: The primary setting Figure 7: Additional distribution analysis for sparse reinforced task vectors trained from Qwen2.5-7BInstruction. Left: Density (1-Sparsity) of task vectors varies between agent models. Right: The number of overlaps with other task vectors. Figure 8: The task vector analysis for agents trained from Llama3.2-3B-Instruction via RL. Left: Density (1-Sparsity) of task vectors varies between tool-using agent, web search agent, and the math reasoning agent models. Right: The number of overlaps with other task vectors. MATH500 (Hendrycks et al., 2021) datasets, provided by LM-Evaluation-Harness (Gao et al., 2024). For the search domain, we follow the evaluation setting provided in ZeroSearch (Sun et al., 2025) and evaluate the agent on NQ (Kwiatkowski et al., 2019) and 2WikiMultiHopQA (Ho et al., 2020). For tool-use, the setting is the same as Section 5.1. We take the average score across tasks for each domain for evaluation."
        },
        {
            "title": "C Additional Experiments",
            "content": "C.1 Additional Reinforced Task Vector Analysis To further verify heterogeneity in reinforced task vectors introduced by Section 3.2, we extend the number, domains, and architecture of the reinforced agents. First, we include extra agents, web search agent ZeroSearch (Sun et al., 2025) and tool-integrated reasoning agent AutoTIR (Wei et al., 2025a), which are both RL-trained from Qwen2.5-Instruct-7B. Figure 7 shows that when including five agents Model Instruction Acc (%) Strict Loose Prompt Acc (%) Strict Loose Instruction Acc (%) Strict Loose Prompt Acc (%) Loose Strict Qwen2.5-7B-Instruct Llama-3.2-3B-Instruction Base Model Code/Math Tool Memory/Search 69.18 73.14 (+3.96) 71.82 (+2.64) 69.78 (+0.60) 64.39 68.11 (+3.72) 66.07 (+1.68) 66.07 (+1.68) Base and Task Experts 58.96 62.48 (+3.52) 61.55 (+2.59) 58.41 (-0.55) 53.23 58.75 (+5.52) 54.34 (+1.11) 53.60 (+0.37) Merged Models 69.90 68.59 (-1.31) 68.11 (-1.79) 67.63 (-2.27) 63.31 61.99 (-1.32) 61.87 (-1.44) 60.67 (-2.64) 59.15 57.49 (-1.66) 57.12 (-2.03) 55.45 (-3.70) 51.20 48.43 (-2.77) 49.35 (-1.85) 46.58 (-4.62) Task Arithmetic (TA) Fisher TIES DARE+TA DARE+TIES 70.74 (+1.56) 72.06 (+2.88) 71.34 (+2.16) 70.38 (+1.20) 69.42 (+0.24) 65.71 (+1.32) 66.91 (+2.52) 67.39 (+3.00) 65.95 (+1.56) 65.11 (+0.72) 58.90 (-0.06) 61.18 (+2.22) 59.52 (+0.56) 58.41 (-0.55) 57.67 (-1.29) 53.23 (0.00) 55.08 (+1.85) 55.64 (+2.41) 53.05 (-0.18) 52.31 (-0.92) 68.82 (-1.08) 67.87 (-2.03) 58.75 (-11.15) 59.11 (-10.79) 58.63 (-11.27) 61.87 (-1.44) 61.75 (-1.56) 58.87 (-4.44) 54.20 (-9.11) 53.48 (-9.83) 57.12 (-2.03) 55.82 (-3.33) 45.84 (-13.31) 46.21 (-12.94) 45.66 (-13.49) 49.72 (-1.48) 49.17 (-2.03) 46.21 (-4.99) 41.04 (-10.16) 39.56 (-11.64) RAM (Ours) RAM+ (Ours) 70.62 (+1.44) 69.18 (0.00) 65.95 (+1.56) 64.75 (+0.36) 59.70 (+0.74) 57.86 (-1.10) 53.23 (0.00) 51.94 (-1.29) 67.39 (-2.51) 66.19 (-3.71) 61.39 (-1.92) 61.27 (-2.04) 55.45 (-3.70) 53.97 (-5.18) 47.87 (-3.33) 47.50 (-3.70) Table 3: Evaluation of General Capabilities on IFEval benchmark. We report Instruction Accuracy and Prompt Accuracy under both Loose and Strict criteria. Values in parentheses denote the absolute change relative to the Base Model. Green values indicate improvement, while orange values indicate regression. RAM demonstrates superior robustness compared to TIES/DARE, especially on the smaller Llama-3.2-3B-Instruction model. used in the main paper, trained via RL to obtain the Coding, Tool, and Memory agents. Llama-3.2-3B-Instruction: To test the generalization of our method on different architecture and scale, we use the fine-tuned Math, Tool, and Search agents based on Llama-3.23B-Instruction. On the Qwen-based agents, RAM not only retains the general capabilities of the Base model but explicitly outperforms it across most metrics (e.g., +1.44 in Loose Instruction Accuracy and +1.56 in Strict Instruction Accuracy). This suggests that the specialized reasoning circuits preserved by RAMs disjoint merging strategy can positively transfer to general instruction following. RAM+ shows slight trade-off, generally maintaining parity with the base model on instruction-level metrics while incurring minor regressions in prompt-level accuracy. Merging on smaller Llama-based models is inherently more challenging due to limited parameter redundancy. While all merging methods exhibit some regression compared to the Base model, RAM demonstrates superior stability with less forgetting on instruction following. Notably, baseline methods like TIES and DARE+TIES suffer from severe performance collapse, dropping over 10 percentage points (e.g., -11.15 in Loose Instruction Accuracy). Even in strict evaluation, TIES fails to maintain robustness. In contrast, RAM avoids this collapse, showing significantly smaller regressions (approx. 2-3) and proving it is much safer mergFigure 9: RAM/RAM+ demonstrates superior tradeoff between merging time and average score compared to the baseline method. ing strategy for smaller architectures compared to aggressive trimming methods. C.3 Merging Efficiency Beyond merged performance, computational efficiency is another important factor for practical model merging. Figure 9 illustrates the comparison between merging time (in seconds) and the average score across benchmarks. Current baselines exhibit clear dichotomy: methods like TA and Fisher are computationally efficient (<110s) but suffer from suboptimal performance (around 58.2), while complex methods like TIES and DARE variants achieve better scores at the cost of significant computational overhead (>400s). In contrast, our proposed methods occupy the Pareto frontier of the efficiency-performance landscape. Specifically, RAM achieves remarkable score of 64.82 in just 75.4 seconds, surpassing DARE+TA in performance while offering 5.5 speedup. Even our more intensive variant, RAM+, establishes new SOTA performance (66.55) while remaining significantly faster than both TIES and DARE. These results demonstrate that RAM effectively identifies critical parameters for merging processing without the extensive computational redundancy found in previous SOTAs. C.4 Detailed Numerical Results for Pairwise Agent Merging In Section 5.2, we visualize the performance of merging two agents using bar charts to highlight overall trends and comparative advantages. Here, we provide the corresponding detailed numerical results for all pairwise agent combinations, including Coding+Tool, Tool+Memory, and Coding+Memory, as shown in Tables 4, 5, and 6, respectively. These tables serve as precise quantitative complement to the bar chart visualizations, enabling fine-grained inspection of per-domain and per-metric behaviors. Coding + Tool. As reported in Table 6, RAM and RAM+ consistently achieve the highest average performance among all merged models. In particular, RAM+ attains an average score of 60.04, significantly outperforming the strongest baseline DARE+TIES (56.74). Notably, RAM/RAM+ improve both coding accuracy (LiveBench / LiveCodeBench) and complex tool-use metrics (Parallel and Parallel-Multiple), indicating that preserving task-unique reinforced updates enables the merged model to simultaneously retain algorithmic reasoning and structured tool invocation capabilities. In contrast, baseline methods exhibit clear trade-off, improving one domain at the expense of the other due to signal dilution in sparse RL task vectors. Tool + Memory. Table 5 presents the detailed results for merging Tool and Memory agents. RAM achieves the highest overall average score (76.67), while RAM+ remains highly competitive (75.86), both surpassing all baselines. RAM-based models show strong performance on long-context memory tasks (HotpotQA and RulerQA) without degrading tool-use accuracy, particularly on challenging Parallel and Parallel-Multiple subsets. These results demonstrate that RAM effectively isolates memoryspecific reinforced updates from tool-specific ones, preventing destructive interference that commonly occurs in averaging-based merging methods. Coding + Memory. As shown in Table 6, merging Coding and Memory agents further stresses the heterogeneity of reinforced task vectors, as these two domains exhibit minimal parameter overlap. Despite this challenge, RAM+ achieves the best average performance (61.21), outperforming all baselines and even exceeding the original Coding agent on LiveBench ACC/UT. This highlights the effectiveness of overlap-aware rescaling in compensating for performance loss in shared subspaces, while fully preserving unique reasoning and memory patterns critical for each task. Across all pairwise combinations, the numerical results reported here are fully consistent with the trends observed in the bar charts in the main text. Specifically, RAM and RAM+ not only deliver the highest average scores but also exhibit superior robustness across heterogeneous domains and metrics. These findings further confirm that the advantages of RAM are not limited to tri-agent merging, but generalize naturally to arbitrary agent combinations, reinforcing its suitability as unified merging method for reinforced agentic models. C.5 Additional Results in Merging Three Agents In Section 5.2, we present the agent performance in three domains. Here, we further provide experiment results on additional tasks and settings to comprehensively verify the advantages of RAM. Overall Analysis. Tables 7, 8, and 9 report comprehensive results of merging three reinforced agents across coding, tool-use, and long-context memory domains, substantially extending the representative results in Section 5.2. These results consistently corroborate the advantages of RAM and RAM+ under wide range of evaluation metrics and task granularities. Coding Domain. As shown in Table 7, RAMbased methods achieve the strongest overall performance among merged models across LiveBench, LiveCodeBench, MBPP, and CodeContests. Notably, RAM attains the highest average score (49.64), outperforming all baseline merging strategies, while RAM+ further improves performance on challenging subsets such as LiveBench ACC/UT and MBPP ACC/UT. Importantly, RAM and RAM+ frequently match or exceed the original Model LiveBench UT ACC LiveCodeBench ACC UT Live Non-Live Avg Para P_Mul Para P_Mul Code Tool Base CURE (Coding) ToolRL (Tool) 28.35 37.70 31.84 40.87 49.27 41.36 Base and Task Models 56.25 36.42 23.43 56.25 45.76 30.23 56.25 42.05 26.76 TA Fisher TIES DARE+TA DARE+TIES RAM RAM+ 37.11 36.72 35.74 35.74 35. 39.45 39.45 49.09 50.29 46.50 47.40 45.79 51.42 51.10 Merged Models 45.83 48.18 44.48 44.62 44.48 50.00 50.00 56.25 56.25 56.25 29.55 30.68 31.21 30.04 31. 31.21 32.53 46.51 47.55 56.25 62.50 41.67 37.50 58.33 37.50 37.50 54.17 58.33 66.67 66.67 66. 68.00 64.00 91.00 86.00 88.50 83.00 90.00 88.50 91.00 90.50 55.00 51.50 89.00 59.00 64.50 90.50 84.50 85.50 91.00 90. 43.75 46.53 54.57 49.26 50.80 55.23 55.86 56.74 59.19 60.04 Table 4: Detailed results of model merging with code and tool using agents. Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the Task Models. Model Tool Memory Live Non-Live Para P_Mul Para P_Mul HotpotQA 14k 7k RulerQA 32k 64k Avg Base ToolRL (Tool) MemAgent (Memory) 56.25 56.25 37.50 TA Fisher TIES DARE+TA DARE+TIES RAM RAM+ 56.25 56.25 43.75 62.50 62.50 56.25 56. Base and Task Models 41.67 58.33 50.00 62.50 58.33 54.17 58.33 58.33 66.67 62.50 68.00 91.00 78.50 55.00 89.00 48. 60.94 58.59 78.91 50.00 48.44 78.12 64.84 59.38 81.25 58.59 46.95 77.34 56.91 63.49 66.27 Merged Models 91.00 90.00 91.00 89.50 88.50 73.50 90.00 89.00 91.50 90. 90.00 89.50 89.50 90.00 72.66 63.28 71.09 57.03 77.34 76.56 76.56 70.31 57.81 71.09 55.47 71.97 78.12 78. 76.56 65.62 76.56 63.28 75.00 78.12 74.22 74.22 64.84 73.44 59.38 78.12 78.12 79.69 74.19 68.33 69.01 66.87 75.60 76.67 75. Table 5: Detailed results of model merging with memory and tool using agents. Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the Task Models. Model Code Memory LiveBench UT ACC LiveCodeBench ACC UT HotpotQA 14k 7k RulerQA 32k 64k Avg Base CURE (Coding) MemAgent (Memory) 28.35 37.70 39.25 40.87 49.27 50.12 23.43 30.23 28.92 36.42 45.76 44. 60.94 58.59 78.91 50.00 56.25 78.12 64.84 60.94 81.25 58.59 44.22 77.34 45.43 47.87 59.84 Base and Task Models TA Fisher TIES DARE+TA DARE+TIES RAM RAM+ 39.06 38.67 38.87 37.69 39.25 37.89 41.21 52.25 50.53 50.88 47.77 49.95 49.56 53. Merged Models 47.89 32.34 46.58 31.46 32.42 47.80 44.51 31.16 44.52 31.02 31.75 31.16 47.17 46.46 71.88 57.03 74.22 78.91 75.78 78.12 78.91 70.31 50.78 71.09 78.12 75. 78.12 76.56 75.00 58.59 78.91 78.12 76.56 82.03 79.69 75.00 55.47 72.66 80.47 76.56 82.03 82.03 57.97 48.64 58.36 59.59 58. 60.83 61.21 Table 6: Detailed results of model merging with coding and memory agents. Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the Task Models. Model LiveBench LiveCodeBench ACC UT BoN ACC BoN UT ACC UT BoN ACC BoN UT ACC UT MBPP BoN ACC BoN UT ACC CodeContests UT BoN ACC BoN UT Base CURE ToolRL MemAgent 28.35 37.70 31.84 39.25 38.09 TA 36.72 Fisher 39.25 TIES 37.50 DARE+TA DARE+TIES 35.93 RAM RAM+ 38.28 40.23 40.87 49.27 41.36 50.12 51.62 48.73 49.88 48.60 45.66 49.71 52.57 30.07 46.01 32.81 39.84 43.75 45.31 44.53 46.10 46. 42.97 46.10 48.56 58.96 43.78 48.48 52.97 57.79 53.08 56.41 57.88 57.98 57.20 23.43 30.23 26.76 28.92 31.95 30.87 30.63 31.94 29. 31.96 31.60 36.42 45.76 42.05 44.80 46.69 45.89 46.32 46.69 39.53 47.72 46.84 Base and Task Models 62.72 70.36 66.97 67.63 42.26 55.60 46.05 48. 26.67 38.36 30.14 32.16 Merged Models 35.61 35.81 36.59 37.18 38.55 37.45 34.64 46.44 53.26 54.30 53.44 51.76 54.36 51. 69.68 68.67 67.30 69.68 70.70 69.46 70.93 68.50 76.38 72.31 73.99 75.71 74.92 73.35 75.71 76.34 75.56 76.98 70.91 79.64 76.02 71. 74.21 75.57 74.66 78.73 80.64 76.47 77.38 76.84 85.78 81.29 77.84 80.39 83.23 82.34 83.83 86.22 81.89 84.13 18.20 26.05 22.18 21. 25.73 24.26 26.05 23.74 21.13 24.06 22.80 28.35 36.81 32.94 31.68 36.70 35.87 37.00 32.82 27.62 35.07 33.21 23.01 32.21 26.78 24. 28.45 27.20 30.50 28.45 27.20 26.78 23.01 32.06 45.61 37.71 36.09 38.70 39.38 42.05 38.02 35.93 44.51 35.46 Avg 41.08 50.92 44.44 46.06 48.54 48.97 49.24 49.30 48.20 49.64 49.04 Table 7: Additional results of model merging on coding domains. Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the Task Models. Models Base CURE ToolRL MemAgent TA Fisher TIES DARE+TA DARE+TIES RAM RAM+ Multiple Parallel Relevance Simple Parallel_multiple Irrelevance Multiple Irrelevance S_java S_javascript Parallel_multiple Parallel S_python Avg Live Non-live 58.59 59.54 76.83 73.98 76.16 74.55 75.50 75.50 75.31 75.50 75.12 56.25 56.25 56.25 37.50 43.75 43.75 43.75 50.00 56. 56.25 56.25 87.50 81.25 93.75 93.75 93.75 93.75 87.50 93.75 93.75 93.75 93.75 68.99 69.77 79.07 69.77 75.19 74.03 75.19 76.36 76. 75.58 75.58 41.67 37.50 58.33 50.00 45.83 41.67 54.17 62.50 58.33 66.67 66.67 Base and Task Models 68.21 68.33 71.95 56. 77.50 76.50 94.00 82.50 Merged Models 68.55 70.02 63.46 63.57 63.80 67.99 67.87 91.00 91.50 92.50 92.00 92.00 92.00 92. 77.50 77.92 82.92 71.67 79.17 81.25 76.25 74.17 76.67 76.67 76.67 54.00 58.00 62.00 66.00 60.00 58.00 65.00 65.00 65.00 65.00 65. 62.00 60.00 62.00 66.00 62.00 58.00 66.00 66.00 68.00 64.00 66.00 55.00 51.50 89.00 48.50 69.50 63.50 67.50 89.50 90.00 91.50 90. 68.00 64.00 91.00 78.50 87.50 86.50 84.00 92.50 91.50 91.00 90.00 87.25 91.50 93.50 90.25 94.75 94.50 94.50 93.75 93.25 91.75 91. 66.34 65.54 77.74 68.04 72.86 71.62 72.71 76.50 76.94 77.51 77.45 Table 8: Additional results of model merging on tool using. Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the Task Models. Model Base CURE ToolRL MemAgent 8K 65.63 46.80 62.50 83.59 72.66 TA 60.16 Fisher 71.88 TIES DARE+TA 71.88 DARE+TIES 75.78 RAM RAM+ 79.68 77.34 Ruler_SQuAD 32K 16K 64K 128K 7K 14K 28K 56K 112K 224K 448K 896K Ruler_HotpotQA 62.50 61.72 57.81 78.12 70.31 67.97 75.00 77.34 78.13 82.03 82.81 64.84 60.94 59.38 81.25 73.44 58.59 75.00 77.34 76. 74.22 78.91 58.59 44.22 46.95 77.34 72.66 60.94 75.78 70.31 74.22 79.69 82.03 Base and Task Models 56.25 61.72 67.18 81. 72.66 67.97 73.44 80.47 78.91 76.56 79.68 60.94 58.59 58.59 78.91 50.00 56.25 48.44 78.12 Merged Models 68.75 69.53 49.22 60.06 82.03 71.88 76.56 76.56 77.34 75.00 75.78 79. 75.00 78.13 51.56 46.88 51.56 81.25 68.75 49.22 71.97 72.65 75.00 78.91 78.91 48.44 47.66 45.31 77.34 67.19 49.22 68.75 71.88 70. 75.78 75.56 42.19 40.63 42.97 79.69 63.28 39.06 67.97 70.31 70.43 70.31 69.53 36.72 35.93 42.19 72.66 64.84 32.81 70.31 73.44 76. 75.78 78.13 27.34 42.97 35.16 76.56 57.03 35.94 62.50 71.88 74.44 71.09 73.43 25.78 35.16 35.94 72.66 48.44 35.94 64.06 70.31 74. 74.22 74.22 Avg 50.06 49.19 50.31 78.36 66.89 51.32 71.58 73.92 75.15 76.08 77.57 Table 9: Additional results of model merging on memory for long-context tasks. Bold and underlined values denote the best and second-best performance among merged models, respectively. Cells highlighted in red indicate the best performance across all evaluated models, including the Task Models. Coding agent on multiple metrics (highlighted in red), indicating that preserving and selectively amplifying task-unique reinforced updates effectively avoids signal dilution that hampers prior methods. In contrast, methods relying on global averaging or random dropping (TA, Fisher, DARE) exhibit inconsistent gains and fail to simultaneously retain high unit-test robustness and generalization accuracy. Tool-Use Domain. Table 8 presents detailed tooluse evaluations over both Live and Non-Live settings. Across diverse function-calling scenarios, including parallel, parallel-multiple, irrelevance detection, and language-specific tasks, RAM achieves the best average score (77.51), while RAM+ remains close second (77.45), both surpassing all existing merging baselines. Crucially, RAM-based models consistently excel in structurally complex settings such as Live Parallel_multiple and NonLive Parallel, where signal dilution in sparse RL task vectors is most detrimental. These results demonstrate that RAM effectively preserves toolspecific reasoning circuits while still benefiting from shared knowledge introduced by other agents. Long-Context Memory Domain. As reported in Table 9, RAM and RAM+ show clear dominance on long-context memory benchmarks across all context lengths. RAM+ achieves the highest overall average score (77.57), outperforming both merged baselines and, in several settings, the specialized MemAgent itself. In particular, RAM+ consistently delivers state-of-the-art performance on long-context HotpotQA (112K896K) and Ruler-SQuAD (16K64K), confirming that overlap-aware rescaling effectively compensates for performance degradation introduced by averaging shared subspaces. By contrast, Fisher and Task Arithmetic suffer from substantial performance drops at long context lengths, reflecting their inability to preserve sparse, task-specific memory-related updates. Across all three domains, the additional results reinforce three central conclusions. First, reinforced task vectors exhibit strong heterogeneity, making uniform merging strategies fundamentally suboptimal. Second, preserving and rescaling task-unique parameters is essential for maintaining expert-level performance after merging. Third, RAM and RAM+ consistently outperform prior SOTA merging methods not only in average performance but also in robustness across metrics, datasets, and context scales. Together, these findings provide strong empirical evidence that RAM is principled and effective solution for merging multiple RL-trained agents into unified generalist model. C.6 Additional Baseline Results In our experiments, we mainly compare baselines that do not need optimization and datasets, which are the same with RAM. Here we further comprehensively compare RAM with WUDI merging (Cheng et al., 2025), which needs much more complex optimization steps. Table 10 shows that WUDI merging can not out-perform our RAM/RAM+, and also outperformed by the most competitive baseline DARE. This is because WUDI merging minimizes interference under the guidance of task vectors, while still can not avoid signal dilution when WUDI merging meets the sparse task vectors."
        },
        {
            "title": "D Baseline Details",
            "content": "D.1 Baselines with Signal Dilution Here we provide detailed introduction of baselines and explain signal dilution that happens in them. In summary, despite their distinct algorithmic designs, Task Arithmetic, TIES, and DARE all inevitably succumb to Signal Dilution in the RL setting due to shared inability to distinguish between shared consensus and unique specialization. Specifically, Task Arithmetic applies global averaging, collaterally suppressing unique task vectors that require no scaling. Similarly, TIES-Merging fails to strictly isolate disjoint unique parameter updates in RL models, which cannot avoid averaging unique regions. Even DARE, despite introducing rescaling mechanism, only compensates for random dropout rather than selected important update regions, and remains dependent on the global scaling of Task Arithmetic to function. Consequently, all three paradigms effectively drive the magnitude of task-specific updates towards 1 τ , underscoring the necessity of distribution-aware merging method. Fisher Fisher merging (Matena and Raffel, 2022) improves upon uniform averaging by weighing parameters according to their diagonal Fisher information , which approximates the posterior precision (or local curvature) of the model. Formally, the Fiτ merged update is computed as τ merged = . (cid:80) Fi (cid:80) Code Tool Memory Model LiveBench UT ACC LiveCodeBench ACC UT Live Non-Live Para P_Mul Para P_Mul HotpotQA 14k 7k RulerQA 32k 64k WUDI 38.09 DARE+TA 37.50 RAM RAM+ 38.28 40.23 50.95 48.60 49.71 52.57 30.04 31.95 31.96 31.60 44.48 46. 47.72 46.84 56.25 50.00 56.25 56.25 37.50 62.50 66.67 70.83 89.00 92. 91.00 90.50 91.00 89.50 91.50 91.00 78.12 76.56 75.78 79.69 78.91 76. 75.00 78.13 82.03 77.34 74.22 78.91 72.66 70.31 79.69 82.03 Avg 62.42 63.33 64.82 66.55 Table 10: Additional comparison results on WUDI merging. However, this method remains susceptible to signal dilution in the sparse RL setting. Consider unique parameter updated solely by task (where τ = 0 and τ i=t = 0). Crucially, the inactive models (not updated parameters) (i = t) still contribute to the normalization term in the denominator, as their Fisher values Fi representing the confidence of the pre-trained base model are typically nonzero. Consequently, the effective scaling factor for . Since the the unique signal becomes denominator accumulates the inertia (precision) of all inactive models, the task-specific update τ is inevitably scaled down, mirroring the dilution effect observed in uniform averaging as the number of tasks increases. Ft Ft+(cid:80) i=t Fi Task Arithmetic. Task Arithmetic (Ilharco et al., 2023) constructs multi-task model by linearly combining task vectors, typically expressed as τ merged = (cid:80) λτ i. While effective for disjoint tasks in SFT, it faces critical trade-off in the RL setting due to the heterogeneity of parameter updates. To prevent catastrophic magnitude shifts in shared subspaces, the scaling factor λ is usually set to be conservative (typically λ 1/N ) to maintain the merged weights within valid optimization landscape. However, this global scaling creates structural conflict: unique components of task vectors, which reside in non-overlapping subspaces and do not suffer from additive interference, are subjected to the same aggressive downscaling. Consequently, the update for task-specific parameter is reduced to 1 τ t, effectively suppressing the critical, idiosyncratic behaviors required for expertlevel performance in domain. TIES-Merging. TIES-Merging (Yadav et al., 2023) attempts to mitigate interference by keeping only the top-k% magnitude parameters (Trimming) and calculating disjoint mean. However, its reliance on uniform trimming rate creates critical vulnerability when handling the heterogeneous sparsity of RL task vectors. As shown in Figure 2, RL agents exhibit vastly different update densities (e.g., Code: 3%, Memory: 54%). Applying fixed (e.g., 20%) inevitably leads to dilemma: it either over-trims dense vectors (losing info) or, more disastrously, under-trims sparse vectors. For sparse agent like the Code model, standard retains large volume of noise parameters alongside the true signal. These noise parameters, mistakenly treated as valid updates, overlap with the active parameter updates of other tasks, inflating the normalization factor in the disjoint mean calculation. Consequently, the critical, unique signal from one task is averaged with noise from others, resulting in signal dilution. DARE. DARE (Yu et al., 2024) randomly drops parameter updates and enlarges others to reduce redundancy, theoretically approximating the original task vectors expectation. However, DARE fails to address signal dilution for two reasons. First, its factor (1/(1 p)) is designed solely to compensate for the random dropout rate p, not considering the shared/unique condition with other models. When combined with Task Arithmetic (DARE+TA or DARE+TIES), the global merging scale λ must still be kept small (e.g., 1/N ) to stabilize shared parameters, inevitably downscaling the unique, sparsity-preserved updates. Second, DARE assumes that parameter updates are highly redundant (a property of SFT), whereas RL updates are inherently sparse and functionally essential. Randomly dropping parameters in an already sparse RL vector risks severing critical reasoning knowledge, which cannot be recovered simply by rescaling the remaining weights. D.2 Hyperparameters Search Table 11 shows the searched ranges of model merging methods hyperparameters. We search for the best performance for evaluation and comparison. Model Merging Methods Search Ranges of Hyperparameters Task Arithmetic scaling term to merge model parameters: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0] Fisher TIES DARE scaling term to merge model parameters: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0], number of examples to compute Fisher information matrix: [256, 512, 1024, 2048] scaling term to merge model parameters: [0.1, 0.3, 0.5, 0.7, 0.9, 1.0], ratio to retain parameters with largest-magnitude values: [0.1, 0.2, 0.3] search the drop rate in [0.1, 0.2, ... , 0.9] Table 11: Searched ranges of hyperparameters of model merging baselines. Code Tool Memory Model LiveBench UT ACC LiveCodeBench ACC UT Live Non-Live Para P_Mul Para P_Mul HotpotQA 14k 7k RulerQA 32k 64k Avg RAM RAM+ RAM+s 38.28 40.23 39.84 49.71 52.57 51.83 31.96 31.60 30.82 47.72 46.84 46.59 56.25 56.25 56. 66.67 70.83 70.83 91.00 90.50 90.00 91.50 91.00 91.50 75.78 79.69 77.34 75.00 78.13 76.56 74.22 78.91 76. 79.69 82.03 77.34 64.82 66.55 65.46 Table 12: The comparison results of soft-saturation rescaling transformation."
        },
        {
            "title": "E Rescaling Variant",
            "content": "Here we first discuss the isotropic assumption mentioned in Section 4.2. While parameter importance varies in practice, this isotropic assumption provides tractable first-order approximation for deriving the scaling rule, which we empirically find robust in Section 5.4. Then we further discuss another rescaling variant here to have deeper understanding on rescaling operation. To bridge the theoretical requirement with numerical stability mentioned in Section 4.2, besides clipping, we further explore soft-saturation transformation ϕ(x) = 1+x to map the unbounded ratio ρt to the bounded interval [0, 1). Our operational scaling rule is thus defined as: λt = 1 + (cid:18) ρt 1 + ρt (cid:19) . (8) Here, the hyperparameter absorbs the dilution factor (1 β). It is worth noting that this normalized formulation is mathematically equivalent to the ratio of shared parameters to the total active parameters. By substituting the definition of ρt from Eq. 2, we obtain: ρt 1 + ρt = (cid:80) i:ci2 mt,i mt0 . (9) Here, the numerator sums the shared parameters, while the denominator mt0 represents the total count of active parameters (satisfying mt0 = (cid:80) i:ci=1 mt,i). This transformation i:ci2 mt,i + (cid:80) provides dual advantage: it retains the monotonicity derived from the conservation principle (higher overlap yields higher compensation) while enforcing strict upper bound to ensure optimization robustness. Table 12 presents the comparative performance of the soft-saturation rescaling strategy, denoted as RAM+s. Empirically, RAM+s achieves an average score of 65.46, consistently outperforming the non-rescaled baseline RAM (64.82) across all three domains. This reinforces our core hypothesis that compensating for signal dilution in unique parameter subspaces is essential for recovering expert capabilities, regardless of the specific scaling function used. However, RAM+s slightly underperforms compared to the clipped linear variant (RAM+, 66.55). While the softsaturation transformation ϕ(x) = 1+x offers theoretically elegant, strictly bounded mapping, it appears to dampen the scaling factor more aggressively than the linear approach. This conservatism limits performance in tasks requiring robust signal preservation, such as Long-Context Memory, where RAM+s scores 77.34 on RulerQA (64k) compared to RAM+s 82.03. Conversely, in the Tool domain, RAM+s remains highly effective, matching RAM+ with score of 70.83 on Live Parallel-Multiple tasks. These findings suggest that while the soft-saturation rule provides stable alternative, the clipped linear rule offers superior trade-off between signal amplification and numerical stability."
        }
    ],
    "affiliations": [
        "Dartmouth College",
        "Georgia Institute of Technology",
        "University of Notre Dame"
    ]
}