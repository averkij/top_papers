{
    "paper_title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching",
    "authors": [
        "Bowen Wen",
        "Shaurya Dewan",
        "Stan Birchfield"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/"
        },
        {
            "title": "Start",
            "content": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching"
        },
        {
            "title": "NVIDIA",
            "content": "5 2 0 2 1 1 ] . [ 1 0 3 1 1 1 . 2 1 5 2 : r Figure 1. Our Fast-FoundationStereo achieves comparable results to MonSter [7] and FoundationStereo [64] while running nearly 10 times faster. Shown are disparity maps obtained by zero-shot inference on in-the-wild images. (Note that our results occasionally exceed those of [7], e.g., the shiny door in the top row, and the paper towel bin in the bottom row.) Abstract Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly perdomain fine-tuning. To bridge this gap, we present FastFoundationStereo, family of architectures that achieve, for the first time, strong zero-shot generalization at realtime frame rate. We employ divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10 faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing new state-of-the-art among real-time methods. Project page: https:// nvlabs.github.io/ Fast-FoundationStereo/ 1. Introduction The field of stereo matching has advanced significantly since its inception exactly 50 years ago [36]. Modern algorithms, driven by an abundance of high-quality training Figure 2. Zero-shot generalization accuracy (on Middlebury-Q dataset) of various stereo methods versus speed, measured on the same hardware NVIDIA 3090 GPU. Our model family achieves real-time performance with only slight decrease in accuracy compared with the best slow method. Green outlined stars are further accelerated by TensorRT. datasets and innovations in deep neural network architectures, now yield impressive results, often approaching saturation on the most demanding benchmarks. Such accuracy is critical for applications requiring precise 3D reconstruction, such as robotics [29] and augmented reality [28]. This remarkable progress, however, has split the field into two distinct research paths [59]. On the one hand, the rise of foundation models in computer vision has pushed stereo research toward strong zero-shot generalization [2, 7, 63, 64]. Such leading zero-shot networks leverage rich priors from computationally intensive foundation models such as DepthAnythingV2 [74] or DINO models [45, 57]; and they employ computationally intensive architectures, such as the Disparity Transformer [64], to perform self-attention for long-range context. Such limitations have, to date, hindered their deployment in any latency-bound system."
        },
        {
            "title": "To address",
            "content": "On the other hand, the non-negotiable constraints of practical applications demand computationally efficient performance. Architectures designed for such real-time inference [21, 31, 70, 71] achieve high frame rates by relying on lightweight backbones, 2D convolutional layers, and local iterative refinement modules. Such methods struggle to generalize due to their reliance upon per-domain finetuning. The difficulty of obtaining the required dense, highquality ground-truth depth at scale has prevented such efficient methods from being used as an off-the-shelf solution for embodied agents operating in in-the-wild environments. this critical gap, we propose FastFoundationStereo  (Fig. 1)  , novel stereo matching approach for both strong zero-shot generalization and realtime inference. Unlike existing real-time methods which sacrifice the rich architectural capacity and typically designed and trained from scratch, our work builds upon the powerful yet computationally intensive FoundationStereo [64]. Addressing its three main components (feature extraction, cost filtering, and disparity refinement), our divide-and-conquer acceleration strategy takes into account the unique properties of each. First, knowledge distillation is leveraged to compress the computationally expensive hybrid feature backbone into single, efficient student backbone that retains the rich monocular and stereo priors. Second, the intensive cost filtering network is divided into blocks, numerous candidate blocks are trained via distillation, and combinatorial search automatically discovers family of effective architectures under varying latency budgets. Third, structured pruning is applied to the refinement module, guided by recurrent dependency graph to identify and remove redundancy, followed by retraining to recover performance. Finally, training is supplemented with large-scale (1.4M pairs) dataset of in-the-wild stereo images, curated via an automatic pseudo-labeling pipeline. Our contributions can be summarized as follows: We present Fast-FoundationStereo, novel stereo matching architecture that achieves both strong zero-shot generalization and real-time inference, with varying accuracy-speed trade-off  (Fig. 2)  . Our method significantly outperforms other real-time models by large margin across multiple public datasets, and even outperforms several recent strongly generalizable models. We present several novelties to address the computational bottleneck of common components adopted in modern stereo matching models, while inheriting the strengths from the teacher model. Our divide-and-conquer strategy includes: (1) distillation from hybrid monocular and stereo priors, (2) cost filtering via efficient blockwise architecture search, and (3) iterative refinement via structured pruning. To harness the large diversity, internet-scale abundance and unique realism from in-the-wild stereo images, we propose an automatic pseudo-labeling pipeline to supplement synthetic training data for knowledge distillation. Our code, models and pseudo-labels will be released upon acceptance. 2. Related Work Generalizable Stereo Matching. Recent progress in generalizable stereo matching has centered on leveraging Vision Foundation Models (VFMs) and monocular priors to achieve strong zero-shot performance. FoundationStereo [64] establishes strong baseline by adapting DepthAnythingV2 with side-tuning, while StereoAnywhere [2] demonstrates robustness where stereo or mono cues fail independently, and MonSter [7] marries monocular depth with stereo matching to unleash complementary strengths. ZeroStereo [63] synthesizes additional training data based on monocular depth estimation and diffusion models. DEFOM-Stereo [24] builds upon depth foundation models, All-in-One [84] systematically transfers VFMs into stereo frameworks, and recent work diving into the fusion of monocular priors [75] analyzes effective integration strategies. Beyond direct adaptation, domain generalization has been pursued through domain-invariant representations [79], learning from foundation models for domain generalized stereo matching [80], and information-theoretic approaches that avoid shortcut learning [9]. Additional architectural innovations include hierarchical visual transformations [5], masked representation learning for domain generalized stereo matching [49], and harnessing broadspectrum task-oriented features [33]. Despite impressive zero-shot generalization, their computational overhead remains prohibitive for real-time applications. Efficiency-Oriented Stereo Matching. Efficiencyoriented stereo matching architectures have traditionally pursued real-time performance through three primary strategies: compact cost volume representations, lightweight processing modules, and streamlined network The first strategy reduces memory footprint designs. via low-resolution feature pyramids [26], 2D cost signatures [76], attention-based disparity selection [68], or learned parameterized functions that replace explicit volumes entirely [77]. The second strategy accelerates cost aggregation by pruning search spaces in coarse-tofine cascades [19], operating in efficient bilateral grid spaces [67], or employing 3D separable convolutions [47] to avoid expensive 3D kernels. The third strategy designs mobile-specific architectures [55], tile-based iterative refinement [58], or binary operations [3] from the ground up, while more sophisticated approaches employ neural arFigure 3. Overview of our framework. Top: Foundational stereo matching networks (e.g., [64]) consist of three key steps: feature extraction, cost filtering, and disparity refinement. Each step is accelerated by divide-and-conquer strategy. Middle-Left: Hybrid monocular and stereo priors from the teacher foundation model are distilled into single backbone student model. Middle-right: Refinement network is pruned by first constructing dependency graph that models the recurrent nature of the GRU module, followed by structured pruning and retraining to recover the accuracy. Bottom: Cost filtering network is divided into separate local blocks; block candidates are trained to match the teacher blocks output, taking as input the local feature from the previous block; and combinatorial search finds the best performing block combination for given runtime constraint. chitecture search to automatically discover efficient networks [8, 61]. While these methods achieve impressive frame rates, they fundamentally sacrifice the rich architectural capacity. In addition, the models are usually designed and trained from scratch, ignoring recent powerful foundation models. Consequently, they remain tethered to perdomain fine-tuning on target distributions, making them unsuitable solutions for in-the-wild environments. models. Knowledge distillation is also often used independently to transfer knowledge from large teacher model to smaller student [78, 83]. Finally, some methods leverage domain-specific knowledge to accelerate computationally expensive components, such as Fast-VGGT [56] which uses token merging. In comparison, accelerating large foundation models for stereo matching has been largely underexplored, leaving substantial research gap. Vision Foundation Model Acceleration. The significant computational overhead of Vision Foundation Models (VFMs) has spurred large body of research focused on their acceleration for practical deployment. recent active area has been the optimization of SAM [27] and VGGT [60], with several distinct approaches. Many works propose efficient architectures, introducing entirely new lightweight models or modifying existing ones for speed [17, 66, 81]. Another common strategy is quantization, which reduces numerical precision to speed up inference, as demonstrated by PTQ4SAM [34] and QuantizedVGGT [16]. Methods like SlimSAM [6] employ structured pruning, followed by distillation to create highly compact 3. Approach Our approach  (Fig. 3)  is based on FoundationStereo [64], which consists of three key steps: feature extraction, cost filtering, and disparity refinement. Each of these steps is accelerated by divide-and-conquer strategy, as detailed in the following subsections. We also describe our automatic data curation pipeline. 3.1. Distilling Hybrid Monocular and Stereo Priors Given pair of left and right images Il, Ir RHW 3, the feature backbone extracts multi-level pyramid features (i) , {4, 8, 16, 32} for the subl RCi , (i) bining the group-wise correlation and concatenation volumes (where is the maximum disparity). To effectively scale the learning process with the abundant training data, FoundationStereo [64] uses dual branch architecture to perform cost filtering. Specifically, 3D hourglass architecture consisting of Axial-Planar Convolution (APC) layers effectively processes VC by enlarging the kernel size over the disparity dimension without significantly increasing memory consumption. Meanwhile, Disparity Transformer branch tokenizes VC and performs multi-head selfattention to further enhance the long-range context reasoning within the 4D cost volume. Direct pruning of the cost filtering modules yields severe performance degradation for only marginal speedup, since the channel dimension in VC is already small (mostly under 100). We avoided direct knowledge distillation, because it requires manually designing the cost-filtering module alternatives, which remain less explored than feature backbones. Instead, we leverage Neural Architecture Search (NAS) [13] to automatically discover non-intuitive designs. In the following, we describe our efficient blockwise search strategy (Fig. 3 bottom). Blockwise Candidate Construction. The cost filtering module is divided into series of operation blocks: Φt(VC) = BN B2 B1(VC), where represents the total number of blocks. Within the 3D hourglass module, blocks are divided at the transition of the channel dimension, which typically corresponds to the spatial dimension change of the feature volume. We define five types of layers: (1) 3D conv layer with varying channel dimensions; (2) 3D deconv layer that doubles the spatial dimensions of the cost volume, (3) APC layer [64] that performs separate spatial and disparity convolution with different respective kernel sizes; (4) residually connected 3D conv layers, similar to ResNet [22]; and (5) feature guided volume excitation [1]. Meanwhile, the entire Disparity Transformer module is regarded as single block consisting of number of repeated multi-head self-attention transformer layers. We reuse the disparity attention layers as in [64] while varying the feed-forward layer dimensions, number of heads, and number of layers. In both cases, the number of layers in each block and the intermediate channel dimension can vary, as long as (1) the entire blocks running time ts is faster than its teacher counterpart tt and (2) the input and output channel dimension remains the same as the original block. Details of the search space can be found in the appendix. Blockwise Distillation and Evaluation. After blockwise candidate construction, we obtain = C1 C2 CN total number of possible cost filtering module candidates, where Ci denotes the number of candidates in block Bi. In practice, when = 8 and Ci = 200, is 2008 1018. As result, standard evolutionary search based NAS methFigure 4. Top: Distilling the hybrid monocular and stereo priors from FoundationStereo [64] into unified single backbone captures similar high-frequency edges and relative depthwhile significantly reducing computational cost. Bottom: Distillation enhances robustness to translucency, which is challenging to traditional stereo matching. sequent cost volume construction and aggregation. To compute such features, FoundationStereo [64] combines DepthAnything V2 [74] with side-tuning CNN. The former provides rich monocular priors learned from largescale internet data, and the latter adapts the monocular features for the binocular stereo setup. Although such hybrid monocular and stereo feature extraction is powerful, it remains significant computational bottleneck. We leverage knowledge distillation to replace the dual module in FoundationStereos backbone with single student module. This approach was chosen because it is agnostic to architecture and allows to build upon the wellestablished feature backbones studied on ImageNet [10, 65]. As an alternative, we also considered model pruning, but it has two drawbacks: (1) it would require us to keep the dual module, which is constrained by the computational bottleneck of its underlying ViT [12]; and (2) any deterioration in accuracy would be difficult to recover without retraining on internet-scale imagery. During distillation, DepthAnything V2 and side-tuning CNN modules from FoundationStereo are frozen and used to predict multi-level feature pyramid (i), which the student model is trained to match via MSE loss. In the case of channel dimension mismatch, linear projection layer is added. Even though the feature extractors take only single image as input, we include both stereo images in each training batch to retain the statistical similarity. To provide family of stereo models with different speedaccuracy trade-off, we train multiple variants of feature extractors [35, 52]. Fig. 4 visualizes examples of distilled features, showing that they capture similar high-frequency edges and relative depth. 3.2. Cost Filtering Blockwise Search Given the unary features extracted in the previous step, the 4 cost volume VC RC 4 is constructed by com4 ods [11, 51] are not tractable, due to the extremely large computational cost. Moreover, training from scratch in the whole search space does not fully leverage the strengths of the teacher model. To overcome these limitations, we train each block Bi independently. Specifically, Bi is treated as standalone network and trained to mimic the teacher counterparts (cid:13)Bi(fi1) Bi(fi1)(cid:13) output: (cid:13) 2 2, given the feature output (cid:13) fi1 from the previous teacher block. For the final block that predicts the initial disparity, smooth L1 loss is computed against the ground truth. The teacher model is frozen throughout the distillation process. After distillation, candidate block Bc is evaluated by replacing its counterpart at block level in the teacher model and inferring the complete model end-to-end on separate validation dataset. Both the relative error metric change mc and running time change that result by introducing Bc tc are measured. Compared with standard NAS, our blockwise distillation reduces training complexity from O(nN ) to O(n) [30, 43], where is the number of per-layer candidates. Furthermore, since Bi is small, the block distillation can be performed efficiently in terms of both speed and memory, allowing easy parallelization. Combinatorial Search. The student cost filtering module is found by solving for the optimal combination of candidate blocks, which can be formulated as: min (cid:88) (mi)ei, s.t. (cid:88) (ti)ei τ, (1) i= i=1 where mi and ti denote the vector of error metric and running time changes, respectively, for all candidates at block Bi; ei denotes the one-hot vector representing the selection operation of candidate block at Bi; and τ denotes the runtime budget relative to the teacher model for the entire cost filtering module. Optimization is performed by Integer Linear Programming (ILP) [39, 42], using different values for τ to obtain family of cost filtering student models with different speed-accuracy trade-off. 3.3. Refinement Pruning Given the initial disparity map d0 (predicted by the filtered cost volume) and the hidden feature (initialized from the context network), the ConvGRU module progressively refines the disparity map. Fig. 5 shows the dependency graph and data flow. At each iteration, ConvGRU module consumes the disparity dk1, hk1 and predicts their updated values dk, hk, resulting in recurrent dependencies. This significant redundancy in refinement module (as shown in Sec. 4.4), motivates the use of structured pruning [14, 23, 40, 44], simple yet effective technique and can benefit from GPU hardware acceleration techniques such as TensorRT. dedenotes where channel dimension Figure 5. Recurrent dependency graph of the refinement module. notes where pruning is performed. remains fixed during the pruning process. Building Recurrent Dependency Graph. The first step in structured pruning is to identify the inter-dependencies between layers, since depth or channel pruning at one layer changes the intermediate feature dimensions fed to adjacent layers. In addition to the normal adjacent layer dependencies which can be automatically constructed by tracing the computation flow [14], we introduce three more pruning constraints given the unique properties of refinement module in stereo matching: (1) within the ConvGRU module, the final layers that predict the disparity map and convex upsampling mask retain fixed output channel dimensions; (2) within the ConvGRU module, the input channel of the layer that consumes hk1, and the output channel of the layer that outputs hk, are inter-dependent and thus jointly pruned; and (3) the motion encoder that consumes the indexed volume feature retains fixed input channel dimension. Pruning and Retraining. To identify which layers or channels to remove, we evaluate their importance using firstorder Taylor expansion [41]. Specifically, inputs are feed forward to the complete teacher model [64] end-to-end with multiple refinement iterations, and accumulate gradients for the refinement module. The importance of each parameter in the refinement module is ranked globally, and the least important α parameters are pruned, where α (0, 1) is the pruning ratio. We also explored isomorphic pruning strategy [15] but observed slightly degraded performance. After pruning, we retrain the refinement module end-to-end (while freezing the rest of the teacher model) to recover the performance, using the loss: = (cid:88) k=1 γKk (cid:13) (cid:13)dk d(cid:13) (cid:13)1 + λ (cid:88) i=1 xi xi2 2 (2) where xi and xi are the per-layer latent features (student and teacher, respectively) from each of the layers; is the ground truth disparity; is the iteration number; γ = 0.9 exponentially increases weights to supervise the iteratively refined disparity; and λ = 0.1 weighs the distillation objective. The initial disparity supervision is excluded since it is not affected by the refinement module. 3.4. Pseudo-Labeling on In-the-Wild Data Real-world data offers greater diversity and realism than synthetic data. However, obtaining real stereo images with 4. Experiments 4.1. Implementation Details Our Fast-FoundationStereo was trained on the same mixed datasets as FoundationStereo [64], as well as the pseudolabeled real data (Sec. 3.4). For deployment, our framework provides the flexibility to assemble different candidates from each step to compose the final stereo matching model (examples are shown in Fig. 2). The final model is then trained end-to-end. Once trained, the fixed set of weights for each candidate model are used to perform zeroshot inference on unseen data. Unless otherwise mentioned, we use 8 refinement iterations and 192 as the maximum disparity for constructing the cost volume. For evaluation, the disparity range is not constrained. 4.2. Benchmark Datasets and Metric Datasets. Four common public datasets were used for evaluation: Middlebury [53] consists of indoor stereo image pairs with high-quality ground-truth disparity captured via structured light. ETH3D [54] provides grayscale stereo image pairs covering both indoor and outdoor scenarios. and KITTI 2012 [18] and KITTI 2015 [38] feature real-world driving scenes, where sparse ground-truth disparity maps are derived from LIDAR sensors. Booster [48] features large variety of translucent and specular scenes and is used to evaluate the robustness to non-Lambertian surfaces. Metrics. BP-X computes the percentage of pixels where the disparity error is larger than pixels. D1, commonly used on KITTI [18, 38], computes the percentage of pixels whose disparity error is larger than 3 pixels and 5% of the ground-truth disparity. Results are evaluated on nonoccluded regions. 4.3. Zero-Shot Generalization Comparison Quantitative Comparison. Comparison of zero-shot generalization on public datasets is shown in Table 1. These datasets are unseen to all the evaluated methods. For costfiltering based methods that support dynamic maximum disparity configuration, 416 is used on Middlebury-H for the best performance; otherwise their default setting is used for other lower resolution datasets. Existing real-time methods are usually not targeted for zero-shot generalization, and are thus mainly trained on SceneFlow [37]. For those competitive ones with publicly released training code [21, 70], we additionally train them on the exact same datasets as ours (including our proposed pseudo-labels). The inference runtimes for all methods are profiled over Middlebury-Q (similar to typical resolution for real-time robotic applications) on the same hardware with NVIDIA 3090 GPU. As can be observed, our Fast-FoundationStereo outperforms other real-time models by significant margin across the board, even when they are trained on the exact same Figure 6. Top: Pseudo-labeling pipeline on in-the-wild internet stereo data. Bottom: Visualization of our generated pseudo-labels. ground-truth metric depth annotation is notoriously difficult. To address this challenge, we propose an automatic data curation pipeline to generate pseudo-labels on internetscale stereo images. As shown in Fig. 6, given rectified stereo pair from Stereo4D [25], the teacher model [64] produces disparity map for the left image. To identify the imperfect predictions which can mislead the subsequent training process for the student model, we also feed left image to monocular depth estimator [46] to obtain corresponding depth map. Both the disparity map and monocular depth are further converted into normal maps via 3D unprojection and Sobel operator using the same set of camera parameters provided by [25]. To assess local geometric consistency, we compute the per-pixel cosine similarity between the two normal maps, which is thresholded to produce consistency mask. Stereo samples with insufficient agreement are discarded. Due to the uniqueness of sky regions (which have infinite depth and are underrepresented in common synthetic datasets used for training), the similarity computation excludes the sky regions, which are detected by open-vocabulary segmentation models [50, 73]. The remaining stereo disparity maps become the final pseudo-labels, where the sky regions are set to zero disparity. The consistency mask can be optionally used to determine the supervision pixels. We subsample the videos temporally by stride of 10, yielding 1.4M suitable stereo pairs in total. In contrast to directly comparing in the depth or disparity space, our proposed normal consistency check is more robust to extremely diverse depth ranges or noisy predictions on in-the-wild images. This automatically pseudolabeled data is included in our final training of student models. Such output-space distillation complements the featurebased distillation performed in previous steps. Method StereoAnywhere [2] DEFOM-Stereo [24] MonSter [7] Zero-RAFT-Stereo [63] FoundationStereo [64] IINet [31] LightStereo-L [21] LightStereo-L [21] RT-IGEV [70] RT-IGEV [70] BANet-2D [71] BANet-3D [71] Ours Middlebury-H BP-2 BP-1 BP-3 BPMiddlebury-Q BP-2 BP-3 9.67 8.84 9.33 8.48 2.49 25.88 37.49 22.64 16.95 12.75 43.78 44.90 4.80 4.75 3.76 4.24 4.68 1.10 16.69 23.76 12.55 11.52 7.82 28.45 30.10 2. 2.45 2.46 2.69 3.32 0.88 13.03 18.48 9.07 9.40 5.73 22.28 24.17 1.60 8.00 7.51 7.08 8.15 2.64 24.90 30.08 16.34 14.02 11.28 37.33 32.02 4.51 3.25 3.50 3.19 4.42 1.30 14.90 17.75 7.70 7.71 5.59 23.51 18.69 2. 2.10 2.22 1.94 3.26 0.96 10.42 13.11 4.99 5.52 3.77 18.62 13.70 1.57 ETH3D BP-2 BP-3 BP-1 KITTI 2012 BP-3 BP0.61 1.03 0.46 1.17 0.30 12.55 37.21 7.70 2.81 2.78 37.87 20.99 0.62 0.41 0.78 0.28 0.85 0.24 9.19 34.15 4.99 2.26 1.63 35.81 18.55 0.50 11.66 13.10 9.58 9.15 8.16 33.12 42.42 17.59 16.70 11.38 42.92 45.43 8. 4.67 5.32 4.39 4.17 3.50 15.71 22.39 6.71 7.28 5.05 22.45 25.20 3.61 3.52 3.39 2.99 2.93 2.47 9.72 14.49 3.97 4.85 3.44 14.48 17.07 2.50 BP-1 1.43 2.16 0.99 2.14 0. 21.21 45.46 16.34 5.66 5.05 44.89 29.27 1.22 D1 2.81 3.12 2.84 2.76 2.30 9.30 13.98 3.73 4.54 3.25 13.88 16.59 2.35 BP-1 21.81 23.92 20.61 21.13 18. 36.22 40.56 27.66 25.89 22.70 42.92 50.26 19.62 KITTI 2015 BP-3 BP-2 6.72 8.12 6.44 7.43 5.20 14.16 19.10 9.07 9.89 7.32 22.45 26.38 5.78 3.79 4.76 3.59 4.67 2.95 7.86 12.35 4.75 6.19 4.24 14.48 17.13 3. D1 3.52 4.58 3.41 4.48 2.80 7.58 12.08 4.51 6.00 4.00 13.88 16.87 3.25 Runtime (ms) 427 371 336 164 496 30 30 30 45 45 29 26 49 (21) Table 1. Zero-shot generalization on public datasets. Methods are grouped based on their feasibility for real-time application. Denotes methods trained only on SceneFlow [37]; others are trained on large-scale combined datasets, or they leverage foundation models pretrained on large-scale datasets. Bold indicates the best method within each group; note that ours is also second-best in each column. The number in parentheses is the runtime using TensorRT. Figure 7. Qualitative results of real-time methods on Middlebury, ETH3D, Booster and KITTI-2015 datasets (top to bottom). Results are obtained by zero-shot inference without training on any split of the target datasets. Indicates methods trained on the exact same datasets as ours, including our proposed pseudo-labels. More results on in-the-wild images can be found in the appendix."
        },
        {
            "title": "Methods",
            "content": "BP-2 BP-4 BP-6 BP-8 EPE (px) Real-time RAFT-Stereo [32] PSMNet [4] GMStereo [72] PCVNet [77] DLNR [82] Selective-IGEV [62] IGEV [69] NMRF [20] StereoAnywhere [2] FoundationStereo [64] RT-IGEV [70] RT-IGEV [70] Ours 17.84 34.47 32.44 22.63 18.56 18.52 16.90 27.08 9.01 5.18 23.09 18.19 6.61 13.06 24.83 22.52 16.51 14.55 14.24 13.23 19.06 5.40 4.07 16.86 13.39 4.62 10.76 20.46 17.96 13.81 12.61 12.14 11.40 15.43 4.12 2.91 14.10 11.37 3. 9.24 17.77 15.02 12.08 11.22 10.77 10.20 13.21 3.34 2.59 12.47 10.16 3.49 3.59 7.26 5.29 4.70 3.97 4.38 3.94 5.02 1.21 1.13 5.03 4.20 1.54 Table 2. Zero-shot generalization on non-Lambertian surfaces, evaluated on the Booster-Q dataset [48]. Denotes training on the exact same datasets as ours (including our proposed pseudo-labels). datasets, including our proposed pseudo-labels. Moreover, our model achieves comparable or even better results than most of the computationally expensive models, including Zero-RAFT-Stereo [63] which leverages additional synthesized training data via multiple large foundation models. Compared to FoundationStereo [64], our method runs more than 10 times faster with only modest increase in error. Robustness to Non-Lambertian Surfaces. Table 2 shows zero-shot generalization results on Booster-Q dataset [48]. Numbers are from the StereoAnywhere paper [2]; FoundationStereo [64] and the most competitive real-time model RT-IGEV [70] are also included for comparison. Qualitative Comparison. Visualizations of zero-shot inference are demonstrated in Figs. 1 and 7. The stereo images represent diverse challenges including textureless regions, transparency, specular highlights, complex illuminations, varying depth ranges, viewing perspectives and both indoor / outdoor scenarios. Despite these challenges, our model significantly outperforms other real-time models. It even achieves comparable or sometimes more favorable results than computationally expensive generalizable models. 4.4. Framework Analysis Effects of Backbone Distillation. Table 3 shows an ablation study on no distillation (feature backbone weights pretrained only on ImageNet [10]) and different distillation losses. By distilling from hybrid monocular and stereo priors from the teacher model, the feature backbone generally enhances zero-shot generalization. The effectiveness is also demonstrated in Fig. 4, where the translucent glass door challenges the traditional stereo matching process without distillation. Effects of Cost Filtering Blockwise Search. Our blockwise search strategy significantly reduces training complex-"
        },
        {
            "title": "Variants",
            "content": "Midd.-H ETH3D KITTI-12 KITTI-15 D1 BP-1 BP-2 D1 No Distillation Cosine Similarity MSE (Ours) 2.87 2.29 2. 2.11 1.19 1.22 2.67 2.39 2.35 4.32 3.31 3.25 Table 3. Ablations on feature backbone distillation strategies. ity from O(nN ) to O(n). However, it leverages surrogate objective, Eq. (1), which accumulates the impacts of perturbing each local block. This is proxy to the actual performance of candidate model, which would otherwise require training the full assembled cost filtering module with the remaining parts of the network end-to-end for evaluation. In order to verify if such proxy is effective, we compare our searched cost filtering candidate, based on Eq. (1), against randomly assembled candidates under the same latency constraint τ . All cost filtering module candidates are trained end-to-end (with the remaining parts from the teacher model), followed by zero-shot evaluation. For each τ , 10 random candidate models are sampled (note that training each of them end-to-end is expensive). As shown (1) as the latency constraint relaxes (τ inin Fig. 8: creases), our architecture search can successfully find better performing candidate models; (2) under varying τ , the searched candidate consistently outperforms randomly assembled candidates; and (3) as τ decreases, some randomly assembled candidates yield substantial performance degradation, highlighting the importance of network design under tight latency constraint. Figure 9. Effects of pruning ratio for accuracy and speed. We also ablate on few competitive real-time methods with this data. Pseudo-labeling enhances generalization performance consistently for all methods. The elevation is even more significant for those methods [21, 70] that were previously trained only on SceneFlow."
        },
        {
            "title": "Method",
            "content": "Midd.-H BP-2 ETH3D BP-1 KITTI-12 D1 KITTI-15 D1 RT-IGEV [70] LightStereo-L [21] Ours 11.52 (8.69) 23.76 (18.41) 2.53 (2.20) 5.66 (5.12) 45.46 (21.12) 1.31 (1.22) 4.54 (3.55) 13.98 (5.27) 2.44 (2.35) 6.00 (4.40) 12.08 (7.63) 3.48 (3.25) Table 4. Results on in-the-wild data without (and with) pseudo-labeling. Runtime Analysis. Fig. 10 shows the detailed runtime decomposition between FoundationStereo [64] and our slowest model from Fig. 2. Results are profiled on the same hardware (NVIDIA 3090 GPU). Each of the three essential steps are accelerated by large margin, leading to total runtime performance boost of more than 10. Figure 10. Runtime decomposition. 5. Conclusion Figure 8. Effects of blockwise architecture search for cost filtering module under varying latency budget τ , evaluated on Middlebury-Q. Effects of Pruning Ratio. Fig. 9 demonstrates how the pruning ratio affects the prediction accuracy on Middlebury-Q dataset and runtime under one refinement iteration. While aggressive pruning dramatically degrades the prediction accuracy, it can be effectively recovered through retraining with Eq. (2), implying large redundancy in the original refinement module. Effects of Pseudo-Labeling. Table 4 ablates on training with pseudo-labeled data and their zero-shot generalization results on public datasets under commonly used metrics. Fast-FoundationStereo bridges the gap between zero-shot generalization and real-time performance. Through principled divide-and-conquer acceleration strategy, we demonstrate that the computational bottlenecks of foundation stereo models can be systematically addressed without sacrificing robustness. Our extensive evaluations confirm that Fast-FoundationStereo not only establishes new state-ofthe-art among real-time methods by substantial margin, but it also competes favorably with computationally intensive generalizable models. For future work, exploring quantization techniques offers an orthogonal avenue to further enhance inference speed, potentially enabling deployment on even more resource-constrained edge devices."
        },
        {
            "title": "References",
            "content": "[1] Antyanta Bangunharcana, Jae Won Cho, Seokju Lee, In So Kweon, Kyung-Soo Kim, and Soohyun Kim. Correlate-andexcite: Real-time stereo matching via guided cost volume excitation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 35423548. IEEE, 2021. 4 [2] Luca Bartolomei, Fabio Tosi, Matteo Poggi, and Stefano Mattoccia. Stereo anywhere: Robust zero-shot deep stereo matching even where either stereo or mono fail. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10131027, 2025. 1, 2, 7 [3] Jiaxuan Cai, Zhi Qi, Keqi Fu, Xulong Shi, Zan Li, Xuanyu Liu, and Hao Liu. Pbcstereo: compressed stereo network with pure binary convolutional operations. In Proceedings of the Asian Conference on Computer Vision, pages 4378 4394, 2022. 2 [4] Jia-Ren Chang and Yong-Sheng Chen."
        },
        {
            "title": "Pyramid stereo",
            "content": "matching network. 2018. 7 [5] Tianyu Chang, Xun Yang, Tianzhu Zhang, and Meng Wang. Domain generalized stereo matching via hierarchical visual transformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95599568, 2023. 2 [6] Zigeng Chen, Gongfan Fang, Xinyin Ma, and Xinchao Wang. Slimsam: 0.1% data makes segment anything slim. Advances in Neural Information Processing Systems, 37: 3943439461, 2024. 3 [7] Junda Cheng, Longliang Liu, Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Yong Deng, Jinliang Zang, Yurui Chen, Zhipeng Cai, and Xin Yang. Monster: Marry monodepth to stereo unleashes power. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 62736282, 2025. 1, 2, 7 [8] Xuelian Cheng, Yiran Zhong, Mehrtash Harandi, Yuchao Dai, Xiaojun Chang, Hongdong Li, Tom Drummond, and Zongyuan Ge. Hierarchical neural architecture search for deep stereo matching. Advances in neural information processing systems, 33:2215822169, 2020. 3 [9] WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, ITSA: An Alireza Bab-Hadiashar, and David Suter. information-theoretic approach to automatic shortcut avoidance and domain generalization in stereo matching netIn Proceedings of the IEEE/CVF Conference on works. Computer Vision and Pattern Recognition (CVPR), pages 1302213032, 2022. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248255, 2009. 4, 7 [11] Travis Desell. Large scale evolution of convolutional neuIn Proceedings ral networks using volunteer computing. of the Genetic and Evolutionary Computation Conference Companion, pages 127128, 2017. 5 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. 4 [13] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: survey. Journal of Machine Learning Research, 20(55):121, 2019. 4 [14] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1609116101, 2023. 5 [15] Gongfan Fang, Xinyin Ma, Michael Bi Mi, and Xinchao Wang. Isomorphic pruning for vision models. In European Conference on Computer Vision, pages 232250. Springer, 2024. [16] Weilun Feng, Haotong Qin, Mingqiang Wu, Chuanguang Yang, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Yulun Zhang, Michele Magno, et al. Quantized visual geometry grounded transformer. arXiv preprint arXiv:2509.21302, 2025. 3 [17] Jianhai Fu, Yuanjie Yu, Ningchuan Li, Yi Zhang, Qichao Chen, Jianping Xiong, Jun Yin, and Zhiyu Xiang. Lite-sam is actually what you need for segment everything. In European conference on computer vision. Springer, 2024. 3 [18] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3354 3361, 2012. 6 [19] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution In Proceedings of multi-view stereo and stereo matching. the IEEE/CVF conference on computer vision and pattern recognition, pages 24952504, 2020. 2 [20] Tongfan Guan, Chen Wang, and Yun-Hui Liu. Neural markov random field for stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54595469, 2024. 7 [21] Xianda Guo, Chenming Zhang, Youmin Zhang, Wenzhao Zheng, Dujun Nie, Matteo Poggi, and Long Chen. Lightstereo: Channel boost is all you need for efficient 2d cost In 2025 IEEE International Conference on aggregation. Robotics and Automation (ICRA), pages 87388744. IEEE, 2025. 2, 6, 7, 8 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2016. 4 [23] Yang He and Lingao Xiao. Structured pruning for deep convolutional neural networks: survey. IEEE transactions on pattern analysis and machine intelligence, 46(5):29002919, 2023. [24] Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, and Rui Huang. Defom-stereo: Depth foundation model based stereo matching. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2185721867, 2025. 2, 7 [25] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah Snavely, and Aleksander Holynski. Stereo4D: Learning how In CVPR, things move in 3d from internet stereo videos. 2025. 6 [26] Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh Kowdle, Julien Valentin, and Shahram Izadi. Stereonet: Guided hierarchical refinement for real-time edge-aware depth prediction. In Proceedings of the European conference on computer vision (ECCV), pages 573590, 2018. 2 [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 40154026, 2023. 3 [28] Chen Kong, James Fort, Aria Kang, Jonathan Wittmer, Simon Green, Tianwei Shen, Yipu Zhao, Cheng Peng, Gustavo Solaira, Andrew Berkovich, et al. Aria gen 2 pilot dataset. arXiv preprint arXiv:2510.16134, 2025. 1 [29] Taeyeop Lee, Gyuree Kang, Bowen Wen, Youngho Kim, Seunghyeok Back, In So Kweon, David Hyunchul Shim, and Kuk-Jin Yoon. Delta: Demonstration and languageguided novel transparent object manipulation. arXiv preprint arXiv:2510.05662, 2025. 1 [30] Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang, Liang Lin, and Xiaojun Chang. Blockwisely supervised neural architecture search with knowledge In Proceedings of the IEEE/CVF Conference distillation. on Computer Vision and Pattern Recognition, pages 1989 1998, 2020. 5 [31] Ximeng Li, Chen Zhang, Wanjuan Su, and Wenbing Tao. Iinet: Implicit intra-inter information fusion for real-time stereo matching. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 32253233, 2024. 2, [32] Lahav Lipson, Zachary Teed, and Jia Deng. RAFT-Stereo: Multilevel recurrent field transforms for stereo matching. In International Conference on 3D Vision (3DV), pages 218 227, 2021. 7 [33] Biyang Liu, Huimin Yu, and Guodong Qi. GraftNet: Towards domain generalized stereo matching with broadIn Proceedings of the spectrum and task-oriented feature. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1301213021, 2022. 2 [34] Chengtao Lv, Hong Chen, Jinyang Guo, Yifu Ding, and Xianglong Liu. Ptq4sam: Post-training quantization for segment anything. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 15941 15951, 2024. 3 [35] Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman Khan, Syed Waqas Zamir, Rao Muhammad Anwer, and Fahad Shahbaz Khan. EdgeNeXt: Efficiently amalgamated cnn-transformer architecture for mobile vision applications. In Proceedings of the European Conference on Computer Vision (ECCV), pages 320, 2022. 4 [36] D. Marr and T. Poggio. Cooperative computation of stereo disparity. Science, 194:283287, 1976. 1 [37] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 40404048, 2016. 6, 7 [38] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 30613070, 2015. 6 [39] Stuart Mitchell, Paul OSullivan, and Christopher DAndrea. PuLP: Linear Programming Toolkit for Python. Optimization Online, 2011. 5 [40] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. ICLR, 2017. [41] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11264 11272, 2019. 5 [42] Pavlo Molchanov, Jimmy Hall, Hongxu Yin, Jan Kautz, Nicolo Fusi, and Arash Vahdat. Lana: latency aware network acceleration. In European Conference on Computer Vision, pages 137156. Springer, 2022. 5 [43] Bert Moons, Parham Noorzad, Andrii Skliar, Giovanni Mariani, Dushyant Mehta, Chris Lott, and Tijmen Blankevoort. Distilling optimal neural networks: Rapid search in diverse spaces. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222912238, 2021. 5 [44] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. Advances in Neural Information Processing Systems, 37:4107641102, 2024. 5 [45] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 1 [46] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110, 2025. 6 [47] Rafia Rahim, Faranak Shamsafar, and Andreas Zell. Separable convolutions for optimizing 3d stereo networks. In 2021 IEEE International Conference on Image Processing (ICIP), pages 32083212. IEEE, 2021. [48] Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, and Luigi Di Stefano. Booster: benchmark for depth from images of specular and transparent surfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2023. 6, 7 [49] Zhibo Rao, Bangshu Xiong, Mingyi He, Yuchao Dai, Renjie He, Zhelun Shen, and Xing Li. Masked representation learning for domain generalized stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 54355444, 2023. 2 [50] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. SAM 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 6 [51] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, pages 47804789, 2019. 5 [52] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted In Proceedings of the residuals and linear bottlenecks. IEEE conference on computer vision and pattern recognition, pages 45104520, 2018. 4 [53] Daniel Scharstein, Heiko Hirschmüller, York Kitajima, Greg Krathwohl, Nera Nešic, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In Pattern Recognition: 36th German Conference, GCPR 2014, Münster, Germany, September 2-5, 2014, Proceedings 36, pages 3142. Springer, 2014. [54] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highresolution images and multi-camera videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 32603269, 2017. 6 [55] Faranak Shamsafar, Samuel Woerz, Rafia Rahim, and Andreas Zell. Mobilestereonet: Towards lightweight deep netIn Proceedings of the ieee/cvf works for stereo matching. winter conference on applications of computer vision, pages 24172426, 2022. 2 [56] You Shen, Zhipeng Zhang, Yansong Qu, and Liujuan Cao. Fastvggt: Training-free acceleration of visual geometry transformer. arXiv preprint arXiv:2509.02560, 2025. 3 [57] Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 1 [58] Vladimir Tankovich, Christian Hane, Yinda Zhang, Adarsh Kowdle, Sean Fanello, and Sofien Bouaziz. Hitnet: Hierarchical iterative tile refinement network for real-time stereo In Proceedings of the IEEE/CVF conference matching. on computer vision and pattern recognition, pages 14362 14372, 2021. 2 [59] Fabio Tosi, Luca Bartolomei, and Matteo Poggi. survey on deep stereo matching in the twenties. International Journal of Computer Vision (IJCV), 2025. 1 [60] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 3 [61] Qiang Wang, Shaohuai Shi, Kaiyong Zhao, and Xiaowen Chu. Easnet: Searching elastic and accurate network architecture for stereo matching. In European Conference on Computer Vision, pages 437453. Springer, 2022. 3 [62] Xianqi Wang, Gangwei Xu, Hao Jia, and Xin Yang. Selective-Stereo: Adaptive frequency information selection In Proceedings of the IEEE/CVF for stereo matching. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1970119710, 2024. 7 [63] Xianqi Wang, Hao Yang, Gangwei Xu, Junda Cheng, Min Lin, Yong Deng, Jinliang Zang, Yurui Chen, and Xin Yang. Zerostereo: Zero-shot stereo matching from single images. ICCV, 2025. 1, 2, 7 [64] Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, and Stan Birchfield. Foundationstereo: ZeroIn Proceedings of the Computer Vishot stereo matching. sion and Pattern Recognition Conference, pages 52495260, 2025. 1, 2, 3, 4, 5, 6, 7, 8 [65] Ross Wightman. Pytorch image models. https://github.com/ rwightman/pytorch-image-models, 2019. 4 [66] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, et al. Efficientsam: Leveraged masked image pretraining for efficient segment anything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1611116121, 2024. 3 [67] Bin Xu, Yuhua Xu, Xiaoli Yang, Wei Jia, and Yulan Guo. Bilateral grid learning for stereo matching networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1249712506, 2021. 2 [68] Gangwei Xu, Junda Cheng, Peng Guo, and Xin Yang. Attention concatenation volume for accurate and efficient stereo In Proceedings of the IEEE/CVF conference matching. on computer vision and pattern recognition, pages 12981 12990, 2022. [69] Gangwei Xu, Xianqi Wang, Xiaohuan Ding, and Xin Yang. Iterative geometry encoding volume for stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2191921928, 2023. 7 [70] Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Junda Cheng, Chunyuan Liao, and Xin Yang. IGEV++: Iterative multirange geometry encoding volumes for stereo matching. arXiv preprint arXiv:2409.00638, 2024. 2, 6, 7, 8 [71] Gangwei Xu, Jiaxin Liu, Xianqi Wang, Junda Cheng, Yong Deng, Jinliang Zang, Yurui Chen, and Xin Yang. Banet: Bilateral aggregation network for mobile stereo matching. ICCV, 2025. 2, 7 [72] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2023. 7 [73] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 29552966, 2023. 6 [74] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In Proceedings of Neural Information Processing Systems (NeurIPS), 2024. 1, 4 [75] Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, and Yunde Jia. Diving into the fusion of monocular priors for generalized stereo matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1488714897, 2025. [76] Kyle Yee and Ayan Chakrabarti. Fast deep stereo with 2d In Proceedconvolutional processing of cost signatures. ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 183191, 2020. 2 [77] Jiaxi Zeng, Chengtang Yao, Lidong Yu, Yuwei Wu, and Yunde Jia. Parameterized cost volume for stereo matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1834718357, 2023. 2, 7 [78] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment anything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023. 3 [79] Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, and Philip Torr. Domain-invariant stereo In Proceedings of the European Conmatching networks. ference on Computer Vision (ECCV), pages 420439, 2020. 2 [80] Yongjian Zhang, Longguang Wang, Kunhong Li, Yun Wang, and Yulan Guo. Learning representations from foundation models for domain generalized stereo matching. In European Conference on Computer Vision, pages 146162. Springer, 2024. 2 [81] Zhuoyang Zhang, Han Cai, and Song Han. Efficientvit-sam: Accelerated segment anything model without performance loss. In CVPRW, 2024. 3 [82] Haoliang Zhao, Huizhou Zhou, Yongjun Zhang, Jie Chen, Yitong Yang, and Yong Zhao. High-frequency stereo matching network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [83] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai. Edgesam: Prompt-in-the-loop distillation for sam. International Journal of Computer Vision, pages 117, 2025. 3 [84] Jingyi Zhou, Haoyu Zhang, Jiakang Yuan, Peng Ye, Tao Chen, Hao Jiang, Meiya Chen, and Yangyang Zhang. Allin-one: Transferring vision foundation models into stereo In Proceedings of the AAAI Conference on Armatching. tificial Intelligence, pages 1079710805, 2025."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}