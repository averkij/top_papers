{
    "paper_title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation",
    "authors": [
        "Min-Seop Kwak",
        "Junho Kim",
        "Sangdoo Yun",
        "Dongyoon Han",
        "Taekyoung Kim",
        "Seungryong Kim",
        "Jin-Hwa Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 4 2 9 1 1 . 6 0 5 2 : r Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation Min-Seop Kwak1,2 Junho Kim1 Sangdoo Yun1 Dongyoon Han1 Taekyoung Kim Seungryong Kim2 Jin-Hwa Kim1,3 1NAVER AI Lab 2KAIST AI 3SNU AIIS"
        },
        {
            "title": "Abstract",
            "content": "We introduce diffusion-based framework that performs aligned novel view image and geometry generation via warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at https://cvlab-kaist.github.io/MoAI/. Figure 1: Our diffusion-based framework generates novel view image and its geometry at arbitrary target poses, from single or multiple unposed input reference images. With an off-the-shelf geometry module, we predict an partial point cloud whose RGB values and geometry are projected to target viewpoint. Our diffusion models inpaint from these partial observations to generate plausible novel view image and geometry, with cross-Modal Attention Instillation (MoAI) ensuring alignment between modalities, combined with partial geometry resulting complete 3D scene. Co-corresponding author."
        },
        {
            "title": "Introduction",
            "content": "Novel view synthesis (NVS), the task of reconstructing 3D scenes from sparse 2D reference images, represents fundamental challenge that requires neural networks to understand and model the underlying 3D structure of the world from limited 2D observations. Seminal works such as NeRF [20] and 3DGS [14] implicitly model volumetric geometry and appearance by optimizing scene-specific representations to fit individual 3D scenes from reference images. To generalize beyond single-scene optimization, feedforward methods [29, 3, 4, 31] have emerged for direct 3D prediction. In addition, the advent of diffusion models [24] has introduced generative NVS methods [6, 25, 23] that achieve remarkable fidelity in novel view synthesis. However, numerous challenges persist. While feedforward methods [29, 3, 4, 31] demonstrate high fidelity in interpolative settings by accurately reconstructing regions visible in reference images, they lack extrapolation capabilities - the ability to synthesize occluded or unseen regions. On the other hand, generative NVS methods [6, 25, 23], trained to generate novel views from predetermined camera pose embeddings, are capable of extrapolation. However, these approaches are limited in synthesizing novel views at arbitrary out-of-domain viewpoints [27] that are underrepresented in the training pose embedding distribution. Consequently, these methods also require known reference camera poses as input, limiting them to posed image novel view synthesis settings. In this context, we build upon warping-and-inpainting methods [5, 25] to introduce an alternative approach to multi-view novel view synthesis. These methods leverage off-the-shelf geometry prediction models [13, 29, 28] to estimate geometry and camera pose from reference image, project this geometry to given target viewpoint, and use this projection to guide the generative process for plausible target view synthesis. We propose to predict geometry from multiple reference views, aggregating and projecting them to novel viewpoint as coarse geometric conditioning that guides spatial cross-attention mechanisms within diffusion networks. By interpreting NVS as an inpainting problem in this manner, our approach enables synthesis at arbitrary viewpoints from unposed reference images, with reconstruction and generative capabilities at extrapolative viewpoints. Additionally, we extend this method to novel view geometry synthesis by training geometry denoising U-Net that similarly inpaints geometry from reference view geometries. This approach offers significant advantage: whereas previous depth prediction methods struggle with scale-shift discrepancy [2, 13] inconsistency of depth scale and offset between predicted and ground truth geometry our method achieves geometric alignment with reference geometry by generating as continuation of the reference geometry. However, we observe that the image inpainting network struggles to synthesize large regions with no reference projections, while the geometry completion network handles such regions robustlyour analysis demonstrates that this disparity stems from the more deterministic nature of geometry completion compared to image inpainting. To counter this issue, we introduce cross-modal attention instillation, shortened as MoAI, in which spatial attention maps from the image denoising network which implicitly model cross-view correspondences are transferred to replace the attention maps within the geometry network, ensuring cross-modal alignment. This creates synergistic multi-task learning framework: the inherently deterministic geometry completion regularizes image generation, while the geometry network leverages rich semantic features from images for improved synthesis quality. Additionally, our proximity-based mesh conditioning incorporates additional geometric cues (depth and normal) into correspondence conditions, facilitating sparse geometry interpolation and filtering of erroneous warped conditions. Our method demonstrates powerful extrapolation capabilities for both novel view image and geometry synthesis, resulting in aligned colored point clouds that achieve 3D scene completion  (Fig. 1)  . Our results show it achieves state-of-the-art performance in extrapolative camera settings while maintaining competitive reconstruction performance and zero-shot generalization to unseen data."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Non-generative few-shot NVS methods Optimization methods. Neural 3D representations such as NeRF [20] and 3DGS [14] require numerous calibrated views to optimize the neural radiance field effectively. Optimization-based 2 few-shot methods [10, 21, 15, 16] alleviate this issue by tailoring single 3D scene from sparse views. For instance, DietNeRF [10] enforces semantic consistency between rendered images from novel viewpoints and available reference images, while RegNeRF [21] regularizes the geometry and appearance of patches from unobserved viewpoints. However, these approaches suffer from limited generalization ability beyond individual scenes and the computational burden of per-scene optimization. Feedforward methods. Feedforward NVS approaches [32, 4, 29, 31, 8] address few-shot novel view synthesis without per-scene optimization. PixelNeRF [32] is among the first to condition NeRF on image inputs using local CNN features, predicting novel view image in feedforward manner. MVSplat [4] improves on this by predicting 3D Gaussians from sparse multi-view images with cost volume for depth estimation, yielding high-quality 3D representations. Subsequent works [29, 17, 8, 31] tackle the pose-free scenario, where networks predict novel views from unposed images; for example, DUSt3R [29] and MASt3R [17] leverage transformers to output the pointmaps and estimate camera poses, and Noposplat [31] jointly predicts 3D representations and poses from sparse inputs. However, these methods generally lack extrapolative capabilities, unable to synthesize novel geometry or appearance in regions unseen or occluded from the reference images. 2.2 Few-shot NVS with generative models Recent diffusion-based NVS approaches [18, 27, 26, 6] leverage their generative capacity to synthesize novel views from one or few images. Zero-123 [18] fine-tunes diffusion model to directly generate novel viewpoint image for relative pose from single image. Similarly, MVDream [26] and CAT3D [6] employ spatial cross-attention between the generating viewpoints to achieve consistent novel view synthesis at target viewpoints. Although these diffusion-based approaches offer strong extrapolative capability, they do not provide explicit geometry for the novel viewpoints, necessitating separate optimization process on NeRF or 3DGS for full geometry. Moreover, because they receive target view camera pose as feature embedding, the range of poses they can generate is limited to the training domain, hindering the direct generation of arbitrary novel poses. 2.3 Warping-and-inpainting methods Diffusion models [24] have demonstrated strong inpainting capabilities [19], which has motivated their application to novel view synthesis (NVS). LucidDreamer [5] leverages off-the-shelf monocular depth estimators [2, 29, 28] to extract geometry from single image, warp it to target viewpoint, and inpaint missing regions, while GenWarp [25] uses predicted geometry as correspondence signal for implicit inpainting. However, because these methods rely primarily on 2D image inpainting without comprehensive understanding of 3D structure, they struggle to synthesize scenes with large view differences, particularly in object-centric scenarios where novel view results in warped geometry covering only small portion of the target viewpoints."
        },
        {
            "title": "3 Method",
            "content": "Given unposed and sparse reference RGB images {In RHW 3}N n=1, with height and weight , the methods objective is the joint prediction of novel view image It and pointmap Xt for target viewpoint πt, leveraging the diffusion models generative capabilities for high-fidelity novel view and geometry synthesis. Our method extends warping-and-inpainting methodology [17, 25] from single-image to multi-view settings. We then also apply same approaches to the geometry domain, performing geometry completion at the target viewpoint from partial geometry predicted by off-the-shelf models (Sec. 3.1). To ensure alignment between target image and geometry at target view, we introduce cross-modal attention distillation (Sec. 3.2), multitask learning that yields synergistic benefits for both modalities. Finally, to handle noise and artifacts in predicted point clouds, we introduce proximity-based mesh conditioning (Sec. 3.3), which prevents erroneous artifacts from degrading generation quality. 3.1 Novel view image generation Our image generation architecture, as shown in the upper section of Fig. 2, consists of two U-Nets: an image reference network and an image denoising network. The image reference network [9, 25] 3 Figure 2: Training methodology. Our method conducts cross-modal attention instillation, replacing the spatial attention maps of geometry denoising networks with those of image denoising networks, so that the image generation U-Net learns more robust representation aligned with the geometry completion task. On the other hand, the geometry prediction networks leverage the rich semantics from image features to enhance geometry completion capability. extracts semantic features from the reference images {In RHW 3}N network utilizes the features from the reference network to generate novel-view image. n=1, and the image denoising Geometry prediction and pointmap projection. We first leverage an off-the-shelf geometry prediction model [29, 28] to obtain the set of corresponding camera poses φ = {πn R44}N n=1 as well as pointmap {Xn RHW 3}N n=1 from reference images. The pointmap Xn [29] is 2D grid of 3D point coordinates, where each element represents the predicted world coordinate for the given pixel. Next, the pointmaps of the reference images, X1, X2 . . . XN , are merged into single point cloud X. We then project onto the target viewpoint πt: Π = Π(X, πt), = (cid:91) n= Xn, (1) resulting in the projected pointmap Π for the target view πt, where Π() denotes projection function. When multiple points project onto single pixel, only the point closest to the target image plane is rendered, as in the standard point cloud rasterization procedure [25]. Pointmap correspondence conditioning. We leverage the projected pointmap Π and the reference view pointmaps {Xn}N n=1 as sparse geometric correspondence condition, which enable the image denoising network to implicitly establish the correspondences between the target viewpoint and reference images. Specifically, we first encode the projected pointmap Π using positional embedding E() and concatenate the resulting Fourier feature E(X Π ) with binary mask Mt, which marks grid pixels with no projected 3D points as 0 and grid pixels with projected 3D points as 1 [25]. This forms the target correspondence condition ct for image denoising network at target viewpoint πt. Similarly, for each reference viewpoint πn, we obtain reference correspondence condition cr which consists of an embedded reference view pointmap Fourier feature E(Xn), concatenated with one-valued tensor mask 1, since every grid pixel in the reference view pointmap has corresponding 3D point due to dense prediction by the off-the-shelf model and therefore marked as 1. As we obtain reference correspondence condition for every reference image {In}N n=1: ct = [E(X Π ), Mt], cr = [E(Xn), 1], cr = {cr n}N n=1. (2) Similarly to Hu et al. [9], these correspondence conditions are first passed through convolutional network, resulting in target and reference correspondence condition features. The target correspondence condition feature is added to the target image latent feature from the first convolutional layer 4 of the image denoising network, while each reference correspondence condition feature is similarly added to the features of its corresponding reference image within the image reference network. Such conditioning guides the image denoising network in identifying relevant spatial correspondences from multiple reference images to ensure consistency in novel view generation. Notice that instead of providing explicit pixel-to-pixel correlation (e.g., warped pixel coordinates [25]) between reference viewpoints and target viewpoint, we directly provide embedded pointmap as condition. This design choice allows the model to flexibly associate each spatial location in the target image with multiple potential correspondences with the reference images for robust reconstruction. Aggregated attention. We conduct an aggregated attention between the target features derived from the image denoising network and the reference features produced by the image reference network. We acquire the key features Kt R1C(W H) and value features Vt R1C(W H) from the spatial self-attention layers of the image denoising network. The key and value features are concatenated along the viewpoint dimension with features from reference viewpoints before an aggregated attention [9, 25] with the target view feature Qt as query: = Qt, = [Kt, K1, K2, . . . KN ], = [Vt, V1, V2, . . . VN ], where K, R(N +1)C(W H). The spatial attention is computed as follows: Attention(Q, K, ) = Softmax (cid:18) QK dk (cid:19) V, (3) (4) (5) (6) where dk is the dimensionality of the key features. This design enables the image denoising network to simultaneously perform cross-attention across all reference images and self-attention within the target latents, ensuring unified novel view synthesis. 3.2 Aligned novel view geometry generation We perform novel view geometry prediction alongside image synthesis using an identical architecture to the image generation architecture: geometry denoising network (U-Net) that predicts target view pointmap, paired with its geometry reference network that receives reference view pointmaps {Xn}N n=1 as input. Similarly to Ke et al. [13], our geometry denoising network and geometry reference network are fine-tuned from an image denoising U-Net to predict pointmap modalities instead of images, detailed in Sec. of the appendix. As described in lower half of Fig. 2, the geometry generation architecture predicts pointmap for the target viewpoint πt, identical to the image denoising network, and therefore receives the same target and reference correspondence conditions, ct and cr, as the image generation architecture. This encourages the pointmap generated by the geometry generation network to align with the generated image. However, we find this naïve approach of giving identical correspondence condition to be insufficient in achieving alignment between the generated image and its geometry. Instead, the two modalities exhibit different behaviors when completing same regions, with geometry denoising significantly outperforming image inpainting, as demonstrated in Fig. 3. This disparity can be attributed to geometry completion being inherently more deterministic due to the coarse nature of the modality, making it substantially easier to learn than image inpainting. As shown in Fig. 3(a-b), when completing partially visible wheel, the image denoising model, shown in (a), fails to establish such correspondences, the geometry prediction model (b) correctly attends to other wheel locations on the object for structural reference, generating higher-accuracy completion result. Figure 3: Per-modality attention visualization. 5 In this manner, in extrapolative scenarios where large portions of the target view are unobserved, the image generation pipeline encounters significant challenges due to the severe ambiguity from partial observations. On the other hand, while geometry completion is more deterministic due to the coarse nature of its representations, this coarseness also restricts the available information for accurate completion, limiting its accuracy and performance overall. Cross-modal attention instillation. In this light, we propose cross-modal attention instillation, where the spatial attention maps for the geometry prediction U-Net is substituted by the attention maps from image denoising U-Net to achieve synergistic effects. Specifically, the key and query features extracted from the image denoising U-Net are leveraged by the spatial attention layers of the geometry denoising U-Net. Let KI and QI denote the key and query features from the image U-Net, respectively, and let VX represent the value features from the geometry U-Net that generates pointmap Xt. The spatial attention is as follows: Attention(QI , KI , VX ) = softmax (cid:19) (cid:18) QI I dk VX , (7) where dk is the dimensionality of the key features. This method offers several key advantages. As shown in Fig. 3, injecting the attention map across the modalities reinforces the alignment between generated images and their geometries. The image denoising U-Net receives deterministic training signals from the geometry completion network, which regularizes its generation process, yielding enhanced consistency and inpainting capability, as demonstrated in Fig. 3 (c). The geometry prediction U-Net also leverages rich semantic features from the image domain to achieve more accurate geometry completion. Additionally, as the attention maps solely serve as structural information for aggregating value features, our architecture prevents the harmful mixing of features from different modalities that is often observed in cross-modal architectures [7]. 3.3 Proximity-based mesh conditioning The off-the-shelf geometry models [29, 13] typically generate sparse 3D point cloud with noise and errors, which becomes particularly severe when the target viewpoint deviates significantly from the reference viewpoints. Erroneous projections from the sparse point cloud causes misalignment in the generation process, as the networks cannot differentiate valids from erroneous signals, this compromises the consistency and accuracy of generated images and their geometry. To address these challenges, we propose proximity-based mesh conditioning. We begin by converting the sparse point cloud into mesh representation using ball-pivoting algorithm [1], which reduces erroneous projections and yields denser, interpolated conditions for the generation networks. Therefore, instead of employing the naïve projected pointmap Π as the correspondence condition, we utilize the pointmap derived from the projected mesh, denoted as Π , as our correspondence condition. Furthermore, we augment the correspondence condition with the meshs depth and normal map, enabling the network to prioritize reliable correspondences while filtering out noise and erroneous projections. Specifically, we channel-wisely concatenate the partial depth map DΠ and normal map Π acquired from our converted mesh to the correspondence condition embedding. Accordingly, our final correspondence conditions are expressed as follows: ), DΠ ct = [E(P Π cr = [E(Pn), Dn, Nn, 1]. , Π , Mt], (8) We further refine the conditioning process by applying normal masking to exclude mesh planes whose normals deviate more than 90 from the target viewpoints direction. These planes typically correspond to surfaces that have been erroneously projected due to the incomplete nature of the acquired geometry, and therefore should be excluded from the correspondence condition. By masking out these areas, we further ensure that the network is not influenced by erroneous or noisy correspondences."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Implementation details For image denoising networks, we initialize from Stable Diffusion 2.1 [24]. Reference networks share identical architecture but omit timestep embeddings, serving only for feature extraction. We train on RealEstate10K [34], Co3D [22], and MVImgNet [33] using pseudo ground-truth geometry from VGGT [28]. During training, only reference pointmaps are used for warping and proximity-based mesh conditioning. At inference, VGGT predicts camera poses and pointmaps for projection to target viewpoints. For geometry denoising networks, we initialize from Marigold [13]specifically the normal prediction head due to its three-channel similarity to pointmaps. We repurpose Marigold to generate complete geometry from incomplete warped RGB conditioning rather than target images. Extrapolative view setting. We define extrapolative camera settings as when the target viewpoint πt lies outside the convex hull of reference camera positions {πn}N n=1 in the camera pose manifold. Formally, if we represent camera poses as points in SE(3), an extrapolative viewpoint cannot be expressed as convex combination πt = (cid:80)N n=1 αn = 1. This geometric constraint means that significant portions of the target view contain regions that are either completely occluded in all reference views or lie beyond the observable scene boundaries. Unlike interpolative settings where novel views can be reconstructed through geometric triangulation and feature matching between overlapping reference regions, extrapolative synthesis requires hallucinating plausible scene content based on learned priors about scene structure and appearance, making it fundamentally generative rather than reconstructive problem. n=1 αnπn where αn 0 and (cid:80)N Figure 4: Qualitative results. We demonstrate our qualitative results on the Co3D [22] dataset, conducting feed-forward novel view synthesis while generating aligned geometry in robust and consistent manner. 4.2 Experimental results Results on Co3D. Figure 4 presents qualitative demonstration of our approach on the Co3D dataset, where our model reconstructs novel view image with its aligned geometry from three reference images (i.e., 3-view setting). The generated images are consistent with the references, and their geometry is accurately aligned with the synthesized views. Visualizations of the pointmap from multiple viewpoints reveal that the generated geometry aligns well without requiring additional scaleand-shift fitting, due to our formulation of the depth prediction task as depth completion problem combined with image inpainting. Note that the novel view images and their geometries are directly 7 generated from our denoising network at target viewpoints, requiring no additional optimization of NeRF or 3DGS to acquire geometry or synthesize unconstrained target view synthesis, overcoming the limitations of previous diffusion-based novel view synthesis models [6, 30, 26]. Views Method Pose-free 2-view PixelSplat [3] MVSplat [4] NoPoSplat [31] Ours (2-view) 1-view LucidDreamer [5] GenWarp [25] Ours (1-view) Extrapolative View Interpolative View PSNR SSIM LPIPS PSNR SSIM LPIPS 14.66 12.22 13. 15.58 11.14 9.85 15.56 0.517 0.416 0.393 0.615 0.423 0. 0.609 0.334 0.423 0.545 0.184 0.440 0.527 0.184 12.75 13.94 14. 16.58 12.09 9.54 14.58 0.329 0.473 0.414 0.643 0.481 0. 0.529 0.637 0.385 0.530 0.152 0.419 0.538 0.202 Table 1: Zero-shot quantitative comparison. We compare our model to existing feedforward NVS methods (2-view setting) and warping-and-inpainting methods (1-view setting) on DTU [34] dataset, which is zero-shot setting for all the models. Our method showing superior performance in both extrapolative and original (interpolative) setting in both single-view and stereo-view settings. Figure 5: Qualitative comparison with inpainting method on DTU [34] dataset. Our qualitative comparison with the naive warping-and-inpainting method demonstrates our models zero-shot generalization capabilities to unseen data, as well as its ability to robustly handle erroneous warped geometries for geometrically consistent generation. Results on DTU. In Table 1 and Fig.5, we compare against feedforward methods (PixelSplat[3], MVSplat [4], DUSt3R [29], NopoSplat [31]) using two reference views, and warping-and-inpainting methods using single reference images (LucidDreamer [5], GenWarp [25]). We evaluate on DTU dataset [11], which was not used in training in all the models, demonstrating zero-shot generalization. For fair comparison, all warping-and-inpainting methods use the same geometry prediction model [28], VGGT. Since our model excels in extrapolative settings, we introduce an extrapolative view selection that samples the furthest target cameras in the dataset. Our method achieves state-ofthe-art performance in both extrapolative and interpolative settings regardless of viewpoint count. Qualitative results in Fig. 5 show our model effectively filters artifacts and noise from predicted point clouds during warping, producing clean, complete images with well-aligned geometry. In contrast, naive inpainting of warped images yields numerous artifacts and inconsistent features, demonstrating our models robust handling of erroneous warped conditions. Results on RealEstate10K. In Table 2, we compare our method against feedforward few-shot novel view synthesis approachesPixelSplat [3], MVSplat [4], DUSt3R [29], and NopoSplat [31]on RealEstate10K [34]. Similarly, we evaluate both interpolative (target near reference views) and extrapolative (target in unobserved locations) conditions using two reference views. For extrapolation, we sample reference images from the latter third of video frames and targets from the first third, exploiting the forward camera progression in source videos. Our model outperforms feedforward methods in extrapolative settings while maintaining competitive interpolative performance. As shown 8 Method Pose-free PixelSplat [3] MVSplat [4] NoPoSplat [31] Ours Extrapolative View Interpolative View PSNR SSIM LPIPS PSNR SSIM LPIPS 14.01 12.13 14.36 17. 0.582 0.534 0.538 0.614 0.384 0.380 0.389 0.229 23.85 23.98 25.03 24. 0.806 0.811 0.838 0.820 0.185 0.176 0.160 0.088 Table 2: In-domain comparison. We compare our model to existing feedforward NVS methods in Realestate10K [34] dataset, our method showing superior performance in extrapolative setting. Figure 6: Qualitative comparison on extrapolative setting. Our qualitative comparison of previous approaches demonstrates our models extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions. in Figure 6, conventional feedforward models struggle with large missing areas due to limited generative capabilities, whereas our approach effectively infers missing geometry and generates plausible imagery for unseen regions while preserving observed region fidelity. Notably, we can realistically inpaint partially visible objects (e.g., sofas, railings) with well-aligned geometry synthesis. We attribute this to geometric awareness from attention distillation, which regularizes structural attention maps and enhances generation. Comparison against LVSM [12]. We compare our method against LVSM [12], leading feedforward approach for novel view synthesis. As shown in Fig. 7, our method demonstrates superior performance in extrapolative scenarios where significant portions of the target view are unobserved in the reference images. While LVSM accurately reconstructs regions with sufficient geometric overlap from reference views, it fails to extrapolate beyond the observable scene boundaries, producing blurry and inconsistent content in occluded areas. In contrast, our generative approach leverages learned scene priors to synthesize plausible content for these missing regions, maintaining sharp detail and geometric consistency throughout the entire target view. This fundamental difference highlights the limitation of purely reconstructive methods in extrapolative settings, Figure 7: Comparison with LVSM. 9 where the lack of reference information necessitates generative capabilities rather than geometric interpolation. 4.3 Ablation PSNR Components Component analysis. We conduct quantitative ablation studies on our method using the RealEstate10K dataset under the extrapolative setting. Our results in Table 3 demonstrate the contribution of each component. The baseline (a) receives no geometric conditioning, while (b) introduces naive pointmap conditioning. When we add proximity-based mesh conditioning (c) and cross-modal attention distillation (d), we observe progressive performance improvements. Table 3: Ablation study. We demonstrate how each of our components contributes to enhanced performance in novel view synthesis. (a) Baseline (b) (a) + Pointmap condition (c) (b) + Proximity-based mesh (d) (c) + Cross-modal instillation 0.260 0.243 0.238 0. 0.559 0.594 0.601 0.614 16.55 16.93 17.01 17.41 SSIM LPIPS Image Geometry Geometry (Recon) Geometry (Inpainting) Method 2-view 3-view 4-view PSNR SSIM LPIPS Abs.Rel δ1.25 Abs.Rel δ1.25 Abs.Rel δ1.25 17.41 20.02 20.08 0.615 0.700 0.701 0.230 0.146 0.144 0.196 0.143 0.140 0.715 0.788 0. 0.152 0.151 0.113 0.819 0.849 0.846 0.308 0.304 0.311 0.531 0.598 0.594 Table 4: Quantitative analysis regarding number of input viewpoints. We demonstrate improved performance with additional viewpoints at inference for both image and geometry generation, despite training only on two-view settings. Analysis on the number of input viewpoints. Our model conducts aggregated attention to generate novel views from reference images, it can receive an arbitrary number of input viewpoints for generation, as an additional reference image corresponds to simply concatenating additional reference viewpoints features within our attention architecture. To demonstrate this, in Figure 8 and Table 5, we increase the number of reference viewpoints for model trained at 2-viewpoint setting and analyze its effects in both image quality and geometric accuracy: the results demonstrate even without being trained on the given number of inputs, our model benefits strongly from additional viewpoints, showing the generalization capability of our aggregated attention architecture to various number of input reference viewpoints. Figure 8: Analysis on number of reference viewpoints. Our models multi-view aggregated attention enables our model to generalize towarding receiving arbitrary number of reference images."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose novel few-shot novel-view synthesis method that overcomes the limitations of existing approaches by jointly predicting images and geometry. By integrating cross-modal attention sharing and geometry-aware correspondence conditioning into warping-and-inpainting framework, our method leverages deterministic cues from geometry completion to regularize the generation process, producing consistent, high-quality novel views even in challenging extrapolative scenarios."
        },
        {
            "title": "A Additional details",
            "content": "A.1 Training details In our training procedure, we initialize the image denoising U-Net from Stable Diffusion 2.1 and fine-tune on RealEstate10K [34], Co3D [22], and MVImgNet [33]. Reference networks share identical architecture and initial weights with the denoising U-Net but exclude timestep embeddings, serving solely for semantic feature extraction. Our model is trained using batch size of 1 with gradient accumulation, employing mixed precision training (bf16) to optimize memory usage and computational efficiency. We utilize 8-bit Adam optimizer with standard hyperparameters: β1 = 0.9, β2 = 0.999, weight decay of 1102, and ϵ = 1108. The learning rate is set to 110 with constant scheduler and minimal warmup of 1 step. For multi-view training, we sample 4 viewpoints total with 3 reference views and 1 target view (index 1), maintaining temporal margin of 30 frames between reference and target images to ensure sufficient viewpoint diversity. All training images are resized to 512512 resolution. We employ xFormers memory-efficient attention mechanisms to handle the computational demands of multi-view aggregated attention. The noise scheduler follows scaled linear beta schedule with 1000 training timesteps, βstart = 0.00085, βend = 0.012, and steps offset of 1, without sample clipping. Validation is performed every 2000 training steps to monitor convergence and prevent overfitting. A.2 Implementation details Geometry for correspondence conditioning is generated using VGGT [28], with only reference view pointmaps used for warping and proximity-based mesh conditioning. Ground truth geometry for MSE loss supervision is also predicted by VGGT using target images, enabling the model to maintain scaleand-shift alignment with reference geometry automatically. This approach allows exact reconstruction of known geometry while completing unknown regions in alignment with reference observations. To accelerate and stabilize training, we separately fine-tune the pointmap denoising U-Net and its reference network, initializing from Marigold [13]s normal prediction weights with identical multiview aggregated attention as the image U-Net. This separate initialization enables both branches to learn robust independent representations before cross-modal attention distillation, where geometry networks benefit from deterministic image cues, significantly improving prediction consistency. However, as discussed in our main paper, geometry-only training cannot achieve perfectly aligned predictions, necessitating cross-modal attention distillation for optimal performance. Camera-space pointmap normalization. key insight in our approach involves normalizing coordinate pointmaps through camera-centric transformation that substantially improves model performance. Specifically, we apply the target viewpoints world-to-camera matrix to convert all predicted pointmap coordinates vlaues into the target cameras local coordinate system. This preprocessing strategy addresses critical issue in multi-view geometric learning: when coordinate pointmaps retain their original world-space values, the model must simultaneously learn to handle dramatic variations in absolute coordinates while capturing subtle geometric relationships. Such dual complexity often hampers training convergence and degrades synthesis quality. Our camera-space transformation eliminates this burden by establishing unified coordinate frame centered on the target view. Within this normalized space, all geometric information is expressed relative to the target cameras position and orientation, creating more conducive learning environment. The model can then dedicate its representational capacity entirely to understanding the geometric correspondence patterns between reference and target configurations, without being distracted by irrelevant absolute positioning. This coordinate system alignment also ensures numerical consistency across training samples, preventing gradient instabilities that can arise from extreme coordinate ranges. The resulting geometric conditioning leads to more stable training dynamics and enhanced ability to synthesize geometrically plausible novel views across diverse camera poses and scene configurations. To validate the effectiveness of our pointmap normalization strategy, we conduct qualitative ablation study comparing synthesis results with and without camera-space coordinate transformation. Our 11 Figure 9: Ablation on pointmap normalization. Ablation study comparing synthesis results with and without camera-space pointmap normalization. Normalization significantly improves geometric consistency, boundary sharpness, and geometric alignment with projected geometry. experimental results in Fig. 9 demonstrates that without normalization, the model struggles to maintain geometric consistency across different viewpoints, producing artifacts such as distorted object boundaries, inconsistent depth relationships, and misaligned features between generated images and their corresponding geometry."
        },
        {
            "title": "B Additional results",
            "content": "B.1 Additional scene completion results Our proposed method demonstrates remarkable flexibility and scalability across varying numbers of input viewpoints in Fig. 10, showcasing its robust generalization capabilities beyond the twoview training configuration. In single-view novel view synthesis experiments conducted on the RealEstate10K dataset, our model successfully generates high-quality novel views from single reference image, effectively leveraging the geometric priors learned during training to infer plausible scene structure and appearance for unseen viewpoints. This single-view capability is particularly challenging as it requires the model to hallucinate significant portions of the target view while maintaining geometric consistency with the limited reference information. For two-view novel view synthesis, as demonstrated in Fig. 11, we conduct comprehensive evaluations across Co3D [22] datasets, demonstrating consistent performance improvements when additional reference information becomes available. The model effectively aggregates information from both reference views through our multi-view attention mechanism, resulting in more accurate geometry estimation and higher-fidelity image synthesis. Our architectures ability to seamlessly handle twoview inputs during inference, despite being trained on this configuration, validates the effectiveness of our correspondence conditioning and attention aggregation strategies. B.2 Qualitative comparison with other generative models Warping-and-inpainting models. In Fig. 12, conduct qualitative evaluation against warping-andinpainting methods using single reference images (LucidDreamer [5], GenWarp [25]) on the DTU dataset [11], which was excluded from training for all methods, demonstrating zero-shot generalization capabilities. For fair comparison, all warping-and-inpainting methods utilize VGGT [28] as the shared geometry prediction model. Since our model excels in extrapolative scenarios, we focus our qualitative analysis on challenging extrapolative view selections that sample the most distant target cameras in the dataset, where substantial viewpoint changes test the limits of each methods synthesis capabilities. Visual results in Fig. 12 demonstrate our models superior ability to effectively filter artifacts and noise from predicted point clouds during the warping process, producing clean, geometrically consistent images with well-preserved structural details. In stark contrast, baseline warping-and-inpainting methods produce numerous visual artifacts, blurred regions, and feature inconsistencies, particularly in occluded areas requiring substantial inpainting. These qualitative comparisons clearly highlight our approachs robust handling of noisy geometric conditions and its capacity to maintain both photometric quality and geometric coherence across challenging viewpoint transformations. Figure 10: Qualitative results on single-view extrapolative setting. Our qualitative comparison of previous approaches demonstrates our models extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions. B.3 Qualitative ablation We conduct qualitative ablation experiments across five Co3D scenes in Fig. 13, evaluating four progressive configurations to demonstrate each components contribution. The totally naive baseline without geometric conditioning struggles with spatial coherence and geometric consistency, producing misaligned features and implausible geometry. Adding pointmap conditioning improves geometric awareness and depth relationships but remains insufficient for complex occlusions and fine-grained details. Mesh-based proximity conditioning yields substantial improvements by providing richer geometric cues, enabling more accurate warping and cleaner geometry synthesis while reducing artifacts. Finally, incorporating cross-modal attention distillation produces the highest quality results through synergistic image-geometry interaction, ensuring well-aligned modalities with superior consistency 13 Figure 11: Qualitative comparison on two-view extrapolative setting. Our qualitative comparison of previous approaches demonstrates our models extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions. and enhanced realism. This progressive enhancement clearly demonstrates how each component contributes essential capabilities that culminate in our methods state-of-the-art performance. B.4 Depth prediction accuracy Method PSNR SSIM LPIPS Abs.Rel δ1.25 Abs.Rel δ1.25 Abs.Rel δ1.25 Image Geometry Geometry (Recon) Geometry (Inpainting) Baseline Ours 14.51 19. 0.551 0.654 0.394 0.201 0.250 0.140 0.685 0.787 0.152 0.094 0.819 0. 0.308 0.198 0.531 0.684 Table 5: Depth prediction accuracy. Incomplete, under experiment being filled. We evaluate the depth prediction accuracy of our method using standard metrics such as PSNR, SSIM, and LPIPS. Quantitative comparisons against baseline methods demonstrate that our crossmodal attention instillation framework significantly reduces depth prediction error, particularly in challenging extrapolative scenarios. The integration of geometric cues via proximity-based mesh 14 Figure 12: Qualitative comparison with other warping-and-inpainting models. Our qualitative comparison of previous approaches demonstrates our models extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions. conditioning plays critical role in refining the depth predictions, as evidenced by our ablation studies. In addition to the numerical evaluations, our qualitative assessments reveal that the depth maps generated by our approach exhibit smooth transitions and accurate handling of occluded regions. The enhanced structural consistency is directly attributable to the attention instillation mechanism, which transfers reliable spatial attention maps from the image domain to the geometry networks. These results underline the effectiveness of our method in achieving high-quality depth estimation that closely aligns with the ground-truth geometry. 15 Figure 13: Ablation results. Our qualitative comparison of previous approaches demonstrates our models extrapolative capabilities to plausibly generate locations not seen in reference images while reconstructing faithfully the known regions."
        },
        {
            "title": "References",
            "content": "[1] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, and G. Taubin. The ball-pivoting algorithm for surface reconstruction. IEEE Transactions on Visualization and Computer Graphics, 5(4):349359, 1999. doi: 10.1109/2945.817351. [2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [3] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. [4] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [5] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domainfree generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. [6] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. [7] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. [8] Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Seungryong Kim, and Chong Luo. Unifying correspondence, pose and nerf for pose-free novel view synthesis from stereo pairs. arXiv preprint arXiv:2312.07246, 2023. [9] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. [10] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58855894, 2021. [11] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aanæs. Large scale multi-view stereopsis evaluation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 406413. IEEE, 2014. [12] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. [13] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [15] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1291212921, 2022. [16] Min-Seop Kwak, Jiuhn Song, and Seungryong Kim. Geconerf: Few-shot neural radiance fields via geometric consistency. arXiv preprint arXiv:2301.10941, 2023. [17] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [18] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 17 [19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. [20] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65 (1):99106, 2021. [21] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54805490, 2022. [22] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. [23] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [25] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, ChiehHsin Lai, Seungryong Kim, and Yuki Mitsufuji. Genwarp: Single image to novel views with semanticpreserving generative warping. arXiv preprint arXiv:2405.17251, 2024. [26] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [27] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. [28] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. arXiv preprint arXiv:2503.11651, 2025. [29] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. [30] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski. Reconfusion: 3d reconstruction with diffusion priors. arXiv, 2023. [31] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. [32] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. [33] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. [34] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "NAVER AI Lab",
        "SNU AIIS"
    ]
}