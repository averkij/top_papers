{
    "paper_title": "FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion",
    "authors": [
        "Haosen Yang",
        "Adrian Bulat",
        "Isma Hadji",
        "Hai X. Pham",
        "Xiatian Zhu",
        "Georgios Tzimiropoulos",
        "Brais Martinez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at a scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce a Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, a problem largely ignored in prior works. Our method, coined Fam diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 2 5 5 8 1 . 1 1 4 2 : r FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion Haosen Yang1,2* Adrian Bulat1 Isma Hadji1 Hai X. Pham1 Xiatian Zhu2 Georgios Tzimiropoulos1,3 Brais Martinez1 1Samsung AI Center, Cambridge, UK 2University of Surrey, UK 3Queen Mary University, UK"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models are proficient at generating high-quality images. They are however effective only when operating at the resolution used during training. Inference at scaled resolution leads to repetitive patterns and structural distortions. Retraining at higher resolutions quickly becomes prohibitive. Thus, methods enabling pre-existing diffusion models to operate at flexible test-time resolutions are highly desirable. Previous works suffer from frequent artifacts and often introduce large latency overheads. We propose two simple modules that combine to solve these issues. We introduce Frequency Modulation (FM) module that leverages the Fourier domain to improve the global structure consistency, and an Attention Modulation (AM) module which improves the consistency of local texture patterns, problem largely ignored in prior works. Our method, coined FAM diffusion, can seamlessly integrate into any latent diffusion model and requires no additional training. Extensive qualitative results highlight the effectiveness of our method in addressing structural and local artifacts, while quantitative results show state-of-the-art performance. Also, our method avoids redundant inference tricks for improved consistency such as patch-based or progressive generation, leading to negligible latency overheads. 1. Introduction Diffusion models [22] demonstrate impressive generative power across range of applications [6, 18, 20, 23, 29, 30, 33]. While powerful, one known shortcoming of diffusion models is their inability to seamlessly scale to higher resolutions beyond the one used during training. It is known that directly generating images at resolutions beyond the training resolution results in severe object repetition and unrealistic local patterns [1, 3, 7]. This is illustrated in Figure 1(a). *This work was conducted while Haosen Yang was an intern at Samsung AI Center, Cambridge, UK. While retraining diffusion models on higher-resolution images is straightforward solution, the computational demands quickly become prohibitive. This restricts applications requiring flexible or high-resolution image generation, e.g. 4K. Therefore, adapting pre-trained diffusion models to generate high-resolution images without additional training is topic of high interest that we tackle in this work. Prior efforts addressing this important problem can be largely categorized into two tracks. The first set of approaches, e.g. [3, 15], propose mechanisms that improve the global structure consistency by steering the high-resolution generation using the image generated at native (i.e. training) resolution. However, the effectiveness of such mechanisms is mixed, with trailing issues like poor detail quality, inconsistent local textures, and even persisting pattern repetitions as shown in Figure 1(b). Furthermore, these works typically operate on patch-based basis, generating one patch at time. Concretely, this means that these methods resort to redundant and overlapping forward passes, leading to large latency overheads. The second group of approaches, e.g. [7, 11, 34], eschews patch-based generation in favor of one-pass approach by directly altering the model architecture. This leads to faster generation, but unfortunately, it comes at the cost of image quality, as shown in Fig. 1 (c). To address the aforementioned limitations, we propose straightforward yet effective approach that takes the best of both worlds. Our method follows the single pass generation strategy for improved latency but, like patch-based approaches, leverages the native resolution generation to steer the high-resolution one. Specifically, our method starts by generating an image at native resolution conditioned on the input text prompt. We then resort to test-time diffusedenoise strategy [3, 8, 27], where the high-resolution denoising stage is guided by the native resolution diffusion process. However, instead of blindly steering the high-res image toward the low-res one as done elsewhere [3, 15], we propose Frequency Modulation (FM) module. In particular, we leverage the Fourier domain to selectively condition low-frequency components during the high-resolution 1 (a) Direct Inference (b) DemoFusion (c) HiDiffusion (d) Ours Figure 1. Comparisons of 3 (3072 3072) image generation based on SDXL [19]. image generation stage, while providing full control over high-frequency components to the denoising process. While the FM module resolves artifacts related to global consistency, artifacts related to inconsistent local texture might still be present, i.e. finer texture generated on semantically related parts of the image might be inconsistent. To tackle this second issue, largely ignored in the literature, we propose an Attention Modulation (AM) mechanism that leverages attention maps from the denoising process at native resolution to condition the attention maps of the denoising process at high resolution. Since attention maps at native resolution encode which regions of the image are semantically related, they regularize the high-res denoising towards consistent finer texture generation. Our method, coined Frequency and Attention Modulated diffusion (FAM diffusion), combines our FM and AM modules to yield superior quality results, see Fig. 1 (d). Our method seamlessly integrates with any latent diffusion model without additional training or architectural changes. We empirically show that our method significantly enhances the quality and efficiency of high-resolution image generation, establishing new state-of-the-art. 2. Related Work Diffusion models have shown impressive performance in generating creative and accurate representations given text prompts [10, 22]. While early work [22] was limited to generating relatively low-resolution images (i.e. 256 256), follow-up work showed that their performance can scale to higher resolutions, e.g. 512 512 with SD1.5 [22] and 1024 1024 with SDXL [19]. However, major shortcoming with all these models is that generation remains limited by the resolution used at training time. Naively targeting higher train-time resolutions quickly results in prohibitive training costs and computational requirements, and the limited availability of high-resolution training data also restricts the diversity of image generation. Thus, adapting pre-trained diffusion models to generate high-resolution images without retraining has emerged as topic of interest. Early works [1, 14] proposed using overlapping patches at native resolution and blending the outputs to produce an image without seams. However, this leads to frequent repetitions and inconsistent global image structure. Therefore, subsequent works introduced various mechanisms to encourage global structural consistency. For instance, DemoFusion [3] proposed patch-based generation process with mechanisms such as skip residuals and progressive upsampling, while AccDiffusion [15] used localized prompting to guide high-resolution generation and improve consistency with images generated at native resolutions. However, these methods still suffer from issues like local repetitions, and inconsistent global coherence. They also have significant latency overheads due to the running cost of multiple backward passes. To mitigate the high latencies, other works aim to generate high-resolution images in single pass by modifying the architecture of the UNet. For example, ScaleCrafter [7] employs dilated convolutions to adjust the receptive field of convolutions in the denoising UNet. HiDiffusion [34] introduces an alternative UNet that dynamically adjusts the feature map size during the denoising process. While these approaches achieve faster generation, they often result in image distortions. More closely related to ours are methods that have approached structural consistency from frequency domain perspective. FouriScale [12] splits the image in Fourier domain, then proceeds to incorporate low-pass filtering operation and impose structural consistency with an image generated at natire resolution. However, this splitting operation results in unrealistic images. HiPrompt [16] decomposes images into spatial frequency components conditioned on local and global prompts, but it often relies on redundant operations that lead to high latencies. ResMaster [25] leverages low-frequency information from the latent representation of the native image to provide desirable global semantics during the denoising process. However, it ignores the noise distribution differences between the current high-resolution denoising step and the native image in 2 latent space. In addition, it still relies on patch-based denoising, making it inefficient. In contrast to these methods, we propose one-pass method that does not alter the model architecture. Importantly, our method introduces complementary novel attention modulation mechanism, which targets local structure consistency; an issue overlooked by all existing works. 3. Method In this work, we leverage pretrained latent diffusion models (LDMs), which have been extensively trained on largescale high-quality data. Our goal is to generate images at higher resolutions than during training, without any additional finetuning or model modification. Sec. 3.1 briefly reviews the diffusion notation and the test-time diffuseIn Sec. 3.2 we present our Frequency denoise strategy. Modulated (FM) denoising approach, which is designed to improve global consistency. Finally, we introduce our Attention Modulation (AM) mechanism, which is designed to improve the consistency of the local texture and highfrequency detail, in Sec. 3.3. We provide an overview of our method in Figure 2. 3.1. Preliminaries Latent Diffusion Models (LDM) [22]: We operate in the realm of LDMs, which first convert image x0 to latent representation z0 using an encoder such that z0 = E(x0), z0 Rchw. During training, Markovian diffusion process progressively adds noise to the input latent z0 according to predefined schedule βt, [1, ] by sampling sequentially from: q(ztzt1) := (zt(cid:112)1 βtzt1, βtI) (1) Conversely, trainable denoising process progressively recovers the original latent z0 using noise estimator Zθ = (µθ, Σθ) parametrized by θ by sampling from: pθ(zt1zt) := (zt1µθ(zt, t), Σθ(zt, t)) (2) During inference, an image is generated by denoising from random noise, zT (0, I) Rchw, through sequential calls to Zθ. The quality of the generated image improves with the number of steps to finally yield the latent 0 Rchw, where we introduce the surepresentation zn perscript to indicates generation at native resolution hw (i.e. same as training resolution). Inference-time diffuse-denoise: Our goal is to use the pretrained parametric denoiser Zθ, without further finetun0 Rcshsw at higher resolution m, ing, to generate zm = sh sw, where is the target resolution scaling factor. The naive approach is to directly start from random (0, I) Rcshsw. noise at the target resolution, zm 3 However, this has been repeatedly shown to lead to suboptimal results, with frequent artifacts and object duplication [3, 7, 34]. This is illustrated in Fig. 3a. Instead, prior works proposed test time diffuse-denoise process [3, 8, 27]. The idea is to start from the output of the denoising process at native resolution, zn 0 rather than noise, which is then upsampled to the target resolution to obtain zm 0 = U(zn 0 , s), where denotes an upsampling function. Next, forward diffusion steps progressively add noise to the latents zm t=1...T . Finally, the backward process denoises from zm 0 . Note that we use and to refer to the latents generated during diffusion and denoising respectively. to yield the final output zm While standard denoising process as in Eq. 2 could be used, it often leads to inconsistent global structures, as shown in Fig. 3b. Instead, the denoising process from Eq. 2 is now defined as: pθ (cid:0)zm t1ft(zm , zm )(cid:1) (3) where ft(.) is tasked with steering the denoising process and improving the consistency between the high-res and low-res images. Previous work [3, 15] define ft(.) as simple weighted linear combination of zm and coin the mechanism skip residual. We show in Fig. 3c that this yields to suboptimal results. In contrast, we propose Frequency Modulated approach to defining ft(.). and zm 3.2. Frequency-Modulated Denoising The conditioning of the denoising steps through the skip residual has been shown to improve consistency between low and high-resolution images. We however observe that it lacks control over the information transferred. More specifically, the goal of the test-time diffuse-denoise process is to take the upsampled low-resolution image and to produce an output that 1) preserves the global structure, and 2) improves the texture and high-frequency details. The skip residual mechanism however steers the output towards the input indiscriminately, which serves the first objective but can negatively impact the latter. It would be desirable to instead harness the global structure information from the diffused latents of the forward process, while allowing the denoising process to handle the generation of details. To this end, we appeal to the frequency domain, where global structure and finer details are captured by lowand highfrequency, respectively [17, 28, 31], and re-define the function ft(.), which controls information transfer from the forward diffusion into the denoising process, in accordance. Let K(t) be high-pass filter for timestep t, the function ft(.) in Eq. 3 is defined as follows: ft(zm , zm ) =IDF T2D(K(t) DF T2D (zm ) + (1 K(t)) DF T2D (zm )), (4) Figure 2. Overview of the FAM diffusion. (a) We first generate an image at native resolution, followed by test-time diffuse-denoise process. We incorporate our Frequency Modulation module and Attention Modulation during high-res denoising to control global structure and fine local texture, respectively. (b) Details of the Frequency Modulation, where we use the Fourier domain to selectively condition low-frequency components during high-res denoising while leaving high-frequency components fully controllable. (c) Details of Attention Modulation, where attention maps from the native image denoising are used to correct the high-res denoising. (a) DI (b) DI* (c) SR (d) FM (e) FM-AM Figure 3. Ablation on the components of FAM diffusion. Direct Inference (DI) at high resolution from noise, Direct Inference from low-res latent (DI*), Skip Residual (SR) from DemoFusion [3], Frequency Modulation (FM), Attention Modulation (AM). where denotes the Hadamard product. Essentially, the high-frequency coefficients of the denoised latent zm are combined with the low-frequency coefficients of the diffused latent zm , modulated by the filter K(t). Eq. 4 can be further reformulated in the time domain as below: (cid:1), ft(zm zm t + κ(t) (cid:0)zm , zm ) = zm (5) where κ(t) = IDF T2D (1 K(t)) Rshsw is convolutional kernel, and denotes the circular convolution operator. Eq. 5 shows that the frequency modulation adds lowfrequency update to the denoised latent zm directed towards the diffused latent zm , subsequently preserving the global structural information from the upsampled latent. Furthermore, the circular convolution κ(t) in Eq. 5 can be interpreted as an additional (non-learnable) convolutional layer of the UNet, effectively providing it with global receptive field and helping generate consistent structure without modifying the UNet architecture [11, 34] or using dilated sampling [3]. The result of our FM approach is shown in Fig. 3d. In comparison, the skip residual approach of DemoFusion, shown in Fig. 3c, produces inconsistencies like missing left nostril and unnaturally small eyes. 3.3. Attention Modulation While the FM module successfully maintains global structure and solves the issue of object duplication as shown in Fig. 3d, we note that local structures can be inconsistently generated due to the discrepancy between trainingtime native resolution and the target inference-time high resolutions. For example, the top image in Fig. 3d shows distorted mouth compared to the one at native resolution. Similarly, in the bottom example, fur texture is incorrectly generated on the shirt collar. That is, the high-frequency detail generated on the shirt collar is semantically related to one generated on the foxs face and not to the other parts of the shirt. We hypothesize this stems from incorrect attention maps during the high-res denoising stage. This motivates us to propose our Attention Modulation (AM) approach. We take inspiration from attention swapping, recent method to combine information from two diffusion processes in more localized manner [4, 5, 13], and extend the idea to transfer local structural information from the denoising process at native resolution to the one at target resolution. In particular, the attention of an input tensor is computed by first projecting it linearly into triplet of query, keys, and values, (Q, K, ), respectively, and the selfattention is computed as: Att(z) = sof tmax (cid:19) (cid:18) d = (6) where indicates the feature dimensionality, and we refer to as the attention matrix. In our case, we modify the self-attention at specific layers of the UNet of the high-resolution denoising process to incorporate information from the attention maps of the native resolution as: = (λ U(M n, s) + (1 λ) m) (7) where and are the attention matrices at native and target resolution respectively, λ is hyperparameter, and is an s-times upsampling function. The new attention matrix is then used instead of during the high-res denoising process in Eq. 6. Applying our AM module at all layers of the UNet can lead to suboptimal performance due to over-regularization. We apply it instead only for layers in up-blocks of the UNet, as they are known to preserve layout information better [13]. Furthermore, we experimented with AM at various stages and found the highest benefit to be at up block 0. Results shown in Fig. 3e demonstrate the benefit of the proposed AM module, particularly regarding better preservation of local structures such as the mouth and shirt collar, highlighted in yellow boxes. 5 4. Experiment 4.1. Experimental setup To demonstrate the effectiveness of our approach, we pair it with well-performing diffusion model like SDXL [19]. For completeness, we also pair our approach with the recent HiDiffusion [34], which specifically changes the attention mechanism of SDXL with windowed attention to improve the model latency. SDXL is trained at 10241024 resolution, which we refer to as 1. We experiment with three unseen higher resolutions such that the model generates 2 2, 3 3, and 4 4 times more pixels than the training setup. In the supplementary, we also include results with various aspect ratios, e.g. 2 4, and also experiment with different variants of Stable Diffusion (SD); namely, SD 1.5 [22], SD 2.1 [22], which generate at 512512 and 768768 pixels respectively. Evaluation set. Following previous work [3, 7, 11, 15] we evaluate performance on subset of the Laion-5B dataset [24]. Given the number of compared methods and significant computational demands associated with the task, we randomly sample 10K images from Laion-5b which we use as our real images set, and we sample 1K captions, which we use as text prompts for the models. Evaluation metrics. Following prior work, we evaluate the quality and diversity of the generated images using Frechet Inception Distance (FID) [9] and Kernel Inception Distance (KID) [2], computed between the generated and real images. Since FID requires resizing images to 299 299, which negatively impacts the assessment, it is typical to adopt their patch-level variants [3, 11, 15, 34]. Specifically, we extract 10 random crops from each image before calculating FID and KID, referring to these metrics as FIDc and KIDc. To further evaluate the semantic similarity between image features and text prompts, we report the CLIP score [21]. To measure the efficiency of each method, we compute latencies on single A40 GPU. 4.2. Main Results [3], AccDiffusion select Demofusion and HiDiffusion [34] as [15], We represenFouriScale [11], tative methods of the current state-of-the-art among high-resolution generation methods. As shown in Table 1, FAM diffusion achieves the best overall performance on FIDc, KIDc, and CLIP Score in all cases. In the case of FID and KID, FAM diffusion provides substantial gains for larger scale factors, while producing similar results to DemoFusion on lower scale factors. However, these metrics heavily downsample high-resolution images before computing the metrics and thus do not capture finer details in the evaluation results. This is widely-known issue for these metrics, as explained in Sec. 4.1. Finally, we note that Method DemoFusion [3] AccDiffusion [15] FouriScale* [12] HiDiffusion [34] HiDiffusion [34] + FAM diffusion SDXL [19] SDXL [19] + FAM diffusion DemoFusion [3] AccDiffusion [15] FouriScale* [12] HiDiffusion [34] HiDiffusion [34] + FAM diffusion SDXL [19] SDXL [19] + FAM diffusion DemoFusion [3] AccDiffusion [15] FouriScale* [12] HiDiffusion [34] HiDiffusion [34] + FAM diffusion SDXL [19] SDXL [19] + FAM diffusion"
        },
        {
            "title": "Scaling Factor",
            "content": "2 2 3 3 4 4 FID 63.24 59.42 78.54 78.02 69.61 59.47 58.91 68.82 73.47 73.57 112.51 76.28 78.41 69.25 65.89 73.97 105.24 129.91 59.05 160.10 58.91 KID 0.0084 0.0068 0.0136 0.0136 0.0140 0.0067 0.0072 0.0159 0.0210 0.0309 0.0325 0.0007 0.0136 0.0007 0.0087 0.0090 0.0342 0.0483 0.0074 0.0602 0.0073 FIDc KIDc CLIP Latency(mins) 36.75 37.23 40.80 51.41 34.26 50.54 33.96 40.24 43.64 65.01 68.84 36.70 69.40 36.40 48.44 54.80 70.45 156.98 44.65 74.37 43. 0.0096 0.0105 0.0130 0.0139 0.0084 0.0136 0.0080 0.0122 0.014 0.0357 0.021 0.010 0.0210 0.010 0.0157 0.0187 0.0223 0.0877 0.0134 0.0242 0.0130 32.0 31.69 29.8 30.5 32.32 30.6 32.35 32.0 31.50 28.54 28.43 32.26 28.44 32.25 30.45 30.15 27.86 24.32 32.31 26.70 32.33 2.5 2.6 2.3 0.6 0.8 0.8 1 8.6 10 6.2 1.5 1.8 2.2 2.5 19.6 20.5 14.7 2.8 3.1 5.4 6.1 Table 1. System-level comparisons with SDXL. * indicates inference with FreeU [26] (a) Low Resolution Att. (b) High Resolution Att. (c) Att. Modulation Figure 4. Visualization of Attention Maps in the UNet: (a) LowResolution Attention map, (b) High-Resolution Attention map, (c) Attention Map when using the AM module our method adds only small latency overheads compared to direct inference on the target resolution, e.g. 0.2, 0.3, and 0.7 min at 2, 3 and 4 scale factors respectively when combined with SDXL. In comparison, DemoFusion adds 14.2 sec latency vs SDXL direct inference at 4 scale factor. When compared to the frequency-based method FouriScale [11], FAM diffusion also shows notable improvements in both quality and latency. For instance, under 4K resolution image generation, it achieves 43.65 (a) Direct Upsampling (b) BSRGAN (c) Ours Figure 5. Qualitative comparison between Direct Upsampling, BSRGAN, and our method. The patches shown were cropped from 4096 4096 resolution image. Zoom in for best view. vs. 70.45 on FIDc and 32.31 vs. 26.67 on CLIP score, while also being faster than FouriScale. Additionally, we observed that FAM diffusion can be seamlessly integrated 6 (a) Native Resolution Image (b) DemoFusion (c) FouriScale* (d) HiDiffusion (e) Our Method Figure 6. Qualitative comparison with other methods based on SDXL. Best viewed when zoomed in. * indicates inference with FreeU [26] 7 produces artifacts and poor local patterns. (3) Compared to Skip Residuals, FM reduces undesirable local patterns by leveraging the low-frequency information of the image at native resolution, which provides better structural guidance. (4) Attention Modulation resolves inconsistencies between local patterns and global structure by utilizing the attention map from the native resolution, offering strong guidance of the semantic relationships among latent tokens. Overall, FM and AM address structural distortions and local pattern inconsistencies in high-resolution images effectively, highlighting the meaningful contributions of FAM diffusion. Effectiveness of the time-aware formulation on the FM module We show here the effect of the time-varying formulation of FM, as illustrated in Figure 7a. Specifically, the FM module incorporates low-frequency information from the corresponding diffused latent at each step t. Instead, we can avoid this time-varying nature and utilize the upsampled latent as single static reference. However, this approach results in images that appear noticeably blurrier and lose finer details associated with high-frequency information, highlighting the importance of the dynamic nature of the FM module throughout the denoising process. Analysis of Attention Modulation To better understand the principles underlying the AM module, we visualize in Figure 4 the self-attention maps of tokens from the mouth region (marked with star) as the query and all tokens as the key and value. The resulting attention map computed using the low-resolution latent primarily encodes coarse information of the semantic relations among parts of the image, but lacks fine-grained contextual information across the entire face. Instead, the attention maps at high resolution are more detailed, but fail to capture semantic relatedness, e.g. the mouth areas are not highlighted. After applying AM, the attention map effectively integrates local-global relationships with enhanced fine-grained detail. This analysis provides visual insights into how AM repairs inconsistencies in local patterns, contributing to more coherent global structures. 5. Conclusion We introduced FAM diffusion, training-free diffusion model for high-resolution image generation. To address issues of object repetition and structural distortion, we propose Frequency Modulated strategy. By leveraging the Fourier domain, this method enhances guidance for highresolution generation while avoiding latency overheads associated with multi-patch approaches. Additionally, we propose an effective Attention Modulation mechanism to address inconsistent texture patterns, challenge largely overlooked in previous works. Extensive quantitative and qualitative evaluations highlight the effectiveness of our method. We further show that, contrary to previous works, our method incurs in marginal latency overheads. local (a) Constant LF (b) Time-aware LF Figure 7. Comparison between Constant LF and Time-aware LF. into single-pass methods, such as HiDiffusion [34], to enhance performance while maintaining fast image generation, achieving an effective latency-quality trade-off. These results quantitatively validate the effectiveness of our method in improving the quality of image generation. In Figure 6, we present comparison between DemoFusion, FouriScale, HiDiffusion, and FAM diffusion. We selected three complex textual prompts to highlight the imagegeneration capabilities of the model. For FouriScale, we used the default setting with FreeU [26]. Firstly, as mentioned above, DemoFusion tends to generate repetitive content and artifacts with unreasonable local structures due to its patch-based generation approach (see for example the two small cat heads generated on the top-right image). FouriScale [11] and HiDiffusion [34] produce visually unappealing structures and extensive areas of irregular textures, which significantly degrade the overall visual quality. Additionally, we compare our method with the superresolution approach BSRGAN [32], as shown in Figure 5. We observe that FAM diffusion effectively introduces or modifies high-frequency details that were not present in the original image, while preserving structural information, leading to more appealing and detailed images. To further illustrate the generality of our approach, in the supplementary material we provide results of our approach in combination with SD1.5 and SD2.1. 4.3. Ablation Study In this section, we conduct ablation studies and use SDXL with the 2 2 scale factor setting. Effectiveness of the components in the FAM diffusion We study the effect of the two components of FAM diffusion, Frequency-Modulated Denoising (FM) and Attention Modulation (AM). The results shown in Figure 3 indicate the following: (1) both direct inference from random noise, and direct inference from the diffused latent at native resolution generate outputs with structural distortions and repeated patterns. (2) while the Skip Residuals of DemoFusion helps maintain the global structure of the image, it still"
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. MultiDiffusion: fusing diffusion paths for controlled image generation. In International Conference on Machine Learning, 2023. 1, 2 [2] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. International Conference on Learning Representations, 2018. 5 [3] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. DemoFusion: Democratising highresolution image generation with no $$$. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 1, 2, 3, 4, 5, 6, 13 [4] Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, and Xin Eric Wang. Photoswap: Personalized subject swapping in images. Neural Information Processing Systems, 2023. 5 [5] Jing Gu, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Yilin Wang, and Xin Eric Wang. SwapAnything: Enabling arbitrary object swapping in personalized image editing. European Conference on Computer Vision, 2024. 5 [6] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. [7] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higherIn Inresolution visual generation with diffusion models. ternational Conference on Learning Representations, 2024. 1, 2, 3, 5 [8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 1, 3 [9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. Neural Information Processing Systems, 2017. 5 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Neural Information Processing Systems, 2020. 2 [11] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. FouriScale: frequency perspective on training-free high-resolution image synthesis. In European Conference on Computer Vision, 2024. 1, 4, 5, 6, 8 [12] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. FouriScale: frequency perspective on training-free high-resolution image synthesis. arXiv preprint arXiv:2403.12963, 2024. 2, 6, 13 [13] Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, and Youngjung Uh. Visual style prompting with swapping selfattention. arXiv preprint arXiv:2402.12974, 2024. [14] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. SyncDiffusion: Coherent montage via synchronized joint diffusions. In Neural Information Processing Systems, 2023. 2 [15] Zhihang Lin, Mingbao Lin, Zhao Meng, and Rongrong Ji. AccDiffusion: An accurate method for higher-resolution imIn European Conference on Computer Viage generation. sion, 2024. 1, 2, 3, 5, 6, 13 [16] Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, Wenhan Luo, Qifeng Liu, and Yike Guo. HiPrompt: Tuningfree higher-resolution generation with hierarchical MLLM prompts. arXiv preprint arXiv:2409.02919, 2024. 2 [17] David Marr and Ellen Hildreth. Theory of edge detection. Proceedings of the Royal Society of London. Series B. Biological Sciences, 207(1167):187217, 1980. 3 [18] Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, and Georgios Tzimiropoulos. You only need one step: Fast super-resolution with stable diffusion via scale distillation. European Conference on Computer Vision, 2024. 1 [19] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations, 2024. 2, 5, 6, 13 [20] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. arXiv preprint arXiv:2209.14988, 2022. [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022. 1, 2, 3, 5 [23] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven In IEEE Conference on Computer Vision and generation. Pattern Recognition, 2023. 1 [24] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training In Neural Information next generation image-text models. Processing Systems - Datasets and Benchmarks Track, 2022. 5 [25] Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, and Yinqiang Zheng. ResMaster: Mastering highresolution image generation via structural and fine-grained guidance. arXiv preprint arXiv:2406.16476, 2024. 2 [26] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. FreeU: Free lunch in diffusion U-Net. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. 6, 7, 8, 13, 14 [27] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 1, 3 [28] BA Wandell. Foundations of vision, 1995. 3 [29] Wenqing Wang, Haosen Yang, Josef Kittler, and Xiatian Zhu. Single image, any face: Generalisable 3D face generation. arXiv preprint arXiv:2409.16990, 2024. 1 [30] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In IEEE International Conference on Computer Vision, 2023. 1 [31] Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, and Fengbo Ren. Learning in the frequency domain. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 3 [32] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind In IEEE International Conference image super-resolution. on Computer Vision, 2021. 8 [33] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE International Conference on Computer Vision, 2023. 1, 12, 17 [34] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Yuhao Chen, Yao Tang, and Jiajun Liang. HiDiffusion: Unlocking higherresolution creativity and efficiency in pretrained diffusion models. In European Conference on Computer Vision, 2024. 1, 2, 3, 4, 5, 6, 8, 10 A. Appendix To complement the main content of the paper, we provide here additional details about the method in Sec. as well as additional quantitative and qualitative results in Sec C. B. Additional technical details B.1. Frequency Modulation details Time-varying high-pass filter definition. In our method, we rely on frequency domain and use high pass filter to steer the denoising process as described in equation (4). In the following, we provide the formal definition of the timevarying high pass filter, K(t), that we used. The high-pass filters K(t) have time-varying cut-off frequencies, defined as follows: ρ(t) = τh(t) = (1 ρ(t)) τw(t) = (1 ρ(t)) (8) (9) (10) where τh(t) and τw(t) are the horizontal and vertical cutoff frequencies at timestep t, respectively. Subsequently, the mask K(t), which is applied on the shifted frequency spectrum centered on (xc, yc), is defined as (a) Swapping (b) Modulation Figure 8. Comparison of Attention Swapping and Modulation our proposed mixing operation in the frequency domain is formulated as below: K(t) = ρ(t), 1, if xc < τw(t) 2 & yc < τh(t) 2 otherwise , (11) ˆZ = DF T2D(z) + (1 K) DF T2D(z) = (WrzWc) + (1 K) (WrzWc) = WrzWc + (1 K) (Wr(z z)Wc) The cut-off frequency grows as the denoising process progresses, while the scaling factor of the low-frequency coefficients decreases. Our frequency modulation is designed such that the guidance from the denoised latent zt becomes more significant as 0. In our experiments, we set = 0.5. the Frequency Modulation in timeDerivation of domain. In the main paper, we mention that our frequency modulation introduced in Eq. (4) can be reformulated in time domain as Eq. (5) and discuss the corresponding benefits. Here, we provide formal derivation to support the equivalence between the two formulations. For ease of presentation, we omit the timestep and resolution notations from operands. Let Rhw be the 2D latent, and = DF T2D (z) Chw be the Fourier transform of z. Written in matrix form, = (WrzWc), (12) where Wr Chh, Wc Cww are the rowand columnwise Fourier transform matrices, respectively. Let Rhw be the high-pass filter defined in the previous section, The inverse DFT of ˆZ, which is the outcome of Eq. 4, is formulated as: ˆz = IDF T2D( ˆZ) (WrzWc + (1 K) (Wr(z z)Wc)) 1 = 1 = 1 WrzWcW 1 + 1 = + (cid:0)W 1 = + (z z), ((1 K) (Wr(z z)Wc)) 1 (cid:1) (cid:0)W 1 (1 K)W 1 Wr(z z)WcW 1 c (cid:1) (1 = IDF T2D(1 K) is convolutional kernel and resulting in Eq. 5 in the main paper, where = 1 K)W 1 denotes circular convolution operator. B.2. Attention Modulation analysis As mentioned in Sec. 3.3, we take inspiration from recent literature using attention swapping to control local texture. However, rather than swapping attention, we mix the two attention paths instead. In Figure 8 we compare attention swapping versus our proposed attention modulation. These results clearly show the benefit of including the attention 11 from the high resolution path rather than directly swapping with the low res pass to avoid loss of information from the high res denoising path. We empirically set λ used in Eq (6) to 0.7. C. Additional experimental results C.1. FAM diffusion with different SD backbones In Table 1 we show that our method outperforms several baselines when combined with SDXL. In addition to those main results, we further combine our FAM diffusion method with various SD backbones. The quantitative results in Table 2 demonstrate that our approach can seamless combine with different variants of SD and provides similarly large improvements in quality and image-text alignment across all experimental settings. C.2. FAM diffusion with different aspect ratios Thus far, we have used our method to generate highresolution images by equally upscaling both the height and width. Here, we study the effect of using Fam diffusion targeting different aspect ratios. In particular, starting from the SDXL model, we use our approach targeting higher resolutions with different aspect ratios. The quantitative results in Table 3 and qualitative results shown in Figures 9 through 11, clearly highlight the versatility of our method that can seamlessly adapt to various settings without compromising quality. C.3. FAM diffusion with different conditioning terms Fam Diffusion enables seamless integration with various LDM-based applications, such as ControlNet [33]. As shown in Figure 12, Fam Diffusion combined with ControlNet [33] achieves controllable high-resolution generation, with examples showcasing the use of images and canny edges as conditions. Method SD 1.5 SD 1.5 + FAM diffusion SD 2.1 SD 2.1 + FAM diffusion SDXL SDXL+ FAM diffusion SD 1.5 SD 1.5 + FAM diffusion SD 2.1 SD 2.1 + FAM diffusion SDXL SDXL + FAM diffusion SD 1.5 SD 1.5 + FAM diffusion SD 2.1 SD 2.1+ FAM diffusion SDXL SDXL + FAM diffusion"
        },
        {
            "title": "Resolution Scale Factor",
            "content": "2 2 3 3 4 4 FIDr KIDr 0.0122 75.36 0.0087 65.07 0.0163 86.62 0.0084 64.77 0.0067 59.47 0.0072 58.91 0.0251 106.50 0.0011 38.19 0.0384 137.05 0.0089 64.8 0.0136 78.41 0.0007 69.25 0.0474 150.84 0.0086 67.77 0.0645 177.06 0.0085 66.32 0.0602 160.10 0.0073 58.91 FIDc KIDc 0.0103 43.99 0.0082 34.06 0.0137 53.67 0.0091 38.18 0.0136 50.54 0.0080 33.96 0.0133 48.92 0.0082 43.99 0.01719 63.91 0.0114 40.49 0.0210 69.40 0.0100 36.40 0.0155 55.97 0.0012 40.21 0.019 69.43 0.0018 41.37 0.0242 74.37 0.0130 43.65 CLIP Score 30.35 30.92 29.66 31.13 30.6 32.35 28.89 30.44 27.81 31.13 28.44 32.25 27.40 30.36 26.36 31.10 26.70 32. Table 2. Comparison of vanilla Stable Diffusion and our FAM diffusion. Method DemoFusion [3] AccDiffusion [15] FouriScale* [12] HiDiffusion [34] SDXL [19] SDXL [19] + FAM diffusion Scaling Factor 2 4 FID 81.69 70.42 71.86 118.56 80.62 63.48 KID 0.0112 0.0119 0.0302 0.038 0.0236 0. FIDc KIDc CLIP 0.0165 54.48 0.0205 55.73 0.0322 63.28 0.021 65.46 0.0302 67.46 0.0115 41.44 29.3 29.0 25.8 26.3 25.5 30.6 Table 3. System-level comparisons with SDXL. * indicates inference with FreeU [26] 13 (a) Native Resolution Image (b) DemoFusion Figure 9. Qualitative comparison with other methods based on SDXL. Best viewed when zoomed in. * indicates inference with FreeU [26]. (Continued in Fig. 10). (c) FouriScale* 14 (a) HiDiffusion Figure 10. Qualitative comparison with other methods based on SDXL (continued from Fig. 9). Best viewed when zoomed in. (b) Our Method 15 (a) FouriScale* (b) HiDiffusion (c) Our Method Figure 11. Qualitative comparison with other methods based on SDXL with arbitrary resolutions. DemoFusion is unable to handle arbitrary resolutions, therefore not included. Best viewed when zoomed in. (a) Image to Image Figure 12. Results of FAM Diffusion combining with ControlNet [33]. All images are generated at 2 (2048 2048).Best viewed when zoomed in. (b) Canny Edges to Image"
        }
    ],
    "affiliations": [
        "Queen Mary University, UK",
        "Samsung AI Center, Cambridge, UK",
        "University of Surrey, UK"
    ]
}