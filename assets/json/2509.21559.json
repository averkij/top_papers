{
    "paper_title": "X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning",
    "authors": [
        "Prasanna Reddy Pulakurthi",
        "Jiamian Wang",
        "Majid Rabbani",
        "Sohail Dianat",
        "Raghuveer Rao",
        "Zhiqiang Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute cosine similarities for ranking. However, this design presents two limitations. Low-quality text-video data pairs could compromise the retrieval, yet are hard to identify and examine. Cosine similarity alone provides no explanation for the ranking results, limiting the interpretability. We ask that can we interpret the ranking results, so as to assess the retrieval models and examine the text-video data? This work proposes X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of the embedding model-based similarity ranking. We first expand the existing benchmarks with additional video annotations to support semantic understanding and reduce data bias. We also devise a retrieval CoT consisting of pairwise comparison steps, yielding detailed reasoning and complete ranking. X-CoT empirically improves the retrieval performance and produces detailed rationales. It also facilitates the model behavior and data quality analysis. Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT."
        },
        {
            "title": "Start",
            "content": "X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning Prasanna Reddy Pulakurthi1, Jiamian Wang1, Majid Rabbani1, Sohail Dianat1, Raghuveer Rao2, and Zhiqiang Tao1 1Rochester Institute of Technology, 2DEVCOM Army Research Laboratory 5 2 0 2 5 2 ] . [ 1 9 5 5 1 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute cosine similarities for ranking. However, this design presents two limitations. Low-quality text-video data pairs could compromise the retrieval, yet are hard to identify and examine. Cosine similarity alone provides no explanation for the ranking results, limiting the interpretability. We ask that can we interpret the ranking results, so as to assess the retrieval models and examine the text-video data? This work proposes X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of the embedding model-based similarity ranking. We first expand the existing benchmarks with additional video annotations to support semantic understanding and reduce data bias. We also devise retrieval CoT consisting of pairwise comparison steps, yielding detailed reasoning and complete ranking. X-CoT empirically improves the retrieval performance and produces detailed rationales. It also facilitates the model behavior and data quality analysis. Code and data are available at: github.com/PrasannaPulakurthi/X-CoT."
        },
        {
            "title": "Introduction",
            "content": "Text-to-video retrieval finds the most relevant video for text query, being widely used for retrieval-augmented generation (Jeong et al., 2025), question-answering (Sun et al., 2024b), and agent memory enhancement (Fan et al., 2024; Sun et al., 2024a), etc. Recent progress mainly depends on embedding models, e.g., CLIP-based (Ma et al., 2022; Wang et al., 2024a,b) or MLLM-based (Jiang et al., 2024; Sun et al., 2024c) for retrieval. However, an embedding model-based retrieval system bears some limitations. First, the model is prone to the data quality of text-video pairs. Public datasets can introduce either flawed videos (e.g., blur, distortion) or crude captions (Radford et al., 2021), undermining the retrieval and making it hard Figure 1: Existing retrieval systems mainly adopt embedding models to compute cosine similarities. We propose LLM CoT reasoning-based retrieval to provide explanations beyond rankings. Our method can also be integrated upon diverse embedding model methods. to track. Second, the embedding model mainly computes the cosine similarity in the latent space, which only tells the ranking but fails to justify the ranking results. Both of these reasons call for an explainable retrieval system to interpret why video candidate was retrieved, so as to assist the users to comprehend the ranking results, assess the retrieval system, and examine the input data quality. To achieve interpretability, this work proposes XCoT, an explainable framework that exchanges traditional cosine similarity-based ranking with LLMbased judgment (see Fig. 1) and devises chainof-thought pipeline for text-video retrieval. Firstly, we expand the existing benchmark datasets with additional video annotations to facilitate the LLMs reasoning and reduce the raw video data bias. Secondly, we define retrieval CoT consisting of pairwise comparison steps upon the BradleyTerry model (Bradley and Terry, 1952). By collecting the stepwise results, the proposed method not only enables the improved ranking performance over embedding model-based baselines but also delivers detailed rationales. In addition, without requiring the paired text-video data training, this method could serve as general processing step that integrates with distinct embedding models. We summarize the contributions as follows: (1) This work proposes X-CoT, an explainable retrieval system upon LLM chain-of-thought reasoning, advancing the trustworthy and trackable retrieval beyond the embedding model design. (2) We collect and release high-quality text annotation data for the raw videos to augment existing benchmark textvideo datasets for future LLM study. (3) This work devises retrieval CoT upon pretrained LLM, being free of optimization and plug-and-play on top of the existing retrieval systems. (4) Experiments demonstrate the remarkable performance boost of X-CoT upon diverse embedding models and benchmark datasets. With X-CoT, we empirically analyze the behaviors of embedding models and identify the inferior text-video data."
        },
        {
            "title": "2 Related Work",
            "content": "Text-Video (T2V) Retrieval has been driven by embedding models like X-CLIP (Ma et al., 2022), Clip4clip (Luo et al., 2022), Clip-vip (Xue et al., 2022), Cap4video (Wu et al., 2023), UMT (Li et al., 2023), and InternVid (Wang et al., 2024d), which learn joint video-text representations for retrieval. MLLMs for Retrieval. Recent advances in MLLMs extend language models with visual understanding, enabling new capabilities in retrieval and reasoning. VLM2Vec (Jiang et al., 2024) excels at text-image retrieval, having been trained for large-scale multimodal embedding tasks. MM-REACT (Yang et al., 2023) combines visual tools with LLM reasoning. While Video-ChatGPT (Maaz et al., 2024) and VideoLLaVA (Lin et al., 2024) allow free-form video understanding through frame-by-frame perception and dialogue. BRIGHT (SU et al., 2025) introduces challenging benchmark focused on reasoningintensive multimodal retrieval, highlighting the need for interpretable and robust systems like ours."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Existing text-to-video retrieval systems are mainly embedding model-based. Given video candidate and text query q, an embedding model produces the video and text embedding, respectively, i.e., zv, zq Rd, where denotes the dimension of the embedding space. Given the features, the system Figure 2: Video annotation collection pipeline. Structured text is constructed to enrich the semantics and assist LLM reasoning. Ground-truth captions are not directly used. Figure 3: Example of one structured video annotation. computes the cosine similarity score for ranking, i.e., s(q, v) = (z zv)/(zq2zv2). However, it is hard to understand the rationale behind specific cosine similarity score, e.g., what is the specific reason that the s(q, v) is high/low for text and video v, which could attribute to either text-video data correspondence or embedding models behavior. To this end, this work studies explainable retrieval. 3.2 Video Annotation Collection Motivation. We first expand the existing text-video benchmarks with additional video annotations for the following reasons. (1) Videos can contain complex semantics, such as scenes with rapid motions or massive objects. Additional annotations provide better chance for video understanding. (2) Video could be noisy and mislead the retrieval due to blur and distortion. Additional annotations provide useful information to describe the video semantics, reducing the bias caused by noisy frames. Data Collection Pipeline. To collect the highquality annotations, we develop an MLLM-based pipeline (see Fig. 2). For every video v, we uniformly sample frames and apply the filters to remove near-duplicates (see Appendix A). We [q, vi, vj]. We adopt LLM to process each tuple, yielding the binary preference (e.g., vi < vj) and the text justification. The structured annotations are employed to facilitate the reasoning. Step 3: Notably, we further refine the ranking by approximating the BradleyTerry (BT) model on the pairwise set via MLE (Hunter, 2004) and compute the ability scores θk with Pr[vi > vj] = θi/(θi + θj). By this means, we correct the comparisons with noisy or cyclic judgments. Accordingly, the final ranking list ˆV is produced by Sorting in descending order. We provide the X-CoT algorithm in Appendix F."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experimental Settings We evaluate X-CoT on four benchmarks: MSRVTT (Xu et al., 2016), MSVD (Chen and Dolan, 2011), LSMDC (Rohrbach et al., 2015), and DiDeMo (Anne Hendricks et al., 2017). We report Recall@K (R@1, R@5, R@10), Median Rank (MdR), and Mean Rank (MnR). We consider three off-the-shelf embedding models to generate the coarse top-K list (K = 20), including CLIP-ViT-B/32 (Radford et al., 2021), Qwen2-VL (Wang et al., 2024c) model by VLM2Vec (Jiang et al., 2024), and X-Pool (Gorti et al., 2022). The former two are zero-shot retrievers, and X-Pool is trained with text-video data. 4.2 Performance Comparison Table 1 and 2 show the text-to-video retrieval performance with the proposed X-CoT on four datasets and three embedding models. X-CoT enables remarkable performance boost over embedding models on all metrics, e.g., +5.6% in R@1 for CLIP on MSVD, +1.9% in R@1 on MSVD for XPool. Overall, LLM CoT reasoning-based retrieval enjoys accurate retrieval over cosine similaritybased ranking upon embedding models. 4.3 Ablation Study We conduct an ablation study toward the X-CoT in Table 3. We adopt the CLIP model as the baseline. We study the effect of the proposed CoT with w/o CoT, i.e., directly ask the LLM to rank the top-K results, leading to significant drop in performance, e.g., 2.9% for R@1 pairwise comparison is much easier than selecting best-of-K. We also find that the CoT model (w/o BT) benefits the retrieval. Jointly considering the CoT and the BT model, the Figure 4: X-CoT pipeline, which contains pairwise comparisons upon LLM for stepwise ranking and reasoning. then adopt an MLLM (Qwen2.5-VL-7B-CaptionerRelaxed) to generate frame-level captions, which are aggregated and rephrased to form structured annotations comprising objects, actions, and scenes, plus high-level video summary. We apply additional post-processing steps to improve annotation quality, including (i) Noun Filter: Extract and retain relevant object and scene tags for grounding entities. (ii) Verb Filter: Extract action-related verbs to support temporal and causal reasoning. (iii) Deduplication: Redundant or semantically equivalent tags (e.g., \"a dog\", \"dog\", \"the dog\") are merged to avoid repetition. (iv) Stop Word Removal: Common stop words (e.g., \"the\", \"is\", \"in\") are filtered out to retain only informative content words. (v) Proofing: Correct grammatical or formatting inconsistencies in the tags. (vi) Normalization: We apply basic text normalization, including lowercasing and punctuation removal. All videos are equipped with structured annotations, as illustrated in Fig. 3. 3.3 Retrieval CoT Given the annotation data, this work adopts LLM reasoning for explainable retrieval. We construct retrieval CoT to jointly produce the ranking and explanations, as shown in Fig. 4. The whole pipeline contains three steps. Step 1: One can optionally adopt diverse embedding models to produce top-K candidate pool for given query. Since the existing embedding model-based methods enable accurate retrieval with large value, one can apply the proposed X-CoT to reason among small range, e.g., = {v1, . . . , vK}, < 25. Step 2: We then generate pairwise combinations of the top-K candidates, forming input tuple Methods How2Cap (Shvetsova et al., 2024) TVTSv2 (Zeng et al., 2023) InternVideo (Wang et al., 2024e) BT-Adapter (Liu et al., 2024) ViCLIP (Wang et al., 2024d) CLIP (Radford et al., 2021) X-CoT (ours) VLM2Vec (Jiang et al., 2024) X-CoT (ours) X-Pool (Gorti et al., 2022) X-CoT (ours) MSR-VTT MSVD R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR 37.6 38.2 40.7 40.9 42.4 31.6 33.7 36.4 37.2 46.9 47.3 20.8 20.5 12.8 12.6 9.3 9.2 62.0 62.4 65.3 64.7 53.8 56.7 60.2 61.8 73.0 73.3 39.0 38.7 27.3 27.1 14.2 14.2 44.5 43.4 49.1 36.5 42.1 46.7 48.4 47.2 49.1 73.3 69.9 64.0 67.4 73.8 74.8 77.2 78. 82.1 79.1 73.9 75.4 82.6 83.2 86.0 86.6 73.3 73.2 74.1 73.5 63.4 64.6 70.7 71.5 82.0 82.1 3.0 3.0 2.0 4.0 4.0 3.0 3.0 2.0 2.0 2.0 3.0 2.0 2.0 2.0 2.0 2.0 Table 1: Text-to-video retrieval performance comparison on MSR-VTT and MSVD. Methods HiTeA (Ye et al., 2023) TVTSv2 (Zeng et al., 2023) InternVideo (Wang et al., 2024e) BT-Adapter (Liu et al., 2024) ViCLIP (Wang et al., 2024d) CLIP (Radford et al., 2021) X-CoT (ours) VLM2Vec (Jiang et al., 2024) X-CoT (ours) X-Pool (Gorti et al., 2022) X-CoT (ours) DiDeMo LSMDC R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR 36.1 34.6 31.5 35.6 18.4 25.2 29.7 33.5 35.8 44.6 45.1 129.6 129.4 119.1 118.9 54.1 54.0 49.7 49.2 34.1 33.9 15.1 15. 15.5 17.3 17.6 19.5 20.1 15.9 17.6 18.2 18.9 23.6 23.8 31.1 32.5 32.4 35.9 28.4 29.0 33.6 35.1 42.9 43.8 39.8 41.4 40.2 45.0 35.3 36.1 41.4 41.9 52.4 53.1 20.0 23.0 31.0 31.0 23.0 23.0 9.0 8.0 60.1 61.9 57.6 61.9 49.4 52.1 57.7 59.2 72.5 73.1 70.3 71.5 68.2 72.6 59.0 60.6 68.4 68.8 81.0 81. 3.0 3.0 6.0 5.0 4.0 3.0 2.0 2.0 Table 2: Text-to-video retrieval performance comparison on DiDeMo and LSMDC. Method R@1 R@5 R@10 MdR MnR 49.7 Baseline w/o CoT 49.7 49.4 w/o BT 49.2 X-CoT 49.4 39.4 51.8 52.1 59.0 58.9 60.4 60.6 25.2 22.3 29.3 29. 6.0 6.0 5.0 5.0 Table 3: Ablation study of proposed X-CoT with CLIPViT-B/32 model (K = 20) and upon DiDeMo Dataset. Figure 5: top-K discussion to facilitate X-CoT. Performance reported with CLIP model on DiDeMo dataset. the explainability of the proposed X-CoT. Fig. 6 discusses the explainability of X-CoT in evaluating the retrieval models behavior. With explanations, one can diagnose the semantic factors that could be missed by the embedding model. e.g., the concept of man plays an important role. In addition, one can evaluate the text-video data quality with the proposed X-CoT. As shown in Fig. 7, the proposed X-CoT fails for the given text query. However, the incorrect retrieval could be attributed to the text flaws by jointly examining the text caption, relevant video, and the CoT explanations. This demonstrates the power of the explainable retrieval system in the text-video data quality assessment. We provide success examples in Appendix H."
        },
        {
            "title": "5 Conclusion",
            "content": "proposed method improves the baseline by 4.5% on R@1. 4.4 Model Discussion In Fig. 5, we discuss the top-K ranges to facilitate X-CoT. X-CoT effectively identifies and ranks relevant candidates as grows, demonstrating an adaptivity to the pool scale. We further discuss This work studied explainable retrieval systems and introduced X-CoT, an LLM CoT reasoningbased retrieval system in place of the embedding model cosine similarity-based ranking. To achieve the goal, we first expand the existing benchmarks with additional video annotation. We then constructed pairwise CoT to provide reasoning and ranking. Experiments show X-CoT improves recould be one of the first efforts in this direction, we will explore more challenging text-to-video retrieval scenarios in future work. While the BradleyTerry (BT) model provides principled way to aggregate pairwise preferences, it also imposes certain constraints. The current formulation relies on binary win/loss outcomes and does not capture the uncertainty or nuanced reasoning strength that LLMs may provide. Future work could explore the incorporation of soft confidence scores or learnable aggregation strategies so that the richness of LLM reasoning in text-to-video retrieval can be better captured."
        },
        {
            "title": "References",
            "content": "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. 2017. Localizing moments in video with natural language. In ICCV. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 44434458, Online. Association for Computational Linguistics. Finale Doshi-Velez and Been Kim. 2017. Towards rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. Yue Fan, Xiaojian Ma, Rongpeng Su, Jun Guo, Rujie Wu, Xi Chen, and Qing Li. 2024. Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding. arXiv preprint arXiv:2501.00358. Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. 2022a. Bridging videoIn text retrieval with multiple choice questions. CVPR. Yuying Ge, Yixiao Ge, Xihui Liu, Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie, and Ping Luo. Figure 6: Explainability discussion. X-Pool fails in ranking highly similar videos. By comparison, X-CoT identifies the relevant video, with subtle differences clearly explained. Figure 7: Explainability discussion. By jointly examining the text caption, relevant video, and the CoT reasoning by X-CoT, one can find the ambiguous (e.g., object) and minor (e.g., stop sign) claims in the text caption, misleading the retrieval and introducing noise. trieval performance while providing explanations, demonstrating its potential for interpretable multimodal retrieval. We hope this work can inspire future endeavors in explainable retrieval."
        },
        {
            "title": "Limitations",
            "content": "This work studies the explainable text-to-video retrieval upon LLM CoT reasoning. potential limitation is that the reasoning and the ranking highly depend on the capacity of the LLM. While modern LLMs demonstrate strong generalization ability, they may be less effective in domain-specific or highly noisy text-video data scenarios, such as very long video comprehension. Considering that this 2022b. Miles: Visual bert pre-training with injected language semantics for video-text retrieval. In ECCV. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embedding space to bind them all. In CVPR. Satya Krishna Gorti, Noël Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. 2022. X-pool: Cross-modal languagevideo attention for text-video retrieval. In CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. David Hunter. 2004. Mm algorithms for generalized bradley-terry models. The annals of statistics, 32(1):384406. Soyeong Jeong, Kangsan Kim, Jinheon Baek, and Videorag: RetrievalSung Ju Hwang. 2025. augmented generation over video corpus. arXiv preprint arXiv:2501.05874. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2024. Vlm2vec: for massive Training vision-language models arXiv preprint multimodal embedding tasks. arXiv:2410.05160. Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. 2022. Align and prompt: Video-and-language pre-training with entity prompts. In CVPR. Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. 2023. Unmasked teacher: Towards training-efficient video foundation models. In ICCV. Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. 2022. X-clip: End-toend multi-grained contrastive learning for video-text retrieval. In ACM international conference on multimedia. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2024. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In ICML. Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. 2015. dataset for movie description. In CVPR. Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, and Hilde Kuehne. 2024. Howtocaption: Prompting llms to transform video annotations at scale. In ECCV. Hongjin SU, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Liu Haisu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In ICLR. Guohao Sun, Yue Bai, Xueying Yang, Yi Fang, Yun Fu, and Zhiqiang Tao. 2024a. Aligning out-ofdistribution web images and caption semantics via evidential learning. In Proceedings of the ACM on Web Conference 2024, WWW 24, page 22712281, New York, NY, USA. Association for Computing Machinery. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2024. Video-LLaVA: Learning united visual representation by alignment before projection. In EMNLP. Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, and Zhiqiang Tao. 2024b. Stllava-med: Self-training large language and vision assistant for medical question-answering. In EMNLP. Ruyang Liu, Chen Li, Yixiao Ge, Thomas H. Li, Ying Shan, and Ge Li. 2024. Bt-adapter: Video conversation is feasible without video instruction tuning. In CVPR. Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, and Zhiqiang Tao. 2024c. Sq-llava: Selfquestioning for large vision-language assistant. In ECCV. Yikun Liu, Yajie Zhang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Xie. 2025. Lamra: Large multimodal model as your advanced retrieval assistant. In CVPR. Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2022. Clip4clip: An empirical study of clip for end to end video Neurocomput., clip retrieval and captioning. 508(C):293304. Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. 2024a. Text is mass: Modeling as stochastic embedding for text-video retrieval. In CVPR. Jiamian Wang, Pichao Wang, Dongfang Liu, Qiang Guan, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. 2024b. Diffusion-inspired truncated sampler for text-video retrieval. In NeurIPS. Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, YuGang Jiang, and Lu Yuan. 2022. Omnivl: One foundation model for image-language and video-language tasks. In NeurIPS. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, and 1 others. 2024c. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. 2024d. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, and 1 others. 2024e. Internvideo: General video foundation models via generative and discriminative learning. In ECCV. Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. 2023. Cap4video: What can In auxiliary captions do for text-video retrieval? CVPR. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msrvtt: large video description dataset for bridging video and language. In CVPR. Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. 2022. Clipvip: Adapting pre-trained image-text model to videolanguage representation alignment. arXiv preprint arXiv:2209.06430. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381. Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian, Ji Zhang, and Fei Huang. 2023. Hitea: Hierarchical temporal-aware video-language pre-training. In ICCV. Ziyun Zeng, Yixiao Ge, Zhan Tong, Xihui Liu, Shu-Tao Xia, and Ying Shan. 2023. Tvtsv2: Learning outof-the-box spatiotemporal visual representations at scale. arXiv preprint arXiv:2305.14173. Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. 2024. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In ICLR. Methods Struct. ann. w/ CLIP Struct. ann. w/ X-CoT R@1 R@5 R@10 MdR MnR 141.9 39.1 16.9 38.7 64.6 33.7 25.5 4.0 30.5 56.7 Table 4: Feeding structured video annotations to CLIP vs. using X-CoT on the MSR-VTT dataset. Annotation Type 20% noisy tags Complete annotations R@1 R@5 R@10 MdR MnR 49.1 62.0 32.3 38.7 64.6 33.7 53.9 56.7 4.0 4.0 Table 5: Effect of noisy structured annotations on X-CoT (MSR-VTT dataset)."
        },
        {
            "title": "A Similar Frame Filtering",
            "content": "To ensure diversity in the frame annotations, we use lightweight ResNet18 (He et al., 2016) model pretrained on ImageNet (Deng et al., 2009) to extract frame-level visual features. Each frame is resized, normalized, and passed through the network to obtain feature embedding, which is L2-normalized. We then compare the current frame to all previously retained frames using cosine similarity, and if the maximum similarity is below threshold (e.g., 0.95), the frame is kept. This process continues sequentially until the final set of non-duplicate frames is obtained, ensuring diversity and promoting frame-level annotation quality. Structured Video Annotations as Input: CLIP vs. X-CoT To test whether video annotations alone would suffice for CLIP, we use structured video annotations instead of the video embeddings and recompute cosine similarity with CLIP. As seen from Table 4, the performance drops compared to using X-CoT, suggesting that LLM reasoning is required to exploit long, verb-rich context."
        },
        {
            "title": "C Robustness to Noisy Annotations",
            "content": "To test the sensitivity of X-CoT to imperfect annotations, we perturb 20% of tags in the structured annotations and re-run X-CoT on MSR-VTT, as shown in Table 5. The proposed X-CoT experiences small performance decline in the noisy scenario, demonstrating the robustness to the annotation data quality. We also observe that the complete annotation gives improved performance, showing the effectiveness of the collected annotation data. Figure 8: Example of collected annotations."
        },
        {
            "title": "Annotation Examples",
            "content": "Fig. 8 and Fig. 9 show examples where structured video annotations provide more accurate scene descriptions than the original dataset captions. These cases reveal: 1. Semantic misalignment in GT labels as shown in Fig. 8 (e.g., labeling \"dancing on beach\" as \"singing\"). 2. Fine-grained object and action detection as shown in Fig. 9 (e.g., political figures identified by name, or scene attributes like \"joyful\" or \"heated\"). Such annotations serve as the foundation for XCoTs reasoning mechanism and improve the overall retrieval reliability."
        },
        {
            "title": "Annotations",
            "content": "We introduce proxy metric to assess the semantic faithfulness of the generated explanations. For each query in the MSR-VTT testing set, we record the top-1 video embedding vori obtained from VLM2Vec. We then apply X-CoT to produce re-ranked top-1 video embedding vxcot and the corresponding explanation embedding eexpl (both derived from VLM2Vec). We compute the similar- (query, vi, vi+1). Thus, although up to (K 1)P = 200 comparisons are possible, only 30-40 unique LLM calls are required on average, saving 85% of LLM calls. Global Aggregation. All newly observed winloss edges are converted to ability scores θk via BradleyTerry maximum-likelihood fit (weak Gaussian prior α = 103). Sorting θk in descending order yields the final ranking ˆV . In addition to the ranking, the individual explanations collected during each pairwise comparison are concatenated and summarized in final single-shot LLM call."
        },
        {
            "title": "G Efficiency and Scalability",
            "content": "In Table 6, we report the runtime and GPU memory cost under different hardware settings (e.g., number of NVIDIA RTX 3090 GPUs). As shown by Table 6, the runtime per query could be drastically reduced as we scale the number of GPUs, being comparable with the CLIP-based embedding model (X-Pool) and the MLLM-based embedding model (VLM2Vec). This enhances the feasibility of realworld deployment. The above speedup is achieved by substantial engineering endeavors, including sliding window, caching, odd-even parallelization, and GPU parallelization. Sliding Window and Caching. Since the embedding model already provides good initial ranking, our proposed method, which builds atop embedding models, only needs to perform small number of local swaps, rather than running total of K(K 1) = 380 LLM calls for top-20 (K = 20) candidates per query. We adopt sliding window strategy that compares only adjacent video pairs (e.g., (v1, v2), (v2, v3), ..., ) across multiple passes. Since many of the pairwise comparisons recur across the passes, we cache the pairwise results to avoid repetitive LLM calls. We empirically find that such strategy can reduce the total number of LLM calls per query by 90% on average (e.g., less than 40 LLM calls per query). Odd-Even Parallelization. In each sliding window pass, for = 20 there will be 19 adjacent pairs. We partition these pairs into odd (e.g., (v1, v2), (v3, v4), ..., (v19, v20)) and even (e.g., (v2, v3), (v4, v5), ..., (v18, v19)) groups, where both the odd and even groups consist of non-overlapping pairs. The comparisons within each group are executed in parallel via multi-threaded dispatch, thereby reducing the wall-clock latency of each pass. GPU Parallelization. For each query, multiFigure 9: Example of collected annotations. ity between two types of video embedding as: simbaseline = coseexpl, vori, simxcot = coseexpl, vxcot. (1) (2) Averaging these values across all queries yields simbaseline = 0.273 and simxcot = 0.350. The +0.077 gain demonstrates that the explanation embeddings align more strongly with the X-CoT reranked results compared to the baseline retrieval, indicating that explanations are semantically faithful to the systems final decision. To further guide future human-centered evaluation, established explanation-quality frameworks such as (Doshi-Velez and Kim, 2017) and (DeYoung et al., 2020) can be applied to assess interpretability and rationalization. X-CoT Pairwise Ranking Algorithm The pseudo-code for the pairwise ranking is provided in Algorithm 1. Given the coarse top-K list = [v1, . . . , vK] (we set K=20), X-CoT performs at most =10 sliding-window sweeps. During each sweep, the list is scanned from left to right; for every adjacent pair (vi, vi+1). An LLM receives the query plus two structured video descriptions and must reply with its choice and reason. If the answer favors vi+1, the two items are swapped. Complexity. In the best-case scenario, the number of pair-wise comparisons is (K 1), and in the worst case, (K 1). LRU Caching. The comparison routine is protected by an LRU cache keyed on the triple Algorithm 1: X-COT RANKING VIA PAIRWISE COMPARISONS Input: Text query q; Top-K candidate list = [v1, . . . , vK]; Number of passes = 10; Output: Sorted list ˆV; Pairwise explanation R; Final explanation E; 1 Initialize pairwise log [ ] 2 Initialize reason list [ ] // pairwise win log for BradleyTerry // natural-language reasons from the LLM // CompareLLM: takes query and pair of candidates, returns the closed match to the query and reason // ExplainLLM: summarizes the full set of pairwise reasons into final explanation 3 for 1 to do 5 6 7 8 for 1 to 1 do (w, r) COMPARELLM(q, V[i], V[i + 1]) // LLM returns winner and reason // Log result and explanation Append to R, and to if = V[i + 1] then Swap V[i] and V[i + 1] // If right candidate wins, swap positions 9 ˆV BRADLEY-TERRY AGGREGATE(L) 10 EXPLAINLLM(R) 11 return ( ˆV, E, R) Methods (#GPU) GPU Memory (GB) Runtime / query (s) X-CoT(1) X-CoT(2) X-CoT(4) X-CoT(8) X-CoT(32) X-Pool VLM2Vec 16.7 3. 33.4 1.8 64.0 0.9 130.2 0.45 535.0 0.10 4.0 0.11 16.6 0. Table 6: Runtime and memory profile of X-CoT with increasing GPU parallelism alongside embedding-based retrieval baselines. local open-source LLM (Qwen 2.5-7B-Instruct-1M) was used (no API cost)."
        },
        {
            "title": "Complete Benchmarking Results",
            "content": "We evaluate two zero-shot models, CLIP (Radford et al., 2021) and VLM2Vec (Jiang et al., 2024), alongside fine-tuned model, X-Pool (Gorti et al., 2022), to assess retrieval performance across diverse settings. The complete benchmarking results for MSR-VTT (Xu et al., 2016) and MSVD (Chen and Dolan, 2011) are presented in Table 7, and for DiDeMo (Anne Hendricks et al., 2017) and LSMDC (Rohrbach et al., 2015) are presented in Table 8. ple LLM calls (i.e., pairwise comparisons) are independent and can be parallelized. We leverage GPU-level concurrency to distribute the LLM calls across multiple devices. Together with the above engineering strategies, we reduce the latency as shown in Table 6. Since we adopt the open-source LLM (Qwen 2.5-7B-Instruct-1M) and the local hardware, no direct monetary cost is incurred. X-CoT Ranking Examples Fig. 10 illustrates how our method re-ranks candidate videos through pairwise reasoning and global aggregation. From the multiple pairwise judgments, culminating in the accurate re-ranking of video showing protester in Brazil speaking to reporter, precisely matching the query. Figure 10: Successful ranking with X-CoT on query about protest in Brazil. The top result is selected through stepwise pairwise comparisons, supported by natural language justifications. Methods ALPRO (Li et al., 2022) BridgeFormer (Ge et al., 2022a) MILES (Ge et al., 2022b) HiTeA (Ye et al., 2023) OmniVL (Wang et al., 2022) ImageBind (Girdhar et al., 2023) How2Cap (Shvetsova et al., 2024) TVTSv2 (Zeng et al., 2023) InternVideo (Wang et al., 2024e) BT-Adapter (Liu et al., 2024) ViCLIP (Wang et al., 2024d) LanguageBind (Zhu et al., 2024) LamRA (Liu et al., 2025) CLIP (Radford et al., 2021) X-CoT (ours) VLM2Vec (Jiang et al., 2024) X-CoT (ours) X-Pool (Gorti et al., 2022) X-CoT (ours) MSR-VTT MSVD R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR 24.1 26.0 26.1 29.9 34.6 36.8 37.6 38.2 40.7 40.9 42.4 42.6 44.7 31.6 33.7 36.4 37.2 46.9 47.3 20.8 20.5 12.8 12.6 9.3 9. 44.7 46.4 47.2 54.2 58.4 61.8 62.0 62.4 65.3 64.7 65.4 68.6 53.8 56.7 60.2 61.8 73.0 73.3 84.9 87.0 82.1 79.1 87.3 87.0 73.9 75.4 82.6 83.2 86.0 86.6 39.0 38.7 27.3 27.1 14.2 14.2 55.4 56.4 56.9 62.9 66.6 70.0 73.3 73.2 74.1 73.5 75.5 78.6 63.4 64.6 70.7 71.5 82.0 82.1 74.9 76.2 73.3 69.9 79.4 79.8 64.0 67.4 73.8 74.8 77.2 78.0 43.6 44.4 44.5 43.4 49.1 52.2 52.4 36.5 42.1 46.7 48.4 47.2 49. 8.0 7.0 7.0 3.0 3.0 2.0 4.0 4.0 3.0 3.0 2.0 2.0 2.0 2.0 2.0 3.0 2.0 2.0 2.0 2.0 2.0 Table 7: Complete Text-to-video retrieval performance comparison on MSR-VTT and MSVD. Methods ALPRO (Li et al., 2022) BridgeFormer (Ge et al., 2022a) MILES (Ge et al., 2022b) HiTeA (Ye et al., 2023) OmniVL (Wang et al., 2022) How2Cap (Shvetsova et al., 2024) TVTSv2 (Zeng et al., 2023) InternVideo (Wang et al., 2024e) BT-Adapter (Liu et al., 2024) ViCLIP (Wang et al., 2024d) LanguageBind (Zhu et al., 2024) CLIP (Radford et al., 2021) X-CoT (ours) VLM2Vec (Jiang et al., 2024) X-CoT (ours) X-Pool (Gorti et al., 2022) X-CoT (ours) DiDeMo LSMDC R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR 23.8 25.6 27.2 36.1 33.3 34.6 31.5 35.6 18.4 37.8 25.2 29.7 33.5 35.8 44.6 45.1 29.0 129.6 129.4 119.1 118.9 54.1 54.0 32.2 30.6 39.8 31.7 41.4 40.2 45.0 35.3 36.1 41.4 41.9 52.4 53.1 42.0 50.7 38.6 20.0 23.0 31.0 31.0 23.0 23.0 9.0 8.0 25.9 24.7 31.1 17.3 32.5 32.4 35.9 28.4 29.0 33.6 35.1 42.9 43. 47.3 50.6 50.3 60.1 58.7 61.9 57.6 61.9 63.2 49.4 52.1 57.7 59.2 72.5 73.1 49.7 49.2 34.1 33.9 15.1 15.0 57.9 61.1 63.6 70.3 68.5 71.5 68.2 72.6 73.4 59.0 60.6 68.4 68.8 81.0 81.8 12.2 11.1 15.5 17.3 17.6 19.5 20.1 15.9 17.6 18.2 18.9 23.6 23.8 6.0 5.0 5.0 3.0 3.0 6.0 5.0 4.0 3.0 2.0 2.0 Table 8: Complete Text-to-video retrieval performance comparison on DiDeMo and LSMDC."
        }
    ],
    "affiliations": [
        "DEVCOM Army Research Laboratory",
        "Rochester Institute of Technology"
    ]
}