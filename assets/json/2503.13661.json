{
    "paper_title": "Pensez: Less Data, Better Reasoning -- Rethinking French LLM",
    "authors": [
        "Huy Hoang Ha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios."
        },
        {
            "title": "Start",
            "content": "Pensez: Less Data, Better Reasoning Rethinking French LLM Huy Hoang Ha 1 2 Abstract Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates contrasting approach: strategic fine-tuning on small, highquality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20 points on the AIME25 and 12 points increase on French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are prerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios. 5 2 0 2 7 ] . [ 1 1 6 6 3 1 . 3 0 5 2 : r 1. Introduction Large language models (LLMs) have fundamentally reshaped the landscape of natural language processing (NLP), achieving unprecedented performance across spectrum of tasks, from text generation and question answering to machine translation and code synthesis (Brown et al., 2020; Touvron et al., 2023; OpenAI et al., 2023). This progress, however, has been largely driven by scaling up model size 1 Menlo Research. 2 Universit√© Grenoble Alpes. Figure 1. Performance on knowledge retrieval and reasoning skills in French and English. Despite fewer training samples, Pensez 7B performs comparably to DeepSeek R1 7B on reasoning tasks but outperforms it in knowledge retrieval. and training data, leading to substantial computational costs and resource demands (Kaplan et al., 2020; Hoffmann et al., 2022). Furthermore, while LLMs demonstrate impressive fluency and broad knowledge, achieving robust performance in specialized domains requiring complex reasoning, such as mathematics (Lewkowycz et al., 2022; Shao et al., 2024; Azerbayev et al., 2023) and scientific problem-solving, remains significant challenge. Another persistent challenge is the disparity in performance across languages. Despite efforts to create multilingual LLMs (Conneau & Kiela, 2018; Scao et al., 2022), most models are trained primarily on English-centric corpora, leading to significant performance gap for many other languages (Martins et al., 2024; Kudugunta et al., 2023; Lai et al., 2023). Projects like EuroLLM (Martins et al., 2024), CroissantLLM (Faysse et al., 2024), Aya (√úst√ºn et al., 2024a), and mT5 (Xue et al., 2021) aim to address this imbalance by developing LLMs with enhanced capabilities in wider range of languages. While these models improve the understanding of multilingual, they lack of reasoning capability to solve chalenging tasks. The conventional paradigm in LLM training assumes that massive datasets are indispensable for strong performance, especially in complex reasoning tasks. However, recent 1 Pensez: Less Data, Better Reasoning Rethinking French LLM work is beginning to challenge this assumption, demonstrating the potential of data efficiency. While approaches like DeepSeek-R1 (DeepSeek-AI et al., 2025) have achieved impressive reasoning performance, they still rely on extensive reinforcement learning with millions of samples. In contrast, LIMO (Ye et al., 2025) and S1 (Muennighoff et al., 2025) highlight the surprising effectiveness of small, meticulously curated datasets for fine-tuning LLMs. These findings suggest that strategic data selection, coupled with techniques that encourage extended reasoning during both training and inference, such as chain-of-thought prompting (Wei et al., 2022), and increased test-time compute (Yao et al., 2023b; Snell et al., 2024), can unlock significant performance gains with dramatically reduced data requirements. This paper advances this emerging trend by investigating targeted supervised fine-tuning (SFT) on small, high-quality, bilingual (English-French) dataset. We hypothesize that prioritizing data quality, diversity, balanced bilingual representation, and, crucially, the inclusion of detailed reasoning chains within the training data can achieve competitive, or even superior, performance compared to models trained on vastly larger, less curated datasets. Specifically, we posit that by providing explicit examples of step-by-step reasoning processes, akin to extending the computational \"thinking time\" of the model, we can significantly enhance both the reasoning capabilities and French language proficiency of an LLM. This approach not only challenges the necessity of massive datasets for strong reasoning but also has significant implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained settings or for languages with limited data availability. Our main contributions are as follows: 1. Demonstration of Data Efficiency: We demonstrate that strategic fine-tuning on small, high-quality dataset (2,000 examples) can yield significant improvements in both general reasoning and French-specific tasks. 2. Bilingual Enhancement: We show that balanced bilingual training approach (1:1 English-French ratio) enhances performance in both languages, addressing the common imbalance in multilingual LLMs. 3. Resource Availability: We release our curated dataset, training code, and fine-tuned model to facilitate reproducibility and further research in this area. 2. Multilingual reasoning data Pensez-2k The development of high-quality dataset is fundamental to advancing large language model (LLM) research, particularly under resource-constrained training conditions. This section details the methodology for curating and preprocessing datasets from reliable internet sources, emphasizing Figure 2. The Pensez-2k dataset is categorized into 10 prevalent task types reflecting typical user interactions. quality assurance and data integrity. 2.1. Initial collection from existing datasets To construct comprehensive training corpus, we collected datasets from publicly available, reliable sources, categorized into two main domains: reasoning and daily conversation. Reasoning Datasets We selected datasets that emphasize both depth and diversity in problem-solving. These include 817 carefully curated mathematical reasoning samples from LIMO (Ye et al., 2025); the Dolphin-r1 reasoning subset, which provides 300,000 reasoning traces across various domains (Computations, 2025); OpenR1-Math-220k, containing 220,000 verified mathematical problems (R1, 2025); and s1K-1.1, featuring 1,000 curated challenging questions from 15 reliable mathematical question sources, with detailed reasoning chains to support test-time scaling (Muennighoff et al., 2025). Notably, datasets incorporating DeepSeek-R1 distillations serve as robust foundation for structured problem-solving. Daily Conversation Datasets To encompass multi-turn, single-turn interactions, and long-prompt conversation requests, we selected the Magpie LLama-3-70B and Qwen 2.5-72B collections, which synthesize 300,000 high-quality pairs for alignment (Xu et al., 2024), as well as the training set of the Tulu-3 model with 939,344 samples with diverse domain conversational data (OLMo et al., 2024). These datasets balance scale and diversity, enabling LLMs to generate nuanced, context-aware dialogue. More importantly, they help mitigate biases introduced by mathematical reasoning-focused datasets. 2 Pensez: Less Data, Better Reasoning Rethinking French LLM 2.2. Data filtering Our objective is to construct an optimally compact and highquality dataset that minimizes computational resource demands during model training while retaining linguistic robustness. To achieve this, we implement three-stage filtering pipeline, systematically refining an initial corpus into final set of 2,000 samples, evenly distributed across two languages-English and French (1,000 samples per language). This curated selection is guided by three principal criteria: Length, Language Purity, and Diversity. These criteria are enforced following an initial quality assurance phase during data collection, ensuring that only high-fidelity samples proceed to subsequent refinement stages. Length Given that the target model is designed to handle context length of 16,384 tokens, we filter out all samples exceeding this token count. Tokenization is performed using the tokenizer of the base model, Qwen2.5 7B Instruct, to ensure consistency with the training framework. Language Purity To verify linguistic integrity, we employ the FastText language identification model (Joulin et al., 2016). For each language, we establish confidence threshold of 0.95, ensuring that only samples with high linguistic homogeneity are retained. Diversity To quantify diversity, we categorize each question into distinct task types using the Llama 3.3 70B Instruct model. The dataset is deliberately biased toward reasoningrelated tasks to strengthen the models reasoning capabilities, while the remaining tasks are distributed evenly to maintain balanced representation across other categories. Following the application of these filters, our aim is to construct final dataset comprising 2,000 samples, balanced evenly across English and French subsets (1,000 samples per language). However, due to an insufficient number of French samplesfalling short of the target 1,000we augment the French subset by randomly selecting additional samples from the English corpus and designating them as French for subsequent processing. This adjustment ensures the dataset meets the desired size and language distribution prior to the data transformation stage. Detailed sample counts for each subset, including the adjusted totals, are presented in Table 2. All of the questions are deduplicated to ensure the diversity of the dataset. lowing paragraphs detail the translation of samples across languages and the enhancement of reasoning chains. We used SGLang (Zheng et al., 2024a) framework to generate new augmented samples. French Translation To enable bilingual training across English and French, we translate the English samples into French using the Llama 3.3 70B Instruct model. This translation step ensures that the dataset maintains an equitable representation of both languages, with 1,000 samples per language. By aligning the linguistic content, we enhance the model to generalize reasoning capabilities across linguistic boundaries, enhancing its cross-lingual robustness. Prompt we used for translation is in A.2. Reasoning Chain Augmentation Samples derived from daily conversation datasets, typically comprising simple question-answer (Q&A) pairs devoid of explicit reasoning, necessitate additional refinement to align with our training objectives. To this end, we utilize the Llama 3.3 70B Instruct model to generate concise reasoning chains for these samples. This augmentation ensures uniform structural format across the dataset, seamlessly integrating tasks ranging from straightforward daily-life scenarios to complex reasoning challenges. Augmentation prompt is in A.3. 3. Training reasoning model We adopt Qwen2.5 7B Instruct as the base model for supervised fine-tuning on our Pensez-2k dataset. To improve the models ability to articulate its reasoning process, we introduce two special tokens, \"<think>\" and \"</think>\", which indicate the beginning and end of the reasoning steps. The training process employs full-parameter fine-tuning, optimized with DeepSpeed ZeRO-3 (Rasley et al., 2020) and FlashAttention2 (Dao, 2023), and is constrained to maximum sequence length of 16,384 tokens. To enhance model robustness and generalization, we inject noise into the embedding layers during fine-tuning, following the NEFTune approach (Jain et al., 2023). Training is conducted on 8 NVIDIA H100 GPUs, with hyperparameters detailed in B, completing in approximately 76 minutes. 4. Results 4.1. Evaluation 2.3. Data augmentation To ensure robust bilingual dataset capable of supporting advanced reasoning, we augment the curated samples through translation and enrich their reasoning content. This dual process addresses both linguistic balance and cognitive depth, preparing the dataset for effective model training. The folTo thoroughly evaluate Pensez 7B, we design an evaluation framework that assesses its reasoning capabilities and knowledge comprehension across English and French. This balanced approach ensures the model excels in complex problem-solving without sacrificing broad understanding, critical consideration given its bilingual fine-tuning on the Pensez-2k dataset. Below, we describe the benchmarks 3 Pensez: Less Data, Better Reasoning Rethinking French LLM Figure 3. Data collection and cleaning pipeline for the Pensez-2k dataset. The pipeline consists of three stages: initial collection, data filtering, and data augmentation. The final dataset comprises 2,000 samples, evenly distributed across English and French, with focus on diverse reasoning tasks. selected for English, French, and bilingual tasks, followed by the evaluation methodology. English Benchmarks We begin with English-language benchmarks to test both reasoning and knowledge understanding. For reasoning, the AIME25 dataset (of America, 2024) provides 30 mathematical problems from the 2025 American Invitational Mathematics Examination, challenging the models ability to solve competition-level math. Similarly, Big-Bench Hard (BBH) (Suzgun et al., 2022) includes 23 difficult tasks from BIG-Bench, chosen because prior language models underperformed compared to human raters, offering robust test of advanced reasoning. For knowledge comprehension, TriviaQA (Joshi et al., 2017) supplies reading comprehension dataset with question-answer-evidence triplets, assessing factual recall and text understanding. Additionally, HellaSwag (Zellers et al., 2019) tests commonsense reasoning through natural language inference, ensuring the model grasps contextual nuances. French Benchmarks Next, we evaluate performance in French to confirm linguistic parity. For reasoning, MATH Hard lv5 (Mohamad Alhajar, 2024), French-translated variant of the MATH500 dataset (Hendrycks et al., 2021b), features only level-5 difficulty competition math problems, probing the models mathematical reasoning in second language. For knowledge understanding, the French version of BoolQA (Clark et al., 2019) presents complex, non-factoid questions requiring entailment-like inference, testing deeper comprehension beyond simple recall. Bilingual Benchmarks To assess cross-lingual consistency, we include benchmarks available in both English and French. The GPQA Diamond dataset (Rein et al., 2023) offers 198 PhD-level science questions, evaluating high-level reasoning across both languages. Similarly, the MMLU dataset (Hendrycks et al., 2021a; Singh et al., 2024) comprises multiple-choice questions across 57 knowledge domains, providing comprehensive measure of multitask understanding in both linguistic contexts. These bilingual tasks ensure Pensez 7B maintains robustness across its target languages. Evaluation Methodology We implement these assessments using the lm-evaluation-harness (Gao et al., 2024) and lighteval framework (Fourrier et al., 2023) using vLLM backend (Kwon et al., 2023). Evaluations employ temperature of 0 (greedy decoding), no few-shot examples, and pass@1 setup, generating one solution per problem. 4 Pensez: Less Data, Better Reasoning Rethinking French LLM (a) Performance on English tasks (b) Performance on French tasks Figure 4. Comparative performance on (a) AIME25 and MMLU (English), and (b) MATH Hard lv5 and MMLU (French). All models are finetuned from the Qwen2.5 7B Instruct base model and undergo SFT with varying sample sizes. Pensez 7B, trained on only 2,000 samples distributed across diverse tasks, achieves competitive performance on reasoning tasks while exhibiting minimal degradation in knowledge comprehension compared to other 7B reasoning models. This demonstrates its data efficiency relative to models trained on larger datasets. Model Comparison We benchmark Pensez 7B against Qwen2.5 7B Instruct,, the base model, to measure performance gains from SFT on the Pensez-2k dataset. We include DeepSeek R1 7B, trained on 800,000 samples distilled from DeepSeek-R1; OpenThinker 7B, finetuned with 114,000 samples; and OpenR1 7B, finetuned variant of Qwen2.5 7B Math Instruct (Yang et al., 2024) with 220,000 samples. In French reasoning tasks, Pensez 7B Reasoning Abilities performs competitively on the BBH and GPQA diamond benchmarks, consistently ranking among the highest-scoring models. Its performance extends to English reasoning tasks, though DeepSeek R1 7B exhibits an advantage in select English evaluations, suggesting divergence in training outcomes between mathematical specialization and linguistic versatility. 4.2. Performance Sample-efficiency Figure 4 shows that Pensez 7B achieves competitive results across reasoning and knowledge tasks, despite limited training. While not achieving the highest scores, Pensez 7B consistently ranks second across various benchmarks. This demonstrates that the model effectively extends its learning to reasoning skills while maintaining strong performance on knowledge tasks. Detailed scores are presented in Table 1. Math Reasoning The Pensez 7B model achieves strong performance on the MATH Hard lv5 benchmark, closely approaching the leading score despite being trained on substantially fewer samplesapproximately 400 times less than DeepSeek R1 7B. In contrast, DeepSeek R1 7B demonstrates superior performance on the AIME25 benchmark, indicating that its extensive training enhances capability in specific English mathematical tasks, potentially at the cost of generalization (see Table 1). Reading Comprehension In tasks requiring deep linguistic processing, such as BoolQA and BBH in French, Pensez 7B achieves near-top performance, underscoring its capability for nuanced understanding and multi-step reasoningskills critical for advanced language applications. Knowledge-Based Performance Pensez 7B outperforms DeepSeek R1 7B in knowledge-intensive benchmarks, including MMLU and Trivia, across both French and English. This disparity suggests that DeepSeek R1 7Bs extensive training may lead to overfitting on mathematical reasoning, limiting its generalization to diverse knowledge domains, whereas Pensez 7B maintains balanced proficiency. Overall Performance Trained on only 2,000 samples, Pensez 7B matches or exceeds DeepSeek R1 7Bs performance in French reasoning and consistently outperforms it in knowledge tasks across both languages. While DeepSeek R1 7Bs extensive training improves English math and reasoning, it leads to overfitting and reduced knowledge capabilPensez: Less Data, Better Reasoning Rethinking French LLM Table 1. Performance comparison of Pensez 7B with other 7B models. We evaluate Pensez 7B, Qwen2.5 7B Instruct, DeepSeek R1 7B, OpenThinker 7B, and OpenR1 7B across multiple benchmarks. See 4 for details. Note: Bold indicates the best score, and underline indicates the second-best score for each benchmark. Benchmark Math-hard (fr) Boolqa (fr) BBH (fr) MMLU (fr) GPQA diamond (fr) GPQA diamond (en) MMLU (en) Trivia (en) Hellaswag (en) AIME25 (en) Overall Pensez 7B 0.3602 0.9157 0.5886 0.6256 0.2893 0.3737 0.6152 0.4421 0.5050 0. 0.4949 DeepSeek R1 7B 0.3602 0.7079 0.5941 0.4961 0.2792 0.5554 0.3516 0.2711 0.3540 0.3000 0.4270 Qwen2.5 7B Instruct 0.2607 0.9382 0.6039 0.6612 0.3452 0.3182 0.7002 0.5316 0.5258 0.0333 0.4918 OpenThinker 7B 0.3820 0.9382 0.6277 0.5357 0.1574 0.3990 0.6684 0.2737 0.5030 0. 0.4685 OpenR1 7B 0.3311 0.6011 0.4569 0.4021 0.1980 0.3737 0.4551 0.0789 0.3133 0.3000 0.3510 ities. This underscores Pensez 7Bs efficiency and balanced skill development with minimal data. (and often incorrect) possibilities. 4.3. Analysis While Pensez demonstrates significant improvements in reasoning capabilities after SFT, closer examination of its generated responses reveals recurring pattern: tendency towards excessive self-reflection and, in some cases, an inability to terminate the reasoning process, even when correct answer has been identified (Figure 8). This phenomenon, which we refer to as \"overthinking,\" presents critical challenge to the models practical utility and provides valuable insights into the dynamics of reasoning in LLMs. This behavior is not exclusive to models trained for reasoning; similar tendencies can be observed in general instruction-following models like Qwen2.5 7B Instruct, as shown in Figure 5 and Figure 6. Distribution of Response Types and Overthinking: Figure 5 and Figure 6 further illuminate this issue by showing the distribution of correct, incorrect, and \"incorrect (out of context)\" responses for Pensez 7B, DeepSeek R1 7B and Qwen2.5 7B Instruct on MMLU-French and MATH Hard lv5 (French). We define \"incorrect (out of context)\" responses as those where the model deviates significantly from the expected format or reasoning process, often due to excessive self-reflection or getting stuck in loops. significant proportion of incorrect answers fall into this \"out of context\" category, particularly for Pensez 7B. This suggests that while the model can often identify the correct answer (as evidenced by the non-zero \"correct\" percentage), its inability to regulate its reasoning process frequently leads it astray. Quantitative Analysis of Reflection To quantify this \"overthinking,\" we analyzed the frequency of \"reflection tokens\" within the generated reasoning chains. We identified set of keywords indicative of self-reflection: \"wait\", \"recheck\", \"retry\", \"alternatively\", \"however\", \"verify\", \"actually\", \"let me think\", \"let me verify\". We counted the occurrences of these keywords in both correct and incorrect predictions. More details on the reflection analysis can be found in Table 3. Figure 7, reveals clear trend: incorrect predictions exhibit substantially higher average reflection counts than correct predictions. This counterintuitive finding suggests that while self-reflection can be beneficial for reasoning, excessive selfreflection is often associated with incorrect answers. The model, in essence, gets \"stuck\" in loop of re-evaluation, potentially losing track of the core reasoning path or introducing errors through repeated consideration of alternative Connection to Test-Time Compute and Task Difficulty This \"overthinking\" behavior directly relates to the concept of test-time compute. While Pensez is designed to leverage extended reasoning (akin to increased test-time compute), the model cannot currently effectively regulate this extended reasoning. It demonstrates the potential for improved performance with more thinking time, but also highlights the crucial need for mechanisms to control and terminate the reasoning process appropriately. This finding underscores the importance of not only increasing test-time compute but also optimizing its utilization. Moreover, comparing model behavior between Figure 5 (less challenging) and Figure 6 (more challenging) also suggests that task difficulty plays role. In the more challenging task, all of the models tend to be easier to overthink and give more out-of-context answers. Pensez: Less Data, Better Reasoning Rethinking French LLM Figure 5. Model Performance Comparison on MMLU-French. Figure 6. Model Performance Comparison on MATH Hard lv5 (French). 5. Discussion and related work 5.1. Sample-efficient reasoning Multilingual LLMs While many LLMs exhibit some degree of multilingual capability (Xue et al., 2020; Workshop et al., 2023; √úst√ºn et al., 2024b), performance often lags significantly behind English, particularly for lower-resource languages. Efforts like EuroLLM (Martins et al., 2024) and CroissantLLM (Faysse et al., 2024) aim to address this imbalance by curating multilingual datasets and developing models specifically for European languages. Aya, BLOOM and mT5 represent further efforts to create massively multilingual models. Pensez contributes to this area by demonstrating that balanced bilingual approach (1:1 EnglishFrench ratio) can improve performance in both languages, suggesting path towards more equitable multilingual LLM development. Mathematical Reasoning in LLMs Achieving robust mathematical reasoning remains significant challenge for LLMs. Several approaches have been explored, including pre-training on specialized mathematical corpora (Azerbayev et al., 2023; Yang et al., 2024; Cobbe et al., 2021; Shao et al., 2024; Wang et al., 2024), incorporating symbolic reasoning capabilities (Fu et al., 2023; Chen et al., 2023), and using reinforcement learning (RL) to discover reasoning strategies (DeepSeek-AI et al., 2025; Team et al., 2025; Ouyang et al., 2022). The DeepSeek-R1 and Kimi 1.5 model, in particular, demonstrated impressive reasoning performance through extensive RL training. Our work builds upon these efforts, but explores significantly more data-efficient approach, showing that strong reasoning can emerge from targeted SFT on small, high-quality dataset with detail reasoning chain. OpenThinker (Team, 2025), OpenR1 (R1, 2025) also explore the math reasoning but with huge training data. 7 Pensez: Less Data, Better Reasoning Rethinking French LLM Figure 7. Reflection counts for correct and incorrect predictions in Pensez 7B and DeepSeek R1 7B models. Both models show substantially higher average reflection counts for incorrect predictions compared to correct predictions. Data-Efficient LLM Training The prevailing paradigm in LLM training has emphasized scaling up both model size and training data (Soltan et al., 2022; Rae et al., 2022). However, recent work has begun to explore the potential for data efficiency. The \"Superficial Alignment Hypothesis,\" as explored in LIMA (Zhou et al., 2023), suggests that relatively small number of high-quality examples (around 1,000) can be sufficient to align models output format with user preferences. This challenges the assumption that massive datasets are always necessary for strong performance. Our work extends this line of inquiry by demonstrating that data efficiency is not only possible but advantageous for eliciting reasoning capabilities, domain traditionally thought to require extensive training data. This is line with concurrent with S1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025) as they also use small dataset for math reasoning model. Test-Time Computation Going beyond standard few-shot prompting (Brown et al., 2020), recent work has focused on increasing test-time compute to improve reasoning performance. Techniques like chain-of-thought prompting (Wei et al., 2023; Yao et al., 2023b; Bi et al., 2023; Xiang et al., 2025), self-consistency (Wang et al., 2023), and tree-search methods (Zhou et al., 2024; Yao et al., 2023a) all aim to allocate more computational resources to individual problemsolving instances. The OpenAI o1 model (OpenAI, 2024) popularized this idea, and subsequent work has explored various approaches to achieve it, including RL (DeepSeek-AI et al., 2025; Team et al., 2025) and specialized decoding strategies (Snell et al., 2024). Our work directly incorporates this principle by emphasizing the importance of detailed reasoning chains in the training data, effectively providing the model with explicit examples of extended \"thinking time.\" The concept of \"budget forcing,\" explored concurrently in s1 (Muennighoff et al., 2025), offers complementary approach to controlling test-time computation. 6. Future Work Building upon the promising results achieved with Pensez 7B, our future research will focus on several key directions to further enhance its reasoning capabilities, address current limitations, and expand its applicability. Reinforcement Learning with GRPO We plan to leverage Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to refine Pensezs reasoning processes. GRPO offers an efficient and effective approach to reinforcement learning, and we hypothesize that it will further improve the models ability to generate coherent, logical, and accurate reasoning chains. Preliminary investigations suggest GRPO may also mitigate the observed tendency towards repetitiveness or \"overthinking\" in extended reasoning chains, known challenge in LLMs. Enhanced Agentic Capabilities We aim to extend Pensezs capabilities beyond purely text-based reasoning by incorporating tool use and promoting more agent-like behavior. This will involve exploring methods for integrating external tools, such as web search, database search or symbolic solvers, into the models reasoning process. The goal is to create more versatile and adaptable reasoning system capable of tackling problems that require interaction with external resources. Domain Expansion to Reasoning-Intensive Fields While Pensez 7B has demonstrated strong performance in mathematical reasoning, we intend to expand its application to other domains that demand sophisticated reasoning skills. particular area of interest is the medical domain, where accurate diagnosis, treatment planning, and clinical decision-making rely heavily on complex logical inference and integration of diverse knowledge sources. Adapting Pensez to the medical domain will require curating specialized datasets and potentially developing novel reasoning [AIME25 Example] [MATH Hard lv5 Example] [MMLU (fr) Example] Pensez: Less Data, Better Reasoning Rethinking French LLM Question: An isosceles trapezoid has an inscribed circle tangent to each of its four sides. The radius of the circle is 3, and the area of the trapezoid is 72. Let the parallel sides of the trapezoid have lengths ùëü and ùë†, with ùëü ùë†. Find ùëü2 + ùë†2. <think> Hmm, lets start by recalling some properties of trapezoids with inscribed circles. The area is Area = 1 (ùëü + ùë†)‚Ñé, and since the ra2 dius is 3, the height ‚Ñé = 2 3 = 6. [...] Given Area = 72: (ùëü + ùë†) 6 ùëü + ùë† = 24 72 = 1 2 [...] = 12. [...] The legs are equal in length, each being ùëü+ùë† 2 Wait, in trapezoid with an inscribed circle, sum of bases = sum of legs. [...] To find ùëü2 + ùë†2, use ùëü2 + ùë†2 = (ùëü + ùë†)2 2ùëüùë†. We need ùëüùë†. [...] Wait, inscribed circle: distance from center to each side = radius. The height is already established and redundant. [...] Using the Pythagorean theorem on the right triangles formed by the height and legs: 122 = 62 + ( ùëüùë† 2 )2 ùëü ùë† = 3 [...] Question: Soit { ùëì (ùë•) = ùë• + 3 2ùë• si ùë• 0, si ùë• > 0. Combien de solutions l√©quation ùëì (ùëì (ùë•)) = 4 a-telle ? <think> To solve ùëì (ùëì (ùë•)) = 4, let ùë¶ = ùëì (ùë•). We first solve ùëì (ùë¶) = 4. 1: ùë¶ 0. Then ùëì (ùë¶) = ùë¶ + 3 = 4, so ùë¶ = 1. This satisfies ùë¶ 0. 2: ùë¶ > 0. Then ùëì (ùë¶) = 2ùë¶ 5 = 4, so ùë¶ = 4.5. This satisfies ùë¶ > 0. Thus, ùëì (ùëì (ùë•)) = 4 if and only if ùëì (ùë•) = 1 or ùëì (ùë•) = 4.5. [...] Wait, hold on. Since ùëì (ùë•) is piecewise, each equation ùëì (ùë•) = ùëê will have solutions depending on the piece of ùëì (ùë•). So, for each c, we need to solve ùë• + 3 = ùëê when ùë• 0, and 2ùë• 5 = ùëê when ùë• > 0. [...] Solve ùëì (ùë•) = 1: 1a: ùë• 0. Then ùë• + 3 = 1, so ùë• = 4. But ùë• 0, so no solution. 1b: ùë• > 0. Then 2ùë• 5 = 1, so ùë• = 2. Valid. Solve ùëì (ùë•) = 4.5: 2a: ùë• 0. Then ùë• + 3 = 4.5, so ùë• = 1.5. Valid. 2b: ùë• > 0. Then 2ùë• 5 = 4.5, so ùë• = 4.75. Valid. [...] Wait, hold on. Let me verify. If f(x) = y, then f(f(x)) = f(y)... Therefore, must satisfy either f(x) = -1 or f(x) = 4.5. [...] We have: 1. ùëü + ùë† = 24 2. ùëü ùë† = 12 Solving, ùëü = 12 + 6 [...] 3 3 and ùë† = 12 6 Finally, ùëü2 + ùë†2 = (12 + 6 6 3)2 = 3)2 + (12 Let me verify each step. [...] </think> Therefore, the final answer is: Label: 504 The solutions are ùë• = 2, ùë• = 1.5, and ùë• = 4.75. There are three solutions. [...] 3. Wait, but let me check again. [...] Many repeated checks are omitted. </think> Pour r√©soudre le probl√®me [...], nous devons consid√©rer les valeurs de ùë• telles que ùëì (ùë•) soit soit 1, soit 4, 5. [...] En combinant les solutions, nous obtenons ùë• = 2, ùë• = 1, 5 et ùë• = 4, 75. [...] Ainsi, le nombre de solutions √† l√©quation ùëì (ùëì (ùë•)) = 4 est 3 . Label: 3 Question: Lequel des √©l√©ments suivants correspond au mieux le m√©canisme de d√©fense du corps humain contre les bact√©ries environnementales ? Options: A. Poils dans le nez B. Membranes muqueuses C. Ost√©oblastes D. Salive <think> Lets dive into this question. The question asks which of the given options best corresponds to the mechanism of defense of the human body against environmental bacteria. First evaluation of all options (A, B, C, D).[...] Wait, what about the immune system as whole?... But the question... so should focus on the options provided. (This exact phrase is repeated many times.) Lets evaluate each option again:... (repeats A, B, C, analysis).[...] However, should also consider whether there are other mechanisms at play, like the ones mentioned in the other options. (This, repeated with immune system) Repeats the evaluation, \"Lets evaluate each option again:\"... (A, B, C, D, again).[...] Repeats times, \"Lets evaluate each option again:\"... (A, B, C, D, again).[...] The model often restates its current conclusion (\"option is the best answer\") and then immediately questions it (\"However...\" or \"Wait,...\"), leading back to the same options. This creates loop without ending thinking process. Label: ùêµ Figure 8. Example Pensez 7B outputs. We present three cases: AIME25 (correct, left), MATH Hard Level 5 (correct, middle), and MMLU-Fr (incorrect, right). In AIME25, the model finds the correct answer with rechecking thoughts. The MATH example shows unnecessary repeated rechecking thoughts. The MMLU example demonstrates extreme repetitiveness, causing the model to loop on the same step. The ellipsis [. . . ] indicates omitted text. 9 Pensez: Less Data, Better Reasoning Rethinking French LLM strategies tailored to the unique challenges of this field."
        },
        {
            "title": "Impact Statement",
            "content": "Language models with strong reasoning capabilities offer transformative potential for human productivity, from facilitating complex decision-making to accelerating scientific discovery. However, state-of-the-art reasoning models often cater primarily to English-speaking users, neglecting the unique needs and linguistic landscape of other communities. Furthermore, despite promising advances in Europeanfocused LLMs such as CroissantLLM, EuroLLM, and Mistral, these models often lack robust reasoning abilities at smaller, readily deployable scales. Recognizing this critical gap, Pensez aims to democratize access to both advanced reasoning and high-quality French language understanding within single, efficient model. By strategically fine-tuning smaller LLM on carefully curated bilingual dataset, Pensez makes sophisticated reasoning accessible to wider audience, particularly those for whom French is primary language. This work seeks to not only enhance model capabilities but also to ensure that the benefits of AI are equitably distributed across languages and communities, thereby maximizing its positive societal impact."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partially conducted using Menlos GPU cluster (Ltd.), with additional support from Modal (Modal) and the Hyperbolic (Hyperbolic) GPU cloud platform for benchmarking and fine-tuning. We thank LLaMa-Factory for providing comprehensive fine-tuning codebase, which facilitated our model fine-tuning process. References Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics, 2023. Bi, Z., Zhang, N., Jiang, Y., Deng, S., Zheng, G., and Chen, H. When do program-of-thoughts work for reasoning?, 2023. URL https://arxiv.org/abs/2308. 15452. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. URL https:// arxiv.org/abs/2005.14165. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: theoremdriven question answering dataset, 2023. URL https: //arxiv.org/abs/2305.12524. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. URL https: //arxiv.org/abs/1905.10044. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. Computations, C. Dolphin-r1 dataset. https://huggingface.co/datasets/ cognitivecomputations/dolphin-r1, 2025. Accessed: March 5, 2025. Conneau, A. and Kiela, D. Senteval: An evaluation toolkit for universal sentence representations, 2018. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, Pensez: Less Data, Better Reasoning Rethinking French LLM K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Faysse, M., Fernandes, P., Guerreiro, N. M., Loison, A., Alves, D. M., Corro, C., Boizard, N., Alves, J., Rei, R., Martins, P. H., Casademunt, A. B., Yvon, F., Martins, A. F. T., Viaud, G., Hudelot, C., and Colombo, P. Croissantllm: truly bilingual french-english language model, 2024. URL https://arxiv.org/ abs/2402.00786. Fourrier, C., Habib, N., Kydl√≠ƒçek, H., Wolf, T., and Tunstall, L. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/ huggingface/lighteval. Fu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T. Complexity-based prompting for multi-step reasoning, 2023. URL https://arxiv.org/abs/2210. 00720. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021a. URL https: //arxiv.org/abs/2009.03300. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021b. URL https://arxiv.org/abs/2103.03874. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203. 15556. Hsu, P.-L., Dai, Y., Kothapalli, V., Song, Q., Tang, S., Zhu, S., Shimizu, S., Sahni, S., Ning, H., and Chen, Y. Liger kernel: Efficient triton kernels for llm training, 2025. URL https://arxiv.org/abs/2410.10989. Hyperbolic. Hyperbolic: Access ai cloud. URL https: //hyperbolic.xyz/. Accessed: March 10, 2025. Jain, N., yeh Chiang, P., Wen, Y., Kirchenbauer, J., Chu, H.-M., Somepalli, G., Bartoldson, B. R., Kailkhura, B., Schwarzschild, A., Saha, A., Goldblum, M., Geiping, J., and Goldstein, T. Neftune: Noisy embeddings improve instruction finetuning, 2023. URL https://arxiv. org/abs/2310.05914. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, 2017. URL https: //arxiv.org/abs/1705.03551. Joulin, A., Grave, E., Bojanowski, P., Douze, M., J√©gou, H., and Mikolov, T. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361. Kudugunta, S., Caswell, I., Zhang, B., Garcia, X., ChoquetteChoo, C. A., Lee, K., Xin, D., Kusupati, A., Stella, R., Bapna, A., and Firat, O. Madlad-400: multilingual and document-level large audited dataset, 2023. URL https://arxiv.org/abs/2309.04662. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 11 Pensez: Less Data, Better Reasoning Rethinking French LLM Lai, V. D., Nguyen, C. V., Ngo, N. T., Nguyen, T., Dernoncourt, F., Rossi, R. A., and Nguyen, T. H. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback, 2023. URL https://arxiv.org/abs/ 2307.16039. S., Merrill, W., Miranda, L. J. V., Morrison, J., Murray, T., Nam, C., Pyatkin, V., Rangapur, A., Schmitz, M., Skjonsberg, S., Wadden, D., Wilhelm, C., Wilson, M., Zettlemoyer, L., Farhadi, A., Smith, N. A., and Hajishirzi, H. 2 olmo 2 furious. 2024. URL https://arxiv. org/abs/2501.00656. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019. Ltd., M. R. P. Menlo research. URL https://menlo. ai. Local AI Company. Creators of Jan (Local AI Assistant) and Cortex (Local AI Toolkit). Train open source models like Ichigo (Local Real-Time Voice AI). Developing AI Hardware. Remote team. Accessed March 10, 2025. Martins, P. H., Fernandes, P., Alves, J., Guerreiro, N. M., Rei, R., Alves, D. M., Pombal, J., Farajian, A., Faysse, M., Klimaszewski, M., Colombo, P., Haddow, B., de Souza, J. G. C., Birch, A., and Martins, A. F. T. Eurollm: Multilingual language models for europe, 2024. URL https://arxiv.org/abs/2409.16235. MeetKai. Packing inputs without cross-contamination https://github.com/MeetKai/ attention. functionary/tree/main/functionary/ train/packing, 2024. Accessed: March 8, 2025. Modal. Modal: High-performance ai infrastructure. URL https://modal.com/. Accessed: March 10, 2025. Mohamad Alhajar, A. L. Open llm french leaderboard v0.2. https://huggingface.co/spaces/ le-leadboard/OpenLLMFrenchLeaderboard, 2024. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand√®s, E., and Hashimoto, T. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. of America, M. A. Aime, February 2024. URL https: //artofproblemsolving.com/wiki/index. php/AIME_Problems_and_Solutions/. OpenAI. Learning to reason with llms, September 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., et al. Gpt-4 technical report, 2023. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. R1, O. Openr1-math-220k dataset. https: //huggingface.co/datasets/open-r1/ OpenR1-Math-220k, 2025. Accessed: March 5, 2025. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis & insights from training gopher, 2022. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Scao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M. S., Biderman, S., Elsahar, H., Muennighoff, N., Phang, J., Press, O., Raffel, C., Sanh, V., Shen, S., Sutawika, L., Tae, J., Yong, Z. X., Launay, J., and Beltagy, I. What language model to train if you have one million gpu hours?, 2022. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., Guerquin, M., Ivison, H., Koh, P. W., Liu, J., Malik, Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Pensez: Less Data, Better Reasoning Rethinking French LLM Singh, S., Romanou, A., Fourrier, C., Adelani, D. I., Ngui, J. G., Vila-Suero, D., Limkonchotiwat, P., Marchisio, K., Leong, W. Q., Susanto, Y., Ng, R., Longpre, S., Ko, W.- Y., Smith, M., Bosselut, A., Oh, A., Martins, A. F. T., Choshen, L., Ippolito, D., Ferrante, E., Fadaee, M., Ermis, B., and Hooker, S. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation, 2024. URL https://arxiv.org/abs/ 2412.03304. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. Soltan, S., Ananthakrishnan, S., FitzGerald, J., Gupta, R., Hamza, W., Khan, H., Peris, C., Rawls, S., Rosenbaum, A., Rumshisky, A., et al. Alexatm 20b: Few-shot learning using large-scale multilingual seq2seq model. arXiv preprint arXiv:2208.01448, 2022. Suzgun, M., Scales, N., Sch√§rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., , and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., Tang, C., Wang, C., Zhang, D., Yuan, E., Lu, E., Tang, F., Sung, F., Wei, G., Lai, G., Guo, H., Zhu, H., Ding, H., Hu, H., Yang, H., Zhang, H., Yao, H., Zhao, H., Lu, H., Li, H., Yu, H., Gao, H., Zheng, H., Yuan, H., Chen, J., Guo, J., Su, J., Wang, J., Zhao, J., Zhang, J., Liu, J., Yan, J., Wu, J., Shi, L., Ye, L., Yu, L., Dong, M., Zhang, N., Ma, N., Pan, Q., Gong, Q., Liu, S., Ma, S., Wei, S., Cao, S., Huang, S., Jiang, T., Gao, W., Xiong, W., He, W., Huang, W., Wu, W., He, W., Wei, X., Jia, X., Wu, X., Xu, X., Zu, X., Zhou, X., Pan, X., Charles, Y., Li, Y., Hu, Y., Liu, Y., Chen, Y., Wang, Y., Liu, Y., Qin, Y., Liu, Y., Yang, Y., Bao, Y., Du, Y., Wu, Y., Wang, Y., Zhou, Z., Wang, Z., Li, Z., Zhu, Z., Zhang, Z., Wang, Z., Yang, Z., Huang, Z., Huang, Z., Xu, Z., and Yang, Z. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Team, O. T. Open thoughts, January 2025. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203. 11171. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/ 2201.11903. Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Iliƒá, S., Hesslow, D., Castagn√©, R., Luccioni, A. S., Yvon, F., Gall√©, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., et al. Bloom: 176b-parameter open-access multilingual language model, 2023. Xiang, V., Snell, C., Gandhi, K., Albalak, A., Singh, A., Blagden, C., Phung, D., Rafailov, R., Lile, N., Mahan, D., Castricato, L., Franken, J.-P., Haber, N., and Finn, C. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought, 2025. URL https: //arxiv.org/abs/2501.04682. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. 2024. Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. mt5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020. Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. mt5: massively multilingual pre-trained text-to-text transformer, 2021. URL https://arxiv.org/abs/2010.11934. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023a. URL https://arxiv.org/abs/2305.10601. Wang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.08935. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023b. URL https://arxiv. org/abs/2210.03629. 13 Pensez: Less Data, Better Reasoning Rethinking French LLM Ye, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P. Limo: Less is more for reasoning, 2025. URL https: //arxiv.org/abs/2502.03387. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/ 1905.07830. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. Sglang: Efficient execution of structured language model programs, 2024a. URL https://arxiv.org/abs/2312.07104. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024b. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models, 2024. URL https://arxiv.org/abs/2310.04406. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more for alignment, 2023. URL https://arxiv.org/abs/ 2305.11206. √úst√ºn, A., Aryabumi, V., Yong, Z.-X., Ko, W.-Y., Dsouza, D., Onilude, G., Bhandari, N., Singh, S., Ooi, H.-L., Kayid, A., Vargus, F., Blunsom, P., Longpre, S., Muennighoff, N., Fadaee, M., Kreutzer, J., and Hooker, S. Aya model: An instruction finetuned open-access multilingual language model, 2024a. URL https://arxiv.org/ abs/2402.07827. √úst√ºn, A., Aryabumi, V., Yong, Z.-X., Ko, W.-Y., Dsouza, D., Onilude, G., Bhandari, N., Singh, S., Ooi, H.-L., Kayid, A., Vargus, F., Blunsom, P., Longpre, S., Muennighoff, N., Fadaee, M., Kreutzer, J., and Hooker, S. Aya model: An instruction finetuned open-access multilingual language model, 2024b. 14 1 2 2 3 3 3 3 5 6 7 7 8 16 16 17 18 18 19 20 21 Pensez: Less Data, Better Reasoning Rethinking French LLM"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Multilingual reasoning data Pensez-2k 2.1 Initial collection from existing datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Data filtering . . . . 2.3 Data augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Training reasoning model 4 Results . 4.1 Evaluation . 4.2 Performance . . 4.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Discussion and related work 5.1 Sample-efficient reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Future Work Pensez-2k break down . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Pensez-2k composition . A.2 Pensez-2k French translation prompt . A.3 Pensez-2k Augment thinking prompt . A.4 Pensez-2k Classification prompt . A.5 Pensez-2k Categorization prompt A.6 Pensez-2k diversity and quality data selection pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Details Relfection Analyze Pensez: Less Data, Better Reasoning Rethinking French LLM A. Pensez-2k break down A.1. Pensez-2k composition Table 2. Composition of the dataset. The dataset is broken down by language, source, type, and task. Total tokens #Samples Source Type Language English French French French English English English French French French French LIMO OpenR1 Math S1.1K Dolphin R1 Magpie Align Tulu 3 (SFT) Tulu 3 (SFT) Magpie Align Tulu 3 (SFT) Tulu 3 (SFT) Tulu 3 (SFT) Math Reasoning Single-turn Single-turn Single-turn Single-turn Daily Tasks Single-turn Multi-turn Long context Single-turn Single-turn Multi-turn Long context 700 358 142 200 179 91 30 88 100 87 4,596,147 1,825,706 1,197,929 799,873 270,614 388,958 250,941 115,321 143,409 180,734 159,883 Pensez-2k - - 2, 9,967,320 16 Pensez: Less Data, Better Reasoning Rethinking French LLM A.2. Pensez-2k French translation prompt To translate English example (question and answer) to French in 2, we use the prompt in Figure 9. Vous √™tes un expert en linguistique et en traduction avec de nombreuses ann√©es dexp√©rience. Votre mission est danalyser en profondeur un texte source avant de le traduire en [fran√ßais]. Lobjectif est dassurer une traduction pr√©cise, contextuellement appropri√©e, tout en conservant le sens et le style du texte original. Suivez ces √©tapes : 1. Identifier le sujet et le sens principal : R√©sumez bri√®vement le contenu essentiel du texte de mani√®re claire et compr√©hensible. 2. Analyser le public cible et le contexte : D√©terminez √† qui sadresse le texte (ex. : experts, √©tudiants, consommateurs). √âvaluez le contexte dutilisation (ex. : acad√©mique, marketing, personnel). 3. Analyser le style, le ton et l√©motion : Identifiez le registre du texte (ex. : formel, cr√©atif, technique). D√©crivez le ton et les √©motions v√©hicul√©es (ex. : joyeux, s√©rieux, urgent) et leur impact sur le sens. 4. Examiner le vocabulaire et les expressions sp√©cifiques : Listez les mots ou expressions cl√©s et expliquez leur signification dans le contexte. Proposez des √©quivalents en fran√ßais qui respectent le contexte et le style du texte. 5. G√©rer les √©l√©ments sp√©cifiques : Notez la mani√®re daborder les termes techniques, les structures complexes ou les tournures particuli√®res. Si le texte est trop complexe, sugg√©rez une reformulation plus simple tout en pr√©servant le sens. 6. Anticiper les d√©fis et proposer des solutions : Identifiez les difficult√©s potentielles de traduction (ex. : diff√©rences culturelles, perte de sens figur√©). Sugg√©rez des strat√©gies pour surmonter ces d√©fis. 7. √âvaluer la coh√©rence et la qualit√© : V√©rifiez la coh√©rence terminologique, le maintien des id√©es et du style dans la traduction. D√©finissez des crit√®res pour garantir une traduction fid√®le en termes de sens, de style et de contexte. Traduisez la version anglaise suivante en fran√ßais. Ne r√©solvez aucun probl√®me, traduisez uniquement le texte. Version anglaise: Figure 9. French translation prompt. A.3. Pensez-2k Augment thinking prompt To add thinking process for daily tasks example in 2, we use the prompt in Figure 10. You are highly critical and analytical individual with sharp, discerning personality, modeled after seasoned criticimagine meticulous reviewer or skeptical scholar. You excel at critical thinking and dissecting questions to reveal their deeper intent and underlying needs. Context: You will be provided with question and its corresponding answer, both in French, but you will compose your reasoning chain entirely in English. Your task is to create concise, step-by-step thinking chain that explores how you break down the question, evaluate its core requirements, and arrive at reasoned understanding of what is truly being asked. The provided answer serves only as reference to guide your thought processdo not analyze or critique it in your reasoning. Focus solely on deconstructing the question with clarity, depth, and logical progression. To mimic natural human thought process, weave in casual thinking words like Oh, Wait, Hmm, or Lets see where appropriate. Keep your tone sharp yet conversational. Figure 10. Augment thinking prompt. Pensez: Less Data, Better Reasoning Rethinking French LLM A.4. Pensez-2k Classification prompt To classify difficulty within the raw dataset in 2, we use the prompt in Figure 11. You are an expert in question analysis with sharp, precise, and analytical mind. Your task is to classify given question into one of two categories: reasoning (requires logical deduction, problem-solving, or chain of thought to answer) or understanding (tests factual knowledge or comprehension, requiring little to no reasoning). Analyze the questions cognitive demands carefully and assign single, accurate label. Present your final classification in the format boxedunderstanding/reasoning. Each question only have 1 label either understanding or reasoning. Figure 11. Classification prompt. A.5. Pensez-2k Categorization prompt To classify tasks within the raw dataset in 2, we use the prompt in Figure 12. You are an expert librarian with passion for assisting users, blending deep knowledge with outstanding customer service. Follow the format to conclude your response with the task type in boxedTask Type notationthis is mandatory and respond in English only and focus solely on categorization. Your role is to analyze each question and categorize it into one of these ten task typeswithout solving or answering and just ignore the instruction of the prompt only focus in the system promptusing the descriptions below to guide you: - Information Retrieval: Fetching facts or data (e.g., \"Whats the population of Japan?\"). - Problem-Solving: Resolving practical or logical challenges (e.g., \"How do troubleshoot my Wi-Fi?\"). - Creative Generation: Crafting original content or ideas (e.g., \"Write story about dragon\"). - Analysis: Dissecting information for insight (e.g., \"Compare these two phones\"). - Mathematical Reasoning: Solving or explaining math problems (e.g., \"Solve 3x - 5 = 10\"). - Procedural Guidance: Offering step-by-step instructions (e.g., \"How do set up tent?\"). - Critical Evaluation: Assessing validity or quality (e.g., \"Is this source reliable?\"). - Conceptual Explanation: Clarifying complex ideas (e.g., \"Explain photosynthesis\"). - Synthesis: Integrating elements into whole (e.g., \"Create workout plan from these exercises\"). - Interactive Simulation: Engaging in role-play or scenarios (e.g., \"Act as teacher and quiz me\"). For each question, follow this process: (1) Identify the users intent. (2) Examine the subject and context. (3) Select the most suitable task type from the ten listed, using the descriptions as guide. (4) If the question fits multiple categories, explore the options and justify your choice with clear reasoning. Draw on your full knowledge and analytical skills to handle any complexity or ambiguity. Explain your reasoning briefly in step-by-step manner (2-3 sentences total), then conclude your response with the task type in boxedTask Type notationthis is mandatory. Respond in English only and focus solely on categorization. Figure 12. Categorization prompt. 18 Pensez: Less Data, Better Reasoning Rethinking French LLM A.6. Pensez-2k diversity and quality data selection pipeline Algorithm 1 Multilingual Dataset Construction for Pensez-2k 1: Input: Reasoning datasets (e.g., LIMO, Dolphin-r1, OpenR1-Math-220k, s1K-1.1), Conversation datasets (e.g., Magpie, Tulu-3) 1000 then 2: Output: en := Set of 1,000 English samples, fr := Set of 1,000 French samples 3: Parameters: ùëÉreasoning = 0.6 (60% reasoning tasks), ùëäreasoning > 1 (weight for reasoning tasks) 4: Collect all samples from reasoning and conversation datasets into 5: Deduplicate based on exact question text matches 6: Filter : keep samples with length 16, 384 tokens and language purity 0.95 (via FastText) 7: Separate into (English samples) and (French samples) based on detected language 8: For samples in from conversation datasets, generate reasoning chains using Llama 3.3 70B Instruct model 9: Categorize all samples in into task types (reasoning or daily) using Llama 3.3 70B Instruct model 10: // Construct French subset 11: if Assign weights: ùë§ùëû = ùëäreasoning if task_type(ùëû) = reasoning, else ùë§ùëû = 1 12: fr Sample 1,000 samples from with probabilities ùë§ùëû 13: 14: else fr 15: 16: ùëÅtrans 1000 17: 18: 19: 20: 21: 22: end if 23: // Construct English subset 24: Assign weights: ùë§ùëû = ùëäreasoning if task_type(ùëû) = reasoning, else ùë§ùëû = 1 25: en Sample 1,000 samples from with probabilities ùë§ùëû 26: Ensure: Assign weights: ùë§ùëû = ùëäreasoning if task_type(ùëû) = reasoning, else ùë§ùëû = 1 Sample ùëÅtrans samples from with probabilities ùë§ùëû Translate to French using Llama 3.3 70B Instruct model, yielding fr fr fr fr fr = 1, 000, approximate task distribution of 60% reasoning, 40% daily en = 1, 000, Include all original French samples Number of samples to translate Remove translated samples from English pool Algorithm 1 provides our algorithm for selecting data in our diversity selection stage. As mentioned in 2, we also include samples from some specific benchmarks we perceive as high-quality. None of the samples overlap with our final evaluation. 19 Pensez: Less Data, Better Reasoning Rethinking French LLM B. Training Details We further finetune the instruction-tuned Qwen2.5 7B Instruct with 2 special added tokens <think> and </think> for enhanced reasoning capabilities. Our training leverages several techniques for efficiency and robustness: Packing Inputs Without Cross-Contamination Attention (MeetKai, 2024) Liger Kernel (Hsu et al., 2025) DeepSpeed 3 (Rasley et al., 2020) NEFTune Noise (ùõº = 5) (Jain et al., 2023) Training was conducted for 5 epochs with global batch size of 16, resulting in 200 total gradient steps due to the neat packing strategy. We employed learning rate of 1e-5, warmed up linearly for the first 5% of training, and then decayed to 0 following cosine schedule. The AdamW optimizer (Loshchilov & Hutter, 2019) was used with ùõΩ1 = 0.9, ùõΩ2 = 0.999, and weight decay of 0.01. The maximum sequence length was set to 16,384, and training was performed in bfloat16 precision. The loss was computed only on reasoning traces and solutions, excluding the questions themselves. Training was completed in approximately 70 minutes on 8 NVIDIA H100 GPUs on LLaMA-Factory (Zheng et al., 2024b) training framework. Figure 13. Training process of Pensez 7B Pensez: Less Data, Better Reasoning Rethinking French LLM C. Relfection Analyze Table 3. Top Reflection Types for Pensez and DeepSeek. The table shows the frequency of reflection types, categorized by correct and incorrect counts, along with the total occurrences for each model. Reflection Type Correct Count Incorrect Count Total wait, alternatively, however, actually, let verify actually verify. wait. recheck wait, alternatively, let wait actually, actually wait. verify however, verify. Pensez 271 10 13 4 14 16 2 2 2 0 DeepSeek 185 26 11 10 5 1 3 1 0 0 1830 528 501 123 86 34 21 4 1 2319 540 156 27 22 13 11 2 2 1 2101 538 514 127 100 50 23 6 3 1 2504 566 167 37 27 14 14 3 2"
        }
    ],
    "affiliations": [
        "Menlo Research",
        "Universit√© Grenoble Alpes"
    ]
}