{
    "paper_title": "ParalESN: Enabling parallel information processing in Reservoir Computing",
    "authors": [
        "Matteo Pinna",
        "Giacomo Lagomarsini",
        "Andrea Ceni",
        "Claudio Gallicchio"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape."
        },
        {
            "title": "Start",
            "content": "ParalESN: Enabling parallel information processing in Reservoir Computing Matteo Pinna * 1 Giacomo Lagomarsini * 1 Andrea Ceni 1 Claudio Gallicchio 1 Abstract Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers promising, scalable, and principled pathway for integrating RC within the deep learning landscape. 6 2 0 2 9 ] . [ 1 6 9 2 2 2 . 1 0 6 2 : r 1. Introduction Reservoir Computing (RC) has emerged as simple yet powerful paradigm for harnessing the rich dynamics of recurrent systems for learning and prediction (Nakajima & Fischer, 2021; Lukoˇseviˇcius & Jaeger, 2009). By fixing *Equal contribution. 1Department of Computer Science, University of Pisa, Correspondence to: Matteo Pinna <matteo.pinna@di.unipi.it>, Giacomo Lagomarsini <giacomo.lagomarsini@phd.unipi.it>. Italy. Preprint. February 2, 2026. 1 non-linear recurrent reservoir and training only linear readout, RC offers favorable training efficiency, strong performance on temporal processing tasks, and intriguing connections to both neuroscience and dynamical systems theory. These properties have led to its success in wide range of domains, from speech recognition to chaotic signal prediction. Despite these strengths, RC faces the same problem of traditional, fully-trainable Recurrent Neural Networks (RNNs): the input signal has to be processed sequentially, which makes training slow and not parallelizable. In an attempt to improve the efficiency of RC, structured operators have been investigated (Rodan & Tino, 2011; Dong et al., 2020; DInverno & Dong, 2025). particularly active research area involves exploring RC implementations in hardware implementations (Gallicchio & Soriano, 2025). An important theoretical insight is that even linear reservoirs, provided that the readout is expressive enough, are universal approximators in the class of fading memory filters (Grigoryeva & Ortega, 2018a;b), thus being able to represent well arbitrary input-output dynamics. In this work, we address limitations of classical RC by introducing Parallel Echo State Networks (ParalESN), novel class of efficient untrained RNNs with diagonal linear recurrence in the complex space, where the recurrence can be parallelized via associative scan. Fig. 1 graphically highlights the proposed model. Our approach rethinks reservoir construction through the lens of structured operators. By combining the dynamical richness of RC with the linear recurrence from Linear Recurrent Unit (LRU) (Orvieto et al., 2023), we bridge gap between classical dynamical systems-inspired learning and contemporary large-scale modeling. The remainder of this work is organized as follows. Section 2 discusses related works. Section 3 presents the proposed approach, ParalESN. Section 4 presents our theoretical analysis. Section 5 presents the experiments. Finally, Section 6 concludes the paper. In the Appendix, we provide mathematical proofs, definitions, and additional experiments. See Appendix for table of contents. 2. Related works Reservoir Computing. Reservoir Computing (RC) (Verstraeten et al., 2007; Nakajima & Fischer, 2021) is popParalESN: Enabling parallel information processing in Reservoir Computing Figure 1. Architectural organization of the proposed ParalESN. The model may have multiple blocks consisting of two components: (i) linear reservoir and (ii) non-linear mixing layer. The first block processes the external input as in traditional shallow architecture. Subsequent blocks process the output of the previous blocks mixing layer, z(ℓ1) . The structure of the reservoir (in blue) includes diagonal, complex-valued transition matrix Λ(ℓ) in . The main branch and the temporal residual connections are scaled by positive coefficient τ and 1 τ , respectively. The mixing layer (in purple) is used to introduce non-linearity in the models dynamics and to mix the components of the reservoir states at each time step. The readout (in orange) is the only trainable component in the system. See Section 3 for details. and dense complex-valued input weight matrix W(ℓ) ular framework for the design of efficient untrained Recurrent Neural Networks (RNNs), developed to address the instabilities of training RNNs (Bengio et al., 1994; Glorot & Bengio, 2010; Pascanu et al., 2013). An RC model consists of two main components: (i) high-dimensional recurrent layer, called reservoir, that is randomly initialized and then left untrained, and (ii) trainable readout layer, that may be trained via lightweight closed-form solutions. Therefore, by design, RC models bypass backpropagation and related vanishing/exploding gradients, and can be trained exceptionally fast in single forward pass. Echo State Networks (ESNs) (Jaeger et al., 2007) established themselves as one of the most successful instance of RC models. ESNs dynamics can be defined as follows: ht = (1 τ )ht1 + τ σ(Whht1 + Winxt + b). (1) where ht RNh and xt RNin are, respectively, the state and the external input at time step t. The transition matrix is denoted as Wh RNhNh , the input weight matrix is denoted as Win RNhNin , RNh denotes the bias vector, σ denotes an element-wise applied non-linearity, and τ (0, 1] denotes the leaky rate hyperparameter. The entries of the matrix Win and are generally sampled randomly from uniform distribution (Gallicchio et al., 2017; Ceni & Gallicchio, 2024) over [ωin, ωin] and [ωb, ωb], respectively. Sampling their entries from Gaussian distributions is also popular (Verstraeten et al., 2007). The entries of Wh are randomly sampled from uniform distribution over [1, 1] and then rescaled to have desired spectral radius ρ1. In practical applications, the spectral radius is generally constrained to be smaller than 1. The final output is retrieved through the linear readout: yt = Woutht + bout. (2) where yt RNout denotes the network output at time step t, and Wout RNoutNh and bout RNout denote the readout weight matrix and bias vector, respectively. The readout is typically optimized via lightweight closed-form solutions, e.g. ridge regression or least squares methods. The models state update function defined in (1) could depend, in principle, from the initial point h0. This leads to non-deterministic behaviors, i.e. the impossibility to determine the state solely from the inputs. To avoid that, the reservoir in ESNs is initialized subject to the Echo State Property (ESP) (Yildiz et al., 2012), useful stability condition for guiding ESN initialization. We say that discrete time dynamical system defined by the transition function (h, x) satisfies the ESP if the states asymptotically depends only on the systems inputs. In other words, for any 1The spectral radius of matrix A, denoted ρ(A), is defined as the largest among the lengths of its eigenvalues. 2 ParalESN: Enabling parallel information processing in Reservoir Computing two initial points h0 and 0, then lim F (ht1, xt) (h t1, xt)2 = 0. (3) Several ESN variants lay their foundation at the intersecIn particular, Deep tion of the RC and DL frameworks. Echo State Networks (DeepESNs) (Gallicchio et al., 2017) generalize the concept of shallow ESNs towards deep architectural constructions, where multiple untrained reservoirs are stacked on top of each other. The increased feedforward depth has been shown to provide advantageous architectural bias relative to shallow ESNs. More recently, Residual Echo State Networks (ResESNs) (Ceni & Gallicchio, 2024) introduced orthogonal residual connections along the temporal dimension to enhance long-term information processing capabilities. Other works have explored structured transforms to improve the efficiency of RC, including Simple Cycle Reservoir (SCR) (Rodan & Tino, 2011), where the transition matrix employs fixed ring topology, and Structured Reservoir Computing (Structured RC) (Dong et al., 2020), where the transition matrix consists of composition of Hadamard and diagonal matrices. Universality of linear reservoirs. ESP guarantees the universality of the reservoir system in approximating fading memory filters (Grigoryeva & Ortega, 2018a). Provided that the ESP holds, similar universality results have also been proven for reservoirs characterized by linear dynamics, when combined with non-linear readouts that universally approximate functions : CNh CNy (Grigoryeva & Ortega, 2018b; Gonon & Ortega, 2020). Therefore, linear recurrence ESNs, in combination with non-linear readouts, can approximately arbitrarily well any time-invariant causal filter with the fading memory property. Transformers. Transformers are the de-facto standard architecture for sequence modeling, managing to replace recurrent models on real-world tasks since their introduction in (Vaswani et al., 2017). Differently from RNNs, transformers are feedforward architecture, and employ selfattention, enabling access to every position of the sequence at any time. This allows for great parallelization and modeling capacity, but comes at cost of quadratic complexity in sequence length, making scaling to long contexts challenging. To address this issue, many variants to the naive self-attention aiming to lower the computational and memory burden while maintaining expressivity have been proposed (Tay et al., 2022). State Space Models. Besides training instabilities, another major drawback of stateful sequence models like RNNs is that the input needs to be processed sequentially, greatly limiting the parallelization capabilities of these type of architectures on modern accelerators. One of the most promising approaches for the parallelization of recurrent models is that of (Deep) State Space Models (SSMs) (Gu et al., 2021). SSMs start from the idea of linear state space dynamical system, and devise initialization and discretization strategies that help improve memory capacity of the system. Linear recurrence is easily parallelizable (see e.g. Martin & Cundy, 2018), greatly improving training and inference efficiency of this family of models. Structured SSMs such as S4 and S5 (Gu et al., 2021; Smith et al., 2022) demonstrated how linear recurrence, along with careful initialization based on HiPPO matrices (Gu et al., 2020), can enhance long memory propagation on long sequence tasks. Building on these foundations, Mamba (Gu & Dao, 2024; Dao & Gu, 2024) proposes selective architecture that allows to change the dynamic based on its inputs. These advances position SSMs as contenders to transformers in sequence modeling, particularly in tasks demanding long memory retention. Linear recurrent unit. Linear recurrent Unit (LRU) (Orvieto et al., 2023) successfully applied linear recurrence to general RNNs, demonstrating that it is possible to deviate from the strict initialization and parametrization rules of SSMs, while retaining their impressive performance. Rather than initializing matrices using HiPPO theory as standard SSMs, LRU employs diagonal transition matrix, whose eigenvalues are initialized inside the unitary complex disk, using de-coupled parametrization of their magnitude and phase. In particular, the magnitude is chosen in an interval [rmin, rmax], which allows finer control over stability and memory capacity of the model. The linear, diagonal structure reduces recurrent updates to parallelizable element-wise operations, and the initialization strategy makes the architecture more stable for longer sequences. 3. ParalESN Inspired by the success of SSMs, we design more scalable and efficient reservoir architecture, which we call ParalESN. ParalESN improves upon previous RC models by employing linear diagonal recurrence, similar to that of LRU, followed by mixing function that combines reservoir states. As is standard in the RC paradigm, only the readout function is trainable. We also explore deep variant of ParalESN, which we denote ParalESN (deep). See Fig. 1 for graphical representation of the architecture. = xt denote the input to the first layer. The reserLet z(0) voir dynamics at layer ℓ are described by: =(1 τ (ℓ)) h(ℓ) h(ℓ) + τ (ℓ) (cid:16) Λ(ℓ) t1 h(ℓ) t1 + W(ℓ) in z(ℓ1) + b(ℓ)(cid:17) , (4) where superscript (ℓ) denotes layer-specific hyperparameters and weights. For each time step {1, . . . , }, xt RNin is the external input, h(ℓ) CNh is the reservoir state, and z(ℓ) RNh is the hidden state after the mixing 3 ParalESN: Enabling parallel information processing in Reservoir Computing Figure 2. (left) Time required to perform the recurrence in ParalESN and traditional ESNs for increasing sequence lengths, assuming 128 recurrent neurons and 5 layers for deep configurations. ParalESN scales logarithmically with sequence length, whereas traditional ESNs scale linearly. (right) Scaling ParalESN and traditional ESNs to high-dimensional reservoirs on the sMNIST task. Traditional ESNs run out-of-memory (OOM) at approximately 100K reservoir neurons, while ParalESN fit into memory due to the reduced memory footprint of their diagonal transition matrices. step. The matrix Λ(ℓ) CNhNh is diagonal transition matrix, b(ℓ) CNh is the bias vector, and τ (ℓ) (0, 1] is the leaky rate. Since the recurrence is linear, the leaky integration can be partially absorbed into the transition matrix. Accounting for leakage, the effective transition matrix becomes Λ(ℓ) = (1 τ (ℓ))I + τ (ℓ)Λ(ℓ) h, where is the identity matrix. The input weight matrix W(1) in CNhNin is dense for the first layer, mapping the external input to the hidden dimension. For subsequent layers, the input weight matrix W(ℓ>1) CNhNh employs the following ring topology (Rodan & Tino, 2011; Verzelli et al., 2020; Tino, 2020; Pinna et al., 2025) to reduce memory overhead: in W(ℓ>1) in = 0 0 w2 0 0 w3 ... ... 0 0 0 0 0 ... . . . wNh w1 0 0 ... 0 , (5) The matrix in (5) shifts the input vector and then applies an element-wise scaling. Consequently, we only store Nh-dimensional vector of scaling coefficients, significantly reducing the memory footprint in deeper layers. The entries of the input weight matrices are initialized by sampling the real and imaginary parts independently from uniform distribution over [1, 1] and then scaling each row of the matrix by (cid:112)1 λi2, where λi is the corresponding diagonal element (eigenvalue) of the transition matrix Λ(ℓ) h. The bias vectors are sampled from uniform distribution and scaled by hyperparameter ω(ℓ) . The diagonal elements of the transition matrix are initialized to control the eigenvalue distribution, with spectral radii sampled uniformly from [ρ(ℓ) min, ρ(ℓ) max], forming complex eigenvalues ρ(ℓ)eiθ(ℓ) max] and phases from [θ(ℓ) min, θ(ℓ) . The mixing function fmix introduces non-linearity into the model and enables interaction among the components of 4 the hidden state, which would otherwise evolve independently due to the diagonal structure of the recurrence. The mixing function is defined as: (cid:16) (cid:16) (cid:17)(cid:17) ℜ mix W(ℓ) + b(ℓ) mix h(ℓ) = (ℓ) z(ℓ) mix(h(ℓ) ) = tanh , (6) where denotes the convolution operator, W(ℓ) mix Ck is 1-D kernel of size k(ℓ), b(ℓ) mix is scalar bias, and ℜ() extracts the real part. The kernel slides across the hidden dimension of h(ℓ) CNh with same-padding, producing an output of identical dimension. We employ 1-D convolution rather than dense matrix, to reduce the memory footprint of the mixing function. The same kernel is shared across all time steps, requiring only + 1 parameters regardless of sequence length or hidden size. The kernel and bias entries are sampled randomly from uniform distribution over [-ω(ℓ) mixb], respectively. mix] and [-ω(ℓ) mixb, ω(ℓ) mix, ω(ℓ) The readout layer aggregates the mixed states from all layers: yt = freadout(z(1) , . . . , z(L) ) (7) For classification tasks, only the final time step is used, = freadout(z(1) , . . . , z(L) ). Fig. 2 graphically demonstrates ParalESNs speed advantage over traditional ESNs with respect to sequence length (left) and its ability to scale to high-dimensional reservoirs (right). ParalESNs recurrence time scales logarithmically with sequence length, rather than linearly, due to its ability to parallelize the recurrence. Note that even the deep configuration of ParalESN is consistently faster than the shallow configuration of ESN, despite consisting of higher number of reservoir layers. Additionally, by employing diagonal transition matrix, ParalESN can scale to higherdimensional reservoirs compared to traditional RC, which generally employs dense Nh Nh transition matrices. See Appendix for computational complexity analysis of ParalESN: Enabling parallel information processing in Reservoir Computing ParalESN and other RC approaches. ParalESN achieves lower time complexity, reducing it from linear to logarithmic with respect to sequence length, and the memory footprint of its parameters doesnt scale quadratically with respect to the hidden size (see Table 5). now show that an ParalESN is as expressive as ESN with linear recurrence and any arbitrary transition matrix Wh CNhNh and MLP readout. Proposition 4.2. Consider an ESN with linear recurrence and 1-layer MLP readout, defined by 4. Theoretical Analysis In this section, we derive simple condition for the ESP to hold. The ESP is necessary to prove result of universality for the class of linear reservoir models (Grigoryeva & Ortega, 2018a). We consider one-layer ParalESN. The sequence of hidden states produced by the model given sequence of inputs {x1, . . . , xT } (CNin )T and starting point h0 CNh is (y0, ..., yT ), with ht = (cid:40) h0 Λhht1 + τ (Winxt + b) if = 0 oth. (8) (9) yt = fout(ht), where fout is the readout function. Because the recurrence is linear, the readout function must be non-linear to preserve expressivity. This definition apparently differs slightly from equations (4), (6), and (7), but since training does not come into play in this section, we can incorporate fmix and freadout in single function fout. 4.1. Echo State Property As first step of the theoretical analysis, we show that it is easy to derive simple condition for the ESP to hold in the case of linear ESN of (8). The same condition on the spectral radius that is necessary for the ESP of ESNs (see, e.g., (Jaeger & Haas, 2004)) is also sufficient in our case. Moreover, in the case of diagonal transition matrix, we can directly control the spectral radius by examining the largest diagonal value in modulus. Theorem 4.1 (Sufficient and necessary conditions for the ESP). ParalESN has the ESP if and only if the diagonal elements λ1, ..., λNh of the transition matrix Λh are such that for each i, λi < 1, where is the complex modulus. The proof is given in Appendix C.1. 4.2. Expressivity of ParalESN An ESN with linear recurrence and MLP readout and the ESP property is universal in the family of fading memory filters, and in particular it is as powerful as non-linear ESNs, that have the same property (Grigoryeva & Ortega, 2018a;b) (see also Appendix for definitions on fading memory filters). We have stated the conditions for ParalESN to have the ESP property in Theorem 4.1. We (cid:40) ht = Whht1 + Winxt yt = Wout(σ(Whht)) = LP (ht) (10) where Wh, Win are matrices of appropriate sizes. Then, there exists ParalESN with MLP readout, defined by (8) and (9) with fout being another MLP, such that for any given input, the two models produce the same output. The proposition can be proven by diagonalizing the full matrix Wh. The proof is given in Appendix C.2. By Proposition 4.2 and results on equivalent expressiveness between ESNs with linear recurrence and standard ESNs, we can deduce that ParalESN and standard ESNs (both with the ESP) are equivalently expressive. We explicitly state this fact in the following corollary. Corollary 4.3. The class of ParalESN models with the ESP, endowed with an MLP readout, is universal in the family of fading memory filters. In practice, in our experiments, the MLP readout is often decomposed into two layers: the first layer, together with the non-linearity, is treated as the fixed mixing function, the last layer is treated as the trainable readout. 5. Experiments We empirically validate the performance of the proposed approach on time series regression and classification tasks (see also Appendix E), and with respect to traditional RC and fully-trainable sequence models. As our fully-trainable models, we consider LSTM, standard Transformer (Vaswani et al., 2017), Linear Recurrent Unit (LRU) (Orvieto et al., 2023) 2, and Mamba (Gu & Dao, 2024) 3 to cover wide range of model classes, including recurrent neural networks, attention-based neural networks, and deep state space models, respectively. See Appendix for details on the experimental setting and Appendix for experiments on hyperparameters sensitivity and comparison with other structured RC approaches. Comparison with respect to traditional RC. Fig. 3 compares ParalESN provides with respect to traditional RC from performance and efficiency perspectives. The left panels presents the trade-off between performance and efficiency, the right panel presents critical difference plot ranking models based on their overall performance. We 2Code from github.com/NicolasZucchet/minimal-LRU. 3Code from github.com/state-spaces/mamba. 5 ParalESN: Enabling parallel information processing in Reservoir Computing Figure 3. (left) Analysis of the trade-off between performance (error for regression tasks and accuracy for classification tasks) and efficiency (training time) for ParalESN and traditional ESNs across all considered benchmarks. For each model, we compute the percentage improvement over the ESN baseline for each task. The normalized scores are then obtained via min-max normalization of these average improvements, mapping them to [0, 1] scale, where 0 corresponds to the worst-performing model and 1 to the bestperforming one. Overall, ParalESN and ParalESN (deep) outperform their counterparts while being more efficient. (right) Critical difference plot computed via Wilcoxon test (Demˇsar, 2006), showing the average rank (lower is better). Models are ranked based on their overall performance across all benchmarks. Cliques (solid lines) connect models for which there is no statistically significant difference in performance. On average, ParalESN (deep) is the top-performing model. observe that ParalESN is both more accurate and more efficient with respect to its counterpart (shallow or deep). In particular, ParalESN (deep), despite consisting of multiple reservoir layers, is able to stay competitive in terms of recurrence speed with respect to shallow ESN. Additionally, ParalESN outperforms its shallow counterpart by statistically significant margin. Although there is not statistically significant difference between the performance of ParalESN (deep) and ESN (deep), the former is the topperforming model while being considerably more efficient. 5.1. Time series regression Memory-based tasks are designed to assess the models ability to recall delayed versions of the input, while forecasting tasks evaluate the models ability to predict future time steps. We consider MemCap (Jaeger, 2001), ctXOR (Verstraeten et al., 2010), and SinMem (Inubushi & Yoshimura, 2017) in our memory-based benchmark. For our forecasting benchmark, we consider Lorenz96 (Lorenz, 1996), Mackey-Glass (MG) (Jaeger & Haas, 2004), NARMA, and selection of real-world time series forecasting tasks from (Zhou et al., 2021), including ETTh1, ETTh2, ETTm1, and ETTm2. Discussion. Table 1 and Table 2 present the test set results on memory-based and forecasting tasks, respectively. Fig. 4 graphically compares the training time of ParalESN to that to traditional ESNs. Our experiments demonstrate that ParalESN can achieve comparable results to traditional RC across wide range of time series regression benchmarks, while offering exceptional advantages from computational efficiency perspective. In all benchmarks, ParalESN trains an entire order of magnitude faster, except for Lorenz25 and Lorenz50, where the relatively small sequence length reduces the advantage of being able to parallelize the recurrence. Observe that even ParalESN (deep), despite consisting of multiple reservoir layers, trains faster than traditional, shallow ESN consisting of just one layer. Indeed, while the readout layer is trained via Ridge regression in both cases, the time required to perform the recurrence through the untrained reservoir is significantly lower in the proposed approach compared to traditional ESNs, thanks to being able to process the sequence in parallel. 5.2. Time series classification The time series classification benchmark consists of selection of tasks from the UEA & UCR repository (Bagnall et al., 2018; Dau et al., 2019). For 1-D pixel-level classification, we consider two variations of the MNIST dataset (LeCun, 1998): (i) sequential MNIST (sMNIST), where pixels are flattened into one-dimensional vector, and (ii) permuted sequential MNIST (psMNIST), where on top of the flattening random permutation is applied to the pixels. Discussion. Tables 3 and 4 present test set results on time series classification tasks from the UEA & UCR repository and on sMNIST and psMNIST, respectively. Fig. 5 visualizes the trade-off between performance and efficiency among all models for the MNIST benchmarks. On time series classification, ParalESN achieves higher test accuracy compared to shallow ESN: +27.9% on Blink, +3.7% on FaultDetectionA, +7.6% on FordA, +5.7% on FordB, and ParalESN: Enabling parallel information processing in Reservoir Computing Table 1. Test set results of memory-based tasks, assuming 128 recurrent neurons for each model. The best result is highlighted in bold. MEMORY-BASED MEMCAP () 101 CTXOR5 () 101 CTXOR10 () 101 SINMEM10 () 101 SINMEM20 () ESN ESN (deep) ParalESN ParalESN (deep) 50.61.6 56.81.3 114.51.4 125.00.2 3.60.1 3.40. 3.90.1 3.60.1 7.70.6 5.21.0 8.20.2 5.60.4 3.60.1 1.20.1 3.70.0 1.00.2 3.70.1 1.60. 3.70.0 2.50.4 Table 2. Test set results of forecasting tasks, assuming 128 recurrent neurons for each model. The best result is highlighted in bold. FORECASTING ESN ESN (deep) ParalESN ParalESN (deep) 102 LZ25 () 10.00.3 9.70.2 10.40.5 10.30.3 102 LZ50 () 30.80.6 30.50.3 30.20.5 29.40.3 104 MG () 3.00.0 2.00.0 2.80.2 2.60.4 102 MG84 () 6.50.4 4.20.2 7.40.4 5.20.5 102 N10 () 2.70.4 3.00.5 3.70.8 4.51.0 102 N30 () 101 ETTH1 () 101 ETTH () 101 ETTM1 () 101 ETTM2 () 10.30.1 10.10. 10.20.1 10.20.2 9.10.2 8.90.1 9.00.2 8.80.1 13.01.7 9.60.5 12.81.2 9.70.9 6.70.1 6.60. 6.50.1 6.50.0 9.97.3 6.00.6 5.10.1 5.00.0 Figure 4. Training time comparison between ParalESNs and traditional ESNs for each memory-based and forecasting benchmark. ParalESN and ParalESN (deep) train order of magnitude faster. +3.3% on StarLightCurves. Similarly, ParalESN (deep) outperforms ESN (deep) by +10.0% on Blink, +8.8% on FaultDetectionA, +2.7% on FordA, +0.7% on FordB, and +2.3% on StarLightCurves. On sMNIST and psMNIST, ParalESN achieves higher test accuracy by +13.4% and +17.1%, respectively, compared to traditional ESN. ParalESN (deep) provides gains of +7.7% and +13.1% over ESN (deep). These performance improvements come alongside substantial efficiency gains, with ParalESN and ParalESN (deep) requiring half or less of the training time, CO2 emissions, and energy of their counterparts. Unlike traditional RC, ParalESN is competitive with LSTMs, Transformers, LRU, and Mamba, while delivering computational savings of orders of magnitude. 6. Conclusions We introduced ParalESN, novel framework for constructing high-dimensional, efficient, and parallelizable untrained RNNs based on linear diagonal recurrence. This work addresses fundamental limitations of traditional RC, including the necessity of processing data sequentially and the prohibitive memory footprint of high-dimensional reservoirs. Results across various time series and 1-D pixel-level classification benchmarks demonstrate that ParalESN is, on average, more accurate and faster than traditional RC, while remaining competitive with fully-trainable sequence models at fraction of their computational cost. 7 ParalESN: Enabling parallel information processing in Reservoir Computing Table 3. Test set results on time series classification tasks, assuming 1024 recurrent neurons for each model. The best result is highlighted in bold. MODEL ESN ESN (DEEP) PARALESN PARALESN (DEEP) BLINK FAULTDETECTIONA FORDA FORDB STARLIGHTCURVES 68.95.5 86.51.8 96.81.1 96.50.4 71.30.8 86.00.6 75.00.4 94.80.9 75.80.9 90.10.8 62.70.8 76.10. 83.40.7 92.80.5 68.41.0 76.81.6 92.10.7 93.80.4 95.40.4 96.10.3 Table 4. Test set results sMNIST and psMNIST tasks. The top-two results are underlined, the best result overall is highlighted in bold. MODEL PARAMS. ACCURACY TIME (MIN.) EMISSIONS (KG) ENERGY (KWH) SMNIST LSTM TRANSFORMER LRU MAMBA ESN ESN (DEEP) PARALESN PARALESN (DEEP) 160k 160k 160k 200k 160k 160k 160k 160k 97.51.4 98.40.1 98.50.2 98.40.1 82.57 91.41.1 96.21.3 97.20. 80.86.8 141.014.1 29.11.85 22.874.48 4.30.1 8.80.1 2.70.7 3.30.5 PSMNIST 0.340.14 0.600.28 0.180.02 0.190.02 0.020.00 0.040.00 0.010.00 0.020.00 1.020.42 1.810.86 0.570.05 0.570.06 0.070.00 0.130. 0.040.1 0.050.00 MODEL PARAMS. ACCURACY TIME (MIN.) EMISSIONS (KG) ENERGY (KWH) LSTM TRANSFORMER LRU MAMBA ESN ESN (DEEP) PARALESN PARALESN (DEEP) 160k 160k 160k 200k 160k 160k 160k 160k 92.80.5 97.40.2 97.80.1 92.60.1 78.21.6 82.13. 96.90.1 95.20.1 89.34.2 156.82.7 33.83.42 43.255.02 4.30.1 7.30.1 2.80.3 3.10.2 0.470.04 0.650.24 0.210.03 0.240.03 0.020.00 0.040.00 0.010.00 0.020.00 1.410.11 1.980.73 0.630.09 0.730.08 0.060.00 0.110. 0.040.00 0.050.00 Figure 5. Trade-off between performance (test accuracy) and efficiency (training time) for ParalESN, traditional RC, and fully-trainable sequence models, for the sMNIST and psMNIST benchmarks. For each model and benchmark, we compute the percentage improvement over the ESN baseline. The normalized scores are then obtained via min-max normalization of these average improvements, mapping them to [0, 1] scale with 0 representing to the worst-performing model and 1 the best-performing one. ParalESN is competitive with fullytrainable models while delivering substantial efficiency improvements. 8 ParalESN: Enabling parallel information processing in Reservoir Computing Acknowledgements This work has been supported by NEURONE, project funded by the European Union - Next Generation EU, M4C1 CUP I53D23003600006, under program PRIN 2022 (prj. code 20229JRTZA), and by EU-EIC EMERGE (Grant No. 101070918). Computational resources were provided by Computing@Unipi, computing service of the University of Pisa."
        },
        {
            "title": "References",
            "content": "Bagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., and Keogh, E. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. Bengio, Y., Simard, P., and Frasconi, P. Learning long-term IEEE dependencies with gradient descent is difficult. transactions on neural networks, 5(2):157166, 1994. Ceni, A. and Gallicchio, C. Residual echo state networks: Residual recurrent neural networks with stable dynamics and fast learning. Neurocomputing, 597:127966, 2024. Courty, B., Schmidt, V., Goyal-Kamal, MarionCoutarel, Feld, B., Lecourt, J., LiamConnell, SabAmine, inimaz, supatomic, Leval, M., Blanche, L., Cruveiller, A., ouminasara, Zhao, F., Joshi, A., Bogroff, A., Saboni, A., de Lavoreille, H., Laskaris, N., Abati, E., Blank, D., Wang, Z., Catovic, A., alencon, Stechly, M., Bauer, C., Lucas-Otavio, JPW, and MinervaBooks. mlco2/codecarbon: v2.4.1, 2024. URL https:// doi.org/10.5281/zenodo.11171501. Dao, T. and Gu, A. Transformers are ssms: generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Dau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A., and Keogh, E. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):12931305, 2019. Demˇsar, J. Statistical comparisons of classifiers over multiple data sets. Journal of Machine learning research, 7 (Jan):130, 2006. Dong, J., Ohana, R., Rafayelyan, M., and Krzakala, F. Reservoir computing meets recurrent kernels and structured transforms. Advances in Neural Information Processing Systems, 33:1678516796, 2020. DInverno, G. A. and Dong, J. Comparison of reservoir computing topologies using the recurrent kernel approach. Neurocomputing, 611:128679, 2025. Gallicchio, C. and Soriano, M. C. Hardware friendly deep reservoir computing. Neural Networks, pp. 108079, 2025. Gallicchio, C., Micheli, A., and Pedrelli, L. Deep reservoir computing: critical experimental analysis. Neurocomputing, 268:8799, 2017. Glorot, X. and Bengio, Y. Understanding the difficulty In Proof training deep feedforward neural networks. ceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249256. JMLR Workshop and Conference Proceedings, 2010. Gonon, L. and Ortega, J.-P. Reservoir computing universality with stochastic inputs. IEEE Transactions on Neural Networks and Learning Systems, 31(1):100112, 2020. doi: 10.1109/TNNLS.2019.2899649. Grigoryeva, L. and Ortega, J.-P. Neural Networks, Echo state net108: doi: are universal. 2018a. works 495508, ISSN 0893-6080. https://doi.org/10.1016/j.neunet.2018.08.025. https://www.sciencedirect.com/ URL science/article/pii/S089360801830251X. Grigoryeva, L. and Ortega, J.-P. Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems. Journal of Machine Learning Research, 19:140, 09 2018b. doi: 10.48550/arXiv.1712.00754. Gu, A. and Dao, T. Mamba: Linear-time sequence In First Conmodeling with selective state spaces. ference on Language Modeling, 2024. URL https: //openreview.net/forum?id=tEYskw1VY2. Gu, A., Dao, T., Ermon, S., Rudra, A., and Re, C. Hippo: recurrent memory with optimal polynomial projections. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Inubushi, M. and Yoshimura, K. Reservoir computing beyond memory-nonlinearity trade-off. Scientific reports, 7(1):10199, 2017. Jaeger, H. Short term memory in echo state networks. 2001. 9 ParalESN: Enabling parallel information processing in Reservoir Computing Jaeger, H. and Haas, H. Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. science, 304(5667):7880, 2004. Tino, P. Dynamical systems as temporal feature spaces. Journal of Machine Learning Research, 21(44):142, 2020. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. AtIn Advances in neural infortention is all you need. mation processing systems, pp. 59986008, 2017. URL http://arxiv.org/abs/1706.03762. Verstraeten, D., Schrauwen, B., dHaene, M., and Stroobandt, D. An experimental unification of reservoir computing methods. Neural networks, 20(3):391403, 2007. Verstraeten, D., Dambre, J., Dutoit, X., and Schrauwen, In The B. Memory versus non-linearity in reservoirs. 2010 international joint conference on neural networks (IJCNN), pp. 18. IEEE, 2010. Verzelli, P., Alippi, C., Livi, L., and Tino, P. Input representation in recurrent neural networks dynamics. arXiv preprint arXiv:2003.10585, 2020. Yildiz, I. B., Jaeger, H., and Kiebel, S. J. Re-visiting the echo state property. Neural networks, 35:19, 2012. Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 1110611115, 2021. Jaeger, H., Lukoˇseviˇcius, M., Popovici, D., and Siewert, U. Optimization and applications of echo state networks with leaky-integrator neurons. Neural networks, 20(3): 335352, 2007. LeCun, Y. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. Lorenz, E. N. Predictability: problem partly solved. In Proc. Seminar on predictability, volume 1, pp. 118. Reading, 1996. Lukoˇseviˇcius, M. and Jaeger, H. Reservoir computing approaches to recurrent neural network training. Computer science review, 3(3):127149, 2009. Martin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=HyUNwulC-. Nakajima, K. and Fischer, I. Reservoir computing. Springer, 2021. Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 2667026698. PMLR, 2023. Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 13101318, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. URL https://proceedings. mlr.press/v28/pascanu13.html. Pinna, M., Ceni, A., and Gallicchio, C. Residual reservoir memory networks. In 2025 International Joint Conference on Neural Networks (IJCNN), pp. 17. IEEE, 2025. Rodan, A. and Tino, P. Minimum complexity echo state network. IEEE Trans. Neural Netw., pp. 114, 01 2011. Smith, J. T., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: survey. ACM Comput. Surv., 55(6), December 2022. ISSN 0360-0300. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811. 10 ParalESN: Enabling parallel information processing in Reservoir Computing A. Table of contents for the Appendix The Appendix is organized as follows: Appendix provides computational complexity analysis. Appendix provides the papers main proofs. Appendix provides useful definitions about concepts used in our theoretical analysis. Appendix provides details on the considered datasets. Appendix discusses the experimental methodology. Appendix presents additional experiments, covering hyperparameters sensitivity and comparison between the proposed approach and other reservoir computing approaches based on structured transforms. B. Computational complexity Here, we compare the time and space complexity of ParalESN, traditional ESNs, and other RC approaches based on structured transforms, including SCR (Rodan & Tino, 2011) and Structured RC (Dong et al., 2020). For simplicity, we consider the case of shallow, single-layer architectures and omit the computational cost of training the readout, as this cost is the same for all models. Table 5 provides an overview of the computational complexity analysis. Regarding time complexity, the diagonal transition matrix of ParalESN allows for number of operations that scales linearly in the reservoir size Nh. Furthermore, the linear recurrence, which makes it possible to compute all time steps in parallel via associative scan, removes the linear dependency on the sequence length, reducing it to logarithmic. Regarding space complexity, ParalESN significantly reduces the memory footprint of its parameters compared to traditional ESNs by employing diagonal transition matrix, storing only Nh diagonal elements rather than full Nh Nh matrix. This is crucial in enabling the construction of larger reservoirs. ESN. We denote with {x1, . . . , xT } an input sequence of length , where each xi RNin . standard nonlinear ESN with Nh hidden units updates its reservoir state according to Equation 1. Each time step requires two matrix-vector multiplications: one with the transition matrix (Nh Nh) and one with the input weight matrix (Nh Nin). Thus, the time complexity for processing the entire sequence is O(cid:0)T (N 2 ), where the equality assumes Nin < Nh, as is usual in ESNs. Regarding space complexity, ESNs store full Nh Nh transition matrix, full Nh Nin input weight matrix, and an Nh-dimensional bias vector. The space complexity for the models parameters is O(Nh(Nh + Nin)). The space complexity for the recurrence is O(T Nh), as we need to store hidden states4. + NhNin)(cid:1) = O(T SCR. The transition matrix employs ring topology parameterization. The state update can be implemented in O(Nh) time, including the multiplication of the state by scaling factor ρ. The time complexity is the same as sequential ParalESN, O(T NhNin). Regarding space complexity, SCR only stores full Nh Nin input weight matrix and an Nhdimensional bias vector. The transformation applied by its transition matrix, arranged as ring topology, simply shifts the vector elements and does not need to be stored. The space complexity for the models parameters is O(NhNin). The space complexity for the recurrence is the same as in traditional ESNs, O(T Nh). Structured RC. The full transition matrix Wh in (1) is replaced by composition of Hadamard and diagonal matrices, reducing the time complexity by log factor in the reservoir size and achieving O(T Nh log Nh), assuming Nin < log Nh. If the Hadamard matrix is stored explicitly, the space complexity for the models parameters is the same as in traditional ESNs, O(Nh(Nh + Nin)), though each element can be stored using single bit rather than full floating-point precision since the matrix is binary. Alternatively, if the Hadamard transform is applied algorithmically via the fast Hadamard transform, no explicit storage is required, and only the diagonal matrices and input weights need to be stored, yielding space complexity of O(NhNin) for the models parameters. The space complexity for the recurrence is the same as in traditional ESNs, O(T Nh). ParalESN. In our approach, the reservoir update uses diagonal transition matrix, which reduces the per-step computation to O(NhNin). Hence, the sequential computation over length-T sequence has total cost O(T NhNin). Furthermore, 4Assuming the case where we are interested in all time steps. 11 ParalESN: Enabling parallel information processing in Reservoir Computing Table 5. Overview of time and space complexity. MODEL TIME COMPLEXITY SINGLE STEP WHOLE SEQUENCE O(N 2 ) ESN O(NhNin) SCR STRUCTURED RC O(Nh log Nh) O(T Nh log Nh) O(T 2 ) O(T NhNin) SPACE COMPLEXITY PARAMETERS RECURRENCE O(Nh(Nh + Nin)) O(T Nh) O(T Nh) O(NhNin) O(T Nh) O(NhNin) PARALESN O(NhNin) O(log (T )NhNin) O(NhNin) O(T Nh) because state updates are linear, the computation can be parallelized along the temporal dimension. Using parallel associative scan algorithm (Martin & Cundy, 2018), the dependence on is reduced to logarithmic factor5, yielding an overall complexity of (log(T ) NhNin). ParalESN stores the Nh elements of its diagonal transition matrix, NhNin input weight matrix, and an Nh-dimensional bias vector; thus, the space complexity for the models parameters is O(NhNin). The space complexity for the recurrence is the same as in traditional ESNs, O(T Nh). Note that the associative scan could in principle be applied to any linear recurrence RC model, but practical limitations remain. Because the algorithm involves repeatedly squaring the transition matrix, it would in general require 3 operations per scan step for full, unstructured matrices. Moreover, in the case of Structured RC, the matrix-matrix multiplication would create full matrix, destroying the Hadamard structure. C. Proofs C.1. Proof of Theorem 4.1 We first prove the sufficient condition. Let h0 and ρ( Λh) < 1 ϵ be the spectral radius of Λh. Because Λh is diagonal, it holds ρ( Λh) = Λh2. Then, 0 be two vectors in CNh , x1, . . . , xN be sequence of inputs, and hN 2 = ΛhhN 1 + τ (WinxN + b) Λhh 1 τ (WinxN + b)2 1)2 = Λh(hN 1 Λh2 (hN 1 = ρ( Λh) (hN 1 (1 ϵ) (hN 1 1)2 1)2 1)2 ... (1 ϵ)N h0 0 0. This proves the sufficient condition. To prove the necessary condition, suppose that some diagonal entry λi of Λh satisfies λi > 1. Then given two initial states h0, 0)i, unrolling the linear recursion we get the explicit formulas for hN and : 0 such that (h0)i = (h hN = ΛN h0 + = ΛN 0 + (cid:88) j=1 (cid:88) j=1 ΛN τ (Winxj + b) ΛN τ (Winxj + b) (11) (12) Subtracting the two terms, we obtain Focusing on component i, we obtain that (hN )i (h term never reaches 0, growing exponentially in magnitude. hN (h0 0). = ΛN )i = λN ((h0)i (h 0)i). Since (h (13) 0)i = (h0)i and λi > 1, this 5Assuming Θ(T / log ) parallel processors. 12 ParalESN: Enabling parallel information processing in Reservoir Computing C.2. Proof of Proposition 4.2 Every square matrix is diagonalizable in the complex field, up to arbitrarily small perturbations in its entries, so we can write Wh = VΛhV1, with CNhNh , and Λh diagonal matrix, having on the diagonal the eigenvalues of Wh. We can rewrite the recurrence in terms of and Λ as ht = VΛhV1ht1 + Winxt. (14) Pre-multiplying each side of (14) by V1, we get complex diagonal ESN having the equations (cid:40)ht = Λh yt = Wout(σ( Wh ht1 + Winxt ht)) where ht = V1ht, Win = V1Win, and Wh = WhV. Therefore, for any possible linear ESN and for any sequence of inputs, there exists an equivalent complex diagonal ESN that, when given the same inputs, produces the same outputs. D. Filters and Fading Memory Property Here, we provide broad overview of the definitions used to characterize the class of filters that ESNs and linear ESNs can universally approximate. Specifically, we introduce the concept of fading memory filters. We will mainly follow (Grigoryeva & Ortega, 2018a) and we point to it for further details. Filters. Informally, filter is function that takes semi-infinite sequences and returns (semi)infinite sequences. More precisely, given Nx and Ny N, we define filter as follows: : (RNx )Z (RNx )Z , (15) where (RN )Z is the set containing the semi-infinite sequences of vectors of dimension , that are indexed by negative integer numbers (0 included)6, = (vi)iZ with vi RN . Two useful properties we would like for filters are causality and time-invariance. filter is causal if its input at time t, yt = U(x )t only depends on the inputs up until time t, i.e. (,t]. Causality of the target function is an intuitively useful property in tasks such as forecasting or autoregressive generation: without this property, seeing the past inputs could not be sufficient to determine univocally the present output. filter is said to be time-invariant if, when given in input two shifted inputs, it outputs two sequences shifted by the same amount. Formally, for any positive integer τ > 0, we can define the time-shift operator Tτ as Tτ (x )t = tτ . filter is time-invariant if it commutes with the time-shift operator. This property is also useful, because it ensures that the filter definition does not depend explicitly on time t. Infinite norm. The space (RN )Z can be endowed with norm, giving us notion of distance. In particular, given the usual euclidean norm for vectors , we define the infinite norm of sequence as the supremum of the norm of any vector of the sequence. = sup iZ vi. distance between two sequences and can then be defined as the infinite norm of their difference: d(v , w) = w. Weighted norm: Intuitively, when measuring the difference between two sequences, often we would like to count more recent values more than ones distant in the past. Since infinite norm counts all values equally, this norm cannot achieve this important requirement. Therefore, we introduce weighting sequence = (w)i (0, 1]Z , such that limi wi = 0, and define the weighted norm as: w = v , where is the element-wise product. 6We could also define filters for infinite sequence, i.e. with inputs and outputs indexed by Z. However, since in practice we are mainly interested in the last point of the sequences, it is common to restrict the definition to semi-infinite sequences. 13 (16) (17) (18) ParalESN: Enabling parallel information processing in Reservoir Computing Fading memory property: Finally, we can give formal definition of fading memory property. The class of filters with fading memory property is the family of function that ESNs, and their linear counterparts, approximate arbitrarily well, thus making them universal. fading memory filter is, simply put, filter that is continuous when considering the distance induced by any weighted norm w: Definition D.1. filter : (RNx )Z (RNy )Z has the fading memory property if for any 1 (RNx )Z and ϵ > 0, it exists δ = δ(ϵ) > 0 such that if for any 2 (RNx )Z that satisfies 1 2w < δ, then U(x 1) U(x 2)w < ϵ. (19) ESNs have been proved universal in approximating arbitrarily well time-invariant, causal filters with the fading memory property (Grigoryeva & Ortega, 2018a) (Theorem 4.1). E. Datasets E.1. Memory-based MemCap. The MemCap task involves outputting delayed version of the input time steps in the past. The memory capacity score, used to assess performance on the MemCap task, is computed by summing the squared correlation coefficients between the k-th time step delayed version of the input and the output for each delay value = 1, . . . , 200. We generate an input sequence uniform in [0.8, 0.8] with length = 7000. The first 5000 time steps are used for training, the next 1000 for validation, and the final 1000 for testing. Both training and inference employ 100 time step washout to warm up the reservoir. ctXOR. Consider one-dimensional input time series x(t) uniformly distributed in (0.8, 0.8) and assume r(t d) = x(t 1)x(t d). The task is to output the time series y(t) = r(t d)2 sign(r(t d)), where is the delay and determines the strength of the non-linearity. We consider delay = 5 (ctXOR5) and delay = 10 (ctXOR10). SinMem. Given one-dimensional input time series x(t) uniformly distributed in (0.8, 0.8), the task is to output the time series y(t) = sin(πx(t d)). For SinMem, we consider delays = 10 (SinMem10) and = 20 (SinMem20). E.2. Forecasting Lorenz96. The Lorenz96 task is to predict the next state of the time series x(t), expressed as the following 5-dimensional chaotic system: x(t) = fi(t) = fi1(t)(fi+1(t) fi2(t)) fi(t) + 8, (20) for = 1, . . . , 5. In our experiments, we focus on predicting the 25-th (Lz25) and 50-th (Lz50) future state of the time series. Thus, the task involves predicting y(t) = x(t + 25) and y(t) = x(t + 50), respectively. We generate time series of length = 1200. The first 400 time steps are used for training, the next 400 for validation, and the final 400 for testing. Mackey-Glass. The Mackey-Glass 17 (MG) task is to predict the next state of the following time series: x(t) = (t) = 0.2f (t 17) 1 + (t 17)10 0.1f (t) . (21) In our experiments, we focus on predicting the 1-st and 84-th future state of the time series. Thus, the task involves predicting y(t) = x(t + 1) (MG) and y(t) = x(t + 84) (MG84), respectively. We generate time series of length = 10000, with the first 5000 time steps used for training, the next 2500 for validation, and the final 2500 for testing. NARMA. Given one-dimensional input time series x(t) uniformly distributed in [0, 0.5], the NARMA task is to predict the next state of the following time series: y(t) = 0.3y(t 1) + 0.01y(t 1) (cid:88) i= y(t i) + 1.5x(t d)x(t 1) + 0.1. (22) 14 ParalESN: Enabling parallel information processing in Reservoir Computing We will consider the NARMA10 (N10) and NARMA30 (N30), with look-ahead delay of = 10 and = 30, respectively. We generate time series of length = 10000, with the first 5000 time steps used for training, the next 2500 for validation, and the final 2500 for testing. Real-world datasets. The ETTh1, ETTh2, ETTm1, and ETTm2 (Zhou et al., 2021) represent real-world and challenging forecasting tasks. In Table 6 an overview of their characteristics. We focus on the multivariate case and predict all provided features. We use delay of 192 time steps. We normalize each feature in the training set independently to zero mean and unit variance. The normalization coefficients computed on the training data are then applied to normalize the validation and test data. Since we observe order-of-magnitude outliers in the training set, we clip the normalized training data to the interval (10, 10) across all datasets. The validation and test data remain unmodified. Table 6. Overview of the real-world forecasting datasets. Training, validation, and test sizes are measured in the number of time steps. DATASET TRAIN VALIDATION TEST # FEATURES ETTH1/2 ETTM1/ 8640 34560 2880 11520 2880 11520 7 7 E.3. Classification In Table 7 an overview of the considered classification datasets. The validation set is obtained via 90 10 stratified split for sMNIST and psMNIST, and via 70 30 stratified split for other tasks. No data augmentation is applied. Table 7. Overview of time series and 1-D pixel-level classification tasks. DATASET TRAIN TEST LENGTH # FEATURES # CLASSES BLINK FAULTDETECTIONA FORDA FORDB SMNIST/PSMNIST STARLIGHTCURVES 500 10912 3601 3636 60000 1000 450 2728 1320 810 10000 8236 510 5120 500 500 784 1024 4 1 1 1 1 2 3 2 2 10 3 F. Experimental setting F.1. Computational resources Experiments are run on system with 4 18-core Intel Xeon Gold 6140M CPUs @ 2.30 GHz (144 threads total) and 4 NVIDIA Tesla V100-PCIE-16GB GPUs. To track computational efficiency metrics, such as training time, energy consumption, and emissions, we use CodeCarbon (Courty et al., 2024). F.2. Model selection Model selection is carried out via Bayesian search, exploring configurations up to maximum runtime of 24hrs. The best configuration is chosen based on the performance achieved on the validation set. For RC models, Table 8 provides an overview of the values explored. Common hyperparameters explored for both ESN and ParalESN include the bias scaling ωb, the leaky rate τ , and the number of layers L. For traditional RC we explore the spectral radius ρ, used to rescale the recurrent weight matrix, and the input scaling ωin, used to scale the input weight matrix. For ParalESN, we explore the minimum and maximum magnitude of the diagonal entries ρmin, ρmax, and the minimum and maximum phase (i.e., the angle with the positive x-axis) θmin, θmax, used to sample the diagonal transition matrix 15 Table 8. Hyperparameters for RC models. HYPERPARAMETERS VALUES concat ωb and inter-ωb τ and inter-τ (ESN) ωin and inter-ωx ρ and inter-ρ (ParalESN) (Reservoir) ρmax and inter-ρmax ρmin and inter-ρmin θmax and inter-θmax θmin and inter-θmin (Mixer) ωmix and inter-ωmix ωmixb and inter-ωmixb and inter-k [False, True] [1, 2, 3, 4, 5] {0, 0.01, 0.1, 1, 10} {0.1, 0.5, 0.9, 1} {0.01, 0.1, 1, 10} {0.1, 0.5, 0.9} {0.1, 0.5, 0.9} {0, 0.1, 0.5, 0.9} { 1 2 π, π, 2π} {0, 1 2 π, π, 2π} {0.01, 0.1, 1, 10} {0, 0.01, 0.1, 1, 10} {3, 5, 7, 9} ParalESN: Enabling parallel information processing in Reservoir Computing and easily control its eigenvalues. Additionally, we explore ωmix and ωmixb for scaling the kernel and the bias vector of the mixing layer, and for determining the kernel size. For the deep configurations, we explore the number of (untrained) reservoir layers and we explore an additional set of hyperparameters for layers beyond the first one, to promote diverse dynamics between the first layer and deeper ones. Additionally, we consider hyperparameter concat [False, True], which determines whether the readout is fed the states of the last layer or the concatenation of the states across all layers. To ensure consistent number of trainable parameters when states are concatenated, the hidden size (i.e., the total number of reservoir units) is evenly split across layers. If an even split is not possible, the remaining neurons are allocated to the first layer. When the readout is implemented as ridge regressor, we explore regularization strength values in {0, 0.01, 0.1, 1, 10, 100}. When the readout is implemented as an MLP, we simply train it with the Adam optimizer with learning rate of 5e4; the rest are the default PyTorch values. See Table 8 for an overview of the values explored. The readout is either implemented as ridge regressor trained via Singular Value Decomposition (SVD) solver for memory-based, forecasting, and time series classification tasks, or as 2-layer Multi-layer Perceptron (MLP) for benchmarks pertaining to the MNIST dataset. Prior to the readout, we always standardize the reservoirs outputs by removing the mean and scaling to unit variance. For fully-trainable models, we employ the Adam optimizer and sweep through the learning rate with log uniform distribution over the interval [0.0001, 0.01], and the weight decay exploring values in {0, 1e 7, 1e 6, 1e 5, 1e 4, 1e 3, 1e 2, 1e 1}. For LRU, we also sweep through ρmin with uniform distribution over the interval [0, 1], ρmax with uniform distribution over the interval [0.8, 1], and θmax with uniform distribution over the interval [0.001, 3.14]. The LSTM employs hidden size of 192, the Transformer employs an input size of 96 and feedforward size of 128 for 3 encoder layers, LRU employs 3 layers with hidden size of 82, Mamba employs 6 layers with model dimension 64, state expansion factor 16, and local convolutional width 4. We train fully-trainable models and the MLP of ParalESN, for at most 100 epochs and with an early-stopping mechanism, to avoid inflating the computational efficiency metrics. For classification, the readout or classification head of each model are fed only the state at the last time step. G. Additional experiments G.1. Hyperparameters sensitivity In Fig. 6, we explore ParalESNs hyperparameter sensitivity on the NARMA10 benchmark. For simplicity, we consider shallow, single-layer ParalESN. For the reservoir layer, the spectral radius, phase, and leaky rate appear to be critical hyperparameters, with significant error increases when these are not tuned properly. In contrast, minor effects are observed for the bias scaling. For the mixing layer, the most important hyperparameter is the input scaling of the kernel, while its bias scaling and kernel size have relatively smaller effects on performance. (a) Reservoir hyperparameters (b) Mixer hyperparameters Figure 6. Analysis of ParalESNs hypeparameter sensitivity for (a) the reservoir and (b) the mixer, on the NARMA10 tasks. 16 ParalESN: Enabling parallel information processing in Reservoir Computing Table 9. Comparison of structured reservoir computing approaches on time series forecasting, assuming 128 recurrent neurons for each model. traditional ESN is included as baseline. Top-two results are underlined, best result is in bold. FORECASTING ESN SCR Structured RC 102 LZ25 () 10.00.3 22.10.4 10.40.2 102 LZ50 () 30.80.6 35.60.8 30.90. 104 MG () 3.00.0 18.74.2 12.60.1 102 MG84 () 6.50.4 32.23.1 28.51.6 102 N10 () 2.70.4 11.10.2 18.70. 102 N30 () 10.30.1 16.10.9 20.90.1 ParalESN 10.40.5 30.20.5 2.80. 7.40.4 3.70.8 10.20.1 G.2. Comparison with structured reservoir computing Here, we compare ParalESN with other reservoir computing approaches based on structured transforms on time series forecasting benchmarks. We consider Simple Cycle Reservoir (SCR) (Rodan & Tino, 2011) and Structured Reservoir Computing (Structured RC) (Dong et al., 2020)7. For both SRC and Structured RC, model selection employs the same methodology described in Section F. Note that in SCR, there is no need to tune the spectral radius since the transition matrix employs fixed ring topology. In Structured RC, rather than tuning the spectral radius of the transition matrix, we tune the reservoir scaling for values in [0.01, 0.1, 1, 10]. Although traditional ESNs are not based on structured transforms, we include their results as baseline. Table 9 presents results. For simplicity, we consider shallow configuration of one layer for all models. ParalESN is consistently one of the top-performing models along with ESN, while other approaches based on structured transforms In particular, compared to SCR and Structured RC, ParalESN achieves consistently underperform across all datasets. lower test error on MG, MG84, and N10 by an entire order of magnitude and half the error on N30. 7Code from github.com/rubenohana/Reservoir-computing-kernels."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Pisa"
    ]
}