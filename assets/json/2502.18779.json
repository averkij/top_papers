{
    "paper_title": "Towards Optimal Multi-draft Speculative Decoding",
    "authors": [
        "Zhengmian Hu",
        "Tong Zheng",
        "Vignesh Viswanathan",
        "Ziyi Chen",
        "Ryan A. Rossi",
        "Yihan Wu",
        "Dinesh Manocha",
        "Heng Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 7 7 8 1 . 2 0 5 2 : r Published as conference paper at ICLR 2025 TOWARDS OPTIMAL MULTI-DRAFT SPECULATIVE DECODING Zhengmian Hu1,2,, Tong Zheng1,, Vignesh Viswanathan2, 3, Ziyi Chen1, Ryan A. Rossi2, Yihan Wu1, Dinesh Manocha1,4, Heng Huang1 1Department of Computer Science, University of Maryland, College Park, MD, USA 2Adobe Research, San Jose, CA, USA 3Manning College of Information & Computer Sciences, University of Massachusetts Amherst, MA, USA 4Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efﬁciency bottleneck. Multi-Draft Speculative Decoding (MDSD) is recent approach where, when generating each token, small draft model generates multiple drafts, and the target LLM veriﬁes them in parallel, ensuring that the ﬁnal output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the veriﬁcation algorithm. For ﬁxed draft sampling method, the optimal acceptance rate is solution to an optimal transport problem, but the complexity of this problem makes it difﬁcult to solve for the optimal acceptance rate and measure the gap between existing veriﬁcation algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing way to efﬁciently compute the optimal acceptance rate. For the ﬁrst time, we measure the theoretical upper bound of MDSD efﬁciency for vocabulary sizes in the thousands and quantify the gap between existing veriﬁcation algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly inﬂuences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing veriﬁcation algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our ﬁndings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of veriﬁcation algorithms that closely match the theoretical upper bound."
        },
        {
            "title": "INTRODUCTION",
            "content": "Autoregressive language models have achieved state-of-the-art results in various language tasks (Brown et al., 2020; Touvron et al., 2023), including chatbots (Luo et al., 2022) and code generation (Chen et al., 2021). These models generate outputs by predicting the next token sequentially. However, this autoregressive decoding process leads to signiﬁcant computational resource requirements and high latency, posing challenges for user experience and limiting potential applications. Speculative decoding (Leviathan et al., 2023; Chen et al., 2023a) has been proposed to address the high inference cost issue. The method uses small, fast draft model to generate candidate results, which are then veriﬁed and corrected by large, accurate target model to maintain the model output distribution. Compared to other acceleration methods, such as knowledge distillation, model quantization, and model pruning, speculative decoding has the advantage of signiﬁcantly reducing inference latency without sacriﬁcing quality of the generated content. *Co-ﬁrst authors. 1 Published as conference paper at ICLR 2025 Multi-Draft Speculative Decoding (MDSD) (Miao et al., 2024; Cai et al., 2024; Li et al., 2024; Spector & Re, 2023) is recent advancement in speculative decoding. When generating each token, the small draft model generates multiple draft tokens instead of single one, as in vanilla speculative decoding. The target LLM veriﬁes these tokens in parallel, ensuring that the ﬁnal output aligns with the target models distribution while achieving higher overall acceptance rate than vanilla speculative decoding, as the multiple drafts provide better coverage of the target models possible outputs. MDSD algorithms have two main design choices: (1) The draft sampling method. Common approaches include sampling with replacement, where each token is independently sampled from the draft model output distribution, and sampling without replacement, where the probability of selecting token is updated after each draw to exclude previously selected tokens. (2) The veriﬁcation algorithm design. Examples include Recursive Rejection Sampling (RRS) (Yang et al., 2024b; Jeon et al., 2024), which sequentially veriﬁes the draft tokens, and K-SEQ (Sun et al., 2024e), which is designed to improve acceptance rate for sampling with replacement. The acceptance rate, measure of MDSD algorithm performance, also depends on these two design choices. Any veriﬁcation algorithm that guarantees the ﬁnal output aligns with the target model distribution can be viewed as transport from the draft tokens distribution to the target models distribution. For ﬁxed draft sampling method, the optimal veriﬁcation algorithm is solution to an optimal transport problem (Sun et al., 2024e), corresponding to an optimal acceptance rate. However, the complexity of this optimal transport problem, with the number of variables and constraints growing exponentially with the number of draft tokens, makes it difﬁcult to ﬁnd efﬁcient solutions. This difﬁculty has led to two open questions: (1) For modern LLMs, where the vocabulary size is typically in the thousands, the optimal acceptance rate has never been computed, to the best of our knowledge. Simple linear program (LP) solvers can only compute the optimal transport for small toy models, making it challenging to measure the optimal acceptance rate in practical scenarios. (2) Although it is widely known that existing veriﬁcation algorithms are only approximate solutions to the optimal transport problem, the gap between their performance and the theoretical upper bound has never been quantiﬁed with respect to real text distribution. Without knowing the optimal acceptance rate, it is difﬁcult to assess how suboptimal these algorithms are. This paper addresses these two open questions. Our contributions include: We transform the problem of solving the optimal acceptance rate corresponding to the optimal transport into subset selection problem by considering the dual of the problem and then applying total unimodularity. This provides novel perspective for understanding the efﬁciency of MDSD. For certain special cases, we propose efﬁcient methods to solve the subset selection problem by noticing convexity-like structures in the set function. This includes sampling with replacement and sampling without replacement. For the ﬁrst time, we provide practical method to compute the theoretical acceptance rate upper bound of MDSD for draft distribution. For the ﬁrst time, we measure the theoretical upper bound of MDSD efﬁciency on real text, and the gap of existing veriﬁcation algorithms. We compare different draft sampling methods through their optimal acceptance rates and observe that sampling without replacement outperforms sampling with replacement. We evaluate existing veriﬁcation algorithms, including K-SEQ for with replacement and RRS for without replacement and with replacement sampling, and ﬁnd that they still have signiﬁcant gaps from the theoretical upper bound. We propose novel draft sampling method that greedily selects high-probability drafts, with only the last draft being random. In some cases, it achieves an even higher optimal acceptance rate than without replacement. We also propose corresponding veriﬁcation algorithm that perfectly reaches the theoretical acceptance rate upper bound. 2 Published as conference paper at ICLR"
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 SPECULATIVE DECODING FOR ACCELERATING LLM INFERENCE Let Σ denote the vocabulary set. We have target model Ptarget(x1, x2, ..., xm), which is probabilistic model that predicts the probability of the next word. Our goal is to sample from this model as the output. The process of single step Multi-Draft Speculative Decoding is as follows: 1. For draft model Pdraft(x1, x2, ..., xm), sample draft tokens 2. Compute the probabilities of the target model x(1), . . . x(n). Ptarget(x1, x2, ..., xm, this step is not much slower than computing Ptarget(x1, x2, ..., xm) alone. x(1)), ..., Ptarget(x1, x2, ..., xm, in parallel: Ptarget(x1, x2, ..., xm), x(n)). Due to parallel computation, 3. Run the veriﬁcation algorithm xm+1 Pverify( 4. If accepted, for some draft x(i). In this case, we can perform another x(i), we have xm+1 = x(i)), generating two tokens in one step and sampling step xm+2 Ptarget(x1, x2, ..., xm, achieving acceleration. x(1), . . . x(n)). b Speculative Decoding can generate multiple steps, with multiple drafts at each step and all drafts forming tree. However, we only consider the single-step case in this paper. For following analysis, we use p() = Ptarget(x1, x2, ..., xm) to denote the target distribution and pdraft for distribution of draft tokens. 2.2 SPECULATIVE DECODING WITH SINGLE DRAFT TOKEN Informally, the veriﬁcation algorithm depends on two distributions and pdraft, and one draft token pdraft. The goal is to output such that the objective max (i = j) is achieved, that is to maximize the probability of random variable to be the same as random variable j. More formally, given Σ and pdraft Σ representing two probability distributions over the space Σ, we seek joint distribution π Π(p, pdraft) such that the marginal distributions are iΣ π(i, i) is maximized. This forms optimal and pdraft, respectively, and the objective max transport problem. The optimal transport is denoted as π p,pdraft Π(p, pdraft), and the optimal objective function value is α(p, pdraft) = The problem can be formulated as an LP by representing the joint distribution as matrix: p,pdraft (i, i). iΣ π max CRΣΣ Ci,i s.t. Ci,j = p(i), Ci,j = pdraft(j), Ci,j 0 i, Σ. (1) XiΣ XjΣ XiΣ The optimal transport has the following closed-form expression p,pdraft (i, j) = π i,j = min(p(i), pdraft(i)) (p(i)pdraft (i))+(pdraft(j)p(j))+ PzΣ(pdraft(z)p(z))+ ( = 6= , and the optimal objective function value is α(p, pdraft) = min(p(i), pdraft(i)) . XiΣ The conditional distribution of given when (i, j) π is denoted as π(j) G(Σ), where πi,j iΣ πi,j πi,j pdraft(j) π(ij) = = . For the optimal transport, this leads to π p,pdraft (ij) = ( p(i) min( (1 p(j) pdraft(j) , 1) pdraft(j) )+ (p(i)pdraft(i))+ PzΣ(pdraft(z)p(z))+ = i 6= . (2) (3) (4) (5) 3 Published as conference paper at ICLR 2025 The basic single-step, single-draft speculative decoding can be improved in two directions. Multistep methods generate draft sequence. Some improvements (Sun et al., 2024d; Hu & Huang, 2024; Sun et al., 2024c) in this scenario are discussed in Appendix B.3. Our paper focuses on the multidraft direction, where multiple draft tokens are generated at each step. 2.3 MULTI-DRAFT SPECULATIVE DECODING For Σ, deﬁne the incidence set Ai := {i Σnj [n], ij = i}. Informally, the veriﬁcation algorithm depends on two distributions and pdraft, where pdraft is now joint distribution of tokens. Common constructions of pdraft include: Sampling with replacement: Given draft model with output distribution q(), independently sample times. For = (i1, . . . ,in) Σn, we have pdraft(i) = j=1 q(ij). Sampling without replacement: pdraft(i) = j=1 qi1,...,ij1 (ij), where qi1,...,ij1 (x) = ( q(x) 1Pz{i1,...,ij1 } q(z) / {i1, . . . ,ij1} {i1, . . . ,ij1} 0 . (6) j=1 qj(ij). Product of different draft distributions: pdraft(i) = Given multiple draft tokens = (i1, . . . ,in) pdraft, the goal is to output such that the objective max (j [n], = ij) or equivalently max (i Ai) is achieved, that is to maximize the probability of random variable to be the same as one of random variable in (i1, . . . ,in). More formally, given Σ and pdraft Σn representing probability distribution over the space Σ and probability distribution over the space Σn, respectively, we seek joint distribution π Π(p, pdraft) such that the marginal distributions are and pdraft, respectively, and the objective π(i,i) is maximized. The optimal transport is denoted as π p,pdraft Π(p, pdraft), max and the optimal objective function value is α(p, pdraft) = The problem can be formulated as an LP by representing the joint distribution as tensor: p,pdraft (i,i). iAi π iAi iΣ iΣ P max CRΣΣn Ci,i iAi XiΣ s.t. Ci,i = p(i) Σ, iΣn Ci,i 0 Σ,i Σn. XiΣ Ci,i = pdraft(i) Σn, (7) The difﬁculty lies in the exponential number of variables and constraints. Several approximation have been proposed for the multi-draft speculative decoding problem, including Recursive Rejection Sampling (RRS) (Yang et al., 2024b; Jeon et al., 2024) and K-SEQ (Sun et al., 2024e). RRS recursively veriﬁes the draft tokens, while K-SEQ improves the acceptance rate for sampling with replacement. Due to space constraints, we move the details of these methods (Appendix A), other related work (Appendix B) and all proofs (Appendix C) to the appendix."
        },
        {
            "title": "3 OPTIMAL ACCEPTANCE RATE AS SUBSET SELECTION PROBLEM",
            "content": "We show that the optimal acceptance rate can be expressed as subset selection problem: α(p, pdraft) = 1 + min HΣ (cid:18)XiH 4 p(i) pdraft(i) . iHn (cid:19) (8) Published as conference paper at ICLR 2025 3.1 DUAL PROBLEM We start from the linear programming formulation (7) and derive an equivalent formulation: max SRΣΣn Si,i iΣn XiΣ s.t. Si,i p(i) Σ, Si,i pdraft(i) Σn, (9) iΣn Si,i 0 Σ,i Σn, Si,i = 0 Σ,i / Ai. XiΣ Lemma 1. The two formulations (7) and (9) are equivalent. This equivalent formulation transforms the transportation problem (Hitchcock, 1941) into bmatching problem, whose dual is w-vertex cover problem (Schrijver et al., 2003) (with detailed derivation in Appendix C.1): min yRΣ,zRΣn yip(i) + zipdraft(i) XiΣ iΣn s.t. yi + zi 1 Σ,i Ai, yi 0 Σ, zi 0 Σn. (10) 3.2 TOTAL UNIMODULARITY The coefﬁcient matrix of the constraints in (10) is totally unimodular (TUM). The ﬁrst set of constraints forms an incidence matrix of bipartite graph, where one side of the nodes corresponds to Σ and the other side corresponds to Σn. There is an edge between and if and only if Ai. Therefore, it is totally unimodular matrix (Biggs, 1993). The second and third sets of constraints have coefﬁcient matrices that are identity matrices, with Σ + Σn variables and Σ + Σn constraints. The concatenation of TUM matrix and an identity matrix is also TUM (Commoner, 1973). Since the right-hand side of the constraints are integers, the dual problem (10) always has an integer optimal solution (Hoffman & Kruskal, 2010). 3.3 SUBSET SELECTION FORMULATION By restricting the variables in (10) to integers, we obtain: min yZΣ,zZΣn yip(i) + zipdraft(i) XiΣ iΣn s.t. yi + zi 1 Σ,i Ai, yi 0 Σ, zi 0 Σn. (11) In the optimal solution, yi and zi will not exceed 1, so they can only take values 0 or 1. Therefore, the problem can be further simpliﬁed as: min y{0,1}Σ min z{0,1}Σn yip(i) + zipdraft(i) XiΣ iΣn s.t. yi + zi 1 Σ,i Ai . Deﬁne = {i Σyi = 1}. The problem becomes: min HΣ The optimal solution for is min z{0,1}Σn p(i) + zipdraft(i) XiH iΣn s.t. zi 1 Σ H,i Ai . z(H)i = 1 0 / ( . xΣH Ax xΣH Ax 5 (12) (13) (14) Published as conference paper at ICLR 2025 Substituting this solution, we obtain the subset selection formulation: Finally, note that min HΣ p(i) + pdraft(i) . XiH iSxΣH Ax pdraft(i) + pdraft(i) = 1 . iSxΣH Ax iHn (15) (16) This completes the derivation of the subset selection formulation (8)."
        },
        {
            "title": "4 COMPUTING OPTIMAL ACCEPTANCE RATE IN SPECIAL CASES",
            "content": "In this section, we discuss how to efﬁciently compute the optimal acceptance rate for certain special cases of the draft distribution pdraft. For any set function , we deﬁne the marginal value of an element with respect to set as (xH) = (H {x}) (H). We also deﬁne the following shorthand notations: (H) = iH p(i), Q(H) = The optimal acceptance rate can be expressed as α(p, pdraft) = 1 + minHΣ (H). iHn pdraft(i), (H) = (H) Q(H). 4. q-CONVEX FUNCTIONS Deﬁnition 2 (q-Convex Function). set function : 2Σ is called q-convex function if there exists function : Σ R0 such that for all Σ and x, Σ with 6= y, we have Q(xH) q(x) Q(yH {x}) q(y) . (17) Intuitively, if we order the elements of Σ arbitrarily and construct sequence of sets Hi by adding elements one by one, then the curve of Q(Hi) against the sum of values is always convex. Theorem 3. For sampling with replacement, the function is q-convex function. Theorem 4. For sampling without replacement, the function is q-convex function. Theorem 5. All q-convex functions are supermodular functions. For both sampling with replacement and without replacement, computing α(p, pdraft) can be formulated as an unconstrained submodular minimization problem, which has polynomial-time algorithms (Iwata, 2008). However, by fully exploiting the properties of q-convex functions, we can solve the problem even faster, as shown in the next section. 4.2 EFFICIENT COMPUTATION Theorem 6. Suppose that is q-convex function, is monotone increasing, and p(x) > 0 for all Σ. For all Σ and x, Σ with 6= y, if q(x) p(y) and (xH) 0, then (yH {x}) 0. p(x) q(y) The above theorem requires p(x) > 0. When p(x) = 0, there exists an optimal set for (8) that contains because is monotone increasing. 4.2.1 ALGORITHM Inspired by Theorem 6, we can compute the optimal acceptance rate efﬁciently as follows: 1. Find an ordering σ of Σ such that q(σ1) 2. Construct sequence of sets Hi = {σ1, . . . , σi}. 3. Compute α(p, pdraft) = 1 + mini (Hi). Intuitively, we sort the elements by the ratio of and in non-increasing order and then perform linear search. p(σ1) q(σΣ) p(σΣ) . 6 Published as conference paper at ICLR 4.2.2 COMPLEXITY OF COMPUTING AND α xH q(x))n. The time For sampling with replacement, has simple expression Q(H) = ( complexity for computing α(p, pdraft) is O(Σ log Σ) for the sorting step, plus O(Σ) for the linear scan. For sampling without replacement, we can compute Q(H) = Wn,H based on the coefﬁcient of Wn,Σ iH (1 + q(i)t) and apply dynamic progenerating function Wn,H = Coeﬀtn GH (t) = Coeﬀ tn gramming with recurrence relation Wn,H{x} = Wn,H + q(x)Wn1,H . The time complexity is O(Σ log Σ) for the sorting step, plus O(nΣ) for computing coefﬁcient of generating function with dynamic programming. Q"
        },
        {
            "title": "5 A GREEDY APPROACH FOR SELECTING DRAFT TOKENS",
            "content": "In this section, we propose novel method for constructing the draft distribution pdraft and corresponding veriﬁcation algorithm that achieves the optimal acceptance rate for this distribution. 5.1 DRAFT CONSTRUCTION Given draft model output distribution Σ, we construct the draft tokens = (i1, . . . ,in) as follows: The ﬁrst 1 tokens are deterministically set to be the top 1 tokens according to the probability in q, i.e., i1, . . . ,in1 = Topn1(q), such that q(i1) q(in1) and maxiΣ{i1,...,in1} q(i) q(in1). Only the last token in is randomly sampled from without replacement (i.e., it is different from the previous 1 tokens): in Topn1(q)() = The resulting draft distribution is q() 1Pn1 j=1 q(ij ) . pdraft(i) = Topn1(q)(in) i1, . . . ,in1 = Topn1(q) i1, . . . ,in1 6= Topn1(q) 0 (cid:26) . (18) 5.2 VERIFICATION ALGORITHM The corresponding optimal transport problem for this draft distribution is simple because only one draft token is random. We can design veriﬁcation algorithm that strictly achieves the optimal acceptance rate for this draft distribution (, with unfolded deﬁnition in Appendix D): p,pdraft (ii) = π πGreedy p,q Topn1(q) (iin) . (19) Theorem 7. The optimal acceptance rate for the greedy draft distribution is α(p, pdraft) = αGreedy(p, pdraft) = p(i) + min(p(i), Topn1(q)(i)) . (20) XiTopn1(q) XiΣ Our subset selection formulation (8) provides convenient way to prove the above theorem. 5.3 CONNECTION TO SPECHUB SpecHub (Sun et al., 2024b) is recently proposed MDSD method that is only applicable to the case of = 2. The draft construction in SpecHub is as follows: First, sample the ﬁrst draft token i1. If i1 = Top1(q) is the token with the highest probability in q, then sample the second draft token i2 without replacement to ensure it is different from i1. If i1 6= Top1(q) is not the token with the highest probability in q, then deterministically set the second draft token to be the token with the highest probability, i.e., i2 = Top1(q). 7 Published as conference paper at ICLR 2025 The resulting draft distribution is: pdraft(i) = q(i1) i2 = Top1(q) q(i1) 1q(i1) q(i2) i1 = Top1(q) 0 otherwise . (21) We note that the greedy method for = 2 is essentially equivalent to SpecHub because both methods ensure that at least one draft token is the token with the highest probability in q. However, the speciﬁc draft distributions are different, leading to simpler veriﬁcation algorithm for the greedy method."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "Table 1: Acceptance rates of different MDSD methods across various models and tasks. α means the gap between veriﬁcation method and the theoretical upper bound, with statistically signiﬁcant differences indicated by directional arrows. Model Pairs Draft Sampling Method Alpaca α α CNN-DailyMail α α WMT14 α α OPT-125M OPT-6.7B LLaMA-68M LLaMA-7B Eagle-0.24B Vicuna-7B Eagle-0.26B Qwen2-7B With Replacement Without Replacement Greedy With Replacement Without Replacement Greedy With Replacement Without Replacement Greedy With Replacement Without Replacement Greedy - - - RRS α RRS α Verify α Verify α - 0.0 - 85.4 0.1 1.5 RRS K-SEQ 85.8 0.1 1.1 αK-SEQ 85.9 0.1 0.9 α 86.9 0.1 88.9 0.1 0.9 89.9 0.1 90.7 0.1 90.7 0.1 71.6 0.1 1.5 RRS K-SEQ 71.9 0.1 1.1 αK-SEQ 72.0 0.1 1.0 α 73.0 0.1 75.7 0.1 0.8 76.5 0.1 78.4 0.1 0.1 78.4 0.1 63.4 0.2 1.0 RRS K-SEQ 63.9 0.2 0.5 αK-SEQ 63.7 0.1 0.7 α 64.4 0.1 70.9 0.2 0.6 71.5 0.1 72.6 0.2 0.2 72.8 0.1 59.6 0.2 1.0 RRS K-SEQ 59.9 0.2 0.8 αK-SEQ 59.9 0.1 0.7 α 60.7 0.1 68.3 0.2 1.1 69.4 0.1 69.9 0.2 0.0 70.0 0.1 Verify α Verify α RRS α RRS α - - - - - - - - - - - - - - - - 0.1 - 63.4 0.1 66.1 0.1 66.1 0.1 70.6 0.1 1.5 71.1 0.1 0.9 71.0 0.1 1.0 72.0 0.1 75.1 0.1 1.0 76.0 0.1 77.0 0.1 0.0 77.1 0.1 59.8 0.1 1.0 60.0 0.1 0.8 60.2 0.1 0.6 60.8 0.1 77.3 0.1 3.3 78.4 0.1 2.2 78.5 0.1 2.2 80.7 0.1 81.5 0.1 2.8 84.3 0.1 84.2 0.1 0.1 84.3 0.1 65.3 0.1 2.2 66.0 0.1 1.5 66.2 0.1 1.3 67.5 0.1 70.5 0.1 1.3 63.3 0.1 0.1 71.8 0.1 73.2 0.1 73.1 0.1 56.7 0.1 1.1 32.9 0.2 0.2 57.0 0.1 0.8 33.0 0.2 0.1 57.1 0.1 0.7 32.9 0.2 0.1 33.1 0.2 57.8 0.1 - 63.4 0.1 0.8 36.9 0.2 36.4 0.2 64.2 0.1 - 39.5 0.2 65.7 0.1 0.1 39.5 0.2 65.8 0.1 46.7 0.1 1.6 38.3 0.1 0.4 47.2 0.1 1.1 38.3 0.1 0.4 47.3 0.1 1.0 38.3 0.1 0.3 48.3 0.1 52.4 0.1 1.7 43.9 0.1 0.1 54.1 0.1 54.0 0.1 53.9 0.1 44.0 0.1 45.5 0.1 45.4 0.1 - 0.4 - 0.1 - 38.7 0.1 - 0.0 - - 0.0 - - 0.1 - - - - The goal of our experiments is to measure the acceptance rates of various MDSD methods on real text distributions and compare them with the theoretical upper bounds. In the previous sections, we analyzed the theoretical acceptance rate α(p, pdraft) for three different draft distributions: sampling with replacement, sampling without replacement, and greedy approach (Section 5). We also 8 Published as conference paper at ICLR 2025 discussed some existing veriﬁcation methods (Appendix A), such as RRS and K-SEQ, whose acceptance rates are expected to be lower than the theoretical upper bound. For K-SEQ, its average acceptance rate αK-SEQ can be derived theoretically (see Appendix A.2 for details). Our efﬁcient computation methods (Section 4) make it possible, for the ﬁrst time, to obtain the theoretical upper bound of MDSD for vocabulary sizes of thousands. To obtain realistic distributions and pdraft, we select real-world datasets for various tasks, including Alpaca (Taori et al., 2023) for instruction-following, WMT14 De-En (Bojar et al., 2014) for translation, and CNN-DailyMail (Hermann et al., 2015) for summarization. For each task, we use an LLM to generate responses on 1024 data samples, with maximum length of 128 tokens. We then measure the logits of the target model and the draft model on these generated responses to construct and pdraft. We evaluated different approaches based on four publicly available large language models, including 1) LLaMA (Touvron et al., 2023), 2) Vicuna (Chiang et al., 2023), the instruction ﬁne-tuned version of LLaMA models, 3) OPT (Zhang et al., 2022), and 4) Qwen2 (Yang et al., 2024a). Speciﬁcally, for the LLaMA family, we select LLaMA-7B as the target model and LLaMA-68M as the draft model, which is consistent with previous work (Miao et al., 2024). For the OPT family, we select OPT-6.7B as the target model and OPT-125M as the draft model. Moreover, for the Vicuna family and the Qwen family, we select Vicuna-7B-v1.3 and Qwen2-7B-Instruct as target models, and we use paired draft models provided by EAGEL (Li et al., 2024), with 0.24B parameters and 0.26B parameters, respectively. Unless otherwise speciﬁed, we use default generation temperature of 0.7 and draft token number of 3. The total computational cost is less than 50 GPU hours on RTXA6000. 6.1 MAIN EXPERIMENT In the main experiment, we compare the acceptance rates of different MDSD methods across various LLMs and tasks. The results are shown in Table 1. We observe that the existing verify methods, RRS and K-SEQ, still have gaps compared to the theoretical acceptance rate upper bound. Sampling without replacement achieves higher acceptance rates than sampling with replacement, both in terms of the theoretical upper bound and the existing veriﬁcation algorithms. We can attribute this to the fact that sampling with replacement may lead to duplicate draft tokens, which are less helpful for acceleration. The greedy method obtains the highest acceptance rate, but this is not always the case, as we will see in the ablation study below that the greedy method performs worse when the temperature is 1. Greedy Optimal Greedy Without Replacement Optimal Without Replacement RRS With Replacement Optimal With Replacement RRS K-SEQ Theory K-SEQ 0.7 α 0.6 0. 0.75 α 0.7 0.65 0.9 0.8 α 0. 0.1 0.3 0.5 0.7 0.9 Temperatures 0.1 0.3 0.5 0.7 0.9 Temperatures 0.1 0.3 0.5 0.7 0.9 Temperatures (a) WMT14 De-En (b) CNN-DailyMail (c) Alpaca Figure 1: Comparison of acceptance rate α for different temperatures across datasets. 6.2 ABLATION STUDY I: IMPACT OF TEMPERATURE We study the impact of different temperatures on the acceptance rates. The temperature affects the distributions of the target model and the draft model, even if the logits remain unchanged. It also affects the output text during the sampling process, resulting in different responses. Figure 1 shows 9 Published as conference paper at ICLR 2025 Greedy Optimal Greedy Without Replacement Optimal Without Replacement RRS With Replacement Optimal With Replacement RRS K-SEQ Theory K-SEQ 0.7 α 0.6 0. 0.8 0.7 α 0.6 0.5 0.9 0. α 0.7 0.6 2 4 6 # Drafts 8 10 2 4 6 # Drafts 8 10 2 4 6 # Drafts 8 10 (a) WMT14 De-En (b) CNN-DailyMail (c) Alpaca Figure 2: Comparison of acceptance rate α for different number of drafts across datasets. Table 2: Acceptance rates of different MDSD methods on MT-Bench based on Eagle framework. Method # Drafts = 2, # Steps = 4 # Drafts = 4, # Steps = 3 EAGLE default sparse tree α Speed α = 0.1 Speed α Speed 75.3 0.3 RRS w/ replacement RRS w/o replacement 79.4 0.3 1.04 ( 0.02) 80.4 0.3 1.03 ( 0.01) 76.8 0.3 1.04 ( 0.02) SpecHub Greedy 84.0 0.3 1.11 ( 0.02) 84.7 0.3 1.13 ( 0.02) 88.8 0.2 1.17 ( 0.01) 79.1 0.3 1.08 ( 0.02) 78.4 0.3 74.7 0.3 - - - - - - - = 0.6 78.8 0.3 RRS w/ replacement RRS w/o replacement 82.4 0.3 1.07 ( 0.02) 88.6 0.2 1.07 ( 0.01) 77.6 0.3 1.05 ( 0.02) SpecHub Greedy 82.3 0.3 1.02 ( 0.02) 82.8 0.3 1.04 ( 0.02) 90.0 0.2 1.09 ( 0.01) 78.3 0.3 1.01 ( 0.02) 84.7 0.3 76.2 0. - - - - - - - = 1.0 76.7 0.3 RRS w/ replacement RRS w/o replacement 76.4 0.3 1.00 ( 0.02) 85.3 0.3 1.05 ( 0.01) 74.1 0.3 1.03 ( 0.02) SpecHub Greedy 79.5 0.3 1.01 ( 0.02) 79.2 0.3 1.02 ( 0.02) 87.8 0.2 1.08 ( 0.01) 72.9 0.3 0.97 ( 0.02) 83.5 0. 72.1 0.3 - - - - - - - the results. We use LLaMA-7B as the target model and LLaMA-68M as the draft model for our ablation studies. We can have the following observations: The impact of temperature is non-monotonic. Moreover, different methods respond differently to temperature changes. At low temperatures, all methods fall into two categories. The ﬁrst includes methods that allow duplicate tokens. When = 0, these methods essentially have only one effective draft token, the one with the largest logits on the draft model. The second includes methods that prevent duplicate tokens. When = 0, these methods always select the top tokens on the draft model. The gap between the optimal acceptance rate and acceptance rates for previously existing veriﬁcation methods, RRS and K-SEQ, gradually increases as the temperature rises. As temperature increases, the gap between methods with replacement sampling and methods without replacement sampling decreases. We can attribute this to the fact that, at high temperatures, the probability distribution is less concentrated, making with replacement sampling strategies have less probability to generate duplicate tokens. 6.3 ABLATION STUDY II: IMPACT OF NUMBER OF DRAFTS We investigate the impact of different numbers of drafts on the acceptance rates. The results are shown in Figure 2. We have the following observations: As the number of drafts increases, the coverage of the target models possible outputs improves, therefore leading to better acceptance rate. This trend holds for all methods. 10 Published as conference paper at ICLR 2025 The draft sampling strategy signiﬁcantly impacts the beneﬁts derived from an increase in the number of drafts. Sampling without replacement generally beneﬁt more from an increase in drafts compared to sampling with replacement. This is because sampling with replacement can lead to redundant drafts, which do not fully leverage the advantages of increasing the number of drafts. The gap between the optimal acceptance rate and acceptance rates for previously existing veriﬁcation methods, RRS and K-SEQ, gradually increases as the number of drafts rises. 6.4 EVALUATING THE GREEDY SAMPLING METHOD ON GENERATION TASKS In this section, we evaluate the effectiveness and generation efﬁciency of the proposed Greedy draft sampling method (Section 5) on real-world generation tasks and compare it with other MDSD methods. We implement the Greedy method within the EAGLE Framework (Li et al., 2024), which supports multi-step MDSD with draft tree structure. We experiment with three types of tree structures: (1) drafts = 2, depths = 4; (2) drafts = 4, depths = 3; and (3) sparse tree with up to 4 drafts and 5 steps, which is the default setting in EAGLE. We conduct experiments on the MT-Bench dataset (Zheng et al., 2023) using Vicuna-7B-v1.3 (Chiang et al., 2023) as the target model and its corresponding Eagle model with 0.24B parameters as the draft model. Table 3 presents the results. As discussed in Section 5.3, the Greedy method and SpecHub have equal acceptance rates when the number of draft tokens is 2. Our experiments conﬁrm this theoretical insight, showing no statistically signiﬁcant difference between the two methods for any temperature. The Greedy method demonstrates improved performance at low temperatures. For example, at T=0.1, it achieves higher acceptance rate compared to RRS without replacement, leading to faster generation. However, as the temperature increases, the performance gain of the Greedy method diminishes. This observation is consistent with the ablation study in Figure 1."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we studied the acceptance rate of Multi-Draft Speculative Decoding (MDSD). On the theoretical side, we discovered an equivalence between the optimal acceptance rate and subset selection problem. We also provided efﬁcient methods to compute the optimal acceptance rate for common draft distributions. On the practical side, for the ﬁrst time, we measured the optimal acceptance rate under real text distributions and quantiﬁed the gap between existing algorithms and the optimal acceptance rate. Furthermore, we proposed practical greedy draft construction method that, in some cases, achieves an even higher acceptance rate than sampling without replacement. We hope that our work will stimulate further research on improving the efﬁciency of large language model inference and make these powerful models more accessible and applicable in real-world scenarios. ACKNOWLEDGMENT This work was partially supported by NSF IIS 2347592, 2347604, 2348159, 2348169, DBI 2405416, CCF 2348306, CNS 2347617."
        },
        {
            "title": "REFERENCES",
            "content": "Norman Biggs. Algebraic graph theory. Number 67. Cambridge university press, 1993. Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp. 1258, 2014. 11 Published as conference paper at ICLR 2025 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024. Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462, 2023b. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna, March 2023. Frederic Commoner. sufﬁcient condition for matrix to be totally unimodular. Networks, 3(4): 351365, 1973. Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and Di He. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Twenty-eighth Conference on Neural Information Processing Systems, pp. 16931701, 2015. Frank Hitchcock. The distribution of product from several sources to numerous localities. Journal of mathematics and physics, 20(1-4):224230, 1941. Alan Hoffman and Joseph Kruskal. Integral boundary points of convex polyhedra. 50 Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art, pp. 4976, 2010. Zhengmian Hu and Heng Huang. Accelerated speculative sampling based on tree monte carlo. In Forty-ﬁrst International Conference on Machine Learning, 2024. Satoru Iwata. Submodular function minimization. Mathematical Programming, 112:4564, 2008. Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott. Recursive speculative decoding: Accelerating llm inference via sampling without replacement. arXiv preprint arXiv:2402.14160, 2024. Ashish Khisti, Arash Behravesh, Hassan Dbouk, Arash Behboodi, Roland Memisevic, and Christos Louizos. Importance weighted multi-draft speculative sampling. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models, 2024. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. 12 Published as conference paper at ICLR 2025 Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. Bei Luo, Raymond YK Lau, Chunping Li, and Yain-Whar Si. critical review of state-of-the-art chatbot designs and applications. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 12(1):e1434, 2022. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and veriﬁcation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pp. 932949, 2024. Giovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023. Jie Ou, Yueming Chen, and Wenhong Tian. Lossless acceleration of large language model via adaptive n-gram parallel decoding. arXiv preprint arXiv:2404.08698, 2024. Alexander Schrijver et al. Combinatorial optimization: polyhedra and efﬁciency, volume 24. Springer, 2003. Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623, 2023. Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912, 2024a. Ryan Sun, Tianyi Zhou, Xun Chen, and Lichao Sun. SpecHub: Provable acceleration to multi-draft speculative decoding. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 20620 20641, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1148. Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Ahmad Beirami, Jae Hun Ro, and Ananda Theertha Suresh. Block veriﬁcation accelerates speculative decoding. In Workshop on Efﬁcient Systems for Foundation Models II@ ICML2024, 2024c. Ziteng Sun, Jae Hun Ro, Ahmad Beirami, and Ananda Theertha Suresh. Optimal block-level draft veriﬁcation for accelerating speculative decoding. arXiv preprint arXiv:2403.10444, 2024d. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. Advances in Neural Information Processing Systems, 36, 2024e. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efﬁcient foundation language models. arXiv preprint arXiv:2302.13971, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024a. Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Inference with reference: Lossless acceleration of large language models. arXiv Furu Wei. preprint arXiv:2304.04487, 2023. Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-candidate speculative decoding. arXiv preprint arXiv:2401.06706, 2024b. 13 Published as conference paper at ICLR Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Francois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023. 14 Published as conference paper at ICLR"
        },
        {
            "title": "A APPROXIMATE SOLUTIONS",
            "content": "A.1 RECURSIVE REJECTION SAMPLING (RRS) Yang et al. (2024b) and Jeon et al. (2024) use the Recursive Rejection Sampling method. Deﬁne the residual distribution Respq Σ for p, Σ as: Respq(i) = (p(i) q(i))+ zΣ(p(z) q(z))+ When = q, Respq can be deﬁned as an arbitrary distribution. For pdraft from sampling with replacement, the RRS algorithm is recursively deﬁned as: p,pdraft (ii) = πRRS,w πRRS,w p,q (ii) where and πRRS,w p,q (ii) = min( p(i1) (1 p(i1) q(i1) , 1) πRRS,w Respq,q(ii2:) q(i1) )+ ( = i1 6= i1 πRRS,w p,q (i()) = p(i) Here i2: denotes the sequence with the ﬁrst element removed. For pdraft from sampling without replacement, the RRS algorithm is deﬁned as: p,pdraft (ii) = πRRS,w πRRS,wo p,q (ii) where πRRS,wo p,q (ii) = min( p(i1) (1 p(i1) q(i1) , 1) πRRS,wo Respq,qi1 (ii2:) q(i1) )+ ( = i1 6= and πRRS,wo p,q (i()) = p(i) (22) (23) (24) (25) (26) (27) (28) The acceptance rates are denoted as αRRS,w(p, pdraft) and αRRS,wo(p, pdraft), respectively. A.2 K-SEQ Sun et al. (2024e) proposed the K-SEQ method to verify drafts sampled with replacement. Deﬁne and let ρ be the solution to the equation XiΣ βp,q(ρ) = min( p(i) ρ , q(i)) 1 (1 βp,q(ρ))n = ρβp,q(ρ) The K-SEQ algorithm is deﬁned as: p,pdraft (ii) = πK-SEQ πK-SEQ p,q,ρ (ii) where πK-SEQ p,q,ρ (ii) = (1 p(i1) ρq(i1) )+ πK-SEQ p,q,ρ (ii2:) + min( p(i1) 0 ρq(i1) , 1) ( = i1 6= i1 and p(i) min πK-SEQ p,q,ρ (i()) = q(i), p(i) ρ (1 βp,q(ρ))n 1(1βp,q(ρ))n βp,q(ρ) (29) (30) (31) (32) (33) The acceptance rate is denoted as αK-SEQ(p, pdraft) = 1 (1 βp,q(ρ))n, which is theoretically guaranteed to achieve (1 e1)-approximation of the optimal acceptance rate. Published as conference paper at ICLR"
        },
        {
            "title": "B RELATED WORKS",
            "content": "B.1 DRAFT MODEL DESIGN In Numerous studies have explored the design of better draft models for speculative decoding. principle, any autoregressive probabilistic model can serve as draft model. The simplest approaches include using n-gram models (Ou et al., 2024) or document retrieval as draft models (Yang et al., 2023; He et al., 2023). Small transformer-based language models have also been employed (Leviathan et al., 2023; Chen et al., 2023a), often with distillation techniques to further increase the overlap between the draft and target models (Zhou et al., 2023). The design of good draft model involves trade-off between its similarity to the target model and its computational complexity. More complex draft models lead to higher acceptance rates due to their closer resemblance to the target model, but they also incur higher computational overhead. To achieve better trade-off, some works have proposed reusing the target models computational results. For example, Monea et al. (2023) use the original model with look ahead tokens, while Cai et al. (2024) add new heads to the last hidden layer of the original model to predict tokens further ahead. Li et al. (2024) reuse the last layer hidden state computation of the large model and introduce new attention layer to predict the next token. Sun et al. (2024a) employ the target model with partial key-value cache as the draft model. B.2 MULTI-DRAFT SPECULATIVE DECODING Many related works on Multi-Draft Speculative Decoding (MDSD) have been introduced in other sections. This paper focuses on the single-step Multi-Draft scenario. When MDSD generates multiple steps, with each step involving multiple drafts, it forms tree structure. Sequoia (Chen et al., 2024) propose dynamic programming algorithm to search for the optimal tree topology. As the tree grows deeper, the acceptance probability of certain branches decreases. Cascade Speculative Drafting (Chen et al., 2023b) addresses this issue by assigning the largest draft model to generate draft tokens at shallower levels, which are more likely to be accepted, and gradually using smaller models to generate drafts for less relevant branches. Khisti et al. (2024) studied the optimal acceptance rate for special case of sampling with replacement for = 2 drafts, and obtained the following result: α(p, pdraft) = min HΣ p(i) + XiH XiΣH 2 q(s) + 2 q(s) ! XiH XiΣH q(s) . (34) This is essentially the same as our result (8) under this special case. However, our theory is more general, without any assumption on the draft sampling methods or the number of draft tokens. B.3 MULTI-STEP SPECULATIVE DECODING The basic single-step, single-draft speculative decoding, as introduced in Section 2.1, can be applied to multiple steps, with each step having only one draft and an independent veriﬁcation process (Leviathan et al., 2023; Chen et al., 2023a). However, such an approach of repeatedly applying single-step veriﬁcation is not optimal for the multi-step scenario. Some works, such as Sun et al. (2024d); Hu & Huang (2024); Sun et al. (2024c), have designed better veriﬁcation algorithms speciﬁcally for the multi-step setting. These algorithms are tailored for the multi-step scenario while remaining compatible with the single-step case, reducing to the basic speculative sampling algorithm when applied to draft sequence of length 1. Multi-step speculative decoding and multi-draft speculative decoding represent different directions for improvement. As shown in Figure 3, Sun et al. (2024e); Khisti et al. (2024) and our work improve speculative decoding from the multi-draft perspective. When there is only single draft, it reduces to the case in Leviathan et al. (2023); Chen et al. (2023a). On the other hand, Sun et al. (2024d); Hu & Huang 16 Published as conference paper at ICLR 2025 Speculative Decoding Leviathan et al. (2023) Chen et al. (2023a) multi-step Sun et al. (2024d) Hu & Huang (2024) Sun et al. (2024c) multi-draft Sun et al. (2024e) Khisti et al. (2024) This paper Figure 3: Different directions for improving speculative decoding. (2024); Sun et al. (2024c) enhance speculative decoding from the multi-step perspective. When there is only single step, it reduces to the case in Leviathan et al. (2023); Chen et al. (2023a). Combining both improvements in the multi-draft and multi-step scenario would be ideal, and could be direction for future research."
        },
        {
            "title": "C PROOFS",
            "content": "Proof of Lemma 1. Let f1(C) and f2(S) denote the objective function values of (7) and (9), respectively. Let v1 = f1(C) and v2 = f2(S) be the optimal objective function values. i,i for Ai and First, we show that the optimal solution of (7) is feasible for (9). Deﬁne Si,i = Si,i = 0 for / Ai. This solution maintains the objective function value, i.e., f2(S) = f1(C). Therefore, v2 = f2(S) f2(S) = f1(C) = v1. Next, we show that the optimal solution of (9) is feasible for (7). Deﬁne pres(i) = p(i) i,i 0 for Σn. We have i,i. This solution has larger objective function value, i.e., f1(C) f1(S). Therefore, v1 = f1(C) f1(C) f2(S) = v2. Combining the two parts, we have v1 = v2, which proves the equivalence of the two formulations. i,i 0 for Σ and pres draft(i) = iΣ draft(i) i,i + pres(i)pres PiΣ pres(i) iΣ pres(i). Deﬁne Ci,i = draft(i) = pdraft(i) iΣn iΣn pres Proof of Theorem 3. For = (i1, . . . ,in) Σn, we have pdraft(i) = Q(H) = in the set H. Therefore, Q(H) = ( j=1 q(ij). The function iHn pdraft(i) represents the probability that all samples drawn with replacement are xH q(x))n. Consider the convex function g(x) = xn. To prove the q-convexity of Q, it sufﬁces to show that for all Σ and x, Σ with 6= y, we have: (Q(H) + q(x))n Q(x)n q(x) (Q(H) + q(x) + q(y))n (Q(H) + q(x))n q(y) (35) This can be rewritten as: g(Q(H) + q(x)) g(Q(x)) q(x) g(Q(H) + q(x) + q(y)) g(Q(H) + q(x)) q(y) (36) Note that both sides are ﬁnite differences of the convex function g. Deﬁne = Q(x), = Q(x) + q(x), and = Q(x) + q(x) + q(y). It sufﬁces to show that: g(b) g(a) g(c) g(b) (37) This follows directly from the convexity of g. Published as conference paper at ICLR 2025 iHn pdraft(i) represents the probability that all Proof of Theorem 4. The function Q(H) = samples drawn without replacement are in the set H. To handle the more complex case of sampling without replacement, we use generating functions. Deﬁne the generating function GH (t) = Coeﬀ tn GH (t). Note that Q(H) = Wn,H Wn,Σ . iH (1 + q(i)t) and the coefﬁcient Wn,H = The coefﬁcients satisfy the following recurrence relation: Wn,H{x} = Coeﬀ tn GH{x}(t) = Coeﬀ tn GH (t) + q(x)tGH (t) = Wn,H + q(x)Wn1,H (38) (39) (40) To prove the q-convexity of Q, it sufﬁces to show that for all Σ and x, Σ with 6= y, we have: Wn,H{x} Wn,H q(x)Wn,Σ Wn,H{x,y} Wn,H{x} q(y)Wn,Σ Applying the recurrence relation, it sufﬁces to show that: Wn1,H Wn1,H{x} Applying the recurrence relation again, it sufﬁces to show that: 0 q(x)Wn2,H (41) (42) (43) This holds because the coefﬁcients of are always non-negative, i.e., Wn2,H 0. Proof of Theorem 5. It sufﬁces to show that for all Σ and x, Σ with 6= y, we have: By the q-convexity of Q, we have: Q(xH) Q(xH {y}) Q(xH) q(x) Q(yH {x}) q(y) (44) (45) Therefore, Q(xH) q(x) Q(xH) + Q(yH {x}) q(x) + q(y) = Q(H {x, y}) Q(H) q(x) + q(y) Q(yH {x}) q(y) (46) Similarly, by symmetry, we can reverse and to obtain: Therefore, This implies that: Q(yH) q(y) Q(H {x, y}) Q(H) q(x) + q(y) Q(xH {y}) q(x) Q(xH) q(x) Q(H {x, y}) Q(H) q(x) + q(y) Q(xH {y}) q(x) Q(xH) Q(xH {y}) Proof of Theorem 6. We have: (xH) = p(x) Q(xH) q(x) p(x) = p(x)(1 Q(xH) q(x) ) 18 (47) (48) (49) (50) (51) Published as conference paper at ICLR 2025 Therefore, (xH) p(x) = 1 q(x) p(x) Q(xH) q(x) 0 By assumption, q(x) p(x) q(y) p(y) . By the q-convexity of Q, we have Q(xH) q(x) Q(yH{x}) q(y) q(y) p(y) Q(yH {x}) q(y) q(x) p(x) Q(xH) q(x) It follows that: (yH {x}) p(y) Q(yH {x}) q(y) = 1 q(y) p(y) (xH) p(x) Proof of Theorem 7. We ﬁrst prove that the acceptance rate of the greedy method is: αGreedy(p, pdraft) p,pdraft (i,i) πGreedy = iAi XiΣ iAi XiΣ p,pdraft (ii)pdraft(i) πGreedy p,pdraft (ii)pdraft(i) πGreedy iAi XiTopn1(q) + p,pdraft (ii)pdraft(i) πGreedy iAi XiΣTopn1(q) p,q Topn1(q) (iin)q Topn1(q)(in) π inΣ XiTopn1(q) + πGreedy p,pdraft (i(Topn1(q), i))q Topn1(q)(i) XiΣTopn1(q) p(i) XiTopn1(q) + p,q Topn1(q) (ii)q Topn1(q)(i) π XiΣTopn1(q) p(i) + min(p(i), Topn1(q)(i)) = = = = = (52) . Therefore, (53) (54) (55) (56) (57) (58) (59) (60) (61) (62) (63) (64) (65) XiTopn1(q) XiΣTopn1(q) iTopn1(q) min(p(i), Topn1(q)(i)) = Note that iTopn1(q) min(p(i), 0) = 0. Next, we compute the optimal acceptance rate. Note that when Topn1(q) * H, we must have Q(H) = 0. When Topn1(q) H, we have Q(H) = iH Topn1(q)(i). Therefore, α(p, pdraft) =1 + min HΣ (H) Q(H) =1 + min HΣ,s.t. Topn1(q)H p(i) Topn1(q)(i) XiH The optimal set is = {i Σq Topn1(q)(i) p(i)} Topn1(q). In this case, α(p, pdraft) 19 (66) (67) (68) (69) Published as conference paper at ICLR 2025 =1 (q Topn1(q)(i) p(i))+ + p(i) XiΣ min(p(i), Topn1(q)(i))+ + XiTopn1(q) p(i) XiTopn1(q) = XiΣ C.1 DERIVATION OF THE DUAL PROBLEM We start from the primal problem (9): max SRΣΣn Si,i iΣn XiΣ s.t. Si,i p(i) iΣn Si,i pdraft(i) XiΣ Si,i 0 Si,i = 0 Σ Σn Σ,i Σn Σ,i / Ai (70) (71) (72) iΣn Si,i p(i) and zi for each constraint We introduce dual variables yi for each constraint iΣ Si,i pdraft(i). The Lagrangian function is: L(S, y, z) = Si,i iΣn XiΣ + yi(p(i) XiΣ + iΣn zi(pdraft(i) iΣn Si,i) Si,i) XiΣ The dual function is: g(y, z) = max SRΣΣnL(S, y, z) s.t. Si,i 0 Si,i = 0 Σ,i Σn Σ,i / Ai Rearranging the Lagrangian function: L(S, y, z) = (1 yi zi)Si,i iΣn XiΣ + yip(i) + XiΣ iΣn zipdraft(i) For the dual function to be bounded, we must have: 1 yi zi 0, Σ,i Ai Therefore, the dual problem is: min yRΣ,zRΣn yip(i) + zipdraft(i) XiΣ s.t.yi + zi 1 yi 0 zi iΣn Σ,i Ai Σ Σn This completes the derivation of the dual problem. 20 (73) (74) (75) (76) (77) (78) (79) (80) Published as conference paper at ICLR"
        },
        {
            "title": "D ADDITIONAL ILLUSTRATION",
            "content": "Illustration of single-step draft tokens generation and veriﬁcation: Input (Context) Draft Model Pdraft Generate Drafts Generate Draft Tokens x(1), . . . , x(n) Parallel Computation Compute Probabilities Ptarget() Draft Tokens Probabilities Veriﬁcation Algorithm Pverify Probabilities Target Model Ptarget Output Token Pseudo code for apply multi-draft speculative sampling for multiple steps, with arbitrary tree topology. m i f e a e o g ( prompt , e o y , f d , g o ) : t f e a e Deco in g t o c r n n g mo el e c . Example e o y : e o y = [ [ 2 ] , [ 1 ] , [ 0 , 1 ] , [ 2 , 1 ] , [ 0 ] , [ 0 , 0 ] , [ 2 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 1 , 1 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 , 0 ] , [ 3 ] , [ 0 , 2 ] , [ 3 , 0 ] , # s [ 1 , 0 ] , e : 4 n s [ 1 , 1 ] , # eco e [ 0 , 0 , 1 ] , [ 0 , 2 , 0 ] , [ 0 , 0 , 2 ] , [ 0 , 2 , 1 ] , [ 0 , 1 , 0 ] , [ 1 , 0 , 0 ] , # r v [ 0 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 2 ] , # r e [ 0 , 0 , 0 , 0 , 1 ] # t v h f t EAGLE e u r wh ere h number ] s r n wh ich f i e u t h e . 21 Published as conference paper at ICLR 2025 t a s # t i r s = {} f s b o = {} g i i i = {} t d t and t u n # e e f o e i [ [ ] ] + e o y : e p i t r t l c d = c d ( e o y , f ) o i e : t e # p no a b h r a # e e f and r o n s b o # . . ( p g h / h t p e t e o 5 . 1 f [ l ( f ) ] , f s b o [ l ( f ) ] ) = e e f k ( f d , f ) # Compute b l d r t s g i i i = p t e s b o ( g o , f ) fro r mo el r d t r c o and e i r s # r e = [ ] l u : u ( f ) i a : break # End no i l a # . . RRS KSeq e o 5 . 2 p o = i a n ( f [ l ( f ) ] , f s b o [ l ( f ) ] , g i i i [ l ( f ) ] ) f . ap en ( p o ) # u h n t e n e n f d e i e ( e o y , f ) : Get p i by one e . l t Exa mp les : r o o h e n e e e p g = [ [ 0 ] , c d ( [ ] , c d ( [ 0 ] , c d ( [ 1 ] , c d ( [ 0 , 0 ] , [ 1 ] , [ 0 , 1 ] , e o y ) > [ [ 0 ] , [ 0 , 0 ] , [ 1 , 0 ] ] [ 1 ] ] e p g ) > [ [ 0 , 0 ] , e o y ) > [ [ 1 , 0 ] ] e o y ) > [ ] [ 0 , 1 ] ] u [ h p i e p g l ( h ) == ( f ) + 1 and h [ : ( f ) ] == f ] 22 Published as conference paper at ICLR 2025 Unfolded deﬁnition of verify algorithm for greedy draft construction. p,pdraft (ii) πGreedy p,q Topn1(q) (iin) =π = , 1) (1 min( p(i)(1PjTopn1(q) q(j)) q(i) p(in)(1PjTopn1(q) q(j)) q(in) p(in)(1PjTopn1(q) q(j)) q(in) SUMMARY OF NOTATIONS (1 )+ )+ p(i)(1PjTopn1(q) q(j)) PzΣ(p(z)(1PjTopn1(q) q(j))I(z /Topn1(q))q(z))+ (p(i)(1PjTopn1(q) q(j))q(i))+ PzΣ(p(z)(1PjTopn1(q) q(j))I(z /Topn1(q))q(z))+ = in Topn1(q) 6= in, / Topn1(q) (81) Σ: The vocabulary set Σ: The probability simplex over vocabulary Σ [n]: The set 1,...,n Ptarget(x1, x2, ..., xm): The target model, probabilistic model that predicts the probability of the next word given the context Pdraft(x1, x2, ..., xm): The draft model used to generate candidate tokens Pverify( x(1), . . . x(n)): The veriﬁcation algorithm that selects the ﬁnal output token from the draft tokens p,pdraft Π(p, pdraft): The optimal transport joint distribution p() = Ptarget(x1, x2, ..., xm): Shorthand for the target distribution pdraft: The distribution of draft tokens π Π(p, pdraft): joint distribution with marginal distributions and pdraft π α(p, pdraft): The optimal acceptance rate Ai := {i Σnj [n], ij = i}: The incidence set for token q(): The shorthand notation of the output distribution of the draft model qi1,...,ij1 (x): The probability of token when sampling without replacement, excluding previously selected tokens Respq Σ: The residual distribution πRRS,w p,pdraft , πRRS,wo p,pdraft : The RRS veriﬁcation algorithms for with/without replacement sampling αRRS,w(p, pdraft), αRRS,wo(p, pdraft): Acceptance rates for RRS with/without replacement βp,q(ρ): function used in the K-SEQ algorithm πK-SEQ p,pdraft : The K-SEQ veriﬁcation algorithm αK-SEQ(p, pdraft): Acceptance rate for the K-SEQ algorithm iH p(i): Sum of target probabilities over set (H) = iHn pdraft(i): Sum of draft probabilities over set Q(H) = (H) = (H) Q(H): Difference between target and draft probabilities over set πGreedy p,pdraft : The greedy veriﬁcation algorithm αGreedy(p, pdraft): Acceptance rate for the greedy draft sampling method Ci,j: Matrix representation of joint distribution (p(i) pdraft(i))+: The positive part of the difference GH (t): Generating function deﬁned as Wn,H : Coefﬁcient of tn in GH (t) iH (1 + q(i)t) 23 Published as conference paper at ICLR 2025 : Dual variables : Variables in the equivalent LP formulation (xH): The marginal value of element with respect to set Topn1(q): The top n-1 tokens according to probability in RΣΣn RΣ, RΣn π(j): The conditional distribution given πi,j: Individual elements of the joint distribution matrix i2:: The sequence with the ﬁrst element removed Hi: Sets constructed by adding elements one by one σ: An ordering of Σ used in the efﬁcient computation algorithm Coeﬀtn : Coefﬁcient of tn in generating function"
        },
        {
            "title": "F ADDITIONAL EXPERIMENTS",
            "content": "Table 3: Average generation length τ of different MDSD methods and their ratio on MT-Bench based on Eagle framework. Method # Drafts = 2, # Steps = 4 # Drafts = 4, # Steps = 3 EAGLE default sparse tree τ τ = 0.1 τ 3.04 0.02 RRS w/ replacement RRS w/o replacement 3.27 0.02 1.07 ( 0.02) 2.96 0.02 1.05 ( 0.01) 3.42 0.03 1.07 ( 0.02) SpecHub Greedy 3.63 0.02 1.19 ( 0.02) 3.62 0.02 1.19 ( 0.02) 3.39 0.01 1.20 ( 0.02) 3.70 0.03 1.16 ( 0.02) 2.83 0.02 3.19 0.03 - - - - - - - = 0.6 3.22 0.02 RRS w/ replacement RRS w/o replacement 3.52 0.02 1.09 ( 0.02) 3.39 0.01 1.09 ( 0.01) 3.71 0.02 1.09 ( 0.02) SpecHub Greedy 3.52 0.02 1.09 ( 0.02) 3.52 0.02 1.09 ( 0.02) 3.45 0.01 1.11 ( 0.01) 3.66 0.02 1.07 ( 0.02) 3.11 0.01 3.41 0. - - - - - - - = 1.0 3.14 0.02 RRS w/ replacement RRS w/o replacement 3.22 0.02 1.02 ( 0.02) 3.25 0.01 1.05 ( 0.01) 3.43 0.02 1.06 ( 0.02) SpecHub Greedy 3.35 0.02 1.07 ( 0.02) 3.33 0.02 1.06 ( 0.02) 3.34 0.01 1.08 ( 0.01) 3.33 0.02 1.03 ( 0.02) 3.09 0. 3.22 0.02 - - - - - - - Remark 8. For # Drafts = 2, # Steps = 4, and = 0.6, three methods - RRS without replacement, SpecHub, and Greedy - show similar average generation lengths. After truncating to two decimal places, they appear to be the same. However, they are actually different numbers: 3.51781, 3.51944, 3.51975."
        }
    ],
    "affiliations": [
        "Adobe Research, San Jose, CA, USA",
        "Department of Computer Science, University of Maryland, College Park, MD, USA",
        "Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA",
        "Manning College of Information & Computer Sciences, University of Massachusetts Amherst, MA, USA"
    ]
}