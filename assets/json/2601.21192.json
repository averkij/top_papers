{
    "paper_title": "Do Reasoning Models Enhance Embedding Models?",
    "authors": [
        "Wun Yu Chan",
        "Shaojin Chen",
        "Huihao Jing",
        "Kwun Hang Lau",
        "Elton Chun-Chai Li",
        "Zihao Wang",
        "Haoran Li",
        "Yangqiu Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 2 9 1 1 2 . 1 0 6 2 : r Do Reasoning Models Enhance Embedding Models? Wun Yu Chan1, Shaojin Chen1, Huihao Jing1, Kwun Hang Lau1, Elton Chun-Chai Li1, Zihao Wang1, Haoran Li1, Yangqiu Song1 1CSE, HKUST Correspondance: wychanbu@connect.ust.hk Reasoning-Embedding Reasoning-Embedding January 30, Abstract State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals null effect: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce Hierarchical Representation Similarity Analysis (HRSA), framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifolds local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between baseand reasoning-initialized models, phenomenon we term Manifold Realignment. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself."
        },
        {
            "title": "1 Introduction",
            "content": "Vector representations of text, known as text embeddings, are core abstraction in modern natural language processing (NLP) (Mikolov et al., 2013). As Large Language Models (LLMs) continue to evolve, embedding models have now been built by adapting decoder-only LLMs (Lee et al., 2025a; Zhang et al., 2025; Lee et al., 2025b) as backbones to leverage the rich semantics and world knowledge stored in their parameters. Most recently, reasoning models optimized via Reinforcement Learning with Verifiable Rewards (RLVR) on base models have demonstrated qualitative leap in complex problem-solving and reasoning (DeepSeek-AI, 2025; Lambert et al., 2024; Xu et al., 2025; Zheng et al., 2025). This development raises natural hypothesis for representation learning: Does the enhanced reasoning translate to superior text embedding space? Intuitively, model that \"thinks\" more deeply should structure semantic relationships more effectively. Counter-intuitively, our results reveal null effect. Across comprehensive benchmarks including MTEB(Multilingual, v2) (Enevoldsen et al., 2025), MTEB(Code, v1) (Muennighoff et al., 2023), and BRIGHT (Su et al., 2025), embedding models initialized from RLVR-tuned reasoning models perform statistically identically to base-initialized models after contrastive learning (van den Oord et al., 2018; Gao et al., 2021). This observation presents scientific puzzle: Why do reasoning and non-reasoning backbones yield indistinguishable results following contrastive learning? In this paper, we argue that existing performance metrics are insufficient for diagnosing the internal dynamics of representations. We introduce Hierarchical Representation Similarity Analysis (HRSA), hierarchical analysis framework inspired by Representational Similarity Analysis (RSA) (Kornblith et al., 2019). HRSA allows us to dissect model similarity at increasing levels of abstraction: Representation Level: Focus on the coordinate basis and features. Geometry Level: Focus on the shape (geometry) of the latent manifold. Figure 1: Latent manifold and model relationships. CL, SFT, and RLVR denote Contrastive Learning, Supervised Fine-Tuning, and Reinforcement Learning with Verifiable Rewards, respectively. indicates the representations of the corresponding models. Suffix -Emb is added to the model name to indicate the embedding model. We demonstrate the ideas of similar and dissimilar representations of RLVR-tuned pairs and SFT-tuned pairs, respectively. Function Level: Focus on the input-output mappings. Applying HRSA uncovers phenomenon we term Manifold Realignment. We find that RLVR largely preserves the global geometry of the latent manifold, including the linear readout associated with downstream tasks, while irreversibly reshaping the manifolds local geometry. The resulting drift in the coordinate basis is modest under typical training regimes but becomes pronounced under prolonged RLVR. Strikingly, when these backbones are later adapted into embedding models via contrastive learning, both baseand reasoning-initialized models exhibit strong realignment even in the presence of coordinate basis changes. We interpret the realignment as evidence that representational drift is largely reversible at the global level, yet accompanied by irreversible local distortions. Overall, our results suggest that, unlike SFT, RLVR primarily optimizes trajectories through an existing semantic landscape rather than fundamentally redrawing the landscape itself. Our contributions are as follows: 1. Systematic Benchmarking: We conduct the first controlled comparison of RLVR-optimized vs. its base model as backbones for text embeddings by fine-tuning diverse suite of state-of-the-art reasoning models into embedding models and evaluate them against their base counterparts, establishing that current RLVR methods do not inherently improve embedding quality. 2. The HRSA Framework: We propose hierarchical RSA framework (Representation, Geometry, Function) to diagnose why models behave similarly, offering toolkit for future interpretability studies, and unifying the disorganized RSA framework. 3. Discover Manifold Realignment: We demonstrate that RLVR do not fundamentally alter the latent manifold, but it can reorganize the local neighborhood structure, and only the coordinate basis will be drifted when training is prolonged. However, the contrastive learning can overwrite the reversible drift and exhibit strong alignment between baseand reasoning-initialized embedding models."
        },
        {
            "title": "2 Embedding Model Performances",
            "content": "We first unify our terminology by explicitly separating starting checkpoint from the fine-tuning stage. Backbone LLMs. Given base model Mbase as trained LLM, we consider reasoning model Mreason as an LLM that undergoes either Supervised Fine-Tuning (SFT) or RLVR directly on top of the base model. We focus on 2 Table 1: Mean embedding benchmark performance (3 seeds). We compare the base backbone Mbase versus its RLVR-tuned reasoning model backbone Mreason; we also include an SFT-tuned backbone for reference. The (Std) column (gray) shows the mean performance gap standard deviation. The near-zero deltas for RLVR indicate that RLVR largely preserves the base models semantic effectiveness, contrasting with larger shifts under SFT. Model Pair Backbone SFT Qwen3-0.6B-Base vs Qwen3-0.6B MTEB(Multilingual, v2) base MEmb reason (Std) MEmb MEmb MTEB(Code, v1) BRIGHT base MEmb reason (Std) MEmb base MEmb reason (Std) 53.50 41.47 -12.03 0.14 55. 56.28 +0.99 0.05 13.06 13.71 +0.65 0.06 RLVR Qwen2.5-1.5B vs Qwen2.5-1.5B-SRL-Zoo 54.73 Qwen2.5-0.5B vs Qwen2.5-0.5B-SRL-Zoo 51.25 46.19 DS-Distill-1.5B vs NV-ProRL 59.85 Qwen3-4B vs Qwen3-4B-PSR 54.54 51.27 46.25 59.79 -0.19 0.07 58.98 +0.02 0.09 57.41 +0.06 0.06 45.47 63.90 -0.06 0. 58.72 -0.26 0.08 17.71 57.53 +0.12 0.07 14.05 9.02 45.87 +0.40 0.07 64.57 +0.67 0.03 18.10 17.89 +0.18 0.04 -0.06 0.05 13.99 +0.45 0.02 9.47 18.17 +0.07 0.03 zero-RL (DeepSeek-AI, 2025) where RLVR starts directly from the base model without performing warm-start SFT stage. Concretely, we evaluate and compare matched pair of Mbase and Mreason, where Mreason must be fine-tuned on Mbase. The SFT-tuned pairs are used as an explicit control to highlight the very close similarity observed in the RLVR-tuned comparisons. Embedding models. We term the base embedding model MEmb reason with the backbone Mbase and Mreason respectively. The embedding models are formed by removing the language modeling head and applying pooling operator to the final-layer hidden states to produce fixed-dimensional vector. We train embedding models with an InfoNCE objective (van den Oord et al., 2018) to align semantically similar texts. Within pair the two embedding models share identical architectures and training recipes, differing only in their backbone initialization (i.e., Mbase vs. Mreason). Appendix provides training details. base and reasoning embedding model MEmb To rigorously assess the impact of RLVR optimization on embedding benchmark, we trained and evaluated multiple matched pairs consisting MEmb reason. We evaluated these models across diverse suite of benchmarks, including MTEB(Multilingual, v2) (Enevoldsen et al., 2025), MTEB(Code, v1) (Muennighoff et al., 2023), and BRIGHT (Su et al., 2025), to ensure coverage of retrieval, clustering, and semantic similarity tasks, as well as the data in the same domain as trained in RLVR. base and MEmb The results, presented in Table 1, reveal that MEmb reason with RLVR-tuned backbone Mreason consistently achieve performance parity with MEmb base across all benchmarks. Rather than interpreting this as limitation, we view it as significant indicator of representational robustness. The RLVR process refines the models trajectory-generation policy without destructively overwriting the rich world knowledge and semantic relationships established during pre-training."
        },
        {
            "title": "3 The HRSA Framework: Dissecting Model Similarity",
            "content": "In Section 2, we established that MEmb reason with RLVR-tuned backbone maintain performance parity with MEmb base across all benchmarks, exhibiting no degradation in general semantic tasks. This macroscopic observation suggests hypothesis that the RLVR optimization trajectory preserves the intrinsic geometry of the pre-trained Latent Manifold Z, altering only the policy for traversing it rather than the landscape itself. To rigorously test this hypothesis, we must look beyond aggregate benchmark scores, which can mask internal representational shifts. We introduce HRSA to dissect the relationship between Mbase and Mreason at three nested levels of abstraction. Crucially, HRSA is not defined by the specific metrics used in this study (e.g., CKA), but by the invariance properties required at each level of abstraction. Researchers can substitute metrics or theoretical constraints, provided they respect the hierarchys invariance rules. See Table 8."
        },
        {
            "title": "3.1 Common Setup and Notation",
            "content": "Figure 2: The overview of HRSA. To analyze the structural differences between models, we compare their representations on shared sequence of token positions. Let X, RN denote the D-dimensional token-level representation matrices produced by Mbase (or MEmb reason) respectively. The i-th row of each matrix, denoted as xi or yi, represents single token embedding such that xi, yi Z, where RD is the latent manifold induced by the distribution of the mapped inputs within the high-dimensional ambient space. base ) and Mreason (or MEmb"
        },
        {
            "title": "3.2 Representation-Level Analysis",
            "content": "Representation-level analysis focuses on the specific coordinate basis of the latent manifold and tests feature-wise correspondences between models. At this level, we treat the coordinate basis themselves as meaningful objects, where rotating or permuting features can change the outcome of the analysis. In other words, representation-level metrics are not invariant to orthogonal transformations or neuron permutations (see Appendix B.1 for formal discussion). Intuitively, this level investigates whether two models implement similar features along similar coordinate basis or realize similar solutions in very different coordinate systems. We operationalize this question with two complementary tools: 1. Dimension-Wise Correlation: probe direct, axis-aligned feature correspondence (Beyer et al., 2020). 2. Orthogonal Procrustes Analysis: probe global linear equivalence (Schönemann, 1966). 3.2.1 Dimension-Wise Correlation Dimension-wise correlation tests whether each coordinate in one model can be matched to the same coordinate in another model. Let X:j and Y:j denote the j-th column vectors (features) of the matrices and Y, respectively, corresponding to feature across all token positions. After centering each column over tokens, we define the correlation for dimension as ρj(X, Y) = , = 1, . . . , D. (1) (X:j)Y:j X:jY:j We summarize {ρj}D j=1 via mean. High per-dimension correlations indicate that many features are already aligned one-to-one without any transformation, while low per-dimension correlations suggest that, even if the models encode similar information overall, information carried by single feature in one model may be distributed across multiple features in the other. 3.2.2 Orthogonal Procrustes Analysis Dimension-wise correlation is strict: it does not allow any mixing between feature dimensions. Orthogonal Procrustes analysis relaxes this by asking whether one representation space can be mapped to the other via single orthogonal transformation. Formally, we solve = arg min OO=I XO Y2 , (2) where RDD is orthogonal and denotes the Frobenius norm. This objective allows global orthogonal mixing of features: each feature of can be an orthogonal combination of features in X. If is near-diagonal Table 2: HRSA result summary. It shows how different training algorithms impact the models manifold. SFT causes fundamental restructuring, whereas RLVR acts as trajectory optimization. Contrastive Learning (CL) successfully realigns the latent manifold. Level Metric Focus 1. Representation 2. Geometry 3. Function Manifold Status Coordinate Basis Global Geometry Local Geometry Linear Readout SFT (Backbone) Mbase vs Mreason Destructive Mixing Anisotropic Distortion Reorganized Degraded RLVR (Backbone) Mbase vs Mreason Preserved Isometric Reorganized Transferred Post-CL (Embedding) base vs MEmb MEmb Re-Aligned reason Stable Irreversible Aligned Fundamental Restructuring Trajectory Optimization Manifold Realignment or near-permutation, then features can be matched almost one-to-one after simple rotation or permutation. This corresponds to relatively localized, interpretable feature correspondences. In contrast, if is dense, then each feature in one model is distributed combination of many features in the other. The same information may be present, but in an entangled, non-localized form. We consider the inverse row entropy of as the quantified metric. See Appendix B.1.2 for the details."
        },
        {
            "title": "3.3 Geometry-Level Analysis",
            "content": "Geometry-level analysis moves one step up in abstraction. Instead of caring about the specific coordinate system, we focus on the shape of the latent manifold. At this level, orthogonal rotations and neuron permutations are treated as irrelevant; the relative arrangement of points is paramount. Geometry-level metrics are therefore invariant to changes of coordinate basis, but sensitive to deformations that alter distances or local neighborhoods. See Appendix B.2. Conceptually, this level investigates whether two models organize embeddings into similar manifold shapes even using different axes. We study geometry-level similarity using two complementary metrics: 1. Linear Centered Kernel Alignment (Linear CKA): measures global geometry of manifold, via their Gram matrices, up to orthogonal transforms and isotropic scaling (Kornblith et al., 2019). 2. k-Nearest Neighbors (k-NN) Overlap: measures local geometry of manifold by quantifying the preservation of nearest-neighbor relationships (Lin & Smith, 2019). 3.3.1 Linear CKA CKA compares representations via their kernel matrices rather than their raw coordinates. In Linear CKA, we consider the linear kernel KX = XX and KY = YY, where KX , KY RN . The HilbertSchmidt Independence Criterion (HSIC) (Gretton et al., 2005) between the kernels KX and KY is HSIC(KX , KY ) = 1 (N 1)2 tr(KX HKY H). (3) where = 1 then 11 RN is the centering matrix, is the identity matrix and 1 is vector of 1. Linear CKA is CKA(X, Y) = HSIC(KX , KY ) (cid:112)HSIC(KX , KX ) HSIC(KY , KY ) . (4) Linear CKA quantifies how similarly two models organize the global geometry of manifolds, capturing features like cluster structure and anisotropy, by assessing whether their manifolds can be aligned through orthogonal transformations and uniform scaling, without the need for nonlinear deformations. Harvey et al. (2025) has shown that Linear CKA quantifies the average alignment between optimal linear readouts across distribution of decoding tasks. However, it can be manipulated without large changes in functional behavior under high dimensions (Davari et al., 2023; Hayne et al., 2024), so it should not be interpreted as direct proxy for linear separability or task equivalence. Here, we use Linear CKA specifically as global geometry descriptor. 5 Dimension-Wise Correlation RLVR SFT Linear CKA SFT RLVR Qwen2.5 vs DS DS vs ProRL Qwen2.5 vs DS DS vs ProRL d e l M B l M m Qwen2.5-Emb vs DS-Emb DS-Emb vs ProRL-Emb Qwen2.5-Emb vs DS-Emb DS-Emb vs ProRL-Emb Reasoning Model Layer Index Figure 3: Heatmap of Dimension-Wise Correlation (left) and Linear CKA (right). Columns: SFT vs. RLVR. Rows: Mbase vs. Mreason and MEmb base vs. MEmb reason. 3.3.2 k-NN Overlap While CKA captures global manifold geometry, k-NN overlap focuses on local manifold geometry. Intuitively, it investigates whether each embeddings local neighborhood is preserved between two models. Let the cosine similarity be sZ(i, j) = zj zi zj , where zi RD is the i-th embedding (row) of the representation mak (i) = TopKj sX(i, j) trix Z. We define the k-nearest neighbor sets under models with representations and as and k (i) = TopKj sY(i, j), respectively. The k-NN overlap score sk is (i)(cid:12) (cid:12) (i)(cid:12) (cid:12) (i) (i) (cid:12) (cid:12)N (cid:12) (cid:12)N sk = (cid:88) 1 i=1 (5) where we use the Jaccard index to quantify agreement of neighbor sets. Because neighborhood relations are preserved under orthogonal transforms and permutations, but disrupted by non-isometric distortions (e.g., anisotropic scaling), k-NN overlap directly reflects how similarly two models instantiate the local geometry of the manifold."
        },
        {
            "title": "3.4 Function-Level Analysis",
            "content": "Function-level analysis abstracts away from the internal representation manifold to focus strictly on the inputoutput transformations exhibited during downstream tasks. Two models may have very different representationor geometrylevel metrics, yet still be functionally similar if the same tasks are solvable with comparable readouts. Conversely, even modest changes in embeddings can yield different behaviors under specific decoders. See Appendix B.3. We instantiate function-level similarity with the Cross-Model Linear Probes (Nikooroo & Engel, 2025) to test whether the same linear readout generalizes across models. 3.4.1 Cross-Model Linear Probes Cross-model linear probes provide task-conditioned measure of function-level similarity between embedding spaces. Let RN be the labels. We first fit linear probe on X: where (WX , bX ) are learned via logistic or ridge regression, depending on the task. We then freeze (WX , bX ) and apply the same linear map to representations from the other model. High cross-model performance (probe trained on ˆy = XWX + bX (6) 6 X, evaluated on Y) relative to self-performance (trained and evaluated on X) indicates that the same linear decision boundary is useful in both spaces. This implies strong function-level similarity: two models support essentially the same set of linearly decodable functions for that task."
        },
        {
            "title": "3.5 From Representations to Functions: How the Levels Fit Together",
            "content": "HRSA forms hierarchy of abstraction over the same underlying representations. Representation level asks whether the latent manifolds of two models share the same coordinate basis. Geometry level discards the choice of coordinate basis and asks whether the latent manifold has similar shape globally and locally. Function level discards most geometric detail and asks which inputoutput mappings are supported and realized. By separating these levels, we can distinguish: Cases where Mbase and Mreason differ mainly by reparameterization (e.g., rotation) but preserve geometry and function. Cases where global or local geometry changes but downstream behavior remains similar, suggesting redundant internal solutions. Cases where modest representational changes inTable 3: The inverse row entropy of the orthogonal matrix O. For each training method (SFT or RLVR), we report Mbase vs. Mreason and MEmb reason (suffix -Emb) comparisons. higher inverse row entropy indicates corresponds to one-to-one feature mapping. base vs. MEmb Model Pair Inverse Row Entropy SFT: Qwen2.5 vs DS Qwen2.5-Emb vs DS-Emb RLVR: DS vs ProRL DS-Emb vs ProRL-Emb 0.108 0.142 0.161 0.863 duce large functional differences in readout directions, revealing sensitive or brittle aspects of the models decision rules. This decomposition allows us to turn the initial puzzlewhy MEmb reason look so similar on benchmarksinto structured investigation of where any differences live. Is the reason of the negligible deviation lived in their underlying backbone models? base and MEmb"
        },
        {
            "title": "4 Evaluation Setups",
            "content": "To empirically validate the hypothesis, we apply the HRSA framework across two dimensions: LLMs (Mbase vs. Mreason) and downstream adaptation (MEmb reason). Furthermore, we extend this analysis by also comparing the SFT-tuned reasoning models, showing the clear differences in SFT-tuned and RLVR-tuned reasoning models. See Appendix for more experiment results. base vs. MEmb Datasets. To verify if the latent manifold is preserved even within reasoning trajectories, we construct Chainof-Thought (CoT) dataset and use the hard-level subset for evaluation. Further generation details are provided in Appendix C. In function-level analysis, we use the AGs News Topic Classification Dataset (Zhang et al., 2015) to evaluate the linear readout directions. Models. For SFT comparison, we use Qwen2.5-Math-1.5B (base) (Yang et al., 2024) vs. DeepSeek-R1-Distill-Qwen1.5B (reasoning) (DeepSeek-AI, 2025). For RLVR comparison, we use DeepSeek-R1-Distill-Qwen-1.5B (base) vs. Nemotron-Research-Reasoning-Qwen-1.5B (Liu et al., 2025b) (reasoning, trained with prolonged RLVR). We abbreviate these as Qwen2.5, DS, and ProRL respectively. Downstream embedding models add -Emb suffix. Additional results for RLVR models with different training algorithms (GRPO (Shao et al., 2024), DAPO (Yu et al., 2025)) and training datasets are in Appendix D, showing consistent latent manifold across variations. We also demonstrate the training dynamic of manifold realignment. Our HRSA analysis examines model activations at every layer, before any pooling. Specifically, for each model in matched pair with layers, we collect the entire set of hidden states: {Xl}L l=1, where each Xl, Yl RN D. This per-layer, per-token perspective preserves the full representational structure for more comprehensive analysis, avoiding any information loss due to pooling. l=1 and {Yl}L 7 SFT RLVR r A (a) Qwen2.5 vs DS (b) Qwen2.5-Emb vs DS-Emb (c) DS vs ProRL (d) DS-Emb vs ProRL-Emb Figure 4: Cross-Model Linear Probe Results. For each dataset split (train, dev, test), the left bar corresponds to Mbase (or MEmb reason). The linear probe is trained on Mbase (or MEmb base in embedding model analysis) representations and evaluated on both models. The smaller the , the stronger the cross-model linear probe transfer. base) and the right bar corresponds to Mreason (or MEmb"
        },
        {
            "title": "5.1 Representation-Level Results",
            "content": "Dimension-Wise Correlation Figure 3 (left) shows that SFT yields weak axis-aligned feature correspondence, while RLVR retains substantially higher per-dimension correlations. Notably, the clearest deviation from diagonal structure appears only under prolonged RLVR (our main RLVR example), whereas contrastive learning largely restores axis alignment between the resulting embedding models, consistent with Manifold Realignment. Table 4: k-NN mean overlap across layers between Mbase and Mreason (and their Emb variants). Higher mean overlap indicates more preservation in local geometry of latent manifold. Model Pairs Mean Overlap k=5 k= k=50 SFT: Qwen2.5 vs DS Qwen2.5-Emb vs DS-Emb Orthogonal Procrustes Analysis Table 3 supports this global alignment perspective. While SFT results in dense orthogonal map (implying high feature mixing), prolonged RLVR yields an that is nearly permutation matrix, becoming strongly permutative after contrastive learning. Table 12 shows that is already near-permutation for most Mbase vs. RLVR-tuned Mreason comparisons. This suggests RLVR does not induce feature mixing; instead, coordinate basis drift is limited to prolonged training scenarios (as in ProRL). RLVR encourages the model to construct correct paths using existing capabilities, learning only the sequence of feature activations required for rewards. Thus, the coordinate basis remains largely unchanged. DS-Emb vs ProRL-Emb RLVR: DS vs ProRL 0.577 0.531 0.455 0.451 0.484 0.474 0.052 0.069 0.068 0.068 0.132 0."
        },
        {
            "title": "5.2 Geometry-Level Results",
            "content": "base and MEmb Linear CKA Figure 3 (right) shows sharp contrast in global manifold geometry. Linear CKA drops under SFT but remains high under RLVR, consistent with an approximately isometric relationship. After contrastive learning, the MEmb reason move even closer in CKA, highlighting Manifold Realignment at the geometry level. RLVR functions as near-isometric transformation, rigidly preserving the shape of the latent manifold. Consequently, the semantic distances established during pre-training remain invariant, which explains why downstream embedding performance does not improve. k-NN Overlap Table 4 shows that RLVR preserves substantially more local structure (higher mean overlap) than SFT, yet overlap remains substantially below 1, indicating local geometry reorganization. This gap persists even when the embedding model manifolds are pulled closer by contrastive learning, showing the idea that RLVR introduces irreversible local geometry reorganization, which is different to the rigid global geometry. We hypothesize that this irreversible local reorganization reflects RLVR optimization in grouping related reasoning steps effectively, clusters the decision trajectory without altering the global semantic map."
        },
        {
            "title": "5.3 Function-Level Results",
            "content": "Cross-Model Linear Probes Figure 4 shows stronger cross-model probe transfer under RLVR than SFT, implying that task-relevant linear readout directions of the latent manifold are more stable. For the embedding model pairs, transfer remains consistently high, reflecting Manifold Realignment: contrastive learning maintains strong functional alignment even when local geometry do not fully coincide."
        },
        {
            "title": "5.4 Manifold Realignment in Training Dy-\nnamics",
            "content": "Figure 5 illustrates the dynamics of adapting LLMs to embedding models over training steps. By applying HRSA to intermediate checkpoints, we observe that manifold realignment occurs rapidly in the early training stages (Steps 0200), after which representational similarity stabilizes. This trajectory demonstrates the manifold realignment, which contrastive learning effectively drives strong alignment between baseand reasoning-initialized embedding models. In contrast, the k-NN mean overlap across layers decreases during this process, confirming that the RLVR-induced reorganization of local geometry is irreversible."
        },
        {
            "title": "6 Related Works",
            "content": "Figure 5: The training dynamics of the embedding model pairs DS-Emb vs ProRL-Emb. Step 0 indicates LLM backbones, and step 781 indicates the final checkpoint of the embedding models."
        },
        {
            "title": "6.1 Reinforcement Learning with Verifiable Rewards (RLVR)",
            "content": "RLVR optimizes models using deterministic, verifiable rewards rather than heuristic preference signals (DeepSeek-AI, 2025). Recent analyses suggest RLVR stays close to the pretrained solution (e.g., KL-anchored/on-policy behavior) (Shenfeld et al., 2025) and improves via weight updates that avoid large principal-subspace changes (Zhu et al., 2025a), without introducing fundamentally novel reasoning beyond the base model (Yue et al., 2025). However, these works do not directly characterize the representational changes induced by RLVR; we show (via HRSA) that RLVR largely preserves global manifold structure while reorganizing local geometry and, with prolonged training, exhibiting some coordinate basis drift."
        },
        {
            "title": "6.2 Embedding Models",
            "content": "Many state-of-the-art text embedding models now leverage decoder-only LLM backbones with bidirectional attention and contrastive training to produce strong encoders (Zhang et al., 2025; Lee et al., 2025b). While reward-driven or RLbased embedding learning has been explored (Tennenholtz et al., 2024; Gui & Cheng, 2025), it remains unclear whether RLVR-tuned reasoning models improve embedding geometry or retrieval. Our study directly tests this connection and finds that RLVR-tuned reasoning models do not reliably enhance embedding quality."
        },
        {
            "title": "6.3 Representational Similarity Analysis",
            "content": "Representational similarity analysis (RSA) and related metrics (e.g., CKA) are widely used to compare layer representations across models and tasks (Kriegeskorte et al., 2008; Kornblith et al., 2019; Klabunde et al., 2023; Yousefi et al., 2023; Liu et al., 2025c). Prior work typically reports single-level alignment and does not organize how changes manifest across abstraction levels, nor does it connect RLVR update properties to representation geometry (Shenfeld 9 et al., 2025; Balashov, 2025). We address this with HRSA, which disentangles coordinate basis, manifold geometry, and readout-direction changes, showing substantial global preservation in RLVR-tuned reasoning models."
        },
        {
            "title": "7 Discussion and Conclusion",
            "content": "In this paper, we introduced HRSA, hierarchical representation similarity analysis framework for diagnosing how training reshapes the latent manifold, and conducted the first systematic benchmarking of RLVR-optimized vs. its base model as backbones for text embedding models. Applying HRSA to base backbones and their RLVR-tuned backbones, we identified which components of the latent manifold change and characterized consistent pattern we term manifold realignment. Across settings, RLVR largely preserves global geometry and linear readout, while producing irreversible reorganization of local geometry. Coordinate basis drift emerges primarily under prolonged RLVR, but appears reversible: subsequent contrastive learning corrects this drift and reinstates strong realignment. These results support the view that RLVR primarily optimizes trajectories through an existing semantic landscape rather than rewriting that landscape itself. As latent-space-centric paradigms such as World Models (Ha & Schmidhuber, 2018) and JEPA (Huang et al., 2025) gain prominence, our findings point to practical trade-off: RLVR tends to preserve the base models representational backbone (which may help retain broad generalization), yet on its own is unlikely to fundamentally improve the underlying global organization of the latent manifold. Put differently, RLVR seems to move behavior mainly by reshaping local geometry (how nearby states relate) while leaving the large-scale coordinate system and linear readout mostly intact. Our analysis also suggests an actionable hypothesis for training design: if RLVRs distinctive footprint is local geometry reorganization under global geometry stability, then similar behavior might be achievable via SFT augmented with geometryand basis-aware regularization. For example, one could explicitly constrain global manifold distances or penalize excessive coordinate basis drift while encouraging controlled local geometry reorganization. Testing whether such constrained SFT can match RLVRs representational effects offers concrete direction for follow-up work. Several open questions remain about the mechanism. In particular, we do not yet fully explain why RLVR produces persistent local geometry reorganization while leaving global geometry and linear readout directions relatively stable, nor what training signals govern the onset and reversibility of coordinate basis drift. Progress here may require controlled interventions (e.g., reward shaping, curriculum, or KL/entropy constraints) paired with HRSA to isolate which components of the RLVR objective drive each geometric effect. Finally, while our experiments focus on text embedding models, the hierarchy of effects uncovered by HRSA reflects training-agnostic geometric signature rather than modality-specific artifact. We therefore expect manifold realignment to be general phenomenon that extends to representation learning in vision and audio, and we position HRSA as practical diagnostic to verify this claim across modalities and objectives."
        },
        {
            "title": "References",
            "content": "An, C., Xie, Z., Li, X., Li, L., Zhang, J., Gong, S., Zhong, M., Xu, J., Qiu, X., Wang, M., and Kong, L. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https: //hkunlp.github.io/blog/2025/Polaris. Balashov, A. Reinforcement learning fine-tunes sparse subnetwork in large language models. arXiv preprint arXiv:2507.17107, 2025. Beyer, A., Kauermann, G., and Schütze, H. Embedding space correlation as measure of domain similarity. In LREC, pp. 24312439. European Language Resources Association, 2020. Bhaskar, A., Ye, X., and Chen, D. Language models that think, chat better. CoRR, abs/2509.20357, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. Davari, M., Horoi, S., Natik, A., Lajoie, G., Wolf, G., and Belilovsky, E. Reliability of CKA as similarity measure in deep learning. In ICLR. OpenReview.net, 2023. 10 de Souza Pereira Moreira, G., Osmulski, R., Xu, M., Ak, R., Schifferer, B., and Oldridge, E. Nv-retriever: Improving text embedding models with effective hard-negative mining. CoRR, abs/2407.15831, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025. Enevoldsen, K. C., Chung, I., Kerboua, I., Kardos, M., Mathur, A., Stap, D., Gala, J., Siblini, W., Krzeminski, D., Winata, G. I., Sturua, S., Utpala, S., Ciancone, M., Schaeffer, M., Misra, D., Dhakal, S., Rystrøm, J., Solomatin, R., Çagatan, Ö. V., Kundu, A., and et al. MMTEB: massive multilingual text embedding benchmark. In ICLR. OpenReview.net, 2025. Gao, T., Yao, X., and Chen, D. Simcse: Simple contrastive learning of sentence embeddings. CoRR, abs/2104.08821, 2021. Gretton, A., Bousquet, O., Smola, A., and Schölkopf, B. Measuring statistical dependence with hilbert-schmidt norms. In Algorithmic Learning Theory (ALT 2005), 16th International Conference, Proceedings, volume 3734 of Lecture Notes in Artificial Intelligence, pp. 6377. Springer, 2005. Gui, Y. and Cheng, J. Search-r3: Unifying reasoning and embedding generation in large language models. CoRR, abs/2510.07048, 2025. Ha, D. and Schmidhuber, J. World models. CoRR, abs/1803.10122, 2018. Harvey, S. E., Lipshutz, D., and Williams, A. H. What representational similarity measures imply about decodable information. In UniReps, volume 285 of Proceedings of Machine Learning Research, pp. 140151. PMLR, 2025. Hayne, L., Jung, H., and Carter, R. M. Does representation similarity capture function similarity? Trans. Mach. Learn. Res., 2024, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021. Huang, H., LeCun, Y., and Balestriero, R. LLM-JEPA: large language models meet joint embedding predictive architectures. CoRR, abs/2509.14252, 2025. Klabunde, M., Amor, M. B., Granitzer, M., and Lemmerich, F. Towards measuring representational similarity of large language models. CoRR, abs/2312.02730, 2023. Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. E. Similarity of neural network representations revisited. In ICML, volume 97 of Proceedings of Machine Learning Research, pp. 35193529. PMLR, 2019. Kriegeskorte, N., Mur, M., and Bandettini, P. A. Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in Systems Neuroscience, Volume 2 - 2008, 2008. ISSN 1662-5137. doi: 10. 3389/neuro.06.004.2008. URL https://www.frontiersin.org/journals/systems-neuroscience/articles/ 10.3389/neuro.06.004.2008. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tülu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models. In ICLR. OpenReview.net, 2025a. Lee, J., Chen, F., Dua, S., Cer, D., Shanbhogue, M., Naim, I., Ábrego, G. H., Li, Z., Chen, K., Vera, H. S., Ren, X., Zhang, S., Salz, D., Boratko, M., Han, J., Chen, B., Huang, S., Rao, V., Suganthan, P., Han, F., Doumanoglou, A., Gupta, N., Moiseev, F., Yip, C., Jain, A., Baumgartner, S., Shahi, S., Gomez, F. P., Mariserla, S., Choi, M., Shah, P., Goenka, S., Chen, K., Xia, Y., Chen, K., Duddu, S. M. K., Chen, Y., Walker, T., Zhou, W., Ghiya, R., Gleicher, Z., Gill, K., Dong, Z., Seyedhosseini, M., Sung, Y., Hoffmann, R., and Duerig, T. Gemini embedding: Generalizable embeddings from gemini. CoRR, abs/2503.07891, 2025b. 11 Lin, L. H. and Smith, N. A. Situating sentence embedders with nearest neighbor overlap. CoRR, abs/1909.10724, 2019. Liu, J., Liu, H., Xiao, L., Wang, Z., Liu, K., Gao, S., Zhang, W., Zhang, S., and Chen, K. Are your llms capable of stable reasoning? In ACL (Findings), pp. 1759417632. Association for Computational Linguistics, 2025a. Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. CoRR, abs/2505.24864, 2025b. Liu, X., Hsiung, L., Yang, Y., and Yan, Y. Spectral insights into data-oblivious critical layers in large language models. CoRR, abs/2506.00382, 2025c. Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient estimation of word representations in vector space. In ICLR (Workshop Poster), 2013. Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. MTEB: massive text embedding benchmark. In EACL, pp. 20062029. Association for Computational Linguistics, 2023. Muennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., and Kiela, D. Generative representational instruction tuning. CoRR, abs/2402.09906, 2024. Nikooroo, S. and Engel, T. Cross-model semantics in representation learning. CoRR, abs/2508.03649, 2025. Schönemann, P. H. generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):110, 1966. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Shenfeld, I., Pari, J., and Agrawal, P. Rls razor: Why online reinforcement learning forgets less. CoRR, abs/2509.04259, 2025. Su, H., Yen, H., Xia, M., Shi, W., Muennighoff, N., Wang, H., Liu, H., Shi, Q., Siegel, Z. S., Tang, M., Sun, R., Yoon, J., Arik, S. Ö., Chen, D., and Yu, T. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In ICLR. OpenReview.net, 2025. Tennenholtz, G., Chow, Y., Hsu, C., Shani, L., Liang, E., and Boutilier, C. Embedding-aligned language models. CoRR, abs/2406.00024, 2024. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In NeurIPS, 2024. Xu, S., Zhou, Y., Wang, W., Min, J., Yin, Z., Dai, Y., Liu, S., Pang, L., Chen, Y., and Zhang, J. Tiny model, big logic: Diversity-driven optimization elicits large-model reasoning ability in vibethinker-1.5 b. arXiv preprint arXiv:2511.06221, 2025. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report. CoRR, abs/2505.09388, 2025. Yousefi, S., Betthauser, L., Hasanbeig, H., Millière, R., and Momennejad, I. Decoding in-context learning: Neuroscienceinspired analysis of representations in large language models. arXiv preprint arXiv:2310.00313, 2023. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W., Zhang, Y., Yan, L., Qiao, M., Wu, Y., and Wang, M. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification, 2015. Zhang, Y., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., Huang, F., and Zhou, J. Qwen3 embedding: Advancing text embedding and reranking through foundation models. CoRR, abs/2506.05176, 2025. Zheng, C., Liu, S., Li, M., Chen, X., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., Zhou, J., and Lin, J. Group sequence policy optimization. CoRR, abs/2507.18071, 2025. Zhu, H., Zhang, Z., Huang, H., Su, D., Liu, Z., Zhao, J., Fedorov, I., Pirsiavash, H., Sha, Z., Lee, J., et al. The path not taken: Rlvr provably learns off the principals. arXiv preprint arXiv:2511.08567, 2025a. Zhu, X., Xia, M., Wei, Z., Chen, W., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement in LLM reasoning. CoRR, abs/2506.01347, 2025b."
        },
        {
            "title": "A Embedding Model Training",
            "content": "In this section, we reveal all the training details of the embedding models. A.1 Training Details We optimize the InfoNCE loss (van den Oord et al., 2018) defined in Equation 7. This objective aims to maximize the similarity between the query and the positive passage p, while simultaneously minimizing the similarity between and the negative passages. Let denote the set of in-batch passages (which includes and negatives from other instances), be the set of hard negatives, and sim be the cosine similarity. The loss is calculated as: L(q, p, B, ) = log exp(sim(q, p)/τ ) dBN exp(sim(q, d)/τ ) (cid:80) (7) We select decoder-only LLMs as the embedding model backbone, take the last layers activation as the final output, and perform mean pooling to obtain fixed-dimension embedding vector. We also enable bi-directional attention in the backbone by discarding the causal attention mask to capture more semantic details and relationships between tokens. We use mixed precision with bfloat16 and gradient checkpointing to reduce the memory pressure on the hardware. We use Flash Attention 2 as the attention backend algorithm. For more details on the settings, the reader can refer to Table 5. We employ the instruction-tuning technique. In particular, we use the instruction template Instruction: {instruction}nQuery: query, where {instruction} and {query} are the placeholders for the instruction and query, respectively. All of our training is conducted on 4x Nvidia L20 GPUs, with VRAM 44GB per GPU. Table 5: Training Hyperparameters Variables Batch Size Learning Rate (LR) LR Warm-up Ratio LR Scheduler Weight Decay Optimizer Padding Side Number of data Number of training steps Number of hard negatives Temperature Pooling Values 2048 2 105 0.03 Cosine 0.05 AdamW Right 1,603,172 782 3 0.02 Mean Although many prior works (Zhang et al., 2025; Lee et al., 2025b,a) use LoRA to train the embedding models, in our work, we discard it, since we find that training without LoRA yields better performance, and full parameters can better record the training dynamics. See Table 6. Table 6: LoRA comparison on performance in MTEB (Multilingual, v2). Model Performance With LoRA DS-Distill-Qwen-1.5B-Emb NV-ProRL-Emb Without LoRA DS-Distill-Qwen-1.5B-Emb NV-ProRL-Emb 42.450 42.064 46.185 46.247 14 A.2 Training Data Statistics We consider wide range of datasets, forming the training dataset by composing 11 separate datasets. We used Qwen3-Embedding-0.6B (Zhang et al., 2025) to mine 3 hard negatives per query, and employ the positive-aware hard negative mining technique introduced in de Souza Pereira Moreira et al. (2024), with 95% margin to the positive score. Table 7: Training Datasets Details Datasets Number of Samples FEVER NaturalQuestions NLI MSMARCO Quora Mr.Tydi DUReader TriviaQA HotpotQA SQuAD T2Ranking 105,893 97,912 277,217 499,184 94,443 102,796 17,493 65,465 167,808 84,494 90,467 Total 1,603,"
        },
        {
            "title": "B HRSA Proof",
            "content": "In this section, we provide more details on HRSA, including all the invariance properties of each level analysis and the proof of their invariance properties. We emphasize again that HRSA is not dependent on the specific metrics selected for this study, such as DimensionWise Correlation or Linear CKA. Rather, it is grounded in the hierarchy of invariance properties established earlier. Consequently, any metric that satisfies the invariance requirements of specific level can be employed to analyze that levels focus. Refer to Table 8 for summary of these properties and catalog of alternative valid metrics. B.1 Representation-Level Proof Definition 1 (Representation-Level Analysis). The representation-level analysis examines the explicit coordinate basis of the latent manifold. metric at this level must demonstrate sensitivity to coordinate basis rotations. Specifically: Non-invariant to: Orthogonal transformations (Rotation/Permutation) and General Linear transformations. B.1.1 Dimension-Wise Correlation Recall the definition of Dimension-Wise Correlation from equation 1. Proposition 1. Dimension-Wise Correlation is non-invariant to orthogonal transformations. Proof. Let RDD be an orthogonal matrix (QQ = I) such that = XQ. The j-th column becomes :j = (cid:80)D k=1 X:kQkj. The correlation of the j-th column becomes: ρj(XQ, ) = ((cid:80) (cid:80) X:kQkj)y:j X:kQkj2y:j2 . (8) Since mixes information from multiple columns X:k into the new column :j, the correlation with the fixed target y:j changes arbitrarily depending on Q. Thus, ρj(XQ, ) = ρj(X, ), satisfying the requirement for coordinate basis sensitivity. 15 Table 8: HRSA Framework Extensibility. The HRSA framework is defined by invariance properties, not specific metrics. Researchers can select alternative metrics (right column) for different modalities or theoretical needs, provided they respect the invariance constraints of the target analysis level. Level Invariance Constraints Default Metric Alternative Valid Metrics Representation Non-Invariant to: Orthogonal Transformation Dimension-Wise Correlation Optimal Transport (Wasserstein) Measures cost to move mass from basis to without rotation. Goal: Assess alignment of specific axes. Orthogonal Procrustes (O) Manifold Alignment Loss Direct penalization of feature mismatch. Geometry Function Invariant to: Orthogonal Transformation Non-Invariant to: Invertible Linear Transforms (Scaling/Shear) Linear CKA k-NN Overlap (Jaccard) RBF Kernel CKA Captures non-linear similarity. Riemannian Metrics Geodesic distance comparison. Invariant to: Any transform preserving Linear Probing Transfer Mutual Information I(X; ) Information theoretic upper bound. the decision boundary. Zero-Shot Accuracy Behavioral Consistency Exact match on downstream tasks. B.1.2 Orthogonal Procrustes Analysis Recall the Orthogonal Procrustes solution defined in Equation 2. To quantify the extent of coordinate alignment, we introduce the inverse row entropy, denoted as Hinv. We interpret the squared elements of each row in as probability distribution. This is mathematically valid because is orthogonal, meaning its rows have unit Euclidean norm (i.e., (cid:80) j(O We compute Hinv by calculating the mean row entropy, normalizing it by the maximum possible entropy (log D), ij)2 = 1). and taking the complement: = 1 log Hinv = 1 (cid:88) (cid:88) (O ij)2 log(O ij)2 i=1 j=1 where ij denotes the element of at row and column j, and represents the dimensionality. The intermediate term is normalized to the range [0, 1]. Consequently, higher Hinv indicates that the coordinate basis is preserved (i.e., is sparse and approximates permutation matrix), whereas lower Hinv indicates that features are \"smeared\" or rotated across multiple dimensions. Proposition 2. The structure of the optimal mapping in Orthogonal Procrustes Analysis is non-invariant to orthogonal transformations. Proof. For any orthogonal matrices Q, RDD, if we transform X, to = XQ, = R, then an optimal map for the new problem is O(X , ) = QO(X, )R. (9) This is conjugation of by orthogonal matrices, which in general destroys diagonality or one-hot structure. Remark. One may claim that Orthogonal Procrustes Analysis should be classified as geometry-level measurement because the residual is invariant to orthogonal transformation. In our work, we only focus on the structure of O, specifically by considering its inverse row entropy. As shown in Proposition 2, remains dependent on the chosen coordinate system. B.2 Geometry-Level Proof Definition 2 (Geometry-Level Analysis). The geometry-level analysis examines the intrinsic shape and topology of the latent manifold Z. Metrics at this level must quantify the arrangement of points relative to one another, independent of the specific coordinate system used to describe them. Invariant to: Similarity transformations, defined as the composition of orthogonal rotation/reflection (Q RDD, QQ = I) and isotropic scaling (c R, > 0). Non-invariant to: Anisotropic linear transformations (e.g., non-uniform scaling, shearing) where the transformation matrix satisfies AA = cI. In the following, we provide proofs for the invariance properties of Linear CKA and Cosine k-NN Overlap. B.2.1 Linear CKA Recall that Linear CKA is defined via the HilbertSchmidt Independence Criterion (HSIC) of centered Gram matrices. Let KX = XX and = 1 11. Proposition 3. Linear CKA is invariant to similarity transformations (cid:55) cXQ where > 0 and is orthogonal. Proof. Let = cXQ. We first derive the Gram matrix for the transformed representation: KX = (cXQ)(cXQ) = c2XQQX . Since is orthogonal (QQ = I), this simplifies to: KX = c2XX = c2KX . Now we examine the HSIC term in the numerator. Using the property tr(cA) = tr(A): HSIC(KX , KY ) = = 1 (N 1)2 tr(KX HKY H) (N 1)2 tr(c2KX HKY H) 1 Similarly, for the normalization term in the denominator: = c2 HSIC(KX , KY ). HSIC(KX , KX ) = 1 (N 1)2 tr(c2KX Hc2KX H) = c4 HSIC(KX , KX ). Substituting these into the full Linear CKA equation: CKA(X , ) = = c2HSIC(KX , KY ) (cid:112)c4HSIC(KX , KX ) HSIC(KY , KY ) c2HSIC(KX , KY ) c2(cid:112)HSIC(KX , KX ) HSIC(KY , KY ) = CKA(X, ). The scalar factors cancel perfectly, proving invariance. Proposition 4. Linear CKA is generally non-invariant to anisotropic linear transformations. Proof. Let = XA, where RDD is invertible and anisotropic (AA = cI). The Gram matrix becomes: KX = XAAX . (10) (11) (12) (13) (14) (15) Let = AA. The numerator HSIC term becomes proportional to tr(XM HKY H). Unlike the isotropic case, the matrix is \"trapped\" between and inside the trace. Unless is scalar multiple of the identity, it reweights the singular values of X, effectively altering the principal components of the representation space. Since Linear CKA measures the alignment of these principal components, CKA(XA, ) = CKA(X, ). 17 B.2.2 k-NN Overlap As defined in the Section 3.3.2, k-NN overlap relies on the ranking of cosine similarities s(u, v) = uv uv . Proposition 5. Cosine-based k-NN Overlap is invariant to similarity transformations. Proof. Let and be any two embedding vectors (rows of X). We apply the transformation = cQx and = cQy, with > 0 and QQ = I. The cosine similarity between the transformed vectors is: s(x, y) = (cQx)(cQy) cQxcQy = c2xQQy (cid:112)(cQx)(cQx)(cid:112)(cQy)(cQy) . Using the orthogonality property QQ = I: s(x, y) = = = c2xy c2xx(cid:112)c2yy c2(xy) cx cy xy xy = s(x, y). (16) (17) Since the pairwise similarity scores remain exactly the same, the ranking of neighbors is preserved. Thus, the set of top-k nearest neighbors is identical: (i), and the overlap score is invariant. (i) = Proposition 6. Cosine-based k-NN Overlap is generally non-invariant to anisotropic linear transformations. Proof. Let = Ax and = Ay with anisotropic A. The transformed similarity is: s(x, y) = xAAy xAAx(cid:112)yAAy . (18) Let = AA. This expression represents the cosine of the angle between and in space equipped with the inner product u, vM = uM v. Because is anisotropic, has distinct eigenvalues. This transformation distorts angles: vectors aligned with the large eigenvectors of are \"pulled\" closer together in angular space, while vectors aligned with small eigenvectors are pushed apart. Consequently, if we have s(x, y) > s(x, z) (meaning is closer neighbor to than z), an anisotropic can reverse this relationship such that s(x, z) > s(x, y). This alters the composition of the k-nearest neighbor sets, changing the overlap score. B.3 Function-Level Proof Definition 3 (Function-Level Analysis). The function-level analysis examines the usable information accessible via linear readouts (probes) or the final behavioral output. This level specifically tests whether two models share the same readout directions for solving task. Invariant to: Isomorphic transformations if and only if the readout mechanism is transformed correspondingly. Non-invariant to: Linear Reparameterization under fixed readout hypothesis. B.3.1 Cross-Model Linear Probes be the optimal probe weights for task on representations X, i.e., = argminwXw Z2. We evaluate Let these weights on : Error = w Z2. 18 Table 9: LLM pairs used in additional HRSA analyses, separated by the training algorithms (SFT, RLVR). Base Model Mbase SFT Qwen2.5-Math-1.5B RLVR Qwen3-4B DeepSeek-R1-Distill-Qwen-7B Qwen2.5-7B Qwen2.5-1.5B Qwen2.5-0.5B DeepSeek-R1-Distill-Qwen-1.5B Qwen3-4B Reasoning Model Mreason Algorithm Data DeepSeek-R1-Distill-Qwen-1.5B SFT Mixed Polaris-4B-Preview Polaris-7B-Preview zero__ppo__think__Qwen2.5-7B Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo Nemotron-Research-Reasoning-Qwen-1.5B Qwen3-4B-PSR DAPO DAPO PPO GRPO GRPO GRPO PSR Math Math Chat Math Math Math Math Table 10: Embedding model pairs used in additional HRSA analyses. All of the embedding models are trained on the same dataset with InfoNCE loss. They are separated by the training algorithms used to train their reasoning model backbone. Base Embedding Model MEmb base Reasoning Embedding Model MEmb reason SFT Qwen2.5-Math-1.5B-Emb Qwen3-0.6B-Base-Emb RLVR Qwen2.5-1.5B-Emb Qwen2.5-0.5B-Emb DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen3-4B-Emb DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen3-0.6B-Emb Qwen-2.5-1.5B-SimpleRL-Zoo-Emb Qwen-2.5-0.5B-SimpleRL-Zoo-Emb Nemotron-Research-Reasoning-Qwen-1.5B-Emb Qwen3-4B-PSR-Emb Proposition 7. Cross-Model Linear Probes are non-invariant to linear reparameterization under fixed readout. Proof. Assume contains the exact same information as but is linearly transformed: = XA (where is invertible). The prediction using transferred weights is: ˆZY = X = (XA)w . (19) = Xw The original prediction was ˆZX = Xw . For the predictions to be identical ( ˆZY = ˆZX ) for all X, we require XAw , implying Aw . This equality only holds if is an eigenvector of with eigenvalue 1. For general transformation A, Aw . Therefore, even if is geometrically isomorphic to X, the cross-model probe will fail if the direction of the solution has shifted. This proves the metric satisfies the requirement set in Definition 3. = = w"
        },
        {
            "title": "C CoT Datasets",
            "content": "To rigorously verify if the latent manifold is preserved within reasoning trajectories (as discussed in Section 4), we constructed specialized CoT-Activations dataset. Unlike standard semantic datasets, this corpus focuses on long-range, multi-step reasoning traces generated by state-of-the-art reasoning models. C.1 Dataset Composition and Hierarchy We curated diverse suite of mathematical reasoning benchmarks to ensure our analysis covers varying degrees of reasoning complexity, ranging from elementary arithmetic to competition-level problem solving. The dataset is stratified into three difficulty levels: 19 Easy: Sourced from GSM8K (Cobbe et al., 2021), focusing on grade-school math word problems that require multi-step arithmetic but limited abstract reasoning. Moderate: Sourced from MATH-500 (Hendrycks et al., 2021) (a curated subset of the MATH benchmark including AMC/AIME problems) and NuminaMath (CN K-12 curriculum). These datasets introduce higherdimensional algebraic and geometric reasoning. Hard: Sourced from LiveMathBench (Liu et al., 2025a) (2025 Hard Subset). These are recent competition-level problems requiring extremely long context windows and complex logical deductions. Table 11 summarizes the statistics of the generated CoT dataset. C.2 Generation Protocol To extract high-quality reasoning traces, we utilized Qwen3-32B (Yang et al., 2025) as the generator backbone. The generation process was designed to maximize the explicitness of the internal reasoning process (the chain of thought). Inference Configuration. We enabled the internal thinking mode (enable_thinking: true) to expose the raw reasoning tokens before the final answer. The generation parameters were set to temperature = 0.6 and nucleus sampling probability = 0.95 to balance creativity with logical coherence. Token Limits. To accommodate deep reasoning, we set high context limit. For standard datasets, we allowed up to 8,000 reasoning tokens. For the LiveMathBench subset, we removed the CoT token limit entirely to allow for exhaustive search trajectories in hard problems. Prompting. We employed standardized two-message chat format to enforce rigorous step-by-step reasoning. The System Prompt was defined as: You are helpful and rigorous math reasoning assistant . Prompt 1: System Prompt of the CoT dataset generation The User Prompt wrapped the specific dataset problem with instructions to act as competition solver: You are an expert competition math solver . Read the problem carefully and solve it step by step . Prompt 2: User Prompt of the CoT dataset generation Problem : { Problem } C.3 Quality Control and Evaluation To ensure that our latent manifold analysis is based on valid reasoning trajectories rather than hallucinations, we implemented strict verification pipeline using an LLM-as-a-Judge approach. We employed DeepSeek-V3.2-exp (DeepSeek-AI, 2025) as the external evaluator. To ensure deterministic and strictly formatted outputs, we used greedy decoding (T = 0) and explicitly disabled the models internal chain-ofthought feature (enable_thinking: False). The interaction was structured as follows: You are precise math answer evaluator . Respond only with 0 or 1. Prompt 3: System Prompt of the external evaluator You are an expert math problem evaluator . Your task is to determine if the provided answer correctly solves the given problem . Prompt 4: User Prompt of the external evaluator 20 Problem : { Problem } Answer : { Answer } Evaluate whether the answer is correct . Respond with ONLY \"1\" if the answer is correct , or \"0\" if it is incorrect . Do not provide any explanation . The evaluators output was parsed using simple inclusion check: if the token 1 appeared in the response, the reasoning trace was marked as valid (correctness_label = 1); otherwise, it was discarded. Table 11: Statistics of the Generated CoT Reasoning Dataset. We report the yield of our generation pipeline across difficulty tiers. Total: Number of initial prompts; Valid: Traces that passed the correctness verification (Correctness = 1). Acc.: The effective yield rate (Valid / Total). Generation Statistics Dataset (Difficulty) Total Valid Acc. (%) GSM8K (Easy) NuminaMath (Moderate) MATH-500 (Moderate) LiveMathBench (Hard) 500 161 479 57 471 154 364 Total / Average 1,197 1,021 92.80 91.30 75.78 56.14 85."
        },
        {
            "title": "D Additional Results",
            "content": "In this section, we demonstrate additional results of HRSA applying on more model pairs, including Mbase vs. Mreason and MEmb reason (suffix Emb). See Table 9 and Table 10 for the detailed model pairs. base vs. MEmb Instead of only considering the CoT dataset, we also apply HRSA with the MMLU-Pro (Wang et al., 2024) dataset to study the difference (if any) between the models in general field rather than only the maths domain. Dimension-Wise Correlation Base Models Mbase vs. Reasoning Models Mreason Dataset: CoT Datset 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q DeepSeek-R1-Distill-Qwen1.5B Polaris-4B-Preview 4 - 3 Q 5 . 0 - 5 . 2 Q 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo Polaris-7B-Preview Nemotron-ResearchReasoning-Qwen-1.5B Dataset: MMLU-Pro DeepSeek-R1-Distill-Qwen1.5B Polaris-4B-Preview 4 - 3 Q 5 . 0 - 5 . 2 Q 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo Polaris-7B-Preview zero__ppo__think__Qwen2.57B Qwen3-4B-PSR zero__ppo__think__Qwen2.57B 7 - 5 . 2 Q 4 - 3 Q 7 - 5 . 2 Q 4 - 3 Q Nemotron-ResearchReasoning-Qwen-1.5B Qwen3-4B-PSR Figure 6: Additional Results on Dimension-Wise Correlation separated by dataset. The vertical axis and horizontal axis are Base Model Layer Index and Reasoning Model Layer Index, respectively. The red background indicates SFT-tuned pairs."
        },
        {
            "title": "Base Embedding Models MEmb",
            "content": "base vs. Reasoning Embedding Models MEmb reason Dimension-Wise Correlation Dataset: CoT Datset - - B 6 . 0 - 3 Q DeepSeek-R1-Distill-Qwen1.5B-Emb Qwen3-0.6B-Emb - 5 . 1 - Q - t - 1 - S D E Nemotron-ResearchReasoning-Qwen-1.5B-Emb Dataset: MMLU-Pro Qwen-2.5-0.5B-SimpleRLZoo-Emb - - B 6 . 0 - 3 Q - 5 . 1 - Q - t - 1 - S D DeepSeek-R1-Distill-Qwen1.5B-Emb Qwen-2.5-0.5B-SimpleRLZoo-Emb Qwen3-0.6B-Emb - B 5 . 1 - M - 5 . 2 Q - B 5 . 0 - 5 . 2 Q - B 5 . 1 - M - 5 . 2 Q - B 5 . 0 - 5 . 2 Q Qwen-2.5-1.5B-SimpleRLZoo-Emb Qwen3-4B-PSR-Emb Qwen-2.5-1.5B-SimpleRLZoo-Emb - E 5 . 1 - 5 . 2 Q - B 4 - 3 Q - B 5 . 1 - 5 . 2 Q - E 4 - 3 Q Nemotron-ResearchReasoning-Qwen-1.5B-Emb Qwen3-4B-PSR-Emb Figure 7: Additional Results on Dimension-Wise Correlation separated by dataset. The vertical axis and horizontal axis are Base Model Layer Index and Reasoning Model Layer Index, respectively. The red background indicates their backbone LLMs are SFT-tuned pairs. 23 Table 12: Inverse row entropy Hinv of the orthogonal matrix for Mbase vs. Mreason across different datasets. Higher inverse row entropy indicates more axis-aligned correspondence, while lower inverse row entropy indicates more globally mixed features. The model pairs are separated by the algorithm used to train their reasoning model Mreason. Orthogonal Procrustes Analysis Base Models Mbase vs. Reasoning Models Mreason Dataset: CoT Dataset Model Pair SFT Qwen2.5-Math-1.5B vs DeepSeek-R1-Distill-Qwen-1.5B Hinv 0. RLVR 0.4365 Qwen3-4B vs Polaris-4B-Preview 0.6576 DeepSeek-R1-Distill-Qwen-7B vs Polaris-7B-Preview 0.6338 Qwen2.5-7B vs zero__ppo__think__Qwen2.5-7B 0.9481 Qwen2.5-1.5B vs Qwen-2.5-1.5B-SimpleRL-Zoo Qwen2.5-0.5B vs Qwen-2.5-0.5B-SimpleRL-Zoo 0.9923 DeepSeek-R1-Distill-Qwen-1.5B vs Nemotron-Research-Reasoning-Qwen-1.5B 0.1613 0.8122 Qwen3-4B vs Qwen3-4B-PSR Dataset: MMLU-Pro Model Pair SFT Qwen2.5-Math-1.5B vs DeepSeek-R1-Distill-Qwen-1.5B Hinv 0. RLVR 0.9711 Qwen3-4B vs Polaris-4B-Preview 0.9623 DeepSeek-R1-Distill-Qwen-7B vs Polaris-7B-Preview 0.9922 Qwen2.5-7B vs zero__ppo__think__Qwen2.5-7B 0.9963 Qwen2.5-1.5B vs Qwen-2.5-1.5B-SimpleRL-Zoo Qwen2.5-0.5B vs Qwen-2.5-0.5B-SimpleRL-Zoo 0.9981 DeepSeek-R1-Distill-Qwen-1.5B vs Nemotron-Research-Reasoning-Qwen-1.5B 0.8336 0.9843 Qwen3-4B vs Qwen3-4B-PSR 24 Table 13: Inverse row entropy Hinv of the orthogonal matrix for MEmb reason across different datasets. Higher inverse row entropy indicates more axis-aligned correspondence, while lower inverse row entropy indicates more globally mixed features. The model pairs are separated by the algorithm used to train their reasoning model backbone Mreason. base vs. MEmb"
        },
        {
            "title": "Base Embedding Models MEmb",
            "content": "base vs. Reasoning Embedding Models MEmb reason Dataset: CoT Dataset Model Pair SFT Qwen2.5-Math-1.5B-Emb vs DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen3-0.6B-Base-Emb vs Qwen3-0.6B-Emb Hinv 0.1429 0.4915 RLVR Qwen2.5-1.5B-Emb vs Qwen-2.5-1.5B-SimpleRL-Zoo-Emb 0.8826 Qwen2.5-0.5B-Emb vs Qwen-2.5-0.5B-SimpleRL-Zoo-Emb 0.9835 DeepSeek-R1-Distill-Qwen-Emb vs Nemotron-Research-Reasoning-Qwen-Emb 0.8637 Qwen3-4B-Emb vs Qwen3-4B-PSR-Emb 0.5863 Dataset: MMLU-Pro Model Pair SFT Qwen2.5-Math-1.5B-Emb vs DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen3-0.6B-Base-Emb vs Qwen3-0.6B-Emb Hinv 0.7105 0.9164 RLVR Qwen2.5-1.5B-Emb vs Qwen-2.5-1.5B-SimpleRL-Zoo-Emb 0.9794 Qwen2.5-0.5B-Emb vs Qwen-2.5-0.5B-SimpleRL-Zoo-Emb 0.9978 DeepSeek-R1-Distill-Qwen-Emb vs Nemotron-Research-Reasoning-Qwen-Emb 0.9814 Qwen3-4B-Emb vs Qwen3-4B-PSR-Emb 0.9933 25 Linear CKA Base Models Mbase vs. Reasoning Models Mreason Dataset: CoT Datset 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q DeepSeek-R1-Distill-Qwen1.5B Polaris-4B-Preview 4 - 3 Q 5 . 0 - 5 . 2 Q 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo Polaris-7B-Preview Nemotron-ResearchReasoning-Qwen-1.5B Dataset: MMLU-Pro DeepSeek-R1-Distill-Qwen1.5B Polaris-4B-Preview 4 - 3 Q 5 . 0 - 5 . 2 Q 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S e Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo Polaris-7B-Preview zero__ppo__think__Qwen2.57B Qwen3-4B-PSR zero__ppo__think__Qwen2.57B 7 - 5 . 2 Q 4 - 3 Q 7 - 5 . 2 Q 4 - 3 Q Nemotron-ResearchReasoning-Qwen-1.5B Qwen3-4B-PSR Figure 8: Additional Results on Linear CKA separated by dataset. The vertical axis and horizontal axis are Base Model Layer Index and Reasoning Model Layer Index, respectively. The red background indicates SFT-tuned pairs."
        },
        {
            "title": "Base Embedding Models MEmb",
            "content": "base vs. Reasoning Embedding Models MEmb reason"
        },
        {
            "title": "Linear CKA",
            "content": "Dataset: CoT Datset - - B 6 . 0 - 3 Q - 5 . 1 - Q - t - 1 - S D E DeepSeek-R1-Distill-Qwen1.5B-Emb Qwen-2.5-0.5B-SimpleRLZoo-Emb Qwen3-0.6B-Emb Nemotron-ResearchReasoning-Qwen-1.5B-Emb Dataset: MMLU-Pro - - B 6 . 0 - 3 Q - 5 . 1 - Q - t - 1 - S D E DeepSeek-R1-Distill-Qwen1.5B-Emb Qwen-2.5-0.5B-SimpleRLZoo-Emb Qwen3-0.6B-Emb - B 5 . 1 - M - 5 . 2 Q - E 5 . 0 - 5 . 2 Q - B 5 . 1 - M - 5 . 2 Q - B 5 . 0 - 5 . 2 Q Qwen-2.5-1.5B-SimpleRLZoo-Emb Qwen3-4B-PSR-Emb Qwen-2.5-1.5B-SimpleRLZoo-Emb - B 5 . 1 - 5 . 2 Q - B 4 - 3 Q - B 5 . 1 - 5 . 2 Q - B 4 - 3 Q Nemotron-ResearchReasoning-Qwen-1.5B-Emb Qwen3-4B-PSR-Emb Figure 9: Additional Results on Linear CKA separated by dataset. The vertical axis and horizontal axis are Base Model Layer Index and Reasoning Model Layer Index, respectively. The red background indicates their backbone LLMs are SFT-tuned pairs. 27 k-NN Overlap Base Models Mbase vs. Reasoning Models Mreason Dataset: CoT Datset 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q 4 - 3 Q DeepSeek-R1-Distill-Qwen-1.5B Polaris-4B-Preview 5 . 0 - 5 . 2 Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S D Polaris-7B-Preview Nemotron-Research-ReasoningQwen-1.5B Model Layer Index Dataset: MMLU-Pro 4 - 3 Q DeepSeek-R1-Distill-Qwen-1.5B Polaris-4B-Preview 5 . 0 - 5 . 2 Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S D Polaris-7B-Preview Nemotron-Research-ReasoningQwen-1.5B 7 - 5 . 2 Q 4 - 3 Q 7 - 5 . 2 Q 4 - 3 Q zero__ppo__think__Qwen2.5-7B Qwen3-4B-PSR zero__ppo__think__Qwen2.5-7B Qwen3-4B-PSR Figure 10: Additional Results on k-NN Overlap separated by dataset. The vertical axis and horizontal axis are Mean Overlap and Model Layer Index, respectively. The red background indicates SFT-tuned pairs. Model Layer Index"
        },
        {
            "title": "Base Embedding Models MEmb",
            "content": "base vs. Reasoning Embedding Models MEmb reason k-NN Overlap - B 5 . 1 - M - 5 . 2 Q - E 5 . 0 - 5 . 2 Q - B 5 . 1 - M - 5 . 2 Q - B 5 . 0 - 5 . 2 Q DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen-2.5-0.5B-SimpleRL-Zoo-Emb DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen-2.5-0.5B-SimpleRL-Zoo-Emb Dataset: CoT Datset Qwen3-0.6B-Emb Nemotron-Research-Reasoning-Qwen-1.5BEmb Model Layer Index Dataset: MMLU-Pro Qwen3-0.6B-Emb Nemotron-Research-Reasoning-Qwen-1.5BEmb Model Layer Index - E - B 6 . 0 - 3 Q - 5 . 1 - Q - t - 1 - S D - E - B 6 . 0 - 3 Q - 5 . 1 - Q - t - 1 - S D - E 5 . 1 - 5 . 2 Q - B 4 - 3 Q - B 5 . 1 - 5 . 2 Q - E 4 - 3 Q Qwen-2.5-1.5B-SimpleRL-Zoo-Emb Qwen3-4B-PSR-Emb Qwen-2.5-1.5B-SimpleRL-Zoo-Emb Qwen3-4B-PSR-Emb Figure 11: Additional Results on k-NN Overlap separated by dataset. The vertical axis and horizontal axis are Mean overlap and Model Layer Index, respectively. The red background indicates their backbone LLMs are SFT-tuned pairs. 29 Cross-Model Linear Probes Base Models Mbase vs. Reasoning Models Mreason Dataset: AGs News Topic Classification 5 . 1 - M - 5 . 2 Q 5 . 1 - 5 . 2 Q 4 - 3 Q DeepSeek-R1-Distill-Qwen-1.5B Polaris-4B-Preview 5 . 0 - 5 . 2 Q Qwen-2.5-1.5B-SimpleRL-Zoo Qwen-2.5-0.5B-SimpleRL-Zoo 7 - Q - t - 1 - S D 5 . 1 - Q - t - 1 - S D 7 - 5 . 2 Q Polaris-7B-Preview zero__ppo__think__Qwen2.5-7B 4 - 3 Q Nemotron-Research-ReasoningQwen-1.5B Qwen3-4B-PSR Dataset Types Figure 12: Additional Results on Cross-Model Linear Probe. The vertical axis and horizontal axis are the Accuracy of the linear probe and Dataset types (train, dev, test), respectively. The red background indicates SFT-tuned pairs."
        },
        {
            "title": "Base Embedding Models MEmb",
            "content": "base vs. Reasoning Embedding Models MEmb reason Cross-Model Linear Probes Dataset: AGs News Topic Classification - B 5 . 1 - M - 5 . 2 Q - B 5 . 0 - 5 . 2 Q - - B 6 . 0 - 3 Q DeepSeek-R1-Distill-Qwen-1.5B-Emb Qwen3-0.6B-Emb - 5 . 1 - Q - t - 1 - S D Qwen-2.5-0.5B-SimpleRL-Zoo-Emb Qwen-2.5-1.5B-SimpleRL-Zoo-Emb - B 5 . 1 - 5 . 2 Q - B 4 - 3 Q Nemotron-Research-Reasoning-Qwen-1.5BEmb Qwen3-4B-PSR-Emb Dataset Types Figure 13: Additional Results on Cross-Model Linear Probe. The vertical axis and horizontal axis are the Accuracy of the linear probe and Dataset types (train, dev, test), respectively. The red background indicates their backbone LLMs are SFT-tuned pairs."
        }
    ],
    "affiliations": [
        "CSE, HKUST"
    ]
}