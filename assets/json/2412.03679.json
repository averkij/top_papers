{
    "paper_title": "Evaluating Language Models as Synthetic Data Generators",
    "authors": [
        "Seungone Kim",
        "Juyoung Suk",
        "Xiang Yue",
        "Vijay Viswanathan",
        "Seongyun Lee",
        "Yizhong Wang",
        "Kiril Gashteovski",
        "Carolin Lawrence",
        "Sean Welleck",
        "Graham Neubig"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness."
        },
        {
            "title": "Start",
            "content": "Seungone Kim1 Juyoung Suk2 Xiang Yue1 Vijay Viswanathan1 Seongyun Lee2 Yizhong Wang3 Kiril Gashteovski4,5 Carolin Lawrence4 Sean Welleck1 Graham Neubig1 Carnegie Mellon University1 NEC Laboratories Europe4 KAIST AI2 University of Washington3 Ss. Cyril and Methodius University of Skopje {seungone, wellecks, gneubig}@cmu.edu"
        },
        {
            "title": "Abstract",
            "content": "Given the increasing use of synthetic data in language model (LM) post-training, an LMs ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in unified setting. To address this gap, we propose AGORABENCH, benchmark that provides standardized settings and metrics to evaluate LMs data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LMs data generation ability doesnt necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data qualityincluding response quality, perplexity, and instruction difficultycollectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and costconscious model selection significantly impact data generation effectiveness. Our code, checkpoints, and data are all publicly available at https://github.com/neulab/data-agora."
        },
        {
            "title": "Introduction",
            "content": "Post-training language models (LMs) on synthetic data is promising approach for improving their ability to solve wide range of tasks (Wang et al., 2023; Honovich et al., 2023; Taori et al., 2023; Liu et al., 2024b). While acquiring data through manual annotation continues to play an important role, synthetic data generation offers scalable complement to human labeling (Viswanathan et al., 2023; Kim et al., 2023b). Hence, numerous works have proposed novel methods to effectively generate high-quality synthetic data (Xu et al., 2024a; Gunasekar et al., 2023; Yue et al., 2023, 2024). As multiple proprietary LMs with comparable performance emerge and open-source LMs steadily catch up (Hurst et al., 2024; Anthropic, 2024; MetaAI, 2024; Team, 2024), measuring each LMs data generation capabilities has become as crucial as developing new data generation methods. Moreover, companies providing proprietary LMs have begun promoting the use of their latest models for generating synthetic data (Nvidia, 2024). Carefully comparing data generation ability across LMs helps validate these claims and allows practitioners to wisely choose models for data synthesis. To systematically compare LMs capabilities as data generators, unified experimental setting is needed, where only the data generator varies, while other components remain fixed. However, as shown in Figure 1, prior works have focused more on demonstrating the effectiveness of their data generation method, leading to various experimental settings that make such comparison challenging. For instance, Self-Instruct (Wang et al., 2023), Alpaca (Taori et al., 2023), WizardLM (Xu et al., 2024a) and Orca (Mukherjee et al., 2023) varied in their choice of LMs for data generation, quantity of synthetic training data, base models used for training, and benchmarks for evaluating the model trained on the synthetic dataset. These heterogeneous settings make it difficult to isolate and measure LMs data generation capabilities, highlighting the need for controlled settings. To this end, we propose AGORABENCH, benchmark for evaluating LMs data generation capabilities across nine settings, combining three domains (math, instruction-following, code) with three data generation methods (instance generation, response generation, quality enhancement). Within each setting, all variables except the data generator are controlled: the same meta-prompt and seed dataset are used, with each LM generating 4 2 0 2 ] . [ 1 9 7 6 3 0 . 2 1 4 2 : r Figure 1: Illustration of the motivation of AGORABENCH: Prior works focused on developing new methods to generate synthetic data. In contrast, our work focuses on systematically comparing different LMs as data generators based on existing data generation methods. Further explanation of data generation methods are covered in Section 2. an identical number of training instances. Llama3.1-8B is trained on each synthetic dataset and evaluated on fixed set of benchmarks spanning different capabilities: mathematics, coding, and general instruction-following. To evaluate the quality of synthetic data, we define metric called Performance Gap Recovered (PGR), which measures the relative improvement of the model trained on the data (denoted as student model) over its base model. Based on this setting, we assess six LMs as data generators: GPT-4o, GPT-4o-mini, Claude3.5-Sonnet, and Llama-3.1-Instruct (8, 70, 405B).1 Our analysis reveals distinct strengths among different LMs in various kinds of data generation methods. For example, GPT-4o demonstrates superior performance in generating new instances (+ 46.75%), outperforming both Claude-3.5-Sonnet (+ 24.14%) and Llama-3.1-405B-Instruct (+ 10.10%). On the other hand, Claude-3.5-Sonnet excels at refining existing instances (+ 17.89%), surpassing both GPT-4o (+ 6.69%) and GPT-4o-mini (+ 5.49%). These findings demonstrate how AGORABENCH can guide practitioners in selecting 1Xu et al. (2024c), contemporaneous work with ours, also measured various LMs data generation capabilities. In contrast to our work, they only examine the response generation setting, whereas we measure three data generation settings and also do number of additional analyses on the relationship between the intrinsic quality of data and PGR. appropriate LMs for their specific needs. Unexpectedly, we also find that LMs with weaker problem-solving ability sometimes outperform stronger ones in data generationfor example, Claude-3.5-Sonnet (+ 23.43%) is less effective than Llama-3.1-8B-Instruct (+ 55.69%) at generating new instances in the code domain. Based on these findings, we investigate whether an LMs data generation ability can be predicted by its problem-solving ability alone. Our analysis reveals no strong correlation between the two capabilities. Instead, multiple intrinsic features of data qualityincluding instruction difficulty, response quality, and response perplexitycollectively influence the student models improvement. Furthermore, we demonstrate that the top-5 principal components extracted from intrinsic measurements can explain 93.4% of the variance in the PGR values. Lastly, we conduct analysis experiments for effective data generation. For instance, we find that the output format of synthetic data significantly impacts performance: data generated using JSON format shows 4.45% lower performance on average across six settings compared to free-form generation. Additionally, in budget-constrained setting, generating more data with weaker model can outperform generating less data with stronger model. We find that generating 50K instances with GPTFigure 2: AGORABENCH tests three data generation methods: generating new instruction and response pairs (left), generating responses (middle), and enhancing the quality of the instruction and/or the response (right). 4o-mini, while 3.4 times cheaper, achieves better performance than generating 10K instances with GPT-4o in two out of three settings."
        },
        {
            "title": "2 Preliminaries: Measuring Data\nGeneration Capabilities of LMs",
            "content": "Notations. Given seed data Dseed and prompt describing the kind of data generation to perform (referred to as meta-prompt) , data generator generates DG = G(Dseed, ), (1) where Dseed and DG can both be expressed as {(Ii, Ri) = 1, . . . , n} with denoting an instruction, denoting corresponding response, and denoting the size of the data. Data Generation Methods. As shown in Figure 2, among the various methods for generating data, most can be grouped into three categories: instance generation, response generation, and quality enhancement. These methods work as follows: Instance Generation: Given small seed dataset Dseed = {(Ii, Ri) = 1, . . . , m}, few instances are randomly sampled from Dseed and used as in-context demonstrations, resulting in the generation of new instances (Honovich et al., 2023; Wang et al., 2023). This process is performed iteratively until DG = {(Ii, Ri) = 1, . . . , n} is constructed where << n. Note that the generated instances could also optionally be used as demonstrations as well. Response Generation: large set of instructions DI = {(Ii) = 1, . . . , n} is given, and iterates through each instruction Ii to generate corresponding response Ri (Xu et al., 2024b). Figure 3: Illustration of Performance Gap Recovered metric: The performance gap recovered metric captures the relative improvement of SDG with respect to Sref where SDG and Sref is both trained from SØ. 1, . . . , n} is given. iterates through each ini and/or stance to refine i, such as by explicitly prompting to make either/both of higher quality (e.g., making the instruction more difficult or of higher educational value) (Xu et al., 2024a; Yue et al., 2024). i, Metric. An LMs data generation ability can be measured by evaluating the performance improvement of student model trained on the teachergenerated data. Specifically, we propose metric, Performance Gap Recovered (PGR), that measures the improvement on benchmark relative to reference model, GR(G, B) = scoreB(SDG) scoreB(SØ) scoreB(Sref ) scoreB(SØ) 100 (2) where SØ denotes pre-trained LM, SDG denotes SØ trained on DG, Sref denotes reference model that shares the same pre-trained model SØ as base model, and scoreB() denotes the score on benchmark B. In our experiments, we use Llama3.1-8B as SØ and Llama-3.1-8B-Instruct as Sref .2 Quality Enhancement: large set of instruci = tions and responses = {(I i, i) 2Note that when measuring scoreB(SØ), SØ can not solve tasks with zero-shot prompting, so we evaluate their perfor-"
        },
        {
            "title": "Instance Generation\nResponse Generation\nQuality Enhancement",
            "content": "GSM8K, MATH (train set) Magpie-Reasoning (math) WebInstruct (math)"
        },
        {
            "title": "Instance Generation\nResponse Generation\nQuality Enhancement",
            "content": "MBPP (train set), xP3x Magpie-Reasoning (code) CoNaLa Inst. Follow"
        },
        {
            "title": "Instance Generation\nResponse Generation\nQuality Enhancement",
            "content": "LIMA Magpie-Pro WebInstruct (code) 14,856 10,000 10,000 874 10,000 10,000 503 10,000 10,000 GSM8K, MATH (test set) GSM8K, MATH (test set) GSM8K, MATH (test set) MBPP, HumanEval (test set) MBPP, HumanEval (test set) MBPP, HumanEval (test set) AlpacaEval 2.0, Arena-Hard AlpacaEval 2.0, Arena-Hard AlpacaEval 2.0, Arena-Hard Table 1: AGORABENCH Settings: For each of the nine settings, an LM being evaluated generates 10K instances with the same meta-prompt and seed data. Note that the seed dataset is also used for training in instance generation. Intuitively, as illustrated in Figure 3, by using Llama-3.1-8B as SØ and Llama-3.1-8B-Instruct as Sref , the PGR value represents how much performance was recovered compared to the post-training process for Llama-3.1-8B-Instruct, which was reportedly extensive, training on 10M+ examples of human-curated data (MetaAI, 2024). For example, PGR value of 50% indicates that SDG has recovered 50% of the improvement achieved by Sref relative to SØ. value above 100% indicates SDG outperforms Sref , while negative value indicates that training on DG degraded performance on compared to few-shot prompting SØ. Training Student Models. When training the student model (SØ), we employ supervised finetuning (SFT), computing the loss only on response tokens. We directly use the generated data DG without filtering and do not consider other posttraining methods, as our goal is to evaluate the raw data generation capabilities of an LM (G) in the most straightforward setting, not to maximize SDGs benchmark performance. The hyperparameters for training are detailed in Appendix D."
        },
        {
            "title": "3 Experimental Setting of AGORABENCH",
            "content": "Among various choices, AgoraBench focuses on three core capabilities that are considered crucial for LMs: instruction following, mathematical reasoning, and coding (Chang et al., 2024; Guo et al., 2023; Hurst et al., 2024; Anthropic, 2024). The overall experimental setting of AGORABENCH including the domains, seed datasets, and benchmarks for each setting is listed in Table 1. mance with few-shot prompting. In contrast, SDG and Sref are evaluated with zero-shot prompting. Domains. AGORABENCH encompasses three domains: math, code, and instruction following. Evaluating three data generation methods across each domain results in nine distinct settings, each with dedicated seed dataset (Dseed) and benchmark (B). For each setting, the LM employed as the data generator produces 10K training instances. Then, the student model is trained using data from single domain to isolate the effect of generated data quality, as cross-domain training could introduce confounding factors through positive or negative transfer (e.g., training on code data improve math (Dong et al., 2023; Zhang et al., 2024)). Seed Datasets. For each setting, we select seed datasets (Dseed) based on different assumptions: For instance generation, since we expand small amount of high-quality data into larger volume, our approach is premised on using highquality, human-crafted data as seed data. Hence, we use the train subsets of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for math, MBPP (Austin et al., 2021) and xP3x (Muennighoff et al., 2023b) for code, and LIMA (Zhou et al., 2024) for instruction following. We exclude instances that exceed 4,096 tokens based on the Llama-3 tokenizer, resulting in 14,856, 874, and 503 seed instances for each of the math, code, and instruction following domains, respectively. For response generation, we simulate how different data generators can attach responses to fixed set of instructions to ultimately create better quality data. While we could take arbitrary data and discard their responses for experiments, we utilize the Magpie dataset because Xu et al."
        },
        {
            "title": "Math Code",
            "content": "Inst. Avg Math Code Inst. Avg Math Code Inst. Avg GPT-4o GPT-4o-mini Claude-3.5-Sonnet Llama-3.1-405B Llama-3.1-70B Llama-3.1-8B 20.6 16.1 8.9 10.4 9.6 6.5 73.6 41.9 23.4 12.6 58.7 55. 46.1 18.0 40.1 7.4 6.5 6.2 46.8 25.3 24.1 10.1 24.9 22.8 46.7 48.1 29.0 31.7 23.0 27.6 28.5 18.9 44.5 35.4 37.1 25.8 30.3 13.7 12.7 4.9 4.5 5.0 35.2 26.9 28.8 24.0 21.5 19. 21.9 17.8 15.7 -11.8 -21.8 -1.7 -8.8 -11.2 16.1 7.5 6.9 15.4 7.1 9.9 21.8 3.6 2.7 3.0 6.7 5.5 17.9 -0.2 -4.1 5.6 Table 2: AGORABENCH RESULTS: How much performance could you recover by generating 10K instances with your LLM, compared to Metas post-training process for training Llama-3.1-8B-Instruct from Llama3.1-8B? The best comparable performances (%) are bolded, and the second-best performances (%) are underlined. Note that the Llama models are instruction-tuned versions and that Inst. denotes instruction-following. (2024b)s setting closely matches our setting - they first extract instructions by prompting LMs with empty chat templates and then generate responses using two different types of LMs (LlamaIn 3-70B-Instruct and Qwen-2-72B-Instruct). our experiments, we sample 10K instances from the Magpie dataset (Xu et al., 2024b) for the instruction following domain and also 10K instances from the Magpie-Reasoning dataset for both math and code domains. For quality enhancement, we test scenarios where complete instances of instructions and responses already exist, but their quality needs improvement before being used for post-training - either because the instructions are too simple or the responses are not sufficiently detailed. We sample 10K instances from WebInstruct (Q-A pairs from the web requiring refinement; see Yue et al. (2024)) for instruction following and math domains. Note that WebInstruct does not contain domain labels, hence we prompt GPT-4o-mini2024-07-18 to prepare separate Dseed (further details are in Appendix B). For the code domain, we use CoNaLa, which contains simple instructions paired with 1-3 line code snippets from StackOverflow (Yin et al., 2018). Benchmarks. We evaluate student model (SDG)s performance using two representative benchmarks for each domain. For math, we use the test subsets of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). For code, we use the test set of MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021). For instruction following, we evaluate on AlpacaEval-2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024)."
        },
        {
            "title": "4 Experimental Results of AGORABENCH",
            "content": "We compare 6 LMs as data generators (G), namely GPT-4o-2024-08-06 (Hurst et al., 2024), GPT-4omini-2024-07-18, Claude-3.5-Sonnet-2024-0620 (Anthropic, 2024), Llama-3.1-405B-Instruct, Llama-3.1-70B-Instruct, and Llama-3.1-8BInstruct (Dubey et al., 2024). Also, we use Llama-3.1-8B as the student model (SØ). The AGORABENCH results are listed in Table 2. GPT-4o is the overall most performant data generator: Out of the nine experimental settings, GPT-4o achieves the highest PGR scores in five settings. Its performance is particularly notable in instance generation, where it outperforms other LMs as data generator across all three domains (math at 20.6%, code at 73.6%, instruction following at 46.1%, and total average at 46.8%), while also achieving the highest average PGR score in response generation (35.2%). Claude-3.5-Sonnet proves particularly effective for quality enhancement: Claude-3.5-Sonnet particularly demonstrates strong performance in quality enhancement, achieving the highest PGR scores in two out of three domains (code at 21.8%, instruction following at 17.9%, and total average at 17.9%). Additionally, it obtains the best PGR score at response generation in the code domain (44.5%), bringing its total number of top performances to three out of nine settings. Weaker LMs can outperform Stronger LMs: We observe cases where LMs with weaker problemsolving abilities achieve higher Performance Gap Recovered (PGR) scores than their stronger counterparts. In the code domain of instance generation, both Claude-3.5-Sonnet (23.4%) and LlamaFigure 4: Problem-solving and data generation capabilities do not strongly correlate: Linear regression between problem-solving ability and data generation ability scores at multiple granularity levels yields either low R2 values (R2 < 0.1) or non-significant relationships (p > 0.05), which indicates that it is hard to predict data generation capabilities only using problem-solving capabilities."
        },
        {
            "title": "API Cost",
            "content": "Prob. Data Gen. Solv."
        },
        {
            "title": "Input Output Avg",
            "content": "GPT-4o GPT-4o-mini Claude-3.5-Sonnet Llama-3.1-405B Llama-3.1-70B Llama-3.1-8B $2.50 $0.15 $3.00 $1.79 $0.35 $0.055 $10.00 $0.60 $15.00 $1.79 $0.40 $0.055 80.9 75.4 80.5 75.0 69.6 50."
        },
        {
            "title": "Agora\nBench",
            "content": "29.5% 19.2% 23.6% 11.3% 14.1% 15.9% Table 3: Comparison of API costs, problem-solving ability, and data generation ability: Our findings reveal that neither the strength nor the cost of an LM guarantees its effectiveness as data generator. Note that the Llama models are instruction-tuned versions, the specific results of the LMs on each benchmark (averaged as Problem Solving average) is in Appendix C, and the AgoraBench results are averaged from Table 2. 3.1-405B-Instruct (12.6%) are outperformed by Llama-3.1-70B-Instruct (58.7%) and Llama-3.18B-Instruct (55.7%). Similarly, in the code domains quality enhancement setting, GPT-4o (- 8.8%) and GPT-4o-mini (-11.2%) show poorer performance compared to other LMs. Interestingly, as shown in Table 3, the LMs that performed worse for these cases actually score higher on code benchmarks (MBPP and HumanEval), indicating that they possess stronger problem-solving capabilities. This contradiction suggests that stronger LM does not necessarily generate better training data. We discuss this phenomenon further in Section 5. GPT-4o, GPT-4o-mini, and Llama-3.1-8BInstruct are effective data generators that balance both cost and performance: Cost is another crucial factor alongside performance when generating large amounts of synthetic data. Table 3 shows the API costs3 benchmark scores (i.e., problem-solving ability) and average performance on AGORABENCH (i.e., data generation ability) are listed in Table 3. and average performance on AGORABENCH for all six LMs. Llama-3.1-8BInstruct outperforms both Llama-3.1-70B-Instruct and Llama-3.1-405B-Instruct while being 6 to 32.5 times less expensive. Similarly, GPT-4o achieves better performance than Claude-3.5-Sonnet at 1.2 to 1.5 times lower cost. These findings suggest that using more expensive LMs does not necessarily guarantee better data generation, highlighting the importance of careful model selection based on specific tasks or domains of interest."
        },
        {
            "title": "5 What makes an effective data",
            "content": "generator? In the previous section, we observed an unexpected finding: LMs with weaker problem-solving ability sometimes outperform stronger LMs when generating the same amount of synthetic data under identical conditions. For better understanding of this phenomenon, we first examine whether there exists strong correlation between problem-solving ability and data generation ability (Section 5.1). Then we investigate whether we can predict the degree 3Pricing is based on https://openrouter.ai/. Figure 5: Through PCA analysis on multiple intrinsic evaluation metrics, we find that there exists interpretable low-dimension principal components that explain the variance of data generation capabilities up to 93.4%. of improvement in the performance of the student model by analyzing the data generated by each LM (Section 5.2). 5. Is the best solver necessarily the best generator? To examine the relationship between data generation and problem-solving capabilities, we performed linear regression analyses comparing two metrics: average performance on multiple benchmarks (GSM8K, MATH, MBPP, HumanEval, AlpacaEval-2.0, Arena-Hard) and scores from AGORABENCH. We conduct this analysis at two levels of granularity. The first analysis (coarsegrained) uses the overall average AGORABENCH score across all domains and data generation settings. The second analysis (fine-grained) examines individual scores from different domains and data generation settings in AGORABENCH separately. The results shown in Figure 4 reveal no strong linear correlation between problem-solving capabilities (benchmark scores) and data generation capabilities (AGORABENCH PGR scores) at either granularity level. This finding suggests that an LMs performance on traditional benchmarks may not predict its effectiveness as data generator."
        },
        {
            "title": "5.2 Can we predict the student model’s",
            "content": "improvement by looking into the data? Given that problem-solving ability does not directly predict data generation ability, we explore what other characteristics might define effective data generators. We hypothesize that good data capable of substantially improving student models share extractable features that can be identified by analyzing their intrinsic properties. This understanding is crucial as it informs us of what properties the data from good data generator might possess. Inspired by Liu et al. (2023b), we conduct an intrinsic evaluation by analyzing various properties of the generated data DG. Intrinsic Evaluation Metrics We evaluate (1) the complexity of the instruction Ii (2) the quality of response Ri, (3) the perplexity of Ri using the student model SØ, (4) the diversity of both instructions and responses separately: Response Quality : We measure the quality of Ri given Ii. We use two methods: LLM-as-a-Judge Score: We prompt an LM to return discrete score between 1 and 5 that represents the quality of Ri. We employ two LM judges: (1) Prometheus-2-8x7B (Kim et al., 2024), an open-source LM specialized on assessing LM output and (2) GPT-4o, proprietary LM widely used as judge. We use different score rubrics for each domain, listed in Appendix G. Reward Model Score: We use reward model to predict scalar value score that represents the quality of Ri. We use SkyworkReward-Llama-3.1-8B (Liu et al., 2024a), one of the top-performing reward models on Reward Bench (Lambert et al., 2024). Instruction Complexity (LLM-as-a-Judge Score): We measure the difficulty of Ii by"
        },
        {
            "title": "Loading Strength Contribution",
            "content": "Prometheus Score (R.Q.) Response Perplexity GPT-4o Score (R.Q.) Problem-solving Ability Skywork-RM Score (R.Q.) Prometheus Score (I.D.) Diversity (I.D.) GPT-4o Score (I.D.) Diversity (R.Q.) 0.256 0.252 0.246 0.240 0.239 0.230 0.226 0.223 0.189 12.18% 12.00% 11.71% 11.42% 11.38% 10.95% 10.76% 10.61% 9.00% Table 4: Mean Contributions of Intrinsic Metrics to Principal Components: Each loading strength represents the average magnitude of features loadings across all principal components and the contribution are normalized values to represent the relative percentage of each features loading strength in the overall component structure. I.D. refers to metrics measuring instruction difficulty and R.Q. refers to metrics measuring response quality. All intrinsic evaluation metrics show substantial contributions (0.189-0.256) to the principal components. The results, shown in Figure 5, reveal that the top five principal components explain approximately 93.4% of the variance in AGORABENCH results (39.2%, 30.4%, 11.9%, 7.0%, and 4.9% respectively). Moreover, we find that analysis of the component weights reveals interpretable patterns. The first principal component (PC-1) is strongly influenced by instruction difficulty and diversityrelated metrics. The second component (PC-2) is affected by response quality and instruction difficulty, while the third component (PC-3) combines diversity-related metrics, response quality, and the LMs problem-solving ability. Additionally, as shown in Table 4, when we analyze the average loading strengths of each intrinsic evaluation metric (average magnitude of features loadings across all principal components, indicating how strongly each metric influences the overall variance in the data), we observe that the contributions range from 0.189 to 0.256, indicating that all the intrinsic evaluation metrics contribute similarly to the PGR results. Also, we find that response quality-related metrics shows slightly stronger contributions than diversity-related metrics or instruction difficulty-related metrics to the PGR results. Lastly, we predict data generation capabilities by performing linear regression on the top-5 principal components, weighting each component by its corresponding regression coefficient, as shown in Figure 6. Compared to using problem-solving scores alone (Figure 4), this approach yields statistically significant relationship (p < 0.001) with improved Figure 6: Principal Components from Intrinsic Metrics Show Stronger Correlation with Data Generation ability: Linear regression using the weighted top-5 principal components yields higher explained variance (R2 = 0.325) and statistical significance (p < 0.001) compared to using problem-solving ability scores alone (R2 < 0.1 or > 0.05; see Figure 4). prompting an LM to return discrete score between 1 and 5 that represents the complexity of Ii. Similarly to evaluating response quality, we use Prometheus-2-8x7B and GPT-4o as judge. Note that the score rubric differs compared to that for evaluating response quality and we use different score rubrics for each domain, listed in Appendix G. Perplexity of Response: We measure the perplexity of Ri conditioned on Ii using the base model SØ (Llama-3.1-8B). Instance Diversity: We separately measure the average cosine similarity of instructions within DI = {(Ii) = 1, . . . , n} and responses within DR = {(Ri) = 1, . . . , n}. This represents the extent to which each instruction or response is widely distributed (i.e., diverse) (Ni et al., 2024). We use dunzhang/stella_en_400M_v5, model that is both high-performing on the MTEB benchmark (Muennighoff et al., 2023a) and efficient. Due to page limits, the full results of the intrinsic evaluation are further detailed in Appendix E. Experiments Inspired by the experiments from Ruan et al. (2024), we conduct Principal Component Analysis (PCA) to investigate whether intrinsic evaluation metrics can explain the variability in AGORABENCH results. We opt for PCA rather than multivariate linear regression due to the interdependence among our intrinsic evaluation metrics. Figure 7: With fixed budget, generating large amounts of data with weaker LMs could sometimes be more effective and cheaper than generating few instances with stronger LMs: Since GPT-4o-mini is 17 times cheaper than GPT-4o, generating 50K instances is 3.4 times cheaper than generating 10K instances with GPT-4o. Yet, generating 50K instances with GPT-4o-mini achieves higher PGR in instruction following and math domains compared to generating 10K instances with GPT-4o."
        },
        {
            "title": "Data Generator",
            "content": "AGORABENCH Meta-prompt Unoptimized Meta-prompt JSON-format Meta-prompt"
        },
        {
            "title": "Math Code",
            "content": "Inst."
        },
        {
            "title": "Math Code",
            "content": "Inst. Avg Math Code Inst."
        },
        {
            "title": "Instance Generation",
            "content": "GPT-4o-mini Llama-3.1-70B Llama-3.1-8B 16.1 9.6 6.5 41.9 58.7 55.7 18.0 6.5 6.2 25.3 24.9 22.8 12.4 7.0 0. 36.8 46.8 43.6 17.6 5.8 4.5 22.3 19.9 16.3 13.8 8.7 6.7 20.5 33.5 31.4 19.5 6.1 4."
        },
        {
            "title": "Quality Enhancement",
            "content": "GPT-4o-mini Llama-3.1-70B Llama-3.1-8B 17.8 -21.8 -1.7 -11.2 6.9 15.4 9.9 2.7 3.0 5.5 -4.1 5.6 13.0 -20.5 -6. -6.3 -5.5 3.7 9.4 2.3 3.5 5.4 -7.9 0.2 15.4 -18.3 -2.7 -13.0 6.5 12.0 9.2 2.4 3. 17.9 16.1 14.2 3.8 -3.1 4.4 Table 5: Performance Gap Recovered (%) results with different meta-prompts on instance generation and quality enhancement. Llama models are instruction-tuned versions and that Inst. denotes instruction-following. explanatory power (R2 = 0.325). However, the moderate R2 value suggests that additional intrinsic measurements beyond our current set might be needed to better predict data generation capabilities. We leave further exploration of this issue to future works."
        },
        {
            "title": "6 Further Analysis Experiments",
            "content": "In this section, we further examine, two critical questions regarding data generation: (1) Should we prioritize quantity using cheaper LMs, or quality using more expensive ones? (Section 6.1) And (2) What is the impact of meta-prompt design, particularly when comparing structured JSON format generation against traditional free-form approaches? (Section 6.2)."
        },
        {
            "title": "6.1 Quantity or quality?",
            "content": "In Section 4, we demonstrated that in some cases, cheaper LMs can be more effective data generators than their expensive counterparts when producing fixed number of instances, though expensive models generally perform better. This raises practical question: Is it more effective to generate larger quantity of instances using cheaper models rather than fewer instances with more expensive ones? We scale up our experiment to generate up to 50K instances using GPT-4o-mini, Llama-3.1-70BInstruct, and Llama-3.1-8B-Instruct across three domains in the instance generation scenario. As shown in Figure 7, generating 50K instances with GPT-4o-mini resulted in better performance than generating 10K instances with GPT-4o at instruction following and math domains and Llama-3.18B-Instruct showed similar patterns in code domain. Given that these LMs are at least five times more cost-effective than GPT-4o, our findings suggest that generating larger volumes of synthetic data with more affordable LMs may be more advantageous than generating smaller datasets with expensive ones. Furthermore, this suggests that instruction diversity or response diversity could affect the PGR results when comparing two settings with different number of training instances."
        },
        {
            "title": "6.2 Effect of Meta-prompts",
            "content": "Recently, Tam et al. (2024) has shown that LMs problem-solving abilities decrease when generating responses in structured formats (e.g., JSON). Given practitioners preference for structured outputs when using LMs (Shorten et al., 2024; Liang et al., 2024) its important to investigate whether this format affects data generation performance. Additionally, we examine the impact of metaprompt design on generation quality. To investigate these questions, we create four additional meta-prompts for comparison. For each setting (instance generation and quality enhancement), we had two co-authors create meta-prompts: one developed an unoptimized version (spending less than 10 minutes)4, while the other created JSON-format version. Table 5 presents our findings. Compared to the other meta-prompts, the AGORABENCH metaprompt achieves the highest scores in five out of six settings, demonstrating the robustness of the setting in AGORABENCH. Comparing the AGORABENCH meta-prompts with unoptimized versions reveals 3.97% performance gap on average, highlighting the importance of meta-prompt optimization. Furthermore, AGORABENCH meta-prompts using free-form generation achieve 4.45% higher performance compared to JSON-format prompts. This aligns with recent findings that structured format requirements may compromise LM output quality (Tam et al., 2024)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce AGORABENCH, benchmark that systematically evaluates LMs data generation capabilities through standardized settings and metrics. Our analysis reveals that models show distinct strengths across different generation methods and domains, highlighting the importance 4This contrasts with the main experiments meta-prompts, which were developed over 2+ hours through iterative trialand-error during the initial experimental phase. of careful data generator selection. While an LMs data generation ability cannot be predicted solely by its cost or problem-solving abilities, we identify interpretable low-dimension principal components from intrinsic evaluation measurements that explain up to 93.4% of the variance and serve as better predictors. Looking ahead, we envision AGORABENCH enabling two key advances in the field. First, since our findings suggest that problem-solving ability is not the primary determinant of data generation quality, researchers can use our benchmark to identify the core capabilities that make an effective data generator and potentially develop specialized LMs specialized in data generation. Second, AGORABENCH can serve as practical evaluation framework for practitioners to assess and improve their data generation pipelines - they can use their custom data generation methods, seed datasets, or meta-prompts and compare against our baseline settings. Furthermore, they can leverage our systematic evaluation methodology to optimize their generation parameters before deploying large-scale data creation. Through these complementary research and applied directions, AGORABENCH aims to accelerate both our theoretical understanding of language models as data generators and their practical deployment in real-world applications."
        },
        {
            "title": "References",
            "content": "Anthropic, A. Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 2024. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are fewshot learners. Advances in neural information processing systems, 2020. Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large lanarXiv preprint guage models trained on code. arXiv:2107.03374, 2021. Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dong, G., Yuan, H., Lu, K., Li, C., Xue, M., Liu, D., Wang, W., Yuan, Z., Zhou, C., and Zhou, J. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D., Yu, L., Liu, Y., Li, J., Xiong, B., Xiong, D., et al. Evaluating large language models: comprehensive survey. arXiv preprint arXiv:2310.19736, 2023. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Honovich, O., Shaham, U., Bowman, S. R., and Levy, O. Instruction induction: From few examples to natural language task descriptions. arXiv preprint arXiv:2205.10782, 2022. Honovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14409 14428, 2023. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kim, S., Joo, S., Kim, D., Jang, J., Ye, S., Shin, J., and Seo, M. The cot collection: Improving zeroshot and few-shot learning of language models via chain-of-thought fine-tuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1268512708, 2023a. Kim, S., Joo, S. J., Jang, Y., Chae, H., and Yeo, J. Cotever: Chain of thought prompting annotation toolkit for explanation verification. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pp. 195208, 2023b. Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 4334 4353, 2024. Lambert, N., Pyatkin, V., Morrison, J., Miranda, L., Lin, B. Y., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: ArenaarXiv preprint hard and benchbuilder pipeline. arXiv:2406.11939, 2024. Liang, J., Li, G., and Yu, Y. Universal and contextindependent triggers for precise control of llm outputs. arXiv preprint arXiv:2411.14738, 2024. Liu, C. Y., Zeng, L., Liu, J., Yan, R., He, J., Wang, C., Yan, S., Liu, Y., and Zhou, Y. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024a. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https:// openreview.net/forum?id=1qvx610Cu7. Liu, R., Wei, J., Liu, F., Si, C., Zhang, Y., Rao, J., Zheng, S., Peng, D., Yang, D., Zhou, D., et al. Best practices and lessons learned on synthetic data. In First Conference on Language Modeling, 2024b. Liu, W., Zeng, W., He, K., Jiang, Y., and He, J. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, 2023b. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pp. 2263122648. PMLR, 2023. MetaAI. Introducing meta llama 3: The most capable openly available llm to date. 2024. URL https: //ai.meta.com/blog/meta-llama-3/. Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2022. Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 20142037, 2023a. Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Le Scao, T., Bari, M. S., Shen, S., Yong, Z. X., Schoelkopf, H., et al. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1599116111, 2023b. Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. Ni, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neubig, G., and You, Y. Mixeval: Deriving wisdom of the crowd from LLM benchmark mixtures. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https:// openreview.net/forum?id=6A29LUZhfv. Nvidia. Leverage the Latest Open Models for Synthetic Data Generation with NVIDIA Nemotron-4-340B. https://developer.nvidia.com/blog/leverage-ourlatest-open-models-for-synthetic-data-generationwith-nvidia-nemotron-4-340b/, 2024. Ruan, Y., Maddison, C. J., and Hashimoto, T. Observational scaling laws and the predictability of langauge In The Thirty-eighth Annual model performance. Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=On5WIN7xyD. Shorten, C., Pierse, C., Smith, T. B., Cardenas, E., Sharma, A., Trengrove, J., and van Luijt, B. Structuredrag: Json response formatting with large language models. arXiv preprint arXiv:2408.11061, 2024. Tam, Z. R., Wu, C.-K., Tsai, Y.-L., Lin, C.-Y., Lee, H.-y., and Chen, Y.-N. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442, 2024. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Viswanathan, V., Zhao, C., Bertsch, A., Wu, T., and Neubig, G. Prompt2model: Generating deployable models from natural language instructions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 413421, 2023. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. Supernaturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 50855109, 2022. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, 2023. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., and Jiang, D. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview. net/forum?id=CfXh93NDgH. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024b. Xu, Z., Jiang, F., Niu, L., Lin, B. Y., and Poovendran, R. Stronger models are not stronger teachers for instruction tuning. arXiv preprint arXiv:2411.07133, 2024c. Yin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig, G. Learning to mine aligned code and natuIn Interral language pairs from stack overflow. national Conference on Mining Software Repositories, MSR, pp. 476486. ACM, 2018. doi: https: //doi.org/10.1145/3196398.3196408. Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mammoth: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2023. Yue, X., Zheng, T., Zhang, G., and Chen, W. MAmmoTH2: Scaling instructions from the web. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https:// openreview.net/forum?id=yVu5dnPlqA. Zhang, X., Chen, Z. Z., Ye, X., Yang, X., Chen, L., Wang, W. Y., and Petzold, L. R. Unveiling the impact of coding data instruction fine-tuning on large language models reasoning. arXiv preprint arXiv:2405.20535, 2024. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024."
        },
        {
            "title": "A Related Work",
            "content": "Conventionally, training LMs on human-crafted data was considered the de facto standard for improving an LMs performance on downstream tasks (Mishra et al., 2022; Wei et al., 2021; Wang et al., 2022; Longpre et al., 2023). Yet, based on the in-context learning abilities of LMs (Brown et al., 2020), series of works demonstrated that LMs could generate novel instances that could be used as post-training data (Wang et al., 2023; Honovich et al., 2022; Kim et al., 2023a). Since then, different works have proposed various data generation methods and prompts to acquire high-quality data, using stronger LMs as data generators. For instance, Taori et al. (2023) used the same data generation method as Wang et al. (2023), but employed InstructGPT instead of GPT3-Davinci and trained Llama-1 instead of T5. Xu et al. (2024a) used ChatGPT as their data generator and proposed method called Evol-Instruct that prompts the data generator to make an existing problem more complex than the original. Mukherjee et al. (2023) used GPT-4 to generate data and improved the original response by adding chainof-thought explanation of how the answer was derived. Xu et al. (2024b) proposed Magpie, data generation method that first prompts an LM with an empty chat template to extract instructions, then iteratively prompts it to generate corresponding responses. While developing new data generation methods is important, choosing which LM to use as data generator is an equally crucial problem for both researchers and practitioners. To the best of our knowledge, Xu et al. (2024c), contemporary work with our work, was the first attempt to measure various LMs data generation capabilities using existing data generation methods. Yet, their settings were confined to our response generation method, whereas we also tested instance generation and quality enhancement methods."
        },
        {
            "title": "Construction",
            "content": "In AGORABENCH, we prepare seed datasets for each domain (instruction-following, math, code) separately in order to prevent positive or negative transfer that occurs during training, which could make it difficult to ground the PGR results to the quality of the synthetic data and LMs data generation capabilities. We use the WebInstruct data (Yue et al., 2024) for math and instruction-following domains in quality enhancement settings. However, the WebInstruct data does not provide labels of whether the given instance is math problem or not. Hence, we prompted GPT-4o-mini-2024-07-18 to classify it using the following prompt: Domain classification for seed data construction Classify whether the following Instance consisted of an Instruction and Response is either related to: 1. Math-related task such as requiring an answer to problem, proving theorem or explaining about mathematical concept. 2. Other tasks Provide your answer in only either 1 or 2, without any greeting message or comment. # Instance: Instruction: <input> Response: <output> # Decision:"
        },
        {
            "title": "C Problem Solving Abilities of LMs\nevaluated as Data Generators",
            "content": "The evaluation results of GPT-4o-2024-08-06, GPT4o-mini-2024-07-18, Claude-3.5-Sonnet-2024-0620, Llama-3.1-405B-Instruct, Llama-3.1-70BInstruct, and Llama-3.1-8B-Instruct are listed in Table 7. We use the settings listed in Appendix D."
        },
        {
            "title": "Student Models",
            "content": "The hyper-parameters used for training student models and hyper-parameters used for evaluating both student models and LMs employed as the data generator are listed in Table 6. For evaluation on MBPP and HumanEval, we use the Evalplus library (Liu et al., 2023a). For evaluation on AlpacaEval and ArenaHard, we use the official library, respectively (Dubois et al., 2024; Li et al., 2024). For GSM8K and MATH, we use the datasets provided in huggingface and use our manual script. All the evaluation scripts are publicly available at our repository. Inference Hyper-parameter Temperature Top_p Max New Tokens Repetition Penalty 0.2 (math) & 0.0 (other domains) 0.95 1024 1.03 Training Hyper-parameter"
        },
        {
            "title": "Base Model\nTorch dtype\nEpoch\nMax Seq Length\nLearning Rate\nTrain Batch Size\nGradient Accumulation\nGPU\nRandom Seed\nTraining Method",
            "content": "meta-llama/Llama-3.1-8B bfloat16 5 4096 1e-5 4 8 H100 (80GB) 4 42 Supervised Fine-tuning Table 6: Hyper-parameters used for inference."
        },
        {
            "title": "E Intrinsic Evaluation of AGORABENCH",
            "content": "The intrinsic evaluation results are listed in Table 8. AGORABENCH Meta-prompts Due to space limits, we present the meta-prompts in our repository and the following link."
        },
        {
            "title": "G Prompt for Intrinsic Evaluation",
            "content": "In the following pages, we list the prompt used for assessing response quality and instruction difficulty with GPT-4o and Prometheus-2-8x7B as well as the score rubrics for used for each domain (instructionfollowing, math, code)."
        },
        {
            "title": "Evaluation Prompt for Response Quality\nand Instruction Difficulty",
            "content": "###Task Description: An instruction (might include an Input inside it), response to evaluate, and score rubric representing evaluation criteria are given. 1. Write detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing feedback, write score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: {instruction} ###Response to evaluate: {response} ###Score Rubric: {score_rubric} ###Feedback:"
        },
        {
            "title": "Math Response Quality Score Rubirc",
            "content": "Does the solution demonstrate mathematical correctness, reasoning, clarity, and precision? Score 1 Description: The solution is incorrect or mathematically flawed, with major errors in reasoning, calculations, or logic, making the answer unusable. Score 2 Description: The solution contains relevant or partially correct information, but has significant errors in calculations or reasoning that substantially affect the result. Score 3 Description: The solution is mostly correct but may contain minor mistakes or gaps in reasoning. The overall structure and approach are sound, but some calculations or logic may need refinement. Score 4 Description: The solution is correct, well-reasoned, and clear, though there may be slight room for improvement or minor refinements to become perfect solution to the problem. Score 5 Description: The solution is excellent, fully correct, and demonstrates high level of mathematical precision, clarity, and creativity, with well-articulated reasoning and no errors. Instruction-following Response Quality Score Rubric Does the response consider wide range of factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail? Score 1 Description: The response is not helpful at all or seems helpful on the surface but is actually incorrect such as including incorrect information, naive miscalculations, or unexecutable code. Score 2 Description: The response contains some relevant or helpful information, but also has major flaws interms of factuality, accuracy, and relevance. Score 3 Description: The response is mostly correct but minor flaws regarding factuality, accuracy, and relevance still exists, while it is overall an okay response. Score 4 Description: The response is accurate, relevant, and helpful, although there are some slight improvements that could be made when an expert evaluates the response. Score 5 Description: The response is excellent. It is completely factual, accurate, relevant, and helpful, demonstrating high degree of depth and creativity."
        },
        {
            "title": "Code Response Quality Score Rubric",
            "content": "How effective, efficient, and logically sound is the code solution, focusing on performance, executability, and correctness? Instruction-following Instruction Difficulty Score Rubric How complex and challenging is the given instruction to answer perfectly? Score 1 Description: The code contains fundamental logic or syntax errors, making it incorrect or unexecutable. It fails to complete the intended task or produces entirely incorrect outputs. Score 2 Description: The code is partially functional but contains major logic errors or inefficiencies that significantly impact performance or correctness. It may run but produces incorrect or incomplete results. Score 3 Description: The code is mostly correct and executable, though there may be minor logic issues, inefficiencies, or suboptimal use of data structures or algorithms. The solution functions as intended, but improvements could make it more efficient or robust. Score 4 Description: The code is fully correct, functional, and reasonably efficient. It completes the task as intended, balancing performance with logical soundness. Minor optimizations could still enhance performance. Score 5 Description: The code is fully correct, optimally efficient, and logically robust, providing the best possible performance for the task. It executes flawlessly without errors or any significant room for improvement. Score 1 Description: The instruction requires only factual knowledge, without any need for reasoning or critical thinking. straightforward, single-step response suffices. Score 2 Description: The instruction requires some reasoning, such as explaining concept involving multiple simple ideas, solving straightforward problem, or providing response that involves few logical steps, though still simple in nature. Score 3 Description: The instruction requires substantial amount of reasoning and the integration of multiple related concepts. Answering it accurately involves multistep process and may require intermediatelevel knowledge or analytical thinking. Score 4 Description: The instruction requires advanced reasoning, demanding deep understanding of complex concepts or substantial problem-solving. Answering it requires carefully navigating multiple interrelated ideas or steps, often involving specialized knowledge or sophisticated analytical skills. Score 5 Description: The instruction is exceptionally challenging and requires highlevel reasoning or novel problem-solving. It involves extensive conceptual understanding, abstraction, and potentially innovative thinking, with substantial effort required to arrive at an accurate and complete answer."
        },
        {
            "title": "Code Instruction Difficulty Score Rubric",
            "content": "How complex and challenging is the math problem to solve? How complex and challenging is the coding problem to solve? Score 1 Description: The problem requires only simple operations or direct application of single, basic concept. Minimal reasoning is needed, and the solution follows immediately from applying known rule or formula. Score 2 Description: The problem requires basic reasoning and involves applying familiar formula or concept with slight variation. It may involve straightforward multistep process, but each step is clear and relies on commonly used methods. Score 3 Description: The problem requires moderate reasoning, combining multiple concepts that interact in meaningful way. Solving it involves several steps and may require logical sequencing or some abstraction, but the approach is approachable with solid foundational understanding. Score 4 Description: The problem demands advanced reasoning, involving multiple interdependent concepts that require careful coordination. Solution steps are less obvious, requiring critical thinking and possibly choosing between multiple solution paths. Solving the problem involves more abstract reasoning or creative application of concepts. Score 5 Description: The problem is extremely complex and demands sophisticated reasoning and problem-solving skills. It may involve novel combinations of concepts, intricate logical chains, or innovative approaches to solve. This level typically requires significant abstraction, exploration of unconventional methods, and flexibility in adapting mathematical tools. Score 1 Description: The problem involves implementing simple functionality or direct operation. It requires minimal logic, with straightforward approach and no complex decision-making. Score 2 Description: The problem requires basic control flow, such as using loops or conditional statements. The logic is clear and sequential, with minimal interaction between different parts of the code. Score 3 Description: The problem involves intermediate logic, combining multiple programming constructs and requiring coherent structure. Solving it requires handling sequence of steps with basic data manipulation, but follows familiar, manageable approach. Score 4 Description: The problem demands advanced reasoning and use of complex data structures or algorithms. It involves non-trivial interactions, such as managing multiple components and optimizing for efficiency. The solution requires significant algorithmic thinking and structured problem decomposition. Score 5 Description: The problem is extremely complex, requiring sophisticated algorithm design, efficient data handling, and advanced techniques. It demands innovative approaches, with intricate component interactions and constraints that need careful optimization. Solving it typically requires deep problem-solving skills and adaptability across programming paradigms. Problem-solving ability"
        },
        {
            "title": "Data Generator",
            "content": "GSM8K MATH MBPP Human Alpaca Arena Eval 2.0 Hard"
        },
        {
            "title": "Average",
            "content": "GPT-4o GPT-4o-mini Claude-3.5-Sonnet Llama-3.1-405B Llama-3.1-70B Llama-3.1-8B 96.1 93.2 96.4 96.8 95.1 78.9 76.6 70.2 71.1 73.8 68.0 34.6 86.2 85.7 89.2 84.5 84.2 68.5 91.5 88.4 92.0 89.0 80.5 69.5 57.5 50.7 52.4 39.3 38.1 24. 77.9 64.2 82.0 66.8 51.6 25.5 80.9 75.4 80.5 75.0 69.6 50.2 Table 7: Problem-solving abilities of LMs measured by benchmark scores."
        },
        {
            "title": "Quality Enhancement",
            "content": "Math Code Inst. Follow Avg Math Code Inst. Follow Avg Math Code Inst. Follow Avg GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct 2.92 2.38 3.24 2.74 2.87 3. 3.73 3.44 4.11 3.63 3.72 3.86 2.13 2.05 2.04 1.96 1.78 1.83 3.72 3.96 3.39 3.20 2.97 1.99 3.93 4.05 3.95 3.76 3.68 3.22 Instruction Difficulty (LLM-as-a-Judge; GPT-4o Score) 3.48 3.42 4.03 3.50 3.45 3. 3.06 2.89 3.54 2.87 2.96 3.08 3.16 2.90 3.60 3.04 3.09 3.20 2.27 2.27 2.27 2.27 2.27 2.27 2.21 2.21 2.21 2.21 2.21 2.21 1.41 1.41 1.41 1.41 1.41 1.41 1.97 1.97 1.97 1.97 1.97 1. Instruction Difficulty (LLM-as-a-Judge; Prometheus-2-8x7B Score) 3.57 3.38 4.51 3.27 3.43 3.48 1.28 1.31 1.34 1.29 1.27 1.33 3.95 3.96 4.03 3.74 3.59 2.51 3.95 3.94 4.45 3.84 3.94 3.99 3.44 3.32 3.18 2.19 2.19 2. 3.75 3.59 4.36 3.58 3.69 3.78 3.00 3.00 3.00 3.00 3.00 3.00 2.76 2.76 2.76 2.76 2.76 2.76 Instruction Difficulty (Perplexity) 2.28 2.23 2.19 1.81 1.74 1.74 2.26 2.28 2.16 1.90 1.86 1. 4.23 2.12 3.48 1.91 1.72 1.81 2.24 2.24 2.24 2.24 2.24 2.24 3.41 3.20 3.63 2.42 2.52 2.48 Response Quality (LLM-as-a-Judge; GPT-4o Score) 4.42 4.35 4.34 4.13 4.12 3.82 4.03 4.09 3.92 3.69 3.56 2. 3.99 3.85 3.80 3.51 3.31 2.90 3.79 3.76 3.75 3.76 3.65 3.26 4.44 4.41 4.24 4.29 4.22 4.17 2.67 2.67 2.67 2.67 2.67 2.67 3.30 2.53 3.09 2.08 2.03 2.09 4.07 4.01 3.93 3.85 3.72 3. Response Quality (LLM-as-a-Judge; Prometheus-2-8x7B Score) 3.49 3.46 3.37 3.24 3.36 3.06 4.07 4.04 4.04 3.92 3.91 3.81 3.83 3.85 3.78 3.64 3.65 3.36 4.02 3.96 3.94 3.81 3.73 3.62 3.28 3.39 3.29 3.42 3.37 3. 3.97 3.93 3.83 3.91 3.86 3.88 Response Quality (Reward Model; Skywork-RM-8B Score) GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct 13.90 5.74 10.67 -0.50 -2.17 -7.71 GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct GPT-4o (2024-08-06) GPT-4o-mini (2024-07-18) Claude-3.5-Sonnet (2024-06-20) Llama-3.1-405B-Instruct Llama-3.1-70B-Instruct Llama-3.1-8B-Instruct 0.4170 0.4091 0.4124 0.3996 0.4003 0.4201 0.4564 0.4558 0.4719 0.4490 0.4520 0.4768 23.20 -1.18 20.22 19.54 20.42 7.16 0.4640 0.5127 0.4872 0.5411 0.5015 0.4785 0.5347 0.5719 0.5648 0.6122 0.5771 0.5651 27.79 7.80 18.20 13.63 11.22 3. 0.3047 0.3013 0.2940 0.2789 0.2682 0.2956 0.2918 0.2814 0.3220 0.2523 0.2596 0.2660 21.63 4.12 16.36 10.89 9.83 0.97 8.82 13.71 5.56 -1.23 -3.26 -3.89 -0.10 23.74 1.67 2.68 2.17 -1.53 Instruction Diversity (c-dist) 0.3952 0.4077 0.3979 0.4065 0.3900 0.3981 0.3737 0.3737 0.3737 0.3737 0.3737 0.3737 0.3958 0.3958 0.3958 0.3958 0.3958 0.3958 Response Diversity (c-dist) 0.4276 0.4364 0.4529 0.4378 0.4296 0.4360 0.4126 0.4095 0.4156 0.4037 0.4012 0. 0.4719 0.4726 0.4647 0.4737 0.4784 0.4778 8.60 20.71 0.17 10.65 5.85 8.72 0.3386 0.3386 0.3386 0.3386 0.3386 0.3386 0.2714 0.2811 0.2788 0.2551 0.2530 0.2530 3.76 3.76 3.69 3.71 3.65 3.58 5.77 19.39 2.46 4.03 1.59 1. 0.3694 0.3694 0.3694 0.3694 0.3694 0.3694 0.3853 0.3877 0.3864 0.3775 0.3775 0.3795 2.44 2.47 2.47 2.45 2.48 2.43 3.37 3.36 3.38 3.35 3.32 3.30 2.03 2.08 1.99 2.10 2.12 2.06 3.62 3.57 3.64 3.36 3.23 3. 3.98 3.92 4.00 3.78 3.73 3.68 1.51 1.38 1.52 1.47 1.49 1.49 2.14 1.98 2.24 2.11 2.21 2.09 3.60 5.50 2.46 3.10 2.84 3.17 3.66 3.22 3.77 3.37 3.22 2.76 3.28 3.04 3.48 3.23 3.20 3. 4.86 3.42 6.29 -1.89 -3.04 -3.68 0.4263 0.4270 0.4307 0.4344 0.4232 0.4302 0.4455 0.4577 0.4610 0.4633 0.4571 0.4738 -7.48 -12.93 -5.31 -10.43 -11.54 -12.61 0.4870 0.4670 0.4903 0.4796 0.4756 0.4619 0.5065 0.5184 0.5141 0.5155 0.5134 0. 1.79 1.81 1.83 1.85 1.87 1.83 2.50 2.53 2.61 2.64 2.76 2.67 3.83 3.97 3.04 3.90 3.98 3.98 3.99 3.96 4.29 3.80 3.80 3.52 3.69 3.73 4.03 3.66 3.62 3.49 -4.73 -5.16 10.76 -7.35 -8.60 -10. 0.2943 0.2956 0.2921 0.3033 0.3018 0.2984 0.2271 0.2257 0.2325 0.2239 0.2233 0.2254 1.91 1.89 1.94 1.92 1.95 1.92 2.67 2.63 2.74 2.70 2.76 2.68 3.15 3.85 2.50 3.03 2.98 3.07 3.76 3.58 3.90 3.51 3.42 3. 3.65 3.57 3.84 3.56 3.52 3.42 -2.45 -4.89 3.92 -6.56 -7.72 -8.81 0.4025 0.3965 0.4044 0.4058 0.4022 0.3968 0.3930 0.4006 0.4025 0.4009 0.3979 0.4045 Table 8: Intrinsic evaluation results of AGORABENCH."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "KAIST AI",
        "NEC Laboratories Europe",
        "Ss. Cyril and Methodius University of Skopje",
        "University of Washington"
    ]
}