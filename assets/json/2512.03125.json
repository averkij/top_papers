{
    "paper_title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "authors": [
        "Xiwen Wei",
        "Mustafa Munir",
        "Radu Marculescu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 5 2 1 3 0 . 2 1 5 2 : r Mitigating Intraand Inter-modal Forgetting in Continual Learning of Unified Multimodal Models Xiwen Wei, Mustafa Munir, Radu Marculescu Department of Electrical and Computer Engineering The University of Texas at Austin {xiwenwei, mmunir, radum}@utexas.edu"
        },
        {
            "title": "Abstract",
            "content": "Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide theoretical explanation rooted in gradient conflict between modalities. To address both intraand inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both interand intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes are publicly available: https://github.com/Christina200/MoDE-official.git."
        },
        {
            "title": "Introduction",
            "content": "Traditional multimodal models are typically divided into two categories: multimodal understanding (e.g. answering questions about images) and multimodal generation (e.g. generating images from text prompts) [1, 2]. Unified Multimodal Generative Models (UMGMs) aim to integrate these two tasks within single framework. Recent advancements in UMGMs [38] have demonstrated strong performance on wide range of tasks, such as visual question answering (VQA), image captioning, visual reasoning, classification, reading comprehension, and image generation. These models typically embed multiple input types into shared representation space and leverage one single transformer backbone to model cross-modal interactions. Training generally follows twostage paradigm: an initial pretraining phase focused on textimage alignment, followed by fine-tuning for specific downstream tasks. During the second stage, instruction tuning (which fine-tunes the model on diverse, task-specific instructions paired with expected outputs) has emerged as widely adopted strategy to better align model behavior with human intent [9, 10]. While UMGMs demonstrate impressive zero-shot performance on wide range of unseen instructions, their effectiveness is inconsistent across all tasks, particularly when the relevant task data is absent from the pretraining corpus. Expanding the training dataset to include new task data can significantly improve performance on those tasks. However, given the continual emergence of novel multimodal 39th Conference on Neural Information Processing Systems (NeurIPS 2025). (a) Continual illustrates catastrophic forgetting during naive sequential instruction tunFigure 1: ing. instruction tuning starts with the Chameleon [3] model on tasks ScienceQATextVQAImageNetGQAVizWiz. Start refers to the performance immediately after the model has been tuned on task, while End denotes performance after completing all tasks. larger gap between the two bars indicates more severe forgetting. (b) visualizes forgetting in both multimodal generation tasks (in red, representing inter-modal forgetting) and multimodal understanding tasks (in green, representing intra-modal forgetting). Our proposed MoDE mitigates both types of forgetting, preserving performance across modalities. tasks, repeatedly merging new data and retraining the model is computationally expensive and impractical. This motivates the need for new approaches that can enable UMGMs to flexibly and efficiently adapt over time. This goal aligns with the principles of continual learning (CL), where models are designed to acquire new capabilities incrementally, similar to how humans learn [11]. Existing studies in CL have shown that sequentially fine-tuned models suffer from catastrophic forgetting, phenomenon where model fine-tuned on new tasks tends to forget or overwrite the previously acquired knowledge [12]. More recently, several works have explored continual instruction tuning for multimodal large language models (MLLMs) [1315]. However, these models are limited to language-only outputs and, as result, prior research has focused exclusively on tasks such as VQA and other multimodal understanding benchmarks. In contrast, UMGMs are capable of both multimodal understanding and generation within single backbone, enabling the production of both textual and visual outputs and thus representing fundamentally different paradigm. This raises new and largely unexplored challenge, whereby forgetting occurs not only within single modality (intra-modal), but also across modalities (inter-modal) during continual instruction tuning of UMGMs. Starting from these overarching ideas, we ask: Can UMGMs continually acquire new capabilities through instruction tuning without suffering catastrophic forgetting, both within modality (e.g., multimodal understanding) and across modalities (e.g., improving multimodal understanding while preserving visual generation)? As shown in Figure 1, UMGMs do experience catastrophic forgetting across range of tasks, including visual reasoning, classification, VQA, and image generation. Also, while existing methods [16, 17] can mitigate either intra-modal or inter-modal forgetting, none of them can effectively address both issues simultaneously. To address this fundamental challenge, we propose Modality-Decoupled Experts (MoDE), new lightweight architecture which incorporates modality-aware sparse mixture of LoRA adapters for text (T-MoE) and single LoRA adapter for images (V-Adapter). The intra-modal forgetting during continual multimodal understanding tasks is mitigated by the T-MoEs routing, which selectively activates the appropriate experts. The inter-modal forgetting in image generation is prevented through modality decoupling and knowledge distillation between the pre-trained model (teacher) and the newly added image LoRA (student). During continual instruction tuning, only the MoDE components are trained, while the pre-trained UMGM parameters remain frozen to preserve their robust crossmodal alignment. Our results show that this design simultaneously mitigates both intra-modal and inter-modal catastrophic forgetting. Our contributions are: 1. We identify and illustrate the unique challenge of inter-modal catastrophic forgetting, in addition to the more common intra-modal forgetting in autoregressive transformers that unify multimodal understanding and generation under continual instruction tuning. We further provide theoretical analysis attributing inter-modal forgetting to modality gradient conflict, and formally prove that modality decoupling can mitigate this conflict both theoretically and experimentally. 2. We propose Modality-Decoupled Experts (MoDE), new lightweight and scalable architecture that tackles both intraand inter-modal forgetting by decoupling modality-specific updates (to reduce inter-modal gradient conflict) and applying knowledge distillation to preserve pre-trained image generation capabilities. 3. Through extensive experiments, we show that MoDE outperforms SOTA baselines (e.g. CLMoE [18], Model Tailor [17]) when mitigating both intraand inter catastrophic forgetting simultaneously. This opens up new possibilities for scalable continual learning in UMGMs. The remainder of this paper is organized as follows: Section 2 reviews related work. Section 3 introduces inter-modal forgetting in UMGMs. Section 4 presents our proposed method, and Section 5 details the experimental results. Finally, Section 6 concludes our paper."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Unified multimodal generative models (UMGM) Early efforts to unify visual understanding and generation [1922] typically combine diffusion models with MLLMs, conditioning the diffusion process on LLM-generated embeddings. While effective for certain tasks, this diffusion+MLLM design lacks tight coupling between image generation and language modeling, resulting in suboptimal performance on instruction-based generation [23]. More recent approaches treat both understanding and generation as unified next-token prediction problem [2426, 3]. These models differ in their encoder and decoder designs, e.g. some [3, 6, 27] employ vision tokenizers such as VQGAN [28] to discretize images into token sequences, while others [29, 30] use semantic encoders like CLIP [31] or SigLIP [32] to represent images as sequences of continuous embeddings. Their decoders typically rely on either VQVAE [33] or diffusion-based generator [34, 22], but all maintain an autoregressive transformer backbone. Our method is designed for transformer-based UMGMs and works with broad class of models built on this autoregressive architecture, regardless of their choice of encoding or decoding mechanism. 2.2 Mixture of experts (MoE) The Mixture of Experts architecture [3538] combines specialized experts with gating mechanism that efficiently allocates tokens. Several studies explore integrating MoE with LoRA [39] to facilitate scalable and efficient training of MLLMs and LLMs [4045]. Recently, the scalability and modularity of MoE have attracted growing attention in continual learning [18, 46, 47]. For instance, LifelongMoE [48] introduces strategy to train new experts while freezing the old ones. However, prior MoE-based CL methods share the same set of experts across modalities, which leads to modality interference during multimodal generation and exacerbates the inter-modal forgetting. Motivated by these limitations, we propose modality-decoupled MoE with LoRA, aiming at mitigating both intermodal forgetting caused by modality interference and intra-modal forgetting within each modality. 2.3 Catastrophic Forgetting Catastrophic forgetting remains fundamental challenge in multimodal continual instruction tuning (MCIT), where multimodal generative model is incrementally adapted to new datasets and task instructions without costly retraining from scratch. Existing methods to address catastrophic forgetting in MCIT typically fall into four categories: regularization-based [49], architecture-based [50, 14], 3 Figure 2: Inter-modal catastrophic forgetting during continual instruction tuning across three VQA tasks. The pre-trained UMGM serves as the upper bound for image generation quality. CLIP scores under each sample reflect text-image alignment in CLIP feature space. Red bounding boxes highlight regions with degraded image quality and low CLIP scores, indicating increasing misalignment between prompts and generated images. For instance, the image generated for the prompt photo of car depicts building instead of car (VQA task #3). replay-based [15], and prompt-based approaches [51, 52]. For instance, EProj [11] applies tasksimilarity-based regularization to constrain model updates that are important for previous tasks. Fwd-Prompt [51] selects prompts using both textual and visual features to preserve earlier knowledge. LLaCA [50] employs exponential moving average (EMA) to merge the weights of old and new models, thereby maintaining both past task performance and pre-trained capabilities. However, these approaches address only the intra-modal catastrophic forgetting, where old tasks, new tasks, and pre-trained capabilities all belong to the same output modality, typically text (e.g., in VQA tasks) [5254]. While this assumption holds for traditional multimodal models with unimodal outputs, it is insufficient for UMGMs which generate multimodal outputs such as text and images. 3 Inter-modal Catastrophic Forgetting in UMGMs Inter-modal catastrophic forgetting can manifest in several forms. For instance, prior work [55] examined the loss of text-only generation abilities in MLLMs when compared to their original LLM counterparts. Other studies have explored modality decoupling strategies in encoders and adapters of UMGMs [56, 57]. In this paper, we restrict our focus to multimodal tasks, specifically the forgetting that occurs in multimodal understanding and generation. In particular, we study visual generation forgetting that arises when tuning models on understanding tasks. Complementary results on understanding forgetting induced by tuning on image generation tasks are provided in Appendix F. To investigate inter-modal catastrophic forgetting in UMGMs from the visual perspective, we conduct preliminary experiment by sequentially tuning Chameleon [3] using LoRA on three VQA datasets [5860], while monitoring image generation performance with prompt-based evaluation. As shown in Figure 2, we observe two key issues in image generation as tuning progresses: 1) The overall image quality degrades significantly, 2) The alignment between the generated image and the input prompt worsens over time. These findings indicate that inter-modal catastrophic forgetting does indeed occur in UMGMs when they are continually fine-tuned on sequence of multimodal understanding tasks. We attribute this forgetting to the UMGMs utilizing unified architecture (e.g., transformer) with shared parameters for both text and visual modalities [61]. When tuning on single modality (e.g., text), the shared parameters are updated in way that interferes with the models performance on the other modality (e.g., image), phenomenon consistent with observations reported in recent studies [62]. To formalize this interference, we introduce the notion of modality gradient conflict. Definition 1 (Modality Gradient Conflict). Let gv = θLv denote the gradient of the imagegeneration loss and gt = θLt denote the gradient of the text-generation loss, both with respect to shared model parameters θ. gv and gt are said to be conflicting with each other if gv, gt < 0, where , denotes the standard Euclidean inner product. Proposition 1. When fine-tuning on the text-generation tasks, stochastic gradient descent (SGD) update with sufficiently small step size η modifies the model parameters as θ θ ηgt. The resulting change in the visual loss is: Lv = Lv(θ ηgt) Lv(θ) = η gt, gv + η2 2 Hvgt, (1) where Hv = 2 generation will increase the visual loss, leading to inter-modal forgetting. θLv is the Hessian of the visual loss. Hence, when gt, gv < 0, step optimizing text Motivated by this, we propose MoDE, modality-decoupled adapter framework that provably eliminates the first-order modality conflict and thereby mitigates the inter-modal catastrophic forgetting. detailed proof of Proposition 1 is available in Appendix A."
        },
        {
            "title": "4 Methodology",
            "content": "4.1 Problem formulation Continual instruction tuning Assume UMGM has been pre-trained with abundant visionlanguage data, captured by trainable parameters θ. We further train to adapt this UMGM to new tasks in sequential manner. Each task is denoted by task descriptor τ {1, 2, ..., S}, and owns an with Nτ data triplets, where img, ins and independent dataset Dτ = ans indicate the input image tokens, instruction text tokens, and answer text tokens, respectively. τ,j , ans τ,j ) τ,j , ins (X img (cid:111)Nτ j=1 (cid:110) We formalize the autoregressive cross-entropy objective for task τ as: max θ E(ximg,xins,xans)Dτ (cid:88) i=1 log pθ(X ans img, ins, ans <i ) (2) where the expectation is taken over the data distribution Dτ , is the length of the answer sequence, ans <i indicate all answer tokens before the index i, ans indicates the i-th answer token. Image generation Let ins be the text prompt tokens, and let img be the image tokens generated autoregressively. Following standard autoregressive factorization, the process can be formalized as: p(X img ins) = (cid:89) i=1 pθ(X img ins, img <i ) (3) where is the length of the image token sequence, img indicate all image tokens before the index <i i, img indicate the i-th image token. Once all tokens are generated, they are detokenized via decoder (such as VQ-VAE) to produce the resultant image. 4.2 Modality-Decoupled Experts (MoDE) MoE adapters are lightweight modules that route inputs through sparse subset of expert networks, enabling task-specific adaptation with minimal interference to shared parameters [63]. Although MoE adapters can effectively mitigate intra-modal catastrophic forgetting, naively sharing their parameters across modalities can lead to modality gradient conflict (see Section 3). To address this issue, we propose Modality-Decoupled Experts (MoDE), modality-decoupled adapter architecture that 5 Figure 3: An autoregressive UMGM with our proposed MoDE integrated into its linear layers (MLPs). V-Adapter (Visual LoRA, in the light blue box): LoRA specialized for both the generation and understanding of image tokens. T-MoE Adapters (Text Mixture-of-Experts LoRA, in the light brown box): MoE-LoRA designed for text tokens, supporting continual learning of multimodal understanding tasks. T-router computes the routing weights gj(x) that determine how much each expert LoRA contributes for given text token. The circled + symbol denotes addition. During continual instruction tuning, the T-MoE primarily updates for text answers, while the V-Adapter handles image tokens. To preserve the models image generation capability and mitigate inter-modal forgetting, we apply knowledge distillation loss from the original (teacher) UMGM to the new (student) models V-Adapter. isolates text and visual updates into separate trainable subspaces. As shown in Figure 3, MoDE mitigates modality gradient conflict while preserving the flexibility and scalability of MoE adapters. Getting now into details, LoRA introduces trainable low-rank update to frozen weight matrix Rdoutdin , where din and dout denote the input and output feature dimensions, respectively. The update is parameterized by two learnable low-rank matrices Rrdin and Rdoutr, such that: (4) where and are learnable matrices with rank min(din, dout) and scaling factor α R. Given an input token representation Rdin, the modified linear transformation : Rdin Rdout becomes: = BA α MoE-LoRA generalizes LoRA by introducing mixture of expert LoRA modules to handle diverse tasks. router assigns each input token Rdin soft distribution over experts: (h) = hW + α hAB (5) gj(x) = softmax(xWg)j (6) where Wg Rdinn is trainable gating matrix with denotes the number of experts, and gj(x) denotes the routing probability of expert j. The resulting update (x) is weighted combination of expert LoRA updates: (x) = gj(x)BjAj (7) j=1 where each expert has Aj Rrdin and Bj Rdoutr, and α is hyper-parameter. Given an input token representation Rdin , the final linear transformation becomes: (cid:88) α (h) = hW + α (cid:88) j=1 6 gj(x)hA (8) MoDE integrates the above two types of adaptations into UMGMs: V-Adapter and T-MoE, as shown in Figure 3. The V-Adapter is LoRA module specialized for visual tokens which adapts the model to both visual understanding and image generation tasks. In contrast, the T-MoE is MoE-LoRA applied to text tokens, designed to enhance continual instruction tuning on multimodal understanding tasks without interfering with visual knowledge. Together, these two components allow MoDE to balance modality-specific adaptation (via the V-Adapter) and task-level flexibility (via the T-MoE) within unified multimodal backbone. To formalize the forward pass of multimodal tokens, we consider frozen linear layer Rdoutdin ] RN din denote the hidden in UMGM. Let = [ht m+L, ht states of sequence of interleaved images and text with length , where the subscript indicates position and the superscript indicates the token modality (t for text and for image). We untie the sequence into the text sequence = [ht ] and the image sequence = m+L+1, . . . , ht [hi m+L], where is the start index of image tokens and is their length. We feed into the V-adapter using Eq. 5 to obtain ˆH = (H i), and feed into T-MoE using Eq. 8 to obtain ˆH = (H t). Finally, we reassemble ˆH and ˆH into their original positions to form the output sequence ˆH. m+L+1, . . . , ht m+1, . . . , hi m+1, . . . , hi 1, . . . , ht m, hi MoDE therefore cleanly decouples different modalities: text tokens benefit from experts diversity for continual instruction tuning, while image tokens rely on visual adapter. Because T-MoE and V-adapter are disjoint, their update directions cannot interfere arbitrarily. Proposition 2. In MoDE, let ϕ denote the parameters of the T-MoE and ψ denote the parameters of the V-adapter. single gradient step θ (cid:55) θ ηϕLt updates only ϕ, and the resulting change in the visual loss satisfies: Lv = λmax (cid:0)2 ϕϕLv (cid:1)(cid:13) (cid:13)ϕLt (cid:13) 2 (cid:13) , (9) η2 2 Thus, MoDE provably bounds the inter-modal interference to O(η2), compared to O(η) under modality-decoupled baseline as in Proposition 1. This theoretical property directly explains the minimal gradient conflict and superior retention of visual generation capabilities observed in our experiments. See Appendix for proof of Proposition 2 and Appendix for its empirical verification. 4.3 Knowledge Distillation During instruction tuning, the V-Adapter learns to understand images, which can degrade its original generation ability. We anchor the V-Adapter to the frozen pre-trained backbone (teacher) via logit-level knowledge distillation (KD), as shown in Figure 3. small subset of images sampled from the LAION-5B dataset [64] serves as the reference data. For each reference token, both teacher (T ) and student (S) predict the next image token. Let zT , zS denote their logits for the i-th image token.Using softening factor β, the KD loss (LKD) is computed as the KullbackLeibler divergence [65] between their softened output probabilities: LKD = β2 (cid:88) i=1 DKL (cid:0)Softmax(zT /β) Softmax(zS /β)(cid:1) (10) where is the number of generated image tokens and denotes the distance between two distributions. Final objective. The T-MoE is trained with the instruction-tuning cross-entropy loss (LCE) as in (2): LT-MoE = LCE = (cid:88) log pθ(X ans img, ins, ans <i ) 1 i=1 The V-Adapter uses weighted combination of the cross-entropy and the distillation losses: LV-Adapter = LCE + λ LKD, λ > 0 (11) (12) This mixed objective balances the updates for visual understanding and image generation in the V-adapter, thereby mitigating inter-modal catastrophic forgetting. In our experiments, we set λ = 0.3 based on its optimal performance (see Appendix for sweep over different λ values). Table 1: Quantitative comparison of inter-modal (image generation) and intra-modal (multimodal understanding) catastrophic forgetting. For image generation, Zero-shot reports the pre-trained reference we aim to preserve. DualPrompt [16] yields low accuracy and low forgetting. This is because adding small set of prompt embeddings slightly biases the feature representations, which is insufficient to learn the new and hard tasks. captures the overall performance drop. Bold numbers are best; underlined numbers are second best in each column. Results are averaged over three different runs. Method Zero-shot Seq LoRA Model tailor [17] DualPrompt [16] MoELoRA [44] CL-MoE [18] MoDE (Ours) Image Generation Multimodal Understanding Text alignment () Image alignment () FID () Accuracy () Forgetting () () 0.2592 0.2162 0.2384 0.2648 0.2248 0.2081 0. 0.5205 0.5150 0.5093 0.5083 0.5095 0.5150 0.5170 52.13 56.12 55.47 56.08 65.16 65.87 53.74 22.48 28.43 32.62 31.92 33.01 32.86 33. - 35.33 27.66 6.82 30.77 30.95 25.99 34.84 28.57 24.70 25.40 24.31 24.46 22."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental setup Evaluation Benchmarks For continual instruction tuning, we evaluate on five datasets: Four visual question answering datasets (ScienceQA [58], TextVQA [66], GQA [59], and VizWiz [67]) and one image classification dataset (ImageNet [60]). For image generation, we use the CustomConcept101 [68] dataset, which contains 101 concepts grouped into 16 broader categories. Evaluation Metrics For continual instruction tuning, we follow the protocols from [69, 70] and report two metrics: (1) Average Accuracy (ACC), defined as the mean accuracy across all tasks after the final round of training; and (2) Average Forgetting (Fgt), which quantifies performance degradation on previous tasks and is computed as the mean reduction from the best observed accuracy to the final accuracy, for each task. We use exact matches to determine answers correctness. We evaluate image generation quality with three metrics: (1) Image alignment. Cosine similarity between CLIP embeddings of the generated image and reference image that depicts the target concept; higher is better (). (2) Text alignment. Cosine similarity between the generated image embedding and the CLIP embedding of its text prompt; higher indicates better prompt fidelity (). (3) Fréchet Inception Distance (FID) [71]. Fréchet distance between features of generated and real images; lower values mean the synthetic images are closer to the real distribution (). Baselines We compare our proposed MoDE with the following continual learning approaches: Seq LoRA, which sequentially trains the model on new tasks using LoRA adapters [39]; Model Tailor [17]; DualPrompt [16]; MoE-LoRA [63]; CL-MoE [18]; and Joint Training (upper bound), which trains the model on all datasets simultaneously and serves as the upper bound reference. The implementation details of baseline methods are provided in Appendix I. We use two autoregressive backbone UMGMs: Chameleon [3] and Janus-Pro [4]. For Chameleon, we adopt the implementation in [72] since the original model and checkpoints are not publicly available. Results on Janus-Pro are provided in Appendix G. See Appendix for implementation details. 5.2 Main results Quantitative results. As shown in Table 1, our proposed MoDE built on Chameleon [3] effectively mitigates both interand intra-modal catastrophic forgetting, outperforming other CL baselines. In the Image Generation columns, MoDE maintains image quality comparable to the pre-trained model (Zero-shot), indicating strong resistance to inter-modal forgetting. For instance, MoDE achieves an FID score of 53.74, closely matching the 52.13 of the pre-trained model (zero-shot). While DualPrompt [16] also preserves image generation quality, its performance in continual instruction tuning (as shown in the Multimodal Understanding columns) is poor: the average accuracy is only 31.92%. Notably, although DualPrompt shows low average forgetting of 6.82%, this is misleading: its best accuracy on ImageNet is only 24.55%, whereas other methods exceed 70%. This low forgetting 8 Table 2: Continual instruction-tuning results on sequence of five datasets. Accuracy (ACC) is exact-match (higher is better); Fgt is average forgetting (lower is better). For each method, the upper row shows the best accuracy during continual instruction tuning, and the lower row shows the final accuracy after completing all tasks. Upper bound reports performance obtained by individually fine-tuning model on each task. Bold numbers denote the best result, and underlined numbers denote the second best. Results are averaged over three different runs. Method Zero-shot Seq LoRA Datasets Metrics ScienceQA TextVQA ImageNet 51.52 23.49 16.53 GQA 14.22 VizWiz ACC () Fgt () 6.64 22.48 - 72.430.82 33.960. 39.341.24 16.841.42 89.411.48 13.680.93 37.931.61 33.201.77 44.381.05 44.380.06 Model tailor [17] 74.900.72 55.401. 40.041.18 17.784.83 78.220.63 14.971.09 37.351.02 31.710.76 43.250.94 43.250.68 28.43 35. 32.62 27.66 DualPrompt [16] 60.010.89 59.641.03 31.481.02 19.683.81 24.550.57 8.610. 29.911.27 30.750.69 40.910.75 40.910.72 31.92 6.82 MoELoRA [44] 71.790.94 50.471. 39.622.08 19.642.86 94.751.53 18.731.96 37.661.23 31.890.78 44.330.65 44.330.70 CL-MoE [18] MoDE (Ours) 71.350.88 55.341.09 38.821.19 17.642.77 90.082.59 15.621.02 37.372.14 31.570.81 44.130.73 44.130.67 70.450.74 56.251. 38.901.47 19.082.47 80.670.45 14.670.76 37.030.47 33.101.16 44.350.62 44.280.09 33.01 30. 32.86 30.95 33.47 25.99 Upper bound 73. 40.74 91.88 36.56 44.15 57.32 - arises not from effective retention, but from limited ability to learn in the first place: if model never learns well, it has little to forget. Although MoE-LoRA [63] performs well in mitigating intra-modal forgetting, it suffers from severe inter-modal forgetting, as evidenced by the worse image generation quality, with an FID of 65.16 compared to 56.12 for the vanilla SeqLoRA. In contrast, our MoDE achieves the highest accuracy of 33.47% and the lowest FID of 53.74, demonstrating its effectiveness in mitigating both inter-modal and intra-modal forgetting simultaneously. Table 2 presents more detailed results of continual instruction tuning. In this setting, our MoDE consistently outperforms all comparison methods and sustains effective learning across all tasks. Qualitative results. As shown in Figure 4, our MoDE can generates higher quality images compared to other methods, effectively preserving the pre-trained models image generation capability. For instance, in the third row of Figure 4, given the prompt Barn in the fall season with leaves all around, MoDE successfully generates yellow leaves surrounding the barn, whereas Model Tailor [17] and CL-MoE [18] fail to capture this detail. See Appendix for more qualitative results. 5.3 Ablation results To validate the performance gains of MoDE, we present our ablation results in Table 3. T-MoE LoRA includes only the T-MoE adapters without the V-adapter. MoDE w/o KD removes knowledge distillation, training both the T-MoE adapters and the V-adapter solely with cross-entropy loss. Of note, MoDE is also robust to task order. See Appendix for more ablation results. Table 3: Ablation study shows that both the modality-specific adapters and knowledge distillation are essential to the effectiveness of MoDE. Model Text alignment () Image alignment () FID () Accuracy () Forgetting () Image Generation Visual Understanding Chameleon [3] + T-MoE LoRA + MoDE w/o KD + MoDE 0.2592 0.2583 0.2364 0.2458 52.13 51.28 54.61 53.74 22.48 33.03 33.07 33.47 - 28.65 26.49 25. 0.5205 0.5317 0.5156 0.5170 9 Figure 4: Qualitative results of image generation on the Chameleon [3] model. Our method generates more visually coherent and faithful images compared to other baselines (e.g., the realistic dog in the first row, steam in the second row). Additional examples are provided in Appendix C. These results demonstrate the effectiveness of both the T-MoE and V-adapter. Using only the T-MoE preserves image generation capabilities by leaving visual components untouched, but results in suboptimal visual understanding, as the model lacks updates necessary for better visual comprehension. We also report the computational efficiency of MoDE, including training time, memory consumption, and parameter overhead, in Appendix H. These results show that MoDE achieves favorable trade-offs between accuracy and efficiency compared to existing methods."
        },
        {
            "title": "6 Conclusion",
            "content": "We have proposed Modality-Decoupled Experts (MoDE) to address both intraand inter-modal catastrophic forgetting in continual instruction tuning for Unified Multimodal Generative Models (UMGMs). We have identified inter-modal forgetting, explained its cause due to the modality gradient conflict, and designed MoDE to decouple modality-specific updates, mitigating conflict theoretically and empirically. Extensive experiments show that MoDE significantly outperforms SOTA baselines, establishing it as strong solution for continual learning in unified multimodal generation."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This work is supported in part by the NSF grant CCF-2531882, and in part by an Amazon Research Award, Spring 2025."
        },
        {
            "title": "References",
            "content": "[1] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in Neural Information Processing Systems, vol. 36, pp. 3489234916, 2023. [2] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. [3] C. Team, Chameleon: Mixed-modal early-fusion foundation models, arXiv preprint arXiv:2405.09818, 2024. [4] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan, et al., Janus: Decoupling visual encoding for unified multimodal understanding and generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025. [5] C. Zhou, L. YU, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettlemoyer, and O. Levy, Transfusion: Predict the next token and diffuse images with one multi-modal model, in The Thirteenth International Conference on Learning Representations, 2025. [6] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al., Emu3: Next-token prediction is all you need, arXiv preprint arXiv:2409.18869, 2024. [7] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z. Shou, Show-o: One single transformer to unify multimodal understanding and generation, in The Thirteenth International Conference on Learning Representations, 2025. [8] S. Kou, J. Jin, Z. Liu, C. Liu, Y. Ma, J. Jia, Q. Chen, P. Jiang, and Z. Deng, Orthus: Autoregressive interleaved image-text generation with modality-specific heads, in Forty-second International Conference on Machine Learning, 2025. [9] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in Neural Information Processing Systems, vol. 36, pp. 3489234916, 2023. [10] D. Chen, J. Liu, W. Dai, and B. Wang, Visual instruction tuning with polite flamingo, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, pp. 1774517753, 2024. [11] J. He, H. Guo, M. Tang, and J. Wang, Continual instruction tuning for large multimodal models, arXiv preprint arXiv:2311.16206, 2023. [12] L. Wang, X. Zhang, H. Su, and J. Zhu, comprehensive survey of continual learning: Theory, method and application, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [13] D. Yu, X. Zhang, Y. Chen, A. Liu, Y. Zhang, P. S. Yu, and I. King, Recent advances of multimodal continual learning: comprehensive survey, arXiv preprint arXiv:2410.05352, 2024. [14] H. Guo, F. Zeng, Z. Xiang, F. Zhu, D.-H. Wang, X.-Y. Zhang, and C.-L. Liu, HiDe-LLaVA: Hierarchical decoupling for continual instruction tuning of multimodal large language model, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), (Vienna, Austria), pp. 1357213586, Association for Computational Linguistics, July 2025. [15] A. Maharana, J. Yoon, T. Chen, and M. Bansal, Adapt-$infty$: Scalable continual multimodal instruction tuning via dynamic data selection, in The Thirteenth International Conference on Learning Representations, 2025. [16] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y. Lee, X. Ren, G. Su, V. Perot, J. Dy, et al., Dualprompt: Complementary prompting for rehearsal-free continual learning, in European Conference on Computer Vision, pp. 631648, Springer, 2022. [17] D. Zhu, Z. Sun, Z. Li, T. Shen, K. Yan, S. Ding, C. Wu, and K. Kuang, Model tailor: Mitigating catastrophic forgetting in multi-modal large language models, in International Conference on Machine Learning, pp. 6258162598, PMLR, 2024. [18] T. Huai, J. Zhou, X. Wu, Q. Chen, Q. Bai, Z. Zhou, and L. He, Cl-moe: Enhancing multimodal large language model with dual momentum mixture-of-experts for continual visual question answering, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1960819617, 2025. [19] Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang, Emu: Generative pretraining in multimodality, in The Twelfth International Conference on Learning Representations, 2024. 11 [20] Q. Sun, Y. Cui, X. Zhang, F. Zhang, Q. Yu, Y. Wang, Y. Rao, J. Liu, T. Huang, and X. Wang, Generative multimodal models are in-context learners, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. [21] S. Tong, D. Fan, J. Zhu, Y. Xiong, X. Chen, K. Sinha, M. Rabbat, Y. LeCun, S. Xie, and Z. Liu, Metamorph: Multimodal understanding and generation via instruction tuning, arXiv preprint arXiv:2412.14164, 2024. [22] C. Wang, G. Lu, J. Yang, R. Huang, J. Han, L. Hou, W. Zhang, and H. Xu, Illume: Illuminating your llms to see, draw, and self-enhance, arXiv preprint arXiv:2412.06673, 2024. [23] D. Ghosh, H. Hajishirzi, and L. Schmidt, Geneval: An object-focused framework for evaluating textto-image alignment, Advances in Neural Information Processing Systems, vol. 36, pp. 5213252152, 2023. [24] H. Liu, W. Yan, M. Zaharia, and P. Abbeel, World model on million-length video and language with blockwise ringattention, in The Thirteenth International Conference on Learning Representations, 2025. [25] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, S. Han, and Y. Lu, VILA-u: unified foundation model integrating visual understanding and generation, in The Thirteenth International Conference on Learning Representations, 2025. [26] H. Tang, H. Liu, and X. Xiao, Ugen: Unified autoregressive multimodal model with progressive vocabulary learning, arXiv preprint arXiv:2503.21193, 2025. [27] Y. Jin, K. Xu, K. Xu, L. Chen, C. Liao, J. Tan, Q. Huang, B. Chen, C. Song, dai meng, D. Zhang, W. Ou, K. Gai, and Y. Mu, Unified language-vision pretraining in LLM with dynamic discrete visual tokenization, in The Twelfth International Conference on Learning Representations, 2024. [28] P. Esser, R. Rombach, and B. Ommer, Taming transformers for high-resolution image synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1287312883, June 2021. [29] L. Qu, H. Zhang, Y. Liu, X. Wang, Y. Jiang, Y. Gao, H. Ye, D. K. Du, Z. Yuan, and X. Wu, Tokenflow: Unified image tokenizer for multimodal understanding and generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 25452555, 2025. [30] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan, Janus-pro: Unified multimodal understanding and generation with data and model scaling, arXiv preprint arXiv:2501.17811, 2025. [31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable visual models from natural language supervision, in International Conference on Machine Learning, pp. 87488763, PmLR, 2021. [32] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023. [33] A. Van Den Oord, O. Vinyals, et al., Neural discrete representation learning, Advances in Neural Information Processing Systems, vol. 30, 2017. [34] R. Huang, C. Wang, J. Yang, G. Lu, Y. Yuan, J. Han, L. Hou, W. Zhang, L. Hong, H. Zhao, et al., Illume+: Illuminating unified mllm with dual visual tokenization and diffusion refinement, arXiv preprint arXiv:2504.01934, 2025. [35] B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby, Multimodal contrastive learning with limoe: the language-image mixture of experts, Advances in Neural Information Processing Systems, vol. 35, pp. 95649576, 2022. [36] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, in International Conference on Learning Representations, 2017. [37] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby, Scaling vision with sparse mixture of experts, in Advances in Neural Information Processing Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 8583 8595, Curran Associates, Inc., 2021. [38] S. Shen, Z. Yao, C. Li, T. Darrell, K. Keutzer, and Y. He, Scaling vision-language models with sparse mixture of experts, in The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 12 [39] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al., Lora: Low-rank adaptation of large language models., ICLR, vol. 1, no. 2, p. 3, 2022. [40] Z. Chen, Z. Wang, Z. Wang, H. Liu, Z. Yin, S. Liu, L. Sheng, W. Ouyang, and J. Shao, Octavius: Mitigating task interference in MLLMs via loRA-moe, in The Twelfth International Conference on Learning Representations, 2024. [41] X. Wu, S. Huang, and F. Wei, Mixture of loRA experts, in The Twelfth International Conference on Learning Representations, 2024. [42] S. Yang, M. A. Ali, C.-L. Wang, L. Hu, and D. Wang, Moral: Moe augmented lora for llms lifelong learning, arXiv preprint arXiv:2402.11260, 2024. [43] D. Li, Y. Ma, N. Wang, Z. Ye, Z. Cheng, Y. Tang, Y. Zhang, L. Duan, J. Zuo, C. Yang, et al., Mixlora: Enhancing large language models fine-tuning with lora-based mixture of experts, arXiv preprint arXiv:2404.15159, 2024. [44] T. Luo, J. Lei, F. Lei, W. Liu, S. He, J. Zhao, and K. Liu, Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models, arXiv preprint arXiv:2402.12851, 2024. [45] J. Yu, Y. Zhuge, L. Zhang, P. Hu, D. Wang, H. Lu, and Y. He, Boosting continual learning of visionlanguage models via mixture-of-experts adapters, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2321923230, 2024. [46] H. Zhao, Z. Wang, Q. Sun, K. Song, Y. Li, X. Hu, Q. Guo, and S. Liu, Llava-cmoe: Towards continual mixture of experts for large vision-language models, arXiv preprint arXiv:2503.21227, 2025. [47] R. Wang and P. Li, LEMoE: Advanced mixture of experts adaptor for lifelong model editing of large language models, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, (Miami, Florida, USA), pp. 25512575, Association for Computational Linguistics, Nov. 2024. [48] W. Chen, Y. Zhou, N. Du, Y. Huang, J. Laudon, Z. Chen, and C. Cui, Lifelong language pretraining with distribution-specialized experts, in International Conference on Machine Learning, pp. 53835395, PMLR, 2023. [49] W. Pian, S. Deng, S. Mo, Y. Guo, and Y. Tian, Modality-inconsistent continual learning of multimodal large language models, arXiv preprint arXiv:2412.13050, 2024. [50] J. Qiao, Z. Zhang, X. Tan, Y. Qu, S. Ding, and Y. Xie, Llaca: Multimodal large language continual assistant, arXiv preprint arXiv:2410.10868, 2024. [51] J. Zheng, Q. Ma, Z. Liu, B. Wu, and H. Feng, Beyond anti-forgetting: Multimodal continual instruction tuning with positive forward transfer, arXiv preprint arXiv:2401.09181, 2024. [52] M. Cao, Y. Liu, Y. Liu, T. Wang, J. Dong, H. Ding, X. Zhang, I. Reid, and X. Liang, Continual llava: Continual instruction tuning in large vision-language models, arXiv preprint arXiv:2411.02564, 2024. [53] Y. Zhai, S. Tong, X. Li, M. Cai, Q. Qu, Y. J. Lee, and Y. Ma, Investigating the catastrophic forgetting in multimodal large language models, arXiv preprint arXiv:2309.10313, 2023. [54] F. Zeng, F. Zhu, H. Guo, X.-Y. Zhang, and C.-L. Liu, Modalprompt: Dual-modality guided prompt for continual learning of large multimodal models, arXiv preprint arXiv:2410.05849, 2024. [55] Y.-K. Zhang, S. Lu, Y. Li, Y. Ma, Q.-G. Chen, Z. Xu, W. Luo, K. Zhang, D.-C. Zhan, and H.-J. Ye, Wings: Learning multimodal llms without text-only forgetting, Advances in Neural Information Processing Systems, vol. 37, pp. 3182831853, 2024. [56] J. Zou, B. Liao, Q. Zhang, W. Liu, and X. Wang, Omnimamba: Efficient and unified multimodal understanding and generation via state space models, arXiv preprint arXiv:2503.08686, 2025. [57] C. Deng, D. Zhu, K. Li, C. Gou, F. Li, Z. Wang, S. Zhong, W. Yu, X. Nie, Z. Song, G. Shi, and H. Fan, Emerging properties in unified multimodal pretraining, arXiv preprint arXiv:2505.14683, 2025. [58] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, in The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [59] D. A. Hudson and C. D. Manning, Gqa: new dataset for real-world visual reasoning and compositional question answering, Conference on Computer Vision and Pattern Recognition, 2019. 13 [60] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255, Ieee, 2009. [61] Z. Xu, M. Liu, Y. Shen, J. Rimchala, J. Zhang, Q. Wang, Y. Cheng, and L. Huang, Modality-specialized synergizers for interleaved vision-language generalists, in The Thirteenth International Conference on Learning Representations, 2025. [62] Z. Ni, L. Wei, S. Tang, Y. Zhuang, and Q. Tian, Continual vision-language representation learning with off-diagonal information, in International Conference on Machine Learning, pp. 2612926149, PMLR, 2023. [63] C. Chen, J. Zhu, X. Luo, H. Shen, J. Song, and L. Gao, Coin: benchmark of continual instruction tuning for multimodel large language models, Advances in Neural Information Processing Systems, vol. 37, pp. 5781757840, 2024. [64] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al., Laion-5b: An open large-scale dataset for training next generation image-text models, Advances in neural information processing systems, vol. 35, pp. 2527825294, 2022. [65] J. M. Joyce, Kullback-Leibler Divergence, pp. 720722. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011. [66] A. Singh, V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach, Towards vqa models that can read, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 83178326, 2019. [67] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, Vizwiz grand challenge: Answering visual questions from blind people, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 36083617, 2018. [68] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu, Multi-concept customization of text-toimage diffusion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19311941, 2023. [69] J. S. Smith, L. Karlinsky, V. Gutta, P. Cascante-Bonilla, D. Kim, A. Arbelle, R. Panda, R. Feris, and Z. Kira, Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1190911919, June 2023. [70] J. Qiao, X. Tan, C. Chen, Y. Qu, Y. Peng, Y. Xie, et al., Prompt gradient projection for continual learning, in The Twelfth International Conference on Learning Representations, 2023. [71] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two time-scale update rule converge to local nash equilibrium, Advances in Neural Information Processing Systems, vol. 30, 2017. [72] E. Chern, J. Su, Y. Ma, and P. Liu, Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation, arXiv preprint arXiv:2407.06135, 2024. [73] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, ReferItGame: Referring to objects in photographs of natural scenes, in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (A. Moschitti, B. Pang, and W. Daelemans, eds.), (Doha, Qatar), pp. 787798, Association for Computational Linguistics, Oct. 2014. [74] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, Generation and comprehension of unambiguous object descriptions, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1120, 2016."
        },
        {
            "title": "A Theoretical Analysis",
            "content": "We formally compare the one-step visual loss drift between modality-coupled architecture (shared parameters for text and image) and our modality-decoupled MoDE design. Let assume the following parameters: θ the full set of trainable parameters, Lt(θ) the text-generation loss, Lv(θ) the image-generation loss, η the learning rate, gt = θLt and gv = θLv the corresponding gradients of text and visual modalities, respectively, Hv = 2 θLv the Hessian of the visual loss. After gradient step θ θ ηgt, the change in the visual loss is, by second-order Taylor expansion: Lv = Lv(θ ηgt) Lv(θ) = η gt, gv + η2 2 Hvgt + o(η2) (13) where o(η2) denotes higher-order terms that vanish faster than η2. Modality-coupled architecture (shared parameters). In the modality-coupled case (e.g., shared MoE-LoRA adapters between modalities), both gt and gv are nonzero over the same parameter space. Thus, the first-order term ηgt, gv is generally nonzero. In the worst case (maximum interference), gt and gv are anti-aligned, so by CauchySchwarz inequality we have: Lcoupled η gt gv + η2 2 λmax(Hv)gt2 + o(η2) (14) Thus, the dominant error scales as O(η). Modality-decoupled architecture (MoDE). parameter blocks ϕ and ψ, respectively; thus: In MoDE, the text and visual tokens update disjoint which implies Lv ϕ = 0, Lt ψ = 0, gt, gv = 0 (orthogonal supports) Therefore, the first-order term in (13) vanishes, and the remaining visual loss drift is purely secondorder: LMoDE η2 2 λmax(Hv)gt2 + o(η2) (15) Thus, the dominant error scales as O(η2). Conclusion. Comparing(14) and(15), we conclude that LMoDE Lcoupled since η2 η for small η. Thus, MoDE provably limits the worst-case drift in image-generation loss to O(η2), compared to O(η) in modality-coupled architecture."
        },
        {
            "title": "B Modality gradient conflict",
            "content": "The theoretical support is provided in Appendix A. This section provides empirical evidence (Figure 5, 6) for our claim. As shown in Figure 5, the modality-coupled MoE LoRA [44] displays significant gradient conflict between text and image modalities, empirically validating our analysis in Section 3. Figure 5: Cosine distance distribution between text and image modality gradients in modality-coupled MoE LoRA [44] on the ScienceQA [58] dataset. The y-axis shows the proportion of parameters corresponding to each cosine distance. Lower cosine distance values indicate greater gradient conflict, with 0 denoting orthogonal update directions. In contrast to the significant conflict shown in Figure 5, Figure 6 clearly shows that MoDE has zero cosine disagreement between modalities. This confirms that its modality-specific update subspaces are completely isolated, both theoretically and empirically. Figure 6: Cosine distance distribution between text and image modality gradients in MoDE. Lower cosince distance values indicate greater gradient conflict, with 0 denoting orthogonal directions. In MoDE, all parameters exhibit perfectly orthogonal gradients, confirming the absence of modality gradient conflict."
        },
        {
            "title": "C Additional Qualitative Results",
            "content": "Table 4 presents the complete qualitative results of MoDE, built on Chameleon [3], in comparison to various baseline methods. Our approach generates images that better align with the prompts and exhibit higher visual fidelity. For example, given the prompt Barn in the fall season with leaves all around, only the original Chameleon and our MoDE successfully generate images featuring yellow leaves around the barn. Table 4: Full qualitative results of our MoDE compared to other baseline methods. Method dog wearing sunglasses on the porch transparent cup filled with steaming hot cocoa Barn in the fall season with leaves all around Marigold flowers in the vase Chameleon [3] Seq LoRA Model Tailor [17] DualPrompt [16] MoELoRA [44] CL-MoE [18] MoDE (Ours)"
        },
        {
            "title": "D Implementation Details",
            "content": "We use Chameleon [3] and Janus-Pro [30] as backbone models. For Chameleon, we adopt the implementation from [72] due to the unavailability of the original checkpoints and code. All LoRA modules are configured with default rank of 8. Training is conducted for one epoch per task on 8 NVIDIA RTX 6000 GPUs, with bf16 mixed precision and per-GPU batch size of 1 (total effective batch size of 8). We use learning rate of 1 104, scheduled via cosine decay with warm-up ratio of 0.1. The softening factor β in knowledge distillation equation (10) is 2.0. The hyperparameter λ is set to 0.3, selected based on sweep over multiple values (see Table 5). This value strikes balance between image generation and visual understanding: higher λ favors generation quality, while lower λ improves understanding performance. higher λ increases the weight of the knowledge distillation loss in the V-adapter (see Eq. 12), constraining the update closer to the pre-trained model to preserve image generation capabilities. However, this also constrains beneficial updates needed for improved visual understanding. Table 5: Performance of MoDE under different values of the loss balancing coefficient λ. We select λ = 0.3 (bolded) as it yields the best balance between image generation fidelity and visual understanding accuracy. Image Generation Visual Understanding Text alignment () Image alignment () FID () Accuracy () Forgetting () 0.2364 0.2458 0.2498 0.2543 0.5156 0.5170 0.5264 0.5190 54.61 53.74 53.12 51.72 33.07 33.47 31.53 32. 26.49 25.99 28.38 27.62 λ 0.0 0.3 0.5 1."
        },
        {
            "title": "E Additional Ablation Results",
            "content": "E.1 Different Tasks Order Following [63], we explore the impact of different tasks order by conducting an additional experiment using different order of tasks, arranged alphabetically: GQA [59], ImageNet [60], ScienceQA [58], TextVQA [66], and VizWiz [67]. Table 6: Results of inter-modal (image generation) and intra-modal (multimodal understanding) catastrophic forgetting on different task orderings. For image generation, Zero-shot reports the pre-trained reference we aim to preserve. Bold shows the best results, underline shows the second best results. Chameleon [3] is used for the backbone. Method Zero-shot Seq LoRA Model tailor [17] DualPrompt [16] MoELoRA [63] CL-MoE [18] MoDE (Ours) Image Generation Multimodal Understanding Text alignment () Image alignment () FID () Accuracy () Forgetting () 0.2592 0.2307 0.2448 0.2491 0.2492 0.2463 0.2525 0.5205 0.4973 0.5166 0.5140 0.5125 0.5090 0.5256 52.13 66.75 53.58 53.07 61.64 59.89 51. 22.48 27.72 34.02 32.05 34.41 34.18 36.68 - 33.10 24.51 7.96 24.72 25.63 22.92 As shown in Table 6, our MoDE consistently outperforms the baseline methods, thus demonstrating its robustness to different task orderings. E.2 Different Number of Experts This this section we provide an ablation study on the number of experts in the T-MoE module. 18 Table 7: Performance of MoDE under different number of experts in T-MoE. Image Generation Visual Understanding # Experts Text alignment () Image alignment () FID () Accuracy () Forgetting () 2 4 6 0.2437 0.2458 0.2499 0.5144 0.5170 0. 53.61 53.74 52.69 32.24 33.47 34.90 26.81 25.99 24.12 As shown in Table 7, increasing the number of experts yields slightly improved text alignment and accuracy, though the gains are modest. This suggests that the performance of MoDE is relatively robust to the number of experts, and 4 experts offer good trade-off between performance and computational cost. E.3 Ablation on Knowledge Distillation and Modality Decoupling We provide the results of adding our cross-modal knowledge distillation (KD) loss to modalitycoupled baselines (SeqLoRA [39], CL-MoE [18], MoELoRA [63]) in Table 8. Although applying KD to vanilla LoRA leads to modest improvements both in image generation quality and multimodal understanding performance, we observe that for MoELoRA and CL-MoE this mitigates visual-generation forgetting at the cost of hurting the multimodal understanding performance. This shows that without explicit modality decoupling, KD locks in the teachers features in way that hinders the models ability to adapt to new multimodal understanding tasks. The results come from single run. Table 8: Effect of adding KD to modality-coupled baselines. Image Generation Visual Understanding Text alignment () Image alignment () FID () Accuracy () Forgetting () Seq LoRA [39] SeqLoRA w/ KD MoELoRA [44] MoELoRA w/ KD CL-MoE [18] CL-MoE w/ KD 0.2162 0.2478 0.2248 0. 0.2081 0.2439 0.5150 0.5180 0.5095 0.5176 0.5150 0.5200 56.12 52.83 65.16 53. 65.87 52.43 28.43 30.81 33.01 30.07 32.86 31.82 35.33 29.85 30.77 35. 30.95 34.74 As shown in Table 9, decoupling textual and visual LoRA without knowledge distillation yields only modest gains in understanding accuracy and visual generation quality compared to vanilla LoRA. This indicates that decoupling alone is insufficient to maintain the rich cross-modal alignment necessary for high-quality generation. Furthermore, Table 3 includes an ablation on MoDE w/o KD (i.e., decoupled T-MoE and V-adapter without knowledge distillation), which further confirms that knowledge distillation plays critical role in the performance improvements observed. Table 9: Ablation on modality-decoupled architecture. Image Generation Visual Understanding Text alignment () Image alignment () FID () Accuracy () Forgetting () Seq LoRA Decoupled LoRA 0.2162 0.2252 0.5150 0. 56.12 54.74 28.43 30.59 35.33 32.79 The ablation results show that MoDE is more than the sum of its parts: only the combination of modality-decoupling plus knowledge distillation leads to consistently superior performance."
        },
        {
            "title": "F Multimodal Understanding Forgetting",
            "content": "In this section, we provide the results of MoDE mitigating catastrophic forgetting of the understanding capabilities induced by fine-tuning UMGMs on image generation tasks. To address this, we conduct new experiments by fine-tuning the linear layers of the Chameleon model [3] on image generation tasks. Specifically, we use the LAION-5B dataset [64] for fine-tuning and apply knowledge distillation using the multimodal understanding sequence from the GQA dataset [59] as reference data. We compare our MoDE method with LoRA fine-tuning across three multimodal understanding benchmarks. Accuracies are reported in Table 10. The difference between zero-shot and fine-tuned performance reflects the degree of forgetting in multimodal understanding. Table 10: Forgetting caused by fine-tuning on image generation tasks. Multimodal Understanding Accuracy (%)"
        },
        {
            "title": "Method",
            "content": "ScienceQA [58] TextVQA [66] ImageNet [60] Zero-shot LoRA [39] MoDE (Ours) 51.52 45.81 51.28 23.49 12.25 23.44 16.53 0.00 16. The results demonstrate that fine-tuning on image generation tasks can indeed cause catastrophic forgetting in multimodal understanding. This is evident because of the substantial drop in performance for the LoRA baseline compared to zero-shot. Our MoDE method remains effective in this scenario, significantly reducing the performance drop and mitigating forgetting relative to the LoRA baseline. This demonstrates that MoDE generalizes beyond the specific forgetting direction explored in the main paper, where understanding tasks overwrite prior image generation capabilities, and is effective regardless of the fine-tuning direction."
        },
        {
            "title": "G Additional Continual Learning Results",
            "content": "G.1 Additional Results with Janus-Pro We provide additional quantitative comparisons using Janus-Pro-1B [30] as the backbone. We further include the Grounding task with referring expression grounding datasets (RefCOCO [73], RefCOCO+ [74], and RefCOCOg [74]). All methods are trained with early stopping. As shown in Table 11, our MoDE outperforms the baseline methods in continual instruction tuning for multimodal understanding. Table 11: Results of continual instruction tuning with Janus-Pro-1B [30] as the backbone. Accuracy (ACC) is exact-match (higher is better); Fgt is average forgetting (lower is better). For each method, the upper row shows the best accuracy during continual instruction tuning, and the lower row shows the final accuracy after completing all tasks. Upper bound reports performance obtained by individually fine-tuning model on each task. Bold shows the best results. Method Zero-shot Seq LoRA Model Tailor [17] CL-MoE [18] MoDE (Ours) Upper bound ScienceQA TextVQA ImageNet GQA VizWiz Grounding ACC () Fgt () Datasets Metrics 82.69 61.27 44.87 61.58 38.62 74.17 44. 81.01 65.45 86.28 40.22 51.34 32.01 47.36 35.76 50.24 42. 46.34 39.10 51.34 13.11 96.89 12.42 92.79 21.18 93.21 34. 95.16 52.32 98.72 20 38.46 58.21 47.25 51.48 32. 56.86 55.04 62.01 58.20 62.55 26.74 56.33 47.71 44.87 9. 55.52 43.92 61.02 58.07 67.23 0.00 41.65 41.65 36.72 36. 38.92 38.92 44.99 44.99 46.13 35.54 - 37. 27.37 29.13 32.01 43.18 21.96 53. 14.48 68.71 - G.2 Additional Results with Chameleon Following the CoIN benchmark [63], we present results on additional datasets using Chameleon [3] as the backbone. We further include the Grounding task with referring expression grounding datasets (RefCOCO [73], RefCOCO+ [74], and RefCOCOg [74]). All methods are trained with early stopping. As shown in Table 12, our MoDE outperforms the baseline methods in continual instruction tuning for multimodal understanding. Table 12: Results on sequence of six datasets. Accuracy (ACC) is exact-match (higher is better); Fgt is average forgetting (lower is better). For each method, the upper row shows the best accuracy during continual instruction tuning, and the lower row shows the final accuracy after completing all tasks. Upper bound reports performance obtained by individually fine-tuning model on each task. Bold shows the best results. Method Zero-shot Seq LoRA Model tailor [17] DualPrompt [16] MoELoRA [44] CL-MoE [18] MoDE (Ours) Upper bound ScienceQA TextVQA ImageNet GQA VizWiz Grounding ACC () Fgt () Datasets Metrics 51.52 72.43 38.67 74.90 56.06 60.01 52.45 71.79 65. 71.35 67.32 71.49 67.87 73.27 23.49 39.34 33.18 40.04 25. 31.48 24.12 39.62 22.36 38.82 32.82 39.00 33.36 40.74 16. 89.41 15.09 78.22 7.10 24.55 3.82 94.75 6.71 90.08 9.12 85.88 9. 91.88 14.22 37.93 34.99 37.35 32.62 29.91 31.44 37.66 30. 37.37 33.29 36.73 33.60 36.56 6.64 44.38 39.34 43.25 37. 40.91 27.16 44.33 42.55 43.73 43.20 44.45 41.91 44.15 0. 18.62 18.62 15.66 15.66 13.82 13.82 37.13 37.13 36.22 36.22 36.64 36. 44.90 18.73 - 29.82 24.64 29. 22.97 25.47 9.57 34.14 24.08 36. 19.12 37.05 18.25 55.25 -"
        },
        {
            "title": "H Computational Analysis",
            "content": "In this section, we report comparisons of training compute, memory footprint, and trainable parameter size. Table 13: Comparison of trainable parameters and training costs. Results come from single run. Method TFLOPs Peak GPU Memory (MB) Training Time (ms) Trainable Params (%) MoELoRA [63] CL-MoE [18] MoDE (Ours) 4.29 4.29 4.19 75265 86300 84980 419.2 421.7 440.3 0.0171 0.0171 0.0211 As shown in Table 13, MoDE introduces only slight increase in training time (440.3 ms vs. 419.2 ms) and peak GPU memory (84.98 GB vs. 75.27 GB) compared to MoELoRA [63], while maintaining comparable TFLOPs. The trainable parameter ratio of MoDE is only 0.0211%, modestly higher than MoELoRA [63] and CL-MoE [18] (0.0171%), indicating that our performance gains are not simply due to increased processing capacity. These results suggest that MoDEs improvements arise from its design (modality-decoupled experts and distillation), rather than significantly higher resource usage."
        },
        {
            "title": "I Baseline Settings",
            "content": "In this section, we provide the experimental settings for the baseline methods used in our experiments. 21 I.1 Overview of Baselines Model Tailor [17] mitigates catastrophic forgetting in MLLMs by (i) identifying sparse model patch via fused salience-and-sensitivity score, and (ii) applying Hessian-based weight compensation so the patch can be safely merged back into the frozen backbone. DualPrompt [16] is replay-free continual learning method grafting two tiny prompt sets onto frozen transformer backbone: shared G-Prompt that encodes task-invariant knowledge and per-task E-Prompts keyed to input features for task-specific skills. MoELoRA [44] extends LoRA by substituting each single low-rank adapter with mixture of experts. The LoRA update is rewritten as: = + α (cid:88) i=1 GiBiAix, where every expert has rank r/N and is gated by Gi, so the total trainable parameters remain identical to vanilla LoRA while the separate experts capture more diverse, task-specific representations. CL-MoE [18] tackles continual VQA on top of an MLLM by (i) introducing Dual-Router MoE: task-level router picks global experts while an instance-level router refines the choice per example, and (ii) applying dynamic momentum update that blends new LoRA-based expert weights with the frozen past experts, so the model absorbs new skills yet preserves prior knowledge, sharply reducing catastrophic forgetting. Seq LoRA is vanilla baseline that applies LoRA adapters in plain sequential fine-tuning across tasks. I.2 Training Settings To ensure fair comparison, we follow the hyperparameters and implementation details from each baselines original codebase: Model Tailor [17]: sparsity level set to 10% (percentage of parameters replaced). DualPrompt [16]: prompt lengths Lg = 5 and Le = 20; prompts are injected at layers startg = 1, endg = 2, starte = 3, and ende = 5. MoE-LoRA [44]: number of experts set to 4, with each expert having LoRA rank 8. CL-MoE [18]: number of experts set to 4, with each expert having LoRA rank 8. Top-K experts selected with = 2. Hyperparameters γ = 0.7, β = 0.5."
        },
        {
            "title": "J Ethics Statement",
            "content": "This work does not involve human subjects, personally identifiable information, or proprietary data. All datasets used (such as ScienceQA, TextVQA, GQA, VizWiz, and ImageNet) are publicly available and commonly used in the research community. We follow the licensing and usage policies associated with each dataset. Our continual learning method, MoDE, improves the robustness of unified multimodal generative models without modifying or exploiting sensitive data. As with all powerful generative models capable of producing synthetic images from text, our method carries potential risk of misuse, such as generating disinformation or harmful visual content. Although MoDE focuses on mitigating forgetting in multimodal learning, enhanced generation capabilities may inadvertently facilitate deceptive or malicious use. We recommend that practitioners deploying such models consider safeguards such as content filtering, provenance tracking, and visible or invisible watermarking of generated content to support responsible use and attribution."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: You should answer [Yes] , [No] , or [NA] . [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. Please provide short (12 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: Delete this instruction block, but keep the section heading NeurIPS Paper Checklist\", Keep the checklist subsection headings, questions/answers and guidelines below. Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The main contributions of this paper is identifying and tackling the challenge of inter-modal catastrophic forgetting in unified multimodal models, which are accurately reflected in the abstract and introduction. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 23 Justification: Our method is applicable to autogressive transformers, which is made clear in the paper. Our experiments are conducted on autoregressive transformers as well. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: We provide the theoretical results on modality gradient conflict in the main paper. The proofs and empirical verification are in the appendix and B, respectively. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our provided details of our architecture as well as experimental details in the paper. And our code would be released upon paper acceptance. 24 Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The model checkpoints and datasets we use in this paper are all open-sourced. And we will provide our code together with the other supplemental materials. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. 25 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We include these details in the supplemental materials. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not applicable to our experiments involving generative models, where evaluation is primarily qualitative or based on metrics not requiring statistical significance testing. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The compute resources information our experiments need is provided in the experiment section of the paper as well as in the supplemental materials. 26 Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: There is no ethical concerns awared in this paper. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The potential societal impact is discussed in the appendix. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? 27 Answer: [NA] Justification: We dont release any pre-trained models or datasets. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The models and datasets we use in the paper have been propoerly cited. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code of this paper is well documented. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects 28 Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: The core method development in this paper does not involve LLMs as any important, original, or non-standard components. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, The University of Texas at Austin"
    ]
}