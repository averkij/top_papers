{
    "paper_title": "UniVBench: Towards Unified Evaluation for Video Foundation Models",
    "authors": [
        "Jianhui Wei",
        "Xiaotian Zhang",
        "Yichen Li",
        "Yuan Wang",
        "Yan Zhang",
        "Ziyi Chen",
        "Zhihang Tang",
        "Wei Xu",
        "Zuozhu Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video foundation models aim to integrate video understanding, generation, editing, and instruction following within a single framework, making them a central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target a single task, rely on task-specific metrics, and typically use short or simple video clips. As a result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, a benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and a newly proposed task, video reconstruction, which assesses how faithfully a model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop a unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 5 3 8 1 2 . 2 0 6 2 : r UniVBench: Towards Unified Evaluation for Video Foundation Models Jianhui Wei1,2*, Xiaotian Zhang1,2*, Yichen Li1,2*, Yuan Wang1* Yan Zhang2, Ziyi Chen2, Zhihang Tang3, Wei Xu2, Zuozhu Liu1 1Zhejiang University 2ByteDance 3Zhejiang Lab {jianhui1.24,zuozhuliu}@intl.zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Video foundation models aim to integrate video understanding, generation, editing, and instruction following within single framework, making them central direction for next-generation multimodal systems. However, existing evaluation benchmarks remain fragmented and limited in scope, as they each target single task, rely on task-specific metrics, and typically use short or simple video clips. As result, they do not capture the unified capabilities that these models are designed to deliver. To address this gap, we introduce UniVBench, benchmark purpose-built for evaluating video foundation models across four core abilities: video understanding, video generation, video editing, and newly proposed task, video reconstruction, which assesses how faithfully model can reproduce video content it has encountered. Our benchmark substantially expands the complexity of evaluation by incorporating 200 high-quality, diverse and multi-shot videos, each paired with detailed captions, multi-format editing instructions, and reference images. All videos are human-created and carefully validated, offering richer cinematic information than prior benchmarks. In addition, we develop unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and scoring across all tasks, enabling fair, scalable, and reproducible comparisons of unified video models. By grounding evaluation in instruction-based multi-shot video tasks, UniVBench provides the first framework for measuring the integrated capabilities that video foundation models aim to achieve. Extensive human annotations ensure our evaluation aligns with human judgment, enabling rigorous assessment and accelerating progress toward robust video intelligence. Code and datasets are available at https://github.com/ JianhuiWei7/UniVBench *Equal contributions. Corresponding author. Video foundation models have recently emerged as promising direction for next-generation multimodal systems. These models aim to integrate understanding and generation within single architecture. However, current approaches remain fundamentally separated. Generationfocused systems [3, 14, 32, 35, 50, 57, 69] excel at synthesis but cannot reason about video content, while understanding models [1, 2, 12, 16, 19, 20, 25, 68, 76] achieve strong perception but cannot generate videos. Emerging unified architectures [5, 9, 28, 4345, 55, 56, 61, 62, 66, 74] attempt to bridge this divide by integrating LLMs with visual tokenizers and video decoders, enabling both video understanding and generation in response to instructions. Despite these architectural advances, critical question remains: does unification actually improve performance across the full spectrum of video tasks? Current benchmarks cannot answer this due to two fundamental limitations. First, existing datasets are task-specific and cannot support unified evaluation. As shown in Table 1, most video understanding benchmarks [4, 29, 54, 75] use copyrighted web videos that may contaminate evaluation data and lack the instructions needed for generation and editing tasks. Video generation benchmarks [7, 18, 22, 24, 52, 71 73] focus on text-to-video synthesis without supporting understanding or editing evaluation. Video editing benchmarks [13, 21, 27, 39, 58, 67] remain limited to single-shot scenarios, lacking the multi-shot content. Beyond task coverage, existing benchmarks also exhibit fragmented evaluation of cinematic qualities. As shown in Table 2, understanding benchmarks like AuroraCap [4] emphasize subject detection and camera motion but ignore style and spatial relationships; generation benchmarks like VBench [18] evaluate subjects and actions but lack systematic lighting and color assessment; editing benchmarks like TGVE [58] focus on subject preservation but omit background and spatial coherence. No existing benchmark systematically evaluates cinematic dimensions across all video tasks. Second, evaluation metrics are fundamentally fragFigure 1. Overview of the UniVBench evaluation setting across 8 Dimensions, 21 Sub-Dimensions, and 6 Tasks. Given source video, T2V synthesizes video using its ground-truth caption, while V2V reconstructs the video based solely on the models self-generated understanding text, enabling direct diagnosis of perceptiongeneration coupling. UniVBench supports six unified tasksvideo captioning (V2T), text-to-video generation (T2V), reference-image video generation (R2V), text-instruction video editing (TV2V), reference-image video editing (RV2V), and video reconstruction (V2V). mented. As shown in Table 3, understanding uses referencebased measures, generation uses distributional metrics that often disagree, and editing requires ad hoc metric combinations [11, 59, 70]. This fragmentation limits cross-task comparison. Moreover, single scalar score severely limits interpretability, obscuring nuanced trade-offs between models strengths and weaknesses, and consequently failing to provide actionable feedback to the training phase. Highquality evaluation is inherently complex, multifaceted, and dynamic. Fixed evaluation criteria may not generalize across diverse video properties: some videos emphasize faithful reconstruction of visual entities, while others prioritize narrative coherence over instance-level fidelity. We introduce UniVBench, the first unified benchmark designed to evaluate video foundation models across their full capability spectrum. The benchmark comprises 200 high-quality, multi-shot videos, each with rich annotations including detailed captions, multi-format editing instructions, and reference images. Crucially, all content is humancreated and copyright-free, enabling fair evaluation of editing, reconstruction, and instruction-following without legal or data contamination concerns. We pair the benchmark with unified agentic evaluation system (UniV-Eval) that standardizes prompting, instruction parsing, and multidimensional scoring across all tasks. This provides consistent, interpretable metrics that enable direct cross-model and cross-task comparison, while supporting fine-grained attribution of errors to perception versus generation components. Our work makes three key contributions: (1) The first multi-shot video dataset specifically designed for unified evaluation, free of copyright and contamination issues; (2) unified agentic evaluation system that enables measurement across understanding, generation, editing, and reconstruction; and (3) principled framework for attributing model capabilities and failures across the perception-generation spectrum. By aligning evaluation with the goals of unified video modeling, our benchmark establishes foundation for measuring progress toward general-purpose, instructionfollowing video intelligence. 2. Related Work 2.1. Video Foundation Models Early progress in video foundation models emerged from text-to-video generation, where diffusion-based frameworks such as ModelScopeT2V [51], LAMP [60], CogVideoX [65], Vidu [3] and Wan [50] achieved highfidelity video synthesis, while autoregressive models like Show-1 [69] and Emu2 [41] introduced unified visual tokens for more controllable generation. However, these approaches are inherently one-directional, focusing on syntheTasks : ①: V2T ②: T2V ③: R2V ④: TV2V ⑤: RV2V ⑥: V2V Benchmark Multi-task Multi-shot Copyright Issue Benchmarks for Video Understanding M-VAD [46] MPII-MD [49] MSR-VTT [63] Charades [38] ActivityNet [23] Youcook2 [75] VATEX [54] AuroraCap [4] ShotBench [29] ① ① ① ① ① ① ① ① ① Benchmarks for Video Generation EvalCrafter [31] FETV [30] MQT [7] VBench [18] GenAIBench [24] LGVQ [72] T2VQA-DB [22] AIGVQA-DB [53] VBench2.0 [73] Q-Eval [71] AIGVE-60K [52] ② ② ② ② ② ② ② ② ② ② ② Benchmarks for Video Editing CCEdit [13] TGVE [58] TGVE+ [39] VE-Bench [42] UNIC [67] VACE-Bench [21] FIVE [27] UniVBench ④ ④ ④ ④ ⑤ ③ ④ ⑤ ④ ①⑥ Yes Yes Potential Yes Yes Yes Yes Potential Yes NA NA NA NA NA NA NA NA NA NA NA No No No No Potential Potential Potential No Table 1. Comparison of benchmarks for video understanding, generation and editing. Multi-task shows the applicable tasks of the benchmark. Multi-shot indicates that whether the video source and text annotations have multi-shot content. Copyright Issue indicates whether the video sources in the dataset are editable without copyright issue. sis without true multimodal reasoning. In parallel, video-totext understanding models including VideoLLaMA3 [68], LLaVA-OneVision [25], and VILA2 [12] extended large multimodal encoders to interpret temporal content and perform grounded question answering. Despite strong perception ability, these encoder-based methods remain limited to understanding and cannot generate or reconstruct visual dynamics, leaving clear separation between perceptionand generation-oriented paradigms. To close this divide, unified video foundation models have recently emerged, seeking to integrate understanding, generation, and editing within single architecture. Representative works such as Chameleon [45], Transfusion [74], Show-o [62], Emu3 [55], and UNIC [66] jointly train autoregressive and diffusion objectives to unify decoding across text and visual tokens. Further developments like NExT-GPT [61], Janus-Pro [5], BAGEL [9], Mogao [28], CoDi-2 [44], Omni-Video [43] and UniVideo [56] incorporate large language models with 3D visual tokenizers and causal VAEs to achieve bidirectional reasoning over text, image, and video modalities. Benchmarks Style Subject Action Backg. Lighting Color Spatial Relationship Camera AuroraCap VGenEval ShotBench FETV VBench VBench2.0 Charades YouCook MPII-MD EvalCrafter TGVE TGVE UNIC FiVE VE-Bench CC-Edit UniVBench Table 2. Comparison of cinematic dimensions across different video evaluation benchmarks. 2.2. Video Benchmark Early video understanding benchmarks like M-VAD [46] and MPII-MD [49] focused on single-shot video captioning with limited scale. Larger benchmarks such as MSRVTT [63] and ActivityNet [23] expanded dataset size and introduced multi-shot content, enabling evaluation of temporal reasoning and long-form video understanding. More recent efforts like AuroraCap [4] and ShotBench [29] have improved annotation quality and introduced shot-level analysis. However, these benchmarks primarily use webscraped videos, raising potential copyright and data contamination concerns when evaluating models trained on largescale internet data. With the emergence of text-to-video models, specialized generation benchmarks have been developed. Early works like FETV [30] and MQT [7] introduced basic quality metrics, while VBench [18] established comprehensive evaluation framework with 16 dimensions covering quality, semantics, and temporal consistency. Subsequent benchmarks like GenAIBench [24], LGVQ [72] , T2VQADB [22] expanded evaluation to include subjective quality assessment and diverse generation scenarios. Recent efforts like VBench2.0 [73], Q-Eval [71] , and AIGVE-60K [52] have scaled up evaluation with larger test sets and more refined metrics. However, all existing generation benchmarks focus exclusively on text-to-video synthesis, lacking support for reference-guided generation or editing. Video editing benchmarks has primarily focused on instruction-following capabilities. [13] and TGVE [58] pioneered text-guided editing assessment, measuring both editing accuracy and video quality preservation. TGVE+ [39] and VE-Bench [42] extended evaluation to"
        },
        {
            "title": "CCEdit",
            "content": "Tasks : ①: V2T ②: T2V ③: R2V ④: TV2V ⑤: RV2V ⑥: V2V Fine-grained Multi-shot Multi-dimension Task Evaluation Method BLEU [34] CLIPScore [17] CIDEr [48] FVD [47] CLIPSIM [22] LPIPS [70] LLM-as-a-Judge [26] UniV-Eval ① ② ① ② ② ② ①⑥ ①⑥ Table 3. Comparison of core capabilities across existing evaluation metrics and our proposed agent-based evaluation system. - indicates the metric is not applicable to this dimension. vide qualitative assessments across multiple tasks. While these methods offer flexibility and can handle diverse inputs including editing scenarios, they typically produce single overall scores without multi-dimensional analysis, limiting their diagnostic value for model development. Overall, no existing evaluation method simultaneously provides finegrained analysis, multi-shot support, multi-dimensional scoring, and cross-task applicability. Our agentic evaluation system addresses these limitations by decomposing evaluation into interpretable dimensions, providing shotlevel attribution, and maintaining consistent criteria across all video tasks. more complex editing scenarios with fine-grained metrics. Recent works like UNIC [67] introduced reference imagebased editing, while VACE-Bench [21] attempted to unify multiple editing modalities. FIVE [27] provided adversarial test cases for robust editing evaluation. Despite these advances, existing editing benchmarks are limited to singleshot videos and do not support multi-shot content, which is essential for evaluating real-world video editing capabilities. Overall, current benchmarks suffer from three critical limitations: they are task-specific, restricted to single-shot scenarios, and understanding benchmarks often use copyrighted content that may contaminate evaluation. Our UniVBench addresses all these limitations by providing the first multi-task, multi-shot benchmark with copyright-free content, enabling comprehensive evaluation across the full spectrum of video tasks. 2.3. Video Evaluation Methods Video evaluation has traditionally relied on task-specific metrics that lack the flexibility required for unified assessment. For video understanding, BLEU [34] and CIDEr [48] measure n-gram overlap between generated and reference captions, providing coarse-grained quality scores but failing to capture semantic nuances or fine-grained errors. For video generation, metrics like FVD [47] assess distributional similarity between generated and real videos, while CLIPScore [17] and CLIPSIM [22] measure semantic alignment between videos and text prompts. LPIPS [70] evaluates perceptual similarity for frame-level reconstruction. For video editing, evaluation typically combines multiple metrics, using LPIPS [70] for background preservation, CLIPScore [17] for instruction alignment, and frameby-frame comparisons for temporal consistency. However, these metrics are fundamentally limited: they operate at the video or dataset level without fine-grained error attribution, most cannot handle multi-shot videos, and each is designed for specific task, limiting cross-task comparison. 3. UniVBench 3.1. Dataset Construction Video Synthesis. To ensure comprehensive cinematic coverage, we adopt eight fundamental dimensions from prior works [14, 18, 24, 64] and extend them with 21 finegrained sub-dimensions (Figure 1): style, subject (category, quality, appearance), action, background, camera (focus, shot size, motion, perspective, angle, height, techniques), lighting (direction, brightness, effect), color (hue, contrast, saturation), and spatial relationships(iter-frame/subject layout, camera-subject position). We pre-classify categories for each sub-dimension (e.g., styles: realistic, animation, 2D; camera movements: static, zoom, pan, tracking; lighting: daylight, golden hour, studio). We recruit 15 professional experts with video production backgrounds who receive detailed training on our dimension taxonomy and annotation guidelines. For script writing, annotators sample random category combinations and compose detailed narratives specifying all dimension attributes shot-by-shot. Each multi-shot script must maintain narrative coherence across shots while covering diverse dimension values. Scripts undergo peer review where second annotator verifies dimension coverage and coherence before generation. We generate videos using top commercial APIs (Hailuo, Kling, Veo3) and apply three-stage human-in-the-loop filtering: (1) automated pre-filtering removes watermarks and IP content via vision-language models, (2) three trained reviewers independently verify each videos adherence to script specifications across all eight dimensions, accepting only videos with unanimous agreement, (3) quality specialists inspect for artifacts, unnatural motion, and temporal inconsistencies. Videos failing any stage are regenerated or discarded. On average, each video undergoes 2.3 generation attempts before approval. This rigorous process yields 100 single-shot and 100 multi-shot videos (avg. 3.72 shots). Recent work [18, 26, 52, 73] has explored LLM-as-aJudge approaches that use vision-language models to proDetailed Captioning. We generate dimension-complete ground-truth captions using Gemini 2.5 Pro through: (1) Figure 2. Workflow of UniV-Eval. The system accepts arbitrary inputs within task setting and performs dynamic evaluation after planning and decomposition. The final results are delivered as fine-grained checklist, providing traceable feedback for training optimization. dimension-wise extraction for all eight dimensions and subdimensions, (2) synthesis into coherent, shot-level descriptions. Three annotators then independently verify each caption against the source video, checking dimension completeness and temporal accuracy. GPT-4o provides additional automated verification by cross-checking factual claims. Captions with any disagreement undergo collaborative review where annotators discuss discrepancies and produce corrected versions. Each caption is revised an average of 1.8 times before finalization. Reference Images. To construct diverse reference image sets for R2V, RV2V tasks, we generate high-fideility reference images using Gemini 2.5 Flash Image (Nano Banana) and Seedream4.0 [37]. We firstly define three type of reference images: subject, style, and scene. For subject, we also define human subjects, animals, non-living objects(clothes, paper, etc.,). For style, it mainly covers 6 major styles: animation(2D, 3D), real(cinematic style, ), arts(Japanese ukiyoe style), sci-fi(cyberpunk style, wasteland style), dressing (rococo, lolita), and materials(clay animation style, building block style). For background, it is divied into natural (with different seasons, weather and time), human crafted (street, buildings), and vritual (magic library) scenes. The generated images are also carefully picked to ensure quality and prevent any infringement. Finally, 864 unique and diverse images are created for reference image related tasks. Video2Video Reconstruction. We innovatively propose new task, Video2Video reconstruction, to evaluate the performance of unified models in both understanding and generation tasks. Specifically, this task first requires the model to understand video and generate corresponding detailed captions, then reconstruct the video based on the generated text. By directly comparing the reconstructed video with the original one, we can assess the unified models capabilities in understanding and generation. high-quality unified model should first generate excellent captions through understanding, and second produce high-quality videos based on text. Failure in either task will result in significant discrepancy between the reconstructed video and the origin. 3.2. UniV-Eval We first introduce the evaluation tasks and corresponding evaluation strategies encompassed by our proposed unified evaluation system. Existing evaluation approaches typically assess the performance of video understanding and generation models on isolated tasks, in decoupled manner, lacking unified and integrated evaluation framework. Meanwhile, these methods often oversimplify the evaluation process, which leads to several potential risks: First, producing single scalar score can severely limit interpretability, failing to reveal fine-grained distinctions between the models strengths and weaknesses. As result, evaluations based solely on aggregate metrics make it challenging to provide actionable feedback for refinement during training. Second, assessing high-quality generation inherently involves complex, multifaceted, and dynamic dimensions. Fixed evaluation criteria may fail to accommodate diverse video attributes. For instance, some test cases emphasize the faithful reconstruction of visual instances, while others prioritize narrative coherence over instance-level fidelity. Therefore, in contrast to performing single-valued and fixed-dimension evaluations of video generation quality, we propose dynamically adaptive, fine-grained agentic system UniV-Eval that decomposes overall generation performance into set of interpretable, multidimensional checklists. This design enables more comprehensive and diagnostic assessment of model capability beyond conventional single-score evaluations. Specifically, as complementary component to UniVBench, our evaluation system centers on the user instruction and standardizes the prompting and instruction parsing procedures across tasks. Given any input (source video, reference image, and reference text), the system enables the evaluation of any output (including both video and text) in unified and consistent manner. Decomposing and Planning. Due to the current limitation on model generation length, long video is mechanically segmented into multiple clips = {ci}C i=1 for both generation and evaluation. As illustrated in Figure 2 (a), the proposed agent system UniV-Eval first decomposes each clip-level input into shot-level units for evaluation. Specifically, the multi-shots video is segmented as = {v1, v2, ..., vn} using PySceneDetect1, thereby determines the number of shot-level units in single clip. Meanwhile, the shot classification agent aligns the reference images and initial user instruction with their corresponding shots: = {i1, i2, ..., in} and = {t1, t2, ..., tn} , resulting in set of shot-level inputs (v, i, t) that serve as the foundation for subsequent evaluation. Notably, in the proposed system, all input modalities are optional, allowing flexible combinations of inputs depending on the evaluation scenario. Shot-level Fine-grained Evaluation. Let the output of the tested model as 1, combining with the input tuple (v, i, t), we invoke the shot evaluation agent to perform assessment. To ensure fine-grained scene and visual understanding at the shot level, we design nine major category groups: subject, relative position, actions, background&scene, color info, lighting info, video style, atmosphere and camera info [6, 10, 15, 36, 40], as shown in the checklist of Figure 2 (b). Each major category is further decomposed into specific, interpretable subcategories (21 in total) that the shot evaluation agent scores and reports, enabling diagnostic, fine-grained feedback at the shot level. The shot evaluation agent performs per-category comparisons between the model output 1 and the input tu1A tool designed to extract the minimal sub-shots from multi-shot videos. ple (v, i, t), producing structured weakness checklist that highlights fine-grained deficiencies useful for targeted training optimization. This checklist is then forwarded to an evaluation score agent, which aggregates the diagnostic signals and issues final scores along six evaluation dimensions for quantitative performance comparison. 4. Experiments 4.1. Implementation Details For fair and reproducible comparison, we evaluate all baselines under unified experimental protocol. For large multimodal models such as GPT-5, commercial Gemini 2.5 Pro [8], Seed 1.6 [16], and Seedance-1.0Lite [14], we directly access their official inference APIs released in late 2025. For open-source baselines, including CogVideoX [65], CoDi-2 [44], Omni-Video [43], Wan2.1VACE [50], and other video generation or editing models, we use their official codebases and pre-trained checkpoints. All models use consistent inference settings: 50 DDIM sampling steps, classifier-free guidance scale of 7.5, and native resolution (typically 720480 for 16:9 videos). All models are executed under consistent settings, including fixed sampling steps, classifier-free guidance scales, and resolution configurations aligned with their default or recommended parameters. When models lack native support for certain tasks, we implement minimal adaptations: for TV2V editing, we concatenate instruction text with source video embeddings; for RV2V editing, we inject reference image features into the diffusion process at intermediate layers. We use Seed-1.6 [16] as the evaluation LLM. All models receive identical inputs per task: groundtruth captions for T2V generation, source videos and editing instructions for TV2V, reference images and prompts for R2V. For V2V reconstruction, we first generate captions using each models understanding component (or GPT-4o for generation-only models), then reconstruct using those captions. Video inputs are center-cropped and resized to each models expected resolution while maintaining aspect ratio. All outputs undergo evaluation by our agentic system using identical prompts, rubrics, and dimension weightings, ensuring differences in scores reflect model capabilities rather than evaluation variance. All experiments are conducted on 8 NVIDIA H100 GPUs 80GB. 4.2. Main Results Our comprehensive evaluation on UniVBench, summarized in Table 4, reveals distinct specialization among current video models, highlighting the performance gap between systems designed for single task versus those for unified tasks. More experiments are provided in the supplementary materials. Task Models Subject Background Action Camera Color Lighting Video Style Relative Position Average Understanding (V2T) Generation (T2V) Generation (R2V) Editing (TV2V) Editing (RV2V) Reconstruction (V2V) Gemini 2.5 Pro Seed 1.6 Qwen3-VL-30B AuroraCap Tarsier2 Showo-2 Seedance-1.0-Pro Wan2.2-14B CoDi-2 Omni-Video CogVideoX Seedance-1.0-Lite Wan2.1-VACE-14B Wan2.1-VACE-14B Omni-Video Wan2.1-VACE-1.5B Wan2.1-VACE-14B CogVideoX-1.5-5B 54.4% 46.3% 15.8% 16.7% 34.3% 25.9% 68.8% 70.0% 6.1% 45.0% 42.6% 64.7% 66.3% 53.1% 57.8% 46.9% 14.6% 14.4% 25.0% 22.2% 65.2% 79.7% 15.6% 52.6% 62.9% 68.2% 51.2% 57.3% 27.0% 22.3% 6.4% 6.9% 32.7% 10.6% 74.3% 62.1% 25.0% 41.2% 33.9% 54.1% 65.8% 42.8% 54.6% 17.6% 25.3% 17.8% 23.9% 20.9% 7.7% 16.3% 13.7% 76.5% 84.8% 72.2% 81.6% 44.7% 54.6% 49.6% 67.6% 61.7% 82.0% 63.8% 49.7% 27.1% 26.4% 4.4% 11.3% 83.4% 69.2% 55.7% 66.2% 83.2% 39.8% 63.1% 75.4% 74.2% 45.3% 62.5% 75.3% 68.9% 71.1% 70.5% 70.1% 74.1% 65.4% 45.9% 24.1% 25.2% 20.1% 18.3% 76.8% 79.9% 37.1% 60.6% 77.8% 73.8% 72.5% 64.0% 37.3% 17.6% 51.3% 6.7% Note: Model types are separated into: Commercial Models, Open-Source Models. 71.5% 59.8% 69.2% 32.7% 77.5% 40.9% 47.2% 15.4% 19.8% 29.0% 68.2% 12.7% 63.3% 37.0% 66.7% 34.8% 29.1% 6.9% 60.4% 6.1% 20.7% 7.1% 56.4% 4.6% 44.4% 33.8% 9.8% 10.8% 21.9% 12.3% 91.6% 90.0% 83.4% 66.5% 81.3% 74.4% 78.4% 71.2% 81.6% 79.5% 79.9% 37.8% 54.1% 42.8% 17.6% 17.8% 21.9% 16.3% 77.9% 74.9% 40.1% 56.2% 65.7% 66.7% 65.1% 66.4% 47.9% 34.9% 62.7% 20.7% Table 4. Performance comparison of different baselines on UniVBench, summarizing results over six tasks, across eight dimensions. Figure 3. Case Study Analysis of UniVBench in T2V and Reconstruction Task. T2V generation uses the ground truth text of the video, while V2V reconstruction relies on models understanidng text. The generated videos are selected from OmniVideo Task-Specific Leaders. In the video understanding (V2T) task, Gemini 2.5 Pro demonstrates superior performance with an average score of 54.1%, significantly outpacing other models. Conversely, unified video models like Showo-2 score (16.3%) in this domain, showcasing their lack of perceptual reasoning. For text-to-video (T2V) generation, Seedance-1.0-Pro achieves the top score of 77.9%. For reconstruction (V2V), Wan2.1-VACE-14B delivers the strongest performance, with scores of 62.7%. Cross-Dimensional Insights. key observation across all tasks is the difficulty models have with the Action dimension, which frequently receives the lowest scores, particularly in video understanding. This suggests that accurately interpreting and synthesizing complex temporal dynamics remains major challenge. In contrast, generative models exhibit greater control over stylistic attributes like Color, Lighting, and Video Style, where they often achieve their highest scores. The Unification Gap. Overall, the results quantitatively indicates that no single model currently excels across the full spectrum of understanding, generation, and editing. The benchmark effectively maps the strengths and weaknesses of existing architectures, providing clear and necessary baseline to guide future efforts in developing truly unified video foundation models. 4.3. Analysis Reconstruction Case Study. We present qualitative results in Figure 3, where T2V generation leverages the videos ground truth text and reconstruction relies on the models self-derived understanding text. comparison of these three sets of videos reveals varying degrees of inconsistency. Notably, the V2V task exhibits more pronounced Figure 5. Human expert annotations used to validate the reliability of UniV-Eval. high alignment with human judgments, with an average agreement of nearly 85%, demonstrating that the proposed metric faithfully reflects human annotations. 5. Conclusion UniVBench addresses critical gap in video foundation model evaluation by providing the first unified framework to comprehensively assess understanding, generation, editing, and reconstruction capabilities. Our benchmark comprises 200 high-quality, multi-shot videos with comprehensive annotations including detailed captions, multi-format editing instructions, and reference images. We establish systematic evaluation across eight fundamental cinematic dimensions decomposed into 21 fine-grained sub-dimensions, providing complete coverage where existing benchmarks exhibit fragmented evaluation of subsets. in existing metrics Our unified agentic evaluation system standardizes assessment across all six tasks with multi-dimensional, shotlevel scoring that enables interpretable analysis, direct cross-task comparison, and precise attribution of failures to perception versus generation componentscapabilities absent relying on single scalar scores. Through this comprehensive framework spanning professional-grade cinematic evaluation, multi-shot temporal assessment, and unified cross-task metrics, UniVBench establishes principled foundation for measuring progress toward general-purpose, instruction-following video intelligence. 6. Limitation and Future Works This work focuses on establishing unified benchmark for evaluation and does not introduce new model architecture. primary limitation is the current scale of our dataset; while the 200 richly annotated videos are sufficient for comprehensive evaluation, they are not enough to train largescale unified video model from the ground up. Therefore, crucial direction for future work is to significantly expand the UniVBench dataset in volume. Looking ahead, our goal Figure 4. An example of evaluation using different metrics, where the blue-highlighted part shows that UniV-Eval provides more detailed, traceable validation and assessment. inconsistencies than its T2V counterpart, indicating information transmission loss during the V2T T2V pipeline. Collectively, these findings clearly highlight the inherent weaknesses of current unified video models. Metrics Case Study. To qualitatively demonstrate the superiority of the proposed UniV-Eval over previous metrics, we present case study in Figure 4. BLEU Score measures the lexical overlap between candidate and reference texts, yet in V2T tasks, the varying effective caption lengths across models can substantially distort BLEU scores. Meanwhile, conventional LLM-as-a-Judge approaches offer fine-grained feedback, but typically consider limited evaluation dimensions and still lack interpretability. In contrast, as shown in Table 3, UniV-Eval implements fine-grained dynamically adaptive evaluation strategy. Human Study. To assess the reliability of UniV-Eval, we randomly sampled 10% of the data and conducted threefold cross-validation study. Human experts reviewed each sample with reference annotations and provided the corresponding labels. As shown in Figure 5, UniV-Eval achieves is to leverage this expanded benchmark to train and validate novel Unified Video Models, using the insights gained from our evaluation framework to drive the development of more integrated and capable systems. 7. Acknowledgement This work is supported by the National Key R&D Program of China (Grant No. 2024YFC3308304), the Pioneer and Leading Goose R&D Program 2025C01128), and the ZJUof Zhejiang (Grant no. Intelligence Healthcare. for Angelalign R&D Center"
        },
        {
            "title": "References",
            "content": "[1] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel BernadettShapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Made laine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Benjamin Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Is abella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Ryan Kiros, Matthew Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Li, Rachel Lim, Molly Lin, Stephanie Lin, Ma teusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, An drey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen OKeefe, Jakub W. Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alexandre Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack W. Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas A. Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll L. Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2023. 1 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. 1 [3] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. ArXiv, abs/2405.04233, 2024. 1, 2 [4] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. 1, 3 [5] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 1, 3 [6] Alec Chillingworth. Your guide to more than 30 different camera shots. StudioBinder, 2024. 6 [7] Iya Chivileva, Philip Lynch, Tomas Ward, and Alan Smeaton. Measuring the quality of text-to-video model outputs: Metrics and dataset. arXiv preprint arXiv:2309.08009, 2023. 1, 3 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6 [9] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 3 [10] AJ Detisch. Film lighting techniques how to get cinematic look. StudioBinder, 2025. 6 [11] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 27582766, 2015. 2 [12] Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jan Kautz, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. Vila2: Vila augmented vila. arXiv preprint arXiv:2407.17453, 2024. 1, 3 [13] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6712 6722, 2024. 1, 3 [14] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 1, 4, 6 [15] Karen Mc Guinness. The 16 types of camera shots & angles. StudioBinder, 2025. 6 [16] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Si-Ming Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Ya-Fang Hu, Yi Lin, Yi Chun Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yueming Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Ben Ben Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Ju Lu, Kai Hua, Kai Liu, Kai Shen, Kai-Hua Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Meng-Han Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qing-Yun Yuan, Qi Shuai, Qiuyan Tao, Ren Kui Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shi feng Yan, Shu Zhong, Shuai Shuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tian-Yi Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, Xian gang Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xincheng Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Ya-Feng Wen, Yi-Mei Du, Yihao Zhang, Yining Ye, Yong-Xu Wu, Yu Liu, Yuanhang Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yu-Qing Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Ze-Bang Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zheng Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, and Zuquan Song. Seed1.5-vl technical report. ArXiv, abs/2505.07062, 2025. 1, 6 [17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 75147528, 2021. 4 [18] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 1, 3, 4 [19] Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan, and Zuozhu Liu. Med-moe: Mixture of domain-specific experts for lightweight medical vision-language models. In Findings of the association for computational linguistics: EMNLP 2024, pages 38433860, 2024. [20] Songtao Jiang, Yuan Wang, Sibo Song, Tianxiang Hu, Chenyi Zhou, Bin Pu, Yan Zhang, Zhibo Yang, Yang Feng, Joey Tianyi Zhou, et al. Hulu-med: transparent generalist model towards holistic medical vision-language understanding. arXiv preprint arXiv:2510.08668, 2025. 1 [21] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 1, 3, 4 [22] Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, and Ning Liu. Subjective-aligned dataset and metric for text-to-video qualIn Proceedings of the 32nd ACM Internaity assessment. tional Conference on Multimedia, pages 77937802, 2024. 1, 3, 4 [23] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 3 [24] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, et al. Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743, 2024. 1, 3, 4 [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, [26] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-ajudge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 27572791, 2025. 4 [27] Minghan Li, Chenxi Xie, Yichen Wu, Lei Zhang, and Mengyu Wang. Five: fine-grained video editing benchmark for evaluating emerging diffusion and rectified flow models. arXiv preprint arXiv:2503.13684, 2025. 1, 3, 4 [28] Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model arXiv preprint for interleaved multi-modal generation. arXiv:2505.05472, 2025. 1, 3 [29] Hongbo Liu, Jingwen He, Yi Jin, Dian Zheng, Yuhao Dong, Fan Zhang, Ziqi Huang, Yinan He, Yangguang Li, Weichao Chen, et al. Shotbench: Expert-level cinematic understanding in vision-language models. arXiv preprint arXiv:2506.21356, 2025. 1, 3 [30] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-tovideo generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. 3 [31] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. 3 [32] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Sheng-Siang Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wenzheng Sun, Xin Han, Yana Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Da Shi, Dingyuan Hu, Enle Liu, Gang Yu, Gege Yang, Guanzhe Huang, Gulin Yan, Hai bo Feng, Hao Nie, Hao Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hong-Wei Guo, Huilin Xiong, Hui Xiong, Jiahao Gong, Jianchang Wu, Jiao Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Jun-Nan Guo, Junzhe Lin, Kai hua Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Li-Li Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiao-Li Chen, Qi He, Qi Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shi kui Yang, Si-Ye Liu, Siqi Liu, Shu-Guang Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xuefeng Zhao, Xuelin Zhang, Xi Zeng, Xiaojian Liu, Xuan Yang, Ya-Nan Dai, Yanbo Yu, Yang Li, Yin-Yong Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, and Yu Luo. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. ArXiv, abs/2502.10248, 2025. 1 [33] Arjun Panickssery, Samuel Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations. Advances in Neural Information Processing Systems, 37: 6877268802, 2024. 1 [34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 4 [35] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Ki ran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam S. Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali K. Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. ArXiv, abs/2410.13720, 2024. 1 [36] Mary Risk. How to use color in film: 50+ examples of movie color palettes. StudioBinder, 2024. [37] Team Seedream, :, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, Xiaowen Jian, Huafeng Kuang, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yanzuo Lu, Zhengxiong Luo, Tongtong Ou, Guang Shi, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Wenxu Wu, Yonghui Wu, Xin Xia, Xuefeng Xiao, Shuang Xu, Xin Yan, Ceyuan Yang, Jianchao Yang, Zhonghua Zhai, Chenlin Zhang, Heng Zhang, Qi Zhang, Xinyu Zhang, Yuwei Zhang, Shijia Zhao, Wenliang Zhao, and Wenjia Zhu. Seedream 4.0: Toward next-generation multimodal image generation, 2025. 5 [38] Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In European conference on computer vision, pages 510526. Springer, 2016. 3 [39] Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, and Yaniv Taigman. Video editing via factorized diffusion distillation. In European Conference on Computer Vision, pages 450466. Springer, 2024. 1, 3 [40] StudioBinder. 50+ types of camera shots, angles, and techniques. StudioBinder, 2025. [41] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In The Twelfth International Conference on Learning Representations, 2024. 2 [42] Shangkun Sun, Xiaoyu Liang, Songlin Fan, Wenxu Gao, and Wei Gao. Ve-bench: Subjective-aligned benchmark suite for In Proceedtext-driven video editing quality assessment. ings of the AAAI Conference on Artificial Intelligence, pages 71057113, 2025. 3 [43] Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, and Hao Li. Omni-video: Democratizing uniarXiv preprint fied video understanding and generation. arXiv:2507.06119, 2025. 1, 3, 6 [44] Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. Codi-2: In-context interleaved and interactive any-to-any generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2742527434, 2024. 3, 6 [45] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1, 3 [46] Atousa Torabi, Christopher Pal, Hugo Larochelle, and Aaron Courville. Using descriptive video services to create large data source for video annotation research. arXiv preprint arXiv:1503.01070, 2015. [47] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. In The Seventh International Conference on Learning Representations, 2019. 4 [48] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluaIn Proceedings of the IEEE conference on computer tion. vision and pattern recognition, pages 45664575, 2015. 4 [49] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks, 2015. 3 [50] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 6 [51] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [52] Jiarui Wang, Huiyu Duan, Ziheng Jia, Yu Zhao, Woo Yi Yang, Zicheng Zhang, Zijian Chen, Juntong Wang, Yuke Xing, Guangtao Zhai, et al. Love: Benchmarking and evaluating text-to-video generation and video-to-text interpretation. arXiv preprint arXiv:2505.12098, 2025. 1, 3, 4 [53] Jiarui Wang, Huiyu Duan, Guangtao Zhai, Juntong Wang, and Xiongkuo Min. Aigv-assessor: benchmarking and evaluating the perceptual quality of text-to-video generation with In Proceedings of the Computer Vision and Pattern lmm. Recognition Conference, pages 1886918880, 2025. 3 [54] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, highquality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. 1, 3 [55] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1, [56] Cong Wei, Quande Liu, Zixuan Ye, Qiulin Wang, Xintao Wang, Pengfei Wan, Kun Gai, and Wenhu Chen. Univideo: Unified understanding, generation, and editing for videos. arXiv preprint arXiv:2510.08377, 2025. 1, 3 [57] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. ArXiv, abs/2509.20328, 2025. 1 [58] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003, 2023. 1, 3 [59] Jay Zhangjie Wu, Guian Fang, Dongrong Joe Fu, Vijay Anand Raghava Kanakagiri, Forrest Iandola, Kurt Keutzer, Wynne Hsu, Zhen Dong, and Mike Zheng Shou. Veditbench: Holistic benchmark for text-guided video editing. Openreview, 2025. 2 [60] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn motion pattern for few-shot video generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 70897098, 2024. 2 [61] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExT-GPT: Any-to-any multimodal LLM. In Proceedings of the International Conference on Machine Learning, pages 5336653397, 2024. 1, [62] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multiIn The Thirteenth International Conference modal model. on Learning Representations, 2025. 1, 3 [75] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, 2018. 1, 3 [76] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Cong He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Ying Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kai Zhang, Hui Deng, Jiaye Ge, Kaiming Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. ArXiv, abs/2504.10479, 2025. 1 single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. 1, 3 [63] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. [64] Yuhang Yang, Ke Fan, Shangkun Sun, Hongxiang Li, Ailing Zeng, FeiLin Han, Wei Zhai, Wei Liu, Yang Cao, and ZhengJun Zha. Videogen-eval: Agent-based system for video generation evaluation, 2025. 4 [65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 6 [66] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 1, 3 [67] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 1, 3, 4 [68] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 1, 3 [69] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, 133(4):18791893, 2025. 1, [70] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 2, 4 [71] Zicheng Zhang, Tengchuan Kou, Shushi Wang, Chunyi Li, Wei Sun, Wei Wang, Xiaoyu Li, Zongyu Wang, Xuezhi Cao, Xiongkuo Min, et al. Q-eval-100k: Evaluating visual quality and alignment level for text-to-vision content. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1062110631, 2025. 1, 3 [72] Zhichao Zhang, Wei Sun, Li Xinyue, Jun Jia, Xiongkuo Min, Zicheng Zhang, Chunyi Li, Zijian Chen, Wang Puyi, Sun Fengyu, et al. Benchmarking multi-dimensional aigc video quality assessment: dataset and unified model. ACM Transactions on Multimedia Computing, Communications and Applications, 21(9):124, 2025. 3 [73] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, WeiShi Zheng, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 1, 3, 4 [74] Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe UniVBench: Towards Unified Evaluation for Video Foundation Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Evaluation Cases C. Evaluation System Prompt In this section, we presents the evaluation resulst in different tasks. In Figure A1 and A2, we present the source video and the reference captions we provided, along with generated video from CogVideoX, OmniVideo and Wan2.2-15B. In Figure A3, we present the results of reference images to video generation by Seedance-Lite. From the rows of images, we can see that current video generation models still struggle to meet the text requirements. In Figure A1, the two animals enter the frame and walk to the front of the camera and wave hands are not captured by CogVideoX and OmniVideo. In Figure A2, the dinosaur-shaped pet bed opens when the cat enters. CogVideoX and OmniVideos results didnt conform to it. In Figure A3, the referenced subject has serious identity shift when cut to the next shot. These qualitative results show that current video generation models still have large room for improvements. B. More Details of UniVBench B.1. Captioning Meta Data Distribution In Figure B4, we provided the video content distribution across each sub-dimensions. This indicates that our dataset is semantically rich and diverse. In this section, we provide detailed description of the system prompts used in UniV-Eval, organized by task categories. Specifically, we present the system prompts corresponding to the six major tasks: V2T (Figure F14), T2V (Figure F15), R2V (Figure F16), TV2V (Figure F17), RV2V (Figure F18), and V2V (Figure F19). It is important to note that, for the V2T task, the evaluation prompt must be used together with predefined template (Figure F20), since the final comparison is conducted between the ground-truth caption and the baseline caption. For the other tasks, the comparison rules for generic objects are illustrated in Figure F21. In practice, these components should be combined to form the complete system prompt used for evaluation. D. Evaluation Cost Average cost of running one case is provided in Table D1. The cost of evaluating one task is less than 10 US dollars. V2V TV2V R2V RV2V T2V I/O total tokens Times (s) 25104 25898 62 16743 44 27534 55 19567 49 V2T 1413 Table D1. The cost of evaluation B.2. Captioning Prompt E. Potential LLM-as-Judge Bias In this subsection, we release our system prompts to generate dense video captions for our benchmark construction. They are shown in Figure B7 to B13. The prompts are divided into two steps: First, the model is tasked to extract the necessary content attributes from the video. This can include: subjects, actions, background, camera information, color, lighting, video style, etc., Then, the model merges them together to generate coherent and structured video script, the format is shown in Figure B5. The essence of video script format is: first describes the fixed, unchanging content, including the overall style and atmosphere of the video. Then, specify the information of the videos first frame, such as the subjects appearing in the first frame, their positions, and initial states. Subsequently, output subject actions, camera movements, and any changing information in chronological orderincluding adjustments to the relative positions of subjects and camera parameters. If the video is multi-shot, appending the keyword: Shot cut, and repeate the first frame description, subject actions, camera movements in chronological order. Self-preference bias exists when the same LLMs act as both evaluatee and evaluator, they can recognize their own outputs and give higher scores, which is well-discussed in existing work [33]. In our settings, evaluatee and evaluators are different. The evaluatee models are video generation models, while the evaluator models are vision-language models. These two models differ significantly in both their architectural designs and training data. F. Evaluation cases Here we provide evaluation case in Figure B6 between human and LLM-as-Judge. While the judge model conducts meticulous, all-dimensional evaluations, it overlooks critical issues. Human evaluators, by contrast, focus on salient errors and ignore subtle details. Below is case analysis. The model evaluates that: the cucumbers in the video have smooth surface, without the wrinkled texture and white dots in the reference image [orange region]; While human evaluates that the the cucumber is cut sideways [red region], which conflicts with the slices on the cutting board. Figure A1. Examples of T2V generation results across different baselines Figure A2. Examples of T2V generation results across different baselines Figure A3. Examples of R2V generation results of Seedance-Lite Figure B4. The meta-data distribution of video content. Figure B5. The script format used to generate the coherent video captions. Red font indicates the content model needs to fill in. Green font indicates the explanation of each field. Figure B6. Evaluation case of LLM as judge and human Figure B7. Captioning prompts used to generate detailed video captions. Figure B8. Captioning prompts used to generate detailed video captions. Figure B9. Captioning prompts used to generate detailed video captions. Figure B10. Captioning prompts used to generate detailed video captions. Figure B11. Captioning prompts used to generate detailed video captions. Figure B12. Captioning prompts used to generate detailed video captions. Figure B13. Captioning prompts used to generate detailed video captions. Figure F14. Captioning prompts used to generate detailed video captions. Figure F15. Evaluation prompts used for V2T task. Figure F16. Evaluation prompts used for R2V task. Figure F17. Evaluation prompts used for TV2V task. Figure F18. Evaluation prompts used for RV2V task. Figure F19. Evaluation prompts used for V2V task. Figure F20. Evaluation Json template used for V2T task. Figure F21. Evaluation Json template used for V2T task."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Zhejiang Lab",
        "Zhejiang University"
    ]
}