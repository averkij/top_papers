{
    "paper_title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space",
    "authors": [
        "Yicheng Chen",
        "Yining Li",
        "Kai Hu",
        "Zerun Ma",
        "Haochen Ye",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to \\textbf{M}aximize the \\textbf{I}nformation \\textbf{G}ain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\\% on AlpacaEval and +6.89\\% on Wildbench."
        },
        {
            "title": "Start",
            "content": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space Yicheng Chen1,2, Yining Li1, Kai Hu1,3, Zerun Ma1, Haochen Ye1, Kai Chen1 1Shanghai AI Laboratory 2Fudan University 3Carnegie Mellon University Project page: https://yichengchen24.github.io/projects/mig 5 2 0 2 8 1 ] . [ 1 5 3 8 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Data quality and diversity are key to the construction of effective instruction-tuning datasets. With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from vast amount of data. Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. However, this absence of comprehensive view of the entire collection often leads to suboptimal results. Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. To bridge this gap, we propose unified method for quantifying the information content of datasets. This method models the semantic space by constructing label graph and quantifies diversity based on the distribution of information within the graph. Based on such measurement, we further introduce an efficient sampling method that selects data samples iteratively to Maximize the Information Gain (MIG) in semantic space. Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. Notably, the model fine-tuned with 5% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73% on AlpacaEval and +6.89% on Wildbench."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have shown remarkable capabilities in following human instructions in wide range of tasks (Wang et al., 2023a). Typically, LLMs first acquire general knowledge through large-scale pretraining and are subsequently refined through instruction tuning to better align with diverse human intentions (Brown et al., Corresponding Author. 1 Figure 1: Comparison with different data selection methods (Lu et al., 2024; Liu et al., 2024b) on the Tulu3 (Lambert et al., 2024) pool using Llama3.1-8B (Touvron et al., 2023), evaluated on (black) knowledge-based benchmarks and (red) human-preference benchmarks. See details in Sec. 4.2. 2020; Taori et al., 2023; Touvron et al., 2023). Instruction tuning utilizes instruction-response pairs to guide base models toward more accurate and contextually appropriate responses. Recent studies (Zhou et al., 2023a; Chen et al., 2024) emphasize the critical role of data engineering in instruction tuning, highlighting data quality rather than quantity as the key to effective instruction tuning. Notably, LIMA (Zhou et al., 2023a) demonstrates that just 1000 high-quality, human-curated instructions can achieve performance comparable to substantially larger datasets. However, manual curation of such datasets is inherently time-consuming and labor-intensive (Chiang et al., 2023). More recently, line of work (Chen et al., 2024; Lu et al., 2024; Liu et al., 2024b; Bukharin et al., 2024) proposes automatic selections of optimal subsets from extensive data pools by defining desirable data characteristics. These approaches (Bukharin et al., 2024; Yu et al., 2024b) posit that quality and diversity are crucial for an effective instructiontuning dataset. Data quality is defined from multiple perspectives, such as instruction complexity (Lu et al., 2024; Zhao et al., 2024), model perplexity and uncertainty (Li et al., 2024b), or scores assigned by advanced external models (Chen et al., 2024; Liu et al., 2024b). However, diversity remains less explicitly quantified, often addressed via heuristic methods such as maximizing label set coverage (Lu et al., 2024), reducing redundancy through diversity filters (Liu et al., 2024b), or enforcing fixed sample distributions per cluster (Ge et al., 2024; Yu et al., 2024b). This narrow focus on diversity only during later selection stages, without comprehensive view of the entire dataset, diminishes the global diversity and representativeness of the sampled data. Alternative methods (Bukharin et al., 2024) employing embedding-based facility location functions (Cornuéjols et al., 1983) quantify diversity but require computationally intensive iterative pairwise distance calculations, making them impractical for large datasets. Additionally, distance-based clustering in the embedding space may fail to capture the semantic intent of complex instructions accurately. To solve these issues, several essential questions are raised: 1) How can we effectively quantify diversity in semantic space while balancing quality and diversity in dataset evaluation? 2) How can we efficiently select data based on such evaluations? To this end, we propose an information-based measure for instruction-tuning datasets and introduce an efficient data selection algorithm that aims to Maximize the Information Gain (MIG). We model the semantic space as label graph, with nodes representing labels and edges capturing semantic relationships. Information in the dataset is distributed across this graph, with the total information being the sum of each labels information. Each data point contributes to its associated labels in proportion to its quality. Thus, the information of each data point measures local data quality, while the total of all label information measures the global diversity of the dataset. To balance quality and diversity, we apply monotonically increasing but marginally diminishing function to compute label information, thereby promoting diversity and preventing excessive data concentration on particular labels. To better model information distribution in semantic space, we propagate information along label graph edges to address semantic correlations and annotation biases. Leveraging the submodularity of our proposed information-based dataset measurement, we implement an efficient greedy algorithm that iteratively selects data points that maximize the information gain according to the current state of the label graph. Through extensive experiments across data pools (Liu et al., 2024a; Teknium, 2023; Lambert et al., 2024) of varying quality and sizes, and LLMs of different families (Touvron et al., 2023; Jiang et al., 2023; Yang et al., 2024), MIG consistently achieves superior performance on both humanpreference and knowledge-based evaluations. As shown in Fig 1, on the Tulu3 (Lambert et al., 2024) pool with Llama3.1-8B as the base model, MIG achieves average improvements of +1.49% on six knowledge-based benchmarks (Clark et al., 2018; Suzgun et al., 2022; Hendrycks et al., 2021; Chen et al., 2021; Cobbe et al., 2021; Zhou et al., 2023b) and +1.96% on three human-preference benchmarks (Zheng et al., 2023; Dong et al., 2024; Lin et al., 2024) compared to previous state-of-the-art data selection methods (Liu et al., 2024b; Bukharin et al., 2024). When combining both evaluations, MIG achieves average improvements of +2.20% compared to the second-best method (Bukharin et al., 2024). Notably, the model fine-tuned with 5% Tulu3 data sampled by MIG outperforms the official SFT model trained on the full dataset by +1.73% (average on nine benchmarks), with substantial boost of +4.59% in human-preference evaluations. MIG also outperforms existing methods on the Openhermes2.5 (Teknium, 2023) and Xsota (Lu et al., 2024; Liu et al., 2024b), further demonstrating its generalizability across different settings. Additionally, MIG significantly enhances sampling efficiency, reducing sampling time by over 100-fold on the Tulu3 data pool compared to embeddingbased methods. In summary, our contributions are as follows: We propose an information-based measurement for instruction-tuning datasets in semantic space. It quantifies quality and diversity within the information distributed across the semantic label graph. We introduce MIG, an efficient data selection algorithm that maximizes the information gain on the label graph iteratively. Extensive experiments on various data pools, base models, and benchmarks demonstrate the effectiveness and generalizability of MIG. The correlation between parameters in MIG and the attributes of sampled data is well studied."
        },
        {
            "title": "2 Related Work",
            "content": "Data Selection for Instruction Tuning. Recent studies (Zhou et al., 2023a; Chen et al., 2024) indicate that increasing data quality and diversity rather than quantity effectively boosts instructionfollowing performance. Consequently, data selection methods aim to identify optimal subsets that meet such characteristics and generally fall into three categories: (1) Quality-based approaches prioritize high-quality data points, where quality is defined through various perspectives, such as instruction complexity and response quality. INSTRUCTMINING (Cao et al., 2024b) identifies natural language metrics indicative of high-quality instruction data. Instruction-Following Difficulty (IFD) (Li et al., 2024b) highlights inconsistencies between models anticipated responses and its self-generated outputs. Nuggets (Li et al., 2023b) measures quality based on the disparity between one-shot and zero-shot performance. LESS (Xia et al., 2024a) uses gradient features to select samples based on their similarity to few representative examples. SelectIT (Liu et al., 2024a) selects highquality data based on intrinsic uncertainty from token, sentence, and model levels. Additionally, some methods employ external LLMs to assess data quality, such as ALPAGASUS (Chen et al., 2024), which uses well-designed prompt applied to ChatGPT to assess the quality of each data tuple. (2) Diversity-based approaches aim to select data subsets with broad coverage of the data pool. DiverseEvol (Wu et al., 2023) iteratively selects samples distant from previously selected data in the embedding space. ZIP (Yin et al., 2024) prioritizes subsets with low compression ratios, implicitly favoring diversity. (3) Comprehensive approaches strive to balance quality and diversity. #InsTag (Lu et al., 2024) employs ChatGPT to generate detailed open-ended tags for instructions and prioritize complex data with more tags while maximizing topic coverage. DEITA (Liu et al., 2024b) prioritizes high-quality data points while avoiding duplicates in the embedding space. CaR (Ge et al., 2024) and kMQ (Yu et al., 2024b) cluster data and sample high-quality points from each cluster. However, these methods typically rely on heuristic rules rather than unified quantitative metric to balance quality and diversity. Submodular function for Diversity Measurement. Traditional submodular functions, such as facility location, graph cut, and log determinant, effectively quantify dataset diversity by identifying representative, non-redundant subsets. Leveraging this property, QDIT (Bukharin et al., 2024) measures diversity using the facility location function (Cornuéjols et al., 1983), combining it linearly with quality scores. Similarly, DPP (Wang et al., 2024) employs the log determinant distance to quantify subset diversity. Although this NPhard problem can be approximated with greedy algorithm following submodularity (Nemhauser et al., 1978; Minoux, 2005), embedding-based methods are inefficient at scale due to the high storage and computational costs of calculating highdimensional pairwise distances. To mitigate this issue, our custom dataset measurement is submodular, which justifies the use of greedy strategy, while MIG samples data in high-level semantic space, substantially reducing computational overhead."
        },
        {
            "title": "3 Method",
            "content": "As shown in Fig. 2(a), we begin by annotating the raw data pool with tagger and scorer. Next, MIG constructs label graph to measure dataset information (Sec. 3.2) and selects subset for subsequent SFT by maximizing the information gain (Sec. 3.3)."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Task. Given data pool DP , budget , and an information measure E(D) over any dataset D, the goal is to select subset DS DP of size that maximizes E(D). Formally, DS = argmax E(D) DDP ,D=N Data. Each data point is formed as: di = {(qj , rj )M j=1, Li, si} (1) (2) )M , rj where (qj j=1 represents rounds of queryresponse pairs used for training, Li is the set of labels (e.g., task category, knowledge domain, and other meta information) associated with di, and si is the quality score. 3."
        },
        {
            "title": "Information Measurement",
            "content": "Label Graph. Previous studies (Lu et al., 2024; Ge et al., 2024; Yu et al., 2024b) assume that labels (including embedding-based clusters) are independent, ignoring the semantic relationships among them. However, such label associations are crucial for accurately capturing the information distribution in semantic space. Intuitively, we can 3 Figure 2: Illustration of (a) Data Selection Pipeline and (b) MIG Sampler. Given the raw data pool, our pipeline first applies tagger and scorer to annotate data. Next, MIG constructs the label graph based on the label set and iteratively selects the data point that maximizes the information gain within the graph. The selected data are used for supervised fine-tuning (SFT) of LLMs. model labels as nodes, their associations as edges, and the intensity of associations as edge weights, thus modeling semantic space as an undirected weighted graph GL = (L, EL), where represents the label set with size of and EL represents edges. Specifically, we use label similarities as edge weights and remove edges whose weights are below threshold to ensure computational efficiency. Therefore, EL can be formed as weighted adjacency matrix WL RKK with elements: wpq = σ[w(lp, lq) ] w(lp, lq) (3) where w(lp, lq) represents textual similarity between label lp and lq, and σ() yields 1 when the input is evaluated as True. Data Point Information. Under the label set L, data point di can be formed as binary label vector with its associated labels Li: vi = {vi = σ(lk Li)}K k=1 (4) The information of di is distributed over Li and is proportional to its quality score si. Thus, the raw information of di can be formed as: ei = si vi (5) Semantic overlaps between labels and annotationinduced bias can lead to inaccurate information distribution. To address this, we introduce information propagation along the edges of the label graph, 4 enabling more accurate modeling of information distribution across the semantic space. Formally, the propagation from lp to lq is: apq = αwpq wp + α (cid:80) k,k=p wpk (6) where wp equals 1 and α is hyperparameter controlling the intensity of information propagation. Let be the propagation matrix, then the propagated information vector of di is: ˆei = Aei (7) Dataset Information. To balance quality and diversity within the label graph, we apply monotonically increasing yet upper-convex function ϕ to compute the label information. The marginally diminishing information gain is negatively correlated with the existing label information. Thus, information gains on labels with less information are prioritized. Formally, the dataset information is: E(D) = Φ( (cid:88) Aei) = Φ(A iD (cid:88) iD sivi) (8) where Φ is nonlinear transformation that applies ϕ element-wise to the input and then aggregates the results by summation."
        },
        {
            "title": "3.3 MIG Sampling",
            "content": "Directly selecting DS from DP is computationally infeasible as the combination CN DP grows quickly. Algorithm 1: MIG Sampling Data: Initial Data Pool DP , Label Sets L, Sample Budget Result: The Sampled Dataset DS 1 Initialize Empty DS; 2 Initialize Propagation Matrix A; 3 while DS < do AΦ(A (cid:80) 4 di argmaxdDP DS DS {di}; DP DP {di}; Ek); GEd; kDS 7 5 6 8 return DS Thus, as shown in Fig 2(b), we follow the submodularity of E(D) (detailed in Appx. C) and propose greedy strategy, iteratively selecting the data point that yields the maximum information gain: dk = argmax dDk {E(Dk {d}) E(Dk S)} (9)"
        },
        {
            "title": "S and Dk",
            "content": "where Dk denote the selected subset and remaining candidate pool at iteration k. We approximate the information gain in Eq. 9 via gradientbased approach: Gk = E(Dk S) = AΦ(A (cid:88) ei) (10) iDk where Φ represents the derivative of Φ. Thus, the selection process can be formed as: dk = argmax"
        },
        {
            "title": "Gked",
            "content": "dDk (11) The sampling is detailed in Alg. 1. Refer to Appx. A.2 for more implementation details of MIG."
        },
        {
            "title": "4.1 Setups",
            "content": "Datasets. To investigate data selection across various scenarios and demonstrate the robustness of MIG, we use three distinct data pools: Tulu3 (Lambert et al., 2024): large-scale, realworld SFT dataset presented by Ai2, containing million-level records across wide variety of subjects, including mathematics, programming, and user dialogues. Openhermes2.5 (Teknium, 2023): dataset with over 1 million data points, sourced from 16 distinct origins, including MetaMath (Yu et al., 2024a), CamelAI (Li et al., 2023a), and others. 5 Xsota (Lu et al., 2024; Liu et al., 2024b): combined data pool consisting primarily of high-quality conversations from datasets such as WizardLM (Alpaca), WizardLM (ShareGPT), UltraChat (Ding et al., 2023), and ShareGPT (Chiang et al., 2023), totaling 300K data points. Benchmarks. We use both human-preference and knowledge-based benchmarks to evaluate model performance comprehensively. The evaluation is conducted using OpenCompass (Contributors, 2023), with the average results reported as normalized scores on percentage scale. Detailed evaluation settings are provided in Appx. A.4. Human-preference Benchmarks. We evaluate open-ended dialogue abilities using model-based evaluation metrics on three benchmarks: AlpacaEvalv2 (Dubois et al., 2024), MTBench (Zheng et al., 2023), and WildBench (Lin et al., 2024). Knowledge-based Benchmarks. We assess the factual knowledge, reasoning, coding, mathematical, and instruction-following abilities using automatic metrics on six benchmarks: ARC (Clark et al., 2018), Big-Bench-Hard(BBH) (Suzgun et al., 2022), MMLU (Hendrycks et al., 2021), HumanEval (Chen et al., 2021), GSM8k (Cobbe et al., 2021), and IFEval (Zhou et al., 2023b). Baselines. We compare our methods against strong data selection approaches: random selection (Xia et al., 2024b), IFD (Li et al., 2024b), ZIP (Yin et al., 2024), #InsTag (Lu et al., 2024), DEITA (Liu et al., 2024b), CaR (Ge et al., 2024), and QDIT (Bukharin et al., 2024). To replicate baselines on Tulu3 and Openhermes2.5, we adjust certain parameters to fit the large-scale datasets, as detailed in Appx A.1. Training. We use LLaMA3.1-8B (Touvron et al., 2023), Mistral-7B-v0.3 (Jiang et al., 2023), and Qwen2.5-7B (Yang et al., 2024) as our base models and fine-tune them using the Llama-Factory framework (Zheng et al., 2024). Please refer to Appx. A.3 for detailed training setup."
        },
        {
            "title": "4.2 Main Results",
            "content": "Main Comparison. Table 1 presents the performance of MIG and baselines across benchmarks. All methods select 50K samples based on the grid search (Sec 4.3). With Llama3.1-8B, MIG outperforms all baselines on most tasks, with average improvements of +1.49% and +1.96% over previous state-of-the-art selection methods on knowledgebased and human-preference evaluations, respectively. MIG surpasses QDIT, the second-best Table 1: Comparison with data selection methods on the Tulu3 pool. HE denotes HumanEval, AE denotes AlpacaEvalv2, MT denotes MTBench, and Wild denotes WildBench. Avgobj and Avgsub represent the average of the normalized knowledge-based and human-preference benchmark scores, respectively. Avg is the mean of Avgobj and Avgsub. MIG achieves the best performance on Avgobj, Avgsub, and Avg on all base models. Base Model Method Data Size ARC BBH GSM HE MMLU IFEval Avgobj AE 8.94 8.57 6.71 12.30 6.58 10.19 12.55 15. 68.79 64.25 59.19 61.32 64.14 66.15 65.63 65.21 67.64 14.66 59.81 48.43 60.94 56.00 52.26 56.16 5.84 5.34 8.20 8.82 11.93 15. 61.41 13.66 76.31 75.53 72.80 75.34 75.14 74.47 75.48 9.07 10.56 7.45 9.07 10.31 13.66 13. 76.30 11.80 MT Wild Avgsub Avg 6. 7.06 6.64 7.03 6.84 6.83 6.95 6.76 7.32 6.84 6.57 6.91 6.96 7.03 6.84 7.17 7.04 7.18 7.33 7.52 7.28 7.39 7. 7.54 -24.66 38.40 53.59 -22.15 -32.10 -20.20 -20.70 -19.95 -20.67 -20.56 39.36 35.69 40.83 38.21 39.50 40.57 41. 51.81 47.44 51.08 51.17 52.83 53.10 53.12 -17.77 42.99 55.32 -25.20 -36.17 -21.66 -20.51 -17.82 -17.74 37.21 34.32 38.82 39.39 41.11 41. 48.51 41.37 49.88 47.69 46.58 48.84 -18.39 42.05 51.73 -23.98 39. 57.74 -18.08 -27.83 -18.80 -19.71 -20.77 -20.46 41.11 38.94 41.62 41.09 42.39 41.52 57.32 55.87 58.48 58.11 58.43 58.50 -14.49 43. 59.81 Llama3.1-8B Mistral-7B-v0.3 Qwen2.5-7B Pool 939K 69.15 63.88 83.40 63.41 Random ZIP IFD #InsTag DEITA CaR QDIT MIG Random ZIP #InsTag DEITA CaR QDIT MIG Pool Random ZIP #InsTag DEITA CaR QDIT MIG 50K 50K 50K 50K 50K 50K 50K 50K 50K 50K 50K 50K 50K 50K 50K 74.24 77.63 75.93 72.54 78.98 78.98 79.66 64.80 63.00 63.56 64.80 66.11 69.04 65.42 70.36 52.54 61.03 69.83 74.07 71.42 70. 51.22 35.98 49.39 48.17 49.39 52.44 53.05 80.00 66.39 72.02 57.93 67.80 72.88 76.27 75.93 64.41 54. 56.90 56.73 57.15 57.72 58.65 58.68 66.34 33.21 66.34 64.82 63.76 59.97 42.07 3.05 40.85 11.59 9.15 42.68 75.25 56.19 66. 45.12 939K 90.51 65.01 85.29 78. 50K 50K 50K 50K 50K 50K 50K 85.42 85.76 88.81 89.15 91.86 89.83 63.87 63.43 63.03 63.22 65.60 69.34 80.74 83.24 84.61 86.13 87.64 87.04 79.27 72.56 81.10 79.27 77.44 81. 90.51 67.39 84.46 79.88 65.77 63.86 65.00 64.39 63.50 64.00 65.15 65. 64.44 60.34 61.68 61.80 61.41 61.95 62.46 60.23 75.15 73.81 73.60 73.50 74.27 73.97 74.72 73. 67.10 61.00 61.00 53.60 65.99 64.33 56.75 57.30 65.06 65.43 63.03 63.22 64.51 55.64 58.23 64.70 64. 58.04 58.23 61.00 58.78 50.28 50.83 61.74 Table 2: Results on different data pools, Openhermes2.5 and Xsota, based on Llama3.1-8B. MIG outperforms all baselines across both data pools. Please refer to Table 5 6 in Appx. for detailed scores on all benchmarks. Openhermes2.5 Xsota Data Size Avgsub Avgobj Avg Data Size Avgsub Avgobj Avg Pool Random #InsTag DEITA CaR QDIT MIG 1M 50K 50K 50K 50K 50K 50K 36.91 61.49 49.20 306K 31.51 52. 42.19 32.99 36.23 36.80 37.51 37.90 55.69 54.12 57.36 55.57 57.71 44.34 45.17 47.08 46.54 47.80 38.12 58. 48.21 6K 6K 6K 6K 6K 6K 29.94 31.89 31.60 31.86 32.52 49.69 46.19 48.70 48.43 49.10 39.81 39.04 40.15 40.15 40. 32.98 50.63 41.80 method, by +2.20% on overall Avg score. Notably, the model trained on 5% data sampled by MIG outperforms the model trained on the full Tulu3 pool by +4.59% on human-preference benchmarks while maintaining comparable knowledge-based performance. Additionally, MIG significantly outperforms embedding-based methods in sampling efficiency due to reduced computational overhead. Please refer to Table 4 in Appx. for detailed sampling times and efficiency analysis. Transferability on Models. Table 1 presents results for Mistral-7B and Qwen2.5-7B. MIG consistently surpasses baselines with Avg improvements of +1.85% and +1.31%, respectively, demonstrating its robustness. Notably, the second-best selection method varies among different base models, further demonstrating the generalizability of MIG. Transferability on Data Pools. Table 2 presents results on different data pools with varying sizes Figure 3: Data scaling experiments on Tulu3 using Llama3.18B. The score reported here is the Avg score. and quality. MIG consistently outperforms all baselines, achieving Avg improvements of +0.41% and +0.99% over previous best methods, further demonstrating its generalizability. Notably, on Xsota, all baselines exhibit performance degradation on knowledge-based evaluations, consistent with the findings in (Xia et al., 2024b). We hypothesize that quality metrics, such as DEITA scores and tag counts, are biased toward multi-round, long samples that enhance subjective chat abilities. However, samples in specific domains, such as math and code, are typically single-turn. MIG mitigates this bias by effectively balancing quality and diversity. Data Scaling. We compare MIG with baseline methods across varying data budgets on the Tulu3 pool, using Llama3.1-8B as the base model. As 6 Figure 4: (a) Derivative of Information Score Functions. (b) Avgobj on Different Information Score Functions. (c) Avgsub on Different Quality Scores. on any given label converges quickly. Fig. 4(b)(c) present the performance on different evaluations, with Φ(x) = x0.8 achieving the best results on human-preference and knowledge-based benchmarks, effectively balancing quality and diversity. Quality Metrics. We implement three alternative quality measurement approaches: the number of tags (Lu et al., 2024), the IFD score (Li et al., 2024b), and the DEITA score (Liu et al., 2024b), to investigate their impact on information measurement. Fig. 5 compares these three quality metrics with baseline score that assigns constant value to all samples. The DEITA scores consistently outperform the other quality metrics in both evaluation settings. Therefore, we adopt the DEITA scores as the default quality measurements for MIG. Label Graph. An essential question in MIG is how to determine an appropriate label graph, including its nodes (label set) and edges (label relationships). Increasing the number of nodes leads to more granular label set, thereby providing broader coverage of knowledge topics. However, excessively large label sets inevitably include outliers or lowquality labels. Similarly, increasing edge density between labels enhances the comprehensiveness of label relationships, but overly dense graphs may result in computational inefficiencies and noise from the embedding model. There is no universally optimal solution, as the ideal label graph depends on the characteristics of the data pool and potentially other parameters in MIG. To explore the relationship between the label graph and the downstream performance of trained models, we conduct an empirical experiment on the Tulu3 pool. Fig. 6(a) shows the downstream performance from set of node counts in the label graph, ranging from 839 to 6738, while Fig. 6(b) presents performance across varying edge densities, with thresholds between 0.8 and 0.94. The observed trends align with our Figure 5: Quantitative results on different quality metrics. DEITA scores achieve the best performance on both humanpreference and knowledge-based evaluations. shown in Fig 3, MIG consistently delivers superior performance at each data budget, demonstrating its robust scalability. Remarkably, MIG achieves comparable performance to the full dataset with only 20K samples, underscoring its efficiency. The observed initial increase and subsequent plateau in performance align closely with findings from previous works (Li et al., 2024b; Liu et al., 2024b), highlighting the importance of data selection."
        },
        {
            "title": "4.3 Analysis",
            "content": "Information Score Function Φ. The information score function Φ is crucial in MIG sampling as it balances quality and diversity. Based on the principles outlined in Sec. 3.2, Φ is expected to be monotonically increasing with diminishing rate of increase. In our experiments, we evaluate two candidate functions: Φ(x) = 1 eαx (α > 0) Φ(x) = xαx (0 < α < 1) (12) (13) Fig. 4(a) compares the decreasing rate in the derivative of these functions under varying parameter settings. Functions that decay rapidly tend to favor diverse label distributions as the information 7 Figure 6: Analysis of Parameters in the Label Graph. The reported score is the average of Avgsub and Avgobj. Please refer to Table 7 8 9 in Appx. for detailed scores on all evaluated benchmarks. (a) Comparison of various node counts (label set size) in the label graph. (b) Comparison of different edge thresholds, with lower threshold indicating dense graph. (c) Comparison of different propagation weights, where smaller weight corresponds to weak propagation. Table 3: Grid search of appropriate data size and training epochs on the Tulu3 pool. We report the AVG score here."
        },
        {
            "title": "MIG",
            "content": "Epoch2 Epoch3 Epoch4 Epoch2 Epoch3 Epoch4 10K 20K 50K 46.76 48.23 49.78 49.42 50.36 51.81 50.39 51.08 50.68 49.13 51.18 52. 52.36 53.71 55.32 51.11 53.14 55.14 initial analysis, showing an unimodal performance curve in both experiments. For the Tulu3 pool, the optimal label graph is achieved with label set size of 4531 and an edge similarity threshold of 0.9. Information Propagation. We conduct series of experiments to study the impact of information propagation intensity in MIG sampling. Appropriate information propagation results in accurate information distribution in the semantic space. Specifically, we experiment with various values of α in Eq. 6, where α is proportional to the intensity of information propagation. Fig 6(c) shows that α = 1.0 yields the best performance, with Avg improvement of 2.76 over the non-propagation. It indicates that information propagation effectively improves the accuracy of information measurement on the label graph. Grid Search. To identify an appropriate data bucket and training epoch on the Tulu3 pool for the main comparison, we perform grid search. Results in Table 3 indicate 50K samples with three training epochs as optimal for Tulu3, consistently maximizing performance for MIG and random selection."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose novel method for measuring instruction-tuning datasets in semantic space. We model the semantic space as label graph and jointly evaluate data quality and diversity. We introduce an upper-convex information score function to balance quality and diversity, and propose information propagation to capture the information distribution accurately Building on the submodularity of such measurement, we propose MIG, an efficient sampling algorithm that iteratively selects samples to maximize the information gain on the label graph. Extensive experiments across diverse data pools and base models validate the effectiveness and generalizability of MIG. Our research bridges the gap between instance-level quality assessment and global dataset-level evaluation, offering unified approach to dataset measurement. We hope our results can inspire dataset measurement-guided data selection in the future. Limitation. Currently, the parameters in MIG are static and depend on grid search to identify the optimal values, which can not be extensively explored. Future work could focus on developing methods to automatically determine the parameters in MIG, such as customizing the information score function for each label, to enhance the flexibility and scalability of MIG."
        },
        {
            "title": "References",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NIPS. Alexander Bukharin, Shiyang Li, Zhengyang Wang, Jingfeng Yang, Bing Yin, Xian Li, Chao Zhang, Tuo 8 Zhao, and Haoming Jiang. 2024. Data diversity matters for robust instruction tuning. In EMNLP. Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024a. Compassjudger-1: All-in-one judge model helps arXiv preprint model evaluation and evolution. arXiv:2410.16256. Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. 2024b. Instruction mining: Instruction data selection for tuning large language models. In COLM. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2024. Alpagasus: Training better alpaca with fewer data. In ICLR. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Gérard Cornuéjols, George Nemhauser, and Laurence Wolsey. 1983. The uncapicitated facility location problem. Technical report, Cornell University Operations Research and Industrial Engineering. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How abilities in large language models are affected by supervised fine-tuning data composition. In ACL. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Mahong Xia, Zhang Li, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and JingBo Zhu. 2024. Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation. In EMNLP. Michael Hahsler, Matthew Piekenbrock, and Derek Doran. 2019. dbscan: Fast density-based clustering with r. Journal of Statistical Software. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In ICLR. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2024. Tülu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. CAMEL: Communicative agents for mind exploration of large language model society. In NIPS. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024a. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. In ACL. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024b. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In NAACL. Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et al. 2023b. One shot learning as instruction data prospector for large language models. arXiv preprint arXiv:2312.10302. Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. 2024. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. arXiv preprint arXiv:2406.04770. Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, and Min Zhang. 2024a. SelectIT: Selective instruction tuning for LLMs via uncertainty-aware self-reflection. In NIPS. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024b. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In ICLR. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2024. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In ICLR. Michel Minoux. 2005. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques: Proceedings of the 8th IFIP Conference on Optimization Techniques Würzburg, September 59, 1977. George Nemhauser, Laurence Wolsey, and Marshall Fisher. 1978. An analysis of approximations for maximizing submodular set functionsi. Mathematical programming. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In EMNLP. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Teknium. 2023. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023a. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023b. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Peiqi Wang, Yikang Shen, Zhen Guo, Matthew Stallone, Yoon Kim, Polina Golland, and Rameswar Panda. 2024. Diversity measurement and subset selection for instruction tuning datasets. arXiv preprint arXiv:2402.02318. Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, and Chang Zhou. 2023. Self-evolved diverse data sampling for efficient instruction tuning. arXiv preprint arXiv:2311.08182. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024a. LESS: Selecting influential data for targeted instruction tuning. In ICML. Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, and Junyang Lin. 2024b. Rethinking data selection at scale: Random searXiv preprint lection is almost all you need. arXiv:2410.09335. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. 2024. Entropy Law: The Story Behind Data Compression and LLM Performance. arXiv preprint arXiv:2407.06645. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024a. Metamath: Bootstrap your own mathematical questions for large language models. In ICLR. Simon Yu, Liangyu Chen, Sara Ahmadian, and Marzieh Fadaee. 2024b. Diversify and conquer: Diversitycentric data selection with iterative refinement. arXiv preprint arXiv:2409.11378. Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Minghao Li, Fei Huang, Nevin L. Zhang, and Yongbin Li. 2024. Tree-instruct: preliminary study of the intrinsic relationship between complexity and alignment. In COLING. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In NIPS. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. 2024. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In ACL. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. Lima: Less is more for alignment. In NIPS. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "A Details of Experiments Setup",
            "content": "A."
        },
        {
            "title": "Implementation Details",
            "content": "A.1 Baseline Settings The specific baseline settings in our experiments are as follows: Random. fixed random seed of 42 is used to ensure reproducibility. IFD (Li et al., 2024b). We follow the setting from (Li et al., 2024a) to directly compute IFD scores using base LLMs for efficiency. ZIP (Yin et al., 2024). The default setting is applied across all data pools. #InsTag (Lu et al., 2024). The open-released InsTagger is used to tag data from the pool, followed by tag normalization, which includes frequency filtering and semantic aggregation. The frequency threshold is set to 2, as InsTagger is fully trained on valid normalized tags, resulting in 9471 tags. For semantic aggregation, we use E5-Mistral-7BInstruct (Wang et al., 2023b) to generate tag embeddings, and the DBSCAN algorithm (Hahsler et al., 2019) is applied with semantic similarity threshold of 0.05, yielding 6738 tags. DEITA (Liu et al., 2024b). For Xsota, we use the released sampled dataset. For the Openhermes2.5 (Teknium, 2023) and Tulu3 (Lambert et al., 2024) pools, we reproduce its method. Quality assessment is conducted using the released quality and complexity scorers. For Repr Filter, we utilize Llama3.1-8B to obtain instance embeddings. threshold of 0.9 is applied to the Tulu3 pool, and 0.95 is used for the Openhermes2.5 pool, as the 0.9 threshold results in insufficient samples for the latter. CaR (Ge et al., 2024). We follow the default setting, using all-mpnet-base-v2 (Reimers and Gurevych, 2019) to obtain data embeddings, PCA to retain 95% of dimensions, and k-Means clustering with the number of clusters as = (cid:112)n/2. For ranking, we use the released IQS model. We maintain the original ratio between n1 and kn2 for different pools. QDIT (Bukharin et al., 2024). Embedding computation follows the default setting with all-mpnetbase-v2. Quality assessment uses the DEITA scores instead of ChatGPT. The hyperparameter α for balancing quality and diversity is set to 0.9 for Xsota and 0.7 for the Tulu3 and Openhermes2.5 pools. The label set in MIG follows #InsTag (Lu et al., 2024), with variations in the label graph across different data pools. Specifically, 3059 tags are used for Xsota, 4531 for Tulu3, and 5166 for Openhermes2.5, with label set size positively correlated to pool size. E5-Mistral-7B-Instruct is used as the embedding model to compute label similarity, with threshold set to 0.9. Quality assessment in MIG uses the DEITA scores. The information score function is set to ϕ(x) = x0.8, and the information propagation weight α is set to 1. A.3 Training Recipes For experiments on Xsota, we follow the default settings from (Liu et al., 2024b), using batch size of 128, learning rate of 2e-5, warm ratio of 0.1, and maximum input length of 2048. For the Tulu3 pool, we adopt the settings from (Lambert et al., 2024), with batch size of 128, learning rate of 5e-6, warm ratio of 0.03, and maximum input length of 4096. For the Openhermes2.5 pool, we follow the settings from (Xia et al., 2024b), setting the batch size to 128, learning rates to 7e-6, warm ratio of 0.01, and maximum input length of 4096. A.4 Evaluation Setup The evaluation of our experiments is implemented using OpenCompass (Contributors, 2023) with greedy inference to ensure uniform evaluation across all models. Human-preference Evaluations. We use the open-source CompassJudger-1-32B (Cao et al., 2024a) for human-preference evaluation. As different benchmarks use different scoring metrics and ranges, we normalize the scores according to the following mapping: AlpacaEvalv2 (Dubois et al., 2024): The score range is 0-100, requiring no special adjustment. MTBench (Zheng et al., 2023): The score range is 0-10, which is mapped by multiplying by 10. WildBench (Lin et al., 2024): The score range is -100-100, which is normalized by adding 100 and dividing by 2. Knowledge-based Evaluations. We conduct evaluations using the following settings: three-shot evaluation on BBH (Suzgun et al., 2022), five-shot evaluation on MMLU (Hendrycks et al., 2021), and zero-shot evaluation on ARC (Clark et al., 2018) and GSM8K (Cobbe et al., 2021). For Hu12 Thus, by the concavity property of ϕ: ϕ(zk(D) + k(e)) ϕ(zk(D)) ϕ(zk(T ) + k(e)) ϕ(zk(T )) (18) Summing over all components k, we get: (D, e) (T, e) (19) Thus, E(D e) E(D) E(T e) E(T ), which satisfies the definition of submodularity. C.2 Cardinality-Constrained Submodular"
        },
        {
            "title": "Maximization",
            "content": "Given that E(D) is submodular, our data selection task, defined in Eq. 1, constitutes cardinalityconstrained submodular maximization problem that is NP-complete. However, greedy algorithm provides well-established approximation guarantee, ensuring that E(Dgreedy) (1 1 )E(D), where represents the optimal solution (Nemhauser et al., 1978). Assuming = , this guarantee is the best achievable for polynomialtime algorithms."
        },
        {
            "title": "D Detailed Results on Benchmarks",
            "content": "We provide detailed scores on full benchmarks in Table 5 6. Table 4: Efficiency comparison of different methods for 50K sampling from the Tulu3 pool, with timing measured on single NVIDIA-L20Y."
        },
        {
            "title": "GPU Time",
            "content": "Random ZIP (Yin et al., 2024) IFD (Li et al., 2024b) #InsTag (Lu et al., 2024) DEITA (Liu et al., 2024b) QDIT (Bukharin et al., 2024) CaR (Ge et al., 2024) MIG 0.09 53.99 0.05 2.33 81.56 86.17 0.85 0. manEval (Chen et al., 2021), we report pass@1 results, and for IFEval (Zhou et al., 2023b), we provide strictly followed scores."
        },
        {
            "title": "B Efficiency Analysis",
            "content": "Table 4 presents the time used for 50K sampling on the Tulu3 pool. Among methods that balance quality and diversity, MIG demonstrates the highest efficiency. Notably, MIG outperforms QDIT (Bukharin et al., 2024) and DEITA (Liu et al., 2024b) significantly, as it eliminates the need for iterative pairwise similarity computations in the embedding space."
        },
        {
            "title": "Strategy in MIG",
            "content": "C.1 Submodularity Analysis We first prove that our dataset measurement function E(D), defined in Eq. 8, is submodular. Specifically, for all , and for all elements / , the following inequality holds: E(D e) E(D) E(T e) E(T ) (14) Proof. Let z(D) = (cid:80) gain from adding an element to is: iD sivi. The marginal (D, e) = (cid:88) [ϕ(zk(D) + k(e)) ϕ(zk(D))] (15) k=1 where k(e) = se(Ave)k 0 represents the incremental contribution of to the k-th component. Since ϕ is monotonically increasing and concave, for any δ 0 and z, we have: ϕ(z + δ) ϕ(z) ϕ(z + δ) ϕ(z) (16) Given , we have: zk(T ) = zk(D) + (cid:88) iT si(Avi)k zk(D) (17) 13 Table 5: Full Results on the Openhermes2.5 Pool."
        },
        {
            "title": "ARC",
            "content": "BBH GSM8K HumanEval MMLU IFEval Avgobj AlpacaEval MTbench Wildbench Avgsub"
        },
        {
            "title": "Pool",
            "content": "72.88 60.53 Random 75.25 70.85 InsTag 69.83 DEITA 62.71 CaR 66.44 QDIT 60.20 68.64 61.85 63.73 62."
        },
        {
            "title": "MIG",
            "content": "78.98 63.33 70.51 51.40 56.25 60.96 55.42 58.61 51.55 51. 50.00 43.90 46.95 44.51 50.00 45.73 64.99 51.23 45.70 58.01 64.37 63.64 63.81 48. 46.03 49.35 46.58 42.70 45.10 46.40 61.49 55.69 54.12 57.36 55.57 57.71 58.30 5. 4.72 5.09 7.83 7.33 9.19 7.83 7.10 6.63 7.14 6.94 7.09 6.99 7.17 -31. -44.12 -35.60 -33.69 -31.43 -30.78 -30.34 36.91 49.20 32.99 36.23 36.80 37.51 37.90 44.34 45.17 47.08 46.54 47. 38.12 48.21 Table 6: Full Results on the Xsota Pool."
        },
        {
            "title": "ARC",
            "content": "BBH GSM8K HumanEval MMLU IFEval Avgobj AlpacaEval MTbench Wildbench Avgsub"
        },
        {
            "title": "Pool",
            "content": "73.22 54.12 Random 61.02 64.07 InsTag 71.86 DEITA 72.88 CaR QDIT 71.53 58.12 51.82 50.82 48.90 51."
        },
        {
            "title": "MIG",
            "content": "74.58 51.93 40.49 32.07 36.62 27.67 20.92 29.95 31.54 45. 42.69 28.66 40.24 46.95 41.46 43.90 61.05 62.31 55.11 63.36 62.68 63.22 62.24 43. 41.96 40.85 38.26 38.26 36.97 39.56 52.88 49.69 46.19 48.70 48.43 49.10 50.63 3. 3.60 5.22 4.22 5.22 5.09 5.34 6.78 6.34 6.56 6.48 6.51 6.55 6.72 -54. -54.39 -50.28 -48.44 -49.46 -46.05 -47.18 31.51 42.19 29.94 31.89 31.60 31.86 32.52 39.81 39.04 40.15 40.15 40. 32.98 41.80 Table 7: Full Results across Different Node Numbers. Node Number ARC BBH GSM8K HumanEval MMLU IFEval Avgobj AlpacaEval MTbench Wildbench Avgsub 839 1629 3070 4531 5683 73.90 76.27 78.31 80.00 83.73 76.27 66.35 66.22 66.80 66.39 66.47 65.92 73.31 72.18 72.25 72.02 72.55 70.13 52.44 56.71 55.49 57.93 55.49 52.44 64.42 64.71 65.03 64.44 64.22 64.78 61.18 58.96 63.22 65.06 64.14 64. 65.27 65.84 66.85 67.64 67.77 65.61 16.02 15.16 14.04 14.66 12.55 11.30 7.18 7.40 7.25 7.32 7.35 7.36 -19.59 -18.15 -17.63 -17.77 -20.15 -19.87 42.67 43.36 42.58 42.99 41.99 41.65 Table 8: Full Results across Different Edge Densities. Node Number ARC BBH GSM8K HumanEval MMLU IFEval Avgobj AlpacaEval MTbench Wildbench Avgsub 0.8 0.86 0.88 0.9 0.92 0.94 77.97 80.68 76.95 80.00 83.73 78.31 65.69 65.72 67.77 66.39 66.39 65.86 70.96 71.49 72.02 72.02 71.80 70. 54.88 55.49 57.93 57.93 58.54 56.10 64.59 64.15 64.99 64.44 64.35 64.81 63.96 62.85 62.85 65.06 65.43 64.14 66.34 66.73 67.09 67.64 68.37 66.60 11.3 14.04 13.42 14.66 12.55 10.68 7.24 7.21 7.04 7.32 6.97 7. -21.45 -19.77 -20.17 -17.77 -19.29 -19.42 40.99 42.08 41.25 42.99 40.87 40.82 Table 9: Full Results across Different Propagation Weights. Node Number ARC BBH GSM8K HumanEval MMLU IFEval Avgobj AlpacaEval MTbench Wildbench Avgsub 0 0.6 0.8 1.0 1.2 2. 74.24 80.68 78.98 80.00 80.00 81.36 65.39 66.78 66.46 66.39 67.42 67.03 71.04 72.33 73.54 72.02 73.39 70.36 49.39 54.88 54.88 57.93 52.44 54.88 64.62 64.66 64.83 64.44 64.89 64.86 63.59 63.40 64.14 65.06 64.14 63. 64.71 67.12 67.14 67.64 67.04 67.08 10.81 13.29 14.04 14.66 12.42 13.54 7.09 7.13 7.14 7.32 7.20 7.06 -21.01 -19.87 -17.71 -17.77 -17.72 -20.49 40.40 41.55 42.19 42.99 41.85 41.30 AVG 53.97 54.60 54.71 55.32 54.88 53.63 AVG 53.67 54.41 54.17 55.32 54.62 53.71 AVG 52.56 54.34 54.67 55.32 54.45 54."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Fudan University",
        "Shanghai AI Laboratory"
    ]
}