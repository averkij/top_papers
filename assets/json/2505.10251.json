{
    "paper_title": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language Conditioned Imitation Learning",
    "authors": [
        "Ji Woong Kim",
        "Juo-Tung Chen",
        "Pascal Hansen",
        "Lucy X. Shi",
        "Antony Goldenberg",
        "Samuel Schmidgall",
        "Paul Maria Scheikl",
        "Anton Deguet",
        "Brandon M. White",
        "De Ru Tsai",
        "Richard Cha",
        "Jeffrey Jopling",
        "Chelsea Finn",
        "Axel Krieger"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Research on autonomous surgery has largely focused on simple task automation in controlled environments. However, real-world surgical applications demand dexterous manipulation over extended durations and generalization to the inherent variability of human tissue. These challenges remain difficult to address using existing logic-based or conventional end-to-end learning approaches. To address this gap, we propose a hierarchical framework for performing dexterous, long-horizon surgical steps. Our approach utilizes a high-level policy for task planning and a low-level policy for generating robot trajectories. The high-level planner plans in language space, generating task-level or corrective instructions that guide the robot through the long-horizon steps and correct for the low-level policy's errors. We validate our framework through ex vivo experiments on cholecystectomy, a commonly-practiced minimally invasive procedure, and conduct ablation studies to evaluate key components of the system. Our method achieves a 100\\% success rate across eight unseen ex vivo gallbladders, operating fully autonomously without human intervention. This work demonstrates step-level autonomy in a surgical procedure, marking a milestone toward clinical deployment of autonomous surgical systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 3 1 5 2 0 1 . 5 0 5 2 : r 2025-7-9 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Ji Woong (Brian) Kim1,2, Juo-Tung Chen1, Pascal Hansen1, Lucy X. Shi2, Antony Goldenberg1, Samuel Schmidgall1, Paul Maria Scheikl1, Anton Deguet1, Brandon M. White1, De Ru Tsai3, Richard Cha3, Jeffrey Jopling1, Chelsea Finn2 and Axel Krieger1 1Johns Hopkins University, 2Stanford University, 3Optosurgical Research on autonomous surgery has largely focused on simple task automation in controlled environments. However, real-world surgical applications demand dexterous manipulation over extended durations and generalization to the inherent variability of human tissue. These challenges remain difficult to address using existing logic-based or conventional end-to-end learning approaches. To address this gap, we propose hierarchical framework for performing dexterous, long-horizon surgical steps. Our approach utilizes high-level policy for task planning and low-level policy for generating robot trajectories. The high-level planner plans in language space, generating task-level or corrective instructions that guide the robot through the long-horizon steps and correct for the low-level policys errors. We validate our framework through ex vivo experiments on cholecystectomy, commonly-practiced minimally invasive procedure, and conduct ablation studies to evaluate key components of the system. Our method achieves 100% success rate across eight unseen ex vivo gallbladders, operating fully autonomously without human intervention. This work demonstrates step-level autonomy in surgical procedure, marking milestone toward clinical deployment of autonomous surgical systems. https://h-surgical-robot-transformer.github.io/ 1. Introduction Autonomous surgery offer the potential to improve surgical outcomes, reduce costs, and expand access to high-quality care. However, most surgical robots today remain teleoperated due to fundamental challenges. From vision perspective, surgical scenes are highly complex, involving morphological variation between patients, constant environmental changes during interventions, and visual occlusions such as blood and smoke from cautery tools. Motion planning in this setting is difficult, because of the partial observability of organs and their unpredictable dynamics. Additionally, surgical tasks must be performed with high precision and safety, making the development of these systems very challenging. Prior works have addressed surgical autonomy through various strategies in simulation [49, 61, 62] and real-world settings [15, 27, 29, 46, 59]. Various studies explored tabletop tasks such as peg transfer, needle pickup, and deformable object manipulation, using model-based strategies [2, 15, 21, 24, 29], reinforcement learning [8, 19, 38, 50, 59, 61], and imitation learning [41, 51, 55, 56, 58]. In particular, learning-based methods show promise in tackling challenging contact-rich manipulation tasks [52], such as suture knot-tying [26], which are otherwise difficult to solve with model-based strategies. Although promising, most learning-based works were demonstrated in controlled environments and have not been extended to realistic in-vivo or ex-vivo settings. Therefore, whether these strategies will succeed in the complex and diverse environment of surgery remains uncertain. Corresponding author(s): Ji Woong (Brian) Kim (jwbkim@stanford.edu) SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning On the other hand, there have been notable in-vivo autonomous demonstrations such as needle steering [27] and anastomosis tasks [46]. Although promising, these studies primarily tackled the navigation steps of the procedure, which is much simpler than manipulation, and relied on handcrafted strategies that were specifically optimized for single application. In-vivo studies demonstrate the promise of robotics being deployed in clinically relevant environments, however, the applied strategies are unlikely to generalize, scale, or address complex manipulation problems which are very common in surgery. In this work, we aimed to move beyond the scope of prior approaches by addressing several critical and previously unaddressed dimensions of surgical autonomy. First, we focus on contact-rich manipulation tasks that require diverse tool use, including grabbing, clipping, and cutting. Second, we conduct this work in realistic ex-vivo setting with significant variability in tissue appearance, anatomy, and morphology across organs, mirroring the diversity encountered in human surgeries. Third, rather than tackling individual skills, we tackle entire surgical steps that unfold over several minutes and require persistent coordination and decision-making. The combination of these challenges has been unexplored in prior work and is non-trivial to solve using conventional approaches. Our goal is to show that these challenges can be overcome with unified design using data-drive methods. Solving this challenge in such generalizable way is essential for progressing toward clinically viable and general-purpose autonomous systems. Movie 1: comprehensive summary of our work. Using cholecystectomy as case study, our framework automates key steps in gallbladder removal, focusing on the complex process of clipping and cutting the cystic duct and artery. The system performs 17 tasks fully autonomously, achieving successful results in all eight ex-vivo studies without human intervention. Robustness is demonstrated through challenging scenarios and appearance variations, where the model adapts and executes tasks confidently, highlighting its potential for generalizing across surgical settings. Towards this end, we present Hierarchical Surgical Robot Transformer (SRT-H), framework for autonomous, step-level autonomy in surgery (Movie 1). SRT-H uses hierarchical architecture composed of high-level (HL) policy that issues natural language instructions, including task and corrective instructions, and low-level (LL) policy that executes low-level trajectories. This structure allows us to decompose complex procedures into shorter tasks and enable the HL policy to correct 2 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure 1 System and task overview. (A) We use the da Vinci Research Kit (dVRK) Si to deploy our policy, which includes an endoscope and two additional wrist cameras mounted for better view of the interactions between instruments and tissue. (B) The autonomous surgical steps include clipping and cutting the gallbladders artery and duct. (C) The before and after pictures illustrate the objective of this procedure; the duct and artery are completely severed, without spilling any of their internal fluids thanks to the use of clips. SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning mistakes made by the LL policy, which will naturally arise during long-horizon steps. Furthermore, using language enables an intuitive interface for intermittent user intervention and fine-tuning. Specifically, users can temporarily override HL decisions with natural language instructions, and these interventions are stored and used for continual learning via DAgger-style loop [45]. SRT-H is built on transformer-based architecture and trained end-to-end via imitation learning, using only red, green, blue (RGB) images paired with language annotations. It avoids reliance on depth sensors, segmentation modules, or specialized hardware. We evaluate SRT-H on the clippingand-cutting step of cholecystectomy, common laparoscopic procedure performed over 700,000 times annually in the United States [1]. This step involves identifying the cystic duct and artery, placing clips, and severing them. By disabling the clip latching mechanism, we enable collection of hundreds of demonstrations from single porcine tissue, making large-scale data collection feasible. In contrast, other steps like dissection are destructive and yield only one demonstration per specimen, motivating our focus on clipping and cutting steps of cholecystectomy. To train and evaluate our system, we collect 16,000 trajectories (approximately 17 hours of data) across 34 ex-vivo porcine gallbladders. We then test SRT-H on eight unseen gallbladders, and in each case, the system successfully completed all 17 required tasks autonomously, generalizing across anatomies and self-correcting its mistakes mid-procedure. Ablation studies highlight the critical role of both the hierarchical structure and the corrective language interface in enabling timely and effective corrective behaviors. Compared to an expert surgeon, our framework shows comparable performance, but requries longer execution time. In summary, SRT-H provides scalable and adaptable framework for autonomous surgery, with potential to advance toward generalizable autonomy in real-world surgical settings and further in vivo studies."
        },
        {
            "title": "RESULTS",
            "content": "In the following sections, we describe the design and workflow of our autonomous surgery system and then present the experiment results. We first evaluate our systems ability to complete the cholecystectomy procedures using eight unseen ex-vivo porcine tissues. The frameworks performance was evaluated based on the success rate, total time, and number of self-corrections made (see Core experiment results\" section). We further evaluated SRT-H against ablative variants to show the effect of different design choices on the performance of the framework. We evaluated these variants based on their success rate, total time, and ability to recover from failure states (see Comparison with variants\" section). The success rate of failure recoveries were evaluated by placing the instruments into failure states and observing whether each variant can recover to complete the procedure successfully. We also independently performed ablative comparisons for the high-level (HL) policy and quantified each design choices effect on its performance (see High-level policy ablative studies\" section). Lastly, we evaluated our framework against an expert surgeon based on the success rate, time to completion, and the smoothness of the trajectories (see Comparison with expert surgeon\" section)."
        },
        {
            "title": "Experiment design",
            "content": "Figure 1A shows the hardware configuration of our system, which consists of da Vinci Research Kit (dVRK) Si with wrist cameras mounted near the instrument tips. The stereo endoscope of the da Vinci Research Kit (dVRK) provides global view of the surgical scene, and the wrist cameras provide close-up view of interactions between instruments and tissue. Prior works [20, 26] demonstrated that wrist cameras can help with generalizing to different workspace heights and out-of-distribution scenarios due to the more consistent view provided by the wrist cameras. Though the size of the wrist cameras used in this study are quite large and perhaps not clinically practical for minimally invasive 4 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure 2 Model overview and architecture. (A) The architecture of our framework consists of high-level policy that generates language instructions given the image observations, and low-level policy that conditions on the language instructions and image observations to generate robot motions in Cartesian space. (B) On more granular level, the high-level policy consists of Swin-T model to encode the visual observations into tokens, that are processed by Transformer Decoder to generate language instructions. The language instructions are processed by pretrained and frozen distilled bidirectional encoder representations from transformers (DistilBERT) model to generate language embeddings. The image observations are passed to an EfficientNet that conditions on the language embeddings through feature-wise linear modulation (FiLM) layers. The combined embeddings are passed to Transformer Decoder to generate sequence of actions that are encoded in delta position and orientation values. 5 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning surgery, their design can be further downsized. In the following, we describe the general workflow of the procedure within cholecystectomy that is automated, its challenges, and the steps for deploying SRT-H. The steps for clipping and cutting the duct and artery are shown in Fig. 1B. The objective of this step is as follows: three clips are added to the left tubular structure (typically the duct) and then three clips are added to the right tubular structure (typically the artery). For each tube, the first two clips are placed proximally near the bottom and the third clip distally at the top. Note that the clips prevent any leakage of biological fluids after the gallbladder is removed; in particular, the two clips placed at the base remain in the patient and must therefore provide secure, long-lasting seal. Then, the tube is transected between the second and third clip of each tube, where there is the most gap for the scissors to enter. In general, the duct and artery are in close proximity, therefore, the left gripper must apply tension at the neck of the gallbladder to stretch the tubes apart and make room for the clip applier or the scissor to enter the gap. After each clip is applied, an assistant on standby near the dVRK loads another clip and also performs tool changes between clip applier and scissors after completion of the relevant steps (filling the role of surgical nurse). There are several challenging elements to this procedure. From visual and anatomical point of view, the appearance of the ducts and arteries vary greatly between patients in terms of their diameter, length, proximity, angle from each other, and the amount of connective tissue left on the surface of the tubes, which can make perception challenging [16]. From manipulation point of view, precise bimanual coordination of the arms is necessary. In particular, when adding clips to the left tube, the left gripper must grab the neck of the gallbladder head and stretch it to make sufficient space between the duct and the artery, and the clip applier must pry in between the tight space between the tubes to successfully apply the clip [34]. During this step, the clip applier can overshoot and miss the duct entirely, mistakenly clip the right tube (artery), or apply the clips at suboptimal location e.g., applying the third clip too close to the second clip so as to leave no space for the scissors to perform the cut. Overall, to succeed in these steps, the policy must perceive and track the location of the deformable duct and artery, keep an internal count of how many clips have been applied so far, detect whether sufficient stretch has been applied to make room for prying in the clip applier tool, and apply the clips at an optimal location without damaging the surrounding tissues. During the autonomous trials when SRT-H is deployed, the operator clicks button on the graphical user interface (GUI) to initiate the system. After the system autonomously applies each clip, the system automatically pauses on its own and waits for the operator to load another clip. The operator then loads another clip and the procedure is resumed. This interaction is repeated for all six clips that are applied to the duct and artery. Between the clip-applying steps, when scissor is required, similar steps are carried out; the robot autonomously requests for tool change, and the operator resumes the procedure after making the tool change. The architectural details of SRT-H are shown in Fig. 2. Briefly, SRT-H is implemented as two transformer decoders, one is part of the HL policy and the other of the low-level (LL) policy. The HL policy takes in history of endoscope images as input and generates three outputs, which includes the task instruction, corrective instructions, and correction flag (boolean). Either the task or corrective instruction is provided as input to the LL policy, with the correction flag serving as binary switch that determines which instruction is sent to the LL policy. The LL policy then takes the given instruction, along with the current observations of the surgical scene, to generate hybrid-relative trajectory [26], the action representation optimized for training on dVRK robots. SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure 3 Core experiment sequences. Images of the initial and final states, as well as observations of the clip positions for the duct and artery before the cut is made for all eight gallbladders. The clips are sufficiently secured around the ducts and arteries, and sufficient space between the second and third clips of each tube is left for the scissors to make the cuts. The individual gallbladders vary noticeably in color, texture, and anatomy. 7 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Table 1 Core Experiment Metrics. Procedures were performed on n=8 ex-vivo porcine gallbladder tissues; the metrics include success rates, total duration, and number of self-corrections over all tasks of the procedure. Success Rate (%) Duration (s) # SelfCorrections Gallbl. 1 Gallbl. 2 Gallbl. 3 Gallbl. 4 Gallbl. 5 Gallbl. 6 Gallbl. 7 Gallbl."
        },
        {
            "title": "Average",
            "content": "100 100 100 100 100 100 100 100 100 290 315 304 300 396 318 274 337 317 2 8 14 3 6 12 1"
        },
        {
            "title": "Core experiment results",
            "content": "For the core experiments, SRT-H was evaluated on eight different unseen gallbladders. Table 1 shows the result of each experiment including the success rate, total duration, and number of self-corrections made. We observe that SRT-H was able to complete all the procedures successfully without any human interventions, and on average completed the procedure within 317 seconds or 5 minutes and 17 seconds. This duration excludes the time of reloading the clips and making tool changes performed by the operator. Furthermore, when failure states were encountered, SRT-H was able to correct its own mistakes and complete the procedure successfully. On average, the self-corrections were made approximately six times throughout the entire procedure. We provide additional information about the individual self-corrections in Fig. S7. Figure 3 shows the placement of each clip before the artery and duct were cut in more detail. It can be observed that the clips fully encompassed the ducts and arteries, maintained close spacing between the bottom two clips on each tube, and left sufficient spacing between the second and third clip on each tube for easy access for the scissors to make the cut. Overall, across diverse tissues, SRT-H demonstrated consistent capability in recognizing the relevant tissue structures, maintaining reasonable pace, and recovering itself from its own failures to complete all cases successfully. In general, the upper-most clips were placed close to the gallbladder infundibulum, but at times, they may not have been positioned at the highest point. To alleviate such placement issues in the future, we may collect additional data where clips are positioned as far up as possible, allowing SRT-H to more accurately replicate ideal placement. Similarly, in some cases, the clips were placed quite low in the surgical field due to suboptimal demonstrations, and these issues could similarly be improved by collecting better demonstrations. Additionally, we encountered non-safety critical robot failure in one of the eight experiments that was not related to SRT-H, when the scissors broke and had to be replaced before continuing. In addition, the dVRK system had to be reinitialized three times during manual tool changes, also unrelated to SRT-H. Note that these issues arose because we were using the very first dVRK Si still undergoing development, and the hardware system was not yet perfected. These hardware-related issues have since been resolved. 8 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Subtask and Recovery Success Rate (n=3) Success Rate vs Amount of Training Data (n=3) ) % ( R c A 100 60 40"
        },
        {
            "title": "Subtask Recovery",
            "content": "0 . 0 0 1 0 . 0 0 0 . 0 0 1 8 . 7 8 . 7 7 0 . 5 7 . 6 6 0 . 0 3 . 3 3 3 . 3 end endto only task wrist no"
        },
        {
            "title": "DAgger\nHL",
            "content": "no (ours) SRT-H ) % ( a e S 100 80 60 40 0 100.0 77.8 66.7 33.3% 66.6% Amount of Training Data 100.0% ) ( T r C 250 200 150 100 50 Average Completion Time (n=3) Subtasks 1st clip 2nd clip cut"
        },
        {
            "title": "Recovery",
            "content": "grab top grab bottom clip caught overshoot 171.0 120.5 23 90 18 27 21 20 only task 66 44 44 47 47 wrist no 133.5 24 90 25 15"
        },
        {
            "title": "DAgger\nHL",
            "content": "no 93.0 29 46 23 21 29 13 (ours) SRT-H 249.5 90 65 90 65 90 end endto Figure 4 Comparisons against variants. (A) We compare the success rate of our method, SRTH, against various variants on subtasks and recovery scenarios for n=3 gallbladders. These three gallbladders are independent of the eight gallbladders used in the experiment. (B) Shows the success rate of SRT-H for n=3 gallbladders with respect to the amount of training data used. (C) Shows the average completion time over n=3 gallbladders for SRT-H and ablative variants. 9 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure 5 Recovering from failure states. We manually place the instruments into failure states to evaluate SRT-Hs ability to recover from disadvantageous states of the environment. Each row illustrates specific failure state and sequence of images that show how SRT-H recovers from it. 10 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning"
        },
        {
            "title": "Comparison with variants",
            "content": "We further evaluated SRT-H against several variants, including SRT-H trained with task instructions only (no corrective instructions), SRT-H trained without wrist cameras, SRT-Hs HL policy trained without additional Dataset Aggregation (DAgger) data (collected using expert language corrections during prior policy rollouts), and end-to-end architecture with only the LL policy. For all the tests, each variant was evaluated based on its success rate and total duration. To ensure fair comparison, all variants were evaluated using the same gallbladders and starting positions, with 90 maximum time limit set for completing each task. The full results of these evaluations are shown in Fig. 4. In terms of success rates (Fig. 4A), the results show that SRT-H scores the highest (100%) in both normal and recovery scenarios  (Fig. 5)  . SRT-H using task instructions was close second, as it also scored highest under normal scenarios (100%), however, due to lack of corrective vocabulary, its performance in recovery scenarios was lower (66.7%). Omitting wrist cameras also reduced the success rates in both scenarios (77.8% and 50% respectively), highlighting its importance in highly diverse ex-vivo scenarios beyond table-top settings. SRT-H without HL fine-tuning resulted in diminished performance (77.8% and 75% respectively), demonstrating the importance of using competent HL policy and the efficacy of fine-tuning the HL policy. The end-to-end policy variant scored the lowest in both scenarios (33.3%). In terms of total duration (Fig. 4C), results show that SRT-H performs the fastest on average for both normal and recovery scenarios. The other variants required more time due to making mistakes, which they could not recover from, or falling into repeating loops of retry behaviors. In general, however, the rate of motion for all variants was similar and their differences were dictated by how competent the policy was in recovery behaviors. We also evaluate how the amount of data affects policy performance. As shown in Fig. 4B, we evaluate SRT-H with 33.3%, 66.6%, and 100% of the entire dataset as training data. These variants scored success rates of 66.7%, 77.8%, and 100%, respectively. This evaluation indicates that beyond the design of the architecture, the amount of data plays critical role in policy performance. High-level policy ablation studies For the HL policy, several design choices were made to address perception challenges arising from differences in gallbladder color, texture, and anatomy. First, in addition to the full view, we incorporate center-cropped version of the most critical operating area as input. The center-crop size is 432 480 pixels and the cropping location is always fixed on the original image. This allows the model to focus on the most relevant information in the surgical field by providing this area at higher resolution compared to the full view. Second, we modify the cross-entropy-based loss function by scaling it with the 洧1 distance between the predicted and reference task instructions. This adjustment is intended to improve the policys ability to distinguish between tasks that are temporally distant but visually similar. Third, to mitigate the effect of occlusions during surgery, we include history of four past image frames, each spaced one second apart, along with the current frame. This temporal context allows the HL policy to retain crucial temporal information, ensuring robust performance even when important details are temporarily obscured. We conduct an ablation study to determine the contribution of each design choice by systematically omitting each one during model training. Performance is evaluated based on both accuracy and F1 score for three classification tasks: predicting task instructions, corrective instructions, and identifying recovery modes. Results show that our HL policy achieved an accuracy and F1 score of approximately 97% for task instruction predictions. Removing the center crop input or using only the cross-entropy (CE) loss for task instructions resulted in decrease in accuracy and F1 score by around 2-2.5%. Omitting 11 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning the observation history led to an even more substantial drop in performance, exceeding 10% for the task instruction predictions and similar decline for the corrective instruction and recovery mode prediction. In the other two prediction tasks, our model also outperformed the variation that excludes the center crop input and the variant that only uses the CE loss without scaling. Although the margin for recovery mode predictions was smaller, with an improvement of around 0.5-1%, the increase in corrective instruction predictions performance was more pronounced. This is particularly evident in the F1 score, highlighting the HL policys ability to issue language corrections more consistently, achieving 2-2.5% improvement. Overall, the HL policy achieved approximately 95% accuracy in identifying recovery modes and around 70% accuracy in predicting corrective instructions, out of 18 possible motion classes (see Supplementary Methods \"\"). We provide additional information on these evaluations in Table S2. As further study, we apply GPT-4o, state-of-the-art general-purpose vision-language model, as the HL policy for surgical task planning. GPT-4o was provided with the current endoscope image and all task instructions it could issue to guide the robot (see Fig. S1). GPT-4o shows shortcomings in domain-specific understanding in issuing the correct task instruction. For example, it initially omitted the crucial step of grabbing gallbladder\" and prematurely initiated the action clipping first clip left tube\". Additionally, GPT-4o incorrectly prompted the go-back from clipping/cutting instructions before completing the task. Thus, GPT-4o would not be able to guide the LL policy through full cholecystectomy procedure, since it was unable to issue the correct task instructions."
        },
        {
            "title": "Comparison with expert surgeon",
            "content": "We perform preliminary comparison between SRT-H and an expert surgeon. Given the same gallbladder, both performed several tasks including adding the first and third clip to the artery and cutting it. Each round, SRT-H was deployed first and the surgeon was asked to repeat the same task. For adding the clips, modified clips with disabled latching mechanism were used. For cutting, right before the policy attempted to close its grippers to complete the cut, the robot was stopped to avoid permanent damage to the tissue. The surgeon had experience in performing both robotic and manual cholecystectomy. The surgeon did not have prior experience with the dVRK system but was given sufficient time to become familiar with using the system. Note that the participating surgeon study did not contribute to the training data. The results are shown in Fig. 6, which shows qualitative comparisons of the trajectories from the endoscope view and also in Cartesian space. We quantitatively report the mean jerk, trajectory length, and total duration during the tasks for both the surgeon and SRT-H. In general, we regard the better performer as the one that performs with the least mean jerk, trajectory length, and total duration. Our results show that the surgeon completes all tasks faster than SRT-H. However, we observed that SRT-H navigated with shorter trajectory length and less mean jerk compared to the surgeon, therefore SRT-H generates smoother and shorter trajectories. However, the surgeon was much faster in executing all the steps. As qualitative comparison, the 2D projections of the trajectories show that SRT-H and surgeon perform the procedure in similar manner, based on the overall shape and appearance of the trajectories. In general, despite these promising findings, we avoid making strong claims that SRT-H outperforms the surgeon. We also lacked sufficient number of gallbladders for more in-depth comparison. more detailed analysis may be addressed in further extension of this work. Our goal is to give an initial intuition of how our frameworks performance compares to that of an experienced surgeon. 12 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning right arm SRT-H left arm SRT-H right arm surgeon left arm surgeon z 60 70 80 20 10 0 10 20 50 60 80 20 10 0 10 20 70 80 20 10 0 10 20 0 10 30 20 10 20 10 0 10 20 10 0 2.544 3.058 31.387 54.005 18 12 SRT-H surgeon MJ TL 2.675 3.050 43.450 63.072 25 19 MJ 2.665 2. TL 44.384 47.348 19 13 ) 洧녴洧녴 洧3 MeanJerk(MJ)(102 Traj.Len.(TL)(洧녴洧녴) Duration(D)(洧) Figure 6 Qualitative motion comparison between SRT-H and surgeon. We evaluate SRT-H against human surgeon on the same gallbladder for the subtasks of applying the first and third clip to the artery, as well as cutting the artery. (A) 2D projection and (B) 3D plot of instrument paths for SRT-H (dark blue and red) and human surgeon (light blue and orange) as absolute positions in mm. (C) Quantitative comparisons between SRT-H and human surgeon based on the total duration of task execution (D in 洧) and trajectory length of the instruments (TL in 洧녴洧녴), as well as the mean jerk (MJ in 102 洧녴洧녴 洧3 ) calculated over the instrument paths. 13 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning"
        },
        {
            "title": "DISCUSSION",
            "content": "In this work, we introduce SRT-H, scalable framework for achieving step-level autonomy in robotic surgery. In comparison to prior work, which primarily focused on assistive tools [35, 42] and task-level autonomy [27, 46, 53], our research takes step forward by moving toward autonomy at the step level. The results of our study demonstrate the effectiveness of SRT-H in automating the clipping and cutting procedure of cholecystectomy intervention. Ablative studies show the effectiveness of our hierarchical design, which incorporates HL and LL policies. This design also demonstrates the ability to generalize across unseen ex-vivo tissues and self-correct errors in real-time. We demonstrate our approach across eight gallbladders, achieving 100% operation success rate."
        },
        {
            "title": "Levels of autonomy",
            "content": "The Level of autonomy (LoA) in medical robots is categorized across distinct levels [18], ranging from pure teleoperation to full autonomy. LoA 0 represents no autonomy, where the robot functions purely as tool controlled by human operator. LoA is defined by robot assistance, where the robot provides continuous control support, such as mechanical guidance or virtual constraints, but the human remains in full control. LoA II refers to task autonomy, where robots autonomously perform specific tasks, like running sutures, initiated by human input via discrete control commands. LoA III, conditional autonomy, allows the system to generate task strategies autonomously but requires the human operator to select among them or approve an autonomously selected strategy. Systems at LoA IV, classified as high autonomy, can make medical decisions independently but still require supervision by qualified doctor. Finally, LoA represents full autonomy, where the robot is capable of performing an entire procedure without any human intervention."
        },
        {
            "title": "Examples of high LoAs",
            "content": "Higher levels of autonomy LoA (IV) have been achieved by few systems. One such system is the CyberKnife [28], which autonomously performs radiosurgery for brain and spine tumors under human supervision. This system operates in highly structured environments, using non-invasive techniques where tissues are rigid and stable, reducing the complexity of automation. Another LoA IV system is the Veebot [40], which autonomously performs blood sampling by identifying and selecting suitable veins. These systems demonstrate progress in autonomous surgery, however, they operate under controlled conditions, and the gap between these systems toward achieving full autonomy in dynamic, soft tissue environments remains considerable. Our present SRT-H work falls in LoA IV, as it is capable of reliable and autonomous execution, while self-correcting its mistakes; note that these self-corrective instructions are generated by itself and not issued by the user of the system. However, our system is not failure-proof to out-of-distribution scenarios, therefore the surgeon should always oversee its operation. Additionally, we briefly mention further evolved definitions of LoA, which include Level of Environmental Complexity (LoEC) and Level of Task Complexity (LoTC) [36]. According to these metrics, our work falls in LoEC IV and LoTC IV. Our work can be categorized into LoEC IV because soft and realistic tissues are involved, although without topological motion (e.g., breathing), which is the further requirement needed to reach LoEC V. In terms of LoTC, our work falls into category IV because we consider advanced surgical tasks that require spatial understanding of the scene, but the model lacks clinical and anatomical knowledge, which is the further requirement to reach LoTC V. 14 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning We also draw direct comparison to highly relevant prior work involving autonomous bowel anastomosis [46]. Although anastomosis may seem like more technically demanding task, our work demonstrates greater step forward in comparison. More specifically, in this earlier work, the procedure took place under highly controlled conditions: the bowels were scaffolded on fixture, fluorescent markers were used for tracking, and specialized needle-throwing device simplified suturing to basic reach task. Even with these advantages, the system occasionally made errors that required manual surgeon intervention. Moreover, the prior approach relied on hand-crafted statemachine with model-based planning, which lacks expressivity. By contrast, our present work requires no special fixtures, tracking markers, or specialized surgical devices. Instead, it employs imitation learning to acquire more sophisticated and adaptable manipulation skills, which are difficult to capture with purely hand-crafted methods. For example, our system can delicately maneuver through the narrow space between the duct and artery, place clips at appropriate locations, and execute precise cuts without harming nearby tissue, all of which would be challenging to program explicitly. Crucially, the model can self-correct during the procedure, reducing the need for human intervention at test time. Furthermore, our method is expressive and scalable: by gathering demonstration data from additional procedures, we can potentially apply the same approach to wide variety of surgical tasks, including anastomosis."
        },
        {
            "title": "Robot transformers",
            "content": "Outside of surgery, advancements in robotics have led to the development of general-purpose tasksolving models [5, 11, 22, 43, 66]. These models are trained by imitation on extensive real-world robotics datasets, processing images from robot cameras, and following natural language task descriptions to generate robotic actions. The resulting controllers exhibit the ability to adapt to novel situations and demonstrate task-solving capabilities that extend well beyond the scope of their training data [66]. These models interpret commands that were not part of the training data and exhibit the ability to reason based on user instructions, such as which object to use as an improvised hammer (a rock) or finding drink that is best for someone who is tired (an energy drink)."
        },
        {
            "title": "Limitations",
            "content": "From ex-vivo to in-vivo One important area for further research is translating our system from ex-vivo experiments to in-vivo clinical environments. Translating from ex-vivo to in-vivo brings several challenges, such as operating in the surgical site, addressing bleeding and tissue motion, and fitting the wrist cameras through laparoscopic ports. Since our approach is robot agnostic, and only depends on the relative position of the robot end-effectors, surgical access and operation do not present many challenges. Since our approach operates through visual guidance (instead of model-based approach) and has the ability to self-correct, we believe it can adapt to motion and blood if it is incorporated as part of the training data or potentially zero-shot (see Fig. S5 for reference). However, further studies are required to confirm this. Additionally, although the current wrist-camera configuration in our work would likely not fit into laparoscopic ports, modern cameras provide strong imaging quality with sub-millimeter form factors [4, 48] and can be easily integrated into surgical tools with minimal size increase of ports. Another concern with the use of wrist cameras may be potential occlusions due to fog and blood on the camera lenses. potential solution to deal with these issues is to translate the strategies used for endoscopic cameras to wrist cameras. For instance, Anti-fogging solutions like Fred [37] may be used for fogging scenarios. For blood wiping, there are commercial solutions like ClickClean [10] or ClearCam [9], which physically remove any occlusions on the lens without removing the surgical tools. Furthermore, normalizing the usage of wrist cameras in the operating room may take time, 15 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning considering they are devices not widely available in the market. Making SRT-H safer further extension fo this work may focus on expanding the systems capabilities to cover broader range of surgical procedures. The presented SRT-H framework supports the ability to learn across multiple surgical procedures using the same model parameters, to which diverse learning is believed to improve performance on individual tasks [5, 52, 66]. Risk management remains crucial aspect of surgical robotics. Further research could incorporate conservative Q-learning [7] and conformal prediction [3, 44] into the SRT-H system to address uncertainty during surgery. Conservative QLearning (CQL) would help prevent overestimation of the SRT-Hs actions in unfamiliar situations, and conformal prediction would provide real-time feedback on the systems confidence levels. Safety switching with robotic systems can be performed with on-site surgeons or through teleoperation, much like the proposed safety protocols used in autonomous driving systems [30, 63]. Additionally, with enhanced perception, it may be possible to simulate robot behaviors in simulation and refine plans before executing in the real-world for greater safety [17]. Finally, although we demonstrate this approach primarily through full autonomy without supervision, our approach also supports real-time language interventions from expert surgeons, making it practical for potential integration into hospitals as tool for surgeons to reduce fatigue on simple procedures or for areas with no access to trained surgeons. Intervention could be requested by the system based on uncertainty calculations and could be performed by remote operator [44]."
        },
        {
            "title": "Data collection",
            "content": "Training data was collected by two experienced human demonstrators on the dVRK system. Dataset 洧냥1, collected by the first demonstrator, contains data from 31 different gallbladders. The second demonstrator collected data for 3 additional gallbladders, which is denoted as dataset 洧냥2. All gallbladder organs were sourced from Animal Technologies, Inc. (Tyler, TX, USA). Note that both data collectors were non-clinical research assistants, trained by surgical resident with extensive experience performing cholecystectomies. The first assistant was the primary data collector and contributed the most to the dataset. By the time the second data collector joined the project, most of the necessary data was collected therefore the contributed dataset was much smaller. We define 洧냥 = 洧냥1 (cid:208) 洧냥2 as the union of both datasets. The visual data includes video streams from the dVRK stereo endoscope, which has resolution of 960 540 pixels, and two wrist cameras, each with resolution of 640 480, mounted on the instruments of the surgical robots left and right arm. Both video and kinematic streams are recorded at 30 frames per second (FPS). Prior to collecting specific task data, demonstrator performed blunt dissection with Maryland forceps on given gallbladder in order to reach the critical view of safety (CVS), where the cystic duct and artery are clearly identifiable. Certain gallbladders with abnormal tissue structures were not used, including the ones where the artery crosses over the duct and where the artery branches (see Fig. S6 for reference). Note that approximately 10% of gallbladders were excluded because of these anatomical anomalies. Although the model can handle such variations if sufficient demonstration data are available, their rarity made it difficult to collect data at scale. Addressing these edge cases through scaled data collection is beyond the scope of this work and is left for future investigation. To simulate an accurate setup for the surgery, an expert surgeon recommended cholecystectomy port locations using plastic abdominal dome. These ports were then isolated and modeled in computer-aided design (CAD) to create an open structure that holds the port locations for each arm of the surgical 16 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning robot, as shown in Fig 1A. This way, the dissection area remains open rather than concealed, which is ideal for frequent wrist camera mounting, clip reloading, and tool switching. This open setup may raise concerns that ambient lighting may effect the lighting conditions. However, we found that its effect on the endoscopic and wrist cameras image quality is negligible. The clipping and cutting portions of cholecystectomy include 17 tasks in total. These include grabbing the gallbladder (1), adding six clips (2 6 = 12), and cutting twice for the duct and artery (2 2 = 4), summing to 17 (1 + 12 + 4 = 17). Note that the tasks for adding the clips and cutting involve two tasks: the motion for adding the clip or cutting and the retraction. In order to acquire multiple trials from single gallbladder, we utilize few tricks. For clipping motions, we use clips with the latching mechanism disabled. This allows us to perform clipping motions repetitively without actually locking it to the duct or the artery. For the cutting motions, we performed the motion of placing the scissors, but we do not close the scissors at the last step. During post-processing, we extend the kinematics data to simulate cutting motion. Using this strategy, it is possible to acquire multiple demonstration data using single gallbladder with minimal damage. This may raise concerns that simply closing the grippers might not guarantee cutting. In practice, if the cut is not successful, which was very rare in our experiments, the policy often tried to cut again because it observed that the duct / artery was not cut and remained intact in the image observation. Also, multiple cuts were generally not necessary because the scissors were very sharp. We note that these strategies simply serve to aid with data collection without harming the tissues and do not take away from the generality of the methods. We use the above logistics to collect many expert demonstrations. Additionally, we further collect samples that show recovery from suboptimal states to augment the dataset. These recovery demonstrations help the learned policies to recover from its own mistakes. After training the policies on the base dataset, we additionally collected DAgger [25] dataset 洧냥洧녫洧녶洧洧 as described in [54] to improve the base model performance by learning from verbal corrections of common mistakes during policy rollout. The DAgger algorithm iteratively collects data from the policys own actions and corrects them using expert feedback to refine the policy. Within our DAgger dataset, only the language predictions are corrected, therefore it is denoted as HL DAgger for the rest of the paper. The language corrections were either issued during the experiment or added during postprocessing. The dataset is summarized in Table 2, providing relevant statistics such as the number of demonstrations, images, and duration for both optimal and recovery demonstrations. These numbers represent the total number of trajectories collected across all gallbladders, encompassing all tasks involved in the clipping and cutting steps of the cholecystectomy procedure. Table 2 Dataset summary. Statistics for the data collected by the two main data collectors (洧냥1 & 洧냥2) and in the HL DAgger experiments (洧냥洧녫洧녶洧洧). Data Collector 1 Data Collector 2 HL DAgger"
        },
        {
            "title": "Recovery Demonstrations",
            "content": "Num. Images Time (s) Num. Images Time (s) 31 12,304 1,472,551 49,085 4,904 704,797 23,493 885 127,325 4,211 263 40,297 1,343 15 264 54,638 1,821 352 75,017 2,500 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning High-level policy"
        },
        {
            "title": "Problem definition",
            "content": "The HL policy, denoted 洧랢HL( 洧녷洧노, 洧녫洧노, 洧녴洧노 洧녶洧노洧녲:洧노), takes as input the current image observation 洧녶 at timestep 洧노, along with 洧녲 preceding observations from the left camera stream of the dVRK Si endoscope. As output, the HL policy generates three predictions: the next task 洧녷洧노 (i.e. surgical phase) to be executed by the LL policy, correction flag 洧녫洧노 indicating whether the robot is in recovery mode, and corrective (motion) instruction 洧녴洧노 that specifies cardinal actions such as move right arm to the right\" or move left arm towards me\", which should be executed instead if the robot is in recovery mode. CE loss is used for all three predicted outputs (see Eq. 1). For the task instruction component, the CE loss is scaled by the 洧1 distance between the predicted and reference label to improve the policys ability to distinguish between tasks that are temporally distant but visually similar. The individual loss components are weighted based on their relative importance to the task. The task instruction has the highest priority, so its weight 洧녻洧녷 = 0.4 is set higher than the weight for the correction flag and corrective instruction predictions, which are set at 洧녻洧녫 = 洧녻洧녴 = 0.3. The resulting objective function minimizes the expected weighted sum of the task, correction, and motion losses and is given as the following. We use hat symbol to denote the outputs predicted by the HL policy, while the corresponding ground-truth values from the dataset are written without the hat. min 洧랢HL 洧댶(洧녶洧노洧녲:洧노,洧녷洧노,洧녫洧노,洧녴洧노 )洧냥 (cid:20) 洧녻洧녷 洧쮺E (洧랢HL( 틙洧녷洧노 洧녶洧노洧녲:洧노), 洧녷洧노) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) Task CE Loss 틙洧녷洧노 洧녷洧노 1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) Task 洧1 Distance (cid:124) + 洧녻洧녫 洧쮺E (洧랢HL(틙洧녫洧노 洧녶洧노洧녲:洧노), 洧녫洧노) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) Correction CE Loss (cid:124) +洧녻洧녴 洧쮺E (洧랢HL( 틙洧녴洧노 洧녶洧노洧녲:洧노), 洧녴洧노) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) Motion CE Loss (1) (cid:21) ."
        },
        {
            "title": "Model architecture",
            "content": "The HL policy architecture, illustrated in Fig. 2 B, consists of vision encoder, Transformer Decoder [60], and separate multi-layer perceptron (MLP) heads to generate the three classification outputs. Each image undergoes preprocessing, including standardization based on the mean and standard deviation of the color channels calculated over the entire dataset, ensuring zero mean and unit standard deviation. The image is resized to 224 224 to match the resolution used for pretraining the vision encoder. Alongside this global view, centered crop that captures the most task-critical region is extracted and resized to 224 224. The centered crop covers the inner 50% of the width and captures the lower 80% of the height, starting from the bottom. This approach is inspired by LLaVAs AnyRes technique [31], which divides images into multiple patches while preserving the global scene context. However, instead of generating multiple patches, we focus on extracting only the most task-relevant patch, emphasizing the center of the surgical area. The vision encoder is the tiny variant of the Swin Transformer [32] pretrained on Imagenet [13]. The Swin Transformer is selected due to its high performance on limited data and its ability to produce compact output token size of 768, which makes it suitable for temporal modeling with downstream Transformer architecture. During surgery, important details are often occluded. For instance, clip could easily be occluded by an instrument. In order to retain information crucial for classification, we include history of 洧녲 = 4 past image frames, each spaced 1 apart, along with the current frame as input to the HL policy, following the approach of Shi et al. [54]. The embeddings from the vision encoder are used as inputs to the Transformer Decoder, configured with eight heads and six layers. To preserve temporal information, sinusoidal position embeddings are added to the input sequence. The vision encoder outputs are passed to the Transformer directly without pooling to preserve spatial information, similar to the approach by Zhao et al. [64]. By assigning unique learnable embeddings as task-specific queries [64], 18 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning the Transformer Decoder can effectively attend to relevant spatial and temporal details, optimizing the alignment of each output with the most appropriate image frames and their features."
        },
        {
            "title": "Training",
            "content": "The HL policy base model is trained on dataset 洧냥 with the AdamW [33] optimizer, learning rate of 1洧5, and weight decay of 5洧2. To improve both convergence and generalization, an annealing cosine weight schedule with linear warmup of five epochs is applied. We also incorporate data augmentation techniques, including RandAugment [12] and coarse dropout from Albumentations [6], in order to boost visual robustness. Due to our specific dataset design, which consists of multiple individual recordings per task rather than continuous procedure recordings, two randomly sampled continuous task recordings are concatenated to artificially generate task transitions during HL policy training. To encourage the policy to learn wider range of task semantics, 60% of the input sequences begin within recovery mode demonstration, exposing the policy to varying task executions and recovery scenarios. During training, we apply prediction offset, where the policy is trained to predict the task instruction 0.5 into the future rather than predicting the current surgical state. This encourages the policy to anticipate upcoming actions and better handle task transitions [54]. After the HL DAgger dataset 洧냥corr is collected, the HL policy is fine-tuned on the merged dataset 洧냥 (cid:208) 洧냥洧녫洧녶洧洧."
        },
        {
            "title": "Inference",
            "content": "Every 3 s, the HL policy predicts new task instruction 洧녷洧노, correction flag 洧녫洧노, and corrective instruction 洧녴洧노. Based on the corrective flag 洧녫洧노, the language instruction provided to the LL policy is then either the task instruction 洧녷洧노 or the corrective motion 洧녴洧노, as defined by Eq. 2: 洧녳洧노 = (cid:40) 洧녷洧노, 洧녴洧노, if 洧녫洧노 = 0 if 洧녫洧노 = 1 (2) . During inference, human supervisor can override the HL policys outputs via voice command or by selecting task instruction or correction from drop-down menu in our application GUI. If manual correction is made, the HL policy outputs are overridden for the following 3 s. Low-level policy"
        },
        {
            "title": "Problem definition",
            "content": "The LL policy is formulated as language-conditioned policy 洧랢LL(洧녩洧노:洧노+洧녲 洧녶洧노, 洧녳洧노) to predict sequence of robot actions 洧녩洧노:洧노+洧녲 based on the current image observation 洧녶洧노 and language instruction 洧녳洧노. 洧녳洧노 can either be 洧녷洧노 or 洧녴洧노 depending on the correction flag 洧녫洧노. The input observations include the stereo endoscopes left image, along with images from the left and right wrist cameras. For the action representation, we adopt the hybrid-relative action representation from [26], which models relative Cartesian translations with respect to the endoscope tip and rotations relative to the end-effector. This formulation compensates for the dVRKs kinematic inconsistencies [23], leading to more consistent multi-step motion predictions. The policy is trained using behavior cloning, where the objective is to minimize the 洧1 loss between the predicted action sequence and reference actions. The objective function is expressed in Eq. 3: min 洧랢LL 洧댶(洧녶洧노,洧녳洧노,洧녩洧노:洧노+洧녲 )洧냥 [洧랢LL(틙洧녩洧노:洧노+洧녲 洧녶洧노, 洧녳洧노) 洧녩洧노:洧노+洧녲 1] . (3) 19 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning"
        },
        {
            "title": "Model architecture",
            "content": "The LL policy is built on decoder-only, BERT-like Transformer [14] that maps visual inputs to robot actions, as shown in Fig. 2B. The visual inputs consist of images from the endoscope and wrist cameras, and they are encoded via pre-trained EfficientNet-B3 [57]. The encodings are then fused with language instruction embeddings from the HL policy using feature-wise linear modulation (FiLM) layers. [39]. Language instructions are encoded using distilled bidirectional encoder representations from transformers (DistilBERT) [47]. The fused visual and language embeddings, along with positional embeddings, are passed into the Transformer Decoder. The action space is 20-dimensional vector representing the relative actions for both robot arms (three for translation, six for rotation, and one for jaw angle per arm). Note that for rotation, we are using the 6D rotation formulation [26, 65], where the rotation is represented by the first two columns of the rotation matrix. Its third column can be extrapolated by multiplying the first two columns, thus recovering the full rotation matrix. The six dimension rotation was shown to be more continuous than other rotation representations and thus easier for neural networks to learn. With action chunking, the decoder outputs 洧녲 20 tensor given the current observation. To optimize performance [64], we predict robot actions for 2 洧 horizon, resulting in chunk size of 洧녲 = 60."
        },
        {
            "title": "Training",
            "content": "During training, the input images were resized to 224 224 pixels. To prevent overfitting, we apply several data augmentation techniques including random cropping, rotation, shifting, color jittering, and coarse dropout using Albumentations [6]. Additionally, we apply 7% random dropout to one of the three input images, preventing the policy from over-relying on any single image observation. To generate corrective language labels from the demonstration data, we examine future chunk of actions and compute the motion trend along each axis. By comparing the magnitudes of motion across axes, we can determine the dominant direction of movement within that action segment. This enables assigning directional motion labels such as \"move left arm to the right\" or \"move the right arm towards me.\" The chunk size here is set to 10 because we want to capture the unit of motions in the collected trajectories. If the chunk size is set too small, the generated instructions would be too noisy, and if the chunk size is too large, the more delicate motions would be ignored. During training, task instructions (e.g. grabbing gallbladder\") are used when sampling from the base dataset, and corrective instructions (e.g., move left arm towards me\") are used when sampling from recovery demonstrations. This enables the LL policy to execute appropriate actions when given task instructions and recover from suboptimal states when given corrective instructions. The policy contains approximately 72 million parameters and is trained on single RTX 4090 GPU (24GB). Each epoch takes around 4 min with batch size of 10, and training runs for 1500 epochs (100 h) before evaluation."
        },
        {
            "title": "Inference",
            "content": "Inference time to produce single action is approximately 20 ms on the same hardware. To optimize performance, we set different execution horizons (the number of actions executed before resampling the LL policy) for various phases of the procedure. For the grabbing gallbladder\" phase, we found through preliminary experiments that shorter horizon caused the robot to change strategies too frequently, leading to hesitation and continuous pose adjustments without fully committing to successful strategy. Setting the execution horizon to 30 timesteps ensures that the robot commits to single strategy. In contrast, for the other phases, we set shorter execution horizon of 20 timesteps to enable more frequent replanning. These phases require high precision, particularly when 20 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning maneuvering the right arm between the duct and artery. Additionally, manual tool switching and clip loading between tasks were necessary during experiments. To manage these transitions, we implemented logic-based state machine to automatically pause both the HL and LL policies during phase transitions. For instance, the pauses were triggered when shifting from going back from the first clip left tube\" to clipping second clip left tube\" or from going back from third clip right tube\" to going to the cutting position on the right tube\"."
        },
        {
            "title": "Statistical analysis",
            "content": "The mean computed in Fig. 4 with sample size 洧녜 and data point 洧논 were computed using the following equation: 洧랞 = 1 洧녜 洧녜 洧녰=1 洧논洧녰,"
        },
        {
            "title": "References",
            "content": "[1] Monica Acalovschi and Frank Lammert. The growing global burden of gallstone disease. World Gastroenterology News, 17(4):69, 2012. [2] Mehrnoosh Afshar, Jay Carriere, Tyler Meyer, Ron S. Sloboda, Siraj Husain, Nawaid Usmani, and Mahdi Tavakoli. model-based multi-point tissue manipulation for enhancing breast brachytherapy. IEEE Transactions on Medical Robotics and Bionics, 4(4):10461056, 2022. [3] Anastasios N. Angelopoulos and Stephen Bates. Conformal prediction: gentle introduction. Foundations and Trends in Machine Learning, 16(4):494591, 2023. ISSN 1935-8237. [4] Manuel Ballester, Heming Wang, Jiren Li, Oliver Cossairt, and Florian Willomitzer. Single-shot synthetic wavelength imaging: Sub-mm precision tof sensing with conventional cmos sensors. Optics and Lasers in Engineering, 178:108165, 2024. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. In Robotics: Science and Systems, 2022. [6] Alexander Buslaev, Vladimir Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr Kalinin. Albumentations: fast and flexible image augmentations. Information, 11(2):125, 2020. [7] Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In Conference on Robot Learning, pp. 39093928. PMLR, 2023. [8] Zih-Yun Chiu, Florian Richter, Emily K. Funk, Ryan K. Orosco, and Michael C. Yip. Bimanual regrasping for suture needles using reinforcement learning for rapid motion planning. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 77377743, 2021. [9] ClearCam. Clearcam the first and only robotic scope cleaner. https://www.clearcam-med. com/, 2025. Accessed: 2025-04-01. 21 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning [10] ClickClean. Clickclean lens cleaning device for minimally invasive surgery. https:// clickclean-medeon.com/, 2025. Accessed: 2025-04-01. [11] Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch칬lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B칲chler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo칚o Silv칠rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Keyvan Majd, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart칤n-Mart칤n, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic learning datasets and RT-X models. https://robotics-transformer-x.github.io, 2023. [12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with reduced search space. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 30083017, 2020. [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. [15] Georgios Fagogenis, Margherita Mencattelli, Zurab Machaidze, Benoit Rosa, Karl Price, Wu, Weixler, Mossab Saeed, John Mayer, and Pierre Dupont. Autonomous robotic intracardiac catheter navigation using haptic vision. Science robotics, 4(29):eaaw1977, 2019. [16] Rohit Gupta, Anil Kumar, Chinniahnapalaya Hariprasad, and Manoj Kumar. Anatomical variations of cystic artery, cystic duct, and gall bladder and their associated intraoperative and 22 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning postoperative complications: an observational study. Annals of Medicine and Surgery, 85(8): 38803886, 2023. [17] Sthithpragya Gupta, Kunpeng Yao, Lo칦c Niederhauser, and Aude Billard. Action contextualization: Adaptive task planning and action tuning using large language models. IEEE Robotics and Automation Letters, 9(11):94079414, 2024. doi: 10.1109/LRA.2024.3460408. [18] Tam치s Haidegger. Autonomy for surgical robots: Concepts and paradigms. IEEE Transactions on Medical Robotics and Bionics, 1(2):6576, 2019. [19] Mustafa Haiderbhai, Radian Gondokaryono, Andrew Wu, and Lueder A. Kahrs. Sim2real rope cutting with surgical robot using vision-based reinforcement learning. IEEE Transactions on Automation Science and Engineering, pp. 112, 2024. [20] Kyle Hsu, Moo Jin Kim, Rafael Rafailov, Jiajun Wu, and Chelsea Finn. Vision-based manipulators need to also see from their hands. In International Conference on Learning Representations, 2022. [21] Junlei Hu, Dominic Jones, Mehmet R. Dogar, and Pietro Valdastri. Occlusion-robust autonomous robotic manipulation of human soft tissues with 3-d surface feedback. IEEE Transactions on Robotics, 40:624638, 2024. [22] Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu-Quan Chong, Chen Wang, Katia Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Zsolt Kira, Fei Xia, and Yonatan Bisk. Toward general-purpose robots via foundation models: survey and meta-analysis. arXiv preprint arXiv:2312. 08782, 2023. [23] Minho Hwang, Brijen Thananjeyan, Samuel Paradis, Daniel Seita, Jeffrey Ichnowski, Danyal Fer, Thomas Low, and Ken Goldberg. Efficiently calibrating cable-driven surgical robots with rgbd fiducial sensing and recurrent neural networks. IEEE Robotics and Automation Letters, 5 (4):59375944, 2020. [24] Minho Hwang, Jeffrey Ichnowski, Brijen Thananjeyan, Daniel Seita, Samuel Paradis, Danyal Fer, Thomas Low, and Ken Goldberg. Automating surgical peg transfer: Calibration with deep learning can exceed speed, accuracy, and consistency of humans. IEEE Transactions on Automation Science and Engineering, 20(2):909922, 2022. [25] Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel Kochenderfer. Hgdagger: Interactive imitation learning with human experts. In 2019 International Conference on Robotics and Automation (ICRA), pp. 80778083, 2019. [26] Ji Woong Kim, Tony Z. Zhao, Samuel Schmidgall, Anton Deguet, Marin Kobilarov, Chelsea Finn, and Axel Krieger. Surgical robot transformer (SRT): Imitation learning for surgical subtasks. In 8th Annual Conference on Robot Learning, 2024. [27] Alan Kuntz, Maxwell Emerson, Tayfun Efe Ertop, Inbar Fried, Mengyu Fu, Janine Hoelscher, Margaret Rox, Jason Akulian, Erin Gillaspie, Yueh Lee, et al. Autonomous medical needle steering in vivo. Science Robotics, 8(82):eadf7614, 2023. [28] Gopalakrishna Kurup. Cyberknife: new paradigm in radiotherapy, 2010. [29] Xiao Liang, Chung-Pang Wang, Nikhil Uday Shinde, Fei Liu, Florian Richter, and Michael Yip. Medic: Autonomous surgical robotic assistance to maximizing exposure for dissection and cautery. arXiv preprint arXiv:2409.14287, 2024. 23 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning [30] Taeyoon Lim, Myeonghwan Hwang, Eugene Kim, and Hyunrok Cha. Authority transfer according to driver intervention intention considering coexistence of communication delay. Computers, 12(11):228, 2023. [31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1001210022, October 2021. [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [34] Arnab Majumder, Maria Altieri, and Michael Brunt. How do do it: laparoscopic cholecystectomy. Annals of Laparoscopic and Endoscopic Surgery, 5, 2020. [35] Jacques Marescaux and Francesco Rubino. The zeus robotic system: experimental and clinical applications. Surgical Clinics, 83(6):13051315, 2003. [36] Tam치s D. Nagy and Tam치s Haidegger. Performance and capability assessment in surgical subtask automation. Sensors, 22(7), 2022. ISSN 1424-8220. doi: 10.3390/s22072501. URL https://www.mdpi.com/1424-8220/22/7/2501. [37] Camran Nezhat and Vadim Morozov. simple solution to lens fogging during robotic and laparoscopic surgery. JSLS: Journal of the Society of Laparoendoscopic Surgeons, 12(4):431, OctDec 2008. [38] Yafei Ou and Mahdi Tavakoli. Sim-to-real surgical robot learning and autonomous planning for internal tissue points manipulation using reinforcement learning. IEEE Robotics and Automation Letters, 8(5):25022509, 2023. [39] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [40] Tekla Perry. Profile: Veebot drawing blood faster and more safely than human can, 2013. [41] Ameya Pore, Eleonora Tagliabue, Marco Piccinelli, Diego DallAlba, Alicia Casals, and Paolo Fiorini. Learning from demonstrations for autonomous soft-tissue retraction. In 2021 International Symposium on Medical Robotics (ISMR), pp. 17, 2021. [42] Karl Price, Joseph Peine, Margherita Mencattelli, Yash Chitalia, David Pu, Thomas Looi, Scellig Stone, James Drake, and Pierre Dupont. Using robotics to move neurosurgeons hands to the tip of their endoscope. Science Robotics, 8(82):eadg6042, 2023. [43] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G칩mez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim칠nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. generalist agent. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. 24 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning [44] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots that ask for help: Uncertainty alignment for large language model planners. In 7th Annual Conference on Robot Learning, 2023. [45] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud칤k (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 627635, Fort Lauderdale, FL, USA, 1113 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/ross11a. html. [46] Hamed Saeidi, Justin Opfermann, Michael Kam, Shuwen Wei, Simon L칠onard, Michael Hsieh, Jin Kang, and Axel Krieger. Autonomous robotic laparoscopic surgery for intestinal anastomosis. Science robotics, 7(62):eabj2908, 2022. [47] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. In 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing @ NeurIPS 2019, 2019. [48] Jack Sayers, Nicole Czakon, Peter Day, Thomas Downes, Ran Duan, Jiansong Gao, Jason Glenn, Sunil Golwala, Matt Hollister, Henry LeDuc, et al. Optics for music: new (sub) millimeter camera for the caltech submillimeter observatory. In Millimeter, Submillimeter, and Far-Infrared Detectors and Instrumentation for Astronomy V, volume 7741, pp. 255266. SPIE, 2010. [49] Paul Maria Scheikl, Bal치zs Gyenes, Rayan Younis, Christoph Haas, Gerhard Neumann, Franziska Mathis-Ullrich, and Martin Wagner. Lapgym - an open source framework for reinforcement learning in robot-assisted laparoscopic surgery. Journal of Machine Learning Research, 24(368): 142, 2023. [50] Paul Maria Scheikl, Eleonora Tagliabue, Bal치zs Gyenes, Martin Wagner, Diego DallAlba, Paolo Fiorini, and Franziska Mathis-Ullrich. Sim-to-real transfer for visual reinforcement learning of deformable object manipulation for robot-assisted surgery. IEEE Robotics and Automation Letters, 8(2):560567, 2023. [51] Paul Maria Scheikl, Nicolas Schreiber, Christoph Haas, Niklas Freymuth, Gerhard Neumann, Rudolf Lioutikov, and Franziska Mathis-Ullrich. Movement primitive diffusion: Learning gentle robotic manipulation of deformable objects. IEEE Robotics and Automation Letters, 9(6):5338 5345, 2024. [52] Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, Ahmed Ezzat Ghazi, and Axel Krieger. Generalpurpose foundation models for increased autonomy in robot-assisted surgery. arXiv preprint arXiv:2401.00678, 2024. [53] Azad Shademan, Ryan Decker, Justin Opfermann, Simon Leonard, Axel Krieger, and Peter CW Kim. Supervised autonomous robotic soft tissue surgery. Science translational medicine, 8(337):337ra64337ra64, 2016. [54] Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. Yell at your robot: Improving on-the-fly from language corrections. In Robotics: Science and Systems, 2024. SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning [55] Changyeob Shin, Peter Walker Ferguson, Sahba Aghajani Pedram, Ji Ma, Erik P. Dutson, and Jacob Rosen. Autonomous tissue manipulation via surgical robot using learning based model predictive control. In 2019 International Conference on Robotics and Automation (ICRA), pp. 38753881, 2019. [56] Hang Su, Andrea Mariani, Salih Ertug Ovur, Arianna Menciassi, Giancarlo Ferrigno, and Elena De Momi. Toward teaching by demonstration for robot-assisted minimally invasive surgery. IEEE Transactions on Automation Science and Engineering, 18(2):484494, 2021. [57] Mingxing Tan. Efficientnet: Rethinking model scaling for convolutional neural networks. In Proceedings of the International Conference on Machine Learning (ICML), 2019. [58] Ajay Kumar Tanwani, Andy Yan, Jonathan Lee, Sylvain Calinon, and Ken Goldberg. Sequential robot imitation learning from observations. The International Journal of Robotics Research, 40 (10-11):13061325, 2021. [59] Brijen Thananjeyan, Animesh Garg, Sanjay Krishnan, Carolyn Chen, Lauren Miller, and Ken Goldberg. Multilateral surgical pattern cutting in 2d orthotropic gauze with deep reinforcement learning policies for tensioning. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 23712378, 2017. [60] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [61] Jiaqi Xu, Bin Li, Bo Lu, Yun-Hui Liu, Qi Dou, and Pheng-Ann Heng. Surrol: An open-source reinforcement learning centered and dvrk compatible platform for surgical robot learning. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 18211828. IEEE, 2021. [62] Qinxi Yu, Masoud Moghani, Karthik Dharmarajan, Vincent Schorp, William Chung-Ho Panitch, Jingzhou Liu, Kush Hari, Huang Huang, Mayank Mittal, Ken Goldberg, et al. Orbitsurgical: An open-simulation framework for learning surgical augmented dexterity. arXiv preprint arXiv:2404.16027, 2024. [63] Tao Zhang. Toward automated vehicle teleoperation: Vision, opportunities, and challenges. IEEE Internet of Things Journal, 7(12):1134711354, 2020. [64] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023. [65] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao. On the continuity of rotation representations in neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. [66] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. 26 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Jie Tan, Marc Toussaint, and Kourosh Darvish (eds.), Proceedings of The 7th Conference on Robot Learning, volume 229, pp. 21652183. PMLR, 2023. 27 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning"
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "Funding: Research reported in this publication was supported by the Advanced Research Projects Agency for Health (ARPA-H) under Award Number 75N91023C00048, as well as NSF/FRR 2144348, NIH R56EB033807, and NSF DGE 2139757. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. Author contributions: Conceptualization: J.W.K., A.K., S.S., D.R.T, R.C., C.F.; Methodology: J.W.K., C.F., A.K.; Software: J.W.K., L.X.S., P.H., J.C., A.D., P.M.S.; Visualization: J.W.K., S.S., P.H., J.C., P.M.S.; Data Curation: A.G., J.W.K., P.H., J.J., B.W.; Formal analysis: J.W.K., P.H., J.C.; Funding acquisition: A.K., R.C.; Supervision: A.K., J.W.K., C.F.; Writingoriginal draft: J.W.K., S.S., P.H., J.C., P.M.S., A.G.; Writingreview and editing: J.W.K., S.S., P.H., J.C., P.M.S., A.G., A.K., C.F., L.X.S, D.R.T, R.C.; Competing interests: Provisional Patent Pending: \"Imitation learning for surgical robots with kinematics errors using self-corrections.\" Richard Cha has ownership interests in and serves as scientific advisor for Optosurgical, LLC. Data and materials availability: All data supporting the conclusions of this paper are included in the main text or Supplementary Materials. The datasets and code used to generate Fig. 4, 6, and S7 are available at Zenodo: https://zenodo.org/records/"
        },
        {
            "title": "Supplementary materials",
            "content": "Supplementary Methods Figs. S1 to S7 Tables S1 to S2 References (7-0) 28 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning"
        },
        {
            "title": "Supplementary Materials",
            "content": "This PDF file includes: Supplementary Methods Tables S1 to S2 Figures S1 to S7 S1 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning"
        },
        {
            "title": "Supplementary materials and methods",
            "content": "High-level policy training configuration The policy consists of approximately 45 million parameters, with 29 million allocated to the encoder and the remaining 16 million shared between the Transformer and MLP heads. The training was conducted on single RTX 4090 GPU (24GB), taking around 20 hours to complete 500 epochs (4,000 iterations each). The inference time is approximately 25 ms. During training, we used 90-10 split for the training and validation sets and selected the model weights that performed best on the validation set. For the HL ablation study, shown in Table S2, we sampled 40,000 sequences from the validation dataset and applied all variants on the same input sequences. During training, we applied RandAugment [12], data augmentation technique that automatically selects and applies subset of augmentations from predefined set of transformations (listed in Table S1)."
        },
        {
            "title": "Corrective language instructions",
            "content": "The high-level policy was capable of generating 18 corrective instructions, including: close left gripper, close right gripper, open left gripper, open right gripper, move left arm to the left, move left arm to the right, move left arm towards me, move left arm away from me, move left arm higher, move left arm lower, move right arm to the left, move right arm to the right, move right arm towards me, move right arm away from me, move right arm higher, move right arm lower, close both grippers, open both grippers. General purpose Vision-Language Model (VLM) as high-level policy an alternative to the selected HL policy, we explore using state-of-the-art general-purpose VLM, GPT-4o, to perform the HL task planning. To evaluate GPT-4os potential, we test it in the same setup as our HL policy, assigning it the role of surgical task planner for the dVRK. GPT-4o is provided with the current endoscope image and the necessary task instructions it could issue to the robot, as shown in Fig. S1. To provide more spatial context, clipping task instructions include additional information on where the clip should be placed (e.g., bottom or top), and filler instructions such as reload clip\" or exchange instrument\" are added. Processing the first frame with GPT-4o already highlights the models lack of domain-specific knowledge and its difficulties with visual recognition of task completion and transitions. For example, the model initially skips the critical step of Grabbing the Gallbladder\" (Fig. S2), only selecting this instruction after being prompted that the gallbladder had not been grabbed yet. Similar errors are observed later; GPT-4o triggered the Clipping the Left Tube (Bottom Clip 1)\" step prematurely, before successfully grabbing the gallbladder (Fig. S3), and incorrectly prompts the transition to go back from clipping before the clip has been set (Fig. S4). These observations indicate that general-purpose VLM like GPT-4o lacks the task-specific precision required for effective surgical task planning. Fine-tuning on this specific domain is required, as reliance on prompt engineering alone proves insufficient for our surgical task planner setup."
        },
        {
            "title": "Supplementary Figures and Tables",
            "content": "S2 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure S1 Prompt given to GPT-4o. Describing first the context including the role and which input data it will receive as well as the task instructions that it can later issue. We only provide endoscope images as input without wrist camera images, consistent with the input of the HL policy. S3 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure S2 Response for input t=0s. Skipping the Grabbing the Gallbladder\" surgical step. Figure S3 Response for input t=2s. Triggering too early to the clipping task instruction. S4 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure S4 Response for input t=5s. Triggering too early to go back from clipping when the clip was not set yet. Table S1 Image transformations used in RandAugment [12]. identity rotate posterize sharpness translate-x translate-y autoContrast equalize solarize contrast shear-x color brightness shear-y Figure S5 Difference between porcine (left) and human (right) gallbladder. S5 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Figure S6 Examples of anomalous gallbladder anatomy. Total # Self Corrections (n=8) Self Corrections per Subtask (n=8) 17 7 1 1 1 1 1 4 5 2 move r. a. move r. a. close r. gripper move r. a. move r. move r. a. move r. a. open l. gripper move l. a. move l. a. move l. c u # 25 20 15 5 0 1 1 1 1 1 1 2 4 8 1 3 1 3 5 3 1 grab clip cut go back Figure S7 High-Level Language Policy corrections. We analyse the occurrence of self corrections that were generated by the High-Level Language Policy to guide the Low-Level Policy in recovery modes over n=8 experiments. Left arm and right arm of the robot are abbreviated to r. a. and l. a., respectively. The cardinal directions are shown as symbols for right , left , higher , lower , towards me , away from me . (A) shows the total number of self corrections over all experiments and (B) detailed view for each correction type distributed over subtasks grouped into grab, clip, cut, and go back. S6 SRT-H: Hierarchical Framework for Autonomous Surgery via Language-Conditioned Imitation Learning Table S2 HL Policy Performance Metrics. Prediction accuracy and F1 score of the HL policy on the individual targets for task instructions, correction flag, and corrective instruction. Ours No Center Crop No 洧1 Dist. No Hist."
        },
        {
            "title": "Corrective Instruction",
            "content": "Accuracy (%) 97.29 F1 Score (%) 97.18 Accuracy (%) 95.48 F1 Score (%) 91.03 Accuracy (%) 69.65 F1 Score (%) 56.37 94.80 94.82 94.70 89.25 67.85 52.73 94.99 94.60 95.15 90. 68.35 53.91 85.44 84.56 87.55 57.07 60.55 44.42 S"
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "Optosurgical",
        "Stanford University"
    ]
}