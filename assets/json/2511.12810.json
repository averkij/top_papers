{
    "paper_title": "MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection",
    "authors": [
        "Leena Alghamdi",
        "Muhammad Usman",
        "Hafeez Anwar",
        "Abdul Bais",
        "Saeed Anwar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \\href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 1 8 2 1 . 1 1 5 2 : r MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Leena Alghamdi1, Muhammad Usman2, Hafeez Anwar3, Abdul Bais4 and Saeed Anwar5 1Information and Computer Science, King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia. 2Faculty of Science, Ontario Tech University, Oshawa L1G 0C5, Canada. 3Department of Computer Science, National University of Computer and Emerging Sciences, Peshawar 24720, Pakistan. 4Electronic Systems Engineering , University of Regina, Regina S4S 0A2, Canada. 5Department of Computer Science and Software Engineering, University of Western Australia, Perth 6009, Australia. Abstract Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose Multi-Scale Recursive Network that extracts multi-scale features via Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting 1 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection small and multiple camouflaged objects. Our model achieves state-of-theart results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet. Keywords: Camouflaged Object Detection, Multi-Scale Recursive Network, Multi-Scale Feature Learning, Recursive Feature Refinement"
        },
        {
            "title": "1 Introduction",
            "content": "Camouflaged object detection (COD) is an emerging and challenging domain in computer vision, focusing on the identification and segmentation of objects that blend seamlessly with their surroundings [1]. The complexity of this task arises from the significant similarity between the camouflaged objects and their backgrounds in terms of color, texture, and size. Additional factors, including low light conditions, occlusion, diminutive size, and complex patterns, further complicate the task in certain scenarios [2]. While COD primarily focuses on the recognition of camouflaged objects, such as animals concealed from predators or soldiers in camouflage uniforms [3], it also has considerable relevance across multiple domains and applications. For instance, it proves beneficial in medical imaging for activities such as polyp segmentation [46] and lung infection detection [7], as well as in the management of agricultural operations [8, 9] and in search-and-rescue missions [10]. Furthermore, it contributes to the development of additional vision-related tasks, including transparent object detection [11] and defect identification [12]. COD10K [13] introduced by Fan et al. is the pioneering COD dataset, comprising 5,066 camouflaged images sourced from real-world contexts. Furthermore, they established one of the initial networks for COD, known as SINet, which incorporates dual-module architecture that implements localization succeeded by object segmentation. Subsequently, an enhanced iteration, SINet-v2 [13], was developed utilizing an optimized decoder and an attention mechanism. In the wake of this development, various advanced deep learningbased networks [1416] have emerged to address this challenge. Nonetheless, numerous models continue to struggle to effectively detect camouflaged objects in complex scenarios, particularly in scenes with small or multiple objects, underscoring the need for further improvements. Figure 1 delineates these challenging camouflage scenarios, which entail detecting various objects within scene (rows 1 and 2), small objects (row 3), and tiny objects (row 4). To address these challenges, we propose MSRNet, an innovative transformer-based network for detecting camouflaged objects that leverages multi-scale feature extraction and recursive feedback feature refinement. Our architecture can process images at multiple scales and effectively extract features by means of Pyramid Vision Transformer encoder. This methodology facilitates the comprehension of global context from low-resolution feature MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Fig. 1 Some challenging camouflage scenarios, including: multiple objects (rows 1 and 2), small objects (row 3), and tiny objects (row 4). maps and the detection of local details from high-resolution feature maps. It effectively addresses the challenges of detecting multiple objects within scene, including those of varying sizes, including tiny ones. features integrates multi-scale Furthermore, our model leveraging Attention-Based Scale Integration Units to incorporate the most relevant features selectively. novel recursive-feedback decoding strategy is implemented, recursively acquiring feedback from lower-resolution feature maps to preserve the global contextual information they possess. Our decoder is equipped with Multi-Granularity Fusion Units that enhance the feature representations for more precise detection. By jointly leveraging multi-scale learning with large input scales and applying recursive-feedback feature optimization, MSRNet effectively captures local and global features. This enables the detection of small and multiple camouflaged objects, thereby addressing the challenges illustrated in Figure 1."
        },
        {
            "title": "2 Related Works",
            "content": "Early works on COD [1721] relied on manually crafted features to distinguish camouflaged objects from their backgrounds. While these methods performed well in simple scenes where objects were somewhat visible, they struggled in complex scenes where objects were nearly invisible or occluded. This limitation arose from their restricted feature representation. Therefore, studies [2225] began to focus on incorporating deep learning-based methods that automatically learn features during training. This advancement enabled the learning of 4 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection more robust features, significantly improving segmentation accuracy in such challenging scenes."
        },
        {
            "title": "2.1 CNN-based methods",
            "content": "Lately, researchers have been addressing the COD task by building CNN-based models. Their methods can be categorized into three main approaches: i) The Multi-scale feature aggregation approach [2, 15, 16, 26, 27], which focuses on merging features from different scales or resolutions to capture more details. Following this approach, C2FNet [15] utilized an attention-induced cross-level module for feature fusion and dual-branch module to generate multi-scale representations while leveraging global context. CubeNet [2] employed square fusion decoders to enhance feature representations and sub-edge decoder to improve object boundary modeling. BSA-Net [26] enhanced boundary understanding by utilizing separate attention mechanism. ZoomNet [16] comprises two modules: one for extracting and merging scale-specific features, and the other for identifying mixed-scale features. DGNet [27] concentrated on separately extracting context and texture features before aggregating them to enhance the detection process. ii) The Multi-stage approach [10, 13, 14, 28 30], which breaks the COD task into multiple focused stages, improves the models ability to manage the tasks complexity. Following this approach, SINet [13] and SINetV2 [10] focused on searching for and identifying camouflaged objects. PFNet [14] applied positioning process to detect objects and focusing process to refine predictions. UGTR [28] produced initial predictions and refined them leveraging attention mechanisms. SegMaR [29] employed an iterative refinement strategy incorporating segmentation, magnification, and reiteration processes. The PreyNet [30] framework consisted of two stages: initial detection and predator learning. iii) The Joint training approach [31 35], which involves training the model on several related tasks to enhance its robustness by allowing it to learn from diverse information sources. Adopting this approach, SLSR [31] executed localization, segmentation, and ranking of camouflaged objects. MGL-R [32] performed object and boundary localization, leveraging mutual learning through graph-based reasoning. UJSC [33] conducted salient and camouflaged object detection simultaneously, utilizing the contradictory information of both tasks. BGNet [34] integrated edge semantics to enhance object detection and boundary localization tasks. FEDER [35] simultaneously tackled COD and edge reconstruction."
        },
        {
            "title": "2.2 Transformers-based methods",
            "content": "Transformers have demonstrated their capability to encode global contextual information more effectively than CNNs. Consequently, they have been extensively utilized in various computer vision tasks, including image classification [3638], image segmentation [39, 40], object detection [41], and salient object detection [4244]. Therefore, Transformer-based models have become the new trend in building COD models, aiming to enhance this task. MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 5 Fig. 2 The five decoding strategies in the literature: (a) The progressive decoding strategy, (b) The dense decoding strategy, (c) The feedback decoding strategy, (d) The separate decoding strategy, and (e) The pyramidal decoding strategy. In addition to the previously introduced main approaches, transformerbased COD methods can also be categorized based on their decoding strategy. Five decoding strategies were utilized in the literature: (a) the progressive decoding strategy, where features are progressively refined and decoded from the lowest-resolution features to the highest-resolution features. (b) The dense decoding strategy, where all features from adjacent resolutions are aggregated. (c) The feedback decoding strategy treats the lowest-resolution features separately and aggregates them with the output to enhance global information learning. Alternatively, feedback can be taken from the highest-resolution features when greater focus on local features is required. (d) The separate decoding strategy processes higher-resolution features and lower-resolution features separately to emphasize local and global information equally. Finally, (e) the pyramidal decoding strategy, where adjacent features are aggregated and decoded layer by layer in progressive manner. Figure 2 demonstrates the five decoding strategies utilized in the literature. Following the multi-stage approach to address the COD task, MSCAFNet [45] extracted multi-resolution features, built module to enhance the resolution-specific features, and then employed progressive decoding to fuse them. SARNet [46] built three-stage architecture that extracted multi-resolution features, applied adjacent-resolution and cross-resolution feature fusion, and finally enhanced features with background and foreground attentions. HitNet [47] used an iterative feedback mechanism to refine feature representations across different resolutions. FSPNet [48] progressively enhanced and decoded multi-resolution features with pyramidal shrinking. TPRNet [49] treated features separately, where progressive refinement was applied to high-resolution features and feature interactions were used for lowresolution features to improve the detection process. On the other hand, by implementing the joint training approach, Liu et al. [50] built DTINet, an interactive transformer that detects camouflaged objects and their boundaries utilizing multi-head self-attention. 6 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Table 1 Summary of Transformer-based COD methods. Approach Type Multi-Scale Feature Aggregation Models Primary features ZoomNeXt Merging multi-scale features with attention, then Decoding Strategy Progressive progressively enhancing and decoding them MSCAF-Net Enhancing resolution-specific features, then applying Progressive cross-resolution fusion SARNet Three-stage architecture: Amplify (fusion), Recognize (enhancement) Search (extraction), CamoFormer Progressively enhancing features using foreground, background, and full image attentions Multi-Stage Techniques HitNet Applying iterative feature refinement with feedback from the high-resolution features to preserve fine details Dense + Feedback Progressive Feedback FSPNet TPRNet Joint Training Approach DTINet utilizing pyramidal resolution features shrinking to encode multiPyramidal Applying interactions across low-level features and progressive refinement on high-level features Applying COD and boundary detection utilizing multi-head self-attention Separate Progressive All models mentioned above have been built upon either CNNs or transformers. Some methods [51, 52] experimented with both backbones. CamoFormer [51] was constructed on the multi-stage approach. It used masked separable attention to identify objects and top-down decoder to refine feature representations progressively. While ZoomNeXt [52] adopted the multi-scale feature aggregation approach and built unified pyramid network for static and dynamic COD. ZoomNeXt utilized multi-head scale integration module and feature refinement mechanism. Both studies found that transformer-based models consistently outperform CNN-based models. Table 1 summarizes the transformer-based COD methods categorized by their main approach and decoding strategy."
        },
        {
            "title": "3 Methodology",
            "content": "Our model starts by taking static image as input. This input then passes through several model components to produce probability map ranging from 0 to 1, representing the likelihood that pixel belongs to the camouflaged object. Our approach utilizes an image pyramid that contains multiple scales of the input image. This multi-scale representation enables the extraction of diverse features at each scale, facilitating the detection of camouflaged objects. Our model components include multi-scale feature encoder, scale-merging network, and recursive-feedback feature-refinement decoder. The multi-scale feature encoder extracts features at each scale. The scale-merging network is designed to integrate these features utilizing attention-based scale-integration units (ABSIUs). Moreover, multi-granularity fusion units (MGFUs) within the decoder refine feature representation, enhancing the models accuracy in detecting camouflaged objects in complex scenes. The following subsections provide more details about the models components. Figure 3 demonstrates the overall architecture of the model. MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 7 Fig. 3 The the overall architecture of MSRNet consists of three scales of the original image, each of which is input into PVT for feature extraction, generating four feature maps of different resolutions: f1, f2, f3, and f4. In the next stage, the feature maps of the same resolution across all scales are merged by the Attention-Based Scale Integration Unit (ABSIU). Each merged feature map is further refined inside the decoder using the Multi-Granularity Fusion Unit (MGFU). The Recursive-Feedback decoding strategy combines feedback from all lower resolutions with the current resolution being processed by the MGFU. Fig. 4 Feature Extraction Approach MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection"
        },
        {
            "title": "3.1 Multi-Scale Feature Encoder",
            "content": "To extract deep features, we utilize the well-known Pyramid Vision Transformer PVTv2 [53] as an encoder, excluding its classification head. Channel dimensionality reduction is applied to all extracted feature maps to enhance computational efficiency for subsequent processing. We extract features from three scales of the input image: the original-size input (1), the main scale of the input image, and 1.5 and 2, the two auxiliary scales. This choice of relatively large input scales enhances the networks ability to learn local features, therefore enabling the detection of small and tiny objects. As demonstrated in Figure 4, this setup generates three sets of feature maps, each corresponding to an input scale and comprising four feature maps with different resolutions, corresponding to the number of encoder stages. These feature maps are denoted as , where ranges from 1 to 4, representing the different resolutions, and belongs to {1.0, 1.5, 2.0}, representing the input scales. In the following stages, these features will be passed to the scale-merging network and then to the recursive-feedback feature-refinement decoder, where they will be integrated and refined."
        },
        {
            "title": "3.2 Scale Merging Network",
            "content": "The scale-merging network aims to integrate features extracted from different input scales utilizing the Attention-Based Scale Integration Unit (ABSIU). This unit employs an attention mechanism to merge features while emphasizing the most significant ones and capturing their relationships. Four ABSIUs are utilized, one for each resolution. For instance, the first ABSIU merges the highest-resolution features across the scales, namely, 1.0 4 . This results in merged feature maps for each distinct resolution. 4 , and 2.0 4 , 1.5 Scale Alignment. Before integration, the features of the auxiliary scales 1.5 and 2.0 are resized to align with the main scale feature 1.0 by down-sampling them via combination of max pooling and average pooling. Attention-Based Scale Integration Unit (ABSIU). is designed to integrate features from multiple scales by adopting multi-head spatial attention mechanism. Spatial attention enables focusing on critical regions in feature maps while preserving location-specific information, which is essential for segmentation. Furthermore, utilizing the multi-head attention mechanism enables the model to learn diverse attention patterns. This unit begins by independently processing each feature map from every scale to enhance scale-specific information. The processed feature maps are then concatenated along the channel dimension and passed through 11 convolutional layer to transform them into common space, preparing them for subsequent processing. To generate the multi-head attention maps, the concatenated features are initially divided into four groups, each containing feature maps from all three MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 9 Fig. 5 The diagram illustrates the Attention-Based Scale Integration Unit (ABSIU) for multi-scale feature integration. Features from the three scales (f 1.0, 1.5, 2.0) are first aligned to common resolution and concatenated. The attention mechanism then applies series of convolutional layers followed by Softmax activation layer to generate threechannel attention maps (A1 ), each channel corresponds to different scale. An element-wise multiplication between the attention maps and their corresponding feature maps (F 1 ) is applied, resulting in three scale-grouped processed feature maps that are then summed to produce multi-scale feature maps. This process is repeated for each attention group, yielding four groups of multi-scale features. Lastly, summation across groups merges features from all attention groups, producing the final output ABSIU . , 2 , , A2 , A3 scales. Each attention head processes group by applying series of convolutional layers followed by Softmax activation layer, resulting in an attention map with three channels, one for each scale. The generated attention maps are applied to another copy of the concatenated features to produce the final fused multi-scale output. The concatenated features are first divided into four groups, as in the attention-generating step, ensuring that each attention map corresponds to its respective scale features. After alignment, an element-wise product is computed between each attention map and its corresponding scale features. This results in three groups of processed feature maps, one for each scale. These three scale-grouped feature maps are summed to produce multi-scale feature maps. This process is repeated for each attention group, yielding four groups of multi-scale features. Lastly, summation across groups merges features from all attention groups. Figure 5 illustrates how this unit operates. 10 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Fig. 6 This diagram illustrates the proposed novel decoding strategy. This strategy combines progressive decoding and feedback decoding. The feedback decoding strategy is an advanced recursive feedback mechanism that takes feedback from lower-resolution feature maps and applies it to all subsequent higher-resolution feature maps, enhancing the networks contextual learning."
        },
        {
            "title": "3.3 Recursive-Feedback Feature Refinement Decoder",
            "content": "In this research, we employ novel decoding strategy to enhance the global context learning in our network, thus enabling it to detect multiple objects in scene. The new decoding strategy combines two known decoding strategies, the progressive decoding strategy and an advanced version of the feedback strategy. The decoder progressively combines the multi-resolution feature maps from the lowest to the highest resolution. The advanced feedback decoding strategy is recursive feedback mechanism that takes feedback from lower-resolution feature maps and applies it to all subsequent higher-resolution ones. This recursively preserves global information from lower-resolution feature maps, enabling strong contextual learning within the network. The proposed novel decoding strategy is illustrated in Figure 6. The decoder is responsible for decoding features and refining their representation. In the proposed network, the feature representation refinement process is performed by the Multi-Granularity Fusion Unit (MGFU) within the decoder. As shown in Figure 3, this unit combines the multi-scale feature maps generated by the ABSIUs and all outputs of previous MGFUs while enhancing their representation. After aggregating all features in the last MGFU, they pass through COD head to generate the final prediction map. This head applies up-sampling to restore the original spatial resolution, 33 convolutional layer to reduce the number of channels and refine features, and 11 convolutional layer to compress the channels into single channel feature map. This feature map contains raw model logits. sigmoid activation function is applied to normalize these predictions to the range [0, 1], representing the probability of each pixel belonging to the camouflaged object. Multi-Granularity Fusion Unit (MGFU). is designed to enhance feature representations by analyzing and integrating features across multiple granularities. It processes features in groups and applies cross-channel interactions. MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 11 This unit commences by expanding the feature space with 11 convolutional layer, which increases the number of channels. Subsequently, the expanded features are divided into six-channel groups {gj}6 j=1, enabling specialized processing across various contexts and facilitating the learning of different feature representations. series of convolutional layers is employed to facilitate feature interactions across the various groups. The first groups features are processed directly through convolutional layer to extract fundamental features. The output is partitioned into three parts: one designated for concatenation with the subsequent group g1 1 (to propagate information), one for computing gate value that weighs the significance of the features g2 1, and one representing the features of this group g3 1. In the intermediate groups, the features from the current group are concatenated with those from the preceding group and processed through convolutional layer, enabling the model to acquire more complex features. Each intermediate groups output is split into three parts, similar to the first group. The last group processes its features similarly, but its output is divided into two parts because there is no subsequent group. }6 After processing all groups, the concatenated gate features {g2 j=1 are passed through gating mechanism to generate channel-wise attention maps, highlighting the most essential channels based on their global context. This is achieved by sequentially applying spatial and channel compression, nonlinearity, channel expansion, and normalization. The produced attention maps are then multiplied by another set of concatenated features {g3 j=1, producing reweighted feature maps, enabling the model to focus on the most relevant features. Ultimately, the output is refined, combined with the original input for residual learning, and subjected to ReLU activation function. This process preserves essential information from the original input while ensuring nonlinearity in the final output, as shown in Figure 7. }"
        },
        {
            "title": "3.4 Loss Function",
            "content": "The binary cross-entropy loss (BCE) is commonly used in binary image segmentation tasks. The BCE loss for pixel at position (i,j) is defined as: ℓi,j BCE = gi,j log pi,j (1 gi,j) log(1 pi,j). (1) where gi,j {0, 1} represents the ground truth and pi,j [0, 1] denotes the predicted value. However, relying solely on BCE during training can lead to ambiguous, uncertain predictions due to the tasks inherent complexity. To address this, we use an additional loss, the Uncertainty Awareness Loss (UAL) [52], which enhances model confidence by penalizing predictions with high uncertainty. The UAL is expressed as: ℓi,j UAL = 1 2pi,j 12 (2) 12 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Fig. 7 Demonstration of all processes in the Multi-Granularity Fusion Unit (MGFU). The MGFU module enhances feature representations by analyzing and integrating features from multiple granularities. It processes features across groups with cross-channel interaction, then adaptively fuses them. The total loss function combines both terms as follows: = LBCE + λLUAL, (3) where λ is balancing factor that controls the contribution of the UAL and increases gradually using the cosine strategy."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Datasets. We utilized four datasets for camouflaged object detection: CAMO [54], CHAMELEON [55], COD10K [13], and NC4K [31]. While subset of these datasets encompasses images intended for various tasks, we specifically focused on the COD images. We employed total of 10,513 images from the following datasets: 1,250 from CAMO, 76 from CHAMELEON, 5,066 from COD10K, and 4,121 from NC4K. Consistent with benchmark practices [13, 16, 27, 52], we allocated 1,000 images from the CAMO dataset and 3,040 images from the COD10K dataset for training, while reserving the remaining for testing. Evaluation Metrics. To evaluate the performance of our image COD model, we employ five widely recognized metrics: (1) The Structure-measure (Sm) [56], MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 13 which assesses the spatial structure of the detected object; (2) The F-measure (Fβ), provides balanced measure of precision and recall; (3) The Weighted F-measure (F ω β ) [57], an enhanced version that offers more reliable evaluation outcomes; (4) The Mean Absolute Error (MAE), which calculates the elementwise difference between the predicted map and the ground truth; (5) The E-measure (Em) [58], which evaluates pixel-level matchings and image-level statistics simultaneously. Implementation Details. The proposed model was built in PyTorch [58] on an NVIDIA RTX A6000 GPU. The training configurations are consistent with the current best practices [13, 16, 27, 52]. The encoders parameters were initialized with those of the PVTv2 encoder pre-trained on ImageNet, while the other model components were initialized randomly. The Adam optimizer was utilized to update the model parameters, with betas set to (0.9, 0.999). The learning rate was set to 0.0001, and stepwise decay was used. The model was trained for 150 epochs, with batch size of 8. During training, the input and ground truth images were bilinearly interpolated to 384384. At testing, input images, prediction maps, and ground truth images were interpolated to 384384. To conduct fair comparison with other methods, we experimented with different input sizes to match their settings. Data augmentation techniques enhanced the training dataset by applying random flipping, rotation, and color jittering."
        },
        {
            "title": "4.2 Results and Comparison",
            "content": "In this section, we present comprehensive quantitative and qualitative comparisons between our model and the current state-of-the-art methods (SOTA). All results for SOTA methods are obtained from their published works. Quantitative Comparisons. We compare our model with 20 SOTA models. Table 2 presents the results obtained from all methods applied to the mentioned COD datasets. It includes the employed backbones, input image dimensions, and the total number of parameters utilized. In the context of CNN-based methodologies, EfficientNet-based models demonstrate superior performance compared to ResNet-based models, particularly for feature extraction tasks. Additionally, our CNN-based model surpasses all SOTA models across all datasets examined. Although CNN-based methods achieve good results on this complex task, the results indicate that vision-based methods consistently outperform them. In particular, our vision-based model achieves state-of-the-art results on the CAMO10K and NC4K datasets without needing any additional training data. Furthermore, our model secures the second rank on both the CAMO and CHAMELEON datasets, trailing only behind ZoomNeXt [52] and SARNet [46], respectively. Moreover, experimental results show that our model typically has fewer parameters than models using the same backbone and configurations, indicating reduced computational burden. 14 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Table 2 Results of different models based on different backbones on static image COD datasets. The highest three results are colored in red (1st), green (2nd), and blue (3rd). Model Backbone Input Size #Params Sm CAMO β MAE ω Fβ Em Sm CHAMELEON β MAE ω Fβ ResNet-50 [59] SINet [13] ResNet-50 [59] C2FNet [15] Res2Net-50 [60] SINetV2 [10] SegMaR [29] ResNet-50 [59] CamoFormer-R [51] ResNet-50 [59] ResNet-50 [59] ZoomNeXt [52] ResNet-50 [59] Ours EfficientNet-B4 [61] DGNet [27] EfficientNet-B4 [61] ZoomNeXt [52] EfficientNet-B4 [61] ZoomNeXt [52] EfficientNet-B4 [61] Ours EfficientNet-B4 [61] Ours ResNet-50 [59] SLSR [31] ResNet-50 [59] MGL-R [32] ResNet-50 [59] PFNet [14] ResNet-50 [59] UJSC [33] ResNet-50 [59] UGTR [28] ResNet-50 [59] ZoomNet [16] Res2Net-50 [60] BSA-Net [26] Res2Net-50 [60] BGNet [34] ResNet-50 [59] FEDER [35] ResNet-50 [59] ZoomNeXt [52] ResNet-50 [59] Ours CamoFormer-P [51] PVTv2-B4 [53] PVTv2-B4 [53] ZoomNeXt [52] PVTv2-B4 [53] Ours PVTv2-B2 [53] MSCAF-Net [45] PVTv2-B2 [53] ZoomNeXt [52] PVTv2-B2 [53] Ours PVTv2-B2 [53] HitNet [47] ViT-B/16 [36] FSPNet [48] PVTv2-B3 [53] SARNet [46] PVTv2-B2 [53] ZoomNeXt [52] PVTv2-B3 [53] ZoomNeXt [52] PVTv2-B4 [53] ZoomNeXt [52] PVTv2-B5 [53] ZoomNeXt [52] PVTv2-B2 [53] Ours PVTv2-B3 [53] Ours PVTv2-B4 [53] Ours PVTv2-B5 [53] Ours 352352 352352 352352 352352 352352 352352 352352 352352 352352 384384 352352 384384 480480 473473 416416 480480 473473 384384 384384 416416 384384 384384 384384 352352 352352 352352 352352 352352 352352 704704 384384 672672 384384 384384 384384 384384 384384 384384 384384 384384 0.745 48.947M 0.796 28.411M 0.820 26.976M 0.815 56.215M 0.817 71.403M 0.822 28.458M 0.820 28.458M 0.838 18.113M 21.381M 0.859 21.381M 0.867 21.381M 0.868 21.381M 0.875 0.787 50.935M 0.775 63.595M 46.498M 0.782 217.982M 0.800 0.784 48.868M 0.820 32.382M 0.794 32.585M 0.812 79.853M 0.802 44.129M 0.833 28.458M 0.816 28.458M 71.403M 0.872 65.374M 0.893 65.373M 0.889 0.873 30.364M 0.868 28.181M 0.869 28.180M 25.727M 0.849 274.240M 0.856 0.874 47.477M 0.874 28.181M 48.056M 0.885 65.374M 0.888 84.774M 0.889 0.873 28.180M 48.056M 0.885 65.373M 0.888 84.773M 0. 0.644 0.719 0.743 0.753 0.752 0.760 0.755 0.768 0.815 0.824 0.827 0.838 0.696 0.673 0.695 0.728 0.684 0.752 0.717 0.749 0.738 0.774 0.754 0.831 0.862 0.861 0.828 0.829 0.832 0.809 0.799 0.844 0.839 0.854 0.859 0.857 0.838 0.855 0.861 0.860 Convolutional Neural Network based Methods 0.827 0.844 0.835 0.872 0.867 0.878 0.881 0.834 0.877 0.879 0.888 0.891 0.841 0.834 0.828 0.848 0.819 0.864 0.858 0.860 0.851 0.874 0.888 0.034 0.032 0.030 0.025 0.025 0.020 0.021 0.029 0.020 0.020 0.020 0.019 0.030 0.030 0.033 0.030 0.031 0.023 0.027 0.027 0.030 0.021 0.020 0.829 0.864 0.895 0.884 0.885 0.885 0.879 0.914 0.920 0.925 0.927 0.936 0.854 0.842 0.855 0.873 0.851 0.892 0.867 0.882 0.873 0.891 0.872 0.702 0.762 0.782 0.795 0.792 0.797 0.792 0.805 0.845 0.852 0.855 0.863 0.744 0.726 0.746 0.772 0.735 0.794 0.763 0.789 0.781 0.813 0. 0.872 0.888 0.888 0.906 0.898 0.912 0.913 0.890 0.911 0.911 0.920 0.923 0.890 0.893 0.882 0.891 0.887 0.902 0.895 0.901 0.887 0.908 0.918 0.806 0.828 0.816 0.860 0.847 0.863 0.867 0.816 0.864 0.865 0.878 0.881 0.822 0.812 0.810 0.833 0.794 0.845 0.841 0.851 0.835 0.858 0.876 0.091 0.080 0.070 0.071 0.067 0.069 0.070 0.057 0.049 0.046 0.047 0.045 0.080 0.088 0.085 0.073 0.086 0.066 0.079 0.073 0.071 0.065 0.071 Vision Transformer based Methods 0.046 0.040 0.041 0.046 0.049 0.050 0.055 0.050 0.046 0.047 0.042 0.040 0.041 0.047 0.043 0.040 0.041 0.854 0.881 0.878 0.852 0.855 0.854 0.831 0.831 0.866 0.863 0.872 0.878 0.875 0.860 0.874 0.878 0. 0.938 0.949 0.944 0.937 0.926 0.924 0.910 0.928 0.935 0.931 0.942 0.943 0.945 0.928 0.941 0.942 0.943 0.910 0.929 0.930 0.912 0.916 0.931 0.921 0.908 0.933 0.922 0.927 0.925 0.924 0.931 0.933 0.932 0.925 0.865 0.894 0.907 0.865 0.876 0.901 0.897 0.851 0.909 0.884 0.898 0.897 0.885 0.904 0.907 0.908 0.893 0.022 0.018 0.017 0.022 0.018 0.017 0.019 0.023 0.017 0.017 0.017 0.016 0.018 0.016 0.016 0.017 0.017 0.882 0.906 0.915 0.876 0.889 0.911 0.900 0.867 0.915 0.896 0.905 0.906 0.896 0.912 0.915 0.916 0.903 Em Sm ω β MAE Fβ Em Sm COD10K NC4K β MAE ω Fβ Em 0.946 0.946 0.961 0.959 0.956 0.969 0.967 0.956 0.965 0.964 0.968 0.970 0.948 0.941 0.945 0.955 0.940 0.958 0.957 0.954 0.954 0.963 0. 0.966 0.977 0.979 0.970 0.971 0.979 0.972 0.965 0.978 0.970 0.977 0.973 0.975 0.976 0.973 0.978 0.971 0.776 0.813 0.815 0.833 0.838 0.855 0.865 0.822 0.868 0.875 0.883 0.887 0.804 0.814 0.800 0.809 0.817 0.838 0.818 0.831 0.822 0.861 0.868 0.869 0.895 0.905 0.865 0.881 0.893 0.871 0.851 0.885 0.887 0.895 0.898 0.898 0.894 0.904 0.907 0.902 0.631 0.686 0.680 0.724 0.724 0.758 0.777 0.692 0.789 0.797 0.808 0.814 0.673 0.666 0.660 0.684 0.666 0.729 0.699 0.722 0.716 0.768 0.786 0.786 0.825 0.848 0.775 0.809 0.827 0.806 0.735 0.820 0.818 0.829 0.838 0.827 0.829 0.847 0.852 0.844 0.043 0.036 0.037 0.034 0.029 0.026 0.024 0.033 0.023 0.021 0.020 0.020 0.037 0.035 0.040 0.035 0.036 0.029 0.034 0.033 0.032 0.026 0. 0.023 0.018 0.016 0.024 0.020 0.019 0.023 0.026 0.021 0.019 0.018 0.017 0.018 0.018 0.017 0.016 0.017 0.679 0.723 0.718 0.757 0.753 0.791 0.808 0.727 0.818 0.824 0.832 0.838 0.715 0.710 0.701 0.721 0.711 0.766 0.738 0.753 0.751 0.801 0.816 0.811 0.845 0.865 0.798 0.834 0.848 0.823 0.769 0.839 0.841 0.848 0.857 0.848 0.849 0.865 0.868 0.862 0.874 0.900 0.906 0.906 0.930 0.926 0.930 0.911 0.937 0.941 0.945 0.947 0.892 0.890 0.890 0.891 0.890 0.911 0.901 0.911 0.905 0.925 0.934 0.939 0.954 0.962 0.936 0.945 0.950 0.938 0.930 0.947 0.948 0.952 0.955 0.956 0.952 0.959 0.962 0.957 0.808 0.838 0.847 0.841 0.855 0.869 0.872 0.857 0.878 0.884 0.889 0.889 0.840 0.833 0.829 0.842 0.839 0.853 0.842 0.851 0.847 0.874 0. 0.892 0.899 0.904 0.887 0.890 0.893 0.875 0.878 0.889 0.892 0.900 0.900 0.903 0.894 0.903 0.905 0.903 0.723 0.762 0.770 0.781 0.788 0.808 0.816 0.783 0.832 0.837 0.843 0.844 0.765 0.739 0.745 0.771 0.746 0.784 0.771 0.788 0.789 0.816 0.814 0.847 0.859 0.870 0.838 0.848 0.852 0.834 0.816 0.851 0.852 0.861 0.865 0.863 0.853 0.867 0.873 0.871 0.058 0.049 0.048 0.046 0.042 0.038 0.038 0.042 0.035 0.032 0.032 0.031 0.048 0.053 0.053 0.046 0.052 0.043 0.048 0.044 0.044 0.037 0.039 0.030 0.029 0.027 0.032 0.031 0.030 0.037 0.035 0.032 0.030 0.028 0.028 0.028 0.030 0.027 0.026 0.027 0.769 0.794 0.805 0.821 0.821 0.836 0.844 0.813 0.857 0.862 0.866 0.866 0.804 0.782 0.784 0.806 0.787 0.818 0.808 0.820 0.824 0.846 0. 0.868 0.879 0.887 0.860 0.872 0.874 0.854 0.843 0.872 0.874 0.880 0.884 0.884 0.874 0.886 0.890 0.889 0.883 0.904 0.914 0.907 0.914 0.925 0.926 0.922 0.932 0.939 0.942 0.943 0.907 0.893 0.898 0.907 0.899 0.912 0.907 0.916 0.915 0.928 0.925 0.946 0.949 0.952 0.942 0.941 0.941 0.929 0.937 0.940 0.943 0.949 0.949 0.951 0.943 0.952 0.953 0.952 Qualitative Comparisons. Figure 8 provides comparative analysis of our model against the top five SOTA methods, using sample images drawn from different datasets. These samples thoroughly illustrate the challenging aspects of the COD task, including variations in object sizes, differing proportions of fine details, and objects with indistinguishable boundaries seamlessly integrated into their backgrounds. The visual results indicate that our model demonstrates superior performance compared to other methods in multiple aspects, such as the ability to capture finer details (all rows), better-defined object areas (rows 1, 2, 5, and 6), more precise corners (rows 3 and 7), detection of small-sized objects (row 5), and the identification of multiple objects present within single scene (row 6). Our model was built with the primary objective of overcoming the limitations of SOTA models in detecting small, multiple camouflaged objects. To improve the detection of small objects, we used larger input scales in our multi-scale feature extraction module, thereby enhancing local feature learning. Furthermore, we introduced novel recursive feedback decoding strategy to strengthen global context learning, enabling our model to better detect multiple objects. Figure 9 shows the success of our methods, allowing the model to detect multiple objects (rows 1-3), small objects (rows 4 and 5), and even tiny objects (rows 6 and 7). This proves the superiority of our model, as it succeeded in complex scenarios where competing SOTA models failed, either wholly or partially. Figure 10 shows some instances where our model does not perform detection perfectly. These instances include detecting minor false areas (rows 1 and 3), failing to identify small parts of an object (row 4), and overlooking some fine details of objects (row 2). However, these failures are minor, and our model still performs better than other SOTA models on identical samples. MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 15 Fig. 8 Visual Comparison between our model and competing SOTA methods on sample images from all datasets."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "In this section, we conduct an ablation study on various components of our model to analyze their impact on overall performance. These components include the encoder, decoder, input shape, and input scales. Table 3 shows the results of all experiments conducted in this ablation study. The base model B0 is fully equipped with all components. This model features PVTv2-B2 as the encoder, an input shape of 352352, input scales of 0.5, 1.0, and 1.5, and employs progressive decoding strategy. This approach relies on progressive feature refinement, where features evolve sequentially through the architecture, allowing for minimal cross-resolution 16 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Fig. 9 Visual Comparison illustrating the superiority of our model in detecting multiple (rows 1-3), small (rows 4 and 5), and tiny (rows 6 and 7) camouflaged objects. interaction and no feedback connections between non-adjacent resolutions. In the following experiments, we modify one component at time. We commenced by studying different decoding strategies. For model M1, we implemented combination of progressive and recursive feedback decoding strategy (as used in MSRNet), where, for each resolution, decoded features from all preceding resolutions are combined with the current-resolution features. This approach provides recursive preservation of global information from lower-resolution feature maps, enabling strong contextual learning within the network and allowing the detection of multiple camouflaged objects in single scene. This approach increases the overall performance by 0.21% compared to the base model. For model M2, we adopted dense progressive decoding strategy. In this context, before refining each resolutions features, all features from the preceding resolutions are combined with the current resolution. This methodology resulted in performance decline of -0.73%. This MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 17 Fig. 10 Some instances where our model does not perform detection perfectly compared to the GT. While the model occasionally (a) detects minor false regions (rows 1 and 3), (b) misses small object parts (row 4), or (c) overlooks fine details (row 2), it outperforms SOTA methods on these challenging samples. outcome may be attributed to the potential disruption of their meticulously learned scale-specific features when raw features from varying non-adjacent resolutions are combined before refining. For instance, coarse-level features at lower resolutions may overpower fine features at higher resolutions. We implemented combination of both decoding strategies in model M3, employing dense-progressive and recursive-feedback decoding. This strategy led to performance degradation of -0.35%, indicating smaller decline than M2. These experiments demonstrate the effectiveness of the recursive feedback decoding strategy in improving overall model performance. They also illustrate the negative impact of aggregating unrefined multi-resolution features, particularly features of non-adjacent resolutions, where features operate at different abstraction levels that might conflict. Consequently, in the subsequent experiments, we shall build upon the recursive-feedback decoding strategy implemented in M1 to evaluate other model components. Furthermore, we investigated the impact of altering the input shape in M4. This experiment employed larger input shape (384384), yielding performance improvement of 1.42%. Larger input shapes correspond to higher resolutions, helping preserve fine details that might otherwise be lost. This is crucial characteristic for detecting camouflaged objects. Regarding the encoder, we analyze the effects of different backbone architectures on performance. In model M5, we employ the PVTv2-B3 backbone, which improves performance by 3.01% over the base model. The PVTv2-B4 in M6 improves performance by 4.63%, while PVTv2-B5 in M7 increases performance by 3.41%. These experiments indicate that PVTv2-B4 delivers the best results. We conducted experiments utilizing various sets of input scales. The M8 model employed input scales of 1.0, 1.5, and 1.7, leading to performance increase of 4.54%. While model M9 utilized input scales of 1.0, 1.5, 18 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection Table 3 Ablation study on various model components, including decoder, input shape, encoder, and input scales. RFD stands for Recursive-Feedback Decoding, DPD means Dense Progressive Decoding, DRFD denotes Dense Recursive-Feedback Decoding, and IS stands for input size and input scales Baseline Models No B0 M1 B0 + RFD M2 B0 + DPD M3 B0 + DRFD M4 M1 + IS (384384) M5 M4 + PVTv2-B3 M6 M4 + PVTv2-B4 M7 M4 + PVTv2-B5 M8 M6 + IS (1, 1.5, 1.7) M9 M6 + IS (1, 1.5, 2) CAMO CHAMELEON COD10K Params Sm 28.18M 0.868 28.18M 0.866 28.18M 0.867 28.18M 0.867 28.18M 0.873 48.06M 0.880 65.37M 0.888 84.77M 0.883 65.37M 0.889 65.37M 0.887 ω 0.829 0.829 0.826 0.828 0.834 0.849 0.862 0.854 0.862 0. β MAE 0.049 0.049 0.049 0.049 0.046 0.043 0.039 0.042 0.040 0.041 Fβ 0.855 0.854 0.850 0.853 0.856 0.869 0.881 0.871 0.880 0.877 Em 0.926 0.925 0.923 0.925 0.930 0.938 0.947 0.942 0.945 0.944 Sm 0.916 0.918 0.914 0.915 0.923 0.926 0.929 0.922 0.929 0.934 ω 0.876 0.881 0.874 0.877 0.888 0.897 0.902 0.888 0.905 0.911 β MAE 0.018 0.018 0.020 0.019 0.017 0.017 0.016 0.017 0.017 0. Fβ 0.889 0.892 0.888 0.890 0.900 0.905 0.911 0.896 0.915 0.918 Em 0.971 0.969 0.962 0.970 0.977 0.975 0.980 0.977 0.974 0.979 Sm 0.881 0.883 0.883 0.883 0.886 0.893 0.900 0.894 0.904 0.908 ω 0.809 0.810 0.810 0.809 0.816 0.828 0.842 0.831 0.848 0.853 β MAE 0.020 0.020 0.020 0.020 0.019 0.018 0.017 0.018 0.017 0.016 Fβ 0.834 0.834 0.834 0.832 0.839 0.848 0.861 0.850 0.865 0. Em 0.945 0.943 0.942 0.943 0.945 0.951 0.958 0.954 0.962 0.962 Sm 0.890 0.891 0.890 0.890 0.891 0.898 0.901 0.902 0.903 0.904 NC4K ω 0.848 0.850 0.846 0.847 0.851 0.861 0.867 0.869 0.869 0.871 β MAE 0.031 0.030 0.031 0.031 0.030 0.028 0.028 0.027 0.027 0.027 Fβ 0.872 0.872 0.869 0.869 0.874 0.881 0.886 0.886 0.887 0. Em 0.941 0.941 0.940 0.940 0.942 0.948 0.951 0.952 0.951 0.951 0.0% 0.21% 0.73% 0.35% 1.42% 3.01% 4.63% 3.41% 4.54% 5.12% and 2.0, achieving the highest performance increase of 5.12%. The results from M9 indicate that using higher input scales enhances overall performance. This is because higher scales correspond to higher resolutions, enabling the model to identify finer features, which is essential for detecting small and tiny, camouflaged objects. Furthermore, model M8 illustrates the importance of using well-distributed scales. Although the scales in this experiment are higher than those of M6, they generated lower performance because the proximity of the scales (e.g., 1.5 and 1.7) may lead to similar feature representations, consequently failing to provide novel information to the network."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose transformer-based multi-scale recursive network (MSRNet) to overcome the challenges of detecting small and multiple camouflaged objects. Our approach extracts multi-scale features via pyramid vision transformer and selectively merges them with specialized AttentionBased Scale Integration Units. To further enhance feature representations, we introduce Multi-Granularity Fusion Units. novel recursive-feedback decoding strategy that preserves global information is developed to enable the detection of multiple objects. We employ large input scales to improve the learning of fine features, allowing the detection of small objects. Extensive experiments across four benchmark COD datasets with 20 SOTA models show that our model achieves SOTA performance on two datasets and ranks second on the other two. Moreover, visual results highlight our models superior ability to detect small, camouflaged, and multiple objects. Notwithstanding the robust performance of our model, extracting multiscale features necessitates increased computational resources. Furthermore, our model is presently designed to apply COD to static images. Future work could focus on optimizing the computational efficiency of the feature-extraction methodology and on investigating the possibility of extending the model to encompass video-based COD."
        },
        {
            "title": "References",
            "content": "[1] Bhajantri, N.U., Nagabhushan, P.: Camouflage defect identification: MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 19 novel approach. In: 9th International Conference on Information Technology (ICIT06), pp. 145148 (2006). IEEE [2] Zhuge, M., Lu, X., Guo, Y., Cai, Z., Chen, S.: Cubenet: X-shape connection for camouflaged object detection. Pattern Recognition 127, 108644 (2022) [3] Singh, S.K., Dhawale, C.A., Misra, S.: Survey of object detection methods in camouflaged image. Ieri Procedia 4, 351357 (2013) [4] Fan, D.-P., Ji, G.-P., Zhou, T., Chen, G., Fu, H., Shen, J., Shao, L.: Pranet: Parallel reverse attention network for polyp segmentation. In: International Conference on Medical Image Computing and Computerassisted Intervention, pp. 263273 (2020). Springer [5] Ji, G.-P., Xiao, G., Chou, Y.-C., Fan, D.-P., Zhao, K., Chen, G., Van Gool, L.: Video polyp segmentation: deep learning perspective. Machine Intelligence Research 19(6), 531549 (2022) [6] Zhao, X., Zhang, L., Lu, H.: Automatic polyp segmentation via multiscale subtraction network. In: Medical Image Computing and Computer Assisted InterventionMICCAI 2021: 24th International Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part 24, pp. 120130 (2021). Springer [7] Fan, D.-P., Zhou, T., Ji, G.-P., Zhou, Y., Chen, G., Fu, H., Shen, J., Shao, L.: Inf-net: Automatic covid-19 lung infection segmentation from ct images. IEEE transactions on medical imaging 39(8), 26262637 (2020) [8] Liu, L., Wang, R., Xie, C., Yang, P., Wang, F., Sudirman, S., Liu, W.: Pestnet: An end-to-end deep learning approach for large-scale multi-class pest detection and classification. Ieee Access 7, 4530145312 (2019) [9] Rizzo, M., Marcuzzo, M., Zangari, A., Gasparetto, A., Albarelli, A.: Fruit ripeness classification: survey. Artificial Intelligence in Agriculture 7, 4457 (2023) [10] Fan, D.-P., Ji, G.-P., Cheng, M.-M., Shao, L.: Concealed object detection. IEEE transactions on pattern analysis and machine intelligence 44(10), 60246042 (2021) [11] Khaing, M.P., Masayuki, M.: Transparent object detection using convolutional neural network. In: Big Data Analysis and Deep Learning Applications: Proceedings of the First International Conference on Big Data Analysis and Deep Learning 1st, pp. 8693 (2019). Springer [12] Zeng, N., Wu, P., Wang, Z., Li, H., Liu, W., Liu, X.: small-sized 20 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection object detection oriented multi-scale feature fusion approach with application to defect detection. IEEE Transactions on Instrumentation and Measurement 71, 114 (2022) [13] Fan, D.-P., Ji, G.-P., Sun, G., Cheng, M.-M., Shen, J., Shao, L.: Camouflaged object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 27772787 (2020) [14] Mei, H., Ji, G.-P., Wei, Z., Yang, X., Wei, X., Fan, D.-P.: Camouflaged object segmentation with distraction mining. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 87728781 (2021) [15] Sun, Y., Chen, G., Zhou, T., Zhang, Y., Liu, N.: Context-aware crosslevel fusion network for camouflaged object detection. arXiv preprint arXiv:2105.12555 (2021) [16] Pang, Y., Zhao, X., Xiang, T.-Z., Zhang, L., Lu, H.: Zoom in and out: mixed-scale triplet network for camouflaged object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21602170 (2022) [17] Beiderman, Y., Teicher, M., Garcia, J., Mico, V., Zalevsky, Z.: Optical technique for classification, recognition and identification of obscured objects. Optics communications 283(21), 42744282 (2010) [18] Galun, Sharon, Basri, Brandt: Texture segmentation by multiscale aggregation of filter responses and shape elements. In: Proceedings Ninth IEEE International Conference on Computer Vision, pp. 716723 (2003). IEEE [19] Guo, H., Dou, Y., Tian, T., Zhou, J., Yu, S.: robust foreground segmentation method by temporal averaging multiple video frames. In: 2008 International Conference on Audio, Language and Image Processing, pp. 878882 (2008). IEEE [20] Hall, J.R., Cuthill, I.C., Baddeley, R., Shohet, A.J., Scott-Samuel, N.E.: Camouflage, detection and identification of moving targets. Proceedings of the Royal Society B: Biological Sciences 280(1758), 20130064 (2013) [21] Zhang, X., Zhu, C., Wang, S., Liu, Y., Ye, M.: bayesian approach to camouflaged moving object detection. IEEE transactions on circuits and systems for video technology 27(9), 20012013 (2016) [22] Cheng, S., Ji, G.-P., Qin, P., Fan, D.-P., Zhou, B., Xu, P.: Large model based referring camouflaged object detection. arXiv preprint arXiv:2311.17122 (2023) MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 21 [23] Luo, X.-J., Wang, S., Wu, Z., Sakaridis, C., Cheng, Y., Fan, D.-P., Van Gool, L.: Camdiff: Camouflage image augmentation via diffusion model. arXiv preprint arXiv:2304.05469 (2023) [24] Luo, Z., Liu, N., Zhao, W., Yang, X., Zhang, D., Fan, D.-P., Khan, F., Han, J.: Vscode: General visual salient and camouflaged object detection with 2d prompt learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1716917180 (2024) [25] Zhang, X., Yin, B., Lin, Z., Hou, Q., Fan, D.-P., Cheng, M.-M.: Referring camouflaged object detection. arXiv preprint arXiv:2306.07532 (2023) [26] Zhu, H., Li, P., Xie, H., Yan, X., Liang, D., Chen, D., Wei, M., Qin, J.: can find you! boundary-guided separated attention network for camouflaged object detection. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 36083616 (2022) [27] Ji, G.-P., Fan, D.-P., Chou, Y.-C., Dai, D., Liniger, A., Van Gool, L.: Deep gradient learning for efficient camouflaged object detection. Machine Intelligence Research 20(1), 92108 (2023) [28] Yang, F., Zhai, Q., Li, X., Huang, R., Luo, A., Cheng, H., Fan, D.-P.: Uncertainty-guided transformer reasoning for camouflaged object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41464155 (2021) [29] Jia, Q., Yao, S., Liu, Y., Fan, X., Liu, R., Luo, Z.: Segment, magnify and reiterate: Detecting camouflaged objects the hard way. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 47134722 (2022) [30] Zhang, M., Xu, S., Piao, Y., Shi, D., Lin, S., Lu, H.: Preynet: Preying on camouflaged objects. In: Proceedings of the 30th ACM International Conference on Multimedia, pp. 53235332 (2022) [31] Lv, Y., Zhang, J., Dai, Y., Li, A., Liu, B., Barnes, N., Fan, D.-P.: Simultaneously localize, segment and rank the camouflaged objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1159111601 (2021) [32] Zhai, Q., Li, X., Yang, F., Chen, C., Cheng, H., Fan, D.-P.: Mutual graph learning for camouflaged object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1299713007 (2021) [33] Li, A., Zhang, J., Lv, Y., Liu, B., Zhang, T., Dai, Y.: Uncertainty-aware joint salient object and camouflaged object detection. In: Proceedings of 22 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1007110081 (2021) [34] Sun, Y., Wang, S., Chen, C., Xiang, T.-Z.: Boundary-guided camouflaged object detection. arXiv preprint arXiv:2207.00794 (2022) [35] He, C., Li, K., Zhang, Y., Tang, L., Zhang, Y., Guo, Z., Li, X.: Camouflaged object detection with feature decomposition and edge reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2204622055 (2023) [36] Dosovitskiy, A.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020) [37] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning, pp. 1034710357 (2021). PMLR [38] Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 568578 (2021) [39] Jiang, Z.-H., Hou, Q., Yuan, L., Zhou, D., Shi, Y., Jin, X., Wang, A., Feng, J.: All tokens matter: Token labeling for training better vision transformers. Advances in neural information processing systems 34, 1859018602 (2021) [40] Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems 34, 1207712090 (2021) [41] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: European Conference on Computer Vision, pp. 213229 (2020). Springer [42] Gu, Y., Wang, L., Wang, Z., Liu, Y., Cheng, M.-M., Lu, S.-P.: Pyramid constrained self-attention network for fast video salient object detection. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1086910876 (2020) [43] Liu, N., Zhang, N., Wan, K., Shao, L., Han, J.: Visual saliency transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 47224732 (2021) MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection 23 [44] Zhuge, M., Fan, D.-P., Liu, N., Zhang, D., Xu, D., Shao, L.: Salient object detection via integrity learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(3), 37383752 (2022) [45] Liu, Y., Li, H., Cheng, J., Chen, X.: Mscaf-net: general framework for camouflaged object detection via learning multi-scale context-aware features. IEEE Transactions on Circuits and Systems for Video Technology 33(9), 49344947 (2023) [46] Xing, H., Gao, S., Wang, Y., Wei, X., Tang, H., Zhang, W.: Go closer to see better: Camouflaged object detection via object area amplification and figure-ground conversion. IEEE Transactions on Circuits and Systems for Video Technology 33(10), 54445457 (2023) [47] Hu, X., Wang, S., Qin, X., Dai, H., Ren, W., Luo, D., Tai, Y., Shao, L.: High-resolution iterative feedback network for camouflaged object detection. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 881889 (2023) [48] Huang, Z., Dai, H., Xiang, T.-Z., Wang, S., Chen, H.-X., Qin, J., Xiong, H.: Feature shrinkage pyramid for camouflaged object detection with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 55575566 (2023) [49] Zhang, Q., Ge, Y., Zhang, C., Bi, H.: Tprnet: camouflaged object detection via transformer-induced progressive refinement network. The Visual Computer 39(10), 45934607 (2023) [50] Liu, Z., Zhang, Z., Tan, Y., Wu, W.: Boosting camouflaged object detection with dual-task interactive transformer. In: 2022 26th International Conference on Pattern Recognition (ICPR), pp. 140146 (2022). IEEE [51] Yin, B., Zhang, X., Fan, D.-P., Jiao, S., Cheng, M.-M., Van Gool, L., Hou, Q.: Camoformer: Masked separable attention for camouflaged object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024) [52] Pang, Y., Zhao, X., Xiang, T.-Z., Zhang, L., Lu, H.: Zoomnext: unified collaborative pyramid network for camouflaged object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024) [53] Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media 8(3), 415424 (2022) [54] Le, T.-N., Nguyen, T.V., Nie, Z., Tran, M.-T., Sugimoto, A.: Anabranch network for camouflaged object segmentation. Computer vision and image 24 MSRNet: Multi-Scale Recursive Network for Camouflaged Object Detection understanding 184, 4556 (2019) [55] Przemys(cid:32)law, S., Hassan, A., Jakub, B., Tomasz, D., Adam, K., Kozie(cid:32)l, P.: Animal camouflage analysis: Chameleon database. Unpublished manuscript 2(6), 7 (2018) [56] Fan, D.-P., Cheng, M.-M., Liu, Y., Li, T., Borji, A.: Structure-measure: new way to evaluate foreground maps. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 45484557 (2017) [57] Margolin, R., Zelnik-Manor, L., Tal, A.: How to evaluate foreground maps? In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255 (2014) [58] Fan, D.-P., Gong, C., Cao, Y., Ren, B., Cheng, M.-M., Borji, A.: Enhanced-alignment measure for binary foreground map evaluation. arXiv preprint arXiv:1805.10421 (2018) [59] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778 (2016) [60] Gao, S.-H., Cheng, M.-M., Zhao, K., Zhang, X.-Y., Yang, M.-H., Torr, P.: Res2net: new multi-scale backbone architecture. IEEE transactions on pattern analysis and machine intelligence 43(2), 652662 (2019) [61] Tan, M.: Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 (2019)"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Software Engineering, University of Western Australia, Perth 6009, Australia",
        "Department of Computer Science, National University of Computer and Emerging Sciences, Peshawar 24720, Pakistan",
        "Electronic Systems Engineering, University of Regina, Regina S4S 0A2, Canada",
        "Faculty of Science, Ontario Tech University, Oshawa L1G 0C5, Canada",
        "Information and Computer Science, King Fahd University of Petroleum and Minerals, Dhahran 31261, Saudi Arabia"
    ]
}