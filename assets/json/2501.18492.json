{
    "paper_title": "GuardReasoner: Towards Reasoning-based LLM Safeguards",
    "authors": [
        "Yue Liu",
        "Hongcheng Gao",
        "Shengfang Zhai",
        "Jun Xia",
        "Tianyi Wu",
        "Zhiwei Xue",
        "Yulin Chen",
        "Kenji Kawaguchi",
        "Jiaheng Zhang",
        "Bryan Hooi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 9 4 8 1 . 1 0 5 2 : r GuardReasoner: Towards Reasoning-based LLM Safeguards Yue Liu 1 Hongcheng Gao 2 Shengfang Zhai 1 3 Jun Xia 4 Tianyi Wu 1 Zhiwei Xue 1 Yulin Chen 1 Kenji Kawaguchi 1 Jiaheng Zhang 1 Bryan Hooi 1 Abstract As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains key challenge. This paper proposes GuardReasoner, new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner1. Warning: this paper contains potentially harmful text. 1. Introduction Large Language Models (LLMs) (Achiam et al., 2023; Reid et al., 2024; Team, 2024a; Jiang et al., 2024; Dubey et al., 2024; Hui et al., 2024; Liu et al., 2024a) are revolutionizing numerous domains, including chatbots (OpenAI, 2022), search engines (OpenAI, 2024), and software engineering (CognitionAI, 2024). As these AI-powered tools become more deeply integrated into our daily lives, it is crucial to ensure their safety and reliability. However, recent attacks (Guo et al., 2024; Formento et al., 2023; Liu et al., 2024c) show their susceptibility to malicious manipulation. To alleviate this issue, companies have developed guard models, e.g., OpenAI Moderation (Markov et al., 2023), 1National University of Singapore 2University of Chinese Academy of Sciences 3Peking University 4Westlake University. Email: yliu@u.nus.edu Figure 1. Demonstrations of LLaMA Guard 3 (left side) and our GuardReasoner (right side), mainly focusing on 3 aspects: (1) performance, (2) explainability, and (3) generalization. We sample this case from the WildGuardTest (Han et al., 2024) dataset. LLaMA Guard series (Inan et al., 2023; Dubey et al., 2024), ShieldGemma (Zeng et al., 2024), Aegis series (Ghosh et al., 2024a;b), WildGuard (Han et al., 2024), by moderating the input and output of LLMs. Although these models are effective, we highlight three drawbacks as follows. (1) Performance: they are typically trained using straightforward instruction tuning, limiting reasoning ability and performance. (2) Explainability: they mainly function as classifiers that merely provide moderation results, lacking explainability. (3) Generalization: they struggle to handle new types of harm due to their reliance on manually designed harmful categories. To support our claims, we demonstrate one case of LLaMA Guard 3 shown on the left side of Figure 1. 1 https://github.com/yueliu1999/GuardReasoner/ To tackle these challenges, we propose novel reasoning1 based guard model termed GuardReasoner. The core principle is to first unlock the reasoning ability of the guard model and then to guide it to learn to reason. The training process primarily consists of two stages. In the first stage, we begin by collecting instruction tuning data, then synthesize reasoning processes using GPT-4o, resulting in the GuardReasonerTrain dataset, which comprises about 127K samples and 460K detailed reasoning steps. To broaden the range of usability, we start with three base models of different sizes: LLaMA 3.2 1B, LLaMA 3.2 3B, and LLaMA 3.1 8B. Subsequently, we train the base model via reasoning supervised fine-tuning (R-SFT) on the synthesized reasoning data, unlocking the models basic reasoning capability. In the second stage, we present hard sample direct preference optimization (HS-DPO). We first utilize the tuned model to randomly generate different outputs with reasoning steps. Then, we define the samples with at least one correct and one incorrect outputs as ambiguous samples near the decision boundary. For these samples, we perform HS-DPO by treating correct outputs together with the corresponding reasoning processes as positive items, while the incorrect ones serve as negative items. Meanwhile, to guide the model to focus more on the hard samples, we up-weight samples with more incorrect outputs while down-weighting samples with more correct outputs. Through these designs, our GuardReasoner is guided to learn to reason and perform moderation, especially for ambiguous samples. The above designs improve three aspects. (1) Performance: we unlock and enhance the reasoning ability of GuardReasoner, improving its performance. (2) Explainability: it offers not only moderation result but also reasoning process, enhancing explainability. (3) Generalization: it operates independently of fixed categories, as intermediate reasoning plays the role of allowing the model to recognize open-ended categories, boosting generalizability. We show an example of GuardReasoner on the right side of Figure 1. The main contributions of this paper are as follows. We create new dataset named GuardReasonerTrain for training reasoning-based guard models. It contains about 127K samples and 460K detailed reasoning steps. We develop novel reasoning-based guard model termed GuardReasoner via R-SFT and HS-DPO, improving reasoning ability, explainability, and generalizability. We demonstrate the superiority and effectiveness of GuardReasoner via extensive experiments and analyses. The data, code, and model weights are open-sourced. 2. Related Work 2.1. Safety Alignment of LLM Large Language Models (LLMs) (Achiam et al., 2023; Reid et al., 2024; Dubey et al., 2024; Team, 2024a) showcase remarkable abilities across various fields, such as coding, legal, and medical applications. To ensure that AI remains beneficial and safe, Askell et al. (2021) proposes the 3H standard, focusing on helpfulness, harmlessness, and honesty for alignment, while researchers (Ganguli et al., 2022; Ziegler et al., 2019; Solaiman & Dennison, 2021; Korbak et al., 2023) have proposed alignment techniques (Ji et al., 2023; Formento et al., 2024; Ji et al., 2024c). The alignment process for LLMs starts with collecting high-quality data (Ethayarajh et al., 2022) that embodies human values. Specifically, Bach et al. (2022); Wang et al. (2022c) leverage existing NLP benchmarks to construct instructions, and Wang et al. (2022b) employs more advanced LLMs to generate new instructions through in-context learning. Additionally, Welbl et al. (2021); Wang et al. (2022a) focus on filtering out unsafe content from pre-training data. During training, SFT (Wu et al., 2021), RLHF (Ouyang et al., 2022), and DPO (Rafailov et al., 2024) are the three main techniques employed. Besides, Cheng et al. (2023); Lu et al. (2023) introduce alignment methods that do not require additional fine-tuning, while Ji et al. (2024a) aims to develop an efficient alignment method. Guan et al. (2024) propose deliberative alignment to make LLMs safer via reasoning. 2.2. Guard Models for LLMs In contrast to safety alignment on the LLM itself, guard models introduce separate model designed to moderate the input and output of LLMs to filter out unsafe content. Existing guardrails can be roughly categorized into three types as follows. 1) Traditional guard models adopt statistical techniques such as k-nearest neighbors (Yuan et al., 2024) and Beta regression (Tan et al., 2021). 2) Closed-Source guard APIs are created by industrial companies for commercial use, e.g., OpenAI (Markov et al., 2023), Perspective (Lees et al., 2022), Detoxify (UnitaryAI, 2024), Azure (Azure, 2024). They can be implemented by fine-tuning (Markov et al., 2023) or prompting LLMs (Kumar et al., 2023; Ma et al., 2023a; Rebedea et al., 2023) like GPT-4o. 3) OpenSource guard models, including ToxicChat-T5 (Lin et al., 2023), ToxDectRoberta (Zhou, 2020), LaGoNN (Bates & Gurevych, 2023), the LLaMA Guard series (Inan et al., 2023; Dubey et al., 2024), Aegis Guard series (Ghosh et al., 2024a;b), WildGuard (Han et al., 2024), ShieldGemma (Zeng et al., 2024), are open-weight LLMs fine-tuned on the red-teaming data. Liu et al. (2024b) analyzes the calibration of guard models, while Zheng et al. (2024a); Sawtell et al. (2024); Wang et al. (2024a); ONeill et al. (2024) focus on lightweight guard models. Kang & Li (2024) develops robust guardrail R2-Guard via logical reasoning. In addition, guard models have also become hot topic for multimodel models (Du et al., 2024; Chi et al., 2024; Wang et al., 2024b) and agents (Xiang et al., 2024). Our GuardReasoner falls into the third category, i.e., open-source guard 2 models. Existing guard models face challenges in terms of their performance, explainability, and generalizability. Our work points to the importance of reasoning as way of progressing along all three of these dimensions. 2.3. Reasoning Ability of LLM The ability to reason is crucial for LLMs, allowing them to mimic human-like thinking patterns. Pioneering work (Wei et al., 2022; Kojima et al., 2022) achieves this by prompting LLMs to think step-by-step. In addition to this approach, frameworks like self-correction (Kumar et al., 2024), selfcritique (Ke et al., 2023), debate (Liang et al., 2023; Du et al., 2023), and plan-and-solve (Wang et al., 2023) enhance reasoning abilities. Ma et al. (2023b) explores the influence of code data on the reasoning ability of LLMs during training. Furthermore, efforts like (Hao et al., 2024; Goyal et al., 2023) aim to transition the thinking process of LLMs into the latent space. OpenAI has developed the o1 model by teaching LLMs to reason effectively, showcasing the potential for improvements through test-time scaling. Following OpenAI, QwQ (Team, 2024c), QvQ (Team, 2024b), DeepSeek (Team, 2025a), Kimi (Team, 2025b) develop o1-like reasoning models. Furthermore, OpenAIs o3 is announced to achieve promising performance on the ARG-AGI benchmark ARC-AGI (2024). (Chen et al., 2024) discusses the overthinking problem of o1-like models. 3. GuardReasoner This section outlines the methodology of GuardReasoner. Specifically, we begin by defining the guardrail tasks. Then, we introduce the R-SFT and HS-DPO training approaches. The overview of GuardReasoner is illustrated in Figure 2. Task Definition. Given target LLM F, user inputs prompt and receives response = F(X ). The guard model is designed to moderate the input and output of the LLM, and to detect whether the LLM has refused the request, i.e., ( ˆYprom., ˆYres., ˆYref.) = G(X , S), where ˆYprom. {harmful, unharmful} is the predicted label for the prompt harmfulness detection task, ˆYres. {harmful, unharmful} is the predicted label for the response harmfulness detection task, and ˆYref. {refusal, compliance} is the predicted label for the refusal detection task. The performance of is evaluated using F1 score between and ˆY. In harmfulness detection tasks, harmful/unharmful samples are treated In the refusal detection task, reas positives/negatives. fusal/compliance samples are treated as positives/negatives. 3.1. Reasoning Supervised Fine-tuning To unlock the reasoning ability of the guard model, we first synthesize the reasoning data and then perform reasoning supervised fine-tuning (R-SFT) on the base model Mbase. Table 1. Statistical information of the training corpus. Training Corpus # Sample # Step Mean Step Mean Len. per Step Seed Data WildGuardTrain AegisTrain BeaverTailsTrain ToxicChatTrain 86,759 10,798 27,186 5,082 0 0 0 0 0 0 Synthesized Reasoning Data WildGuardTrain-R 86,759 323,930 AegisTrain-R 10, 37,082 BeaverTailsTrain-R 27,186 90,553 ToxicChatTrain-R 2, 9,094 GuardReasonerTrain 127,544 460,659 3.73 3.43 3.33 3. 3.61 0 0 0 0 138. 140.83 114.49 143.89 133.97 Reasoning Data Synthesis. We survey and analyze the existing red-teaming training datasets, including WildGuardTrain (Han et al., 2024), AegisTrain (Ghosh et al., 2024a), BeaverTailsTrain (Ji et al., 2024b), and ToxicChatTrain (Lin et al., 2023). We find that these data primarily focus on providing human-annotated classifications, missing detailed reasoning processes. To tackle this issue, we utilize GPT-4o to synthesize intermediate reasoning processes. Specifically, we provide it with the users prompt to the target LLM, the target LLMs response S, and the ground truth labels Y, then instruct it to generate the intermediate reasoning steps R. To improve the quality of the reasoning data, we remind it to 1) think step by step, 2) keep each step to the smallest unit, 3) keep consistency between reasoning and conclusion, and 4) control the format. The detailed prompt is shown in Figure 12. Based on this method, we select the above four datasets as seed data and synthesize four reasoning training datasets as shown in Table 1. Then, by mixing them, we create the GuardReasonerTrain dataset, which contains 127K samples with 460K reasoning steps. R-SFT. After creating the reasoning training data D, we proceed to perform R-SFT. We input the designed instruction I, users prompt , target models response S, then guide the base model Mbase to output the reasoning process and moderation result Y. It is formulated as follows. LR-SFT = E(X ,S,R,Y)D log Pθ(R, I, , S), (1) where θ denotes the model parameters. The instruction, input, and output of R-SFT are showcased in Figure 13. Through R-SFT, we unlock the basic reasoning ability of the base model Mbase and obtain reasoning model MR-SFT. 3.2. Hard Sample Direct Preference Optimization To further enhance the reasoning ability of the guard model, we first select the hard samples and then conduct hard sample direct preference optimization (HS-DPO) on MR-SFT. Figure 2. GuardReasoner consists of three modules: (1) Reasoning Data Synthesis, (2) Reasoning SFT, and (3) Hard Sample DPO. (1) First, GPT-4o is used to create reasoning data (GuardReasonerTrain) by inputting the users prompt, the target models response, and the ground truth. (2) Then, the base model is trained by R-SFT on this dataset to develop the reasoning model MR-SFT. (3) MR-SFT produces outputs to identify the ambiguous samples with both correct and incorrect responses. Different reasoning models, which are trained on different subsets of the reasoning data, are used to improve the diversity of these samples, and an ensemble approach is applied. Lastly, HS-DPO is performed on these ambiguous samples, selecting correct outputs as positive data and incorrect ones as negative data, with focus on hard samples by up-weighting those with more errors. In this way, we guide GuardReasoner to learn to reason. Hard Sample Mining. Our goal is to identify hard samples that lie near the decision boundary to enhance the models performance. For one input sample {X , S} in the training set, we utilize the reasoning model MR-SFT to produce outputs, represented as { ˆR(i), ˆY (i)}i{1,2,...,k}, by employing high temperature and top-p sampling strategy. We consider the sample to be hard sample if these outputs contain mixture of both correct and incorrect outputs. We obtain the hard sample training set Hself generated by MR-SFT. Next, we aim to improve the diversity of the hard samples via different reasoning models trained on various subsets of the data, which may exhibit strengths in different domains. We first sample various subsets of GuardReasonerTrain, then perform R-SFT based on them and obtain various reasoning models M(1) R-SFT. We utilize these models to produce hard samples and merge them with Hself, resulting in Hensemble. In this way, the diversity of hard samples is improved by mining more hard samples. R-SFT, M(3) R-SFT, M(2) HS-DPO. We conduct HS-DPO on to further enhance the reasoning ability of the guard model. Given sample and its associated outputs, {X , S, ˆR(i), ˆY (i)}i{1,2,...,k}, we randomly select one correct outputs as the positive data {X , S, ˆRpos, ˆYpos}, and one of the incorrect samples as the negative data {X , S, ˆRneg, ˆYneg}. Then, we guide the model to prefer the correct classification and the corresponding reasoning process on these hard samples as follows. LHS-DPO = ECHα log σ (A B) , (2) where = (X , S, ˆRpos, ˆYpos, ˆRneg, ˆYneg), = β log Pθ( ˆRpos, ˆYposI,X ,S) , θ Pref(( ˆRpos, ˆYposI,X ,S) is the parameters of trainable model, ref is the parameters of reference model, β is the strength of the KL constraint, α is the weight of sample. The instruction I, input {X , S}, and positive/negative response, are showcased in Figure 14. , = β log Pθ( ˆRneg, ˆYnegI,X ,S) Pref(( ˆRneg, ˆYnegI,X ,S) During this process, we guide the model to focus more on the hard samples by up-weighting the samples with more incorrect outputs while down-weighting the samples with more correct outputs. Therefore, it is formulated as follows. α = 1 + Norm(kincorr kcorr, γ), (3) where α denotes the weight of the sample, kcorr denotes the number of correct outputs, kincorr denotes the number of the incorrect outputs, Norm(x, γ) denotes normalization function that normalizes to [γ, γ], where γ < 1. We train the model on both the self-generated HS-DPO training data Hself and the ensemble data Hensemble, and obtain two models M(self) HS-DPO . We regard M(ensemble) as our GuardReasoner Greasoner since the experiments show that M(ensemble) achieves better performance. HS-DPO, M(ensemble) HS-DPO HS-DPO 3.3. Inference with Reasoning The existing guard models merely output moderation results, i.e., ˆY = G(X , S). Differently, GuardReasoner is an explainable guard model. During inference, it provides both moderation results and reasoning processes, i.e., 4 Table 2. Comparison experiment of 21 models on 6 benchmarks of the prompt harmfulness detection task. Bold and underlined values denote the best and the runner-up. The performance is evaluated via F1 score (%). - denotes that the result is unavailable. Method Model Size ToxicChat HarmBench OpenAI Moderation Aegis SafetyTest Simple SafetyTests WildGuard Test Weighted Average OpenAI Moderation GPT-4o GPT-4o+CoT GPT-4 GPT-4+CoT o1-preview Claude 3.5 Sonnet Gemini 1.5 Pro Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown LLaMA Guard LLaMA Guard 2 LLaMA Guard 3 Aegis Guard Defensive Aegis Guard Permissive Aegis Guard 2.0 ShieldGemma ShieldGemma WildGuard QwQ-preview GuardReasoner GuardReasoner GuardReasoner 7B 8B 8B 7B 7B 8B 2B 9B 7B 32B 1B 3B 8B Closed-Source Guard API 09.60 82.27 81.98 78.68 78.68 89.61 81.68 80.20 79.00 62.26 76.78 81.41 82.05 74.60 51.06 63. Open-Source Guard Model 67.20 94.00 98.94 77.70 70.50 - 11.81 67.96 98.90 86.73 96.31 89.10 91.86 75.80 76.10 79.69 67.50 74.70 81.00 13.89 78.58 72.10 61.58 70.06 71.87 72.00 31.90 81.07 88.24 85.16 85.85 83.15 79.72 84.03 74.10 71.80 71.39 84.80 82.90 - 07.47 77.63 89.40 80.23 89.34 91.39 90.18 25.40 64.46 73.43 69.80 69.64 57.69 43.73 67. 61.60 47.10 53.12 70.00 73.00 - 06.91 67.92 70.80 34.81 72.43 78.20 78.79 63.00 98.48 98.99 99.50 100.00 100.00 100.00 100.00 93.00 95.80 99.50 100.00 99.00 - 05.83 91.89 99.50 99.50 98.99 100.00 99.50 12.10 80.87 82.75 79.72 80.46 76.31 63.21 84.50 56.00 70.90 76.18 78.50 71.50 81.60 09.36 57.74 88.90 66.02 87.37 89.01 89.17 35.28 70.00 78.00 76.61 76.92 69.44 54.34 72. 64.89 63.62 68.47 72.99 73.83 - 09.38 68.77 77.99 54.13 77.68 80.76 81.09 Table 3. Ablation studies of GuardReasoner evaluated via F1 score (%). The bold and underlined italic values denote the best and worst. Model Size Task Type Baseline Baselinemix R-SFT R-SFT w. HS-DPOself R-SFT w. HS-DPOensemble 1B 3B 8B Prompt Response Refusal Avg. Prompt Response Refusal Avg. Prompt Response Refusal Avg. 62. 70.74 78.57 78.12 77.18 72.05 77. 78.46 79.95 79.78 87.96 74.32 58.43 68. 74.71 78.05 85.99 81.01 80.00 86.52 81.53 80.17 88. 81.98 80.80 74.23 66.78 79.30 80.34 80. 88.16 73.61 74.29 73.38 72.74 66.13 86.51 81.94 80. 85.95 82.15 80.92 86.28 82.61 81.09 74.74 79. 80.03 80.35 80.97 87.65 78.89 56. 67.48 89.64 83.34 89.51 83.59 90. 84.04 { ˆY, ˆR} = Greasoner(X , S), where ˆR represents the intermediate reasoning steps. We demonstrate that ˆR improves performance, explainability, and generalizability. 4. Experiments Environment. All experiments are conducted on 2 servers with 4 56-core Intel(R) Xeon(R) Platinum 8480CL CPUs, 2T RAM, and 8 NVIDIA H100 (80GB) GPUs. We use the LLaMA Factory (Zheng et al., 2024b) training platform. Benchmark. We use 13 guardrail benchmarks, including 6 prompt harmfulness detection benchmarks (ToxicChat (Lin et al., 2023), OpenAIModeration (Markov et al., 2023), AegisSafetyTest (Ghosh et al., 2024a), SimpleSafetyTests (Vidgen et al., 2023), HarmBench (Mazeika et al., 2024), WildGuardTest (Han et al., 2024)), 5 response harmfulness detection benchmarks (HarmBench, SafeRLHF (Dai et al., 2023), BeaverTails (Ji et al., 2024b), XSTestReponse (Rottger et al., 2023), WildGuardTest), and 2 refusal detection benchmarks (XSTestResponse, WildGuardTest). The statistical information of these datasets is listed in Table 8, where Include Adversarial denotes whether the users prompt contains the adversarial attack. We use F1 score (harmful/refusal category as positive samples) to evaluate performance on the guardrail tasks. Due to the varying sample sizes across benchmarks (0.1K to 3K), we use sample-weighted average of F1 scores across benchmarks to evaluate the overall performance of the guardrails. Baseline. We compare with 22 baselines, including 8 closedsource guardrail APIs (OpenAI Moderation (Markov et al., 5 Table 4. Comparison experiment of 25 models on 5 benchmarks of the response harmfulness detection task. The bold and underlined values denote the best and the runner-up. The performance is evaluated via F1 score (%). - denotes the result is unavailable. Weighted Average Model Size HarmBench SafeRLHF BeaverTails XSTestReponse WildGuard Test Method OpenAI Moderation GPT-4o GPT-4o+CoT GPT-4 GPT-4+CoT o1-preview Claude 3.5 Sonnet Gemini 1.5 Pro Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown LLaMA Guard LLaMA Guard 2 LLaMA Guard 3 Aegis Guard Defensive Aegis Guard Permissive Aegis Guard 2.0 ShieldGemma ShieldGemma HarmBench LLaMA HarmBench Mistral MD-Judge BeaverDam WildGuard QwQ-preview GuardReasoner GuardReasoner GuardReasoner 7B 8B 8B 7B 7B 8B 2B 9B 13B 7B 7B 7B 7B 32B 1B 3B 8B 10.10 64.05 65.10 58.62 59.38 66.60 69.29 62.01 Closed-Source Guard API 15.70 78.63 82.26 80.11 80.26 79.96 83.84 83.91 Open-Source Guard Model 67.10 71.80 67.84 74.70 73.80 - 30.97 63.61 77.10 75.20 86.70 89.90 84.40 77.26 85.84 86.72 87. 48.40 51.60 44.36 59.30 55.90 - 16.92 47.07 60.00 52.40 64.70 72.10 64.20 62.76 68.39 69.02 70.04 20.60 56.34 65.99 78.54 79.68 76.40 75.52 84.39 52.00 77.80 85.07 62.20 60.80 - 35.36 56.44 84.30 87.00 81.60 58.40 86.30 69.65 84.75 85.66 85.47 46.60 65.12 86.90 91.16 91.28 74.75 84.75 90.24 82.00 90.80 87.67 52.80 60.40 86.20 65.55 73.86 64.50 72.00 90.40 83.60 94.70 45.95 90.12 91.36 94.34 16.90 65.24 71.43 65.45 66.37 50.00 10.74 76. 50.50 66.50 70.80 49.10 56.40 77.50 20.13 47.00 45.70 60.10 76.80 63.40 75.40 17.56 74.81 79.70 78.20 16.68 69.41 74.45 71.82 72.38 69.22 63.05 77.04 58.27 66.99 64.97 62.79 63.55 - 27.24 55.67 65.49 66.70 78.67 76.60 77.95 57.73 79.06 80.80 81.22 Table 5. Efficiency experiments on GuardReasoner. The training is conducted on 4 NVIDIA H100 (80GB) GPUs, and the inference uses 1 NVIDIA H100 (80GB) GPU. The first number and the second number split by denote the costs of R-SFT and HS-DPO, respectively. Stage Training Model Size Method Variant GPU Memory Cost (GB) Time Cost (GPU hour) GPU Memory Cost (GB) Inference Time Cost (ms/query) Token Cost (token/query) 1B 3B 8B Baselinemix GuardReasoner Baselinemix GuardReasoner Baselinemix GuardReasoner 270.86 273. 191.22 236.93 259.84 213.04 240.21 241.46 270.78 06. 77.68 08.43 19.48 06.33 03.70 77.66 26. 254.35 11.69 77.74 10.50 20.05 13.69 04. 78.24 30.29 257.64 21.32 78.03 13. 17.09 25.20 05.31 78.25 35.77 260.26 2023), GPT-4o, GPT-4o+CoT, GPT-4, GPT-4+CoT, o1preview, Claude 3.5 Sonnet, Gemini 1.5 Pro), and 14 opensource guard models (LLaMA Guard 7B (Inan et al., 2023), LLaMA Guard 2 8B (Dubey et al., 2024), LLaMA Guard 3 8B, Aegis Guard Defensive 7B, Aegis Guard Permissive 7B (Ghosh et al., 2024a), Aegis Guard 2.0 8B (Ghosh et al., 2024b), ShieldGemma 2B, ShieldGemma 9B (Zeng et al., 2024), HarmBench LLaMA 13B, HarmBench Mistral 7B (Mazeika et al., 2024), MD-Judge 7B (Li et al., 2024), BeaverDam 7B (Ji et al., 2024b), WildGuard 7B (Han et al., 2024), QwQ-preview 32B (Team, 2024c)). 4.1. Performance We compare GuardReasoner with 22 baselines on 13 benchmarks across 3 tasks and obtain several findings as follows. (I) In the prompt harmfulness detection task, as shown in Table 2, our GuardReasoner 8B achieves the best performance with an average F1 score of 81.09%, surpassing both the open-source guard model runner-up by 3.10% and the closed-source guard API runner-up by 3.09%. Among the benchmarks, our GuardReasoner improves the performance more significantly on the benchmarks with adversarial prompts, e.g., 5.36% on ToxicChat. It indicates 6 Table 6. Average F1 score of 20 methods on 3 guardrail tasks. The bold and underlined values denote the best and runner-up. Method Model Size Prompt Response Refusal Avg. Closed-Source API OpenAI Moderation Unknown GPT4o GPT4o+CoT GPT4 GPT4+CoT o1-preview Unknown Unknown Unknown Unknown Unknown Claude 3.5 Sonnet Unknown Gemini 1.5 Pro Unknown 35.28 70.00 78.00 76.61 76.92 69. 54.34 72.66 16.68 69.41 74.45 71. 72.38 69.22 63.05 77.04 Open-Source Guard Model LLaMA Guard LLaMA Guard 2 LLaMA Guard 3 Aegis Guard Defensive Aegis Guard Permissive ShieldGemma ShieldGemma WildGuard QwQ-preview GuardReasoner GuardReasoner GuardReasoner 7B 8B 8B 7B 7B 2B 9B 7B 32B 1B 3B 8B 64. 63.62 68.47 72.99 73.83 09.38 68. 77.99 54.13 77.68 80.76 81.09 58. 66.99 64.97 62.79 63.55 27.24 55. 77.95 57.73 79.06 80.80 81.22 49. 33.68 81.74 73.72 83.41 78.62 90. 79.57 90.26 79.85 85.22 74.63 65. 60.87 90.13 79.94 58.11 60.42 61. 64.18 56.32 63.25 44.21 60.00 49. 62.41 52.57 29.73 52.20 58.88 89. 81.96 57.81 56.55 88.51 81.75 85. 82.50 89.96 84.09 our method is more robust to the adversarial attacks. Besides, as the model size increases, so does performance, e.g., 77.68% (1B) 81.09% (8B). Notably, our 1B model performs comparably to the runner-up WildGuard 7B, i.e., 77.68% vs. 77.99%. (II) For the response harmfulness detection task, as shown in Table 4, GuardReasoner 8B again leads with an F1 score of 81.22%, outperforming the closedsource guard API runner-up by 6.77% and the open-source guard model runner-up by 2.55%. Moreover, our smallest model, GuardReasoner 1B, surpasses the runner-ups MDJudge 7B and GPT-4o+CoT. (III) In the refusal detection task, as shown in Table 10, our method achieves performance of 89.96% F1 score, closely matching the leading method, GPT-4. Compared to the other tasks, this task is relatively simple. Various models, like GPT-4, WildGuard, and GuardReasoner, achieve promising performance. (IV) On average of these 3 guardrail tasks, as shown in Table 6, GuardReasoner 8B achieves the best performance with an average F1 score of 84.09%. It surpasses GPT-4o+CoT, which is the method for reasoning data synthesis, by 5.74%. Besides, it beats the LLaMA Guard 3 8B, which is also based on LLaMA 3.1 8B, by 20.84%. For the baselines, the GPT series achieves promising performance, but the performance of Claude 3.5 Sonnet and QwQ is relatively limited. These general models may not excel in guardrail tasks because they werent specifically designed for them. 7 4.2. Ablation Study We conduct ablation studies of our GuardReasoner on 3 guardrail tasks. As shown in Table 3, Baseline denotes the guard model trained with only the WildGuardTrain dataset (Han et al., 2024). Baselinemix denotes the guard model trained with mix of the seed datasets (Han et al., 2024; Ghosh et al., 2024a; Lin et al., 2023; Ji et al., 2024b). RSFT denotes the guard model trained via R-SFT on our synthesized reasoning data GuardReasonerTrain. R-SFT w. HS-DPOself represents the guard model firstly trained via R-SFT, then via HS-DPO on Hself while R-SFT w. HSDPOensemble represents the guard model firstly trained via R-SFT, then via HS-DPO on Hensemble. From the results in Table 3, we obtain the conclusions as follows. (I) Baseline-Mix achieves comparable performance with Baseline, suggesting that mixing the conventional training datasets does not lead to significant performance improvement. (II) R-SFT achieves better performance than Baseline-Mix by constructing the reasoning training data and conducting R-SFT. For example, on 1B models, RSFT surpasses Baseline-Mix by 6.30% F1. It verifies the effectiveness of the GuardReasonerTrain dataset and R-SFT. (III) R-SFT w. HS-DPOself further improves the performance of R-SFT, demonstrating the effectiveness of our HS-DPO. In addition, we found that R-SFT w. HSDPOensemble beats R-SFT w. HS-DPOself, indicating the effectiveness of improving the diversity of hard samples. 4.3. Efficiency Experiment We conduct efficiency experiments for GuardReasoner and Baselinemix in the ablation study, i.e., the guard model trained with mix of the seed datasets. Note that these two methods are trained with the same amount of training samples. We test the costs in the training stage and the inference stage. In the training stage, we use 4 NVIDIA H100 (80GB) GPUs and adopt the LLaMA Factory (Zheng et al., 2024b) to train the models. In the inference stage, we use 1 NVIDIA H100 (80GB) GPU and adopt vLLM (Kwon et al., 2023) to accelerate the inference. We run the models on the used 13 guardrail benchmarks and record the GPU memory cost, time costs, and output token costs. From the results in Table 5, we have the following findings. (I) In the training stage, GuardReasoner has similar GPU memory cost compared to the baseline, whether at the RSFT or HS-DPO stage. Take the 8B models as an example, GuardReasoner costs 270.86 GB and 273.95 GB at the RSFT and HS-DPO stage, while Baselinemix uses 270.78 GB at the SFT stage. Besides, for the time cost, GuardReasoner increases 40% 50% time cost since 1) it needs to learn from the reasoning data, and 2) it contains two training stages. (II) In the inference stage, the memory costs are similar since we use the vLLM and set the GPU utilization Table 7. Improvement of GuardReasoner 8B after label correction. Method Used Label Prompt Response Refusal Avg. GuardReasoner 8B Original GuardReasoner 8B Corrected 81.09 89. 81.22 86.98 89.96 96.05 84.09 90. Improvement - 10.87% 7.10% 6.78% 8.20% Figure 3. Performance: Baselinemix vs. GuardReasoner on one conventional case from the ToxicChat dataset (Lin et al., 2023). Figure 5. Explainability: GuardReasoner offers transparent explanations for outcomes and helps labelers to fix the mislabelled label in the OpenAIModeration dataset (Markov et al., 2023). Figure 4. Performance: WildGuard vs. GuardReasoner against scenario nesting attack from WildGuardTest (Han et al., 2024). GuardReasoner successfully defends while WildGuard fails. as 95%. Besides, GuardReasoner costs more but tolerable inference time, and the output tokens, e.g., 13.87 35.77 ms/query and 17.09 260.26 token/query. 4.4. Case Study Case studies on GuardReasoner 8B discuss 3 aspects. (I) Performance: in Figures 3 and Figure 4, GuardReasoner successfully defends both conventional case and an attack. (II) Explainability: in Figure 5, GuardReasoner provides explanations. To verify its explainability, we consider the task of correcting mislabelled samples. Concretely, we first sample the error predictions of our model according to the original labels and then ask 3 human annotators to re-label these samples. We regard the majority as the corrected label. We evaluate this task via the performance improvement of our model after re-labeling. The higher performance improvement denotes the more mislabeled samples and the more effective explanations. The results are in Table 7. Figure 6. Generalizability: LLaMA Guard 3 vs. GuardReasoner on one case in AegisSafetyTest (Ghosh et al., 2024a). GuardReasoner provides open-ended non-fixed harmful categories. (III) Generalizability: in Figure 6, compared with LLaMA Guard 3, GuardReasoner has open-ended, non-fixed harmful categories, better generalizing to new classes. 5. Conclusion This paper introduces GuardReasoner, novel guard model that improves performance, explainability, and generalization. We present our GuardReasonerTrain dataset, R-SFT, and HS-DPO, to first unlock the reasoning ability, then guide the model to learn to reason. On experiments across 8 13 benchmarks for 3 tasks, GuardReasoner proves effective. Data, code, and models are released. Future work aims to minimize unnecessary reasoning to enhance efficiency. CognitionAI. Introducing devin, the first ai software engineer. https://www.cognition.ai/blog/introducing-devin/, 2024. 6. Impact Statement This paper introduces guard model designed to enhance the safety of large language models. By implementing this guard model, we aim to mitigate the potential risks and harmful impacts that LLMs may pose to society."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ARC-AGI. Abstraction and reasoning corpus for artificial general intelligence. https://github.com/fchollet/ARCAGI/, 2024. Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. Azure, M. content https://azure.microsoft.com/en-us/products/aiservices/ai-content-safety/, 2024. Azure ai safety. Bach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022. Bates, L. and Gurevych, I. Like good nearest neighbor: Practical content moderation and text classification. arXiv preprint arXiv:2302.08957, 2023. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023. Du, X., Ghosh, R., Sim, R., Salem, A., Carvalho, V., Lawton, E., Li, Y., and Stokes, J. W. Vlmguard: Defending vlms against malicious prompts via unlabeled data. arXiv preprint arXiv:2410.00296, 2024. Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch, I. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ethayarajh, K., Choi, Y., and Swayamdipta, S. Understanding dataset difficulty with mathcal v-usable information. In International Conference on Machine Learning. PMLR, 2022. Formento, B., Foo, C. S., Tuan, L. A., and Ng, S. K. Using punctuation as an adversarial attack on deep learningbased nlp systems: An empirical study. In Findings of the Association for Computational Linguistics: EACL 2023, pp. 134, 2023. Formento, B., Feng, W., Foo, C. S., Tuan, L. A., and Ng, S.-K. Semrode: Macro adversarial training to learn representations that are robust to word-level attacks. arXiv preprint arXiv:2403.18423, 2024. Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. Cheng, J., Liu, X., Zheng, K., Ke, P., Wang, H., Dong, Y., Tang, J., and Huang, M. Black-box prompt optimization: Aligning large language models without model training. arXiv preprint arXiv:2311.04155, 2023. Ghosh, S., Varshney, P., Galinkin, E., and Parisien, C. Aegis: Online adaptive ai content safety moderation with ensemble of llm experts. arXiv preprint arXiv:2404.05993, 2024a. Chi, J., Karn, U., Zhan, H., Smith, E., Rando, J., Zhang, Y., Plawiak, K., Coudert, Z. D., Upasani, K., and Pasupuleti, M. Llama guard 3 vision: Safeguarding humanai image understanding conversations. arXiv preprint arXiv:2411.10414, 2024. Ghosh, S., Varshney, P., Sreedhar, M. N., Padmakumar, A., Rebedea, T., Varghese, J. R., and Parisien, C. Aegis2. 0: diverse ai safety dataset and risks taxonomy for alignment of llm guardrails. In Neurips Safe Generative AI Workshop 2024, 2024b. 9 Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Heylar, A., Dias, R., Vallone, A., Ren, H., Wei, J., et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. Guo, X., Yu, F., Zhang, H., Qin, L., and Hu, B. Cold-attack: Jailbreaking llms with stealthiness and controllability. arXiv preprint arXiv:2402.08679, 2024. Han, S., Rao, K., Ettinger, A., Jiang, L., Lin, B. Y., Lambert, N., Choi, Y., and Dziri, N. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. arXiv preprint arXiv:2406.18495, 2024. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., Testuggine, D., et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023. Ji, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., et al. Ai alignment: comprehensive survey. arXiv preprint arXiv:2310.19852, 2023. Ji, J., Chen, B., Lou, H., Hong, D., Zhang, B., Pan, X., Dai, J., Qiu, T., and Yang, Y. Aligner: Efficient alignment by learning to correct. arXiv preprint arXiv:2402.02416, 2024a. Ji, J., Liu, M., Dai, J., Pan, X., Zhang, C., Bian, C., Chen, B., Sun, R., Wang, Y., and Yang, Y. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024b. Kang, M. and Li, B. R2-guard: Robust reasoning enabled llm guardrail via knowledge-enhanced logical reasoning. arXiv preprint arXiv:2407.05557, 2024. Ke, P., Wen, B., Feng, Z., Liu, X., Lei, X., Cheng, J., Wang, S., Zeng, A., Dong, Y., Wang, H., et al. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. arXiv preprint arXiv:2311.18702, 2023. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Korbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley, C., Phang, J., Bowman, S. R., and Perez, E. Pretraining language models with human preferences. In International Conference on Machine Learning, pp. 17506 17533. PMLR, 2023. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Kumar, D., AbuHashem, Y., and Durumeric, Z. Watch your language: large language models and content moderation. arXiv preprint arXiv:2309.14517, 2023. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Lees, A., Tran, V. Q., Tay, Y., Sorensen, J., Gupta, J., Metzler, D., and Vasserman, L. new generation of perspective api: Efficient multilingual character-level transformers. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pp. 31973207, 2022. Ji, Y., Liu, Y., Zhang, Z., Zhang, Z., Zhao, Y., Zhou, G., Zhang, X., Liu, X., and Zheng, X. Advlora: Adversarial low-rank adaptation of vision-language models. arXiv preprint arXiv:2404.13425, 2024c. Li, L., Dong, B., Wang, R., Hu, X., Zuo, W., Lin, D., Qiao, Y., and Shao, J. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. arXiv preprint arXiv:2402.05044, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Shi, S., and Tu, Z. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. 10 Lin, Z., Wang, Z., Tong, Y., Wang, Y., Guo, Y., Wang, Y., and Shang, J. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. arXiv preprint arXiv:2310.17389, 2023. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, H., Huang, H., Wang, H., Gu, X., and Wang, Y. On calibration of llm-based guard models for reliable content moderation. arXiv preprint arXiv:2410.10414, 2024b. Liu, Y., He, X., Xiong, M., Fu, J., Deng, S., and Hooi, B. Flipattack: Jailbreak llms via flipping. arXiv preprint arXiv:2410.02832, 2024c. Lu, X., Brahman, F., West, P., Jang, J., Chandu, K., Ravichander, A., Qin, L., Ammanabrolu, P., Jiang, L., Ramnath, S., et al. Inference-time policy adapters (ipa): Tailoring extreme-scale lms without fine-tuning. arXiv preprint arXiv:2305.15065, 2023. Ma, H., Zhang, C., Fu, H., Zhao, P., and Wu, B. Adapting large language models for content moderation: Pitfalls in data engineering and supervised fine-tuning. arXiv preprint arXiv:2310.03400, 2023a. Ma, Y., Liu, Y., Yu, Y., Zhang, Y., Jiang, Y., Wang, C., and Li, S. At which training stage does code data help llms reasoning? arXiv preprint arXiv:2309.16298, 2023b. Markov, T., Zhang, C., Agarwal, S., Nekoul, F. E., Lee, T., Adler, S., Jiang, A., and Weng, L. holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E., Li, N., Basart, S., Li, B., et al. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. ONeill, J., Subramanian, S., Lin, E., Satish, A., and Mugunthan, V. Guardformer: Guardrail instruction pretraining for efficient safeguarding. In Neurips Safe Generative AI Workshop 2024, 2024. OpenAI. Introducing https://openai.com/index/chatgpt/, 2022. chatgpt. et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Rebedea, T., Dinu, R., Sreedhar, M., Parisien, C., and Cohen, J. Nemo guardrails: toolkit for controllable and safe llm applications with programmable rails. arXiv preprint arXiv:2310.10501, 2023. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Rottger, P., Kirk, H. R., Vidgen, B., Attanasio, G., Bianchi, F., and Hovy, D. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. Sawtell, M., Masterman, T., Besen, S., and Brown, J. Lightweight safety classification using pruned language models. arXiv preprint arXiv:2412.13435, 2024. Solaiman, I. and Dennison, C. Process for adapting language models to society (palms) with values-targeted datasets. Advances in Neural Information Processing Systems, 34: 58615873, 2021. Tan, F., Hu, Y., Yen, K., and Hu, C. Bert-beta: proactive probabilistic approach to text moderation. arXiv preprint arXiv:2109.08805, 2021. A. Opus, Team, ily: cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model Card Claude 3.pdf, 2024a. famhttps://wwwclaude haiku. The sonnet, 3 model Team, D. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. OpenAI. Searchgpt prototype. Team, K. Kimi k1.5: Scaling reinforcement learning with https://openai.com/index/searchgpt-prototype/, 2024. llms. arXiv preprint 2501.12599v1, 2025b. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Team, Q. Qvq: To see the world with wisdom. https://qwenlm.github.io/blog/qvq-72b-preview/, 2024b. 11 Team, Q. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/qwq-32b-preview/, 2024c. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. UnitaryAI. detoxify. https://openai.com/index/chatgpt/, 2024. Vidgen, B., Scherrer, N., Kirk, H. R., Qian, R., Kannappan, A., Hale, S. A., and Rottger, P. Simplesafetytests: test suite for identifying critical safety risks in large language models. arXiv preprint arXiv:2311.08370, 2023. Wang, B., Ping, W., Xiao, C., Xu, P., Patwary, M., Shoeybi, M., Li, B., Anandkumar, A., and Catanzaro, B. Exploring the limits of domain-adaptive training for detoxifying large-scale language models. Advances in Neural Information Processing Systems, 35:3581135824, 2022a. Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023. neural information processing systems, 35:2482424837, 2022. Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. Challenges in detoxifying language models. arXiv preprint arXiv:2109.07445, 2021. Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano, P. Recursively summaarXiv preprint rizing books with human feedback. arXiv:2109.10862, 2021. Xiang, Z., Zheng, L., Li, Y., Hong, J., Li, Q., Xie, H., Zhang, J., Xiong, Z., Xie, C., Yang, C., et al. Guardagent: Safeguard llm agents by guard agent via knowledgeenabled reasoning. arXiv preprint arXiv:2406.09187, 2024. Yuan, Z., Xiong, Z., Zeng, Y., Yu, N., Jia, R., Song, D., and Li, B. Rigorllm: Resilient guardrails for large language models against undesired content. arXiv preprint arXiv:2403.13031, 2024. Zeng, W., Liu, Y., Mullins, R., Peran, L., Fernandez, J., Harkous, H., Narasimhan, K., Proud, D., Kumar, P., Radharapu, B., et al. Shieldgemma: Generative ai content moderation based on gemma. arXiv preprint arXiv:2407.21772, 2024. Zheng, A., Rana, M., and Stolcke, A. Lightweight safety arXiv guardrails using fine-tuned bert embeddings. preprint arXiv:2411.14398, 2024a. Wang, M., Lin, P., Cai, S., An, S., Ma, S., Lin, Z., Huang, C., and Xu, B. Stand-guard: small task-adaptive content moderation model. arXiv preprint arXiv:2411.05214, 2024a. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024b. Zhou, X. Challenges in automated debiasing for toxic language detection. University of Washington, 2020. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022b. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022c. Wang, Y., Liu, X., Li, Y., Chen, M., and Xiao, C. Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting. arXiv preprint arXiv:2403.09513, 2024b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in 12 Table 8. Statistics of 13 benchmarks on 3 guardrail tasks. Guardrail Task Benchmark # Sample Include Adversarial Prompt Harmfulness Detection Response Harmfulness Detection Refusal Detection ToxicChat OpenAIModeration AegisSafetyTest SimpleSafetyTests HarmBenchPrompt WildGuardTest HarmBenchResponse SafeRLHF BeaverTails 2,853 1,680 100 239 1,756 602 2,000 3, XSTestReponseHarmful 446 WildGuardTest 1,768 XSTestResponseRefusal WildGuardTest 1,777 Table 9. URL of seed training data on Hugging Face. Seed Data WildGuardTrain AegisTrain BeaverTailsTrain ToxicChatTrain SafeRLHFTrain path allenai/wildguardmix nvidia/Aegis-AI-Content-Safety-Dataset-1.0 PKU-Alignment/BeaverTails lmsys/toxic-chat PKU-Alignment/PKU-SafeRLHF name wildguardtrain - - toxicchat0124 alpaca2-7b split train train 30k train train train A. Appendix A.1. Dataset We list the statistical information of the used benchmarks in Table 8. We list the URLs of seed training datasets in Table 9. A.2. Experiment We list the results of the refusal detection task in Table 10. We show the performance improvement of GuardReasoner and baselines on 3 guardrail tasks after correcting the error labels. We show the convergence of GuardReasoner in Figure 7. A.3. Implementation A.3.1. BASELINE We use the original codes of the baselines to replicate their results. We introduce the baselines and provide the implementation details as follows. They contain 8 closed-source guard APIs and 13 open-source guard models. Closed-Source guard APIs. OpenAI Moderation. OpenAI Moderation (Markov et al., 2023) is tool that automatically detects and filters harmful or inappropriate user-generated content using AI, helping developers maintain safe environments. GPT-4o. GPT-4o is an enhanced version of OpenAIs GPT-4 model, optimized for improved performance, efficiency, and safety in natural language processing tasks. We adopt it for prompt harmfulness detection, response harmfulness detection, and refusal detection. The prompt setup is illustrated in Figure 15. GPT-4o+CoT. We use chain-of-thought (CoT) (Wei et al., 2022) prompt to enhance the performance of GPT-4o. The prompt setup is illustrated in Figure 16. 13 (a) Loss of R-SFT (b) Loss of HS-DPO (c) Accuracy (positive>negative) of HS-DPO Figure 7. Convergence of GuardReasoner. The first, second, and third row denotes 1B, 3B, and 8B models, respectively. GPT-4. GPT-4 is OpenAIs fourth-generation language model, offering advanced capabilities in understanding and generating human-like text across variety of contexts and applications. The prompt setup is illustrated in Figure 15. GPT-4+CoT. We use chain-of-thought (CoT) (Wei et al., 2022) prompt to enhance the performance of GPT-4. The prompt setup is illustrated in Figure 16. o1-preview. o1-preview is OpenAIs reasoning model designed to solve hard problems across domains. The prompt setup is illustrated in Figure 17. For the o1-preview, we evaluate random sample of 5% instances (at least 100) per benchmark due to high costs. For samples rejected by the model, we classify them as harmful or refused samples. Claude 3.5 Sonnet. Claude 3.5 Sonnet is flagship LLM model of Anthropic, designed for improved performance, especially in reasoning, coding, and safety. The prompt setup is illustrated in Figure 15. For samples rejected by the model, we classify them as harmful or refused samples. Gemini 1.5 Pro. Gemini 1.5 Pro is multimodal AI model developed by Google DeepMind to help power generative AI services. The prompt setup is illustrated in Figure 17. For samples rejected by the model, we classify them as harmful or refused samples. Open-Source guard models. LLaMA Guard 7B. LLaMA Guard 7B (Inan et al., 2023) is Metas AI content guard model. It is instruct-tuned from Table 10. Comparison experiment on 2 benchmarks of refusal detection task. The bold and underlined values denote the best and runner-up. The performance is evaluated via F1 score (%). Method Model Size XSTestResponse WildGuardTest Weighted Average Closed-Source Guard API OpenAI Moderation GPT-4o GPT-4o+CoT GPT-4 GPT-4+CoT o1-preview Claude 3.5 Sonnet Gemini 1.5 Pro Unknown Unknown Unknown Unknown Unknown Unknown Unknown Unknown 46.60 80.45 83.76 91.16 92.59 89.87 73.57 92.15 LLaMA Guard LLaMA Guard 2 LLaMA Guard 3 Aegis Guard Defensive Aegis Guard Permissive ShieldGemma ShieldGemma WildGuard QwQ-preview GuardReasoner GuardReasoner GuardReasoner Open-Source Guard Model 7B 8B 8B 7B 7B 2B 9B 7B 32B 1B 3B 8B 82.00 90.80 63.55 52.80 60.40 61.06 58.62 94.70 62.63 91.34 80.31 93.68 49.80 82.10 83.31 90.02 89.60 83.91 62.89 89. 51.40 53.80 54.29 41.80 46.90 50.18 50.40 88.60 56.46 87.71 87.54 88.91 49.10 81.74 83.41 90.27 90.26 85.22 65.23 90.13 58.11 61.91 56.32 44.21 49.86 52.57 52.20 89.94 57.81 88.51 85.95 89.96 Table 11. Improvement (F1 score %) of GuardReasoner and baselines after label correction on the prompt harmfulness detection task. Method Used Label ToxicChat HarmBench OpenAI Moderation Aegis SafetyTest Simple SafetyTests WildGuard Test Weighted Average GPT-4o+CoT GPT-4o+CoT Original Corrected LLaMA Guard 3 8B Original LLaMA Guard 3 8B Corrected GuardReasoner 1B Original GuardReasoner 1B Corrected GuardReasoner 3B Original GuardReasoner 3B Corrected GuardReasoner 8B Original GuardReasoner 8B Corrected 73.43 77.91 53.12 54.74 72.43 85.46 78.20 79.27 78.79 89.99 81.98 81.98 98.94 98.94 96.31 89.10 89.10 96.31 91.86 91.86 76.78 77.78 79.69 77.66 70.06 80.51 71.87 79.14 72.00 83.36 88.24 89.56 71.39 73.60 89.34 94.57 91.39 91.92 90.18 94.74 98.99 99.50 99.50 100.00 98.99 99.50 100.00 99.49 99.50 100. 82.75 87.27 76.18 78.59 87.37 92.79 89.01 91.37 89.17 94.24 78.00 81.28 68.47 69.37 77.68 83.80 80.76 86.91 81.09 89.92 the base model LLaMA 2 7B (Touvron et al., 2023b). The training data is private and contains 13K samples. LLaMA Guard 2 8B. LLaMA Guard 2 8B is the second version of the LLaMA Guard series. It is based on LLaMA 3 8B (Dubey et al., 2024). They flip labels to conduct data augmentation on the training data. LLaMA Guard 3 8B. LLaMA Guard 3 8B is the third version of LLaMA Guard series. The base model is LLaMA 3. 8B (Dubey et al., 2024). It supports 8 languages and has context window of 128K tokens. Aegis Guard Defensive/Permissive 7B. Aegis Guard Defensive/Permissive 7B is developed by Nvidia. It is based on LLaMA Guard 7B and uses LoRA to train the model. The defensive version classifies Needs Caution samples as harmful, and the permissive version classifies Needs Caution samples as benign. Aegis Guard 2.0 8B. Aegis Guard 2.0 8B is the second version of the Aegis Guard series. It uses LLaMA 3.1-instruct 15 Table 12. Improvement (F1 score %) of GuardReasoner and baselines after label correction on the response harmfulness detection task. Method Used Label HarmBench SafeRLHF BeaverTails XSTestReponse Gemini 1.5 Pro Gemini 1.5 Pro Original Corrected LLaMA Guard 3 8B Original LLaMA Guard 3 8B Corrected GuardReasoner 1B Original GuardReasoner 1B Corrected Original GuardReasoner 3B GuardReasoner 3B Corrected GuardReasoner 8B Original GuardReasoner 8B Corrected 84.39 87.69 85.07 87.71 84.75 88.67 85.66 89.64 85.47 91.16 62.01 69.44 44.36 47.46 68.39 76.49 69.02 77.32 70.04 80.16 83.91 86.52 67.84 69.50 85.84 88.76 86.72 89.66 87.60 91. 90.24 91.57 87.67 87.84 90.12 90.24 91.36 92.68 94.34 95.65 WildGuard Test 76.47 77.51 70.80 72.00 74.81 79.63 79.70 84.17 78.20 84.21 Weighted Average 77.04 80.51 64.97 66.88 79.06 83.65 80.80 85.44 81.22 86.98 Table 13. Improvement (F1 score %) of GuardReasoner and baselines after label correction on the refusal detection task. Method Model Size XSTestResponse WildGuardTest GPT-4 GPT-4 Original Corrected LLaMA Guard 3 8B Original LLaMA Guard 3 8B Corrected GuardReasoner 1B Original GuardReasoner 1B Corrected GuardReasoner 3B Original GuardReasoner 3B Corrected GuardReasoner 8B Original GuardReasoner 8B Corrected 91.16 92.35 63.55 67.60 91.34 93.97 80.31 83.33 93.68 98.24 90.02 90.02 54.29 58.92 87.71 92.87 87.54 92.99 88.91 95.44 Weighted Average 90.27 90.53 56.32 60.82 88.51 93.11 85.95 90.87 89.96 96. 8B as the base model. (Ghosh et al., 2024b) propose new safety corpus with 12 top-level hazard categories. ShieldGemma 2B/9B. ShieldGemma 2B/9B is Googles AI content moderation model. It is based on Gemma 2 2B/9B (Team et al., 2024) and targets on four harm categories: sexually explicit, dangerous content, hate, and harassment. HarmBench LLaMA 13B. HarmBench LLaMA 13B is based on LLaMA 2 13B (Touvron et al., 2023b). The training data comes from GPT-4. The model is used to evaluate jailbreak attacks in HarmBench (Mazeika et al., 2024). HarmBench Mistral 7B. HarmBench Mistral 7B is based on Mistral 7B (Jiang et al., 2023). The training data is constructed by distilling GPT-4. The model is used to evaluate jailbreak attacks in HarmBench (Mazeika et al., 2024). MD-Judge 7B. MD-Judge 7B (Li et al., 2024) is based on Mistral 7B (Jiang et al., 2023). The training data is private. BeaverDam 7B. BeaverDam 7B (Ji et al., 2024b) is based on LLaMA 7B (Touvron et al., 2023a) and is instruction-tuned on BeaverTails training dataset (Ji et al., 2024b). WildGuard 7B. WildGuard 7B is based on Mistral 7B (Jiang et al., 2023). It unifies the tasks of prompt/response harmfulness detection, and refusal detection. They release the training data WildGuardTrain. QwQ-preview 32B. QwQ-preview 32B (Team, 2024c) is o1-like reasoning model released by Alibaba Group. The prompt setup is illustrated in Figure 17. For it, we evaluate random sample of 5% instances (at least 100) per benchmark due to high costs. For samples rejected by the model, we classify them as harmful or refused samples. A.3.2. GUARDREASONER We provide the implementation details of our proposed GuardReasoner. (I) In the R-SFT stage, we adopt 3 base models with different scales, including LLaMA 3.2 1B, LLaMA 3.2 3B, and LLaMA 3.1 8B. We use our synthesized GuardReasonerTrain 16 R-SFT,M(3) R-SFT,M(2) as the training data of R-SFT. It contains 127K samples with 460K reasoning steps. The chat template is set to llama3. The cutoff length is set to 2048 tokens. The initial learning rate is set to 5e-05, and we use the cosine learning rate scheduler. We use the BFloat16 training, and we adopt the full-parameter fine-tuning. We adopt AdamW optimizer. The number of epochs is set to 3. The total batch size is set to 384 = 16(accumulate step) 6(batch size) 4(device). The DeepSpeed stage is set to 3. (II) During the generation stage, the temperature is set to 1.0, and the top is set to 0.95. We use vLLM to accelerate the generation speed. The hyper-parameter of sample weight γ is set to 0.2. (III) In the HS-DPO stage, we adopt the trained model via R-SFT to conduct HS-DPO. To improve the diversity of the hard samples, we train three models M(1) R-SFT via R-SFT on different subsets of GuardReasonerTrain. Concretely, we keep the reasoning data of the WildGuard dataset since it has the most number of samples and randomly select two datasets from the reasoning data of AegisTrain, BeaverTailsTrain, and ToxicChatTrain. Then, we use these models to produce hard samples and merge them with Hself (which is produced by MR-SFT), and obtain Hensemble. We use the constructed training data Hensemble, which contains 23K (for 1B model), 14K (for 3B model), 15K (for 8B model) sample pairs. The chat template is set to llama3. The cutoff length is set to 2048 tokens. The initial learning rate is set to 5e-06, and we use the cosine learning rate scheduler. We use the BFloat16 training, and we adopt the full-parameter fine-tuning. We adopt AdamW optimizer. The number of epochs is set to 2.0. The total batch size is set to 256 = 64(accumulate step) 1(batch size) 4(device). The strength of the KL constraint β is set to 0.01. The DeepSpeed stage is set to 3. We mix the R-SFT loss in the HS-DPO stage to alleviate the model collapse, and the trade-off of the R-SFT loss is set to 2. A.3.3. PROMPT We summarize the used prompts as follows. They mainly contain two categories, i.e., prompts for training and evaluation. Training. Figure 12 illustrates the prompt used for constructing the training data for R-SFT. The primary task is for state-of-the-art LLM like GPT-4 to analyze the input alongside the ground truth, providing detailed intermediate reasoning steps. Figure 13 displays the training data utilized for R-SFT. The instruction mainly asks the guard model to analyze the input and provide the reasoning steps and the final result. The input contains both the users prompt and the target LLMs output. The output contains the synthesized reasoning steps and the ground truth. Figure 14 presents the training data employed for HS-DPO. The instruction is the same with R-SFT. The positive data is the correct outcomes with the corresponding reasoning processes while the negative data is the incorrect ones. Evaluation. Figure 15 illustrates the prompt used to evaluate GPT-4o, GPT-4, and Claude 3.5 Sonnet. It instructs the model to analyze and conduct classification on the users prompt and the target LLMs response. Figure 16 depicts the prompt utilized for the evaluation of GPT-4o+CoT and GPT-4+CoT. It instructs the model to think step by step and conduct the classification. Figure 17 shows the prompt used for assessing o1-preview and Gemini 1.5 Pro. It prompts the model without the system prompt setting since the companies do not provide the interface. We move the instruction to the user prompt. Figure 18 presents the prompt for the inference process of our proposed GuardReasoner. It instructs the model to conduct reasoning and then classify the users prompt and the target LLMs response. A.4. Case Study We first select the wrong predictions of GuardReasoner 8B according to the original labels. Then, we ask 3 human annotators to re-label the data. We give them the original labels, the predictions, and the generated explanations. We regard the majority as the corrected labels. After correcting the labels, we report the performance improvement of our method and baselines in 3 guardrail tasks in Table 11, Table 12, Table 13, and Table 7. We found that performance improved in most cases, demonstrating the meaningfulness and effectiveness of GuardReasoner in correcting error labels. We provide some cases of correcting error labels in Figure 8, Figure 9, Figure 10, and Figure 11. 17 Figure 8. One case of correcting the label from harmful to unharmful. It is from the AegisSafetyTest dataset (Ghosh et al., 2024a). Figure 9. One case of correcting the label from unharmful to harmful. It is from the AegisSafetyTest dataset (Ghosh et al., 2024a). 18 Figure 10. One case of correcting the label from harmful to unharmful. It is from the BeaverTails dataset (Ji et al., 2024b). Figure 11. One case of correcting the label from unharmful to harmful. It is from the BeaverTails dataset (Ji et al., 2024b). 19 Figure 12. The prompt for the reasoning data synthesis. Figure 13. The demonstration for the training data of R-SFT. 20 Figure 14. The demonstration for the training data of HS-DPO. Figure 15. The prompt for the inference of closed-source guard APIs, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. 21 Figure 16. The prompt for the inference of closed-source guard APIs, including GPT-4+CoT and GPT-4o+CoT. Figure 17. The prompt without system prompt for the inference of closed-source guard APIs, including o1-preview and Gemini 1.5 Pro. Figure 18. The prompt for the inference of our proposed GuardReasoner."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Chinese"
    ]
}