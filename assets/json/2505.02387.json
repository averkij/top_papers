{
    "paper_title": "RM-R1: Reward Modeling as Reasoning",
    "authors": [
        "Xiusi Chen",
        "Gaotang Li",
        "Ziqi Wang",
        "Bowen Jin",
        "Cheng Qian",
        "Yu Wang",
        "Hongru Wang",
        "Yu Zhang",
        "Denghui Zhang",
        "Tong Zhang",
        "Hanghang Tong",
        "Heng Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 7 8 3 2 0 . 5 0 5 2 : r RM-R1: Reward Modeling as Reasoning Xiusi Chen1, Gaotang Li1, Ziqi Wang1, Bowen Jin1, Cheng Qian1, Yu Wang2, Hongru Wang1, Yu Zhang3, Denghui Zhang4, Tong Zhang1, Hanghang Tong1, Heng Ji1 1University of Illinois Urbana-Champaign 2University of California, San Diego 3Texas A&M University 4Stevens Institute of Technology {xiusic, gaotang3, htong, hengji}@illinois.edu"
        },
        {
            "title": "Abstract",
            "content": "Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning score or judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. In this work, we introduce new class of generative reward models Reasoning Reward Models (REASRMS) which formulate reward modeling as reasoning task. We propose reasoning-oriented training pipeline and train family of REASRMS, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RMR1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful REASRM training. To facilitate future research, we release six REASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are rapidly becoming more capable and thus playing increasingly important roles for wide spectrum of applications that require understanding [12], generation [6] and complex decision-making [16, 22] capabilities. Reward models (RMs) play critical role in LLM post-training, particularly in reinforcement learning with human feedback (RLHF) [25, 4], where they serve as scalable proxies for human evaluators. growing body of research has focused on developing domain-specific reward models, such as mathematical verifiers [35, 10]. In this work, we are interested in advancing generalist reward modeling, where reward model is expected to provide robust and accurate supervision across wide range of domains. Generalist reward modeling poses Equal contribution Preprint. Under review. Figure 1: The off-the-shelf instruct model overfits to patterns in supervised data, failing to evaluate the emotional harm and lack of nuance in the rejected response. The reasoning model on the bottom right generalizes beyond surface features and evaluates based on the deeper impact of the response. greater challenges, as the evaluation criteria are more diverse and complex compared to specialized settings. However, developing strong generalist reward models is crucial for enabling broader, more versatile LLM applications. Existing research on reward modeling can be broadly classified into two categories: (1) scalar-based reward models (Scalar RM) [21] and (2) generative reward models (GenRM) [40]. Scalar-based approaches frame reward modeling as classification problem, typically training sequence-level classifier on top of language model. In contrast, generative-based approaches retain the original language model decoding head and leverage the models generative abilities to produce free-form pairwise judgments. While scalar-based methods are direct and often effective, they are opaque, offering no intermediate reasoning steps to justify the models decisions. This lack of transparency may limit their capacity to handle more challenging, reasoning-intensive preference tasks. On the other hand, although generative-based methods provide greater transparency by exposing intermediate reasoning traces, their reasoning is often superficial and unhelpful for reliable judgment, leading to suboptimal performance [24, 5]. As illustrated in Figure 1, correct preference judgments require accurately perceiving the question, understanding the evaluation rubrics (i.e., the major aspects to evaluate) and constructing coherent, convincing argumentsclosely mirroring how humans approach grading tasks. coherent, contemplative, and logically grounded reasoning process could naturally support solving challenging pairwise preference tasks, particularly as reasoning domains already constitute major component of modern reward modeling and we have witnessed the success of reasoning models in those domains [11]. Motivated by these observations, we explore the following central question: Can we cast reward modeling as reasoning task? In this work, we unleash the reasoning potential of reward models and propose new class of models: Reasoning Reward Models (REASRMS). We find that integrating long reasoning chains during the judging process significantly enhances downstream reward model performance, consistently outperforming its non-reasoning counterparts (e.g., scalar RM and ordinary GenRM). We explore several strategies for adapting instruction-tuned language models into logically coherent REASRMS. Notably, we observe that directly applying reinforcement learning with verifiable rewards (RLVR) to generalist reward modeling fails to fully unleash the models reasoning potentials for reward modeling. Through series of studies, we design reasoning training pipeline that introduces stage of structured reasoning distillation prior to RLVR, ultimately resulting in the development of RM-R1. We observe that the distillation stage is critical, as RLVR alone often biases the model toward focusing on superficial features. Starting from an instruction-tuned model, RM-R1 training leverages novel Chain-of-Rubrics (CoR) prompting framework that elicits structured and coherent reasoning. Specifically, the model first classifies each input task into one of two categories: reasoning or chat. For chat tasks, the model 2 generates set of evaluation rubrics, justifications for the rubrics, and evaluations tailored to the specific question. For reasoning tasks, correctness is generally preferred, so we directly let the model first solve the problem itself before evaluating the pairwise model responses. This finer-grained task classification enables RLVR to fully exploit reasoning supervision for reasoning-heavy tasks while producing coherent, high-quality evaluation chains for chat tasks. In addition, we explore how to directly adapt existing reasoning models into reward models. Since these models have already undergone substantial reasoning-focused distillation, we fine-tune them using RLVR without requiring additional distillation stages. Based on our training recipes, we release family of REASRM models ranging from 7B to 32B. Empirically, our reasoning reward models consistently yield highly interpretable and coherent reasoning traces, achieving better or comparative performance on RewardBench [18], RM-Bench [23], and RMB [43], outperforming 70B, 340B, GPT-4o, and Claude models by up to 13.8% in accuracy. Beyond final performance, we conduct extensive empirical analyses of RM-R1, including studies of its scaling effects, ablations of our training recipes, comparisons with non-reasoning baselines, training dynamics, and detailed case studies. In summary, our main contributions are as follows: We demonstrate that reasoning abilities are crucial for reward models, and propose to formulate reward modeling as reasoning process to enhance interpretability and accuracy. We design training recipe based on reasoning-oriented distillation and RL that produces set of reward models - RM-R1that can outperform larger models by up to 13.8% in accuracy. We present systematic empirical study of different training recipes for REASRMS, providing insights into the impact of diverse training strategies on the final reward model performance. We open-source the related resources, including code, data, and model checkpoints, to facilitate reproducibility and follow-up research."
        },
        {
            "title": "2 Related Work",
            "content": "Reward Models (RMs). Early RMs were typically outcome-focused: trained to predict human preference rankings for complete outputs [42]. Recent advances have looked at providing process supervision, which rewards or evaluates the steps of models reasoning rather than only the final answer. series of works propose to train process reward models that judge the correctness of intermediate reasoning steps [20, 8, 29]. limitation of many PRMs is their heavy reliance on curated step-level human labels or specific schemas, and they often remain domain-specific. Zhang et al. [40] propose Generative Verifiers, framing reward modeling as next-token prediction task. This allows the reward model to leverage chain-of-thought and even use majority voting over multiple sampled rationales to make more reliable judgments. DeepSeek-GRM [24] and JudgeLRM [5] have studied using reasoning models as generative reward models, which are the most relevant research to ours. However, their main focus is on the effect of scaling inference-time computation on reward modeling. On the contrary, our work is the first to provide systematic empirical comparison of different reward model training paradigms, shedding light on when and why distilled and RL-trained reward model like RM-R1 has advantages over the conventional approaches. Reinforcement Learning from Human Feedback (RLHF). Early works [7] first demonstrated that reinforcement learning could optimize policies using reward model trained from human pairwise preferences. Subsequent studies applied RLHF to large-scale language models using policy optimization algorithms such as PPO [28]. For example, Ziegler et al. [45] fine-tuned GPT-2 via PPO on human preference rewards, and Stiennon et al. [33] showed that RLHF could significantly improve the quality of summarization by optimizing against learned preference model. More recently, Ouyang et al. [25] used similar PPO-based pipeline to train InstructGPT, establishing the modern RLHF paradigm for instruction-following models. Recently, Verifiable supervision techniques have also emerged: DeepSeek-R1 [11] uses form of self-verification during RLHF to reward correct reasoning steps, rather than only final-answer quality. This method incentivizes policies to produce outputs that can be verified for correctness, bridging the gap between pure preference-based feedback and ground-truth signals. However, even with such innovations, most RLHF implementations still treat reward modeling and reasoning as separate stages. 3 Figure 2: Training pipeline of RM-R1. Starting from generative reward model (GenRM), RM-R1 training involves two stages: Distillation and Reinforcement Learning (RL). In the Distillation stage, we use high-quality synthesized data to bootstrap RM-R1s reasoning ability. In the RL stage, RM-R1s reasoning ability for reward modeling is further strengthened. After distillation, GenRM evolves into REASRM. RM-R1 further differentiates itself by being RL finetuned on preference data."
        },
        {
            "title": "3 RM-R1",
            "content": "Figure 2 illustrates the overall training pipeline of RM-R1. Starting from any off-the-shelf instructiontuned model (e.g., Qwen-2.5-14b-instruct), we synthesize high-quality reasoning traces and distill RM-R1 on the synthesized reasoning traces. After distillation, RM-R1 unlocks the basic reasoning ability for reward modeling. However, distilled models often still suffer from overfitting to certain patterns in the distillation data, thus, the models ability to generalize may be limited. To further improve RM-R1s performance, we use RL to continually train and derive the full model of RM-R1. In this section, we begin by defining the reward modeling task (Section 3.1), then briefly describe the distillation process (Section 3.2). Later, we introduce in detail how we conduct RL training, especially our customized rollout strategy and reward definition (Section 3.3). 3.1 Task Definition Motivation: Collecting explicit scalar rewards for LLM outputs from human annotators is often expensive and impractical. It is challenging to assign consistent numerical scores due to factors like subjective biases and variable interpretation of scales. Conversely, obtaining pairwise preference judgmentssimply indicating which of two responses is betteris significantly more intuitive and reliable for annotators. Such preference data is easier to acquire at scale and inherently aligns with human decision-making processes, resulting in more consistent and actionable training signals. Motivated by this practicality, recent approaches advocate training reward models using pairwise preference datasets. Among them, training GenRMs in an LLM-as-a-judge fashion has emerged as particularly promising [26, 39, 41]. Unlike traditional classifiers or regressors that output single scalar values, generative reward models produce comprehensive, textual judgments, including rationales and explicit reasoning about their preferences. This approach leverages LLMs natural ability for nuanced evaluation and interpretability, effectively capturing complex, multi-dimensional criteria beyond simplistic scalar rankings. Formal Definition: Given preference dataset: , y(i) where is prompt, ya and yb are two different responses for x. {a, b} is the ground truth label that indicates the preferred response. We define the generative reward modeling task as follows: = {(x(i), y(i) , l(i))}N i=1, 4 Let rθ denote generative reward model parameterized by θ. (x(i), y(i) , y(i) modeled by: For each data sample , l(i)), rθ generates textual judgment consisting of tokens = (j1, j2, . . . , jT ), rθ(jx, ya, yb) = (cid:89) t=1 rθ(jtx, ya, yb, j<t). Note that contains rθs prediction of the preferred response ˆl j. The goal of reward modeling is : max rθ (x,ya,yb,l)D,ˆlrθ(jx,ya,yb) (cid:104) 1(ˆl = l) (cid:105) . (1) 3.2 Distillation of Reasoning Trace For an instruct model, e.g., Qwen-2.5-14b-instruct [38], it is quite intuitive to turn it into GenRM simply by prompting. However, without fine-tuning on reward modeling reasoning traces, these models may struggle to conduct consistent judgments. To bootstrap an instruct models reasoning potential, we start with training an instruct model with long reasoning traces synthesized for reward modeling. Specifically, we subsample data samples from and denote it as Dsub. Given data sample (x(i), y(i) , l(i)) Dsub, we ask an oracle model like o3 or claude-3-7-sonnet to generate its structured reasoning trace r(i) justifying why y(i) is chosen as the correct answer of x(i). We then construct the reasoning trace ground truth: , y(i) trace = r(i) y(i) y(i) , (2) where denotes string concatenation. Given all the synthesized reasoning traces r(i), the final distillation dataset is defined as: Ddistill = {(x(i), y(i) trace)}M i=1, (3) Formally, the objective of distillation is to adjust θ to maximize the likelihood of generating the desired reasoning trace and response given the prompt x. We minimize the negative log-likelihood (NLL) loss: Ldistill(θ) = (cid:88) (cid:88) (x,y)Ddistill t=1 log rθ (yt x, y<t) (4) where y<t = (y1, y2, ..., yt1) denotes the sequence of tokens preceding position t. 3.3 RL Training Although distilling is proper way to turn general generative model into GenRM, it often suffers from overfitting to certain patterns and constrains the models ability to generalize its reasoning abilities for critical thinking [32] , which is essential for reward modeling. To address this, we propose to integrate RL as more powerful learning paradigm to enhance the models ability to conduct reward-based reasoning. Training policy model using RL has been widely seen in the preference optimization phase of LLM post-training [25], and it is quite natural to extend this paradigm for training REASRM. To be specific, we directly treat our reward model rθ(j x, ya, yb) as if it is policy model: max rθ (x,ya,yb,l)D,ˆlrθ(jx,ya,yb) [R(x, j)] βDKL (rθrref ) , (5) where rref is the reference reward model. In practice, we use the checkpoint before training as rref , and that means rref could be an off-the-shelf LLM or the LLM obtained after the distillation step in Section 3.2. R(x, j) is the reward function, and DKL is KL-divergence. The denotes input prompts drawn from the preference data D. The indicates the text generated by the reward model, which includes the reasoning trace. Chain-of-Rubrics (CoR) Rollout for Instruct Models Please act as an impartial judge and evaluate the quality of the responses provided by two AI Chatbots to the Clients question displayed below. First, classify the task into one of two categories: <type> Reasoning </type> or <type> Chat </type>. - Use <type> Reasoning </type> for tasks that involve math, coding, or require domain knowledge, multi-step inference, logical deduction, or combining information to reach conclusion. - Use <type> Chat </type> for tasks that involve open-ended or factual conversation, stylistic rewrites, safety questions, or general helpfulness requests without deep reasoning. If the task is Reasoning: 1. Solve the Clients question yourself and present your final answer within <solution> ... </solution> tags. 2. Evaluate the two Chatbot responses based on correctness, completeness, and reasoning quality, referencing your own solution. 3. Include your evaluation inside <eval> ... </eval> tags, quoting or summarizing the Chatbots using the following tags: - <quote_A> ... </quote_A> for direct quotes from Chatbot - <summary_A> ... </summary_A> for paraphrases of Chatbot - <quote_B> ... </quote_B> for direct quotes from Chatbot - <summary_B> ... </summary_B> for paraphrases of Chatbot 4. End with your final judgment in the format: <answer>[[A]]</answer> or <answer>[[B]]</answer> If the task is Chat: 1. Generate evaluation criteria (rubric) tailored to the Clients question and context, enclosed in <rubric>...</rubric> tags. 2. Assign weights to each rubric item based on their relative importance. 3. Inside <rubric>, include <justify>...</justify> section explaining why you chose those rubric criteria and weights. 4. Compare both Chatbot responses according to the rubric. 5. Provide your evaluation inside <eval>...</eval> tags, using <quote_A>, <summary_A>, <quote_B>, and <summary_B> as described above. 6. End with your final judgment in the format: <answer>[[A]]</answer> or <answer>[[B]]</answer> Important Notes: - Be objective and base your evaluation only on the content of the responses. - Do not let response order, length, or Chatbot names affect your judgment. - Follow the response format strictly depending on the task type. Figure 3: The system prompt used for RM-R1 rollout (for instruct models). 3.3.1 Chain-of-Rubrics Rollout To facilitate the distilled models to proactively generate critical reasoning traces, we design system prompt as shown in Figure 3 during rollout. Intuitively, reward modeling for general domain (chat, safety, etc.) and reasoning domain (math, code, etc.) should focus on different angles. For example, for the chat domain, we may care more about some aspects that can be expressed in textual rubrics (e.g., be polite), yet for the reasoning domain, we usually care more about logical coherence and answer correctness. Based on this intuition, we instruct rθ to classify each preference data sample {(x, yc, yr)} into one of the two <type>: Chat or Reasoning. For each <type>, we ask rθ to practice different reasoning step by step: For reasoning tasks, we ask rθ to solve on its own. During the <eval> phase, rθ compares yc and yr conditioned on its own </solution> and selects an <answer>. Regarding the Chat type, we instead ask rθ to think about and justify the <rubric> for grading the chat quality (including safety). Since Chat scenarios do not have verifiable reward, we instruct the model to select the <answer> based on its own <rubric>. Large reasoning models such as DeepSeek-R1-distilled models [11] do not have system prompt, so we show the user prompt for rollouts in Figure 4. 3.3.2 Reward Design Rule-based reward mechanisms have demonstrated strong empirical performance to facilitate reasoning since DeepSeek-R1 [11], and have been commonly employed ever since. In our training, 6 Chain-of-Rubrics (CoR) Rollout for Reasoning Models Please act as an impartial judge and evaluate the quality of the responses provided by two AI Chatbots to the Client question displayed below. ... [Pairwise Input Content] ... Output your final verdict at last by strictly following this format: is better, or <answer>[[B]]</answer> if Chatbot is better. <answer>[[A]]</answer> if Chatbot Figure 4: The user prompt used for RM-R1 rollout (for reasoning models). we further simplify the reward formulation of DeepSeek-R1 and merely focus on correctness-based component, in line with prior works [30, 19]. We have also tried adding the format reward to the overall reward, but found that the task performance does not have significant difference. The rationale behind only focusing on correctness is that the SFT models have already learned to follow instructions and format their responses properly. During experiments, we also observe that adding format reward makes the advantage computation significantly slower, since regular expression matching grows linearly with the response token length. Formally, our reward is defined as follows: R(x, jya, yb) = (cid:40) 1, if ˆl = 1, otherwise. (6) 3.3.3 Group Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO) [30] is variant of Proximal Policy Optimization (PPO) [28], which obviates the need for additional value function approximation, and uses the average reward of multiple sampled outputs produced in response to the same prompt as the baseline. More specifically, for each prompt x, GRPO samples group of outputs {y1, y2, , yG} from the old policy πθold and then optimizes the policy model by maximizing the following objective: JGRPO(θ) = xD, {ji}G i=1rθold (jx) (cid:20) 1 (cid:88) i=1 1 ji ji (cid:88) (cid:110) t=1 min (cid:16) rθ(ji,t x, ji,<t) rθold (ji,t x, ji,<t) ˆAi,t, clip(cid:0) rθ(ji,t x, ji,<t) rθold(ji,t x, ji,<t) , 1 ϵ, 1 + ϵ(cid:1) ˆAi,t (cid:17) β DKL [rθ( x) πref( x)] (7) (cid:111)(cid:21) where β is hyperparameter balancing the task specific loss and the KL-divergence. Specifically, ˆAi is computed using the rewards of group of responses within each group: {r1, r2, . . . , rG}, and is given by the following equation: ˆAi = ri mean({r1, r2, , rG}) std({r1, r2, , rG}) . (8)"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup 4.1.1 Benchmarks In this paper, we consider the following three benchmarks: RewardBench [18]: RewardBench is one of the first endeavors towards benchmarking reward models through prompt-chosen-rejected trios, covering four categories: chat, chat-hard, reasoning, safety, with 358. 456, 740. 1431 samples, respectively. 7 RM-Bench [23]: Building on RewardBench, RM-Bench evaluates reward models for their sensitivity to subtle content differences and robustness against style biases. It includes four categories: Chat, Safety, Math, and Code, with 129, 441, 529, and 228 samples, respectively. Each sample contains three prompts of varying difficulty. RM-Bench is the most reasoning-intensive benchmark among those we consider. RMB [43]: Compared with RewardBench and RM-Bench, RMB offers more comprehensive evaluation of helpfulness and harmlessness. It includes over 49 real-world scenarios and supports both pairwise and Best-of-N (BoN) evaluation formats. RMB comprises 25,845 instances in total37 scenarios under the helpfulness alignment objective and 12 under harmlessness. 4.2 Preference Datasets We consider the following datasets for training (A summary is presented in Appendix B): Skywork Reward Preference 80K [21] is high-quality collection of pairwise preference data drawn from variety of domains, including chat, safety, mathematics, and code. It employs an advanced data filtering technique to ensure preference reliability across tasks. However, we identify notable issue with this dataset: all samples from the magpie_ultra source exhibit strong spurious correlation, where rejected responses consistently contain the token <im_start>, while accepted responses do not. Additionally, responses from this source show systematic biasaccepted responses are typically single-turn, while rejected responses are multi-turn. This problematic subset constitutes approximately 30% of the Skywork dataset and primarily covers mathematics and code domains. To avoid introducing spurious correlations into training, we exclude all magpie_ultra data and retain only the cleaned subset for our experiments. Code-Preference-Pairs is high-quality coding preference dataset. It is constructed by prompting model with original code, introducing deliberate bugs, and manipulating examples (e.g., swapping broken and corrected versions, removing error comments) to generate fine-grained preference pairs. We subsample 8K examples from this dataset for use in our experiments. Math-DPO-10K [17] is high-quality stepwise preference dataset focused on mathematical reasoning. We use the full dataset in our experiments. 4.3 Baselines We compare RM-R1 with RMs from three categories: Scalar RMs. Scalar RMs produce score for model response directly, predicting preference through single numeric scores without explicit reasoning traces. This category includes models such as RM [33], SteerLM-RM [37], Nemotron-RM [2], Starling-RM [44], ArmoRM [34], SkyworkRM [21], etc. These models typically show strong performance in clearly defined tasks but often lack interpretability and nuanced reasoning capability. GenRMs. Generative reward models provide more nuanced feedback by generating textual judgments. Examples include Llama [9], Qwen [38], Claude [3], GPT-4 variants [1, 14], Gemini-1.5-pro [27] and Self-taught Evaluators [36]. GenRMs leverage generative capabilities of LLMs to produce explanations and rationales alongside numerical scores, enhancing interpretability and decision transparency. REASRMS. Reasoning-enhanced reward models explicitly incorporate reasoning processes into their judgments, often trained through critiques or chain-of-thought strategies. Notable examples are Critique-RM [39], DeepSeek-GRM [24] and our proposed RM-R1 models. These models excel in tasks demanding rigorous reasoning, safety evaluations, and nuanced preference judgments due to their grounding in structured critical thinking. 4. Implementation Details Our training framework is based on verl [31] and OpenRLHF [13]. We use the training sets detailed in Section 4.3. For Instruct models, we use 8.7k data for distillation and 64k for RLVR. For Deepseek-Distilled models, we use the full data for RLVR. The full implementation details are provided in Appendix C. 8 Table 1: Results of our proposed method and baselines on the RewardBench. Bold numbers indicate the best performance, Underlined numbers indicate the second best. Models Scalar RMs SteerLM-RM 70B Cohere-0514 Nemotron-4-340B-Reward GenRMs Llama3.1-8B-Instruct Prometheus-8*7B-v2 Llama3.1-70B-Instruct Llama3.1-405B-Instruct Claude-3-5-sonnet-20240620 GPT-4o-0806 Gemini-1.5-pro-0514 Self-taught-evaluator-llama3.1-70B SFR-LLaMa-3.1-70B-Judge-r REASRMS SynRM RM-R1-DeepSeek-Distilled-Qwen-7B CLoud DeepSeek-GRM-16B RM-R1-Qwen-Instruct-7B DeepSeek-GRM-27B RM-R1-Qwen-Instruct-14B RM-R1-DeepSeek-Distilled-Qwen-14B RM-R1-DeepSeek-Distilled-Qwen-32B RM-R1-Qwen-Instruct-32B Chat Chat_Hard Safety Reasoning Overall 91.3 96.4 95.8 85.5 93.0 97.2 97.2 96.4 96.1 92.3 96.9 96.9 38.0 88.9 97.0 90.8 94.1 94.1 93.6 91.3 95.3 95.3 80.3 71.3 87.1 48.5 47.1 70.2 74.6 74.0 76.1 80.6 85.1 84. 82.5 66.2 58.0 74.3 74.6 78.3 80.5 79.4 80.3 83.1 92.8 92.3 91.5 75.6 80.5 82.8 77.6 81.6 86.6 87.9 89.6 91.6 74.1 78.4 84.0 84.7 85.2 88.0 86.9 89.3 91.1 91.9 90.6 97.7 93.6 72.1 77.4 86.0 87.1 84.7 88.1 92.0 88.4 97. 87.1 87.0 92.0 81.8 86.7 83.8 92.0 95.5 96.8 95.2 88.8 89.4 92.0 70.4 74.5 84.0 84.1 84.2 86.7 88.2 90.0 92.7 70.4 80.1 82.8 82.9 85.2 86.0 88.2 88.9 90.9 92.9 4.5 Main Results For the baselines, we try our best to reproduce the numbers if essential resources are open-sourced (e.g., model checkpoints, system prompts). Otherwise, we use the numbers reported in the corresponding tech report or benchmark leaderboard. For each benchmark, we cutoff and select the best-performing models in each category for brevity. Table 1, Table 2, and Table 3 report the performance of RM-R1 and baseline models on RewardBench, RM-Bench, and RMB, respectively. Our key findings are summarized below: RM-R1 Achieves State-of-the-Art Performance. We first compare our two strongest modelsRM-R1-Qwen-Instruct-32B and RM-R1-DeepSeek-Distilled-Qwen-32Bagainst topperforming baselines. On Reward Bench, RM-R1-Qwen-Instruct-32B achieves state-of-the-art performance among all existing generative reward models, outperforming closed-source commercial systems such as GPT-4o, Gemini-1.5-Pro, and models up to 340B in size. On RM-Bench, RM-R1-DeepSeek-Distilled-Qwen-32B surpasses the previous state-of-the-art by up to 12.8%, marking significant improvement. On RMB, RM-R1-Qwen-Instruct-32B attains near state-ofthe-art performance with 73.0%, closely matching the best prior result of 73.8%. Notably, this model is trained on only 73K preference examples, demonstrating strong data efficiency. Reasoning Training Demonstrates Effectiveness. Our reasoning-based training pipeline yields substantial gains. For example, RM-R1-Qwen-Instruct-14B consistently outperforms DeepSeek-GRM-27B, reasoning model nearly twice its size. We provide detailed analysis of the key training design choices that contribute to this success in Section 5.2, and further highlight the advantages of reasoning-based training over non-reasoning counterparts in Section 5.3. RM-R1 Generalizes Across Domains. reward models based on Qwen-2.5-Instruct exhibit including chat, safety, and reasoning, with no observable domain bias. Meanwhile, models trained supervifrom DeepSeek-Distilled-Qwen sion and achieve stronger performance in reasoning-centric benchmarks. Specifically, RM-R1-DeepSeek-Distilled-Qwen-32B sets new state-of-the-art on RM-Bench with strong performance across all domains, reasoning-focused from extensive Our generalist benefit 9 Table 2: The full results of tested reward models on RM-Bench. Chat, Math, Code, Safety show the models Average Accuracy on each domain. Easy, Normal, Hard show the models Accuracy on each difficulty level across all domains. Bold numbers indicate the best performance, Underlined numbers indicate the second best. Models Scalar RMs Chat Math Code Safety Easy Normal Hard Avg 56.4 steerlm-70b 58.2 tulu-v2.5-70b-preference-mix-rm Mistral-7B-instruct-Unified-Feedback 56.5 57.4 RM-Mistral-7B 59.9 Eurus-RM-7b 61.7 internlm2-7b-reward 62.7 GRM-llama3-8B-sftreg 63.1 internlm2-20b-reward 71.3 Llama-3-OffsetBias-RM-8B 71.2 Nemotron-340B-Reward 71.2 URM-LLaMa-3.1-8B 69.5 Skywork-Reward-Llama-3.1-8B 53.0 49.3 51.4 55.5 58.0 51.7 57.0 52.7 60.2 56.9 71.4 49.7 62.5 57.8 66.8 56.7 61.9 53.2 59.8 59.4 61.8 54.1 60.6 54.5 GenRMs tulu-v2.5-dpo-13b-chatbot-arena-2023 64.9 56.3 tulu-v2.5-dpo-13b-nectar-60k 67.2 stablelm-2-12b-chat 66.4 tulu-v2.5-dpo-13b-stackexchange-60k 58.8 Nous-Hermes-2-Mistral-7B-DPO 68.4 tulu-v2.5-dpo-13b-hh-rlhf-60k 66.4 tulu-2-dpo-13b 78.6 SOLAR-10.7B-Instruct-v1. 52.3 50.5 52.4 52.6 54.9 51.6 49.9 54.2 55.6 51.3 51.1 52.3 51.4 51.8 52.3 49.6 REASRMS RM-R1-Qwen-Instruct-7B RM-R1-DeepSeek-Distilled-Qwen-7B RM-R1-Qwen-Instruct-14B RM-R1-Qwen-Instruct-32B RM-R1-DeepSeek-Distilled-Qwen-14B RM-R1-DeepSeek-Distilled-Qwen-32B 66.6 64.0 75.6 75.3 71.8 74.2 67.0 54.6 83.9 56.2 75.4 60.6 80.2 66.8 90.5 69.5 91.8 74.1 51.2 87.1 86.8 87.2 86.5 85.5 90.0 86.5 89.6 87.5 93.1 95.7 62.3 73.8 65.2 69.0 73.9 76.5 85.4 78. 92.6 85.3 93.6 93.9 94.1 95.4 48.3 72.8 87.1 88.6 87.2 85.4 83.5 82.6 84.6 81.0 84.0 89.0 82.8 86.7 69.1 79.5 69.5 53.6 86.9 57.5 79.2 75.9 82.6 86.3 86.2 89.5 54.9 65.6 67.3 67.1 70.2 70.7 72.7 71.6 72.2 71.4 73.2 74.7 60.2 64.3 63.5 63.0 61.1 63.0 66.7 67. 71.7 73.1 77.5 80.5 83.6 85.4 54.3 52.5 50.7 63.0 35.3 63.2 34.9 63.5 40.2 65.9 45.1 67.1 48.6 68.2 50.7 68.3 50.2 69.0 56.1 69.5 53.0 70.0 46.6 70.1 29.5 57.5 25.4 58.8 46.6 59.7 37.2 59.9 49.1 59.9 69.6 62.1 37.7 63.8 69.4 64.8 59.7 70.2 68.1 72.4 68.8 76.1 70.4 79.1 74.4 81.5 76.7 83.9 91.8% in math and 74.1% in code, outperforming previous best models by substantial margins (66% in math and 59% in code). It also achieves the strongest reasoning performance among our released models on RewardBench. However, in terms of data efficiency, our Instruct-based models reach competitive reasoning performance and superior overall results using only 8.7K examples for distillationcompared to the 800K examples used in training DeepSeek-Distilled. Additional Analysis. Across all benchmarks, we observe that larger models consistently achieve higher performance. This trend is not coincidental. In Section 5.1, we conduct detailed analysis and show that reasoning reward models benefit more from increased model scale, reinforcing the effectiveness of scaling in this setting. We also conduct more fine-grained analysis of the training dynamics and case study of RM-R1 in Section 5.4 and 5.5."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we present series of empirical studies and ablation experiments aimed at understanding the key ingredients for training effective reasoning reward models. Our analysis spans scaling effects, design decisions, reasoning ablations, training dynamics, and case studies. 5.1 Scaling Effects We begin by investigating how model performance varies with scale, considering both model size and inference-time compute. While prior work has shown that larger models generally achieve lower pretraining loss with stronger capabilities [15], and recent studies highlight the benefits of increased inference budgets [15], reward modeling presents more nuanced picture. In some casessuch as 10 Table 3: The leaderboard of RMB, ranked by the average score of all subsets. Bold numbers indicate the best performance, Underlined numbers indicate the second best. Models Scalar RMs Helpfulness Harmlessness BoN Pairwise BoN Pairwise Overall Tulu-v2.5-13b-preference-mix-rm Skywork-Reward-Gemma-2-27B Internlm2-20b-reward ArmoRM-Llama3-8B-v0.1 Internlm2-7b-reward Eurus-RM-7b Skywork-Reward-Llama-3.1-8B Starling-RM-34B GenRMs Llama2-70b-chat Llama3.1-8B-Instruct Gemini-1.5-pro Mixtral-8x7B-Instruct-v0.1 skywork-critic-llama3.1-8B skywork-critic-llama3.1-70B Llama3.1-70B-Instruct Mistral-Large-2407 Claude-3-5-sonnet Qwen2-72B-Instruct GPT-4o-2024-05-13 REASRMS RM-R1-DeepSeek-Distilled-Qwen-7B RM-R1-Qwen-Instruct-7B Deepseek-GRM-27B-RFT RM-R1-DeepSeek-Distilled-Qwen-14B Deepseek-GRM-27B RM-R1-Qwen-Instruct-14B RM-R1-DeepSeek-Distilled-Qwen-32B RM-R1-Qwen-Instruct-32B 0.355 0.472 0.585 0.636 0.626 0.679 0.627 0.604 0.289 0.365 0.536 0.480 0.600 0.640 0.648 0.678 0.705 0.645 0. 0.451 0.543 0.592 0.593 0.623 0.594 0.620 0.636 0.562 0.653 0.763 0.787 0.782 0.818 0.781 0.774 0.613 0.675 0.763 0.706 0.725 0.753 0.811 0.817 0.838 0.810 0.815 0.658 0.740 0.801 0.765 0.805 0.776 0.782 0.791 0.351 0.561 0.499 0.497 0.563 0.543 0.603 0.674 0.249 0.267 0.299 0.491 0.578 0.614 0.558 0.583 0.518 0.649 0. 0.429 0.608 0.548 0.613 0.570 0.620 0.618 0.682 0.545 0.721 0.670 0.663 0.712 0.693 0.759 0.795 0.602 0.653 0.661 0.671 0.578 0.614 0.739 0.725 0.764 0.789 0.814 0.664 0.765 0.765 0.769 0.761 0.778 0.771 0.809 0.453 0.602 0.629 0.646 0.671 0.683 0.693 0.712 0.438 0.490 0.565 0.587 0.620 0.655 0.689 0.701 0.706 0.723 0. 0.551 0.664 0.670 0.685 0.690 0.692 0.698 0.730 scalar-valued sequence models from Skywork [21]the performance gap between the small model (e.g., 8b) and the large model (e.g., 27b) is minimal (often less than 1%), showing no clear advantage from scaling. In this subsection, we show that this trend does not hold for reasoning reward models, where scaling brings clear and substantial improvements. 5.1.1 Model Sizes We now analyze the impact of model scale. Our study is based on the Qwen-2.5-Instruct model family at three sizes: 7B, 14B, and 32B. We evaluate performance improvements resulting from our training procedure described in Section 3, with results averaged across three key benchmarks: RewardBench, RM-Bench, and RMB. For each model size, we compare the original and post-training performance. Figure 5a plots the absolute improvement with respect to the original model performance (in %) as function of model size. Observing an approximately linear trend, we fit linear regression model and extrapolate to hypothetical scales of 3B and 72B, shown using faint markers and dashed extensions. The results strongly support scaling law for reasoning reward models (RM-R1): larger models not only result in an absolute better final performance but also consistently yield greater performance gains. This aligns with the intuition that our training effectively leverages the superior reasoning capabilities of larger models. Furthermore, these findings provide compelling evidence that reinforcement learning benefits from scale as well. (a) Model Size (b) Inference Compute Figure 5: Scaling effect of RM-R1. We find that (a) larger models benefit more from reasoning training, and (b) longer reasoning chains result in better reward modeling performance. 5.1.2 Inference-time Computation Next, we examine the effect of inference-time computationhow model performance varies with different compute budgets (measured in number of tokens allowed during inference). Since this is particularly relevant to reasoning-focused models, we fix our base model to DeepSeek-R1-Distill-Qwen-14B. We evaluate average performance across the three key benchmarks using wide range of inference-time compute budgets: 512, 1024, 2048, 4096, and 8192 tokens. To ensure fair comparison, we match the training compute to the inference budget in each setting (i.e., we allow maximum of tokens during training for compute budget of at inference). All models are trained using GRPO with identical datasets and hyperparameter configurations. Figure 5b shows the relationship between compute budget and performance. We observe clear improvement trend as the inference budget increases. This highlights the benefits of long reasoning chains in reward modeling. Takeaway 1: Scaling improves reward model performance: we observe near-linear trend with both model size and inference-time compute. Larger models consistently benefit more from our reasoningbased training pipeline, and longer reasoning chains become increasingly effective under higher compute budgets. 5.2 Training Recipes We next investigate the key ingredients underlying the successful training of RM-R1. Through series of ablation studies, we examine our design choices to identify effective strategies for training high-quality reasoning reward models. We compare the following settings: Cold Start RL. Prior work on reasoning model training typically adopts straightforward pipeline based on reinforcement learning with verifiable rewards [5, 24]. This approach generally involves pure RL, with rule-based rewards centered on answer correctness and format compliance. Such strategies have achieved notable success in advanced mathematical problem solving. In this setting, we replicate this conventional training setup. Specifically, we use combination of format reward and an answer reward: Rformat = (cid:26)1 0 if format matches otherwise Ranswer = (cid:26)1 0 if answer matches otherwise (9) 12 The total reward is the sum = Ranswer + Rformat. We use the prompt template shown in Figure 7, simplified version of Figure 3. In this setup, all input prompts are treated uniformlythat is, chat and reasoning tasks are not distinguished. Cold Start RL + Query Categorization (QC). This setting largely follows the previous one, with key modification: prompting the LM to first categorize the task into reasoning or chat, and then apply different strategies for handling those tasks. Intuitively, reinforcement learning alone can effectively explore reasoning tasks, domain where it has already achieved considerable success. Here, we incorporate the system prompt shown in Figure 3, which explicitly distinguishes between chat and reasoning tasks. For reasoning tasks specifically, we note that answer quality is closely tied to correctness, and that high-level rubrics may be less effective than simply evaluating whether the model can solve the problem and verify its own answer. Thus, this setting emphasizes correctness-based evaluation guided by task classification in the prompt. Distilled + RL + QC (RM-R1). Building on the previous setup, we introduce an additional distillation stage from stronger teacher models as warm start before RL training. The motivation is that with RL alone, weaker models (especially at smaller scales) often fail to explore high-quality rubrics and convincing reasoning chains for chat tasks throughout the RL training process. Distilling strong reasoning traces on small subset of data can effectively mitigate this limitation. We now expand on the details of generating high-quality reasoning chains. We first use the same prompt to query Claude-3.7-Sonnet, generating initial reasoning traces. However, approximately 25% of these traces are incorrect, primarily on harder chat tasks. To correct these cases, we pass the original prompt, the incorrect trace, and the correct final answer to OpenAI-O3, which then generates corrected reasoning trace aligned with the right answer. This two-stage process yields high-quality distillation set. We deliberately choose the orderfirst Claude, then O3based on qualitative observations: Claude excels at solving easier tasks and maintaining attention to safety considerations, whereas O3 performs better on harder helpfulness tasks but tends to overemphasize helpfulness at the expense of safety. We select approximately 12% of the training data (slightly fewer than 9K examples) for distillation. This is then followed by RL training. Table 4: Ablation Study of the Designs of the Qwen-2.5-Instruct-32B on Reward Bench. Safety Reasoning Reward Bench Chat Chat Hard Method Instruct (Original) Instruct + Cold Start RL Instruct + Cold Start RL + QC Instruct + Distilled + RL + QC 95.8 93.8 95.0 95.3 74.3 78.1 78.1 83.1 86.8 90.3 88.9 91.9 86.3 90.6 95.8 95.2 85.8 88.2 89.8 92.9 In Table 4, we present the results of the ablation studies described above, using the Qwen-2.5-Instruct-32B model as the Instruct (Original) model. Several key conclusions emerge: RL training alone is insufficient. While Cold Start RL slightly improves performance on hard chat and reasoning tasks, it fails to close the gap to fully optimized models. Query Categorization significantly boosts reasoning performance. Incorporating explicit query categorization into the prompt notably improves reasoning performance, suggesting that clearer task guidance benefits learning. Distillation further enhances performance across all axes. Seeding the model with high-quality reasoning traces before RL yields the strongest results, with improvements observed on both hard tasks and safety-sensitive tasks. 13 Takeaway 2: Directly replicating reinforcement learning recipes from mathematical tasks is insufficient for training strong reasoning reward models. Explicit query categorization and targeted distillation of high-quality reasoning traces are both crucial for achieving robust and generalizable improvements. 5.3 Effectiveness of Reasoning Training We now analyze the impact of reasoning-based training. Throughout this subsection, we use SFTto refer to supervised fine-tuning on final answers only, and distilled to refer to supervised fine-tuning on reasoning chains generated by stronger model. Prior work has shown that SFT alone can yield substantial performance gains [21]. Here, we demonstrate that reasoning-based training can outperform such answer-only approaches. We consider the following experimental settings: Instruct + SFT. This approach fine-tunes the instruct model directly toward producing the correct final answer using the full dataset, without providing any intermediate reasoning chains. Instruct + Distilled + SFT. This approach applies SFT (with respect to the answer directly) after the distillation stage, serving as direct comparison point with RM-R1. Instruct + RM-R1 (Distilled + RL). This is the full approach proposed in this paper, following the procedure detailed in Section 3 and Section 5.2. Instruct + Distilled. This setting uses the model checkpoint immediately after the distillation stage, before any RL fine-tuning. Table 5: Comparison of reasoning-based training versus SFT, measured across the three key benchmarks. Methods marked with * involve reasoning training. We can find that reasoning training consistently results in better-performing reward model. Method RewardBench RM-Bench RMB Average Full Data Instruct + SFT Instruct + Distilled + SFT Instruct + RM-R1 * 9k Data Instruct + SFT Instruct + Distilled* 90.9 91.2 92.9 88.8 89.0 75.4 76.7 79.1 74.8 76. 65.9 65.4 73.0 66.9 72.0 77.4 77.8 81.7 76.6 79.2 In summary, both (+ RM-R1) and (+ Distilled) represent reasoning-based approaches, while the remaining methods are purely non-reasoning-based approaches. In Table 5, we report the results measured across the three benchmarks. The findings clearly demonstrate that reasoning training significantly benefits reward model performance. Under fair comparisons (i.e., training on exactly the same amount of data), reasoning-based models consistently outperform their SFT-only counterparts. In particular, even high-quality distillation alone, applied to small subset of the data, provides notable gains, highlighting the value of incorporating structured intermediate reasoning. Takeaway 3: Reasoning training substantially improves reward modeling. It not only enables better generalization across tasks but also provides consistent gains even under limited data scenarios compared to direct-answer SFT approaches. 5.4 Training Dynamics We analyze the training dynamics of RM-R1 using the Qwen-2.5-14B-Instruct model by tracking both response length and reward progression throughout RL training. We consider two settings: (a) 14 (a) Cold Start RL (b) Warm Start RL Figure 6: RL training dynamics under different settings: (a) Cold Start RL (Eq. 9) and (b) Warm Start RL (Eq. 6). In Cold Start RL, the response length steadily increases as the model learns to reason, but training becomes unstable near the end. In Warm Start RL, the model exhibits more stable training, with effective refinement of reasoning traces throughout the process. Cold Start RL, and (b) Warm Start RL following reasoning-chain distillation. We present the finding in Figure 6. In the Cold Start RL setting, we observe that the model gradually learns to reason, as reflected by steady increase in response length over the course of training. However, training becomes unstable near the end, with sharp drop in the reward curve, suggesting potential issues such as overfitting. In contrast, under Warm Start RL, the model begins with stronger initial reasoning abilities, exhibiting longer responses from the outset. Interestingly, the model first learns to produce more concise reasoning traces before gradually increasing response length again as training progresses. The reward curve rises smoothly and consistently throughout training, demonstrating more stable and efficient learning compared to the Cold Start setting. 5.5 Case Study To gain deeper insights into RM-R1, we conduct case study using Qwen-14B-Instruct, comparing its behavior with cold-start RL model. The results, summarized in Table 6, reveal the following key observations: Transparent Judging Process. The reasoning traces generated by RM-R1 are highly interpretable and coherent, reflecting the models perception of human preferences. It explicitly articulates why certain responses are better, providing transparency into its evaluation process. High-Quality, Question-Dependent Rubrics. RM-R1 accurately understands the question and the context of comparison, correctly prioritizing accuracy as the most critical rubric for medical-related questions. In contrast, the cold-start RL model often overlooks the most important factors and instead emphasizes superficial or broadly defined features (e.g., relevance) that are less discriminative. The ability to generate high-quality, question-specific rubrics stems from the knowledge acquired during the distillation stage. Faithful Adherence to Rubrics and Content-Based Judgement. RM-R1 grounds its evaluation in the actual content of the chatbot responses. For example, it correctly identifies inaccuracies in chatbot As response based on factual content rather than surface presentation. Furthermore, it systematically evaluates different aspects of the rubric, leading to structured, interpretable, and verifiable judging process."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "In this paper, we revisited reward modeling through the lens of reasoning. We introduced RM-R1, family of REASRMS that effectively generate explicit chains of rubrics and rationales, and scale with both model size and inference compute. Across three public benchmarks, RM-R1 matched or surpassed commercial and open-source RMS while producing more interpretable judgments. Ablation investigations reveal that (1) task-type classification, (2) bootstrapping from high-quality reasoning traces, and (3) RL fine-tuning are all indispensable. Qualitative analyses further showed 15 Question: symptoms of sickle-cell disease Chatbot A: 1. Painful red or yellow skin lesions; 2. Frequent infections; 3. Fatigue and weakness; 4. Anemia; 5. Slower growth and development; 6. Jaundice; 7. Swelling of hands and feet; 8. Vision loss; 9. Tiredness and weakness after physical activity; 10. Breathlessness; 11. Fever, chills, and night sweats. Chatbot Sickle cell disease is genetic disorder that affects the shape of red blood cells. It is most common in African Americans, but can also affect people from other ethnic groups. The symptoms of sickle cell disease can vary widely from person to person, but some common symptoms include: 1. Fatigue: People with sickle cell disease often experience fatigue and weakness, even when they have not been physically active.; 2. Pain: Sickle cell disease can cause pain in the body, particularly in the arms, legs, back, and abdomen. Pain can be mild or severe and can last for days or weeks.; 3. Fever: People with sickle cell disease may be prone to infections and may experience frequent fevers.; 4. Anemia: Sickle cell disease can cause anemia, condition in which the body does not have enough red blood cells to carry oxygen throughout the body.; 5. Delayed growth and development: Children with sickle cell disease may be smaller than their peers due to delayed growth and development.; 6. Infections: People with sickle cell disease are at increased risk for infections, particularly in the bloodstream and urinary tract.; 7. Pigmentation: Sickle cell disease can cause dark areas of pigmentation in the body, particularly on the hands and feet. Its important to note that not everyone with sickle cell disease will experience all of these symptoms, and some people may not experience any symptoms at all. Therefore, its important to consult with healthcare provider if you suspect you or someone you know may have sickle cell disease. Cold-start RL evaluation <type>Chat</type> <rubric> Relevance 40 % Comprehensiveness 30 % Clarity/organisation 30 % </rubric> <eval> Chatbot provided list of symptoms that are directly relevant to sickle cell disease. The response is comprehensive and includes wide range of symptoms commonly associated with the condition. The symptoms are listed in clear and organized manner, making it easy for the client to understand the information. Chatbot Bs response is less organized than Chatbot As response, and the explanation of sickle cell disease may be unnecessary for the clients question.</eval> <answer>[[A]]</answer> RM-R1 evaluation <rubric> 1 Accuracy 40 % 2 Comprehensiveness 30 % 3 Clarity 20 % 4 Helpfulness 10 % <justify> Accuracy is weighted highest because providing correct information about medical conditions is crucial. Comprehensiveness is important because ...</justify> </rubric> <eval> Accuracy of information: Chatbot provides list of symptoms without explanation, but some listed items (e.g. painful red or yellow skin lesions, vision loss) are not typical of sickle-cell disease. Chatbot offers more accurate information, explaining each symptoms nature and severity. ... Comprehensiveness:...Clarity and organization:...Helpfulness:... </eval> <answer>[[B]]</answer> Table 6: Case Study of RM-R1 (succinct version). Complete model responses are shown in Table 7 and Table 8. Cold-Start RL evaluation tends to focus on superficial features (e.g., number of suggestions, relevance), whereas RM-R1 produces high-quality rubrics (e.g., accuracy), faithfully follows them, and evaluates responses based on their actual content. that RM-R1 learns to prioritize high-impact rubrics, faithfully follow its own criteria and justify coherently. Future work includes automatic rubric induction, where an explicit rubric library can be reused and composed, so that rollout length could be reduced; Another direction is active preference collection, where REASRMS use active learning to query human preference only when the current rubric set is insufficient for new preference sample. Finally, it would be natural to extend our study to multimodal/agentic reward modeling scenarios."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. [3] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1:1, 2024. [4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [5] Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. Judgelrm: Large reasoning models as judge. arXiv preprint arXiv:2504.00050, 2025. [6] Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, and Wei Wang. Minprompt: Graph-based minimal prompt data augmentation for few-shot question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 254266, 2024. [7] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [8] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024, 2024. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. Bridging language and items for retrieval and recommendation. arXiv preprint arXiv:2403.03952, 2024. [13] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [16] Martin Klissarov, Devon Hjelm, Alexander Toshev, and Bogdan Mazoure. On the modeling capabilities of large language models for sequential decision making. arXiv preprint arXiv:2410.05656, 2024. [17] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Stepdpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. [18] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. [19] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. [20] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [21] Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. [22] Ollie Liu, Deqing Fu, Dani Yogatama, and Willie Neiswanger. Dellma: Decision making under uncertainty with large language models. arXiv preprint arXiv:2402.02392, 2024. [23] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024. [24] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [26] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [27] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [28] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [29] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [32] Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander Alemi, and Andrew Wilson. Does knowledge distillation really work? Advances in neural information processing systems, 34:69066919, 2021. [33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. 18 [34] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1058210592, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-emnlp. 620. [35] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [36] Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators. arXiv preprint arXiv:2408.02666, 2024. [37] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. HelpSteer: Multi-attribute helpfulness dataset for SteerLM. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 33713384, Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.naacl-long.185. [38] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [39] Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, et al. Self-generated critiques boost reward modeling for language models. arXiv preprint arXiv:2411.16646, 2024. [40] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao. [42] Jialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei Zhou, Jinjie Gu, and Lei Zou. comprehensive survey of reward models: Taxonomy, applications, challenges, and future. arXiv preprint arXiv:2504.12328, 2025. [43] Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, et al. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893, 2024. [44] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness & harmlessness with rlaif, November 2023. [45] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 19 Chain -of -Rubrics (CoR) Roll-out for Instruct Models (no categorization of task types) Please act as an impartial judge and evaluate the quality of the responses provided by two AI Chatbots to the Clients question displayed below. Instructions 1. Begin your evaluation by generating the rubric criteria tailored to the Clients question and context. Enclose the rubric in <rubric> . . . </rubric> tags. 2. Assign weights to each rubric item based on their relative importance. 3. Within <rubric>, include <justify> . . . </justify> section explaining the rationale behind the chosen criteria and weights. 4. Compare both Chatbot responses using the rubric. 5. Include your evaluation in <eval> . . . </eval> tags. Support your analysis using: - <quote_A> . . . </quote_A> for direct quotes from Chatbot - <summary_A> . . . </summary_A> for paraphrased summaries of Chatbot - <quote_B> . . . </quote_B> for direct quotes from Chatbot - <summary_B> . . . </summary_B> for paraphrased summaries of Chatbot 6. Conclude with your final judgment using: <answer>[[A]]</answer> or <answer>[[B]]</answer> Important Notes: - Be objective and base your evaluation strictly on the content of the responses. - Do not let the response order, length, or Chatbot names bias your judgment. Figure 7: The system prompt of the ablation study on cold start RL without categorization of task types. Supplementary Information for Section 20 Cold-start RL evaluation <type>Chat</type> <rubric> The evaluation criteria for the responses are as follows: - <item>Relevance to the question</item> (40%): The response should directly address the symptoms of sickle cell disease. - <item>Comprehensiveness</item> (30%): The response should include wide range of symptoms commonly associated with sickle cell disease. - <item>Clarity and organization</item> (30%): The response should be easy to read and understand, with symptoms listed in clear and organized manner. <justify> The question specifically asks for the symptoms of sickle cell disease, so the responses should be directly relevant to this topic. Comprehensiveness is important because sickle cell disease can have variety of symptoms, and comprehensive list will provide more complete picture of the condition. Clarity and organization are important because the symptoms can be complex, and well-organized response will make it easier for the client to understand the information. </justify> </rubric> <eval> <quote_A> 1. Painful red or yellow skin lesions 2. Frequent infections 3. Fatigue and weakness 4. Anemia 5. Slower growth and development 6. Jaundice 7. Swelling of hands and feet 8. Vision loss 9. Tiredness and weakness after physical activity 10. Breathlessness 11. Fever, chills, and night sweats. </quote_A> <summary_A> Chatbot provided list of symptoms that are directly relevant to sickle cell disease. The response is comprehensive and includes wide range of symptoms commonly associated with the condition. The symptoms are listed in clear and organized manner, making it easy for the client to understand the information. </summary_A> <quote_B> Sickle cell disease is genetic disorder that affects the shape of red blood cells. It is most common in African Americans, but can also affect people from other ethnic groups. The symptoms of sickle cell disease can vary widely from person to person, but some common symptoms include: 1. Fatigue: People with sickle cell disease often experience fatigue and weakness, even when they have not been physically active. 2. Pain: Sickle cell disease can cause pain in the body, particularly in the arms, legs, back, and abdomen. Pain can be mild or severe and can last for days or weeks. 3. Fever: People with sickle cell disease may be prone to infections and may experience frequent fevers. 4. Anemia: Sickle cell disease can cause anemia, condition in which the body does not have enough red blood cells to carry oxygen throughout the body. 5. Delayed growth and development: Children with sickle cell disease may be smaller than their peers due to delayed growth and development. 6. Infections: People with sickle cell disease are at increased risk for infections, particularly in the bloodstream and urinary tract. 7. Pigmentation: Sickle cell disease can cause dark areas of pigmentation in the body, particularly on the hands and feet. </quote_B> <summary_B> Chatbot provided detailed explanation of sickle cell disease and list of symptoms. The response is relevant to the question and includes wide range of symptoms commonly associated with the condition. However, the response is less organized than Chatbot As response, and the explanation of sickle cell disease may be unnecessary for the clients question. </summary_B> </eval> <answer>[[A]]</answer> Table 7: Cold-Start RL Judges. The rubrics and evaluation mainly center around superficial shallow features (e.g., number of points listed) without actually taking their accuracy into account."
        },
        {
            "title": "B Training Dataset Details",
            "content": "A global statistics of our training dataset is summarized in Table 9. 21 Cold-start RL evaluation <type>Chat</type> <rubric> 1. Accuracy of information (40 %): How correctly does the response describe the symptoms of sickle-cell disease? 2. Comprehensiveness (30 %): How thoroughly does the response cover the range of symptoms associated with sickle-cell disease? 3. Clarity and organization (20 %): How well-structured and easy to understand is the information presented? 4. Helpfulness for the user (10 %): How useful is the information for someone seeking to understand sickle-cell disease symptoms? <justify> These criteria focus on evaluating how well each response addresses the users question about sickle-cell disease symptoms. Accuracy is weighted highest because providing correct information about medical conditions is crucial. Comprehensiveness is important, as sickle-cell disease has multiple symptoms that vary in severity and presentation. Clarity and organization affect how easily the user can understand the information. Helpfulness considers how practical and actionable the information is for the user. </justify> </rubric> <eval> Comparing the two responses: Accuracy of information: <quote_A>1. Painful red or yellow skin lesions 2. Frequent infections 3. Fatigue and weakness 4. Anemia 5. Slower growth and development 6. Jaundice 7. Swelling of hands and feet 8. Vision loss 9. Tiredness and weakness after physical activity 10. Breathlessness 11. Fever, chills, and night sweats.</quote_A> <quote_B>1. Fatigue: People with sickle-cell disease often experience fatigue and weakness, even when they have not been physically active. 2. Pain: Sickle-cell disease can cause pain in the body, particularly in the arms, legs, back, and abdomen. Pain can be mild or severe and can last for days or weeks. 3. Fever: People with sickle-cell disease may be prone to infections and may experience frequent fevers. 4. Anemia: Sickle-cell disease can cause anemia, condition in which the body does not have enough red blood cells to carry oxygen throughout the body. 5. Delayed growth and development: Children with sickle-cell disease may be smaller than their peers due to delayed growth and development. 6. Infections: People with sickle-cell disease are at increased risk for infections, particularly in the bloodstream and urinary tract. 7. Pigmentation: Sickle-cell disease can cause dark areas of pigmentation in the body, particularly on the hands and feet.</quote_B> Chatbot provides list of symptoms without explanation, but some listed items (e.g. painful red or yellow skin lesions, vision loss) are not typical of sickle-cell disease. Chatbot offers more accurate information, explaining each symptoms nature and severity. Comprehensiveness: Chatbot lists 11 symptoms, several of which are inaccurate or uncommon. Chatbot lists 7 symptoms that are more representative and supplies contextual detail about variability and medical advice. Clarity and organization: Chatbot As plain numbered list lacks context, making severity unclear. Chatbot structures the list with explanations, improving readability and understanding. Helpfulness: Chatbot is less helpful due to inaccuracies and missing context. Chatbot is more useful, providing accurate descriptions, context, and guidance. </eval> <answer>[[B]]</answer> Table 8: RM-R1 RL Judges. The rubrics and evaluation are well-designed and reasonable, assessing the effectiveness of the model response."
        },
        {
            "title": "C Implementation Details",
            "content": "Our training pipeline consists of two stages: distillation and RLVR (Reinforcement Learning with Verifiable Rewards). Distillation Stage. We use the SFTTrainer from OpenRLHF with fixed batch size of 128 and micro-batch size of 1, training for single epoch. To optimize GPU memory usage, we enable gradient checkpointing, FlashAttention, and Adam offloading. The learning rates are set based on model size: 5e6, 3e6, 2e6 for models of size 7B, 14B, 32B respectively. 22 Source Size Domain magpile_pro_llama3.1* offset_bias* helpsteer2* wildguard* magpile_pro* Code-Preference-Pairs Math-DPO-10K"
        },
        {
            "title": "29682 Reasoning\nChat (length bias)\n8504\nChat\n7221\nSafety\n6709\nChat\n2030\n8000\nReasoning\n10000 Reasoning",
            "content": "Table 9: Global Statistics of our Training Dataset. * indicates the source is from Skywork-RewardPreference-80K-v0.2. RLVR Stage. We use the VERL framework for all GRPO training. The training batch size is fixed at 1024, with mini-batch size of 128. We adopt Fully Sharded Data Parallel (FSDP) to improve memory efficiency. For rollout generation, we use vLLM with tensor parallelism size 4 and GPU memory utilization capped at 0.4. Sampling follows default parameters (temperature = 1.0, top-p = 1.0). KL regularization is applied with coefficient of 1e3 and clip ratio of 0.2. Each prompt is sampled with 7 candidate responses. The maximum input sequence length is 4,096 tokens, and the maximum response length is 8,192 tokens. Learning rates are set separately for the two model variants: Instruct models: 1e6, 7e7, 5e7 for 7B, 14B, 32B models. Reasoning models: 1e6, 1e6, 8e7 for 7B, 14B, 32B models. We train the 7B, 14B, 32B models on 1, 2, 4 nodes respectively, each equipped with 8 H100 GPUs."
        }
    ],
    "affiliations": [
        "Stevens Institute of Technology",
        "Texas A&M University",
        "University of California, San Diego",
        "University of Illinois Urbana-Champaign"
    ]
}