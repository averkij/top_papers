{
    "paper_title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
    "authors": [
        "Xuekai Zhu",
        "Daixuan Cheng",
        "Dinghuai Zhang",
        "Hengli Li",
        "Kaiyan Zhang",
        "Che Jiang",
        "Youbang Sun",
        "Ermo Hua",
        "Yuxin Zuo",
        "Xingtai Lv",
        "Qizheng Zhang",
        "Lin Chen",
        "Fanghao Shao",
        "Bo Xue",
        "Yunchong Song",
        "Zhenjie Yang",
        "Ganqu Cui",
        "Ning Ding",
        "Jianfeng Gao",
        "Xiaodong Liu",
        "Bowen Zhou",
        "Hongyuan Mei",
        "Zhouhan Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning."
        },
        {
            "title": "Start",
            "content": "FlowRL: Matching Reward Distributions for LLM Reasoning 2025-09-17 FlowRL: Matching Reward Distributions for LLM Reasoning Xuekai Zhu1, Daixuan Cheng6, Dinghuai Zhang3, Hengli Li5, Kaiyan Zhang4, Che Jiang4, Youbang Sun4, Ermo Hua4, Yuxin Zuo4, Xingtai Lv4, Qizheng Zhang7, Lin Chen1, Fanghao Shao1, Bo Xue1, Yunchong Song1, Zhenjie Yang1, Ganqu Cui2, Ning Ding4,2, Jianfeng Gao3, Xiaodong Liu3, Bowen Zhou4,2, Hongyuan Mei8, Zhouhan Lin1,2 1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 6 Renmin University of China # hongyuanmei@gmail.com # xuekaizhu0@gmail.com FlowRL 8 Toyota Technological Institute at Chicago Corresponding Authors. 4 Tsinghua University 7 Stanford University 3 Microsoft Research 5 Peking University Abstract We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (e.g., PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into normalized target distribution using learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as key step toward efficient exploration and diverse reasoning in LLM reinforcement learning. 5 2 0 S 8 1 ] . [ 1 7 0 2 5 1 . 9 0 5 2 : r Figure 1 Top: Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on single high-reward peak, leading to mode collapse and higher KL divergence. Bottom: Performance comparison. FlowRL consistently outperforms GRPO across math and code domains. FlowRL: Matching Reward Distributions for LLM Reasoning 1. Introduction Reinforcement learning (RL) plays crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share fundamental limitation in their reward-maximizing objective. Reward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training? In this paper, we propose FlowRL, policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce learnable partition function that normalizes scalar rewards into target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy. We compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10.0% and 5.1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approachs effectiveness in exploring multiple solution strategies. Contributions. We summarize the key contributions of this work as follows: We propose FlowRL, policy optimization algorithm that shifts from reward maximization to 2 FlowRL: Matching Reward Distributions for LLM Reasoning reward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods. We introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues. FlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration. 2. Preliminaries Reinforcement Learning for Reasoning. We formulate reasoning as conditional generation problem, where the policy model receives question and generates an answer Y. The objective is to learn policy 𝜋𝜃(yx) that produces high-quality answers under task-specific reward signals 𝑟. To better illustrate the policy optimization procedure, we provide detailed formulation of GRPO below. For each question x, GRPO samples group of answers {y1, y2, . . . , y𝐺} from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective: J𝐺𝑅𝑃𝑂(𝜃) = 𝔼[x𝑃 ( X),{y𝑖 }𝐺 𝑖=1𝜋𝜃𝑜𝑙𝑑 ( x) ] 𝐺 (cid:26) y𝑖 1 𝐺 1 y𝑖 𝑖=1 𝑡=1 𝔻KL(𝜋𝜃𝜋ref) = 𝜋ref (y𝑖x) 𝜋𝜃(y𝑖x) log 𝜋ref (y𝑖x) 𝜋𝜃(y𝑖x) 1, min (cid:20) 𝜋𝜃(y𝑖,𝑡 x, y𝑖,<𝑡) 𝜋𝜃𝑜𝑙𝑑 (y𝑖,𝑡 x, y𝑖,<𝑡) ˆ𝐴𝑖,𝑡, clip (cid:18) 𝜋𝜃(y𝑖,𝑡 x, y𝑖,<𝑡) 𝜋𝜃𝑜𝑙𝑑 (y𝑖,𝑡 x, y𝑖,<𝑡) , 1 𝜖, 1 + 𝜖 (cid:19) (cid:21) ˆ𝐴𝑖,𝑡 𝜆𝔻𝐾𝐿 (cid:2)𝜋𝜃𝜋𝑟𝑒 𝑓 (cid:3) (cid:27) , (1) where 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values {𝑟1, 𝑟2, . . . , 𝑟𝐺} as 𝐴𝑖 = 𝑟𝑖 mean( {𝑟1,𝑟2, ,𝑟𝐺 } ) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses critic model to estimate the advantage and employs importance sampling to stabilize policy updates. std( {𝑟1,𝑟2, ,𝑟𝐺 } ) GFlowNets. Generative Flow Networks [Bengio et al., 2023a] are probabilistic framework for training stochastic policies to sample discrete, compositional objects (e.g., graphs, sequences) in proportion to given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙(𝑠0) at the initial state 𝑠0. The output flow is equal to the outcome reward 𝑟(𝑠𝑛) conditioned at the final state 𝑠𝑛. Following Lee et al. [2024], we use 3-layer MLP to parameterize 𝑍𝜙. This flow-balancing mechanism facilitates the discovery of diverse, high-reward solutions by ensuring proper exploration of the solution space. See Appendix for detailed GFlowNets background. Figure 2 GFlowNets [Bengio et al., 2023a], flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙(𝑠0) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards. 3 FlowRL: Matching Reward Distributions for LLM Reasoning 3. Methodology In this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive flow-balanced objective, termed FlowRL. 3.1. From Reward Maximization to Distribution Matching As illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to target reward distribution. simple yet effective way to achieve this is to minimize the reverse KL divergence1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is scalar reward, rather than full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable. Inspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce learnable partition function 𝑍𝜙(x) to normalize scalar rewards into valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as: min 𝜃 DKL (cid:18) 𝜋𝜃(y x) (cid:13) (cid:13) (cid:13) (cid:13) exp( 𝛽𝑟(x, y)) 𝑍𝜙(x) (cid:19) 𝜋𝜃(y x) exp( 𝛽𝑟(x, y)), (2) where 𝑟(x, y) is the reward function, 𝛽 is hyperparameter, 𝑍𝜙(x) is the learned partition function, and the resulting target distribution is defined as 𝜋(y x) = . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization. exp( 𝛽𝑟 (x,y) ) 𝑍𝜙 (x) While the KL-based formulation provides principled target distribution, we derive more practical, RL-style objective that facilitates efficient policy optimization. Proposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]: min 𝜃 DKL (cid:18) 𝜋𝜃(y x) (cid:13) (cid:13) (cid:13) (cid:13) exp( 𝛽𝑟(x, y)) 𝑍𝜙(x) (cid:19) min 𝜃 (cid:0)log 𝑍𝜙(x) + log 𝜋𝜃(y x) 𝛽𝑟(x, y)(cid:1) 2 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) Trajectory Balance (3) Remark 2 (Trajectory balance as practical surrogate for KL minimization). Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides practical optimization approach by using stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙(x) as learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks. 1We use reverse KL since we can only sample from the policy model, not the target reward distribution. FlowRL: Matching Reward Distributions for LLM Reasoning 3.2. FlowRL As established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges: Problem I: Exploding gradients from long trajectories. Trajectory balance is sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃(y x) decomposes into token-wise sum, (cid:205)𝑡 log 𝜋𝜃(y𝑡 y<𝑡, x), causing the gradient norm to potentially scale with sequence length. Problem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃old , enabling In contrast, the KL-based trajectory balance objective assumes fully ondata-efficient training. policy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines. These limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating reference model as prior constraint on the reward distribution. Specifically, we modify the original exp( 𝛽𝑟(x, y)) to include the reference model: exp ( 𝛽 𝑟(x, y)) 𝜋ref (y x), where 𝑟(x, y) denotes the outcome reward commonly used in reinforcement learning and 𝜋ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟(x, y) as ˆ𝑟𝑖 = (𝑟𝑖 mean(r))/std(r), where = {𝑟1, 𝑟2, . . . , 𝑟𝐺} denotes the set of rewards within sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective2: (4) (cid:0)log 𝑍𝜙(x) + log 𝜋𝜃(y x) 𝛽 ˆ𝑟𝑖 (x, y) log 𝜋ref (y x)(cid:1) 2 min 𝜃 (5) Remark 3 (Reward shaping via length normalization). Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths (e.g., CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃(y x) = (cid:205)y log 𝜋𝜃( 𝑦𝑡 𝑦<𝑡, x) to scale with sequence length. To address 𝑡=1 this, we apply form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 log 𝜋𝜃(y x), balancing the contributions of long and short sequences and stabilizing the learning signal. Remark 4 (Importance sampling for data-efficient training). To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃(y x)/𝜋old(y x), which serves as coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach[𝜋𝜃(y x)]/𝜋old(y x). For additional stability, we incorporate PPO-style clipping to bound the importance weights: 𝑤 = clip (cid:16) 𝜋𝜃 (yx) , 1 𝜖, 1 + 𝜖 (cid:17) detach . 𝜋old (yx) 2The substitution replaces 𝛽𝑟(x, y) in trajectory balance objective Eq. 3 with 𝛽𝑟(x, y) + log 𝜋ref (y x) to incorporate the reference model constraint. 5 FlowRL: Matching Reward Distributions for LLM Reasoning Incorporating these improvements into Eq. 5, we arrive at the following FlowRL objective: FlowRL LFlowRL = 𝑤 (cid:18) log 𝑍𝜙(x) + 1 log 𝜋𝜃(y x) 𝛽ˆ𝑟(x, y) log 𝜋ref (y x) (cid:19) 1 (6) where the clipped importance weight 𝑤 and normalized reward ˆ𝑟(x, y) are defined as: 𝑤 = clip( 𝜋𝜃(y x) 𝜋old(y x) , 1 𝜖, 1 + 𝜖)detach, ˆ𝑟𝑖 = 𝑟𝑖 mean(r) std(r) . (7) We use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL. Implementation details and theoretical analysis are provided in 5 and B, respectively. 4. Related Work 4.1. Reinforcement Learning for Reasoning Reinforcement learning has emerged as powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is classical technique for mitigating mode collapse by promoting diversity in the policys output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation. 4.2. GFlowNets GFlowNets [Bengio et al., 2023a] represent class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving maximum entropy reinforcement learning problem within modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have 6 FlowRL: Matching Reward Distributions for LLM Reasoning inspired us to enhance reinforcement learning from more foundational standpoint using GFlowNets principles. comprehensive overview of GFlowNets theory can be found in Appendix C. 4.3. Flow-Matching Policies Flow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration. 5. Experiment Settings Backbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙. For the policy model 𝜋𝜃, we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙, following Lee et al. [2024], we use randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15. Baselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts. Training Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size. Evaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we 7 FlowRL: Matching Reward Distributions for LLM Reasoning AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Avg Backbone 4.6 R++ PPO GRPO FlowRL 14.8+10.2 26.9+22.3 23.1+18.5 24.0+19.4 Qwen2.5-32B-Base, Max Response Len=8K 2.1 9.2+7.1 20.4+18.3 14.6+12.5 21.9+19.8 27.0 17.49.6 28.8+1.8 19.08.0 38.2+11.2 Qwen2.5-7B-Base, Max Response Len=8K 52.5 44.48.1 69.2+16.7 61.6+9.1 80.8+28.3 28.6 52.7+24.1 76.4+47.8 76.9+48.3 73.8+45.2 Backbone 4.4 R++ PPO GRPO FlowRL 11.0+6.6 9.4+5.0 13.5+9.1 15.4+11.0 2.1 5.4+3.3 7.3+5.2 9.8+7.7 10.8+8. 30.8 66.7+35.9 63.4+32.6 64.5+33.7 54.5+23.7 54.5 54.30.2 58.0+3.5 57.1+2.6 67.0+12.5 22.4 24.4+2.0 26.5+4.1 23.1+0.7 31.4+9.0 21.4 24.5+3.1 37.9+16.5 34.9+13.5 51.8+30.4 24.0 27.3+3.3 27.3+3.3 26.9+2.9 34.6+10.6 22.7 27.1 43.3 38. 48.4 23.0 31.5 32.0 32.5 35.6 Table 1 Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales. Models LiveCodeBench CodeForces HumanEval+ Avg@16 Pass@16 Rating Percentile Avg@16 Backbone R++ PPO GRPO DeepSeek-R1-Distill-Qwen-7B, Max Response Len=8K 19.4 886.7 56.8+37.4 1208.0+321.3 73.7+54.3 1403.1+516.4 1313.8+427.1 67.1+47.7 1549.5+662.8 83.3+63. 49.5 52.7+3.2 54.5+5.0 52.3+2.8 56.3+6.8 30.7 30.50.2 35.1+4.4 32.8+2.1 37.4+6.7 80.9 76.64.3 82.3+1.4 80.10.8 83.3+2.4 FlowRL Table 2 Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks. use sampling parameters of temperature = 0.6 and top_p = 0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration. 6. Results 6.1. Main Results Our experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains 8 FlowRL: Matching Reward Distributions for LLM Reasoning Method FlowRL w/o IS Zhang et al. [2025a] AIME 2024 AIME 2025 AMC 2023 MATH-500 Minerva Olympiad Avg 15.41 6. 10.41 10.83 7.91 6.66 54.53 41.40 53.75 66.96 56. 66.50 31.41 22.19 30.97 34.61 25.52 33.72 35.63 26. 33.67 Table 3 Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling. across diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRLs flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches. 6.2. Ablation Studies We conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparameter 𝛽, we conduct series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7. Figure 3 Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance. 7. Analysis 7.1. Diversity Analysis To assess solution diversity, we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals 9 FlowRL: Matching Reward Distributions for LLM Reasoning Table 4 Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM 3, identity loops 2), while FlowRL follows more diverse solution path. Question GRPO Content (boxed = actions; 𝑘 = repeated; . . . = omitted) Let be the set of rectangular boxes with surface area 54 and volume 23. Let 𝑟 be the radius of the smallest sphere that can contain each box in B. If 𝑟2 = 𝑝 with 𝑞 gcd( 𝑝, 𝑞) = 1, find 𝑝 + 𝑞. . . . denote 𝑎, 𝑏, 𝑐 . . . 2(𝑎𝑏+𝑏𝑐+𝑐𝑎) = 54, 𝑎𝑏𝑐 = 23 . . . 𝑑 = . . . 𝑎2 + 𝑏2 + 𝑐2, 𝑟 = 𝑑/2 (𝑎+𝑏+𝑐)2 = 𝑎2+𝑏2+𝑐2 + 2(𝑎𝑏+𝑏𝑐+𝑐𝑎) . . . AMGM 3: AMGM (1) . . . AMGM (2) . . . . . . 𝑎 = 𝑏 = 𝑐 (contradiction) . . . back to (𝑎+𝑏+𝑐)2 . . . no factorization . . . (𝑎+𝑏+𝑐)3 identity loop 2: AMGM (3) loop (1) loop (2) . . . . . . FlowRL . . . . . . let 𝑎, 𝑏, 𝑐 with 2(𝑎𝑏+𝑏𝑐+𝑐𝑎) = 54, 𝑎𝑏𝑐 = 23 . . . (𝑎+𝑏+𝑐)2 𝑎2+𝑏2+𝑐2 = 𝑠2 54 . . . 𝑎 = 𝑏 . . . 𝑑 = rational root 𝑎 = 2 . . . back-sub 𝑐 = 23/𝑎2 . . . 𝑎2+𝑏2+𝑐2 = 657 factor (𝑎 2)(𝑎2 + 2𝑎 23) . . . branch 𝑎 = 1 + 2 64 . . . Answer 721 . . . 16 . . . 𝑟2 = 657 𝑎2 + 𝑏2 + 𝑐2, 𝑟 = 𝑑/2 𝑎3 27𝑎 + 46 = 0 . . . 6 . . . significant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks. 7.2. Case Study Table 4 illustrates the behavioral differences between GRPO and FlowRL on representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏, derives cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPOs reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐. In contrast, FlowRLs distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏, which transforms the problem into tractable cubic equation 𝑎3 27𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization. Figure 4 GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO. FlowRL: Matching Reward Distributions for LLM Reasoning 8. Conclusion In this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns."
        },
        {
            "title": "Acknowledgments",
            "content": "We are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work."
        },
        {
            "title": "References",
            "content": "Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning, pages 151160. PMLR, 2019. Brian Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929, 2025. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS), 2021. Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research, 24(210):155, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html. Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research, 24(1):1000610060, 2023b. Chen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles 11 FlowRL: Matching Reward Distributions for LLM Reasoning Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Miruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155, 2024. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309, 2024. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems, 32, 2019. Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257, 2021. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pages 18611870. Pmlr, 2018. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Haoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=fNMKqyvuZT. Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and Neal. The wake-sleep algorithm for unsupervised neural networks. Science, 268 5214:115861, 1995. Edward Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363, 2023. 12 FlowRL: Matching Reward Distributions for LLM Reasoning Edward Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60. Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501, 3262:3233, 2025. Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML), 2022. Moksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv, abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319. Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML), 2023b. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/. Google Blog (The Keyword), Published Mar. 25, 2025. Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv, abs/2310.02710, 2023. Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024. Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 38433857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023a. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023b. FlowRL: Matching Reward Distributions for LLM Reasoning Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning, 2022. Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507, 2025a. Zhen Liu, Tim Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR, 2025b. Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. Jiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets. MAA. American mathematics competitions - amc. https://maa.org/, 2023. MAA. American invitational mathematics examination - aime. https://maa.org/, 2025. Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning, pages 2346723483. PMLR, 2023. Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems, 35: 59555967, 2022. Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR), 2023. David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053, 2025. Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics, pages 25932601. PMLR, 2024. OpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/, 2024. Accessed: 2024. Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544, 2022. Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a. 14 FlowRL: Matching Reward Distributions for LLM Reasoning Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML), 2023b. Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR), 2023c. Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI), 2023d. Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=KVf2SFL1pi. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces, 2025. Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073, 2025. Abhinav Rastogi, Albert Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv, abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Richard Sutton, Andrew Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience, 11(1):126134, 1999a. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/. FlowRL: Matching Reward Distributions for LLM Reasoning Daniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics, pages 42134221. PMLR, 2024. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025b. Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2362523635, 2025. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. David W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv, abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133. Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2, 2022a. Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML), 2022b. Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv, abs/2305.17010, 2023b. Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a. Dinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b. Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research, 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42. 16 FlowRL: Matching Reward Distributions for LLM Reasoning Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025b. Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024. Heiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. variational perspective on generative flow networks. ArXiv, abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672. FlowRL: Matching Reward Distributions for LLM Reasoning A. Proof of Proposition 1 We begin by analyzing the gradient of the KullbackLeibler (KL) divergence between the policy 𝜋𝜃(y x) and the target reward distribution exp( 𝛽𝑟 (x,y) ) : 𝑍𝜙 (x) (cid:18) 𝜃 𝐷KL = 𝜃 𝜋𝜃(y x) 𝜋𝜃(y x) log 𝜃𝜋𝜃(y x) log (cid:19) exp( 𝛽𝑟(x, y)) 𝑍𝜙(x) (cid:20) 𝜋𝜃(y x) 𝑍𝜙(x) exp( 𝛽𝑟(x, y)) (cid:20) 𝑍𝜙(x)𝜋𝜃(y x) (cid:21) exp( 𝛽𝑟(x, y)) (cid:21) 𝑑y 𝑑y + 𝜋𝜃(y x)𝜃 log (cid:20) 𝑍𝜙(x)𝜋𝜃(y x) exp( 𝛽𝑟(x, y)) (cid:21) 𝑑y 𝜋𝜃(y x) 𝜃 log 𝜋𝜃(y x) log (cid:20) 𝑍𝜙(x)𝜋𝜃(y x) exp( 𝛽𝑟(x, y)) (cid:21) 𝑑y + 𝜋𝜃(y x) 𝜃 log 𝜋𝜃(y x) 𝑑y (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) =𝜃 𝜋𝜃 (yx) 𝑑y=𝜃1=0 (cid:125) (cid:124) (8) = = = 𝜋𝜃(y x) 𝜃 log 𝜋𝜃(y x) log = 𝔼y𝜋𝜃 ( x) (cid:20) log (cid:18) 𝑍𝜙(x)𝜋𝜃(y x) exp( 𝛽𝑟(x, y)) (cid:19) (cid:20) 𝑍𝜙(x)𝜋𝜃(y x) exp( 𝛽𝑟(x, y)) (cid:21) 𝑑y 𝜃 log 𝜋𝜃(y x) (cid:21) Next, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as: (y, x; 𝜃) = (cid:18) log 𝑍𝜙(x) 𝜋𝜃(y x) exp( 𝛽𝑟(x, y)) (cid:19) 2 . Taking the gradient of this objective with respect to 𝜃 yields: 𝜃L (𝜃) = 2 𝔼y𝜋𝜃 ( x) (cid:20)(cid:18) log 𝑍𝜙(x) 𝜋𝜃(y x) exp( 𝛽𝑟(x, y)) (cid:19) 𝜃 log 𝜋𝜃(y x) (cid:21) (9) (10) Thus, minimizing the KL divergence is equivalent (up to constant) to minimizing the trajectory balance loss, confirming Proposition 1. B. Theoretical Analysis We conduct an interpretation of FlowRL that clarifies the role of each component in the objective. Proposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy: max 𝜃 𝔼y𝜋𝜃 𝛽 𝑟(x, y) (cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) reward log 𝑍𝜙(x) + log 𝜋ref (yx) . + (𝜋𝜃) (cid:32)(cid:32) (cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) entropy (11) Remark 6 (FlowRL beyond reward maximization). Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024]. 18 FlowRL: Matching Reward Distributions for LLM Reasoning The proof of Proposition 5 is provided as below. Recall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of KL divergence: (cid:18) 𝐷KL 𝜋𝜃(y x) exp( 𝛽 𝑟(x, y)) 𝜋ref (y x) 𝑍𝜙(x) (cid:19) = 𝜋𝜃(y x) log (cid:20) 𝑍𝜙(x)𝜋𝜃(y x) exp ( 𝛽 𝑟(x, y)) 𝜋ref (y x) (cid:21) 𝑑y (12) Rearranging the terms, we obtain: arg min 𝜃 𝐷KL (cid:26) (cid:26) = arg min 𝜃 = arg max 𝜃 = arg max 𝜃 (cid:18) 𝜋𝜃(y x) exp ( 𝛽 𝑟(x, y)) 𝜋ref (y x) 𝑍𝜙(x) (cid:19) 𝜋𝜃(y x) log 𝔼y𝜋𝜃 ( x) log 𝔼y𝜋𝜃 ( x) log (cid:20) 𝑍𝜙(x)𝜋𝜃(y x) exp ( 𝛽 𝑟(x, y)) 𝜋ref (y x) (cid:21) (cid:20) exp ( 𝛽 𝑟(x, y)) 𝜋ref (y x) 𝑍𝜙(x) (cid:20) exp ( 𝛽 𝑟(x, y)) 𝜋ref (y x) 𝑍𝜙(x) (cid:21) (cid:21) 𝑑y 𝜋𝜃(y x) log 𝜋𝜃(y x)𝑑y (13) (cid:27) (cid:27) + (𝜋𝜃) Finally, we express the FlowRL objective in its compact form: max 𝜃 𝔼y𝜋𝜃 ( x) 𝛽𝑟(x, y) (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) reward log 𝑍𝜙(x) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) normalization + log 𝜋ref (yx) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) prior alignment . + (𝜋𝜃) (cid:32)(cid:32) (cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) entropy (14) Therefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙(x) ensures consistency with properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to few high-reward modes. The reference policy 𝜋ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term (𝜋𝜃) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL. C. GFlowNets We follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let denote the compositional objects and 𝑅 be reward function that assigns nonnegative values to each object 𝑥 X. GFlowNets aim to learn sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e.,𝜋(𝑥) 𝑅(𝑥). This process can be represented as directed acyclic graph (DAG) = (S, A), where the vertices 𝑠 are referred to as states, and the directed edges (𝑢 𝑣) are called actions. The generation of an object 𝑥 corresponds to complete trajectory 𝜏 = (𝑠0 𝑠𝑛) within the DAG, beginning at the initial state 𝑠0 and ending at terminal state 𝑠𝑛 X. The state flow 𝐹(𝑠) is defined as non-negative weight assigned to each state 𝑠 S. The forward policy 𝑃𝐹 (𝑠 𝑠) specifies the transition probability to child state 𝑠, while the backward policy 𝑃𝐵 (𝑠 𝑠) specifies the transition probability to parent state 𝑠. To this end, detailed balance objective enforces local flow consistency across every edge (𝑠 𝑠) A: (𝑠 𝑠) A, 𝐹𝜃(𝑠)𝑃𝐹 (𝑠 𝑠; 𝜃) = 𝐹𝜃(𝑠)𝑃𝐵 (𝑠 𝑠; 𝜃). (15) 19 FlowRL: Matching Reward Distributions for LLM Reasoning To achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as: 𝑍𝜃 𝑛 (cid:214) 𝑡=1 𝑃𝐹 (𝑠𝑡 𝑠𝑡1; 𝜃) = 𝑅(𝑥) 𝑛 (cid:214) 𝑡=1 𝑃𝐵 (𝑠𝑡1 𝑠𝑡; 𝜃). (16) Furthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖: 𝑗 = {𝑠𝑖 𝑠 𝑗}, offering more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL. Models AIME 2024 AIME 2025 AMC 2023 MATH-500 Minerva Olympiad Avg Backbone 4.37 R++ PPO GRPO 10.57+6.20 9.95+5.58 14.01+9.64 14.32+9.95 FlowRL Qwen2.5-7B Base Model 2.08 5.10+3.02 7.34+5.26 10.73+8.65 10.05+7.97 30.78 66.02+35.24 63.63+32.85 64.10+33.32 55.08+24. 54.48 54.290.19 57.72+3.24 57.41+2.93 66.78+12.30 22.38 24.47+2.09 26.22+3.84 23.17+0.79 31.52+9.14 24.02 27.30+3.28 27.35+3.33 27.11+3.09 34.60+10.58 23.02 31.29 32.03 32.76 35.39 Table 5 Math reasoning performance (Avg@64) at temperature = 0.6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting. Models AIME 2024 AIME 2025 AMC 2023 MATH-500 Minerva Olympiad Avg Qwen2.5-7B Base Model Backbone 3.39 R++ PPO GRPO 10.63+7.24 10.52+7.13 12.50+9.11 14.22+10.83 FlowRL 1.51 4.63+3.12 6.51+5.00 10.10+8.59 9.58+8.07 23.90 66.99+43.09 63.04+39.14 64.72+40.82 52.92+29.02 45.18 54.36+9.18 57.46+12.28 57.15+11.97 66.20+21.02 16.98 23.89+6.91 25.91+8.93 23.28+6.30 30.32+13. 18.27 26.65+8.38 27.16+8.89 26.90+8.63 34.47+16.20 18.20 31.19 31.77 32.44 34.62 Table 6 Math reasoning performance (Avg@64) at temperature = 1.0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average. 20 FlowRL: Matching Reward Distributions for LLM Reasoning Models AIME 2024 AIME 2025 AMC 2023 MATH-500 Minerva Olympiad 𝛽 = 5 𝛽 = 10 𝛽 = 15 𝛽 = 30 20.79 25.27 31.41 30.03 13.54 14.79 15.41 15.00 10.00 10.20 10.83 10.83 28.72 32.39 34.61 35.03 58.91 64.30 66.96 69. 56.09 59.53 54.53 50.62 Avg 31.34 34.41 35.63 35.09 Table 7 Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽. Diversity Evaluation Prompt System: You are evaluating the DIVERSITY of solution approaches for mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies. PROBLEM: {problem} 16 SOLUTION ATTEMPTS: {formatted_responses} EVALUATION CRITERIA - Rate diversity from 1 to 5: Score 1 - Minimal Diversity: 14+ responses use essentially identical approaches Same mathematical setup, same variable choices, same solution path Only trivial differences (arithmetic, notation, wording) Indicates very low exploration/diversity in the generation process Score 2 - Low Diversity: 11-13 responses use the same main approach 1-2 alternative approaches appear but are rare Minor variations within the dominant method (different substitutions, orderings) Some exploration but heavily biased toward one strategy Score 3 - Moderate Diversity: 7-10 responses use the most common approach 2-3 distinct alternative approaches present Noticeable variation in problem setup or mathematical techniques Balanced mix showing reasonable exploration Score 4 - High Diversity: 4-6 responses use the most common approach 3-4 distinct solution strategies well-represented Multiple mathematical techniques and problem framings Strong evidence of diverse exploration strategies Score 5 - Maximum Diversity: No single approach dominates (3 responses use same method) 4+ distinctly different solution strategies Wide variety of mathematical techniques and creative approaches Excellent exploration and generation diversity IMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY number from 1 to 5."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "Peking University",
        "Renmin University of China",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Stanford University",
        "Toyota Technological Institute at Chicago",
        "Tsinghua University"
    ]
}