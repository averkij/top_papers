{
    "paper_title": "Free(): Learning to Forget in Malloc-Only Reasoning Models",
    "authors": [
        "Yilun Zheng",
        "Dongyang Ma",
        "Tian Liang",
        "Jiahao Xu",
        "Xinting Huang",
        "Lihui Chen",
        "Haitao Mi",
        "Yan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state. Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 2 0 3 0 8 0 . 2 0 6 2 : r Free(): Learning to Forget in Malloc-Only Reasoning Models Yilun Zheng1,2,, Dongyang Ma1,, Tian Liang1,, Jiahao Xu1, Xinting Huang1, Lihui Chen2, Haitao Mi1, Yan Wang1,, 1Tencent AI Lab 2Nanyang Technological University Equal contribution, Project Lead yanwang.branden@gmail.com Reasoning models enhance problem-solving by scaling test-time compute, yet they face critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to fundamental architectural flaw: standard LLMs operate as malloc-only engines, continuously accumulating valid and redundant steps alike without mechanism to prune obsolete information. To break this cycle, we propose Free()LM, model that introduces an intrinsic self-forgetting capability via the Free-Module, plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining compact and noise-free state. Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves 3.3% average improvement over top-tier reasoning baselines, even establishing new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think. Date: February 11, (cid:135) Code Models Data"
        },
        {
            "title": "1 Introduction",
            "content": "Reasoning models have unlocked powerful problemsolving skills by scaling test-time computeusing more thinking tokens to reason before answering. However, this approach faces critical bottleneck: performance does not simply improve with longer thoughts. Instead, excessive reasoning often causes accuracy to drop [1] or even leads to severe degeneration (e.g., falling into repetitive loops). Consequently, the utility of test-time scaling is bounded: empirically, if solution is not reached once thinking tokens occupy the majority of the context window (approx. 7090%), further reasoning typically yields no benefit. We validate this bottleneck in Figure 1 (Left) using Qwen3-8B, reasoning model with 32k context window. Across 480 reasoning trajectories sampled Figure 1 Empirical observation of Qwen3-8B reasoning on AIME benchmarks. Left: Standard LLMs passively accumulate tokens, causing the reasoning process to eventually crash (degenerate). Right: Free()LM integrates an intrinsic free() mechanism. By periodically identifying and pruning redundant reasoning steps, it actively maintains compact, noise-free state, enabling sustainable long-chain reasoning. from AIME 24&25 [2, 3], degeneration rises sharply once thinking tokens exceed 16k. Specifically, among the 31 instances reaching the context limit, 26 (84%) were already trapped in repetitive loops, and the degeneration ratio hits 100% by 48k, causing total reasoning collapse. This phenomenon reveals an architectural flaw: standard reasoning models operate as malloc-only engines. They passively accumulate every step without free() mechanism to discard useless information. Consequently, instead of boosting performance, we face paradox: with greater thinking comes not greater power, but greater noise. To break this malloc-only cycle, we introduce Free()LM. Our design stems from simple observation: complex reasoning produces temporary waste, such as intermediate steps or trial paths that become obsolete once resolved. Free()LM integrates an intrinsic free() mechanism to periodically prune these redundancies. Specifically, such self-forget capability is implemented by plugging in an additional LoRA adapter, which we term the Free-Module. By dynamically merging and unmerging this module, Free()LM switches between two modes: (1) Reasoning mode (unmerged), where the model remains equivalent to the original backbone, focusing purely on generating next tokens to solve the problem; and (2) Cleaning mode (merged), where the module activates to scan the context, identify useless segments, and output explicit commands (specifying prefix and suffix) to prune them. By dynamically switching between these two modes, the model continuously maintains compact, noise-free state for sustainable reasoning. We achieve 3.3% average gain across six reasoning benchmarks (e.g., IMOanswerBench [4], HLE [5]) for reasoning models ranging from 8B to 685B. Specifically, Free()LM pushes DeepSeek V3.2-Speciale to new SOTA on IMOanswerBench with 2.3% accuracy boost. More importantly, Free()LM solves the collapse of long-horizon reasoning: while the standard Qwen3-235B drops to 0% accuracy on complex tasks requiring >80k thinking tokens, Free()LM recovers performance to 50%. These results prove our core point: sustainable intelligence requires the freedom to forget as much as the power to think. In summary, our contributions are as follows: We propose Free()LM to break the malloc-only limit of current models. Using plug-and-play LoRA adapter (the Free-Module), Free()LM enables models to prune redundant context and maintain compact, noise-free state. We design an efficient pruning method. By generating only prefixs and suffixs, the model can remove large redundant chunks with minimal cost. We validate Free()LM on six benchmarks using top-tier models from 8B up to 685B. It achieves 3.3% average boost and sets new SOTA on IMOanswerBench with DeepSeek V3.2-Speciale."
        },
        {
            "title": "2 Method",
            "content": "To address the aforementioned malloc-only limitation of standard reasoning models, we propose Free()LM, novel architecture that augments the LLM backbone with lightweight, plug-and-play LoRA adapter, Free-Module. As illustrated in Figure 2, by dynamically merging and unmerging this module, Free()LM switches between two distinct modes: Reasoning (Unmerged): In this mode, the model remains equivalent to the original backbone, focusing exclusively on generating reasoning tokens to solve the problem. Cleaning (Merged): Upon activation, the Free-Module is merged into the backbone. The model shifts its focus to memory management, scanning the context to identify redundancies and outputting pruning commands. In the following subsections, we detail the Free()LM architecture in 2.1 and the training methodology in 2.2."
        },
        {
            "title": "2.1 Architecture & Inference",
            "content": "Structurally, the Free-Module is LoRA adapter [6] attached to the standard LLM backbone. Once merged (Step 2 in Figure 2), Free()LM switches to Cleaning Mode. In this state, the model halts reasoning and scans 2 Figure 2 The Free()LM Inference Framework. The model operates on cyclic Reasoning-Cleaning mechanism. Reasoning: The Main model generates tokens normally with the Free-Module unmerged. Cleaning: Upon reaching chunk limit, the Free-Module is merged to identify and prune redundant chunks. Resumed Reasoning: The module is unmerged, and reasoning resumes on the cleaned context. the existing context to identify redundancies. It outputs structured pruning command in JSON format: [{\"prefix\": \"...\", \"suffix\": \"...\"},...] Here, prefix and suffix serve as unique string anchors defining the span [start, end] to be pruned. This anchor-based design enables the module to efficiently prune very long chunks of redundancy by generating only few command tokens. Pruning & Resumed Inference: Upon receiving the command, an external Python executor parses the JSON and prunes the targeted spans via string matching: context = re.sub(prefix + r.*? + suffix, <Del>, context) Following pruning, the Free()LM unmerges the Free-Module to revert to Reasoning Mode. To resume generation on the cleaned context, we identify two strategies: 1. Re-prefilling. We reuse the KV cache for the unchanged prefix and strictly re-prefill the altered suffix. 2. KV Cache Pruning. We directly excise the deleted memory blocks. To address the positional shift of subsequent tokens, we rotate their cached Key-Value states to re-align with the correct position indices. [7] Our pilot experiments on smaller models indicate that both strategies yield nearly identical performance. However, since Strategy 2 is not natively supported by standard serving frameworks like vLLM [8], we adopt Strategy 1 for our main experiments to ensure compatibility and efficiency. Triggering: We introduce hyperparameter, the Pruning Interval (Lclean). The model simply merges the Free-Module to trigger cleaning cycle for every Lclean reasoning tokens generated."
        },
        {
            "title": "2.2 Training: Learning to Forget",
            "content": "Given the concept of active context management, natural starting point is to explore In-Context Learning (ICL) solutions: asking the model itself to conduct self-correction or employing strong external LLM to identify redundancy. Unfortunately, our preliminary experiments (detailed in Table 1) reveal the severe limitations of this approach. Even with extensive prompt optimization and utilizing powerful models like Gemini-2.5-Pro [9] as experts, the performance gains on Qwen3-8B were marginal ( 1%). This finding 3 Figure 3 The Data Construction Pipeline. (a) Data Synthesis: We segment raw trajectories into 1k-token chunks and employ Gemini-2.5-Pro to sequentially generate candidate training instances. (b) Reward Mechanism: By executing = 8 parallel rollouts, we retain an instance only if the pruned context Cnew maintains or improves accuracy compared to the original (Acc(Cnew) Acc(Craw)). suggests that effectively executing the free() operationdistinguishing necessary historical context from obsolete noiseis complex capability that must be acquired through explicit training. To address the lack of ground-truth labels, we propose data pipeline centered on sophisticated reward mechanism. Specifically, we first synthesize large pool of candidate free() operations via ICL solutions, and subsequently filter for high-quality instances using the reward mechanism. The overall pipeline is illustrated in Figure 3. Data Synthesis We randomly select 1,000 trajectories from DeepMath-103k [10] and synthesize supervision signals using Gemini-2.5-Pro. Crucially, to mimic the run-time behavior, we adopt sequential pruning strategy. We segment each raw trajectory into sequential 1k-token chunks {u1, u2, . . . , un}. The cleaning process is performed iteratively: when the oracle pruns the current chunk uk, it is conditioned on the already cleaned history prefix (composed of previous cleaned chunks) rather than the original raw context. This dependency chain ensures that the training data reflects the fragmented memory states the model will actually encounter during inference. This pipeline yielded 8,000 candidate instances. k1 Reward Mechanism To filter the synthesized candidates, we employ rigorous rejection sampling strategy. We treat the dataset as pairs of {Craw, O}, where Craw is the original context and denotes the candidate pruning command. For each pair, we execute 8 independent reasoning rollouts on both the original context Craw and the cleaned context Cnew = Apply(Craw, O). candidate operation is retained if and only if the pruning preserves or improves the accuracy: Acc(Cnew) Acc(Craw) This strict criterion ensures that the module learns to prune noise without reducing the probability of reaching the correct solution. Consequently, this verification process distilled the dataset down to 6,648 high-quality training instances."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate Free()LM across diverse set of benchmarks to verify its effectiveness, generalization capability, and efficiency. Our experiments cover models ranging from 8B to 685B parameters."
        },
        {
            "title": "3.1 Settings",
            "content": "Backbone Models. We evaluate Free()LM across three model scales: Qwen3-8B (thinking mode)1, Qwen330B-A3B-Thinking-25072, and Qwen3-235B-A22B-Thinking-25073 (hereafter denoted as Qwen3-8B, Qwen330B-A3B, and Qwen3-235B-A22B for brevity). We set the context window to 32k for Qwen3-8B and Qwen3-30B-A3B, as most reasoning paths fit within this limit. For the largest model, Qwen3-235B-A22B, we allocate 128k window for the Vanilla baseline to accommodate its extensive generation length, while restricting Free()LM to 64k, leveraging its ability to actively free up memory during inference. Compared Methods. We compare our Free()LM with three types of baselines: Vanilla: The standard backbone model, Qwen3-8B, Qwen3-30B-A3B, and Qwen3-235B-A22B [11] without any context management or compression techniques. Heuristic Compression: Since KV cache compression naturally results in shorter context, we adapt H2O [12] and ThinkCleary (TC) [13] as heuristic baselines. We apply these methods dynamically during reasoning to prune tokens based on their accumulated attention scores. Note that due to their incompatibility with the vLLM [8] serving framework, we evaluate them exclusively on Qwen3-8B. ICL Methods: the ICL solutions discussed in Section 2.2. We instruct both the backbone models (denoted as No Train) and the SOTA Gemini-2.5-Pro [9] to execute free() operations via sophisticatedly optimized prompt detailed in Appendix A.1. Benchmarks. To comprehensively evaluate Free()LM, we curate diverse benchmark suite designed to stress-test two critical hypotheses: (1) whether the learned active forgetting capability effectively enhances long-horizon reasoning as intended ; and (2) whether the free() mechanism is safe, preserving performance on general tasks where extensive pruning is unnecessary. To assess the first hypothesis (Long-Horizon Reasoning), we employ suite of challenging benchmarks: AIME 24&25 [2, 3],4 BrUMO25 [14], HMMT [15], BeyondAIME [16], Humans Last Exam (HLE, text-only subset) [5], and IMOAnswerBench [4]. We selected these datasets for two primary reasons. First, they demand complex, multi-step logic that typically yields trajectories exceeding 20k tokensa regime where standard models frequently suffer from catastrophic degeneration. Second, the recency of datasets like BeyondAIME (June 2025) and IMOAnswerBench (November 2025) minimizes data contamination risks, ensuring robust evaluation of intrinsic reasoning capabilities. To assess the second hypothesis (General Reasoning & Safety), we evaluate Free()LM on standard benchmarks requiring shorter reasoning chains: BBH [17], MMLU-Pro [18], MMLU-STEM [19], and GPQA-Diamond [20]. These tasks serve as control group to confirm that Free()LM maintains the baseline performance of standard models on general-purpose queries. Implementation Details. For mathematical reasoning tasks, we standardize outputs using the prompt: Please reason step by step, and put your final answer within boxed{}. Correctness is evaluated by extracting the answer from boxed{} and matching it against the ground truth, following the evaluation pipeline [10]. We report three key metrics: pass@1, the number of response tokens (#Token), and the response reduction ratio () relative to the vanilla model. For all experiments, we employ sampling-based decoding with temperature=0.7, top_k=20, and top_p=0.95. To balance computational cost with statistical reliability, we configure the number of rollouts based on dataset size: 8 rollouts for AIME2425, BrUMO25, HMMT, and BeyondAIME (which have limited questions), and 1 rollout for the larger HLE and IMOAnswerBench datasets. For Free()LM, the pruning interval Lclean is set to 5000 with maximum of 50 iterations. 1https://huggingface.co/Qwen/Qwen3-8B 2https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507 3https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507 4AIME24 [2] and AIME25 [3] are combined into single dataset, hereafter referred to as AIME2425. Table 1 Performance of Qwen3 models. We report pass@1 (p@1) performance computed over 8 rollouts, along with the average number of response tokens (#Token). For the Average columns, brackets represent the absolute change for p@1 and the relative change for #Token (where blue indicates improvement and red indicates regression). Model Setting Qwen3 -8B Qwen3 -30B Qwen3 -235B Vanilla H2O TC No Train Gemini Free()LM Vanilla No Train Gemini Free()LM Vanilla No Train Gemini Free()LM AIME2425 p@1 #Token 71.67 60.00 51.67 73.33 71.67 17.6k 17.8k 20.3k 15.3k 13.7k 13.0k BrUMO25 p@1 #Token 69.58 70.00 66.67 70.83 68.75 15.8k 18.3k 16.6k 16.4k 15.2k 13.7k HMMT p@1 #Token 38.75 46.67 26.67 42.50 44. 19.4k 21.5k 20.5k 19.9k 18.4k 16.4k BeyondAIME p@1 #Token 42.38 35.00 42.00 43.38 43.63 19.5k 20.5k 19.7k 20.6k 17.6k 15.4k 75.00 83.33 85.21 87.92 87.92 92.29 90.63 93.54 93. 14.9k 15.9k 15.4k 14.5k 21.7k 16.2k 18.5k 16.4k 72.50 82.92 86.25 84.17 85.42 91.67 92.08 93.75 94. 15.1k 17.1k 15.9k 14.0k 19.9k 17.1k 17.7k 16.4k 49.58 60.83 63.33 65.42 70.42 85.00 80.00 84.17 84.58 21.0k 20.4k 19.0k 19.1k 29.1k 24.4k 25.6k 21.7k 45.88 56.50 59.25 59.75 62.25 69.00 68.63 70.38 69.75 22.5k 20.8k 19.7k 18.2k 31.0k 22.9k 26.5k 21.3k 5.89 5.10 5.38 8.99 9.78 9.59 9.82 16.13 15.20 16.59 18.03 IMOAnswer p@1 #Token 38.50 36.25 34.00 38.75 40.00 19.4k 20.7k 20.3k 17.4k 17.8k 14.3k 40.50 52.25 53.25 54.00 58.00 61.00 58.25 62.25 62.75 21.1k 19.4k 20.4k 18.1k 32.3k 24.4k 27.6k 24.2k Average p@1 44.24 42.05 (-2.19) 37.66 (-6.58) 45.78 (+1.54) 45.55 (+1.31) 48.14 (+3.90) 57.47 59.51 (+2.04) 60.14 (+2.67) 62.30 (+4.83) 69.18 67.46 (-1.72) 70.11 (+0.93) 70.47 (+1.29) #Token 17.5k 19.0k (+8.6%) 18.8k (+7.4%) 17.0k (-2.9%) 15.8k (-9.7%) 13.8k (-21.1%) 18.1k 17.7k (-2.2%) 17.2k (-5.0%) 15.9k (-12.2%) 26.1k 20.5k (-21.5%) 22.4k (-14.2%) 19.3k (-26.1%) 13.3k 15.3k 15.3k 12.3k 11.8k 9.9k 13.9k 12.7k 12.8k 11.3k 22.5k 18.2k 18.7k 15.8k HLE p@1 #Token 4.59 4.39 4."
        },
        {
            "title": "3.2 Main Results",
            "content": "We begin by addressing the core question: Can Free()LM enhance reasoning performance through active pruning? As shown in Table 1, Free()LM consistently improves accuracy across all benchmarks while maintaining significantly more compact context. Higher Accuracy with Deep Pruning: The most striking finding is the substantial boost in reasoning capability. On the Qwen3-8B benchmark, Free()LM achieves an average Pass@1 of 48.14%, outperforming vanilla inference (44.24%) by +3.9%. Crucially, this scaling is achieved by retaining less rather than generating more. We observe distinct Deep Pruning phenomenon: Free()LM yields significantly shorter average response length (13.8k) than the Gemini-2.5-Pro (15.8k), while surpassing its accuracy by +2.3%. Our qualitative investigation reveals the decisive factor: while Gemini often mistakenly removes useful cluesforcing the backbone to regenerate themFree()LM precisely targets true redundancy. As illustrated in the case study (Figure 5), Free()LM maintains noise-free reasoning chain without any observed regeneration. This proves the effectiveness of our reward mechanism: we successfully trained module that knows exactly what to prune, reaching precision level that even top-tier models struggle to achieve. The Failure of Heuristic Compression: The results for H2O and TC are puzzling. Not only did they fail to improve accuracy, but they also failed to reduce the response length. close inspection reveals the cause: catastrophic degeneration. By using heuristics for pruning, these methods disrupt the reasoning chain, causing the model to get stuck in repetitive loops. Ultimately, instead of making the model more intelligent, these heuristic methods render the reasoning process prone to crashing and produce even longer, redundant outputs. Scalability: On the massive Qwen3-235B-A22B, Free()LM demonstrates its true value on challenging tasks like HLE and IMOAnswer. It boosts performance significantly (11.7% relative improvement on HLE) while reducing context length by massive 27.5%. This sends clear message: bigger is not always better if the context is cluttered. For these giants to perform at their peak, the ability to free() redundant information is as vital as the capacity to store it. Short-Reasoning Tasks: common concern is whether pruning hurts general capabilities. Table 2 allays this fear, showing Free()LM maintains parity across broad benchmarks. Since the backbone is frozen, the Free-Module is triggered significantly less often on short-reasoning tasks. As result, Free()LM closely tracks the performance of the vanilla model without any degradation in general knowledge. Table 2 Performance of Free()LM (Qwen3-8B Backbone). Qwen3-8B 82.86 Free()LM 82.89 BBH MMLU-P MMLU-S GPQA 92.29 92.55 75.57 75.58 Method 59.41 61.36 6 Figure 4 Performance vs. Reasoning Length on HLE. While standard Qwen3-235B-A22B suffers from Degradation on trajectories longer than 80k tokens, Free()LM exhibits striking rebound in accuracy. On these cases, the Free-Module reduces the context length by 45%, effectively mitigating context pollution."
        },
        {
            "title": "3.3 Cross-Model Generalization",
            "content": "Following the superior performance of Free()LM, we investigate critical question: is the capability of trained Free-Module model-specific, or does it generalize across different architectures? To explore this, we first apply the Free-Module trained on Qwen3-8B to perform the free() operation for the much larger Qwen3-235B-A22B on IMOAnswer. Results in Table 3 demonstrate that the 8B Free-Module achieves performance gains (+1.5%) comparable to those of the 235B Free-Module (+1.75% as shown in Table 1). Inference Setup Table 3 Cross-Model Generalization. The 8B Free-Module enhances 235B/685B performance on IMOAnswerBench. However, critical question remained: is this generalization capability merely byproduct of the shared model family? To challenge this hypothesis, we introduced completely different architecture, DeepSeek-V3.2-Speciale [21], into our evaluation. The results addressed this concern: even on this alien architecture, the Qwen-backboned 8B FreeModule improved Pass@1 by 2.3% while simultaneously slashing response tokens by 45.99%. This substantial improvement across distinct architectures suggests that the 8B-module has acquired universal context-cleaning capability. This cross-model generalization inspires plug-and-play deployment strategy: serving the Free-Module as Universal Context Pruning Service. In this paradigm, any backbone model can simply invoke this service upon reaching trigger condition, pruning its context, and resume inference without requiring model-specific adaptation. DeepSeek-V3.2 Token Reduction + Free-Mod (8B) + Free-Mod (8B) Qwen3-235B 45.99% Pass@1 21.34% 61.00 83.54 62.50 85."
        },
        {
            "title": "3.4 Analysis on Reasoning Length",
            "content": "Figure 4 breaks down the performance on HLE. To isolate the impact of reasoning depth, we categorize test instances based on the token count of the response generated by the vanilla Qwen3-235B-A22B model. The results reveal stark contrast: Reasoning models suffer from Degradation. For manageable lengths, the vanilla model performs well. However, as trajectories extend beyond 80k tokens, the accumulated noise triggers catastrophic degradation, and resulting in complete loss of accuracy to 0%. Free()LM recovers the Lost Intelligence. In these same deep waters (>80k tokens), Free()LM exhibits striking rebound, with accuracy surging back to 50%. Although the limited sample size in this tail warns 7 Free()LM: Stable Deletion Gemini Deletion: Regeneration [Current step] According to previous analysis, two non-empty intersections. However, the problem states three regions. perhaps made an error in considering Case 2 Case B? Therefore, Wait, let me check again. ... Therefore, Case 2 Case is empty. Therefore, the only non-empty intersections are Case 1 Case and Case 2 Case A. But the problem states three regions. So, where is the third region? [Next step](continues after deletion) Suppose we consider the entire plane + + = 75. The inequalities divide it into regions. Since the problem says three regions, maybe theres another region where both inequalities hold? ... [Current step] Alternatively, since Γ is the nine-point circle, maybe there are some symmetries? Maybe use coordinates ... find circumcircle of D, E, Let me find the equation of the circle passing through these three points. [Next step](re-generated) Also, angle at is 60, which can be used to find coordinates. Maybe use coordinates ... find circumcircle of D, E, Let me find the equation of the circle ... Figure 5 Case study comparing Free()LM versus Gemini deletion. Deleted spans are shown in red, new generated content in green, and re-generated content matching previous deletions in blue. Free()LM (left) successfully prunes redundant reasoning; in contrast, Gemini (right) erroneously deletes critical logical anchors, forcing the subsequent reasoning model to re-generate the previously pruned context to restore the integrity of the reasoning chain. against claiming definitive intelligence boost, the ability to sustain reasoning capability is undeniable. The grey dashed line explains this recovery. On these ultra-long paths, the Free-Module aggressively compresses the context by 40%50%. This operation successfully brings massive 100k+ trajectory back down to the 40k70k range, which is sweet spot where the Qwen3-235B-A22B backbone operates most comfortably. We may simply conclude the result as: with greater context comes greater necessity for Free()LM."
        },
        {
            "title": "3.5 System Performance",
            "content": "Finally, we evaluate the engineering impact on Qwen3-235B-A22B using the HLE benchmark. We deploy the model across two 8H20 nodes using Tensor Parallelism (TP=16). The serving backend is vLLM (v0.8.5) on CUDA 12.6, configured with FlashAttention-2 and BF16 precision. We start with the cost: Free()LM incurs 56% increase in latency per sample. This overhead primarily stems from three sources: (1) the time spent decoding free() commands, (2) the re-prefilling latency incurred after executing deletion, and (3) the regeneration of information if the model occasionally over-prunes. Table 4 Efficiency Comparison. Per-sample latency and KV cache memory usage for Qwen3-235B on HLE. Metric Latency (s) KV (GB) Base 353.2 6.14 Free()LM 552.3 3.34 +56.4% -45.6% However, the upside is substantial: we achieve 45% reduction in KV cache usage (6.14 GB 3.34 GB). In real-world serving, where memory bandwidth is often the bottleneck, this saving is critical. Furthermore, the current latency is not hard limit. By adopting Strategy 2 (KV Cache Pruning) to eliminate the re-prefilling step mentioned in (2), we estimate the overhead could be cut down to 20%, although we have not yet implemented this strategy in vLLM."
        },
        {
            "title": "3.6 Case Study",
            "content": "We qualitatively compare Free()LM and an ICL-based Gemini baseline using an AIME2425 example (Figure 5). Free()LM (left) demonstrates precise deletion: when the model begins redundant self-correction (denoted by red), the Free-Module removes it, allowing inference to resume (denoted by green) without re-generating the pruned content. Conversely, Gemini (right) misjudges essential coordinate-setup as redundant. Its immediate regeneration (denoted by blue) confirms the deletion was erroneous, as the backbone still required that information. This prune-then-regenerate cycle highlights Geminis inability to distinguish truly disposable context, resulting in wasted computation. Thus, it explains why Free()LM outperforms the Gemini-based Free-Module in Table 1."
        },
        {
            "title": "4 Related Work",
            "content": "Free()LM tackles context accumulationa challenge where unbounded intermediate thoughts crowd out information needed for subsequent reasoning. We situate our approach at the intersection of KV cache compression, context window expansion, and CoT overthinking."
        },
        {
            "title": "4.1 KV Cache Compression",
            "content": "The quadratic growth of the Key-Value (KV) cache is primary bottleneck in LLM inference. To bound memory, heuristic eviction methods like StreamingLLM [22] and H2O [12] preserve \"anchor\" or high-attention tokens. SnapKV [23] and PyramidKV [24] refine this via saliency clustering and layer-wise adaptive budgets. Other strategies include reconstruction-based methods like KVzip [25] and low-bit quantization such as KIVI [26]. However, lossy compression can induce \"distraction\" [27], shifting attention to irrelevant content and degrading performance on precise reasoning tasks. Unlike these throughput-centric approaches, Free()LM implements logic-aware pruning. By training Free-Module to detect semantic redundancy within reasoning traces, Free()LM improves efficiency and maintains reasoning quality."
        },
        {
            "title": "4.2 Long Context Window",
            "content": "Expanding the effective context window is major research frontier. Early architectural innovations modified positional encodings to extend context limits. ALiBi [28] replaces absolute positions with relative biases to improve length extrapolation. YaRN [29] and LongRoPE [30] extend RoPE via rescaling/interpolation, enabling million-token contexts and pushing theoretical limits beyond 2M tokens. System-level approaches such as Ring Attention [31] distribute attention computation across GPUs using blockwise schemes, further increasing feasible context length. Training-based methods such as LongLoRA [32] adapt pre-trained models to longer contexts efficiently. Despite these advances, (author?) [33] show performance can degrade as context grows due to distraction, even with perfect retrieval; (author?) [34] similarly reports accuracy decay beyond 32k tokens across diverse reasoning tasks. This is orthogonal but complementary to Free()LM: rather than expanding the window, we address the memory leak by teaching models to actively free() obsolete information."
        },
        {
            "title": "4.3 Overthinking",
            "content": "While Chain-of-Thought (CoT) [35] improves model reasoning, it can induce overthinking: excessive, redundant steps that consume context without improving the final answer [36]. This is especially acute in multi-step math, where models may explore dead ends, repeat computations, or over-verify. The rise of o1-style long-reasoning models intensifies this issue, with 10k100k+ tokens for single problem; (author?) [37] shows redundancy can appear even in trivial arithmetic. Prior work mitigates overthinking via budgeted generation [38], pruning based on token importance [13], or stopping rules such as entropy monitoring [39]. Other methods optimize conciseness through training, e.g., O1-Pruner [40] uses reinforcement learning to reward shorter correct reasoning, and DeepCompress [41] employs adaptive length rewards based on problem difficulty. Unlike methods that primarily control output length, Free()LM enables dynamic context cleaning during inference through trainable Free-Module that free()s redundant reasoning steps, keeping the workspace efficient throughout generation."
        },
        {
            "title": "5 Conclusion",
            "content": "We demonstrate that the prevailing malloc-only paradigmwhere context is treated as passive, appendonly bufferis fundamentally unsustainable for long-horizon reasoning. In standard LLMs, the continuous accumulation of redundant tokens eventually overwhelms the model, leading to the total collapse of reasoning performance. To break this cycle, we introduce Free()LM, completing the memory management cycle with the missing free() operation. 9 Our work makes three primary contributions. First, we provide the first systematic evidence that active context pruning directly enhances reasoning performance across all model scales, from 8B to 685B. Second, we show that our plug-and-play Free-Module effectively maintains noise-free state, yielding 3.3% average gain over leading baselines and setting new SOTA on IMOanswerBench. Third, and most crucially, we demonstrate that Free()LM solves the failure of long-reasoning: while standard models like Qwen3-235B-A22B drop to 0% accuracy on complex tasks, Free()LM restores performance to 28% by precisely pruning redundancy. Ultimately, Free()LM suggests shift in the scaling laws of test-time compute. The path to long-horizon intelligence lies not merely in expanding the context window, but in mastering the art of forgetting. This transition from malloc-only to malloc + free provides the foundation for the next generation of efficient, self-sustaining reasoning agents."
        },
        {
            "title": "References",
            "content": "[1] Kelly Hong, Anton Troynikov, and Jeff Huber. Context rot: How increasing input tokens impacts llm performance. Technical report, Chroma, July 2025. [2] Yifan Zhang and Team Math-AI. American invitational mathematics examination (aime) 2024, 2024. [3] Yifan Zhang and Team Math-AI. American invitational mathematics examination (aime) 2025, 2025. [4] Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, and Junehyuk Jung. Towards robust mathematical reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025. [5] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, and Hugh Zhang et. al. Humanitys last exam, 2025. [6] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [7] Dongyang Ma, Yan Wang, and Tian Lan. Block-attention for efficient prefilling. In The Thirteenth International Conference on Learning Representations. [8] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, and Evan Rosen et. al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. [10] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. [11] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and Chujie Zheng et. al. Qwen3 technical report, 2025. [12] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. [13] Daewon Choi, Jimin Lee, Jihoon Tack, Woomin Song, Saket Dingliwal, Sai Muralidhar Jayanthi, Bhavana Ganesh, Jinwoo Shin, Aram Galstyan, and Sravan Babu Bodapati. Think clearly: Improving reasoning via redundant token pruning. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2143721451, Suzhou, China, November 2025. Association for Computational Linguistics. [14] Mislav Balunović, Jasper Dekoninck, Ivo Petrov, Nikola Jovanović, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. 10 [15] Mislav Balunović, Jasper Dekoninck, Ivo Petrov, Nikola Jovanović, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, February 2025. [16] ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. [https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME](https://huggingface.co/datasets/ ByteDance-Seed/BeyondAIME), 2025. [17] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [18] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NIPS 24, Red Hook, NY, USA, 2024. Curran Associates Inc. [19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [20] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [21] DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and Chengda Lu et. al. Deepseek-v3.2: Pushing the frontier of open large language models, 2025. [22] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [23] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. [24] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Junjie Hu, and Wen Xiao. PyramidKV: Dynamic KV cache compression based on pyramidal information funneling. In Second Conference on Language Modeling, 2025. [25] Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae Lee, Sangdoo Yun, and Hyun Oh Song. Kvzip: Query-agnostic kv cache compression with context reconstruction. arXiv preprint arXiv:2505.23416, 2025. [26] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [27] Alex Chen, Renato Geh, Aditya Grover, Guy Van den Broeck, and Daniel Israel. The pitfalls of kv cache compression, 2025. [28] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [29] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. [30] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens, 2024. [31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. [32] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient finetuning of long-context large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 11 [33] Yufeng Du, Minyang Tian, Srikanth Ronanki, Subendhu Rongali, Sravan Babu Bodapati, Aram Galstyan, Azton Wells, Roy Schwartz, Eliu Huerta, and Hao Peng. Context length alone hurts LLM performance despite perfect retrieval. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2328123298, Suzhou, China, November 2025. Association for Computational Linguistics. [34] Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, and Hinrich Schütze. Nolima: Long-context evaluation beyond literal matching, 2025. [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [36] Cheng-Han Chiang and Hung-yi Lee. Over-reasoning and redundant calculation of large language models. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161169, St. Julians, Malta, March 2024. Association for Computational Linguistics. [37] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do NOT think that much for 2+3=? on the overthinking of long reasoning models. In Forty-second International Conference on Machine Learning, 2025. [38] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware LLM reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 2484224855, Vienna, Austria, July 2025. Association for Computational Linguistics. [39] Tianyi Jiang, Yi Bin, Yujuan Ding, Kainian Zhu, Fei Ma, Jingkuan Song, and Heng Tao Shen. Explore briefly, then decide: Mitigating llm overthinking via cumulative entropy regulation, 2025. [40] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025. [41] Tian Liang, Wenxiang Jiao, Zhiwei He, Jiahao Xu, Haitao Mi, and Dong Yu. Deepcompress: dual reward strategy for dynamically exploring and compressing reasoning chains, 2025. [42] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https: //github.com/huggingface/trl, 2020. [43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019."
        },
        {
            "title": "A Experimental Details",
            "content": "A.1 Prompts System Prompt: Role: AI Reasoning Analyst Your task is to act as an AI Reasoning Analyst. You will be given Chain-of-Thought (CoT) reasoning and your goal is to identify and mark for deletion any paragraphs from the CoT reasoning that are redundant or irrelevant. Instructions: 1. Analyze the CoT Reasoning: Carefully read and understand the reasoning, context, and goals of the CoT reasoning. 2. Identify Redundant or Irrelevant Paragraphs: paragraph is considered redundant or irrelevant if it does not directly support or inform the overall reasoning process. This could include tangential thoughts, corrected errors, or superseded lines of reasoning. 3. Mark for Deletion: For every irrelevant paragraph you detect, generate JSON object that uniquely identifies it. Each object must include prefix and suffix extracted from the paragraph. item Format the Output: Your final output must be single JSON object containing list of these prefix/suffix objects. 4. <DELETED> in the CoT reasoning indicates that the content has already been removed. 5. It is possible that you dont need to delete any paragraphs in the CoT reasoning. Constraints: If no paragraphs are redundant or irrelevant, output an empty list: []. If multiple paragraphs are redundant, include separate JSON object for each one. Do not delete the first or the last paragraph in the CoT reasoning. User Prompt: ### CoT Reasoning: <PREVIOUS_COT> Figure 6 Deletion Prompts for Free()LM. The system prompt (top) provides the operational logic for identifying redundant reasoning steps, while the user prompt (bottom) delivers the actual CoT context for the agent to process. The prompts utilized in this study are detailed in Figure 6. We employ consistent prompt for both training corpus preparation and alignment evaluation. To ensure structured response format, we implement JSON schema to guide the Free-Modules output, as illustrated in Figure 7. Both vLLM [8] and Gemini natively support this structured output functionality. A.2 Training Details Since Free()LM is trained to perform deletion over intermediate reasoning produced by backbone models that do not expose explicit reasoning traces as supervision, we disable the thinking mode in the chat template during prompt construction. This design ensures that the Free-Module operates solely on the visible model-generated reasoning, matching the inference-time setting. We train the Free-Module on the Qwen3-8B backbone using TRL (v0.19.1) [42], while the larger Qwen3-30BA3B and Qwen3-235B-A22B variants are trained with Megatron-LM (v0.14.1) [43]. All models are trained for 5 epochs with learning rate of 1 105 and global batch size of 16. To optimize memory efficiency and throughput, we use the AdamW optimizer together with FlashAttention-2 for attention computation. We employ DeepSpeed ZeRO Stage 3 for Qwen3-8B and Stage 2 for Qwen3-30B-A3B and Qwen3-235B-A22B, balancing memory savings and communication overhead across model scales. All training runs are conducted using BF16 precision. For parameter-efficient fine-tuning, we apply LoRA to all linear layers with rank = 128, scaling factor α = 256, and dropout rate 0.1. The backbone model parameters remain frozen throughout training; only the 13 Response Schema (JSON): { \"type\": \"items\": \"array\", { \"type\": \"properties\": { \"object\", \"prefix\": { \"type\": \"description\": \"Beginning text of paragraph to delete.\" \"string\", }, \"suffix\": { \"type\": \"description\": \"Ending text of paragraph to delete.\" \"string\", } }, \"required\": [\"prefix\", \"suffix\"] }, \"description\": \"A list of prefix/suffix pairs that uniquely identify paragraphs that are redundant or irrelevant and should be deleted.\" } Figure 7 Structured Output Schema for Free()LM. The JSON schema enforces consistent prefix/suffix format, ensuring that redundant or irrelevant reasoning steps identified by the agent can be programmatically parsed and removed from the context. Free-Module parameters are updated. A.3 Evaluation Details For prefilling, we append the models previously generated response to the end of the prompt after applying the chat template, and re-run the prompt through the model to resume generation from the refined context. This procedure ensures that subsequent tokens are generated conditionally on the updated reasoning trace, rather than restarting inference from scratch. In practice, we leverage vLLMs support for efficient prefilling and KV cache reuse. When the context is modified by free() operation, we reuse the cached keyvalue states corresponding to the unchanged prefix, and only re-prefill the altered suffix. This avoids recomputing attention for the entire context and significantly reduces the overhead of resuming inference."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Tencent AI Lab"
    ]
}