{
    "paper_title": "Pillar-0: A New Frontier for Radiology Foundation Models",
    "authors": [
        "Kumar Krishna Agrawal",
        "Longchao Liu",
        "Long Lian",
        "Michael Nercessian",
        "Natalia Harguindeguy",
        "Yufu Wu",
        "Peter Mikhael",
        "Gigin Lin",
        "Lecia V. Sequist",
        "Florian Fintelmann",
        "Trevor Darrell",
        "Yutong Bai",
        "Maggie Chung",
        "Adam Yala"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints."
        },
        {
            "title": "Start",
            "content": "Pillar-0: New Frontier for Radiology Foundation Models Kumar Krishna Agrawal 3 Yufu Wu , Peter Mikhael Maggie Chung 10, , Adam Yala 1,* , Gigin Lin 2, 2, 1, , Longchao Liu 3,5,6 , Long Lian , Michael Nercessian 7 , Florian Fintelmann , Lecia V. Sequist , Natalia Harguindeguy 1 , Trevor Darrell 1 , Yutong Bai , , 2, 8,9 2, 1 2 3 5 6 Department of Electrical Engineering and Computer Science, UC Berkeley, USA Computational Precision Health, UC Berkeley and UC San Francisco, USA Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital at Linkou, Taiwan Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA Department of Medical Imaging and Radiological Sciences, Chang Gung University, Taiwan Clinical Metabolomics Core and Imaging Core Laboratory, Institute for Radiological Research, Chang Gung Memorial Hospital 8 at Linkou and Chang Gung University, Taiwan 7 Mass General Brigham Cancer Institute, USA Massachusetts General Hospital, USA Harvard Medical School, USA Department of Radiology and Biomedical Imaging, UC San Francisco, USA 9 *"
        },
        {
            "title": "Project lead",
            "content": "Core contributor (these authors contributed equally; order among core contributors was determined at random) Co-senior author"
        },
        {
            "title": "Abstract",
            "content": "Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth, contributing to burnout and challenges in care delivery. Foundation models offer path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. Here, we introduce Pillar-0, radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from large academic center, together with RATE, scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using large language models. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford abdomen-pelvis CT dataset, including Merlin (82.2 vs 80.6 AUROC), which uses the Stanford dataset for development. Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained >95 AUROC when 1 using only 20 of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints. 5 2 0 2 1 2 ] . [ 1 3 0 8 7 1 . 1 1 5 2 : r Pillar-0: New Frontier for Radiology Foundation Models 1. Main Radiology serves key role in modern clinical practice, as it allows for the visualization of disease and guides patient management. Imaging utilization has continued to grow significantly year over year, with studies reporting annual growth rates ranging from 5 to 7% [1, 2]. This growth has far outpaced the expansion of the radiology workforce, resulting in radiologist burnout and challenges in traditional patient care delivery models [3, 4, 5, 6]. Although numerous artificial intelligence (AI) tools have been proposed to improve the detection of pathology on imaging studies, including commercially available tools for the detection of lung nodules [7] and intracranial hemorrhage [8], their impact on overall radiology efficiency is limited. These tools assist with only small fraction of radiologists tasks. In practice, radiologists perform comprehensive image interpretation with wide range of findings across all organ systems, modalities, and protocols [9, 10, 11, 12]. Assisting with this workload requires technology that can address the full spectrum of image findings. Foundation models learn broad, transferable representations from diverse datasets and therefore hold promise for enabling comprehensive image interpretation [13, 14]. An ideal radiology foundation model would 1) enhance performance across wide range of downstream tasks, including classification, localization, prognosis, and report generation; 2) drastically reduce the amount of training data required for finetuning; and 3) serve as de-facto platform for downstream model development. Despite extensive effort [15, 16, 17, 18, 19, 20], these goals remain largely unrealized for computed tomography (CT) and magnetic resonance imaging (MRI) due to several challenges. Challenges in Modeling Volumetric Imaging. From computational perspective, the primary challenge in modeling volumetric medical imaging is resolution. Medical volumes are immense in both spatial scale and bit-depth. For example, CT scans of the abdomen-pelvis often contain 512 512 768 pixels and are over 4,000 times larger than normal 224 224 ImageNet images [21]. The cost of traditional vision transformers scales quadratically with input size, making direct 3D modeling computationally prohibitive. Consequently, leading foundation models from Google (MedGemma)[17], Microsoft (MedImageInsight)[18], and Alibaba (Lingshu)[19] process CT exams as independent 2D slices, ignoring volumetric structure and losing essential contextual information. We hypothesize that leveraging the native 3D structure of CT exams has the potential to deliver significant performance improvements. Moreover, CT scans are acquired in 12-16 bit voxels, capturing up to 65,536 grayscale values, which are significantly richer than the 8-bit (256-value) pixels found in ImageNet [21]. Radiologists routinely apply specialized windowing strategies to dynamically view different tissue types (e.g., bone, soft-tissue, lung, etc.) with different intensity ranges [22, 23]. However, leading industry models downsample these rich voxels to 8-bit, resulting in loss of subtle contrast [18]. We hypothesize that preserving the dynamic range of volumetric imaging would similarly improve the capability of radiology models. Limitations of Current Evaluation Frameworks. Progress in developing radiology foundation models rests on the rigor of their evaluation. However, the leading evaluation benchmarks exhibit limitations that parallel the shortcomings of the models they seek to assess. Visual question-answering benchmarks like VQA-RAD [24] and SLAKE [25] convert high-dimensional medical volumes into downsampled 2D slices stored as 8-bit JPEGs and pair them with simplified questions that often do not reflect real diagnostic tasks. PMC-VQA [26] compounds this problem by sourcing image-text pairs from scientific publications rather than clinical imaging, limiting their relevance to routine clinical practice. These benchmarks fail to capture the richness of inputs (i.e., full resolution DICOM volumes) or the diversity of clinically meaningful outputs (i.e., wide range of imaging findings) across modalities and indications [27, 28, 29]. As result, researchers cannot answer the core clinical question: whether model can detect hundreds of diverse imaging findings within high-resolution 3D volumes, as radiologists do in clinical practice. Consequently, shortage of rigorous benchmarks hinders the development of robust radiology foundation models. We hypothesize that improved evaluation frameworks, designed to leverage the full complexity of radiology data, are critical for catalyzing meaningful progress in radiology foundation models. Given these challenges, there remains an unmet need for general-purpose radiology foundation models. State-of-the-art models for tasks ranging from disease detection [30] to risk prediction [31] continue to rely on natural-image pretraining or manual feature engineering (e.g., radiomics). To address this gap, we introduce Pillar-0, radiology foundation model that advances the broad frontier of CT and MRI understanding, improving performance across hundreds of radiological tasks. 2 Pillar-0: New Frontier for Radiology Foundation Models Figure 1: Overview of Pillar-0 and key results across modalities and tasks. (A) We evaluate Pillar-0 on abdomen-pelvis CT, chest CT, head CT, and breast MRI using RATE, clinically grounded evaluation framework designed to overcome the limitations of existing radiology benchmarks. Pillar-0 substantially outperforms competitive baselines across modalities. (B) Pillar-0s capabilities extend to real-world clinical prediction tasks outside the standard of care, setting new state-of-the-art for future lung cancer risk prediction with Sybil-1.5 (Pillar-0 finetuned). On rigorous multi-institution external validation, Sybil-1.5 outperforms Sybil [31], strong specialist baseline, by wide margin. (C) Finally, Pillar-0 demonstrates superior data efficiency, reaching 95 AUROC on the RSNA Intracranial Hemorrhage detection benchmark [32] using 20-30 less training data relative to best-in-class natural image-pretrained (Swin3D-t [33]) and radiology-pretrained (Merlin [15]) models. The entire Pillar-0 systemopen-code, open-weights, and open-evaluationis released to the community. 3 Pillar-0: New Frontier for Radiology Foundation Models Our work provides the following contributions: 1. Clinically Grounded Evaluation Framework. We develop Radiology Text Engine (RATE), clinically grounded evaluation framework for medical volumes. team of board-certified radiologists curated broad list of 366 radiological findings abdomen-pelvis CT, chest CT, head CT, and breast MRI exams, and validated that open large language models (LLMs) such as Qwen3 [34] could extract the findings from radiology reports with near perfect accuracy. RATE provides an efficient LLM interface to automatically extract large labeled datasets of imaging findings from unstructured radiology reports, establishing the foundation for reproducible model comparison. 2. Dominant Performance on Internal Test Sets. We release Pillar-0, radiology foundation model pretrained across CT exams of the abdomen-pelvis (n=42,990), chest (n=86,411), and head (n=14,348), as well as 11,543 breast MRI exams. For internal validation, we evaluated Pillar-0 on held-out imaging exams spanning head CT (n=4,906), chest CT (n=10,646), abdomen-pelvis CT (n=14,230), and breast MRI (n=1,585). Pillar-0 obtained an average AUROC of 90.11, 88.0, 86.4, and 82.9 across these modalities, reflecting an absolute improvement of 7.815.8 over the next best model. Across 366 evaluated tasks, Pillar-0 was the top-performing model in 87.2% (319/366), outperforming MedGemma [17], MedImageInsight [18], Lingshu [19], and Merlin [15]. 3. Outperforming Foundation Models in External Validation. We externally validated Pillar-0 on CT abdomen-pelvis dataset from Stanford (USA)[15], previously used to develop the Merlin CT foundation model. Here, Pillar-0 outperformed MedGemma, MedImageInsight, Lingshu, and Merlin. Moreover, when we retrained Pillar-0 using only Merlins training data, Pillar-0 still outperformed Merlin (82.2 vs 80.6), despite leveraging fewer forms of supervision. 4. Improving Ability to Predict Future Cancer from Asymptomatic Screening. Sybil [31] previously demonstrated that low-dose CTs could be used to predict the risk of future lung cancer. This prediction task, which humans cannot perform, is outside the distribution of Pillar-0 pretraining. By finetuning Pillar-0, we significantly outperformed the original Sybil model in predicting lung cancer risk. This improvement generalized to the same external validation test sets used to validate the original Sybil, Massachusetts General Hospital (MGH, Boston, USA) and Chang Gung Memorial Hospital (CGMH, Taipei, Taiwan), with condordance index [35] improvements of 5.9 (81.0 to 86.9) and 1.9 (81.5 to 83.4) respectively. 5. Reducing Downstream Training Data Needs by >20x. We showed that Pillar-0 significantly improves the sample efficiency of building radiology AI tools with case study on brain hemorrhage detection using the RSNA Brain CT challenge dataset [32]. When finetuning foundation models for hemorrhage detection, we found that Pillar-0 achieved dominant performance across 1 all data regimes. Moreover, Pillar-0 reached >95 AUROC when using only 20 of the data of the next most sample-efficient baseline. 6. Open Science. To enable the broader research community to benefit from our advances, we release broad suite of open-source tools, including evaluation, volume preprocessing, model pretraining, finetuning and inference repositories. We also release all pretrained models. 2. Results 2.0. Overview of Pillar-0 and Core Innovations We introduce Pillar-0, general-purpose foundation model for volumetric medical imaging, built on key innovations across tokenization, model architectures, and pretraining (Figure 2). To leverage the full bit-depth of CT and MRI scans, we propose custom tokenizers inspired by radiology workflows. Rather than using single 8-bit range, we project volume patches into multiple intensity range windows to highlight different tissue properties across image channels (Figure 2a). Next, to reason over large spatial context, we leverage multi-scale attention and the Atlas neural network architecture [36] (Figure 2b). Atlas processes abdomen-pelvis CT scans at 175 the speed of comparable vision transformer. Finally, to distill rich radiology report supervision during Pillar-0 pretraining, we leverage asymmetric contrastive learning analogous to CLIP [37], where we learn to align Atlas volume representations with Qwen-8B [34] text representations (Figure 2c). Using large text encoders, such as modern LLMs, instead of smaller models (e.g., RoBERTa; <500M parameters) [38], boosts radiology foundation model performance and yields much stronger correlation between pretraining loss and downstream results, enabling more predictable scaling. Additional dataset and methodological context is provided in Sections A.0.1 and A.0.2. 1While AUROCs and concordance (C-)indices range from 0-1, we report all AUROCs and C-indices as 100x the value (i.e., 90.1 instead of 0.901) for better legibility. 4 Pillar-0: New Frontier for Radiology Foundation Models Figure 2: Pillar-0 key innovations across tokenization, architecture, and pretraining. (A) Modality-specific multi-windowing converts fullresolution CT and MRI volumes into multi-channel inputs that emulate radiologist workflow presets, preserving clinically relevant contrast. Training with multi-windowing leads to 4.6 point gain in AUROC in abdomen-pelvis CT. (B) The Atlas vision backbone employs hierarchical Multi-Scale Attention to efficiently process long-context volumes [36]. As result, Pillar-0 is 175 faster than ViT-S, and achieves state-of-the-art performance with fewer parameters than other medical foundation models. (C) Asymmetric contrastive pretraining aligns Atlas volume embeddings with embeddings from much larger frozen LLM text encoder. Using this powerful text encoder leads to much stronger correlation between CLIP loss and downstream performance, providing reliable signal for clinical utility to guide pretraining experiments. 2.1. RATE: Clinically Grounded Evaluation Framework We introduce RATE, unified framework designed to evaluate any vision model on full-fidelity medical volumes, using authentic clinical tasks derived from real-world radiology practice (Figure 3). Existing radiology benchmarks fall short along three key dimensions (Figure 3a). First, none use full-resolution volumetric datainstead, most use 2D slices in the format of JPEG images [24, 25], or snapshots from medical textbooks [28, 29]. Second, the task definitions and accompanying labels are not sourced from routine clinical practice. Task definitions often do not reflect the typical responsibilities of radiologists, and instead rely on hand-crafted or automated prompts which can be as simple as identifying the imaging modality [26, 24]. Labels are typically synthetic or manually annotated, rather than extracted from 5 Pillar-0: New Frontier for Radiology Foundation Models Figure 3: RATE: clinically grounded evaluation framework for volumetric radiology. (A) Comparison of RATE to existing benchmarks (VQA-RAD [24], SLAKE [25], PMC-VQA [26], OmniMedVQA [27], MMMU [28], RadLE [39], MedXpertQA [29]) along three axes: inputs, task and label source, and extensibility. RATE is the only framework that takes full-resolution volumes as input, uses clinically grounded tasks with labels derived from routine clinical practice, and can be extended to any radiology image-report dataset. (B) RATEs benchmark generation engine applies large language model to unstructured radiology reports to extract answers to set of clinically grounded queries. (C) RATE-Evals, the model evaluation engine, provides standardized evaluation protocol with per-task linear probing (RATE Predictors) for any vision model. The inputs to the engine are full-resolution medical volumes and RATE-extracted labels from corresponding reports. Together, RATE and RATE-Evals enable extensible, clinically aligned evaluation of radiology foundation models. normal clinical workflows [39]. Finally, they are not extensible, offering only fixed dataset without pathway to expand or adapt tasks [27, 39]. RATE addresses all three key limitations. Board-certified radiologists curated 366 diverse radiologic findings reflecting real-world practice across our modalities. RATE then uses open large language models to extract binary labels for each task from radiology reports, enabling scalable and clinically grounded benchmarking (Figure 3b). Full details are provided in Section A.1. Building on this framework, we introduce RATE-Eval, standardized protocol for assessing pretrained vision encoders on real-world medical imaging datasets. RATE-Evals employs the binary labels generated by RATE in linear probe setup, freezing the encoder and training single linear classifier per task over its embeddings (Figure 3c). Performance on held-out exams measures the quality and transferability of the learned representations, analogous to CLIP-style linear evaluation [40]. Additional details appear in Section A.1.2. 2.2. Pillar-0 Obtains Dominant Performance on Internal Test Sets Pillar-0 achieves dominant overall performance across all evaluated modalities in UCSF held-out test sets. We compare Pillar-0 against four competitive baselines representing the current state of medical imaging models (Figure 4a). MedGemma, Lingshu, and MedImageInsight are large 2D medical models trained on diverse imaging mixtures, but do not natively process volumetric inputs. Merlin [15] is an open-source 3D model trained on an institutional dataset of abdomen-pelvis CTs. Additional information on the baselines is provided in Section A.1.3. 6 Pillar-0: New Frontier for Radiology Foundation Models Figure 4: Pillar-0 achieves dominant performance over MedGemma, MedImageInsight, Lingshu, and Merlin on internal UCSF test sets across abdomen-pelvis CT, chest CT, head CT, and breast MRI. For each modality, Pillar-0 attains the highest average AUROC, with modality-level AUROC improvements of 7.8-15.8 points over the closest baseline. Aggregated over modalities, Pillar-0 wins on 319 of 336 findings (87.2%), winning at least 84.3% in every modality. Pillar-0: New Frontier for Radiology Foundation Models We assess Pillar-0 and all baselines using binary labels for 366 radiologist-curated findings derived through RATE, and apply the RATE-Evals framework for model comparison. The UCSF held-out test sets include 14,230 abdomen-pelvis CT, 10,646 chest CT, 4,906 head CT, and 1,585 breast MRI exams, providing diverse, clinically grounded benchmark. Full details of the evaluation protocol are provided in Section A.2. As shown in Figure 4b, Pillar-0 outperforms all baselines by large margin in each modality. Overall, Pillar-0 performs the best on 319/366 questions (87.2%), far exceeding all baselines (MedGemma, 16/366, 4.3%; MedImageInsight, 16/366, 4.3%; Lingshu, 9/366, 2.5%; Merlin, 6, 1.6%). At the modality level, Pillar-0 achieves an average AUROC computed across all questions in each modality of 86.4, 88.0, 90.1, and 82.9 for abdomen-pelvis CT, chest CT, head CT, and breast MRI respectively. In comparison, the second strongest model in each modality is MedGemma, which attains 77.9, 80.2, 80.2 and 67.1 average AUROC, meaning Pillar-0 beats the closest baseline by 7.8-15.8 points. Compared to all models, Pillar-0s question-level win rates are 84.3% for abdomenpelvis CT, 90.2% for chest CT, 93.1% for head CT, and 91.4% for breast MRI, consistently surpassing those of all baselines. No baseline exceeds 6.9% in any modality. Compared to MedGemma head-to-head, Pillar-0s question level win rates are 90.4% for abdomenpelvis CT, 92.4% for chest CT, 96.6% for head CT, and 94.3% for breast MRI. detailed question-level comparison between Pillar-0 and MedGemma is provided in Section B. Per-finding metrics can be found in Sections (abdomen-pelvis CT), (chest CT), (head CT) and (breast MRI). 2.3. Pillar-0 Outperforms Foundation Models on External Test Sets Pillar-0 demonstrates strong external generalization, outperforming all baselines evaluated on the Stanford Merlin Abdominal CT Dataset [15]. This dataset, which was previously used to develop Merlin, contains 25,494 abdomen-pelvis CTreport pairs from 18,317 patients. Using RATE, we extracted 202 clinically relevant findings from this cohort, and evaluated all models with RATE-Evals (see Section A.3 for details). Pillar-0 achieved an average AUROC of 82.2, outperforming Merlin (80.6), for which this dataset is internal, as well as MedGemma (72.6), MedImageInsight (74.9), and Lingshu (72.1)  (Table 1)  . This dataset also provides an opportunity for detailed head-to-head comparison with the Merlin model, which was trained using both radiology reports and electronic health record codes. To isolate the effect of data source, we trained Pillar-0 (Stanford Only) using the Pillar-0 recipe with the Stanford Merlin Abdominal CT dataset. Despite leveraging only text supervision, Pillar-0 (Stanford Only) still outperforms Merlin (82.2 vs 80.6), showing that the gains are attributable to improved methods. Moreover, Pillar-0 (Stanford Only) performs similarly to Pillar-0, demonstrating strong generalization. Finally, we evaluated Pillar-0 as an initialization for specialist foundation model training by constructing Pillar-0 (UCSF + Stanford), which initializes from Pillar-0, and is further pretrained on the Stanford Merlin Abdominal CT data with the Pillar-0 recipe. We find that Pillar-0 (UCSF + Stanford) substantially outperforms Merlin (84.9 vs 80.6), indicating that models built on top of Pillar-0 benefit from superior initialization. Additional details are noted in Section A.3. Table 1: Pillar-0 outperforms all baselines on external validation on the Stanford Merlin Abdominal CT Dataset. Notably, Pillar-0 outperforms Merlin, which was developed using this dataset. Pillar-0 (Stanford Only), pretrained with the Pillar-0 recipe on the Stanford data alone, also outperforms Merlin. Pillar-0 (UCSF + Stanford), which is initialized from Pillar-0 and then finetuned on the Stanford dataset, pushes performance even further, establishing best average AUROC by wide margin."
        },
        {
            "title": "Dataset",
            "content": "Average AUROC on Merlin RATE-Eval"
        },
        {
            "title": "Mixture of medical imaging\nMixture of medical imaging\nMixture of medical imaging",
            "content": "Merlin (Stanford) Pillar-0 (Stanford Only) Merlin-Abd-CT Merlin-Abd-CT Pillar-0 Pillar-0 (UCSF + Stanford) UCSF-Abd-CT UCSF-Abd-CT + Merlin-Abd-CT 72.6 74.9 72.1 80.6 82. 82.2 84.9 2.4. Pillar-0 Improves the Ability to Predict Future Cancer from Asymptomatic Screening In addition to replicating tasks captured in routine radiology reports, Pillar-0 can significantly improve performance in tasks beyond the current standard of care. Here, we study this capability in the context of lung cancer screening, where we train models to predict future cancer risk from single low-dose CT (LDCT). 8 Pillar-0: New Frontier for Radiology Foundation Models Lung cancer screening with LDCT reduces lung cancer-specific mortality by 20% among patients with history of tobacco smoking [41]. More accurate prediction of future lung cancer risk could enhance screening efficiency by personalizing follow-up intervals, maintaining long-term engagement, and identifying high-risk individuals who may not meet traditional smoking-based eligibility criteria. Sybil [31] was recently developed to predict lung cancer risk from single LDCT and has been clinically validated across multiple healthcare settings, including in patients who have never smoked [42]. We finetuned Pillar-0 on LDCTs from the National Lung Screening Trial (NLST) to develop Sybil-1.5 (Pillar-0 finetuned), new state-of-the-art model for lung cancer risk prediction. Sybil-1.5 predicts the location of suspicious lesions and outputs six annual risk scores corresponding to lung cancer diagnoses 16 years after screening (Figure 5a). We assessed performance using Unos concordance index [35] and AUROC across each year, and validated the model on the same NLST held-out test set used to evaluate Sybil (N=6,282), as well as two external cohorts: MGH (8,821 LDCTs, Protocol 2020P002652) and CGMH (12,280 LDCTs, IRB202301073B0). Additional training and evaluation details are provided in Section A.4. Across all test sets and evaluation metrics, Sybil-1.5 (Pillar-0 finetuned) consistently outperforms Sybil (Figure 5b). On the NLST held-out test set, Sybil-1.5 improves 1-year AUROC compared to Sybil from 91.5 (95% CI 87.8 to 95.2) to 94.5 (95% CI 92.0 to 96.9) (p=0.04). Sybil-1.5 also yields better generalization to external validation sets. For MGH and CGMH, the 1 year AUROC improves from 85.9 (95% CI 82.6 to 89.2) to 90.8 (95% CI 88.5 to 93.1) (p<0.001) and from 95.1 (95 % CI 91.2 to 99.0) to 96.8 (95 % CI 91.2 to 99.0) (p=0.03). We find that Sybil-1.5 performs well across race, sex, age, and smoking status, shown in detailed subgroup analyses in the supplemental materials  (Table 9)  . Figure 5: Finetuning Pillar-0 sets new state-of-the-art for future lung cancer risk prediction. (A) Illustration of Sybil-1.5 (Pillar-0 finetuned), trained on chest CTs and annotations from NLST to predict multi-year cancer risk and bounding boxes of suspicious regions. (B) Performance of Sybil and Sybil-1.5 on NLST, MGH, and CGMH cohorts, reported as 1-, 3-, and 5-year AUROC and 6-year overall concordance index. Across all datasets and time horizons, Sybil-1.5 improves risk stratification over Sybil. 2.5. Pillar-0 Reduces Downstream Training Data Needs in Brain Hemorrhage Detection by >20 Pillar-0 achieves substantial gains in sample efficiency, outperforming competitive baselines using only small fraction of the labeled data (Figure 6). We evaluated Pillar-0 and baselines on the 2019 RSNA Brain Hemorrhage Detection Challenge dataset (RSNA-2019) [32], following the protocol described in Section A.5. RSNA-2019 comprises 21,744 unique head CT exams from three institutions (Stanford University, Universidade Federal de S√£o Paulo, and Thomas Jefferson University Hospital), with neuroradiologist annotations for the presence of five intracranial hemorrhage (ICH) subtypes. In this case study, we finetuned Pillar-0 along with two general-purpose 3D backbones (Swin3D-t and 3D ResNet-18; Kinetics-400 pretraining) and three radiology-specific models (MedicalNet 3D ResNet-18 [43], RadImageNet ResNet-50 [44], and Merlin [15]) to predict the presence or absence of each of the ICH subtypes, and the presence or absence of any ICH overall. We trained models using 2.5-100% of the available training data, and report AUROC on the \"any ICH\" task as the primary metric. 9 Pillar-0: New Frontier for Radiology Foundation Models Figure 6: Pillar-0 dramatically improves data efficiency for brain hemorrhage detection on RSNA-2019. (A) AUROC for detecting any intracranial hemorrhage as function of the fraction of training exams (2.5-100%). Across all data fractions Pillar-0 outperforms radiology-pretrained models (Merlin, RadImageNet [44], MedicalNet [43]; purple) and natural image-pretrained models (Swin3D-K, RN3D-K; gray). With only small fraction of the training data, Pillar-0 matches or exceeds the best baselines trained on the full dataset. (B) Fraction of training data required for each model to reach an AUROC of 95.0; Pillar-0 reaches this threshold with 20-40 fewer samples than baselines. MedicalNet and RadImageNet do not attain this performance even with the full dataset. Across all data fractions, Pillar-0 outperforms every baseline (Figure 6a). With just 2.5% of the training dataset, Pillar-0 achieves an AUROC of 95.0, outperforming the other baselines at this data fraction by 8.0-18.1 AUROC points. Swin3D-t, the best-performing baseline, requires 50% of the samples to achieve this performance, while Merlin and ResNet3D require 75% and 100%, respectively, representing 20to 40-fold improvement in sample efficiency (Figure 6b). Using 10% of the training dataset, Pillar-0 matches or outperforms the best baselines using the full dataset, and exceeds the other baselines by 3.410.5 AUROC points at this data fraction. With just 2.5% of the training dataset, Pillar-0 comes within 1 AUROC point of the best baseline model using the full dataset (Swin3D-t). Full results across all data fractions are shown in Table 10. 2.6. Ablation Study To identify the core components driving the performance of Pillar-0, we conduct ablations along three principal axes: tokenization, model architecture, and pretraining objective. We use the Stanford Merlin CT dataset as testbed to make our ablations fully reproducible. Across these experiments, we find that Pillar-0 gains arise not from any single design choice, but from the combined effect of modality-aware tokenization, an efficient multi-scale 3D architecture, and strong asymmetric contrastive objective. Tokenization: Multi-Windowing. We evaluate our modality-native multi-window tokenization against standard minmax normalization baseline (Figure 1a). Multi-windowing substantially improves downstream performance, achieving an AUROC of 82.2 on Merlin RATE-Evals compared to 77.6 with minmax scaling. This highlights the importance of preserving subtle contrast information for CT understanding. Architecture Ablation: Multi-Scale Attention for 3D Efficiency. To assess architectural efficiency for high-resolution volumetric inputs, we compare our multi-scale Atlas backbone to standard ViT-S [45] model using identical input volumes (384384384) and patch sizes (6 6 6). Under this matched configuration, ViT-S requires approximately 38.8 seconds per sample at inference, whereas Atlas requires only 0.2 seconds, delivering 175 speedup with similar parameter counts (79M for Pillar-0 vs. 123M for ViT-S; Figure 2b). This demonstrates that multi-scale attention enables practical 3D modeling. Compared to MedGemma, MedImageInsight, and Lingshu, which process slices independently, Pillar-0 is 32, 15 and 55 faster, respectively. 10 Pillar-0: New Frontier for Radiology Foundation Models Pretraining Ablation: Text Encoder. We study the impact of encoder asymmetry in contrastive learning by pairing the vision encoder with either standard text encoder (RoBERTa-Base [38], 125M parameters) or large-scale LLM-based encoder (Qwen3 [34], 8B). Using Qwen3 yields substantially stronger correlation between contrastive training loss and downstream RATE performance (Pearson = 0.256 vs. 0.947; Figure 2c), enabling the loss to serve as reliable proxy for clinical utility. The larger text encoder also improves overall performance, increasing downstream AUROC from 76.6 to 82.2 on Merlin Rate-Evals. Together, these results highlight the importance of high-capacity text encoder in producing clinically aligned representations. 3. Discussion Foundation models have transformed diverse areas of AI, including natural language processing [46], computer vision [47], and biology [13], by offering general-purpose backbones for downstream developers. To advance the field, these tools should improve performance on broad spectrum of tasks, including those outside of their pretraining distribution. Moreover, they should reduce the required data to reach target performance level. The development of these technologies rests on rigorous evaluation benchmarks that test models on realistic inputs and tasks that closely mirror real-world use. Despite extensive recent work in radiology foundation models, leading systems neither model the rich 3D structure of volumetric medical imaging nor evaluate their performance on clinically relevant benchmarks. In this work, we present Pillar-0, radiology foundation model designed to advance comprehensive understanding of CT and MRI imaging. To support rigorous evaluation, we introduce RATE, clinically grounded framework that leverages large language models to extract structured labels for hundreds of radiologic findings from unstructured reports. RATE enables reproducible, scalable benchmarking across modalities by aligning model evaluation with radiologist-prioritized findings and diagnoses. Using RATE, we show that Pillar-0 establishes new performance frontier across abdomen-pelvis CT, chest CT, head CT, and breast MRI. We show that Pillar-0 outperforms leading 2D and 3D medical models, and that high-fidelity volumetric representations yield substantially more transferable features. Pillar-0 generalizes strongly across institutions and provides superior initialization for downstream development, outperforming Merlin on its own in-domain dataset, and exceeding its performance even when pretrained on the same data. In addition to radiologist-performed tasks, Pillar-0 enables risk modeling tasks beyond human perceptual limits. When finetuned on NLST data, Pillar-0 sets new state of the art for long-horizon lung cancer risk prediction and generalizes across external cohorts. Pillar-0 also offers marked gains in sample efficiency. In brain hemorrhage detection, it matches or exceeds the best baselines trained on the full RSNA-2019 dataset using only 10% of the training set, reducing data requirements by more than an order of magnitude. Ablations show that these gains arise from the combination of multi-window tokenization, an efficient multi-scale 3D backbone, and asymmetric contrastive pretraining with strong text encoder, which together produce predictable, clinically aligned scaling behavior. The clinical relevance of these advances is considerable. Pillar-0 innovations across tokenization, architecture, and pretraining allow it to leverage the rich 3D spatial context and intensity patterns encoded in volumetric CT and MRI. This yields higher performance on both core diagnostic tasks and tasks that exceed human perceptual capabilities, such as long-horizon cancer risk prediction. By reducing downstream training data requirements by more than an order of magnitude, Pillar-0 addresses one of the most persistent barriers in clinical AI: the scarcity of large, high-quality labeled datasets in many radiologic subspecialties. This sample efficiency makes it feasible to build accurate specialist models even in domains with limited, heterogeneous, or costly annotations. Combined with its strong external generalization and superior utility as an initialization for downstream model development, Pillar-0 is suited to serve as robust foundation for radiology AI and enable applications that were previously infeasible. By releasing all models, code, and the RATE framework, we substantially lower the computational and data barriers required to build volumetric AI systems. This democratization enables even resource-constrained groups to build high-quality models with far less data, accelerating progress across the field. While Pillar-0 substantially advances the frontier of volumetric radiology foundation models, there remain obvious opportunities for further improvement. First, our pretraining data was from single large tertiary academic medical center, using only 218, 217 CTs and MRIs in total, and we leveraged small (89M parameter) vision encoder. We expect that significantly scaling data and model capacity will unlock further model improvements. Moreover, the scanner vendors, acquisition protocols, patient demographics, and disease prevalence may not fully capture the diversity of imaging practices and patient populations seen across other healthcare systems. Larger and more diverse datasets will be important for further improving model generalizability. Second, RATE extracted binary clinical labels from radiology reports. This approach inherits the inherent limitations of report-based supervision: radiologists may omit normal or incidental findings, and textual descriptions do not always reflect the full spectrum of imaging appearance [11, 12]. Additionally, treating missing mentions as negative labels, while consistent with radiology reporting behavior, introduces potential source of label noise. RATE does not directly assess localization, segmentation, and temporal evolution, which are critical for many clinical applications. There remain opportunities to expand Pillar-0: New Frontier for Radiology Foundation Models RATE to capture other clinically grounded tasks, offering rich feedback for the development of radiology foundation models. Finally, Pillar-0 solely relies on contrastive pretraining, omitting many additional sources of supervision, including full report generation and additional clinical context. 12 Pillar-0: New Frontier for Radiology Foundation Models A. Methods A.0. PillarA.0.1. Development Dataset We identified 71,510 abdomen-pelvis CT, 107,923 chest CT, 24,042 head CT, and 14,742 breast MRI exams performed between 2001 and 2025 in adult patients at UCSF. Exams were retrieved using the institutional radiology database (mPower Clinical Analytics; Nuance Communications) and the Automated Image Retrieval platform. For the breast MRI dataset, we employed regular expressions on the series descriptions to retain T1 fat saturation, T2 fat saturation, and peak contrast-enhanced series, disambiguating between post-contrast series using acquisition time. For the CT datasets, we first excluded biopsies and non-reportable exams. To eliminate redundant series, we grouped series by acquisition time and selected the axial series with the lowest slice thickness. For exams where multiple series remain, we select series at random. The dataset was divided patient-wise into training, validation, and test splits with complete exam counts in Table 2. Tables 3, 4, 5, and 6 contain detailed breakdowns of patient demographics and scanner models for each modality. Table 2: Summary of pretraining datasets. The count of exams per split is depicted for each modality."
        },
        {
            "title": "Split",
            "content": "Abdomen-Pelvis CT"
        },
        {
            "title": "Total",
            "content": "42, 990 14, 290 14, 230 86, 411 10, 866 10, 646 14, 348 4, 788 4, 906 71, 510 107, 923 24, 11, 543 1, 614 1, 585 14, 742 155, 292 31, 558 31, 367 218, 217 Table 3: Dataset characteristics for head CT. The age value is provided as mean standard deviation. Gender is provided as percentages of the total patients (n=19,118). Manufacturer is provided as the percentage of total exams (n=24,042)."
        },
        {
            "title": "Demographics",
            "content": "Patients (n=19,118)"
        },
        {
            "title": "Age\nGender\nFemale\nMale",
            "content": "- 61.2 19.4 9, 364 9,"
        },
        {
            "title": "Manufacturer",
            "content": "Exams (n=24,042)"
        },
        {
            "title": "GE Medical Systems\nSiemens\nSiemens Healthineers\nPhilips\nSamsung\nAgfa\nTOSHIBA\nNeuroLogica",
            "content": "23, 716 229 37 24 21 12 2 1 48.98% 50.83%"
        },
        {
            "title": "Value",
            "content": "98.64% 0.95% 0.15% 0.10% 0.09% 0.05% 0.01% 0.00% 13 Pillar-0: New Frontier for Radiology Foundation Models Table 4: Dataset characteristics for abdomen-pelvis CT. The age value is provided as mean standard deviation. Gender is provided as percentages of the total patients (n=45,483). Manufacturer is provided as the percentage of total exams (n=71,510). Abdomen-Pelvis CT"
        },
        {
            "title": "Demographics",
            "content": "Patients (n=45,483)"
        },
        {
            "title": "Age\nGender\nFemale\nMale",
            "content": "- 58.8 17.1 22, 876 22, 556 50.23% 49.59%"
        },
        {
            "title": "Value",
            "content": "91.30% 4.81% 3.40% 0.38% 0.10% 0.00% 0.00%"
        },
        {
            "title": "Manufacturer",
            "content": "Exams (n=71,510) GE Medical Systems Siemens Philips Siemens Healthineers TOSHIBA Agfa AMICAS Inc. 65, 289 3, 442 2, 433 273 70 2 1 Table 5: Dataset characteristics for chest CT. The age value is provided as mean standard deviation. Gender is provided as percentages of the total patients (n=49,775). Manufacturer is provided as the percentage of total exams (n=107,923)."
        },
        {
            "title": "Demographics",
            "content": "Patients (n=49,775)"
        },
        {
            "title": "Age\nGender\nFemale\nMale",
            "content": "- 60.6 17.1 23, 661 26,"
        },
        {
            "title": "Manufacturer",
            "content": "Exams (n=107,923)"
        },
        {
            "title": "GE Medical Systems\nSiemens\nAGFA\nAGFA Healthcare\nVisage PR\nSiemens Healthineers\nPhilips\nAGFA Healthcare Informatics\nImbio\nBunkerhill\nTERARECON\nPhilips Medical Systems\nUnknown",
            "content": "90, 761 4, 773 4, 374 1, 467 488 405 159 54 37 31 3 2 5, 368 47.54% 52.37%"
        },
        {
            "title": "Value",
            "content": "84.10% 4.42% 4.05% 1.36% 0.45% 0.38% 0.15% 0.05% 0.03% 0.03% 0.00% 0.00% 4.97% 14 Pillar-0: New Frontier for Radiology Foundation Models Table 6: Dataset characteristics for breast MRI. The age value is provided as mean standard deviation. Gender is provided as percentages of the total patients (n=6,444). Manufacturer is provided as the percentage of total exams (n=14,742)."
        },
        {
            "title": "Demographics",
            "content": "Patients (n=6,444)"
        },
        {
            "title": "Age\nGender\nFemale\nMale",
            "content": "- 50.4 12.9 6,"
        },
        {
            "title": "Manufacturer",
            "content": "Exams (n=14,742)"
        },
        {
            "title": "GE Medical Systems\nSiemens\nInvivo\nAGFA\nVisage PR\nSiemens Healthineers\nSiemens\nVisage Imaging\nAGFA Healthcare\nUnknown",
            "content": "9, 551 4, 502 137 119 22 17 9 6 2 377 99.60% 0.28%"
        },
        {
            "title": "Value",
            "content": "64.79% 30.54% 0.93% 0.81% 0.15% 0.12% 0.06% 0.04% 0.01% 2.56% A.0.2. Pillar-0 Training Recipe Architecture. The backbone of Pillar-0 is the Atlas[36] vision encoder, which leverages Multi-Scale Attention to enable efficient long-context image modeling. Pillar-0 is trained on full-resolution medical volumes and leverages fine-scale patch sizes  (Table 7)  . This configuration ensures anatomical coverage while maintaining sensitivity to small findings: 63 patches preserve fine lesions (on the order of few millimeters), while our input resolution is large enough to capture the entire field of view. As shown in Section 2.6, training at this resolution is untenable for other transformer-based architectures. The Atlas backbone replaces ùëÇ(ùëÅ 2) self-attention with an ùëÇ(ùëÅ log ùëÅ ) multi-scale mechanism, making high-resolution full-volume 3D training computationally feasible. We use 3-stage Atlas-S configuration, with 2 blocks in each of the first 2 stages, and 8 blocks in the last stage. Table 7: Patch size and resolution by imaging modality. Our inputs range from 32k-256k tokens per volume after patchification, necessitating the use of an efficient architecture for long-context modeling (Atlas)."
        },
        {
            "title": "Patch size",
            "content": "Resolution (HWD)"
        },
        {
            "title": "Tokens per volume",
            "content": "Abdomen-pelvis CT Chest CT Head CT Breast MRI 6 6 6 8 8 4 8 8 4 12 12 6 384 384 384 256 256 256 256 256 128 384 384 192 256k 64k 32k 32k Radiology-specific tokenizer. CT volumes are calibrated in physically meaningful Hounsfield units (HU), spanning wide dynamic range (roughly 1000 to +3000 HU). Critical structures occupy subspaces of this dynamic range, but standard display technologies make it impossible to render all anatomically relevant structures with optimal contrast at once. Rather than single global min-max normalization or single window, we emulate radiologist practice via modality-specific multi-windowing. For each CT modality, we define small set of anatomically motivated window presets (e.g., lung, soft tissue, mediastinum, bone). Each preset is applied independently to the raw HU volume, clipped to the specified window width and level, linearly rescaled to [0, 1], and treated as separate channel. The model thus receives multi-channel view in which different channels emphasize different anatomical structures, analogous to radiologist scrolling through window presets at the workstation. Pillar-0: New Frontier for Radiology Foundation Models Breast MRI lacks standardized physical unit and is typically acquired as multiple complementary series (e.g., T1-weighted, T2-weighted, diffusion-weighted). For MRI we therefore adopt an adaptive high-contrast windowing strategy. For each series, we compute foreground intensity histogram and set the window to span the 1st to 99th percentile of this distribution, followed by linear rescaling to [0, 1]. This yields robust, data-driven normalization that maximizes contrast for relevant tissues across heterogeneous scanners and protocols. Section 2.6 shows that this modality-specific, multi-window representation substantially outperforms simple min-max baseline. The code for this functionality is included in our vision engine, with details in Section A.6. For the inputs to the text encoder, we use radiology reports processed by the RATE Report preprocessing and text encoder. pipeline (Section A.1). We first remove the comparisons section, which describes longitudinal changes relative to prior exams and is not directly observable from single study. We then extract the findings section verbatim, isolating the radiologists image-centric description while excluding clinical history, acquisition parameters, and administrative metadata. The resulting findings text serves as clean, clinically grounded caption for vision-language pretraining. Each findings section is tokenized and passed through the frozen Qwen3-Embedding-8B model; the pooled text representation is then projected into the shared embedding space by learned linear layer. The first stage in our pretraining pipeline is supervised classification on ImageNet-1K upsampled to Pretraining pipeline. 1, 024 1, 024 resolution. Concretely, we optimize cross-entropy loss with the AdamW optimizer [48], global batch size of 2,048, and weight decay 0.24, using cosine learning rate decay and linear warmup schedule. Training is performed for 320 epochs (30 epoch warmup) on 32H100 GPUs, reaching 80.1% top-1 validation accuracy after approximately 33.5 hours. The resulting checkpoint serves as the initialization for all subsequent vision-language pretraining runs. Starting from the ImageNet-pretrained weights, we perform single-modality vision-language pretraining separately for abdomenpelvis CT, chest CT, head CT, and breast MRI. We find empirically that single-modality pretraining is easier to tune compared to cross-modality pretraining. For each modality, we instantiate vision encoder and small projection head. The text encoder is frozen Qwen3-Embedding-8B large language model [34], which is substantially larger than the vision encoder (8B vs 79M). We refer to this training setup as asymmetric contrastive learning, in which only the vision encoder and the projection layers are updated. We utilize contrastive learning objective for vision-language pretraining, analogous to CLIP, which encourages matched volumereport pairs to have high similarity while treating all other combinations in the batch as negatives. Contrastive learning benefits from large batches, but high-resolution 3D volumes quickly exhaust GPU memory. To obtain an effective global batch size of up to 256, far exceeding what fits on single 8H100 node, we accumulate features from 8 mini-batches to compose the full similarity matrix, and use gradient accumulation across mini-batches, considering examples from all other mini-batches as negatives. In particular, we use mini-batch size of 32 with AdamW optimizer and maximum learning rate of 2.5 104 , and cosine learning rate schedule with linear warm-up of 200 steps. Model release. We release all modality-specific vision-language pretrained checkpoints for Pillar-0 via HuggingFace at https://huggingface.co/collections/YalaLab/pillar-0. In addition, we release our vision-language pretraining code at https: //github.com/YalaLab/pillar-pretrain. A.1. RATE: Clinically Grounded Evaluation Framework A.1.1. RATE We developed Radiology Text Engine (RATE) to convert unstructured radiology reports into structured artifacts which can be used for evaluating vision encoders on medical volumes. RATE takes radiology report as input and produces binary clinical labels corresponding to an expert-curated set of Yes/No questions capturing clinically relevant findings for each exam. key feature of this system is its extensibility: new modalities and question sets can be incorporated by providing additional radiologist-specified queries. RATE operates through pipeline built on single large language model (Qwen3-30B-A3B-FP8) using specific prompts. Using this setup, the system identifies binary Yes/No answers to expert-curated questions directly from the report text, generating structured labels suitable for evaluating clinical finding identification. The framework also produces records that facilitate quality control, allowing researchers to review and validate the extracted labels. Pillar-0: New Frontier for Radiology Foundation Models Additionally, the system enables the extraction of clinically grounded captions for vision-language pretraining. To do this, it removes the \"comparisons\" section, which describes longitudinal changes, and then extracts the \"findings\" section verbatim. We release the full RATE implementation, including prompts, templates, and evaluation utilities at https://github.com/YalaLab/ rate. A.1.2. Using RATE to evaluate any vision encoder RATE-Evals is modular framework to systematically assess the performance of any vision encoder on medical imaging tasks. It is designed with extensibility in mind: we provide simple templates for datasets and models that can be easily modified to support additional use-cases. We release the evaluation code at https://github.com/YalaLab/rate-evals. We evaluate models using linear probing on the embeddings to predict the binary clinical labels generated by RATE. This approach measures how well frozen vision encoders support curated clinical tasks, serving as direct indicator of the quality and transferability of the extracted representations. For each model, image embeddings are extracted from the frozen encoder and used to train lightweight linear classifier that predicts the binary clinical labels defined by RATE. This is formulated as multi-label classification problem in which each question is treated as an independent binary target. Optimization is performed with Adam [49] optimizer with learning rate 103 , batch size 8,192, and no weight decay for 1,000 epochs using class-balanced binary cross-entropy loss to correct for label imbalance. Performance is summarized as per-question AUROC. Because radiology reports often omit the mention of normal or absent findings, missing answers (i.e., answers to the queries are not mentioned in the reports) are treated as negative by default. Alternatively, the system supports masking mode that restricts training of the linear probes and analysis to explicitly labeled samples. RATE-Evals has registry-based design that allows new datasets and models to be added as needed. It includes built-in support for the Merlin Abdominal CT Dataset [15], as well as lightweight synthetic dataset for rapid iteration. It also provides implementations for Pillar-0, and all external baselines evaluated in this work (Section A.1.3). A.1.3. Baselines MedGemma. Multimodal 4B/27B models with SigLIP-400M [50] vision encoder paired with an LLM; also 27B text-only variant. Training data is comprised of medical imaging datasets including MIMIC-CXR (chest X-rays), proprietary de-identified CT and MRI exams (represented as sets of 2D slices), and additional data from histopathology, dermatology, and ophthalmology. Training follows multi-stage procedure beginning with adaptation of the vision encoder on medical imagetext pairs, followed by multimodal pre-training and post-training. In our implementation, each volume is first min-max normalized. Following MedGemmas documentation, we resize each slice to 896 896 and pass it through the vision encoder, treating the depth dimension as the batch dimension. We then extract features from the vision encoder, and apply average pooling to produce single representation per volume. MedImageInsight. DaViT-based vision transformer [51] trained on public and proprietary datasets including chest X-ray, CT, MRI, ultrasound, histopathology and additional domains. The model uses CLIP-style contrastive learning with the UniCL objective [52] to align paired image and text embeddings. Following MedImageInsights official implementation, we perform median pooling from the spatial features from the vision tower to produce single representation per volume. Merlin. 3D ResNet image encoder coupled with transformer-based text encoder. The training data consists of abdominal CT exams linked to EHR diagnosis labels and associated radiology reports. The training objective combines binary cross-entropy loss to predict diagnosis codes with an InfoNCE loss to align 3D CT volumes with the unstructured report text. Our implementation is based on Merlins official implementation, and was checked with Merlin author Ashwin Kumar. Built on the Qwen2.5-VL backbone [53] with ViT-based vision encoder [45]. The training data includes CT, MRI, Lingshu. ultrasound, histopathology, and other medical modalities. For volumetric modalities (e.g., CT/MRI), each 2D slice is processed independently. Training follows four-stage pipeline consisting of: (1) medical shallow alignment, (2) deep alignment, (3) 17 Pillar-0: New Frontier for Radiology Foundation Models instruction tuning, and optionally (4) reinforcement learning. Our reproduction uses https://huggingface.co/lingshu-medicalmllm/Lingshu-7B processes each slice by resizing to 896 896, applying min-max normalization, extracting encoder features, and average pooling to produce single representation per volume. The implementations of all our baseline models are in RATE-Evals. A.2. Internal Validation For internal evaluation, we used the UCSF held-out test set described in Section A.0.1 comprised of 14,230 abdomen-pelvis CT, 4,906 head CT, 10,646 chest CT, and 1,585 breast MRI. Using RATE (Section A.1), 366 radiologist-curated clinical findings were extracted: 210 abdomen-pelvis CT, 29 head CT, 92 chest CT, and 35 breast MRI findings. We then applied the RATE-Evals (Section A.1) protocol to compute performance for Pillar-0 and for all baseline models described in Section A.1.3. To assess label quality, standardized quality control was performed on the binary clinical labels extracted from each modality. Board-certified radiologists manually reviewed RATE outputs for 20 randomly sampled reports per modality (80 total) and adjudicated the corresponding Yes/No findings. We observed 100% agreement between RATE-derived labels and radiologist adjudications in this sample, supporting the use of RATE labels as high-fidelity evaluation data. Questions with no positive examples in the test set were excluded from performance estimates. A.3. External Validation For external evaluation, we used the Merlin Abdominal CT Dataset [15], which comprises 25,494 abdomen-pelvis CTreport pairs from 18,317 patients. The cohort consists of exams acquired in the Stanford Hospital Emergency Department between 20122018, identified using abdomen-pelvis CT CPT codes (7219274178) via the STARR tool. For each exam, the DICOM series with the largest slice count was selected and converted to compressed, de-identified NIf TI volume. As defined in the original dataset splits, the held-out test set used for evaluation contains 5,137 CT exams. We applied RATE using the same abdomen-pelvis CT query set used in our internal evaluation (Section A.2). After excluding questions with no positive instances in the Merlin test split, this yielded 202 radiologist-curated clinical findings. For this analysis, we also pretrained two variants of Pillar-0. The first followed the complete Pillar-0 pretraining recipe (Section A.0.2) but used only the Merlin training corpus to isolate the effect of data source. The second began from the pretrained Pillar-0 checkpoint (Section A.0.2) and underwent an additional 14 epochs of contrastive pretraining on the Merlin dataset (early stopping in 50-epoch run) to assess how much performance can be gained by building on Pillar-0. We evaluated all baselines, Pillar-0, and both Pillar-0 variants using RATE-Evals (Section A.1.2). A.4. Lung Cancer Case Study We developed Sybil-1.5 by finetuning Pillar-0 on the same data used to develop Sybil[31]. We applied for and were granted access to the radiologic and clinical data from sample of 15,000 National Lung Screening Trial (NLST) [41] participants in the LDCT arm, including all lung cancers in that arm. All NLST participants signed an institutional review board (IRB)approved informed consent form. Following Sybils data and image suitability protocol, we used dataset of 28,162 LDCTs in the training set, 6,839 LDCTs in the development set, and 6,282 LDCTs in the test set, with 1,444 (5.1%), 337 (4.9%), and 299 (4.8%) positive LDCTs, corresponding to lung cancers diagnosed over the subsequent 6 years, respectively. Following IRB-approved protocols, we also used 8,821 LDCTs from MGH (Protocol 2020P002652), including 169 (1.9%) confirmed cancers. From CGMH (IRB202301073B0), we used 12,280 LDCTs from CGMH, including 101 (0.8%) cancers. CT volumes were cropped to 256 256 256 and multi-windowing with 11 windows was used. We built upon the Pillar-0 vision encoder by adding two components: (i) cumulative probability layer that outputs year-by-year cancer risk and (ii) DETR-based head for bounding box prediction [54]. For bounding box prediction, all multi-scale vision features are interpolated to the spatial resolution of the finest feature map and concatenated along the channel dimension. For risk prediction, attention pooling is applied to the finest-scale features to produce global representation. The entire model was then trained end-to-end using the original Sybil survival and attention regularization losses [31] combined with DETRs bounding-box regression and matching losses [54]. We release our complete code for finetuning Pillar-0 for flexible downstream use-cases at https://github.com/YalaLab/ pillar-finetune. 18 Pillar-0: New Frontier for Radiology Foundation Models A.5. Sample Efficiency Case Study We used 21,744 unique exams from the published RSNA-2019 training set, and further split patient-wise into 15,166, 3,261, and 3,317 exams for training, validation, and test, respectively. All models were trained up to 10,000 steps, or until performance plateaued (defined as no improvement in validation metric after 1,000 iterations), and the peak validation set performance was recorded. We observed sufficient convergence within this budget for all models. We used fixed learning rate of 1 105 with AdamW optimizer, batch size of 8, gradient accumulation over 4 batches, and applied random rotations to the training set for all experiments. All models used identical CT preprocessing. CTs were cropped or padded to 256 256 128, and we used multi-windowing to generate an 11-channel volume from each CT. For the baseline models, we prefixed linear projection to transform the input to the expected number of channels for that model, implicitly allowing each model to choose its windowing strategy. We used the official PyTorch implementations for Swin3D-t and 3D ResNet-18. For MedicalNet and RadImageNet, we mapped the publicly available weights to the PyTorch ResNet implementations. For Merlin, we used an adapted version of the official implementation. We applied attention pooling to the feature maps to get an output vector for classification for all models except Merlin, which outputs pooled feature vector directly. For RadImageNet, which is 2D model, we applied the encoder to each slice, concatenated the per-slice feature maps, and then applied attention pooling. A.6. RAVE: Unified, Efficient Radiology Data Processing We introduce Radiology Vision Engine (RAVE), framework that unifies data compression with standardized preprocessing to make training on large-scale medical volume datasets feasible under hardware constraints. Our datasets comprise millions of DICOM slices (tens of TBs) which are prohibitively large to store on local NVMe storage. RAVE converts DICOM series and NIf TI volumes into compact formats using High Efficiency Video Coding (HEVC), which exploits slice-to-slice redundancy to achieve high compression ratios while preserving detail. This compression enables us to keep the entire training corpus in limited NVMe capacity, improving training efficiency. In addition to compression, RAVE provides preprocessing framework supporting isotropic resampling, spatial normalization to fixed dimensions, and multi-windowing, yielding standardized, GPU-ready tensors. Full details can be found in our open-source code at https://github.com/YalaLab/rave. Table 8: Suite of released open-source tools"
        },
        {
            "title": "Link",
            "content": "Vision-language pretraining checkpoints for Pillar-0 Finetuning Pillar-0 code Vision-language pretraining code RATE RATE-Evals RAVE https://huggingface.co/collections/YalaLab/pillar-0 https://github.com/YalaLab/pillar-finetune https://github.com/YalaLab/pillar-pretrain https://github.com/YalaLab/rate https://github.com/YalaLab/rate-evals https://github.com/YalaLab/rave 19 Pillar-0: New Frontier for Radiology Foundation Models"
        },
        {
            "title": "References",
            "content": "[1] U.s. diagnostic imaging services market size, share & industry analysis, by procedure (ct, mri, x-ray, ultrasound, and others), by application (cardiology, neurology, oncology, orthopedics, gynecology, and others), by payor (public health insurance and private health insurance/out of pocket), and by setting (hospital in-patient, hospital outpatient (hopd), freestanding imaging centers, and others), and country forecast, 2025-2032 (2025). [2] Global medical imaging market projected to reach $82.6 billion by 2029, according to bcc research (2025). [3] The radiologist shortage: Rising demand, limited supply, strategic response (2025). [4] Rimmer, A. Radiologist shortage leaves patient care at risk, warns royal college. BMJ (2017). [5] Jing, A. B., Garg, N., Zhang, J. & Brown, J. J. Ai solutions to the radiology workforce shortage. npj Health Systems (2025). [6] Mirak, S. A., Tirumani, S. H., Ramaiya, N. & Mohamed, I. The growing nationwide radiologist shortage: Current opportunities and ongoing challenges for international medical graduate radiologists. RSNA (2025). [7] Hendrix, W. et al. Deep learning for the detection of benign and malignant pulmonary nodules in non-screening chest ct scans. communications medicine (2023). [8] Fang, Z. et al. Automated real-time assessment of intracranial hemorrhage detection ai using an ensembled monitoring model (emm). npj digital medicine (2025). [9] Dogra, S., Zhang, X., Silva, E. & Rajpurkar, P. The financial, operational, and clinical advantages of generalist radiology ai. RSNA (2025). [10] Jones, C. M. et al. Assessment of the effect of comprehensive chest radiograph deep learning model on radiologist reports and patient outcomes: real-world observational study. BMJ Open (2021). [11] Lumbreras, B., Donat, L. & Hern√°ndez-Aguado, I. Incidental findings in imaging diagnostic tests: systematic review. BJR (2010). [12] Berland, L. L. et al. Managing incidental findings on abdominal ct: White paper of the acr incidental findings committee. Journal of the American College of Radiology (2010). [13] Brixi, G. et al. Genome modeling and design across all domains of life with evo 2 (2025). URL https://www.biorxiv.org/ content/10.1101/2025.02.18.638918v1. [14] Wang, J. et al. Omnivl:one foundation model for image-language and video-language tasks (2022). URL https://arxiv. org/abs/2209.07526. 2209.07526. [15] Blankemeier, L. et al. Merlin: vision language foundation model for 3d computed tomography. Research Square rs (2024). [16] Wu, C., Zhang, X., Zhang, Y., Wang, Y. & Xie, W. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data (2023). URL https://arxiv.org/abs/2308.02463. 2308.02463. [17] Sellergren, A. et al. Medgemma technical report. arXiv preprint arXiv:2507.05201 (2025). [18] Codella, N. C. et al. Medimageinsight: An open-source embedding model for general domain medical imaging. arXiv preprint arXiv:2410.06542 (2024). [19] Xu, W. et al. Lingshu: generalist foundation model for unified multimodal medical understanding and reasoning. arXiv preprint arXiv:2506.07044 (2025). [20] Zhang, S. et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs (2025). URL https://arxiv.org/abs/2303.00915. 2303.00915. [21] Deng, J. et al. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248255 (2009). 20 Pillar-0: New Frontier for Radiology Foundation Models [22] Bae, K. T., Mody, G. N., Balfe, D. M. & Charles F. Hildebolt, P., DDS. Ct depiction of pulmonary emboli: Display window settings. RSNA (2005). [23] Pellakuru, S. R. et al. Role of windowing image technique to decipher soft tissue pathologies. MDPI (2025). [24] Lau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. dataset of clinically generated visual questions and answers about radiology images. Scientific data 5, 110 (2018). [25] Liu, B. et al. Slake: semantically-labeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), 16501654 (IEEE, 2021). [26] Zhang, X. et al. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415 (2023). [27] Hu, Y. et al. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2217022183 (2024). [28] Yue, X. et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 95569567 (2024). [29] Zuo, Y. et al. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362 (2025). [30] Hirsch, L. et al. High-performance open-source ai for breast cancer detection and localization in mri. Radiology: Artificial Intelligence (2025). [31] Mikhael, P. Sybil: validated deep learning model to predict future lung cancer risk from single low-dose chest computed tomography. Journal of Clinical Oncology (2023). [32] Anouk Stein, M. et al. Rsna intracranial hemorrhage detection. https://kaggle.com/competitions/ rsna-intracranial-hemorrhage-detection (2019). Kaggle. [33] Yang, Y.-Q. et al. Swin3d: pretrained transformer backbone for 3d indoor scene understanding (2023). URL https: //arxiv.org/abs/2304.06906. 2304.06906. [34] Zhang, Y. et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176 (2025). [35] Uno, H., Cai, T., Pencina, M. J., DAgostino, R. B. & Wei, L. J. On the c-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in Medicine (2011). [36] Agrawal, K. K. et al. Atlas: Multi-scale attention improves long context image modeling (2025). URL https://arxiv.org/ abs/2503.12355. 2503.12355. [37] Radford, A. et al. Learning transferable visual models from natural language supervision (2021). URL https://arxiv.org/ abs/2103.00020. 2103.00020. [38] Liu, Y. et al. Roberta: robustly optimized bert pretraining approach (2019). URL https://arxiv.org/abs/1907.11692. 1907.11692. [39] Datta, S. et al. Radiologys last exam (radle): Benchmarking frontier multimodal ai against human experts and taxonomy of visual reasoning errors in radiology. arXiv preprint arXiv:2509.25559 (2025). [40] Radford, A. et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, 87488763 (PmLR, 2021). [41] Team, N. L. S. T. R. Reduced lung-cancer mortality with low-dose computed tomographic screening. New England Journal of Medicine 365, 395409 (2011). [42] Lee, J., Chae, K. & Lu, M. External testing of deep learning model for lung cancer risk from low-dose chest ct. Radiology (2025). Pillar-0: New Frontier for Radiology Foundation Models [43] Chen, S., Ma, K. & Zheng, Y. Med3d: Transfer learning for 3d medical image analysis. arXiv preprint arXiv:1904.00625 (2019). [44] Mei, X. et al. Radimagenet: An open radiologic deep learning research dataset for effective transfer learning. Radiology: Artificial Intelligence (2022). [45] Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020). [46] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding (2019). URL https://arxiv.org/abs/1810.04805. 1810.04805. [47] LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature (2015). [48] Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [49] Kingma, D. P. & Ba, J. Adam: method for stochastic optimization (2017). URL https://arxiv.org/abs/1412.6980. 1412.6980. [50] Zhai, X., Mustafa, B., Kolesnikov, A. & Beyer, L. Sigmoid loss for language image pre-training (2023). URL https: //arxiv.org/abs/2303.15343. 2303.15343. [51] Ding, M. et al. Davit: Dual attention vision transformers. In European conference on computer vision, 7492 (Springer, 2022). [52] Yang, J. et al. Unified contrastive learning in image-text-label space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1916319173 (2022). [53] Bai, S. et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025). [54] Carion, N. et al. End-to-end object detection with transformers. In European conference on computer vision, 213229 (Springer, 2020). Pillar-0: New Frontier for Radiology Foundation Models A. Sybil vs Sybil 1.5 Table 9: Sybil-1.5 performance on held-out NLST test set for demographic and behavioral subgroups. AUROC at 1, 3, 5 years and C-index values are followed by 95% confidence intervals, which are capped at 100."
        },
        {
            "title": "Age",
            "content": "50-60 60-"
        },
        {
            "title": "Smoking status",
            "content": "1-yr AUC 3-yr AUC 5-yr AUC C-index 96.0 (93.3, 99.6) 95.9 (93.3, 99.3) 82.0 (73.0, 92.3) 84.8 (80.0, 89.8) 77.1 (68.8, 86.1) 80.7 (76.1, 85.5) 77.0 (69.1, 85.8) 79.9 (75.5, 84.9) 96.2 (94.2, 99.2) 91.6 (86.3, 98.4) 82.1 (76.3, 88.5) 85.7 (80.4, 91.6) 75.6 (69.8, 81.7) 82.6 (77.4, 88.0) 75.0 (69.4, 81.0) 82.2 (77.3, 87.4) 94.2 (91.6, 97.4) 99.5 (98.9, 100.0) 97.8 (95.7, 100.0) 82.9 (78.5, 88.0) 98.6 (97.1, 100.0) 79.4 (58.8, 100.0) 77.6 (73.0, 82.7) 96.4 (92.8, 100.0) 73.3 (54.4, 96.8) 77.0 (72.6, 81.6) 96.0 (91.9, 100.0) 71.9 (55.0, 94.3)"
        },
        {
            "title": "Current smoker\nNot current smoker",
            "content": "94.2 (90.5, 99.5) 95.0 (92.0, 98.8) 83.0 (77.2, 88.9) 83.4 (77.0, 90.9) 75.9 (70.1, 81.9) 79.5 (72.9, 87.0) 75.0 (69.5, 80.8) 79.2 (72.6, 86.3) Smoking duration 40 pack-years > 40 pack-years 96.4 (93.9, 100.0) 92.6 (88.6, 97.8) 82.4 (74.5, 91.2) 82.1 (77.1, 87.4) 79.2 (71.1, 87.7) 74.5 (69.1, 80.0) 79.2 (71.4, 87.9) 73.6 (68.3, 79.0) Table 10: Sample efficiency results on the RSNA-2019 dataset, in terms of validation set AUROC across training data fractions."
        },
        {
            "title": "Model",
            "content": "Pillar-0 Swin3D-t Merlin 3D ResNet-18 RadImageNet MedicalNet 2.5 95.0 85.6 87.0 84.3 82.5 76.9 Training data (%) 5 25 50 75 100 95.5 90.6 90.6 89.1 85.3 83.1 95.9 92.2 91.7 89.6 87.9 86. 96.5 94.1 94.5 91.9 87.9 89.2 96.8 95.3 94.8 93.4 91.2 90.4 96.7 95.7 95.2 94.7 91.9 90.5 96.7 95.9 95.6 95.1 92.1 90.4 23 Pillar-0: New Frontier for Radiology Foundation Models B. Comparing Pillar-0 to MedGemma Figure 7: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 1/7. 24 Pillar-0: New Frontier for Radiology Foundation Models Figure 8: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 2/7. Pillar-0: New Frontier for Radiology Foundation Models Figure 9: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 3/7. 26 Pillar-0: New Frontier for Radiology Foundation Models Figure 10: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 4/7. Pillar-0: New Frontier for Radiology Foundation Models Figure 11: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 5/7. 28 Pillar-0: New Frontier for Radiology Foundation Models Figure 12: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 6/7. Pillar-0: New Frontier for Radiology Foundation Models Figure 13: Pillar-0 vs MedGemma head-to-head on all UCSF Abdomen-Pelvis CT RATE-Evals tasks. Pillar-0 wins on 190/210 (90.5%, green bars); MedGemma wins on 20/210 (9.5%, red bars). Part 7/7. 30 Pillar-0: New Frontier for Radiology Foundation Models Figure 14: Pillar-0 vs MedGemma head-to-head on all UCSF Chest CT RATE-Evals tasks. Pillar-0 wins on 85/92 (92.4%, green bars); MedGemma wins on 7/92 (7.6%, red bars). Part 1/3. Pillar-0: New Frontier for Radiology Foundation Models Figure 15: Pillar-0 vs MedGemma head-to-head on all UCSF Chest CT RATE-Evals tasks. Pillar-0 wins on 85/92 (92.4%, green bars); MedGemma wins on 7/92 (7.6%, red bars). Part 2/3. 32 Pillar-0: New Frontier for Radiology Foundation Models Figure 16: Pillar-0 vs MedGemma head-to-head on all UCSF Chest CT RATE-Evals tasks. Pillar-0 wins on 85/92 (92.4%, green bars); MedGemma wins on 7/92 (7.6%, red bars). Part 3/3. Pillar-0: New Frontier for Radiology Foundation Models Figure 17: Pillar-0 vs MedGemma head-to-head on all UCSF Head CT RATE-Evals tasks. Pillar-0 wins on 28/29 (96.6%, green bars); MedGemma wins on 1/29 (3.4%, red bars). 34 Pillar-0: New Frontier for Radiology Foundation Models Figure 18: Pillar-0 vs MedGemma head-to-head on all UCSF Breast MRI RATE-Evals tasks. Pillar-0 wins on 33/35 (94.3%, green bars); MedGemma wins on 2/35 (5.7%, red bars). Pillar-0: New Frontier for Radiology Foundation Models C. Performance on full set of RATE-Evals tasks on UCSF Abdomen-Pelvis CT test set Table 11: Model Performance (AUC 100) for UCSF Abdomen-Pelvis CT Finding Biliary findings Are there any biliary cystadenomas? Are there any biliary hamartomas? Are there any biliary stents? Are there any choledochal cysts? Are there any gallbladder polyps? Are there any gallbladder stones or cholelithiasis? Is the gallbladder surgically absent? Is there gallbladder adenomyomatosis? Is there gallbladder cancer? Is there gallbladder wall thickening? Is there porcelain gallbladder? Bone/Fracture findings Are there any femoral fractures? Are there any fractures (general)? Are there any osteolytic lesions? Are there any rib fractures? Are there any spinal fractures? Cardiac findings Is there any coronary atherosclerosis? Is there cardiomegaly? Chest/Lung findings Are there any lung masses or nodules? Is there any atelectasis in the visible lung? Is there any free air or pneumoperitoneum? Is there pneumobilia? Cyst findings Are there any Bartholin gland cysts? Are there any Gartner duct cysts? Are there any Nabothian cysts? Are there any enteric duplication cysts? Are there any epithelial cysts? Are there any mucinous cystic neoplasms? Are there any serous cystic neoplasms? Is there acute cholecystitis with rupture? Is there acute cholecystitis? Is there emphysematous cholecystitis? Device/Procedural findings Are there any Foley catheters? Are there any central venous catheters? Are there any drainage catheters? Are there any extracorporeal membrane oxygenation (ECMO) devices? Are there any gastrostomy tubes? Are there any post-nephrectomy changes? Gastrointestinal findings Are there any colonic carcinomas? Are there any duodenal carcinomas? Are there any esophageal carcinomas? Are there any esophageal or gastric varices? Are there any gastric carcinomas? LingShu MedGemma MedImageInsight Merlin Pillar-0 Model AUC (x 100) NA 71.2 83.1 29.8 67.2 64.2 71.7 68.8 77.1 69.9 65. 84.8 73.0 71.5 72.9 72.6 83.5 81.7 68.7 78.7 85.7 76.8 65.1 NA 74.7 76.6 60.8 71.3 74.8 69.2 77.0 70.0 90.2 84.7 86.4 96.5 89.2 72. 65.8 63.0 78.4 84.3 74.9 NA 57.2 92.2 29.2 68.0 66.1 77.0 68.1 85.6 73.6 67.1 86.7 74.0 76.0 74.4 74.6 86.0 86.6 72.0 80.8 89.4 84. 77.3 NA 71.6 49.1 63.5 71.9 77.9 71.2 78.2 80.8 92.3 86.6 90.2 97.6 92.2 82.7 73.8 91.9 89.9 88.9 79.8 NA 60.4 85.8 9.2 66.9 68.3 85.0 69.9 82.2 75.5 54. 84.0 72.1 72.4 73.4 72.1 84.8 83.7 71.2 80.2 90.7 83.0 77.0 NA 70.5 59.4 63.1 72.8 66.7 82.6 86.0 75.6 91.9 84.3 88.1 98.2 90.5 79. 77.2 88.9 76.9 87.9 78.0 NA 52.7 92.3 22.6 62.9 66.0 78.4 64.9 79.5 72.9 61.7 85.3 71.5 70.6 72.6 70.6 84.3 85.4 69.6 77.9 87.2 84. 70.6 NA 69.7 52.3 60.0 64.2 57.6 56.9 74.9 84.4 91.4 84.3 87.0 99.4 88.0 73.2 71.4 83.3 91.5 90.6 76.5 NA 74.4 99.0 25.6 72.1 90.7 97.4 78.4 96.8 87.4 86. 88.6 79.2 83.3 77.8 83.1 90.4 91.8 76.4 82.5 92.9 98.7 83.6 NA 81.8 50.7 75.3 75.0 88.1 93.1 93.3 97.8 96.7 90.4 94.3 99.3 96.6 95. 87.9 97.2 88.2 94.6 84."
        },
        {
            "title": "Continued on next page",
            "content": "36 Pillar-0: New Frontier for Radiology Foundation Models Finding LingShu MedGemma MedImageInsight Merlin Table 11 Model Performance (AUC 100) for UCSF Abd CT (continued) Model AUC (x 100) Are there any gastric foreign bodies (not related to medical devices or post-surgical changes)? Are there any gastrointestinal stromal tumors (GISTs)? Are there any nasogastric tubes? Is bowel obstruction present? Is there surgical gastric conduit? Is there achalasia or scleroderma suspected based on gastrointestinal imaging findings? Is there bowel obstruction? Is there gastric volvulus? Is there gastrointestinal lymphoma? Is there inflammatory bowel disease? Is there large bowel obstruction? Is there pneumatosis intestinalis? Is there small bowel obstruction? Kidney-related findings Are there any adrenal adenomas? Are there any adrenal hemorrhages? Are there any adrenal infarcts? Are there any adrenal masses? Are there any adrenal myelolipomas? Are there any complex renal cysts? Are there any percutaneous nephrostomy tubes/catheters? Are there any post-renal transplantation changes? Are there any renal abscesses? Are there any renal cell carcinomas? Are there any renal hypodensities? Are there any renal infarcts? Are there any renal lacerations? Are there any renal stones / nephrolithiasis? Are there any simple renal cysts? ureteral Are double-J there any catheters/stents? Are there any ureteral stones? Is there adrenal hyperplasia? Is there horseshoe kidney? Is there hydronephrosis? Is there nephrocalcinosis? Is there polycystic kidney disease? Is there renal lymphomas? Liver-related findings Are there any hepatic abscesses? Are there any hepatic adenomas? Are there any hepatic changes post-resection? changes postAre there any hepatic transplantation? Are there any hepatic cysts? Are there any hepatic focal nodular hyperplasias? Are there any hepatic hemangiomas? Are there any hepatic infarcts? Are there any hepatic masses? Are there any hepatic metastases? Are there any hepatic regenerative nodules? Are there any intra-hepatic stones? Are there any liver lacerations? 52.3 68.5 92.3 88.0 71.2 82. 88.4 93.4 81.9 83.9 89.4 87.7 87.8 68.4 60.7 75.3 63.2 71.4 64.6 88.7 87.0 61.4 70.9 62.9 76.6 NA 67.6 66.7 91.0 85.0 57.6 67.7 70.6 75.9 62.5 NA"
        },
        {
            "title": "77.1\nNA\n70.0\n83.1",
            "content": "63.0 78.0 62.3 71.8 70.0 74.4 70.6 73.6 89.7 48.9 63.9 94.4 92.5 77.3 77.4 92.9 97.3 70.5 87.0 93.7 91.4 92. 68.5 49.4 86.6 64.8 74.7 66.9 93.1 93.5 52.0 77.3 64.7 83.0 NA 70.0 67.9 98.4 87.4 59.3 73.9 76.4 77.0 74.2 NA"
        },
        {
            "title": "81.7\nNA\n77.9\n91.8",
            "content": "65.6 79.6 64.6 77.0 74.4 81.2 64.3 93.5 84.8 42.5 75.0 91.2 93.2 78.7 83.2 93.1 85.8 79.6 87.4 94.7 89.0 93. 70.9 64.3 85.8 64.8 77.9 66.5 87.0 92.2 53.8 76.9 64.4 77.4 NA 70.3 68.1 84.3 86.6 62.2 78.6 76.1 77.2 75.0 NA"
        },
        {
            "title": "77.0\nNA\n78.2\n84.0",
            "content": "65.7 75.1 65.7 73.4 74.5 81.1 60.9 96.2 88.4 47.9 64.9 93.2 90.2 71.5 80.6 90.5 80.9 74.4 82.7 89.4 78.2 91. 68.9 66.3 57.8 64.0 70.2 63.5 85.5 88.6 58.1 71.6 62.7 75.2 NA 67.1 66.0 93.7 82.5 48.9 68.1 74.1 59.1 70.2 NA"
        },
        {
            "title": "77.1\nNA\n78.4\n91.6",
            "content": "64.1 79.2 61.9 78.5 72.7 79.9 64.1 85.1 71.8 Pillar-0 56.8 68.2 96.3 95.0 93.6 92. 94.9 99.3 87.6 91.6 93.3 86.9 95.6 70.6 60.2 80.8 68.5 74.3 81.9 98.2 99.4 89.4 91.0 76.0 89.3 NA 76.7 84.7 97.9 94.2 59.4 73.3 94.7 80.9 89.4 NA 94.1 NA 89.2 96. 90.0 92.1 79.1 92.6 88.6 94.5 75.5 98.9 91."
        },
        {
            "title": "Continued on next page",
            "content": "37 Pillar-0: New Frontier for Radiology Foundation Models Table 11 Model Performance (AUC 100) for UCSF Abd CT (continued) Model AUC (x 100) Finding LingShu MedGemma MedImageInsight Merlin Pillar-0 Are there any sites of active extravasation in the liver? Are there any transjugular intrahepatic portosystemic shunts? Is hepatic steatosis, fatty liver, focal fatty infiltration suspected based on the imaging findings? Is polycystic liver disease present? Is there any intrahepatic biliary ductal dilation? Is there any portal venous gas? Is there cirrhosis with decompensation? Is there cirrhosis with portal hypertension? Is there cirrhosis? Is there hepatic lymphoma? Is there hepatoblastoma? Is there hepatocellular carcinoma? Is there hepatomegaly? Is there portal venous occlusion? Is there viral hepatitis? is there any extrahepatic biliary ductal dilation? Mass/Tumor findings Are there any Wilms tumors? Are there any bladder transitional cell carcinomas? Are there any neuroendocrine tumors? Are there any rectal carcinomas? Are there any solid pseudopapillary tumors? Are there any testicular masses? Are there any vaginal or vulvar cancers? Is there adenoma malignum? Is there cholangiocarcinoma? Is there fibrolamellar carcinoma? Is there mesenteric carcinoid tumor? Is there periductal/intraductal cholangiocarcinoma? Is there peritoneal carcinomatosis or omental caking? Is there prostate cancer? Other findings Are there any angiomyolipomas? Are there any bladder stones? Are there any dysplastic nodules? Are there any enteric diverticula? Are there any femoral/ventral/Spigelian hernias? Are there any hydroceles? Are there any iliopsoas muscle abscesses? Are there any inguinal hernias? Are there any intraductal papillary mucinous neoplasms (IPMN)?"
        },
        {
            "title": "Are there any intraductal papillary mucinous",
            "content": "neoplasms with worrisome features? Are there any leiomyomas? Are there any leiomyosarcomas? Are there any oncocytomas? Are there any osteosclerotic lesions? Are there any pheochromocytomas? Are there any scrotal hematomas? Are there any subcapsular hemorrhages? 76.2 88.7 72.7 61.4 72. 82.6 91.1 88.1 86.9 68.9 NA 90.9 72.1 78.1 79.6 75.7 NA 70.4 64.4 66.9 NA 77.3 97.3 74.1 81.0 8.1 57.4 63.0 82.7 92.0 62.6 81.3 52.4 72.5 73. 78.9 79.5 79.7 72.6 51.5 75.6 73.0 NA 71.0 82.3 NA 80.6 88.1 96.2 75. 77.1 76.7 85.2 92.9 91.8 90.6 55.2 NA 92.3 83.0 84.6 83.6 78.0 NA 74.4 73.4 69.9 NA 81.4 94.8 72.3 84.4 56.6 68.5 72.3 87.9 93. 64.6 83.3 48.7 75.1 76.9 79.5 76.8 81.1 75.3 66.7 78.3 83.9 NA 73.9 93.5 NA 82.1 71.2 87. 75.2 74.5 76.6 81.9 93.5 91.7 90.3 75.3 NA 92.5 85.0 85.8 83.9 76.5 NA 74.7 73.8 74.9 NA 85.1 92.7 74.1 84.7 35.6 64.5 87.2 88. 91.6 65.4 85.0 71.1 74.2 75.2 83.5 76.9 79.1 75.6 70.4 79.6 84.3 NA 71.5 80.9 NA 82.1 71. 93.2 75.3 77.6 78.6 88.0 93.8 93.2 90.9 71.2 NA 93.2 78.5 84.8 82.7 76.6 NA 67.8 67.7 73.7 NA 66.2 49.9 65.4 84.2 87.4 67.0 85. 84.2 90.3 65.4 78.4 61.9 73.0 72.9 81.7 77.4 77.9 66.8 64.1 77.5 90.7 NA 70.4 73.1 NA 79. 93.9 98.7 83.6 94.0 92.3 84.2 97.5 97.4 96.0 83.1 NA 97.8 87.3 92.6 90.7 90.7 NA 85. 84.2 90.0 NA 95.4 97.2 83.5 96.9 74.7 88.3 98.7 93.9 97.2 66.8 85.7 58.8 77.2 83.9 85.8 90.9 80.5 77.5 65. 90.3 96.4 NA 79.7 87.8 NA 87."
        },
        {
            "title": "Continued on next page",
            "content": "38 Pillar-0: New Frontier for Radiology Foundation Models Table 11 Model Performance (AUC 100) for UCSF Abd CT (continued) Model AUC (x 100) Finding LingShu MedGemma MedImageInsight Merlin Pillar-0 Are there any urachal diverticula? Are there any ureteroceles? Are there any varicoceles? Are there changes status post hysterectomy? Is anasarca suspected based on the imaging findings? Is appendicitis suspected based on the imaging findings? Is hiatal hernia present? Is metastatic disease suspected based on the imaging findings? Is osteopenia suspected based on the imaging findings? Is there Fourniers gangrene / necrotizing fasciitis? Is there adnexal torsion? Is there angiosarcoma? Is there any adenomyosis? Is there any ascites? Is there any consolidation? Is there any evidence of enteric contrast leak suggestive of postoperative leakage/fistula after surgery? Is there any pleural effusion? Is there ascending cholangitis? Is there bladder rupture? Is there diverticulitis? Is there epididymitis? Is there epithelioid hemangioendothelioma? Is there evidence of sleeve gastrectomy or fundoplication? Is there hemochromatosis? Is there hiatal hernia? Is there ileus? Is there intussusception? Is there nephritis? Is there primary sclerosing cholangitis? Is there prostatitis? Is there prostatomegaly or prostatic hyperplasia? Is there pseudomembranous colitis? Is there pseudomyxoma peritonei? Is there retroperitoneal fibrosis? Is there retroperitoneal hemorrhage or retroperitoneal hematomas? Is there retroperitoneal liposarcoma? Is there submucosal edema, enterocolitis, or gastritis? Is there testicular (segmental) infarct? Is there testicular torsion? Is there volvulus? Is there xanthogranulomatous pyelonephritis? Pancreas-related findings Are there any ductal pancreatic carcinomas? Are there any pancreatic tumors? Is chronic pancreatitis suspected based on imaging findings? Is cystic fibrosis suspected based on the pancreatic imaging findings? Is there acute pancreatitis? 60.4 11.5 61.4 77.4 95.3 82.5 73.3 74.5 83. 94.5 99.7 87.8 60.4 84.9 81.6 83.4 89.8 67.4 75.7 73.9 95.7 NA 69.6 NA 73.2 88.1 57.2 74.5 80.6 66.2 83.4 85.3 44.1 55.0 87.1 81.6 79. NA NA 81.9 82.9 80.6 75.1 72.6 57.6 80.5 82.0 10.1 65.1 79.7 96.2 87. 76.5 79.0 84.8 96.6 99.8 98.0 62.3 87.6 83.8 87.7 91.7 79.1 73.5 78.5 84.2 NA 73.3 NA 76.6 90.6 64.8 76.6 74.7 67.8 85. 90.9 60.8 54.0 89.2 85.1 82.1 NA NA 82.5 86.9 84.8 77.3 75.0 53.3 84. 88.9 37.4 60.4 79.9 95.4 86.0 75.1 77.8 82.6 99.8 88.5 98.0 61.4 88.2 82.8 88. 91.0 89.3 62.4 77.2 81.8 NA 68.0 NA 75.1 90.4 70.1 78.1 79.3 61.2 84.3 86.8 69.4 73.2 88.7 80.4 82.5 NA NA 84.1 74.4 85.9 78.1 73. 36.7 87.1 76.8 14.5 71.7 77.7 95.5 82.3 72.2 76.0 81. 94.9 86.5 92.3 55.4 87.2 79.1 84.2 89.7 79.1 32.9 76.3 78.2 NA 69.0 NA 72.2 87.7 60.4 76.4 77.3 64.7 82.8 82.6 69.6 63.6 86.0 61.7 79. NA NA 78.1 69.2 82.0 76.2 72.9 39.1 85.6 84.3 75.4 81.4 90.2 96.4 92. 81.0 87.0 84.3 99.0 99.9 39.7 71.3 91.6 85.7 92.8 96.5 98.1 91.6 92.8 99.0 NA 90.9 NA 81.0 93.0 71.6 91.0 91.1 91.3 90. 96.9 74.7 88.2 93.7 78.8 87.9 NA NA 89.5 99.5 96.1 85.9 83.2 96.4 95."
        },
        {
            "title": "Continued on next page",
            "content": "39 Pillar-0: New Frontier for Radiology Foundation Models Table 11 Model Performance (AUC 100) for UCSF Abd CT (continued) Model AUC (x 100) Finding LingShu MedGemma MedImageInsight Merlin Pillar-0 Is there an annular pancreas? Is there any pancreatic pseudocysts? Is there autoimmune pancreatitis? Is there groove pancreatitis? Is there main pancreatic duct dilatation? Is there necrotizing pancreatitis? Is there pancreatic atrophy? Pelvic/GYN findings Are there any cervical masses? Are there any endometrial polyps? Are there any endometriomas? Are there any fractures of the pelvic girdle or sacrum? Are there any intra-uterine devices? Are there any ovarian cancers? Are there any ovarian teratomas? Are there any ovarian tumors? Are there any uterine malformations? Is there any lymphadenopathy in retroperitoneum, peritoneum or pelvis? Is there deep infiltrative endometriosis? Is there endometrial thickening? Is there pelvic inflammatory disease? Is there polycystic ovarian syndrome? Spleen-related findings Are there any accessory spleens? Are there any splenic abscesses? Are there any splenic hamartomas? Are there any splenic infarcts? Are there any splenic lacerations? Are there any splenic pseudocysts? Is splenic lymphoma present? Is the spleen surgically absent? Are there any post-splenectomy changes? Is there splenomegaly? Vascular findings Are there any aortic dissections? Are there any aortic intramural hematomas? Are there any aortic stents? Are there any aortic valve calcifications? Are there any deep venous thromboses? Are there any penetrating atherosclerotic aortic ulcers? Are there coronary artery calcifications? Is abdominal aortic aneurysm suspected based on the imaging findings?"
        },
        {
            "title": "Is aortic atherosclerosis suspected based on",
            "content": "the imaging findings?"
        },
        {
            "title": "Is arterial or aortic thrombosis suspected",
            "content": "based on the imaging findings? 43.5 73.9 NA 39.8 74.2 88.3 70.3 85.0 70.0 82.1 76.7 87.4 80.5 73.6 78.6 58.5 65.5 89.5 73.8 85.6 88.1 54.6 73.6 43.5 75.4 40.6 68.2 49.4 56. 82.4 79.2 89.4 96.4 87.3 69.3 82.4 83.9 85.9 77.8 75.6 Abbreviations: AUC, area under the curve; NA, not applicable. 76.6 76.0 NA 65.3 78.2 94.4 72.9 90.9 54.7 85.5 78.7 90.9 84.6 75.9 84.8 57.6 68.7 88.0 80.7 82.5 61.7 56.7 79.7 38.9 81.5 66.4 65.3 47.5 62.0 86. 86.3 78.8 97.5 89.9 74.1 85.1 86.5 91.4 81.3 80.3 62.3 77.1 NA 69.0 78.1 92.6 71.7 85.9 53.6 78.8 78. 86.9 84.5 70.3 83.6 66.4 70.1 65.6 78.8 88.0 12.0 57.4 82.3 60.2 81.5 70.7 64.8 51.8 59.9 89.8 86.1 79.9 96.8 86.5 74.9 85.5 84.6 90. 78.9 78.3 81.0 69.0 NA 72.9 75.8 92.0 71.1 88.5 91.3 71.4 74.4 90.7 81.6 73.9 79.5 61.7 68.2 69.4 74.5 96.8 96. 55.5 62.9 53.8 80.1 66.8 45.6 50.7 45.6 90.3 80.2 76.8 92.8 87.7 71.2 78.8 84.2 87.4 78.5 76. 41.3 87.7 NA 88.0 88.4 98.2 81.7 94.3 65.5 82.5 82.0 98.8 96.2 79.4 87.4 75.1 75.3 97.8 87.4 97.7 69.4 55.9 99.9 66.2 91.9 87.0 59.9 95.6 55.5 94. 92.0 97.1 98.4 91.3 83.7 90.4 89.6 95.7 84.8 87.1 40 Pillar-0: New Frontier for Radiology Foundation Models D. Performance on full set of RATE-Evals tasks on UCSF Chest CT test set Table 12: Model Performance (AUC 100) for UCSF Chest CT Finding LingShu MedGemma MedImageInsight Merlin Pillar-0 Model AUC (x 100) Bone/Fracture findings Is there any fracture of rib or clavicle? Is there any fracture of spine? Is there any osteolytic lesion? Breast findings Is there any breast implant? Is there any breast mass? Cardiac findings Are there any coronary soft plaques? Is cardiac tamponade suspected? Is cardiomegaly suspected? Is dilated cardiomyopathy suspected? Is hypertrophic cardiomyopathy suspected? Is ischemic cardiomyopathy suspected? Is there any coronary atherosclerosis? Is there any heart valve calcification? Is there any heart valve replacement? Chest/Lung findings Is COVID-19 suspected? Is aspiration pneumonia suspected? Is pulmonary edema suspected? Is pulmonary embolism suspected? Is pulmonary hypertension suspected? Is radiation pneumonitis suspected? Is there any atypical cyst in the lung? Is there any calcification in the lung? Is there any cavitation in the lung? Is there any chest tube? Is there any consolidation in the lung? Is there any ground-glass nodules in the lung? Is there any mass in the lung? Is there any part-solid nodules in the lung? Is there any pneumoconiosis? Is there any pneumomediastinum? Is there any pneumothorax? Is there any post-lung resection scar? Is there any solid nodules in the lung? Device/Procedural findings Is miliary tuberculosis suspected? Is non-tuberculous mycobacterial infection suspected? Is primary tuberculosis suspected? Is there any central venous catheter? Is there any endotracheal tube? Gastrointestinal findings Is there any nasogastric tube? Liver-related findings Is cirrhosis suspected? Is diffuse fatty liver / steatosis suspected? Is there any liver cyst? Is there any liver mass? Mass/Tumor findings Is there any mediastinal mass? 73.5 73.8 73.3 91.2 79.2 66.9 77.1 85.8 90.8 93.9 62.6 75.4 75.7 88. 88.8 81.6 91.1 81.2 82.3 70.8 55.7 60.1 63.0 94.6 81.4 64.3 66.4 75.5 90.2 90.4 91.3 77.6 62.3 86.8 83.3 83.2 81.4 97.4 96.0 85.7 80.2 73.3 78.1 67. 74.9 74.7 78.3 94.4 79.4 86.8 78.7 89.0 85.7 77.4 71.2 76.9 78.0 94.4 86.1 84.0 91.7 85.5 84.9 80.8 70.6 62.4 74.6 96.8 85.8 66.9 73.8 77.5 92.3 91.8 93.2 84.5 64.6 91.7 86.4 89.2 91.5 98. 97.7 91.1 81.7 75.0 78.7 73.2 75.0 74.2 75.1 93.7 79.2 78.0 85.6 90.3 89.7 70.4 73.7 76.7 77.5 90. 81.5 85.5 91.2 84.8 85.5 84.4 58.3 62.1 77.8 96.9 86.3 66.6 75.3 79.2 88.0 93.4 94.8 82.8 65.8 89.2 84.6 92.2 83.7 98.0 95.7 92.9 85.0 77.5 79.4 74. 70.8 71.4 68.4 85.9 72.9 75.4 81.4 83.4 85.5 79.9 76.7 74.8 76.7 91.2 80.9 79.4 89.4 77.6 78.6 67.9 58.4 60.3 62.6 92.2 79.3 62.3 65.9 73.0 81.2 90.9 88.8 70.7 60.5 69.6 81.7 74.7 82.0 97. 95.7 87.5 82.9 70.7 72.8 62.7 84.4 91.7 84.9 99.1 83.5 90.2 92.6 90.5 89.9 97.8 85.1 85.1 88.4 99. 93.6 90.8 93.0 85.0 87.8 92.0 86.6 80.3 85.6 98.8 91.1 76.3 82.1 88.6 98.0 95.8 95.6 94.4 76.2 83.8 93.3 83.0 95.6 99.1 98.1 94.4 89.8 88.2 83.9 86."
        },
        {
            "title": "Continued on next page",
            "content": "41 Pillar-0: New Frontier for Radiology Foundation Models Table 12 Model Performance (AUC 100) for UCSF Chest CT (continued) Model AUC (x 100) Finding LingShu MedGemma MedImageInsight Merlin Pillar-0 Is there any tracheal mass? Other findings Is aspergillosis suspected? Is sarcoidosis suspected? Is there any ECMO? Is there any arthritis? Is there any atelectasis? Is there any bronchiectasis? Is there any bulla, bleb, or pneumatocele? Is there any carotid atherosclerosis? Is there any centrilobular emphysema? Is there any crazy paving pattern? Is there any empyema? Is there any goiter? Is there any ground-glass opacity? Is there any hemothorax? Is there any hiatal hernia? Is there any honeycombing? Is there any interlobular septal thickening? Is there any lymphadenopathy in axilla? Is there any lymphadenopathy in mediastinum or hila? Is there any lymphadenopathy in neck? Is there any mosaic attenuation? Is there any mucous plugging? Is there any myxoma? Is there any osteopenia? Is there any osteosclerotic lesion? Is there any pace-maker or defibrillator? Is there any panbronchiolitis? Is there any paraseptal emphysema? Is there any peribronchial thickening? Is there any pericardial effusion? Is there any pleural effusion? Is there any pleural plaques? Is there any pleural thickening? Is there any reticulation or reticular pattern? Is there any sternotomy? Is there any teratoma? Is there any thymoma? Is there any thyroid nodule? Is there any tree-in-bud or focal bronchiolitis? Pancreas-related findings Is chronic pancreatitis suspected? Spleen-related findings Is splenomegaly suspected? Vascular findings Is aortic dissection suspected? Is there any aberrant subclavian artery? Is there any aortic aneurysm? Is there any aortic atherosclerosis? Is there any coronary artery stenting? Is there any post-coronary artery bypass graft? Is there any single coronary artery? 82.4 82.8 75.8 99.0 78.9 77.9 79.9 76.9 64.2 81.0 89.8 88.8 64.3 70.6 94.0 80.5 97.6 87.0 72.1 75.8 69.0 79.4 74.5 6.4 80.3 69.9 95.4 51.1 79.1 73.5 75.2 92.5 79.1 82.5 80.6 88.4 NA 46.2 68.8 76. 71.6 90.4 72.4 50.0 78.4 77.4 69.3 90.1 29.0 Abbreviations: AUC, area under the curve; NA, not applicable. 88.4 86.9 78.3 99.0 85.5 80.0 82.7 81.1 58.7 85.7 90.4 90.1 65.1 74.0 93.9 84.6 98.9 88.3 73.6 79. 72.1 81.7 76.2 8.6 80.4 72.2 97.9 45.8 84.2 77.3 77.4 94.2 78.9 85.3 83.7 94.2 NA 55.9 69.3 79.1 71.0 93.0 77.0 52.3 83.6 80.3 70.6 95.6 29.6 3.1 86.2 81.1 98.6 81.6 79.9 82.7 78.5 64.3 83.8 89.5 95.2 65.8 74.3 95.4 84.7 98.7 86.7 72.3 79. 73.2 81.0 75.6 1.4 79.0 69.9 96.5 67.8 81.9 75.5 78.3 94.4 79.7 84.7 82.9 90.4 NA 61.8 69.9 77.9 75.3 93.3 80.5 58.0 81.4 79.6 71.6 91.0 41.4 18.4 75.3 73.1 97.9 75.4 74.7 75.8 72.5 68.6 79.1 87.2 85.2 62.0 67.8 93.2 77.9 94.4 83.2 67.8 74. 65.7 74.6 70.9 32.5 71.5 67.2 95.6 71.3 79.3 69.3 73.9 89.2 81.2 75.7 75.2 84.4 NA 51.6 65.3 71.5 71.7 91.4 70.1 51.6 75.5 79.1 71.0 85.5 29.1 90.5 92.6 94.8 98.7 80.5 85.4 88.4 89.1 83.0 92.7 91.8 97.0 82.3 83.6 96.8 93.0 98.9 89.4 84.6 87. 77.8 88.0 84.1 17.2 75.2 80.8 99.0 93.8 91.0 83.7 87.7 97.6 90.7 89.3 89.4 96.4 NA 91.0 75.0 88.7 87.8 94.8 85.2 80.8 91.7 84.4 88.1 98.0 42.1 42 Pillar-0: New Frontier for Radiology Foundation Models E. Performance on full set of RATE-Evals tasks on UCSF Head CT test set Table 13: Model Performance (AUC 100) for UCSF Head CT Finding LingShu MedGemma MedImageInsight Merlin Pillar-0 Model AUC (x 100) Bone/Fracture findings Are there any fractures of skull vault (calvarial fracture)? Are there any osteolytic lesions? Brain findings Are there any brain tumors? Is cerebral edema present ? Is hydrocephalus present? Is intra-cranial hemorrhage (ICH) present? Is intra-cranial metastasis suspected based on the imaging findings? Is meningioma suspected based on the imaging findings? Is there any brain atrophy? Is there any deep brain stimulation (DBS) device present? Is there cerebral / cerebellar herniation? Device/Procedural findings Are any post-craniectomy changes? Are there any post-craniotomy changes? Are there any post-radiotherapy changes? Is there transtentorial herniation? Mass/Tumor findings Is mass effect present? Other findings Are there any osteosclerotic lesions? Is acute infarct suspected based on the imaging findings? Is chronic infarct suspected based on the imaging findings? Is epidural hemorrhage (EDH) present? Is external ventricular drainage (EVD) present? Is hemorrhagic transformation of infarct present? Is present? Is present? intra-parenchymal hemorrhage (IPH) intra-ventricular hemorrhage (IVH) Is subarachnoid hemorrhage (SAH) present? Is subdural hemorrhage (SDH) present? Is there any loss of the gray-white matter junction? Is there midline shift? Is there subfalcine herniation? 74.0 52.2 62.8 70.7 79.8 68.8 65.9 64.6 82.6 84.2 78. 69.1 69.5 54.9 79.9 70.7 59.9 68.2 73.4 66.3 77.2 74. 68.1 72.4 70.2 62.0 60.2 73.6 73.5 78.3 71. 65.9 76.3 83.0 78.0 72.7 71.0 79.5 97.2 79.5 90.4 92.3 68.3 82.5 80. 69.7 70.1 72.0 76.0 97.5 77.0 78.9 88. 78.7 71.5 66.5 85.7 71.3 78.2 74.4 66.2 79.5 89.8 81.2 73.7 71. 80.8 96.9 86.4 89.5 91.4 66.2 89.4 82.2 71.0 69.2 72. 73.5 95.4 82.4 82.4 89.4 81.9 75.1 67.0 88.7 81. 66.3 63.0 56.4 70.4 81.7 71.4 60.2 62.4 72.9 94.3 74. 77.5 79.9 49.2 75.1 71.4 55.1 64.4 65.9 73.4 93.0 63. 69.9 82.3 76.6 63.0 58.2 75.9 67.6 Abbreviations: AUC, area under the curve; NA, not applicable. 78. 72.0 86.2 92.1 95.2 93.8 90.2 78.6 85.5 99.9 95.1 94.7 96.0 83.7 96. 93.4 68.4 90.2 83.8 91.3 98.3 94.3 95. 96.2 93.4 92.6 85.1 96.2 95.6 43 Pillar-0: New Frontier for Radiology Foundation Models F. Performance on full set of RATE-Evals tasks on UCSF Breast MRI test set Table 14: Model Performance (AUC 100) for UCSF Breast MRI Finding Breast findings Are any foci of enhancement present in the breast? Are any masses present in the breast? Are any simple or complicated breast cysts present? Are there any masses in the breast contralateral to the known cancer? Is angiosarcoma in the breast suspected based on the imaging findings? Is breast abscess suspected based on the imaging findings? Is breast cancer recurrence suspected based on the imaging findings? Is carcinoma in the breast suspected based on the imaging findings? Is fat necrosis or oil cyst in the breast suspected based on the imaging findings? Is fibroadenoma in the breast suspected based on the imaging findings? Is hamartoma in the breast suspected based on the imaging findings? Is intraductal papilloma in the breast suspected based on the imaging findings? Is lipoma in the breast suspected based on the imaging findings? Is non-mass enhancement present in the breast? Is phyllodes tumor in the breast suspected based on the imaging findings? Is pseudoangiomatous stromal hyperplasia (PASH) in the breast suspected based on the imaging findings? Is there any internal mammary lymphadenopathy? Is tubular adenoma in the breast suspected based on the imaging findings? Device/Procedural findings Are any post-operative hematomas or seromas present? Are any post-partial mastectomy (lumpectomy) changes present? Liver-related findings Are any liver cysts present? Are any liver masses present? Mass/Tumor findings Are any metastases present in the visible scan field? Is the margin of the mass spiculated? Is the shape of the mass irregular? Is there any satellite mass around the main tumor? Is there any skin thickening associated with the known cancer?"
        },
        {
            "title": "Is there evidence of tumor involvement of the pectoralis",
            "content": "muscle? Is there evidence of tumor involvement of the skin? Other findings Are any galactoceles present? Are any post radical mastectomy changes present? Are any post simple mastectomy changes present? Are there any biopsy clips? LingShu MedGemma MedImageInsight Pillar-0 Model AUC (x 100) 59.2 56.6 58.2 56.5 NA NA 58.1 57.6 68.1 51.5 10.7 55. 52.5 59.6 51.1 72.6 55.5 NA 54.4 60.0 54.3 57. 70.4 60.9 60.2 65.2 58.7 66.6 49.3 53.2 67.0 66.6 55.6 65.8 65.4 73.5 63.4 NA NA 60.6 65.7 78.0 61.5 33. 24.8 95.5 62.6 60.4 36.8 64.4 NA 66.6 75. 64.4 58.6 74.2 70.1 67.5 73.1 68.9 74.4 63.2 27.6 95.9 93.8 67.0 66.0 64.9 76.1 60. NA NA 61.9 64.9 69.5 72. 25.4 21.8 11.4 61.8 50.4 59.3 63.5 NA 60.6 71.3 65.1 63.4 77.0 70.7 67.8 70.1 64.1 75.1 63.4 29.0 93.4 91.0 65. 79.6 80.9 77.8 71.7 NA NA 72.6 80.6 80. 67.9 94.0 99.3 67.0 73.2 85.2 80. 77.4 NA 81.6 88.4 79.2 81.3 83.3 88.0 84.8 84.8 88.8 78.1 85. 77.9 97.2 94.0 74."
        },
        {
            "title": "Continued on next page",
            "content": "44 Pillar-0: New Frontier for Radiology Foundation Models Finding LingShu MedGemma MedImageInsight Pillar-0 Table 14 Model Performance (AUC 100) for UCSF Breast MR (continued) Model AUC (x 100) Is Pagets disease suspected based on the imaging findings? Is implant-associated anaplastic large-cell lymphoma (BIA-ALCL) suspected based on the imaging findings? Is mastitis suspected based on the imaging findings? Is there any axillary lymphadenopathy ipsilateral? Is there any nipple retraction or inversion? Is there evidence of capsular contracture around the implant? Is there evidence of extra-capsular implant rupture? Is there evidence of intra-capsular implant rupture? Is there fast initial enhancement and washout delayed enhancement? NA NA 22.7 66.6 61.4 NA NA 65.0 55.3 NA NA 93.3 73.9 68.1 NA NA 94.4 67. NA NA 97.1 73.6 69.9 NA NA 90.3 65.8 NA NA 85.7 83.2 89.3 NA NA 99.6 87.0 Abbreviations: AUC, area under the curve; NA, not applicable. 45 Pillar-0: New Frontier for Radiology Foundation Models G. Performance on full set of RATE-Evals tasks on Merlin Abdomen-Pelvis CT test set Table 15: Model Performance (AUC 100) for Merlin Finding Biliary findings Are there any biliary cystadenomas? Are there any biliary hamartomas? Are there any biliary stents? Are there any choledochal cysts? Are there any gallbladder polyps? Are there any gallbladder stones or cholelithiasis? Is the gallbladder surgically absent? Is there gallbladder adenomyomatosis? Is there gallbladder cancer? Is there gallbladder wall thickening? Is there porcelain gallbladder? Bone/Fracture findings Are there any femoral fractures? Are there any fractures (general)? Are there any osteolytic lesions? Are there any rib fractures? Are there any spinal fractures? Cardiac findings Is there any coronary atherosclerosis? Is there cardiomegaly? Chest/Lung findings Are there any lung masses or nodules? Is there any atelectasis in the visible lung? Is there any free air or pneumoperitoneum? Is there pneumobilia? Cyst findings Are there any Bartholin gland cysts? Are there any Gartner duct cysts? Are there any Nabothian cysts? Are there any enteric duplication cysts? Are there any epithelial cysts? Are there any mucinous cystic neoplasms? Are there any serous cystic neoplasms? Is there acute cholecystitis with rupture? Is there acute cholecystitis? Is there emphysematous cholecystitis? Device/Procedural findings Are there any Foley catheters? Are there any central venous catheters? Are there any drainage catheters? Are there any extracorporeal membrane oxygenation (ECMO) devices? Are there any gastrostomy tubes? Are there any post-nephrectomy changes? Gastrointestinal findings Are there any colonic carcinomas? Are there any duodenal carcinomas? Are there any esophageal carcinomas? Are there any esophageal or gastric varices? Are there any gastric carcinomas? Atlas MedGemma MedImageInsight Merlin Pillar-0 Model AUC (x 100) 86.8 73.2 94.3 39.4 62.5 73.2 94.8 68.7 98.4 81.7 75.6 82.4 75.7 77.2 80.3 76. 84.3 87.2 65.9 77.9 87.6 92.1 82.3 87.7 87.3 NA 62.9 72.8 73.6 87.3 95.5 85.7 93.3 86.0 92.7 95.9 92.7 81.1 76.1 94.1 92.2 92.6 88. 34.7 58.6 89.0 52.4 77.3 65.8 79.2 70.8 53.7 71.7 9.8 79.9 74.1 75.6 76.5 73.9 85.5 82.3 63.0 73.9 83.0 80.6 74.7 66.8 83.8 NA 56.4 70.4 52.6 80.7 76.4 64. 90.0 84.9 89.8 100.0 88.7 76.2 65.4 81.4 61.8 85.4 77.5 40.5 63.1 83.7 73.6 69.3 66.9 77.4 69.1 63.1 74.1 34.2 78.0 74.6 73.7 76.5 76. 84.7 82.3 62.6 74.4 84.2 79.7 77.5 75.0 81.6 NA 58.1 71.0 52.2 80.2 77.9 63.5 89.0 85.1 88.5 98.9 89.0 69.0 75.3 78.7 69.2 88.8 79. 39.7 68.7 98.5 38.0 69.2 74.2 96.7 66.5 91.6 79.0 57.1 83.0 74.6 75.6 79.3 73.3 87.0 87.7 63.5 76.1 87.3 95.4 85.3 82.1 84.9 NA 58.9 70.1 78.4 73.8 84.9 82. 94.4 87.2 93.2 99.8 93.4 77.1 82.3 85.1 94.2 95.2 88.2 93.5 85.0 98.6 60.4 67.0 87.5 98.0 77.8 98.6 86.5 86.5 85.7 78.0 83.1 81.8 80. 87.2 87.4 66.6 77.5 88.6 97.4 85.7 92.0 87.7 NA 68.6 73.3 62.9 92.9 96.0 95.4 96.3 91.4 95.4 99.1 95.3 91.5 78.3 87.1 99.1 94.8 88."
        },
        {
            "title": "Continued on next page",
            "content": "46 Pillar-0: New Frontier for Radiology Foundation Models Table 15 Model Performance (AUC 100) for Merlin (continued) Finding Are there any gastric foreign bodies (not related to medical devices or post-surgical changes)? Are there any gastrointestinal stromal tumors (GISTs)? Are there any nasogastric tubes? Is bowel obstruction present? Is there surgical gastric conduit? Is there achalasia or scleroderma suspected based on gastrointestinal imaging findings? Is there bowel obstruction? Is there gastric volvulus? Is there gastrointestinal lymphoma? Is there inflammatory bowel disease? Is there large bowel obstruction? Is there pneumatosis intestinalis? Is there small bowel obstruction? Kidney-related findings Are there any adrenal adenomas? Are there any adrenal hemorrhages? Are there any adrenal infarcts? Are there any adrenal masses? Are there any adrenal myelolipomas? Are there any complex renal cysts? Are there any percutaneous nephrostomy tubes/catheters? Are there any post-renal transplantation changes? Are there any renal abscesses? Are there any renal cell carcinomas? Are there any renal hypodensities? Are there any renal infarcts? Are there any renal lacerations? Are there any renal stones / nephrolithiasis? Are there any simple renal cysts? ureteral Are double-J there any catheters/stents? Are there any ureteral stones? Is there adrenal hyperplasia? Is there horseshoe kidney? Is there hydronephrosis? Is there nephrocalcinosis? Is there polycystic kidney disease? Is there renal lymphomas? Liver-related findings Are there any hepatic abscesses? Are there any hepatic adenomas? Are there any hepatic changes post-resection? changes postAre there any hepatic transplantation? Are there any hepatic cysts? Are there any hepatic focal nodular hyperplasias? Are there any hepatic hemangiomas? Are there any hepatic infarcts? Are there any hepatic masses? Are there any hepatic metastases? Are there any hepatic regenerative nodules? Are there any intra-hepatic stones? Are there any liver lacerations? Atlas 72. 35.9 91.3 93.8 87.2 75.0 93.7 99.6 63.9 80.9 89.9 80.1 94.8 71.0 85.6 NA 68.6 81.4 73.2 95.7 92.9 94.0 72.8 66.3 57.1 91.7 65.4 70.1 93. 87.2 55.5 88.9 93.0 72.6 95.7 NA 90.6 1.1 85.8 88.5 75.4 58.9 68.8 96.5 80.6 92.7 92.8 67.7 82.9 Model AUC (x 100) MedGemma MedImageInsight Merlin 38.2 40.7 93.7 86.7 72.8 47.0 86.5 93.0 64.7 78.9 90.5 77.2 85. 73.5 71.8 NA 67.8 78.5 65.0 89.8 89.4 48.1 80.7 66.3 73.4 89.7 61.5 68.6 96.6 71.7 50.6 54.5 74.9 59.8 89.6 NA 80.0 71.8 75.3 75.3 64.1 63. 62.8 81.3 67.5 81.2 70.6 65.2 82.9 49.6 32.7 89.0 89.8 74.2 63.6 89.6 90.3 49.4 79.6 88.9 78.1 90.0 73.0 79.2 NA 68.4 79.0 70.1 87. 87.0 28.8 84.8 66.3 72.8 81.7 60.5 69.0 94.2 74.9 60.9 76.4 76.0 64.6 92.9 NA 79.1 84.2 74.3 73.0 65.2 67.6 61.6 95.4 68.4 81.9 88.9 67.9 82. 66.7 41.5 95.2 94.2 88.1 86.1 94.2 95.6 55.0 79.1 92.7 79.3 94.7 71.4 87.5 NA 66.6 74.8 65.5 92.9 94. 81.8 77.4 66.2 70.7 95.2 62.0 68.1 95.9 78.7 74.7 60.6 79.9 67.4 95.7 NA 88.5 89.3 87.9 91.7 66.1 70.3 60.0 99.0 75.3 90.0 90.8 72.5 79.0 Pillar38.3 49.3 97.1 94.8 91.9 90.8 94.7 99.3 75.2 83.8 93.2 80.0 95.4 70.6 86.5 NA 67.9 72.8 80.3 98.9 98. 97.8 86.5 79.2 68.7 98.9 70.4 81.1 98.0 89.8 69.3 93.0 94.7 72.3 99.3 NA 92.6 59.6 89.4 93.9 88.5 65.5 83.7 97.7 89.5 96.6 97.3 76.2 84."
        },
        {
            "title": "Continued on next page",
            "content": "47 Pillar-0: New Frontier for Radiology Foundation Models Table 15 Model Performance (AUC 100) for Merlin (continued) Model AUC (x 100) Finding Are there any sites of active extravasation in the liver? Are there any transjugular intrahepatic portosystemic shunts? Is hepatic steatosis, fatty liver, focal fatty infiltration suspected based on the imaging findings? Is polycystic liver disease present? Is there any intrahepatic biliary ductal dilation? Is there any portal venous gas? Is there cirrhosis with decompensation? Is there cirrhosis with portal hypertension? Is there cirrhosis? Is there hepatic lymphoma? Is there hepatoblastoma? Is there hepatocellular carcinoma? Is there hepatomegaly? Is there portal venous occlusion? Is there viral hepatitis? is there any extrahepatic biliary ductal dilation? Mass/Tumor findings Are there any Wilms tumors? Are there any bladder transitional cell carcinomas? Are there any neuroendocrine tumors? Are there any rectal carcinomas? Are there any solid pseudopapillary tumors? Are there any testicular masses? Are there any vaginal or vulvar cancers? Is there adenoma malignum? Is there cholangiocarcinoma? Is there fibrolamellar carcinoma? Is there mesenteric carcinoid tumor? Is there periductal/intraductal cholangiocarcinoma? Is there peritoneal carcinomatosis or omental caking? Is there prostate cancer? Other findings Are there any angiomyolipomas? Are there any bladder stones? Are there any dysplastic nodules? Are there any enteric diverticula? Are there any femoral/ventral/Spigelian hernias? Are there any hydroceles? Are there any iliopsoas muscle abscesses? Are there any inguinal hernias? Are there any intraductal papillary mucinous neoplasms (IPMN)?"
        },
        {
            "title": "Are there any intraductal papillary mucinous",
            "content": "neoplasms with worrisome features? Are there any leiomyomas? Are there any leiomyosarcomas? Are there any oncocytomas? Are there any osteosclerotic lesions? Are there any pheochromocytomas? Are there any scrotal hematomas? Are there any subcapsular hemorrhages? Atlas 89.1 96.8 81. 86.9 87.9 92.6 97.7 96.7 94.5 62.5 NA 96.7 83.9 88.8 31.1 86.4 NA 95.7 77.3 90.4 NA NA NA 61.3 95.9 NA 48.4 96.2 92.9 95. 65.5 79.1 NA 80.9 77.9 85.0 90.7 81.1 74.4 58."
        },
        {
            "title": "83.6\nNA\nNA\n58.3\nNA\n97.9\n73.6",
            "content": "MedGemma MedImageInsight Merlin Pillar-0 51.4 91. 74.6 90.0 73.2 92.8 91.5 90.9 87.6 67.9 NA 78.5 79.7 79.1 62.8 71.3 NA 83.9 54.6 91.0 NA NA NA 56.8 88.9 NA 67.4 86.5 88. 62.7 64.4 64.5 NA 77.9 73.1 84.9 81.2 80.2 74.8 69."
        },
        {
            "title": "73.6\nNA\nNA\n59.9\nNA\n82.3\n52.6",
            "content": "24.5 91.6 73.6 97.5 73.3 92.4 92.2 91.9 88.8 74.0 NA 84.7 83.5 81.6 24.5 71.3 NA 80. 55.5 79.1 NA NA NA 76.1 84.9 NA 58.9 84.7 87.9 67.5 66.4 69.6 NA 76.7 73.2 79.7 83.0 79.5 74.2 76."
        },
        {
            "title": "74.2\nNA\nNA\n59.0\nNA\n75.2\n54.3",
            "content": "92.5 95.7 78.9 87.9 87.9 87.9 97.9 97.7 94.6 63.8 NA 94.4 80.7 90.5 43.6 83.9 NA 87. 62.3 87.2 NA NA NA 61.5 97.8 NA 48.6 98.5 92.3 94.9 64.9 79.0 NA 79.5 76.6 79.4 88.5 82.3 73.8 51."
        },
        {
            "title": "82.6\nNA\nNA\n58.1\nNA\n96.7\n73.4",
            "content": "96.7 99.3 83.1 98.7 91.6 86.6 98.6 98.3 96.1 75.4 NA 98.4 84.8 89.6 26.9 90.2 NA 99. 72.3 94.3 NA NA NA 57.2 98.3 NA 60.7 99.0 95.2 95.9 66.9 77.6 NA 81.4 80.6 81.2 91.7 80.7 74.7 70. 88.5 NA NA 63.4 NA 99.0 81."
        },
        {
            "title": "Continued on next page",
            "content": "48 Pillar-0: New Frontier for Radiology Foundation Models Table 15 Model Performance (AUC 100) for Merlin (continued) Model AUC (x 100) Finding Are there any urachal diverticula? Are there any ureteroceles? Are there any varicoceles? Are there changes status post hysterectomy? Is anasarca suspected based on the imaging findings? Is appendicitis suspected based on the imaging findings? Is hiatal hernia present? Is metastatic disease suspected based on the imaging findings? Is osteopenia suspected based on the imaging findings? Is there Fourniers gangrene / necrotizing fasciitis? Is there adnexal torsion? Is there angiosarcoma? Is there any adenomyosis? Is there any ascites? Is there any consolidation? Is there any evidence of enteric contrast leak suggestive of postoperative leakage/fistula after surgery? Is there any pleural effusion? Is there ascending cholangitis? Is there bladder rupture? Is there diverticulitis? Is there epididymitis? Is there epithelioid hemangioendothelioma? Is there evidence of sleeve gastrectomy or fundoplication? Is there hemochromatosis? Is there hiatal hernia? Is there ileus? Is there intussusception? Is there nephritis? Is there primary sclerosing cholangitis? Is there prostatitis? Is there prostatomegaly or prostatic hyperplasia? Is there pseudomembranous colitis? Is there pseudomyxoma peritonei? Is there retroperitoneal fibrosis? Is there retroperitoneal hemorrhage or retroperitoneal hematomas? Is there retroperitoneal liposarcoma? Is there submucosal edema, enterocolitis, or gastritis? Is there testicular (segmental) infarct? Is there testicular torsion? Is there volvulus? Is there xanthogranulomatous pyelonephritis? Pancreas-related findings Are there any ductal pancreatic carcinomas? Are there any pancreatic tumors? Is chronic pancreatitis suspected based on imaging findings? Is cystic fibrosis suspected based on the pancreatic imaging findings? Is there acute pancreatitis? Atlas 81.7 50.4 62.5 88.1 96.1 88. 79.2 86.4 87.8 NA 90.2 62.0 85.9 86.9 87.6 87.9 97.6 84.9 88.0 92.8 93.9 NA 85.5 NA 79.3 88.7 66.4 85.8 95.8 NA 92. 79.0 84.6 80.0 85.8 64.5 75.3 NA NA 89.9 25.8 96.8 75.6 74.6 96.6 91. MedGemma MedImageInsight Merlin Pillar-0 69.5 66.8 68.2 78.5 94.0 78. 77.0 79.5 90.3 NA 80.5 37.3 73.9 81.4 84.3 82.0 91.8 53.4 16.2 76.9 89.8 NA 78.5 NA 77.0 85.4 62.4 76.7 47.8 NA 88. 80.2 76.7 43.0 82.5 88.1 71.6 NA NA 80.4 1.4 72.9 72.7 70.5 80.0 75. 71.4 45.8 71.3 77.9 93.6 76.1 75.6 79.2 88.7 NA 88.2 50.2 62.9 82.7 84.6 84. 91.7 56.1 41.8 77.6 91.6 NA 76.3 NA 75.5 86.5 62.8 76.9 69.5 NA 88.7 80.8 82.1 58.6 82.2 77.2 73.4 NA NA 84.0 98.9 82.3 73.7 69. 81.1 76.0 74.7 67.8 78.7 85.8 95.1 85.4 77.6 84.7 87. NA 89.1 60.2 76.1 85.7 87.1 88.1 97.5 85.8 56.1 85.2 99.2 NA 86.2 NA 77.5 87.7 70.5 83.0 89.7 NA 92.3 86.8 80.5 85.3 82.4 76.7 73. NA NA 86.4 92.7 95.9 76.6 73.4 94.2 87.1 74.0 58.6 84.6 92.2 95.1 91. 81.7 89.8 88.0 NA 93.8 46.9 77.9 87.7 87.8 90.2 97.8 89.1 99.1 92.9 97.4 NA 89.9 NA 81.7 88.2 70.2 87.4 99.2 NA 93. 87.6 87.0 89.9 87.8 62.4 77.1 NA NA 90.2 81.9 97.5 78.1 78.0 97.4 93."
        },
        {
            "title": "Continued on next page",
            "content": "49 Pillar-0: New Frontier for Radiology Foundation Models Table 15 Model Performance (AUC 100) for Merlin (continued) Model AUC (x 100) Finding Atlas MedGemma MedImageInsight Merlin Pillar-0 Is there an annular pancreas? Is there any pancreatic pseudocysts? Is there autoimmune pancreatitis? Is there groove pancreatitis? Is there main pancreatic duct dilatation? Is there necrotizing pancreatitis? Is there pancreatic atrophy? Pelvic/GYN findings Are there any cervical masses? Are there any endometrial polyps? Are there any endometriomas? Are there any fractures of the pelvic girdle or sacrum? Are there any intra-uterine devices? Are there any ovarian cancers? Are there any ovarian teratomas? Are there any ovarian tumors? Are there any uterine malformations? Is there any lymphadenopathy in retroperitoneum, peritoneum or pelvis? Is there deep infiltrative endometriosis? Is there endometrial thickening? Is there pelvic inflammatory disease? Is there polycystic ovarian syndrome? Spleen-related findings Are there any accessory spleens? Are there any splenic abscesses? Are there any splenic hamartomas? Are there any splenic infarcts? Are there any splenic lacerations? Are there any splenic pseudocysts? Is splenic lymphoma present? Is the spleen surgically absent? Are there any post-splenectomy changes? Is there splenomegaly? Vascular findings Are there any aortic dissections? Are there any aortic intramural hematomas? Are there any aortic stents? Are there any aortic valve calcifications? Are there any deep venous thromboses? Are there any penetrating atherosclerotic aortic ulcers? Are there coronary artery calcifications? Is abdominal aortic aneurysm suspected based on the imaging findings?"
        },
        {
            "title": "Is aortic atherosclerosis suspected based on",
            "content": "the imaging findings?"
        },
        {
            "title": "Is arterial or aortic thrombosis suspected",
            "content": "based on the imaging findings? 3.9 89.6 NA 77.7 81.4 91.4 79.2 88.0 56.2 83.0 80.5 95.3 88.7 76.9 89.1 70.8 72.7 76.6 86.9 95.1 NA 58.6 80.4 12.0 91.3 90.1 NA 68.1 NA 91.6 88.9 71.6 86.9 87.3 72.2 67.8 85.1 83.8 84.8 74.6 Abbreviations: AUC, area under the curve; NA, not applicable. 27.6 72.4 NA 54.0 71.9 83.7 78.4 61.2 61.7 77.9 77.6 91.7 77.0 60.7 77.7 62.8 67.4 22.5 75.7 83.2 NA 58.0 91.2 17.3 86.5 87.5 NA 72.0 NA 84. 66.9 21.5 95.1 89.0 71.2 73.8 87.0 87.7 85.8 69.2 29.9 73.3 NA 63.1 72.3 82.5 78.1 66.6 65.3 77.1 76. 90.0 79.5 63.3 81.4 63.3 69.2 89.9 72.5 83.8 NA 56.8 94.5 58.4 87.8 88.7 NA 78.6 NA 87.3 75.9 41.5 94.3 88.3 73.6 92.3 85.3 85. 84.3 70.2 27.2 85.8 NA 73.8 81.0 92.1 80.3 79.7 69.3 83.4 77.3 96.4 88.5 80.7 87.5 75.7 70.6 79.3 84.5 96.6 NA 57.4 64.0 10.4 88.9 93.2 NA 84.3 NA 95.4 81.6 81.5 92.0 90.6 68.3 94.5 87.4 91.9 87.9 73. 7.0 92.0 NA 95.1 85.6 92.4 80.2 89.2 79.8 97.3 82.0 99.5 90.3 77.3 88.1 76.2 75.4 97.4 88.9 96.5 NA 58.4 82.8 23.8 95.2 89.3 NA 91.2 NA 93. 93.1 85.9 94.2 90.4 73.3 95.5 87.7 95.1 87.8 72."
        }
    ],
    "affiliations": [
        "Clinical Metabolomics Core and Imaging Core Laboratory, Institute for Radiological Research, Chang Gung Memorial Hospital at Linkou and Chang Gung University, Taiwan",
        "Computational Precision Health, UC Berkeley and UC San Francisco, USA",
        "Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA",
        "Department of Electrical Engineering and Computer Science, UC Berkeley, USA",
        "Department of Medical Imaging and Intervention, Chang Gung Memorial Hospital at Linkou, Taiwan",
        "Department of Medical Imaging and Radiological Sciences, Chang Gung University, Taiwan",
        "Department of Radiology and Biomedical Imaging, UC San Francisco, USA",
        "Harvard Medical School, USA",
        "Mass General Brigham Cancer Institute, USA",
        "Massachusetts General Hospital, USA"
    ]
}