{
    "paper_title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
    "authors": [
        "Zhuoran Zhang",
        "Feng Zhang",
        "Shangyuan Li",
        "Yang Shi",
        "Yuanxing Zhang",
        "Wei Chen",
        "Tengjiao Wang",
        "Kam-Fai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review."
        },
        {
            "title": "HYBRID ATTRIBUTION PRIORS FOR EXPLAINABLE\nAND ROBUST MODEL TRAINING",
            "content": "Feng Zhang1,2 Shangyuan Li1,2 Yang Shi1,3 Zhuoran Zhang1,2 Yuanxing Zhang3 Wei Chen1,2 Tengjiao Wang1,2 Kam-Fai Wong4 1School of Computer Science, Peking University 2Key Lab of High Confidence Software Technologies, Peking University 3Kling Team 4Department of Systems Engineering and Engineering Management, CUHK 5 2 0 2 9 ] . [ 1 9 1 7 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, especially classification. With the growing emphasis on interpretability and robustness, explanation-guided learning offers an effective framework by incorporating attribution-based supervision during training. However, how to derive general and reliable attribution priors remains an open challenge. Upon analyzing representative attribution methods in classification tasks, we find that while these methods reliably highlight class-relevant tokens, they tend to focus on common keywords shared by semantically similar classes. Since these classes are already prone to confusion under standard training, the attributions fail to provide sufficient discriminative cues, limiting their ability to enhance model differentiation. To address this challenge, we introduce ClassAware Attribution Prior (CAP), novel attribution prior extraction framework designed to guide language models in capturing fine-grained class distinctions, thus producing more salient and discriminative attribution priors. Building on this, we propose CAPHybrid, which integrates priors from CAP and existing attribution techniques to form more comprehensive and balanced supervisory signal. By aligning the models self-attribution with these enriched priors, our approach encourages the model to capture diverse decision-relevant features. Extensive experiments across full-data, few-shot, and adversarial settings demonstrate that our method consistently enhances both model interpretability and robustness."
        },
        {
            "title": "INTRODUCTION",
            "content": "Small language models (SLMs), such as Bert (Devlin et al., 2019) and Roberta (Liu et al., 2019b), have become indispensable, particularly for classification scenarios, due to their efficient inference capabilities in resource-constrained environments and the ease of deployment on devices. SLMs power wide range of essential applications, such as intent detection in dialogue systems (Zhang et al., 2023), sentiment analysis in social media (Wankhade et al., 2022), and topic categorization on content platforms (Li et al., 2022b). In recent years, the performance of SLMs on standard classification benchmarks has improved considerably (Wang et al., 2019), achieving competitive accuracy with significantly smaller model sizes. The focus has gradually shifted to improving the interpretability and robustness of SLMs so that they can meet the real-world demand for transparent model decision-making mechanisms (Taha et al., 2024). Some researches focus on explanationguided learning (Liu & Avci, 2019), in which models are trained not only to predict correct labels but also to align attribution priors to identify the most relevant input components for given prediction, thus enhancing interpretability and potentially improving robustness. Current methods for obtaining attribution priors fall into two main categories. The first derives priors from the trained model itself or from larger, pre-trained variants of the same architecture using techniques such as attention attribution or integrated gradients (Wu et al., 2023). These self-derived priors reflect the model own insights into the data and knowledge, but can be biased, especially in small models with limited capacity. The second category involves human-annotated priors that 1 Preprint. Under review. Figure 1: Training model with attribution produced by different methods for the same case. encode linguistic or domain-specific knowledge (Jayaram & Allaway, 2021), offering greater transparency and generalizability. However, these priors are expensive to construct and lack scalability. So there raises natural question: How to obtain attribution priors that are both reliable and scalable, which capturing both model-internal reasoning and external linguistic knowledge? To explore this question, we try to solve the following three problems: Q1: Do existing attribution methods exhibit systematic limitations in SLM-based text classification? Q2: Large language models inherently encode rich prior knowledge and possess strong language modeling capabilities. But how can we effectively extract high-quality attribution priors from them and integrate these priors with existing methods to enhance explanation-guided learning? Q3: Will the refined attribution prior consistently achieve good interpretability and robust supervision across tasks in variety of settings? Specifically, we first analyze the characteristics of the existing typical attribution methods on classification tasks. For the traditional self-attribution method, such as integrated gradients, they inevitably inherit biases from the training data and cannot introduce novel and complementary knowledge. For the typical LIME and SHAP methods that can extract attribution scores from LLMs to guide SLMs, we observe two key phenomena: homogenization and class confusion. First, we find that both methods produce highly correlated attribution scores and consistently highlighting tokens with high affinity to the target label. Second, the class-level analysis of the top LIME-ranked words reveals that certain classes share many frequent keywords, showing clear word overlap among various classes. Importantly, these confused classes correspond to those most often misclassified by the vanilla-trained target model. Although LIME captures class-relevant cues, it fails to emphasize discriminative features that distinguish these ambiguous classes, thus directly using such priors to supervise training may amplify errors in confused classes. As shown in Figure 1, if the prior only captures the keywords set and alarm in the set alarm category, it may lead to more serious misjudgments for close categories with similar keywords, such as the example case from query alarm. Ideally, the attribution would fully comprehend the semantics, and assign high attribution score to the keyword check for this case. To solve these issues, we introduce Class-Aware Attrbution Prior (CAP), novel attribution prior extraction method that enriches the input text with task instructions and label space. By prompting LLMs in this way, CAP captures class distributions and generates more salient and discriminative attribution priors. Motivated by the complementarity among different attribution methods, we then fuse attribution priors from CAP and existing methods creating more refined and reliable prior signal. By subsequently aligning the models self-attributions with this rich prior knowledge during training, the SLM learns more reliable and general reasoning patterns. We evaluate our method across three diverse datasets on full-data training, few-shot learning, and adversarial settings. Extensive results demonstrate that the refined attribution prior consistently improves classification accuracy while significantly enhancing interpretability and robustness to adversarial perturbations. Our main contributions are summarized as follows: Through analysis of attribution scores from existing methods on text classification tasks, we identify two key limitations: homogenization and class confusion, This will lead to misclassification of semantically similar categories that share overlapping keywords under the attribution guided learning paradigm. 2 Preprint. Under review. We propose class-aware attribution prior method that extracts salient and discriminative priors, along with hybrid variant that captures more comprehensive attribution signals. We evaluate the effectiveness of our method across three experimental settings, focusing on accuracy, interpretability, and robustness. In addition to quantitative results, we also conduct qualitative analyses to further validate the impact of our method."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 MODEL ROBUSTNESS AND INTERPRETABILITY Robustness and interpretability are critical components of any responsible and trustworthy models, where their black-box nature makes them vulnerable to adversarial attacks (Jyoti et al., 2022), especially in safety-critical or risk-sensitive domains (Tjoa & Guan, 2021; Kaur et al., 2023; Shi et al., 2025). Several studies have investigated adversarial attacks and defenses to bolster model robustness (Zhang et al., 2021; Hendrycks et al., 2020). Likewise, recent efforts have focused on enhancing the stability of explanation techniques (Wang et al., 2020; Li et al., 2023; Zhang et al., 2025). Researchers have examined the connections between robustness and interpretability, highlighting the elicitation of more reliable explanations as key pathway to enhance interpretative robustness (Li et al., 2022a). For interpretability, in recent years, many explanation techniques have been developed to better understand the decision making process of deep neural networks (Bach et al., 2015; Ribeiro et al., 2016; Selvaraju et al., 2017; Lyu et al., 2024). Integrated gradients (Sundararajan et al., 2017) addresses input-sensitivity by estimating each feature contribution along the straight-line path from baseline input to the actual input. LIME (Ribeiro et al., 2016) generates perturbed samples by masking different subsets of features around the target instance, computes the predictions on these samples, and then fits locally weighted surrogate model to attribute importance. SHAP (Lundberg & Lee, 2017) provides unified interpretation framework that encompasses six prior methods, assigning each feature an importance score for the given prediction. 2.2 ATTRIBUTION-DRIVEN LEARNING Recent studies have shown that integrating explanation signals can enhance model performance (Gao et al., 2022a;b; Rao et al., 2023). (Ross et al., 2017) regularize models by penalizing input gradients to match expert-defined attribution maps. e2KD (Parchami-Araghi et al., 2024) improves distillation faithfulness by aligning GradCAM explanations (Selvaraju et al., 2017) or B-cos model attributions (Bohle et al., 2022) between teachers and students. In natural language processing domains, several studies leverage human-annotated rationales as attribution priors. (Jayaram & Allaway, 2021) crowdsource word importance annotations on small subset to construct attribution priors and apply them into mean attention weights attribution methods for detecting opinions and beliefs from text. (Liu & Avci, 2019) collect known toxic words and identity terms, and achieves better performance in text classification by setting an attribution target of 1 on toxic words and 0 for identity terms. (Zhong et al., 2019) propose to enhance faithful explanations of model attention for fine-grained sentiment analysis by aligning the model attention with human rationales. AD-KD (Wu et al., 2023) leverages the multi-view attribution distillation for model compression. (Pimparkhede & Bhattacharyya, 2025) propose to use main predicate in the text utterances along with the arguments of the main predicate to serve as explanation signals for intent classification. Although effective, these methods depend on costly human annotations or require parameter updates, limiting their scalability. In this paper, we propose novel approach that automatically extracts comprehensive attribution priors from LLMs without any human effort or model fine-tuning."
        },
        {
            "title": "3 ANALYSIS OF ATTRIBUTION PRIORS",
            "content": "To answer the Q1, we analyze the characteristics of existing attribution methods in this section. 3.0.1 INTEGRATED GRADIENT As typical white-box attribution method, Integrated Gradients (IG) computes token importance based on the models internal gradient information. Following (Wu et al., 2023), common ap3 Preprint. Under review. proach in attribution-prior guided training for small language models (SLMs) is to first train model on the dataset, then extract attribution scores from this trained model over the training data, and incorporate these scores as priors during subsequent training. This methods advantage lies in the strong alignment between the attribution priors and the models own decision process, enabling the model to reinforce its learned reasoning patterns. However, this approach also has limitations: since the attribution depends on the initially trained model, it is susceptible to biases present in that model; moreover, IG captures only local gradient-based information and may not effectively leverage semantic label information or external knowledge. This limitation is particularly pronounced in few-shot or fine-grained settings, where distinguishing between semantically similar classes requires richer and more discriminative attribution signals. 3.0.2 LIME AND SHAP We investigate two representative black-box attribution methods, LIME and SHAP, which are capable of extracting rationale priors from large language models predictions. These methods provide additional explanatory information from large models that can be used to guide the training of smaller models. Firstly, we take text as the input and the corresponding label as the target output to automatically extract priors by estimating the impact of each input token on the model target output, thus yielding the correpsonding attribution scores. This leads to question: what characteristics define high-quality attribution priors? To explore this, we analyze the attribution scores generated by LIME and SHAP on the Banking77 dataset and observe two key phenomena: homogenization and class confusion. First, both methods highlight words with high affinity to the target label. We also calculate the Pearson correlation coefficient between two scores, and find that it reaches 0.8, suggesting that the two methods focus on the similar subset of words. Second, we examine class-level behavior by identifying the words with higher LIME attribution scores for each class. By calculating the lexical overlap across classes, we observe that while many classes have distinct keywords, subset exhibits substantial overlap in their attributed terms, called confused classes. Directly using such priors to supervise training may amplify errors in these categories, as they encourage the model to focus on overlapping words. We further analyze the relationship between the misclassified classes from models trained solely with classification loss and the confused classes. Fgiure 2 show the relatinonshaip between misclassified samples under standard training and confused classes of LIME. The horizontal axis shows the top 15 classes sorted by the number of misclassifications, with higher values indicating more frequent errors. The vertical axis represents the degree of keyword overlap between class pairs based on LIME attributions, where higher values indicate greater overlap. As shown in Fgiure 2, the majority of misclassifications occur in regions with high overlap levels, i.e., confused classes. This suggests that LIME attribution scores can not solve the error of standard trained model and relying solely on LIME priors may worsen inter-class confusion. This analysis highlights that highquality attribution priors should not only exhibit strong affinity with the target label but also capture salient and discriminative features critical for distinguishing ambiguous classes. Figure 2: Distribution of wrong predictions across overlap levels. Darker colors indicate more misclassified samples. Under standard training, wrong predictions tend to occur in classes with higher overlap in LIME attribution scores."
        },
        {
            "title": "4 THE PROPOSED METHOD",
            "content": "As shown in Figure 3, we introduce an explainable framework that generates comprehensive attribution priors to guide the target model toward more faithful and robust learning process for answering Q2. To capture the salient information for each class, we propose novel class-aware attribution extraction module to extract priors from large language models automatically. Further we conduct hybrid attribution prior fusion to obtain comprehensive multi-view priors. By leveraging their complementary characteristics, the fused prior highlights both high-affinity frequent tokens 4 Preprint. Under review. Figure 3: The illustration of our proposed framework. To extract salient priors for ambiguous classes, we first introduce class-aware attribution priors derived from large language models. We then perform hybrid attribution prior fusion to obtain more comprehensive multi-view priors. In the explanation-guided learning module, these priors guide the model toward greater faithfulness and robustness through attribution alignment. and discriminative class-boundary cues. Finally, during explanation-guided learning, we align the target model attribution scores with the fused priors, resulting in more faithful and robust training. 4.1 CLASS-AWARE ATTRIBUTION PRIORS To address the lack of discriminative attribution signals for ambiguous samples in difficult classes, we propose Class-aware Attribution Priors (CAP) that enhance saliency by emphasizing the key features distinguishing one class from another. As illustrated in Figure 3, we enrich the input text with task instructions and label space, which prompts the LLM to capture the class distribution and produce more salient insights. We then randomly mask individual words and query the LLM for the true-label probability of each masked variant. By using an efficient and stable factorization of the linear equations between these probabilities and the attribution priors, we derive per-word scores. Given sentence = [w1, w2, ..., wd] with words, we leverage the perturbation vector {0, 1}d, where 0 and 1 indicate whether the corresponding word is removed or included, respectively. Due to the significant computational overhead for 2d possible scenarios, we sample perturbation vectors for predicting the attribution priors α Rd. Specifically, to construct mi, we first sample the number of masked words from the range [2, 2 +1]. Then we generate the binary mask containing exactly zeros, where masked positions are chosen uniformly at random. For the sepcific case = 1, each word is masked individually. By repeating this procedure times, we obtain m1, ..., mn to predict and learn the attribution priors α. α = argmin{ (cid:88) i=1 (zi αTmi)2 + λα2}, (1) where zi is the targeted score, which is positively correlated with the probability of answers, and λ is the regularization coefficient. Since probabilities for multi-token answers can be exceedingly small, we apply logarithmic transformation. Specifically, we set zi = 1 , and pi is computed as follows: log pi pi = pΘLLM (yT (Inst., mi)), (2) where is the instruction template, Inst. denotes the task instructions and label space, ΘLLM is the parameters of the frozen large language model, and denotes the element-wise masking operation. 5 Preprint. Under review. To solve Eq. (1), we set its derivative to zero and solve for α, yielding: (cid:88) i=1 (mimi T)α + λα = (cid:88) i=1 zimi. (3) The detailed theoretical proof is provided in Appendix A.3. We then construct the matrix form. Let = [m T] Rnd, = [z1, z2, ..., zn] Rd, the Eq. (3) is: T, ..., mn T, m2 (M TM + λI)α = Tz. (4) We define = TM +λI, and = Tz. We show that is symmetric positive definite matrix, and provide the proof in Appendix A.4. Hence, admits unique Cholesky factorization (Press, 2007): = LLT, where is lower triangular matrix with real and positive diagonal entries. To solve LLTα = b, we then solve the two triangular systems in sequence Ly = and LTα = y. Since is lower triangular matrix, we solve by forward substitution. Having computed y, we solve α by backward substitution because LT is upper triangular matrix. This factorization avoids any explicit matrix inversion and yields α efficiently and stably. 4.2 HYBRID ATTRIBUTION PRIORS Each method derives importance scores via distinct mechanisms, yielding its own advantages and shortcomings, as also corroborated by (Han et al., 2022). To leverage their complementary strengths and enhance overall explanation reliability and model robustness, we incorporate attribution priors from three sources: LIME, our proposed CAP derived from LLMs, and IG from the same model series. Specifically, LIME excels at identifying tokens with high affinity to the target label, CAP emphasizes salient class-boundary terms to distinguish similar classes, and IG captures the internal sensitivity of the model based on gradient information. For fusion, we explore two aggregation strategies: word-wise mean and word-wise max. Before aggregation, all attribution score vectors are normalized to ensure comparability. Formally, given attribution priors αlime, our proposed CAP priors αcap, and IG signals αig, we define the fused prior as αhybrid = aggregate ({(αlime), (αcap), (αig)}) (5) where aggregate denotes either the element-wise mean or max operation. This results in comprehensive word-level attribution prior that maintains full coverage while integrating multi-source rationale signals. 4.3 EXPLANATION-GUIDED LEARNING After obtaining attribution priors, we conduct word-level attribution alignment between the priors and the target model self-attribution. Specifically, we leverage integrated gradients to attribute the prediction of the target model to input features. Formally, given an input sentence = [e1, e2, ..., ed] consisting of tokens, ei Rl is the i-th token embedding encoded by the embedding layer, and is the dim of token embedding. For each feature e, is baseline feature, and the target model is (). IG estimates the contribution of each input by integrating the gradients of along the straight-line path from to as the integral path: (e) (e) = (cid:88) i= IGi(F, e) = (cid:88) [(ei i) i=1 (cid:90) µ=0 (e + µ (e e)) ei dµ]. (6) Then IG attribution scores αself is normalized for the subsequent alignment process. For clarity, let ap denote the normalized external attribution priors, and as represent the normalized self-attribution scores from the target model. We use Mean Squared Error (MSE) as the distance metric for alignment: La = ap as2. (7) For overall objective, we jointly optimize the standard vanilla cross-entropy loss between the model predictions and the ground-truth labels, and our attribution-driven objective to train the compact model. The overall objective is defined as: (8) where Lce = 1 i=1 yi log pθs(yixi) is the cross-entropy loss function, is the number of training samples and β is the hyperparameter that balances attribution guidance against the standard classification objective. = Lce + βLa, (cid:80)N Preprint. Under review."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 DATASETS AND BASELINES We evaluate on three intent classification datasets: HWU64 (Liu et al., 2019a), Banking77 (Casanueva et al., 2020) and Clinc150 (Larson et al., 2019). For the baseline, we select two methods with different principles commonly used in explanation guided learning for comparison: Integrated Gradients (IG) (Sundararajan et al., 2017) and LIME (Ribeiro et al., 2016). Further details are presented in Appendix A.2. 5.2 ADVERSARIAL DATASET CONSTRUCTION To assess the robustness of classifiers trained on the specific datasets, we automatically generate adversarial test sets using two complementary attack strategies: keyword addition and keyword replacement. Keyword Addition: Inject one or more keywords from an adversarial class into each original test example. The true label remains unchanged, so robust model should still predict the original class. Keyword Replacement: Replace keywords from the original class with those from the adversarial class, aiming to preserve fluency while misleading the model toward the adversarial label. In the adversarial class selection, we first applied our method CAP to calculate the wordlevel attribution scores of training dataset for each class. These scores are subsequently aggregated to obtain class-level keyword list. Pairwise difficulty is quantified by measuring the overlap of keyword distributions between classes. For every source class, we randomly sample adversarial target classes; the value of is determined by the total number of classes in the dataset (1 for HWU64, 2 for Banking77, and 3 for Clinc150). We construct this adversarial dataset by using GPT-4o, the generation prompt templates are provided in Appendix A.7. 5.3 IMPLEMENTATION DETAILS For evaluation metrics, we report accuracy to assess overall performance for full-data, few-shot and adversarial settings. Additionally, following (DeYoung et al., 2020), we report comprehensiveness and sufficiency to quantify how well the model explanations reflect the decision process. Specifically, given sample xi, the comprehensiveness measures the drop in confidence when the rationale ri is removed. Com = pθs (yixi) pθs (yixi ri) pθs(yixi) , (9) where yi denotes the predicted label of xi, and pθs(yixi) is the probability that model θs assigns to yi given input xi. higher comprehensiveness score indicates that ri is critical to the model decision, whereas lower score implies that the rationale has little influence. Conversely, sufficiency quantifies how well the extracted rationale alone supports the prediction. Suf = pθs(yixi) pθs(yiri) pθs(yixi) . (10) smaller sufficiency score, that is minimal drop in confidence when using only ri, indicates that the rationale by itself is sufficient for predicting yi. For parameter settings, we use RoBERTa-Base as the standard small language model for both training and extracting Integrated Gradients (IG) attributions. For extracting attribution scores using LIME and our proposed CAP method, we consistently adopt Llama-3-8B-Instruct as the large language model. Hyperparameters for training and the selection of aggregation can be found in the Appendix A.5 and A.6. 5.4 RESULT ANALYSIS To answer Q3, we conduct comprehensive experiments and report the evaluation results in both few-shot and full-data training scenarios. 7 Preprint. Under review. Methods Baseline HWU64 Banking77 Clinc150 Acc AccAD Com Suf Acc AccAD Com Suf Acc AccAD Com Suf Roberta-Base 75.56 62.36 65.52 34.97 74.04 53.82 67.06 38.30 87.99 71.70 65.38 30.91 Hybrid size = IG LIME CAP 77.04 65.80 68.93 32.67 79.09 56.57 68.05 36.79 89.58 72.29 67.09 26.36 77.13 66.96 68.79 31.53 78.50 56.96 73.21 33.19 89.24 74.49 71.23 26.68 76.57 66.31 68.35 29.76 78.83 57.24 72.95 30.98 89.20 73.39 65.69 26.35 Hybrid size = 2 77.88 66.40 66.86 25.49 78.57 57.12 72.33 31.83 89.64 73.38 67.50 26.97 IG+LIME CAP+IG 78.16 66.91 66.56 31.16 78.89 57.17 69.42 37.02 89.58 74.16 69.95 27.76 CAP+LIM 77.50 67.38 69.12 24.55 78.31 57.32 73.87 31.70 89.00 74.19 73.66 24.34 Hybrid size = 3 CAPHybrid 79.09 70.63 69.17 30.66 78.63 58.05 73.97 31.57 89.69 74.70 70.59 25.53 Table 1: Results on three datasets in the 5-shot setting. The Hybrid Size here represents the supervision attribution composed of several attributions combined together, e.g. CAP+LIM means that we merged attribution from CAP and LIME. The best and second-best results in each column are shown in bold and underlined. 5.4.1 FEW-SHOT RESULTS To evaluate whether our hybrid attribution prior can facilitate more reliable reasoning under lowresource conditions, we conduct experiments on 5-shot subsets of all three datasets. Table 1 reports the detailed results that on all of three datasets, single-prior (IG, LIME, and CAP) outperform the RoBERTa-Base baseline across all evaluation metrics. Each method exhibits unique strengths: IG consistently improves classification accuracy (e.g., 79.09% on Banking77), suggesting it helps the model focus on decision-critical tokens. LIME, by contrast, yields the highest comprehensiveness in most cases (e.g., 71.23% on Clinc150), indicating that its token-level perturbation captures prior more effectively. CAP achieves the best sufficiency score (29.76% on HWU64), showing that it guides the model to make predictions based on compact and essential evidence. This also illustrates our point of view that these attribution methods show different preferences in not only attribution mechanisms but also in the supervision of downstream tasks. In particular, combining these priors leads to synergistic effect. Hybrid variants generally outperform their individual counterparts, suggesting the complementarity of different attribution strategies. Among them, CAP+LIM achieves the lowest sufficiency and high comprehensiveness, which validates the effectiveness of our proposed CAP. Our final method, CAPHybrid, which integrates CAP, IG, and LIME priors, achieves the highest overall accuracy (79.09%) in HWU64 dataset. Simultaneously, it maintains strong interpretability, with comprehensiveness score of 69.12% and sufficiency score of 30.66%, both ranking second among all methods. This suggests that CAPHybrid not only improves prediction accuracy but also promotes more faithful and stable reasoning processes. In the in-domain adversarial setting, the RoBERTa-Base baseline exhibits substantial performance drop compared to the original test set, highlighting the vulnerability of models under standard training. In contrast, CAPHybrid significantly improves adversarial accuracy across all three datasets, with notable gains on HWU64 (8%) and Banking77 (4%). These results demonstrate that our method enhances robustness of models in more challenging settings. 5.4.2 FULL TRAINING RESULTS Table 2 reports the results on Banking77 under the full-data training setting. As shown, our method continues to demonstrate strong performance. CAPHybrid achieves the highest test accuracy 93.51%(vs 92.63%) and adversarial accuracy 84.97%(vs 70.29%), along with the best comprehensiveness 68.21%(vs 56.73%) and sufficiency 23.98%(vs 23.98%). These results confirm that our attribution-guided supervision remains effective even when ample training data is available. Preprint. Under review. Method Accuracy Test Adversarial Base IG LIME CAP CAPHybrid 92.63 93.02 92.82 93.41 93.51 70.29 77.91 75.85 78.20 84.97 Com Suf 56.73 61.75 62.87 68.00 68.21 35.87 32.81 32.23 24.30 23. Table 2: Performance on Banking77 under the full-train setting. The best and second-best results in each column are shown in bold and underlined. Com and Suf stand for the comprehensiveness and sufficiency calculated on test set respectively. 5.5 QUALITATIVE ANALYSIS OF DATASET PROPERTIES AND METHOD EFFECTIVENESS To further investigate the factors affecting the effectiveness of our method and to better understand the conditions under which attribution priors offer the greatest benefit, we conduct qualitative analyses centered on two key observations. First, Figure 4(d) illustrates the relationship between the number of corrected cases achieved by our attribution-guided training and the lexical overlap between the ground-truth labels and the models original incorrect predictionsa proxy for instance difficulty. The results reveal that our method is particularly effective in handling challenging examples (difficulty levels 3 and 4), where the confusing classes share high degree of token-level similarity. This highlights the models enhanced ability to tease apart subtle semantic distinctions that are often lost under standard training. Second, we analyze the overall structure of each dataset by computing the mean sentence embedding for each class and measuring pairwise class similarities. As shown in the similarity heatmaps Figure 4(a)-(c), Banking77 and HWU64 contain clusters of highly similar classes, indicating the presence of inherent difficulty imbalance. In contrast, Clinc150 exhibits more uniformly distributed and well-separated class representations, with less inter-class confusion. This distinction clarifies our empirical findings: on HWU64 and Banking77, baseline models frequently misclassify these fine-grained categories due to spurious token co-occurrence patterns. Our approach, by aligning predictions with richer and more discriminative attribution priors, mitigates this issue and enables the model to focus on features that are more indicative of class-level meaning rather than surface lexical overlap. In contrast, for Clinc150, even simple priors derived from LIME suffice to provide meaningful guidance, thereby limiting the room for further gains. Consequently, our method achieves marginal improvements in that setting. These findings suggest that our attribution-based supervision not only improves robustness to lexical ambiguity but also enhances the models capacity for deeper semantic reasoning, particularly in challenging scenarios involving closely related or fine-grained classes. (a) HWU64 (b) Banking (c) Clinc150 (d) Breakdown Figure 4: Subfigure (a)-(c) show the class similarity of HWU64, Banking77 and Clinc150 datasets. Darker colors denote higher similarities. Subfigure (d) shows the imporvements performance breakdown across difficulty levels in 5-shot Banking77 dataset. 9 Preprint. Under review."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We identify critical limitation in existing attribution guided learning: when categories exhibit high semantic overlap, models trained with surface-level priors often fail to distinguish fine-grained classes. To address this, we propose Class-Aware Attribution Priors (CAP), which inject taskspecific and label-informed semantic constraints into model training, encouraging more meaningful feature attribution. We further introduce CAPHybrid, fusion approach that integrates multiple complementary attribution sources to form richer, more balanced priors. Our method consistently enhances performance, interpretability, and robustness across various datasets, offering promising direction toward building more transparent and reliable models."
        },
        {
            "title": "REFERENCES",
            "content": "Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7), 2015. Moritz Bohle, Mario Fritz, and Bernt Schiele. B-cos networks: Alignment is all we need for interpretability. In CVPR, pp. 1031910328, 2022. Inigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. Efficient intent detection with dual sentence encoders. In The 2nd Workshop on Natural Language Processing for Conversational AI, pp. 3845, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pp. 41714186, 2019. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. ERASER: benchmark to evaluate rationalized NLP models. In ACL, pp. 44434458, 2020. Yuyang Gao, Tong Steven Sun, Guangji Bai, Siyi Gu, Sungsoo Ray Hong, and Liang Zhao. RES: robust framework for guiding visual explanation. In KDD, pp. 432442, 2022a. Yuyang Gao, Tong Steven Sun, Liang Zhao, and Sungsoo Ray Hong. Aligning eyes between humans and deep neural network through interactive attention alignment. Proc. ACM Hum. Comput. Interact., 6(CSCW2):128, 2022b. Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju. Which explanation should choose? function approximation perspective to characterizing post hoc explanations. In NeurIPS, 2022. Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In ACL, pp. 27442751, 2020. Sahil Jayaram and Emily Allaway. Human rationales as attribution priors for explainable stance detection. In EMNLP, pp. 55405554, 2021. Amlan Jyoti, Karthik Balaji Ganesh, Manoj Gayala, Nandita Lakshmi Tunuguntla, Sandesh Kamath, and Vineeth N. Balasubramanian. On the robustness of explanations of deep neural network models: survey. CoRR, abs/2211.04780, 2022. Davinder Kaur, Suleyman Uslu, Kaley J. Rittichier, and Arjan Durresi. Trustworthy artificial intelligence: review. ACM Comput. Surv., 55(2):39:139:38, 2023. Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. An evaluation dataset for intent classification and out-of-scope prediction. In EMNLP, pp. 1311 1316, 2019. Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, and Yunan Zhang. Unifying model explainability and robustness for joint text classification and rationale extraction. In AAAI, pp. 1094710955, 2022a. 10 Preprint. Under review. Dongfang Li, Baotian Hu, Qingcai Chen, and Shan He. Towards faithful explanations for text classification with robustness improvement and explanation guided training. CoRR, abs/2312.17591, 2023. Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S. Yu, and Lifang He. survey on text classification: From traditional to deep learning. ACM Trans. Intell. Syst. Technol., 13(2):31:131:41, 2022b. Frederick Liu and Besim Avci. Incorporating priors with feature attribution on text classification. In ACL, pp. 62746283, 2019. Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. Benchmarking natural lanIn IWSDS, volume 714, pp. guage understanding services for building conversational agents. 165183, 2019a. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019b. Scott M. Lundberg and Su-In Lee. unified approach to interpreting model predictions. In NeurIPS, pp. 47654774, 2017. Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Towards faithful model explanation in NLP: survey. Comput. Linguistics, 50(2):657723, 2024. Amin Parchami-Araghi, Moritz Bohle, Sukrut Rao, and Bernt Schiele. Good teachers explain: Explanation-enhanced knowledge distillation. In ECCV, pp. 293310, 2024. Sameer Pimparkhede and Pushpak Bhattacharyya. Main predicate and their arguments as explanation signals for intent classification. In NAACL, pp. 1077810789, 2025. William Press. Numerical recipes 3rd edition: The art of scientific computing. Cambridge university press, 2007. Sukrut Rao, Moritz Bohle, Amin Parchami-Araghi, and Bernt Schiele. Studying how to efficiently and effectively guide models with explanations. In ICCV, pp. 19221933, 2023. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. why should trust you?: Explaining the predictions of any classifier. In KDD, pp. 11351144, 2016. Andrew Slavin Ross, Michael C. Hughes, and Finale Doshi-Velez. Right for the right reasons: Training differentiable models by constraining their explanations. In IJCAI, pp. 26622670, 2017. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, pp. 618626, 2017. Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, et al. Mavors: Multi-granularity video representation for multimodal large language model. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 1099411003, 2025. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, pp. 33193328, 2017. Kamal Taha, Paul D. Yoo, Chan Yeob Yeun, Dirar Homouz, and Aya Taha. comprehensive survey of text classification techniques and their research applications: Observational and experimental insights. Comput. Sci. Rev., 54:100664, 2024. Erico Tjoa and Cuntai Guan. survey on explainable artificial intelligence (XAI): toward medical XAI. IEEE Trans. Neural Networks Learn. Syst., 32(11):47934813, 2021. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2019. 11 Preprint. Under review. Zifan Wang, Haofan Wang, Shakul Ramkumar, Piotr Mardziel, Matt Fredrikson, and Anupam Datta. Smoothed geometry for robust attribution. In NeurIPS, 2020. Mayur Wankhade, Annavarapu Chandra Sekhara Rao, and Chaitanya Kulkarni. survey on sentiment analysis methods, applications, and challenges. Artif. Intell. Rev., 55(7):57315780, 2022. Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang, and Rui Wang. AD-KD: attribution-driven knowledge distillation for language model compression. In ACL, pp. 84498465, 2023. Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu, and In So Kweon. survey on universal adversarial attack. In IJCAI, pp. 46874694, 2021. Feng Zhang, Wei Chen, Fei Ding, and Tengjiao Wang. Dual class knowledge propagation network for multi-label few-shot intent detection. In ACL, pp. 86058618, 2023. YiFan Zhang, Yang Shi, Weichen Yu, Qingsong Wen, Xue Wang, Wenjing Yang, Zhang Zhang, Liang Wang, and Rong Jin. Debiasing multimodal large language models via penalization of language priors. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 42324241, 2025. Ruiqi Zhong, Steven Shao, and Kathleen R. McKeown. Fine-grained sentiment analysis with faithful attention. CoRR, abs/1908.06870, 2019."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THE USE OF LARGE LANGUAGE MODELS During the preparation of this paper, we used large language models (LLMs) solely as generalpurpose writing assistants. Specifically, LLMs were employed to help refine the clarity, grammar, and readability of our drafts, as well as to suggest alternative phrasings in English. Importantly, all conceptual contributions including the design of research questions, development of methods, execution of experiments, and interpretation of results were conceived and carried out entirely by the authors. The authors carefully reviewed and edited all text suggested by LLMs to ensure accuracy and originality, and we take full responsibility for the final content of the paper. A.2 DATASETS AND BASELINES We evaluate on three multi-domain intent classification datasets, each covering diverse real-world scenarios: (1) HWU64 (Liu et al., 2019a) contains 64 intents drawn from 18 everyday domains (e.g., alarm, cooking, transportation), with utterances balanced across intents. (2) Banking77 (Casanueva et al., 2020) comprises 13,083 labeled customer utterances spanning 77 fine-grained intents in the banking domain. (3) Clinc150 (Larson et al., 2019) offers 22,500 user utterances evenly distributed over 150 intents in 10 general domains, facilitating robust cross-domain evaluation. For the baseline, we select two methods with different principles commonly used in explanation guided learning for comparison: Integrated Gradients (IG) (Sundararajan et al., 2017) We first train model using only the classification loss and then compute IG attribution scores on the training set using this model. These attribution scores are subsequently incorporated as supervision signals to train new model from scratch. LIME (Ribeiro et al., 2016) We employ LIME to compute attribution scores on the training dataset from large language model, and use these scores as external supervision to guide the training of the smaller model. A.3 PROOF OF EQ. (3) This section describes the proof of Eq. (3). For clarity, we first restate our objective, i.e. Eq. (1): α = argmin{ (cid:88) i=1 (zi αTmi)2 + λα2}, (11) 12 Preprint. Under review. where α Rd is the attribution parameter vector to be estimated, each mi Rd is mask vector with associated score zi, and λ > 0 is the regularization coefficient. To find the minimizer, we calculate its derivative. Specifically, let L(α) be the function, L(α) = (cid:88) i= (zi αTmi)2 + λαTα. The derivative of L(α) with respect to α is as follows, αL = 2 (cid:88) i=1 (zi αTmi)mi + 2λα. To find the minimizer, we set αL = 0, yielding (cid:88) (αTmi)mi + λα = i=1 (cid:88) i= zimi. (12) (13) (14) Next we transform the left-hand side of Eq. (14). For (αTmi)mi item, let = αTmi, which is Tα. We have scalar. Then tmi = mit and αTmi = mi (αTmi)mi = mit (15) (16) = mi(mi = (mimi Tα) T)α. T)α into Eq. (14) gives (mimi T)α + λα = (cid:88) i=1 zimi, Substituting (αTmi)mi = (mimi (cid:88) which is exactly Eq. (3). i= A.4 PROOF OF POSITIVE DEFINITE MATRIX We now show that = TM + λI, is both symmetric and positive definite, where is the identity matrix and λ > 0. Symmetry. AT = (M TM + λI)T = (M TM )T + (λI)T = TM + λI = A. Positive Definiteness. For any nonzero real column vector x, we have xTAx = xT(M TM + λI)x = xT(M TM )x + xT(λI)x = (xTM T)(M x) + λxTx = x2 + λx2. Since x2 0 for all and λx2 > 0 whenever = 0, it follows that xTAx > 0, = 0, (17) thus is positive definite. 13 Preprint. Under review. Dataset Banking77 Clinc150 HWU64 Aggretion Mean Max Mean Max Mean Max IG+LIME CAP+IG CAP+LIME CAPHybrid 78.57 78.81 77.89 78.63 76.91 78.89 78.31 78.47 89.64 89.58 89.00 89.69 89.44 89.31 88.71 88.98 77.88 78.16 77.50 77.41 77.53 78.04 77.29 79. Table 3: The comparision of accuracy for different aggregation methods on the 5-shot setting. {original text} {original label} {adversarial label} Generate keyword addition adversarial example for the following Task: text by adding keywords from adversarial class without changing the classification result. Original text: Original class: Adversarial class: Adversarial class keywords: Adversarial class example sentences: {adversarial sentence} Requirements: 1. 2. overly explicit 3. 4. 5. Adversarial example: Ensure the new text is still classified as the target class Ensure the text remains natural and fluent Only return the modified text, nothing else Preserve the original meaning as much as possible Add 1{2 keywords from the source class, but avoid making the intent {adversarial keywords} Table 4: The template of the prompt we used for generate Keyword Addition adversarial examples. A.5 TRAINING PARAMETER SETTING When calculating our class-aware arribution priors, we set the λ = 0.1 and = 100 in Eq. (1). And when training the Roberta-Base model, we set the learning rate to 5e-5, batch size to 16, and use early stopping with patience of 10. Additionally, we apply gradient norm clipping with maximum norm of 1.0. For our attribution-aligned training, we experiment with attribution loss weighting coefficient β = 1. While both mean and max aggregation strategies were considered for hybrid prior methods, we report only the test results corresponding to the combination that achieved the best validation performance. The comparison experiment can be found in Appendix A.6. A.6 THE COMPARSION OF DIFFERENT AGGREGATION OPERATION We compare different aggregation strategies in terms of their impact on classification accuracy. As shown in Table 3, the mean aggregation achieves the best predictive performance in most cases. A.7 ADVERSARIAL DATASET GENERATION PROMPT In this section, we present all the prompts utilized for constructing the adversarial datasets. 14 Preprint. Under review. {original text} {original label} {adversarial label} Generate keyword replacement adversarial example by replacing Task: original class keywords in the following text with keywords from adversarial class. Original text: Original class: Adversarial class: Adversarial class keywords: Adversarial class example sentences: {adversarial sentence} Requirements: 1. optionally add new ones 2. conjunctions) 3. 4. 5. Adversarial example: Ensure the new text remains semantically coherent and realistic Ensure the modified text is classified into the new target class Only return the modified text, nothing else Preserve the original sentence structure (e.g., questions, Replace target keywords with suitable original-class keywords, and {adversarial keywords} Table 5: The template of the prompt we used for generate Keyword Replacement adversarial examples."
        }
    ],
    "affiliations": [
        "Department of Systems Engineering and Engineering Management, CUHK",
        "Key Lab of High Confidence Software Technologies, Peking University",
        "Kling Team",
        "School of Computer Science, Peking University"
    ]
}