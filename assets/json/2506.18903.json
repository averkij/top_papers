{
    "paper_title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory",
    "authors": [
        "Runjia Li",
        "Philip Torr",
        "Andrea Vedaldi",
        "Tomas Jakab"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control."
        },
        {
            "title": "Start",
            "content": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory Philip Torr Andrea Vedaldi"
        },
        {
            "title": "Runjia Li",
            "content": "University of Oxford {runjia, phst, vedaldi, tomj}@robots.ox.ac.uk v-mem.github.io 5 2 0 2 3 2 ] . [ 1 3 0 9 8 1 . 6 0 5 2 : r Figure 1. VMem enables interactive scene generation from single image along user-specified trajectories in an auto-regressive manner. The green region shows results with memory, maintaining consistency when generating previously observed parts of the scene. The red region, without memory, exhibits visual drift highlighted with red ellipses, demonstrating the effectiveness of VMem for consistent scene generation."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We propose novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control. We consider the problem of generating long videos that explore an imagined space following camera path specified interactively by the user. In this paradigm, the user tells the model which camera path to follow for the next few frames, observes the generated content, and then decides where to explore next based on what they have seen. For example, possible exploration of house may involve visiting the kitchen, the living room, and the bathroom, eventually returning to the kitchen. Throughout the video, the scene must remain consistent, ensuring that the kitchen looks the same upon return. The ability to generate such videos is essential for immersive applications such as games, where players can navigate generated worlds. However, even recent large-scale interactive video generators such as Googles Genie 2 [21] struggle to achieve this goal. This problem has so far been addressed by two types of methods. First, out-painting-based methods [7, 12, 16, 17, 19, 22, 25, 27, 37, 42] iterate between generating new 2D views of the scene and estimating its 3D geometry. They use the estimated geometry to partially render new viewpoint and then employ an out-painting model to complete the missing parts, thus adding one more image to the col1 lection. However, errors in out-painting, 3D reconstruction, and stitching accumulate over time, leading to severe degradation of the generated content after short while. Second, multi-view/video-based methods [21, 23, 26, 36, 41] condition the generation of novel view on previously generated views using geometry-free approach that does not explicitly estimate the scene geometry. While this avoids the accumulation of errors in the reconstructed 3D scene geometry, it comes at high computational cost, limiting the number of conditioning views to small context window consisting of only the most recent few frames. This constraint negatively impacts the long-term consistency of the generated images. In this work, we revisit the second class of methods and propose conditioning not on the most recently generated views, but on the most relevant ones for generating the next image, thereby maintaining high degree of consistency within limited computational budget. Given novel viewpoint of the scene, the most relevant past views are those that have already observed the parts of the scene currently being generated. This implies that, for each part of the scene, we must remember which views have previously observed it and allow one to retrieve them from memory. To achieve this, we introduce the Surfel-Indexed View Memory, abbreviated as VMem, memory mechanism that anchors previous views to the 3D surface elements they observe. Given new viewpoint, we retrieve the past views that best capture the currently observed surfaces and use them to condition the generation of the novel view. To create the memory, we estimate the geometry of each new view using an off-the-shelf point map predictor. This is similar to out-painting methods but, crucially, we do not use the estimated geometry as the final representation of the scene; instead, we use it to construct VMem, which is memory of past views. We represent the scene geometry using surfels which, compared to meshes, are more robust, and, compared to point clouds, can represent occlusions. Each surfel stores in its attributes set of indexes corresponding to past viewpoints that observed it. To retrieve relevant past viewpoints, we render the surfels with their attributes from the novel viewpoint and splat them onto an image grid. Each pixel in the resulting image then corresponds to set of viewpoint indexes. We select the top most frequently represented viewpoint indexes and use them to retrieve past views from the database, where each view is represented by an RGB image and its corresponding camera parameters. key advantage of our approach over inpainting-based methods is that it does not require highly accurate scene geometry. As long as we successfully retrieve the most relevant past views, our method remains robust. By leveraging Surfel-Indexed View Memory, we significantly reduce the memory and computational burden of conditioning on large number of previous views, as required by multi-view/video-based methods, while improving long-term consistency across generated novel views. Our approach represents step toward scalable, realistic, and long-term autoregressive scene generation, making the following contributions: 1. We introduce Surfel-Indexed View Memory, plug-andplay module to index past views geometrically and use them to condition the generation of novel views. 2. We show that our method can generate long-term coherent videos of scenes, outperform existing approaches. 3. We validate our approach on number of challenging benchmarks, outperforming the current open-source state-of-the-art methods. 2. Related Work Novel view synthesis (NVS) is challenging and ill-posed problem. NVS methods can generally be categorized into two main approaches: view interpolation methods [3 6, 9, 20, 31, 38], which generate views that stay close to the given inputs, and view extrapolation or perpetual view generation, where novel views extend significantly beyond the original scene, often introducing large amounts of new content. The latter is particularly difficult when only single image is available. Between these two categories lie methods focusing on single-view scene reconstruction, which have been mostly successful for scenes containing single object [1, 13, 18, 28, 30, 39] or highly bounded scenes [29]. However, their extrapolation capabilities remain very limited. The most relevant to our work are models for singleimage perpetual view generation, which typically fall into two main categories: those that incorporate explicit geometric modeling with inpainting techniques and those directly based on image or video generation. Inpainting-based perpetual view generation. Inpaintingbased methods [7, 12, 16, 17, 19, 22, 25, 27, 37, 42] use pre-trained 3D reconstruction methods to generate explicit 3D representationssuch as meshes, point clouds, or Gaussian splatsfrom an initial image. These representations are reprojected into novel views, where 2D inpainting models fill in missing regions. For example, SceneScape [7] reconstructs mesh from the first image using pre-trained depth estimator. diffusion-based inpainting model then completes the projected novel view, which is reprojected back to refine and extend the mesh. This process is iterated to generate additional views. MultiDiff [19] also relies on depth estimator to warp the reference image into multiple novel views, training diffusion model to inpaint missing regions simultaneously. CamCtrl3D [22] follows similar approach but incorporates ray map to better condition the diffusion model for view synthesis. ViewCrafter [42] uses pre-trained pointmap estimator to create 3D point cloud Figure 2. Method overview. Given novel camera viewpoints {cT +m}M m=1, we first use our proposed memory module Surfel-Indexed View Memory (s) to retrieve the most relevant past views vt (s) as conditioning reference views. For each retrieved view, the camera parameters ct are represented using Plucker embeddings, which are spatially resized via interpolation to match the resolution of the encoded image latents. The corresponding reference images xt are encoded using VAE to obtain latent feature maps. These resized Plucker embeddings and VAE-encoded latents are then fed into the camera-conditioned image-set generator ψ to synthesize the m=1. Once the new views are generated, we update the surfel-indexed memory (s) (s+1) by appending the novel views {xT +m}M new view indices {T + m}M m=1 to existing surfels or creating new ones based on the estimated geometry of the generated views. This process is repeated autoregressively for each subsequent generation step. and trains video diffusion model for inpainting. GenWarp [27], uniquely, warps 2D coordinate representation of the reference view into novel views and uses it to condition generation. While these methods improve geometric consistency, they are highly susceptible to errors in depth or pointmap estimation, which can propagate distortions across generated views. Once the 3D representation is constructed, any inaccuracies from depth or pointmap estimation become difficult to correct. Moreover, scaling these approaches to large scenes is computationally intensive, as storing and processing explicit high-fidelity 3D representations requires substantial memory. Multi-view-based perpetual view generation. Another category of methods [8, 23, 26, 32, 36, 41, 46] avoids explicitly modeling the 3D geometry of the scene. Instead, these approaches generate novel views by conditioning on previously rendered or generated views. GeoGPT [26] establishes this direction by using an autoregressive likelihood model to synthesize novel perspectives from single input image. Building upon this, LookOut [23] improves consistency by generating view sequences along predefined camera trajectory, conditioning on up to two preceding views. It also incorporates postprocessing step that reuses previously generated frames as additional context for future predictions. Other methods, such as [32, 41], enhance novel view synthesis by employing cross-view attention or enforcing epipolar constraints within diffusion models. MotionCtrl [36], in particular, leverages geometric priors from pre-trained video diffusion model to synthesize camera-controllable videos. All of the aforementioned methods are trajectory-based, requiring the input views to follow continuous spatial path. In contrast, image-set models like CAT3D [8] are capable of synthesizing novel views from sparse and unordered set of input images, without any trajectory constraint. Most recently, SEVA [46] has emerged as strong open-source alternative in this image-set modeling paradigm. However, all these methods are computationally expensive, with an O(n2) complexity for attention, which restricts the number of conditioning views to small context window containing only few of the most recently generated frames. This limitation negatively impacts the longterm consistency of the generated images. Several concurrent works have explored memory-based approaches for consistent scene generation. StarGen [44] and WorldMem [40] use distance and field-of-view-based spatial memory retrieval, which can recover spatially correlated views but lack geometric reasoning for occlusion handling. Genie [2, 21] maintains memory through recurrent state features. Gen3C [24] conditions on stored point clouds similar to ViewCrafter [43], inheriting the limitations of inpainting-based methods. In contrast, our surfelbased memory explicitly models occlusion and provides more principled view selection. 3. Method Let x1 RHW 3 be an RGB image of scene that we wish to explore, and let {ct}T t=1 be sequence of camera parameters specifying path through the scene. Our goal is to generate corresponding video, i.e., sequence of images {xt}T t=1, where x1 is the given input image, and each 3 Figure 3. Visualization of our surfel-based memory index. Each surfel in the 3D scene stores indices of all views that have observed it. For visualization, we color-code each surfel by the indices of the contributing views. This spatial index enables efficient retrieval of relevant past views: when generating new view, we identify which surfels are visible from the target viewpoint and retrieve the views that previously observed those same regions, naturally accounting for occlusion. subsequent frame xt is consistent with the previous ones and reflects the specified camera ct. Moreover, we aim to do so autoregressively, generating novel views at time for the given cameras (with kept small to ensure interactivity) while ensuring consistency with previously generated views. This enables interactive scene exploration, where the user can specify the next camera positions at each step. We define view as tuple vt = (xt, ct) consisting of an image xt and its corresponding camera ct at timestep t. At generation step s, we have generated = sM, 0 frames so far. The goal is to generate new RGB images {xT +m}M m=1 for the next target camera parameters {cT +m}M m=1, given the previously generated views (s) = {v1, v2, . . . , vT }. After generation, we update +M . challenge with this approach is that the context (s) grows unbounded over time. Video generators addressing this problem [23, 26, 36, 41] typically consider fixedlength subset of (s), conditioning their generation only on the most recent views {vT L+1, . . . , vT }. This limitation results in severe inconsistencies when the generated sequence extends beyond the (small) context window. We solve this problem by dynamically retrieving the most relevant subset of past views, denoted as (s). To achieve this, we introduce an efficient data structure, the Surfel-Indexed View Memory, that stores and retrieves past views based on their approximate 3D geometry. We first describe the Surfel-Indexed View Memory (Sec. 3.1) and then introduce the novel view generator (Sec. 3.2). An overview of the method is provided in Fig. 2. 3.1. Surfel-Indexed View Memory the problem of generating the next views Consider {xT +m}M m=1 of the scene as seen from the cameras {cT +m}M m=1. The generator must consider the previously generated views and ensure {xT +m}M m=1 are consistent with those, while generating new content as needed. However, not all past views are equally correlated with the novel views {xT +m}M m=1. We prioritize past views that have likely observed the largest portion of the scene currently being generated. For example, if the current locale is surrounded by walls, parts of the scene behind those walls are likely less relevant for generating new view of that locale. Conversely, views that share similar visible region with the target view can provide more useful information for generating its content. This principle drives our design for view memory mechanism. We maintain coarse model of the scene geometry using surfels (visualized in Fig. 3), simple surface primitives that account for occlusion. Each surfel stores the indices of the past views that observed it. To generate new view, the surfels visible from the new viewpoint are retrieved, using the associated indices to vote for which views to consider for generation. This process is illustrated in Fig. 4. Specifically, we define surfel as the tuple sk = (pk, nk, rk, Ik) , where pk R3 denotes the surfels 3D position, nk R3 is its surface normal, rk is the surfel radius, and Ik {1, 2, . . . , } is subset of past view indices that observed the surfel. The surfel-based memory indexing of past views (s), encapsulating the scene, is represented as set of surfels (s) = {sk}N (s) k=1 , where (s) is the number of surfels at generation step s. We also maintain an octree to quickly retrieve surfels based on their geometry. Next, we describe how to read from and write to this memory. Reading from the memory. To retrieve the most relevant past views from the memory (s) for the novel cameras {cT +m}M m=1, we first compute their average pose cs SE(3) as reference (refer to Appendix A). Then we render the surfels (s) from the averaged camera pose cs. Each surfel is rendered as splat with its attributes, accounting for relative depth and occlusions, and contributing to the image based on its coverage. The rendered attributes correspond to the indices of the previous views. The core intuition is that views observing the largest portion of the scene from the perspective of the novel camera are most relevant for novel view synthesis. To identify these views, we rank 4 Figure 4. Surfel-Indexed View Memory. Reading from the memory involves rendering the surfels (s) along with their attributes, which contain past view indices represented by frame indices. We then select the most frequently represented frame indices in the rendered image to retrieve the most relevant past views from (s). Writing to the memory involves estimating the geometry of the newly generated views {xT +m}M m=1 are appended to the surfels observed in these views, and the novel views are stored in memory, updating (s) (s+1) and (s) (s+1). m=1 in the form of surfels and merging them with the existing surfels. The frame indices {T + m}M past view indices based on their frequency across all rendered pixels. We then select the top-K most frequently represented indices and use them to retrieve the corresponding past views, = {vt}tI , from the memory (s). This subset of views is used to feed the new view generator, as described in Sec. 3.2. To avoid oversampling repeatedly visited regions, we apply non-maximum suppression algorithm that reduces redundancy in memory and promotes broader scene coverage among the top-K views. During retrieval, we retain only the most frequently referenced view among those with similar poses. Additionally, during writing to the memory, we merge surfels by comparing their associated camera posesif two poses are highly similar, we discard the older one. Writing to the memory. After generating the new views {vT +m = (xT +m, cT +m)}M m=1, we add them to the memory (s) and update the surfel-based memory index (s). We use an off-the-shelf point map estimator ϕ, such as CUT3R [33], to output their corresponding point maps +m}M {P m=1. Specifically, we jointly estimate the point maps for the newly generated views along with the retrieved past views . This joint estimation ensures that the new point maps are aligned with the coordinate frame of the existing scene geometry in the surfel-based memory index (s). Next, we convert the estimated point maps to set of new surfels. For each newly generated frame {T + 1, + 2, . . . , + }, we process its point map . Since our approach only requires coarse geometry, we first downsample the point map by factor σ, yielding smaller point map Pt RH 3, where = H/σ and = W/σ. For each pixel location (u, v) in the downsampled point map, we have 3D point pu,v,t, and we create new surfel sk centered on it. We compute each surfels normal using the cross-product of displacement vectors to neighboring pixels. For pixel at 2D location (u, v) in the image grid, we use neighboring pixels to estimate the local surface normal: nk = (pu+1,v,t pu1,v,t) (pu,v+1,t pu,v1,t) (pu+1,v,t pu1,v,t) (pu,v+1,t pu,v1,t) . where pu,v,t represents the 3D point at image coordinates (u, v) in frame t. To ensure that the surfels reasonably cover the scene, we use heuristic to compute the surfels radius, which is proportional to the depth of the surfel and inversely proportional to the focal length and the cosine of the angle between the surfel normal and the viewing direction: rk = 1 2 Du,v,t/ft α + (1 α) nk (pu,v,t Ot) , where Du,v,t represents the depth at pixel (u, v) in frame t, Ot is the camera center of frame t, pu,v,t Ot is the viewing direction, and ft is the focal length at time t. The factor α is used to avoid extreme values when nk (pu,v,t Ot) 0. Finally, we check if the index (s) already contains surfel sk similar to the new surfel sk. Two surfels match if their centers are within distance and the cosine similarity between their normals is above threshold θ. If such surfel sk is found, we add the frame index to the surfels set of past view indices: Ik Ik {t}, and discard sk. If not, we set Ik = {t} and add sk to the index (s+1). This process transforms the surfel memory from (s) to (s+1) with (s+1) surfels for the next generation step. 5 3.2. New View Generation We use camera-conditioned image-set generator ψ to generate new views. The generator ψ takes retrieved reference views = {vt = (xt, ct)}tI and the target camera poses {cT +m}M m=1 for the new frames to be generated and samples the new views: {xT +m}M m=1 ψ (cid:0){(xt, ct)}tI , {cT +m}M m=1 (cid:1) , where {1, 2, . . . , } are the frame indices of the retrieved reference views. Specifically, we base our generator on the recently released pre-trained SEVA [46] model. Given the plug-andplay nature of our memory module, VMem can work with other image-set generators as well. While our model can operate with SEVA out-of-the-box, we find that fine-tuning it for our task further improves the efficiency and quality of the generated images. This is due to two key reasons. First, SEVA is trained to work with large fixed total number of reference and target frames, which requires high computational resources unavailable to most. Our memoryefficient design selects only the most relevant reference views from all possible context views, leading to better efficiency. In addition, the interactive nature of our task requires generating only small number of new frames at time, before the user inputs new commands. Thus, we finetune the model to work with smaller fixed number of reference and target views. Second, unlike SEVAs original setting where the target views may be spatially sparse, our target views follow coherent trajectory with smoothly varying camera poses. This structured setting simplifies the generation task and enables the model to benefit from domain-specific adaptation through fine-tuning. Additional training details are provided in Sec. 4.1. 4. Experiments We conduct experiments on long-term, short-term, and cycle-trajectory perpetual view generation tasks, comparing our models to previous open-source single-view approaches. Our results show that VMem can generate more coherent long-term scenes and demonstrates significantly better camera control than all existing open-source video generation methods. 4.1. Implementation Details The novel view generator of VMem is trained on the RealEstate10K dataset [47] using SEVA [46] as pretrained backbone. The number of context views is set to 4, and the number of target views is also 4. We randomly sample context views online during training. We use LoRA [14] with rank 256 to fine-tune the base model. The model is trained for 600,000 iterations on 8 A40 GPUs, with batch 6 size of 24 per GPU. The model is optimized using AdamW with learning rate of 3 106, weight decay of 104, and cosine annealing schedule. During inference, the classifier-free guidance scale is set to 3. The scaling factor σ for the point map is set to 0.03, and α is set to 0.2 in the surfel radius calculation. 4.2. Evaluation Metrics We evaluate the quality of generated novel views based on three key factors: (1) the overall quality of generated novel views, (2) the models ability to preserve input image details across views, and (3) the alignment between generated camera poses and ground truth. For (1), we compare the distribution of the generated novel views to that of the test set using Frechet Image Distance (FID) [11]. For (2), we measure peak signal-to-noise ratio (PSNR) for pixel differences, along with LPIPS [45] and SSIM [35], as done in [23]. For (3), following [10], we express each estimated camera pose relative to the first frame and normalize the translation by the furthest frame. We use DUSt3R [34] to extract poses from generated views. We then calculate the rotation distance Rdist by comparing the the ground truth and extracted rotation matrices of each generated novel view sequence, expressed as: Rdist = arccos (cid:32) tr(Rgen RT gt) 1 2 (cid:33) , where denotes rotation matrix. We also compute the translation distance tdist as: tdist = tgt tgen2 . 4.3. Datasets and Evaluation Settings We consider two real-world datasets for new view synthesis evaluation: RealEstate10K [47] and Tanks-andTemples [15]. RealEstate10K comprises video clips of indoor scenes. We use the training split of RealEstate10K to train VMem. For evaluation, following [23, 41], we only consider test sequences with at least 200 frames. Starting from the first frame of the ground truth test sequence, we generate 20 images using 20 camera poses along the ground truth trajectory, spaced ten frames apart, with the final pose 200 frames from the initial view. Short-term evaluations use the fifth generated image, while long-term evaluations focus on the final image. For cycle evaluation, each model generates frames from the initial to the final camera pose and then back to the start, with evaluations conducted only on the return trajectory. The goal of this evaluation is to test the models ability to maintain scene consistency when revisiting previously traversed paths. For Tanks-andTemples, which includes out-of-domain indoor and outdoor scenes featuring much larger motion speeds, we use all six advanced scenes and evaluate cycle-trajectory settings for the first 50 frames without skipping frames. Figure 5. Generalization to long sequences with revisitations. We compare scene generation with and without our proposed memory module (VMem). For each sequence, an input image is shown on the left, followed by generated frames at selected frame indices. Our method (top row in each pair) maintains high spatial consistency across revisited viewpoints, while the baseline without memory (bottom row) suffers from visual drift and structural degradation over time. Short-term (50thframe) Long-term (> 200thframe) LPIPS PSNR SSIM FID Rdist Tdist LPIPS PSNR SSIM FID Rdist Tdist GeoGPT [26] Lookout [23] PhotoNVS [41] GenWarp [27] MotionCtrl [36] ViewCrafter [42] VMem 0.444 0.378 0.333 0.436 0.424 0.377 0.287 13.35 14.43 15.51 12.03 12.00 16.97 18.49 0.144 0.148 0.262 0.406 26.72 28.86 21.76 29.69 19.25 25.39 17.12 1.241 0.564 0.341 1.562 0.219 0.876 0.059 0.356 0.208 0. 0.674 0.658 0.588 0.613 0.605 0.592 0.493 9.54 10.51 11.54 9.56 9.13 9.74 13.12 0.085 0.083 0.148 0.183 41.87 58.12 41.95 36.40 35.45 34.12 27.15 1.142 0.850 0.748 2.177 0.811 0.924 0.251 0.609 0.827 0. Table 1. Quantitative single-view NVS comparison on RealEstate10K Lower is better for LPIPS, FID, Rdist, Tdist; higher is better for PSNR, SSIM. VMem surpasses all baseline methods on this benchmark, but its true advantagespatial consistencyis not fully reflected, as the trajectories in RealEstate10K rarely revisit previously observed areas. 4.4. Cycle-Trajectory Perpetual View Generation The key advantage of VMem is its ability to remember previously generated content across different locations, as demonstrated by the cycle-trajectory setting. VMem surpasses all baseline methods on both in-domain (Tab. 2) and out-of-domain (Tab. 3) benchmarks. Additionally, qualitative comparisons in Fig. 6 clearly illustrate that VMem can generate realistic content in unseen regions with signifi7 cantly more precise camera control than both inpaintingbased and multi-view/video-based baselines. Furthermore, VMem preserves the memory of previously generated frames, ensuring scene consistency upon revisiting previous locationsan ability that none of the multi-view/videobased baseline methods possess. Figure 6. Qualitative comparison of the cycle trajectory frames (> 400 frames). Under the cycle trajectory setting, VMem can realistically infer and generate unseen content with significantly more precise camera control than both inpainting-based and multi-view/videobased baseline methods. Moreover, it retains memory of all previously generated frames, ensuring consistency when revisiting locationsan ability none of the multi-view/video-based baseline methods [23, 36, 42] possess. Method LPIPS PSNR SSIM FID Rdist Tdist Look-out [23] GenWarp [27] MotionCtrl [36] ViewCrafter [42] VMem 0.809 0.507 0.589 0.401 0.371 8.41 11.13 9.07 11.82 14.82 0.069 0.134 0.096 0.217 0. 38.34 32.94 26.86 24.72 24.97 1.262 0.848 0.871 0.902 0.793 0.341 0.241 0.293 0.492 0.124 Table 2. Quantitative comparison on cycle trajectories from RealEstate10K. LPIPS (lower is better), PSNR (higher is better), Rdist (lower is better), Tdist (lower is better). Method LPIPS PSNR SSIM Rdist Tdist Look-out [23] GenWarp [27] MotionCtrl [36] ViewCrafter [42] VMem 0.792 0.521 0.692 0.494 0.472 8.52 10.27 8.21 13.62 14.11 0.008 0.129 0.082 0.129 0.121 0.727 0.785 0.842 1.643 1.204 1.499 1.982 0.129 0.492 0. Table 3. Quantitative comparison on cycle trajectories from Tanks and Temples. LPIPS (lower is better), PSNR (higher is better), SSIM (higher is better), Rdist (lower is better), Tdist (lower is better). View Retrieving Approaches LPIPS PSNR SSIM Rdist Tdist Temporal Field of View (FOV) VMem 0.194 0.424 0.371 7.52 13.11 14.82 0.018 0.192 0. 1.942 0.458 1.782 0.285 0.793 0.124 Table 4. Ablation study. LPIPS (lower is better), PSNR (higher is better), SSIM (higher is better), Rdist (lower is better), Tdist (lower is better). The setting without Surfel-Indexed Memory uses the latest views to generate the novel view. 4.5. Longand Short-Term Perpetual View Generation To further validate our approach, we evaluate VMem on the widely recognized benchmark established in [23, 41] for both short-term and long-term perpetual generation tasks. As demonstrated in Tab. 1, VMem substantially outperforms all baseline methods across all metrics, achieving superior scene extrapolation and significantly improved camera control. These quantitative improvements are further supported by qualitative comparisons in Fig. 6. Notably, VMem enables the generation of consistent, high-quality long videos of scenes across diverse scenarios beyond those in RealEstate10K, as shown in Fig. 5. We believe this represents crucial step forward in advancing the creation of immersive virtual worlds. 4.6. Ablation Study We also ablate our memory module design on the cycletrajectory setting to validate the effectiveness of the proposed components of VMem. Specifically, we compare the performance of VMem against temporal-only and fieldof-view-based (FOV) retrieval. For temporal-only, we do not use any memory mechanism and simply condition the novel view generator with the most recent views. For the FOV-based memory mechanism, we select the top-K context views that have the highest field-of-view overlap with the target view. Specifically, we project uniform grid of 3D points from the target camera frustum and compute the number of points visible from each past view based on depth consistency and camera intrinsics. The FOV overlap score is defined as the proportion of target-view frustum points that are also visible from the candidate context view. 8 We rank all past views by this score and retrieve the top-K views with the highest overlap, thus encouraging the selection of views that share the most visual coverage with the target frame. As shown in Tab. 4, Surfel-Indexed View Memory achieves the best performance. These results highlight the effectiveness of our memory design in capturing long-range dependencies and maintaining consistency across complex camera trajectories. 5. Conclusion We introduced Surfel-Indexed View Memory (VMem), novel plug-and-play memory mechanism for long-term autoregressive scene generation using surfel-indexed view memory. By anchoring past views to surfel-based representation of the scene and retrieving the most relevant ones, our approach improves long-term scene consistency while reducing computational costs. Experiments on long-term scene synthesis benchmarks demonstrate that VMem outperforms existing methods in coherence and image quality. Our work advances scalable video generation with applications in virtual reality, gaming, and other domains. Acknowledgments. The authors of this work are supported by Clarendon scholarship, ERC 101001212-UNION, and AIMS EP/S024050/1. The authors would like to thank Xingyi Yang, Zeren Jiang, Junlin Han, Zhongrui Gui for their insightful feedback."
        },
        {
            "title": "References",
            "content": "[1] Mark Boss, Zixuan Huang, Aaryaman Vasishta, and Varun Jampani. SF3D: Stable fast 3D mesh reconstruction with arXiv uv-unwrapping and illumination disentanglement. preprint arXiv:2408.00653, 2024. 2 [2] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 3 [3] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1412414133, 2021. 2 [4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-Stage Diffusion NeRF: Unified Approach to 3D Generation and Reconstruction, 2023. arXiv:2304.06714 [cs]. [5] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024. [6] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. 2024. [7] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. arXiv preprint arXiv:2302.01133, 2023. 1, 2 [8] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron, and Ben Poole. CAT3D: create anything in 3d with multi-view diffusion models. arXiv, 2405.10314, 2024. 3 [9] Ruiqi Gao*, Aleksander Holynski*, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole*. Cat3d: Create anything in 3d with multi-view diffusion models. Advances in Neural Information Processing Systems, 2024. 2 [10] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 6 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [12] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 79097920, 2023. 1, [13] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. In Proc. ICLR, 2024. 2 [14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In Proc. ICLR, 2022. 6 [15] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36 (4):113, 2017. 6 [16] Jing Yu Koh, Harsh Agrawal, Dhruv Batra, Richard Tucker, Austin Waters, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Simple and effective synthesis of indoor 3d scenes. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11691178, 2023. 1, 2 [17] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from sinIn Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 1445814467, 2021. 1, 2 [18] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. RealFusion: 360 reconstruction of any object from single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 9 [19] Norman Muller, Katja Schwarz, Barbara Rossle, Lorenzo Porzi, Samuel Rota Bul`o, Matthias Nießner, and Peter Kontschieder. Multidiff: Consistent novel view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1025810268, 2024. 1, 2 [20] Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis In Proceedings of the IEEE/CVF confrom sparse inputs. ference on computer vision and pattern recognition, pages 54805490, 2022. 2 [21] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale foundation world model, 2024. 1, 2, 3 [22] Stefan Popov, Amit Raj, Michael Krainin, Yuanzhen Li, William T. Freeman, and Michael Rubinstein. Camctrl3d: Single-image scene exploration with precise 3d camera control, 2025. 1, 2 [23] Xuanchi Ren and Xiaolong Wang. Look outside the room: Synthesizing consistent long-term 3d scene video from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3563 3573, 2022. 2, 3, 4, 6, 7, [24] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 3 [25] Chris Rockwell, David Fouhey, and Justin Johnson. Pixelsynth: Generating 3d-consistent experience from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1410414113, 2021. 1, 2 [26] Robin Rombach, Patrick Esser, and Bjorn Ommer. Geometry-free view synthesis: Transformers and no 3d priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1435614366, 2021. 2, 3, 4, 7 [27] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim, and Yuki Mitsufuji. Genwarp: Single image to novel views with semantic-preserving generative warping. arXiv preprint arXiv:2405.17251, 2024. 1, 2, 3, 7, 8 [28] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv.cs, abs/2310.15110, 2023. [29] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. arxiv, 2024. 2 [30] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter Image: Ultra-fast single-view 3D reconstruction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [31] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from In Proceedings of the IEEE/CVF sparse and noisy poses. Conference on Computer Vision and Pattern Recognition, pages 41904200, 2023. 2 [32] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, JiaBin Huang, and Johannes Kopf. Consistent view syntheIn Proceedings of sis with pose-guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1677316783, 2023. 3 [33] Qianqian Wang*, Yifei Zhang*, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 5 [34] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. [35] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [36] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. 2023. 2, 3, 4, 7, 8 [37] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. SynSin: End-to-end view synthesis from single image. In CVPR, 2020. 1, 2 [38] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: In Proceedings of 3d reconstruction with diffusion priors. the IEEE/CVF conference on computer vision and pattern recognition, pages 2155121561, 2024. 2 [39] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2 [40] Zeqi Xiao, Yushi Lan, Yifan Zhou, Wenqi Ouyang, Shuai Yang, Yanhong Zeng, and Xingang Pan. Worldmem: Longterm consistent world simulation with memory, 2025. 3 [41] Jason Yu, Fereshteh Forghani, Konstantinos Derpanis, and Marcus Brubaker. Long-term photometric consistent novel view synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 70947104, 2023. 2, 3, 4, 6, 7, [42] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 1, 2, 7, 8 10 the resulting point maps and surfel representations remain consistent and causal. We then save the optimized depth maps of the newly generated frames + 1, . . . , + for future prediction. C. Limitation and Discussion Evaluation Protocol. Since there is no established benchmark for evaluating long-term consistency in scene video generation, we adopt cyclic trajectories as proxy for assessment. However, these trajectories remain relatively simple and contain only limited occlusions, which means the full potential of VMem in handling occlusions is not fully demonstrated. Moreover, existing evaluation metrics primarily capture low-level texture similarity in hallucinated content, rather than assessing true multi-view consistencyan inherent limitation of single-view perpetual generation. As such, there is clear need for more standardized evaluation protocols, which we leave for future exploration. Limited Training Data and Computing Resources. Due to limited computational resources, VMem was fine-tuned only on the RealEstate10K dataset [47], which primarily consists of indoor scenes and limited number of outdoor real-estate scenarios. Consequently, the model may struggle to generalize well to broader contexts. Specifically, performance might degrade when dealing with natural landscapes or images containing moving objects, compared to indoor environments. However, we believe this limitation mainly stems from insufficient dataset diversity and computational constraints. Inference Speed. Due to the multi-step sampling process of diffusion models, VMem requires 4.16 seconds to generate single frame on an RTX 4090 GPU. This falls short of the real-time performance needed for applications such as virtual reality. We believe that future advancements in single-step image-set models and improvements in computational infrastructure hold promise for significantly accelerating inference speed. Future Improvement. Since our memory module relies heavily on the capabilities of the off-the-shelf image-set model and the point map predictor, the performance of VMem is expected to improve as these underlying models continue to advance. [43] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. ViewCrafter: taming video diffusion models for high-fidelity novel view synthesis. arXiv, 2409.02048, 2024. 3 [44] Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, Nan Wang, Haomin Liu, and Guofeng Zhang. Stargen: spatiotemporal autoregression framework with video diffusion model for scalable and controllable scene generation, 2025. 3 [45] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [46] Jensen (Jinghao) Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. 3, [47] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 6, 11 Stereo magnification:"
        },
        {
            "title": "Appendix",
            "content": "A. Average Pose Calculation To compute the average camera pose for rendering surfels, we average translations tT +m with simple mean, and rotations RT +m by converting them to quaternions qm, aligning signs to common hemisphere, and normalizing the mean quaternion: = (cid:80)M (cid:80)M m=1 qm m=1 qm , qm = sign(qm q1) qm. The final average pose is = (cid:21) (cid:20)R(q) 0 1 , where R(q) denotes the rotation matrix from and = 1 (cid:80)M m=1 tT +m. B. Autoregressive Point Map Prediction Since we generate point maps for each view in an autoregressive manner, it is crucial to maintain their consistency across shared coordinate space. Point-map estimators such as CUT3R include an optimization stage that jointly refines the depth, camera parameters, and point maps. To ensure fixed camera trajectory, we freeze the camera parameters, which are user-defined inputs. Additionally, at each generation step when we have frames generated so far, we freeze all previously predicted depth maps for frames 1, 2, . . . , during optimization. This ensures that"
        }
    ],
    "affiliations": [
        "University of Oxford"
    ]
}