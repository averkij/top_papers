{
    "paper_title": "Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage",
    "authors": [
        "Saehyung Lee",
        "Seunghyun Yoon",
        "Trung Bui",
        "Jing Shi",
        "Sungroh Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that our proposed evaluation method better aligns with human judgments of factuality than existing metrics and that existing approaches to improve the MLLM factuality may fall short in hyper-detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 2 ] . [ 1 4 8 4 5 1 . 2 1 4 2 : r Toward Robust Hyper-Detailed Image Captioning: Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage Saehyung Lee 1 * Seunghyun Yoon 2 Trung Bui 2 Jing Shi 2 Sungroh Yoon"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) excel at generating highly detailed captions but often produce hallucinations. Our analysis reveals that existing hallucination detection methods struggle with detailed captions. We attribute this to the increasing reliance of MLLMs on their generated text, rather than the input image, as the sequence length grows. To address this issue, we propose multiagent approach that leverages LLM-MLLM collaboration to correct given captions. Additionally, we introduce an evaluation framework and benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that our proposed evaluation method better aligns with human judgments of factuality than existing metrics and that existing approaches to improve the MLLM factuality may fall short in hyper-detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight limitation of VQA-centric benchmarking by demonstrating that an MLLMs performance on VQA benchmarks may not correlate with its ability to generate detailed image captions. 1. Introduction Numerous image captioning methods utilizing deep neural networks (DNNs) have been proposed (Vinyals et al., 2015; Xu et al., 2015). However, they are generally limited to generating short captions, which constrains their broader application in real-world scenarios. For instance, in cases such as assistance for visually impaired individuals, where it is necessary to provide highly detailed descriptions of the scene in front of the user, these methods may not be suitable. *Work done during internship at Adobe Research. 1Department of Electrical and Computer Engineering, Seoul National University 2Adobe Research 3Interdisciplinary Program in Artificial Intelligence, Seoul National University. Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>. 1 Following the recent success of large language models (LLMs) (Brown et al., 2020), there have been attempts to use text and information from other modalities as input to LLMs. Notably, many studies have explored multimodal large language models (MLLMs) that incorporate visual information (Li et al., 2023a; Dai et al., 2023; Liu et al., 2024b). These models have demonstrated significantly superior performance compared to traditional models in tasks such as visual question answering (VQA) and captioning (Liu et al., 2024a). In particular, MLLMs, leveraging the advanced language capabilities of LLMs, are able to generate much longer and more detailed captions than conventional captioning models. However, these generated captions frequently contain inaccurate information, including descriptions of objects that are not present in the input image (Leng et al., 2024). Such hallucination problems hinder the practical application of MLLMs in real-world settings. Three major approaches have been recently proposed to improve the factuality of MLLMs: (i) Decoding-based methods (Leng et al., 2024) reduce the probabilities of hallucinationrelated tokens during the models decoding process without requiring additional training; (ii) Training-based methods (Liu et al., 2023a) further train the models on curated multimodal datasets to ensure they generate only accurate responses; (iii) Corrector-based methods (Zhou et al., 2024) employ corrector model that detects and either removes or revises hallucinations present in the models responses. In this paper, we propose Caption factuality enhancing MultiAgent System (CapMAS), multiagent approach to correct hyper-detailed image captions. Unlike existing corrector-based approaches that require training corrector (Lee et al., 2024), CapMAS improves the factuality of detailed image captions by leveraging the collaboration between an LLM and MLLM, without the need for additional training. Moreover, unlike methods that target specific types of hallucinations (Li et al., 2023b; Zhou et al., 2024), our approach does not pre-define the hallucination types, allowing it to address broader range of issues. The method proceeds as follows: (i) an LLM decomposes given detailed caption into atomic propositions; (ii) an MLLM verifies the truthfulness of each atomic proposition based on the image; and (iii) the LLM revises the caption accordingly. Our design is parSubmission and Formatting Instructions for ICML 2025 ticularly motivated by the observation that, as the length of models response increases, hallucinations generated later in the sequence become more difficult for existing methods (Wang et al., 2023; Zhou et al., 2024) to detect. Evaluating the factuality of detailed captions is not straightforward. Through experiments, we demonstrate that conventional caption evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee & Lavie, 2005), and CIDEr (Vedantam et al., 2015), as well as recently proposed methods (Hessel et al., 2021; Petryk et al., 2024), fail to accurately assess the factuality of detailed captions. To address this issue, we propose GPTbased method for factuality evaluation and validate its effectiveness through experiments that include human evaluations. Even if caption contains factual information, however, it may still be considered inadequate if it does not sufficiently capture the visual information. To measure the coverage of captions, we construct detailed VQA dataset through collaboration between humans and an AI agent (Achiam et al., 2023). If caption fully encapsulates the information of given image, questions about the image should be answerable accurately using only the caption, without referencing the image itself. Our experiments surprisingly reveal that methods designed to improve the factuality of MLLMs, which have proven effective in tasks like VQA (Huang et al., 2024), may be ineffective for hyper-detailed image captioning tasks that require longer responses. In contrast, CapMAS significantly enhances the factuality of captions and can be applied in plug-and-play manner to any captioning model; notably, this improvement extends to captions generated by the state-ofthe-art closed model, GPT-4V (Achiam et al., 2023). Finally, we highlight an issue with the current VQA-centric benchmarking (Duan et al., 2024) by showing that an MLLMs performance on VQA benchmarks may not correlate with its ability to generate detailed image captions. 2. Related Work Multimodal large language models. LLMs that process inputs from multiple modalities, including text and other types of data, are referred to as multimodal LLMs (Yin et al., 2023a). Among these, LLMs that handle visual input have been the most actively researched, and the MLLMs discussed in this paper are focused on this category. Research on these models primarily explores methods for fusing the output of an independent vision encoder into the input of an LLM. The BLIP models (Li et al., 2023a; Dai et al., 2023) align the frozen vision encoder and LLM using lightweight transformer (Vaswani, 2017) called QFormer. The trainable input tokens of the Q-Former interact with the output tokens from the vision encoder through cross-attention, transforming them into input tokens for the LLM. The LLaVA models (Liu et al., 2024b;a) use simple MLP connector to align the vision encoder with the LLM. All output tokens from the vision encoder, passed through the MLP connector, are used as input to the LLM. The vision encoders parameters remain fixed during the training of the MLP connector and the LLM. Unlike existing MLLMs, the InternVL models (Chen et al., 2024c;b) have demonstrated the effectiveness of increasing the size of both the vision encoder and the vision-language connector. They utilize 6-billion parameter vision encoder and an 8-billion parameter vision-language connector. The connector is obtained by fine-tuning the pre-trained multilingual LLaMA (Cui et al., 2023). Despite the many advancements in open-source MLLMs, closed-source MLLMs such as GPT-4V or GPT-4o1 still outperform them significantly. As result, these GPT models represent the upper bound performance in benchmarks and are commonly used to evaluate MLLMs (Petryk et al., 2024). In our work, we demonstrate that captions generated by GPT-4V can be improved using our method, and we use GPT-4o to evaluate captions. MLLM hallucinations and mitigation strategies. MLLMs sometimes generate inaccurate responses. For example, they may incorrectly describe the characteristics of objects in an input image, misrepresent relationships between objects, or even describe objects that do not exist. To mitigate these hallucination problems, decoding-based methods apply penalties to the probabilities of tokens that are likely to be hallucinations during the decoding process. For instance, VCD (Leng et al., 2024) induces hallucinations using corrupted images, while OPERA (Huang et al., 2024) leverages the correlation between high attention weights assigned to few summary tokens and hallucinations. Training-based methods focus on exploring training data that can suppress the generation of hallucinations. Liu et al. (2023a) demonstrated that hallucinations can be alleviated by incorporating negative samplesdescriptions that explicitly state the absence of certain objects in given imageinto visual instruction tuning datasets. Corrector-based methods (Zhou et al., 2024; Lee et al., 2024) detect, remove, and revise hallucinations present in MLLM responses by using corrector model. is obtained by supervised fine-tuning This model pre-trained MLLM. The corrector model then revises the initial response based on the given image. Caption evaluation methods. Since short image captions are relatively easy to obtain reference captions for, we can use matching-based caption evaluation methods (Hossain et al., 2019) to assess them. However, for long and detailed captions generated by MLLMs, the number of reference captions required for such evaluations becomes exceedingly 1https://openai.com/index/hello-gpt-4o/ 2 Submission and Formatting Instructions for ICML 2025 Figure 1: The process of generating data sample for evaluating hallucination detection methods in detailed image captioning tasks. Human annotators identify and label object hallucinations within the caption generated by LLaVA-NeXT (Liu et al., 2024a) for an image. large. Thus, it becomes impractical to evaluate detailed captioning using traditional approaches. Hessel et al. (2021) proposed CLIPScore, reference-free evaluation method. CLIPScore measures the distance between an image and its caption within the joint representation space of CLIP (Radford et al., 2021). Additionally, the authors introduced RefCLIPScore, which uses both the image and reference captions within that same representation space. Chan et al. (2023) addressed the limitations of matching-based methods by utilizing an LLM. The LLM-based metric they proposed, CLAIR, assigns scores to captions based on reference captions using an LLM. Similarly, ALOHa (Petryk et al., 2024) detects object hallucinations by comparing generated captions to reference captions using an LLM. 3. Method In this paper, we propose multiagent-based caption correction method. Corrector-based methods typically detect and remove hallucinations within model responses. Unlike existing approaches, which require the corrector model training, our method employs collaboration between an MLLM and LLM. Moreover, in contrast to previous methods that are limited to correcting specific types of hallucinations (Zhou et al., 2024), our approach is free from such constraints. We also propose framework for evaluating the detailed image captioning capabilities of an MLLM. Unlike existing methods, our proposed evaluation approach allows for assessing image captioning models in terms of both factuality and coverage, evaluating each of these aspects separately. 3.1. Motivating Observations Here, we examine the performance of existing hallucination detection methods on tasks that require generating long responses. To facilitate these analyses, we construct dataset as follows: (i) We prompt an MLLM with Describe the given image in very detailed manner. and collect the models responses for specified image set; (ii) For the convenience of our analysis, we use an LLM to identify objects that may be hallucinations; (iii) Human annotators then label each parsed object as either hallucination or not, (a) Confidence-Token Index (b) Consistency-Token Index Figure 2: The hallucination scores of the Confidence and Consistency methods based on object positions within detailed captions. Object hallucinations near the end of the captions (192+) are undetectable by both methods. based on the corresponding image. We use LLaVA-NeXT (Liu et al., 2024a) and GPT-4 as the MLLM and LLM, respectively. Figure 1 illustrates the process of constructing the dataset. To build the dataset, we use subset of IIW-400 (Garg et al., 2024). We detect object hallucinations using two of the most widely adopted hallucination detection methods: 1. Confidence (Zhang et al., 2023; Zhou et al., 2024): This method detects hallucinations using the predicted probability pobj for the object token during LLaVA-NeXT generation. For multi-token objects, the token probabilities are multiplied. The hallucination score Hobj = log pobj, increases with the likelihood of hallucination. 2. Consistency (Wang et al., 2023; Zhao et al., 2024): This method assumes that hallucinations are sensitive to decoding randomness. Using stochastic decoding, we have LLaVA-NeXT generate 40 captions per image and count the occurrence tobj of each object in the dataset of Figure 1. The hallucination score is Hobj = log tobj 40 . Figure 2 presents the hallucination scores of each method by the position of objects appearing within detailed captions. The horizontal axis of the graphs represents bins of object token indices, with larger token indices indicating positions closer to the end of the caption. The vertical axis represents the mean and standard deviation of the hallucination scores within each bin. Note that Figure 2a reflects the positions and hallucination scores during greedy decoding, 3 Submission and Formatting Instructions for ICML 2025 Table 1: Performance comparison of hallucination detection methods for the dataset of Figure 1. Method AUROC FPR95 Confidence Consistency Object Detector Isolation 57.5 73.5 61.5 81.4 95.1 75.6 95.7 71. while Figure 2b is derived from the average positions and hallucination scores across 40 stochastic decoding iterations. Figure 2 demonstrates that hallucinations generated after the 192nd token are undetectable by the Confidence and Consistency methods. Based on these results, we can infer that existing hallucination detection methods may be ineffective in detecting hallucinations in long detailed captions. Our hypothesis regarding these results is that as MLLM outputs become longer, they become more strongly grounded in the text they generate rather than the given image. In fact, our hypothesis is supported by several recent studies. For example, Liu et al. (2024c) demonstrated that as MLLM responses lengthen, the attention weights assigned to image tokens decrease, and Zhong et al. (2024) showed that MLLM responses are significantly influenced by prior dialogue. Based on this hypothesis, we test method for determining whether each object is hallucination by disconnecting it from its context (Isolation). The Isolation method involves querying the LLaVA-NeXT model with parsed objects using the prompt template, Is there {} in the photo? along with the image. When the probability of the Yes token for the object query is pYesobj, the hallucination score is defined as Hobj = log pYesobj. We compare the object hallucination detection performance of the Isolation method with that of the Confidence method, the Consistency method, and method based on an object detector (Object Detector) introduced in recent studies (Yin et al., 2023c; Ge et al., 2024). We measure their detection performance on the dataset of Figure 1 using Area Under the Receiver Operating Characteristic (AUROC) and False Positive Rate at 95% true positive rate (FPR95). Table 1 demonstrates that the Isolation method outperforms the others. This suggests that breaking long caption into smaller units and examining each individually can help detect hallucinations in detailed captions. Comparison with existing studies. Actually, the concept of decomposing text into smaller units and assessing the factuality of each has been introduced in prior studies (Min et al., 2023; Jing et al., 2024). Here, we summarize the key differences between our study and previous research: 1. Unlike existing studies, which do not rigorously justify the need for decomposition, we empirically demonstrate the motivation behind our approach (Section 3.1). 2. While previous studies focused solely on proposing evaluation metrics, our research advances further by introducing system that leverages the decomposition process to generate improved image captions (Section 3.2). 3. Existing evaluation metrics assess factuality using unimodal data (either text or image). In contrast, our proposed evaluation metric utilizes multimodal data (both text and image) for factuality assessment (Section 3.3). 4. Previous studies focus solely on measuring the factuality of generated text. In contrast, our study proposes method that assesses both the factuality and coverage of any given image caption (Section 3.3). 5. We demonstrate that our metric correlates more strongly with human evaluations than existing metrics and is robust against their critical limitations (Section 4.2). 3.2. Caption Factuality Enhancing MultiAgent System To address various types of hallucinations comprehensively, we first decompose each detailed caption into atomic propositions using an LLM. An atomic proposition is claim or statement that must either be true or false. For example, the caption house has red roof and chimney is broken down into house has red roof and house has chimney. We use an LLM to perform this process, but we allow flexibility in cases where the results do not strictly conform to the definition of an atomic proposition. We then investigate the truth of each decomposed unit using an MLLM. Each unit is converted into True/False question and independently fed to the MLLM. The hallucination score H(u) for the unit is defined as follows: log (min (p (Tx, Q(u)) (Fx, Q(u)) , ϵ)) (1) (T) and (F) represent the MLLMs token probabilities for the True and False tokens, respectively. and ϵ denote the input image and very small constant near zero. Q() is function that converts the input text into True/False question, which we implement by prepending True or False? to the input. Each unit is included in either the True set or the False set F, based on its hallucination score. To achieve this, we introduce hyperparameter π, such that = {uH(u) π} and = {uH(u) > π}. Finally, the initial caption, along with the corresponding sets and F, is provided to an LLM, which corrects the initial caption to ensure it contains only factual information. We name this pipeline, which improves the factuality of detailed image captions through the collaboration of pretrained LLM and MLLM, Caption factuality enhancing MultiAgent System (CapMAS). CapMAS is training-free and can be applied in plug-and-play manner to any captioning model. Unlike existing methods that can only address predefined types of hallucinations, CapMAS can detect and correct all hallucinations at the atomic unit level. 4 Submission and Formatting Instructions for ICML 2025 Figure 3: Overview of CapMAS. The decomposer LLM breaks an initial caption into atomic units. These units are converted into True/False questions and fed into the MLLM along with the image, where each unit is assigned hallucination score according to Equation (1). Units are classified as True or False based on the threshold π, and the corrector LLM then revises the initial caption accordingly. The pipeline of CapMAS is illustrated in Figure 3. 3.3. Evaluation Methods Traditional caption evaluation methods rely on word matching with reference captions, suitable for short captions generated by conventional models. However, MLLMs produce longer and more detailed captions, making it impractical to obtain sufficient reference captions for accurate evaluation. Given the enriched content of these image captions, rather than simply evaluating them as good or bad, we aim to assess them systematically by considering two key perspectives: Factuality: The degree to which the content of the caption is factual and free from hallucinations. Coverage: The extent to which the caption captures the information contained in the image. We propose evaluation methods for detailed image captions from these two perspectives. Factuality. If human were to measure the factuality of text, it would be natural to decompose the text into units that can be classified as true or false, and then calculate the proportion of true units (Maynez et al., 2020). We adopt this approach to measure the factuality of captions, utilizing the state-of-the-art model GPT-4o. In our framework, GPT4o decomposes each caption into atomic propositions and determines their truthfulness based on the corresponding Table 2: Meta-evaluation results across various caption evaluation methods. DOCCI and its synthetic hallucinatory captions are used for the meta-evaluation. The highest-rated caption for each method is highlighted in bold. The full table is in Appendix D. Caption Evaluation Metric CIDEr CLIP-S RefCLIP-S CLAIR ALOHa Ours Clean Object Attribution Relation 6.4 4.8 6.2 6.7 81.3 81.0 80.9 81.4 75.5 75.3 75.2 75.6 86.9 85.2 80.0 83.5 36.2 31.5 34.3 36.9 62.8 52.3 60.9 51. image and reference caption. If the number of atomic propositions judged as true and false are and , respectively, the factuality of the caption is defined as +F . To validate this evaluation method, we use the DOCCI dataset (Onoe et al., 2024), which contains human-annotated detailed image captions. Specifically, for each image in subset of the dataset, we prepare the following four types of captions (details provided in Appendix F): 1. Clean: The original caption (e.g., An indoor view captures cat on wooden floor, attempting to catch large pale peacock feather flying above it). 2. Object: description of an object likely present but not in the image is added to the Clean caption (e.g., An indoor view captures cat on wooden floor, attempting Submission and Formatting Instructions for ICML 2025 Figure 4: An example of our coverage evaluation data sample. The dataset consists of multiple-choice questions with four or fewer options. As demonstrated, the dataset includes questions with varying levels of granularity, ranging from broad to highly detailed. We have an LLM solve these problems using only the provided captions. to catch large pale peacock feather flying above it. small red ball is rolling near the cat). 3. Attribution: Some object attributions in the Clean caption are modified to be inconsistent with the image (e.g., An indoor view captures cat on metal floor, attempting to catch small dark peacock feather flying above it). 4. Relation: Some object relationships in the Clean caption are altered to conflict with the image (e.g., An indoor view captures cat on wooden floor, attempting to catch large pale peacock feather flying below it). We evaluate the four types of captions using various metrics (BLEU, ROUGE, METEOR, CIDEr, CLIP-S, RefCLIP-S, CLAIR, and ALOHa), including our own, to determine whether the hallucinations in the three modified types are reflected in the scores. For fair comparison, all methods requiring GPT (CLAIR, ALOHa, and ours) use GPT-4o, and all methods requiring reference captions (BLEU, ROUGE, METEOR, CIDEr, CLAIR, ALOHa, and ours) use separate set (Garg et al., 2024) of human-annotated captions. Table 2 shows that existing metrics are unreliable for evaluating the factuality of detailed image captions. Specifically, CLIP can only process up to 77 tokens and operates like bag-of-words model (Yuksekgonul et al., 2023). This prevents CLIP-based metrics from capturing the full content of detailed image captions, particularly missing Relation hallucinations. ALOHa effectively addresses Object and Attribution hallucinations but fails to capture Relation hallucinations due to its algorithmic limitations. CLAIR detects and reflects all three types of hallucinations in the scores. However, CLAIR does not focus solely on factuality; instead, it allows the GPT model to directly score each caption, applying the evaluation criteria implicitly defined by the GPT model. In contrast, our metric exclusively considers the factuality of the caption. While it does not assign perfect score to the Clean captions due to GPT-4os limitations in image understanding, it successfully assigns the highest score to Clean among the four caption sets. Coverage. An image caption with only factual information is not high-quality if it focuses solely on trivial aspects of the image. To assess the coverage of captioning models, we propose QA-based metric and benchmark dataset. Our coverage evaluation method is based on the assumption that if an image caption fully captures the information in the image, visual questions about that image should be answerable by referencing the caption alone. Our goal is to evaluate hyper-detailed image captions. Therefore, the visual questions for evaluation must include variety of detailed and nuanced questions about the images. Given the limitations of existing VQA datasets in this regard (Yin et al., 2023b; Li et al., 2023b; Yue et al., 2024), we construct new VQA dataset. However, creating new VQA dataset that includes variety of detailed questions requires substantial labor. To reduce the associated costs, we follow the process outlined below to construct our dataset: 1. Generating more than 50 questions per image in the IIW400 dataset using GPT-4o. 2. Deduplicating the questions for each image using Sentence-BERT (Reimers & Gurevych, 2019). 3. Human labelers refine or remove ambiguous, flawed, or common-knowledge questions. 4. Human labelers annotate the correct answers to the remaining and revised questions. Our coverage evaluation dataset contains total of 19,899 multiple-choice questions, with each image averaging 49.8 questions. We present an example of our dataset in Figure 6 Submission and Formatting Instructions for ICML 2025 4. While our benchmark dataset can also be used to assess the visual understanding capabilities of MLLMs, we use it to evaluate the coverage of captioning models by having an LLM answer the questions based on the generated captions. Table 3: Comparison of correlations between human preferences and automated metrics in terms of factuality. FAITHSCORE FACTSCORE Ours Spearmans ρ 62.5 67.9 70.2 4. Experimental Results and Discussion 4.1. Experimental Setup We adopt LLaVA-v1.5-7B, LLaVA-NeXT-7B, LLaVANeXT-13B, InternVL-Chat-V1.5, and GPT-4V as the models for both captioning and CapMASs fact-checking. We use LLaMA-3-8B (AI@Meta, 2024) or GPT-4 as the decomposer and corrector LLMs in CapMAS. Our experiments utilize the IIW-400 and DOCCI datasets, which contain images paired with highly detailed, hallucination-free captions. These high-quality reference captions enable precise evaluation of the captioning models. We employ our proposed factuality and coverage metrics, along with CLAIR, all based on GPT-4o, to evaluate the generated captions. To ensure robust evaluation, we summarize the captions (Ge et al., 2024) generated from five different prompts using LLaMA-3-8B. The only CapMAS hyperparameter, π, is tuned on validation set of five examples sampled from the DCI dataset (Urbanek et al., 2024). The prompt templates are provided in Appendix F. 4.2. Our Metrics Correlation with Human Evaluation We obtain human evaluation data to validate the reliability of our factuality metric and compare it with existing ones. Human labelers assess which captionLLaVA-v1.5-7Bs or InstructBLIPsis more factual for each image in subset of the DOCCI test set (refer to Appendix for further details). Using this dataset, we compare our metric with FAITHSCORE (Jing et al., 2024) and FACTSCORE (Min et al., 2023), both of which evaluate the factuality of decomposed units: FactScore uses only the reference caption, FaithScore only the image, and our metric combines both. Table 3 shows that our metric exhibits stronger correlation with human evaluation than existing metrics. However, this is not the sole reason to adopt our proposed metric for evaluating the factuality of detailed image captions. Metrics that rely solely on unimodal information are inherently susceptible to undesirable biases. For instance, metrics like FACTSCORE, which depend exclusively on reference captions, introduce stylistic biases tied to the specific style, tone, or phrasing of the references, unfairly favoring or penalizing captions based on these factors. In contrast, as demonstrated in Appendix B, our metric is free from such biases. 4.3. Improving Captioning Model Factuality Our proposed CapMAS exhibits loose factuality-coverage trade-off depending on the hyperparameter π. Specifically, as π decreases, the threshold for determining factual propositions becomes stricter, leading to more propositions being identified for correction. Consequently, factuality increases while coverage decreases (an ablation study on π is provided in Appendix C). We first investigate whether CapMAS can enhance the factuality of various MLLMs while minimizing the reduction in coverage. Table 4 demonstrates that CapMAS can significantly enhance the factuality of all tested MLLMs while minimizing coverage loss. The substantial improvement in factuality, compared to the relatively minor coverage loss in the captioning models, is also reflected in the increased CLAIR scores. Using more advanced LLM in CapMAS does not necessarily result in greater performance gains. When applying CapMAS to the LLaVA and InternVL models, there is minimal difference between the results obtained with LLaMA-3-8B and those with GPT-4. This suggests that the LLMs role in CapMAS is relatively straightforward. CapMAS can improve detailed image captioning even for the state-of-the-art MLLM, GPT-4V. It can significantly enhance factuality even when used with MLLMs far less capable than GPT-4V. However, in such cases, there is considerable loss in coverage, as many visual elements recognized by GPT-4V are identified as hallucinations by CapMAS. With InternVL-Chat-V1.5, CapMAS maintains GPT-4Vs coverage while improving factuality. We additionally provide qualitative comparison in Figure 5 between LLaVA-NeXT-7B with and without the application of CapMAS (referencing the first two rows of Table 4). 4.4. Comparison with Other Methods Various methods have been proposed to mitigate hallucinations in MLLMs, and they have primarily been validated on VQA and simple captioning benchmarks. We compare CapMAS with two recent decoding-based methods (VCD and OPERA), two corrector-based methods (LURE and Volcano), and one training-based method (LRV) from the perspective of detailed image captioning. All methods, except for LRV and LURE, use LLaVA-v1.5-7B, while the LRV and LURE methods employ the MiniGPT-4 model (Zhu et al., 2023) as provided by their respective authors. For reference, VisualFactChecker (VFC) (Ge et al., 2024) 7 Submission and Formatting Instructions for ICML 2025 Table 4: Effectiveness of our proposed method across various captioning models. In the CapMAS column, the LLM represents the decomposer and corrector, while the MLLM represents the fact-checker. Avg. denotes the average of CLAIR, Factuality, and Coverage. Captioner CapMAS Metric LLM MLLM CLAIR Factuality Coverage Avg. LLaVA-NeXT-7B LLaVA-NeXT-13B InternVL-Chat-V1. GPT-4V - LLaMA-3-8B GPT-4 - LLaMA-3-8B GPT-4 - LLaMA-3-8B GPT-4 - LLaMA-3-8B LLaMA-3-8B LLaMA-3-8B - LLaVA-NeXT-7B LLaVA-NeXT-7B - LLaVA-NeXT-13B LLaVA-NeXT-13B - InternVL-Chat-V1.5 InternVL-Chat-V1.5 - LLaVA-NeXT-7B LLaVA-NeXT-13B InternVL-Chat-V1.5 68.8 74.1 74.6 70.2 75.5 73.4 74.9 78.2 77. 82.4 83.3 81.9 84.6 59.9 72.2 73.4 62.1 77.9 79.3 65.5 75.9 75.7 77.1 83.3 85.3 82.1 47.9 46.9 46. 48.5 45.8 45.1 48.2 47.3 47.3 53.5 50.8 48.4 53.5 58.9 64.4 64.7 60.3 66.4 65.9 62.9 67.1 66. 71.0 72.4 71.9 73.4 Table 5: Performance comparison between our proposed method and other methods regarding detailed image captioning. Base refers to the default image captioning of LLaVA-v1.5-7B. Method CLAIR Factuality Coverage Avg. Base VCD (Leng et al., 2024) OPERA (Huang et al., 2024) LURE (Zhou et al., 2024) Volcano (Lee et al., 2024) LRV (Liu et al., 2023a) CapMAS (ours) 62.1 59.7 59.1 57.2 63.9 39.7 66. 52.8 44.6 53.0 51.9 53.7 29.1 63.4 34.3 39.3 34.1 27.6 37.7 37.8 33.1 49.7 47.9 48.7 45.6 51.7 35.5 54.3 is also pipeline composed of pre-trained models that revise initial captions, similar to our approach. However, the inability to reproduce VFC, as its authors have not provided the necessary resources for reproduction, prevents direct comparison with our method. Nonetheless, we can infer that our method outperforms VFC in terms of both applicability and performance because 1) VFC specifically targets object hallucinations, and 2) it employs an object detector (Liu et al., 2023b) for hallucination detection (see Table 1). Table 5 shows that the decoding-based methods are ineffective for detailed image captioning. Ironically, applying VCD significantly reduces the factuality of the LLaVA model while increasing coverage. Volcano yields only slight improvements in LLaVAs captions. However, CapMAS substantially enhances the factuality of the captioning model compared to the other methods. These results suggest that methods proposed to enhance MLLM factuality should be evaluated not only on tasks requiring short responses, such as VQA, but also on detailed image captioning tasks. 4.5. Consistency Between MLLM Captioning and VQA Evaluation results Currently, MLLM evaluations are conducted on tasks that require only short responses, such as VQA tasks (Duan et al., 2024). However, to assess the potential of MLLMs in realworld applications, such as visual assistants, it is essential to evaluate their detailed image captioning abilities. The ranking of models used in our experiments, including LLaVAv1.5-7B, LLaVA-NeXT-7B, LLaVA-NeXT-13B, InternVLChat-V1.5, and GPT-4V, is consistent across both our captioning evaluation results and widely used benchmarks like MMMU (Yue et al., 2024). However, for instance, some MLLMs may be optimized for VQA tasks that require only short responses, allowing them to rank highly on common VQA benchmarks, yet their limited image captioning abilities could restrict their practical use. To investigate this, we evaluate the detailed image captioning capabilities of various MLLMs and examine whether their rankings are consistent with their rankings on widely used VQA benchmarks. We adopt InstructBLIP-7B (Dai et al., 2023), Idefics2-8B (Laurencon et al., 2024), and MiniCPM-V-2.6 (Yao et al., 2024) as additional MLLMs for the experiment. Table 6 presents the evaluation results of MLLMs responses to the prompt Describe the given image in very detailed manner as well as the performance of these models on various VQA tasks. From these results, we observe that the performance of an MLLM on widely used benchmarks does not necessarily reflect its capabilities in detailed image captioning. Specifically, Idefics2-8B ranks mid-tier among the tested models in VQA tasks but falls into the lowestperforming group in terms of detailed image captioning. Its Submission and Formatting Instructions for ICML 2025 Table 6: Detailed image captioning and VQA performance of various MLLMs. OpenCompass (Duan et al., 2024) includes MMBench v1.1 (Liu et al., 2023c), MMStar (Chen et al., 2024a), MMMU val (Yue et al., 2024), MathVista (Lu et al., 2024), OCRBench (Liu et al., 2024d), AI2D (Kembhavi et al., 2016), HallusionBench (Guan et al., 2024), and MMVet (Yu et al., 2023). For POPE (Li et al., 2023b), we report the average F1 score across the three categories: adversarial, popular, and random. We report the sum of the perception and cognition scores for MME (Yin et al., 2023b). The best results for each metric are shown in bold. Model InstructBLIP-7B LLaVA-v1.5-7B LLaVA-NeXT-7B LLaVA-NeXT-13B Idefics2-8B InternVL-Chat-V1.5 MiniCPM-V-2.6 GPT-4V Detailed Image Captioning Visual Question Answering CLAIR Factuality Coverage Avg. OpenCompass MME POPE Avg. 57.2 61.1 63.8 64.5 58.1 72.4 73.1 82.4 44.4 56.3 58.5 62.8 85.2 67.6 68.9 78.6 30.3 30.5 42.2 43.0 13.4 46.0 43.6 52.6 43.9 49.3 54.8 56.8 52.2 62.0 61.9 71.2 31.1 36.9 44.7 47.6 53.0 61.7 65.2 63.5 1391.4 1808.4 1769.1 1745.6 1847.6 2189.6 2268.7 2070. 86.1 86.1 87.5 87.8 86.2 87.5 83.2 81.8 38.4 44.6 50.8 53.1 57.6 65.9 68.6 66.4 high factuality but low coverage indicates that Idefics2-8B has been trained to provide short and concise answers; this conclusion remains unchanged even when using Idefics28B-Chatty (Laurencon et al., 2024). Despite being relatively small model, MiniCPM-V-2.6 attracted attention by outperforming GPT-4V on benchmarks. However, our results show that the model significantly underperforms GPT4V in detailed image captioning. Additionally, we find that the factuality of the captions cannot be reliably predicted from the accuracy of MLLMs on POPE (Li et al., 2023b), which was proposed to evaluate object hallucinations. Based on these experimental results, we raise concerns about the current MLLM evaluations that are centered around VQA tasks. We encourage the community to also evaluate MLLMs from the perspective of detailed image captioning in order to showcase their full potential. 5. Conclusion Detailed image captioning tasks are closely linked to critical applications, such as visual assistance for the impaired. Our research aims to assess and enhance the potential of MLLMs in these real-world contexts. We propose CapMAS, method that improves detailed image captions through the collaboration of pre-trained MLLM and LLM. In addition, we introduce framework and benchmark dataset for evaluating the factuality and coverage of captioning models. Our experiments validate the proposed evaluation framework and demonstrate that CapMAS significantly improves the factuality of captioning models. We additionally present the following two key observations: Methods designed to improve MLLM factuality, which have been validated primarily on VQA or short captioning tasks, may be ineffective for detailed image captioning and can even reduce the factuality of the backbone model. High performance on commonly used VQA-centric benchmarks does not necessarily indicate that the model will excel in hyper-detailed image captioning. These observations raise concerns about the current VQAcentric trend in MLLM evaluation. We encourage the community to evaluate MLLMs and related algorithms not only on VQA tasks but also on detailed image captioning tasks to gain more comprehensive understanding of their potential."
        },
        {
            "title": "Impact Statement",
            "content": "This research contributes to developing more accurate and reliable image captioning systems, which are crucial for accessibility technologies. The proposed multiagent approach mitigates the risks of misinformation and hallucinations in AI-generated content, enhancing the safety and trustworthiness of AI systems. However, as image captioning models become more detailed, ethical concerns, particularly privacyrelated, may emerge. Future research should address these challenges to ensure the responsible deployment of this technology."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. Banerjee, S. and Lavie, A. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on in9 Submission and Formatting Instructions for ICML 2025 trinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Chan, D., Petryk, S., Gonzalez, J., Darrell, T., and Canny, J. Clair: Evaluating image captions with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 13638 13646, 2023. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024a. Chen, Z., Wang, W., Tian, H., Ye, S., Gao, Z., Cui, E., Tong, W., Hu, K., Luo, J., Ma, Z., et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024b. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024c. Cui, Y., Yang, Z., and Yao, X. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177, 2023. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., InstructBLIP: Towards Li, B., Fung, P., and Hoi, S. general-purpose vision-language models with instrucIn Thirty-seventh Conference on Neural tion tuning. Information Processing Systems, 2023. URL https: //openreview.net/forum?id=vvoWPYqZJA. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., Lin, D., and Chen, K. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. URL https://arxiv.org/abs/2407.11691. Garg, R., Burns, A., Ayan, B. K., Bitton, Y., Montgomery, C., Onoe, Y., Bunner, A., Krishna, R., Baldridge, J., and Soricut, R. Imageinwords: Unlocking hyper-detailed image descriptions. arXiv preprint arXiv:2405.02793, 2024. Ge, Y., Zeng, X., Huffman, J. S., Lin, T.-Y., Liu, M.- Y., and Cui, Y. Visual fact checker: Enabling highfidelity detailed caption generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1403314042, 2024. Guan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu, X., Wang, X., Chen, L., Huang, F., Yacoob, Y., et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14375 14385, 2024. Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., and Choi, Y. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Hossain, M. Z., Sohel, F., Shiratuddin, M. F., and Laga, H. comprehensive survey of deep learning for image captioning. ACM Computing Surveys (CsUR), 51(6):1 36, 2019. Huang, Q., Dong, X., Zhang, P., Wang, B., He, C., Wang, J., Lin, D., Zhang, W., and Yu, N. Opera: Alleviating hallucination in multi-modal large language models via overtrust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1341813427, 2024. Jing, L., Li, R., Chen, Y., and Du, X. FaithScore: Fine-grained evaluations of hallucinations in In Al-Onaizan, large vision-language models. (eds.), FindY., Bansal, M., and Chen, Y.-N. ings of the Association for Computational Linguistics: EMNLP 2024, pp. 50425063, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp. 290. URL https://aclanthology.org/2024. findings-emnlp.290. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Laurencon, H., Tronchon, L., Cord, M., and Sanh, V. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. Lee, S., Park, S., Jo, Y., and Seo, M. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 391404, 2024. Leng, S., Zhang, H., Chen, G., Li, X., Lu, S., Miao, C., and Bing, L. Mitigating object hallucinations in large visionlanguage models through visual contrastive decoding. In Submission and Formatting Instructions for ICML 2025 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1387213882, 2024. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in large visionarXiv preprint arXiv:2305.10355, language models. 2023b. Lin, C.-Y. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 74 81, 2004. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023a. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024a. https://llava-vl.github.io/blog/ and Lee, Y. ing, ocr, and world knowledge, URL 2024-01-30-llava-next/. Llava-next: J. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023b. Liu, S., Zheng, K., and Chen, W. Paying more attention to image: training-free method for alleviating hallucination in lvlms. arXiv preprint arXiv:2407.21771, 2024c. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023c. Liu, Y., Li, Z., Huang, M., Yang, B., Yu, W., Li, C., Yin, X., lin Liu, C., Jin, L., and Bai, X. On the hidden mystery of ocr in large multimodal models, 2024d. URL https: //arxiv.org/abs/2305.07895. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 11 Maynez, J., Narayan, S., Bohnet, B., and McDonald, R. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 19061919, 2020. Min, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.-t., Koh, P., Iyyer, M., Zettlemoyer, L., and Hajishirzi, H. FActScore: Fine-grained atomic evaluation of factual In Bouamor, precision in long form text generation. H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1207612100, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.741. URL https:// aclanthology.org/2023.emnlp-main.741. Onoe, Y., Rane, S., Berger, Z., Bitton, Y., Cho, J., Garg, R., Ku, A., Parekh, Z., Pont-Tuset, J., Tanzer, G., et al. Docci: Descriptions of connected and contrasting images. arXiv preprint arXiv:2404.19753, 2024. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Petryk, S., Chan, D., Kachinthaya, A., Zou, H., Canny, J., Gonzalez, J., and Darrell, T. Aloha: new measure for hallucination in captioning models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pp. 342357, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Reimers, N. and Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/ abs/1908.10084. Urbanek, J., Bordes, F., Astolfi, P., Williamson, M., Sharma, V., and Romero-Soriano, A. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26700 26709, 2024. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Submission and Formatting Instructions for ICML 2025 Zhang, T., Qiu, L., Guo, Q., Deng, C., Zhang, Y., Zhang, Z., Zhou, C., Wang, X., and Fu, L. Enhancing uncertaintybased hallucination detection with stronger focus. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 915932, 2023. Zhao, Y., Yan, L., Sun, W., Xing, G., Meng, C., Wang, S., Cheng, Z., Ren, Z., and Yin, D. Knowing what llms do not know: simple yet effective self-detection method. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 70447056, 2024. Zhong, W., Feng, X., Zhao, L., Li, Q., Huang, L., Gu, Y., Ma, W., Xu, Y., and Qin, B. Investigating and mitigating the multimodal hallucination snowballing in large visionlanguage models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1199112011, 2024. Zhou, Y., Cui, C., Yoon, J., Zhang, L., Deng, Z., Finn, C., Bansal, M., and Yao, H. Analyzing and mitigating object hallucination in large vision-language models. In The Twelfth International Conference on Learning Representations, 2024. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Vedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show and tell: neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 31563164, 2015. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., and Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 20482057. PMLR, 2015. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023a. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023b. Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y., Li, K., Sun, X., and Chen, E. Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint arXiv:2310.16045, 2023c. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Yuksekgonul, M., Bianchi, F., Kalluri, P., Jurafsky, D., and Zou, J. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, 2023. 12 Submission and Formatting Instructions for ICML 2025 A. Human Evaluation Dataset Construction The results in Table 3 are obtained through the following process: 1. Captions are generated for 100 DOCCI test images using LLaVA-v1.5-7B and InstructBLIP. 2. Human labelers evaluate the captions from LLaVA-v1.5-7B and InstructBLIP for each image in terms of factuality. 3. Caption pairs with similar factuality quality are excluded. 4. For the remaining pairs, the correlation between human decisions and those made by each automated metric is measured. B. Undesirable Bias in FACTSCORE Metrics that rely solely on unimodal information are inherently susceptible to undesirable biases. For instance, metrics like FACTSCORE, which depend exclusively on reference captions, introduce stylistic bias tied to the specific style, tone, or phrasing of the references, unfairly favoring or penalizing captions based on these factors. To demonstrate this, we compare FACTSCORE with our factuality metric using human-labeled captions that are hallucination-free but stylistically different from DOCCI captions (HUMAN) (Garg et al., 2024). Table 7 shows that, due to its stylistic bias, FACTSCORE assigns lower scores to these human-labeled captions, even though they are clearly superior to LLaVA-v1.5-7B and InstructBLIP captions in terms of factuality. In contrast, our factuality metric remains robust against such bias. Table 7: Comparison of correlations between human preferences and automated metrics in terms of factuality. Task Spearmans ρ FACTSCORE LLaVA-v1.5-7B vs. InstructBLIP HUMAN vs. LLaVA-v1.5-7B vs. InstructBLIP 67.9 18.3 Ours 70.2 61.4 C. Ablation Study Table 8: Effectiveness of our proposed method across various captioning models as function of π. In the CapMAS column, the LLM represents the decomposer and corrector, while the MLLM represents the fact-checker. Captioner LLaVA-NeXT-7B LLaVA-NeXT-13B InternVL-Chat-V1.5 LLM - LLaMA-3-8B LLaMA-3-8B LLaMA-3-8B - LLaMA-3-8B LLaMA-3-8B LLaMA-3-8B - LLaMA-3-8B LLaMA-3-8B LLaMA-3-8B CapMAS MLLM - LLaVA-NeXT-7B LLaVA-NeXT-7B LLaVA-NeXT-7B - LLaVA-NeXT-13B LLaVA-NeXT-13B LLaVA-NeXT-13B - InternVL-Chat-V1.5 InternVL-Chat-V1.5 InternVL-Chat-V1.5 π - 1.0 0.5 0.3 - 1.0 0.5 0.3 - 1.0 0.5 0. Metric CLAIR Factuality Coverage 68.8 74.1 73.6 72.2 70.2 75.5 74.8 72. 74.9 78.2 79.0 77.7 59.9 72.2 76.9 76.8 62.1 77.9 79.9 80.5 65.5 75.9 78.8 81.7 47.9 46.9 43.7 40.0 48.5 45.8 42.1 39. 48.2 47.3 46.0 42.5 Our proposed method features single hyperparameter, π, which serves as the threshold for classifying atomic propositions as hallucinations or non-hallucinations. Table 8 presents the effects of CapMAS across various models as function of π. The results reveal loose trade-off between factuality and coverage depending on π. Specifically, in all tested settings, as π increases, factuality tends to decrease while coverage increases. 13 D. The complete version of Table 2 Submission and Formatting Instructions for ICML 2025 Table 9: Meta-evaluation results across various caption evaluation methods. DOCCI and its synthetic hallucinatory captions are used for the meta-evaluation. The highest-rated caption for each method is highlighted in bold. BLEU ROUGE METEOR Evaluation Metric CLIP-S CIDEr RefCLIP-S CLAIR ALOHa 4.2 4.9 4.1 4.1 22.0 22.3 21.8 21.8 13.7 14.5 13.6 13.7 6.4 4.8 6.2 6. 81.3 81.0 80.9 81.4 75.5 75.3 75.2 75.6 86.9 85.2 80.0 83.5 36.2 31.5 34.3 36.9 Ours 62.8 52.3 60.9 51. Caption Clean Object Attribution Relation E. Case Figure 5: An example of caption generated by CapMAS, with LLaVA-NeXT-7B as both the captioning and fact-checking model and LLaMA-3-8B as both the decomposer and corrector LLM. F. Prompt Templates Figure 6: The five prompt inputs used to generate captions in our experiments. 14 Submission and Formatting Instructions for ICML 2025 Figure 7: The prompt input for LLaMA-3-8B serving as the decomposer. Figure 8: The prompt input for LLaMA-3-8B serving as the corrector. Figure 9: The prompt input for LLaMA-3-8B serving as the summerizer. We use the prompt employed in the work of (Ge et al., 2024). Submission and Formatting Instructions for ICML 2025 Figure 10: The prompt input for GPT-4o used to create the meta-evaluation dataset of Table 2. Figure 11: The prompt input for GPT-4 used to create the dataset of Figure 1. We use the prompt employed in the work of (Ge et al., 2024)."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Department of Electrical and Computer Engineering, Seoul National University",
        "Interdisciplinary Program in Artificial Intelligence, Seoul National University"
    ]
}