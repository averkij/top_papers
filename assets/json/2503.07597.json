{
    "paper_title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
    "authors": [
        "Yuhong Zhang",
        "Guanlin Wu",
        "Ling-Hao Chen",
        "Zhuokai Zhao",
        "Jing Lin",
        "Xiaoke Jiang",
        "Jiamin Wu",
        "Zhuoheng Li",
        "Hao Frank Yang",
        "Haoqian Wang",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates."
        },
        {
            "title": "Start",
            "content": "HumanMM: Global Human Motion Recovery from Multi-shot Videos Jing Lin1,2 Yuhong Zhang1,2,* Guanlin Wu2,3, Ling-Hao Chen1,2, Zhuokai Zhao4 Xiaoke Jiang2 Jiamin Wu2,5 Zhuoheng Li6 Hao Frank Yang3 Haoqian Wang1 Lei Zhang2 6HKU 1Tsinghua University 3Johns Hopkins University 4University of Chicago 2IDEA Research {dsyuhong, guanlinwu0930, thu.lhchen}@gmail.com Project page: https://zhangyuhong01.github.io/HumanMM 5HKUST 5 2 0 2 0 1 ] . [ 1 7 9 5 7 0 . 3 0 5 2 : r Figure 1. Recovering human motion from multi-shot videos. Top: We take two multi-shot table tennis game videos with shot transitions as input. We aim to recover two motions of two athletes (Long MA and Zhendong FAN) from two videos, respectively. The first video is recorded by three shots (①, ②, and ③ ), and the second one is recovered by two shots (④ and ⑤ ). Bottom: We recover two motions (Long MA in green and Zhendong FAN in pink), different shots, and camera poses for each multi-shot video. The recovered motion is aligned with the motion in the videos."
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing *Equal contribution, Core contributor, Corresponding author. Work done by Yuhong Zhang, Guanlin Wu, Ling-Hao Chen, Jing Lin and Jiamin Wu during the internship at IDEA Research. methods primarily focus on single-shot videos, where continuity is maintained within single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating shot transition detector and robust alignment module for accurate pose and orientation continuity across shots. By leveraging custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates. 1 1. Introduction In recent years, significant advances have been made in 3D human pose estimation, particularly in enhancing the accuracy of human motion recovery (HMR)1 from monocular video sequences. HMR has demonstrated extensive applications in areas such as human-AI interaction [1, 2], human motion understanding [36], and motion generation [3, 7 24]. While existing methods [25] have achieved relatively high performance in recovering human mesh in camera coordinates, estimating human motion in world coordinates remains challenging [2629] due to inaccurate camera pose estimation and the complexity of reconstructing human motion spatially. Most current progress in 3D human motion community mainly benefits from large scale data [25, 2731], and longsequence videos. These resources enhance estimation accuracy for HMR methods and improve the understanding and generation of longer motion sequences for tasks such as motion understanding [3, 32, 33] and generation [3, 7 20, 3447], even when annotations are derived from markerless capturing methods like pseudo labels [4851]. promising approach to enlarge the scale of the motion databases is to estimate human motions from unlimited online videos in markerless manner. However, many longsequence online videos are recorded with multiple shots, referred to as multi-shot videos2, especially prevalent in domains such as sports broadcasting, talk shows, and concerts. In filmmaking and television live show, shot denotes an individual camera view capturing specific moment or action from particular vantage point [52]. Segmenting multi-shot videos into separate shots inevitably reduces the length of the video sequences, which can be detrimental to tasks that benefit from longer sequences, such as long motion generation [47, 53]. This limitation is highlighted in the existing datasets [54, 55], where the longest clip is less than 20 seconds after segmentation, as shown in Fig. 2. Moreover, focusing exclusively on online single-shot videos diminishes the utilization ratio of available online videos and may negatively impact the diversity of scenarios represented in the created datasets. Therefore, how to address the issue of discontinuities caused by shot transitions is notoriously difficult in the community. To resolve this problem, previous works [56 59] have proposed algorithms to address human mesh recovery in camera space from movies containing shot 1In this paper, the human mesh recovery refers to recovery in the camera coordinates and the human motion recovery denotes recovery in the world coordinates. Unless specified otherwise, HMR refers to human motion recovery. 2In this paper, multi-shot video refers to long-sequence video containing multiple shot transitions. We assume that the camera intrinsics remain consistent across different shots within multi-shot video. Figure 2. The comparison between the distribution of sequence lengths in different existing large-scale markerless motion datasets with ours. The x-axis and y-axis denote the duration time (s) and percentage of video number, respectively. Our dataset (in green) contains more portion of long-sequence videos in general. change between long shots and close-ups. However, recovering human motions in world coordinates from multi-shot videos presents two fundamental challenges that remain underexplored. 1) How to align the human motion and orientation in the world coordinates during shot transitions? Ensuring continuity of human orientation and pose across shots is complicated by factors such as partial visibility of human body (e.g. transitioning from long shot to close-up) and changes in human orientation (e.g. two long shots from different viewpoints). These issues, caused by abrupt changes in camera viewpoints, necessitate robust alignment mechanisms. 2) How to reconstruct accurate human motion in world coordinates? Existing approaches employ Simultaneous Localization and Mapping (SLAM) methods to estimate camera parameters, which are then used to project recovered human meshes from camera to world coordinates [2629]. This process requires highly accurate camera estimation and must address motion consistency and foot sliding in the recovered human motion within the world space. Despite these challenges, human motion in multi-shot videos often remain continuous across shots, even as camera viewpoints change. This observation suggests that with appropriate handling of shot transitions and camera motion, it is possible to reconstruct consistent and complete 3D human motions throughout multi-shot videos. In this paper, we propose novel framework HumanMM, Human Motion recovery from Multi-shot videos, to address these challenges. It integrates human pose estimation across shots with robust camera estimation in the world space. Firstly, we develop shot transition detector to identify frames with shot transitions. To ensure more robust camera pose estimation, we introduce an enhanced SLAM method incorporating long-term tracking of feature points and exclusion of moving human from bundle adjustment process. We utilize existing HMR method integrated with our enhanced camera estimation to get the initial human parameters for each separated shot. Subsequently, we implement an alignment module to align human orientation based on stereo calibration and smooth human poses through trained multi-shot HMR encoder, which effectively captures the temporal context of human movements across different shots. Finally, after aligning human and camera parameters between shot transitions, we train motion decoder and trajectory refiner to smooth the human pose and mitigate issues such as foot sliding, thereby enhancing the overall motion consistency in the reconstructed 3D human motions. Our contributions can be summarized as follows. We present the first approach to reconstruct human motion from multi-shot videos in world coordinates. We introduce HumanMM, HMR framework for multiIt includes an enhanced camera trajectory shot videos. estimation method, human motion alignment module and motion integrator to ensure accurate and consistent recovery of human pose and orientation in world coordinates across different shots in the whole video. We develop multi-shot video dataset ms-Motion to evaluate the performance of HMR from multi-shot videos, based on existing public datasets such as AIST [60] and Human3.6M [61]. Extensive experiments on related benchmarks verify the effectiveness of our method. 2. Related Work 2.1. HMR from One-shot Video One-shot videos, captured with single camera without shot transitions, has been extensively studied within the community for human mesh and motion recovery. Human mesh recovery in camera coordinates can be broadly categorized into two approaches: optimizationbased methods [6266] and regression-based methods [30, 6770]. With the significant advancements of transformer [71], HMR2.0 [25] has surpassed previous methods and benefits several downstream tasks related to HMR. Although there are several previous works tried to recover motions in world coordinates with multi-camera capture system [60, 72] and IMU-based methods [73, 74] and enjoy relatively satisfying results, this setup limits their use for applications of infinite in-the-wild monocular videos. To address this limitation, several attempts [2629] integrate SLAM into the HMR pipeline by first estimating the camera pose using SLAM methods, e.g. DROID-SLAM [75] or DPVO [76], and then project the recovered human motion from camera to world coordinates. To exclude the inconsistencies caused by dynamic objects, such as moving humans, TRAM [27] modifies DROID-SLAM by incorporating human masking and depth-based distance rescaling. However, DROID-SLAM performs dense bundle adjustment (DBA) on feature maps from downsampled images and selects features based only on two consecutive frames rather than long-term video sequences [7577]. Consequently, masking significantly reduces the number of informative and consistent features, especially when humans occupy large portions of the image, leading to inaccuracies. Therefore, developing SLAM method that retains sufficient and representative features for DBA after masking is important. 2.2. HMR from Multi-shot Video Multiple shots are fundamental elements of cinematic storytelling and live performances, utilizing various camera positions and focal lengths to create immersive and detailed viewing experiences for audiences [52]. However, most marker-based motion capture (MoCap) datasets [60, 72, 73, 78, 79] consist single-shot videos only, resulting in limited research on HMR from multi-shot videos. Recovering human motion from multi-shot videos in camera coordinates is already challenging. This is because treating each pose estimation result of each shot separately leads to inconsistencies when combining all estimations, caused by partially or fully invisible human bodies across shot transitions. Pavlakos et al. [56] addresses this issue by focusing on shot changes from long shots to close-ups, which are common in film. They develop smoothness constraints within temporal Human Mesh and Motion Recovery (t-HMMR) model to infer motions during occlusions caused by shot transitions. Advancements in HMR methods [29] for single-shot videos in world coordinates have paved the way for extending HMR to multi-shot videos with varying camera viewpoints. However, aligning human orientation, body pose, and translation continuously across multi-shot videos in world coordinates underexplored. Effective alignment is crucial to maintain motion continuity and coherence, especially when dealing with diverse camera perspectives and abrupt transitions between shots. In summary, while substantial progress has been made in HMR from single-shot videos, extending these techniques to multi-shot videos requires addressing additional complexities related to camera pose alignment and motion consistency across shot transitions. We address this challenge by proposing novel pipeline that ensures accurate and continuous 3D HMR from multi-shot monocular videos. 3. Method In this section, we propose HumanMM to recover human motion from multi-shot videos. The system overview is shown in Fig. 3. Given an input video sequence = {It}T t=1 of length , where It denotes the t-th frame, our objective is to recover human motion in world coordinates. We begin by detecting shot transition frames based on human bounding box (a.k.a. bbox) and 2D keypoints (a.k.a. KPTs) through shot transition detector (Sec. 3.2). For each clipped shot, we initialize the camera pose (camera rotation and camera translation) and recover initial human R, Γ θ, β, τ ϕ, β, Γ, τ Figure 3. The overview of HumanMM. HumanMM processes multi-shot video sequences by first extracting motion feature such as keypoints and bounding boxes, using ViTPose [80] and image feature using ViT [81]. These features are then segmented into singleshot clips via Shot Transition Detection (Sec. 3.2). Initialized camera (camera rotation and camera translation T) and human (SMPL) parameters for each shot are estimated using Masked LEAP-VO (Sec. 3.3) and GVHMR [29]. Human orientation is aligned across shots through camera calibration (3.4.1), and ms-HMR (Sec. 3.4.2) ensures consistent pose alignment. Finally, bi-directional LSTM-based trajectory predictor with trajectory refiner predicts trajectory based on aligned motion and mitigates foot sliding throughout the video. motion in world coordinates (Sec. 3.3). The initialized SMPL parameters and camera poses are then fed into human motion alignment module (Sec. 3.4), which aligns human orientations via camera calibration based on human 2D KPTs and smooth the human pose by incorporating pose information across different shots. Additionally, it refines the entire motion sequence through whole video using temporal motion encoder ms-HMR. Finally, we introduce postprocessing module for motion integration (Sec. 3.5). 3.1. Preliminary: 3D Human Model Our method aims to recover motions in world coordinates in the SMPL [82] format, whose pose at frame can be represented as Mt(θt, βt, Γt, τt) R68903. Here, the body pose, body shape, root orientation, and translation are θt R233, βt R10, Γt R3, and τt R3, respectively. We use K2D to denote human 2D KPTs at each frame t. 3.2. Shot Transition Detector For Multi-shot Video Our algorithm begins with shot transition detection in one video. As shown in Fig. 3, the shot transition detector has three key components, scene transition detector, bounding box (a.k.a. bbox) tracking, and human keypoints tracking. (1) Scene change transition detector. Initially, we employ the SceneDetect [83] algorithm to identify scene changes based on significant variations in the background. However, the SceneDetect fails to detect shot transitions when background changes are unnoticeable, illustrated in Fig. 4. Subsequently, we leverage the following modules to bridge the gap. (2) Bbox tracking for shot transition. As shot change often accompanies with sudden change of human subject size, we track humans in video via mmtracking [84]. Consequently, we compute the Intersection over 4 Figure 4. Shot transition detection examples. Examples (a), (b), and (c) illustrate multi-shot scenarios in online videos. (a) shows scene transitions detectable by SceneDetect. (b) illustrates significant position changes undetectable by SceneDetect but resolvable with bbox tracking-based method. (c) shows pose or orientation transition, requiring pose tracking-based methods as they cannot be addressed by either SceneDetect or bbox tracking. Union (IoU) between neighbor bboxes and identify shot transition when the IoU falls smaller than manually tuned threshold. (3) Human pose tracking for shot transition detection. To achieve finer granularity, we additionally introduce human 2D KPTs to detect extreme corner shot changes in video. By thresholding the IoU of corresponding keypoints between neighbor frames, we can accurately identify shot transitions even with subtle human movements. As each separate module cannot identify all kinds of shot transitions, the three modules are jointly used to clip video into several sub-sequences serially. 3.3. Human Motion and Camera Pose Estimation"
        },
        {
            "title": "For Each Shot",
            "content": "After obtaining the clipped videos, our next goal is to estimate the camera pose and SMPL parameters in the world coordinates for each clipped video. The estimated camera pose and motions for each shot will be used to construct the whole motion sequence in the next stage (Sec. 3.4). How to estimate the camera parameters accurately? Our approach for camera parameter calculation is based on visual odometry (VO) estimation method, LEAP-VO [77]. Utilizing the CoTracker method [85], LEAP-VO estimates the visibility and trajectories of selected points by analyzing image gradients across the video sequence. LEAPVO subsequently computes confidence scores for each trajectory, retaining only those with high confidence while discarding trajectories shorter than predefined threshold. The remaining trajectories undergo bundle adjustment (BA) within fixed window size to estimate the camera poses. However, simply applying LEAP-VO in the camera estimation process is still unsatisfactory in most human-centric scenarios. The primary limitation stems from the dynamic movements of human subjects, which typically occupy substantial portion of each image in human-centric videos. This dynamic presence introduces noise into the camera pose estimation in world coordinates, as the estimation process relies heavily on the relationship between the camera and the static environment. To address this issue, we propose Masked LEAP-VO algorithm. Our approach involves inputting the image It and the human bbox at frame into SAM [86] to generate human mask. We then assign visibility value of zero to points within the human mask, effectively excluding these trajectories from the BA process. For clarity, we denote SBA as the window size of BA, ˆn denotes the number of filtered point trajectories, and wij,ˆn as the normalized weight based on confidence score and visibility. For estimating the camera poses = {R, T} of orientation and translation, the reprojection loss function for BA can then be formulated as follows, = arg min G,di, ˆn (cid:88) (cid:88) (cid:88) jijSBA ˆn wij,ˆnF (Gi, Gj , di, ˆn) Πij (pi, ˆn), where F(Gi, Gj, di,ˆn) denotes the point positions calculated by camera pose at frame and with depth di,ˆn. Πij(pi,ˆn) denotes the position for project position of pi,ˆn from frame to j. Consequently, we obtain the camera rotation Rt and translation Tt from camera pose Gt at t. Recovering human motion in world coordinates with estimated camera parameters. Given an input video, we feed the estimated camera parameters (Rt and Tt) into the state-of-the-art motion recovering model, GVHMR [29], , βw θw , Γw , τ Initialized human parameters θw and camera parameters Rt, Tt will input to human motion alignment. , Γw = GVHMR(It, Rt, Tt). , βw , τ (1) 3.4. Aligning Human Motion Between Shots Based on initialized world motion for each individual shot, the subsequent question is how to merge discontinuous motions from different shots into continuous motion sequence 5 Figure 5. Human orientation alignment module. Following shot transition after the foremost purple human mesh (shot ① captured by camera C0), the unaligned (blue) and aligned (green) motions are captured as shot ② and shot ③ by camera 0 and C1, 0 = C0. To achieve human orientation alignment respectively. from shot ① to ③, the camera rotation matrix from 0 to C1 is computed and applied as the offset of human orientation. as whole in world coordinates. straightforward solution is to align all motion sequences to the world coordinate system of the first shot. However, finding the correspondence between different shots is still under-explored and challenging. To resolve this issue, we decompose the motion parameters into camera-dependent and camera-independent ones. The former (Sec. 3.4.1) achieves alignment between shots via human orientation alignment based on camera calibration, whereas the latter (Sec. 3.4.2) is trainable module to enhance the continuity of human motion sequence. These two key designs ensure consistent motion sequence between frames when encountering shot transitions. 3.4.1 Aligning Human Orientations Between Shots t, τ t, βi t, Γi t, Ti , Ri After obtaining the initial SMPL and camera parameters {θi t} for each shot, directly concatenating motions between shots result abrupt changes of human poses and orientations. To address this issue, we introduce the Orientation Alignment Module (OAM), as shown in Fig. 5, to align human orientations. As the whole motion sequence is continuous, we have the following assumption. Assumption 1 Human orientations and translations during the shot transition in world coordinates are continuous. To align the orientations between two frames with shot transition under Assumption 1, we decompose the human orientation with shot transitions in world coordinates as, (2) R(Γworld) = RδcamR(Γview), where Rδcam represents the camera rotation on the Y-axis between current t-th and previous 1-th frame, Γview denotes the human orientation estimated by the current shot, and R() : R3 R9 is the mapping from axis angle to rotation matrix. As Γview in current shot can be estimated independently, mentioned in Sec. 3.3, obtaining accurate Γworld in Eq. (2) remains key challenge to estimate the relative camera rotation Rδcam between frames in shot transitions. Estimating the relative camera pose Rδcam between transition frames. Different from our approach of estimating camera pose in each shot (Sec. 3.3), we do not mask the human subject when estimating camera rotation Rδcam. Instead, we use human 2D KPTs as explicit feature matching. Specifically, we filter out unmatched keypoints based on their visibility and unaligned direction using RANSAC [87], effectively addressing camera pose estimation during shot transitions. This procedure is referred to as Camera Calibration (a.k.a. epipolar-geometry-based camera extrinsics estimation), and is detailed below. In Camera Calibration, we assume that the human translations remain unchanged across the shot transition, implying that only the cameras orientation changes (i.e. Assumption 1). Consequently, we calculate the orientation offset by determining the change in camera orientation using camera calibration. We begin by extracting human 2D KPTs from two consecutive frames during the shot transition. Due to the shot transition, the visibility of 2D KPTs may vary, e.g. occlusion in some shots. Therefore, we employ EDPose [88] to filter out invisible 2D KPTs between shot transition frames. Subsequently, RANSAC identifies matching 2D KPTs corresponding to the most possible camera rotation direction. These matched 2D KPTs facilitate the estimation of the aligned camera rotation Rδcam. The detailed estimation process is as follows. We detected of the denote = shot the in two frames 1 ), , (x(N ) 1 , y(2) [(x(1) 1 ), (x(2) 1 , y(1) R2N and S2 = [(x(1) , y(N ) 2 , y(2) 2 ), (x(2) 2 , y(1) )] 2 R2N . The essential matrix = [T]R should satisfy the following orthogonal property such that, 2D transition , y(N ) 1 2 ), , (x(N ) KPTs as S1 )] 2 1 ES2 = 0. (3) Once is obtained by solving Eq. (3), we enforce the rank2 constraint on through SVD decomposition and subsequently derive the aligned camera rotation Rδcam between two frames (cf . Hartley et al. [89] for more details). In summary, we reformulate the alignment problem of human orientation in shot transitions as estimating the relative camera rotation Rδcam between frames. Accordingly, we obtain the camera rotation Rδcam via camera calibration. 3.4.2 Aligning Human Poses Between Shots In shot transition, video sequences recorded by two shots are often with various occlusions. However, unoccluded body parts in two shots can be complementary to each other for motion alignment. Thus, we introduce the multi-shot HMR (ms-HMR, i.e. EM ()) module to refine the whole motion sequence. As shown in Fig. 6, the ms-HMR is Trans- {θt}T t=1 {ϕt}T t= Figure 6. ms-HMR Structure. The initial human pose parameters θ across multiple video shots are input into transformer with shot-index-based positional encoding. This enables ms-HMR to generate consistent human poses across all shots in the video. former encoder-like architecture, whose input and output are the estimated global motion and the refined global motion, respectively. The process can be formulated as, ϕ1, ϕ2, , ϕT = EM (θ1, θ2, , θT ), (4) where ϕ denotes the refined motion of each frame. With this design, our method can adapt to diverse occlusions of human body brought by shot transitions. 3.5. Post-processing Module for Motion Integration Trajectory Predictor and Trajectory Refiner. Based on the aligned human pose and orientation, we introduce bi-directional LSTM trajectory predictor to recover footground contact probabilities pc , and root velocity vt as, , vt = LSTM(ϕm pc 1 , Γ1, F(I1), ϕm 2 , Γ2, F(I2), , , ΓT , F(IT )), ϕm (5) where F() denotes the image feature of each frame extracted by ViT [81]. Accordingly, the contact probabilities pc , and velocity vt are supervised by the ground-truth labels with MSE loss and are used to reconstruct the trajectory. Besides, we extend the trajectory refiner in WHAM [28] to alleviate foot sliding problem in our estimated trajectory. 4. Benchmarking Multi-shot Motion Recovery Dataset Construction. To create multi-shot 3D human motion dataset, we introduce ms-Motion by processing existing public 3D human datasets with multiple camera settings and ground truth human and camera parameters, specifically AIST [60] and Human3.6M (H3.6M) [61]. In 6 Dataset Models 2-Shot 3-Shot 4-Shot PA. WA. RTE ROE Jitter F.S. PA. WA. RTE ROE Jitter F.S. PA WA RTE ROE Jitter F.S. ms-AIST ms-H3.6M SLAHMR [2023] 72.34 341.75 65.34 336.82 WHAM [2024] 60.72 231.36 GVHMR [2024] 36.82 121.35 Ours 9.62 4.39 6.20 2.56 96.26 62.59 3.26 84.48 25.24 2.75 96.58 34.87 7.65 69.23 33.27 2.66 SLAHMR [2023] 80.67 352.61 16.67 111.97 37.80 7.93 71.32 313.58 11.41 82.42 18.40 5.09 WHAM [2024] 81.93 18.45 8.80 64.63 254.30 GVHMR [2024] 53.39 19.05 4.17 40.52 132.13 Ours 6.94 3. 80.35 510.77 10.33 101.36 72.39 4.43 78.68 451.32 70.33 357.16 38.52 141.38 89.84 24.06 2.99 102.88 603.93 83.77 563.17 99.69 34.46 9.42 39.63 161.52 67.71 35.07 3.55 90.32 803.69 12.11 104.07 80.37 16.52 90.07 26.29 3.62 5.57 8.96 104.53 35.67 9.78 70.31 39.49 4.09 4.55 5.14 7.55 3.64 97.15 562.10 16.91 118.46 52.23 9.96 107.90 748.58 17.85 116.72 65.15 11.58 90.50 512.66 12.91 90.34 18.40 5.69 79.51 423.98 12.36 84.85 18.87 5.03 91.63 19.47 10.65 80.79 296.74 85.25 58.26 18.36 10.62 85.19 471.53 61.22 19.77 5.12 50.59 147.62 45.35 145.36 58.26 17.35 4. 9.12 6.20 5.33 Table 1. Quantitative comparison of different HMR methods on ms-Motion dataset. We record the results for ms-AIST and ms-H3.6M separately. PA. and WA. means PA-MPJPE and WA-MPJPE respectively, while F.S. is Foot Sliding. Our proposed method has achieved the best performance in PA-MPJPE, WA-MPJPE, RTE and ROE across ms-Motion among these methods. Dataset Duration(s) Videos FPS Max Length Min Length Shots ms-Motion 23.7 600 30 1478 314 2, 3, Table 2. Statistics of the ms-Motion dataset. By shots, we mean the number of shot transitions in single video. our construction pipeline, we randomly separate each original one-shot video into several clips. Then, we choose each clip from different shots and concatenate them together as one video recorded by multiple shots. For example, AIST provides each video with eight cameras C0, C1, ..., C7 from different view point and we choose video and split it into 5 clips at t0, t1, ..., t4. For frames in these separated clips, we choose frames shot by random camera for each clip and combine five clips as one multi-shot video. Therefore, we construct multi-shot version of AIST and H3.6M, which are named ms-AIST and ms-H3.6M subsets. Then we combine them and name this new dataset ms-Motion. The detailed statistics of ms-Motion are shown in Tab. 2. We do not compare with other existing 3D human datasets as they contain limited number of multi-shot videos. Benchmark Evaluation Protocol. To evaluate the performance of our proposed methods on multi-shot videos, our target is to evaluate metrics for accurately reflecting the performance on videos with shot transitions. To this end, we use Root Orientation Error (a.k.a. ROE in deg ) to measure the performance of the proposed method on human orientation alignment across different shots. Besides, we use Root Translation Error (a.k.a. RTE in m) to assess the performance of the proposed method on global trajectory recovery. Jitter ( 10m ps3 ) is also used to evaluate the stability of recovered human pose from multi-shot videos. We also include foot sliding (cm), the averaged displacement of foot vertices during contact with the ground, to assess the precision of recovered motion in the world coordinates [28]. 5. Experiment 5.1. Datasets and Metrics Evaluation Datasets. To evaluate the performance of our proposed pipeline for multi-shot videos, we use ms-Motion dataset and EMDB-1 dataset [73] with self-added noise for the evaluation of ablation study. For camera trajectory estimation, we use EMDB-1 and EMDB-2 split [73] as they contain the GT moving camera trajectory. Our self-created dataset contains 600 multi-shot videos, 42.7K frames, totaling 237 minutes. EMDB-1 split contains 17 video sequences totaling 13.5 minutes and EMDB-2 split contains 25 sequences in total of 24.0 minutes. Evaluation Metrics. For shot detection we use Recall, Precision and F1 Score as evaluation metrics. For 3D human pose estimation-related tasks, we use ROE, RTE, jitter, and foot-sliding for evaluating the human motion recovery results on multi-shot videos. For the ablation study of our proposed pipeline, we evaluate the Procrustes-aligned Mean Per Joint Position Error (a.k.a. PA-MPJPE) and Per Vertex Error (a.k.a. PVE) as additional metrics besides previous mentioned ones. For camera pose estimation, we use absolute trajectory error (a.k.a. ATE) (m), Relative Pose Error (a.k.a. RPE) rotation (deg), and RPE translation (m). 5.2. Implementation Details The ms-HMR, the trajectory, and foot sliding refiner are trained on the AMASS [78], 3DPW [79], Human3.6M [61], and BEDLAM [90] datasets, and evaluated on EMDB and our ms-Motion. During training, we introduce random rotational noise (ranging from 0 to 1 radian) along the y-axis to the root pose Γ and random noise to the body pose θ at random positions to simulate the inaccuracies of pre-estimated human motions caused by shot transitions in multi-shot videos. This strategy enables the network to robustly recover smooth and consistent human motion from noisy initial parameters. The benchmark test results were obtained after training for 80 epochs on one NVIDIA-A100 GPU. 5.3. Main Results: Comparison of Global Human Motion Recovery Results on the Benchmark We compare our proposed method HumanMM with several state-of-the-art HMR methods (SLAHMR [26], WHAM [28] and GVHMR [29]) on our proposed benchmark ms-Motion. As illustrated in Tab. 1, our proposed 7 Methods PA-MPJPE MPJPE WA-MPJPE W-MPJPE PVE ACCEL RTE ROE Foot Sliding Baseline (Concat) w/o ms-HMR w/o OAM w/o trajectory predictor w/o trajectory refiner HumanMM (Ours) 106.48 78.24 73.56 50.49 50.49 50.49 141.67 101.52 92.13 83.68 83.68 83. 273.15 246.42 243.65 231.75 198.58 194.77 553.67 436.57 425.18 432.17 397.65 393.21 122.15 85.77 79.64 75.77 75.77 75.77 6.14 5.87 5.67 5.75 5.23 5.16 10.86 3.89 6.61 5.52 4.06 3.54 91.55 50.63 76.74 47.68 47.68 47. 14.91 3.54 4.45 4.96 7.84 3.28 Table 3. Ablation studies on different combinations of HumanMM modules. We evaluate the contributions of each key module in HumanMM (ms-HMR, OAM, trajectory predictor and trajectory refiner) to overall performance using various human motion recovery metrics. The complete HumanMM model achieves state-of-the-art performance with minimal foot sliding and robust global motion recovery. Methods ms-Motion Methods ATE RPE Trans. RPE Rot. Recall Precision F1 Score Scenes Detect (SD) [83] SD+Bbox Tracking (Bbox) SD+Bbox+Pose Tracking 0.74 0.88 0.96 0.72 0.85 0. 0.70 0.86 0.92 DPVO (w/o mask) Masked DPVO LEAP-VO (w/o mask) Masked LEAP-VO 0.48 0.50 0.50 0.49 1.07 0.86 0.83 0.83 1.26 1.21 1.21 1.19 Table 4. Comparison between difference shot detection algorithms. We evaluate our shot transition detector on our proposed multi-shot video human motion dataset ms-Motion. Table 6. Camera tracking results on EMDB-2 [73]. Our method maintains its comparable performance in both RPE Trans. and RPE Rot. metrics, illustrating its effectiveness. Methods ATE RPE Trans. RPE Rot. DPVO (w/o mask) Masked DPVO LEAP-VO (w/o mask) Masked LEAP-VO 0.48 0.48 0.50 0.51 1.85 1.57 0.93 0. 1.06 0.97 0.97 0.95 Table 5. Camera tracking results on EMDB-1 [73]. Our method has achieved 50% on RPE trans. than that of the original DPVO and achieve the best performance in RPE rot. metrics. method has achieved the best performance for PA&WAMPJPE, RTE and ROE through videos with all numbers of shots across ms-AIST and ms-H3.6M, indicating that our method reconstructs both the global human motion and orientations in the world coordinates more accurately and robustly. For the foot sliding metric, our method also performs as the best on ms-H3.6M across all numbers of shots. 5.4. Ablation Studies Human-centric Scene Shot Boundary Detection Evaluation. To evaluate the performance of our proposed Shot Transition Detector, we test the algorithm on our proposed multi-shot human motion recovery benchmark and compare the output frame list of shot transitions with the ground truth (GT) of our dataset. As shown in Tab. 4, by applying the proposed finer granularity shot detection methods, the number of recall, precision, and F1 score all increases consistently. The combination of three steps (ScenesDetect, bbox tracking, and pose tracking) has achieved 0.96, 0.88, and 0.92 on the recall, precision, and F1 score, respectively, which indicates comparable performance in shot boundary detection. Besides, as can be seen in the results, the latter two steps of shot detection contribute to the fine-grained Methods WA-MPJPE W-MPJPE RTE Jitter F.S. DPVO Masked DPVO LEAP-VO Masked LEAP-VO 305.40 303.90 284.10 283.70 117.10 116.40 112.80 112. 5.10 4.10 3.10 3.10 17.90 17.40 16.30 16.30 3.50 3.50 3.50 3.50 Table 7. Global motion recovery results on EMDB-2 [73]. We input estimated camera parameters from different methods into GVHMR for the comparison on human motion recovery metrics. final results significantly and jointly. Key modules in the Proposed Method. We compare our methods with four variants on EMDB with noise dataset, as shown in Tab. 3, ms-HMR is the key component for the improvement in PA-MPJPE and PVE, which indicates more accurate modeling of the whole motion sequence. This design serves as recovery module to estimate some invisible body parts in some shots. Additionally, the orientation alignment module (OAM, in Sec. 3.4) is also critical block for accurate human orientation estimation, indicated by the metric ROE. This module helps to model the global human motion between shots. For foot sliding, the results in Tab. 3 also show that the trajectory refiner (Sec. 3.5) in our method helps mitigate the foot sliding issue. Comparison on Camera Trajectory Estimation. To evaluate the performance of our proposed camera trajectory estimation method Masked LEAP-VO, we evaluate the camera trajectory accuracy on EMDB-1 and EMDB-2. For more convenient comparison, we introduce two baselines, DPVO [76], which has been widely used in HMR methods such as WHAM [28] and GVHMR [29], and LEAPVO [77]. To provide more intuition about the insights of masking dynamic humans in the video, we also implement variant, Masked DPVO, by applying SAM at the patchify stage of DPVO to exclude patches containing human pixFigure 7. Qualitative comparison of different HMR methods on ms-Motion dataset. The side view of the rendered mesh for input mutli-shot video is shown in (a), while the top view is shown in (c). We also draw the comparison of the human trajectory as shown in (b). Our method is the most similar as GT in both rendered motion and trajectories among these methods. Methods 2-shot 3-shot 4-shot PA. WA. RTE ROE Jitter F.S. PA. WA. RTE ROE Jitter F.S. PA. WA. RTE ROE Jitter F.S. SLAHMR [2023] 75.54 326.15 65.14 339.85 WHAM [2024] 66.18 234.96 GVHMR [2024] 33.96 109.72 Ours 9.35 4.65 5.82 2.12 94.63 81.12 92.13 66.58 63.74 25.29 35.77 33.49 3.15 81.22 523.98 11.59 105.44 74.12 25.85 2.82 83.10 501.29 35.63 4.72 72.63 359.99 1.69 40.12 131.39 31. 89.88 97.45 68.33 7.82 6.42 3.94 4.55 93.45 3.05 100.20 801.63 588.40 90.52 5.85 165.23 42.64 3.01 811.31 14.85 109.56 80.69 14.12 3.98 94.36 9.52 96.75 3.52 69.15 27.29 31.66 32.22 5.97 9.02 4. Table 8. Quantitative results on ms-Motion (20). We select 20 videos from ms-Motion and generate 2-, 3-, and 4-shot versions for each, yielding total of 60 videos for evaluation. PA. and WA. means PA-MPJPE and WA-MPJPE respectively, while F.S. is Foot Sliding. The results follow similar trend to those on original ms-Motion, showcasing ms-Motions effectiveness in evaluating different HMR methods. els. As shown in Tab. 5 and Tab. 6, compared with baseline methods, our key design of masking dynamic human subjects improves the result in both RPE Translation and RPE Rotation while maintaining competitive ATE. This result indicates the effectiveness of the design of masking dynamic human subjects in the process of camera trajectory estimation. Compared with the DPVO baseline, our method achieves 50% RPE translation on EMDB-1. In addition, we run GVHMR [29] on EMDB-2 with different estimated camera trajectories. The results is shown in Tab. 7, which further illustrates the effectiveness of our method. Further evaluation on ms-Motion. To ensure balanced evaluation and evaluate potential data bias in the ms-Motion dataset, where videos in the 2-, 3-, and 4-shot categories are mutually exclusive, we create multiple versions of the same video across different shot categories for further evaluation. Specifically, we randomly select 20 videos and create their 2-, 3-, and 4-shot versions, resulting in total of 60 videos. We then evaluate SLAHMR [26], WHAM [28], GVHMR [29], and our method on these videos. The results, presented in Tab. 8, follow the same evaluation pattern as those on the full ms-Motion dataset in Tab. 1, which indicates the effectiveness of our proposed dataset in evaluating different human motion recovery methods. 6. Conclusion and Discussion In this paper, we introduce HumanMM, the Conclusion. first framework designed for human motion recovery from multi-shot videos in world coordinates. HumanMM addresses the challenges inherent in multi-shot videos by 9 incorporating three key components: an enhanced camera trajectory estimation method called masked LEAP-VO, human motion alignment module that ensures consistency across different shots, and post-processing module for seamless motion integration. Extensive experiments demonstrate that HumanMM outperforms existing human motion recovery methods across various benchmarks, achieving state-of-the-art accuracy on our newly created multi-shot human motion dataset, ms-Motion. Limitations and Future Work. While HumanMM represents an dvancement in human motion recovery from multishot videos in world coordinates, its performance may decline when faced with an excessive number of shot transitions. Despite these challenges, HumanMM provides solid baseline for human motion recovery from multi-shot videos and can be employed in annotating markerless human motion datasets. Our newly introduced dataset, msMotion, offers valuable benchmark for evaluating general human motion recovery methods in world coordinates, especially regarding their performance on multi-shot videos. Based on the proposed method, our future work aims to enlarge the related datasets for larger-scale motion databases."
        },
        {
            "title": "Acknowledgement",
            "content": "by funded partially This work was the Shenzhen Science and Technology Project under Grant KJZD20240903103210014. The author team would also like to convey sincere thanks to Ms. Yaxin Chen from IDEA Research for the expressive dance motion used in the demo presentation."
        },
        {
            "title": "References",
            "content": "[1] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In CVPR, pages 2042820437, 2022. 2 [2] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. In ICLR, 2024. 2 [3] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In ECCV, pages 580 597, 2022. 2 [4] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. NeurIPS, 2024. [5] Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, and Yangang Wang. Synthesizing physically plausible human motions in 3d scenes. In 3DV, 2024. [6] Jingbo Wang, Ye Yuan, Zhengyi Luo, Kevin Xie, Dahua Lin, Umar Iqbal, Sanja Fidler, and Sameh Khamis. Learning human dynamics in autonomous driving scenarios. In ICCV, pages 2073920749, 2023. 2 [7] Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In ECCV, pages 358374, 2022. 2 [8] Mathis Petrovich, Michael Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In ECCV, pages 480497, 2022. [9] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE TPAMI, 2024. [10] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. In ICLR, 2022. [11] Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Humanise: Language-conditioned human motion generation in 3d scenes. NeurIPS, pages 14959 14971, 2022. [12] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In CVPR, pages 1800018010, 2023. [13] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In ICCV, pages 1601016021, 2023. [14] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In CVPR, pages 1473014740, 2023. [15] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. Human motion diffusion as generative prior. In ICLR, 2024. [16] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided motion diffusion for controllable human motion synthesis. In CVPR, pages 2151 2162, 2023. [17] Ling-Hao Chen, Shunlin Lu, Wenxun Dai, Zhiyang Dou, Xuan Ju, Jingbo Wang, Taku Komura, and Lei Zhang. Pay attention and move better: Harnessing attention for interactive motion generation and training-free editing, 2024. [18] Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. In ICLR, 2024. [19] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. In ICLR, 2024. [20] Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. ICML, 2024. 2 [21] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. ECCV, 2024. [22] Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, and Tongliang Liu. Humanmac: Masked motion In ICCV, pages completion for human motion prediction. 95449555, 2023. [23] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Scamo: Exploring the scaling law in Ruimao Zhang. arXiv preprint autoregressive motion generation model. arXiv:2412.14559, 2024. [24] Wenxun Dai, Ling-Hao Chen, Yufei Huo, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Real-time controllable motion generation via latent consistency model. 2025. 2 [25] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa*, and Jitendra Malik*. Humans in 4D: Reconstructing and tracking humans with transformers. In ICCV, 2023. 2, 3 [26] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In CVPR, 2023. 2, 3, 7, 9, 8 [27] Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from inthe-wild videos. ECCV, 2024. 2, 3 [28] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael J. Black. WHAM: Reconstructing world-grounded humans with accurate 3D motion. In CVPR, 2024. 6, 7, 8, [29] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view In ACM SIGGRAPH Asia, 2024. 2, 3, 4, 5, coordinates. 7, 8, 9 [30] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics from video. In CVPR, 2019. 3 [31] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael J. Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. Pace: Human and motion estimation from in-the-wild videos. In 3DV, 2024. 2 10 [32] Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. Motionllm: Understanding human behaviors from human motions and videos. arXiv preprint arXiv:2405.20340, 2024. 2 [33] Matthias Plappert, Christian Mandery, and Tamim Asfour. Learning bidirectional mapping between human wholebody motion and natural language using deep recurrent neural networks. RAS, 109:1326, 2018. [34] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot textdriven generation and animation of 3d avatars. ACM SIGGRAPH, 2022. 2 [35] Nikos Athanasiou, Mathis Petrovich, Michael Black, and Gul Varol. Teach: Temporal action composition for 3d humans. In 3DV, pages 414423, 2022. [36] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: framework In CVPR, for denoising-diffusion-based motion synthesis. pages 97609770, 2023. [37] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In ICCV, 2023. [38] Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory and language control for human motion synthesis. ECCV, 2024. [39] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modIn CVPR, pages 19001910, eling of 3d human motions. 2024. [40] Jinpeng Liu, Wenxun Dai, Chunyu Wang, Yiji Cheng, Yansong Tang, and Xin Tong. Plan, posture and go: Towards open-world text-to-motion generation. ECCV, 2024. [41] Bo Han, Hao Peng, Minjing Dong, Yi Ren, Yixuan Shen, and Chang Xu. Amd: Autoregressive motion diffusion. In AAAI, pages 20222030, 2024. [42] Zhenyu Xie, Yang Wu, Xuehao Gao, Zhongqian Sun, Wei Yang, and Xiaodan Liang. Towards detailed text-tomotion synthesis via basic-to-advanced hierarchical diffusion model. In AAAI, pages 62526260, 2024. [43] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast, high-quality motion generation. ECCV, 2024. [44] Mathis Petrovich, Or Litany, Umar Iqbal, Michael Black, Gul Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control for text-driven 3d human motion generation. In CVPRW, pages 19111921, 2024. [45] Boxun Hu, Mingze Xia, Ding Zhao, and Guanlin Wu. Mona: Moving object detection from videos shot by dynamic camera, 2025. [46] Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Move as you say interact as you can: Language-guided human motion generation with scene affordance. In CVPR, pages 433444, 2024. [47] Zhongfei Qing, Zhongang Cai, Zhitao Yang, and Lei Yang. Story-to-motion: Synthesizing infinite and controllable character animation from long text, 2023. 2 [48] Hui En Pang, Zhongang Cai, Lei Yang, Tianwei Zhang, and Ziwei Liu. Benchmarking and analyzing 3d human pose and shape estimation beyond algorithms. In NeurIPS, 2022. 2 [49] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Neuralannot: Neural annotator for 3d human mesh training sets. In CVPRW, 2022. [50] Gyeongsik Moon, Hongsuk Choi, Sanghyuk Chun, Jiyoung Lee, and Sangdoo Yun. Three recipes for better 3d pseudogts of 3d human mesh estimation in the wild. In CVPRW, 2023. [51] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In CVPR, pages 469480, 2023. 2 [52] C.J. Bowen and R. Thompson. Grammar of the Edit. Focal Press, 2013. 2, 3 [53] Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, Gul Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control for text-driven 3d human motion generation. In CVPRW, 2024. 2 [54] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. NeurIPS, 2023. 2 [55] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, pages 51525161, 2022. 2 [56] Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Human mesh recovery from multiple shots. In CVPR, 2022. 2, 3 [57] Peng Wu, Xiankai Lu, Jianbing Shen, and Yilong Yin. Clip fusion with bi-level optimization for human mesh reIn ACM MM, page construction from monocular videos. 105115, New York, NY, USA, 2023. Association for Computing Machinery. [58] Kuan-Chieh Wang, Zhenzhen Weng, Maria Xenochristou, Joao Pedro Araujo, Jeffrey Gu, Karen Liu, and Serena Yeung. Nemo: 3d neural motion fields from multiple video instances of the same action. In CVPR, 2023. [59] Fabien Baradel, Thibault Groueix, Philippe Weinzaepfel, Romain Bregier, Yannis Kalantidis, and Gregory Rogez. Leveraging mocap data for human mesh recovery. In 3DV, pages 586595, 2021. 2 [60] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++, 2021. 3, 6, 7 [61] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE TPAMI, 36(7):13251339, 2014. 3, 6, 7 [62] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM TOG, 36(6), 2017. 3 [79] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and moving camera. In ECCV, 2018. 3, 7 [80] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. ViTPose: Simple vision transformer baselines for human pose estimation. In NeurIPS, 2022. 4 [81] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 4, 6 [82] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. ACM TOG, 34(6):248:1248:16, 2015. 4 [83] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: holistic dataset for movie understanding. In ECCV, 2020. 4, 8 [84] MMTracking Contributors. MMTracking: OpenMMLab https : / / video perception toolbox and benchmark. github.com/open-mmlab/mmtracking, 2020. 4 [85] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoIn ECCV, 2024. 5, Tracker: It is better to track together. 3 [86] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In ICCV, 2023. 5, 2 [87] Martin A. Fischler and Robert C. Bolles. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381395, 1981. 6 [88] Jie Yang, Ailing Zeng, Shilong Liu, Feng Li, Ruimao Zhang, and Lei Zhang. Explicit box detection unifies end-to-end multi-person pose estimation. In ICLR, 2023. 6 [89] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, USA, 2 edition, 2003. 6 [90] Michael J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. BEDLAM: synthetic dataset of bodies exhibiting detailed lifelike animated motion. In CVPR, pages 8726 8737, 2023. 7 [63] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from single image. In Computer Vision ECCV 2016. Springer International Publishing, 2016. [64] Anurag* Arnab, Carl* Doersch, and Andrew Zisserman. Exploiting temporal context for 3d human pose estimation in the wild. In CVPR, 2019. [65] Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. STAR: Sparse Trained Articulated Human Body Regressor, page 598613. Springer International Publishing, 2020. [66] Yinghao Huang, Federica Bogo, Christoph Lassner, Angjoo Kanazawa, Peter V. Gehler, Javier Romero, Ijaz Akhter, and Michael J. Black. Towards accurate marker-less human shape and pose estimation over time. In 3DV, 2017. 3 [67] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018. 3 [68] Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. Vibe: Video inference for human body pose and shape estimation. In CVPR, 2020. [69] Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis. Convolutional mesh regression for single-image human shape reconstruction. In CVPR, 2019. [70] Zhengyi Luo, S. Alireza Golestaneh, and Kris M. Kitani. 3d human motion estimation via motion compression and refinement. In ACCV, 2020. 3 [71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS. Curran Associates, Inc., 2017. [72] Chun-Hao P. Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael J. Black. Capturing and inferring dense full-body human-scene contact. In CVPR, pages 1327413285, 2022. 3 [73] Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Jose Zarate, and Otmar Hilliges. EMDB: The Electromagnetic Database of Global 3D Human Pose and Shape in the Wild. In ICCV, 2023. 3, 7, 8 [74] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. ACM TOG, 40(4), 2021. 3 [75] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. In NeurIPS, pages 1655816569. Curran Associates, Inc., 2021. 3 [76] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. NeurIPS, 2023. 3, 8, [77] Weirong Chen, Le Chen, Rui Wang, and Marc Pollefeys. Leap-vo: Long-term effective any point tracking for visual odometry. In CVPR, 2024. 3, 5, 8 [78] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: Archive of In ICCV, pages 5442 motion capture as surface shapes. 5451, 2019. 3, 7 12 2 3 7 7 7 7 . . . 8 . 8 . 10 . . . . . HumanMM: Global Human Motion Recovery from Multi-shot Videos"
        },
        {
            "title": "Contents",
            "content": "A. Masked DPVO Detailed Algorithm B. Masked LEAP-VO Detailed Algorithm C. Camera Calibration Procedure D. Implementation Details of HumanMM . . . D.1. Training Details . D.2. ms-Motion Benchmark Details . . D.3. Baseline Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Visualization of Comparison between Existing Methods E.1. Visualization of Comparison between Existing Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2. Visualization of in-the-wild multi-shot video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A. Masked DPVO Detailed Algorithm In this section, we provide detailed exposition of the algorithm underpinning our proposed Masked DPVO, as introduced in the context of Comparison on Camera Trajectory Estimation in Sec. 5.4, building on the foundation of DPVO [76]. In accordance with the principles of DPVO, scene is conceptualized as composition of camera poses SE(3)N and collection of square image patches derived from video frames. The inverse depth is denoted as d, while pixel coordinates are represented by (x, y). Each patch Pk is modeled as 4p2 homogeneous array, where denotes the patch width, as illustrated below, Pk = 1 , x, y, R1p2 . (6) Let i, represent the indices corresponding to frame and frame j, respectively. The projection matrix Pkj, which maps the patch Pk, extracted from frame i, to frame j, can then be expressed as follows, where, is calibration matrix, defined as, Pkj KGjG1 K1Pk. = fx 0 cx 0 0 fy cy 0 0 0 1 0 0 0 0 1 . (7) (8) In the original DPVO framework, patches are selected randomly, as this approach has been shown to achieve comparable improvements in performance. However, critical issue arises when selected patch corresponds to region on moving object. In such cases, the estimated camera pose becomes inaccurate, negatively impacting the reconstruction of human motion in world coordinates. Given that our primary focus involves human-centric video data, where moving humans often occupy significant portion of the image, excluding regions corresponding to moving humans presents straightforward yet effective strategy for improving camera pose estimation accuracy. To address this, we incorporate SAM [86] into the DPVO pipeline. Using the bounding box of detected human as prompt, SAM generates human mask. During the patch extraction process across frames, if randomly selected patch falls within the human mask, it is excluded and new patch is selected instead. The resulting patch after applying the human mask is denoted as ˆPk. The corresponding masked projection matrix is subsequently represented as Pkj, ˆPkj KGjG1 K1 ˆPk. (9) Following the principles of DPVO, bipartite patch graph is constructed to capture the relationships between patches and video frames. In this graph, edges connect patches and frames that are within specified distance, defined by the temporal relationship between frame and frame j). The graph is inherently dynamic, as older frames are discarded as newer frames are introduced. For each edge (k, j) in the patch graph, the visual alignment, determined by the current estimates of depth and camera poses, is evaluated. This is achieved by computing correlation features Rpp77 (visual similarities), which represent visual similarities. These correlation features are computed as follows, Cuvαβ = guv ( ˆPkj(u, v) + αβ) (10) where u, are the index of each pixel in patch k, () is the bilinear sampling and is 7 7 grid indexed by α and β. Based on extracted features and correlations, DPVO uses MLP layers to predict trajectory update δkj and confidence weight Σkj. Then, differentiable bundle adjustment (BA) layers is proposed to solve both the depth and camera poses. BA operates on the patch graph and outputs updates to depth and camera pose. The optimization objective of BA is shown as, (cid:88) (cid:13) (cid:13)KGjG1 (cid:13) K1 ˆPk (cid:104) ˆP kj + δkj (k,j)E (cid:105)(cid:13) 2 (cid:13) (cid:13) Σkj (11) where Σ is the Mahalanobis distance and ˆP kj is the center of ˆPkj. DPVO then applies two Gauss-Newton iterations to the linearized objective, optimizing the camera poses and inverse depth component at the same time. The main intuition for this optimization is to refine the camera poses and depth such that the induced trajectory updates agree with the predicted trajectory updates. 2 B. Masked LEAP-VO Detailed Algorithm In this section, we illustrate the detailed algorithm of our proposed camera estimation method Masked LEAP-VO. We start with the preliminaries about tracking any point (TAP), which has been used in LEAP-VO [77]. Given an input video sequence = {It}T t=1 of length , where It denotes the t-th frame, the goal of TAP is to track set of query points across these frames. For specific query point with the 2D pixel coordinate xq R2 in frame It, TAP predicts both the visibility Vq = [vq ] of this point through the whole video. Thus, we have, ] and trajectory Xq = [xq 1, , xq 1, , vq (X, V) = TAP(I, xq , It) (12) LEAP-VO uses CoTracker [85] to as their TAP module. During inference, CoTracker will produce point feature ft for each frame It and then concatenate these point features into tensor Fq = [f1, , fT ]. With predicted and F, LEAP-VO tries to predict whether each point query is on the dynamic object (dynamic label md) using anchor-based trajectory comparison to alleviate the negative effect they bring to the camera pose estimation. Subsequently, LEAP-VO uses temporal probability modeling to get the confidence of estimated trajectories of each point query. Let = [a, b], where a, represent the and coordinates of all points in X, respectively. The probability density function for coordinate is modeled using multivariate Cauchy distribution, p(aIt,xi) = γ( 2 )π 2 (cid:80) 1 γ( 1+T 2 ) 2 [1+(aµa)T Σ1 (aµa)] , 1+S 2 (13) and similarly for p(bIt, xi). Here, γ denotes the Gamma function. LEAPVO then filters out trajectories shorter than predefined threshold and inputs the remaining trajectories into bundle adjustment (BA) stage with sliding window optimization. However, as we mentioned in Sec. 3.3, the performance of this method is still satisfactory as the exclusion of the moving object is not complete. Since we are dealing with human-centric videos, we can simply apply SAM to get the mask of human and exclude these points. Thus, we set visibility of trajectories that contains points inside human mask to zero. The adjusted BA function is shown as Sec. 3.3 in Sec. 3.3. 3 C. Camera Calibration Procedure In this section, we show the detailed procedure of the camera calibration we mentioned in Sec. 3.4.1. We denote the selected feature points of two frames during shot transition in world coordinate as W1 = [(x(1) W2 = [(x(1) w1, y(1) w2, y(1) w1 , z(1) w2 , z(1) w1 ), (x(2) w2 ), (x(2) w1, (y(2) w2, (y(2) w1 , z(2) w2 , z(2) w1 ), , (x(N ) w2 ), , (x(N ) w1 , y(N ) w2 , y(N ) w1 , z(N ) w2 , z(N ) w1 )], w2 )], respectively. Suppose we have the initialized camera translation matrix and camera rotation matrix during shot transition from Masked LEAP-VO in Sec. 3.3, where = and since = x(1) w1 y(1) w1 z(1) r11 r12 r13 r21 r22 r23 r31 r32 r33 , = , tx ty tz r11 r12 r13 r21 r22 r23 r31 r32 r33 + x(1) w1 y(1) w1 z(1) w1 . tx ty tz Thus, according to Epipolar Constraint, (cid:104) w2 y(1) x(1) w2 z(1) (cid:105) Thus, by substituting Eq. (15) to Eq. (16), 0 tz tz ty ty 0 tx 0 tx = 0. x(1) w2 y(1) w2 z(1) w2 (cid:104) w2 y(1) x(1) w2 z(1) w2 (cid:105) 0 tz tz ty ty 0 tx 0 tx r11 r12 r13 r21 r22 r23 r31 r32 r33 + x(1) w1 y(1) w1 z(1) w1 0 tz tz ty ty 0 tx 0 tx = 0. tx ty tz Lets denote [T] as [T] = 0 tz tz ty ty 0 tx 0 tx . As result, the essential matrix can be denoted as = [T]R, = Since e11 e12 e13 e21 e22 e23 e31 e32 e33 = 0 tz tz ty ty 0 tx 0 tx r11 r12 r13 r21 r22 r23 r31 r32 r33 . 0 tz tz ty ty 0 tx 0 tx = 0, tx ty tz by constructing essential matrix and elimination of the second term, the Eq. (17) can then be rewritten as (cid:104) w2 y(1) x(1) w2 z(1) w2 (cid:105) e11 e12 e13 e21 e22 e23 e31 e32 e33 = 0 x(1) w1 y(1) w1 z(1) w1 4 (14) (15) (16) (17) (18) (19) (20) (21) As our method is targeted for the in-the-wild multi-shot videos, we typically do not know the intrinsics for each shot. Thus, we assume that through multi-shot video, the camera intrinsics are the same. The camera intrinsic matrix is denoted as, = fx 0 ox 0 fy oy 0 0 1 (22) Similar as mentioned in Sec. 3.4.1, we denote the coordinates of detected 2D KPTs of two frames in the shot transition as S1 = [(x(1) 1 , y(1) )] R2N . Based on intrinsic matrix K, )] R2N and S2 = [(x(1) 2 ), , (x(N ) 1 ), , (x(N ) 2 ), (x(2) 1 ), (x(2) 2 , y(1) 2 , y(2) 1 , y(2) , y(N ) , y(N ) 1 1 2 (cid:104) x(1) w2 y(1) w2 z(1) (cid:105) (cid:104) x(1) 2 = y(1) 2 (cid:105) 1 Thus, we can substitute Eq. (23) to Eq. (21), 1 0 fx ox 1 fx fy 0 oy fy 0 0 , 1 x(1) w1 y(1) w1 z(1) w2 = 1 0 fx ox 1 fx fy 0 oy fy 0 0 1 x(1) 1 y(1) 1 1 (cid:104) 2 y(1) x(1) (cid:105) 2 1 0 0 fx ox 1 0 fx fy 0 oy 1 fy e11 e12 e13 e21 e22 e23 e31 e32 e33 1 0 0 fx ox 1 0 fx fy 0 oy 1 fy x(1) 1 y(1) 1 1 = 0. We can then denote fundamental matrix as, = and we can rewrite Eq. (24) as, f11 f12 f13 f21 f22 f23 f31 f32 = 1 0 0 fx ox 1 0 fx fy 0 oy 1 fy e11 e12 e13 e21 e22 e23 e31 e32 e33 1 0 0 fx ox 1 0 , fx fy 0 oy 1 fy (cid:104) x(1) 2 y(1) (cid:105) 1 f11 f12 f13 f21 f22 f23 f31 f32 f33 x(1) 1 y(1) 1 1 = 0. (23) (24) (25) (26) One clarification is that the fundamental matrix here is denoted as essential matrix in Eq. (3) in Sec. 3.4.1. After we derive Eq. (26), we can expand it as linear equation, illustrated as, (cid:16) f11x(1) 1 + f12y(1) 1 + f13 (cid:17) 2 + f31x(1) x(1) 1 + f32y(1) 1 + f33 + (cid:16) f21x(1) 1 + f22y(1) 1 + (cid:17) y(1) 2 = 0. (27) This illustrates the case for one correspondence, since we have correspondences, we can build up linear equation system to solve F. Thus, we create matrix based on S1 and S2, shown as, y(1) x(1) 2 x(1) 2 x(1) ... ... y(i) 2 x(i) 2 x(i) x(i) ... ... 2 x(N ) y(N ) 2 x(N ) x(N ) x(1) 2 y(1) ... 2 y(i) x(i) ... y(N ) x(N ) 1 y(1) 2 y(1) ... y(i) 2 y(i) ... y(N ) y(N ) 1 2 x(1) 2 ... x(i) 2 ... x(N ) 2 x(1) 1 ... x(i) 1 ... x(N ) 1 y(1) 2 ... y(i) 2 ... y(N ) 2 y(1) 1 ... y(i) 1 ... y(N ) 1 1 ... 1 ... = (28) 1 1 1 1 1 1 1 . 1 Let denotes the flattened version F, Af = 2 x(1) x(1) ... 2 x(i) x(i) ... 2 x(N ) x(N ) 1 1 1 2 y(1) x(1) ... 2 y(i) x(i) ... y(N ) x(N ) 1 2 x(1) 2 ... x(i) 2 ... x(N ) 2 1 2 x(1) y(1) ... 2 x(i) y(i) ... 2 x(N ) y(N ) 1 1 2 y(1) y(1) ... 2 y(i) y(i) ... y(N ) y(N ) 1 2 1 y(1) 2 ... y(i) 2 ... y(N ) 2 x(1) 1 ... x(i) 1 ... x(N ) 1 y(1) 1 ... y(i) 1 ... y(N ) 1 1 ... 1 ... 5 = 0. (29) f11 f12 f13 f21 f22 f23 f31 f32 f33 Then, we can solve by applying singular value decomposition (SVD) on A. Decompose into three matrices = ΣV . Substituting into Eq. (29), ΣV = 0 ΣV = 0 ΣV = 0. (30) Since is in the null space of A, from decomposition, the null space is spanned by the last columns of corresponding to zero singular values in Σ. Thus, we can extract the last column of (denoted as vn) and assign = vn. After we get , we can get the fundamental matrix F. We can then derive essential matrix from shown as, = KFK = fx 0 0 0 fy 0 ox oy 1 f11 f12 f13 f21 f22 f23 f31 f32 f33 fx 0 ox 0 fy oy 0 0 1 Then, we can derive camera rotation and camera translation by applying SVD on essential matrix E, = ΣV = u11 u12 u13 u21 u22 u23 u31 u32 u33 σ1 0 0 0 σ2 0 0 0 σ3 = u11 u12 u13 u21 u22 u23 u31 u32 u33 0 1 0 1 0 0 0 0 1 v11 v12 v13 v21 v22 v23 v31 v32 v33 v11 v12 v13 v21 v22 v23 v31 v32 v33 , = u13 u23 u33 (31) (32) (33) The output here is denoted as the Rδcam in Sec. 3.4.1. D. Implementation Details of HumanMM D.1. Training Details The ms-HMR, the trajectory, and foot sliding refiner are trained on the AMASS [78], 3DPW [79], Human3.6M [61], and BEDLAM [90] datasets, evaluate on EMDB and our ms-Motion. During training, we introduce random rotational noise (ranging from 0 to 1 radian) along the y-axis to the root orientation Γ and random noise to the body pose θ at random positions to simulate the inaccuracies of pre-estimated human motions caused by shot transitions in multi-shot videos. This strategy enables the network to robustly recover smooth and consistent human motion from noisy initial parameters. The introduction of these noise perturbations stems from the observation that relying solely on our orientation alignment module may fall short in fully aligning the root pose across different shots in challenging scenarios, particularly when significant angular discrepancies occur during shot transitions. To overcome this limitation, our trainable module is designed not only to align root poses with camera parameters but also to ensure smooth transitions in local poses across shot boundaries. This strategy significantly enhances the robustness of our method in managing abrupt orientation changes caused by multi-shot video transitions. D.2. ms-Motion Benchmark Details Our ms-Motion Benchmark is building on existing multi-view datasets AIST [60], H36M [61]. The AIST dataset provides world translation as ground truth. H36M dataset does not include such labels; therefore, we process human world translation from [29] as the ground truth. For benchmarking, the test results were obtained after training HumanMM for 80 epochs on single NVIDIA-A100 GPU (1.3 days). This computational setup ensures efficient convergence of the model while maintaining high level of accuracy in human motion recovery. D.3. Baseline Setting The baseline mentioned in Tab. 3 in Sec. 5.4 is implemented as follows. For the input multi-shot videos, we process them by using our proposed shot transition detector and human and camera parameters initialization as shown in Fig. 3 in Sec. 3. Next, we directly concatenate the human translation, root orientation, and body pose based on the relative offsets between frames. This method could achieve global motion recovery in few scenarios with simple motions. However, our observations reveal that this approach suffers from noticeable issues, such as foot sliding, motion truncation, and motion collapse. As there were no existing methods for global human motion recovery, this approach can serve as our baseline for conducting ablation studies to evaluate the effectiveness of our proposed training and optimization modules. 7 E. Visualization of Comparison between Existing Methods E.1. Visualization of Comparison between Existing Methods In this section, we present visual comparisons between our proposed method and existing approaches, including SLAHMR [26], GVHMR [29], as well as the ground truth. These comparisons aim to highlight the advancements achieved by our method in accurately reconstructing human motion. To ensure comprehensive evaluation, we provide visualizations from multiple viewpoints: side view (a), an alternative side view (b), top-down view (c), and reconstructed motion trajectories (d). The side view (a) allows for detailed examination of the overall pose accuracy and alignment over time, emphasizing the consistency and anatomical plausibility of the reconstructed movements. The alternative side view (b) provides additional insights into the depth and spatial relationships of the poses, capturing nuances that might be less apparent from single perspective. The top-down view (c) reveals the positional alignment and the spatial coherence of the trajectories, showcasing the robustness of our approach in reconstructing dynamic and complex motions. Finally, the motion trajectories (d) offer quantitative and qualitative representation of the movement paths, highlighting the differences between methods and their ability to accurately track and reproduce the ground truth trajectories. These visual comparisons underscore our method ability in delivering precise, consistent, and realistic human motion recovery from multi-shot videos. 9 E.2. Visualization of in-the-wild multi-shot video To further validate the performance of our proposed method, we apply HumanMM on set of in-the-wild videos and provide the visualizations as shown in the figure. The frames at the top illustrate key moments in the video sequence, with the corresponding reconstructions visualized in 3D space below. The 3D reconstructions effectively capture the dynamic human poses and trajectories over time. In subfigure (a), the side view highlights the accurate reconstruction of intricate human motion, illustrating the consistency and smoothness of the trajectories even for fast and complex movements. Subfigure (b) presents an alternative side view, offering another perspective that underscores the spatial coherence of the reconstructions. Subfigure (c) provides top-down view, offering an insightful perspective into the overall trajectory and positional accuracy of the reconstructed poses as they evolve over time. This view particularly emphasizes the spatial distribution and alignment of the poses in the reconstructed scene. These visualizations collectively demonstrate the robustness of HumanMM in handling challenging, dynamic motions in diverse environments, further showcasing its applicability to real-world scenarios."
        }
    ],
    "affiliations": [
        "HKU",
        "HKUST",
        "IDEA Research",
        "Johns Hopkins University",
        "Tsinghua University",
        "University of Chicago"
    ]
}