{
    "paper_title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
    "authors": [
        "Haokun Zhao",
        "Xiang Zhang",
        "Jiaqi Wei",
        "Yiwei Xu",
        "Yuting He",
        "Siqi Sun",
        "Chenyu You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 8 3 5 1 0 . 0 1 5 2 : r TimeSeriesScientist: General-Purpose AI Agent for Time Series Analysis Haokun Zhao1,2,, Xiang Zhang3,, Jiaqi Wei4,, Yiwei Xu5, Yuting He6, Siqi Sun7, Chenyu You 1Stony Brook University, 2University of California, San Diego, 3University of British Columbia, 4Zhejiang University, 5University of California, Los Angeles, 6Case Western Reserve University, 7Fudan University Equal contribution Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics (Zhang et al., 2024) and self-planning over the input; Forecaster performs model fitting and validation and based on the results to adaptively select the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces clear and rigorous report that makes the forecasting workflow more transparent and interpretable. Github: https://github.com/Y-Research-SBU/TimeSeriesScientist/ Website: https://y-research-sbu.github.io/TimeSeriesScientist/ Corresponding Author: chenyu.you@stonybrook.edu"
        },
        {
            "title": "Introduction",
            "content": "Time series forecasting guides decision making in domains as diverse as energy (Liu et al., 2023), finance (Zhu and Shasha, 2002), climate (Schneider and Dickinson, 1974), and public health (Matsubara et al., 2014). In practice, organizations manage tens of thousands of short, noisy time series data with heterogeneous sampling, missing values, and shifting horizons (Makridakis et al., 2020; Taylor and Letham, 2018; Makridakis et al., 2022). The dominant cost in forecasting is often not model fitting, but rather building reliable data processing and evaluation pipelines. This process is non-trivial for short and noisy series with irregular sampling and intermittent observations, and they remain largely manual in practice (Tawakuli et al., 2025; Shukla and Marlin, 2021; Moritz and Bartz-Beielstein, 2017). Despite the availability of strong libraries that streamline modeling itself (Alexandrov et al., 2019; Herzen et al., 2022; Wei et al., 2025a; Jiang et al., 2022), end-to-end pipelines still require substantial human effort to tailor preprocessing, validation, and ensembling to each new collection of series. Most advances in forecasting now arrive as expert models tuned to specific domains, or universal approaches that optimize only the model while leaving the rest of the pipeline untouched (Shchur et al., 2023; Gruver et al., 2024; Roque et al., 2024). Such systems can reach SOTA in-domain performance yet degrade under distribution shift because they rely on dataset or distribution-specific tuning rather than generalizable reasoning about 1 (a) (b) Figure 1 Performance comparison of TSci with five LLM-based baselines. TSci outperforms LLM-based baselines on eight benchmarks spanning five domains (Figure 1a). The comprehensive report generated by TSci outperforms LLM-based baselines across five rubrics (Figure 1b). the series (Zhang et al., 2023a). AutoML for forecasting (Shchur et al., 2023) centers on model selection and ensembling, but with limited attention to data quality. And it lacks the capacity to reason about temporal structure, adapt tools to heterogeneous series, and justify choices in natural language. Meanwhile, Time-LLM (Jin et al., 2023) achieves strong in-domain performance, yet it still primarily targets the model rather than the end-to-end pipeline (Gruver et al., 2024). These limitations motivate an agentic approach, one that treats time series forecasting as sequential decision process over data preparation, model selection, validation, and ensembling, with explicit planning, tool use, and transparent rationales. To this end, we introduce TimeSeriesScientist (TSci), the first end-to-end, agentic framework that leverages multimodal knowledge to automate the entire workflow human scientist would follow for univariate time series forecasting. Rather than committing to single universal model, TSci orchestrates four specialized agents throughout the process. First, Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics. It generates compact set of visualizations leveraging LLM multimodal ability and outputs an analysis summary of temporal structure that guides subsequent steps. Next, Planner selects candidate models from pre-defined model library based on the multimodal diagnostics and optimizes hyperparameters through validation-driven search. Then, Forecaster reasons over validation results and adaptively selects an ensemble strategy to produce the final prediction along with natural-language rationales. Finally, Reporter consolidates all intermediate statistical analyses and forecasting results and outputs comprehensive report. This design transforms forecasting into an adaptive, interpretable, and extensible pipeline, bridging the gap between human expertise and automated decision-making. Across eight public benchmarks spanning five domains, TSci consistently outperforms both statistical and LLM-driven baselines, reducing forecasting error by 10.4% and 38.3% on average, respectively. Ablations show that each module contributes materially to the performance. Our evaluation of the report generator further demonstrates its technical rigor and clear communication, supporting practical deployment in settings that demand transparency and auditability. Our main contributions are as follows: 1) We introduce TimeSeriesScientist, the first end-to-end, agentic framework for univariate time series forecasting with tool-augmented LLM reasoning; 2) We propose plot-informed multimodal diagnostics, where lightweight vision encoder converts plots into descriptors guiding preprocessing, analysis, and model selection; 3) We show that TSci outperforms both statistical and LLM-diven baselines across diverse benchmarks; and 4) We provide comprehensive evaluation of its generated reports, demonstrating both technical rigor and communication quality. 2 Figure 2 Overview of our proposed TSci framework. This collaborative multi-agent system is designed to analyze and forecast general time series data, just like human scientist. Upon receiving input time series data, the framework executes structured four-agent workflow. Curator generates analytical reports (Section 3.2), Planner selects model configurations through reasoning and validation (Section 3.3), Forecaster integrates model results to produce the final forecast (Section 3.4), Reporter generates comprehensive report as the final output of our framework (Section 3.5)."
        },
        {
            "title": "2 Related work",
            "content": "Time Series Forecasting. Univariate time series forecasting has evolved from classical statistical methods (e.g., ARIMA, ETS, and TBATS), which exploit linear trends and seasonalities (Box et al., 2015; Hyndman and Khandakar, 2008; De Livera et al., 2011), to global deep learning models (e.g., DeepAR, N-BEATS, and PatchTST) that capture nonlinear patterns and long-term dependencies (Salinas et al., 2020; Oreshkin et al., 2019; Nie et al., 2023). More recently, foundation-style approaches (e.g., Chronos, TimesFM, Lag-Llama) and prompt-based adaptations (Zhang et al., 2025a) of LLMs (e.g., GPT4TS, Time-LLM) have demonstrated zero-shot and few-shot forecasting capabilities (Ansari et al., 2024; Das et al., 2024; Zhang et al., 2023b; Rasul et al., 2023; Zhou et al., 2023; Jin et al., 2023), treating time series as sequences to be modeled in analogy with language (Liu et al., 2022). While these advances highlight trend toward general-purpose and transferable forecasters, existing work remains largely model-centric: the broader pipeline of preprocessing, evaluation design, and ensemble synthesis continues to rely heavily on manual effort. This gap motivates our pursuit of an end-to-end, LLM-powered agentic framework that integrates reasoning, tool use, and automation across the entire forecasting workflow. Multi-agent System. Large language models have enabled the rise of multi-agent systems (Cao et al., 2025; Wei et al., 2025b; Xiong et al., 2025), where specialized agents collaborate via communication and tool use (Zhang et al., 2025b) to tackle complex analytical tasks. Frameworks such as CAMEL (Li et al., 2023), AutoGen (Wu et al., 2023a), and DSPy (Khattab et al., 2024) demonstrate how plannerexecutor architectures can coordinate agents for reasoning, retrieval, and problem solving (Khattab et al., 2024). Recent applications show their utility for domains like business intelligence and financial forecasting (Wawer and Chudziak, 2025). Despite this progress, existing systems rarely address the unique challenges of time series: heterogeneous sampling and multimodal data that are often irregular or asynchronous (Chang et al., 2025), and the need for transparent ensemble reporting of forecasts (Zhao and jiekai ma, 2025). This leaves open the opportunity for multi-agent, domain-agnostic framework that leverages LLM reasoning to automate forecasting pipelines while ensuring interpretability and auditability. 3 Figure 3 Workflow of Curator. The raw dataset is first diagnosed and processed into cleaned dataset D. Next, the agent generates tailored visualizations to expose temporal structures and facilitate interpretability. Finally, the agent integrates the processed data and visualizations to extract trends, seasonality, and stationarity, producing comprehensive analysis summary S."
        },
        {
            "title": "3 TimeSeriesScientist",
            "content": "TSci acts as human scientist, having the ability to systematically perform data analysis, model selection, forecasting, and report generation by utilizing LLM reasoning abilities. TSci integrates four specialized agents, each assigned distinct role, and collaboratively engages in the whole process: (1) Curator : Performs LLM-guided diagnoses augmented by external tools that reason over data statistics and output multimodel summary guiding subsequent steps; (2) Planner : Narrows the model configuration space by leveraging multimodal diagnostics and validation-driven search; (3) Forecaster : Reasons over validation results to adaptively select model ensemble strategy and produces the final forecast; and (4) Reporter : Generates comprehensive report consolidating all intermediate statistical analyses and forecasting results."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We first formally formulate the univariate time series forecasting problem. Let = {xtT +1, ..., xt1, xt} R1T be given univariate time series with values in the historical data, where each xti, for = 0, ..., 1, represents recorded value of the variable at time i. The forecasting process consists of estimating the value of yt+i R1H , denoted as ˆyt+i, = 1, ..., H, where is the horizon of prediction. The overall objective is to minimize the mean average errors (MAE) between the ground truths and predictions, i.e., 1 i=1 yt+i ˆyt+i. (cid:80)H In our proposed framework, given univariate time series data D, the system generates comprehensive report containing: statistics of the input data, visualizations, proposed model combinations that best fit the data, and the final forecasting result. This framework significantly reduces manual effort and time cost, while providing human scientists with detailed and reliable analytical output."
        },
        {
            "title": "3.2 Curator",
            "content": "Data preprocessing is critical in time series forecasting, as it ensures data quality, improves model accuracy, and directly impacts the reliability of analytical results (Chakraborty and Joseph, 2017; Esmael et al., 2012; Zhang et al., 2023c; Shih et al., 2023). Curator leverages LLM reasoning ability, augmented with specialized tools to transform the raw series into clean and informative form that downstream agents can depend on. It operates in three coordinated steps. Details are in Figure 3. 4 Quality Diagnostics & Preprocessing. High-quality input is critical for reliable forecasting. Rather than computing fixed summaries, Curator leverages LLM-driven reasoning to both diagnose issues and execute appropriate preprocessing. Specifically, given univariate series = {xt}T t=1, the agent first outputs vector containing data statistics S, missing value information , outlier information O, and data-process strategy π. This process can be formalized as: = Af (D) = (cid:16) S, M, O, π (cid:17) , (1) where Af denotes the quality diagnostics operator, = (µ, σ, xmin, xmax, τtrend) denotes basic data statistics containing mean, standard deviation, min/max value, and trend, π = (m, h) denotes LLM-recommended missing value and outlier handling strategies. Based on processing strategy π, the agent applies transformation ϕ : RT RT to the raw input series D, and get processed series = ϕ(D) = {xt}T t=1, where xt denotes the processed value at time step t. By coupling quality diagnostics with preprocessing, the agent tailors data-aware strategies, yielding well-conditioned preprocessed dataset that supports subsequent steps. Details about strategies and transformations can be found in Appendix A. Visualization Generation. Visualizations greatly aid human scientists in comprehending complex time series data and identifying critical temporal patterns. Inspired by this practice, the agent automates the creation of insightful visualizations leveraging natural language prompts and reasoning from an LLM. This step can be formalized as generating visualization suite given processed dataset: = Av( D), where Av denotes the visualization generator. Specifically, it generates three primary visualization types tailored to input data characteristics: (1) Time series overview plot: Visualize data statistics, illustrate moving averages and standard deviations. (2) Time series decomposition analysis plot: Reveals temporal patterns, long-term trends, and seasonal cycles. (3) Autocorrelation analysis plot: Identify temporal dependencies, detect non-stationarity, and guide the later selection of appropriate model parameters. Details about the plots are provided in Appendix E. Temporal Structure Profiling. To effectively support downstream forecasting, an overall analysis is important in uncovering temporal structures and statistical properties that are essential for informed model selection and interpretation. This step conducts analysis through prompting to extract meaningful patterns and features from preprocessed time series data. The objective is to detect trends, seasonality, and stationarity, thereby guiding the selection of suitable forecasting models. Formally, given the processed dataset and visualizations , the agent generates an analysis report through LLM reasoning: = Ac( D, ) = {t, s, u}, where Ac denotes the profiling step, t, s, denote trend, seasonality, and stationary, respectively. The outcome of Curator is comprehensive analysis summary = {Q, V, A}, where Q, V, are the outputs from each step, respectively."
        },
        {
            "title": "3.3 Planner",
            "content": "Planner narrows the hypothesis space of model configurations by reasoning on the analysis summary C. Rather than exhaustively trying all candidates, it prioritizes models that are most consistent with data characteristics. Concretely, Planner operates in three coordinated steps. Model Selection. Planner extracts visual features from visualizations via lightwise pattern recognition and LLM reasoning. It then maps the recognized data pattern to suitable model families and forms candidate pool Mp, which has np candidate models from pre-defined model library M: Mp = Select(M; np), where Mp = np. Concretely, the agent may choose to use Prophet when recognizing weak trend with long seasonal span. Details about the model library can be found in Appendix C. Moreover, for each mi Mp, the agent generates rationale ri explaining how data patterns in analysis report motivate the choice of mi. Hyperparameter Optimization. For each model mi Mp, let Θi denote its hyperparameter space. We sample up to configurations Ci = {θ(j) j=1 Θi and evaluate each on the validation set Dval. The optimal configuration θ is selected by minimizing validation MAPE (Mean Absolute Percentage Error): }N θ = arg min θiCi MAPEval (cid:0)mi(θi)(cid:1), (2) 5 Algorithm 1: Hyperparameter Optimization for Candidate Models Input: Validation set Dval = {xt}Tval t=1 , Candidate model pool Mp Output: Validation metrics Sval, Optimal hyperparameter set Θ 1: for mi Mp do 2: Θi ProposeHyperparams(mi) # define hyperparameter space Sample Ci (Θi, ) # sample configs from the hyperparameter space 3: (cid:1) # select best hyperparameters θ arg minθiCi MAPEval 4: 5: mi(θ 6: 7: Θ[mi] θ 8: end for 9: 10: return Sval, Θ (cid:0)mi(θi), Dval ) # instantiate tuned model , Dval) # record validation metrics # record chosen hyperparameters Sval[mi] Evaluate(m where MAPEval(mi(θi)) = 100% Dval (cid:88) Dval (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) xt ˆx(i,θi) xt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . (3) Here ˆx(i,θi) denotes the prediction at time step produced by model mi instantiated with hyperparameters θi, and xt is the corresponding ground-truth value. Analogously, we also compute MAEval for comprehensive performance profile, which allows for robustness checks against different error metrics. The detailed hyperparameter optimization procedure is summarized in Algorithm 1. Model Ranking. After hyperparameter optimization, each candidate model mi is instantiated with its optimal configuration θ and associated validation metrics. To select high-quality subset for ensemble construction, the np tuned models are ranked by their validation performance. We primarily adopt validation MAPE for ranking. Specifically, the top models with the lowest validation MAPE scores are retained: Mselected = (cid:8)m(1)(θ (1)), . . . , m(k)(θ (k))(cid:9), MAPEval (cid:0)m(1) (cid:1) MAPEval (cid:0)m(k) (cid:1). Here m(j)(θ (j)) denotes the j-th ranked model, ordered by ascending validation MAPE. The output of this stage is the selected models set Mselected together with tuned hyperparameters Θ and validation metrics Sval, which serve as the foundation for ensemble construction."
        },
        {
            "title": "3.4 Forecaster",
            "content": "Ensemble forecasting combines complementary biases to surpass single models, cutting error under concept drift (Zhang et al., 2023d), yielding broad gains across heterogeneous patterns (Liu et al., 2025), excelling on benchmarks (Oreshkin et al., 2020), and maintaining robustness across epidemic phases (Adiga et al., 2023). Forecaster takes the top-k selected models Mselected and their validation metrics Sval as input. The agent leverages an LLM-guided policy to select an ensemble strategy from among three families: singlebest selection, performance-aware averaging, or robust aggregation. The ensemble strategy and (if applicable) weights are fixed before touching the test set to avoid data leakage. With the ensemble strategy determined, Forecaster tests the ensemble model on the held-out test horizon of length to output the final forecast, and reports test metrics Stest for comparative evaluation. This procedure balances performance and stability while attenuating outliers and regime-specific brittleness. Implementation details and ensembling rules can be found in Appendix B."
        },
        {
            "title": "3.5 Reporter",
            "content": "A clear, well-structured output is essential for human scientists. Reporter outputs comprehensive report that consolidates all intermediate statistical analyses and forecasting results. Specifically, includes: (1) an ensemble forecast ˆxens t+1:t+H completed with confidence intervals; (2) performance summary presenting test metrics for each model alongside the ensemble; (3) an interpretability report in which an LLM generates natural-language explanations of (i) the rationale for selecting specific models, (ii) the derivation of ensemble 6 Figure 4 Demonstration of the output comprehensive report R. The report consists of five parts, consolidating results, diagnostics, interpretations, and decision provenance into transparent output. weights, (iii) the systems confidence in its forecast, and (iv) any underlying assumptions or limitations; (4) visualization suite containing detailed plots for exploratory analysis and presentation; and (5) full workflow documentation that records every decision made at each phase of the pipeline. demonstration of the generated report is in Figure 4. The system achieves interpretability through LLM reasoning at each decision point, providing natural language explanations for model selection, hyperparameter choices, and ensemble construction strategies. This transparency enables users to understand and trust the forecasting process while benefiting from the automated optimization capabilities of the multi-agent architecture."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we present the experiment results of TSci in comparison with both statistical and LLM-based baselines and provide comprehensive analysis. Our framework achieves superior performance over statistical models and state-of-the-art large language models across diverse benchmarks and settings. To ensure fairness, we strictly follow the same evaluation protocols for all baselines. Unless otherwise specified, we adopt GPT-4o (Wu et al., 2023b) as the default backbone."
        },
        {
            "title": "4.1 Performance Analysis",
            "content": "Results. Our brief results in Table 1 demonstrate that TSci consistently outperforms LLM-based baselines across eight benchmarks and significantly so for the majority of them. Compared with the second-best baseline, TSci reduces MAE by an average of 38.2%. Figure 5 visualizes the performance comparison across datasets and horizons. The results highlight the robustness and generalization capability of TSci across heterogeneous domains, confirming its advantage as unified solution for time series forecasting. Figure 1a visualizes the performance comparison using min-max inversion (maps the lowest-MAE method to 100, the highest-MAE maps to 20, and others scale proportionally). Figure 6 reports MAE on four ETT-small datasets across multiple horizons. TSci dominates statistical methods on most datasets and horizons, particularly as the forecast length increases. At short horizons, locally autoregressive structure can make simple linear models (e.g., linear regression) competitive, which match or slightly exceed TSci. But their advantage diminishes as horizon increases or patterns deviate from near-linear dynamics. The aggregate trend favors TSci, reflecting its capacity to adapt to diverse regimes while preserving short-term fidelity. Full results by datasets and horizons are provided in Appendix F. 7 Table 1 Time Series forecasting results compared with five LLM-based baselines. lower value indicates better performance. Red : the best, Blue : the second best. Method GPT-4o Gemini-2.5 Flash Qwen-Plus DeepSeek-v3 Claude-3.7 TSci (Ours) Metric MAE MAPE(%) MAE MAPE(%) MAE MAPE(%) MAE MAPE(%) MAE MAPE(%) MAE MAPE(%) ETTh1 ETTh2 ETTm1 ETTm2 Weather ECL 2.01e1 1.82e1 5.75 9.94 6.13e1 6.33e Exchange 1.60e-1 ILI 2.17e5 183.8 264. 85.7 50.7 10.9 260.2 26.2 26. 5.20 1.10e1 7.31 1.60e1 6.52e1 8.86e 1.28e-1 2.46e5 61.1 81.0 59.9 74. 11.8 45.4 19.9 29.3 1.15e1 3.27e 5.09 1.07e1 4.29e1 1.66e3 8.5e-2 3.37e 113.8 175.6 48.4 71.7 6.4 62. 13.6 37.0 1.22e1 2.01e1 8.17 9. 5.20e1 68.3e3 1.75e-1 2.24e5 134.9 121. 117.2 39.7 8.3 235.7 26.7 26. 9.16 1.16e1 6.22 6.94 4.56e1 8.44e 7.3e-2 1.79e5 111.0 118.6 65.9 41. 6.9 32.2 11.8 19.7 2.02 4. 2.73 4.87 2.91e1 6.67e2 4.50e-2 1.41e 23.3 24.7 29.8 31.6 4.4 40. 6.8 16.2 1st Count 0 0 0 1 8 Figure 5 Performance comparison of TSci with five LLM-based baselines across eight datasets."
        },
        {
            "title": "4.2 Generated Report Evaluation",
            "content": "The final comprehensive report serves as crucial interface to access and interpret the outcomes of the framework. We evaluate the quality of the generated reports from comprehensive perspective. Evaluation Metrics. Here, we describe the details of the five rubrics that comprehensively evaluate the generated report: Analysis Soundness (AS): Evaluates the rigor and correctness of exploratory data analysis, including the handling of missing values, anomaly detection, and identification of seasonality or trends. Model Justification (MJ): Assesses whether the chosen forecasting models are appropriate for the data characteristics and whether the selection is supported by clear, evidence-based justification. Interpretive Coherence (IC): Measures the logical consistency and alignment of the reports reasoning, ensuring interpretations of diagnostics, errors, and results form coherent narrative. Actionability Quotient (AQ): Judges the extent to which the report provides concrete, evidence-backed, and practically useful recommendations for decision making or system improvement. Structural Clarity (SC): Examines the organization, readability, and professionalism of the report, including section structure, flow, and correct referencing of figures and tables. The five rubrics comprehensively evaluate the generated report along two dimensions: AS and MJ assess the technical rigor of analysis and modeling choices, while IC, AQ, and SC assess the communication quality and 8 Figure 6 Performance comparison of TSci with statistical baselines on ETT-small benchmarks. practical usefulness of the report. For each rubric, we compute the win rate, defined as the proportion of pairwise comparisons in which our frameworks report is judged superior to baseline, excluding ties. Results. As shown in Table 2, TSci consistently outperforms all baselines across the five rubrics. The largest gains appear in AS and MJ, where win rates exceed 80% for all comparisons, underscoring the rigor and appropriateness of our analyses and model choices. Strong performance is also observed in IC and AQ (mostly above 75%), indicating coherent reasoning and actionable recommendations. While the advantages of SC are smaller, our framework still delivers consistently structured and professional reports. Taken together, these results validate that TSci not only surpasses baselines in predictive quality, but also generates reports that are technically rigorous, interpretable, and practically useful. Figure 1b visualizes the win rate comparison (highest win rate maps to 100, the lowest to 20, and others scale linearly). Table 2 Win rate (%) of TSci against LLM-based baselines across five rubrics. Baseline AS MJ IC AQ SC TSci vs GPT-4o TSci vs Gemini-2.5 Flash TSci vs Qwen-Plus TSci vs DeepSeek-v3 TSci vs Claude-3.7 80.8 81.8 83.3 92.3 84. 84.6 81.8 83.3 84.6 87.5 80.8 63.6 79.2 80.8 84.6 76.9 68.2 75.0 76.9 80.8 71.4 53.8 75.0 76.9 53."
        },
        {
            "title": "4.3 Model Analysis",
            "content": "Our results in Figure 7 indicate that ablating any of the data pre-processing, data analysis, or model optimization module degrades time-series forecasting performance. Effect of data preprocessing module. Removing the data preprocessing module in Curator leads to an average of 41.80% increase in MAE, which is the largest increase among the three modules. More specifically, the performance degeneration increases with increasing prediction horizons within one dataset. These findings demonstrate that data pre-processing contributes the most to the robustness of TSci, and underscore that cleaning, resampling, and outlier handling are crucial for analysis and especially long-horizon forecasts. Effect of data analysis module. The analysis module in Curator profiles each series and serves for downstream strategies. Removing the module harms MAE of 28.3% on average. Two minute-level cases show small improvements (ETTm1-96 and ETTm-720), suggesting minute-level data at very short/long horizons may benefit from further tuning of preprocessing and search. Overall, analysis guidance stabilizes model choice and horizon-specific settings. Effect of model optimization module. The model optimization module performs parameter search for selected forecast models. Removing this module leaves reasonable but suboptimal configuration, producing 36.2% MAE drop on average and marked decline on long horizons or high-variance series where horizon chunking and window sizing matter. 9 Figure 7 Ablation study of TSci with three variants: w/o Data Pre-process, w/o Data Analysis, and w/o Parameter Optimization. TSci attains the lowest MAE on six out of eight settings."
        },
        {
            "title": "4.4 Case Study",
            "content": "We present case study on the ECL dataset with horizon = 96, case where our framework surpasses other baselines by large margin. We analyze the analysis summary generated by Curator and the final report to highlight the effectiveness and interpretability of our agentic design. The data analysis summary, visualization, and final comprehensive report are provided in Appendix G. The whole dataset is first divided into 25 slices, and we take one slice for study. The analysis summary in Appendix G.1 shows that the series exhibits strong cyclical fluctuations with noticeable peaks and troughs, but no persistent long-term trend. Statistical summaries indicate symmetric distribution with light tails, as evidenced by near-zero skewness and negative kurtosis. Seasonal decomposition further confirms strong seasonal component, while stationarity tests suggest that the data is non-stationary. Based on the analysis, Planner selected three models capable of handling non-stationary and seasonal signals, including ARIMA, Prophet, and Exponential Smoothing from the model library. The Visualization highlighted the cyclical nature of the data and irregular spikes, reinforcing the importance of models that adapt to seasonality. Following this, Forecaster produced ensemble forecasts and assigned higher weights to models capturing seasonal dynamics. Figure 14 shows the ensemble forecast with individual model predictions. While individual models such as ARIMA and Prophet struggled with accumulated errors over the horizon = 96, our ensemble remained stable and aligned with the seasonal cycles. The ensemble strategy given by the LLM mitigates errors from the individual model and produces more stable forecast. The final comprehensive report further provided human-readable explanations, linking the model choices directly to the identified seasonality and non-stationarity in the data. This case study demonstrates that our framework is not only more accurate than baselines but also produces interpretable outputs. The generated reports bridge the gap between automated forecasting and human reasoning by explaining why certain models are preferred, how data characteristics influence forecasts, and where potential risks (e.g., non-stationarity, irregular spikes) lie."
        },
        {
            "title": "5 Conclusions and Future Work",
            "content": "We introduced TimeSeriesScientist, the first end-to-end, agentic framework that automates univariate time series forecasting via LLM reasoning. Extensive experiments across diverse benchmarks show consistent gains over state-of-the-art LLM baselines, demonstrating both prediction accuracy and report interpretability. This work provides the first step toward unified, domain-agnostic approach for univariate time series forecasting, bridging the gap between traditional forecasting methods and the emerging capabilities of foundation models. Future directions include extending to multimodal settings for broader applicability and incorporating external knowledge and efficiency-oriented designs to enhance interpretability and scalability. We hope this work inspires further research at the intersection of time series forecasting, agentic reasoning, and foundation models."
        },
        {
            "title": "References",
            "content": "Xiang Zhang, Senyu Li, Ning Shi, Bradley Hauer, Zijun Wu, Grzegorz Kondrak, Muhammad Abdul-Mageed, and Laks VS Lakshmanan. Cross-modal consistency in multimodal large language models. arXiv preprint arXiv:2411.09273, 2024. Hengbo Liu, Ziqing Ma, Linxiao Yang, Tian Zhou, Rui Xia, Yi Wang, Qingsong Wen, and Liang Sun. Sadi: self-adaptive decomposed interpretable framework for electric load forecasting under extreme events. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. Yunyue Zhu and Dennis Shasha. Statstream: Statistical monitoring of thousands of data streams in real time. In VLDB02: Proceedings of the 28th International Conference on Very Large Databases, pages 358369. Elsevier, 2002. Stephen Schneider and Robert Dickinson. Climate modeling. Reviews of Geophysics, 12(3):447493, 1974. Yasuko Matsubara, Yasushi Sakurai, Willem Van Panhuis, and Christos Faloutsos. Funnel: automatic mining of spatially coevolving epidemics. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 105114, 2014. Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: 100,000 time series and forecasting methods. International Journal of Forecasting, 36(1):5474, 2020. Sean Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):3745, 2018. Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. M5 accuracy competition: Results, findings, and conclusions. International journal of forecasting, 38(4):13461364, 2022. Amal Tawakuli, Bastian Havers, Vincenzo Gulisano, Daniel Kaiser, and Thomas Engel. Survey:time-series data preprocessing: survey and an empirical analysis. Journal of Engineering Research, 13(2):674711, 2025. ISSN 2307-1877. doi: https://doi.org/10.1016/j.jer.2024.02.018. URL https://www.sciencedirect.com/science/article/pii/ S2307187724000452. Satya Narayan Shukla and Benjamin M. Marlin. survey on principles, models and methods for learning from irregularly sampled time series, 2021. URL https://arxiv.org/abs/2012.00168. Steffen Moritz and Thomas Bartz-Beielstein. imputets: Time series missing value imputation in r. Journal, 9(1): 207218, 2017. Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner Turkmen, and Yuyang Wang. Gluonts: Probabilistic time series models in python, 2019. URL https://arxiv.org/abs/ 1906.05264. Julien Herzen, Francesco Lassig, Samuele Giuliano Piazzetta, Thomas Neuer, Leo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, Maxime Dumonal, Jan Koscisz, Dennis Bader, Frederick Gusset, Mounir Benheddi, Camila Williamson, Michal Kosinski, Matej Petrik, and Gael Grosch. Darts: User-friendly modern machine learning for time series, 2022. URL https://arxiv.org/abs/2110.03224. Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi Sun. Alignrag: Leveraging critique learning for evidence-sensitive retrieval-augmented reasoning. arXiv preprint arXiv:2504.14858, 2025a. Xiaodong Jiang, Sudeep Srivastava, Sourav Chatterjee, Yang Yu, Jeffrey Handler, Peiyi Zhang, Rohan Bopardikar, Dawei Li, Yanjun Lin, Uttam Thakore, Michael Brundage, Ginger Holt, Caner Komurlu, Rakshita Nagalla, Zhichao Wang, Hechao Sun, Peng Gao, Wei Cheung, Jun Gao, Qi Wang, Marius Guerard, Morteza Kazemi, Yulin Chen, Chong Zhou, Sean Lee, Nikolay Laptev, Tihamer Levendovszky, Jake Taylor, Huijun Qian, Jian Zhang, Aida Shoydokova, Trisha Singh, Chengjun Zhu, Zeynep Baz, Christoph Bergmeir, Di Yu, Ahmet Koylan, Kun Jiang, Ploy Temiyasathit, and Emre Yurtbay. Kats, 3 2022. URL https://github.com/facebookresearch/Kats. Oleksandr Shchur, Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, and Yuyang Wang. Autogluon-timeseries: Automl for probabilistic time series forecasting, 2023. URL https://arxiv.org/abs/2308.05566. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters, 2024. URL https://arxiv.org/abs/2310.07820. Luis Roque, Carlos Soares, Vitor Cerqueira, and Luis Torgo. Cherry-picking in time series forecasting: How to select datasets to make your model shine, 2024. URL https://arxiv.org/abs/2412.14435. 11 Yi-Fan Zhang, Qingsong Wen, Xue Wang, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Onenet: Enhancing time series forecasting models under concept drift by online ensembling, 2023a. URL https://arxiv.org/abs/2309.12659. Mingxuan Jin, Haixu Zhang, Wenjie Wang, Yasha Wang, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023. George E. P. Box, Gwilym M. Jenkins, Gregory C. Reinsel, and Greta M. Ljung. Time Series Analysis: Forecasting and Control. John Wiley & Sons, 5th edition, 2015. Rob J. Hyndman and Yeasmin Khandakar. Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(3):122, 2008. Alysha M. De Livera, Rob J. Hyndman, and Ralph D. Snyder. Forecasting time series with complex seasonal patterns using exponential smoothing. Journal of the American Statistical Association, 106(496):15131527, 2011. David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. In International Journal of Forecasting, volume 36, pages 11811191. Elsevier, 2020. Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansion analysis for time series forecasting. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS), pages 112, 2019. Yuqi Nie, Guolin Zhang, Jiyan Wang, and Vincent Y. F. Tan. time series is worth 64 words: Long-term forecasting with transformers. In Proceedings of the 11th International Conference on Learning Representations (ICLR), 2023. Xiang Zhang, Juntai Cao, Jiaqi Wei, Chenyu You, and Dujian Ding. Why prompt design matters and works: complexity analysis of prompt search space in llms. arXiv preprint arXiv:2503.10084, 2025a. Abdul Fatir Ansari, Anastasia Borovykh, Marin Biloˇs, and et al. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024. Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting, 2024. URL https://arxiv.org/abs/2310.10688. Xiang Zhang, Ning Shi, Bradley Hauer, and Grzegorz Kondrak. Bridging the gap between babelnet and hownet: Unsupervised sense alignment and sememe prediction. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 27892798, 2023b. Kashif Rasul, Dhruv Dalal, Malte Muller, et al. Lag-llama: Towards foundation models for probabilistic time series forecasting. arXiv preprint arXiv:2310.08278, 2023. Tian Zhou, Weijia Ma, Yuxuan He, Ziqing Liu, et al. Gpt4ts: Large language models are zero-shot time series forecasters. arXiv preprint arXiv:2310.02029, 2023. Puyuan Liu, Xiang Zhang, and Lili Mou. character-level length-control algorithm for non-autoregressive sentence summarization. Advances in Neural Information Processing Systems, 35:2910129112, 2022. Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Chenyu You, Shafiq Joty, and Giuseppe Carenini. Multi2: Multi-agent test-time scalable framework for multi-document processing. arXiv preprint arXiv:2502.20592, 2025. Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, et al. From ai for science to agentic science: survey on autonomous scientific discovery. arXiv preprint arXiv:2508.14111, 2025b. Fei Xiong, Xiang Zhang, Aosong Feng, Siqi Sun, and Chenyu You. Quantagent: Price-driven multi-agent llms for high-frequency trading. arXiv preprint arXiv:2509.09995, 2025. Zhilin Zhang, Xiang Zhang, Jiaqi Wei, Yiwei Xu, and Chenyu You. Postergen: Aesthetic-aware paper-to-poster generation via multi-agent llms. arXiv preprint arXiv:2508.17188, 2025b. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society, 2023. URL https://arxiv.org/abs/2303.17760. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023a. URL https://arxiv.org/abs/2308.08155. 12 Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into self-improving pipelines. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Micha(cid:32)l Wawer and Jaros(cid:32)law Chudziak. Integrating traditional technical analysis with ai: multi-agent llm-based approach to stock market forecasting. In Proceedings of the 17th International Conference on Agents and Artificial Intelligence, page 100111. SCITEPRESS - Science and Technology Publications, 2025. doi: 10.5220/0013191200003890. URL http://dx.doi.org/10.5220/0013191200003890. Ching Chang, Jeehyun Hwang, Yidan Shi, Haixin Wang, Wen-Chih Peng, Tien-Fu Chen, and Wei Wang. Time-imm: dataset and benchmark for irregular multimodal multivariate time series, 2025. URL https://arxiv.org/abs/2506. 10412. Yikai Zhao and jiekai ma. Faithful and interpretable explanations for complex ensemble time series forecasts using surrogate models and forecastability analysis. In KDD 2025 Workshop on AI for Supply Chain: Today and Future, 2025. URL https://openreview.net/forum?id=hrONr7A1yC. Suman Chakraborty and Antony Paul Joseph. Preprocessing of time series data for prediction with neural networks: Case study with stock market data. In 2017 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC), pages 14. IEEE, 2017. Beshah Ayalew Esmael, Abera Teshome, Ayalew Teklu, Belete Tesfaye, and Luiz F. Scavarda. study on preprocessing techniques, feature selection and classification approaches for road traffic prediction. Procedia-Social and Behavioral Sciences, 54:11151124, 2012. Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. Dont trust chatgpt when your question is not in english: study of multilingual abilities and types of llms. arXiv preprint arXiv:2305.16339, 2023c. Po-Chun Shih, Yung-Chun Chen, and Yao-Hsin Tseng. Time series preprocessing and feature engineering for forecasting tasks. In Proceedings of the 2023 International Conference on Data Science, pages 2231. ACM, 2023. YiFan Zhang, Qingsong Wen, Xue Wang, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Onenet: Enhancing time series forecasting models under concept drift by online ensembling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023d. URL https://openreview.net/forum?id=Q25wMXsaeZ. Zhining Liu, Ze Yang, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Yada Zhu, Hendrik Hamann, Jingrui He, and Hanghang Tong. Breaking silos: Adaptive model fusion unlocks better time series forecasting, 2025. URL https://arxiv.org/ abs/2505.18442. Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting, 2020. URL https://arxiv.org/abs/1905.10437. Aniruddha Adiga, Gursharn Kaur, Lijing Wang, Benjamin Hurt, Przemyslaw Porebski, Srinivasan Venkatramanan, Bryan Lewis, and Madhav Marathe. Phase-informed bayesian ensemble models improve performance of covid-19 forecasts. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1564715653, 2023. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis, 2023b. URL https://arxiv.org/abs/2210.02186. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, (cid:32)Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, (cid:32)Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Gemini Team, Google. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. Technical report, Google, June 2025. URL https://storage.googleapis.com/ deepmind-media/gemini/gemini v2 5 report.pdf. Includes discussion of the Gemini 2.5 Flash model. Alibaba Cloud. Qwen-plus: Enhanced large language model with balanced performance, speed, and cost. Alibaba Cloud Model Studio Documentation, 2025a. URL https://www.alibabacloud.com/help/en/model-studio/ use-qwen-by-calling-api. Model belonging to Qwen3 series, supports large context window (131K tokens). Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Google Cloud. Claude 3.7 sonnet extended thinking hybrid reasoning model. Vertex AI Documentation, 2025b. URL https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude. Describes Claude 3.7 Sonnet capabilities, including extended thinking and agentic coding."
        },
        {
            "title": "Table of Contents",
            "content": "Data Processing Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Outlier Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Outlier Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16 Missing-Value Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Ensemble Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Model Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Dataset Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 LLM Guided Data Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Technical Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Output and Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Full Experiment Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case Study on ECL Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Analysis Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .24 Comprehensive Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Data Processing Strategies",
            "content": "We formalize leakage-safe toolkit for detecting and repairing data issues in time series {xt}T t=1. All statistics are estimated on rolling (local) windows to accommodate non-stationarity. Let and denote the sets of outlier and missing indices, respectively. The agent reasons on data statistics and A.1 Outlier Detection Rolling IQR. On window Wt of length w, compute its first and third quantile: Q1(Wt), Q3(Wt), IQRt = Q3 Q1. The outlier criterion: xt is outlier if xt < Q1 α IQRt or xt > Q3 + α IQRt, with common choice α=1.5. If strong seasonality exists, set to one or two seasonal cycles. Rolling Z-Score. Estimate µt, σt within window Wt and define zt = xt µt σt , xt is outlier if zt > α, (4) (5) (6) typically α [3, 4] for online detection. For skewed/heavy-tailed data, replace µt and σt by the median and MAD: µt median(Wt), σt 1.4826 MAD(Wt), (7) then apply the same threshold on zt. 15 Percentile Rule. Using empirical quantiles within Wt (adaptive) or from the training segment (frozen), xt is outlier if xt < Plower or xt > Pupper, (8) e.g., (Plower, Pupper) = (1%, 99%) or (0.5%, 99.5%). A.2 Outlier Handling Clipping / Winsorization. Let and be lower/upper bounds from non-outliers (or from quantiles such as P1%, P99%): xclean = L, U, xt, xt < L, xt > U, otherwise. (9) Interpolation (Segment-Aware). For contiguous outlier segment [a, b] with nearest clean neighbors τ0 < and τ1 > b, xclean = xτ0 + τ0 τ1 τ0 (cid:0)xτ1 xτ0 (cid:1), = a, . . . , b. (10) For isolated points, this reduces to the two-point linear case (xclean = xt1+xt+1 2 ). Forward/Backward Fill. Short gaps in level-like processes: xclean = xt1 (FFill), xclean = xt+1 (BFill). Local Mean/Median Replacement. Within causal neighborhood Nt (e.g., last points), xclean = 1 Nt (cid:88) iNt xi or xclean = median{xi : Nt}. Median is preferred under heavy tails or residual outliers. (11) (12) Light Causal Smoothing. After replacement, apply causal moving average to suppress residual spikes: xclean = 1 w1 (cid:88) i=0 xti. (13) Use small to limit lag and peak attenuation. A.3 Missing-Value Handling Linear Interpolation (Segment-Aware). For missing segment [a, b] bounded by clean points τ0 < and τ1 > b, xt = xτ0 + τ0 τ1 τ0 (cid:0)xτ1 xτ (cid:1), = a, . . . , b. Forward/Backward Fill. xt = xt1 (FFill), xt = xt+1 (BFill). Local Mean/Median Fill. Estimate within local window (prefer causal in evaluation): xt = 1 (cid:88) i=1 xi or xt = median{x1, . . . , xn}. 16 (14) (15) (16) Zero Fill (Semantic Zero Only). xt = 0, (17) used only when zero has clear meaning (e.g., counts/absence)."
        },
        {
            "title": "B Ensemble Strategy",
            "content": "Setup. Let Mselected = {mi(θ i=1 be the top-k models returned by Planner with tuned hyperparameters θ and validation scores Sval. For each model mi, we compute scalar validation loss si (lower is better) by aggregating the normalized metric vector ℓi RM (e.g., MAE, MAPE): )}k si = (cid:88) m=1 αm norm(ℓi,m) , αm 0, (cid:88) αm = 1. (18) On the test horizon of length H, model mi outputs ˆx(i) horizon-wise fixed weights wi 0, (cid:80) hyperparameters; no test data is touched. 1:H . An ensemble produces ˆxh = (cid:80)k with wi = 1. All choices below depend only on Sval and pre-specified i=1 wi ˆx(i) (A) SingleBest Selection. Pick the model with the best validation score and use it alone: = arg min i[k] si, wi = 1, wj=i = 0. When used. Prefer (19) if the leader is clearly ahead: gap = s(2) s(1) s(1) δ, with s(1) s(2) s(k), (19) (20) where δ is small margin (default δ = 0.05). This avoids diluting dominant model with weaker ones. (B) Performance-Aware Averaging. Assign higher weights to better validation performance while preventing over-concentration. We use temperatured inverse-loss scheme with shrinkage: wi = (si + ε)β, β > 0, ε > 0, wperf = exp(cid:0) log wi/τ (cid:1) j=1 exp(cid:0) log wj/τ (cid:1) = (cid:80)k wi = (1 λ) clip(cid:0)wperf , wmin, wmax (cid:80)k 1/τ j=1 1/τ 1 (cid:1) + λ j , , (21) (22) (23) with defaults β=1, τ =1, λ=0.1, wmin=0.02, wmax=0.80, and ε=108. When multiple metrics are used, si comes from (18) with minmax normalization inside norm() across the candidates. The shrinkage in (23) stabilizes weights in small-k regimes and under close scores. (C) Robust Aggregation. When candidate predictions disagree substantially, use distribution-robust, order-statistic based pooling at each horizon index h: Median: = median(cid:8)ˆx(1) ˆxmed , . . . , ˆx(k) (cid:9), Trimmed mean: ˆxtrim = 1 2ρk kρk (cid:88) i=ρk+ ˆx(i) h:, (24) (25) where ˆx(i) h: denotes the i-th smallest prediction at step and ρ [0, 0.25) is the trimming fraction (default ρ = 0.1). Median (24) has 50% breakdown point; the trimmed mean (25) trades slightly lower robustness for variance reduction. Notes on implementation. (i) Weights wi are horizon-wise constant to avoid step-wise overfitting; (ii) when Curator applies scaling (e.g., z-score), ensembling is performed in the scaled space and then inverted; (iii) performance aggregation (18) can emphasize primary metric by setting its αm larger (we use αMAE=αMAPE=0.5 by default); (iv) computational cost is O(kH) for all strategies; (v) for k=1, (19) is used by definition."
        },
        {
            "title": "C Model Library",
            "content": "Here is full list of time series models that we implement. The 21 models can be divided into 5 categories: 1) Traditional Statistical models; 2) Regression-based machine learning (ML) models; 3) Tree-based Models (Ensemble method); 4) Neural Network Models (Deep Learning); 5) Specialized Time Series Models. Details are listed in Table 3. Table 3 Implemented time series forecasting model library in model library.py."
        },
        {
            "title": "Function name",
            "content": "Statistical (7) ML regression (6)"
        },
        {
            "title": "ARIMA\nRandomWalk\nExponentialSmoothing\nMovingAverage\nTBATS\nTheta\nCroston",
            "content": "LinearRegression PolynomialRegression RidgeRegression LassoRegression ElasticNet SVR predict arima predict random walk predict exponential smoothing predict moving average predict tbats predict theta predict croston predict linear regression predict polynomial regression predict ridge regression predict lasso regression predict elastic net predict svr Tree-based (4) Neural networks (2) Specialized (2) RandomForest GradientBoosting XGBoost LightGBM NeuralNetwork LSTM Prophet Transformer predict random forest predict gradient boosting predict xgboost predict lightgbm predict neural network predict lstm predict prophet predict transformer"
        },
        {
            "title": "D Experimental Details",
            "content": "D.1 Implementations We use OpenAI GPT-4o (OpenAI et al., 2024) as the default backbone model. Due to limited budget, we divided all datasets into 25 slices and conducted experiments on these slices instead of the entire dataset. The input time series length for each slice is set as 512, and we use four different prediction horizons {96, 192, 336, 720}. The evaluation metrics include mean absolute error (MAE) and mean absolute percentage error (MAPE). We report the averaged results from the 25 slices. D.2 Dataset Details Dataset statistics are summarized in Table 4. We evaluate the univariate time series forecasting performance on the well-established eight different benchmarks, including four ETT datasets, Weather, Electricity, Exchange, and ILI from Wu et al. (2023b). D.3 Baselines We benchmark TSci against several leading large language models, including GPT-4o, Gemini-2.5 Flash (Gemini Team, Google, 2025), Qwen-Plus (Cloud, 2025a), DeepSeek-v3 (Liu et al., 2024), and Claude-3.7 (Cloud, 2025b). 18 Table 4 Summary of datasets across different domains."
        },
        {
            "title": "Duration",
            "content": "ETTh1, ETTh2 Electricity ETTm1, ETTm2 Electricity"
        },
        {
            "title": "Environment\nElectricity\nEconomic\nHealth",
            "content": "17,420 69,680 52,696 26,304 7,588 966 1 hour 15 mins 10 mins 1 hour 1 day 1 week 2016.07.01 - 2018.06.26 2016.07.01 - 2018.06.26 2020.01.01 - 2021.01.01 2016.07.01 - 2019.07.02 1990.01.01 - 2010.10.10 2002.01.01 - 2020.06."
        },
        {
            "title": "E Visualizations",
            "content": "E.1 LLM Guided Data Visualizations Our framework generates comprehensive visualizations during the pre-processing stage to facilitate data understanding and quality assessment. The visualization pipeline employs multi-panel approach to systematically examine time series characteristics. Time Series Overview Plot. The primary visualization component displays the raw time series data with temporal indexing on the x-axis and corresponding values on the y-axis. This panel serves as the foundational view for identifying global patterns, potential anomalies, and overall data structure. The visualization incorporates grid lines with reduced opacity (α = 0.3) to enhance readability while maintaining focus on the data trajectory, as shown in Figure 8. (a) Time Series Plot (b) Rolling Statistics Plot Figure 8 Example of time series overview plot on one slice of ECL dataset with input length = 512. Figure 8a displays the raw data. Figure 8b shows the rolling mean and rolling standard deviation of the data slice. Time Series Decomposition Analysis Plot. To comprehensively understand the underlying structure of the time series data, we employ seasonal decomposition to decompose the original series into four interpretable components, as shown in Figure 9. The decomposition follows the additive model Xt = Tt + St + Rt, where Xt represents the original observed values, Tt denotes the trend component capturing long-term systematic changes, St indicates the seasonal component revealing periodic patterns with fixed frequency, and Rt represents the residual component containing random noise and unexplained variations. The trend component helps identify the overall direction and magnitude of change over time, while the seasonal component exposes recurring patterns that may be crucial for forecasting accuracy. The residual component serves as diagnostic tool to assess the adequacy of the decomposition and identify potential anomalies or structural breaks. This four-panel visualization provides essential insights for selecting appropriate preprocessing strategies and forecasting models, as the presence of strong trends or seasonality directly informs the choice of detrending methods and seasonal adjustment techniques. Autocorrelation Analysis Plot. To assess the temporal dependencies and identify potential patterns in the time series data, we employ the autocorrelation function (ACF) and partial autocorrelation function (PACF) 19 Figure 9 Example of time series decomposition analysis plot on ECL dataset with input length =512. Figure 7(a) is the plot of the original time series Xt. Figure 7(b) is the plot of the trend Tt. Figure 7(c) is the plot of the seasonal component St. Figure 7(d) is the plot of the residual component Rt. plots, as shown in Figure 10. The ACF measures the linear relationship between observations at different time lags, revealing the overall memory structure and helping identify seasonal patterns, trends, and the presence of unit roots. The PACF, on the other hand, measures the correlation between observations at specific lag while controlling for the effects of intermediate lags, providing insights into the optimal order of autoregressive models and helping distinguish between autoregressive and moving average components. These diagnostic plots are essential for model identification in ARIMA modeling, as they reveal the underlying stochastic process characteristics and guide the selection of appropriate differencing operations and model parameters. The ACF and PACF analysis enables us to understand the temporal structure of the data, identify potential non-stationarity issues, and inform the choice of appropriate forecasting models based on the observed correlation patterns. E.2 Technical Implementation Details All visualizations are generated using Matplotlib and seaborn libraries with consistent styling parameters to ensure reproducibility and professional presentation. The time series plots employ line width of 2.0 pixels with standardized color palette (#c83e4b for primary series), while distribution plots utilize 22 subplot layout combining time series visualization, histogram with kernel density estimation (KDE), box plots, and Q-Q plots for comprehensive distributional analysis. Rolling statistics plots compute moving averages and standard deviations using configurable window sizes (default 24 periods) with distinct color coding for trend and volatility components. Seasonal decomposition leverages the statsmodels.tsa.seasonal.seasonal decompose function with additive decomposition and configurable seasonal periods, while autocorrelation analysis employs 20 Figure 10 Example of autocorrelation analysis plot on ECL dataset with input length = 512. Figure 8(a) is the ACF plot, and Figure 8(b) is the PACF plot. plot acf and plot pacf functions with 40-lag windows for optimal model identification. All plots feature white backgrounds with black grid lines (major grid: solid lines, 0.5px width, 30% opacity; minor grid: dotted lines, 0.3px width, 20% opacity) and are saved as high-resolution PDF files (300 DPI) with tight bounding boxes to ensure publication-quality output. The visualization generation process is fully automated through LLM-driven configuration, allowing dynamic adaptation of plot parameters based on data characteristics and analysis requirements. E.3 Output and Integration The visualization pipeline generates standardized output files in PDF format, with configurable save paths and automatic directory creation. Each visualization includes comprehensive logging for audit trails and debugging purposes. The system integrates seamlessly with the broader time series prediction framework, automatically generating visualizations during the pre-processing stage and storing them for subsequent analysis and reporting phases. These pre-processing visualizations serve as the foundation for data-driven decision making, enabling researchers and practitioners to understand their time series data characteristics before proceeding to model selection and forecasting stages."
        },
        {
            "title": "F Full Experiment Results",
            "content": "Here we present the full experiment results of our TSci on eight datasets against five LLM-based baselines, as shown in Table 5 and Table 6. 1st Count row at the end of Table 6 indicates the number of test cases where the model achieves the best performance across all datasets. TSci achieves superior performance across the majority of datasets and forecasting horizons (Figure 5), demonstrating its LLM-driven reasoning capacity in time series forecasting. Figure 11 shows the complete result of TSci compared with three statistical baselines 21 on eight datasets. Figure 12 and 13 show the MAE and MAPE distribution across datasets and horizons. Table 5 Time series forecasting results. lower value indicates better performance. Red : the best, Blue : the second best. Methods GPT-4o Gemini-2.5 Flash Qwen-Plus DeepSeek-v3 Claude-3.7 TSci (Ours) Metric MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE 1 E 2 E 1 E 2 E 96 192 336 720 Avg 96 192 336 720 Avg 96 192 336 720 Avg 96 192 336 720 Avg 6.39 1.10e1 2.18e1 4.14e1 2.01e1 1.09e1 1.45e1 2.21e1 2.53e1 1.82e1 2.68 5.84 6.86 7.62 5.75 5.52 9.22 1.11e1 1.39e1 9. 69.3 89.9 319.2 256.9 183.8 190.9 304.2 441.2 121.9 264.6 24.3 78.8 147.9 91.7 85.7 29.6 43.2 61.5 68.4 50.7 4.99 5.04 5.29 5.46 5.20 1.16e1 1.30e1 8.76 1.08e1 1.10e 5.91 8.21 8.06 7.04 7.31 1.30e1 1.41e1 1.33e1 2.34e1 1.60e1 71.8 57.4 51.6 63.5 61.1 74.7 102.3 65.5 81.6 81.0 43.0 56.1 61.5 79.0 59.9 58.2 58.7 78.6 103.4 74. 6.50 9.79 1.30e1 1.67e1 1.15e1 3.34e1 1.65e1 2.49e1 5.58e1 3.27e1 4.01 5.56 8.48 2.31 5.09 7.84 7.24 1.18e1 1.57e1 1.07e1 74.8 108.3 143.0 129.2 113.8 320.1 118.7 74.5 189.0 175. 43.9 67.4 70.6 11.9 48.4 109.2 28.6 46.0 102.9 71.7 7.24 8.60 1.55e1 1.76e1 1.22e1 1.02e1 1.27e1 1.62e1 4.13e1 2.01e1 3.53 7.94 1.23e1 8.97 8.17 4.81 7.06 1.09e1 1.33e1 9. 90.9 104.8 198.4 145.6 134.9 47.2 147.2 118.6 173.4 121.6 31.7 91.4 206.1 139.4 117.1 20.7 35.2 44.9 57.9 39.7 5.58 5.99 7.99 1.71e1 9.16 8.56 9.62 9.95 1.82e1 1.16e 3.09 5.80 8.23 7.78 6.22 4.35 9.08 7.97 6.38 6.94 75.3 82.9 124.8 161.0 111.0 202.7 107.0 70.4 94.0 118.5 26.8 52.0 67.1 117.9 65.9 47.8 39.1 34.3 43.0 41. 1.81 2.05 2.68 1.53 2.02 4.50 4.47 3.81 6.88 4.91 1.68 1.89 3.26 4.10 2.73 3.63 4.77 5.12 5.96 4.87 13.9 31.0 31.7 16.7 23.3 18.9 12.8 10.7 56.2 24. 15.7 19.9 31.5 52.0 29.8 40.5 30.5 27.6 27.6 31.6 Figure 11 Performance comparison of TSci with three statistical baselines across eight datasets."
        },
        {
            "title": "G Case Study on ECL Dataset",
            "content": "G.1 Analysis Summary This analysis summary presents the findings from time series forecasting experiment conducted on the ECL dataset. The analysis focused on understanding the trend, seasonality, and stationarity of the data, and potential improvements for future forecasting efforts. 22 Table 6 Time series forecasting results (continuing). lower value indicates better performance. Red : the best, Blue : the second best. Methods GPT-4o Gemini-2.5 Flash Qwen-Plus DeepSeek-v3 Claude-3.7 TSci (Ours) Metric MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE h W e h I 96 192 336 720 Avg 96 192 336 720 Avg 96 192 336 720 Avg 24 36 48 60 Avg 2.16e1 4.07e1 6.89e1 1.14e2 6.13e1 2.09e3 3.64e3 5.85e3 1.38e4 6.33e3 6.21e-2 1.09e-1 1.52e-1 3.14e-1 1.60e-1 1.58e5 1.93e5 2.45e5 2.72e5 2.17e 5.1 9.5 6.6 22.4 10.9 63.6 109.0 252.3 615.9 260.2 9.5 17.6 26.0 51.9 26.2 18.4 24.0 28.9 33.4 26.2 6.59e1 3.84e1 5.92e1 9.74e1 6.52e1 7.37e2 1.35e3 7.79e2 6.75e2 8.86e 5.46e-2 2.34e-1 1.06e-1 1.15e-1 1.28e-1 2.48e5 2.53e5 2.83e5 1.98e5 2.46e5 15.5 9.0 4.4 18.5 11.8 22.8 41.1 63.5 54.2 45.4 8.8 35.3 17.1 18.4 19.9 28.5 32.0 34.1 22.6 29. 2.25e1 3.17e1 6.96e1 4.79e1 4.29e1 1.09e3 1.42e3 1.18e3 2.97e3 1.66e3 3.21e-2 6.04e-2 7.96e-2 1.70e-1 8.50e-2 3.49e5 3.05e5 3.67e5 3.29e5 3.37e5 5.2 7.4 6.4 6.6 6.4 32.6 42.6 72.6 103.8 62. 5.1 10.2 12.3 26.9 13.6 38.9 32.5 41.3 35.5 37.1 1.54e1 2.84e1 8.06e1 8.37e1 5.20e1 1.36e3 2.06e3 4.89e3 1.90e4 6.83e3 5.46e-2 8.40e-2 2.25e-1 3.37e-1 1.75e-1 1.86e5 1.92e5 2.58e5 2.62e5 2.24e 3.6 6.6 8.5 14.4 8.3 42.2 62.1 182.1 656.5 235.7 8.3 13.5 35.9 49.1 26.7 21.6 22.8 30.5 31.3 26.5 1.83e1 3.97e1 7.24e1 5.19e1 4.56e1 8.21e2 5.05e2 1.26e3 7.93e2 8.44e 3.08e-2 8.75e-2 6.65e-2 1.06e-1 7.30e-2 1.56e5 1.86e5 1.76e5 1.98e5 1.79e5 4.3 9.3 6.6 7.5 6.9 23.9 15.2 39.8 50.0 32.2 4.8 14.6 10.8 17.1 11.8 17.4 20.3 19.0 22.1 19. 1.63e1 1.60e1 6.13e1 2.29e1 2.91e1 3.94e2 4.50e2 9.68e2 8.56e2 6.67e2 2.46e-2 3.85e-2 5.76e-2 5.76e-2 4.50e-2 1.41e5 1.48e5 1.34e5 1.40e5 1.41e5 3.8 3.6 5.0 5.1 4.4 11.2 13.6 77.3 58.8 40. 3.8 5.8 8.9 8.8 6.8 16.5 16.8 15.6 16.2 16.3 1st Count 0 3 2 3 35 Analysis Summary generated from Curator Trend Analysis: { Description: The time series data exhibits an overall upward trend over the observed period. This is evident from the increasing values towards the end of the series compared to the beginning. The trend is not strictly linear, as there are fluctuations and periods of decline, but the general direction is upward., Strength: Moderate to strong, with noticeable periods of growth, particularly towards the latter part of the series., Supporting Evidence: The Basic Time Series Plot and Rolling Statistics visualizations show clear upward trajectory, especially in the latter half of the data. }, Seasonality Analysis: { Description: There are indications of seasonality in the data, with recurring patterns that suggest periodic fluctuations. These patterns are not strictly regular but suggest some form of cyclical behavior., Periodicity: The exact periodicity is not immediately clear from the data provided, but the Seasonal Decomposition visualization should provide insights into the seasonal component., Supporting Evidence: The Seasonal Decomposition visualization will help identify and confirm any seasonal patterns present in the data. }, Stationarity: { Description: The time series appears to be non-stationary, as indicated by the presence of trend and potential seasonality. Non-stationarity is also suggested by the rolling statistics, which show changing mean and variance over time., Implications: Non-stationarity implies that the data may need to be transformed (e.g., differencing, detrending) before applying certain forecasting models that assume stationarity., Supporting Evidence: The Rolling Statistics visualization shows variations in mean and variance, indicating non-stationarity. }, Potential Issues for Forecasting: { Non-Stationarity: The presence of trend and potential seasonality suggests non-stationarity, which may require transformation., Figure 12 Slice-level MAE distributions across datasets and horizons. The 24 grid organizes subplots by dataset; within each subplot, four horizons are separated by dashed lines, and six methods are shown as grouped boxplots. Y-axis uses log10 scale; lower is better. Structural Breaks: There may be structural breaks or regime shifts, particularly around periods of rapid change in the data., Data Quality: No explicit data quality issues are mentioned, but any missing values or outliers should be addressed before modeling., Supporting Evidence: The Basic Time Series Plot and Rolling Statistics visualizations highlight periods of rapid change and variability. }, Summary of Key Findings: { Trend: The data shows moderate to strong upward trend., Seasonality: Potential seasonal patterns are present, though further analysis is needed to confirm periodicity., Stationarity: The series is non-stationary, requiring transformation for certain models., Forecasting Challenges: Addressing non-stationarity and potential structural breaks will be crucial for accurate forecasting., Actionable Insights: Consider transformations such as differencing or detrending to achieve stationarity. Use models that can handle non-stationary data or incorporate trend and seasonality components. } G.2 Visualization Figure 14 shows the ensemble forecast with individual model predictions and confidence intervals on the ECL dataset. 24 Figure 13 Slice-level MAPE distributions across datasets and horizons. The 24 grid organizes subplots by dataset; within each subplot, four horizons are separated by dashed lines, and six methods are shown as grouped boxplots. Y-axis uses log10 scale; lower is better. Figure 14 Case study of ensemble forecast with individual model predictions on ECL dataset. G.3 Comprehensive Report Comprehensive report This report presents the findings from time series forecasting experiment conducted on an industrial dataset. The primary objective was to evaluate the performance of various forecasting models and their ensemble on specific data slice. The analysis focused on understanding data characteristics, model performance, and potential improvements for future forecasting efforts. 25 ## Key Findings and Model Performance ### Data Characteristics - **Trend**: The dataset exhibits strong upward trend. - **Seasonality**: Presence of seasonal patterns, though not strongly pronounced. - **Stationarity**: The data is non-stationary, necessitating transformations for certain models. ### Model Performance - **Ensemble Model**: Utilized weighted average approach with weights assigned to Exponential Smoothing (35%), ARIMA (40%), and Random Forest (25%). - **MSE**: 209,950.78 - **MAE**: 393.83 - **MAPE**: 11.20% - **Individual Models**: - **Exponential Smoothing**: - MSE: 192,879.83 - MAE: 377.15 - MAPE: 10.73% - **ARIMA**: - MSE: 205,582.59 - MAE: 390.10 - MAPE: 11.09% - **Random Forest**: - MSE: 242,977.49 - MAE: 423.16 - MAPE: 12.02% ## Issues and Limitations - **Non-Stationarity**: The presence of trend complicates modeling and may require differencing or transformation. - **Structural Breaks**: Potential structural breaks or regime shifts were noted, particularly around the midpoint of the series. - **Model Performance**: While Exponential Smoothing and ARIMA performed relatively well, Random Forest showed higher error metrics, indicating potential overfitting or inadequacy for this dataset. ## Recommendations 1. **Data Preprocessing**: Further address non-stationarity through differencing or transformation techniques. 2. **Model Selection**: Consider refining model hyperparameters and exploring additional models like SARIMA or advanced machine learning techniques. 3. **Ensemble Strategy**: Re-evaluate the ensemble weighting strategy to optimize performance based on individual model strengths. This analysis provides comprehensive overview of the current forecasting capabilities and outlines actionable steps for enhancing future model performance."
        },
        {
            "title": "H Prompts",
            "content": "Prompt for Curator PREPROCESS SYSTEM PROMPT = You are the Data Preprocessing Chief Agent for an advanced time series forecasting system. Your mission is to ensure that all input data is of the highest possible quality before it enters the modeling pipeline. 26 Background: - You have deep expertise in time series data cleaning, anomaly detection, and preparation for machine learning and statistical forecasting. - You understand the downstream impact of preprocessing choices on model performance and interpretability. Your responsibilities: - Rigorously assess the quality of the input time series, identifying missing values, outliers, and structural issues. - For each issue, recommend the most appropriate handling strategy, considering both statistical best practices and the needs of advanced forecasting models. - Justify your recommendations with clear reasoning, referencing both the data characteristics and potential modeling implications. - If relevant, suggest additional preprocessing steps (e.g., resampling, detrending, feature engineering) that could improve results. - Always return your decisions in structured Python dict, and ensure your reasoning is transparent and actionable. You have access to: - The raw time series data (as Python dict) - Any prior preprocessing history or known data issues Your output will directly determine how the data is prepared for all subsequent analysis and modeling. DATA PREPROCESS PROMPT = You are time series data preprocessing expert. Given the following time series data (as Python dict): {{data.to dict(orient=list)}} Please: 1. Assess the overall data quality. 2. Recommend missing value handling strategy (choose from: interpolate, forward fill, backward fill, mean, median, drop, zero). 3. Recommend an outlier handling strategy (choose from: clip, drop, zero, interpolate, ffill, bfill, mean, median, smooth). 4. Optionally, suggest any other preprocessing steps if needed. Return your answer as Python dict: { quality assessment: string, missing value strategy: string, outlier strategy: string, other suggestions: string } ANALYSIS REPORT GENERATION PROMPT = Given the following preprocessed time series data and generated visualizations, please provide comprehensive analysis report. Data (as Python dict): {{sample}} Generated Visualizations: {{visualizations}} Note: This data has already been preprocessed - missing values and outliers have been handled. Please provide comprehensive analysis including: 1. Data Overview: - basic stats: mean, std, min, max, trend - data characteristics: seasonality, stationarity, patterns 2. Data Quality Assessment: - data quality score: overall quality score (0-1) after preprocessing - data characteristics: key characteristics of the cleaned data 27 3. Insights from Visualizations: - key patterns: patterns observed in the data - seasonal components: any seasonal patterns - trend analysis: overall trend direction and strength - distribution characteristics: data distribution insights 4. Forecasting Readiness: - data suitability: how suitable this data is for forecasting - potential challenges: any challenges for forecasting models - data strengths: strengths of this dataset 5. Model and Feature Recommendations: - model suggestions: suitable model types for this data - feature engineering: suggested features to create - preprocessing effectiveness: how well the preprocessing worked IMPORTANT: Return ONLY the JSON object below, with NO markdown formatting, NO code blocks, NO explanations. Just the raw JSON. dataoverview: basicstats: mean: float, std: float, min: float, max: float, trend: string , datacharacteristics: seasonality: string, stationarity: string, patterns: [string] , qualityassessment: dataqualityscore: float, datacharacteristics: string , visualizationinsights: keypatterns: [string], seasonalcomponents: string, trendanalysis: string, distributioncharacteristics: string , forecastingreadiness: datasuitability: string, potentialchallenges: [string], datastrengths: [string] , recommendations: modelsuggestions: [string], featureengineering: [string], preprocessingeffectiveness: string DATA VISUALIZATION PROMPT = Given the following preprocessed time series data and generated visualizations, please provide comprehensive analysis report. Data (as Python dict): {{sample}} Generated Visualizations: {{visualizations}} Note: This data has already been preprocessed - missing values and outliers have been handled. Please provide comprehensive analysis including: 1. Data Overview: - basic stats: mean, std, min, max, trend - data characteristics: seasonality, stationarity, patterns 2. Data Quality Assessment: - data quality score: overall quality score (0-1) after preprocessing - data characteristics: key characteristics of the cleaned data 3. Insights from Visualizations: - key patterns: patterns observed in the data - seasonal components: any seasonal patterns - trend analysis: overall trend direction and strength - distribution characteristics: data distribution insights 4. Forecasting Readiness: - data suitability: how suitable this data is for forecasting - potential challenges: any challenges for forecasting models - data strengths: strengths of this dataset 5. Model and Feature Recommendations: - model suggestions: suitable model types for this data - feature engineering: suggested features to create - preprocessing effectiveness: how well the preprocessing worked IMPORTANT: Return ONLY the JSON object below, with NO markdown formatting, NO code blocks, NO explanations. Just the raw JSON. dataoverview: basicstats: mean: float, std: float, min: float, max: float, trend: string , datacharacteristics: seasonality: string, stationarity: string, patterns: [string] , qualityassessment: dataqualityscore: float, datacharacteristics: string , visualizationinsights: keypatterns: [string], seasonalcomponents: string, trendanalysis: string, distributioncharacteristics: string , forecastingreadiness: datasuitability: string, 29 potentialchallenges: [string], datastrengths: [string] , recommendations: modelsuggestions: [string], featureengineering: [string], preprocessingeffectiveness: string DATA ANALYSIS PROMPT = Given the following time series data (as Python dict): {{sample}} Please analyze the data quality and provide the following information as JSON file: 1. Basic statistics for each column: - mean: float - std: float - min: float - max: float - trend: increasing/decreasing/stable 2. Missing value information: - missing count: int (total missing values) - missing percentage: float (percentage of missing values) 3. Outlier information: - outlier count: int (total outliers detected) - outlier percentage: float (percentage of outliers in the data, between 0 and 1) 4. Data quality assessment: - data quality score: float (0-1, where 1 is perfect quality) - main issues: list of strings (e.g., [missing values, outliers, noise, ...]) 5. Recommended preprocessing strategies: - missing value strategy: string (choose from: interpolate, forward fill, backward fill, mean, median, drop, zero) - outlier detect strategy: string (choose from: iqr, zscore, percentile, none) - outlier handle strategy: string (choose from: clip, drop, interpolate, ffill, bfill, mean, median, smooth) IMPORTANT: Return ONLY the JSON object below, with NO markdown formatting, NO code blocks, NO explanations. Just the raw JSON: basicstats: mean: float, std: float, min: float, max: float, trend: string , missinginfo: missingcount: int, missingpercentage: float , outlierinfo: outliercount: int, outlierpercentage: float , qualityassessment: dataqualityscore: float, mainissues: [string] , recommendedstrategies: missingvaluestrategy: string, outlierdetectstrategy: string, outlierhandlestrategy: string VISUALIZATION DECISION PROMPT= Given the following time series data: Data shape: {{data.shape}} Data columns: {{list(data.columns)}} Please decide what visualizations would be most useful for understanding this data. Consider the data characteristics and quality issues. Choose from these visualization types: - time series: Basic time series plot - distribution: Histogram, box plot, KDE - rolling stats: Rolling mean, std, etc. - autocorrelation: ACF/PACF plots - seasonal decomposition: Trend, seasonal, residual components IMPORTANT: Return ONLY the JSON object below, with NO markdown formatting, NO code blocks, NO explanations. Just the raw JSON: visualizations: [ name: string, type: string, description: string, features: [string], title: string, xlabel: string, ylabel: string, additionalelements: [string], plotspecificparams: ] Prompt for Planner SYSTEM PROMPT = You are the Principal Data Analyst Agent for state-of-the-art time series forecasting platform. Background: - You are an expert in time series statistics, pattern recognition, and exploratory data analysis. - Your insights will guide model selection, hyperparameter tuning, and risk assessment. Your responsibilities: - Provide comprehensive statistical summary of the input data, including central tendency, dispersion, skewness, and kurtosis. - Detect and describe any trends, seasonality, regime shifts, or anomalies. - Assess stationarity and discuss its implications for modeling. - Identify potential challenges for forecasting, such as non-stationarity, structural breaks, or data quality issues. - Justify all findings with reference to the data and, where possible, relate them to best practices in time series modeling. 31 - Always return your analysis in structured Python dict, with clear, concise, and actionable insights. You have access to: - The cleaned time series data (as Python dict) - Visualizations (if available) to support your analysis Your output will be used by downstream agents to select and configure forecasting models. ANALYSIS PROMPT = Given the following time series data and visualizations, please provide comprehensive analysis. Data (as Python dict): {{sample}} {{viz info}} Please analyze: 1. Trend analysis - overall direction and strength 2. Seasonality analysis - any recurring patterns 3. Stationarity - whether the data is stationary 4. Potential issues for forecasting 5. Summary of key findings Return your analysis in clear, structured format. Prompt for Planner SYSTEM PROMPT = You are the Model Selection and Validation Lead Agent for an industrial time series forecasting system. Background: - You are highly skilled in matching data characteristics to appropriate forecasting models and in designing robust validation strategies. - You understand the strengths, weaknesses, and requirements of wide range of statistical and machine learning models. Your responsibilities: - Review the data analysis summary and select the top 3 most suitable forecasting models from the provided list. - For each model, recommend hyperparameter search space tailored to the datas characteristics and modeling goals. - Justify each model choice and hyperparameter range, referencing both the analysis and your domain expertise. - Consider diversity in model selection to maximize ensemble robustness. - Always return your decisions in structured Python dict, with clear reasoning for each choice. You have access to: - The data analysis summary (as Python dict) - The list of available models Your output will directly determine which models are trained and how they are tuned. MODEL SELECTION PROMPT= You are time series model selection agent. Given the analysis report analysis and available models available models, select the best candidates models that are most suitable for the data and propose hyperparameters for each model. For each model, you should propose hyperparameter search space tailored to the data characteristics and modeling goals. Justify each model choice and hyperparameter range, referencing both the analysis and your domain expertise. Return your answer in the following JSON format with an array of selected models: selectedmodels: [ model: string, hyperparameters: ..., 32 reason: string model: string, hyperparameters: ..., reason: string , , ] Below is an example of the output: selectedmodels: [ model: ARIMA, hyperparameters: p: [0, 1, 2], d: [0, 1], q: [0, 1, 2], , reason: string , ] IMPORTANT REQUIREMENTS: 1. Return EXACTLY candidates models in the selected models array 2. Each model must have model, hyperparameters, and reason fields 3. The model field must be one of the available models: available models 4. The hyperparameters field should contain 2-3 parameter search spaces as arrays 5. Return ONLY the JSON object, no markdown formatting, no explanations before or after 6. Ensure the JSON is valid and properly formatted Prompt for Forecaster SYSTEM PROMPT = You are the Ensemble Forecasting Integration Agent for high-stakes time series prediction system. Background: - You are an expert in ensemble methods, model averaging, and uncertainty quantification for time series forecasting. - Your integration strategy can significantly impact the accuracy and reliability of the final forecast. Your responsibilities: - Review the individual model forecasts and any available visualizations. - Decide the most appropriate ensemble integration strategy (e.g., best model, weighted average, trimmed mean, median, custom weights). - If using weights, specify them and explain your rationale. - Justify your integration choice, considering model diversity, agreement, and historical performance. - Assess your confidence in the ensemble and note any risks or caveats. - Always return your decision in structured Python dict, with transparent reasoning. You have access to: - The individual model forecasts (as Python dict) - Visualizations of the forecasts and historical data - Prediction tools for different models (ARMA, LSTM, RandomForest, etc.) Your output will be used as the final forecast for this time series slice. 33 ENSEMBLE DECISION PROMPT= You are an ensemble forecasting expert. Given the following individual model forecasts: json.dumps(individual forecasts, indent=2) {{viz info}} Please: 1. Decide the best ensemble integration strategy (choose from: best model, weighted average, trimmed - mean, median, custom weights). 2. If using weights, specify the weights for each model. 3. Justify your choice. 4. Assess your confidence in the ensemble. IMPORTANT: Return your answer ONLY as JSON object, with NO markdown formatting, NO code blocks, NO explanations. Just the raw JSON: integrationstrategy: string, weights: modelname: float (if applicable), selectedmodel: string (if bestmodel), reasoning: string, confidence: string MODEL WEIGHTS PROMPT = You are an ensemble forecasting expert. Given the following individual model forecasts: json.dumps(individual forecasts, indent=2) viz info Please: 1. Decide the best ensemble integration strategy (choose from: best model, weighted average, trimmed - mean, median, custom weights). 2. If using weights, specify the weights for each model. 3. Justify your choice. 4. Assess your confidence in the ensemble. IMPORTANT: Return your answer ONLY as JSON object, with NO markdown formatting, NO code blocks, NO explanations. Just the raw JSON: integrationstrategy: string, weights: modelname: float (if applicable), selectedmodel: string (if bestmodel), reasoning: string, confidence: string"
        }
    ],
    "affiliations": [
        "Case Western Reserve University",
        "Fudan University",
        "Stony Brook University",
        "University of British Columbia",
        "University of California, Los Angeles",
        "University of California, San Diego",
        "Zhejiang University"
    ]
}