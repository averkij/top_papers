{
    "paper_title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
    "authors": [
        "Christian Simon",
        "Masato Ishii",
        "Wei-Yao Wang",
        "Koichi Saito",
        "Akio Hayakawa",
        "Dongseok Shim",
        "Zhi Zhong",
        "Shuyang Cui",
        "Shusuke Takahashi",
        "Takashi Shibuya",
        "Yuki Mitsufuji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations."
        },
        {
            "title": "Start",
            "content": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models Christian Simon Masato Ishii Wei-Yao Wang Koichi Saito Shuyang Cui Akio Hayakawa Dongseok Shim Zhi Zhong Takashi Shibuya Yuki Mitsufuji, Shusuke Takahashi 6 2 0 2 5 ] . [ 2 1 8 9 0 2 . 2 0 6 2 : r Sony Group Corporation Sony AI {first name.last name}@sony.com"
        },
        {
            "title": "Abstract",
            "content": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates hierarchical method and noncausal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations. Our project page: https://echoesovertime.github.io. 1. Introduction Video-to-Audio (V2A) is generative task that aims to produce realistic and contextually aligned audio from silent video inputs. This capability holds substantial promise for enhancing sound design workflows, particularly in domains such as film and gaming [27, 53]. Despite its potential, existing V2A methods [4, 19, 30, 41, 54] are primarily tailored for short-form audio generation, typically spanning 810 seconds. Among these, diffusion-based approaches [4, 30, 54] have shown superior performance over Figure 1. Long-Video to Audio (LV2A) task overview. The challenge is framed as training models on fixed-length segments while requiring them to generalize to variable-length (long-form) audio outputs during inference. transformer-based autoregressive models [41], largely by denoising fixed-length noise segments, which is strategy well-suited for brief clips. However, extending these models to long-form video inputs is challenging due to limited training data and the substantial memory requirements for modeling extended audio sequences. For instance, on some publicly available long audio-video datasets [10, 11], the distributions mostly cover only up-to 1 minute video. When applied to long-video-to-audio (LV2A) tasks, existing models trained on fixed-length segments struggle to accommodate longer sequence generation in testing, thereby constraining their effectiveness in real-world applications. We are interested in train-short and test-long problems where the longer video duration (up to 5 minutes) could be generated properly using only short clips in our training data as shown in Figure 1. Generating short clips for each short duration could be an alternative for LV2A [55]. Despite its practicality, this method often results in fragmented audio experiences, marked by disjointed transitions, unaligned sound events, and degraded audio quality stemming from its limited grasp of long-form video context. Please see Sec. 1 to see our early observations. In particular, we identify that existing V2A models [4, 19, 30, 41, 49, 54] expose structural constraints that reduce the generalizability in terms of various length generation and performance. The core base architecture of these models relies on transformer models [48]. Thus, these existing models depend on explicit positional encodings that are difficult to tame when dealing with longer sequence generation. Explicit positional encodings often hurt generalization to longer sequences [21].Fortunately, Mamba [6, 13] is introduced as an alternative to transformer modules, showing strong performance on various tasks and modalities [6, 13, 15, 16, 39]. Thus, there is an alternative to avoid using explicit positional encodings, which is deteriorating in generating long outputs. To tackle the challenges in LV2A generation, we introduce MMHNet, novel framework that reconceptualizes the task as one of multimodal alignment across modalities with varying token lengths. Our proposed method could effectively align between modalities and handle long video and audio without further adjustment in the model during inference.MMHNet combines multimodal video-toaudio (V2A) model with the HNet architecture [18], enabling audio synthesis conditioned on diverse multimodal inputs while effectively aligning visual and textual modalities. HNet enhances token processing through hierarchical structure, moving beyond conventional attention mechanisms. By replacing standard attention blocks with HNet and incorporating dynamic chunking, routing, and smoothing modules, MMHNet achieves effective and coherent audio generation over long durations. Unlike causal models, MMHNet leverages video conditions, which are noncausal, maintaining global receptive field that supports high-quality audio synthesis for long videos. Our method operates in compressed space during early layers, where multimodal alignment occurs to effectively integrate tokens from different sources and reduce redundancy. This approach leverages inherent overlaps in visual and audio data (e.g., similar frames and sound events within the same timeframe) [36, 50]. We introduce multimodal based routing to bridge distinct modalities and apply time-based token routing to reduce temporal complexity and enhance cross-modal alignment. To evaluate MMHNets capabilities, we introduce long-form V2A evaluation benchmark built upon the UnAV100 [10] and LongVale [11] datasets. Our experimental results show that MMHNet not only sets new standard in long-form audio generation but also consistently delivers high-quality outputs across different durations. The contributions of our work are threefolds: We introduce the length generalization challenge by training on short, fixed-length audio-visual data and evaluating on long-form video-to-audio (V2A) generation tasks using the UnAV100 and LongVale datasets. We propose MMHNet, multimodal hierarchical network that integrates MMAudio and hierarchical networks for efficient and consistent long-form audio generation. We conduct extensive experiments across long-form benchmarks, validating MMHNets superior performance and ability to scale with video duration. 2. Related work Video-to-audio generation. Video-to-audio synthesis aims to generate sound that is both semantically and temporally aligned with visual content. Existing methods typically fall into two categories: 1) those that inject visual features into pre-trained text-to-audio (TTA) models, and 2) those that train video-to-audio (V2A) models from scratch. Approaches like T2AV [32] and FoleyCrafter [54] enhance visual consistency and alignment by integrating visual and textual embeddings into audio generation pipelines. Meanwhile, models e.g., Diff-Foley [30] and Frieren [51] leverage contrastive pre-training and flow matching to improve multimodal coherence. MMAudio [4] further advances this field with hybrid architecture combining multimodal and single-modality diffusion transformer (DiT) blocks [33], incorporating synchronization features validated by Synchformer [20] and visual semantic features from CLIP [37]. V-AURA [49] is proposed as an autoregressive method to generate audio from given video frames. However, all of these methods are only well-suited for short-form videoto-audio generation, which limits the capability in generating audio beyond the duration covered during training. HunyuanVideo-Foley [40] was recently introduced, showcasing strong audio generation capabilities from diverse inputs such as text, SigLIP visual embeddings [47], and Synchformer [20]. Nevertheless, previous approaches have yet to fully unlock the potential for generating audio beyond the scope of training data. Multimodal models. Multimodal conditioning (e.g., video and text) is vital to current generative models [4, 30, 38, 43, 49], with many V2A systems relying on Transformer architectures for multimodal processing. While Transformers are effective in multimodal tasks [33], their dependence on positional embeddings limits generalization to durations beyond training. Position scaling, e.g., NTK or interpolation, is often required to extend their temporal range [3, 34, 46]. In contrast, Mamba [6, 13] processes sequences without positional embeddings, enabling efficient long-duration generation without modifications. Long video-to-audio generation. LoVA [5] represents the current state-of-the-art in long-video-to-audio generation, leveraging DiT-based architectures [33] to produce coherent and temporally aligned audio tracks from extended video inputs. It significantly outperforms earlier models in generating synchronized and contextually appropriate audio for Figure 2. We analyze the role of positional embeddings in V2A models such as MMAudio [4], built on MMDiT [24]. Without positional embeddings (a), MMAudio fails to capture temporal structure, producing redundant audio dominated by prominent visual objects (e.g., car crashing). With adjusted positional embeddings (b), alignment improves but sound quality degrades over long sequences (see scene C). (c) On UnAV100 [10], both configurations show performance drops across durations, with MMAudio without positional embeddings performing worst in distribution matching (FDP AN ) and multimodal alignment (IB-Score). long-form video content. Despite its strengths, LoVA exhibits limitations when tasked with generating audio beyond the one-minute mark, often resulting in noticeable degradation in audio quality and coherence. Autoregressive models [41, 49] offer an alternative approach, showing promise in long-form generation due to their step-by-step prediction capabilities. However, they are prone to error accumulation over time, which can lead to drift and loss of fidelity in extended sequences. Another promising direction involves agent-based methods [55], which divide long videos into shorter, manageable segments and generate audio for each clip independently. While this segmentation strategy can improve scalability and maintain quality, it introduces additional complexity by requiring accurate text descriptions for each segment and precise control over clip transitions to ensure seamless audio continuity. 3. Pilot Study Why do Transformer-based V2A models fail to generalize to long sequences? We observe that certain aspects of the current Transformer architecture in V2A, specifically positional embeddings [3] and attention logit exploding [14], pose challenges to length generalization unless substantial modifications are made in the inference mode. Problems with positional embeddings. Positional embeddings like RoPE [45] are essential for Transformer-based models, as they provide positional awareness for tokens. Without them, the model loses this capability. Training without positional embeddings is only viable when training and testing use identical sequence lengths. To explore this, we conduct pilot study analyzing pretrained Transformer-based model (e.g., MMAudio [4]) trained on 8-second audio-visual data and tested on longer sequences (e.g., 40 seconds), As illustrated in Figure 2, pretrained video-to-audio models without positional embeddings would perform poorly. Also, the generated sound becomes homogeneous for the model without positional embeddings because attention modules are orderless and the semantic meaning becomes less on point relative to the positions as shown in Figure 2 (a). Figure 2 (c) shows that increasing durations degrade the Transformer based V2A model significantly, 3-4 points drop for distribution matching (FDP AN s) and multimodal alignment (IB) scores. Designing network without positional embeddings is preferable in this case to avoid unnecessary adjustments when generating longer sequences during testing. 4. Proposed Method Let be dataset where each sample (x, c) comprises an audio and associated conditions (e.g., video frames and text caption). The objective is to train model on to learn conditional distribution pmodel(x c) that closely approximates the true data distribution pdata(x c) via flow matching [28, 29]. Our focus lies particularly on scenarios where represents long-form audio, significantly exceeding the lengths typically handled by existing methods, which often operate on short clips of approximately 10 seconds during both training and inference. To effectively model long-form audio distributions, we design the core architecture using Mamba-2 variants [6, 42], which enable token processing without relying on positional embeddings. This choice is motivated by our observation in Sec. 3 that positional embeddings tend to degrade performance in long audio generation scenarios. Additionally, 3 to enhance cross-modal alignment, we incorporate routing strategies that reduce token redundancy by compressing repetitive information, thereby improving efficiency and coherence across modalities. 4.1. Preliminaries Flow matching. We employ the conditional flow matching objective [28, 29] for generative modeling. For detailed methodology, we refer readers to [28]. Briefly, during inference, sample is generated by first drawing noise x0 from standard normal distribution. An ODE solver is then used to numerically integrate from time = 0 to = 1, guided by learned, time-dependent conditional velocity vector field: vθ(t, c, x) : [0, 1] RC Rd Rd, where denotes the conditioning input (e.g., video and text), and is point in the vector field. The velocity field is parameterized by deep neural network with parameters θ. At training time, we learn the parameters θ of the deep neural network by minimizing the following objective: Et,(x0,x1,c)q(x0)q(x1,c) vθ(t, c, xt) u(xt x0, x1)2(cid:105) (cid:104) (1) where U[0, 1] is sampled uniformly from the interval [0, 1], and q(x0)q(x1, c) denotes the joint distribution over the prior and training data. The interpolated point xt is defined as: , xt = tx1 + (1 t)x0, and the corresponding flow velocity at xt is given by: u(xt x0, x1) = x1 x0. (2) (3) Our model is designed to predict the flow over steps during training. To ensure efficiency and practicality in sample generation, we perform flow matching within the latent space. 4.2. Base Architecture Multimodal (MM) flow-matching model. To support multimodal generation, we adopt the MMAudio [4] model structure following the MM-DiT block architecture from SD3 [8] and FLUX [24] with multiple streams of modilities and single-modality blocks. This design choice enables us to construct deeper networks without increasing the overall parameter cost, compared to architectures that process all modalities at every layer. This multimodal architecture allows the model to dynamically attend to different modalities based on the input context, thereby enabling efficient joint training on both audio-visual and audio-text datasets. Multimodal conditioning inputs. To incorporate global context into the network, we adopt global conditioning via adaptive layer normalization (adaLN) [35], where global features are injected through learned scale and bias parameters. Specifically, we compute global conditioning vector cg R1D, which is shared across all Transformer blocks with average-pooled visual and text features. To further enhance audio-visual synchrony, we also employ token-level conditioning, allowing the model to adapt more precisely to local variations across modalities. In our implementation, we make use of semantic video representation from CLIP [37], motion-audio synchronized representations from Synchformer [20], and the text representations from CLIP [37]. 4.3. Core Network No positional embeddings by replacing attention modules in single modality blocks. Traditional attention mechanisms in Transformers [48] face significant challenges when applied to long-form audio generation. These modules rely heavily on positional embeddings to compute attention scores between queries and keys. However, such embeddings are typically fixed during training and do not generalize well when the number of tokens changes at inference time, leading to degraded performance on longer sequences. This limitation necessitates more adaptable modeling approach that can handle variable-length inputs without relying on rigid positional encoding schemes. To address this, we adopt the Mamba-2 architecture [6], which inherently supports sequence modeling without explicit positional embeddings. Mamba-2 leverages state-space model formulation, where adaptive tokens provide contextual information to transition matrix, enabling the model to capture temporal dependencies across token sequences more flexibly. This design allows for robust generalization to longer sequences not seen during training, without requiring architectural modifications or extrapolation techniques, as is often necessary with Transformer-based models using rotary positional embeddings (RoPE) [44]. Here, we briefly introduce Mamba-2s parameterization [6] as our basic model in this work. Let xi, yi be the input and output of the state space model (SSM), respectively. The model is parameterized by: R<0, Rn, and Rn. Then, the discretization is written as αℓ = eℓAℓ (0, 1) and γℓ = ℓ. We formally define the SSM in Mamba: hℓ = αℓhℓ1 + γℓBℓxℓ, yℓ = Chℓ with the matrix form as follows: = (cid:16) CB(cid:17) X, (4) (5) where RLL is the structured mask matrix consisting of αℓ, B, RLN , RLD are the SSM parameters and inputs, respectively. Non-Causal Mamba-2 modules. We adopt Non-Causal Mamba-2 [42] for two key reasons: 1) video conditions 4 Figure 3. Overview of our proposed framework. Left: comprehensive end-to-end flow-matching model that operates across both multimodal and single-modal blocks, handling inputs in both compressed and original spaces. Middle: temporal routing mechanism designed to efficiently process tokens in time-aware manner. Right: multimodal routing strategy that leverages strong correlations between the two modalities for enhanced integration. are available offline, eliminating the need for sequential token processing, and 2) multimodal fusion across multiple modalities is difficult without predefined order. The original Mamba-2 [6], being causal, restricts information flow to one direction, requiring multiple passes to integrate modalities and complicating temporal alignment. Non-Causal Mamba-2 addresses these limitations by enabling omnidirectional information flow, allowing global hidden states to combine all modalities simultaneously without constrained by scanning orders. Non-Causal Mamba-2 also mitigates modulation decay, common issue in causal models where conditioning signals weaken over time providing more robust and flexible foundation for multimodal fusion in longform generation tasks [52]. The key distinction between causal and non-causal Mamba-2 lies in the formulation of the structured mask matrixM . In causal Mamba, incorporates the product of transformation matrices across the sequence, expressed as Aℓ:i = (cid:81)ℓ Ai. This sequential multiplication leads to decaying [52] over long sequences. In contrast, non-causal Mamba defines using the inverse of each transformation matrix: = 1 . By avoiding cuAi mulative products over time, non-causal Mamba does not experience the same decay phenomenon, making it more stable for long-range dependencies. 4.4. Hierarchical Framework Long video and audio recordings often include significant amount of redundant information, which can lead to inefficiencies when processing with large number of tokens, especially in tasks involving multimodal alignment. To address this challenge, we propose hierarchical framework designed to selectively route only the most important tokens to the main processing network, thereby reducing computational load while preserving critical information. For example, in the case of audio streams, we implement temporal routing that focuses on identifying the specific timeframes where sound events actually occur. This approach effectively filters out redundant audio data, which is especially useful in scenarios where audio and video streams need to be synchronized, as these streams often contain overlapping or repetitive content. Furthermore, for multimodal processing, we introduce multimodal (MM) routing mechanism that selects key tokens based on high similarity between the two modalities e.g., audio and visual data. This selective routing ensures that only the most relevant and informative tokens are passed forward, facilitating more efficient and accurate multimodal alignment. Routing mechanism. We define routing mechanism based on similarity between two sets of tokens RLD and RLD, with the similarity function defined as: sim(q, k) = qk qk , (6) where this similarity function is used in temporal routing and MM routing layers. Temporal routing layers. In temporal data e.g., audio and video events, the boundaries occur when there are contextual shifts between sound events. Based on this observation, 5 we opt to mask tokens that have high similarities and keep the tokens that contain distinct temporal information. Let qℓ = Wqxℓ and kℓ = Wkxℓ, we use cosine similarity in computing token selection: pℓ = (cid:16) 1 1 sim(qℓ, kℓ1) (cid:17) . (7) MM routing layers. Multimodal alignment between one and another modality (i.e., and ) might experience deteriorating behavior due to large number of tokens to be processed. Selected important tokens for feed forwarding to main networks are tokens with high similarity to the referenced modality. For instance, synchronized audio-visual (i.e. Synchformer [20]) features could be used to align with text condition. Let qMℓ = WqxMℓ and kM = WkxM ℓ , we compute MM routing as follows: ℓ pℓ = (cid:16) 1 2 1 + sim(qMℓ, kM ℓ (cid:17) ) . (8) We only process tokens with bℓ = 1{sim(qℓ,kℓ )0.5}. As conditions are from the pretrained models (e.g., Synchformer [20], visual CLIP [37], and text CLIP [37] ), we expect higher probability for token matching scores (i.e., > 0.5). Chunking with downsampling. The downsampler compresses encoder outputs xs into reduced set of vectors xs+1 using boundary indicators {bs,ℓ}Ls ℓ=1. Among potential compression strategies, we adopt direct selection of boundary-marked vectors because of simplicity and effectiveness as suggested in HNet [18]. Dechunking with upsampling. After the tokens are processed through the main network, we could obtain output tokens x. The upsampler is specifically designed to decompress tokens of smaller size back to their original dimensions, enabling more details processing in the later stages. We define the dechunking with upsampling as follows: aℓ = pbℓ ℓ (1 pℓ)1bℓ = (cid:40) if bℓ = 1, pℓ, 1 pℓ, otherwise. (9) (10) Then, we make use of Straight-Through Estimator (STE) [1], allowing gradient flow and stop for selected and unselected tokens STE(aℓ) = aℓ + stopgrad(1 aℓ), and the output tokens at position ℓ could be expressed xℓ = x(cid:80)ℓ . Next, the upsampling function can be defined as: Upsampler( x, a)ℓ = STE(aℓ) xℓ. k=1 bk 5. Experiments Settings. In our evaluation of long-form audio generation capabilities, we adopt methodology where the model is initially trained using audio clips of fixed, relatively short duration, specifically, segments lasting 8 seconds. After this training phase, we rigorously test the models ability to generalize by presenting it with much longer audio sequences, each exceeding the original 8-second length. We set multimodal blocks = 5 and single modal blocks = 4 for the small version (S), and we use = 10 and = 7 for the large version (L). Please see our supplementary materials for the detail architecture and setup, Datasets. We train on VGGSound [2] on 8 second audiovideo data and several text-to-audio datasets. This datasets have been widely used by our comparing methods. In our experiments, we evaluate on UnAV100 [10] and LongVale [11] for comparing with the state-of-the-arts on LV2A generation. The test set of UnAV100 consists of 2K videos with durations of 10-60 seconds, and LongVale has around 1K test videos ranging from 10 to 500 seconds. For completion, we also evaluate on the VGGSound dataset. Baselines. To demonstrate the effectiveness of our approach in LV2A scenarios, we compare it against LoVA [5], recent method specifically designed for LV2A tasks. Additionally, we evaluate our method against the original MMAudio [4], incorporating frequency scaling of positional embeddings based on the given durations and Neural Tangent Kernel (NTK) [46]. From conceptual standpoint, autoregressive models are inherently capable of generating longer video sequences by leveraging context window shifts. To assess this capability, we include comparison with V-AURA [49]. We also compare with recent V2A model so-called HunyuanVideo-Foley [40]. Evaluation on audio-video forms. We evaluate our model across four key dimensions: distribution matching, audio quality, semantic consistency, and temporal synchronization. Previous metrics are only feasible for relatively short audio duration. In our experiments, we conduct an evaluation based on multiple chunks of the audio to match the duration on which the pretrained classifier models are trained. This is to reduce errors where the classifier models cannot directly be applied to long audio-video forms. Distribution matching. To measure how closely the generated audio matches the statistical properties of real audio, we compute the Frechet Distance (FD) and KullbackLeibler (KL) divergence using established audio embedding models. Specifically, we report FD scores using VGGish [9] (FDVGG), PaSST [23] (FDPaSST), PANNs [22] (FDPANNs). PaSST operates at 32 kHz and produces global features, while PANNs and VGGish operate at 16 kHz, with VGGish processing non-overlapping 0.96-second segments. KL divergence is computed using PANNs (KLPANNs) and PaSST (KLPaSST) as classifiers. Audio quality, semantic consistency, and temporal synchronization. We assess the standalone quality of generated audio using the Inception Score (IS), with PANNs [22] UnAV100 Method Size Distribution Matching FDVGG FDPANNs FDPASST KLPASST Audio Quality ISCPANNs ISCPASST MM Align. Temporal Align. IB-Score DeSync MMAudio-S [4] MMAudio-L [4] MMAudio-L + NTK [4] LoVA [5] V-AURA [49] HunyuanVideo-Foley-XXL [40] 5.13B MMHNet - (ours) MMHNet - (ours) 157M 4.62 3.86 1.03B 4.08 1.03B 1.06B 3.36 695M 4.57 4.89 157M 3.35 1.80 1.09B 10.69 9.01 8.43 7.50 6.16 10.28 5.87 5.29 349.11 296.79 301.45 223.29 321.64 284.36 217.00 209.06 1.85 1.79 1.76 1.56 1.69 1.80 1.29 1. 5.92 6.18 6.05 6.17 7.35 5.60 7.62 8.10 6.02 6.41 6.17 8.46 5.22 6.01 8.21 7.35 28.63 30.71 30.24 24.62 29.36 32.90 36.82 36.27 0.906 0.593 0.599 1.232 1.191 0.757 0.439 0.410 LongVale Method Size Distribution Matching FDVGG FDPANNs FDPASST KLPASST Audio Quality ISCPANNs ISCPASST MM Align. Temporal Align. IB-Score DeSync 157M 8.86 MMAudio-S [4] 7.20 1.03B MMAudio-L [4] 6.41 1.03B MMAudio-L + NTK [4] 7.62 1.06B LoVA [5] V-AURA [49] 695M 6.46 HunyuanVideo-Foley-XXL [40] 5.13B 14.56 157M 3.35 MMHNet - (ours) 3.23 1.09B MMHNet - (ours) 22.23 16.12 13.76 21.81 14.87 28.00 10.10 10.03 550.57 531.55 481.45 527.58 498.74 750.96 323.39 331.75 2.34 2.05 2.05 2.36 1.87 2.58 1.75 1.64 3.27 2.72 2.84 2.46 3.22 2.28 3.68 4.25 3.42 2.59 2.76 3.57 2.67 2.40 3.50 3.20 21.52 21.60 23.43 17.04 19.67 18.75 30.62 30. 0.972 0.678 0.666 1.233 1.282 1.082 0.438 0.465 Table 1. Comparison of methods across various evaluation metrics on UnAV100 [10] and LongVale [11]. VGGSound Method MMAudio-S [4] MMAudio-L [4] LoVA [5] V-AURA [49] MMHNet-S (ours) MMHNet-L (ours) FDVGG ISCPANNs ISCPASST IB-Score DeSync 12.46 13.33 9.91 - 12.52 15.34 18.02 17.40 9.73 10.08 16.73 20. 0.444 0.442 - 0.654 0.460 0.460 32.27 33.22 - 27.64 32.11 32.11 1.66 0.97 1.70 2.88 1.54 2.09 Table 2. Comparison of methods under fixed audio length 10 seconds on VGGSound. Baseline results are based on the reports in [4, 5]. serving as the classifier. To evaluate how well the generated audio semantically aligns with the input video, we use ImageBind [12] to extract cross-modal embeddings. The cosine similarity between visual and audio features is averaged to yield the IB-score. We evaluate audio-visual alignment using the DeSync score, which estimates the temporal offset (in seconds) between audio and video streams. This is computed using Synchformer [20], model trained to predict synchronization errors. We assess alignment over the full 4.8-second context window following the setting in MMAudio [4]. 5.1. Comparison with the state-of-the-arts As shown in Table 1, our proposed model significantly outperforms existing state-of-the-art methods across broad spectrum of evaluation metrics. In particular, the IB-score, which measures the alignment between video and audio, Figure 4. Visualization of audio spectogram from MMHNet and competing methods on UnAV100. demonstrates notable improvement, surpassing recent state-of-the-arts HunyuanVideo-Foley [40] by 3.9 on the UnAV100 dataset. This reflects our models enhanced ability to capture and synchronize multimodal information effectively. Additionally, our method achieves consistently superior desynchronization scores, further emphasizing its robustness in handling complex audio-visual alignment tasks. These results collectively underscore the effectiveness of our approach in addressing real-world chal7 UnAV100 Threshold FDVGG FDPANNs ISCPANNs IB-Score DeSync Core Network FDVGG FDPANNs ISCPANNs IB-Score DeSync Transformers Causal Mamba-2 Non-Causal Mamba-2 3.36 2.28 3.35 9.00 9.18 5.87 6.42 5.85 7. 28.41 33.32 36.82 0.638 0.497 0.439 LongVale Core Network FDVGG FDPANNs ISCPANNs IB-Score DeSync Transformers Causal Mamba-2 Non-Causal Mamba4.63 6.62 3.52 9.72 18.45 10.10 2.95 3.74 3.68 18.65 17.42 30.62 0.700 0.743 0.438 Table 3. Ablation of the core networks of MMHNet by comparing among transformers, Causal Mamba-2 and Mamba-2. Evaluation is performed on UnAV100 and LongVale datasets. UnAV100 Variant Non-Hierarchical Hierarchical Variant Non-Hierarchical Hierarchical FDPANNs FDPASST ISCPANNs ISCPASST IB-Score DeSync 6.31 5.87 264.43 217. 6.58 7.62 LongVale 6.72 8.21 35.00 36.82 0.621 0.439 FDPANNs FDPASST ISCPANNs ISCPASST IB-Score DeSync 11.76 10. 442.12 323.39 2.59 3.68 2.29 3.50 26.34 30.62 0.669 0.438 Table 4. Comparison of non-hierarchical and hierarchical methods. lenges in multimodal synchronization. Moreover, we observe that autoregressive methods (e.g., V-AURA) struggle with length generalization, as evidenced by their comparatively poor performance among recent state-of-the-art techniques. Figure 4 illustrates that previous methods fail to generate sound accurately aligned with the input video frames. On the LongVale dataset, our proposed method consistently outperforms state-of-the-art approaches by substantial margin (0.23 on DeSync scores) compared to the second best performing method as shown in Table 1. Since LongVale contains samples with significantly longer durations (up to 7 minutes), this highlights that previous methods struggle with audio-video alignment and temporal synchronization when handling very long videos. On VGGSound, where training and testing use identical durations, our proposed method performs on par with MMAudio [4], strong baseline, and surpasses it on several key metrics (ISC scores), as shown in Table 2. Please see our supplementary materials for generated samples and additional experiments. 5.2. Analysis and Ablation Study Transformers Vs. Causal Mamba-2 Vs. Non-Causal Mamba-2. We also provide comparison with different types of core networks in Table 3. Transformers are done without positional embeddings attached to the tokens. Causal Mamba-2 [6] runs through tokens sequentially. Then, Non-causal Mamba-2 [42] is used for our case to process long sequences and multimodal tokens more efficiently compared to causal Mamba-2. Hierarchical Vs. Non-Hierarchical methods. We ablate 8 0.3 0.4 0.5 0.6 0.7 3.24 3.52 1.80 3.64 15. 8.13 6.94 5.29 6.91 33.06 7.45 8.61 8.10 8.66 2.81 33.44 35.55 36.27 35.58 0.02 0.460 0.431 0.410 0.426 1.210 Table 5. Comparison of various threshold values on UnAV100. Figure 5. Comparison with past methods on various duration splits of audio-video data on UnAV100 (FDPANNs and IB-Score ). on having the structure of models with tokens in the compressed space with tokens in the original space via routing mechanisms. We observe that the model with compressed space yields better alignment between modalities in long audio generation forms in Table 4. Token selection thresholds. As reported in Tab. 5, we systematically evaluated multiple threshold values to analyze their impact on overall performance. Among the tested settings, threshold of 0.5 consistently produced the strongest results across all evaluation metrics. Performance across various durations. We provide some analysis of different time durations to see the length generalization capability of our proposed method against the state-of-the-art method in V2A generation tasks (e.g., MMAudio [4]). We show that past methods (e.g., MMAudio) fail to consistently maintain the performance across It is shown that FDPANN scores are different durations. plummeting to 3.5 points across video durations from 10 to 60 seconds, while our MMHNet can maintain the performance well. Also, MMHNet outperforms past methods (e.g., V-AURA and LoVA) in LV2A across durations as shown in Figure 5. 6. Conclusions This paper presents hierarchical method so-called MMHNet, novel framework for long-form video-to-audio generation that tackles the challenge of length generalization, training on short clips while generating highquality, contextually aligned audio for much longer videos. MMHNet combines hierarchical modeling with NonCausal Mamba-2 architecture to overcome limitations of transformer-based models that rely on positional embeddings and struggle with long sequences. Hierarchical token routing and dynamic chunking efficiently align multimodal inputs (video, text, audio) while reducing complexity, and non-causal modeling ensures robust generalization."
        },
        {
            "title": "References",
            "content": "[1] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic arXiv preprint neurons for conditional computation. arXiv:1308.3432, 2013. 6 [2] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. 6 [3] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. 2, 3 [4] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Mmaudio: Taming multimodal joint training for high-quality video-toaudio synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2890128911, 2025. 1, 2, 3, 4, 6, 7, 8 [5] Xin Cheng, Xihua Wang, Yihan Wu, Yuyue Wang, and Ruihua Song. Lova: Long-form video-to-audio generation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 2, 6, 7 [6] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. 2, 3, 4, 5, 8, [7] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736740. IEEE, 2020. 1 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 4 [9] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Ryan Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 776780. IEEE, 2017. 6 [10] Tiantian Geng, Teng Wang, Jinming Duan, Runmin Cong, and Feng Zheng. Dense-localizing audio-visual events in untrimmed videos: large-scale benchmark and baseline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2294222951, 2023. 1, 2, 3, 6, 7 [11] Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audiolanguage-event benchmark towards time-aware omni-modal perception of long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 18959 18969, 2025. 1, 2, 6, [12] Rohit Girdhar, Alexander Kirillov, Mathilde Caron, Ross Imagebind: arXiv preprint Girshick, Piotr Dollar, and Ishan Misra. One embedding space to bind them all. arXiv:2305.05665, 2023. 7 [13] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2 [14] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 39914008, 2024. 3 [15] Ali Hatamizadeh and Jan Kautz. Mambavision: hybrid mamba-transformer vision backbone. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2526125270, 2025. [16] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: dit-style zigzag mamba diffusion model. In Arxiv, 2024. 2 [17] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced textto-audio generation, 2023. 1 [18] Sukjun Hwang, Brandon Wang, and Albert Gu. Dynamic chunking for end-to-end hierarchical sequence modeling. arXiv preprint arXiv:2507.07955, 2025. 2, 6 [19] Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. arXiv preprint arXiv:2110.08791, 2021. 1, 2 [20] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 53255329. IEEE, 2024. 2, 4, 6, 7, 1 [21] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36: 2489224928, 2023. [22] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. In IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 28802894. IEEE, 2020. 6 [23] Khaled Koutini, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. 6 [24] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3, 4 [25] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: universal neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022. 1 [26] Boyu Li, Haobin Jiang, Ziluo Ding, Xinrun Xu, Haoran Li, Dongbin Zhao, and Zongqing Lu. Selu: Self-learning embodied mllms in unknown environments. arXiv preprint arXiv:2410.03303, 2024. 1 [27] Sizhe Li, Yiming Qin, Minghang Zheng, Xin Jin, and Yang Liu. Diff-bgm: diffusion model for video background music generation. In CVPR, 2024. 1 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3, 4 [29] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3, 4 [30] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36:4885548876, 2023. 1, 2 [31] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, and Wenwu Wang. WavCaps: ChatGPT-assisted weaklylabelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 115, 2024. 1 [32] Shentong Mo, Jing Shi, and Yapeng Tian. Text-toaudio generation synchronized with videos. arXiv preprint arXiv:2403.07938, 2024. 2 [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2 [34] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. 2 [35] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 4 [36] Mark D. Plumbley, Thomas Blumensath, Laurent Daudet, Remi Gribonval, and Mike E. Davies. Sparse representations in audio and music: From coding to source separation. Proceedings of the IEEE, 98(6):9951005, 2010. 2 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 2, 4, 6, [38] Koichi Saito, Julian Tanke, Christian Simon, Masato Ishii, Kazuki Shimada, Zachary Novack, Zhi Zhong, Akio Hayakawa, Takashi Shibuya, and Yuki Mitsufuji. Soundreactor: Frame-level online video-to-audio generation. arXiv preprint arXiv:2510.02110, 2025. 2 [39] Siavash Shams, Sukru Samet Dindar, Xilin Jiang, and Nima Mesgarani. Ssamba: Self-supervised audio representation In 2024 IEEE learning with mamba state space model. Spoken Language Technology Workshop (SLT), pages 1053 1059. IEEE, 2024. 2 [40] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideofoley: Multimodal diffusion with representation alignment for high-fidelity foley audio generation, 2025. 2, 6, 7 hear your true colors: Im- [41] Roy Sheffer and Yossi Adi. In ICASSP 2023-2023 IEEE age guided audio generation. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 1, 2, 3 [42] Yuheng Shi, Minjing Dong, Mingjia Li, and Chang Xu. Vssd: Vision mamba with non-causal state space duality. arXiv preprint arXiv:2407.18559, 2024. 3, 4, 8, [43] Christian Simon, Masato Ishii, Akio Hayakawa, Zhi Zhong, Shusuke Takahashi, Takashi Shibuya, and Yuki Mitsufuji. Titan-guide: Taming inference-time alignment for guided the text-to-video diffusion models. IEEE/CVF International Conference on Computer Vision (ICCV), pages 1666216671, 2025. 2 In Proceedings of [44] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transarXiv preprint former with rotary position embedding. arXiv:2104.09864, 2021. 4 [45] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [46] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Proc. NeurIPS, pages 75377547. Curran Associates, Inc., 2020. 2, 6 [47] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2, 4 [49] Ilpo Viertola, Vladimir Iashin, and Esa Rahtu. Temporally In ICASSP aligned audio for video with autoregression. 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. 2, 3, 6, 7 [50] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video Understanding, 2024. arXiv:2412.20504 [cs]. 2 [51] Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation network with 10 rectified flow matching. Advances in Neural Information Processing Systems, 37:128118128138, 2024. [52] Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan Kautz, Pavlo Molchanov, and Yingyan Celine Lin. Longmamba: Enhancing mambas long context capabilities via training-free receptive field enlargement. arXiv preprint arXiv:2504.16053, 2025. 5 [53] Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation. In ECCV, 2024. 1 [54] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 1, 2 [55] Yehang Zhang, Xinli Xu, Xiaojie Xu, Li Liu, and Yingcong Chen. Long-video audio synthesis with multi-agent collaboration. arXiv preprint arXiv:2503.10719, 2025. 1, 3 11 Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide the details of experimental settings, our proposed method, and additional results. 7. Datasets and Settings Training datasets. As stated in the main paper, our primary video-to-audio dataset is VGGSound, which serves as the core resource for training and evaluation. To further enhance the capability of our model, we incorporate additional training using text-to-audio datasets, specifically WavCaps [31] and Clotho [7]. These supplementary datasets provide rich textual descriptions paired with audio, enabling the model to learn from diverse textual cues. It is important to emphasize that, when leveraging these datasets, we only utilize the textual information to complement audio generation, without introducing any extra visual context. This approach ensures that the improvements gained from these resources stem solely from text-based learning rather than multimodal inputs (i.e., visual cues). Evaluation datasets. For the UnAV100 benchmark, we utilize the official test set provided by UnAV100 in its original form, without introducing any modifications. During the evaluation phase, captions are deliberately withheld for all instances within this set. This design ensures that the task remains strictly focused on video-to-audio generation, eliminating any dependency on textual inputs and thereby preserving the video-to-audio evaluation setting. For the LongVale datasets, the original evaluation sets predominantly consist of short video clips, many of which have audio segments shorter than one minute. To address this limitation and create more balanced evaluation scenario, we selectively sample additional videos from the training split of LongVale [11] and eliminate short videos from the original test set. These selected videos are incorporated into the evaluation set to increase diversity and length. As result of this augmentation, the final evaluation set comprises around 1K videos, each averaging approximately 45 seconds in duration. This adjustment ensures more representative and robust evaluation for tasks involving video-to-audio generation. 8. The Details of MMHNet Flow matching. We use flow matching in our proposed approach. To be specific, we use 25 training steps and apply the same for inference. We train with learning rates of 1e-4 with the AdamW optimizer for 200K iterations. Temporal synchronization features. The temporal synchronization feature is encoded using Synchformer model [20]. 1D convolutional layer (kernel size = 7, padding = 3) is employed to project the input into hidden representation, followed by SELU activation function [26]. Subsequently, ConvMLP layer with kernel size of 3 and padding of 1 is applied. Visual and text semantic features. Semantic visual and textual features are encoded using the CLIP model [37] to capture cross-modal representations. The subsequent projection layer incorporates ConvMLP block with kernel size of 3 and padding of 1, enabling local spatial interactions while preserving the original sequence length. Audio features. 1D convolutional layer (kernel size = 7, padding = 3) is utilized to project the input into hidden representation, followed by SELU activation function [26]. Subsequently, ConvMLP layer with kernel size of 7 and padding of 3 is applied. Audio Variational Auto Encoder (VAE). As described in the main paper, audio latents are obtained by first applying short-time Fourier transform (STFT) to the input audio and extracting the magnitude component as mel spectrograms. We use 44 kHz audio with latent frame rate of 43.07. The mel bins, FFT size, hop size, and window size are set to 128, 2048, 512, and 2048, respectively. These spectrograms are then encoded into latent representations using pretrained VAE. During inference, the generated latents are decoded back into spectrograms via the VAE and subsequently converted into audio waveforms using pretrained Vocoder, such as BigVGAN-V2 [25]. For the VAE architecture, we adopt the 1D convolutional design from Make-An-Audio 2 [17], employing downsampling factor of 2. Non-Causal Mamba-2. For the Non-Causal Mamba component, we adopt VSSD [42] as the primary building block. This module largely follows the architectural principles of Mamba-2 [6], but with key distinction: the computation is performed in non-sequential manner. By removing the strict sequential processing constraint, the model can process multiple tokens simultaneously and capture global view of the entire token sequence. 9. Additional Experiments We conduct further ablation study to observe the performance gain of each specific module in our proposed model. Note that we conduct this ablation study using the small version of MMHNet. Ablation on routing strategies. We ablate on having the 1 Figure A6. Visualization of heatmaps for activation matrices in Causal Mamba-2 and Non-Causal Mamba-2 within MMHNet: (a) Causal Mamba-2, used as Transformer replacement, shows activation scores in the transition matrix that gradually decay during extended audio generation (up to 5 minutes). (b) Non-Causal Mamba-2 maintains visible activation scores in the transition matrix prior to routing. (c) After routing, the transition matrix becomes more pronounced in the compressed representation space. UnAV100 Variant No Routing Temp. Routing Temp. + MM Routing FDPANNs FDPASST ISCPANNs ISCPASST IB-Score DeSync 6.31 6.57 5.87 264.43 214.01 217.00 6.58 7.06 7.62 6.72 7.09 8.21 35.00 33.82 36. 0.621 0.474 0.439 Table A6. Ablation study on routing strategies using the UnAV100 dataset. UnAV100 FDPANN FDPASST ISCPANN ISCPASST IB-Score DeSync W/o Pos. Emb. W/ Pos. Emb. 3.24 3. 220.37 217.00 7.76 7.62 8.30 8.21 36.75 36.82 0.425 0.439 Table A7. We compare our proposed approach with and without positional embeddings applied on input conditions. structure of temporal and MM routing in our proposed network structure as shown in Table A6. We observe that the model with temporal routing mechanism could improve DeSync scores, which are related to temporal synchronization between audio and visual modalities. Ablation on additional position embeddings for the temporal sync. condition. Beyond the current framework, we also conducted an experiment to assess the impact of positional embeddings. Specifically, we examined whether removing them would degrade performance and whether our design choice could be justified. As shown in Table A7, the use of positional embeddings has minimal impact on overall performance. Analysis on Causal-Mamba and Non-Causal Mamba attention maps. Figure A6 illustrates the activation maps of the transition matrix of Mamba-2 across all tokens, taken from the first single-modal layer. From these visualizations, we observe that the activation scores in Causal Mamba-2 exhibit noticeable decay as more tokens are processed. 2 Specifically, the activations are concentrated within the initial segment of the sequence, primarily spanning the first 250300 tokens, which corresponds to approximately 10 seconds of audio. This pattern suggests that the models attention is biased toward early tokens, with diminishing influence on later tokens. Running time. We evaluated the time required to convert long videos into audio across multiple samples. Our proposed method achieves speed improvement in the wall clock time compared to MMAudio [4], despite sharing similar MMDiT-like architecture. For example, our approach with large version can generate 500 seconds of audio in approximately 60 seconds, whereas MMAudio takes about 120 seconds for the same task, up to 2 improvement. All measurements were conducted on an H100 GPU with 80GB memory. Similarity metrics. We also evaluated alternative similarity metrics, but, as shown in Tab. A8, they consistently underperform cosine similarity across most evaluation measures. This behavior is expected. Our routing mechanism relies on CLIP-based condition encoders, and CLIP is explicitly trained with cosine-similarity objective. Using mismatched distance metric would fundamentally misalign with the geometry of the CLIP embedding space and degrade token selection. Consequently, cosine similarity is the only principled and effective choice for our routing mechanism. Distance Metric FDVGG FDPANNs ISCPANNs IB-Score DeSync Euclidean Dot Product Cosine Similarity 35.48 35.39 36.27 0.433 0.424 0. 8.80 8.57 8.10 6.76 7.03 5.29 3.53 3.69 1.80 Table A8. UnAV100. Comparison with different distance metrics on UnAV100 Variant FDPANNs FDPASST ISCPANNs ISCPASST IB-Score DeSync 7.70 CFG=2.0 6.88 CFG=3.0 8.10 CFG=4.0 8.40 CFG=5.0 8.26 CFG=6. 241.36 257.02 209.06 183.86 224.04 0.586 0.571 0.410 0.412 0.435 33.08 28.91 36.27 36.77 35.14 6.5 7.33 5.29 5.80 6.75 7.16 6.30 7.35 7.30 7.26 Table A9. Analysis on different CFG scores. Performance across different hyperparameters. We analyze different classifier-free guidance (CFG) values to identify the optimal setting for achieving the best results, as shown in Table A9. Based on this evaluation, we use CFG value of 4.0 as the hyperparameter across all experiments."
        }
    ],
    "affiliations": [
        "Sony AI",
        "Sony Group Corporation"
    ]
}