{
    "paper_title": "Z1: Efficient Test-time Scaling with Code",
    "authors": [
        "Zhaojian Yu",
        "Yinghao Wu",
        "Yilun Zhao",
        "Arman Cohan",
        "Xiao-Ping Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research."
        },
        {
            "title": "Start",
            "content": "Z1: Efficient Test-time Scaling with Code Zhaojian Yu1, Yinghao Wu1, Yilun Zhao2, Arman Cohan2, Xiao-Ping Zhang1 1Tsinghua University 2Yale University"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., <think>. . . </think>) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.1 5 2 0 2 1 ] . [ 1 0 1 8 0 0 . 4 0 5 2 : r Figure 1: Comparing Z1 with R1-like models on context window (a) and test-time scaling (b). Z1 models exhibit more efficient test-time compute scaling than R1-Distill-7B, with its shifted thinking window. Z1-7B exhibits efficient test-time scaling across 3 different reasoning tasks (LiveCodeBench, MATH500, GPQA Diamond) and matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. 1Our model, data, and code are open-source at https://github.com/efficientscaling/Z1 Preprint."
        },
        {
            "title": "Introduction",
            "content": "Large Reasoning Models (LRMs), such as OpenAI o1 [1] and DeepSeek R1 [2], have demonstrated remarkable advances in complex reasoning tasks through test-time compute scaling [3], particularly in competitive mathematics and programming. These models, trained with large-scale Reinforcement Learning (RL) [4, 5], have emerged step-by-step reasoning abilities to solve complex problems effectively. However, the elaborate reasoning process also leads to super long contexts and numerous thinking tokens, challenging the efficient utilization of LRMs. Existing open-source works, such as s1 [6] and LIMO [7], train non-reasoning models into reasoning models with manually curated problems and distilled long chain-of-thought (CoT) [3] trajectories yet do not address the challenges posed by test-time compute scaling with respect to long contexts and an excessive thinking tokens. For example, s1 [6] introduces budget forcing, which either appends ending words to truncate reasoning processes or extrapolation words (e.g., \"wait\") to inspire the model to continue thinking, thereby precisely controlling context length. While extrapolation words can control model reasoning, direct truncations may disrupt the models thinking process, consequently degrading performance. Thus, we propose the problem: \"Is there an efficient test-time scaling way to reduce the models thinking tokens consumption while preserving its reasoning performance?\". In this paper, we implement efficient test-time scaling with code-related trajectory fine-tuning and present the shifted thinking mode of LRMs: weak reasoning to simple problems, strong reasoning to complex problems, which significantly reduces the thinking tokens consumption of LRMs in problem solving. Specifically, we create the Z1-Code-Reasoning-107K dataset, comprising 107K simple and complex code-related problems paired with their reasoning traces distilled from the QwQ-32B-Preview [8] model. We train Qwen2.5-Coder-7B-Instruct [9] into reasoning model, with this 107K code-related long and short trajectory dataset. We eliminate the context split with delimiters (e.g., <think>...</think>) and introduce novel Shifted Thinking Window: (I) For simple problems, the models generate solutions with in low reasoning token computation. (II) For complex problems, we cap the thinking tokens; if the model outputs exceeds this threshold, we append hint phrase to the end of reasoning trace, forcing it to produce an answer based on the existing thought process. Shifted Thinking Window enables model that get trained with long and short trajectories to adjust their reasoning level as the complexity of problems, thereby avoiding the overthinking of LRMs. Fine-tuned on long and short reasoning trajectories and equipped with the Shifted Thinking Window, our model Z1-7B exhibits efficient test-time scaling across different reasoning tasks and matches R1-Distill-Qwen-7B performance with about 30% (as shown in Figure 1) its average thinking tokens. Furthermore, we conduct data ablation experiments to identify the critical factors driving reasoning elicitation. We design three greedy dataset sampling strategies and train Qwen2.5-Coder-7B-Instruct with these subsets, highlighting two crucial factors (Mean Trajectory Length and Training Sample Size) in trajectory dataset: (1) longest-greedy sampling, which prioritizes the longest token-length samples to ensure the inclusion of the most extended reasoning traces; (2) shortest greedy sampling, which selects the shortest token-length samples; and (3) random sampling under different sample sizes (16K, 64K). Our results reveal that the model trained on the longest-greedy subset exhibits the best performance, underscoring the importance of trajectory length in training dataset for efficient test-time scaling. We also investigate model performance across varying training sample sizes, by comparing the subsets (16K, 64K, full) of different sizes. Experimental results demonstrate that the full Z1-Code-Reasoning-107K dataset outperforms all smaller subsets, highlighting the significance of dataset size for reasoning model fine-tuning. In summary, our contributions are threefold: (1) We implement efficient test-time scaling with code and successfully generalize it to reasoning tasks beyond code. (2) We propose the shifted thinking window, method that prevents overthinking by adapting to the complexity of reasoning tasks, significantly improving thinking efficiency of reasoning models. (3) Through data ablation studies, we dissect reasoning datasets with code and identify key factors for effective reasoning elicitation."
        },
        {
            "title": "2 Z1: Efficient Test-time Scaling with Code",
            "content": "As illustrated in Figure 2, to achieve efficient test-time scaling, we train non-reasoning models of code-related reasoning trajectories with varying lengths and introduce shifted thinking window to replace the context split with delimiters (e.g., <think>...</think>). After supervised fine-tuning (SFT), 2 Figure 2: Overview of Z1 training and inference. Fine-tuned with long and short trajectory data, Z1 could solve simple and complex problems in shifted thinking window efficiently. our model could adjust its reasoning level according to the complexity of the input problem. In this section, we detail our methodology, including the creation of the reasoning dataset with code (Section 2.1) and the implementation of the shifted thinking window (Section 2.2). 2.1 Dataset Creation We create an efficient test-time scaling dataset that integrates both short reasoning trajectories for simple problems and strong reasoning trajectories for complex problems, emphasizing diverse reasoning trajectory lengths in the training set. However, existing reasoning trajectory datasets predominantly feature complex problems with long chains of thought (CoT), posing challenge for training efficient reasoning models due to the lack of short and straightforward trajectories. To address this, we approach the problem from the perspective of evolving question complexity and select problems from Code Evol-Instruct dataset [10], which evolves in depth and breadth to cover wide range of complexities and has proven effective in non-reasoning models [11]. We generate reasoning trajectories using the QwQ-32B-preview model and truncated the trajectories length to 8192 tokens, removing approximately 3% of samples with repetitive reasoning processes to mitigate excessive cyclic thinking in the training data. The remaining reasoning trajectories, paired with Figure 3: The comparison between Z1-Code-Reasoning-107K and OpenThoughts-114K. We computed the top-50 most frequent trigrams in both datasets. Circle size reflects word frequency, with larger circles indicating higher frequencies. 3 their problems, constitute the Z1-code-reasoning-107K dataset, with less than 1% of the data being truncated. To further analyze our dataset, we compare the top-50 trigram word frequencies of Z1-CodeReasoning-107K with OpenThoughts-114K [12] dataset2. Z1-code-reasoning-107K exclusively contains code-related reasoning trajectories, while OpenThoughts-114K is reasoning dataset distilled from DeepSeek R1, featuring 114K high-quality examples spanning math, science, code, and puzzles. Figure 3 illustrates trend in the word frequency distributions of Z1-code-reasoning-107K and OpenThoughts-114K: high-frequency trigrams exhibit homogeneity, while mid-frequency trigrams show differentiation. For example, the high-frequency trigrams in both datasets (e.g., \"I need to,\" \"we need to,\" \"the number of\") indicate the models summarization of the next reasoning step, which highlights the commonality between code trajectory and other complex problems. In contrast, mid-frequency trigrams in Z1-code-reasoning-107K, such as \"iterate through the\" and \"for each,\" capture loop-based logic characteristic of code-related trajectories, distinct from the mathematical logic exemplified by trigrams (e.g., \"a+b\", \"equal to the\") in OpenThoughts-114K. This underscores the unique reasoning patterns inherent in code-related trajectories. 2.2 Shifted Thinking Window To enforce \"think-before-answer\" pattern, existing LRMs like DeepSeek R1 typically use delimiters (e.g., <think>...</think>) to split the context window into two parts, where the model first reasons in the thinking window and then outputs the final answer in the answering window. However, this pattern often introduces unnecessary reasoning when processing simple problems that do not require deep thought. In the training and inference of our model, we eliminate this context split, allowing the model to flexibly fine-tune and generalize across short and long trajectories and avoiding the overthoughts for simple problems. We refer it to Shifted Thinking Window, where the models context window is not rigidly divided into two parts by delimiters but instead of shifted window: (I) For simple problems, the model fine-tuned on both short and long trajectories can directly output concise reasoning and answers within the context. (II) For complex problems, we cap the maximum thinking length within which the model can either reason or provide an answer; if the reasoning trajectory exceeds this maximum length, the end of the models output will be appended with hint phrase to enforce direct answer. The essence of shifted thinking: weak reasoning for simple problems and strong reasoning for complex problems significantly reduces unnecessary reasoning by model, thereby demonstrating more efficient test-time compute scalings."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Implementation Details Following the previous work [6], we take model that has already been pretrained and instruction tuned and further finetune it for reasoning. Specifically, we select Qwen2.5-Coder-Instruct series models, which have already achieves good performance on various code-related benchmarks. For all training samples, we avoid using delimiters (e.g., <think>...</think>) to separate the whole trajectory into thinking part and the answering part. This adjustment allows the LRM to avoid mandated overthinking, enabling more automatic and efficient test-time scaling: weak reasoning for simple problems and strong reasoning for complex problems. We perform supervised finetuning on Qwen-2.5-Coder-7B-Instruct [9] using our Z1-Code-Reasoning107K dataset, yielding Z1-7B. We do not compute loss on questions, only on reasoning trajectories and solutions. For fine-tuning hyperparameters, we train our model with learning rate of 1e-5 warmed up linearly for 100 steps and then decayed over the rest of training (836 steps in total training) following cosine schedule. We train all the models in bfloat16 precision with Pytorch Fully Shard Data Parallel (FSDP) and set global batch size to 128 for 2 epochs using 8 NVIDIA A100-80G GPUs. In addition, all other settings not mentioned in this paper follow the default values of Huggingface Trainer3. 2https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k 3https://huggingface.co/docs/transformers/main_classes/trainer Table 1: Results on 4 benchmarks. We evaluate Z1 models with shifted thinking window. For other models without reported score, we budget force it by adding \"the final answer is:\". Data Source MATH 500 GPQA Diamond LiveCode Bench BigCode Bench-Hard AVG Model o1-preview o1-mini N/A N/A API only 85.5 90. 73.3 60.0 Open Weights Deepseek-R1 R1-Distill-Qwen-32B R1-Distill-Qwen-7B QwQ-32B-Preview N/A R1/800K R1/800K N/A 97.3 94.3 83.3 90.6 71.5 62.1 49.1 60. Non-reasoning Model Deepseek-V3 GPT-4o-0513 Qwen2.5-Coder-7B-Ins N/A N/A N/A 90.2 75.8 68.6 59.1 46.5 37.4 Open Weights and Data Sky-T1-32B-Preview s1.1-7B OpenThinker-7B Z1-7B QwQ/17K R1/1K R1/114K QwQ/107K 88.6 79.2 83.0 76. 56.8 31.8 42.4 47.5 43.2 53.7 77.9 - 40.5 59.9 56.3 43.4 32.3 - 15.2 25. 35.3 23.0 27.7 29.7 23.6 3.4 25.0 27.7 25.0 20.3 26.4 4.7 17.6 22. 56.3 57.9 67.6 - 44.1 58.9 58.3 47.7 39.7 - 31.7 42.1 45.4 3.2 Evaluation Setup Benchmarks We select three representative reasoning benchmarks covering different topics: LiveCodeBench [13] continuously collects new problems over time from contests across three competition platforms, including LeetCode, AtCoder, and CodeForces. Unless otherwise specified, we benchmarks LLMs on such competition-level programming tasks with the latest full set (880 problems until Feb, 2025) of LiveCodeBench v5. GPQA Diamond [14] consists of 198 PhD-level science questions from Biology, Chemistry and Physics. Experts with PhDs in the corresponding domains only achieved 69.7% on GPQA Diamond, which show its inherent difficulty and challenges. MATH500 [15] is benchmark of competition math problems of varying difficulty. Following previous work [2], we evaluate our model on the same subset selected by OpenAI [4]. Alongside the three common reasoning benchmarks, we also incorporate non-reasoning benchmark: BigCodeBench is benchmark mainly focusing on more challenging and practical code generation with complex instructions and diverse function calls. In this section, we adopt the BigCodeBench-Hard-Instruct (148 problems included) subset to evaluate LRM on short trajectory thinking. For all benchmarks, we generate sample for each question with temperature of 0 (greedy) to measure accuracy. Through these benchmarks, we can evaluate the reasoning ability of LLMs from difference perspectives. Baselines We benchmark Z1 against series of top-tier models: OpenAI o1-series models [1]: o1-mini and o1-preview, representing close-source test-time scaling models; Deepseek-R1 series [2]: Deepseek-R1, R1-Distill-Qwen (32B and 7B) and Qwens QwQ-32B-Preview [16], open-weight reasoning models; Sky-T1-32B-Preview [17], s1.1-7B [6], OpenThinker-7B [12], open models with open reasoning data; Deepseek-V3 [18], GPT-4o [19], Qwen2.5-Coder-7B-Instruct [9], four representative non-reasoning models. Our model, Z1, is fully open including weights, reasoning data, and code. We evaluate Z1 using shifted thinking window with maximum thinking tokens of 4,096. For all baseline models, we use the reported results whenever available. If no reported scores are provided, we evaluate the model using budget forcing with the configuration provided. 3.3 Main Result Table 1 presents the results of Z1 and other models on 4 benchmarks, highlighting the following salient observations: 5 Table 2: Z1-Code-Reasoning-107K Data Ablations. We use maximum of around 4,096 thinking tokens for all scores in this table. The Random Sampling method does not alter the average trajectory length of the training samples. Both length-greedy sampling methods (longest and shortest) utilize the same number of training tokens (74M). Subset Full Random Longest Shortest Training Dataset Dataset Size (Samples) Dataset Size (Tokens) Mean Trajectory Length 107K 64K 16K 124M 19M 74M 1,156 1,157 1,159 GPQA Diamond Average Thinking Time LiveCode Bench Average Thinking Time MATH 500 Average Thinking Time AVG Average Thinking Time Evaluation 47.5 2,470 35.3 866 76.4 1,185 53.1 1,507 40.9 1, 32.2 864 72.4 1,046 48.5 1,236 41.9 2,241 34.1 811 74.4 1, 50.1 1,390 33K 74M 2,216 42.4 2,695 32.7 927 77.2 1,229 50.8 1, 90K 74M 807 39.4 1,979 34.1 763 73.8 1,030 49.1 1,257 (1) Z1 models achieve comparable performance level with GPT-4o on benchmarks of complex problems. (Avg. 45.4 vs 47.7) This result highlights the success of test-time scaling with code, where performance improvements are achieved by leveraging extended reasoning traces during inference, rather than solely relying on increased model size. (2) Trained with trajectory data with code, Z1-7B outperforms other 7B-scale language reasoning models. This outcome underscores the effectiveness of our test-time scaling approach, particularly when fine-tuned with code-realted reasoning data. (3) Fine-tuning the model exclusively with code-related reasoning data enables it to generalize across different domains. Z1 models, fine-tuned on large amount of trajectories data with code, displays superior generalization on GPQA Diamond (47.5%) and MATH500 (76.4%). This suggests the effectiveness of code-related trajectory training for language reasoning elicitation. 3.4 Data Ablations To further investigate the critical factors influencing effective reasoning elicitation in training data, we designed an ablation study with random sampling and greedy sampling strategies (as shown in Algorithm 1) and obtain representative subsets with two key factors (Mean Trajectory Length and Training Sample Size) that influences Z1s reasoning elicitation: (1) Random Sampling: Samples are chosen randomly, serving as baseline for comparison. We conduct random sampling with varying sample sizes (16K, 64K) to assess the impact of training samples on efficient test-time scaling. (2) Longest Greedy Sampling: At each step, we select only the samples with the highest token counts, ensuring the subset contains the longest training examples in terms of reasoning traces. (3) Shortest Greedy Sampling : At each step, we select only the samples with the lowest token counts, maximizing the number of samples included in the subset while adhering to the token budget. For all subsets, we calculate the Mean Trajectory Length (MTL) as follows: MTL = 1 (cid:88) i=1 Lengthi(tokens) (1) Where denotes the number of training samples and Lengthi represents the trajectory length (tokens) of the i-th training sample. For all benchmarks, we calculated the Average Thinking Time (ATT) as follows: ATT = 1 n (cid:88) i=1 Lengthi(tokens) 6 (2) Where denotes the number of problems in benchmarks and Lengthi represents the trajectory length of the i-th problem. We fine-tune Qwen2.5-Coder-7B-Instruct on these representative subsets. Table 2 presents the evaluation results of models fine-tuned on different subsets of the training data, highlighting the following observations: Impact of Mean Trajectory Length Under the same training budget of 74M tokens, we sample two subsets with different mean trajectory length using two strategies: longest-greedy and shortest-greedy sampling. As shown in Table 2, the subset sampled via longest-greedy strategy exhibits significantly higher MTL (2,216) compared to the shortest-greedy subset (807). This difference in training trajectory length translates into notable performance distinctions during evaluation. Specifically, the model fine-tuned on the longest-greedy subset demonstrates longer Average Thinking Time (AVG score: 1,617 vs. 1,257) and higher Benchmark Score (AVG score: 50.8 vs. 49.1) compared to the model trained on the shortest-greedy subset. These results underscore the critical role of Trajectory Length in the training set, suggesting that longer trajectories enhance the models capacity for test-time scaling by encouraging more deliberate and extended reasoning during inference. Impact of Training Sample Size To investigate the effect of training sample size, we randomly sample subsets of varying sizes (16K and 64K) from the original 107K dataset and compare the resulting models performance. As shown in Table 2, the model fine-tuned on the full 107K dataset achieves an Average Thinking Time of 1,507 and Benchmark Score of 53.1, outperforming the model trained on the Random-64K subset (1,390 and 50.1, respectively). In contrast, the smallest subset, Random-16K, yields the shortest Average Thinking Time (1,236) and the lowest performance (48.5). This observation indicates that larger training sample size increase the effective thinking time thereby enhancing its overall performance, despite their Mean Trajectory Length remaining nearly identical across the randomly sampled subsets (1,157 for 16K, 1,156 for 64K) and the full dataset (1,159). Algorithm 1 Greedy Sampling (Longest or Shortest) if mode = longest then else if mode = shortest then Find with maximum token count 1: Input: Set of training samples S, token budget B, strategy mode {longest, shortest} 2: Output: Subset based on selected strategy 3: Initialize , total tokens 0 4: while = and < do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end while 16: return end if if + token_count(s) then S {s} + token_count(s) Find with minimum token count end if {s} 3.5 Test-time Scaling Comparison To further compare the reasoning efficiency of Z1 to other models, we analyze the test-time scaling of Z1-7B and R1-Distill-Qwen-7B on three benchmarks: MATH500, GPQA, and LiveCodeBench. We equip Z1-7B with Shifted Thinking Window by imposing cap of different maximum thinking tokens. For R1-Distill-Qwen-7B, We budget force [6] it by adding \"the final answer is:\", since R1-Distill-Qwen-7B cant adapt to the shifted thinking window without long and short trajectory fine-tuning. Figure 4 illustrates the reasoning efficiency of Z1-7B compared to the baseline models, revealing the following key observations: Z1-7B demonstrates more efficient test-time scaling than R1-Distill-Qwen-7B on reasoning tasks, by achieving comparable results with significantly fewer thinking tokens. Z1-7B exhibits 7 Figure 4: Test-time scaling comparison between Z1-7B and R1-Distill-Qwen-7B. R1-DistillQwen-7B cant adapt to the shifted thinking window without long and short trajectory fine-tuning. We budget force it by adding \"the final answer is:\". superior test-time scaling efficiency compared to R1-Distill-Qwen-7B on reasoning tasks, by delivering better performance with significantly fewer thinking tokens. For example, Z1-7B outperforms R1-Distill-Qwen-7B while requiring only half the average thinking time (approximately 2,000+ tokens) on the GPQA Diamond benchmark, whereas R1-Distill-Qwen-7B relies on budget exceeding 4,096 thinking tokens. This underscores Z1-7Bs remarkable efficiency in reasoning-intensive tasks. In contrast, R1-Distill-Qwen-7B demonstrates notably weaker performance at lower average thinking times (ATT), only achieving competitive results when ATT is substantially increased. 3.6 Case Study Figure 5 illustrates problem-solving example involving three models: Qwen2.5-Coder-7B, Z1-7B, and R1-Distill-Qwen-7B. In this instance, we present simple problem, for which the Qwen2.5-Coder7B-Instruct model delivers correct answer accurately. By comparison, R1-Distill-Qwen-7B adopts context split to enforce thinking, requiring extensive deliberation that consumes 1,784 tokens before arriving at solution. This protracted process underscores its inefficiency in optimizing thinking time for simpler tasks. Z1-7B employs the Shifted Thinking Window to effectively bypassing unnecessary overthinking within the given context, demonstrating its advantage for balancing accuracy and efficiency in problem-solving."
        },
        {
            "title": "4 Related Work",
            "content": "Large Reasoning Models OpenAI o1 and o3 series models [1], which get trained with large-scale RL and learn to reason using chain-of-thought [20], have demonstrated strong reasoning ability in various complex downstream tasks with consistent gains from scaling test-time compute. After the release of o1, Deepseek-R1 [2] replicates the performance of o1 through interleaved supervised fine-tuning and reinforcement learning. The R1-Distill series models, fine-tuned on samples distilled from DeepSeek-R1, also achieve test-time scaling through non-reinforcement learning (non-RL) approaches. In the realm of non-RL data distillation training, many open-source work, such as SkyT1 [17], s1 [6], and LIMO [7] have successfully developed competitive reasoning models comparable to o1-preview. For reinforcement learning researches, models like QwQ-32B [8], Kimi-K1.5 [21], and PRIME-7B [22] have matched or even surpassed o1-previews performance. Our model, Z1, finetuned on 107K short and long distilled trajectory samples, employs the Shifted Thinking Window to enable weak reasoning for simple problems and strong reasoning for complex ones. Combining short and long trajectory training data and shifted thinking windows, our approach mitigates overthinking and achieves efficient test-time scaling. Large Language Models for Code The development of large language models (LLMs) for code has undergone significant evolution from pre-trained models such as Codex, StarCoder [23], DeepSeekCoder [24], and Qwen2.5-Coder Base [9] to instruction-tuned variants like WizardCoder [11], WaveCoder [25], and Qwen2.5Coder-Instruct [9]. This evolution, with the advancement of test-time 8 Write Python script to calculate the number of letter and in string. Input: = strawberry Output: : 1, : 3 [Qwen2.5-Coder-7B-Ins] [Z1-7B] [R1-Distill-Qwen-7B] Here is Python script that can solve the problem: def count_letters(s): a_count = s.count(a) r_count = s.count(r) return {a: a_count, r: r_count} = strawberry print(count_letters(s)) In this script, we define function count_letters... [120+ tokens omitted] To solve this problem, we need... [50+ tokens omitted] 1. Approach [94 tokens omitted] 2. Solution Code def count_letters(s): count_a = 0 count_r = 0 for char in s: if char == a: count_a += 1 elif char == r: count_r += 1 print(\"a :\", count_a) print(\"r :\", count_r) 3.Explanation [104 tokens omitted] This approach ensures that we... [28 tokens omitted] <think> Okay, need to write Python script that counts the number of as and rs... [1787 tokens omitted] </think> To solve this problem... [45 tokens omitted] 1. Approach [92 tokens omitted] 2. Solution Code = input().strip() a_count = s.count(a) r_count = s.count(r) print(fa: {a_count}) print(fr: {r_count}) 3.Explanation [100+ tokens omitted] This approach ensures that the solution is... [22 tokens omitted] Figure 5: Example model outputs. We present question that Qwen2.5-Coder-7B-Instruct (left) can correctly answer without requiring deep reasoning. The response from Z1 (middle) avoids excessive additional reasoning. In contrast, R1-Distilled-Qwen-7B generates 1,784 tokens of reasoning after the delimiter. scaling, has led to divergence in model capabilities. On one hand, large reasoning models (LRMs) tailored for competitive programming (e.g., o1-Pro and o1-IOI [26]) have emerged, leveraging chainof-thought (CoT) reasoning to achieve human-level performance in programming contests. On the other hand, LLMs designed for software engineering (SE) tasks, such as Llama3-SWE-RL [27], have been developed to address benchmarks like SWE-Bench [28] and SWE-Lancer [29]. These software engineering-focused LRMs incorporate real-world SE workflows (e.g., Agentless [30]) and reinforcement learning, progressively enabling automated project management. In this work, we demonstrate that efficient test-time scaling with code can mitigate the tendency of LRMs to overthink coding problems. By optimizing thinking and answering strategies, we enhance model performance while reducing computational overhead, offering novel perspective on the future directions of LLMs for code. Our findings contribute to both competitive programming and software engineering applications, bridging the gap between theoretical advancements and practical deployment."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce an efficient test-time scaling method to elicit model reasoning abilities use fewer thinking tokens consumption. We train our Z1 model with long and short code-related trajectory dataset and equip Z1 with shifted thinking window, new approach to enable LRM to perform weak reasoning to simple problems and strong reasoning to complex problems. Trained with long and short trajectories and reasoning with shifted thinking window, Z1 matches state-of-the-art performance with comparable parameters and demonstrates efficient test-time compute scaling on various reasoning benchmarks. Furthermore, our systematic analysis of key factors for efficient reasoning elicitation provides valuable insights for future research, contributing to the development of more advanced and open-sourced reasoning models."
        },
        {
            "title": "References",
            "content": "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [4] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [5] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. [6] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [7] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [8] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [9] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [10] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. [11] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. [12] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [13] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Tianjun Zhang Fanjia Yan, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint, 2024. [14] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [16] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [17] NovaSky Team. Sky-t1: Fully open-source reasoning model with o1-preview performance in $450 budget. https://novasky-ai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. 10 [18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [19] OpenAI. Gpt-4o. 2024. [20] OpenAI. Learning to reason with llms, September 2024. [21] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [22] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. [24] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [25] Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. Wavecoder: Widespread and versatile enhancement for code large language models by instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51405153, 2024. [26] Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. [27] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. [28] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [29] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. [30] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [31] Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces cots. https://huggingface.co/datasets/open-r1/ codeforces-cots, 2025."
        },
        {
            "title": "Appendix Contents",
            "content": "1 Introduction 2 Z1: Efficient Test-time Scaling with Code"
        },
        {
            "title": "2.1 Dataset Creation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2 Shifted Thinking Window . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Experiments 3.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.3 Main Result .",
            "content": "."
        },
        {
            "title": "3.4 Data Ablations",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.5 Test-time Scaling Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.6 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Related Work 5 Conclusion Dataset Details A.1 Comparison of Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Word frequency details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation Details B.1 Prompts for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Test-time Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Examples C.1 Examples on GPQA Diamond . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Examples on LiveCodeBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Examples on MATH500 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 4 4 4 5 6 7 8 8 9 13 13 15 15 15 16 18"
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Comparison of Datasets We analyzed several recent open-source reasoning datasets. Table 3 presents several key characteristics of these datasets, including the number of samples, minimum and maximum token counts, domain, and dataset source. The Z1-Code-Reasoning dataset features shorter reasoning trajectories, which effectively enables our Z1 model to think quickly on simple problems while go into deeper reasoning on more challenging ones. This approach prevents overthinking and makes significant contribution to achieving efficient test-time scaling. Table 3: The list of existing open-source reasoning datasets. Dataset Samples Min. tokens Max. tokens Domain Dataset Source s1 [6] s1.1 [6] CodeForces-CoTs [31] OpenR1-Math-220k4 OpenThoughts [12] Z1-Code-Reasoning 1K 1K 48K 22K 114K 107K 667 923 523 4,307 25 7,850 26,685 25,156 18,611 91, 8,169 General General Gemini 2.0 Flash Thinking Experimental Deepseek R1 Competition Code Deepseek R1 Math General Deepseek R1 Deepseek R1 General Code QwQ-32B-Preview A.2 Word frequency details In Section 2.1 and Figure 3, we analyzed the word frequency statistics of our dataset, Z1-CodeReasoning-107k, in comparison to the previous Openthought-114k dataset. We performed statistical analysis of word frequencies at the triplet level for both datasets, with the specific top 50 word frequencies detailed in Tables 4 and 5. Due to the inclusion of reasoning data with varying trajectory lengths in our Z1-Code-Reasoning-107k dataset, there is noticeable reduction in the overall word count. Additionally, as introduced in Section 2.1, our dataset not only contains common logical reasoning connectives but also incorporates greater proportion of code-related content. Table 4: Word frequency in Z1-Code-Reasoning-107K trigram words count trigram words need to we need to the number of ### explanation 1. ### solution code based on the ### approach 1. this approach ensures to create approach ensures that the sum of list of to solve this solve this problem, this problem, we solution code python problem, we need 72090 70123 53932 37352 33629 31656 30991 30213 30150 28633 28373 28242 27732 26417 26398 25448 25446 to ensure that can use the - for each code python def function that iterate through the you need to be able to need to find how you can final solution to to check if ## final solution ### explanation: 1. keep track of sum of the to calculate the count 18080 17828 15033 14631 14000 13698 13485 13358 13292 13280 13149 13119 13089 12451 12198 12010 Continued on next page 4https://huggingface.co/datasets/open-r1/OpenR1-Math-220k 13 Table 4: word frequency in 107K trigram words count trigram words to find the ensure that the check if the ensures that the for in you can use the function should - if the the length of 23880 make sure that 22804 21960 we can use 21685 20988 20674 20524 20434 need to create to get the need to handle if there are - we use count 11800 11610 11448 11366 11150 11089 11045 11042 Table 5: Word frequency in OpenThoughts-114k trigram words count trigram words the number of we need to the sum of need to in terms of to find the but the problem let me check the answer is need to find for example, if the problem is is equal to in this case, ) and ( sum of the according to the but how to if we can but in the but according to + + and the problem states total number of the problem says at least one let me try the distance from but this is + is less than so the code return your final your final response final response within but let me both sides by this is 603860 the total number equal to the 484418 321257 which is the 188694 173819 169782 169082 155703 155213 151935 151677 146089 142341 139293 134985 133514 123939 119825 wait, but the 116574 but since the 113484 must be 113061 111663 110523 107778 with respect to 106790 the midpoint of the same as such that the the angle between count 106781 106282 104615 98058 97784 97748 97535 94306 91141 90788 89894 89123 89120 89120 87315 86006 85909 85052 83803 81221 81158 80311 80211"
        },
        {
            "title": "B Evaluation Details",
            "content": "B.1 Prompts for Evaluation We use prompts from Qwen2.5-Coder Github for LiveCodeBench 5 and BigCodeBench 6 and the following prompts for MATH500 and GPQA Diamond: Prompts <im_start>system Please reason step by step, and put your final answer within boxed{}. <im_end> <im_start>user {promblem} <im_end> <im_start>assistant B.2 Test-time Values In Section 3.5, we presented the results of Z1-7B and R1-Distill-Qwen-7B on three reasoning benchmarks, highlighting Z1-7Bs more efficient test-time scaling capability. Table 6 provides more detailed account of the experimental results. By varying the maximum number of thinking tokens, we assessed the test-time scaling abilities of both models across different lengths of thinking trajectories. Z1-7B demonstrated strong performance even under tighter constraints on thinking tokens, showcasing its efficient test-time scaling capability. In contrast, R1-Distill-Qwen-7B required more thinking tokens to achieve comparable level of performance. Table 6: Evaluation details on three reasoning benchmarks Model GPQA Avg. think tokens Pass@1 LiveCodeBench Avg. think tokens Pass@1 MATH 500 Avg. think tokens Pass@1 R1DistillQwen7B Z1-7B 512 - 1011 - 1971 3607 256 510 946 1548 2470 18.2 - 19.2 - 26.3 35.4 25.3 28.8 33.3 41.9 47.5 512 - 1023 - 1978 256 502 732 866 - 13.9 - 20.7 - 28.8 40.0 30.3 31.9 35.0 35.3 - 512 719 1016 1337 1760 2541 255 471 707 823 1185 59.8 62.4 72.0 76.0 83.4 87. 37.2 55.8 71.2 74.2 76.4 5https://github.com/QwenLM/Qwen2.5-Coder/blob/main/qwencoder-eval/ instruct/livecode_bench/lcb_runner/prompts/code_generation.py 6https://github.com/QwenLM/Qwen2.5-Coder/blob/main/qwencoder-eval/ instruct/BigCodeBench/model.py"
        },
        {
            "title": "C Examples",
            "content": "C.1 Examples on GPQA Diamond Figure 6: Examples of the comparison between Z1-7B and R1-Distill-Qwen-7B on GPQA. 16 Figure 7: Examples of the comparison between Z1-7B and R1-Distill-Qwen-7B on GPQA. 17 C.2 Examples on LiveCodeBench Figure 8: Examples of the comparison between Z1-7B and R1-Distill-Qwen-7B on LiveCodeBench. 18 Figure 9: Examples of the comparison between Z1-7B and R1-Distill-Qwen-7B on LiveCodeBench. 19 C.3 Examples on MATH500 Figure 10: Examples of the comparison between Z1-7B and R1-Distill-Qwen-7B on MATH500. 20 Figure 11: Examples of the comparison between Z1-7B and R1-Distill-Qwen-7B on MATH500."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Yale University"
    ]
}