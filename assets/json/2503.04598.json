{
    "paper_title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
    "authors": [
        "Zhijian Zhuo",
        "Yutao Zeng",
        "Ya Wang",
        "Sijun Zhang",
        "Jian Yang",
        "Xiaoqing Li",
        "Xun Zhou",
        "Jinwen Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm."
        },
        {
            "title": "Start",
            "content": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Zhijian Zhuo 1 2 Yutao Zeng 2 Ya Wang 2 Sijun Zhang 2 Jian Yang Xiaoqing Li 3 Xun Zhou 2 Jinwen Ma 1 5 2 0 2 6 ] . [ 1 8 9 5 4 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Transformers have become the de facto architecture for wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/ BryceZhuo/HybridNorm. 1. Introduction Transformers have become the backbone of large language models (LLMs) and wide range of machine learning applications. These architectures are capable of modeling 1School of Mathematical Sciences, Peking University 2SeedFoundation-Model, ByteDance 3Capital University of Economics and Business. Correspondence to: Yutao Zeng <yutao.zeng@outlook.com>, Ya Wang <wangyazg@gmail.com>. Preprint. Figure 1. Training dynamics for 1.2B dense models with Pre-Norm, HybridNorm and HybridNorm under 1T training tokens. We present the training loss, validation loss, and downstream performance on HellaSwag and ARC-Easy (0.8 EMA smoothed), demonstrating that HybridNorm achieves superior performance. long-range dependencies through self-attention mechanisms, which have made them the preferred choice for variety of tasks, including language modeling, machine translation, and image processing (Devlin et al., 2019; Brown et al., 2020; Dosovitskiy et al., 2021). However, as transformer models become deeper and more complex, ensuring stable training remains significant challenge. One critical factor that influences training stability is the choice of normalization methods, which is crucial for mitigating issues such as internal covariate shift and gradient instability (Xiong et al., 2020). Effectively addressing these challenges is crucial for fully harnessing the potential of deep transformer models in large-scale applications. In transformers, Layer Normalization (LayerNorm) (Ba et al., 2016) plays central role in stabilizing training by normalizing the activations within each layer. The two predominant strategies for applying LayerNorm are Pre-Layer Normalization (Pre-Norm) and Post-Layer Normalization HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Figure 2. Illustrations of different transformer layer structures: (a) Post-Norm architecture; (b) Pre-Norm architecture; (c) Pre-Norm with QK-Norm architecture; (d) HybridNorm architecture. (Post-Norm), each with its respective benefits and trade-offs. In the Pre-Norm architecture, normalization is applied before the residual addition, resulting in more prominent identity path that facilitates faster convergence and more stable gradients (Xiong et al., 2020). This design is particularly advantageous when training deep models, as it helps mitigate gradient-related issues that can arise during backpropagation. However, while Pre-Norm can stabilize training, it often leads to inferior final performance compared to Post-Norm (Xie et al., 2023). In contrast, PostNorm applies normalization after the residual connection, resulting in stronger regularization effects, which contribute to improved model performance. This approach has been shown to improve the generalization ability of transformers, particularly in very deep networks (Wang et al., 2024). Despite the benefits of each approach, there is an inherent trade-off between training stability and final model performance. Pre-Norm structures typically stabilize training but may underperform in terms of generalization, while PostNorm architectures provide better performance but can be more difficult to train, especially in deep models. To reconcile these trade-offs, we propose hybrid normalization method that applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network (FFN), which is named as HybridNorm. The QKV normalization in the attention mechanism stabilizes the flow of information between layers by normalizing the query, key, and value components, while Post-Norm in the FFN ensures effective depth in the deeper layers of the transformer. Through extensive experiments on large-scale models, we validate the effectiveness of our approach. Our results show that the hybrid normalization method significantly outperforms both Pre-Norm and Post-Norm across multiple benchmarks, providing stable training process and improved model performance. Specifically, our method achieves superior results in the context of LLMs, where the benefits of both normalization schemes are most pronounced. We believe that this hybrid approach offers promising solution for enhancing the training stability and performance of deep transformer architectures, particularly in the rapidly evolving field of LLMs. The main contributions of this paper can be summarized as follows. We propose HybridNorm, novel hybrid normalization structure that combines the advantages of Pre-Norm and Post-Norm, offering simple yet effective solution to enhancing performance in large transformer models. Our method is designed to exploit the strengths of both normalization approaches, ensuring robust convergence during training and superior final performance. We provide an empirical analysis of the proposed hybrid normalization method, demonstrating its potential benefits in terms of gradient flow stability, regularization effects, and model robustness. This analysis high2 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization lights how our approach addresses the key challenges posed by deep transformer architectures. Through extensive experiments on large-scale models, we empirically validate the effectiveness of our approach. Our results show that hybrid normalization significantly outperforms both Pre-Norm and Post-Norm across variety of tasks, leading to more stable training and improved model performance, particularly in the context of large language models (LLMs). The remainder of this paper is organized as follows. Section 2 reviews related work on normalization techniques in transformers. Section 3 introduces our proposed HybridNorm method, detailing its design and theoretical motivation. Section 4 presents the experimental setup and evaluation results, demonstrating the effectiveness of HybridNorm across various large-scale transformer models. Section 5 concludes the paper. 2. Related Work Architecture Modifications in Transformers. Recent efforts in transformer architecture modifications have sought to optimize both the computational efficiency and the expressiveness of the model. These efforts include changes to the attention mechanism and feed-forward networks all aimed at improving performance on variety of tasks, ranging from language modeling to vision tasks (Vaswani et al., 2017; Dosovitskiy et al., 2021). For example, Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024), Mixture of Experts (MoE) (Yuksel et al., 2012). While these modifications contribute to more efficient training, they also require careful integration with other components, such as normalization layers, to maintain model stability and performance. Normalization Types in Transformers. Normalization layers are integral to the success of deep learning models, and transformers are no exception. The most commonly used normalization technique in transformers is LayerNorm (Ba et al., 2016), which normalizes the activations of each layer independently. However, alternative methods such as RMSNorm (Zhang & Sennrich, 2019), which normalizes using root mean square statistics, have been proposed as more effective alternatives in certain settings. These methods are designed to mitigate the challenges of internal covariate shift and gradient instability, which are critical for the success of large-scale transformer models. Normalization Settings in Attention. For training stability, QK-Norm (Henry et al., 2020; Dehghani et al., 2023) modifies the standard attention mechanism by applying normalization directly to the query (Q) and key (K) components during attention computation. Building upon this, QKVNorm (Menary et al., 2024; Rybakov et al., 2024) extends the approach by normalizing the Query (Q), Key (K), and Value (V) components. This comprehensive normalization ensures that all critical components of the attention mechanism are normalized, resulting in enhanced stability and improved performance. Location of Normalization Layers. Another line of research focuses on the location of the normalization. The choice between Pre-Norm and Post-Norm architectures has been widely studied in the transformer literature (Vaswani et al., 2017; Klein et al., 2017; Wang et al., 2019). Pre-Norm, where normalization is applied before the residual connection, has been shown to be more stable in deep networks and accelerates convergence (Xiong et al., 2020). Although Post-Norm is more challenging to train, it tends to deliver better final performance by normalizing after the residual connection (Liu et al., 2020). DeepNorm (Wang et al., 2024) was proposed as strategy to address training instability in deep transformers, which scales the residual connections by carefully chosen factor to improve gradient flow and mitigate exploding or vanishing gradients. The method most similar to ours is Mix-LN (Li et al., 2025), which applies Post-Norm to the earlier layers and Pre-Norm to the deeper layers, achieving improved training stability and better performance. In contrast, our HybridNorm integrates Pre-Norm and Post-Norm within each transformer block. 3. Method In this section, we first review the two predominant normalization strategies in transformer architectures: Post-Norm and Pre-Norm. We then introduce our proposed hybrid normalization method, HybridNorm, and provide its formal definition. 3.1. Preliminaries Scaled Dot-Product Attention. The scaled dot-product attention computes the attention scores between the Query (Q) and Key (K) matrices, scaled by the square root of the key dimension dk, and applies these scores to the Value (V) matrix. The formulation is expressed as attn(Q, K, ) = softmax (cid:19) (cid:18) QK dk V, (1) where Q, K, Rndk represent the query, key, and value matrices respectively, and is the sequence length. Multi-Head Attention. Multi-head attention (MHA) extends the scaled dot-product attention mechanism by splitting the query, key, and value matrices into heads, each of size dk = d/h. Each head independently computes attention scores, and the outputs are concatenated and linearly projected to the original dimension, MHA(X) = Concat(head1, . . . , headh)W O, (2) 3 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization where headi = attn(Qi, Ki, Vi) for = 1, 2, . . . , h, {i}h i=1 = Split(XW) for {Q, K, }, and WQ, WK, WV , WO Rdd are learnable parameters. By enabling the model to focus on different subspaces of the input representation, MHA enhances the transformers capacity to capture diverse patterns in the input sequence. Feed-Forward Networks (FFN). In transformers, the FeedForward Network (FFN) is applied independently to each position of the input sequence. 3.2. Post-Norm and Pre-Norm The transformer architecture is composed of stack of blocks, each consisting of two key components: MHA and FFN. Residual connections and normalization layers are applied around both the MHA and FFN in each block to facilitate effective training and improve model stability. Figure 2 (a)&(b) illustrate Post-Norm and Pre-Norm, respectively. Post-Norm. Post-Norm applies the normalization layer after the residual connection in each transformer sub-layer. Formally, the output of Post-Norm can be expressed as = Norm(MHA(X l) + l), l+1 = Norm(FFN(Y l) + l), (3) (4) where Norm denotes RMSNorm (Zhang & Sennrich, 2019) or LayerNorm (Ba et al., 2016). Pre-Norm. In contrast, Pre-Norm normalizes the input to the sub-layer, which allows for more prominent identity path. The output of Pre-Norm is given by = MHA(Norm(X l)) + l, l+1 = FFN(Norm(Y l)) + l. (5) (6) This structure facilitates better gradient flow and stable convergence, particularly for deep models. However, its reliance on normalization before the residual connection can lead to suboptimal performance compared to Post-Norm, as the normalization does not account for the interaction between the residual connection and the sub-layers output. 3.3. HybridNorm To address the trade-offs between Post-Norm and Pre-Norm, we propose HybridNorm, hybrid normalization strategy that integrates their strengths. Specifically, HybridNorm combines QKV-Norm (Menary et al., 2024; Rybakov et al., 2024) in MHA and Post-Norm in FFN. QKV Normalization in Attention. In the attention mechanism, the query, key, and value matrices are normalized individually before computing the attention output. The normalized QKV matrices are then used in the scaled dotproduct attention. QKV-Norm enhances the stability of model training and leads to improved downstream performance. Formally, attention with QKV-Norm is defined as attnQKV (Q, K, ) = softmax (cid:18) Norm(Q)Norm(K) (cid:19) Norm(V ). (7) dk And we denote the multi-head attention with attnQKV as MHAQKV . HybridNorm Architecture. Combining the above, the overall output of transformer block with HybridNorm can be expressed as = MHAQKV (X l) + l, l+1 = FFN(Norm(Y l)) + Norm(Y l). (8) (9) The architecture illustration can be found in Figure 2(d) and the pseudocode is shown in Algorithm 1. By integrating QKV normalization in the attention mechanism and PostNorm in the FFN, HybridNorm achieves stable training dynamics and enhanced final performance. The theoretical gradient analysis can be found in Appendix A. Remark 3.1. The method most closely related to ours is Mix-LN (Li et al., 2025), which applies Post-Norm to the earlier layers and Pre-Norm to the deeper layers, resulting in enhanced training stability and performance. In contrast, our proposed HybridNorm integrates Pre-Norm and Post-Norm within each transformer block, providing unified approach to leverage the benefits of both normalization strategies. Moreover, experiments demonstrate that HybridNorm achieves superior downstream performance compared to Mix-LN (see Table 4). Special Treatment of First Block. Inspired by prior work (DeepSeek-AI, 2024), which employs the Mixture of Experts (MoE) architecture with specialized handling of the first layer, we explore the impact of introducing specialized normalization to the first transformer block. In our approach, the first layer of the transformer is treated differently by applying Pre-Norm on MHA and FFN, while maintaining QKV-Norm. Specifically, the structure of our first layer is defined as 0 = MHAQKV (Norm(X 0)) + 0, 1 = FFN(Norm(Y 0)) + 0. (10) (11) We refer to this variation of HybridNorm, which incorporates the specialized first block treatment, as HybridNorm. This design aims to stabilize the training of the first transformer block and boost overall performance by improving the flow of gradients in the early stages of training. 4. Experiments In this section, we demonstrate the effectiveness of HybridNorm through extensive experiments in LLMs. 4 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Table 1. Downstream evaluation results of 1.2B dense models with Pre-Norm, HybridNorm, and HybridNorm under 1T training tokens. Methods BasicArithmetic HellaSwag SciQ ARC-C ARC-E PIQA OpenbookQA COPA Avg. Pre-Norm HybridNorm HybridNorm 44.10 44.12 47.21 63.41 91.80 64.22 91.88 65.12 91. 39.20 39.13 37.06 69.82 71.05 71.79 75.19 74.72 75.72 38.40 38.88 39.16 82.00 62.99 82.00 63.25 85.78 64.15 4.1. Experiment Settings Baseline. We evaluate HybridNorm across two series of models: dense models and Mixture of Experts (MoE) models. The dense models include two scales: 550M and 1B, with the latter containing approximately 1.27 billion parameters and utilizing an architecture similar to Llama 3.2 (Dubey et al., 2024). All analytical experiments are conducted on the 550M dense models. For the MoE model, we use the OLMoE framework (Muennighoff et al., 2025), which activates 1.3B parameters out of total of 6.9B parameters (MoE-1B-7B). Both models are trained from scratch on the OLMoE Mix dataset 1 (Muennighoff et al., 2025). Model Configuration. The 550M dense model has model dimension of 1536, an FFN dimension of 4096, and utilizes 16 attention heads with 4 key/value heads per attention head. The 1.2B model features larger model dimension of 2048 and an FFN dimension of 9192, with 32 attention heads and 8 key/value heads per attention head. The MoE-1B-7B model employs 16 attention heads, model dimension of 2048, and an FFN dimension of 1024. Notably, it features 8 experts out of 64, providing more fine-grained distribution of computational resources. All models consist of 16 layers and are trained with consistent context length of 4096. More details can be found in Appendix B. Hyperparameters. Model weights are initialized using Megatron initialization (Shoeybi et al., 2019) (See Section 4.4 for more details). For the optimization, we apply the AdamW optimizer with β1 = 0.9 and β2 = 0.95. All models are trained on sequences of 4096 tokens. For the dense model, we set the initial learning rate to 3e-4, decaying to 3e-5 using cosine scheduler. The MoE model starts with learning rate of 4e-4, decaying according to cosine schedule. We summarize the hyperparameters in Table 7. Evaluation Metrics. To evaluate the performance of LLMs with HybridNorm, we employ diverse set of open benchmarks, including ARC-Easy (ARC-E) (Clark et al., 2018), ARC-Challenge (ARC-C) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), CoQA (Reddy et al., 2019), Winogrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2021), BoolQ 1OLMoE Mix is available at https://huggingface. co/datasets/allenai/OLMoE-mix-0924. (Clark et al., 2019), COPA (Gordon et al., 2012), CSQA (Talmor et al., 2019), OBQA (Mihaylov et al., 2018), and SocialIQA (Sap et al., 2019). We leverage the LM Eval Harness (Gao et al., 2023) for standardized performance evaluation. 4.2. Main Results on Dense Models We evaluate the performance of HybridNorm and HybridNorm on 1.2B dense transformer models. Figure 1 compares the training dynamics of dense models with different normalization methods. As shown in the figure, models with HybridNorm and HybridNorm exhibit consistently lower training loss and validation perplexity throughout training compared to Pre-Norm, highlighting their effectiveness in enhancing training stability and convergence. Table 1 presents the downstream evaluation results across diverse set of benchmarks. HybridNorm consistently outperforms Pre-Norm in most tasks, achieving the highest average score. Notably, it demonstrates substantial improvements in tasks such as BasicArithmetic (+3.11), HellaSwag (+1.71), and COPA (+3.78), indicating enhanced generalization and robustness. These results underscore the scalability of HybridNorm in larger transformer models, further validating its effectiveness in improving both training stability and downstream performance. More results can be found in Figure 7. 4.3. Main Results on MoE Models For MoE models, we conduct experiments on MoE-1B7B with 8 experts selected from pool of 64. Figure 3 presents the training dynamics of MoE models under different normalization strategies. Throughout the training, HybridNorm consistently achieves lower training loss and validation perplexity compared to Pre-Norm. These findings indicate that HybridNorm effectively alleviates optimization difficulties in large-scale MoE models, resulting in more stable training and enhanced downstream performance. Further, as shown in Table 2, HybridNorm consistently outperforms Pre-Norm across various downstream tasks, achieving the highest average score. Notably, it demonstrates significant improvements in ARC-C (+2.35), ARC-E (+2.40), and OpenbookQA (+0.81), highlighting its ability to enhance generalization across diverse benchmarks. 5 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Figure 3. Training dynamics for MoE-1B-7B models with Pre-Norm and HybridNorm under 500B training tokens. We present the training loss, validation loss, and downstream performance on HellaSwag and MMLU Var, demonstrating that HybridNorm achieves superior performance. Table 2. Downstream evaluation results of MoE-1B-7B with Pre-Norm and HybridNorm under 500B training tokens. Methods HellaSwag ARC-C ARC-E PIQA WinoGrande OpenbookQA BoolQ COPA Avg. Pre-Norm HybridNorm 69.94 70.71 39.92 42.27 73.37 75.77 77.82 78. 63.34 64.58 42.37 43.18 67.47 68.41 85.40 86.00 64.95 66.12 Table 3. Training loss and validation perplexity of 550M dense models with Pre-Norm and HybridNorm under various initialization methods and 400B training tokens. Method Initialization Loss PPL on C4 Pre-Norm HybridNorm Normal Depth-Scaled Megatron Normal Depth-Scaled Megatron 2.75 2.76 2.76 2.76 2.76 2.74 20.29 20.49 20.44 20.44 20.40 20.00 4.4. Ablation Studies Initialization. To evaluate the sensitivity of Pre-Norm and HybridNorm to initialization schemes, we conduct ablation studies comparing three widely used initialization strategies: Normal initialization (Nguyen & Salazar, 2019), DepthScaled initialization (Zhang et al., 2019; Gururangan et al., 2023), and Megatron initialization (Shoeybi et al., 2019). Normal initialization initializes all weights of linear layers using truncated normal distribution with mean zero and 2.5d, where is the hidden dimenstandard deviation 1/ sion. Depth-Scaled initialization and Megatron initialization introduce scaling factors to stabilize training in deep architectures. Specifically, Depth-Scaled initialization scales down the output projections of the attention and FFN by 2l, where is the layer index. In contrast, Megafactor of 2L, tron initialization scales down these projections by where is the total number of layers, mitigating gradient variance accumulation in very deep transformers. As shown in Table 3, Pre-Norm and HybridNorm exhibit sensitivity across different initialization methods, achieving the lowest training loss and perplexity under Normal initialization and Megatron initialization, respectively. Therefore, we set the default initialization method for Pre-Norm to Normal initialization and for HybridNorm to Megatron initialization in all experiments, respectively, which ensures that even under settings that may be more favorable to baseline models, the superiority of our approach is effectively demonstrated. Normalization Position. We investigate the impact of the position of normalization layers within the transformer block. First, we examine the effect of varying the placement of QKV normalization (e.g., normalization setting in attention). We extend the normalization setting by considering not only the Query (Q), Key (K), and Value (V) components but also the Context (C), which refers to the output of the attention mechanism. For instance, QKVC-Norm applies normalization to all four components: Query, Key, Value, and Context, while KV-Norm and KC-Norm focus on the normalization of the Key-Value and Key-Context pairs, respectively. QKVC-Post refers to transformer blocks that employ QKVC-Norm in the MHA while using Post-Norm in the FFN. Second, we explore the effect of integrating QKV-Norm into different transformer architectures. For instance, Pre-QKV-Pre refers to configuration where QKVNorm is applied with Pre-Norm in the MHA layer, while the FFN layer utilizes Post-Norm. Other configurations follow similar definitions. Finally, we compare various hybrid 6 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Table 4. Abalation study of the position of normalization layers on 550M dense models with 400B tokens. We report the training loss, the perplexity on C4 and Pile, and the accuracy on HS (HellaSwag). Methods Loss C4 Pile HS QKVC-Post QKC-Post QK-Post KV-Post KC-Post Pre-QKV-Post Pre-QKV-Pre Pre-QK-Pre QKV-Pre Post-Norm Pre-Norm Mix-LN Post-Pre Pre-Post HybridNorm HybridNorm 2.74 2.73 - 2.74 2.75 2.74 2.74 2.75 2.74 2.76 2.75 2.76 2.75 2.74 2.74 2.73 20.05 20.00 20.11 20. 20.13 19.97 20.22 19.96 20.43 20.30 20.43 20.26 20.15 20.00 19.85 10.34 10.31 diverge 10.38 10.47 10.37 10.33 10.43 10.33 10.57 10.48 10.56 10.46 10. 10.29 10.25 52.68 52.26 52.10 51.15 52.65 53.05 52.29 52.57 51.20 51.97 51.29 51.19 52.42 53.35 53. combinations of Pre-Norm and Post-Norm. Pre-Post refers to transformer blocks that apply Pre-Norm in the MHA and Post-Norm in the FFN, whereas Post-Pre adopts the opposite configuration. Mathematical formulas for methods mentioned above can be found in Appendix E. As shown in Table 4, HybridNorm (also called QKV-Post) and its variant HybridNorm consistently surpass other configurations. Notably, HybridNorm achieves the lowest training loss and perplexity while attaining the highest accuracy on HellaSwag. Specifically, by comparing HybridNorm with the first block in Table 4, we find that QKV-Norm is the most effective normalization setting in the attention. Similarly, comparing HybridNorm with the second block, we observe that combining QKV-Norm with Post-Norm in the FFN yields superior performance. From the third block, one can see that the Pre-Post configuration indeed leads to improved performance, while replacing Pre-Norm in the MHA with QKV-Norm to form HybridNorm further enhances performance, achieving the best results. Special Treatment of First Block. For the special treatment of the first block, we test different architectures, such as adding normalization layer after embedding (call EmbedNorm) and armed the first block with QKV-norm and Pre-Norm in FNN (call First-QKV-Pre), which formulations are: 0 = MHAQKV (X 0) + 0, 1 = FFN(Norm(Y 0)) + 0. As shown in Figure 4, we can see that, except for EmbedFigure 4. Training loss and accuracy on HellaSwag of 550M dense models with different normalization methods for the first block. Figure 5. Gradient norm comparison of Pre-Norm, Post-Norm, and HybridNorm at different training steps. Norm, the special treatment of the first block effectively reduces training loss and improves downstream performance. 4.5. Gradient Analysis To gain deeper insights into the stability improvements introduced by HybridNorm, we analyze gradient norms across training iterations. As shown in Figure 5, we compare the gradient norms of Pre-Norm, Post-Norm, and HybridNorm at steps 1 and 100. The results indicate that Pre-Norm tends to exhibit gradient explosion in deeper models, while PostNorm suffers from vanishing gradients, both of which hinder effective optimization. In contrast, HybridNorm maintains well-balanced gradient flow throughout training, effectively mitigating these issues. An intuitive understanding is that Pre-Norm tends to amplify gradients, while Post-Norm diminishes them. HybridNorm alternates between these two 7 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Table 5. Performance of dense models with different depths under 400B training tokens. We report the training loss, the perplexity on C4 and Pile, and the accuracy on HellaSwag and PIQA. Models 550M, 16 Layers 543M, 29 Layers Loss C4 Pile HellaSwag PIQA Loss C4 Pile HellaSwag PIQA Post-Norm Pre-Norm HybridNorm HybridNorm 2.76 20.43 2.75 2.74 2.73 10.57 20.30 10.48 20.00 10.29 19.85 10.25 51.20 51.97 53.35 53.36 71.80 71.14 71.96 71. diverge 2.73 2.72 2.71 19.88 19.67 19.52 10.31 10.18 10.10 53.86 54.89 54.54 71.63 72.62 72. 4.7. Deeper Models To further evaluate the robustness of HybridNorm and HybridNorm in deeper architectures, we conduct experiments on transformers with depths ranging from 16 to 29 layers while maintaining comparable parameter budget. This setup allows for fair comparison of different normalization strategies in deep transformer architectures. As shown in Table 5, both HybridNorm and HybridNorm consistently outperform Pre-Norm and Post-Norm across various depths, demonstrating their effectiveness in stabilizing deep model training. particularly striking observation is that Post-Norm fails to converge at 29 layers, reinforcing its well-documented instability in deeper architectures. In contrast, HybridNorm and HybridNorm not only ensure stable training across all depths but also achieve significantly lower training loss and perplexity on both the C4 and Pile datasets. These improvements indicate that HybridNorm-based normalization strategies mitigate optimization difficulties that commonly arise in deep transformers. Furthermore, HybridNorm achieves the highest accuracy on challenging downstream benchmarks such as HellaSwag and PIQA, suggesting that its benefits extend beyond mere training stability to enhanced generalization on real-world tasks. These results provide strong empirical evidence that HybridNorm-based normalization schemes enable deeper transformer training while preserving superior optimization efficiency and downstream task performance. 5. Conclusion In this paper, we have presented HybridNorm, novel hybrid normalization approach that effectively integrates the strengths of Pre-Norm and Post-Norm to address the longstanding trade-offs in transformer training. We have provided empirical insights into how HybridNorm stabilizes gradient flow while maintaining strong regularization effects, demonstrating its potential to improve both convergence and final model performance. Extensive experiments across diverse benchmarks validate the efficacy of our approach, with results consistently showing that HybridNorm Figure 6. Scaling law curves of Pre-Norm and HybridNorm. normalization strategies, leading to more stable gradient propagation during backpropagation and effectively preventing gradient explosion or vanishing. This balanced gradient propagation contributes to smoother optimization dynamics and faster convergence, further reinforcing the effectiveness of HybridNorm in stabilizing transformer training. 4.6. Scaling Laws Experiments We compare the loss scaling curves between Pre-Norm and HybridNorm across range of dense model sizes, from 151M to 1.2B parameters. The model sizes used for the scaling law experiments are detailed in Table 6, and all models are trained using the same setting and hyperparameters for fair comparison, as specified in Table 7. Models with 151M, 285M, 550M, and 1.2B parameters are trained on 200B, 200B, 300B, and 1T tokens, respectively. As shown in Figure 6, HybridNorm exhibits superior scaling properties, demonstrating lower training loss as the model size increases. This highlights its capacity to maintain both training stability and performance, even for extremely large models, thereby making it highly suitable for scaling to billion-parameter regimes. 8 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization outperforms traditional normalization strategies in terms of stability and accuracy. Our findings underscore the necessity of re-evaluating normalization position within transformer architectures, thereby laying the groundwork for further exploration of hybrid strategies. We posit that HybridNorm signifies substantial advancement in the design of more robust and performant deep transformer models, offering practical advantages for the training of next-generation largescale neural networks."
        },
        {
            "title": "Impact Statement",
            "content": "This paper proposes HybridNorm, simple yet effective hybrid normalization technique that improves the training stability and performance of transformers. It has the potential to assist the LLM community in advancing transformer architectures and enhancing their overall effectiveness. While there may be societal implications of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In NeurIPS, 2020. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers to 22 billion parameters. In ICML, 2023. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. Gordon, A., Kozareva, Z., and Roemmele, M. Semeval2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In * SEM 2012: The First Joint Conference on Lexical and Computational SemanticsVolume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 394398, 2012. Gururangan, S., Wortsman, M., Gadre, S. Y., Dave, A., Kilian, M., Shi, W., Mercat, J., Smyrnis, G., Ilharco, G., Jordan, M., Heckel, R., Dimakis, A., Farhadi, A., Shankar, V., and Schmidt, L. Openlm: minimal but performative language modeling (lm) repository, 2023. URL https://github.com/ mlfoundations/open_lm/. GitHub repository. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. Henry, A., Dachapally, P. R., Pawar, S. S., and Chen, Y. Query-key normalization for transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 42464253, 2020. 9 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Klein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. M. Opennmt: Open-source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations, pp. 6772, 2017. Skjonsberg, S., Wadden, D., Wilhelm, C., Wilson, M., Zettlemoyer, L., Farhadi, A., Smith, N. A., and Hajishirzi, H. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Ormaniec, W., Dangel, F., and Singh, S. P. What does it mean to be transformer? insights from theoretical hessian analysis. In The Thirteenth International Conference on Learning Representations, 2025. Li, P., Yin, L., and Liu, S. Mix-LN: Unleashing the power of deeper layers by combining pre-LN and post-LN. In The Thirteenth International Conference on Learning Representations, 2025. Reddy, S., Chen, D., and Manning, C. D. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Liu, L., Liu, X., Gao, J., Chen, W., and Han, J. Understanding the difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 57475763, 2020. Menary, S., Kaski, S., and Freitas, A. Transformer normalisation layers and the independence of semantic subspaces. arXiv preprint arXiv:2406.17837, 2024. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, 2018. Muennighoff, N., Soldaini, L., Groeneveld, D., Lo, K., Morrison, J., Min, S., Shi, W., Walsh, E. P., Tafjord, O., Lambert, N., Gu, Y., Arora, S., Bhagia, A., Schwenk, D., Wadden, D., Wettig, A., Hui, B., Dettmers, T., Kiela, D., Farhadi, A., Smith, N. A., Koh, P. W., Singh, A., and Hajishirzi, H. OLMoe: Open mixture-of-experts language models. In The Thirteenth International Conference on Learning Representations, 2025. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention. In Proceedings of the 16th International Conference on Spoken Language Translation, 2019. Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35: 2719827211, 2022. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., Guerquin, M., Ivison, H., Koh, P. W., Liu, J., Malik, S., Merrill, W., Miranda, L. J. V., Morrison, J., Murray, T., Nam, C., Pyatkin, V., Rangapur, A., Schmitz, M., Rybakov, O., Chrzanowski, M., Dykas, P., Xue, J., and Lanir, B. Methods of improving llm training stability. arXiv preprint arXiv:2410.16682, 2024. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., and Wei, F. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., and Chao, L. S. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019. HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, 2017. Xie, S., Zhang, H., Guo, J., Tan, X., Bian, J., Awadalla, H. H., Menezes, A., Qin, T., and Yan, R. Residual: Transformer with dual residual connections. arXiv preprint arXiv:2304.14802, 2023. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In ICML, 2020. Yuksel, S. E., Wilson, J. N., and Gader, P. D. Twenty years of mixture of experts. IEEE transactions on neural networks and learning systems, 23(8):11771193, 2012. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 47914800, 2019. Zhang, B. and Sennrich, R. Root mean square layer normalization. In NeurIPS, 2019. Zhang, B., Titov, I., and Sennrich, R. Improving deep transformer with depth-scaled initialization and merged attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 898909, 2019. 11 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization A. Theoretical Gradient Analysis For simplicity, we consider single-head attention layer. The input is Rsd, representing sequence of tokens with RMS(x) for Rd, where dimension d. Throughout this section, we denote RMSNorm as Norm(), i.e., Norm(x) = α RMS(x) = (cid:112)(x2 d)/d. For further simplicity, we set α = 1d. The learnable parameters WQ, WK, WV Rddk and WO Rdkd. Let XN = Norm(X), = 1 , and = softmax(M ). The output of the attention dk block with Pre-Norm is then given by 1 + + x2 XN WQW = AXN WV WO. (12) Defining = XWQ, = XWK, and = XWV , with their normalized counterparts QN = Norm(Q), KN = Norm(K), and VN = Norm(V ), the output of the attention block with QKV-Norm is formulated as SN = AN VN WO, (13) here AN = softmax(MN ) and MN = 1 dk QN . Following the prior work of Noci et al. (2022); Ormaniec et al. (2025), we analyze the gradients by computing derivatives using row-wise vectorization and arranging the Jacobian in the numerator layout, i.e., X = vecr(Y ) vecr(X) . The following derivation primarily relies on the chain rule and the following rule AW = B, (14) where is the Kronecker product. We first present the following extension of Lemma 2 in Noci et al. (2022). Lemma A.1 ((Extention of Lemma 2 in Noci et al. (2022))). The gradients of the attention with Pre-Norm defined in Eq. (12) are given by WO WV WQ WK = softmax = softmax (cid:18) XN WQW dk (cid:19) XN WV Id, (cid:18) XN WQW (cid:19) = (cid:0)Is X = (cid:0)Is X dk (cid:1) M (cid:1) M XN , (cid:18) XN XN WK (cid:18) XN WQ XN dk dk (cid:19) , (cid:19) Kdk,d, where the gradients of the softmax with respect to its inputs is M = blockdiag (cid:19) (cid:18) Ai,: Mi,: = blockdiag (cid:0)diag(Ai,:) Ai,:A i,: (cid:1) , (15) (16) (17) (18) (19) Ai,: is the i-th row of in column vector format, and the commutation matrix Kdk,d is permutation matrix that transforms the row-wise vectorization of WK into the column-wise vectorization of WK, i.e., Kdk,dvecr(WK) = vecr(W ). The gradient of XN with respect to is XN = Norm(X) = blockdiag (cid:18) Norm(Xi,:) Xi,: (cid:19) = blockdiag (cid:32) (cid:32) Xi,: Id (cid:33)(cid:33) , Xi,:X i,: Xi,:2 2 (20) (21) (22) (23) (24) (25) HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization where Xi,: is the i-th row of represented as column vector. The definitions of QN i.e., for {Q, K, }, , KN , and VN follow similarly, = blockdiag (cid:32) (cid:32) dk i,: 2 Idk (cid:33)(cid:33) . i,: i,: i,: 2 2 Lemma A.2. The gradients of the attention with QKV-Norm defined in Eq. (12) are SN WO SN WV SN WQ SN WK Norm(XWV ) Id, = softmax (cid:18) = softmax (cid:18) Norm(XWQ)Norm(XWK) dk (cid:18) Norm(XWQ)Norm(XWK) (cid:19) = (cid:0)Is = (cid:0)Is Norm(XWV )(cid:1) AN MN Norm(XWV )(cid:1) AN MN dk (cid:19) (X Idk ), (cid:19) VN (cid:19) QN (X Idk ), (cid:18) Is Norm(XWK) (cid:18) Norm(XWQ) Is (cid:19) Kdk,s KN (X Idk ), dk dk where the definition of AN MN is similar to M and Kdk,s is the commutation matrix s.t., Kdk,svecr(KN ) = vecr(K ). Proof. For SN WO , according to Eq. (14), we obtain SN WO = AN VN Id = softmax (cid:18) Norm(XWQ)Norm(XWK) dk (cid:19) Norm(XWV ) Id. For SN WV , using the chain rule and Eq. (14), we have SN WV = SN VN VN = (A ) (cid:18) = softmax WV VN (cid:18) Norm(XWQ)Norm(XWK) (X Idk ) dk (cid:19) (cid:19) VN (X Idk ). For SN WQ , using the chain rule and Eq. (14), we obtain SN WQ = SN AN AN MN = (cid:0)Is = (cid:0)Is QN MN QN WQ (cid:18) Is KN (cid:1) AN MN dk Norm(XWV )(cid:1) AN MN (cid:19) QN (X Idk ) (cid:18) Is Norm(XWK) dk (cid:19) QN (X Idk ). Similarly, for SN WK , we have SN WK = SN AN AN MN = (cid:0)Is = (cid:0)Is MN KN KN K WK (cid:18) QN Is dk (cid:1) AN MN Norm(XWV )(cid:1) AN MN (cid:19) vecr(K ) KN vecr(KN ) (cid:18) Norm(XWQ) Is dk (X Idk ) (cid:19) Kdk,s KN (X Idk ). 13 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Theorem A.3. For the attention with Pre-Norm, we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) WO WV WQ WK (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F = (sdWV 2) , (cid:16) = dWOF (cid:17) , = = (cid:18) (sd)3/2 2 dk (cid:18) (sd)3/2 dk (cid:19) WK2WV 2WO2 WQ2WV 2WO2 (cid:19) , . For the attention with QKV-Norm, we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) SN WO SN WV SN WQ SN WK (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F = (sd) , (cid:18) = = = σV (cid:32) min s3/2dk σQ min (cid:18) s3/2dk σK min dk (cid:19) WO2 , (cid:33) WO2 , (cid:19) WO2 , (26) (27) (28) (29) (30) (31) (32) (33) where σQ min, σK min, σV min are minimal singular value of XWQ, XWK, XWV , respectivaly. Theorem A.3 presents the gradient norms of different methods. In the attention with Pre-Norm, the gradient of the weight matrix exhibits strong dependencies on other weights; for instance, WQ and WK are influenced by all three other weight matrices but not by themselves. In contrast, in the attention with QKV-Norm, the gradient of each weight matrix depends at most on itself and WO. This suggests that the gradient of the attention with Pre-Norm is more tightly coupled with other weight matrices compared to the gradient of the attention with QKV-Norm. Therefore, during the gradient optimization process, if the norm of certain weight becomes excessively large, it is more challenging to control in the attention with Pre-Norm, leading to an increase in gradient magnitude. This, in turn, creates vicious cycle that may result in model collapse. In contrast, the attention with QKV-Norm alleviates this issue to some extent, which significantly benefits the stability of model training. The proof is primarily based on the following facts tr(B C) = tr(B)tr(C) (B C)(D E) = (BD) (CE) CF = BF CF C2 = B2C2 BC2 B2C2 BCF B2CF BF CF If Rs, pi 0 and (cid:80)s i=1, then diag(p) pp2 1 2 If Rss is stochastic matrix, i.e., A1s = 1s and each entry is nonnegative, then A2 AF XN = sd 14 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Proof. According to fundamental algebraic operations, we obtain (cid:13) (cid:13) (cid:13) (cid:13) WO (cid:13) (cid:13) (cid:13) (cid:13)F = AXN WV IdF = AXN WV IdF dA2XN WV 2 sdWV 2, (cid:13) (cid:13) (cid:13) (cid:13) WV (cid:13) (cid:13) (cid:13) (cid:13)F = (cid:13) (cid:13)AXN (cid:13) (cid:13)F = AXN WOF A2XN WOF dWOF , (cid:13) (cid:13) (cid:13) (cid:13) WQ (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13) WK (cid:13) (cid:13) (cid:13) (cid:13)F = = = = = = = = = = = 1 dk 1 2 dk (sd)3/2 dk 2 1 dk 1 dk 1 dk (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 dk 1 dk 2 (sd)3/2 dk 2 (cid:13) (cid:13) (cid:0)Is (cid:13) (cid:13) V (cid:1) M (cid:18) XN XN WK dk (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F 1 dk 1 dk 1 dk (cid:13) (cid:0)Is (cid:13) X (cid:1) blockdiag(diag(Ai,:) Ai,:A i,:)(XN XN WK)(cid:13) (cid:13)F (cid:13) (cid:13)blockdiag((W X )(diag(Ai,:) Ai,:A i,:))(XN XN WK)(cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) XN XN 1,: V (diag(A1,:) A1,:A 1,:))XN WK ... s,: V (diag(As,:) As,:A s,:))XN WK (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F XN WO2WV 2XN 2 1 2 WK2XN WK2WV 2WO2XN 3 WK2WV 2WO2, (cid:13) (cid:13) (cid:0)Is (cid:13) (cid:13) (cid:13) (cid:13) (cid:0)Is (cid:13) (cid:13) X X (cid:1) M (cid:1) M (cid:18) XN WQ XN dk (cid:18) XN WQ XN dk (cid:19) Kdk,d (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:0)Is (cid:13) X (cid:1) blockdiag(diag(Ai,:) Ai,:A i,:)(XN WQ XN )(cid:13) (cid:13)F (cid:13) (cid:13)blockdiag((W X )(diag(Ai,:) Ai,:A i,:))(XN WQ XN )(cid:13) (cid:13)F (XN WQ) 1,: X (XN WQ) s,: V (diag(A1,:) A1,:A ... (diag(As,:) As,:A 1,:))XN s,:))XN (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) WQ2XN WO2WV 2XN 2 1 2 XN WQ2WV 2WO2XN 3 WQ2WV 2WO2. 15 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Therefore, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) WO WV WQ WK (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F = (sdWV 2) , (cid:16) = dWOF (cid:17) , = = (cid:18) (sd)3/2 2 dk (cid:18) (sd)3/2 dk (cid:19) WK2WV 2WO2 WQ2WV 2WO2 (cid:19) , . As for the attention with QKV-Norm, we have (cid:13) (cid:13) (cid:13) (cid:13) SN WO (cid:13) (cid:13) (cid:13) (cid:13)F = AN VN IdF = AN VN IdF dAN 2VN sd. For SN WV , we have (cid:13) (cid:13) (cid:13) (cid:13) SN WV (cid:13) (cid:13) (cid:13) (cid:13)F = (cid:13) (cid:13) (A ) (cid:13) (cid:13) (cid:13) (cid:13)A sWO2 VN (cid:13) (cid:13) (cid:13) (cid:13) VN (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) (cid:13) VN (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (X Idk ) (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F . (cid:33)(cid:33) Vi,:V i,: Vi,:2 2 (cid:19)(cid:19) (cid:13) (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F blockdiag Idk (cid:32) (cid:32) dk Vi,:2 (cid:18) dk V1,:2 (cid:18) 1,: s,: Idk V1,:V 1,: V1,:2 2 ... (cid:18) (cid:18) dk Vs,:2 (cid:19)(cid:19) Idk Vs,:V s,: Vs,:2 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F According to Eq. (21), we get (cid:13) (cid:13) (cid:13) (cid:13) VN (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F = = = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:118) (cid:117) (cid:117) (cid:116)dk (cid:118) (cid:117) (cid:117) (cid:116)dk sdk . σV min (cid:88) i=1 (cid:88) i=1 Xi,:2 2 Vi,:2 Xi,:2 2 Xi,:2 2 Similarly, we can get It follows that (cid:13) (cid:13) (cid:13) (cid:13) QN (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F sdk σQ min , (cid:13) (cid:13) (cid:13) (cid:13) KN (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F sdk σK min . (cid:13) (cid:13) (cid:13) (cid:13) SN WV (cid:13) (cid:13) (cid:13) (cid:13)F σV min dk WO2. (34) 16 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization (cid:13) (cid:13) (cid:0)Is (cid:13) (cid:13) (cid:13) (cid:13) (cid:0)Is (cid:13) (cid:13) (cid:1) AN MN N (cid:13) (cid:0)Is (cid:13) (cid:18) Is KN dk (cid:13) (cid:13) (X Idk ) (cid:13) (cid:13)F (cid:19) QN (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:1) blockdiag(diag((AN )i,:) (AN )i,:(AN ) (cid:1) AN MN (X Idk ) (Is KN ) QN (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)blockdiag(W (diag((AN )i,:) (AN )i,:(AN ) i,:)KN )(cid:13) (cid:13)2 i,:) (Is KN )(cid:13) (cid:13)2 sdk σQ min For SN WQ , we have (cid:13) (cid:13) (cid:13) (cid:13) SN WQ (cid:13) (cid:13) (cid:13) (cid:13)F = = 1 dk 1 dk σQ min σQ min s3/2dk 2σQ min WO2VN 2 1 2 KN 2 WO2. Similarly, for SN WK , we have Therefore, (cid:13) (cid:13) (cid:13) (cid:13) SN WK (cid:13) (cid:13) (cid:13) (cid:13)F s3/2dk 2σK min WO2. (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) SN WO SN WV SN WQ SN WK (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F = (sd) , (cid:18) = = = σV (cid:32) min s3/2dk σQ min (cid:18) s3/2dk σK min dk (cid:19) WO2 , (cid:33) WO , (cid:19) WO2 . 17 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization B. Details of Experiments B.1. Architectures of Different Models For dense models, we adopt decoder-only transformer architecture akin to Llama 3.2 (Dubey et al., 2024), with model sizes ranging from 151M to 1.2B parameters. For the MoE model, we follow the structure of OLMoE (Muennighoff et al., 2025). The specific architecture of models is summarized in Table 6. Table 6. Model architecture for dense models and MoE models. Dense-151M Dense-285M Dense-550M Dense-543M Dense-1.2B MoE-1B-7B Model Dimension FFN Dimension Attention heads Key/Value Heads Layers Vocabulary Size Weight Typing Context Length Expert Granularity 768 2048 16 4 12 100278 True 4096 - 1024 4096 16 4 12 100278 True 4096 - 1536 4096 16 4 16 100278 True 4096 - 1024 4096 16 4 29 100278 True 4096 - 2048 9192 32 8 16 100278 True 4096 - 2048 1024 16 16 16 50280 False 4096 8 in B.2. Hyperparameters for Pretraining For the training hyperparameters, we primarily adopt the configuration outlined in OLMo 2 (OLMo et al., 2024) and OLMoE (Muennighoff et al., 2025). The training hyperparameters for our models across different sizes are presented in Table 7. The model is trained using the AdamW optimizer with learning rate (LR) of 3e-4 (4e-4 for MoE), which is scheduled to decay following cosine function. The minimum LR is set to 3e-5 (5e-5 for MoE) to prevent excessively small updates in the later stages of training. weight decay of 0.1 is applied to regularize the model and prevent overfitting. The AdamW optimizer employs β1 = 0.9 and β2 = 0.95 to control the first and second momentum estimates, respectively. Gradient clipping is utilized with threshold of 1 to mitigate the impact of large gradients during optimization. The models training also incorporates warmup phase with total of 8,388,608,000 tokens (10,485,760,000 for MoE). The initialization of the models parameters follows normal distribution with standard deviation defined as 1/ 2.5d, where is the model dimension. Furthermore, the initialization is truncated at 3 standard deviations to ensure more stable starting point for training. The RoPE (Rotary Position Embedding) parameter θ is set to 500,000 (10000 for MoE), controlling the scale of position encodings. Finally, the activation function used in the model is SwiGLU, which has been shown to outperform traditional activation functions in various tasks. C. Additional Experimental Results C.1. Overall Results for Dense Mdoels Overall results for the dense model are presented in Figure 7, depicting validation losses and downstream evaluations over 1T training tokens. The comparison includes models with Pre-Norm, HybridNorm, and HybridNorm. One can see that both HybridNorm and HybridNorm outperform Pre-Norm, with HybridNorm achieving the lowest training and validation losses while delivering the best downstream performance. C.2. Overall Results for MoE Mdoels Overall results for the MoE model are presented in Figure 8, illustrating validation losses and downstream evaluations over 500 billion training tokens. The comparison focuses on models employing Pre-Norm and HybridNorm. As shown, HybridNorm consistently achieves lower training and validation losses while delivering superior downstream performance. 18 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Table 7. Hyperparameters for Pretraining. Optimizer Learning Rate (LR) Minimum LR LR Schedule Weight Decay β1 β2 Gradient Clipping Batch Size Warmup Tokens Init Distribution Init std Init Truncation RoPE θ Activation Load Balancing Loss Weight Router z-loss Weight"
        },
        {
            "title": "Dense Model",
            "content": "MoE-1B-7B AdamW 3e-4 3e-5 cosine 0.1 0.9 0.95 1 1024 8,388,608,000 Megatron 2.5d 1/ 3 std 500000 SwiGLU - - AdamW 4e-4 5e-5 cosine 0.1 0.9 0.95 1 1024 10,485,760,000 Megatron 2.5d 1/ 3 std 10000 SwiGLU 0.01 0.001 D. PyTorch Style Implementation of HybridNorm Algorithm 1 PyTorch style pseudocode for Transformer block with HybridNorm # q_norm, k_norm, v_norm, ffn_norm are normalization layers # attn_proj and attn_out are linear layers # attn is the attention # ffn is the feedforward network def forward(x): # Attention block res = # shape (b, s, d) q, k, = attn_proj(x).split((d, d, d), dim=-1) # shape (b, s, d) q, k, = q.view(b, s, h, dk) q, k, = q_norm(q), k_norm(k), v_norm(v) = attn(q,k,v) = attn_out(x) + res # shape (b, s, d) # dk = / h, is the number of attention heads # FFN block = ffn_norm(x) = ffn(x) + return E. Formulas for Different Positions of Normalization Layers In this section, we present the mathematical formulations for various normalization techniques. We begin by introducing the normalization layer within the attention mechanism. Vanilla scaled dot-product attention are show in Eq. 1, and attention with QKV-Norm is defined in Eq. 7. Similarly, attention with QK-Norm is defined as attnQK(Q, K, ) = softmax (cid:18) Norm(Q)Norm(K) dk (cid:19) V. Attention with KV-Norm is defined as attnKV (Q, K, ) = softmax (cid:18) QNorm(K) dk (cid:19) Norm(V ). 19 (35) (36) HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization As mentioned in Section 4.4, we extend traditional normalization approaches by considering not only the Query (Q), Key (K), and Value (V) components but also the Context (C), which refers to the output of the attention mechanism. And attention with QKVC-Norm is defined as attnQKV C(Q, K, ) = Norm softmax (cid:18) (cid:18) Norm(Q)Norm(K) dk (cid:19) (cid:19) Norm(V ) . Attention with QKC-Norm is defined as attnQKC(Q, K, ) = Norm softmax (cid:18) (cid:18) Norm(Q)Norm(K) dk (cid:19) (cid:19) . Attention with KC-Norm is defined as attnKC(Q, K, ) = Norm softmax (cid:18) (cid:18) QNorm(K) dk (cid:19) (cid:19) . Then we denote MHA with attn# as MHA# for # {QKV C, QKV, QKC, QK, KV, KC}, MHA(X)# = Concat(head1, . . . , headh)W O, (37) (38) (39) (40) where headi = attn#(Qi, Ki, Vi) for = 1, 2, . . . , h, {i}h WQ, WK, WV , WO Rdd are learnable parameters. i=1 = Split(XW) for {Q, K, }, and With the aforementioned definitions in hand, we present the mathematical formulations for the methods discussed in the Ablation Study below (# {QKV C, QKV, QKC, QK, KV, KC}). #-Post: #-Pre: Pre-#-Post: Pre-#-Pre: Pre-Post: Post-Pre: = MHA#(X l) + l, l+1 = FFN(Norm(Y l)) + Norm(Y l). = MHA#(X l) + l, l+1 = FFN(Norm(Y l)) + l. = MHA#(Norm(X l)) + l, l+1 = FFN(Norm(Y l)) + Norm(Y l). = MHA#(Norm(X l)) + l, l+1 = FFN(Norm(Y l)) + l. = MHA(Norm(X l)) + l, l+1 = FFN(Norm(Y l)) + Norm(Y l). = MHA(Norm(X l)) + Norm(X l), l+1 = FFN(Norm(Y l)) + l. 20 (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) (51) (52) HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Figure 7. Overall loss and downstream evaluations for the 1.2B dense models with 1T training tokens. 21 HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization Figure 8. Overall loss and downstream evaluations for the MoE models with 500B training tokens."
        }
    ],
    "affiliations": [
        "Capital University of Economics and Business",
        "School of Mathematical Sciences, Peking University",
        "SeedFoundation-Model, ByteDance"
    ]
}