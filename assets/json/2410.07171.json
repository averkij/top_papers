{
    "paper_title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation",
    "authors": [
        "Xinchen Zhang",
        "Ling Yang",
        "Guohao Li",
        "Yaqi Cai",
        "Jiake Xie",
        "Yong Tang",
        "Yujiu Yang",
        "Mengdi Wang",
        "Bin Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp"
        },
        {
            "title": "Start",
            "content": "Preprint. ITERCOMP: ITERATIVE COMPOSITION-AWARE FEEDBACK LEARNING FROM MODEL GALLERY FOR TEXT-TO-IMAGE GENERATION Xinchen Zhang1 Ling Yang2 Guohao Li5 Yaqi Cai4 Jiake Xie3 Yong Tang3 1Tsinghua University 2Peking University Yujiu Yang1 Mengdi Wang6 Bin Cui2 3LibAI Lab 5University of Oxford 6Princeton University https://github.com/YangLing0818/IterComp 4USTC 4 2 0 2 9 ] . [ 1 1 7 1 7 0 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop compositionaware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020; Peebles & Xie, 2023) has recently brought unprecedented progress to the field of text-to-image generation, with powerful models like DALL-E 3 (Betker et al., 2023), Stable Diffusion 3, (Esser et al., 2024) and FLUX (BlackForest, 2024) demonstrating remarkable capabilities in generating aesthetic and diverse images. However, these models often struggle to follow complex prompts to achieve precise compositional generation (Omost-Team, 2024; Yang et al., 2024b; Zhang et al., 2024b), which requires the model to possess robust, comprehensive capabilities in various aspects, such as attribute binding, spatial relationships, and non-spatial relationships (Huang et al., 2023). To enhance compositional generation, some works introduce additional conditions such as layouts/boxes (Li et al., 2023; Zhou et al., 2024; Wang et al., 2024a; Zhang et al., 2024b). InstanceDiffusion (Wang et al., 2024a) controls the generation process using layouts, masks, or other conditions through trainable instance masked attention layers. Although these layout-based methods demonstrate strong spatial awareness, they struggle with image realism, especially in generating non-spatial relationships and preserving aesthetic quality (Zhang et al., 2024b). Another potential solution leverContributed equally. Contact: yangling0818@163.com Corresponding authors. 1 Preprint. Figure 1: Motivation of IterComp. We select three types of compositional generation methods. The results show that different models exhibit distinct strengths across various aspects of compositional generation. fig. 3 further demonstrated these distinct strengths quantitatively. ages the impressive reasoning abilities of Large Language Models (LLMs) to decompose complex generation tasks into simpler subtasks (Yang et al., 2024b; Omost-Team, 2024; Wang et al., 2024b). RPG (Yang et al., 2024b) employs MLLMs as the global planner to transform the process of generating complex images into multiple simpler generation tasks within subregions. However, it requires designing complex prompts for LLMs, and it is challenging to achieve precise generation results due to their intricate outputs (Yang et al., 2024b). We conducted extensive experiments to explore the unique strengths of different models in compositional generation. As shown in the left example in fig. 1, text-to-image model FLUX (BlackForest, 2024) demonstrates impressive performance in attribute binding and aesthetic quality due to its advanced training techniques and model architecture. In contrast, layout-to-image model InstanceDiffusion (Wang et al., 2024a) struggles to capture fine-grained visual details, such as night scene or golden light. In the right example of fig. 1, where the text prompt involves complex spatial relationships between multiple objects, FLUX (BlackForest, 2024) exhibits limitations in spatial awareness. In contrast, InstanceDiffusion (Wang et al., 2024a) excels in handling spatial relationships through layout guidance. This demonstrates that different models exhibit distinct strengths across various aspects of compositional generation. Moreover, fig. 3 further demonstrated these distinct strengths quantitatively. Naturally, pertinent question arises: Is there method capable of excelling in all aspects of compositional generation? In order to enable the diffusion model to improve compositional generation comprehensively, we present new framework, IterComp, which collects composition-aware model preferences from various models, and then employs novel yet simple iterative feedback learning framework to achieve comprehensive improvements in compositional generation. Firstly, we select six open-sourced models excelling in different aspects of compositionality to form our model gallery. We focus on three essential compositional metrics: attribute binding, spatial relationships, and non-spatial relationships to curate new composition-aware model preference dataset, which consists of large number of image-rank pairs. Next, to comprehensively capture diverse composition-aware model preferences, we train reward models to provide fine-grained compositional guidance during the finetuning of the base diffusion model. Finally, given that compositional generation is difficult to optimize, we propose iterative feedback learning. This approach enhances compositionality in closed-loop manner, allowing for the progressive self-refinement of both the base diffusion model and reward models in multiple iterations. We theoretically and experimentally demonstrate the effectiveness of our method and its significant improvement in compositional generation. Our contributions are summarized as follows: We propose the first iterative composition-aware reward-controlled framework IterComp, to comprehensively enhance the compositionality of the base diffusion model. We curate model gallery and develop high-quality composition-aware model preference dataset comprising numerous image-rank pairs. 2 Preprint. We utilize new iterative feedback learning framework to progressively enhance both the reward models and the base diffusion model. Extensive qualitative and quantitative comparisons with previous SOTA methods demonstrate the superior compositional generation capabilities of our approach."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Compositional Text-to-Image Generation Compositional text-to-image generation is complex and challenging task that requires model with comprehensive capabilities, including the understanding of complex prompts and spatial awareness (Yang et al., 2024b; Zhang et al., 2024b). Some methods enhance prompt comprehension by using more powerful text encoders or architectures (Esser et al., 2024; Betker et al., 2023; Hu et al., 2024; Dai et al., 2023). Stable Diffusion 3 (Esser et al., 2024) utilizes three different-sized text encoders to enhance prompt comprehension. DALL-E 3 (Betker et al., 2023) enhances the understanding of rich textual details by expanding image captions through recaptioning. However, compositional capability such as spatial awareness remains limitation of these models (Li et al., 2023; Chen et al., 2024a). Other methods attempt to enhance spatial awareness by the control of additional conditions (e.g., layouts) (Yang et al., 2023; Dahary et al., 2024). BoxDiff (Xie et al., 2023) and LMD (Lian et al., 2023b) guide the generated objects to strictly adhere to the layout by designing energy functions based on cross-attention maps. ControlNet (Zhang et al., 2023) and T2I-Adapter (Mou et al., 2024) specify high-level image features to control semantic structures. Although these methods enhance spatial awareness, they often compromise image realism (Zhang et al., 2024b). Additionally, some approaches leverage the powerful reasoning capabilities of LLMs to assist in the generation process (Yang et al., 2024b; Omost-Team, 2024; Wang et al., 2024b). RPG (Yang et al., 2024b) employs MLLM to decompose complex compositional generation tasks into simpler subtasks. However, these methods require designing complex prompts as inputs to the LLM, and the diffusion model struggles to produce precise results due to the LLMs intricate outputs (Yang et al., 2024b). In contrast, our method extracts these preferences from different models in model gallery and trains composition-aware reward models to refine the base diffusion model iteratively, achieving robust compositionality across multiple aspects. Diffusion Model Alignment Building on the success of reinforcement learning from human feedback (RLHF) in Large Language Models (LLMs) (Ouyang et al., 2022; Bai et al., 2022), numerous methods in diffusion models have attempted to use similar approaches for model alignment (Lee et al., 2023; Fan et al., 2024; Sun et al., 2023). Some methods use pretrained reward model or train new one to guide the generation process(Zhang et al., 2024a; Black et al., 2023; Deng et al., 2024; Clark et al., 2023; Prabhudesai et al., 2023). For instance, ImageReward (Xu et al., 2024) manually annotated large dataset of human-preferred images and trained reward model to assess the alignment between images and human preferences. Reward Feedback Learning (ReFL) is proposed for tuning diffusion models with the ImageReward model. RAHF (Liang et al., 2024a) is trained on RichHF-18K, high-quality dataset rich in human feedback, and is capable of predicting the unreasonable parts in generated images. Some methods bypass the training of reward model and directly finetune diffusion models on human preference datasets (Yang et al., 2024a; Liang et al., 2024b; Yang et al., 2024c). Diffusion-DPO (Wallace et al., 2024) reformulates Direct Preference Optimization (DPO) to account for diffusion models notion of likelihood, utilizing the evidence lower bound to derive differentiable objective. The potential for alignment in diffusion models goes beyond this. We iteratively align the base model with composition-aware model preferences from the model gallery, effectively enhancing its performance on compositional generation."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we present our method, IterComp, which collects composition-aware model preferences from the model gallery and utilizes iterative feedback learning to enhance the comprehensive capability of the base diffusion model in compositional generation. An overview of IterComp is illustrated in fig. 2. In section 3.1, we introduce the method for collecting the composition-aware model preference dataset from the model gallery. In section 3.2, we describe the training process for the composition-aware reward models and multi-reward feedback learning. In section 3.2, we 3 Preprint. Figure 2: Overview of IterComp. We collect composition-aware model preferences from multiple models and employ an iterative feedback learning approach to enable the progressive self-refinement of both the base diffusion model and reward models. propose the iterative feedback learning framework to enable the self-refinement of both the base diffusion model and reward models, progressively enhancing compositional generation. 3.1 COLLECTING HUMAN PREFERENCES OF COMPOSITIONALITY Compositional Metric and Model Gallery We focus on three key aspects of compositionality: attribute binding, spatial relationships, and non-spatial relationships (Huang et al., 2023), to collect composition-aware model preferences. We initially select six open-sourced models excel in different aspects of compositional generation as our model gallery: FLUX-dev (BlackForest, 2024), Stable Diffusion 3 (Esser et al., 2024), SDXL (Podell et al., 2023), Stable Diffusion 1.5 (Rombach et al., 2022), RPG (Yang et al., 2024b), and InstanceDiffusion (Wang et al., 2024a). Human Ranking on Attribute Binding For attribute binding, we randomly select 500 prompts from each of the following categories: color, shape, and texture in the T2I-CompBench (Huang et al., 2023). Three professional experts ranked the images generated by the six models for each prompt, and their rankings were weighted to determine the final result. The primary criterion is whether the attributes mentioned in the prompt were accurately reflected in the generated images, especially the correct representation and binding of attributes to the corresponding objects. Human Ranking on Complex Relationships For spatial and non-spatial relationships, we select 1,000 prompts for each category from the T2I-CompBench (Huang et al., 2023) and apply the same manual annotation method to obtain the rankings. For spatial relationships, the primary ranking criterion is whether the objects are correctly generated and whether their spatial positioning matches the prompt. For non-spatial relationships, the focus is on whether the objects display natural and realistic actions. 2 Analysis of Composition-aware Model Preference Dataset For each prompt, we obtain 6 images and (cid:0)6 (cid:1) = 15 image-rank pairs. As shown in table 1, in total, we collected dataset with 22,500 image-rank pairs for model preference in attribute binding, 15,000 for spatial relationships, and 15,000 for non-spatial relationships. We visualize the proportion of generated images ranked first for each model in fig. 3. The results demonstrate that different models exhibit distinct strengths across various aspects of compositional generation, and this dataset effectively captures diverse range of composition-aware model preferences. 3.2 COMPOSITION-AWARE MULTI-REWARD FEEDBACK LEARNING Composition-aware Reward Model Training To achieve comprehensive improvements in compositional generation, we utilize three types of composition-aware datasets described in section 3.1, decomposing compositionality into three subtasks and training specific reward model for each. 4 Preprint. Table 1: Statistics on the composition-aware model preference dataset. The dataset consists of 3,500 text prompts, 27,500 images, and 52,500 image-rank pairs. Counts Category Texts Images Image-rank pairs Attribute Binding Spatial Relationship Non-spatial Relationship 1,500 1,000 1,000 9,000 6,000 6, Total 3,500 21,000 22,500 15,000 15,000 52,500 Figure 3: The proportion of each model ranked first. 0 and xl Specifically, the reward model Rθi(c, x0) is trained using the input format xw 0 c, where xw 0 denoting the winning and losing images, denoting the text prompt. We select two images corresponding to the same prompt from the composition-aware model preference datasets to form an input image-rank pair, and trained the reward model using the following loss function: 0 xl L(θi) = (c,xw 0 ,xl 0)Di (cid:2)log (cid:0)σ (cid:0)Rθi (c, xw 0 ) Rθi (cid:0)c, xl 0 (cid:1)(cid:1)(cid:1)(cid:3) (1) where denotes the composition-aware model preference dataset, σ() is the sigmoid function. The three composition-aware reward models apply BLIP (Li et al., 2022; Xu et al., 2024) as feature extractors. We combine the extracted image and text features with cross attention mechanism, and use learnable MLP to generate score scalar for preference comparison. Multi-Reward Feedback Learning Due to the multi-step denoising process in diffusion models, yielding likelihoods for their generations is impossible, making the RLHF approach used in language models unsuitable for diffusion models. Some existing methods (Xu et al., 2024; Zhang et al., 2024a) finetune diffusion models directly by treating the scores of the reward model as the human preference loss. To optimize the base diffusion model using multiple composition-aware reward models, we design the loss function as follows: L(θ) = λEcj (cid:88) (ϕ (Ri (cj, pθ (cj)))) (2) where = {c1, c2, . . . , cn} denotes the prompt set, pθ(c) denotes the generate image of diffusion model with parameter θ under the condition of prompt c. We calculate the loss for each reward model Ri() and sum them to obtain the multi-reward feedback loss. 3.3 ITERATIVE OPTIMIZATION OF COMPOSITION-AWARE FEEDBACK LEARNING Compositional generation is challenging to optimize due to its inherent complexity and multifaceted nature, requiring both our reward models and base diffusion model to excel in aspects such as complex text comprehension and the generation of complex relationships. To ensure more thorough optimization, we propose an iterative feedback learning framework that progressively refines both the reward models and the base diffusion model over multiple iterations. At the (k + 1)-th iteration of the optimization described in section 3.2, we denote the reward models and the base diffusion model from the previous iteration as Rk() and pk θ (), respectively. For each prompt in the datasets Dk, we sample an image θ (c) and expand the composition-aware model preference dataset Dk with the sampled image. The image rankings for each prompt are updated using the trained reward model Rk θ (), while preserving the relative ranks of the initial six images. Following this process, we update the composition-aware model preference dataset to more comprehensive version, denoted as Dk+1. Using this dataset, we finetune both the reward models and the base diffusion model to get Rk+1() and pk+1 (). The detailed process of iterative feedback learning can be found in algorithm 1. 0 = pk θ Effectiveness of Iterative Feedback Learning Through this iterative feedback learning framework, the reward models become more effective at understanding complex compositional prompts, providing more comprehensive guidance to the base diffusion model for compositional generation. The optimization objective of the iterative feedback learning process is formalized in the following lemma (proof provided in the appendix A.2): Preprint. Algorithm 1 Iterative Composition-aware Feedback Learning Dataset: Composition-aware model preference dataset D0 = {((c1, xw Prompt set = {c1, c2, . . . , cn} Input: Base model with pretrained parameters pθ, reward model R, reward-to-loss map function ϕ, reward re-weight scale λ, iterative optimization iterations iter Initialization: Number of noise scheduler time steps , time step range for finetuning [T1,T2] 0), . . . , (cn, xw 0 , xl 0 , xl 0)} (cid:0)c, xl 0 (cid:1)(cid:1)(cid:1) // Reward model loss // Update the reward models // Pick random timestep [T1, T2] 1: for = 0, . . . , iter do for (ci, xw 2: 3: 4: 0 , xl 0) Dk do log (cid:0)σ (cid:0)Rk θi (ci, xw Rk Rk θi end for // Get Rk+1 after training for ci do (c, xw 0 ) Rk θi 0 , xl 0) θi+1 rand(T1, T2) zT (0, I) for = T, . . . , + 1 do no grad: zj1 pk θi end for with grad: zt1 pk (zt) θi x0 VaeDec(z0) zt1 λϕ((cid:80) (ci, x0)) pk pk θi θi+1 θ Rk+1 (zj) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: // Predict image from the original latent // Multi-reward feedback learning loss θ // Update the base diffusion model 0) Dk do 0 , xl 0 pk+1(ci) end for // Get pk+1 after training for (ci, xw end for Dk+1 rank(Dk 0) 16: 17: 18: 19: 20: 21: end for // Sample images from the optimized base diffusion model // Expand the dataset and update ranking Lemma 1. The unified optimization framework of iterative feedback learning can be formulated as: max θ J(θ) = [cC,(xw 0 ,xl 0)p θ (c)] (cid:34) (cid:32) log σ β log θ (xw pref (xw 0:T c) 0:T c) β log (cid:33)(cid:35) θ pref (cid:0)xl (cid:0)xl 0:T c(cid:1) 0:T c(cid:1) (3) where p() denotes the optimized base diffusion model. We simplify the bilevel problem of iterative feedback learning into single-level objective. Based on this, we present the following theorem regarding the gradient of this objective: Theorem 1. Assume that Fθ(c, xw 0 , xl 0) = log σ (cid:18) β log θ (xw pref(xw 0:T c) 0:T c) β log (cid:19) θ(xl pref(xl 0:T c) 0:T c) , the gradient of optimization object can be written as the sum of two terms: θJ(θ) = T1 + T2, where: T1 = (cid:2)(cid:0)θ log pθ (xw 0:T c) + θ log pθ (cid:0)xl (cid:0)c, xw 0 , xl 0 (cid:1)(cid:3) T2 = [cC,(xw 0 ,xl 0)p θ (c)][θ[Fθ(c, xw 0:T c(cid:1)(cid:1) Fθ 0 , xl 0)]] (4) (5) It is evident that T2 represents the gradient form of direct preference optimization. In addition, we have another term T1, which guides the gradient of optimization objective. As shown in eq. (4), the gradient directs the generation of xw 0). The gradient term T1 helps the model better distinguish between winning and losing samples, increasing the probability of generating high-quality images while reducing the probability of generating low-quality images. This improves the models alignment with the reward models preferences during generation, thereby enhancing the comprehensive capabilities of compositional generation. 0 to optimize the implicit reward function Fθ(c, xw 0 and xw 0 , xl Superiority over Diffusion-DPO and ImageReward Here we clarify some superiorities of IterComp over Diffusion-DPO (Wallace et al., 2024) and ImageReward (Xu et al., 2024). Our IterComp first focuses on composition-aware rewards to optimize T2I models for realistic complex generation 6 Preprint. scenarios, and constructs powerful model gallery to collect multiple composition-aware model preferences. Then our novel iterative feedback learning framework can effectively achieve progressive self-refinement of both base diffusion model and reward models over multiple iterations. Figure 4: Qualitative comparison between our IterComp and three types of compositional generation methods: text-controlled, LLM-controlled, and layout-controlled approaches. IterComp is the first reward-controlled method for compositional generation, utilizing an iterative feedback learning framework to enhance the compositionality of generated images. Colored text denotes the advantages of IterComp in generated images. 7 Preprint. Table 2: Evaluation results about compositionality on T2I-CompBench (Huang et al., 2023). IterComp consistently demonstrates the best performance regarding attribute binding, object relationships, and complex compositions. We denote the best score in blue and the second-best score in green . The baseline data is quoted from GenTron (Chen et al., 2024b). Model Attribute Binding Object Relationship Complex Color Shape Texture Spatial Non-Spatial Stable Diffusion 1.4 (Rombach et al., 2022) Stable Diffusion 2 (Rombach et al., 2022) Attn-Exct v2 (Chefer et al., 2023) Stable Diffusion XL (Betker et al., 2023) PixArt-α (Chen et al., 2023) ECLIPSE (Patel et al., 2024) Dimba-G (Fei et al., 2024) GenTron (Chen et al., 2024b) GLIGEN (Li et al., 2023) LMD+ (Lian et al., 2023a) InstanceDiffusion (Wang et al., 2024a) IterComp (Ours) 0.3765 0.5065 0.6400 0.6369 0.6886 0.6119 0.6921 0.7674 0.4288 0.4814 0.5433 0. 0.3576 0.4221 0.4517 0.5408 0.5582 0.5429 0.5707 0.5700 0.3998 0.4865 0.4472 0.6217 0.4156 0.4922 0.5963 0.5637 0.7044 0.6165 0.6821 0.7150 0.3904 0.5699 0.5293 0. 0.1246 0.1342 0.1455 0.2032 0.2082 0.1903 0.2105 0.2098 0.2632 0.2537 0.2791 0.3196 0.3079 0.3096 0.3109 0.3110 0.3179 0.3139 0.3298 0.3202 0.3036 0.2828 0.2947 0. 0.3080 0.3386 0.3401 0.4091 0.4117 - 0.4312 0.4167 0.3420 0.3323 0.3602 0."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets and Training Setting The reward models are trained on the composition-aware model preference dataset, consisting of 3,500 prompts and 52,500 image-rank pairs. For training the three reward models, we finetune BLIP and the learnable MLP with learning rate of 1e 5 and batch size of 64. During the iterative feedback learning process, we randomly select 10,000 prompts from DiffusionDB (Wang et al., 2022) and use SDXL (Betker et al., 2023) as the base diffusion model, finetuning it with learning rate of 1e 5 and batch size of 4. We set = 40, [T1, T2] = [1, 10], ϕ = ReLU, and λ = 1e 3. All experiments are conducted on 4 NVIDIA A100 GPUs. Baseline Models We curate model gallery of six open-source models, each excelling in different aspects of compositional generation: FLUX (BlackForest, 2024), Stable Diffusion 3 (Esser et al., 2024), SDXL (Betker et al., 2023), Stable Diffusion 1.5 (Rombach et al., 2022), RPG (Yang et al., 2024b), and InstanceDiffusion (Wang et al., 2024a). To ensure the base diffusion model thoroughly and comprehensively learns composition-aware model preferences, we progressively expand the model gallery by incorporating new models (e.g., Omost (Omost-Team, 2024), Stable Cascade (Pernias et al., 2023), PixArt-α (Chen et al., 2023)) at each iteration. For performance comparison in compositional generation, we select several state-of-the-art methods, including FLUX (BlackForest, 2024), SDXL (Betker et al., 2023), and RPG (Yang et al., 2024b) to compare with our approach. We use GPT-4o (OpenAI, 2024) for the LLM-controlled methods and to infer the layout from the prompt for the layout-controlled methods. 4.2 MAIN RESULTS Qualitative Comparison As shown in fig. 4, IterComp achieves superior compositional generation results compared to the three main types of compositional generation methods: text-controlled, LLM-controlled, and layout-controlled approaches. In comparison to text-controlled methods FLUX (BlackForest, 2024), IterComp excels in handling spatial relationships, significantly reducing errors such as object omissions and inaccuracies in numeracy and positioning. When compared to LLM-controlled methods like RPG (Yang et al., 2024b), IterComp produces more reasonable object placements, avoiding the unrealistic positioning caused by LLM hallucinations. Compared to layout-controlled methods like InstanceDiffusion (Wang et al., 2024a), IterComp demonstrates clear advantage in both semantic aesthetics and compositionality, particularly when generating under complex prompts. Quantitative Comparison We compare IterComp with previous outstanding compositional text/layout-to-image models on the T2I-CompBench (Huang et al., 2023) in six key compositional scenarios. As shown in table 2, IterComp demonstrates remarkable preference across all evaluation tasks. Layout-controlled methods such as LMD+ (Lian et al., 2023a) and InstanceDiffusion 8 Preprint. Table 3: Evaluation on image realism. Table 4: Evaluation on inference time. Model CLIP Score Aesthetic Score ImageReward Model Inference Time Stable Diffusion 1.4 (Rombach et al., 2022) Stable Diffusion 2.1 (Rombach et al., 2022) Stable Diffusion XL (Betker et al., 2023) GLIGEN (Li et al., 2023) LMD+ (Lian et al., 2023a) InstanceDiffusion (Wang et al., 2024a) IterComp (Ours) 0.307 0.321 0. 0.301 0.298 0.302 0.337 5.326 5.458 5.531 4.892 4.964 5.042 5.936 -0.065 0.216 0. -0.077 -0.072 -0.035 1.437 FLUX-dev Stable Diffusion XL (Betker et al., 2023) Omost (Omost-Team, 2024) RPG (Yang et al., 2024b) InstanceDiffusion (Wang et al., 2024a) IterComp (Ours) 23.02 s/Img 5.63 s/Img 21.08 s/Img 15.57 s/Img 9.88 s/Img 5.63 s/Img (Wang et al., 2024a) excel in generating accurate spatial relationships, while text-to-image models like SDXL (Betker et al., 2023) and GenTron (Chen et al., 2024b) exhibit particular strengths in attribute binding and non-spatial relationships. In contrast, IterComp achieves comprehensive improvement in compositional generation. It obtains the strengths of various models by collecting composition-aware model preferences, and employs novel iterative feedback learning to enable self-refinement of both the base diffusion model and reward models in closed-loop manner. IterComp achieves high level of compositionality while simultaneously enhancing the realism and aesthetics of the generated images. As shown in table 3, we evaluate the improvement in image realism by calculating the CLIP Score, Aesthetic Score, and ImageReward. IterComp significantly outperforms previous models across all three scenarios, demonstrating remarkable fidelity and precision in alignment with the complex text prompt. These promising results highlight the versatility of IterComp in both compositionality and fidelity. We provide more quantitative comparison results between IterComp and other diffusion alignment methods in appendix A.3. IterComp requires less time to generate high-quality images. In table 4, we compare the inference time of IterComp with other outstanding models, such as FLUX (BlackForest, 2024), RPG (Yang et al., 2024b) in generating single image. Using the same text prompts and fixing the denoising steps to 40, IterComp demonstrates faster generation, because it avoids the complex attention computations in RPG and Omost. Our method can incorporate composition-aware knowledge from different models without adding any computational overhead. This efficiency highlights its potential for various applications and offers new perspective on handling complex generation tasks. User Study We conducted comprehensive user study to evaluate the effectiveness of IterComp in compositional generation. As illustrated in fig. 5, we randomly selected 16 prompts for each comparison, and invited 23 users from diverse backgrounds to vote on image compositionality, resulting in total of 1,840 votes. The results show that IterComp received widespread user approval in compositional generation. 4.3 ABLATION STUDY Figure 5: Results of user study. (a) Impact on CLIP Score. (b) Impact on Aesthetic Score. (c) Impact on ImageReward. Figure 6: Ablation study on the model gallery size. Effect of Model Gallery Size In the ablation study on model gallery size, as shown in fig. 6, we observe that increasing the size of the model gallery leads to improved performance for IterComp across various evaluation tasks. To leverage this finding and provide more fine-grained reward Preprint. Figure 7: Ablation study on the iterations of feedback learning. Figure 8: The generation performance of integrating IterComp into RPG and Omost. guidance, we progressively expand the model gallery over multiple iterations by incorporating the optimized base diffusion model and new models such as Omost (Omost-Team, 2024). Effect of composition-aware iterative feedback learning We conducted an ablation study (see fig. 7) to evaluate the impact of composition-aware iterative feedback learning. The results show that this approach significantly improves both the accuracy of compositional generation and the aesthetic quality of the generated images. As the number of iterations increases, the models preferences gradually converge. Based on this observation, we set the number of iterations to 3 in IterComp. 4.4 GENERALIZATION STUDY IterComp can serve as powerful backbone for various compositional generation tasks, leveraging its strengths in spatial awareness, complex prompt comprehension, and faster inference. As shown in fig. 8, we integrate IterComp into Omost (Omost-Team, 2024) and RPG (Yang et al., 2024b). The results demonstrate that equipped with the more powerful IterComp backbone, both Omost and RPG achieve excellent compositional generation performance, highlighting IterComps strong generalization ability and potential for broader applications. 10 Preprint."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose novel framework, IterComp, to address the challenges of complex and compositional text-to-image generation. IterComp aggregates composition-aware model preferences from model gallery and employs an iterative feedback learning approach to progressively refine both the reward models and the base diffusion models over multiple iterations. For future work, we plan to further enhance this framework by incorporating more complex modalities as input conditions and extending it to more practical applications."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "The author team would like to deliver sincere thanks to Ruihang Chu from Tsinghua University for his significant suggestions for the refinement of this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. BlackForest. Black forest labs; frontier ai lab, 2024. URL https://blackforestlabs.ai/. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer guidance. Vision, pp. 53435353, 2024a. Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 64416451, 2024b. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multi-subject text-to-image generation. arXiv preprint arXiv:2403.16990, 2024. Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. Fei Deng, Qifei Wang, Wei Wei, Tingbo Hou, and Matthias Grundmann. Prdp: Proximal reward difference prediction for large-scale reward finetuning of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 74237433, 2024. 11 Preprint. Mucong Ding, Souradip Chakraborty, Vibhu Agrawal, Zora Che, Alec Koppel, Mengdi Wang, Amrit Bedi, and Furong Huang. Sail: Self-improving efficient online alignment of large language models. arXiv preprint arXiv:2406.15567, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang. Dimba: Transformer-mamba diffusion models. arXiv preprint arXiv:2406.01159, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2251122521, 2023. Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023a. Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv preprint arXiv:2305.13655, 2023b. Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1940119411, 2024a. Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Ji Li, and Liang Zheng. Step-aware preference optimization: Aligning preference with denoising performance at each step. arXiv preprint arXiv:2406.04314, 2024b. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 42964304, 2024. Omost-Team. Omost github page, 2024. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. 12 Preprint. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: In Proceedings of the IEEE/CVF resource-efficient text-to-image prior for image generations. Conference on Computer Vision and Pattern Recognition, pp. 90699078, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. arXiv Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. preprint arXiv:2306.00637, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning text-to-image generation with image understanding feedback. In Synthetic Data for Computer Vision Workshop@ CVPR 2024, 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62326242, 2024a. Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. arXiv preprint arXiv:2407.05600, 2024b. Zijie Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. arXiv preprint arXiv:2210.14896, 2022. Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 74527461, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. 13 Preprint. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 89418951, 2024a. Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering textto-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024b. Shentao Yang, Tianqi Chen, and Mingyuan Zhou. dense reward view on aligning text-to-image diffusion with preference. arXiv preprint arXiv:2402.08265, 2024c. Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng In ProLiu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14246 14255, 2023. Jiacheng Zhang, Jie Wu, Yuxi Ren, Xin Xia, Huafeng Kuang, Pan Xie, Jiashi Li, Xuefeng Xiao, Weilin Huang, Min Zheng, et al. Unifl: Improve stable diffusion via unified feedback learning. arXiv preprint arXiv:2404.05595, 2024a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin Cui. Realcompo: Dynamic equilibrium between realism and compositionality improves text-to-image diffusion models. arXiv preprint arXiv:2402.12908, 2024b. Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 68186828, 2024. Preprint."
        },
        {
            "title": "A APPENDIX",
            "content": "This supplementary material is structured into several sections that provide additional details and analysis related to IterComp. Specifically, it will cover the following topics: In appendix A.1, we provide preliminary about Stable Diffusion (SD) and Reward Feedback Learning (ReFL). In appendix A.2, we provide detailed theoretical proof of the effectiveness of iterative feedback learning. In appendix A.3, we present the quantitative comparison results between IterComp and other diffusion alignment methods. In appendix A.4, we provide more visualization results for IterComp and its base diffusion model, SDXL. A.1 PRELIMINARY Stable Diffusion Stable Diffusion (SD) (Rombach et al., 2022) performs multi-step denoising on random noise zT (0, I) to generate clear latent z0 in the latent space under the guidance of text prompt c. During the training, an input image x0 is processed by pretrained autoencoder to obtain its latent representation z0. random noise ϵ (0, I) is injected into z0 in the forward process as follow: (6) where αt is the noise schedule. The UNet ϵθ is trained to predict the added noise with the optimization objective: 1 αtϵ αtz0 + zt = min θ L(θ) = E[z0E(x0),ϵN (0,I),t] (cid:104) ϵ ϵθ(zt, t, τ (c))2 2 (cid:105) (7) where E() denote the preteained encoder of VAE, τ () denotes the pretrained text encoder. Reward Feedback Learning Reward Feedback Learning (ReFL) (Xu et al., 2024) is proposed to align diffusion models with human preferences. The reward model serves as the preference guidance during the finetuning of the diffusion model. ReFL begins with an input prompt and random noise zT (0, I). The noise zT is progressively denoised until it reaches randomly selected timestep t. The latent z0 is directly predicted from zt, and the decoder from pretrained VAE is used to generate the predicted image x0. The pretrained reward model R() provides reward score as feedback, which is used to finetune the diffusion model as follows: L(θ) = EcC (R (c, x0)) min θ (8) where the prompt is randomly selected from the prompt dataset C. A.2 THEORETICAL PROOF OF THE EFFECTIVENESS OF ITERATIVE FEEDBACK LEARNING A.2.1 PROOF OF LEMMA 1 Proof of Lemma 1. Considering the general form of RLHF, we change the optimization problem of iterative feedback learning to bilevel optimization (Wallace et al., 2024; Ding et al., 2024): (cid:2)log σ (cid:0)R (c, xw 0 ) (cid:0)c, xl (cid:1)(cid:1)(cid:3) [cC,(xw 0 ,xl := arg max 0)p EcC R(c)] (cid:2)Ex0p(c)R(c, x0)(cid:3) βDKL[p (x0:T c) pref (x0:T c)] min s.t. (9) 0 R denotes the optimized base models under the guidance of reward model R. We have the where reparameterization of the reward model (also shown in previous works by (Wallace et al., 2024)): (cid:20) (cid:21) R(c, x0) = βEpR(x1:T x0,c) log (x0:T c) pref (x0:T c) + β log Z(c) (10) (11) Z(c) = (cid:88) pref (x0:T c) exp (R(c, x0)/β) 15 Preprint. Substituting this reward reparameterization into eq. (9), we get the new optimization objective as: min E [cC,(xw 0 ,xl 0)p R(c)] (cid:34) (cid:32) log σ β log (xw pref (xw 0:T c) 0:T c) β log (cid:33)(cid:35) pref (cid:0)xl (cid:0)xl 0:T c(cid:1) 0:T c(cid:1) This new optimization objective is denoted as J(p max J(p R) = [cC,(xw 0 ,xl 0)p R(c)] log σ (cid:34) (cid:32) R), we get: (xw pref (xw β log 0:T c) 0:T c) β log pref (cid:33)(cid:35) (cid:0)xl (cid:0)xl 0:T c(cid:1) 0:T c(cid:1) We use pθ to parameterize the policy and formulate the final optimization objective as: max θ J(θ) = [cC,(xw 0 ,xl 0)p θ (c)] (cid:34) (cid:32) log σ β log θ (xw pref (xw 0:T c) 0:T c) β log A.2.2 PROOF OF THEOREM 1 (cid:33)(cid:35) θ pref (cid:0)xl (cid:0)xl 0:T c(cid:1) 0:T c(cid:1) (12) (13) (14) Proof of Theorem 1. The gradient of the optimization objective in eq. (14) can be written as: (cid:34) 0:T c(cid:1) 0:T c) 0:T c(cid:1) θ (xw pref (xw 0:T c) 0:T c) 0:T c) pθ(xl θJ(θ) = θ (cid:0)xl (cid:0)xl θ pref pθ(xw β log β log log σ (cid:88) (cid:32) (cid:33)(cid:35) c,xw 0 ,xl Assume that: Fθ(c, xw 0 , xl 0) = log σ (cid:32) β log θ (xw pref (xw 0:T c(cid:1) = pθ (xw 0:T c) 0:T c) 0:T c) pθ(xl β log 0:T c) θ pref (cid:0)xl (cid:0)xl (cid:33) 0:T c(cid:1) 0:T c(cid:1) ˆpθ (cid:0)xw 0:T , xl The gradient can be decomposed into two terms: θJ(θ) = θ (cid:88) ˆpθ c,xw 0 ,xl 0 (cid:0)xw 0:T , xl 0:T c(cid:1) Fθ(c, xw 0 , xl 0) (15) (16) (17) (cid:88) = c,xw (cid:124) 0 ,xl 0 θ ˆpθ (cid:0)xw 0:T ,xl 0:T c(cid:1)Fθ(c, xw 0 , xl 0) [cC,(xw 0 ,xl 0)p θ (c)][θ[Fθ(c, xw 0 , xl (cid:123)(cid:122) T2 0)]] (cid:125) +E (cid:124) (cid:125) (cid:123)(cid:122) T1 By expanding the distribution ˆpθ in T1, more specific form is obtained: (cid:0)xw 0:T c(cid:1) Fθ(c, xw 0 , xl 0) 0:T ,xl θ ˆpθ T1 = (cid:88) c,xw 0 ,xl 0 = (cid:2)(cid:0)θ log pθ (xw 0:T c) + θ log pθ (cid:0)xl 0:T c(cid:1)(cid:1) Fθ (cid:0)c, xw 0 , xl 0 (cid:1)(cid:3) (18) (19) A.3 QUANTITATIVE COMPARISON WITH OTHER DIFFUSION ALIGNMENT METHODS. We compare IterComp with state-of-the-art diffusion alignment methods, Diffusion-DPO (Wallace et al., 2024) and ImageReward (Xu et al., 2024) in terms of image compositionality and realism. We calculate the average results of these models on T2I-CompBench (Huang et al., 2023), and evaluate image realism via CLIP Score and Aesthetic Score. As demonstrated in table 5, IterComp significantly outperforms previous diffusion alignment methods across all three scenarios. IterComp aggregates composition-aware model preferences from multiple models, which are used to train reward models. Guided by these composition-aware reward models, it achieves comprehensive improvements in compositional generation. Its superior performance in image realism is attributed to the effectiveness of iterative feedback learning, where the self-refinement of both the base diffusion model and reward models across multiple iterations drives significant gains in both compositionality and realism. 16 Preprint. Table 5: Comparison between IterComp and other diffusion alignment methods. Model Average Result on T2I-CB CLIP Score Aesthetic Score Stable Diffusion XL (Betker et al., 2023) Diffusion-DPO (Wallace et al., 2024) ImageReward (Xu et al., 2024) IterComp (Ours) 0.4441 0.4417 0.4639 0.5554 0.322 0.326 0.323 0. 5.531 5.572 5.613 5.936 A.4 MORE VISUALIZATION RESULTS Figure 9: More visualization results for IterComp and its base diffusion model, SDXL."
        }
    ],
    "affiliations": [
        "LibAI Lab",
        "Peking University",
        "Princeton University",
        "Tsinghua University",
        "USTC",
        "University of Oxford"
    ]
}