{
    "paper_title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication",
    "authors": [
        "Alessandro Giagnorio",
        "Antonio Mastropaolo",
        "Saima Afrin",
        "Massimiliano Di Penta",
        "Gabriele Bavota"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance."
        },
        {
            "title": "Start",
            "content": "Noname manuscript No. (will be inserted by the editor) Quantizing Large Language Models for Code Generation: Differentiated Replication Alessandro Giagnorio Antonio Mastropaolo Saima Afrin Massimiliano Di Penta Gabriele Bavota 5 2 0 2 0 ] . [ 1 3 0 1 7 0 . 3 0 5 2 : r Received: date / Accepted: date Abstract Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLMs trainable parameters the better its ability to implement code. However, when it comes to deploying LLMbased code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit preciA. Giagnorio and G. Bavota SEART @ Software Institute Universit`a della Svizzera italiana, Switzerland E-mail: {alessandro.giagnorio, gabriele.bavota}@usi.ch A. Mastropaolo and S. Afrin William & Mary, United States E-mail: {amastropaolo, safrin}@wm.edu M. Di Penta University of Sannio, Italy E-mail: dipenta@unisannio.it 2 Alessandro Giagnorio et al. sion, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), code-specific calibration dataset helps to limit the loss of performance. Keywords Empirical Software Engineering Pre-Trained Models AI For Software Engineering Green AI 1 Introduction The advent of deep learning (DL) in software engineering pushed the boundaries of automated code generation, with techniques and tools able to implement complete software pieces of functionality starting from natural language description. The most recent and advanced code generators are grounded on Large Language Models (LLMs), namely DL models featuring billion parameters. Besides closed commercial software-specific tools such as GitHub CoPilot (cop, 2021) and general purpose ones such as GPT4 (Achiam et al., 2023), there also exist several open-source LLMs specifically pre-trained on code-related data. These include CodeLlama (Roziere et al., 2023) and DeepSeek Coder (Guo et al., 2024), both being families of code-specialized LLMs of different sizes. While LLMs opened up new frontiers in code generation, their usage has downsides. On the one hand, developers could leverage closed models, often available through APIs and often also integrated into the development environments. This is the case, for example, of GitHub CoPilot (cop, 2021) or GPT4 (Achiam et al., 2023). While in such cases, the training and inference are performed on third-party servers, using such models introduces data privacy issues (Novelli et al., 2024; Wu et al., 2024; Huang et al., 2023). Also, such models are general-purpose and not customized to work on specific, proprietary technology. An alternative would be to leverage open-source models by training and using them locally. However, the large number of trainable parameters implies conspicuous cost and environmental impact during the training phase. Also, such models impose hardware requirements for the inference phase (i.e., when the LLM is used to generate recommendations). For example, DeepSeek Coder-33B would require, at inference time, 64GB of memory (VRAM), which is more than what most of modern laptops are equipped with. Certainly, this solution could be circumvented by deploying models for inference on local servers and using them as services, yet this still implies costs, increased complexity, and network overhead. One way of reducing models requirements at inference time is quantization. In its general meaning, the term quantization refers to the process of approximating large set of numbers with smaller set. This happens, for example, in signal processing, when an audio (analog) signal is converted into its digital form using given number of bits. The more bits, the better the sound quality, but the larger the resulting file. In the context of LLM, quantization implies representing the neural network weights, usually expressed as Title Suppressed Due to Excessive Length 3 16-bit floating-point (fp16), with simpler values, e.g., 8-bit integers int8. The quantization procedure is performed on the already trained model which, in our context, could be one of the code-specific LLMs described above (e.g., DeepSeek Coder). To do so, an additional (but cheap) calibration process is performed to best approximate the original model weights. Then, in the compression phase, the original weights of the model are replaced by the learned approximations. This drastically reduces the memory footprint. Wei et al. (2023a) have been the first investigating code generation via quantization, showing that by quantizing the weights of 16B-parameters LLM (i.e., CodeGen (Nijkamp et al., 2023)) to int8 it was possible to reduce its memory footprint by 29% with negligible reduction in the code generation capabilities. Due to the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present differentiated replication of the study conducted by Wei et al. (2023a). In particular, we experiment with the latest advancements in model quantization techniques, which allow us to push the compression to extreme quantization levels such as 2 bits per model parameter. Also, we consider different types of calibration datasets to guide the quantization process (i.e., the training data used to approximate the original weights), including code-specific ones. Last but not least, we focus on newer and larger LLM-based code generators, up to 34B parameters, and extend the study to two programming languages (Python and Java) instead of Python-only (Wei et al., 2023a). We consider two families of code-specific LLMs, namely CodeLlama (Roziere et al., 2023) and DeepSeek Coder (Guo et al., 2024), experimenting with three of their variants: CodeLlama 7B, 13B, and 33B, and DeepSeek Coder 1.3B, 7B, and 33B. We apply on them the recently proposed Additive Quantization with Learned Multi-Codebooks (AQLM) (Egiazarian et al., 2024) quantization techniques, which achieved new state-of-the-art results for NLP benchmarks at extreme quantization levels (e.g., 2 bits). In particular, we experiment with quantizations at 8, 4, 3, and 2 bits. We assess the code generation capabilities of the baseline (i.e., the original model) and of the quantized models on two benchmarks, MultiPL-E (Cassano et al., 2023) and McEval (Chai et al., 2024). We focus on the Java and Python instances from the two benchmarks for total of 414 implementation problems. We report the percentage of cases in which the subject models managed to correctly implement the required code at the first attempt. On top of that, we experiment with three different calibration datasets: (i) general-purpose dataset with randomly sampled content (referred to as the random dataset); (ii) mixed dataset comprising 50% GitHub code files and 50% samples from Stack Overflow discussions; and (iii) code-specific dataset containing only Java and Python code. The obtained results show that, thanks to novel quantization techniques, such as AQLM, it is possible to quantize LLMs for the task of code generation down to 4 bits without observing any significant decrease in performance. Such process results in memory footprint reduced by 70%, allowing the deployment of very large models also on hardware-constrained devices (and, in general, lowering the deployment cost). Interestingly, the impact of extreme 4 Alessandro Giagnorio et al. quantization levels on software-related practices has yet to be explored. To the best of our knowledge, we are the first to demonstrate the feasibility of reducing precision below 4 bits, going as low as 3 and 2 bits, across multiple large code models and benchmarks. Yet, this results in decrease in performance, though it can be mitigated by adopting calibration dataset that includes code. Finally, we found that larger code models are more resilient to information loss during extreme quantization (e.g., 2-bit quantization). Our study, on the one side, confirms the findings by Wei et al. (2023a) with larger models, multiple programming languages, and different quantization techniques. On the other side, and more importantly, it shows that the recent progress makes quantization even more attractive, due to the potential for extreme quantization. By pushing the precision down to 3 and 2 bits, we are laying the groundwork for more efficient code-specialized LLMs. This, in turn, reduces computational costs and memory footprint, with trade-off in terms of performance that remains manageable. We release all code and data used in our study, as well as the quantized models (Giagnorio et al., 2025). The paper is organized as follows. Section 2 provides background notions and discusses related literature concerning on the one hand the use of LLMs in software engineering and, on the other hand, techniques to improve the efficiency of deep learning models. Section 3 defines the study and details its planning and analysis methodology. Results are reported and discussed in Section 4. Then, Section 5 outlines implications of our finding for both research and practice, while the studys threats to validity are discussed in Section 6. Finally, Section 7 concludes the paper and outlines directions for future work. 2 Background and Related Work In this section, we start (Section 2.1) by highlighting the multifaceted capabilities of LLMs in handling various complex tasks, particularly those related to software engineering, such as bug-fixing and code generation. Next (Section 2.2), we review the relevant literature concerning, on the one hand, the environmental impact of machine learning models and, on the other hand, techniques to make models more efficientespecially in the context of software engineeringand consequently more environment-friendly. In particular, we focus on model compression techniques. Finally (Section 2.3), we explain the quantization technique employed in our study, specifically AQLM. 2.1 Large Language Models for Software Engineering LLMs are specialized subset of neural network models engineered to understand, manipulate, and generate natural language. The remarkable capabilities of these AI-driven models arise from their extensive pre-training on vast amounts of textual data from diverse sources, combined with the large number Title Suppressed Due to Excessive Length 5 of parameters they encompass. Notable examples in this field include GPT-4 (Achiam et al., 2023), Claude (Anthropic, 2024), and Gemini (Team et al., 2023), all of which boast over 100 billion parameters. LLMs have been successfully applied across various areas in software engineering including, but not limited to, requirements analysis (Marques et al., 2024), code generation (Li et al., 2022; Mastropaolo et al., 2024), code translation (Fan et al., 2023), code summarization (Haldar and Hockenmaier, 2024; Ahmed et al., 2024; Ahmed and Devanbu, 2022; Sun et al., 2024a), and test case generation (Tufano et al., 2020). While general-purpose LLMs like GPT4 and Claude can support various domains, researchers have also developed domain-specific models tailored to tackle software engineering activities and code-related tasks. For instance, open-source LLMs such as InCoder (Fried et al., 2023), CodeLlama (Roziere et al., 2023), StarCoder (Li et al., 2023b), DeepSeek-Coder (Guo et al., 2024) and SantaCoder (Allal et al., 2023) have been designed specifically to handle these tasks, offering specialized capabilities that can substantially automate software-related tasks. 2.2 Efficiency and Environmental Impact of AI models for SE The deployment of billion-parameter LLMs imposes substantial computational demands and considerable energy requirements, often reliant on non-renewable resources, sparking concerns about environmental sustainability. Recently, Castano et al. (2023) documented the correlation between the DL models carbon footprint and various factors like model size and parameter count. The significant energy consumption and carbon emissions associated with LLMs have also been widely documented (Patterson et al., 2021; Strubell et al., 2020), emphasizing the urgent need for techniques aimed at making these models energyefficient. In the remainder of this section, we highlight recent advancements in Green AI and how these are being translated into the software engineering domain. Parameter-Efficient Fine-Tuning (PEFT) optimizes fine-tuning by updating only subset of the models parameters. This can be done by introducing additional layers (Houlsby et al., 2019), decomposing weight gradients into specialized matrices (Hu et al., 2022), or via prompt-tuning techniques (Lester et al., 2021; Li and Liang, 2021). PEFT has been applied in tasks like code generation and code summarization. For example, Wang et al. (2023) introduced Adapter tuning for code search and summarization, while Ayupov and Chirkova (2022) evaluated the effectiveness of two PEFT methods, Adapters (Wang et al., 2023) and LoRA (Hu et al., 2022), for tasks such as code generation and summarization, reporting significant performance gains. Weyssow et al. (2023) explored four PEFT techniques: LoRA, IA3 (Liu et al., 2022), prompt tuning (Lester et al., 2021), and prefix tuning (Li and Liang, 2021) in the context of code generation. Another study (Liu et al., 2023) compared Adapter, LoRA, prefix tuning, and MHM (He et al., 2022) PEFT methods for code generation and summarization. The above-discussed works 6 Alessandro Giagnorio et al. report positive findings, showing the effectiveness of PEFT even when compared to classic fine-tuning updating all model parameters. Knowledge Distillation (KD) belongs to the class of model compression techniques and aims at teaching smaller model to emulate larger one (Hsieh et al., 2023). Noteworthy KD applications in code-related models include CodeAlpaca (Chaudhary, 2023), Magicoder (Wei et al., 2023b), WaveCoder (Yu et al., 2023), and MFTCoder (Liu et al., 2023), most of which focus on code generation. Studies like the one by Su and McMillan (2024) have shown that distilled models can match the performance of much larger counterparts like GPT-3.5 in specific tasks (e.g., code summarization) without significant losses in performance metrics. Although KD unlocks the possibility of using smaller models mimicking the capabilities of larger one, an expensive training phase remains necessary. Quantization is model compression technique that aims at reducing the models memory footprint by representing its parameters (weights and/or activations) in lower-precision format, such as 8-bit integers, instead of the standard 16-bit or 32-bit floating points (Gholami et al., 2022). Quantization can be applied through two possible strategies: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT (Esser et al., 2020) integrates quantization during training, necessitating complete model retraining. Specific adaptations for LLMs, such as LLM-QAT (Liu et al., 2024) and EdgeQAT (Shen et al., 2024), can usually be applied to LLMs having limited size. Indeed, the high GPU time and memory demands render QAT impractical for large-scale LLMs, making PTQ preferable alternative. PTQ (Cai et al., 2020) converts pre-trained models to fixed-point networks without revisiting the initial training stages. This method requires the model parameters to be adjusted to minimize quantization errors using calibration dataset, but such an additional training is quite cheap to run. PTQ techniques can be further divided into weights-only and weight-activation quantization. Weights-only quantization, such as GPTQ (Frantar et al., 2022) and strategies in PB-LLM (Yuan et al., 2024) and SpQR (Dettmers et al., 2024), focuses on minimizing precision loss by adjusting weight bit-widths and applying scale transformations for critical weight channels. Weight-activation quantization, exemplified by SmoothQuant (Xiao et al., 2023) and LLM.int8() (Dettmers et al., 2022), compresses both weights and activations, utilizing techniques like mixed-precision decomposition and channel-wise scaling. For ultra-low bit quantization ( 4), advanced PTQ methods such as QuIP# (Tseng et al., 2024) and Additive Quantization of Language Models (AQLM ) (Egiazarian et al., 2024) have been developed, with the latter outperforming QuIP# and achieving Pareto-optimal performance (i.e., maximization of accuracy for given models size) in extreme compression scenarios (less than 3 bits per parameter). This is done thanks to the application of novel technique named MCQ (Multi-Codebook Quantization). These techniques have been tested on LlaMA (Touvron et al., 2023) and OPT (Zhang et al., 2022) on natural language-related benchmarks. Title Suppressed Due to Excessive Length In the software engineering domain, little research has been conducted in this direction. Sun et al. (2024b) studied Dynamic Inference to optimize the code completion task and improve LLMs efficiency. Their proposed technique (Stop&Exit Controller) skips, on average, 1.7 layers of the analyzed LLMs, improving the inference time by 11.2% with negligible accuracy loss. Differently from this study, we target the code generation task and aim at reducing the models memory footprint while preserving the original performance. dAloisio et al. (2024) investigated the effects of several compression techniques (knowledge distillation, quantization, and pruning) on code tasks such as vulnerability detection, code summarization, and code search. Their findings show that, while these techniques can help to reduce computational costs and inference time, they may result in significant performance loss. Quantization, instead, is the most promising technique for achieving comparable accuracy to the original model at lower memory costs. Our study explores more in-depth the quantization technique on the code generation task and employs the state-of-the-art technique for lowering the model precision down to 2 bits. The most related study is the investigation by Wei et al. (2023a), which inspired our work. In this research, the authors explored 8-bit quantization of code models (PLBART, Code-T5, InCoder, CodeGen) for code generation. Their findings revealed that 8-bit quantized versions of the CodeGen and InCoder models achieved improved energy efficiency accompanied by limited drop in performance, particularly for the largest model featuring 16B parameters. Our study looks at new state-of-the-art code models - CodeLlama and DeepSeek-Coder (up to 34B parameter models), and employs AQLM for extreme quantization down to 2 bits. Further details about AQLM are discussed in Section 2.3. Moreover, as recent research (Williams and Aletras, 2024) highlights the critical role of calibration data in determining model performance, our study also investigates the effect of different calibration datasets on the quantization outcomes of these large code models. As documented later, we experiment with variety of calibration datasets, including those that combine code and natural language, showing their benefits at extreme quantization levels (3 and 2 bits). 2.3 Additive Quantization of Language Models (AQLM) For compressing LLM weights, the direct quantization method involves selecting an appropriate quantization grid and normalization for each subcomponent of the matrix. Subsequently, weights are mapped onto this grid either through direct rounding (Round-To-Nearest) (Dettmers et al., 2022) or through more sophisticated methods. In contrast, AQLM employs MultiCodebook Quantization (MCQ): generalization of vector quantization (VQ) used in the approximate nearest neighbor search algorithms quantizes multiple vector dimensions simultaneously by learning codebookssets of learnable 8 Alessandro Giagnorio et al. candidate vectors for data encoding. In VQ, database vector is divided into sub-groups, with each group encoded using vector from learned codebook. This process facilitates efficient computation of distances or dot products for similarity searches by exploiting the linearity of dot products. Additive Quantization (AQ) (Babenko and Lempitsky, 2014), popular MCQ algorithm, performs vector optimization in way that for linear layer with din input and dout output features, characterized by its weight matrix Rdoutdin and set of calibration inputs Rdinn, the objective is to find configuration of quantized weights Ë†W that minimizes the squared error between the outputs of the original and compressed layers. To do so, AQ divides weight rows into groups of consecutive elements and represents each group by summing vectors selected from multiple codebooks C1, . . . , CM , where each codebook contains 2B vectors for B-bit encoding. Each groups weight is quantized by combining chosen vector from each codebook, encoded as one-hot vector bm. The full-weight representation is then achieved by concatenating these quantized segments according to the following structure: Wci = m=1 Cm,bi,1,m m=1 Cm,bi,g,din ,m where denotes concatenation and each bi,j,m is one-hot code that specifies the selection from the m-th codebook for the i-th output unit and the j-th group of input dimensions. With these techniques, AQLM ensures the integrity of the output from each layer and Transformer block. 3 Design The goal of this study is to conduct differentiated replication of the work by Wei et al. (2023a) to investigate the impact of AQLM, the quantization technique summarized in Section 2.3, on the performance of LLMs when applied to the code generation task. As previously explained, AQLM allows the exploration of extreme quantization levels, down to 2 bits, which may unlock new possibilities in the deployment of LLM-based code generators on constrained hardware environments. The context of the study consists of two state-of-theart code LLMs, i.e., CodeLlama (Roziere et al., 2023) and DeepSeek-Coder (Guo et al., 2024), and two multilingual code generation benchmarks, i.e., MultiPL-E (Cassano et al., 2023) and McEval (Chai et al., 2024). In particular, the study aims at addressing the following research questions (RQs): RQ1: How does low-bit quantization affect the models code generation ability? In RQ1, we aim to investigate the effect of low-bit quantization on the models ability to generate code starting from textual description. As compared to the work by Wei et al. (2023a), which mostly focuses on 8-bit quantization, we experiment with 8, 4, 3, and 2-bit quantization. The goal Title Suppressed Due to Excessive Length 9 is to carefully study the trade-off between lost in code generation accuracy and saving in terms of memory footprint. The RQ1s findings will provide insights into the extent to which LLM-based code generators can be compressed without significantly compromising their performance. RQ2: Which impact does the calibration dataset have on model performance? In RQ2, we analyze the impact of the calibration dataset on model performance. The calibration dataset is used by the quantization technique to learn how to best approximate the original models weights. We experiment with various types of calibration datasets, including code-specific ones, which may be more suitable in the context of code generation and help in minimizing the loss in performance. Such an analysis is particularly important for the quantization of models targeting software engineering tasks and has not been conducted in the work of Wei et al. (2023a) where the authors focus solely on examining the extent to which the size of the calibration dataset, built on CodeSearchNet (Husain et al., 2019), affects quantization performance. RQ3: How does extreme quantization affect model accuracy across different model sizes? In RQ1 and RQ2 we fix the size of the experimented models to 7B for both CodeLlama and DeepSeek Coder while experimenting with four quantization levels (8, 4, 3, and 2 bits). In this research question, we are interested in determining the extent to which the findings of the previous RQs remain valid when quantizing LLMs featuring tens of billions of parameters. More specifically, in RQ3 we consider models having different sizes (i.e., 1B, 7B, 13B, 33B, and 34B) while fixing the quantization level to 2 bits. 3.1 Code-Specific Large Language Models Used in the Study We focus on two state-of-the-art code LLMs, namely CodeLlama and DeepSeekCoder. Both these LLMs have shown their effectiveness when applied to code generation tasks (Li et al., 2023a; Coignion et al., 2024; Ren et al., 2024). 3.1.1 CodeLlama (Roziere et al., 2023) CodeLlama is family of open-source LLMs tailored for coding applications, built on the general-purpose Llama 2 (Touvron et al., 2023) LLM. CodeLlama features Llama 2 models further trained on corpus of 500 billion tokens, including code and natural language. Multiple versions of these models are publicly accessible1 and are available in various sizes, with parameters ranging from 7B to 70B. Additionally, CodeLlama is available in three specialized formats: general-purpose version for coding, an Instruct version optimized 1 https://huggingface.co/codellama 10 Alessandro Giagnorio et al. for instruction tuning, and Python-specialized variant. CodeLlama has been selected as representative model of code-related LLMs given its former successful applications in the automation of various code-related tasks (Weyssow et al., 2023; Huang et al., 2024; Sun et al., 2024a). In our study, we exploit the general-purpose CodeLlama versions featuring 7B, 13B, and 34B parameters. While we attempted to experiment with the 70B version, the computational cost would have been too high for our infrastructure 3.1.2 DeepSeek-Coder (Guo et al., 2024) DeepSeek-Coder features open-source LLMs ranging in size from 1B to 33B parameters. Each model is available in two operational modes: Instruct (optimized for instruction tuning) and Base. These models have undergone extensive pre-training on 2 trillion tokens (including code-related documents) and have shown superior performance compared to many state-of-the-art LLMs for automating software engineering tasks. Notably, DeepSeek-Coder has outperformed much larger models like GPT-3.5 (Brown et al., 2020). Additionally, the mid-sized version of DeepSeek-Coder, with 6.7B parameters, has proven competitive with the 33B version of CodeLlama. In our study, we use the Base version of DeepSeek-Coder in its sizes 1B, 7B, 13B, and 33B parameters. 3.2 Quantization Technique As previously explained, we rely on the state-of-the-art method for quantization proposed by Egiazarian et al. (2024), namely AQLM. To clarify the specifics of our study, let us briefly review the fundamental workings of AQLM. Given codebook consisting of 2B candidate vectors and being the number of available codebooks, AQLM encodes groups of consecutive weights from each model layer by expressing them as the sum of vectors selected from distinct codebooks. model can be quantized at different levels of precision, measured in bits per parameter, based on the values assigned to (number of weights to quantize together), (number of codebooks), and (number of bits per codebook). The choice of these values may have an impact on quantization time and, above all, on the final memory footprint and accuracy of the resulting quantized model. The specific configurations we use for each experimented quantization level are reported in Table 1. few clarifications are needed about these configurations. First, the values for the g, M, and parameters are derived from the targeted quantization level. For example, setting = 8, = 4, = 15 results in 8 bits per models parameters, while lowering = 2 halves the precision to 4 bits. Second, given parameters configuration, the obtained precision slightly varies among the models. For example, CodeLlama 7B quantized using g=8, =1, and =15, results in an average of 2.02 bits per parameter, compared to 1.92 bits per parameter for the 33B version. Finally, for the smallest model (i.e., DeepSeek-Coder 1B), which is only used in the context of RQ3 Title Suppressed Due to Excessive Length 11 with the extreme 2-bit quantization, we had to lower to 14 (rather than 15) to obtain quantization level close to 2 bits (i.e., 2.05 bits per parameter). Table 1 AQLM configuration used for the models quantization: represents the number of weights to quantize together, is the number of codebooks, and is the number of bits per codebook. Precision 8 bit 4 bit 3 bit 2 bit Values = 8, = 4, = 15 = 8, = 2, = 15 = 8, = 2, = 12 = 8, = 1, = 15 Besides the specific AQLM parameters configuration, we had to create the calibration dataset used by the technique to minimize the quantization error (see Section 2.3). This dataset is usually composed of textual documents either sourced from the training set of the model being quantized or mined from publicly available sources (Williams and Aletras, 2024). Since the authors of CodeLlama and DeepSeek-Coder have not made their training datasets publicly available, we extracted calibration samples from RedPajama (Computer, 2023), one of the largest open-source datasets. RedPajama features 1.2 trillion tokens collected from diverse sources, including Common Crawl (Crawl, 2025), C4 (Raffel et al., 2020), GitHub (GitHub, 2025), Arxiv, Wikipedia, and StackExchange. Most of these sources are very likely to be part of the tuning datasets used for the experimented models. We create three different calibration datasets since they will be the focus of RQ2. For each of them, we selected 1,024 samples from RedPajama, each having sequence length of 4,096 tokens, mimicking what was done by Egiazarian et al. (2024) when presenting AQLM. This means that from each of the 1,024 documents selected from RedPajama (e.g., Wikipedia article), we only extract 4,096 consecutive tokens. The three variations of the calibration dataset differ in the nature of the documents they feature: Random dataset: We randomly sampled the 1,024 samples from all the documents in the RedPajama dataset. Such dataset simulates the choice of general purpose calibration dataset, which may or may not be optimal in the context of code generation. Table 2 shows the distribution of the selected samples for both families of models (i.e., CodeLlama and DeepSeek-Coder). Note that we had to build different calibration datasets for the two families of LLMs since they exploit different tokenizers. Thus, document may feature tokens for CodeLlama and for DeepSeek-Coder, with m. Mixed dataset: This dataset features 50% of samples coming from GitHub code file instances, with an equal distribution of Python and Java code (since those are the two languages we experiment with), and the remaining 50% of samples coming from Stack Overflow discussions having question 12 Alessandro Giagnorio et al. score greater than 10 and at least one reference to the terms Python or Java. In both cases, the samples are again extracted from the RedPajama dataset, e.g., we randomly select 512 GitHub files (256 Java and 256 Python) from RedPajama. Code dataset: In this case, the whole 1,024 instances come from the GitHub subset of RedPajama, with 512 being Java files and 512 being Python files. Table 2 Distribution of the Random dataset for CodeLlama (left) and DeepSeek-Coder (right) models. Source Common Crawl C4 Github Arxiv Wikipedia StackExchange Count 834 76 62 28 20 4 Source Common Crawl C4 Github Arxiv Wikipedia StackExchange Count 835 70 61 30 24 4 We use the Random dataset to address RQ1, while all three datasets are used to answer RQ2. In RQ3, we will exploit the dataset providing the best results as output of RQ2. 3.3 Evaluation Dataset Similarly to previous studies (Chen et al., 2021; Wei et al., 2023a; Cassano et al., 2024), we evaluate the models code generation ability by providing specification as input and expecting valid implementation as output. More specifically, the code generation task considers functions signature and documentation (i.e., natural language description) as specification and its body as the expected implementation. We rely on two multilingual benchmarks: MultiPL-E (Cassano et al., 2023) and McEval (Chai et al., 2024). The first includes two popular code generation datasets, i.e., HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). Among these two datasets, we only used HumanEval, which was found to be more challenging than MBPP (Cassano et al., 2023). HumanEval originally included 164 hand-written Python prompts along with canonical solutions and unit test cases for evaluating model generations. The MultiPL-E benchmark retains 161 of these Python tasks, 158 of which have also been translated into Java. Note that three tasks were not translated because they used Pythonic syntax not directly mappable into Java constructs. McEval is collection of human-annotated coding tasks spanning different programming languages. Each problem has signature, docstring with an exhaustive description of the program requirements, list of test cases, and label indicating the level of difficulty (easy, middle, or hard) of the code to implement. We extracted the Title Suppressed Due to Excessive Length 13 Python and Java problems from this benchmark, which amounted to 50 and 53 samples, respectively. We removed 8 problems from the Python samples that overlap with the HumanEval dataset, thus reducing the total number of Python samples to 42. The final Python (Java) problems feature 23 (30) easy tasks, 10 (13) middle-level samples, and 9 (10) hard problems. Our decision of focusing on Python and Java is due to their popularity and to the fact that they are considered high-resource languages, namely languages for which LLMs can exploit massive training material, thus usually exhibiting good code generation performances. This cannot really be said for low-resource languages (e.g., R, Racket), which would have introduced an additional challenge for the LLMs besides the extreme quantization we experiment with. 3.4 Study Procedure and Data Analysis To answer RQ1 we start by quantizing CodeLlama 7B and DeepSeek-Coder 7B via AQLM (Egiazarian et al., 2024) by leveraging the Random calibration dataset. The quantization is performed at 8, 4, 3, and 2 bits following the configurations in Table 1. The resulting models and the baseline (i.e., the non-quantized model) are then tested against the Java and Python samples from the two benchmarks to see how quantization affects model performance. In particular, given code generation task, the candidate code (i.e., the one generated by an LLM) is validated against task-specific unit tests and labeled as pass if it satisfies all assertions or fail otherwise. As done by Wei et al. (2023a), as performance metric, we use pass@1, which assesses the ability of the model to predict correct solution with single attempt (Chen et al., 2021) (i.e., the model is only allowed to output single candidate solution). To account for the nondeterminism of the LLM, we invoke the model 20 times per code generation problem using temperature of 0.2 and maximum output length of 1.024 tokens for MultiPL-E problems and 1.500 tokens for the McEval benchmark, ensuring an output window capable of handling the more demanding requirements of McEval. As anticipated, at each of the 20 invocations, the model is only allowed to generate single candidate solution. Finally, we compute the pass@1 score for each code generation task, i.e., the percentage of cases (out of 20) in which the LLM produced correct solution with the given single attempt. Finally, we report the average score across all code generation tasks. We compare the base models pass@1 score on the Java and Python benchmarks to that of its quantized version. We complement this metric with statistical tests that consider the distribution of correct and incorrect predictions for each code generation task. In particular, we employ McNemars test (McNemar, 1947) for pairwise comparisons of dichotomous results from two distinct treatments and the Odds Ratio (OR) effect size to determine the magnitude of these differences. To adjust p-values for multiple comparisons, we use the Benjamini-Hochberg procedure (Yoav and Yosef, 1995). Alessandro Giagnorio et al. To answer RQ2, we quantized CodeLlama 7B and DeepSeek-Coder 7B again at the same quantization levels used in RQ1 (i.e., 8, 4, 3, and 2 bits) using, however, the two additional calibration datasets presented in Section 3.2 (i.e., Mixed and Code dataset). Given the goal of RQ2 (i.e., investigating the impact on the performance of using different calibration datasets), in this context the baseline is represented by the quantized models used in RQ1, which exploited the Random calibration dataset. Thus, we compare the pass@1 score achieved by the models quantized with the Mixed and Code calibration datasets against one of the models quantized with the Random dataset. We exploit the same statistical analysis previously described for RQ1 (i.e., adjusted p-values resulting from McNemars test, and OR as effect size). As per RQ3, we quantize models of different sizes (e.g., 7B, 13B, and 34B versions of CodeLlama and 1B, 7B and 33B versions of DeepSeek-Coder) to the lowest precision, i.e., 2 bits per parameter. We then observe how the performance drop between the base (non-quantized) model and the 2-bit quantized version varies with respect to the number of models parameters, because we want to check the extent to which larger models are impacted by extreme quantization. In RQ3 we leverage the best-performing calibration dataset resulting from RQ2 during the quantization procedure. Again, pass@1 score is our dependent variable, and we mirror the previously described statistical analysis also in this RQ. Finally, in RQ1 and RQ3, we also experiment with an additional recently proposed strategy to further boost the performance of quantized models (Tseng et al., 2024; Egiazarian et al., 2024; Leconte et al., 2024). Such strategy, known as end-to-end fine-tuning, can potentially extend the boundaries of the Pareto optimality, particularly when quantizing at low bit-size precision, such as 2 bits. The basic idea behind this additional fine-tuning (details in Appendix of Egiazarian et al. (2024)) is to use the non-quantized LLM as teacher model from which its quantized version can distill knowledge in order to minimize differences in their predictions on given dataset. We run the end-to-end fine-tuning on the calibration dataset employed for the quantization process and re-used the same hyperparameter configuration by Egiazarian et al. (2024): 5 training epochs, learning rate of 1e-5 combined with the Adam optimizer (Zhang, 2018), and an early stopping mechanism to prevent overfitting. We discuss the effect that this further optimization strategy has on the performance of quantized models at 3 and 2-bit levels. We do not present this analysis in RQ2 since our only goal there is to isolate the impact of the type of calibration dataset. 4 Study Results In this section, we report and discuss the results of our study to address the research questions formulated in Section 3. When reporting the results of the statistical tests, we adopt the notation in Table 3 for the significance level. Title Suppressed Due to Excessive Length 15 Table 3 Statistical significance levels for the discussion of the results. Symbol p-value 0.1 0.05 < 0.1 0.01 < 0.05 0.001 < 0.01 < 0.001 * ** *** Table 4 Pass@1 score of the quantized models in comparison to their corresponding baseline models Model Params Precision Size Python Java pass@1 p-value OR pass@ p-value OR CodeLlama - Base (Roziere et al., 2023) DeepSeek-Coder - Base (Guo et al., 2024) CodeLlama - Base (Roziere et al., 2023) DeepSeek-Coder - Base (Guo et al., 2024) - t l c 7B 7B 7B 7B Float16 - Baseline 8-bit 4-bit 3-bit 2-bit Float16 - Baseline 8-bit 4-bit 3-bit 2-bit Float16 - Baseline 8-bit 4-bit 3-bit 2-bit Float16 - Baseline 8-bit 4-bit 3-bit 2-bit 13.48 GB 7.47 GB 4.00 GB 3.80 GB 2.26 GB 13.48 GB 7.48 GB 4.00 GB 3.80 GB 2.27 GB 13.48 GB 7.47 GB 4.00 GB 3.80 GB 2.26 GB 13.48 GB 7.48 GB 4.00 GB 3.80 GB 2.27 GB 29.8 29.7 29.1 24.3 16.4 45.8 46.2 45.2 41.1 27.6 12.9 12.9 15.2 10.0 5.6 41.8 42.5 40.7 36.2 13. *** *** 1.02 1.17 2.88 7.86 0.93 1.1 1.9 7.89 *** *** * *** *** *** 1.0 0.66 1.92 3.44 0.89 1.2 2.21 16.73 32.2 31.6 30.7 26.5 14.1 41.4 41.9 41.4 37.7 23.2 29.3 29.2 25.3 21.3 11.4 42.6 42.8 45.9 34.5 23.6 * *** *** 1.12 1.29 2.52 20.83 0.92 1.01 1.69 7.78 *** *** *** *** *** * *** *** 1.02 1.96 2.85 6.94 0.98 0.7 2.02 3.22 4.1 How does low-bit quantization affect the models code generation ability? Fig. 1 provides handy graphical representation of the memory saving (xaxis) versus the relative performance loss in terms of pass@1 score (y-axis) when comparing models quantized at different bit precisions (8, 4, 3, and 2 bits per parameter) and the baseline models using fp16 precision. The blue lines represent CodeLlama 7B while the orange lines DeepSeek-Coder 7B. The left charts relate to the Python benchmarks (from MultiPL-E and McEval) while the right one to the Java benchmarks. Table 4 reports the detailed numbers on top of which Fig. 1 has been built: For each LLM subject of RQ1 (i.e., CodeLlama and DeepSeek-Coder 7B) we show, both for the baseline fp16 precision model as well as for all its quantized versions, (i) their memory footprint in terms of GB, (ii) the pass@1 score they achieved on both the Python and the Java benchmarks; and (iii) the results of the statistical tests (i.e., adjusted p-value and OR), in which we compare each quantized model against the baseline. For example, looking at CodeLlama 7B, the fp16-precision baseline requires 13.48GB of memory to achieve 29.8% of pass@1 score on the Python MultiPL-E benchmark, while its 4-bit quantized version only uses 4.00GB of memory with pass@1 of 29.1%. This difference in performance is not statistically significant. When mapping this back to Fig. 1 (a) (blue line in the left chart), we can indeed see that the 4-bit quantized model, when compared to the fp16-precision baseline, allows achieving 70% 16 Alessandro Giagnorio et al. (a) Python MultiPL-E benchmark (b) Java MultiPL-E benchmark (c) Python McEval benchmark (d) Java McEval benchmark Fig. 1 Accuracy loss and memory saving of the quantized models compared to the baseline model for the Python (left) and Java (right) benchmarks. The dotted red lines highlight the models performance after the 4-bit quantization. of memory reduction (i.e., 13.484.00 (i.e., 29.129.8 ) decrease in pass@1 score. 13.48 29. ) at the rather small cost of relative -2% Looking at Fig. 1, the first conclusion that can be made is that, independently from the model and programming language, 4-bit quantization is safe bet, ensuring high memory reduction with very similar performance in terms of code generation. The result of the 4-bit CodeLlama model on the Java McEval benchmark is the only exception to this finding, with statistically significant decrease of pass@1 by 4%. To determine whether this behavior is due to the higher complexity of the benchmark, we checked the difficulty level of each prompt for which the base model returned correct implementation while the quantized model did not. We discovered that 34 failed generations refer to easy problems, 16 to middle-level tasks, and 14 to hard tasks, which is consistent with the distribution of problem levels in the benchmark (30 easy, 10 middle, Title Suppressed Due to Excessive Length 17 and 12 hard prompts). After manual inspection, we found that the quantized model mainly failed due to assertion errors or calls to non-existent APIs. When moving towards more extreme quantizations (i.e., 3 and 2-bit), the price to pay becomes higher, with statistically significant loss in pass@1 (see Table 4) which can reach relative -67.22% (2-bit quantization, DeepSeek Coder, McEval Python benchmark). Except for the previous case, DeepSeekCoder achieves better overall performance than CodeLlama, both in terms of code generation capabilities at fp16-precision as well as in loss of performance with quantization, which is in most of cases lower on DeepSeek-Coder. Findings With 4-bit quantization, it is possible to reduce memory footprint by 70% while preserving the code generation capabilities of the LLM. At more extreme quantization levels, instead, the models suffer major decreases in performance. The work we partially replicate (Wei et al., 2023a) concluded that 8-bit was safe choice. Our findings, achieved with more recent quantization technique, push the boundaries further down. As anticipated in Section 3.4 we also experimented with end-to-end finetuning to boost the performances at 3 and 2-bit quantization, as suggested by (Egiazarian et al., 2024). Table 5 presents comparison of the pass@1 score achieved by the 3and 2-bit precision models, both with (dark grey) and without (light grey) end-to-end fine-tuning. For reference, the fp16-precision baseline is reported as well. In this case, the results of the statistical tests refer to the comparison between quantized model with end-to-end fine-tuning versus the same quantized models without end-to-end fine-tuning. In other words, statistically significant p-value indicates significant boost of pass@1 score given by the additional fine-tuning. Our findings show that post-quantization fine-tuning can help in significantly boosting the performance of the 2-bit quantized models, with an average relative increase in the pass@1 score of 29.31% for CodeLlama and 24.23% for DeepSeek-Coder. In the case of 3-bit models, no significant differences are observed. Still, the 2-bit models, despite the significant increase in performance, exhibit considerable degradation when compared to their fp16-precision versions. Findings Fine-tuning after quantization can help in boosting the performance of 2-bit quantized models, while it does not really help already at 3-bit precision. In summary, 4-bit is still the recommended quantization level given the current state-of-the-art. 4.2 Which impact does the calibration dataset have on model performance? In RQ2, we analyze whether the calibration dataset used at quantization time plays major role in the code generation capabilities of the quantized LLMs. 18 Alessandro Giagnorio et al. Table 5 Pass@1 accuracy of the quantized models compared to their fine-tuned versions Model Params Precision Size Python Java pass@1 p-value OR pass@1 p-value OR CodeLlama - Base (Roziere et al., 2023) DeepSeek-Coder - Base (Guo et al., 2024) CodeLlama - Base (Roziere et al., 2023) DeepSeek-Coder - Base (Guo et al., 2024) - t l c 7B 7B 7B 7B Float16 - Baseline 3-bit 2-bit 3-bit + Fine-tuning 2-bit + Fine-tuning Float16 - Baseline 3-bit 2-bit 3-bit + Fine-tuning 2-bit + Fine-tuning Float16 - Baseline 3-bit 2-bit 3-bit + Fine-tuning 2-bit + Fine-tuning Float16 - Baseline 3-bit 2-bit 3-bit + Fine-tuning 2-bit + Fine-tuning 29.8 13.48 GB 24.3 3.80 GB 2.26 GB 16.4 3.80 GB 24.0 2.26 GB 19.9 45.8 13.48 GB 41.1 3.80 GB 2.27 GB 27.6 3.80 GB 41.8 2.27 GB 33.0 12.9 13.48 GB 10.0 3.80 GB 2.26 GB 5.6 3.80 GB 10.8 2.26 GB 7.6 41.8 13.48 GB 36.2 3.80 GB 2.27 GB 13.7 3.80 GB 35.6 2.27 GB 20.2 *** 32.2 26.5 14.1 0.91 27.8 2.01 19.0 41.4 37.7 23.2 1.13 37.7 2.67 26.8 29.3 21.3 11.4 1.37 22.0 1.81 14.3 42.6 34.5 23.6 0.9 32.4 2.38 27.0 *** * *** * *** 1.31 2.87 0.99 1. *** ** 1.13 1.84 0.79 1.49 ** Table 6 Pass@1 accuracy of the quantized models using different calibration datasets Model Params Precision Size Python Java pass@ p-value OR pass@1 p-value OR CodeLlama - Base (Roziere et al., 2023) 7B DeepSeek-Coder - Base (Guo et al., 2024) 7B CodeLlama - Base (Roziere et al., 2023) 7B DeepSeek-Coder - Base (Guo et al., 2024) 7B - t l c Float16 - Baseline 8-bit with Random samples 8-bit with Mixed samples 8-bit with Code samples 4-bit with Random samples 4-bit with Mixed samples 4-bit with Code samples 3-bit with Random samples 3-bit with Mixed samples 3-bit with Code samples 2-bit with Random samples 2-bit with Mixed samples 2-bit with Code samples Float16 - Baseline 8-bit with Random samples 8-bit with Mixed samples 8-bit with Code samples 4-bit with Random samples 4-bit with Mixed samples 4-bit with Code samples 3-bit with Random samples 3-bit with Mixed samples 3-bit with Code samples 2-bit with Random samples 2-bit with Mixed samples 2-bit with Code samples Float16 - Baseline 8-bit with Random samples 8-bit with Mixed samples 8-bit with Code samples 4-bit with Random samples 4-bit with Mixed samples 4-bit with Code samples 3-bit with Random samples 3-bit with Mixed samples 3-bit with Code samples 2-bit with Random samples 2-bit with Mixed samples 2-bit with Code samples Float16 - Baseline 8-bit with Random samples 8-bit with Mixed samples 8-bit with Code samples 4-bit with Random samples 4-bit with Mixed samples 4-bit with Code samples 3-bit with Random samples 3-bit with Mixed samples 3-bit with Code samples 2-bit with Random samples 2-bit with Mixed samples 2-bit with Code samples 29.8 13.48 GB 7.47 GB 29.7 7.47 GB 29.7 7.47 GB 29.2 29.1 4.00 GB 4.00 GB 29.0 4.00 GB 30.2 3.80 GB 24.3 3.80 GB 28.2 3.80 GB 27.0 2.26 GB 16.4 2.26 GB 23.9 2.26 GB 24.1 45.8 13.48 GB 7.48 GB 46.2 7.48 GB 45.4 7.48 GB 45.9 4.00 GB 45.2 4.00 GB 44.5 4.00 GB 44.2 3.80 GB 41.1 3.80 GB 43.7 3.80 GB 42.5 2.27 GB 27.6 2.27 GB 35.7 2.27 GB 34.8 12.9 13.48 GB 7.47 GB 12.9 7.47 GB 13.7 7.47 GB 12.3 4.00 GB 15.2 4.00 GB 13.0 4.00 GB 11.1 3.80 GB 10.0 3.80 GB 12.3 3.80 GB 10.8 2.26 GB 5.6 2.26 GB 11.1 2.26 GB 6.1 41.8 13.48 GB 7.48 GB 42.5 7.48 GB 42.7 7.48 GB 41.3 4.00 GB 40.7 4.00 GB 39.0 4.00 GB 39.8 36.2 3.80 GB 3.80 GB 35.5 3.80 GB 36.5 2.27 GB 13.7 2.27 GB 26.2 2.27 GB 24. *** *** *** *** *** *** *** *** *** *** *** 32.2 31.6 0.99 32.3 0.88 32.0 30.7 0.96 31.4 1.22 29.8 26.5 2.16 28.4 1.95 28.0 14.1 3.95 21.5 4.7 19.4 41.4 41.9 0.87 43.2 0.94 41.7 41.4 0.89 41.8 0.86 40.6 37.7 1.4 39.1 1.2 38.7 23.2 3.05 27.4 2.57 27.5 29.3 29.2 1.19 28.6 0.88 29.5 25.3 0.62 30.3 0.43 25.8 21.3 1.86 25.5 1.35 19.9 11.4 4.54 12.8 1.17 12.8 42.6 42.8 1.04 42.5 0.83 42.7 45.9 0.79 42.8 0.86 46.3 34.5 0.92 42.8 1.06 45.6 23.6 4.39 29.1 4.54 28.0 ** * *** *** 1.13 1.08 1.12 0.85 1.38 1.28 4.21 2.51 1.25 0.96 1.07 0.89 1.23 1.14 1.66 1.8 *** *** 0.88 1.05 2.2 *** 1.1 1.92 *** 0.77 1.37 1.36 0.97 0.99 0.74 1.04 2.19 *** *** 2.43 1.55 1.55 ** ** * Title Suppressed Due to Excessive Length 19 Specifically, we compare the pass@1 scores achieved in the previous experiment, i.e., using the Random calibration dataset, with those obtained with the Mixed and Code datasets. Table 6 reports the results of the three treatments. In Table 6 the statistical tests compare each model quantized with the Mixed or with the Code dataset to its same version quantized using the Random dataset (e.g., CodeLlama 8-bit with Mixed dataset vs CodeLlama 8-bit with Random dataset). For 8-bit and 4-bit models, we do not observe statistically significant difference in pass@1 score between models quantized with different calibration datasets. This holds both for CodeLlama and DeepSeek-Coder and in both languages (Python and Java). This finding reveals that code models are robust to the calibration dataset provided at quantization time for target precisions greater or equal to 4 bits per parameter. The only exceptions are the CodeLlama 4-bit model on the McEval Java benchmark and the DeepSeekCoder 4-bit model on the McEval Python benchmark, which are compared with Random quantization that already performs better than the fp16precision baseline. The 4-bit CodeLlama model, instead, shows statistically significant improvement on the McEval Python benchmark after the Mixed calibration, thus recovering from the performance gap with the baseline observed in Section 4.1. On the other hand, we notice that 3-bit and 2-bit precision models are more sensible to the samples provided in the calibration dataset. Indeed, both models show statistically significant improvement in performance when calibration datasets feature code samples, suggesting their need to better approximate the model weights at extreme quantization levels. Thanks to code-related calibration datasets, 3-bit quantization might be considered an option in very resource-constrained hardware devices. However, when comparing the 3-bit models quantized using the Mixed dataset against the fp16-precision baseline, the difference in pass@1 score is still statistically significant. Similarly, the 2-bit models continue to suffer from significant performance gap despite major improvements brought by the code-related calibration datasets. This calls for additional research on optimizing calibration datasets for code-related tasks, which could help enhance performance even further. Indeed, we only experimented with three variants of the calibration datasets, which, due to the unavailability of the training sets used for CodeLlama and DeepSeek-Coder, we cannot guarantee to be representative of the training instances (a condition that is expected to help the quantization process (Egiazarian et al., 2024)). Although our results do not show clear winner between the two datasets, providing both code and technical language generally yields better performance for both models. This is particularly evident in the CodeLlama family of models, as they are more susceptible to sophisticated natural language requirements, such as those found in the McEval dataset. We believe including code and domain-specific natural language in the calibration dataset can help 20 Alessandro Giagnorio et al. models that frequently struggle to follow complex instructions, like CodeLlama. Findings Calibration datasets significantly impact the performance of the quantized models when targeting precision levels lower than 4 bits per parameter. Providing code-related samples improves the quantization process, resulting in more performant code generation models. 4.3 How does extreme quantization affect model accuracy across different model sizes? In RQ2, we reported findings related to the quantization of CodeLlama and DeepSeek-Coder when fixing their size to 7B parameters. In RQ3, we explore the impact of extreme quantization on models of different sizes, namely CodeLlama 7B, 13B, and 34B, and DeepSeek-Coder 1B, 7B, and 33B. We decided to focus only on single quantization level (i.e., 2-bit) since the cost of running these experiments with models up to 34B parameters is extremely high. Also, we already observed that 4-bit quantization can preserve the LLMs code generation capabilities when using 7B-parameter model. Thus, we wanted to investigate whether larger models could handle the most extreme quantization level. We adopt the Mixed calibration dataset in this RQ. Table 7 reports the achieved results, with the statistical tests comparing the quantized models to the fp16-precision baseline and the Dec (%) column showing the percentage relative decrease in pass@1 score. Note that in this RQ, we use 2-bit quantization without and with end-to-end fine-tuning since we observed its benefits for the 2-bit precision in RQ1. As can be seen in the table, an increase in the models parameters results in lower performance degradation at the extreme 2-bit quantization level. This finding suggests that extreme quantization is less problematic on very large models. However, Table 7 still highlights statistically significant difference in performance when comparing the 2-bit quantized models to the fp16-precision baselines. The only exception to this trend is CodeLlama 7B on the Python McEval benchmark, where it closes the gap with the base model due to the significant contribution of the mixed calibration. More interestingly, 2-bit quantizations of large models may outperform their smaller variants at full precision. As an example, CodeLlama 34B quantized to 2 bits per parameter outperforms its smaller 7B-parameter variant at fp16 precision on both Python (37.1 vs 29.8) and Java (32.7 vs 32.2) MultiPLE benchmarks while requiring less memory (9.38GB vs 13.48GB). However, there is no clear general trend behind this finding, which seems to be model and language-dependent. For what concerns the fine-tuned variants, we observe that training after quantization can considerably help smaller models to achieve better accuracy. For example, when fine-tuning DeepSeek-Coder 1B, the performance decrease on Java (MultiPL-E) passes from -77.1% to -49%, while the one on Python Title Suppressed Due to Excessive Length 21 Table 7 Pass@1 accuracy of the quantized models on different sizes Model Params Precision Size (GB) Python Java pass@1 p-value OR Dec (%) pass@1 p-value OR Dec (%) CodeLlama - Base (Roziere et al., 2023) DeepSeek-Coder - Base (Guo et al., 2024) CodeLlama - Base (Roziere et al., 2023) DeepSeek-Coder - Base (Guo et al., 2024) - t l c 7B 13B 34B 1B 7B 33B 7B 13B 34B 1B 7B 33B Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning Float16 - Baseline 2-bit 2-bit + Finetuning 13.48 2.26 2.26 24.25 3.98 3.98 62.74 9.54 9.54 2.57 0.61 0.61 13.48 2.27 2.27 62.16 9.38 9. 13.48 2.26 2.26 24.25 3.98 3.98 62.74 9.54 9.54 2.57 0.61 0.61 13.48 2.27 2.27 62.16 9.38 9.38 29.8 23.9 25.5 34.3 30.9 30.1 41.9 37.1 36.0 28.4 13.9 21.7 45.8 35.7 36.4 52.1 43.4 43.0 12.9 11.1 13.0 18.9 9.4 10.4 29.0 17.6 19.0 23.8 4.4 6.9 41.8 26.2 30.1 55.5 36.9 39.8 *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** 2.68 2.01 1.47 1.66 1.62 1.97 7.56 2.41 3.4 3.4 2.6 3.25 1.3 0.98 6.71 4.43 4.43 3.27 28.17 12.83 5.68 4.5 4.8 3.4 -19.8 -14.4 -9.9 -12.2 -11.5 -14.1 -51.1 -23.6 -22.1 -20.5 -16.7 -17.5 -14.0 0.8 -50.3 -45.0 -39.3 -34.5 -81.5 -71.0 -37.3 -28.0 -33.5 -28.3 32.2 21.5 26.5 38.3 27.7 32.8 44.1 32.7 36.1 28.8 6.6 14.7 41.4 27.4 32.8 47.3 34.5 38.7 29.3 12.8 18.3 40.9 22.3 27.8 39.2 25.2 31.6 42.0 8.5 15.5 42.6 29.1 31.0 57.0 39.2 44. *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** 4.38 1.98 3.35 1.84 3.64 2.8 23.58 6.58 6.21 2.84 3.43 2.34 8.29 4.08 5.6 3.4 3.6 1.88 45.37 24.42 2.16 2.32 3.58 3.38 -33.2 -17.7 -27.7 -14.4 -25.9 -18.1 -77.1 -49.0 -33.8 -20.8 -27.1 -18.2 -56.3 -37.5 -45.5 -32.0 -35.7 -19.4 -79.8 -63.1 -31.7 -27.2 -31.2 -22. goes from -51.1% to -23.6%. In general, we observe that fine-tuning helps reduce the quantization error on smaller models. However, as the number of model parameters increases, we notice that the impact of fine-tuning gradually diminishes and eventually becomes counterproductive. Findings When the target model features large number of parameters, 2-bit quantization has lower negative impact on code generation performance. Given the observed trend, we may expect extremely large code LLMs having hundreds of billions of parameters to possibly be able to support 2-bit quantization without any significant loss in performance. However, further empirical analyses are needed to generalize this hypothesis. 5 Implications of our Findings The results of our investigation contribute to the growing body of knowledge on quantization methods for code-LLMs, building solid foundations for future research in this area. In particular, our findings empirically show the feasibility of applying advanced quantization techniques, such as AQLM, to reduce the memory footprint of LLMs powering AI systems for SE while maximizing performance. Based on the results of our investigation, we can emphasize two key takeaways: Code-LLMs can be safely quantized down to 4-bit precision without compromising performances in source code generation tasks. We 22 Alessandro Giagnorio et al. empirically showed that quantizing code-LLMs down to 4-bit precision can effectively reduce memory footprint by approximately threefold (x3) without sacrificing performance. This achievement is relevant in different contexts and domains. First, it enables the deployment of larger code models in hardwareconstrained environments characterized by limited resources. The consequence is reduction in energy consumption and an enhancement in terms of scalability, sustainability, and accessibility of advanced AI systems grounded on LLMs for code. Moving along these lines, the use of quantized methods also holds potential in the areas of security and privacy. Outsourcing code to external tools and services to leverage code-LLMs automation capabilities can pose significant security risks, including the possibility of data leakage (Novelli et al., 2024; Wu et al., 2024; Huang et al., 2023). In contrast, reduced models can be run locally thanks to quantization without significantly compromising performance, eliminating the need to depend on third-party solutions and addressing security and privacy concerns. Moreover, for small businesses and startups that lack access to large-scale computing infrastructure, quantization provides an opportunity for innovation and offers competitive edge, as these organizations can adopt state-of-the-art AI solutions for automating software engineering-related practices (Hou et al., 2024). Another compelling application is in the educational setting, where students may not have access to high-performance computing resources like dedicated servers. quantized model that can run locally offers valuable opportunity to enrich education in the software engineering field, giving students access to cutting-edge technology that enhances their learning experience. Additional research is needed on more extreme quantization levels. While quantization has proven to be an effective tool when bit precision is not pushed to the extreme (i.e., 2-bit), we observed that in cases of ultralow-level quantization, external factors such as the calibration dataset play significantly more critical role. Therefore, the influence of the calibration dataset is not negligible and requires careful consideration. Optimizing the structure and composition of the calibration dataset becomes essential in such scenarios, as it can greatly affect the overall performance of the quantized model. Additionally, with the emergence of very Large Language Models, models with 100 billion parameters or more are becoming the standard rather than the exception, even in software engineering. Codex (Chen et al., 2021), the model behind GitHub Copilot, is based on GPT-3, which boasts over 100 billion parameters. However, the application of quantization to models of this size remains largely unexplored. Investigating quantization at this scale offers significant opportunities and could yield new insights that are not yet fully understood, given the current state of affairs. Title Suppressed Due to Excessive Length 23 6 Threats to Validity Threats to construct validity concern the relationship between theory and observation. This threat is mainly related to the measurements made to address our research questions. As far as models accuracy is concerned, we leverage the pass@k measure, used in previous work (Chen et al., 2021; Cassano et al., 2023, 2024; Wei et al., 2023a) including the Wei et al. study we aimed at partially replicating (Wei et al., 2023a). Threats to internal validity concern factors internal to our study that could affect our results. These threats mainly concern the choice of hyperparameters for the quantization process, for the end-to-end fine-tuning, and for the inference phase. In Section 3, we explain, for the different phases of our study, how the different hyperparameters have been chosen. More specifically, in some cases, we based our choices on what was done in previous work, including the original AQLM paper (Egiazarian et al., 2024) and the quantization study we replicated (Wei et al., 2023a). In other cases, we made empirical choices, for example when determining the number of epochs in the end-to-end fine-tuning. Threats to conclusion validity concern the relationship between experimentation and outcome. To support our findings, we leverage suitable statistical procedures, i.e., McNemars test (with adjusted p-values using the BenjaminiHochberg procedure (Yoav and Yosef, 1995)) and Odds Ratio. Threats to external validity concern the generalizability of our findings. Concerning the studied LLMs, we focused on two widely used models of code, CodeLLama and DeepSeek-Coder. Also, we experimented with different model sizes, up to 34B. However, it is possible that other code-based LLMs or generalpurpose LLMs may be differently impacted by the quantization process. Another threat to external validity concerns the generalizability to other tasks. In this work, we evaluated the effects of quantization on code generation tasks. Therefore, the obtained results may not generalize to other SE-related tasks. 7 Conclusion and Future Work Previous work by Wei et al. (2023a) showed quantization to be an effective way of managing the size of state-of-the-art LLMs that power AI-driven solutions for the software engineering domain. However, the recent surge in quantization techniques and code-LLMs made us question on what would be the pros and cons of quantization given (i) new advancements, including not only adopting extreme quantization precision levels but also different types of calibration datasets and (ii) large code models featuring up to 34 Billion parameters. To further explore quantization for code-LLMs, we partially replicated the foundational study by Wei et al., confirming that quantization, as proposed, is an effective approach for reducing memory usage. Building on their work, we expanded our studys scope by focusing on source code generation, incor24 Alessandro Giagnorio et al. porating two programming languagesJava and Pythonand evaluating two state-of-the-art code models: (i) CodeLlama and (ii) DeepSeek-Coder. We investigated low-bit quantization (i.e., 3 and 2 bits), pushing the boundaries of extreme quantization for code models while also incorporating different calibration datasets: one consisting of randomly selected elements, another featuring mix of code and technical natural language, and third made up entirely of code. Our findings revealed that code models can be quantized down to 4 bits, achieving 70% reduction in memory usage while maintaining the original performance of the non-quantized model. Moreover, we observed that the choice of calibration dataset significantly impacts the models performance. Specifically, when applying low-bit quantization (4 bits), the selection of curated examplesinstead of randomly sampled oneswas beneficial in mitigating performance degradation. Last but not least, we found that larger code models (with 33 billion parameters) were more resilient to information loss during quantization, showing smaller drop in performance compared to smaller models when the bit precision was reduced to the point of ultra low-bit quantization, i.e., 2 and 3 bits. In conclusion, our research not only reaffirms the benefits of quantization, as highlighted by Wei et al., but also extends the possibilities of what can be achieved with modern code models when using state-of-the-art quantization techniques. It expands the potential for efficient AI-driven software engineering solutions and paves the way for new research opportunities while providing practical applications for industry professionals. Our next step is to assess the impact of quantization on wider range of code-related tasks, such as code summarization and program repair, beyond just code completion. These additional analyses would offer more comprehensive understanding of how quantization affects code models across the entire spectrum of code-related tasks, thereby helping to generalize our findings further. Declarations Fundings Di Penta acknowledges the Italian PRIN 2022 project TRex-SE: Trustworthy Recommenders for Software Engineers, grant n. 2022LKJWHC. Giagnorio and Bavota thank the Swiss National Science Foundation (SNSF) for the funding provided under the project PARSED (grant agreement No. 219294). Ethical approval Not applicable. Title Suppressed Due to Excessive Length 25 Informed Consent Not applicable. Author Contributions Alessandro Giagnorio: Developed the research idea, designed the study, conducted experiments, performed statistical analysis, participated in the writing. Antonio Mastropaolo: Developed the research idea, designed the study, participated in the writing. Saima Afrin: Developed the research idea, designed the study, participated in the writing. Massimiliano Di Penta: Developed the research idea, designed the study, participated in the writing. Gabriele Bavota: Developed the research idea, designed the study, participated in the writing. Data Availability Statement Code and data used for conducting the study are publicly available in our replication package (Giagnorio et al., 2025). In particular, we provide (i) the code needed to run quantization, (ii) different quantized models, (iii) all the predictions generated by each quantized model as well as by the baseline, and (iv) additional scripts used in the computation of our findings. Conflict of Interest The authors have no conflicts of interest to declare that are relevant to the content of this article. Clinical trial number Not applicable. References (2021) GitHub Copilot Your AI pair programmer. https://github.com/ features/copilot/, accessed: 2025-02Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, Almeida D, Altenschmidt J, Altman S, Anadkat S, et al. (2023) Gpt-4 technical report. arXiv preprint arXiv:230308774 26 Alessandro Giagnorio et al. Ahmed T, Devanbu (2022) Few-shot training llms for project-specific codesummarization. In: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, pp 15 Ahmed T, Pai KS, Devanbu P, Barr (2024) Automatic semantic augmentation of language model prompts (for code summarization). In: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pp 113 Allal LB, Li R, Kocetkov D, Mou C, Akiki C, Ferrandis CM, Muennighoff N, Mishra M, Gu A, Dey M, et al. (2023) Santacoder: dont reach for the stars! arXiv preprint arXiv: The (2024) haiku. Anthropic net, de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_ 3.pdf sonhttps://www-cdn.anthropic.com/ family: Opus, 3 model claude URL Austin J, Odena A, Nye M, Bosma M, Michalewski H, Dohan D, Jiang E, Cai C, Terry M, Le Q, et al. (2021) Program synthesis with large language models. arXiv preprint arXiv:210807732 Ayupov S, Chirkova (2022) Parameter-efficient finetuning of transformers for source code. arXiv preprint arXiv:221205901 Babenko A, Lempitsky (2014) Additive quantization for extreme vector compression. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 931 Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell A, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems 33:18771901 Cai Y, Yao Z, Dong Z, Gholami A, Mahoney MW, Keutzer (2020) Zeroq: novel zero shot quantization framework. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 1316913178 Cassano F, Gouwar J, Nguyen D, Nguyen S, Phipps-Costin L, Pinckney D, Yee MH, Zi Y, Anderson CJ, Feldman MQ, et al. (2023) Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering Cassano F, Gouwar J, Lucchetti F, Schlesinger C, Freeman A, Anderson CJ, Feldman MQ, Greenberg M, Jangda A, Guha (2024) Knowledge transfer from high-resource to low-resource programming languages for code llms. Proceedings of the ACM on Programming Languages 8(OOPSLA2):677 708 Castano J, MartÄ±nez-Fernandez S, Franch X, Bogner (2023) Exploring the carbon footprint of hugging faces ml models: repository mining study. In: 2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), IEEE, pp 112 Chai L, Liu S, Yang J, Yin Y, Jin K, Liu J, Sun T, Zhang G, Ren C, Guo H, et al. (2024) Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:240607436 Chaudhary (2023) Code alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/codealpaca Title Suppressed Due to Excessive Length 27 Chen M, Tworek J, Jun H, Yuan Q, Pinto HPdO, Kaplan J, Edwards H, Burda Y, Joseph N, Brockman G, et al. (2021) Evaluating large language models trained on code. arXiv preprint arXiv:210703374 Coignion T, Quinton C, Rouvoy (2024) performance study of llmgenerated code on leetcode. In: Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, pp 7989 Computer (2023) Redpajama dataset 1t. https://huggingface.co/ datasets/togethercomputer/RedPajama-Data-1T, [Online] Crawl (2025) Common crawl. https://commoncrawl.org/, [Online] dAloisio G, Traini L, Sarro F, Di Marco (2024) On the compression of language models for code: An empirical study on codebert. arXiv preprint arXiv: Dettmers T, Lewis M, Belkada Y, Zettlemoyer (2022) Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35:3031830332 Dettmers T, Svirschevski R, Egiazarian V, Kuznedelev D, Frantar E, Ashkboos S, Borzunov A, Hoefler T, Alistarh (2024) Spqr: sparse-quantized representation for near-lossless LLM weight compression. In: The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, OpenReview.net, URL https://openreview.net/ forum?id=Q1u25ahSuy Egiazarian V, Panferov A, Kuznedelev D, Frantar E, Babenko A, Alistarh (2024) Extreme compression of large language models via additive quantization. In: Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, OpenReview.net, URL https://openreview.net/forum?id=5mCaITRTmO Esser SK, McKinstry JL, Bablani D, Appuswamy R, Modha DS (2020) Learned step size quantization. In: 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net, URL https://openreview.net/forum?id=rkgO66VKDS Fan A, Gokkaya B, Harman M, Lyubarskiy M, Sengupta S, Yoo S, Zhang JM (2023) Large language models for software engineering: Survey and open problems. In: 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), IEEE, pp 3153 Frantar E, Ashkboos S, Hoefler T, Alistarh (2022) Gptq: Accurate posttraining quantization for generative pre-trained transformers. arXiv preprint arXiv:221017323 Fried D, Aghajanyan A, Lin J, Wang S, Wallace E, Shi F, Zhong R, Yih S, Zettlemoyer L, Lewis (2023) Incoder: generative model for code infilling and synthesis. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, OpenReview.net, URL https://openreview.net/forum?id=hQwb-lbM6EL Gholami A, Kim S, Dong Z, Yao Z, Mahoney MW, Keutzer (2022) survey of quantization methods for efficient neural network inference. In: LowPower Computer Vision, Chapman and Hall/CRC, pp 291 28 Alessandro Giagnorio et al. Giagnorio A, Mastropaolo A, Afrin S, Di Penta M, Bavota (2025) Replication package. https://doi.org/10.5281/zenodo.13752774 GitHub (2025) Github. https://github.com/, [Online] Guo D, Zhu Q, Yang D, Xie Z, Dong K, Zhang W, Chen G, Bi X, Wu Y, Li Y, et al. (2024) Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:240114196 Haldar R, Hockenmaier (2024) Analyzing the performance of large language models on code summarization. In: Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), ELRA and ICCL, Torino, Italia, pp 9951008, URL https://aclanthology.org/2024.lrec-main.89/ He J, Zhou C, Ma X, Berg-Kirkpatrick T, Neubig (2022) Towards unified view of parameter-efficient transfer learning. In: The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, OpenReview.net, URL https://openreview.net/forum?id= 0RDcd5Axok Hou X, Zhao Y, Liu Y, Yang Z, Wang K, Li L, Luo X, Lo D, Grundy J, Wang (2024) Large language models for software engineering: systematic literature review. ACM Transactions on Software Engineering and Methodology 33(8):179 Houlsby N, Giurgiu A, Jastrzebski S, Morrone B, De Laroussilhe Q, Gesmundo A, Attariyan M, Gelly (2019) Parameter-efficient transfer learning for nlp. In: International conference on machine learning, PMLR, pp 27902799 Hsieh C, Li C, Yeh C, Nakhost H, Fujii Y, Ratner A, Krishna R, Lee C, Pfister (2023) Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In: Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Association for Computational Linguistics, pp 80038017, URL https:// doi.org/10.18653/v1/2023.findings-acl.507 Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen (2022) Lora: Low-rank adaptation of large language models. In: The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, OpenReview.net, URL https://openreview. net/forum?id=nZeVKeeFYf9 Huang K, Zhang F, Li Y, Wright S, Kidambi V, Manral (2023) Security and privacy concerns in chatgpt. In: Beyond AI: ChatGPT, Web3, and the Business Landscape of Tomorrow, Springer, pp 297328 Huang K, Zhang J, Meng X, Liu (2024) Template-guided program repair in the era of large language models. In: 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), IEEE Computer Society, pp 367379 Husain H, Wu H, Gazit T, Allamanis M, Brockschmidt (2019) Codesearchnet challenge: Evaluating the state of semantic code search. CoRR abs/1909.09436, DOI 10.48550/arXiv.1909. Leconte L, Bedin L, Nguyen VM, Moulines (2024) Reallm: general framework for llm compression and fine-tuning. URL https://arxiv.org/abs/ Title Suppressed Due to Excessive Length 29 2405.13155, 2405.13155 Lester B, Al-Rfou R, Constant (2021) The power of scale for parameterefficient prompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, Association for Computational Linguistics, pp 30453059, DOI 10.18653/ V1/2021.EMNLP-MAIN.243, URL https://doi.org/10.18653/v1/2021. emnlp-main.243 Li J, Li G, Li Y, Jin (2023a) Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology Li R, Allal LB, Zi Y, Muennighoff N, Kocetkov D, Mou C, Marone M, Akiki C, Li J, Chim J, et al. (2023b) Starcoder: may the source be with you! arXiv preprint arXiv: Li XL, Liang (2021) Prefix-tuning: Optimizing continuous prompts for generation. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Association for Computational Linguistics, pp 45824597, DOI 10.18653/V1/2021.ACL-LONG.353, URL https://doi.org/10.18653/v1/2021.acl-long.353 Li Y, Choi D, Chung J, Kushman N, Schrittwieser J, Leblond R, Eccles T, Keeling J, Gimeno F, Dal Lago A, et al. (2022) Competition-level code generation with alphacode. Science 378(6624):10921097 Liu H, Tam D, Muqeeth M, Mohta J, Huang T, Bansal M, Raffel CA (2022) Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems 35:19501965 Liu J, Sha C, Peng (2023) An empirical study of parameter-efficient finetuning methods for pre-trained code models. In: 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), IEEE, pp 397408 Liu Z, Oguz B, Zhao C, Chang E, Stock P, Mehdad Y, Shi Y, Krishnamoorthi R, Chandra (2024) LLM-QAT: data-free quantization aware training for large language models. In: Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Association for Computational Linguistics, pp 467484, DOI 10.18653/V1/2024.FINDINGS-ACL.26, URL https://doi.org/10. 18653/v1/2024.findings-acl.26 Marques N, Silva RR, Bernardino (2024) Using chatgpt in software requirements engineering: comprehensive review. Future Internet 16(6):180 Mastropaolo A, Ferrari V, Pascarella L, Bavota (2024) Log statements generation via deep learning: Widening the support provided to developers. Journal of Systems and Software 210:111947 McNemar (1947) Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika 12(2):153157 30 Alessandro Giagnorio et al. Nijkamp E, Pang B, Hayashi H, Tu L, Wang H, Zhou Y, Savarese S, Xiong (2023) Codegen: An open large language model for code with multi-turn program synthesis. In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, OpenReview.net Novelli C, Casolari F, Hacker P, Spedicato G, Floridi (2024) Generative AI in EU law: Liability, privacy, intellectual property, and cybersecurity. CoRR abs/2401.07348 Patterson D, Gonzalez J, Le Q, Liang C, Munguia LM, Rothchild D, So D, Texier M, Dean (2021) Carbon emissions and large neural network training. arXiv preprint arXiv:210410350 Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ (2020) Exploring the limits of transfer learning with unified textto-text transformer. Journal of Machine Learning Research 21(140):167, URL http://jmlr.org/papers/v21/20-074.html Ren H, Zhan M, Wu Z, Zhou A, Pan J, Li (2024) Reflectioncoder: Learning from reflection sequence for enhanced one-off code generation. arXiv preprint arXiv:240517057 Roziere B, Gehring J, Gloeckle F, Sootla S, Gat I, Tan XE, Adi Y, Liu J, Remez T, Rapin J, et al. (2023) Code llama: Open foundation models for code. arXiv preprint arXiv: Shen X, Kong Z, Yang C, Han Z, Lu L, Dong P, Lyu C, Li Ch, Guo X, Shu Z, et al. (2024) Edgeqat: Entropy and distribution guided quantization-aware training for the acceleration of lightweight llms on the edge. arXiv preprint arXiv:240210787 Strubell E, Ganesh A, McCallum (2020) Energy and policy considerations for modern deep learning research. In: Proceedings of the AAAI conference on artificial intelligence, vol 34, pp 1369313696 Su CY, McMillan (2024) Distilled gpt for source code summarization. Automated Software Engineering 31(1):22 Sun W, Miao Y, Li Y, Zhang H, Fang C, Liu Y, Deng G, Liu Y, Chen (2024a) Source code summarization in the era of large language models. In: 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), IEEE Computer Society, pp 419431 Sun Z, Du X, Song F, Wang S, Li (2024b) When neural code completion models size up the situation: Attaining cheaper and faster completion through dynamic model inference. In: Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pp 1 Team G, Anil R, Borgeaud S, Alayrac JB, Yu J, Soricut R, Schalkwyk J, Dai AM, Hauth A, Millican K, et al. (2023) Gemini: family of highly capable multimodal models. arXiv preprint arXiv:231211805 Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, Rozi`ere B, Goyal N, Hambro E, Azhar F, et al. (2023) Llama: Open and efficient foundation language models. arXiv preprint arXiv:230213971 Tseng A, Chee J, Sun Q, Kuleshov V, Sa CD (2024) Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks. In: FortyTitle Suppressed Due to Excessive Length 31 first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, OpenReview.net, URL https://openreview. net/forum?id=9BrydUVcoe Tufano M, Drain D, Svyatkovskiy A, Deng SK, Sundaresan (2020) Unit test case generation with transformers and focal context. arXiv preprint arXiv:200905617 Wang D, Chen B, Li S, Luo W, Peng S, Dong W, Liao (2023) One adapter for all programming languages? adapter tuning for code search and summarization. In: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), IEEE, pp 516 Wei X, Gonugondla SK, Wang S, Ahmad W, Ray B, Qian H, Li X, Kumar V, Wang Z, Tian Y, et al. (2023a) Towards greener yet powerful code generation via quantization: An empirical study. In: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp 224236 Wei Y, Wang Z, Liu J, Ding Y, Zhang (2023b) Magicoder: Source code is all you need. arXiv preprint arXiv:231202120 Weyssow M, Zhou X, Kim K, Lo D, Sahraoui (2023) Exploring parameterefficient fine-tuning techniques for code generation with large language models. ACM Transactions on Software Engineering and Methodology Williams M, Aletras (2024) On the impact of calibration data in posttraining quantization and pruning. In: Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp 1010010118 Wu X, Duan R, Ni (2024) Unveiling security, privacy, and ethical concerns of chatgpt. Journal of Information and Intelligence 2(2):102115 Xiao G, Lin J, Seznec M, Wu H, Demouth J, Han (2023) Smoothquant: Accurate and efficient post-training quantization for large language models. In: International Conference on Machine Learning, PMLR, pp 3808738099 Yoav B, Yosef (1995) Controlling the false discovery rate: practical and powerful approach to multiple testing. Journal of the Royal Statistical Society Series (Methodological) 57(1):289300 Yu Z, Zhang X, Shang N, Huang Y, Xu C, Zhao Y, Hu W, Yin (2023) Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. arXiv preprint arXiv:231214187 Yuan Z, Shang Y, Dong (2024) PB-LLM: partially binarized large language models. In: The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, OpenReview.net, URL https://openreview.net/forum?id=BifeBRhikU Zhang S, Roller S, Goyal N, Artetxe M, Chen M, Chen S, Dewan C, Diab M, Li X, Lin XV, et al. (2022) Opt: Open pre-trained transformer language models. arXiv preprint arXiv:220501068 Zhang (2018) Improved adam optimizer for deep neural networks. In: 2018 IEEE/ACM 26th international symposium on quality of service (IWQoS), Ieee, pp"
        }
    ],
    "affiliations": [
        "Software Institute UniversitÃ  della Svizzera italiana, Switzerland"
    ]
}