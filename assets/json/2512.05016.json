{
    "paper_title": "Generative Neural Video Compression via Video Diffusion Prior",
    "authors": [
        "Qi Mao",
        "Hao Cheng",
        "Tinghan Yang",
        "Libiao Jin",
        "Siwei Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression."
        },
        {
            "title": "Start",
            "content": "Qi Mao1(cid:66), Hao Cheng1, Tinghan Yang1, Libiao Jin1, Siwei Ma2 1 School of Information and Communication Engineering, Communication University of China 2 School of Computer Science, Peking University {qimao, yangtinghan, libiao}@cuc.edu.cn, chenghao@mails.cuc.edu.cn, swma@pku.edu.cn 5 2 0 2 4 ] . [ 1 6 1 0 5 0 . 2 1 5 2 : r Figure 1. Qualitative comparison on ultra-low bitrate video compression. Traditional and learned codecs produce blurry frames. Generative approaches such as GLC-Video [38] yield sharper textures but introduce structural hallucinations and unstable details, causing pronounced temporal flickering (see Fig. 2). Leveraging video-native diffusion prior, GNVC-VD produces coherent fine textures with strong temporal stability. Zoom in for best view."
        },
        {
            "title": "Abstract",
            "content": "We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their framewise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces unified flow-matching latent refinement module that leverages video diffusion transformer to jointly enhance intraand inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns correction term that adapts the diffusion prior to compression-induced degradation. conditioning adapter further injects compressionaware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression. 1. Introduction Neural video compression (NVC) [16, 18, 2427, 31, 43] has advanced rapidly in recent years, with learned codecs now surpassing traditional hybrid standards such as HEVC [45] and VVC [3] in ratedistortion optimization. However, when bitrate drops to the ultra-low regime, distortion-driven objectives (e.g., MSE) tend to oversmooth textures and erase fine structures, causing sharp decline in perceptual realism. Improving perceptual quality under such extreme compression remains an open and fundamental challenge for NVC. In the image domain, this perceptual collapse has been largely alleviated. Recent generative image codecs [4, 17, 20, 29, 33, 37, 50, 55] leverage large pre-trained GANs [8, 10] or diffusion models [42] to recover high-frequency textures, producing visually convincing reconstructions even at extremely low bitrates. This naturally raises the question: can the same strategy be extended to video compression? Unfortunately, videos impose much stricter requirementtemporal coherence. Although recent perceptual video codecs [32, 38] integrate image generative priors [8, 42], such priors remain inherently static and lack any modeling of temporal dynamics. As result, codecs built upon them remain fundamentally frame-level: even with adjacent-frame conditioning, the generative prior cannot capture long-range temporal structure. Consequently, the restored appearance drifts over time, leading to the wellknown perceptual flickering that becomes especially severe at ultra-low bitrates, as illustrated in Fig. 2. Recently, video diffusion models (VDMs)especially those based on diffusion transformers (DiTs) [21, 30, 47, 53]offer natural path forward. Trained on large-scale video data, they learn spatio-temporal latent representations that capture appearance, motion, and long-range dependencies within unified structure, enabling the synthesis of sequences with coherent texture and motion. These properties make VDMs an ideal generative prior for video compression, motivating us to rethink decoding not as independent frame reconstruction but as sequence-level conditional denoising process guided by video-native model. Building on this insight, we introduce GNVC-VD, the first generative NVC framework that fully leverages pre-trained video diffusion transformer (VideoDiT). Unlike prior perceptual codecs [32] constrained by image generative priors and thus limited to frame-wise enhancement, GNVC-VD redesigns the entire coding pipeline around sequence-level compression and generative refinement, enabling the diffusion prior to guide reconstruction beyond frame-wise prediction. At its core, GNVC-VD integrates two tightly coupled components: (1) conditional contextual transform codec that compresses the spatio-temporal latent representations while preserving long-range temporal structure, and (2) flow-matchingbased latent refinement module that performs sequence-level generative denoising across both intraand inter-frame latents, driven by the video DiT. Rather than denoising from pure Gaussian noiseas done in video generation GNVC-VD refines the decoded spatio-temporal latents directly, learning correction term that adapts the pre-trained diffusion prior to compressioninduced distortions. compression-aware conditioning adapter modulates intermediate DiT activations, allowing the generative prior to restore sharp textures while maintaining temporal coherence even at ultra-low bitrates. We further ensure compatibility between compressed latents and the diffusion manifold by adopting two-stage training strategy that first aligns the codecs latent space with the generative prior and then fine-tunes in the pixel domain, yielding stable and coherent refinement across diverse bitrate settings. Powered by video-native generative prior and unified codec design, GNVC-VD consistently outperforms both traditional and learned codecs in perceptual quality, while markedly reducing the flickering artifacts that persist in prior generative approacheseven under extreme bitrate constraintsas illustrated in Fig. 1 and Fig. 2. In summary, our main contributions are as follows: We introduce GNVC-VD, the first generative NVC framework that leverages video-native diffusion model to enable sequence-level latent compression and refinement, overcoming the frame-wise limitations of image-based generative priors. We propose DiT-based latent refinement mechanism that uses flow-matching and compression-aware conditioning to adapt the video diffusion prior for reconstructing compressed spatio-temporal latents, enabling effective generative correction within the codec. Extensive experiments across multiple benchmarks demonstrate that GNVC-VD delivers state-of-the-art perceptual performance below 0.03 bpp, restoring sharper textures and significantly reducing flickering compared with traditional, learned, and prior generative codecs. 2. Related Works Neural Video Compression [11, 16, 18, 2427, 31, 32, 38, 40, 43] has made substantial progress in recent years, achieving strong RD performance across wide range of biFigure 2. (a) Spatial and tx comparisons. Traditional and learned codecs lose fine textures, while GLC-Video [38] exhibits sharp but unstable structures that cause temporal flickering. GNVC-VD preserves clean textures and stable motion. (b) Frame-wise warp error Ewarp further confirms GNVC-VDs temporal stability, in contrast to the large fluctuations of GLC-Video. trates. By learning compact latent representations and temporal dependencies in an end-to-end manner, NVC methods outperform traditional hybrid codecs in both PSNR and MS-SSIM metrics. Prior works can be broadly categorized into residual-based [16, 31], 3D autoencoder-based [11, 40], and conditional coding-based architectures [18, 2427, 43]. Among these, conditional coding approachessuch as the DCVC family [18, 2427, 43]have set new state-of-theart results by using decoded features as context to guide motion estimation, latent prediction, and entropy modeling. However, as these methods are primarily optimized for distortion-oriented objectives (e.g., MSE), their reconstructions tend to be overly smooth and lack fine textures at extremely low bitrates. This reveals the inherent limitation of current NVC frameworks and calls for perceptually optimized compression to better preserve visual realism under extreme bitrate constraints. Perceptual Compression with Generative Prior. To enhance perceptual quality at ultra-low bitrates, recent studies introduce generative priorstermed generative compression [2, 4, 17, 20, 23, 28, 29, 3234, 37, 38, 50, 55]which leverage learned generative models to guide reconstruction and recover realistic textures beyond pixel fidelity. In the image domain, early works incorporate adversarial losses [2, 34, 37] or VQ-based tokenization [17, 33, 50] to achieve perceptually convincing reconstructions under extreme rate constraints. Diffusion-based frameworks [4, 20, 23, 28, 29, 55] further improve perceptual quality by reformulating decoding as conditional denoising guided by compact latents and leveraging large-scale text-to-image diffusion models prior [42]. Extending this paradigm to videos, recent methods such as GLC-Video [38] and DiffVC [32] adapt pre-trained image generative models by either encoding frame-wise generative embeddings or applying diffusion-based frame enhancement. However, these approaches still depend on image-domain priors without explicit temporal modeling, leading to flickering and motion inconsistency at ultralow bitrates, as illustrated in Fig. 2(b). In contrast, our work is the first to introduce video generative prior into NVC, jointly encoding spatio-temporal latents and performing unified latent refinement to achieve temporally coherent and perceptually realistic reconstructions under extreme compression. Video Diffusion Models have emerged as powerful generative frameworks capable of synthesizing high-quality, temporally coherent video sequences. Early works [15, 44] extended 2D UNet-based image diffusion into 3D UNets for spatio-temporal generation, while latent diffusion approaches [5, 14, 41, 49] improved efficiency by operating in compressed latent spaces. More recent DiT-based architectures [21, 30, 47, 53] represent videos as sequences of latent tokens, enabling long-range temporal reasoning and disentangled modeling of appearance and motion. Building on these advances, we employ pre-trained video diffusion model as video-native prior within NVC. Instead of initializing diffusion from Gaussian noise as in video generation, GNVC-VD performs refinement directly on decoded spatio-temporal latents, learning correction term that compensates for compression-induced degradation. 3. Proposed Method In this work, we aim to achieve perceptually faithful and temporally coherent video reconstruction under extreme compression (<0.03 bpp). Unlike prior generative codecs [32, 38] that operate at the frame level with imagebased priors [8, 42], our GNVC-VD processes entire video sequences using pre-trained video diffusion model [47], enabling joint spatio-temporal latent compression and refinement across intraand inter-frames. This design effectively captures long-range temporal dependencies and restores fine details while mitigating flickering artifacts. We first outline the overall framework (Section 3.1), followed by the spatio-temporal latent compression (Section 3.2), the diffusion-based latent refinement (Section 3.3), and the twostage training strategy (Section 3.4). Figure 3. Overview of the proposed GNVC-VD framework. (a) Overall pipeline composed of two key modules: (b) Contextual Latent Codec for spatio-temporal latent compression (Section 3.2), and (c) VideoDiT-based refinement module that performs flow-matching latent refinement (Section 3.3). 3.1. Framework of GNVC-VD As illustrated in Fig. 3(a), given an input video R(1+T )HW 3, 3D causal VAE encoder from Wan2.1 [47] encodes it into compact spatio-temporal latent sequence: x1 = E(V ), x1 = {lt}1+T /4 t=1 . (1) Here x1 is spatio-temporal latent sequence, and each lt RH/8W/816. To reduce latent-wise redundancy, each latent lt is compressed by contextual transform coding module as shown in Fig. 3(b), which consists of an analysis transform ga and synthesis transform gs: ˆyt = Quant(cid:0)ga(lt ˆlt1)(cid:1), ˆlt = gs(ˆyt, ˆlt1), (2) where ˆlt1 provides temporal context to reduce redundancy across frames. The quantized latent ˆyt is entropy-coded with learned probabilistic model to produce compact bitstream. The reconstructed latent sequence xc = {ˆlt}1+T /4 is refined using the pre-trained VideoDiT from Wan2.1 [47]. Gaussian noise ϵ (0, I) is added to obtain the noisy initialization xtN = xc + ϵ, which is iteratively denoised under the flow-matching formulation to generate the refined latent sequence x1 = {lt}1+T /4 t=1 : t=1 x1 = VideoDiT(cid:0)xtN {ft}1+T / t=1 (cid:1), (3) Table 1. Main symbols and notations. Definition Symbol V, E, x1, xc lt, ˆlt, lt ft x0, xtN tN , τ x1 L, τ vτ , vpre, Target / pre-trained / residual velocity fields Input / reconstructed video 3D causal encoder / decoder (Wan2.1 [47]) Clean / compressed latent sequences Original, decoded, refined latent at time Contextual feature extracted from ˆlt Contextual feature sequence, = {ft}1+T /4 Gaussian noise / noised latent Flow start time / intermediate time Refined latent at τ = 1 Flow steps / time interval t=1 cess, conditioning adapter blocks are inserted into the transformer layers to inject contextual features = {ft}1+T /4 extracted by the contextual transform codec, aligning compression-domain cues with the diffusion latent space and promoting stable refinement over time. t=1 Finally, the 3D causal decoder reconstructs the video: = D(x1). (4) This pipeline integrates transform-based compression with diffusion-based generative refinement, achieving perceptually faithful and temporally coherent reconstruction under extreme compression. For clarity, the main symbols and notations used in this paper are summarized in Table 1. 3.2. Spatio-Temporal Latent Compression where {ft}1+T /4 denotes the contextual feature sequence extracted from the compression domain. During this prot=1 To exploit temporal correlations in the latent space, GNVCVD employs contextual latent codec that performs transform coding on the spatio-temporal representations extracted by the 3D causal encoder, as illustrated in Fig. 3(b). The latent sequence x1 = {lt}1+T /4 is partitioned along the temporal axis, where each lt captures compact sequence-level appearance and motion cues due to the encoders temporal downsampling (T/4). t=1 For the anchor latent l1 (corresponding to the I-frame), we apply separate transform coding module [12] without temporal conditioning to initialize the sequence. For predictive latents {lt}t>1, each lt is encoded conditioned on the previously decoded latent ˆlt1 to reduce temporal redundancy, following the design philosophy of DCVC-RT [18]. temporal context feature ft1 is extracted from ˆlt1 and injected into both the analysis transform ga and the synthesis transform gs, extending Eq. (2) to: ˆyt = Quant(cid:0)ga(lt ft1)(cid:1), ˆlt = gs(ˆyt, ft1), (5) where ˆyt is entropy-coded by learned probabilistic model. This conditional coding process yields compact, motionaware latent representations that preserve temporal continuity and serve as the foundation for the diffusion-based refinement described in Section 3.3. 3.3. Flow-Matching Latent Refinement To further enhance perceptual quality at extremely low bitrates, GNVC-VD introduces unified latent refinement module that leverages the pre-trained VideoDiT as powerful video-native prior. Unlike prior diffusion-enhanced codec [38] that refine frame-wise latents independently, our method performs refinement directly in the 3D latent space and jointly enhances the entire sequence of Iand Pframe latents, ensuring spatio-temporal coherence and texture consistency. Preliminary on Flow Matching. Recent VideoDiT architectures adopt flow-matching formulation [21, 47] to train the diffusion models, which formulates generative modeling as learning continuous velocity field vτ that transports noisy sample x0 (0, I) toward the data manifold x1. Given probability path xτ , the model predicts the instantaneous velocity vτ = dxτ that aligns with the target flow dτ from x0 to x1, allowing deterministic generation and partial denoising without stochastic sampling. Motivation and Formulation. After compression in Section 3.2, the decoded latent xc can be regarded as perturbed version of the original latent x1: xc = x1 + e, (6) where denotes the quantization error. In video generation, flow-matching models reconstruct data by traversing the full probability path from Gaussian noise x0 (0, I) to x1, which is inefficient in video compression scenarios since xc already lies close to the data manifold. We instead initialize the refinement from xc, injecting Gaussian noise x0 at partial noise level tN [0, 1], where tN controls the degree of perturbation applied to xca larger tN introduces stronger noise and thus longer refinement path: xtN = tN xc + (1 tN )x0, (7) and define continuous probability flow path parameterized by τ [tN , 1], where τ denotes the flow time variable integrating from the partially noised state xtN to the clean latent x1: xτ = τ tN 1 tN x1 + 1 τ 1 tN xtN . The corresponding target velocity field is expressed as: vτ = (x1 x0) (cid:125) (cid:124) (cid:123)(cid:122) vpre-train tN 1tN (cid:124) , (xc x1) (cid:123)(cid:122) (cid:125) vfine (8) (9) t=1 where vpre-train is the velocity field learned by the pretrained diffusion model, and vfine denotes the correction term adapting the generative prior to compression-induced degradation. Implementation. In practice, Gaussian noise is injected into the decoded latent sequence xc = {ˆlt}1+T /4 at noise level tN to obtain xtN , which is refined via deterministic flow integration steps with step size τ = (1 tN )/L using the frozen VideoDiT backbone. While the pre-trained VideoDiT provides the baseline velocity field vpre-train, we introduce conditioning adapter layers into its transformer blocks to estimate the correction term vfine in Eq. (9). These adapters take the contextual feature sequence {ft}1+T /4 produced by the contextual latent codec in Section 3.2 as conditioning input, and modulate intermediate VideoDiT representations accordingly, thereby aligning the generative prior with the compressed latent distribution. The refined latent sequence x1 = {lt}1+T /4 is then decoded by the 3D causal decoder to produce the perceptually enhanced video . As illustrated in Fig. 3(c), this adapter-driven refinement efficiently compensates for quantization artifacts while maintaining spatio-temporal coherence under extreme bitrate constraints. t=1 t=1 3.4. Two-Stage Training Strategies To effectively integrate the video diffusion prior under practical bitrate constraints, GNVC-VD adopts two-stage compression-aware training scheme that progressively bridges the gap between codec learning and generative refinement. Direct end-to-end optimization is unstable due to the mismatch between the diffusion manifold and the quantized latent representations from the codec. Hence, training proceeds in two phases: (1) latent-level alignment to make enhanced latents consistent with the ground-truth diffusion Figure 4. Ratedistortion curves on the HEVC-B [9], UVG [36], and MCL-JCV [48] in the ultra-low bitrate regime (< 0.03 bpp). We report perceptual quality in terms of LPIPS and DISTS in the ultra-low bitrate regime (< 0.03 bpp). GNVC-VD consistently achieves the best perceptual quality, clearly outperforming traditional codecs (HEVC, VVC), learned codecs (DCVC-FM, DCVC-RT), and generative baselines (GLC-Video). latents, and (2) pixel-level fine-tuning for perceptually faithful reconstruction. Stage I: Latent-Level Alignment. Given the pre-trained 3D VAE encoderdecoder (E, D), we first align the enhanced latent x1 (produced by the diffusion refinement) with the ground-truth latent x1 obtained from E. This stage jointly trains the conditional transform codec and the diffusion adapter to ensure that the refined latents recover the semantic and structural details lost during quantization. The latent-level objective combines ratedistortion optimization with conditional flow-matching loss: Llatent = R(ˆy) + λrx1 x12 2 + LCFM, (10) where λr controls the strength of latent reconstruction fidelity. the pixel domain to enhance perceptual quality and temporal coherence. Partially noised latents xtN are initialized from xc and refined into x1 through fixed flow steps, which are then decoded into reconstructed frames = D(x1). The training objective combines perceptual, distortion, and rate regularization: Lpixel = R(ˆy) + λr (cid:16) V 2 2 + λlpipsLLPIPS(V, ) (cid:17) 2 + x1 x12 2 (12) , + xc x12 where λr controls the overall strength of the reconstruction and alignment terms, and λlpips balances perceptual quality against pixel fidelity. Through this fine-tuning, the diffusion prior is adapted to the compression domain, enabling visually coherent and perceptually rich reconstructions under extreme bitrate constraints. LCFM = Eτ [tN ,1], xτ , xc (cid:2)vθ(xτ , τ, xc) vτ 2 2 (cid:3) . (11) 4. Experiments Here, vθ() is the velocity field predicted by the VideoDiT backbone, and vτ is the target velocity field defined in Eq. (9). This latent-level training encourages the codec and diffusion adapter to recover semantically faithful latents consistent with the ground-truth diffusion manifold. Stage II: Pixel-Level Fine-Tuning. After latent-level alignment, we fine-tune the entire GNVC-VD pipeline in 4.1. Experimental Setup Datasets. For training, we use the Vimeo-90k dataset [51] with 5-frame clips to pre-train GNVC-VD, and extend the original Vimeo videos [1] into longer 25-frame sequences for fine-tuning. Evaluation is conducted on widely used benchmarks, including HEVC Class [9], UVG [36], and MCL-JCV [48]. Table 2. Temporal consistency and semantic continuity comparison on HEVC-B. Lower Ewarp and higher CLIP-F indicate better low-level temporal and semantic coherence. Additional results are provided in Appendix Section C.2. Method Ewarp CLIP-F Traditional / Learned Codecs HEVC VVC DCVC-FM DCVC-RT Generative Codec GLC-Video W/o Latent Refinement W/o Stage Loss W/o Stage II Loss GNVC-VD (Ours) 23.3 24.4 59.8 59.2 86.5 43.1 60.6 145.9 66.6 0.982 0.984 0.984 0.984 0.979 0.968 0.982 0.957 0. Figure 5. Qualitative comparison across different codecs at ultra-low bitrates. Compared with traditional, learned, and prior generative codecs, GNVC-VD preserves finer structures. More visual examples are available in the Appendix Section C.5. Compared Methods. We compare GNVC-VD against several state-of-the-art video compression approaches spanning traditional hybrids (HEVC [45] and VVC [3] ), neural codecs (DCVC-FM [27] and DCVC-RT [18]), and recent generative compression models (PLVC [52] and GLCVideo [38]). All learned baselines are evaluated using official implementations or author-reported results for fair comparison. Following prior NVC studies [18, 2427, 43], we evaluate the first 96 frames of each test sequence under low-delay prediction configuration. Since the video diffusion prior is pre-trained in RGB space, all baselines are tested under the same RGB mode to ensure fair comparison protocol. Evaluation Metrics. We evaluate GNVC-VD along three dimensions: perceptual quality, compression efficiency, and temporal consistency. Perceptual quality is measured using LPIPS-VGG [54] and DISTS [7], where the VGGbased LPIPS variant is used as it aligns better with subjective perception in generative compression. Compression efficiency is quantified by Bits Per Pixel (BPP). Temporal consistency is assessed using CLIP-F [39] and the warp error Ewarp [22], where CLIP-F evaluates semantic continuity across frames, and Ewarp measures low-level alignment by comparing ˆXt with the flow-warped ˆXt+1 as Ewarp = (cid:0) (cid:80) ( ˆXt+1, Ftt+1)i2 ˆX 2, with Ftt+1 estimated by RAFT [46] and Mt denoting the non-occlusion mask. Additional metrics and results are provided in Appendix Section C.1. (cid:1)1 (cid:80) t 4.2. Comparison Results Quantitative Comparisons. We quantitatively evaluate GNVC-VD against representative learned and generative video codecs on UVG [36], MCL-JCV [48], and HEVC Class [9]. As summarized in Fig. 4, GNVC-VD delivers consistent gains across perceptual quality metrics. On Figure 6. Visual comparison of temporal consistency. Groundtruth frames at t=0, 1, 5 are shown on the left. On the right, GLCVideo [38] displays clear temporal flickeringtextures drift and vary across frameswhile GNVC-VD produces stable, temporally coherent reconstructions. Implementation Details. We adopt Wan2.1 [47] as the pre-trained video diffusion model. The contextual transform coding network follows the architecture of DCVCRT [18], and we use conditioning adapter blocks similar to those in VACE [19]. In Stage I, GNVC-VD is trained on 5-frame Vimeo-90k clips using 256 256 patches, batch size of 8, and learning rate of 1 104 for 40k iterations with λr = 0.5. We then fine-tune on longer Vimeo sequences by resizing the shorter side to 512 pixels and randomly cropping 256 256 patches. The model is trained for 30k iterations on 9-frame clips and another 30k on 13-frame clips, both with learning rate of 1 104. In Stage II, we fine-tune for an additional 100k iterations with λr {0.05, 0.1, 0.25, 0.5} and batch size of 2, while progressively reducing the learning rate from 5 105 to 1 105. The partial noise level is fixed at tN = 0.7, and the number of flow refinement steps is set to 5. All experiments are conducted on two NVIDIA A800 GPUs. Additional implementation details are provided in Appendix Section A. Table 3. BD-Rate (%) comparisons anchoring by VVC [3]. Method HEVC-B MCL-JCV UVG LPIPS DISTS LPIPS DISTS LPIPS DISTS VVC HEVC DCVC-FM DCVC-RT PLVC GLC-Video GNVC-VD (Ours) 0.0 32.0 -10.7 -20.9 -3.4 -79.1 -89.4 0.0 -2.64 33.6 15.4 -22.75 -94.8 -94. 0.0 35.7 -2.1 -14.0 37.3 -74.8 -90.8 0.0 -12.2 -53.9 30.2 88.4 -99.7 -96.2 0.0 27.7 -27.7 -30.7 -20.6 -60.0 -86.5 0.0 -6.5 12.9 2.03 -85.0 -10.3 -96.1 Table 4. Ablation studies on BD-LPIPS and BD-DISTS, anchoring by our full model. Negative values indicate improvements over the anchor, while positive values indicate degradations. Method HEVC-B UVG LPIPS DISTS LPIPS DISTS W/o Latent Refinement +0.181 +0.016 W/o Stage Loss in Eq. (10) W/o Stage II Loss in Eq. (12) +0.252 +0.132 +0.021 +0.217 +0.159 +0.016 +0. +0.129 +0.017 +0.183 GNVC-VD (Ours) 0 0 0 UVG, GNVC-VD achieves over 98% BD-rate reduction in DISTS and 56% in LPIPS compared with the distortionoriented baseline DCVC-RT [18]. Compared with the generative codec GLC-Video [38], GNVC-VD further reduces BD-rate by 86% in DISTS and 21% in LPIPS, as reported in Table 3. Beyond spatial perceptual quality, GNVC-VD also demonstrates superior temporal coherence, achieving higher CLIP-F scores and substantially lower Ewarp than GLC-Video, indicating more stable motion reconstruction and reduced frame-level flickering, as illustrated in Table 2. Qualitative Comparisons. Fig. 5 illustrates visual comparisons among representative methods. GNVC-VD produces perceptually sharper and more realistic reconstructions under extreme compression. traditional and distortion-oriented neural codecs (e.g. DCVC-RT) yield spatially over-smoothed results, while generative codecs like GLC-Video exhibit temporal flickering and motion inconsistencies, as illustrated in Fig. 6. Benefiting from our flow-matching latent refinement, GNVC-VD preserves motion continuity and spatio-temporal coherence across frames, achieving stable visual quality even below 0.03 bpp. In contrast, 4.3. Ablation Studies We conduct ablation studies on three key components of GNVC-VD: (1) the flow-matching latent refinement module, (2) the Stage latent-level alignment loss in Eq. (10), and (3) the Stage II pixel-level fine-tuning loss in Eq. (12). As shown in Table 4, Table 2, and Fig. 7, the codec-only baseline produces strong over-smoothing and severely degraded perceptual quality. Removing the Stage loss weakens the compatibility between codec latents and the difFigure 7. Qualitative ablation results. We visualize the impact of each module in GNVC-VD. Without flow-matching refinement, results become over-smoothed; removing Stage weakens latentprior alignment and reduces detail reconstruction; removing Stage II limits pixel-level adaptation. The full model consistently restores sharper details, validating the effectiveness of all components. fusion prior, resulting in poorer detail recovery. Training without Stage II converges faster but yields inferior reconstructions due to insufficient pixel-level adaptation. In contrast, the full GNVC-VD model achieves the best perceptual quality while preserving motion consistencyrather than relying on the artificial temporal stability that oversmoothed variants exhibit simply because fine details are lost. 5. Conclusion and Future Work We introduced GNVC-VD, generative NVC framework that leverages pre-trained video diffusion prior to achieve perceptually coherent reconstruction at extremely low bitrates. Unlike approaches based on image-domain priors, GNVC-VD performs sequence-level latent denoising guided by spatio-temporal diffusion dynamics, enabling the recovery of sharp textures and temporally consistent motion within unified codec architecture. Extensive experiments demonstrate that GNVC-VD substantially improves perceptual quality and markedly reduces flickering artifacts, preserving realistic motion and fine details even below 0.03 bpp. While GNVC-VD exhibits strong perceptual and temporal performance, further advances remain possible. The transform coding module could be made more efficient, and accelerating diffusion-based refinement is an important direction for future research. Overall, GNVC-VD underscores the potential of video-native generative priors for next-generation perceptual video compression."
        },
        {
            "title": "References",
            "content": "[1] Original Vimeo links. https : / / github . com / anchen1011 / toflow / blob / master / data / original_vimeo_links.txt. 6 [2] Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Generative adversarial In Pronetworks for extreme learned image compression. ceedings of the IEEE/CVF international conference on computer vision, pages 221231, 2019. 3 [3] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary Sullivan, and Jens-Rainer Ohm. Overview of the versatile video coding (VVC) standard and its applications. IEEE TCSVT, 31(10):37363764, 2021. 2, 7, 8 [4] Marlene Careil, Matthew Muckley, Jakob Verbeek, and Stephane Lathuili`ere. Towards image compression with perfect realism at ultra-low bitrates. In The Twelfth International Conference on Learning Representations, 2023. 2, 3 [5] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024. 3 [6] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 79397948, 2020. [7] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli. Image quality assessment: Unifying structure and texture similarity. CoRR, abs/2004.07728, 2020. 7 [8] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 3 [9] Flynn, Sharman, and Rosewarne. Common Test Conditions and Software Reference Configurations for HEVC Range Extensions, document JCTVC-N1006. Joint Collaborative Team Video Coding ITU-T SG, 16. 6, 7 [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 2 [11] Amirhossein Habibian, Ties van Rozendaal, Jakub Tomczak, and Taco Cohen. Video compression with rateIn Proceedings of the IEEE/CVF distortion autoencoders. international conference on computer vision, pages 7033 7042, 2019. 2, 3 [12] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57185727, 2022. 5, 12 [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [14] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 3 [15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. ArXiv, abs/2210.02303, 2022. 3 [16] Zhihao Hu, Guo Lu, and Dong Xu. Fvc: new framework In Protowards deep video compression in feature space. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15021511, 2021. 2, 3 [17] Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Generative latent coding for ultra-low bitrate image comIn Proceedings of the IEEE/CVF Conference on pression. Computer Vision and Pattern Recognition, pages 26088 26098, 2024. 2, 3 [18] Zhaoyang Jia, Bin Li, Jiahao Li, Wenxuan Xie, Linfeng Qi, Houqiang Li, and Yan Lu. Towards practical real-time neural video compression. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-25, 2024, 2025. 2, 3, 5, 7, 8, 12 [19] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 7 [20] Anle Ke, Xu Zhang, Tong Chen, Ming Lu, Chao Zhou, Jiawen Gu, and Zhan Ma. Ultra lowrate image compression with semantic residual coding and compression-aware diffusion. arXiv preprint arXiv:2505.08281, 2025. 2, [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3, 5 [22] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In Proceedings of the European conference on computer vision (ECCV), pages 170185, 2018. 7 [23] Eric Lei, Yigit Berkay Uslu, Hamed Hassani, and Shirin Saeedi Bidokhti. Text+ sketch: Image compression at ultra low rates. arXiv preprint arXiv:2307.01944, 2023. 3 [24] Jiahao Li, Bin Li, and Yan Lu. Deep contextual video compression. Advances in Neural Information Processing Systems, 34:1811418125, 2021. 2, 3, 7 [25] Jiahao Li, Bin Li, and Yan Lu. Hybrid spatial-temporal entropy modelling for neural video compression. In Proceedings of the 30th ACM International Conference on Multimedia, 2022. 12 [26] Jiahao Li, Bin Li, and Yan Lu. Neural video compression In IEEE/CVF Conference on Comwith diverse contexts. puter Vision and Pattern Recognition, CVPR 2023, Vancouver, Canada, June 18-22, 2023, 2023. [27] Jiahao Li, Bin Li, and Yan Lu. Neural video compression with feature modulation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 17-21, 2024, 2024. 2, 3, 7 [28] Zhiyuan Li, Yanhui Zhou, Hao Wei, Chenyang Ge, and Jingwen Jiang. Towards extreme image compression with latent feature guidance and diffusion prior. IEEE Transactions on Circuits and Systems for Video Technology, 2024. 3 [29] Zhiyuan Li, Yanhui Zhou, Hao Wei, Chenyang Ge, and Ajmal Mian. Rdeic: Accelerating diffusion-based extreme image compression with relay residual diffusion. IEEE Transactions on Circuits and Systems for Video Technology, 2025. 2, 3 [30] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 2, 3 [31] Guo Lu, Wanli Ouyang, Dong Xu, Xiaoyun Zhang, Chunlei Cai, and Zhiyong Gao. Dvc: An end-to-end deep video compression framework. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1100611015, 2019. 2, 3 [32] Wenzhuo Ma and Zhenzhong Chen. Diffusion-based perceptual neural video compression with temporal diffusion information reuse. arXiv preprint arXiv:2501.13528, 2025. 2, [33] Qi Mao, Tinghan Yang, Yinuo Zhang, Zijian Wang, Meng Wang, Shiqi Wang, Libiao Jin, and Siwei Ma. Extreme imIn 2024 Data age compression using fine-tuned vqgans. Compression Conference (DCC), pages 203212. IEEE, 2024. 2, 3 [34] Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression. In Advances in Neural Information Processing Systems (NeurIPS), 2020. arXiv:2006.09965. 3 [35] Fabian Mentzer, George Toderici, Michael Tschannen, and Eirikur Agustsson. High-fidelity generative image compression. Advances in neural information processing systems, 33:1191311924, 2020. 12 [36] Alexandre Mercat, Marko Viitanen, and Jarno Vanne. UVG dataset: 50/120fps 4K sequences for video codec analysis and development. In Proceedings of the 11th ACM Multimedia Systems Conference, pages 297302, 2020. 6, 7 [37] Matthew Muckley, Alaaeldin El-Nouby, Karen Ullrich, Herve Jegou, and Jakob Verbeek. Improving statistical fidelity for neural image compression with implicit local likeIn International Conference on Machine lihood models. Learning, pages 2542625443. PMLR, 2023. 2, 3 [38] Linfeng Qi, Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Generative latent coding for ultra-low bitrate IEEE Transactions on Cirimage and video compression. cuits and Systems for Video Technology, 2025. 1, 2, 3, 5, 7, 8, [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 7 [40] Oren Rippel, Sanjay Nair, Carissa Lew, Steve Branson, Alexander Anderson, and Lubomir Bourdev. Learned video compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 34543463, 2019. 2, 3 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [43] Xihua Sheng, Jiahao Li, Bin Li, Li Li, Dong Liu, and Yan Lu. Temporal context mining for learned video compression. IEEE Transactions on Multimedia, 2022. 2, 3, 7 [44] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023. [45] Gary Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (HEVC) standard. IEEE TCSVT, 22(12):16491668, 2012. 2, 7 [46] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. 7 [47] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 4, 5, 7 [48] Haiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang, Ioannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. MCL-JCV: JND-based H. 264/AVC video quality assessment dataset. In 2016 IEEE international conference on image processing (ICIP), pages 15091513. IEEE, 2016. 6, 7 [49] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [50] Naifu Xue, Qi Mao, Zijian Wang, Yuan Zhang, and Siwei Ma. Unifying generation and compression: Ultra-low bitrate image coding via multi-stage transformer. In 2024 IEEE International Conference on Multimedia and Expo (ICME), pages 16. IEEE, 2024. 2, 3 [51] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William Freeman. Video enhancement with task-oriented International Journal of Computer Vision, 127(8): flow. 11061125, 2019. [52] Ren Yang, Radu Timofte, and Luc Van Gool. Perceptual learned video compression with recurrent conditional gan. In IJCAI, pages 15371544, 2022. 7, 12 [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [54] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7 [55] Tianyu Zhang, Xin Luo, Li Li, and Dong Liu. Stablecodec: Taming one-step diffusion for extreme image compression. arXiv preprint arXiv:2506.21977, 2025. 2, 3 A. Test Settings For fair comparison with both traditional codecs and neural video compression methods, all approaches are evaluated in the RGB color space. A.1. Test Sequences The raw videos are stored in YUV420 format. We convert them to RGB using the BT.709 standard. For evaluation, we extract the first 96 frames of each sequence. For codecs that require input resolutions to be multiples of 64, we apply zero-padding before encoding and crop the decoded frames back to their original size. A.2. Traditional Codecs evaluate two representative and VTM-17.02. We HM-16.251 nally in 10-bit YUV444, and final puted in RGB. We use the official figurations coder lowdelay vtm.cfg (VTM). traditional codecs, Both operate interresults are comlow-delay conand enencoder lowdelay rext.cfg (HM) A.3. Neural-based Codecs Implementation details for neural codecs are summarized below: DCVC-FM / DCVC-RT. We use the official code and checkpoints from the authors GitHub repository3 . All frames are processed in RGB, and the GOP size is set to 96. GLC-Video. We use the reconstructed videos and bitrates provided directly by the original authors of GLC-Video [38]. All evaluation metrics are computed from the provided reconstructions. PLVC. PLVC [52] is evaluated using its official implementation4 and pre-trained weights. Since PLVC adopts HiFiC [35] for I-frame coding, we use its PyTorch implementation5 for consistency. GNVC-VD. Due to training and inference constraints, GNVC-VD processes each 96-frame sequence as four GOPs with lengths of 25, 25, 25, and 21 frames, respectively. B. Model Implementation Details Fig. 8 illustrates the detailed architecture of the proposed Contextual Latent Codec module. We use two separate neural networks to perform transform coding on the anchor latent l1 and the predictive latents {lt}t>1. 1https://vcgit.hhi.fraunhofer.de/jvet/HM 2https : / / vcgit . hhi . fraunhofer . de / jvet / VVCSoftware_VTM 3https://github.com/microsoft/DCVC 4https://github.com/RenYang-home/PLVC 5https://github.com/JustinTan/highfidelitygenerative-compression Anchor latent (I-frame). The processing pipeline for the anchor latent is shown in Fig. 3(a). We adopt design similar to ELIC [12], where the analysis and synthesis transforms (gs and ga) are constructed from cascaded residual bottleneck blocks [13] and attention blocks [6]. joint spacechannel context model estimates the probability distribution of the quantized anchor latent ˆy1. Predictive latents (P-frames). As illustrated in Fig. 3(b), for the predictive latents, we follow the architecture of DCVC-RT [18], where the transforms gs and ga are built from cascaded DC Blocks [18]. To balance coding efficiency and reconstruction quality, we adopt the two-step distribution estimation scheme described in [25]. C. Additional Experiments C.1. Additional Metrics Evaluation the For more comprehensive comparison, we report ratedistortion curves of all baseline methods and our GNVC-VD in terms of PSNR, MS-SSIM, and LPIPS-Alex in Fig. 9. The VGG-based LPIPS variant correlates more strongly with human perception in generative compression. Therefore, in the main paper, perceptual comparisons are reported using LPIPS-VGG, which provides more reliable indicator of perceptual fidelity. However, because the AlexNet-based LPIPS metric is more commonly used in the learned compression literature, we additionally include LPIPS-Alex results here for completeness. Compared with perceptual codecs such as GLC-Video [38] and PLVC [52], GNVC-VD achieves clear improvements in distortion-oriented metrics (PSNR and MS-SSIM) while also delivering notably better perceptual quality (LPIPSAlex), consistent with the LPIPS-VGG and DISTS improvements reported in the main paper. Relative to MSEoptimized codecs, although small gap remains in PSNR and MS-SSIM, GNVC-VD provides substantially superior perceptual fidelity. C.2. Additional Analysis on Ewarp and CLIP-F Table 5 presents the Ewarp and CLIP-F results, including bpp, Ewarp, and CLIP-F for each video sequence. GLC-Video, as an image-prior-based generative codec, shows weak temporal consistency across most sequences, whereas our video-prior-based GNVC-VD achieves markedly stronger temporal coherence. Although GNVC-VD attains slightly lower semantic consistency than traditional and MSE-optimized codecs, it still substantially outperforms the image-prior-based GLC-Video. C.3. Complexity We analyze the computational complexity of the proposed GNVC-VD in terms of model size and inference latency. As summarized in Table 6, GNVC-VD contains toFigure 8. Architecture of the Contextual Latent Codec module. Table 5. Detailed bpp, Ewarp, and CLIP-F results for all codecs on HEVC-B. Video name HEVC VVC DCVC-FM DCVC-RT GLC-Video GNVC-VD bpp Ewarp CLIP-F bpp Ewarp CLIP-F bpp Ewarp CLIP-F bpp Ewarp CLIP-F bpp Ewarp CLIP-F bpp Ewarp CLIP-F BasketballDrive 0.0098 0.0073 BQTerrace 0.0098 Cactus 0.0091 Kimono1 0.0087 ParkScene 61.14 7.98 15.06 24.18 8.07 0.974 0.993 0.972 0.986 0. 0.0087 0.0066 0.0077 0.0082 0.0081 65.34 7.89 15.27 25.45 8.03 0.976 0.991 0.973 0.988 0.991 0.0069 259.44 5.61 0.0060 9.35 0.0060 17.42 0.0066 7.24 0.0063 0.972 0.993 0.976 0.989 0.989 0.0052 255.84 5.61 0.0050 9.12 0.0051 17.78 0.0051 7.45 0. 0.972 0.992 0.976 0.991 0.990 0.0089 263.77 41.47 0.0066 27.45 0.0064 40.30 0.0097 59.88 0.0056 0.967 0.984 0.973 0.987 0.983 0.0057 263.33 15.69 0.0065 14.96 0.0061 20.89 0.0060 18.48 0.0064 average result 0. 23.29 0.982 0.0079 24.40 0.984 0. 59.81 0.984 0.0052 59.16 0.984 0. 86.57 0.979 0.0061 66.67 0.967 0.991 0.979 0.988 0.986 0. tal of 2334.5M parameters, including 126.9M in the 3D VAE, 53.1M in the Contextual Latent Codec module, and 2154.5M in the VideoDiT. Table 7 reports the per-frame encoding and decoding time on single A800 GPU. At resolution of 1920 1080, GNVC-VD runs at 153 ms for encoding and 1557 ms for decoding. The latency decreases to 58/386 ms at 1080 720 and 25/129 ms at 640 480, respectively. Table 6. Parameter count of each major module in the proposed GNVC-VD framework. Module Name Parameters (M) 3D VAE Contextual Latent Codec VideoDiT Total 126.9 53.1 2154.5 2334.5 Figure 9. Ratedistortion curves of all codecs evaluated using LPIPS-Alex, PSNR, and MS-SSIM. Table 7. Coding speed with different resolutions on single A800 GPU. Resolutions 1920 1080 720 640 480 Encoding Decoding 153 ms 1557 ms 58 ms 386 ms 25 ms 129 ms C.4. User Study To assess perceptual quality and temporal stability, we conducted user study comparing GNVC-VD with VVC, DCVC-RT, DCVC-FM, and GLC-Video. In each trial, participants viewed the reference video at the top and two reconstructed versions below itone produced by GNVCVD and the other by baseline codec. The leftright order was randomized to avoid positional bias. Participants were instructed to select the reconstruction that better matched the reference in terms of perceptual quality and temporal stability. As illustrated in Fig. 10, across all pairwise comparisons, GNVC-VD received strong user preference, achieving over 85% preference against both traditional and neural codecs, and nearly unanimous preference against the image-prior-based GLC-Video. These subjective findings are consistent with the objective evaluations, providing complementary assessment of GNVC-VDs perceptual fidelity and temporal coherence. C.5. Additional Visual Examples We provide additional qualitative comparisons on three datasets: HEVC Class B, MCL-JCV, and UVG. As shown Figure 10. User study results comparing GNVC-VD against VVC, DCVC-RT, DCVC-FM, and GLC-Video. The bars show the percentage of participants who preferred GNVC-VD in pairwise comparisons. in Fig. 11, GNVC-VD consistently outperforms prior stateof-the-art methods, delivering higher visual fidelity across diverse content while operating at the lowest bitrate. Figure 11. Visual comparisons across several test sequences, including ground truth, VVC, DCVC-RT, DCVC-FM, GLC-Video, and our GNVC-VD. Zoomed-in patches highlight texture preservation and perceptual differences. Bitrate (bpp) and LPIPS scores are shown beneath each reconstruction."
        }
    ],
    "affiliations": [
        "School of Computer Science, Peking University",
        "School of Information and Communication Engineering, Communication University of China"
    ]
}