{
    "paper_title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
    "authors": [
        "Fanqing Meng",
        "Jiaqi Liao",
        "Xinyu Tan",
        "Wenqi Shao",
        "Quanfeng Lu",
        "Kaipeng Zhang",
        "Yu Cheng",
        "Dianqi Li",
        "Yu Qiao",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive \\textbf{Phy}sics \\textbf{Gen}eration \\textbf{Ben}chmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 3 6 3 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Technical report",
            "content": "TOWARDS WORLD SIMULATOR: CRAFTING PHYSICAL COMMONSENSE-BASED BENCHMARK FOR VIDEO GENERATION Fanqing Meng,1,2, Jiaqi Liao,2, Xinyu Tan, Wenqi Shao2,, Quanfeng Lu2, Kaipeng Zhang2 Yu Cheng4, Dianqi Li, Yu Qiao2, Ping Luo3,2, 1Shanghai Jiao Tong University 3The University of Hong Kong 2OpenGVLab, Shanghai AI Laboratory 4 The Chinese University of Hong Kong Project Page: https://phygenbench123.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench , comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models understanding of physical commonsense. Alongside PhyGenBench , we propose novel evaluation framework called PhyGenEval . This framework employs hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval , we can conduct large-scale automated assessments of T2V models understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic physical phenomenons). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We release the data and codes at https://github.com/OpenGVLab/PhyGenBench"
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-video (T2V) models such as Sora have made significant strides in visualizing complex ideas and scenes based on textual input (Yang et al., 2024; Wang et al., 2023). These advancements are increasingly viewed as promising path towards constructing universal simulators of the physical world, which holds immense promise for video generation (Zhu et al., 2024), autonomous driving (Gao et al., 2024), and the development of embodied agents (Mazzaglia et al., 2024). Cognitive psychology posits that intuitive physics, which is demonstrated even by human infants (Wood et al., 2024; Battaglia et al., 2013), is essential for achieving this goal. Intuitive physics emphasizes rendered scenes should be visually and interactively natural to humans, rather than adhere to strict physical accuracy. Consequently, on the path towards developing world simulator (Xiang et al., 2024),video generation should first be capable of accurately reproducing simple yet fundamental Corresponding Authors: shaowenqi@pjlab.org.cn; pluo@cs.hku.hk Equal contribution"
        },
        {
            "title": "Technical report",
            "content": "Figure 1: Samples of videos generated by Kling or Gen-3 in PhyGenBench with 4 different aspects. The results show that current T2V models struggle to generate videos that align with physical commonsense (e.g., the lack of planes reflection in water in the first video of the second row). physical phenomenons. However, even state-of-the-art models trained on vast resources (Tan et al., 2024) encounter difficulties in correctly generating seemingly trivial physical phenomenons, as depicted in Figure 1, the model fails to understand that the stone should sink in water. This clear pitfall shows substantial gap between current video generation models and humans understanding of basic physics. It reveals how far these models are from being true world simulators. Given this context, it becomes important to assess the extent to which current T2V models can capture intuitive physics in their generated outputs. This requires the development of comprehensive evaluation frameworks that beyond traditional metrics. While numerous Text-to-Video (T2V) evaluation benchmarks have emerged (Sun et al., 2024; Huang et al., 2024), they primarily focus on various qualities of generated videos (e.g., motion smoothness, background consistency) or spatial relationships, failing to address the critical issue of whether the generated videos adhere to fundamental physical laws. Although some studies have explored the alignment of generated videos with dynamic motions naturalness (Bansal et al., 2024), their benchmarks fail to succinctly capture fundamental physical laws or propose sufficiently robust evaluation methods. Therefore, the development of benchmarks and evaluation methodologies specifically tailored to assess intuitive physics in generated videos remains critical yet largely unexplored frontier. There are two challenges impeding the evaluation of physical commonsense in T2V models. On one hand, there is lack of benchmarks focused on evaluating physical commonsense. This requires selecting semantically simple physical phenomenons that exhibit clear physical phenomena, allowing for accurate assessment by either humans or machines. On the other hand, there is lack of corresponding evaluation metrics. Traditional metrics like FVD (Unterthiner et al., 2018) exhibit limitations in detecting implausible motions (Brooks et al., 2022) and necessitate reference videos, which are often challenging to procure for novel scenes. Recent studies have used video-based VLMs for comprehensive video evaluation (He et al., 2024b; Sun et al., 2024). However, they often struggle to correctly assess physical commonsense. This limitation stems from their inadequate understanding of physical laws (Jassim et al., 2023) and the fact that these methods are not specifically designed to evaluate physical laws. To address these challenges, we propose PhyGenBench and PhyGenEval to automate the evaluation of physical commonsense understanding capability from T2V models. PhyGenBench is designed to evaluate physical commonsense based on fundamental physical laws in text-to-video generation. Inspired by (Halliday et al., 2013), we categorize physical commonsense in the world into four main areas: mechanics, optics, thermal, and material properties. And we identify significant physical"
        },
        {
            "title": "Technical report",
            "content": "laws and easily observable physical phenomenons for each category, resulting in comprehensive 27 physical laws and 160 validated prompts in the proposed benchmark. Specifically, we start from fundamental physical laws. Through brainstorming, we construct prompts that easily reflect physical laws using sources like textbooks (Harjono et al., 2020). This process results in comprehensive but simple set of prompts reflecting physical commonsense, which are sufficiently clear for evaluation. As shown in Figure 1, the correctness of physical commonsense in PhyGenBench can be observed through clear phenomena (e.g., plane should have reflections in water) On the other hand, benefiting from the simple yet clear physical phenomena in PhyGenBench prompts, we can propose PhyGenEval , which is novel video evaluation framework for assessing physical commonsense correctness in PhyGenBench . PhyGenEval first uses GPT-4o to analyze physical laws in text, addressing the poor understanding of physical common sense in video-based VLMs. Moreover, considering that previous evaluation metrics did not specifically target physical correctness, we propose three-tier hierarchical evaluation strategy for this aspect, transitioning from image-based to comprehensive video analysis: single image, multiple images, and full video stages. Each stage employs distinct VLMs along with custom instructions generated by GPT-4o to form judgments. By combining PhyGenBench and PhyGenEval , we can efficiently evaluate different T2V models understanding of physical commonsense at scale, producing results highly consistent with human feedback. The contributions of our work are three-fold. i): We proposed PhyGenBench , which compasses wide range of clear physical phenomenons and explicit physical laws. This benchmark can comprehensively measure whether T2V models understand intuitive physics and indirectly assess their gap from world simulator capabilities ii): Along with the benchmark, we propose an automated evaluation framework - PhyGenEval , which overcomes the challenges of assessing the correctness of physical commonsense with other metrics and demonstrates high consistency with human feedback on PhyGenBench , enabling users to conduct large-scale automated testing of various T2V models. iii): We conduct extensive evaluations of popular T2V models, even the best-performing model, Gen-3, scores only 0.51. This indicates that current models are still far from functioning as world simulators. Based on our evaluation results, we conduct an in-depth analysis and discover that addressing issues such as dynamics is still challenging through prompt engineering or simply scaling up model. We hope this work inspires the community to focus on the learning of physical commonsense in T2V models, rather than merely using them as tools for entertainment."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 BENCHMARKS FOR TEXT-TO-VIDEO GENERATION The rapid advancement of text-to-video (T2V) generation models has necessitated various benchmarks for accurate assessment. Traditional works in video generation, such as FVD (Unterthiner et al., 2018), rely on datasets like UCF-101 (Soomro, 2012) and Kinetics-400 (Kay et al., 2017), which are limited in scope. Recent benchmarks, including VBench (Huang et al., 2024) and EvalCrafter (Liu et al., 2024c), aim to comprehensively evaluate general video quality across multiple dimensions. In contrast, some studies focus on fine-grained evaluation of text-to-video (T2V) models from specific aspects. For instance, T2V-CompBench (Sun et al., 2024) assesses compositional generation capabilities, while DEVIL (Liao et al., 2024) evaluates dynamic characteristics of generated videos. Although some research like VideoPhy (Bansal et al., 2024) efforts address the dynamic motions naturalness of video generation, their benchmarks fail to succinctly capture fundamental physical laws. Consequently, most existing works overlook this crucial aspect, which forms the foundation for realizing world simulator. To address this gap, we introduce PhyGenBench , benchmark designed to comprehensively measure T2V models understanding of physical commonsense. 2.2 EVALUATION METRICS FOR TEXT-TO-VIDEO GENERATION Conventional approaches to video quality assessment often employ metrics such as FVD (Unterthiner et al., 2018) and IS (Salimans et al., 2016). However, the detection of unrealistic motions is difficult for them (Brooks et al., 2022), and FVD requires reference video that is hard to obtain for novel scenes, making it challenging to evaluate the correctness of physical commonsense. Recent studies have explored the use of advanced vision-language models (VLMs) as evaluators. For"
        },
        {
            "title": "Technical report",
            "content": "Figure 2: (a) is the overview of the proposed PhyGenBench . (b) is the PhyGenBench data pipeline, which covers four physics categories. We select key physical laws and manually craft initial prompts that reflect the corresponding physical phenomena. GPT-4o adds details and enhances diversity by varying objects. After manual review, we obtain 160 T2V prompts. instance, VideoScore (He et al., 2024b) leverages human feedback to train models for video quality assessment, while T2V-CompBench (Sun et al., 2024) utilizes powerful models like LLaVA (Liu et al., 2024a) to evaluate the correctness of spatial relationships. Although few works demonstrate improved alignment with human judgments, they fall short in generalizing to assessments of physical commonsense correctness. To address this limitation, we introduce PhyGenEval , novel method designed to evaluate physical commonsense correctness on PhyGenBench . We validate the efficacy of our approach through comprehensive human correlation studies."
        },
        {
            "title": "3 PHYGENBENCH",
            "content": "Inspired by (Swartz, 1985), we first define the following terms: Physical Commonsense: Basic intuitive understanding of how physical objects and actions behave in everyday life; Physical Laws: Universal scientific principles that describe consistent behaviors in nature; Physical Phenomenon: Observable events or processes caused by the interaction of physical laws. The purpose of PhyGenBench is to evaluate whether T2V models understand physical commonsense, while each prompt in PhyGenBench presents clear physical phenomenon and an underlying physical law. Overview. As illustrated in Figure 2 (a), PhyGenBench encompasses four major categories of physical commonsense: Mechanics, Optics, Thermal, and Material Properties. It incorporates 27 physical phenomena with intrinsic physical laws reflected by the corresponding designed 160 prompts: 1. Mechanics covers 7 common mechanical laws: gravity, buoyancy, solid pressure, atmospheric pressure, elasticity, friction, and surface tension, with 40 validated prompts. For example, we use piece of iron is gently placed on the surface of the water in tank filled with water to test T2V models understanding of Buoyancy, where the iron should sink due to its higher density compared to water. 2. Optics categorizes 6 aspects based on light phenomena: reflection, refraction, scattering, dispersion, interference & diffraction, and straight-line propagation, yielding 50 prompts. prompt like kite soaring above smooth and tranquil pond is used to test reflection generation capability."
        },
        {
            "title": "Technical report",
            "content": "3. Thermal considers 6 phase transitions: Solidification, Melting, Liquefaction, Boiling, deposition, Sublimation, comprising 30 prompts. Inspired by ChronoMagicBench (Yuan et al., 2024), the vaporization (boiling) process is evaluated by the prompt timelapse capturing the transformation of water as the temperature rapidly rises above 100C. 4. Material Properties includes 5 physical properties (color, hardness, solubility, combustibility, and flame reaction) and 3 chemical properties (acidity, redox potential, and dehydrating properties), resulting in 40 prompts. We reflect material properties, e.g., hardness, through the prompts with expected phenomena, e.g., an egg being hurled with significant force towards rock, where the egg should break while the rock remains intact. Multiple physical laws could be included in single prompt, which may bring confusion to the evaluation of physical common sense in video generation, even for human annotators. To avoid this, we carefully curate prompts to ensure one-to-one correspondence for each physical phenomenon it reflects, with clear physical law inside. By incorporating physical laws from four distinct physical categories, PhyGenBench offers thorough assessment of current T2V models understanding of physical commonsense. Benchmark Construction. As shown in Figure 2 (b), we develop comprehensive approach to create PhyGenBench . The methodology encompasses five steps: 1) Conceptualization: Following (Halliday et al., 2013), We first identify key physical commonsense from four major categories of physics. For each category, we select specific physical laws from textbooks (Harjono et al., 2020), which can be widely recognized and can be easily demonstrated through clear, observable physical phenomenon. 2) Prompt Engineering: For each physical law, we manually craft the initial T2V prompts to clearly depict the underlying physical phenomenon 3) Prompt Augmentation: To enhance the models video generation capabilities, we augment the initial T2V prompts by adding additional details, such as more precise descriptions of objects and actions (Yang et al., 2024). This augmentation process is carefully designed to avoid revealing the expected physical phenomenon. 4) Diversity Enhancement: Following T2V-CompBench (Sun et al., 2024), we employ GPT-4o to perform object substitution on the augmented prompts. This step increases the diversity of the benchmark. 5) Quality Control: We conduct thorough review of the prompts and their associated physical laws to ensure accuracy and relevance. Specifically, we ensure that the T2V prompts and corresponding physical laws are clear and accurate. We then randomly use the current T2V model to check if the prompts are simple enough for the model to generate semantically accurate videos. This methodology yields robust and comprehensive benchmark for assessing T2V models comprehension of physical commonsense, providing valuable tool for advancing research in this domain. For more detailed information about the dataset, please refer to the Appendix A"
        },
        {
            "title": "4 PHYGENEVAL",
            "content": "PhyGenEval aims to assess whether the physical phenomena in the generated videos conform to the corresponding physical laws. To obtain clear judgment, we decompose the evaluation into semantic alignment (SA) and physical commonsense alignment (PCA). While SA evaluates whether the semantic meaning inferred by the generated video and the input prompt are matched, PCA measures whether the evaluated physical laws are grounded in the videos. For example, for the scene an egg collides with stone, SA requires video containing the egg, the stone, and the collision action. PCA necessitates video for the whole physical motions in which the egg hits stone and then breaks, while the stone remains intact. Following (He et al., 2024b), we convert both SA and PCA to four-point scale, as well as the human ratings. 4.1 SEMANTIC ALIGNMENT EVALUATION Directly asking the Vision-Language Model(VLM) to align the semantic meaning between videos and input prompts are difficult, as prompts usually are mixed with semantic entities and physical phenomena, and the intermediate outcomes are subtly implied by the videos. For example, in prompt like timelapse captures the transformation of soup as the temperature rises above 100C, possible video generation would appear like The video shows soup, but there is no transformation of the soup. To address the challenge, we first employ GPT-4o to extract object and action from the original text prompt, we then utilize GPT-4o to sequentially determine the presence of extracted ob-"
        },
        {
            "title": "Technical report",
            "content": "Figure 3: An overview of the proposed PhyGenEval . PhyGenEval is divided into three parts: Key Physical Phenomena Detection, Physics Order Verification, and Overall Naturalness Evaluation. Each part uses an appropriate VLM in combination with physical-based customized questions generated by GPT-4o. The final score is the combined result of the three parts. For the example in the figure, the three-stage scores are 0, 1 (only q1 is correct), and 0. The final score is calculated as 0 according to 4.2. jects in the video and verify the occurrence of specified actions. This decomposition provides more fine-grained captures and prevents the model from confusing semantic and physical correctness during evaluation. Experimental results demonstrate that our automated evaluation method aligns more closely with human judgment and outperforms previous methods (He et al., 2024b; Sun et al., 2024) in PhyGenBench (Appendix B.1). 4.2 PHYSICAL COMMONSENSE EVALUATION To evaluate physical correctness in the video, we evaluated multiple common evaluation metrics comparing human assessments*. Experimental results in Table 1 demonstrate that these methods struggle to generalize to the assessment of physical commonsense correctness on PhyGenBench , e.g., VideoScore (He et al., 2024b) has only spearman correlation of 0.19 on PhyGenBench , which is most correlated with human assessments except PhyGenEval . We attribute it to the main factor: Directly using video-based VLMs fails to comprehend the embedded physical commonsense (Jassim et al., 2023), as current methods are not designed with physical commonsense as foundation. To fully understand the physical commonsense in the video, there are three key factors need to solve: i): Physical processes typically exhibit clear key phenomena depicted by the input prompt (e.g., the egg breaks upon hitting the rock.). It is necessary to identify these key physical phenomena ii): Physical processes are characterized by causality, maniand detect their presence in videos. fested in the correct sequence of critical events(e.g., The egg touchs the rock first, then breaks.). The correct sequence order validates the correctness of physical processes. iii): Physical processes need to possess overall naturalness, which represents the realistic of the overall process. To address these factors, we design progressive strategy that starts with key physical phenomena, then moves through the sequence of several key phenomena, and finally evaluates the overall naturalness of the entire video. This hierarchical and refined approach reduces the difficulty compared to existing methods that directly uses VLMs to evaluate physical commonsense, enabling PhyGenEval to achieve results closely aligned with human judgements. Key Physical Phenomena Detection. This stage aims to detect whether the key physical phenomena occur in the video. Here we define the key phenomena as an observable and distinctive *Annotators are asked to score the correctness of physical commonsense in the video. Details refer to Section 5 and Appendix C."
        },
        {
            "title": "Technical report",
            "content": "occurrence (e.g., specific frame) within physical process that can directly reveal the corresponding physical law, like deformations or color changes. For each input prompt in PhyGenBench , we craft retrieval prompt pr and set of physics-related questions Q, where the retrieval prompt is used to locate the key phenomena frame, and physical-related questions are utilized to check whether the expected physics phenomena are present in the keyframe. As illustrated in Figure 3 (a), we first obtained both and Pr by prompting GPT-4o with the input T2V prompt and corresponding physical law. Following (Hessel et al., 2021), keyframe Ii from the video based on the retrieval prompt, where Ii is the i-th frame in the video. By using the keyframe, we define confidence score of the key phenomena in the video:. (cid:88) (VLM(Ij, q) + VLM(Ij, pr)) , Skey = max i2ji+2 qQ where VLM(Ij, q) reflects the presence of physical phenomena in Ij for each related question from Q. VLM(Ij, pr) checks whether Ij matches the retrieval prompt, which ensures key phenomena occur at the correct frame. Since videos may contain semantic errors, its also important for determining if key physical phenomena occur (e.g., an egg shouldnt break in mid-air before hitting rock). We consider adjacent 5 frames near the keyframe to enhance the robustness. For example, the egg may not be cracked just when it first contacts the stone. We instantiate VLM-based evaluator VLM() with VQAScore (Lin et al., 2024), which has been shown promising evaluation results on visual question-answering. Physics Order Verification. In this stage, we verify whether key physical phenomena occur in the correct order. The correct physical sequence is an ordered series of events in physical process that reflects causality, which represents the necessary prerequisites and temporal order of key physical phenomena. As an example, the egg should first touch the stone and then crack. Considering current models in PhyGenBench generally maintain outcome consistency (Huang et al., 2024) (e.g., the egg would not reassemble itself after it is broken). we approach this direction by investigating the order correctness from the keyframes (Figure 3 (b)), e.g., the keyframe of the egg hits the stone should be ahead of the keyframe of the broken egg. Similar to the single image evaluation, we prompt GPT-4o to generate retrieval prompt pr and three physical-related questions (q1, q2, q3). pr is used to locate the keyframe (e.g., the moment the egg slightly touches the stone.). While q1, q2, and q3 are questions to check the order correctness from the first frame to the keyframe, from the keyframe to the last frame, and from the first frame to the last frame, respectively. Similarly, we first use CLIPScore to locate the key frame Ii, then the order correctness scores of Sbefore and Safter are defined as: Sbefore = max i2ji (VLM(I0, Ij, q1) + VLM(Ij, pr)) Safter = max iji+2 (VLM(Ij, I1, q2) + VLM(Ij, pr)) q3 assesses the overall physical sequence coherence of the video. The score of answering q3 is defined as by Sall = VLM(I0, Ii2:i+2, I1, q3), which evaluates the overall sequence (similar to the input video but using manually selected key frames). Here we employ GPT-4o or LLaVAInterleave (Li et al., 2024) as the VLM-based evaluator VLM(), as they demonstrate exceptional multi-image comprehension capabilities. The overall score of whole physical order evaluation can be formulated as Sorder = Sbefore + Safter + Sall Overall Naturalness Evaluation. This stage aims to evaluate the overall naturalness of the video. we define naturalness as the dynamic progression that aligns with real-world physical phenomenons (Liao et al., 2024). For each prompt in PhyGenBench , we obtain naturalness evaluation standard, denoted as gspec, which is used to assess the naturalness for video. As shown in Figure 3 (c), we first refer to DEVIL (Liao et al., 2024) to establish general evaluation standard: ggen, applicable to all T2V prompts. Besides, we use each input T2V prompt p, the corresponding physical law l, and general evaluation standard ggen to guide GPT-4o in generating detailed evaluation standard: gspec, for the given prompt. Finally, we require the VLM to score based on p, l, gspec, and the corresponding video denoted by I0:1. Formally, we define the overall naturalness score as: Snatural = VLM(I0:1, p, l, gspec) We implement the VLM-based evaluator VLM() using InternVideo2 (Wang et al., 2024) and GPT4o, both of which have demonstrated promising results in video understanding."
        },
        {
            "title": "Technical report",
            "content": "Table 1: PCA correlation results with proposed PhyGenEval in video generation. PhyGenEval is significantly closer to human feedback on PhyGenBench compared to other metrics. Metric DEVIL (Liao et al., 2024) VideoPhy (Bansal et al., 2024) VideoScore (He et al., 2024b) PhyGenEval Mechanics Optics Thermal Material Overall τ () ρ() τ () ρ() 0.03 0.16 0.15 0.03 0.00 0.03 0.15 0.14 0.08 0.18 0.77 0.72 0.20 0.75 0.07 0.76 τ () 0.10 0.08 0.14 0.73 ρ() 0.11 0.08 0.15 0.75 τ () 0.27 0.13 0.14 0.81 ρ() 0.29 0.14 0.15 0.84 τ () 0.17 0.03 0.17 0.78 ρ() 0.18 0.04 0.19 0.81 Overall Score. We first discretize Skey, Sorder, and Snatural from the three stages into four-point scale, then take their average and apply floor rounding as the final score. For robust purposes, we evaluate Sorder with both GPT4o and LLaVA-Interleave and Snatural with both GPT4o and InternVideo2. The final score is calculated as the ensemble of two methods. Detailed calculation protocols are provided in Appendix B."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "Experiments Setup. We evaluate 5 open-source models including OpenSora V1.2 (Zheng et al., 2024), Lavie (Wang et al., 2023), CogVideoX 2b (Yang et al., 2024), CogVideoX 5b (Yang et al., 2024), and Vchitect2.0 (Wang et al., 2023), as well as proprietary models Kling (kli, 2024), Pika (Pik, 2023), and Gen-3 (gen, 2024). We compare our proposed metric with existing metrics or benchmarks: Videophy (Bansal et al., 2024), VideoScore (He et al., 2024b) and DEVIL (Liao et al., 2024) More Detailed information is provided in Appendix C. For human evaluation, we compared the results across 8 T2V models. We randomly select 64 prompts from PhyGenBench and generate 64 videos for each T2V model. Therefore we need evaluation 512 videos. We ask three annotators to provide semantic and physical scores for each video. Each annotator will give an integer score of 0-3 for the semantic and physical scores, and the final score is the average of the three scores and rounded up. Finally, we calculate the correlation between the human scores and automatic evaluation scores using Kendalls τ and Spearmans ρ. we pue more detailed information about human evaluation in Appendix C.1. Human Evaluation. As shown in Table 1, current video generation evaluation metrics largely overlook physical correctness. In contrast, PhyGenEval implements detailed design for evaluating physical correctness, demonstrating strong correlations with human judgments across all categories. Its overall correlation coefficient reaches 0.81, indicating that PhyGenEval serves as an effective human-aligned physical commonsense correctness evaluator for PhyGenBench . We put more results in Appendix C.2 We conduct several case studies to illustrate the differences between various metrics more clearly. As shown in Figure 4, (a) and (f) reveal that VideoScore and DEVIL are prone to misclassifying videos that have smooth and consistent motion but violate fundamental physical laws. Specifically, as for (a), when an egg exhibits rubber-like elasticity upon impact with rock instead of breaking, these metrics incorrectly evaluate it as physically correct. VideoPhy exhibits similar limitations. In (c), it incorrectly assesses rock floating on water instead of sinking as physically correct. Furthermore, our analysis reveals major flaw in these three methodologies: they cannot incorporate domain-specific physical commonsense. As illustrated in (e), where the flame from burning copper appears red instead of green, these metrics fail to identify the mistake. This indicates their inability to incorporate domain-specific physical commonsense. In contrast, PhyGenEval demonstrates robust integration of physical commonsense and comprehensive video content analysis, resulting in more accurate and physically consistent evaluations in PhyGenBench . Quantitative Evaluation. We conduct extensive experiments on wide range of popular video generation models. As illustrated in Table 2, even the best-performing model, Gen-3, only attains PCA score of 0.51 on PhyGenBench . This indicates that even for prompts containing obvious Note that we ask the annotators to focus on the correctness of the physical phenomena for physical scores."
        },
        {
            "title": "Technical report",
            "content": "Figure 4: Different video generation evaluation metric in PhyGenBench . Except for the proposed PhyGenEval , the current methods cannot reasonably assess the correctness of physical commonsense in videos from PhyGenBench . Table 2: Evaluation results of PCA with the proposed PhyGenEval in videos generated by several models . The results reveal that all models score very low in physical commonsense accuracy, highlighting that current T2V models face significant challenges in correctly grasping physical commonsense. Model Size Mechanics() Optics() Thermal() Material() Average() Human() CogVideoX (Yang et al., 2024) CogVideoX (Yang et al., 2024) Open-Sora V1.2 (Zheng et al., 2024) Lavie (Wang et al., 2023) Vchitect 2.0 (Wang et al., 2023) 2B 5B 1.1B 860M 2B Pika (Pik, 2023) Gen-3 (gen, 2024) Kling (kli, 2024) - - - 0.38 0.39 0.43 0.30 0.41 0.35 0.45 0.45 0.43 0.55 0.50 0.44 0.56 0.56 0.57 0. 0.34 0.40 0.44 0.38 0.44 0.43 0.49 0.50 0.39 0.42 0.37 0.32 0.37 0.39 0.51 0.40 0.39 0.45 0.44 0.36 0.45 0.44 0.51 0. 0.31 0.37 0.35 0.30 0.36 0.36 0.48 0.44 physical commonsense, current T2V models struggle to generate videos that comply with intuitive physics. It indirectly reflects that these models are still far from achieving the world simulator. Furthermore, we identify the following key observations: 1): Across various categories of physical commonsense, all models consistently demonstrate superior performance in the domain of optics compared to other areas. Notably, Vchitect2.0 and CogVideoX-5b achieve PCA score in the optics domain comparable to that of closed-source models. We posit that this superior performance in the optics domain can be attributed to the abundant and explicit representation of optical knowledge in pre-training datasets, thereby enhancing the models comprehension in this area. 2): Kling and Gen3 exhibit significantly higher performance compared to other models. Specifically, Gen-3 demonstrates robust understanding of material properties, achieving score of 0.51, which substantially surpasses other models. Kling performs particularly well in thermal, attaining the highest score of 0.50 in this domain. 3): Among open-source models, Vchitect2.0 and CogVideoX 5b perform comparatively well, both exceeding the performance level of Pika. In contrast, Lavie consistently exhibits lower physical correctness across all categories. The different video cases for 4 physical commonsense categories are Qualitative Evaluation. illustrated in Figure 5. Our main observations are as follows: In mechanics, the models struggle to generate simple physically accurate phenomenons. As shown in Figure 5, all models fail to depict the glass ball sinking in water. As for (b), instead showing it floating on the surface, OpenSora and Gen-3 even produce videos where the ball is suspended. Additionally, the models do not capture special physical phenomenonss, such as the state of water in zero gravity, as seen in (a). In optics, the models perform relatively better. (c) and (d) show the models handling reflections of balloons in water and colorful bubbles, though OpenSora and CogVideoX still produce reflections with noticeable distortions in (d). In thermal, the models fail to generate accurate videos of phase transitions. For the"
        },
        {
            "title": "Technical report",
            "content": "Figure 5: Qualitative comparisons of four categories. Current models perform relatively well in generating optical phenomenons but are weaker in mechanics, thermal, and material properties. melting phenomenon in (e), most models show incorrect results, with CogVideoX even producing video where the ice cream increases in size. Similar errors appear in the sublimation process in (f), with only Gen-3 showing partial understanding. Regarding material properties, (g) shows all models failing to recognize that an egg should break when hitting rock, with Kling displaying the egg bouncing like rubber ball. For simple chemical reactions, such as the black bread experiment in (h), none of the models demonstrate an accurate understanding of the expected reaction. Ablation Study. We conduct detailed robustness analysis of the design elements in PhyGenEval, including the role of each level in the three-tier evaluation framework and the impact of the two-stage strategy proposed in overall naturalness evaluation. Experimental results show that the key designs of PhyGenEval are essential. Detailed results are provided in Appendix C.3."
        },
        {
            "title": "6 DISCUSSION",
            "content": "To explore potential solutions for the challenges posed by PhyGenBench , We focus on widely used and proven-effective methods such as scaling laws (Kaplan et al., 2020), prompt engineering (Fu et al., 2024), and some method like Venhancer (He et al., 2024a) aimed to improve general video quality (Huang et al., 2024). And we determine whether they can resolve the inability of current T2V models to generate videos aligned with physical commonsense. Through quantitative and qualitative analysis, we find: 1) Scaling up models can solve some issues but still fails to handle dynamic physical phenomenons, which we believe requires extensive training on synthetic data. 2) Prompt engineering like (Fu et al., 2024) only solves few simple issues (e.g., flame color), highlighting the difficulty and significance of PhyGenBench . 3) While some methods improve general video quality, they do not enhance the models understanding of physical commonsense. More detailed results are provided in Appendix D."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we explore the gap between current T2V models understanding of physical commonsense and their role as world simulators. To achieve this, we introduce PhyGenBench and PhyGenEval . PhyGenBench is benchmark specifically designed to assess models understanding"
        },
        {
            "title": "Technical report",
            "content": "of physical commonsense, featuring various physical laws and simple, clear physical phenomenons. Alongside PhyGenBench , we propose novel three-tier hierarchical evaluation framework called PhyGenEval to automate the evaluation process. Experimental and analytical results show that current T2V models struggle to generate videos that align with physical commonsense, highlighting significant gap from world simulation. Moreover, simply scaling up models or applying prompt engineering fails to address issues in PhyGenBench , such as those involving dynamics."
        },
        {
            "title": "REFERENCES",
            "content": "Pika, 2023. URL https://www.pika.art/. Gen-3, 2024. URL https://runwayml.com/blog/introducing-gen-3-alpha/. Kling, 2024. URL https://kling.kuaishou.com/. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. Peter Battaglia, Jessica Hamrick, and Joshua Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, 2013. Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, MingYu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems, 35:3176931781, 2022. Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i chalarXiv preprint lenge: Can text-to-image generation models understand commonsense? arXiv:2406.07546, 2024. Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024. David Halliday, Robert Resnick, and Jearl Walker. Fundamentals of physics. John Wiley & Sons, 2013. Ahmad Harjono, Gunawan Gunawan, Rabiatul Adawiyah, and Lovy Herayanti. An interactive eInternational Journal of Emerging book for physics to improve students conceptual mastery. Technologies in Learning (iJET), 15(5):4049, 2020. Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024a. Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Mantisscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024b. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024."
        },
        {
            "title": "Technical report",
            "content": "Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, and Elia Bruni. Grasp: novel benchmark for evaluating language grounding and situated physics understanding in multimodal language models. arXiv preprint arXiv:2311.09048, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. Mingxiang Liao, Hannan Lu, Xinyu Zhang, Fang Wan, Tianyu Wang, Yuzhong Zhao, Wangmeng Zuo, Qixiang Ye, and Jingdong Wang. Evaluation of text-to-video generation models: dynamics perspective. arXiv preprint arXiv:2407.01094, 2024. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint arXiv:2404.01291, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physicsgrounded image-to-video generation. In European Conference on Computer Vision ECCV, 2024b. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2213922149, 2024c. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and Sai Rajeswar. Multimodal foundation world models for generalist embodied agents. arXiv preprint arXiv:2406.18043, 2024. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2vcompbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. Norman Swartz. The concept of physical law. Cambridge University Press, 1985. Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation. arXiv preprint arXiv:2408.02629, 2024. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018."
        },
        {
            "title": "Technical report",
            "content": "Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. Justin Wood, Tomer Ullman, Brian Wood, Elizabeth Spelke, and Samantha MW Wood. arXiv preprint Object permanence in newborn chicks is robust against opposing evidence. arXiv:2402.14641, 2024. Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora. Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024."
        },
        {
            "title": "A PHYGENBENCH DETAILS",
            "content": "A.1 DETAILED OVERVIEW fine-grained analysis of the dataset is essential for comprehensive understanding of the benchmark. As shown in Table 3, PhyGenBench covers 4 major domains in physics, encompassing 27 representative physical laws, which enables it to provide more comprehensive and fine-grained evaluation of models physical capabilities. We generated 1280 videos by evaluating 8 advanced models. Additionally, our captions encompass totally 165 unique objects and 42 unique actions with an average length of 18.75 words. A.2 DIFFERENCE BETWEEN VIDEOPHY AND OURS VIDEOPHY Bansal et al. (2024) comprises 688 curated simple prompts that focus on interactions between three types of physical materials: solid-solid, solid-fluid, and fluid-fluid, but lack annotations of physical laws. The dataset is designed to evaluate models understanding of physical commonsense, featuring limited range of physical phenomenons such as rigid body interactions, fluid dynamics, and contact forces. We are better suited than Videophy for evaluating physical commonsense due to two significant differences. Table 3: Details of PhyGenBench Statistic Number"
        },
        {
            "title": "Physical Laws\nDomains\nOptics\nMechanics\nThermal\nMaterial Properties",
            "content": "Total Captions Total T2V Models Total Generated Videos Unique Objects Unique Actions Average Length of Caption 27 4 50 40 30 40 160 8 1280 165 42 18.75 First As shown in Figure 2, PhyGenBench includes 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which comprehensively assess models understanding of physical commonsense. While Videophy primarily focuses on interactions between solid-fluid, solid-solid, and fluid-fluid, limiting its coverage and overlooking common physical laws such as phase transitions and basic material properties. What more, Videophy lacks annotations of physical laws making it hard for VLM model to evaluate. Second, as shown in Table 4, the average SA score of PhyGenBench (0.80) significantly outperforms that of Videophy (0.63). This indicates that PhyGenBench prompts are well-suited and easy for T2V models to generate high-quality, wellaligned videos, which benefits evaluation of physical correctness. In contrast, as shown in Figure 6, We find that prompts from Videophy pose challenges for T2V models in generating text-aligned and high-quality videos for two main reasons: 1. The prompts lack detail and specificity. For instance,A tissue blots tear from an eye is overly simplistic (without augmentation). Modern T2V models, such as CogVideo5B Yang et al. (2024), are typically trained with longer and more descriptive captions, which enhance their ability to comprehend and generate content based on prompts. 2. The scenes are often complex and unrealistic. For example, The wristwatch knob winds the inner spring tightly describes process involving intricate internal mechanisms that are not visible externally. As result, it is exceedingly difficult for T2V models to generate such scenes accurately. Table 4: Comparison of SA results for video generation between Videophy and PhyGenBench . We randomly select 64 prompts from both Videophy and PhyGenBench , use different T2V models to generate videos, and then ask annotators to score based on our cretiera in Figure 9. The results show that PhyGenBench SA scores significantly outperform Videophy. Model Size Videophy() PhyGenBench () CogVideoX (Yang et al., 2024) Vchitect 2.0 Kling Average 0.48 0.63 0.77 0.63 0.78 0.84 0.89 0.80 5B 2B - -"
        },
        {
            "title": "Technical report",
            "content": "Figure 6: Samples of videos generated by Kling, Vchitect, and Cogvideo5b in Videophy. All T2V models struggle to achieve proper text alignment and produce high-quality videos, making it meaningless to evaluate physical correctness in Videophy."
        },
        {
            "title": "B PHYGENEVAL DETAILS",
            "content": "B.1 SEMANTIC ALIGNMENT DETAILS To reduce the complexity for VLM models to evaluate sementic correctness of generated videos between prompts, we adopt two-stage strategy. Initially, we employ GPT-4o to extract objects and actions from the original text prompt. Subsequently, we employ GPT-4o to determine whether the extracted objects are present in the video and to verify the occurrence of specified actions. For each video, GPT-4o first assesses the presence of the objects mentioned in the prompt (e.g., egg) within the video frames. This evaluation is performed according to Question 1 (Q1), where GPT-4o assigns score from 0 to 2 based on the completeness of object presence: score of 2 is given if all the objects are present, 1 if some of the objects are missing, and 0 if none of the objects appear in the video. After determining object presence, GPT-4o moves on to Question 2 (Q2) to check if the specified action (e.g., pour out) is performed in the video. It assigns binary score (0 or 1) depending on whether the action is present (1) or absent (0). Finally, these scores are combined to form the overall semantic alignment score. we put more details about other metric baselines in Appendix C.1. B.2 PHYSICAL COMMONSENSE ALIGNMENT DETAILS In this section, we use the same notation as in Section 4.2 and provide more detailed description of the calculation and design of the method. Key Phenomena Detection. We categorize the T2V prompts into monotonic processes (eg. melting with increasing temperature) and non-monotonic processes (eg. an egg hitting rock) based on the physical phenomena they represent. For prompt with monotonic processes, we only consider using the Last Frame as the retrieval prompt, resulting in single question. We can directly calculate VLM(Imgj, Q), where the score for the corresponding video of this prompt ranges from 0 to 1. For prompt with non-monotonic processes, we consider both the intermediate key frames and the Last Frame, resulting in two questions. For the intermediate key frames, we calculate VLM(Imgj, Q) + VLM(Imgj, Pr), which ranges from 0-2. Consequently, the score range for videos corresponding to this prompt is 0 to 3. For specific calculatation, we need to calculate VLM(Ij, pr) and VLM(Ij, q), where Imgj is the j-th frame in the video. For VLM(Ij, pr), the calculation involves assessing the matching degree between the key frame and the retrieval prompt, which can be directly obtained using the original calculation method in (Lin et al., 2024). For VLM(Ij, q), we follow the computation approach from ChronoMagicBench (Yuan et al., 2024), we derive VLM(Ij, q) by determining the ratio of the VQAScore for the affirmative statement to the combined VQAScores for both the affirmative"
        },
        {
            "title": "Technical report",
            "content": "and negative statements. We perform the calculations of VLM(Ij, pr) and VLM(Ij, q) for each key frame within the specified range to obtain the physical correctness score for the problem. Key Sequence Verification. For this stage, which weve primarily introduced in Section 4, we focus on key calculation points. The score calculation formula for q1 is Sbefore = maxi2ji (VLM(I0, Ij, q1) + VLM(Ij, pr)). Here, VLM(Ij, pr) determines if the retrieved key frame satisfies the retrieval prompt,as the physical phenomenon should occur in the keyframe primarily located in Key Phenomena Detection, which is crucial for Key Sequence Verification (e.g.the expected physical phenomenon of egg cracking should occur in the keyframe when the egg hits the stone, rather than other frames when the egg is in the air or else). VLM(I0, Ij, q1) assesses the correctness of the Key Sequence order in the video. Notably, we calculate VLM(Ij, pr) using VQAScore, yielding decimal between 0 and 1, while VLM(I0, Ij, q1) employs VLM (GPT-4V or LLaVA-Interleave) for question-answering, scoring 1 or 0 based on the models Yes or No response. Overall Naturalness Evaluation. Here we mainly explain how to get the score of this part based on the evaluation results under the two-stage strategy described in Section 4. Specifically, we ask the video-based VLM to select the most appropriate option for the video according to the detailed scoring criteria generated by the LLM, and then we map the options to scores (Completely Fantastical to Almost Realistic corresponds to 0-3 points) Overall Score. We detail the discretization and calculation process of the scores here. In the stage of key phenomena detection, we categorize the prompts into monotonic and non-monotonic processes based on the physical phenomena they represent. For monotonic processes, the score range is 0-1, which we directly discretize by averaging into integer values from 0-3. Specifically, for non-monotonic processes with score range of 0-3, we discretize the scores to [1, 1.5, 2.25]. This is because no points should be awarded if the physical phenomena are incorrect (VLM(Ij, pr) = 1 and VLM(Ij, q) = 0), even with accurate retrieval. (e.g., The egg hits the stone and does not break) In the stage of key sequence verification, we have three multi-image problems. One point is awarded for each correct answer, resulting in final integer score from 0-3. Similar to the stage, of key phenomena detection we need to consider both the accuracy of key frame retrieval and the physical question answering. Therefore, we design the following: for Q1, when maxi2ji (VLM(I0, Ij, q1) + VLM(Ij, pr)) and VLM(Ij, pr) > 0.5, the question is considered correct. The process for q2 is similar. For q3, it is marked correct when VLM(I0, Ii2:i+2, I1, q3). In the stage of overall naturalness evaluation, as we require video-based direct option selection, choosing Completely Fantastical, Clearly Unrealistic, Slightly Unrealistic, and Almost Realistic is scored as 0, 1, 2, and 3 points respectively. Finally, we average all scores and round down to obtain the final score."
        },
        {
            "title": "C EXPERIMENT",
            "content": "C.1 EXPERIMENTS SETUP T2V model Implementation details. Open-Sora 1.2 (Zheng et al., 2024) is an open-source project with the goal of reproducing Sora. CogVideoX 2b Yang et al. (2024) and CogVideoX 5b are large-scale diffusion transformer models for text-to-video generation, incorporating 3D Variational Autoencoder (VAE) for efficient video compression and an expert transformer with Expert Adaptive LayerNorm to improve text-video alignment. LaVie Wang et al. (2023) is cascaded video latent diffusion model. Vchitect2.0 Wang et al. (2023), developed by the Shanghai AI Lab, is an advanced video generation model featuring Parallel Transformer architecture to scale up video diffusion models and empower video creation. Evaluation Metrics details. We compare our proposed PhyGenEval with some evaluation metrics from previous methods like VideoPhy (Bansal et al., 2024) and VideoScore (He et al., 2024b). VideoPhy fine-tunes VLM with the VIDEOPHY dataset proposed by themselves, which includes human feed back about the semantic alignment and dynamic motion correctness about videos. VideoScore is trained on the VIDEOFEEDBACK dataset proposed by themselves, Initialized from the Mantis model. VideoScore provides automatic assessments of video quality based on human"
        },
        {
            "title": "Technical report",
            "content": "Table 5: Details about evaluation models. The table shows duration, FPS, and resolution for each model."
        },
        {
            "title": "Model",
            "content": "Duration (s) FPS Resolution Open-Sora 1.2 (Zheng et al., 2024) CogVideoX 2b CogVideoX 5b Lavie Vchitect2.0 Pika (Pik, 2023) Gen-3 (gen, 2024) Kling (kli, 2024) 4 6 6 4 5 3 11 5 24 8 8 8 24 24 30 1280 720 720 480 640 360 512 320 768 432 1280 720 1280 768 1280 720 Table 6: SA correlation results with proposed PhyGenEval in video generation. higher score indicates better performance for category. Bold stands for the best score, Metric VideoPhy (Bansal et al., 2024) VideoScore (He et al., 2024b) Grid-LLaVA (Sun et al., 2024) PhyGenEval (Grid-LLaVA) PhyGenEval Mechanics Optics Thermal Material Overall τ () 0.20 0.14 0.39 0.35 0.48 ρ() τ () ρ() 0.25 0.03 0.03 0.16 0.13 0.14 0.49 0.45 0.43 0.48 0.46 0.38 0.67 0.64 0.52 τ () 0.20 0.23 0.30 0.41 0.46 ρ() 0.24 0.02 0.33 0.44 0.49 τ () 0.18 0.02 0.22 0.42 0.47 ρ() 0.22 0.02 0.26 0.45 0.50 τ () 0.13 0.05 0.35 0.42 0.53 ρ() 0.17 0.05 0.39 0.44 0.56 scoring criteria. To compare with PhyGenEval on SA and PCA, We only choose the text alignment and fact consistency criteria. Specifically, for the semantic alignment evaluation, we compare the Grid-LLaVA method proposed by T2V-CompBench, which extends the LLaVA (Liu et al., 2024a) model to handle multi-frame inputs by sampling 6 frames uniformly from video to create an image grid. For the physical commonsense alignment evaluation, we also compare with DEVIL (Liao et al., 2024), which uses Gemini 1.5 Pro (Reid et al., 2024) to assess the overall naturalness of videos and applies the same scoring standard prompt to all videos. Furthermore, to evaluate the effectiveness of our PhyGenEval designs, we conduct large amount of ablation studies and pue more details in Appendix C.3. Human evaluation details. Here, we provide detailed explanation of the human evaluation described in Section 5. Specifically, we require annotators to score based on the standards outlined in Figure 9, covering both semantic alignment and physical commonsense alignment. For example, as for the video shown in Figure 9, The egg bounces off the rock like rubber ball, completely violating physical laws like dynamics, the annotator gives score of 0 for physical commonsense alignment. However, since the video fully includes the egg, the rock, and the collision action, the annotator gives score of 3 for semantic alignment. C.2 QUANTITATIVE EVALUATION Comparison result about semantic alignment. Here we design new baseline PhyGenEval (Grid-LLaVA) to illustrate the superiority of the method, which uses the two-stage strategy proposed in PhyGenEval from Appendix B.1, but replaces the VLM with Grid-LLaVA proposed in T2V-CompBench (Sun et al., 2024). As shown in Table 6, PhyGenEval achieves the highest correlation scores across all categories, demonstrating its effectiveness as human-aligned semantic commonsense correctness evaluator for PhyGenBench . Compared to other methods, PhyGenEval consistently outperforms previous baselines like VideoPhy, VideoScore, and Grid-LLaVA. Specifically, PhyGenEval obtains an overall Kendalls τ of 0.53 and Spearmans ρ of 0.56, surpassing the Grid-LLaVA (τ : 0.35, ρ: 0.39). The results clearly show that our PhyGenEval design provides more accurate and reliable semantic commonsense evaluation in PhyGenBench ."
        },
        {
            "title": "Technical report",
            "content": "Table 7: SA evaluation results with proposed PhyGenEval in video generation. Both machine and human evaluations indicate that most models achieve good semantic scores on PhyGenBench . This suggests that the scenarios in PhyGenBench are simple enough to clearly reflect physical phenomena. Model Size Mechanics() Optics() Thermal() Material() Average() Human() CogVideoX (Yang et al., 2024) CogVideoX (Yang et al., 2024) Open-Sora V1.2 (Zheng et al., 2024) Lavie (Wang et al., 2023) Vchitect 2.0 (Wang et al., 2023) 2B 5B 1.1B 860M 2B Pika (Pik, 2023) Gen-3 (gen, 2024) Kling (kli, 2024) - - - 0.63 0.78 0.73 0.47 0.92 0.63 0.84 0.88 0.67 0.88 0.85 0.63 0.89 0.81 0.93 0.91 0.61 0.78 0.82 0.73 0. 0.73 0.82 0.87 0.63 0.64 0.73 0.53 0.74 0.69 0.78 0.74 0.64 0.78 0.79 0.58 0.84 0.72 0.85 0.85 0.64 0.78 0.70 0.55 0. 0.65 0.86 0.89 Quantitative result about semantic alignment. As shown in Table 7 , nearly all models achieve relatively high SA scores, whether evaluated by machines or humans. This suggests that the scenarios in PhyGenBench are relatively straightforward, making it easier to assess physical commonsense. Among all the models, Kling achieved the highest SA score, with human evaluation score of 0.89, reflecting its strong instruction understanding and video generation capabilities. C.3 ABLATION STUDY The Component in PhyGenEval on physical commonsense alignment evaluation. We conduct series of ablation studies to demonstrate the necessity of our method design by examining its correlation with human evaluation results, similar to those described in Section 5. Specifically, we compare: 1) The effectiveness of two-stage evaluation method proposed in Section 4.2 2) The effect of the various stages of PhyGenEval , as proposed in Section 4.2; 3) Performance differences when using various VLMs and their ensembles in PhyGenEval , as outlined in Section 4.2. Notice that PhyGenEval for physical commnonsense alignment evaluation consists of three stages: key phenomena Detection, key sequence verification, and overall naturalness evaluation. And We denote them as PhyGenEval -S, PhyGenEval -M, and PhyGenEval -V based on the VLM they used. 1) We demonstrate that employing two-stage strategy, as outlined in Section 4.2, yields superior results when assessing the physical commonsense correctness of the entire video compared to onestage strategy. Specifically, the one-stage strategy refers to not using LLM to rewrite the scoring template, but instead applying single scoring template for all prompts corresponding videos, allowing the VLM to score them. This method is proposed in DEVIL (Liao et al., 2024). To verify the superiority of the two-stage strategy, we use InternVideo2 and GPT-4o as VLMs and perform both the one-stage and two-stage strategies. We label these as PhyGenEval -V(Intern) and PhyGenEval - V(GPT-4o), respectively. As shown in Table 8, the evaluation results produced by the two-stage strategy are more consistent with human judgments for both InternVideo2 and GPT-4o. We attribute this improvement to the incorporation of LLM (GPT-4o) for better comprehension of physical commonsense text, which effectively reduces the complexity of the task for VLMs in evaluating the physical correctness of videos. 2) PhyGenEval for physical commnonsense alignment evaluation consists of three stages. We investigate the contribution of each stage to the final performance. Table 9 presents results using one or two stages (employing ensemble strategies when multiple VLMs are applicable). We find that optimal performance is achieved only when all three stages are used concurrently, demonstrating the rationale behind PhyGenEval design. 3) Given potential biases in single models and the costs associated with closed-source models, we offer two PhyGenEval computation methods: using GPT-4o or alternative open-source models (LLaVA-Interleave (Li et al., 2024) and InternVideo2 (Wang et al., 2024)). Table 10 shows that even using only open-source models achieves high correlation coefficient of 0.66. Notably, ensembling both methods yields the best results. Considering PhyGenBench relatively small size, we find this computational cost acceptable. Therefore we recommend users ensemble these methods. The Component in PhyGenEval on semantic alignment evaluation. we also perform necessary ablation experiments to validate the necessity of our SA evaluation design. Specifically, we"
        },
        {
            "title": "Technical report",
            "content": "Table 8: Comparison of PCA correlation results of the two-stage strategy for the video stage in PhyGenEval Metric Mechanics Optics Thermal Material Overall τ () ρ() τ () ρ() τ () ρ() τ () ρ() τ () ρ() One Stage Strategy PhyGenEval -V(Intern) 0.03 0.04 0.20 0.21 0.26 0.27 0.20 0.41 PhyGenEval -V(GPT) 0.39 0.12 0.11 0.19 0.06 0. 0.06 0.10 0.11 0.21 0.19 0.39 PhyGenEval -V(Intern) PhyGenEval -V(GPT) 0.01 0.47 0.01 0.51 0.06 0.50 0.06 0. 0.08 0.46 0.08 0.49 0.10 0.53 0.11 0.58 0.07 0.53 0.08 0. Two Stage Strategy Table 9: Comparison of PCA correlation results using each stage in PhyGenEval"
        },
        {
            "title": "Overall",
            "content": "τ ()"
        },
        {
            "title": "0.50\nPhyGenEval -S\n0.46\nPhyGenEval -M\n0.26\nPhyGenEval -V\nPhyGenEval -SM 0.58\n0.56\nPhyGenEval -SV\nPhyGenEval -MV 0.50\n0.72\nPhyGenEval",
            "content": "ρ() 0.54 0.49 0.30 0.61 0.59 0.53 0.75 τ () 0.43 0.49 0.44 0.47 0.41 0.50 0.76 ρ() 0.45 0.53 0.47 0.50 0.43 0.53 0. τ () 0.50 0.55 0.33 0.58 0.58 0.53 0.73 ρ() 0.54 0.59 0.35 0.62 0.60 0.57 0.75 τ () 0.72 0.53 0.48 0.66 0.70 0.60 0. ρ() 0.77 0.57 0.52 0.70 0.74 0.64 0.84 τ () 0.56 0.55 0.42 0.60 0.59 0.57 0.78 ρ() 0.61 0.60 0.46 0.64 0.62 0.61 0. compare: 1) VLM Model Selection: We leverage GPT-4o (Achiam et al., 2023) as more robust VLM model for SA evaluation. 2) Effectiveness of our two-stage evaluation method proposed in Appendix B.1 1) As shown in Table 6, using GPT-4o in PhyGenEval is much better than using LLaVA, which achieve higher Kendalls τ of 0.53 compared to 0.42, and higher Spearmans ρ of 0.56 versus 0.44. This indicates stronger alignment between GPT-4os evaluations and human annotations compared to open-source vlm models like Grid-LLaVA (Sun et al., 2024), justifying its selection as the preferred VLM model in the SA evaluation design. Since PhyGenBench includes limited number of prompts, we believe that the cost of using GPT-4o is acceptable relative to the improvement in performance. 2) To validate the effectiveness of the two-stage strategy, we compare it with the method in T2VCompBench (Sun et al., 2024), which directly uses Grid-LLaVA to apply the same scoring standard prompt for semantic alignment evaluation across all videos. For fairness, we also use Grid-LLaVA but implement the two-stage strategy proposed in Appendix B.1. As shown in Table 6, PhyGenEval - Grid-LLaVA outperforms Grid-LLaVA, achieving higher Kendalls τ score of 0.42 compared to 0.35, and higher Spearmans ρ score of 0.44 versus 0.39. This result demonstrates the effectiveness of our Two Stage Evaluation Method. By decomposing the evaluation into object detection and action detection, we effectively reduces the complexity of the task for VLMs in evaluating the sementic correctness of videos. Table 10: Comparison of PCA correlation results using different models such as GPT-4o or opensourced models in PhyGenEval Metric PhyGenEval (Open) PhyGenEval (GPT4o) PhyGenEval Mechanics Optics Thermal Material Overall τ () 0.54 0.59 0.72 ρ() 0.57 0.63 0.75 τ () 0.59 0.53 0.76 ρ() 0.62 0.57 0.77 τ () 0.55 0.64 0.73 ρ() 0.58 0.68 0.75 τ () 0.65 0.73 0.81 ρ() 0.69 0.77 0.84 τ () 0.62 0.66 0.78 ρ() 0.66 0.71 0."
        },
        {
            "title": "D DISCUSSION",
            "content": "The Impact of Scaling on Physical Commonsense in Video Generation. Scaling laws have been extensively validated in video generation models (Kaplan et al., 2020). We investigate their efficacy in addressing the challenges of physical commonsense presented in PhyGenBench . As shown in Table 2, CogVideo 5B demonstrates improvements over CogVideo 2B, albeit with limited progress in the Mechanics category. Our qualitative analysis, illustrated in Figure 7, reveals significant advancements in static scenes with CogVideo 5B. It accurately captures complex phenomena such as colorful bubbles resulting from interference and diffraction, and oxidation-induced rusting of iron. In thermal, despite imperfections, CogVideo 5B generates more realistic boiling simulations compared to its predecessor. However, both models struggle with simple motion dynamics, exemplified by their inability to accurately depict bouncing football. We posit that while scaling up enhances the models capacity to generate videos that align with physical commonsense for individual objects, it may be insufficient for physical phenomenons involving dynamic physical laws. Addressing these challenges likely requires extensive training on carefully curated synthetic data, as suggested by (Liu et al., 2024b). This approach could potentially bridge the gap in the models grasp of fundamental physical laws. Figure 7: The qualitative comparison of CogVideoX 2B and CogVideoX 5B. The result shows that simply scaling up can solve some issues, but dynamic physical phenomenons involving the design of motion patterns remain challenging. Rewriting prompt. We aim to explore whether GPT-augmented prompts can address the PhyGenBench challenges. Specifically, we rewrite the original prompts using GPT, adding expected physical outcomes and processes. For example, after bottle of juice is slowly poured out in the space station, releasing the liquid into the surrounding area, we add The liquid forms floating globules, spreading out and moving randomly through the air. in the end. As shown in Table 11, we use CogVideoX 5b and Kling as representative models for open-source and closed-source systems, respectively, to conduct tests. The results indicate that prompt rewriting does help the models generate images aligned with physical laws, but it is still far from resolving the issues highlighted by PhyGenBench . Both CogVideoX 5b and Kling exhibit some growth, but even for Kling, it only achieves score of 0.56. This demonstrates that current models still severely lack the ability to accurately render physical scenes, and this deficiency cannot be easily resolved through simple prompt rewriting. To illustrate this issue more clearly, as shown in Figure 8, our qualitative analysis shows that rewriting prompts can only address simple issues (e.g., flame color"
        },
        {
            "title": "Technical report",
            "content": "Table 11: Evaluation results of PCA using the proposed PhyGenEval after rewriting prompts . The results indicate that although using rewritten prompts leads to some improvement, it is still insufficient to address the challenges highlighted by PhyGenBench . Model Size Mechanics() Optics() Thermal() Material() Average() CogVideoX (Yang et al., 2024) Kling CogVideoX (Yang et al., 2024) Kling Before Rewriting Prompt 0.39 0.45 0.55 0.58 After Rewriting Prompt 0.39 0.50 0.62 0.64 0.40 0. 0.53 0.61 5B - 5B - 0.42 0.40 0.52 0.48 0.45 0. 0.52 0.56 reactions), but remains ineffective for more complex physical processes (e.g., egg breaking, stone sinking). Figure 8: The qualitative comparison of effects before and after using rewritten prompts. The results indicate that rewriting prompts addresses only few basic issues (such as flame color reactions), while the majority of problems remain unsolved. The robustness of PhyGenBench and PhyGenEval . VEnhancer (He et al., 2024a) is generative space-time enhancement framework that improves existing videos by adding spatial details and synthetic motion in the temporal domain. After enhancement by VEnhancer, Vchitect2.0 shows significant improvement on VBench, even surpassing Kling. However, VEnhancer only enhances the visual quality of videos (e.g., making them more coherent and clear) without addressing the models poor understanding of physical commonsense."
        },
        {
            "title": "Technical report",
            "content": "Table 12: PCA evaluation results with proposed PhyGenEval in videos after VEnhancer. The results indicate that employing VEnhancer fails to enhance the models comprehension of physical commonsense. Model Size Mechanics() Optics() Thermal() Material() Average() Vchitect 2.0 Vchitect 2.0 (Venhancer) 2B 2B 0.41 0. 0.56 0.56 0.44 0.42 0.37 0.38 0.45 0.45 As shown in Table 12, Vchitect enhanced by VEnhancer still scores similarly to the original version on PhyGenBench . We calculate high Spearman coefficient of 0.86 between model scores on PhyGenBench before and after VEnhancer enhancement. This indicates that PhyGenEval primarily focuses on physical correctness and is robust to other factors affecting visual quality. Furthermore, it demonstrates that even if model can generate videos with better general quality (e.g., ranking higher on VBench), it doesnt necessarily imply better understanding of physical common sense. This highlights the distinction between PhyGenBench and benchmarks like VBench that evaluate video quality."
        },
        {
            "title": "Technical report",
            "content": "Figure 9: Detailed diagram of the human evaluation process. We ask the annotators to score the semantic alignment and physical commonsense alignment of the video according to the scoring criteria in the figure."
        }
    ],
    "affiliations": [
        "OpenGVLab, Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}