{
    "paper_title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL",
    "authors": [
        "Shenao Zhang",
        "Donghan Yu",
        "Yihao Feng",
        "Bowen Jin",
        "Zhaoran Wang",
        "John Peebles",
        "Zirui Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 0 1 8 5 2 . 9 0 5 2 : r Learning to Reason as Action Abstractions with Scalable Mid-Training RL Shenao Zhang1,2,, Donghan Yu1, Yihao Feng1, Bowen Jin1,3,, Zhaoran Wang2, John Peebles,, Zirui Wang1, 1Apple, 2Northwestern University, 3UIUC, Work done at Apple, Equal advising Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires mid-training stage. An effective mid-training phase should identify compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), scalable mid-training algorithm. Specifically, we derive sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces. Correspondence: Shenao Zhang: shenao@u.northwestern.edu; John Peebles: john@johnpeebles.com; Zirui Wang: ziruiw@apple.com Date: October 1,"
        },
        {
            "title": "Introduction",
            "content": "The potential of reinforcement learning (RL) as universal policy-improvement operator has been demonstrated with remarkable success in training large language models (LLMs), spanning applications in preference optimization (Ouyang et al., 2022), mathematics (Guo et al., 2025; Zeng et al., 2025b), code generation (Yang et al., 2025; Zeng et al., 2025a), and agentic tasks (Team et al., 2025; Zhou et al., 2025b). central factor behind these successes is the strengthened policy prior, often established through mid-training (Wang et al., 2025c; Su et al., 2025), which is continued pre-training on expert data sampled from the optimal policy. Despite its widespread use, the precise role of mid-training in shaping post-training RL remains poorly understood. This gap hinders the design of principled and effective mid-training algorithms. Existing heuristic metrics, such as the performance or entropy of the initial RL policy, offer only indirect signals and provide no guarantee of improved downstream outcomes. An ideal mid-training algorithm should extract from finite expert demonstrations set of useful actions sufficient for all tasks, and enable efficient selection among them during RL. We formalize this by presenting the first theoretical analysis of how mid-training shapes post-training RL. Specifically, mid-training should identify an action subspace that simultaneously minimizes (1) the approximation error induced by pruning the action space, and (2) the post-training RL error incurred when planning within the pruned space. Corresponding to these two objectives, we disclose two key factors that determine the effectiveness of mid-training: the efficiency of pruning the decision space and its impact on the convergence of subsequent RL. The first factor governs the learned prior encoded in the initial RL policy, while the second dictates the policys potential 1 to be improved through online interactions. Our results show that pruning efficiency is inversely related to the cardinality of the smallest near-optimal action subset, and that post-training RL converges faster when actions are temporally extended. Together, these findings suggest that mid-training should operate in the space of action abstractions rather than primitive actions. Intuitively, learning high-level, transferable skills benefits from compact set of decisions and reduced effective horizon, making pruning more efficient and planning more tractable. To uncover this action hierarchy, we derive temporal variational lower bound for the next-token prediction (NTP) objective. This bound can be optimized via an iterative expectationmaximization procedure, involving self-supervised RL step that treats the log-likelihood of expert actions as reward to discover the hidden latent sequence, and supervised fine-tuning step on the bootstrapped data. With an appropriate latent prior, the KL divergence enforces temporal consistency, ensuring that the latents function as coherent action abstractions. These design choices yield scalable mid-training algorithm, Reasoning as Action Abstractions (RA3), where the KL penalty naturally regulates the need for rational rollouts, thereby controlling computational cost. We evaluate RA3 on code generation tasks using Qwen and Llama base models ranging in size from 1B to 8B. The mid-training corpus consists of 3.5M code snippets totaling 1B tokens. Our experiments show that fine-tuning on data bootstrapped with action abstractions substantially reduces cross-entropy loss and improves performance across multiple benchmarks, including HumanEval, MBPP, and their extended variants. On average, RA3 achieves 4-point improvement over NTP and an 8-point improvement over the base models. Furthermore, RA3 accelerates RLVR convergence and attains higher asymptotic performance on HumanEval+, MBPP+, LiveCodeBench, and CodeForces. These results underscore the scalability and effectiveness of learning action abstractions during mid-training."
        },
        {
            "title": "2 Background",
            "content": "Imitation Learning. task = (S, A, R, γ) is an MDP with state space S, (primitive) action space A, reward function R, and discount factor γ < 1. In the language setting, state corresponds to the context of all previously generated tokens, and each primitive action is single token. State transitions are either deterministic, by appending the new action tokens to the context, or governed by the external environment. An expert policy is the policy that maximizes the expected return: πE argmax π Eatπ (cid:104) R(st, at) + γV π (cid:105) M(st+1) = Eatπ (cid:104)(cid:88) γtR(st, at) (cid:105) . It is worth noting that when the task is inherently solvable within single step, such as math problems where r(s0, agt) = 1, the expert policy deterministically outputs the ground-truth answer agt at s0, i.e., πE(agts0) = 1, to maximize the return. However, since most mid-training math data includes explicit human reasoning before reaching agt, we instead focus on the multi-step decision-making domains, such as code generation and agentic tasks, where expert trajectories are sequences of actions. Next-token prediction (NTP) during mid-training can be viewed as imitation learning on an offline expert dataset DE, collected by rolling out πE on sampled tasks p(M). Its objective is to maximize the conditional log-likelihood: JNTP(π) = E(s0:T ,a0:T )DE (cid:104) (cid:105) log π(a0:T s0:T ) = EDE (cid:35) log π(at st) , (2.1) (cid:34) (cid:88) t= where st S, at A, is the sequence length of an expert demonstration, π is the training policy, and s0 is the beginning of the sentence (BOS) token. The formula in (2.1) applies not only to token-level actions but also to coarser-grained actions, such as sentence-level actions. NTP is widely used across different stages of LLM trainingpre-training, continued pre-training (or midtraining when the goal is to acquire reasoning foundations before RLVR), and supervised fine-tuning. In this work, we focus primarily on mid-training, the second stage in the three-stage pipeline: pre-training, mid-training, and RLVR post-training. 2 RL with Verifiable Reward. The objective of post-training RL is to maximize the expected return. In RLVR, common setup is to formulate the problem as single-step MDP with binary, outcome-based reward, defined as r(s, o) = verifier(s, o) to check whether the model response matches the ground-truth answer associated with the prompt question s. We adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025) as our default RLVR algorithm in experiments. Its objective is JGRPO(π) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) (cid:18) i=1 min (cid:18) π(oi s) πold(oi s) Ai, clip (cid:18) π(oi s) πold(oi s) (cid:19) (cid:19) (cid:19) , 1 ϵ Ai βDKL(π, πref) , where oi πold(s), ϵ, β are hyperparameters, and the advantage is calculated within the group G: Ai = (cid:0)r(s, oi) mean({r(s, oi)}G i=1)(cid:1)/(cid:0)std({r(s, oi)}G i=1)(cid:1)."
        },
        {
            "title": "3 How Mid-Training Shapes Post-Training RL",
            "content": "The goal of post-training RL is to minimize regret: min π EMp(M)[V M(s0) π M(s0)], (2.2) (3.1) that is, to learn π that is near-optimal across tasks p(M). In what follows, to distinguish from the primitive action space introduced in Section 2, we consider unified action space Z, which generalizes by including action abstractions that will be defined later. The major role of mid-training is to identify subspace of useful actions to avoid exploring the vast language space during RL. To formalize this, we first define the quality of an action subset. Consider MZ = (S, , R, γ), which is identical to except that the action space is restricted to subset Z. We say is ϵ-optimal for if ϵ-optimal policies can be constructed using only the actions in . Formally, Definition 3.1 (ϵ-Optimal Task Action Subset). An action subset is called ϵ-optimal for task if the optimal values in and MZ satisfy (M, ) := M(s0) MZ (s0) ϵ. This definition allows us to connect mid-training with post-training RL through the following decomposition: Lemma 3.2 (Regret Decomposition). For any Z, the post-training RL regret in (3.1) satisfies EMp(M) (cid:2)V M(s0) π M(s0)(cid:3) = EMp(M)[(M, )] (cid:125) (cid:123)(cid:122) (cid:124) action-set pruning error + EMp(M)[V MZ (s0) π (cid:124) (cid:123)(cid:122) post-training RL error MZ (s0)] (cid:125) . Lemma 3.2 indicates that mid-training should identify an action subspace (cid:98)Z that minimizes two sources of error: (1) the approximation error from pruning the full action space down to (cid:98)Z, and (2) the post-training RL error incurred when planning within (cid:98)Z. Corresponding to these two terms, we will reveal two key factors that govern the effectiveness of mid-training: pruning efficiency, which determines the initial RL policy prior, and the impact on RL convergence, which determines how efficiently the policy can be further improved through online interactions."
        },
        {
            "title": "3.1 Mid-Training Acquires Policy Priors via Action Space Pruning",
            "content": "Mid-training prunes the action space by assigning low probability to actions unlikely to be useful across tasks. Formally, mid-training algorithms aim to prune away all suboptimal actions, defined as follows. Definition 3.3 (Suboptimal Action). Let Zϵ(M) denote the union of ϵ-optimal action subsets in M. An action is (ϵ, σ)-suboptimal if EMp(M)[1(z / Zϵ(M))] σ. central metric of mid-training pruning is how efficiently the algorithm can eliminate useless actions, since it directly determines the quality of the resulting action space given fixed number of expert demonstrations. To formalize this, we introduce the following notion. 3 Definition 3.4 (Minimal Size of ϵ-Optimal Action Subset). is an ϵ-optimal action subset if it is ϵ-optimal for all tasks. Let denote the minimal size of such subset. Notably, always exists and is finite since (M, A) = 0 and itself is an optimal action set. Now we are ready to give our result on the pruning efficiency during mid-training. Theorem 3.5 (Pruning Efficiency). Denote DE as the number of expert demonstrations during mid-training and (cid:98)Z as the resulting pruned action space. If (cid:16) DE = Θ log(Z/δ)/σ (cid:17) , then with probability at least 1 δ, all the (ϵ, σ)-suboptimal actions can be pruned away from Z, and the pruning error in Lemma 3.2 satisfies EMp(M)[(M, (cid:98)Z)] ϵ. Theorem 3.5 shows that the number of expert samples required to prune suboptimal actions decreases as both and shrink. With higher pruning efficiency, i.e., smaller and Z, the probability that suboptimal actions survive mid-training decreases, leading to smaller pruning error. This result highlights the importance of compact decision space and motivates defining as space of action abstractions instead of primitive actions A. Action abstractions are defined analogously to Markov options (Puterman, 1994; Sutton et al., 1999; Precup, 2000), representing the abstraction of temporally-extended primitive actions. Each corresponds to high-level intention that executes sequence ai, . . . , ai+τ , with duration τ p( s, z). Notably, the above result for also applies to as special instantiation of Z, with τ = 1 and aj = zj. Compared to the space of primitive actions A, the action abstraction has substantially smaller near-optimal action subset Z, since each action abstraction corresponds to transferable skill spanning multiple tasks. Theorem 3.5 implies that learning directly at the primitive action level demands significantly more expert data, while temporal abstractions enable mid-training to more efficiently approximate near-optimal action set and provide stronger policy prior, thereby reducing the burden on post-training RL."
        },
        {
            "title": "3.2 Mid-Training Accelerates RL Convergence",
            "content": "The analysis above focuses on pruning error while holding the post-training RL error fixed. However, even with zero pruning error (e.g., by retaining the full primitive action space (cid:98)Z = A), RL may still be highly inefficient for long-horizon tasks due to the vast action space. We now analyze how mid-training influences the convergence of online RL. Our result is based on value iteration due to its simplicity. Theorem 3.6 (RL Convergence Rate). For any action subspace Z, to achieve ε-optimality satisfying VN ε, the required number of iterations is lower-bounded by 1 ε(1γ) , where γ = sups,0 E[γτ s, z] γ and Rmax = maxs,a R(s, a). 1γ log Rmax The result reveals that reasoning structures acquired during mid-training affect convergence through the duration τ of the temporally-extended actions. Longer actions reduce γ, leading to faster RL convergence than NTP-style mid-training, where τ = 1 and γ = γ. This makes intuitive sense as each Bellman backup jumps across τ steps in one shot, which shortens the effective planning horizon and shrinks the error faster per iteration. Similar 1/(1 γ) dependency also appears in bounds for broader RL algorithms, such as policy gradient (Agarwal et al., 2021; Zhang et al., 2023), though we omit them due to additional assumptions. Intuitively, there are high-level skills\" that are shared across tasks. Leveraging these skills yields compact decision set and shortens the planning horizon, which makes pruning more efficient and planning more tractable. We illustrate this intuition with two examples in Figure 1."
        },
        {
            "title": "4 From Primitive Actions to Temporal Abstractions",
            "content": "In Section 3, we analyze the benefits of learning temporal action abstractions during mid-training from two perspectives: their efficiency in pruning the action space and their ability to accelerate subsequent RL. In what follows, we present principled method for extracting temporal abstractions from primitive actions in mid-training expert demonstrations. 4 Figure 1 (Left): The probabilistic graphical model of the action hierarchy. (Middle & Right): Examples of primitive actions in expert demonstrations (blue) and the hidden high-level temporal abstractions (green), in web agent and code generation domains, respectively."
        },
        {
            "title": "4.1 Temporal Variational Bound",
            "content": "We begin by seeking an alternative way to maximize the likelihood beyond predicting the next tokens. Specifically, we derive sequential Evidence Lower Bound (ELBO) for the NTP objective: Theorem 4.1 (Temporal ELBO). The next-token prediction objective in (2.1) is lower bounded by JNTP(π) (π, q) = E(s0:T ,a0:T )DE ,ztq log π(atst, zt) DKL (cid:16) q(ztst, z0:t1) p(ztst, z0:t1) (cid:17) (cid:35) , (cid:34) (cid:88) t=0 where p(ztst, zt1) is the prior distribution of zt. The ELBO introduces sequence of latents z0:T to model the observed primitive actions a0:T . Intuitively, these latents represent hidden thoughts or intentions behind the expert decisions a0:T that are not explicitly available in the mid-training data. Amortized variational inference employs parametric variational posterior q(ztst, z0:t1) as tractable surrogate for the intractable true posterior over z. Maximizing the ELBO amounts to estimating the latent trajectories z0:T that rationalize expert behavior, thereby tightening the bound on the marginal likelihood p(a0:T s0:T ). We maximize the ELBO by alternatively optimizing and π, in an Expectation-Maximization (EM) manner. In each EM iteration i, the step fixes πi and updates q: qi = argmax (πi, q) = argmax EDE ,ztq (cid:34) (cid:88) t=0 log πi(at st, zt) DKL (cid:124) (cid:123)(cid:122) RL reward at step (cid:35) . (cid:0)q p(cid:1) (cid:125) (4.1) This corresponds to -horizon RL procedure where the per-step reward is the log-likelihood of the observed expert actions, with KL penalty that will be discussed later. Intuitively, (4.1) encourages the latent sequence z0:T sampled from qi to explain\" the expert decisions a0:T . The -step then fixes the updated qi and optimizes π: πi+1 = argmax π (π, qi) = argmax π EDE ,ztqi (cid:2)log π(at st, zt)(cid:3), (4.2) which is simply imitation learning on expert trajectories bootstrapped with the inferred latents zt."
        },
        {
            "title": "4.2 Temporally-Consistent Latents as Action Abstractions",
            "content": "We have reformulated likelihood maximization on primitive actions a0:T as optimizing an ELBO that learns from the bootstrapped sequence with latent trajectories sampled from the variational posterior q(ztst, z0:t1). 5 Recall that our analysis in Section 3 reveals the benefits of compact sets of high-level action abstractions. We will show in the following how this can be achieved by properly designing the latent prior. The latent zt in the ELBO is defined per-step for every [0, ]. To represent temporally extended 1. This is enforced by defining abstractions spanning τ p( st, zt) steps, we require zt = zt+1 = = zt+τ the prior p(zt+1st+1, zt) in Theorem 4.1 to have large probability mass on zt, and uniformly distributed at all other positions: p(zt st, zt1) = αδ(zt1) + (1 α)U (zt), (4.3) where α [0, 1] is hyperparameter, δ() is the Dirac delta function, and () is the uniform distribution over Z. The delta mass helps preserve temporally-consistent latent as high-level action abstraction (reusing zt1), while the uniform term promotes diversity across latents."
        },
        {
            "title": "5 RA3: A Scalable Mid-Training Algorithm",
            "content": "In the context of LLMs, the action abstraction serves similar role as rational or intention, and the primitive action corresponds to the actual token or operation in the environment. Taking code generation as an example, zt might capture reasoning steps that precede writing the next code block at+τ . In addition to the theoretical benefits discussed in Section 3, the temporal consistency of the latents also enables scaling to mid-training-sized data. Directly generating rationals z0:T requires rollouts, proportional to the total number of tokens in the data. This prohibits us from making full use of the high-quality data in the mid-training corpus, which typically contains billions of tokens. Fortunately, the temporal consistency of latents rescues us from resampling rollout zt at each timestep t, as it remains fixed for τ steps. To exploit this, we define two types of latents: = <act> and that begins with <think>. When the latent is temporally-consistent at timestep t, at is generated directly without sampling new rationals since zt = zt1 and at shares the same high-level intention as at1. For this reason, we use zt = <act> to indicate it is temporally-consistent. Rollouts terminate immediately upon sampling <act>. BIn contrast, full rollouts are sampled only when think initiates new rationale, significantly reducing RL inference cost. This design modifies the KL term in the ELBO as follows. Proposition 5.1. With the prior defined in (4.3), the KL term in Theorem 4.1 satisfies (cid:16) DKL q(ztst, z0:t1) p(ztst, z0:t1) (cid:17) (cid:16) = DKL Bern(qact) Bern(α) (cid:17) (1 qact)H (cid:16) q(ztst, z0:t1) (cid:17) , where qact = q(zt = <act> st, z0:t1), Bern() is the Bernoulli distribution, and (cid:16) DKL Bern(qact) Bern(α) (cid:17) (cid:104) = Ezq 1(zt = <act>) log qact α + 1(zt = <act>) log 1 qact 1 α (cid:105) . The KL decomposes into Bernoulli KL regularizer and an entropy term. By setting α > qact, the KL discourages unnecessary thinking, since it assigns larger penalty to zt = <act> than zt = <act>. The penalty difference defines threshold, guiding the model to generate new rationals only when they improve the log-likelihood reward by more than this threshold. In implementation, instead of tuning α, we apply reward shaping and assign fixed penalty to zt = <act>. Proposition 5.1 also indicates that the additional training cost relative to NTP can be adjusted by α. In the extreme case where α = 1, RA3 degenerates to NTP, since DKL(Bern(qact) Bern(1)) is infinite for all qact < 1, i.e., the policy receives an infinite penalty for generating any rationals. We provide the pseudocode in Algorithm 1. Algorithm 1 Reasoning as Action Abstractions (RA3) for Mid-Training 1: Input: Base LLM π0, mid-training dataset DE, penalty hyperparameter c. 2: for EM iteration in 1, 2, do Optimize qi = argmaxq 3: Fine-tune πi+1 = argmaxπ t=0 log πi(atst, zt) c1(zt = <act>)] via RL. (cid:2)log π(at st, zt)(cid:3) via NTP. ,ztq[(cid:80)T Dei Dmi ,ztqi 4: 1The notation here deviates slightly from Section 2, where all τ identical latents are written as single z. 6 We enforce the two types of latents discussed above by incorporating simple format reward in the RL step that assigns zero reward to the wrong format, which we omit in the pseudocode for clarity. We optimize the -horizon RL objective using policy gradient. Advantages are calculated in way similar to (2.2) in GRPO: after sampling length-T rollouts, we set the baselines as b(st) = (cid:80)G /G that are independent of actions, and combine it with the state-action value to calculate the advantage at each step. t=t rg (cid:80)T g="
        },
        {
            "title": "6 Related Work",
            "content": "LLM Mid-Training. RL has long been utilized for training language models (Nguyen et al., 2017; Paulus et al., 2017; Jaques et al., 2020; Ramamurthy et al., 2022; Ouyang et al., 2022). However, its potential as universal policy-improvement operator has only recently been realized, as reasoning models learn to cast intermediate thoughts as actions and optimize them via RL (Guo et al., 2025; Zeng et al., 2025b; Liu et al., 2025; Zhang et al., 2025; Team et al., 2025). This paradigm also informs the design of the policy prior. To obtain strong initial policies in terms of both performance and exploration diversity, mid-training (Xu et al., 2025; Wang et al., 2025c; Su et al., 2025) has become an essential step, which performs reasoning or agentic continued pre-training on high-quality expert data. Previous work that leverages next-token prediction during mid-training is an imitation learning process, where the data comes from rollouts of optimal expert policies, such as human demonstrations in device control and code writing (Rawles et al., 2023; Huang et al., 2024; Bai et al., 2024). Anchoring our findings, prior work observes that learning action hierarchies with abstraction-based reasoning outperforms training only on primitive actions alone (Xu et al., 2024; Chen et al., 2024; Wang et al., 2025b; Xue et al., 2025). For similar reasons, some mid-training datasets augment the expert data with synthetic reasoning distilled from frontier LLMs (Wang et al., 2025b; LI et al., 2024). However, such distillation introduces distributional shift, and it remains unclear how much student models benefit compared to our method, which learns its own reasoning via RL. Moreover, RA3 avoids the high cost of reasoning distillation at scale. In fact, in the code generation domain of interest here, most mid-training datasets consist primarily of human-written code scraped from the internet, without costly reasoning relabeling. Self-Supervised RL. Optimizing the temporal variational bound involves self-supervised RL step with logprob of the expert action as reward (Zhong et al., 2025; Ruan et al., 2025; Zhou et al., 2025a; Dong et al., 2025; Wang et al., 2025a). Compared to Zhong et al. (2025); Ruan et al. (2025) that also uses EM-style updates, we are motivated to learn action abstractions with the ELBO derived for temporal sequences. Moreover, Dong et al. (2025) hand-crafts an entropy-based rule to determine reasoning positions and trains on only 4k samples with instructed reasoning models. In contrast, RA3 is theoretically grounded, scales to mid-training setups, and enables the model to autonomously decide when to skip unnecessary reasoning through the temporal consistency of latents. Markov Options. RL based on options (Sutton et al., 1999; Precup, 2000; Bacon et al., 2017) equips agents with varying courses of actions at extended time scales and learn in the MDP with them. It provides natural form of hierarchical structure that facilitates learning in long-horizon tasks (Jong et al., 2008). Our analysis of the decision space size is partly inspired by studies on option transfer in lifelong and multi-task RL (Brunskill and Li, 2013, 2014). But unlike those works, which focus on option transferability, we are primarily concerned with designing mid-training algorithms for LLMs and understanding their impact on post-training RL."
        },
        {
            "title": "7 Experiments",
            "content": "Experiment Setups. We focus on Python code generation in our experiments. Primitive actions are defined at the granularity of single line of code. For the two types of latents z, to avoid additional fine-tuning on special tokens, we remove the newline character at the end of and set <act>=n, <think>=n#. Thus, after line at, the model either outputs only before at+1 or generates comment line as high-level abstraction to guide code writing. The format reward is non-zero for the think action if it begins and ends with and the first non-space token is #. This design ensures that the reasoning bootstrapped data has the correct syntax and no additional filtering of the thinking sequences is needed. For the RL step of RA3, We replace the -horizon objective with 5-step truncated return, attributing credit for action only to rewards 7 within the next five steps. The RL optimization is implemented analogously to multi-turn RL: the code lines at and reasoning comments zt alternate as turns. We adopt asynchronous rollout with the SGLang engine (Zheng et al., 2024), since batched inference often incurs idle time between turns. That is, batched rollouts must wait for all zt to be generated before proceeding to the next turn, which is particularly inefficient when temporal consistency is intended to minimize reasoning frequency. We apply RA3 on three pre-trained models: Qwen-2.5-1.5B (Hui et al., 2024), Llama-3.2-1B, and Llama-3.1-8B (Dubey et al., 2024). The mid-training data is drawn from the Python subset of the continued pre-training corpus of Huang et al. (2024), containing 2.36M high-quality internet code snippets (834M tokens) and 129K code-only synthetic snippets (120M tokens). This leads to total mid-training size of 3.5M code snippets with 1B tokens. Each EM iteration involves 400 gradient updates, with the first 40 being the RL policy gradient. short warmup stage of 10 RL gradient updates is applied without KL penalty, allowing the policy to initially focus on extracting the hidden rationales. The hyperparameters of the step of RA3 is the same as NTP: batch size of 512 with learning rate of 2e 5. The RL step of RA3 sets the maximum length of as 16, with sampling temperature of 1.0 and group size of = 3. The entropy coefficient is set to 0.001 and we do not regularize the reference KL. The RL batch size is set to 1024 with learning rate of 2e 6 and the penalty is set to = 0.05. AdamW optimizer is used (Loshchilov and Hutter, 2017) for all algorithms. Mid-Training Results. We begin by analyzing the results during the EM steps of RA3. The RL training reward in the first step is shown in Figure 2. We find that the model quickly learns to maximize the reward, meaning that most data and compute can be allocated to reasoning bootstrapping and supervised fine-tuning in the step. We also provide two pairs of examples of the training data before and after the RL steps in Figure 3. After RL, the policy demonstrates the ability to extract high-level abstractions of expert behaviors embedded in the training data. These abstractions often correspond to transferable skills, such as dummy head creation and BFS, which in turn enhance the models generalizability. Figure 2 RL training reward curve. Figure 3 Examples of the data from mid-training and after reasoning bootstrapping, where transferable skills, such as dummy head creation and BFS, are abstracted and incorporated into the data. Figure 4 Data bootstrapped with reasoning learned in the step reduces the CE loss during the step fine-tuning. 8 For the step, an important metric is the cross-entropy (CE) loss, i.e., the negative log-likelihood of the next token. It can be observed from Figure 4 that fine-tuning on reasoning-bootstrapped data significantly accelerates learning. This supports our hypothesis on the mid-training data: while expert rollouts provide only primitive actions (raw code lines), hidden reasoning trajectories guide those decisions. Extracting such reasoning via RL makes learning easier and more efficient. We further evaluate the resulting models on the HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), HumanEval+, and MBPP+ (Liu et al., 2023) benchmarks using the BigCode evaluation harness (Ben Allal et al., 2022). We test the improvement of each EM iteration of RA3 by the average scores on these benchmarks, and compare with the NTP checkpoints that are trained on the same data. Figure 5 tracks performance across EM iterations, showing that reasoning as action abstractions reduces CE loss and improves evaluation accuracy with fewer data compared to NTP. Figure 5 Evaluation results during mid-training, with accuracies averaged across four benchmarks. Table 1 reports pass@k for the best NTP and RA3 checkpoints. RA3 consistently outperforms NTP, underscoring the generalization benefits of temporal action abstractions. Post-Training RLVR Results. We next study the impact of mid-training on post-training RL. We use GRPO as the default RLVR algorithm, and leverage the off-the-shelf DeepCoder codebase (Luo et al., 2025). We use the AReaL-boba-2-RL-Code dataset (Fu et al., 2025) for training, which contains 7.7K data, filtered from TACO (Li et al., 2023), Mattern et al. (2025), and LiveCodeBench (Jain et al., 2024). Instead of using the chat format and parsing the code within block, we provide the function signature at the end of the instruction. This way, base models that do not support chat templates can directly complete the function body following the given signature. We run independent RLVR training with different random seeds (3 for small models and 2 for the 8B model), and the evaluation results are reported in Figure 6. We observe that mid-training significantly improves the RLVR performance compared to the base models. Besides, RA3 is able to learn more effectively during RLVR, in terms of both convergence speed and asymptotic performance, supporting our theoretical results in Section 3. Table 1 Mid-training performance comparison: pass@k on HumanEval, MBPP, HumanEval+, and MBPP+. Model Method HumanEval MBPP HumanEval+ MBPP+ Avg. p@1 p@5 p@1 p@5 p@1 p@ p@1 p@5 p@1 p@5 Llama 3.2 1B Qwen 2.5 1.5B Llama 3.1 8B Base NTP RA3 Base NTP RA3 Base NTP RA3 18.9 21.3 25.0 37.2 41.5 48. 36.6 48.2 50.0 30.4 34.1 38.1 53.7 57.3 63.6 57.5 62.0 66.2 25.8 27.8 32.8 38.6 43.4 45. 45.2 48.6 48.0 17.1 17.7 22.0 32.3 35.4 42.7 30.5 42.7 44.5 25.7 29.5 33.5 48.1 53.3 59. 54.7 60.1 61.3 31.5 34.4 39.4 43.4 46.6 49.7 51.6 51.1 53.2 47.9 51.8 54.6 61.0 64.8 64. 65.7 67.4 67.8 23.3 25.3 29.8 37.9 41.7 46.6 41.0 47.7 48.9 35.3 40.3 43.1 54.8 58.7 62. 59.3 63.1 64.6 37.0 45.8 46.1 56.5 59.2 61.3 59.1 62.9 63.0 9 Figure 6 RLVR evaluation results (mean and standard error across independent runs) of different mid-training algorithms. Ablation Study. We investigate the role of the KL penalty when training the Qwen model. Recall that regulates the temporal consistency of the latent action by enforcing threshold for when reasoning should occur. As shown in Figure 7, small causes the q-policy to generate redundant values at nearly every timestep. This behavior is expected: for most steps, there exists some zt = <act> that explains at+1 more effectively (i.e., yields higher log-likelihood RL reward) than the null action zt = <act>. However, such behavior offers no advantage over primitive-action NTP, since neither the decision space nor the effective planning horizon is reduced. Moreover, the computational overhead of RA3 can be controlled through c: large penalty = 0.2 requires that <think> improves the expert log-likelihood by at least 0.2 to be rewarded, an unlikely event. In this regime, the policy learns to output only <act>, and RA3 degenerates into NTP. Our default = 0.05 strikes balance: reasoning is triggered at fewer than 40% of lines, producing short rationales while maintaining efficiency. Figure 7 Effect of penalty on (a) the behavior of the RL step, (b) the mean accuracy of HumanEval and MBPP, (c) the average length of z, and (d) the ratio of full rollout samples."
        },
        {
            "title": "8 Conclusion",
            "content": "An effective mid-training stage should extract from expert demonstrations the subset of actions that are sufficient for all tasks and structure the decision space to enable fast RL convergence. In this paper, we present the first formal analysis of how mid-training design choices shape post-training RL. Our findings highlight two critical roles of mid-training: (1) efficiently pruning the action space, which forms strong policy prior, and (2) accelerating subsequent RL convergence, which maximizes policy improvement through online interactions. Both insights favor learning in the space of temporally extended actions, rather than the large primitive action space of next-token prediction. Guided by this principle, we propose novel mid-training algorithm based on the temporal variational bound. This method iteratively uncovers temporally-consistent reasoning trajectories via RL and fine-tunes on the resulting bootstrapped data. Experiments on code generation tasks confirm 10 that reasoning as action abstraction enhances generalizability of the initial policy and improves post-training RLVR in both convergence speed and asymptotic performance."
        },
        {
            "title": "References",
            "content": "Alekh Agarwal, Sham Kakade, Jason Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):176, 2021. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von Werra. framework for the evaluation of code generation models. https://github.com/bigcode-project/bigcode-evaluation-harness, 2022. Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. arXiv preprint arXiv:1309.6821, 2013. Emma Brunskill and Lihong Li. Pac-inspired option discovery in lifelong reinforcement learning. In International conference on machine learning, pages 316324. PMLR, 2014. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. Comments as natural logic pivots: Improve code generation via comment perspective. arXiv preprint arXiv:2404.07549, 2024. Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei. Reinforcement pre-training. arXiv preprint arXiv:2506.08007, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Shane Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. arXiv preprint arXiv:2010.05848, 2020. 11 Nicholas Jong, Todd Hester, and Peter Stone. The utility of temporal abstraction in reinforcement learning. In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 1, pages 299306, 2008. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/aimo-progress-prize/blob/main/ report/numina_dataset.pdf), 2024. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. Justus Mattern, Sami Jaghouar, Manveer Basra, Jannik Straube, Matthew Di Ferrante, Felix Gabriel, Jack Min Ong, Vincent Weisser, and Johannes Hagemann. Synthetic-1: Two million collaboratively generated reasoning traces from deepseek-r1, 2025. URL https://www.primeintellect.ai/blog/synthetic-1-release. Khanh Nguyen, Hal Daumé III, and Jordan Boyd-Graber. Reinforcement learning for bandit neural machine translation with simulated human feedback. arXiv preprint arXiv:1707.07402, 2017. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Romain Paulus, Caiming Xiong, and Richard Socher. deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. Doina Precup. Temporal abstraction in reinforcement learning. University of Massachusetts Amherst, 2000. Martin Puterman. Markov decision processes: Discrete stochastic dynamic programming, 1994. Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. Yangjun Ruan, Neil Band, Chris Maddison, and Tatsunori Hashimoto. Reasoning to learn from latent thoughts. arXiv preprint arXiv:2503.18866, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, et al. Scaling agents via continual pre-training. arXiv preprint arXiv:2509.13310, 2025. Richard Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181211, 1999. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Haozhe Wang, Haoran Que, Qixin Xu, Minghao Liu, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Wei Ye, Tong Yang, Wenhao Huang, et al. Reverse-engineered reasoning for open-ended generation. arXiv preprint arXiv:2509.06160, 2025a. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025b. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025c. Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin Kim, Yunsheng Li, Liliang Ren, Yelong Shen, et al. Phi-4-mini-reasoning: Exploring the limits of small reasoning language models in math. arXiv preprint arXiv:2504.21233, 2025. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718, 2025a. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025b. Shenao Zhang, Boyi Liu, Zhaoran Wang, and Tuo Zhao. Model-based reparameterization policy gradient methods: Theory and practical algorithms. Advances in Neural Information Processing Systems, 36:6839168419, 2023. Shenao Zhang, Yaqing Wang, Yinxiao Liu, Tianqi Liu, Peter Grabowski, Eugene Ie, Zhaoran Wang, and Yunxuan Li. Beyond markovian: Reflective exploration via bayes-adaptive rl for llm reasoning. arXiv preprint arXiv:2505.20561, 2025. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, et al. Brite: Bootstrapping reinforced thinking process to enhance language model reasoning. arXiv preprint arXiv:2501.18858, 2025. Xiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min Lin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493, 2025a. Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478, 2025b."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Proof of Lemma 3.2 Proof. Since the policy π selects actions from (cid:98)Z, it is an admissible policy in both and dynamics, rewards, and discount factors are identical for and follow from basic algebra. (cid:98)Z , we have π = π (cid:98)Z (cid:98)Z . Because the . The results then A.2 Proof of Theorem 3.5 Before the proof, we first define the subset of suboptimal actions. Definition A.1 (Suboptimal Action Subset). is (ϵ, σ)-suboptimal if EMp(M)[1((M, ) > ϵ)] σ. 13 Proof. For an (ϵ, σ)-suboptimal action subset , according to Definition A.1, we know that Pr Mp(M) [(M, ) > ϵ] σ. Therefore, for randomly drawn task p(M), the probability that is not pruned away is no more than 1 σ. With DE independent expert demonstrations in mid-training, we reject if (s0) + ϵ for any [1, DE]. The probability that is not pruned away during mid-training is thus no more than (1 σ)DE . We denote it as Mi(s0) Mi p(cid:0)survive(Z (cid:1)) (1 σ)DE eσDE , where the second inequality holds since 1 + ex for all real x. Let = { (cid:98)Z : (cid:98)Z = Z} be the set of all Z-length subsets. We care about the event that there exist some suboptimal subsets of actions that are not pruned away, i.e., survive: (cid:32) (cid:91) (cid:33) survive(Z ) (cid:88) p(cid:0)survive(Z )(cid:1) (cid:19) (cid:18)Z eσDE = Θ (cid:16) ZZ(cid:17) eσDE , where the inequalities hold by applying union bound over all candidate subsets. Plugging in DE = Θ(Z log(Z/δ)/σ) gives us (cid:32) (cid:91) (cid:33) survive(Z ) δ, i.e., all the (ϵ, σ)-sumoptimal action subsets are pruned away with probability at least 1 δ. Then the result follows by rejecting all the actions that are not in the union of the subsets that survive. That is, with probability at least 1 δ, all the (ϵ, σ)-suboptimal actions can be pruned away from Z. According to Definition A.1, any pruned action set (cid:98)Z satisfies EMp(M)[1((M, (cid:98)Z) ϵ)] σ. Thus, with probability at least 1 δ, the pruning error EMp(M)[(M, (cid:98)Z)] ϵ. A.3 Proof of Theorem 3. Proof. We first define the value iteration operator in the temporally extended action space as (T )(s) = max zZ (cid:110) Eτ,s (cid:2)Rτ + γτ (s) s, z(cid:3)(cid:111) , where p(ss, z) = (cid:80) steps after taking action at s, and Rτ = (cid:80)τ For any functions V1, V2 and any state s, we have k=1 γkRk. γjp(s, τ = js, z), p(s, τ = js, z) is the joint probability of transitioning to in τ (T V1)(s) (T V2)(s) = max zZ max zZ (cid:110) (cid:2)Rτ + γτ V1(s) s, z(cid:3)(cid:111) Eτ,s E(cid:2)γτ V1(s) V2(s) s, z(cid:3), (cid:110) Eτ,s (cid:2)Rτ + γτ V2(s) s, z(cid:3)(cid:111) max zZ where the inequality holds since max max max(f g) for any f, g. Let be the sup norm on functions : (cid:55) R. Then (T V1)(s) (T V2)(s) max zZ (cid:110) E(cid:2)γτ (cid:3) V1 V2 (cid:111) γ V1 V2 . 14 Since the above inequality holds for all S, taking the supremum over gives us V1 V2 γ V1 V2. Therefore, is γ-contraction on (RS , ). By Banachs fixed-point theorem, has unique fixed point and value iteration Vn+1 = Vn converges geometrically: Vn γn V0 γnRmax/(1 γ), where the last inequality holds since V0 = 0 and the maximum value satisfies Vmax = (cid:80) Rmax/(1 γ). In order to get VN ε, we obtain γtRmax = 1 1 γ log Rmax ε(1 γ) . A.4 Proof of Theorem 4.1 Proof. For the maximum likelihood objective, after introducing the sequence z0:T of latents, we have log p(a0:T s0:T ) = log = log (cid:88) z0:T (cid:88) z0:T p(a0:T , z0:T s0:T ) p(a0:T , z0:T s0:T ) = log Ez0:T q(s0:T ) q(z0:T s0:T ) q(z0:T s0:T ) (cid:21) (cid:20) p(a0:T , z0:T s0:T ) q(z0:T s0:T ) p(a0:T , z0:T s0:T ) q(z0:T s0:T ) (cid:20) log (cid:21) , (A.1) Ez0:T q(s0:T ) where the last inequality follows from Jensens inequality. From the probabilistic graphical model, we obtain that q(z0:T s0:T ) = q(z0 s0:T ) (cid:89) t=1 q(zt s0:T , z0:t1) = q(z0 s0) (cid:89) t=1 q(zt st, z0:t1). (A.2) Besides, we have from the Bayes rule that p(a0:T , z0:T s0:T ) = p(a0, z0 s0:T ) = p(a0, z0 s0:T ) (cid:89) t=1 (cid:89) t= p(zt, at s0:T , z0:t1, a0:t1) p(zt s0:T , z0:t1, a0:t1)p(at s0:T , z0:t, a0:t1) = p(a0, z0 s0) = p(z0 s0) (cid:89) t= (cid:89) t=1 p(zt st, z0:t1)p(at st, zt) p(zt st, z0:t1) (cid:89) t= p(at st, zt). (A.3) 15 Plugging (A.2) and (A.3) into (A.1) gives us log p(a0:T s0:T ) Ez0:T q(s0:T ) (cid:20) log p(z0 s0) (cid:81)T t=1 p(zt st, z0:t1) (cid:81)T t=0 p(at st, zt) (cid:21) = Eztq(st,z0:t1) = Eztq(st,z0:t1) (cid:34) (cid:88) t=0 (cid:34) (cid:88) t= q(z0 s0) (cid:81)T log p(at st, zt) log t=1 q(zt st, z0:t1) (cid:35) q(zt st, z0:t1) p(at st, zt) log p(at st, zt) DKL (cid:16) q(zt st, z0:t1) p(zt st, zt1) (cid:17) (cid:35) , where p(ztst, zt1) is the prior distribution of the latent zt. A.5 Proof of Proposition 5.1 Proof. According to the definition of KL divergence, we have (cid:16)"
        },
        {
            "title": "DKL",
            "content": "q(ztst, z0:t1) p(ztst, z0:t1) (cid:17) = qact log = qact log = qact log qact α qact α qact α + + + (cid:88) zt=act (cid:88) zt=act (1 qact) log (1 qact) log q(ztst, z0:t1) p(ztst, z0:t1) q(zt st, z0:t1) (1 α)U (zt) (cid:88) (cid:20) (1 qact) log zt=act q(zt st, z0:t1) 1 qact + log 1 qact 1 α (cid:21) log (zt) (cid:16) = DKL Bern(qact) Bern(α) (cid:17) (1 qact)H (cid:16) q(ztst, z0:t1) (cid:17) , where the terms that are constant w.r.t. is omitted."
        }
    ],
    "affiliations": [
        "Apple",
        "Northwestern University",
        "UIUC"
    ]
}