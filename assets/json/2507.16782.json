{
    "paper_title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection",
    "authors": [
        "Changhao Li",
        "Xinrui Chen",
        "Ji Wang",
        "Kang Zhao",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Quantization is a key technique to reduce network size and computational complexity by representing the network parameters with a lower precision. Traditional quantization methods rely on access to original training data, which is often restricted due to privacy concerns or security challenges. Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated from pre-trained models, eliminating the need for real training data. Recently, ZSQ has been extended to object detection. However, existing methods use unlabeled task-agnostic synthetic images that lack the specific information required for object detection, leading to suboptimal performance. In this paper, we propose a novel task-specific ZSQ framework for object detection networks, which consists of two main stages. First, we introduce a bounding box and category sampling strategy to synthesize a task-specific calibration set from the pre-trained network, reconstructing object locations, sizes, and category distributions without any prior knowledge. Second, we integrate task-specific training into the knowledge distillation process to restore the performance of quantized detection networks. Extensive experiments conducted on the MS-COCO and Pascal VOC datasets demonstrate the efficiency and state-of-the-art performance of our method. Our code is publicly available at: https://github.com/DFQ-Dojo/dfq-toolkit ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 8 7 6 1 . 7 0 5 2 : r Task-Specific Zero-shot Quantization-Aware Training for Object Detection Changhao Li1* Atlanta, USA cli911@gatech.edu Xinrui Chen2* Shenzhen, China cxr22@tsinghua.org.cn Ji Wang3* Beijing, China wangji20@tsinghua.org.cn Kang Zhao4 Beijing, China zhaok14@tsinghua.org.cn Jianfei Chen4 Beijing, China jianfeic@tsinghua.edu.cn 1 School of Computational Science and Engineering, Georgia Institute of Technology 2 Shenzhen International Graduate School, Tsinghua University 3 School of Software, Tsinghua University 4 Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Quantization is key technique to reduce network size and computational complexity by representing the network parameters with lower precision. Traditional quantization methods rely on access to original training data, which is often restricted due to privacy concerns or security challenges. Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated from pre-trained models, eliminating the need for real training data. Recently, ZSQ has been extended to object detection. However, existing methods use unlabeled task-agnostic synthetic images that lack the specific information required for object detection, leading to suboptimal performance. In this paper, we propose novel task-specific ZSQ framework for object detection networks, which consists of two main stages. First, we introduce bounding box and category sampling strategy to synthesize task-specific calibration set from the pre-trained network, reconstructing object locations, sizes, and category distributions without any prior knowledge. Second, we integrate task-specific training into the knowledge distillation process to restore the performance of quantized detection networks. Extensive experiments conducted on the MSCOCO and Pascal VOC datasets demonstrate the efficiency and state-of-the-art performance of our method. Our code is publicly available at https://github.com/DFQ-Dojo/dfqtoolkit. *Equal contribution Corresponding Author Object detection neural networks are integral to wide array of computer vision applications, ranging from autonomous driving to surveillance systems [1, 38, 39, 45]. As the demand for deploying deep neural networks on resource-constrained devices continues to grow, quantization has emerged as critical technique to reduce network size and computational complexity while maintaining performance [7, 11, 20, 52]. However, traditional quantizationaware training methods often require access to the entire original training data, which may become inaccessible if the data are very huge or subjected to privacy protection. [26, 43]. In these scenarios, Zero-shot Quantization (ZSQ) [3, 34, 40, 54, 57] presents promising solution, enabling the quantization of neural networks without relying on original training data. These methods primarily train their model on synthetic data generated by inverting the network with randomly sampled labels. Most prior work in this area has focused on classification networks. Specifically, they use model inversion to synthesize data by optimizing the gauss noise image through gradient descent. Their loss functions are often designated to classification tasks, which generate classification task-specific images. For example, GDFQ [50] introduced knowledge-matching generator to synthesize label-oriented data using cross-entropy loss and batch normalization statistics (BNS) alignment. TexQ [6] emphasized the detailed texture feature distribution in real samples and devised texture calibration for labeled image generation. PSAQ-ViT [31] introduced patch similarity aware Figure 1. Comparative analysis of different synthetic images and their impact on zero-shot quantization-aware training with object detection network Mask R-CNN. It is observed that task-specific calibration set matters and results in better performance on the MS-COCO dataset. Inspired by this, we integrate novel task-specific manners into data synthesis and quantized detection network finetuning processes. strategy to invert labeled images from Vision Transformers for quantization. These methods leverage synthetic data generated from the full-precision network to calibrate (i.e., post-training quantization) or finetune the quantized network (i.e., quantization-aware training) for accurate quantization. more comprehensive discussion of data-driven quantization and ZSQ are presented in Appendix A. Recently, ZSQ has been extended to downstream tasks such as object detection, but its application is limited due to inherent complexity. Classification tasks require only randomly assigned category as the target label, but object detection demands that the target label comprise both the bounding box location and the classification label, making it difficult to determine. Consequently, existing approaches for detection networks drop the detection training loss and instead adopt task-agnostic strategy for data generation and model quantization. For instance, PSAQ-ViT V2 [32] introduced an adaptive teacherstudent strategy to generate task-agnostic images for finetuning the quantized model via knowledge distillation. Similarly, MimiQ [10] proposed inter-head attention similarity and applied head-wise structural attention distillation to align the attention maps of the quantized network with those of the full-precision teacher across downstream tasks. CLAMP-ViT [48] employed two-stage approach, cyclically adapting between data generation and quantization. Although task-agnostic strategy enhances generalizability across different downstream tasks, it leads to lack of task-specific bounding box size and location information in the object detection network, potentially resulting in suboptimal performance. We argue that incorporating task-specific information into ZSQ can significantly increase its effect. By augmenting training loss with object categories and bounding box information, our method can outperform previous taskagnostic methods and, in some settings, may even achieve comparable results to networks trained with full real-data. The proposed task-specific framework consists of two stages. In the generation stage, we introduce novel bounding box and category sampling strategy to synthesize calibration set from pre-trained detection network, which reconstructs the location, size and category distribution of objects within the data without any prior knowledge. In the quantization stage, we integrate the detection training loss into the distillation process to further amplify the efficacy of quantized detection network finetuning. Extensive experiments on MS-COCO and Pascal VOC confirm the state-of-the-art performance of our method. For example, when quantizing YOLOv5-l to 6-bit, we achieve 1.7% mAP improvement over LSQ trained with full real data. Furthermore, tests on YOLO11 and Swin Transformer models show our approach surpasses task-agnostic ZSQ by 2-3% in mAP across various quantization settings. Specifically, our contributions are threefold: 1. Drawback of task-agnostic calibration is revealed. We emphasize task-specific synthetic images for zeroshot quantization of object detection networks. By developing task-specific approach that optimizes both data synthesis and finetuning, we unlock the full performance potential of quantized object detection networks. 2. Task-specific object detection images synthesis. We propose bounding box sampling method tailored for object detection networks to reconstruct object categories, locations, and sizes in synthetic samples without any prior knowledge. 3. Task-specific quantized network distillation. We integrate object detection task-specific finetuning into quantized network distillation, effectively restoring the performance of quantized object detection networks. 2. Motivation 2.1. Preliminary on Quantization Network quantization has emerged as critical technique for reducing network size and computational cost while maintaining performance [7, 11, 20, 52]. Given floating-point tensor wf (weights or activa2 tions) and quantization bit width b, commonly used pertensor symmetric quantizer LSQ [15] for both weights and activations to quantize the data ˆwf can be defined as: wint = clip( wf , 2b1, 2b1 1), ˆwf = wint s. (1) (2) Here, wint denotes the quantized integer representation of the data, input rounds the input to its nearest integer, and the step size is quantization parameter that is obtained with calibration set and updated during quantizationaware training. Notably, the calculation of quantization parameters requires access to real training data as the calibration set, rendering traditional quantization methods inapplicable when training data is unavailable. 2.2. Revisiting ZSQs for Object Detection Task-specific calibration set matters. ZSQs synthesize images as the calibration set for quantization. Despite the proven efficacy of task-agnostic images in enhancing various downstream tasks within the ZSQ framework, it is intuitively evident that these images lose considerable amount of task-specific information, which potentially compromise the performance of specific tasks. As illustrated in Fig. 1, we visualize four types of synthetic images utilized for zero-shot quantization-aware training: Gaussian noise, classification task-specific, task-agnostic, and object detection task-specific images. The Gaussian noise image serves as baseline for comparison. Task-agnostic images capture the general features extracted by the networks backbone, whereas task-specific images are derived through model inversion with corresponding labels, such as classification categories and object detection bounding box localizations. The visualization analysis in Fig. 1 reveals that task-specific images extract richer set of features compared to other types of images, including object location, label, and size. Subsequently, we conduct comparative analysis of synthetic images to explore their efficacy in zero-shot quantization-aware training, indicating that task-mismatched images lead to performance degradation, as demonstrated by the classification task-specific image in Fig. 1(b), whereas task-specific images enhance performance, as shown by the object detection task-specific image in Fig. 1(d). Therefore, task-specific calibration is crucial in the zero-shot quantization for detection tasks. Challenges on task-specific ZSQ for detection. While task-specific zero-shot quantization has achieved remarkable success in classification tasks, extending it to object detection faces significant challenges. First, the label sampling methods in the classification task for synthesizing labeled images cannot be directly extended to object detection. For classification networks, data 3 synthesis typically requires only randomly sampled category ID label [6, 9, 46, 47, 59]. In contrast, for object detection, the location and size of objects within the samples remain unknown and elusive in zero-shot scenarios, making artificial reconstruction without ground-truth information extremely challenging. Besides, object detection datasets typically exhibit significant category imbalance, which is not captured by random category sampling methods designed for classification tasks (e.g., categorybalanced ImageNet or CIFAR-10/CIFAR-100). Therefore, randomly sampling object categories, locations and sizes often results in implausible category distribution, relative positions and sizes, leading to unrealistic synthetic data. Furthermore, the task-specific finetuning strategy for detection networks using synthetic calibration data remains underexplored. The currently used logits alignment methods, which are designed for classification networks, may not be sufficient for the more complex object detection networks. This insufficiency makes it difficult to fully leverage the limited synthetic data efficiently, thereby hindering the performance improvement of quantized detection models. 3. Methodology In this section, we provide an overview of the proposed framework in Fig. 2. It contains two stages: generating task-specific calibration set and performing quantizationaware training (QAT) with task-specific distillation. 3.1. Preliminaries on Task-Agnostic Data Synthesis The calibration set utilized in model quantization needs to reflect the inherent distribution of model. Zero-shot quantization seeks to generate synthetic calibration set that matches the models distribution [3]. The synthetic calibration set can be derived through noise optimization [3, 58, 59], which is usually instantiated by distribution approximation [3, 54]. Existing zero-shot methods in object detection typically require generating calibration set the same size as the training set (120k images for MS-COCO) [4]. In contrast, we only generate small amount of calibration set to extract features of the data. Given batch of inputs RN 3HW , where each pixel is initialized from random Gaussian noise xi,c,h,w (0, 1), and pre-trained full-precision detection network ϕ(θ), synthetic calibration set are obtained through optimizing the inputs to match the batch normalization statistics (BNS) [55] in convolutional neural networks (e.g., YOLO [49], Mask R-CNN [21]): min Lprior(x) = l= (cid:88) (µl(θ, x)µl(θ)2+σl(θ, x)σl(θ)2), (3) where µl(θ)/σl(θ) are mean/variance parameters stored in the l-th BN layer of ϕ(θ) and µl(θ, x)/σl(θ, x) are Figure 2. Overall architecture of our method. Our framework comprises: 1) constructing task-specific condensed calibration set and 2) conducting quantization-aware training with task-specific distillation. See Section 3 for details. mean/variance statistic calculated on inputs using ϕ(θ). It enforces feature similarities at all levels by minimizing the distance between the feature map statistics for the synthesized image and the real image. For vision transformer models (e.g., ViT [14], Swin Transformer [35]), since they only use layer normalization (LN) and do not store any runtime statistical information, we adopt the Patch Similarity Entropy loss (LP SE), as described in [31], as Lprior to align inputs with the original data. Besides the statistical alignment objective function, regularization term consisting of the total variance and l2 norm of the input image is always involved in the final loss function to steer images away from unrealistic images [37]: min Lreg(x) = αT LT (x) + αl2 x2 2, (4) where LT promotes similarity between adjacent pixels by minimizing their Frobenius norm, consequently enhancing the smoothness, αT and αl2 are hyper-parameters balancing the importance of two terms. Finally, we can regard task-agnostic data synthesis as regularized minimization problem and optimize the following function: min αpriorLprior(x) + Lreg(x). (5) 3.2. Stage I: Task-Specific Calibration Set Synthesis Task-Specific Loss Calculation As discussed in Section 2.2, the object detection task requires labels that include the precise location and size of objects within an image, making artificial reconstruction extremely challenging. Consequently, previous zero-shot quantization approaches for object detection commonly utilize task-agnostic loss, as described in Eq. 5, to generate images [10, 32, 48]. However, our experiments indicate that this approach leads to Figure 3. (a) Images generated by Adaptive Label Sampling on YOLOv5 detector pre-trained on MS-COCO. (b) Adaptive Label Sampling can generate category distribution frequency similar to MS-COCO in zero-shot setting. significant loss of task-specific information, ultimately resulting in degraded performance on downstream tasks. Therefore, in this section, we introduce the training loss of the object detection network to optimize the sampled inputs, aiming to recover task-specific information. Given full-precision detection network ϕ(θ) and Gaussian-initialized input x, the standard form of the training loss in classification networks is expressed as Lclassif y(ϕ(x), c), where the target label is an integer generated through random sampling. However, in object detection tasks, label information is more complex, often encompassing the objects position and size. This can be formulated as Ldetect(ϕ(x), y), which typically consists of three components: box category loss Lcategory, box dimension loss Lbox, and grid location loss Lconf . Through this formulation, we can reconstruct the category and coordinate information of bounding boxes in real images. Com4 bining with the task-agnostic loss, we derive the following task-specific data synthesis function: min αpriorLprior(x) + αdetectLdetect(ϕ(x), y) + Lreg(x). (6) Adaptive Label Sampling To obtain task-specific information in zero-shot scenario, we propose an adaptive label sampling method for label synthesis that effectively extracts essential bounding box coordinates and category information for object detection. Our approach relies solely on pre-trained network and does not require any additional information (e.g., metadata, feature activations) or extra networks (e.g., pre-trained generative models). Motivated by [56], which integrates soft labels into the data reconstruction process to better align synthetic data and labels, we propose an adaptive label sampling method, detailed as follows: We start by randomly generating label containing single object, where the bounding box coordinates and category are uniformly sampled within valid range (Details in Table 6). This label is then used as the ground truth target, and the input is optimized using Eq. 6. After fixed number of iterations, we re-detect objects in the input using pretrained full-precision detection network. High-confidence regions from the teacher networks feature map are added as new labels, while low-confidence regions are removed, ensuring that each image retains at least one label. The overall process is detailed in Algorithm 1. The input image and target labels are updated alternately, progressively aligning with each other throughout the process. We also provide visualization of this alignment in Fig. 5. With this sampling strategy, we eliminate the need for real detection labels. Besides, as presented in Fig. 3(b), the approach can eventually produce bounding box categories that closely resemble the actual distribution, while also reconstructing objects relative positions, sizes, and counts. This capability enables tackling the challenging zero-shot quantization-aware training for object detection. Task-Specific Data Synthesis Through adaptive label sampling, we obtain labels that resemble the bounding box category and coordinate information in real images. We then fix the generated labels and optimize newly Gaussian-initialized input toward these targets using the task-specific training loss introduced in Eq. 6. See Appendix B.1 for more details. Compared to randomly sampling multiple bounding box coordinates and category labels, our adaptive label sampling method effectively utilizes the knowledge embedded in pre-trained object detection networks, resulting in higherquality image synthesis. As illustrated in Fig. 3(a), images generated using our approach accurately capture object locations within the image, producing more realistic results. Further studies, as discussed in Appendix E, demonstrate that our method yields clearer objects and more coherent layouts than those generated through random sampling. 3.3. Stage II: QAT with Task-Specific Distillation In this section, we propose to minimize the knowledge gap between the full-precision pre-trained network (teacher) and the low-precision quantized network (student) through knowledge distillation. Knowledge distillation [22] is widely used technique for knowledge transfer. Previous studies [13, 29] have used it to enhance performance in classification tasks involving quantized CNNs and LLMs. However, the hidden states from backbone and prediction head of full-precision pretrained YOLO network contain much of the statistical information from real training data [55], which is challenging to fully utilize. To address this, we propose using feature-level distillation to align intermediate features and predictionmatching distillation to ensure consistency between the predictions of the quantized and pre-trained networks. Prediction-matching Distillation As proposed in Section 3.2, our synthetic calibration set {(ˆxi, ˆyi)}N i=1 is the result of the network backpropagating through pre-defined labels, directly aligning predictions of the quantized network with the targets would lead to severe over-fitting issues. Therefore, we introduce the KullbackLeibler (KL) divergence loss [28] between the predictions of the quantized network and the full-precision network as soft labels to align their outputs, thereby recovering the performance of the quantized network, which is formulated as: min θ LKD = τ 2 N (cid:88) i=1 KL(zF (ˆxi; θ), zQ(ˆxi; θ)), (7) i=1 is batch of the calibration set where { ˆxi}N images, zF (ˆxi; θ)/zQ(ˆxi; θ) are output predictions from fullprecision / quantized network and τ is the distilling temperature. We denote parameters of full-precision / quantized network as θ/θ. Feature-level Distillation We extend the knowledge transfer approach to the feature level by introducing feature distillation method that explicitly aligns intermediate features between the teacher and student. This significantly improves training stability in low-bit settings, where QAT at ultra-low bit widths often leads to rapid error accumulation. By ensuring feature consistency between the teacher and student through feature distillation, we effectively reduce error propagation throughout the training process. In the quantization-aware training stage, given batch of synthetic image {ˆxi}N i=1, we impose the mean squared error constraints between the feature maps from teachers 5 Table 1. Comparison with real data QATs on YOLOv5/YOLO11 on MS-COCO validation set. Real Data Num Data Prec. YOLOv5-s YOLOv5-m YOLOv5-l YOLO11-s YOLO11-m YOLO11-l mAP / mAP50 120k(full) FP 37.4/56.8 45.4/64. 49.0/67.3 47.0/65.0 51.5/70.0 53.4/72.5 120k(full) 120k(full) 2k 2k 2k 120k(full) 120k(full) 2k 2k 2k 120k(full) 120k(full) 2k 2k 2k W8A8 W6A6 W4A8 35.7/54.9 35.4/54.6 31.6/50.6 31.5/50.3 35.8/55.0 31.5/49.9 32.3/50.9 28.9/47.2 28.6/46.7 32.7/51. 32.2/51.0 32.3/51.1 28.1/46.5 29.3/47.8 33.0/52.5 43.2/62.2 43.3/62.4 36.5/55.6 36.6/55.8 43.6/62.3 41.3/60.0 41.3/60.3 35.0/53.9 34.2/52.6 41.0/59.7 41.0/59.9 41.2/60.1 35.8/54.6 37.8/56.9 42.6/61.7 46.0/64.9 46.3/64.9 40.3/59.1 40.1/58.6 47.3/65.6 43.3/62.1 43.4/62.3 37.7/55.7 37.5/55.8 45.1/63. 44.6/63.5 44.4/63.2 39.0/57.5 40.6/59.7 46.2/64.7 44.9/61.8 45.1/61.8 44.0/60.8 43.8/60.7 45.6/62.3 43.0/59.7 43.2/59.8 41.5/58.3 41.6/58.2 43.0/59.3 42.4/59.1 42.7/59.3 40.9/57.5 40.7/57.3 42.6/58.9 49.1/66.2 49.6/66.7 47.6/64.5 47.8/64.7 50.0/66.5 47.4/64.2 47.6/64.3 45.0/61.9 44.8/61.7 47.1/63. 47.6/64.4 47.8/64.8 45.2/62.4 45.2/62.3 47.7/64.1 50.4/67.4 50.9/67.7 48.8/65.8 48.5/65.3 51.8/68.4 48.6/65.3 48.9/65.8 45.8/62.5 45.9/62.8 48.4/64.6 48.7/65.6 49.4/66.3 46.1/63.0 46.4/63.4 49.4/65.7 LSQ LSQ+ LSQ LSQ+ Ours Method Pre-trained LSQ LSQ+ LSQ LSQ+ Ours LSQ LSQ+ LSQ LSQ+ Ours and students. With being the number of distilling network layers, the feature distillation loss Lf eat is expressed as: min θ Lf eat = 1 N (cid:88) (cid:88) i=1 l=1 l (ˆxi; θ)f l (ˆxi; θ)2 2. (8) Task-Specific Quantization-Aware Training Previous works do not explicitly incorporate task-specific loss into their QAT training objectives [10, 32, 48]. While this allows their networks to remain flexible and adaptable to various downstream tasks (e.g. instance segmentation, object classification), we find that it often compromises performance on the target task object detection in our case. To mitigate this, we introduce the task-specific training loss Ldetect during the QAT phase, enabling the quantized network to learn bounding box information directly from labels. To this end, the total loss for quantization-aware training can be summarized as: LQ = βKLLKD + βf eatLf eat + βdetectLdetect, (9) min θ where βKL, βf eat and βdetect are hyper-parameters to balance the three terms. 4. Experiments and Results In this section, we validate the proposed task-specific scheme on MS-COCO 2017 [33] and Pascal VOC [16] datasets. Following LSQ [15], we apply symmetric quantization to both weights and activations. Through extensive experiments, we demonstrate that our method is effective Figure 4. Visualization of images generated by YOLO11 and Transformer-backbone Mask R-CNN. More examples can be found in Appendix E. on various architectures, including the YOLOv5 [51] series, YOLO11 [24] series, CNN-based Mask R-CNN [21], as well as Transformer-based Mask R-CNN [35] for object detection tasks. We further compare our approach with standard quantization-aware training baselines, includImpleing LSQ [15] and LSQ+ [2], using real data. mentation details are provided in Appendix B, and ablation studies on different settings and components are presented in Appendix C. We also present visualizations of images generated by various models, including YOLO11 and Transformer-based Mask R-CNN, as shown in Fig. 4. More images and further analysis are provided in Appendix E. 6 4.1. Comparison with YOLO Networks For YOLO object detection networks, we conduct experiments using the widely adopted YOLOv5-m/s/l models as well as the latest YOLO11-s/m/l. As baselines, we include competitive real-data QAT methods such as LSQ [15] and LSQ+ [2]. Extensive experiments show that our method outperforms both LSQ and LSQ+, which rely on full realimage training, by using only small amount of ground truth label information. We compare our performance against LSQ and LSQ+, both trained on 120k real images from the MS-COCO dataset. In contrast, our approach utilizes only 2k ground truth labels for calibration set generation. The results are summarized in Table 1, with further details on the performance of the YOLOv5 series models at lower precision provided in Appendix C.5. Bit-width Our method demonstrates strong performance across different bit-widths. Notably, when quantizing both weights and activation parameters to 8-bit precision, we find that our approach outperforms full-data LSQ/LSQ+ across all YOLOv5 and YOLO11 models by 0.3%1.0%. Even when the quantization precision is further reduced to 6-bit or lower, our method still achieves results comparable to full-data LSQ/LSQ+, while significantly surpassing LSQ/LSQ+ with the same data amount by 2%6%. Network Size Larger networks tend to exhibit poor performance with existing quantization-aware training methods, particularly in low-bit-width cases. For instance, in the 6-bit case, LSQ+ applies to YOLOv5-s resulting in 5.1% decrease in mAP compared to the pre-trained network, which achieves 5.6% with YOLOv5-l. In contrast, our approach yields only 4.7% gap in mAP when quantizing YOLOv5-s to 6-bit precision, and the difference further reduces to 3.9% with YOLOv5-l. Efficiency Our method performs quantization-aware training using condensed synthetic detection calibration set, just 1/60 the size of the original, yet achieves superior performance compared to traditional QAT methods that require the full training dataset. This enables more efficient training, reducing computational costs and time while producing higher-quality low-precision models. 4.2. Comparison with Mask R-CNN Networks CNN-Backbone Mask R-CNN We further conduct experiments on Mask R-CNN with CNN backbone. Results in Table 2 compare LSQ baselines trained on 120k/5k real samples from the MS-COCO/Pascal VOC datasets with our method, which uses only 2k/50 synthetic samples. As presented, we achieve state-of-the-art results on both VOC and MS-COCO datasets, surpassing LSQ trained with full real data at 8-bit width. Specifically, on the smaller VOC dataset, our method surpasses LSQ trained with the entire dataset by 0.5% while using only 1/100 of the trainTable 2. Comparison with real data QATs on CNN-based Mask R-CNN. Real Data Num Data Precision mAP Dataset VOC MS-COCO Method Pre-trained LSQ LSQ Ours Pre-trained LSQ LSQ Ours LSQ LSQ Ours 5k(full) 5k(full) 50 50 FP32 W8A8 120k(full) FP 120k(full) 2k 2k 120k(full) 2k 2k W8A8 W4A8 75.6 72.4 70.9 72. 38.1 35.0 32.9 35.2 34.6 32.3 34.6 Table 3. Comparison with real data QATs on Transformer-based Mask R-CNN on MS-COCO validation set. Real Data Num Data mAP / mAP Prec. Swin-T Swin-S LSQ LSQ Ours Method Pre-trained LSQ LSQ Ours LSQ LSQ Ours 120k(full) FP 46.0/68.1 48.5/70.2 120k(full) 2k 2k 120k(full) 2k 2k 120k(full) 2k 2k W8A8 W6A6 W4A8 45.9/68.0 44.4/65.9 45.1/66. 44.7/66.8 41.2/62.9 42.0/63.0 45.5/64.7 43.3/65.2 43.0/64.2 48.1/69.7 47.0/68.6 47.1/68.8 47.1/68.8 44.4/65.9 45.1/65.8 47.8/69.4 45.9/67.3 46.2/67.1 ing data, and exceeds LSQ trained on similar dataset size by 2%. On the larger MS-COCO dataset, our approach outperforms LSQ trained with the full dataset by 0.2% using only 1/60 of the training data, and surpasses LSQ trained on comparable dataset size by 2.3%. These findings highlight the robustness and strong generalization ability of our method across datasets of varying scales. Transformer-Backbone Mask R-CNN In this section, we validate the proposed method on Transformer-based object detection networks. Specifically, we use Swin-T/S [35] as the backbone model, combined with the Mask R-CNN prediction head to form our model. The results on the MSCOCO dataset are presented in Table 3. Our method can be seamlessly extended to Transformerbased object detection networks. Compared to LSQ trained with the same amount of data, our approach consistently outperforms it by 0.3%0.8% across different bit-widths. While our method shows slight performance drop of 0.8%2.7% compared to LSQ trained on the full dataset, it significantly improves QAT efficiency by requiring substantially less data. 7 Table 4. Comparison with Task-Agnostic Methods on MS-COCO validation set. Ldetect represents the task-specific training loss. All methods use 2k synthetic images for QAT. mAP / mAP50 Method Prec. YOLO11-s YOLO11-m Swin-T Swin-S Pre-trained FP 47.0/65.0 51.5/70.0 46.0/68.1 48.5/70.2 Ours w/o Ldetect Ours w/o Ldetect Ours w/o Ldetect W8A8 W4A W6A6 45.6/62.3 43.6/60.2 42.6/58.9 39.7/56.0 43.0/59.3 40.7/57.0 50.0/66.5 49.8/66.7 47.7/64.1 47.3/64. 47.1/63.2 46.6/63.2 45.1/66.7 44.6/66.8 43.0/64.2 43.0/64.8 42.0/63.0 40.4/61.7 47.1/68.8 45.1/67.1 46.2/67.1 43.6/65. 45.1/65.8 42.8/64.6 Table 5. Comparison with Data free Methods on MS-COCO validation set. All methods use 2k synthetic images for W6A6 QAT on YOLOv5-s. Info. denotes detailed information about labels including bouding box categories and coordinates. Distri. represents quantity distribution information about the labels per image. - indicates that the network diverges. Method Real Label Info. Distri. mAP mAP50 32.7 51.4 Gaussian noise Tile(Out-of-distri.) Tile(In-distri.) MultiSample(Out-of-distri.) MultiSample(In-distri.) Ours(Adaptive Label Sampling) - 23.9 24.0 28.2 29.7 32. - 39.0 39.3 46.7 48.0 50.0 4.3. Comparison with Task-Agnostic Methods Previous zero-shot quantization works in object detection primarily employ task-agnostic training methods for both image generation and quantization-aware training [10, 32, 48]. We are the first to introduce task-specific loss at both stages. Through extensive experiments, as shown in the Table 4, we demonstrate that incorporating task-specific loss improves detection performance across various models, including YOLO11-s/m and Swin-T/S, as well as across different quantization levels, such as 6-bit and 8-bit precision. This improvement is due to two key factors. First, taskspecific training loss enriches the image generation process with detailed information, such as bounding box categories, size and coordinates, resulting in distribution that more closely resembles real images. Second, during quantization-aware training, it enables the model to learn directly from labels, enhancing its ability to extract meaningful information from images. 4.4. Comparison with Data Free Methods Furthermore, we explore completely data-free scenario where no information about real images or labels is available and demonstrate the robustness of our adaptive label 8 sampling method. The results are presented in Table 5. First, we establish weak baseline by using Gaussian noise as targets for QAT training and find that the quantized network fails to converge. This highlights the importance of synthetic data quality for QAT. For other proxy datasets, we primarily consider two types: in-distribution and out-ofdistribution. In-distribution datasets assume that the number of bounding boxes per image is known, making the generated images statistically closer to real ones. In contrast, out-of-distribution datasets assume no prior knowledge about the original labels. For baseline methods, we include Tile, which divides the image into uniform grids and randomly generates unique bounding box in each grid, including its category and coordinate information, and MultiSample, which randomly samples multiple labels for each image. As shown in Table 5, QAT using images generated by our adaptive label sampling method surpasses the best in-distribution proxy dataset by 2.3% at 6-bit precision. Furthermore, when comparing our sampling method with images generated using real labels, we observe only 0.7% performance gap. This demonstrates that our adaptive label sampling method can effectively extract real label information from the network, thus enabling the generation of high-quality synthetic data. Additional results across different precision levels are also provided in Appendix C.4. 5. Conclusions For the first time, we revisit the current task-agnostic zeroshot quantization (ZSQ) methods for object detection tasks and identify the inherent limitations in their performance due to their task-agnostic nature. we propose novel zeroshot quantization framework specifically tailored for object detection. The proposed framework consists of two key components: novel task-specific synthesis process for generating the calibration set and task-specific distillation process. The task-specific synthesis process leverages bounding box and category sampling strategy to extract more relevant information from the original model, while the task-specific distillation process utilizes this information to fine-tune the quantized model, thereby significantly enhancing its performance. Extensive experiments demonstrate that our proposed method is both efficient and accurate, achieving performance comparable to traditional quantization-aware training (QAT) methods, such as Learned Step Size Quantization (LSQ), which rely on full real data, while significantly outperforming task-agnostic counterparts. This empowers zero-shot quantization with immense practical significance for object detection tasks."
        },
        {
            "title": "References",
            "content": "[1] Abhishek Balasubramaniam and Sudeep Pasricha. Object detection in autonomous vehicles: Status and open challenges. arXiv preprint arXiv:2201.07706, 2022. 1 [2] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 696 697, 2020. 6, 7, 12 [3] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Zeroq: novel In Proceedings of the zero shot quantization framework. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1316913178, 2020. 1, 3, 14, 15 [4] Akshay Chawla, Hongxu Yin, Pavlo Molchanov, and Jose Alvarez. Data-free knowledge distillation for object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 32893298, 2021. 3 [5] Xinrui Chen, Renao Yan, Junru Cheng, Yizhi Wang, Yuqiu Fu, Yi Chen, Tian Guan, and Yonghong He. Adeq: Adaptive diversity enhancement for zero-shot quantization. In International Conference on Neural Information Processing, pages 5364. Springer, 2023. 12 [6] Xinrui Chen, Yizhi Wang, Renao Yan, Yiqing Liu, Tian Guan, and Yonghong He. Texq: Zero-shot network quantization with texture feature distribution calibration. Advances in Neural Information Processing Systems, 36, 2024. 1, 3, [7] YH Chen, TJ Yang, Emer, and Sze Eyeriss. v2: flexible accelerator for emerging deep neural networks on mobile devices., 2019, 9. DOI: https://doi. org/10.1109/JETCAS, pages 292308, 2019. 1, 2 [8] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Pact: Parameterized clipping activaGopalakrishnan. arXiv preprint tion for quantized neural networks. arXiv:1805.06085, 2018. 12 [9] Kanghyun Choi, Deokki Hong, Noseong Park, Youngsok Kim, and Jinho Lee. Qimera: Data-free quantization with synthetic boundary supporting samples. Advances in Neural Information Processing Systems, 34:1483514847, 2021. 3 [10] Kanghyun Choi, Hye Yoon Lee, Dain Kwon, SunJong Park, Kyuyeun Kim, Noseong Park, and Jinho Lee. Mimiq: Lowbit data-free quantization of vision transformers with enarXiv preprint couraging inter-head attention similarity. arXiv:2407.20021, 2024. 2, 4, 6, 8, 12 [11] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware acceleration for neural networks: comprehensive survey. Proceedings of the IEEE, 108(4):485532, 2020. 1, 2 [12] Terrance DeVries and Graham Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 13 [13] Xin Ding, Xiaoyu Liu, Yun Zhang, Zhijun Tu, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, et al. Cbq: Cross-block quantization for large language models. arXiv preprint arXiv:2312.07950, 2023. 5 [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 4 [15] Steven Esser, Jeffrey McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra Modha. preprint Learned quantization. size arXiv:1902.08153, 2019. 3, 6, 7, 12, 13 arXiv step [16] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object International journal of computer classes (voc) challenge. vision, 88:303338, 2010. 6, [17] Alex Finkelstein, Ella Fuchs, Idan Tal, Mark Grobman, Niv Vosco, and Eldad Meller. Qft: Post-training quantization via fast joint finetuning of all degrees of freedom. In Computer VisionECCV 2022 Workshops: Tel Aviv, Israel, October 23 27, 2022, Proceedings, Part VII, pages 115129. Springer, 2023. 12 [18] Elias Frantar and Dan Alistarh. Optimal brain compression: framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:44754488, 2022. 12 [19] Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, and Minyi Guo. Squant: On-the-fly data-free quantization via diagonal hessian approximation. arXiv preprint arXiv:2202.07471, 2022. 12 [20] Song Han, Huizi Mao, and William Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. 1, 2 [21] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 3, 6 [22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. DistillarXiv preprint ing the knowledge in neural network. arXiv:1503.02531, 2015. 5 [23] Yongkweon Jeon, Chungman Lee, and Ho-young Kim. Genie: show me the data for quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1206412073, 2023. 14, 15 [24] Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, 2023. 6 [25] Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 43504359, 2019. 12 [26] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: whitepaper. arXiv preprint arXiv:1806.08342, 2018. 9 [27] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: whitepaper. arXiv preprint arXiv:1806.08342, 2018. 12 [28] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1): 7986, 1951. 5 [29] Huantong Li, Xiangmiao Wu, Fanbing Lv, Daihai Liao, Thomas Li, Yonggang Zhang, Bo Han, and Mingkui Tan. Hard sample matters lot in zero-shot quantization. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 2441724426, 2023. 5, 12 [30] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. 12 [31] Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Patch similarity aware data-free quantization for In European conference on computer vision transformers. vision, pages 154170. Springer, 2022. 1, [32] Zhikai Li, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Psaq-vit v2: Toward accurate and general data-free quantization for vision transformers. IEEE Transactions on Neural Networks and Learning Systems, 2023. 2, 4, 6, 8, 12 [33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6, 12 [34] Yuang Liu, Wei Zhang, and Jun Wang. Zero-shot adversarial quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512 1521, 2021. 1 [35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 4, 6, 7 [36] Ilya Loshchilov and Frank Hutter. tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 13 Sgdr: StochasarXiv preprint [37] Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 51885196, 2015. 4 [38] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. 3d object detection for autonomous driving: International Journal of Computer comprehensive survey. Vision, 131(8):19091963, 2023. 1 [39] Pawan Kumar Mishra and GP Saroha. study on video surveillance system for object detection and tracking. In 2016 3rd international conference on computing for sustainable global development (INDIACom), pages 221226. IEEE, 2016. 1 [40] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325 1334, 2019. [41] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1325 1334, 2019. 12 [42] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 71977206. PMLR, 2020. 12 [43] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. arXiv white paper on neural network quantization. preprint arXiv:2106.08295, 2021. 1 [44] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. arXiv white paper on neural network quantization. preprint arXiv:2106.08295, 2021. 12 [45] Kanyifeechukwu Jane Oguine, Ozioma Collins Oguine, and Hashim Ibrahim Bisallah. Yolo v3: Visual and real-time object detection model for smart surveillance systems (3s). In 2022 5th Information Technology for Education and Development (ITED), pages 18. IEEE, 2022. 1 [46] Biao Qian, Yang Wang, Richang Hong, and Meng Wang. In Proceedings of the Adaptive data-free quantization. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79607968, 2023. [47] Biao Qian, Yang Wang, Richang Hong, and Meng Wang. Rethinking data-free quantization as zero-sum game. In Proceedings of the AAAI conference on artificial intelligence, pages 94899497, 2023. 3 [48] Akshat Ramachandran, Souvik Kundu, and Tushar Krishna. Clamp-vit: Contrastive data-free learning for adaptive postIn European Conference on training quantization of vits. Computer Vision, pages 307325. Springer, 2024. 2, 4, 6, 8, 12 [49] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 3 [50] Xu Shoukai, Li Haokun, Zhuang Bohan, Liu Jing, Cao Jiezhang, Liang Chuangrun, and Tan Mingkui. Generative low-bitwidth data free quantization. In The European Conference on Computer Vision, pages 117. Springer, 2020. 1, 12 [51] Ultralytics. Yolov5: state-of-the-art real-time object detection system. 2021. 6 [52] Huan Wang, Suhas Lohit, Michael Jones, and Yun Fu. What makes good data augmentation in knowledge distillation-a statistical perspective. Advances in Neural Information Processing Systems, 35:1345613469, 2022. 1, [53] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740, 2022. 12 10 [54] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative lowbitwidth data free quantization. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part XII 16, pages 117. Springer, 2020. 1, 3 [55] Hongxu Yin, Pavlo Molchanov, Jose Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepIn Proceedings of the IEEE/CVF Conference inversion. on Computer Vision and Pattern Recognition, pages 8715 8724, 2020. 3, 5 [56] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from new perspective. Advances in Neural Information Processing Systems, 36, 2024. 5 [57] Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static input quanIn Proceedings of the IEEE/CVF Winter Confertization. ence on Applications of Computer Vision, pages 38693878, 2023. [58] Xiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, and Xianglong Liu. Diversifying sample generation for accurate data-free quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1565815667, 2021. 3 [59] Yunshan Zhong, Mingbao Lin, Gongrui Nan, Jianzhuang Liu, Baochang Zhang, Yonghong Tian, and Rongrong Ji. Intraq: Learning synthetic images with intra-class heterogeneIn Proceedings of ity for zero-shot network quantization. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1233912348, 2022. 3 11 A. Related Works This section provides brief overview of the studies relevant to our work, focusing on data-driven quantization and zero-shot quantization. Data-driven Quantization Post-training quantization (PTQ) and quantization-aware training (QAT) [27, 44] are the most commonly employed quantization methods. PTQ methods typically utilize small calibration set, often subset of the training data, to optimize or fine-tune quantized networks [17, 18]. For instance, AdaRound [42] introduced layer-wise adaptive rounding strategy, challenging the quantizers of rounding to the nearest value. Additionally, BRECQ [30] implemented block-wise and stage-wise reconstruction techniques, striking balance between layer-wise and network-wise approaches. QDrop [53] innovatively proposed randomly dropping activation quantization during block construction to achieve more uniformly optimized weights. Despite their simplicity and minimal data requirements, PTQ methods often face challenges related to local optima due to the limited calibration set available for fine-tuning. On the other hand, most QAT approaches leverage the entire training dataset to quantize networks during the training process [25]. PACT [8] introduced parameterized clipping activation technique to optimize the activation clipping parameter dynamically during training, thereby determining the appropriate quantization scale. LSQ [15] proposed estimating the loss gradient of the quantizers step size and learning the scale parameters alongside other network parameters. LSQ+ [2], an extension of the LSQ method, introduced versatile asymmetric quantization scheme with trainable scale and offset parameters capable of adapting to negative activations. Both QAT and PTQ methods rely on training data for quantization, rendering them impractical when faced with privacy or confidentiality constraints on the training data. Zero-shot Quantization Zero-Shot Quantization (ZSQ) is valuable approach that eliminates access to real training data during the quantization process. Presently, most ZSQ research is confined to classification tasks. Data-free quantization (DFQ) represents subset of ZSQ methods that enable quantization without relying on any data. For instance, DFQ [41] introduced scale-equivariance property of activation functions to normalize the weight ranges across the network. SQuant [19] developed an efficient data-free quantization algorithm that does not involve back-propagation, utilizing diagonal Hessian approximation. However, due to the absence of data, DFQ methods may not be suitable for low-bit-width configurations. For example, in the case of 4-bit MobileNet-V1 on ImageNet, SQuant achieved only 10.32% top-1 accuracy. Another branch of ZSQ methods leverages synthetic data [5, 29] generated by the fullprecision network. GDFQ [50] introduced knowledgematching generator to synthesize label-oriented data using cross-entropy loss and batch normalization statistics (BNS) alignment. TexQ [6] emphasized the detailed texture feature distribution in real samples and devised texture calibration for data generation. Recently, the latest works extended ZSQ to more downstream tasks including object detection. PSAQ-ViT V2 [32] introduce an adaptive teacherstudent strategy to the patch similarity metric in PSAQ-ViT and generate task-agnostic images to fine-tune the quantized model with knowledge distillation for segmentation and object detection. Similarly, MimiQ [10] proposed inter-shead attention similarity and apply head-wise structural attention distillation to align the attention maps of the quantized network to those of the full-precision teacher across downstream tasks. CLAMP-ViT [48] employed two-stage approach, cyclically adapting between data generation and model quantization for classification and object detection tasks. However, the synthetic images they use in downstream tasks are task-agnostic, without containing the specific information required for the corresponding task. Our work validates task-specific image lifting performance of quantized model, yielding state-of-the-art results in ZSQ for object detection. B. Implementation Details We report mAP and mAP50 on the validation sets of MSCOCO 2017 [33] and Pascal VOC [16] for the object detection task. Our replication experiments not only contain one-stage YOLO networks such as classic YOLOv5 and recent YOLO11 networks, but also include two-stage Mask R-CNN networks with both ResNet and Transformer-based backbones. All experiments are conducted using pretrained model as the teacher. The YOLOv5/YOLO11 experiments are executed on two NVIDIA GeForce RTX 4090 GPUs, while the Mask R-CNN/ViT experiments are conducted on 4 and 8 GPUs, respectively. B.1. Adaptive Label Sampling While theoretically merging labels and image updates into single stage seems feasible, our experiments in Section C.1 indicate that continuously evolving target can negatively impact the quality of the final generated images. To address this issue, we first conduct rapid adaptive label sampling at low resolution (160) and then use the fixed labels to generate high-resolution images (640). We also provide details of the initial label random generation in adaptive label sampling in Table 6, and an outline of the overall algorithm in Algorithm 1. Fig. 5 provides visual representation of the algorithms process. 12 Table 6. Bounding box sampling details: we start by sampling one object for each image, where represents the number of categories. We assume that the relative width and height of the image are both 1. Wmin and Hmin are set to 0.2, while Wmax and Hmax are set to 0.8. denotes uniform distribution."
        },
        {
            "title": "Sampling Distribution Description",
            "content": "Y[i,0] Y[i,1] Y[i,2] Y[i,3] Y[i,4] Y[i,5] - U(0, C) U(W/2, 1 W/2) U(H/2, 1 H/2) U(Wmin, Wmax) U(Hmin, Hmax) Batch index Category Bounding box x-center Bounding box y-center Bounding box width Bounding box height Algorithm 1 Adaptive Label Sampling Algorithm Input: current stage image and labels {img, tgts}, pre-trained detection network teacher, filtering threshold: confidence conf thresh, iou iou thresh 1. new tgts = teacher(img).predictions[conf > conf thresh] 2. ious = IOU(new tgts, tgts) # Add labels that do not overlap with the existing labels. 3. add tgts = new tgts[(max(ious, dim = 1) < iou thresh)] # Remove labels from the existing list that are not detected by teacher. 4. minus tgts = (max(ious, dim = 0) < iou thresh).bool() 5. tgts = tgts[ minus tgts] 6. tgts = cat([tgts, add tgts], dim = 0) B.2. Calibration Set Generation We apply Eq. 6 and set the optimal trade-off parameters for {αdetect, αBN , αT , αl2} as {0.5, 0.01, 0, 5e-4} for the YOLOv5 series model, {1e-3, 1e-3, 0, 5e-5} for the YOLO11 series model, {5.0, 2e-3, 0, 1.5e-5} for CNN-backbone Mask R-CNN model and {10.0, 1.0, 0, 1e-3} for Transformer-backbone Mask R-CNN model. We generate Xinv by optimizing the cost function for 2500/3000/8000/4000 iterations for YOLOv5 series model, YOLO11 series model, CNN-backbone Mask R-CNN model and Transformer-backbone Mask R-CNN model respectively. We use Adam as the optimizer with an initial learning rate of 1e-2, adjusted by cosine annealing [36]. We also use cutout [12] as data augmentation method to enhance the diversity of the synthetic calibration set. B.3. Quantization Aware Training Since the original LSQ is only evaluated in classification tasks on ImageNet, we extended it to object detection tasks. For each of our networks, LSQ is attached to all internal layers except the first and last layers following [15]. Our training data are from the synthesized calibration set aforementioned. During QAT , we use per-tensor symmetric quantization for both activations and weights and learn the quantization scaling/bias factor via backpropagation with the Adam optimizer. The learning rate for YOLOv5, YOLO11, CNN-backbone Mask R-CNN and Transformer-backbone Mask R-CNN are 1e-4, 1e-5, 1e-4 and 1e-6 respectively. Other experimental hyper-parameters follow official implementations. We use Eq. 9 as our loss function, with optimized hyper-parameters for {βdetect, βKL, βf eat} being {0.04, 0.1, 1.0} for the YOLOv5 series model, { 0.01, 0.1, 1.0 } for the YOLO11 series model, {0.04, 0.2, 0.1 } for CNN-backbone Mask R-CNN model, and {1.0, 1.0, 1.0} for Transformer-backbone Mask R-CNN model. After training, we report mAP/mAP50 as our results. C. Ablation Study C.1. Adaptive Label Sampling Stage We conduct ablations on the impact of the sampling stage number for the YOLOv5-s model, results are shown in Table 7. Overall, the two-stage sampling strategy outperforms the one-stage strategy, which we attribute to the continuous variation of targets causing fluctuation in the regression targets of the image, thus hindering stable convergence. It also matches the performance of the three-stage approach. Ultimately, we opt for the two-stage strategy to strike balance between performance and cost. Table 7. Ablations on Adaptive Label Sampling stages number. One stage: update images and labels simultaneously in one process. Two stages: Sample out labels first, then synthesize images with fixed labels. Three stages: Generate images with one random label first, then sample out labels with fixed images, and finally synthesize images with fixed labels Stages Num Precision mAP mAP50 One Two Three W6A6 W5A5 W4A4 W6A6 W5A5 W4A4 W6A6 W5A5 W4A4 30.6 25.2 16.0 32.1 26.3 15.8 31.7 26.1 15. 48.8 41.1 27.9 50.1 42.3 28.1 49.3 42.5 27.8 C.2. Calibration Set Size After hyper-parameters are fixed, the calibration set size is searched for its optimal trade-off between computation cost and effectiveness with grid search by quantizing YOLOv5-s to 4-8 bits, as displayed in Table 8. When reaches 2k, the performance of the quantized network approaches convergence. Further increasing the size will lead to increased data generation time and computational costs. Figure 5. An overview of Adaptive Label Sampling process. We randomly initialize the label and initialize the input image using Gaussian noise. For Every fixed interval, we use pre-trained object detection model to re-detect objects in and update the target y. In the subsequent iterations, is optimized toward the updated target y. We observe that, over iterations, and become increasingly aligned with each other. To avoid complex searches, the same is used for all experiments. While this may not be optimal for all networks, it is sufficient to demonstrate the superiority of our approach. C.3. Modules Ablation on key modules of the QAT stage including LKD (Kullback-Leibler Divergence), Ldetect , and Lf eat is conducted. As presented in Table 9, dropping one or two of them results in mAP loss. The largest mAP loss (7.2%) occurs when both LKD and Lf eat are removed, indicating their cooperative relationship: Lf eat constrains features of network layers, facilitating LKD to align the networks predictions with the targets. C.4. Full Comparison with Data-Free Methods In this section, we present comparison of our adaptive label sampling method with other baseline methods under the data-free scenario across multiple precision levels. From the results in Table 10, we observe that despite being restricted from accessing specific real label information or distribution details, our method consistently outperforms other data-free approaches. Moreover, it achieves results comparable to those obtained using real labels across different precision levels. C.5. Comparison with YOLOv5 at Lower Precision In Table 11, we provide additional performance results of the YOLOv5 series models on the MS-COCO dataset at lower precision levels. Notably, even at ultra-low 4-bit precision, our method still outperforms LSQ trained with full data in most cases. For example, on YOLOv5-l, our approach surpasses the best baseline by 1.7%, with the gap further increasing to 6.3% at 5-bit precision. C.6. Comparison with Other ZSQ methods In this section, we compare our approach with two widelyused zero-shot quantization (ZSQ) methods, Genie [23] and ZeroQ [3], to highlight the impact of incorporating taskspecific information. Notably, both baselines are taskagnostic and therefore lack essential knowledge specific to object detection. As shown in Table 12, task-specific information plays critical role in quantization-aware training (QAT). While all methods perform comparably under W4A8 post-training quantization (PTQ), our method achieves significant 3.3% improvement in mAP after QAT, outperforming all task-agnostic baselines. 14 Table 8. detailed analysis of calibration set size across different bit widths using YOLOv5-s on MS-COCO validation set. Method Real Data"
        },
        {
            "title": "LSQ",
            "content": "120k (Full)"
        },
        {
            "title": "Ours",
            "content": "5k 4k 3k 2k 1k mAP W4A4 W5A5 W6A6 W7A7 W8A8 23.3 19.1 18.9 19.2 19.0 18. 26.9 28.0 27.9 27.9 27.4 27.8 31.5 32.6 32.8 32.7 32.7 32.6 33.4 34.9 34.7 35.0 34.7 34. 35.7 35.7 35.8 36.0 35.4 35.6 Table 9. Ablations on modules. We use 2k calibration set and report mAP/mAP50 of 4-bit YOLOv5-s on MS-COCO validation set. (Ours) Table 12. The validity of task-specific information. mAP/mAP50 of YOLOv11s model on the MS-COCO validation set is reported. Experiments are conducted under the same PTQ or QAT settings, with 512 images from different ZSQ methods. Lf eat LKD Ldetect mAP mAP50 19.0 16.8 11.8 33.4 30.1 21.5 Table 10. Comparison with Data free Methods on MS-COCO validation set across multiple precision levels. All methods utilize 2k synthetic images for QAT on YOLOv5-s. Prec. Method Real Label Real label Data distri. mAP mAP50 28.0 45.8 W5A W4A4 Gaussian noise Tile(Out-of-distri.) Tile(In-distri.) MultiSample(Out-of-distri.) MultiSample(In-distri.) Ours(Adaptive Label Sampling) Real Label Gaussian noise Tile(Out-of-distri.) Tile(In-distri.) MultiSample(Out-of-distri.) MultiSample(In-distri.) Ours(Adaptive Label Sampling) - 16.1 17.7 21.9 22.5 26.1 19.0 - 5.4 6.8 11.9 13.1 15.0 - 27.9 31.0 37.3 37.4 42. 33.4 - 11.1 13.4 22.3 23.3 27.0 Table 11. Comparison with real data QATs on YOLOv5 on MSCOCO validation set. Real Data Num Data mAP / mAP50 Prec. YOLOv5-s YOLOv5-m YOLOv5-l LSQ LSQ+ LSQ LSQ+ Ours Method Pre-trained LSQ LSQ+ LSQ LSQ+ Ours 120k(full) FP 37.4/56.8 45.4/64.1 49.0/67.3 120k(full) 120k(full) 2k 2k 2k 120k(full) 120k(full) 2k 2k 2k W5A W4A4 26.9/44.9 27.0/44.9 24.7/42.2 25.0/42.9 28.0/45.8 23.3/40.0 23.3/40.2 17.2/32.2 17.3/32.1 19.0/33.4 32.9/50.6 33.1/51.0 31.2/49.3 31.2/49.2 37.1/55.7 27.9/45.4 27.7/44.6 25.5/42.3 26.1/42.6 29.5/47.1 35.2/53.0 35.2/53.4 35.2/53.1 34.8/52.7 41.5/59. 33.1/50.3 33.3/50.9 28.9/45.7 28.6/45.8 35.0/52.6 15 Precision Quantizer setting Genie [23] ZeroQ [3] Ours W8A W6A6 W4A8 PTQ QAT PTQ QAT PTQ QAT 45.8/62.3 39.8/54. 39.7/54.9 36.9/51.9 11.2/18.0 34.6/49.1 46.0/62.5 43.9/60.4 40.1/55.5 39.5/55.4 11.3/18.3 37.9/53.9 46.0/62.7 45.9/62.2 40.3/55.8 42.8/59.2 11.2/18.2 41.2/57. D. Sample Efficiency We also demonstrate that by employing Adaptive Label Sampling, we achieved comparable or even superior results on QAT using synthetic calibration set that is only 1/60 the size of the original training dataset. Additionally, by integrating self-distillation into the fine-tuning process of the quantized object detection network, we enabled more efficient knowledge transfer. In the initial stage, utilizing 8 RTX 4090 GPUs for image generation allow us to produce 256 images every 20 minutes, resulting in total of 160 minutes to generate 2,000 images. It is important to note that the calibration set we generate captures the overall characteristics of the original training set, allowing it to be reused multiple times during the quantizationaware training process. As the number of training iterations increases, our method progressively enhances the training convergence speed, achieving up to 16 faster convergence compared to the LSQ method trained on the entire real dataset. The corresponding results are visually illustrated in Fig. 6. E. Additional Qualitative Results Visualization for different object detection models In this section, we present visualizations of images generated by all the models discussed in this paper, including YOLOv5, YOLO11, as well as CNN and Transformerthe generated objects. Consequently, compared to multilabel sampling, our Adaptive Label Sampling method captures the models internal information more effectively, producing higher-quality and more coherent images. Qualitative results for object detection performance In this section, we present visualizations illustrating the object detection capabilities of various neural networks. Specifically, we randomly selected four images from the MSCOCO validation set and used the detection results of fullprecision YOLOv5-s network as the reference. The visual comparisons in Fig. 9 display the detection results of neural networks trained with our adaptive label sampling method under 4-bit quantization-aware training (QAT). The visualizations demonstrate that our quantized network can effectively detect objects. For example, when multiple teddy bears are present in an image, it accurately identifies each one. Similarly, when there is only single object, such as bear, it correctly recognizes it with confidence levels comparable to those of the full-precision network. Figure 6. (a) Our synthetic condensed calibration set is 1/60 the size of the MS-COCO training set. (b) The training convergence speed can be improved by up to 16 compared to LSQ. backbone Mask R-CNN, as shown in Fig. 7. As observed, despite initializing the images with Gaussian noise and without referencing any real image data, our generated images can still accurately restore the real-world positions and sizes of objects. For instance, in the images generated by YOLOv5, ballet dancer can be seen gracefully performing. In the images produced by YOLO11, there is person sitting on bench waiting for someone, as well as table with several pizzas on it. In the CNNbackbone Mask R-CNN generated images, herdsman is riding horse while giraffe roams aimlessly. Meanwhile, in the Transformer-backbone Mask R-CNN generated images, television is displaying program. By leveraging task-specific image generation, we can create more realistic images across different network architectures without relying on any external support. Qualitative results for synthetic data In this section, we visualize the advantage of our Adaptive Label Sampling method over random sampling for multiple labels. As shown in Fig. 8, the left side illustrates our Adaptive Label Sampling method, which initially starts with singlelabel random sampling as presented in Table 6. After Adaptive Label Sampling, the model leverages the information stored during pre-training and add objects it considers highly confident, ultimately producing high-quality images. For instance, you can observe person riding horse, three boats gently floating on the shimmering water, and someone about to sit and rest next to couch, among other realistic scenes. Next, we use the obtained labels to perform multi-label random sampling by generating the corresponding object sizes and locations based on the sampling distribution in Table 6. The resulting images are shown on the right side of Fig. 8. In this scenario, the image quality deteriorates significantly, and the visual features fail to accurately reflect 16 Figure 7. Visualization of images composed by different architecture-based object recognition networks. 17 Figure 8. comparison of the image quality generated by various sampling methods. Figure 9. Qualitative analysis of object detection performance across different neural networks"
        }
    ],
    "affiliations": [
        "Dept. of Comp. Sci. and Tech., Institute for AI, Tsinghua-Bosch Joint ML Center, Tsinghua University",
        "School of Computational Science and Engineering, Georgia Institute of Technology",
        "School of Software, Tsinghua University",
        "Shenzhen International Graduate School, Tsinghua University"
    ]
}