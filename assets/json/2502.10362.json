{
    "paper_title": "CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages",
    "authors": [
        "Shangda Wu",
        "Zhancheng Guo",
        "Ruibin Yuan",
        "Junyan Jiang",
        "Seungheon Doh",
        "Gus Xia",
        "Juhan Nam",
        "Xiaobing Li",
        "Feng Yu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "CLaMP 3 is a unified framework developed to address challenges of cross-modal and cross-lingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalities--including sheet music, performance signals, and audio recordings--with multilingual text in a shared representation space, enabling retrieval across unaligned modalities with text as a bridge. It features a multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, a web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents a wide array of global musical traditions. To advance future research, we release WikiMT-X, a benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts."
        },
        {
            "title": "Start",
            "content": "CLaMP 3: Universal Music Information Retrieval Across Unaligned Modalities and Unseen Languages"
        },
        {
            "title": "Feng Yu",
            "content": "Details of authors, correspondence, and affiliations are on Page 9 https://sanderwood.github.io/clamp3 5 2 0 2 4 1 ] . [ 1 2 6 3 0 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "CLaMP 3 is unified framework developed to address challenges of cross-modal and crosslingual generalization in music information retrieval. Using contrastive learning, it aligns all major music modalitiesincluding sheet music, performance signals, and audio recordings with multilingual text in shared representation space, enabling retrieval across unaligned modalities with text as bridge. It features multilingual text encoder adaptable to unseen languages, exhibiting strong cross-lingual generalization. Leveraging retrieval-augmented generation, we curated M4-RAG, web-scale dataset consisting of 2.31 million music-text pairs. This dataset is enriched with detailed metadata that represents wide array of global musical traditions. To advance future research, we release WikiMT-X, benchmark comprising 1,000 triplets of sheet music, audio, and richly varied text descriptions. Experiments show that CLaMP 3 achieves state-of-the-art performance on multiple MIR tasks, significantly surpassing previous strong baselines and demonstrating excellent generalization in multimodal and multilingual music contexts."
        },
        {
            "title": "Introduction",
            "content": "Music Information Retrieval (MIR) is field that aims at developing computational tools for processing, organizing, and accessing music data. core challenge in MIR is retrieving musical contentwhether sheet music, performance signals, or audio recordingsbased on natural language queries (a fast-paced classical piano piece). This connection enables applications such as automatic music tagging, where models assign genres (jazz, folk) or descriptive attributes (melancholic, upbeat), facilitating music organization, search, and recommendation. By integrating NLP methodologies, MIR enables more intuitive access to musical content, making it more interpretable and searchable through text. Figure 1: CLaMP 3 demonstrates robust cross-modal and cross-lingual generalization. Supervised alignment (solid arrows) links paired modalities, while emergent alignment (dashed arrows) bridges unaligned ones. multilingual text encoder enables retrieval in languages unseen (grayed-out bubbles) during alignment. These capabilities position MIR as critical bridge between music and language, supporting various applications beyond retrieval and annotation. For instance, cross-modal representations enable text-to-music generation models (Agostinelli et al., 2023; Chen et al., 2024) to create music based on text descriptions. MIR also aids in the automatic evaluation of these models by assessing how closely the generated music aligns with text descriptions or resembles the ground truth (Copet et al., 2023; Retkowski et al., 2024). Despite these advancements, MIR faces significant challenges in addressing the complexities of multimodality and multilinguality. Music exists in many forms: sheet music offers human-readable representations for theoretical analysis and education; performance signals (e.g., MIDI) capture timing and dynamics for precise digital editing; and audio recordings serve as the primary medium for listening. While these modalities complement each other, their heterogeneous representational structures complicate unified computational processing. Adding to this complexity, as universal medium, music is described in numerous languages, crossing cultural and linguistic boundaries. Musical terminology, descriptions, and cultural references vary significantly between linguistic communities, each bringing their own rich vocabulary and cultural context. To build global and accessible MIR systems, it is essential to process and understand these diverse expressions effectively. Unfortunately, the development of MIR is limited not only by the lack of music-text pairs but also by the general scarcity of paired data across different musical modalities. As result, most research focuses on retrieval between specific modality pairs, such as text and audio (Huang et al., 2022; Doh et al., 2024; Zhu et al., 2025) or text and sheet music (Wu et al., 2023a). This narrow focus restricts the potential for cross-modal interactions, preventing more comprehensive understanding of music. Additionally, existing text data is often short-form, like tags, with few long-form descriptions (Wu et al., 2023b), leading to shallow semantics. These datasets are also predominantly in English (Doh et al., 2023b), with limited representation of other languages, neglecting musics global and multilingual nature. To tackle these challenges, unified framework is crucial for aligning musical modalities and bridging linguistic gaps, particularly in the absence of paired training data. Large Language Models (LLMs) present promising solution by addressing the limitations of text semantics and the scarcity of linguistic diversity in music-text datasets. These models excel at transforming basic metadata into fluent and contextually rich descriptions (Doh et al., 2023a; Bai et al., 2024). Furthermore, their multilingual capabilities allow them to support wide array of languages (Wu et al., 2024), enhancing semantic depth and enabling more inclusive access across diverse linguistic and cultural contexts. In this paper, we introduce CLaMP 3, universal MIR framework that processes music and text while aligning them into shared representation space. It covers all major music modalities: 1) sheet music, 2) performance signals, and 3) audio recordings, along with 4) multilingual text. Each modality is encoded through its respective feature extractor. To unify these representations, we employ contrastive learning (Sohn, 2016), aligning both musical and textual features. This enables seamless cross-modal retrieval and integration across diverse musical formats and languages. To address the shortage of paired music-text data, we use Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) to create M4-RAG, dataset of 2.31 million music-text pairs covering various musical modalities. Starting with basic metadata like song titles and artist names, we retrieve relevant web documents and use an LLM to generate detailed annotations. These annotations include short tags, long descriptions, and multilingual translations, providing rich and diverse information. In addition, we present WikiMT-X, the first benchmark to align text, audio, and sheet music. It includes 1,000 triplets with diverse text annotations, such as genre labels and detailed long-form descriptions, including background context, musical analysis, general descriptions, and scene depictions. WikiMT-X facilitates evaluation across modalities and semantic perspectives, providing holistic framework to assess models ability to align and interpret musical content. Experiments demonstrate that CLaMP 3 achieves state-of-the-art performance on various MIR tasks, including text-to-audio and text-tosymbolic music retrieval, significantly surpassing all baselines. It also excels in multilingual retrieval, generalizing to languages not present during alignment. By leveraging text as bridge, CLaMP 3 enables emergent cross-modal retrieval, connecting musical modalities without paired training data. Overall, this work contributes: CLaMP 3 unifies musical modalities and languages in shared representation space, achieving strong performance on wide range of MIR tasks and generalizing to unseen languages with emergent cross-modal alignment. We curate M4-RAG, dataset of 2.31 million music-text pairs with diverse annotations, spanning 27 languages and 194 countries, addressing critical gap in high-quality training data for music and language tasks. WikiMT-X links text, audio, and sheet music with 1,000 triplets, offering first-of-its-kind resource to evaluate models holistically across different modalities and semantic aspects. To support future research, we have publicly released the complete codebase, pre-trained weights of CLaMP 3, 1.56 million audio-text training pairs, and the WikiMT-X benchmark1. 1https://github.com/sanderwood/clamp3 Figure 2: CLaMP 3 uses contrastive learning to align features across modalities. Sheet music and performance signals are segmented into units (bars or MIDI messages) and processed by the symbolic music encoder, while audio is segmented into 5-second clips and processed through the audio feature extractor and audio music encoder. Both symbolic and audio representations are aligned with text representations from the multilingual text encoder."
        },
        {
            "title": "2.1 Training Objective",
            "content": "CLaMP 3s optimization objective is to minimize the InfoNCE loss (Oord et al., 2018), aligning embeddings using contrastive learning: LCL ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 log exp(sim(zt , zm j=1 exp(sim(zt )/τ ) , zm )/τ ) (cid:80)N , (1) and zm where zt are text and music embeddings, sim(, ) is the similarity function (e.g., dot product), and τ is the temperature parameter. Positive pairs are aligned text-music samples, while negatives are unrelated samples from the same batch. Inspired by ImageBind (Girdhar et al., 2023), we adopt multi-stage strategy using text as bridge to address the lack of paired music data: Stage 1: The text encoder is first trained to align with one music encoder (e.g., symbolic encoder). Stage 2: It is then aligned with another music encoder (e.g., audio encoder), freezing the text encoder to prevent representation drift. Stage 3: The text encoder is unfrozen to refine its alignment with the music encoder from Stage 2. Stage 4: The text encoder is frozen again to prevent shifts while re-aligning with the Stage 1 music encoder to fix alignment drift from Stage 3. This strategy minimizes modality interference while mapping all modalities into shared representation space for effective cross-modal transfer."
        },
        {
            "title": "2.2 Core Components",
            "content": "CLaMP 3 consists of several transformer-based encoders (Vaswani et al., 2017) for each modality: Multilingual Text Encoder: The text encoder in CLaMP 3 is based on XLM-R-base (Conneau et al., 2020), model pre-trained on 2.5 TB of CommonCrawl data across 100 languages. It has 12 layers and hidden size of 768, enabling strong cross-lingual generalization to unseen languages. Symbolic Music Encoder: CLaMP 3 uses M3 (Wu et al., 2024), self-supervised model for encoding symbolic music, including multi-track voice-interleaved ABC notation and lossless MIDI encoding via MIDI Text Format (MTF). M3 segments ABC into bars and MIDI into messages, treating each segment as patch. The model has 12 encoder layers, hidden size of 768, and processes up to 512 patches or 32,768 characters per input. Audio Music Encoder: It is 12-layer transformer with 768-dimensional hidden size, trained from scratch for audio processing. This encoder leverages pre-trained features from MERT-v1-95M (Li et al., 2024), where MERT serves as frozen audio feature extractor. Each 5-second clip is represented by single embedding, obtained by averaging across all MERT layers and time steps. CLaMP 3 processes up to 128 such embeddings, covering 640 seconds of audio, allowing it to capture highlevel audio patterns over extended durations. All encoders process their outputs through linear layer, followed by average pooling, to generate single global semantic feature for each input. Table 1: Metadata overview for M4-RAG, grouped into basic information, annotations, and translations. In Annotations, Region and Language are written in English; other fields follow the Language specification. Category Field Content Avg Bytes Basic Title Artists Music Title Artist names Annotations Translations Region Country of origin Language Document language Genres Genre list Tags Keywords/playlists Background Background context Analysis Description General description Scene Musical analysis Scene depiction Translation language Language Background Translated background Analysis Description Translated description Scene Translated analysis Translated scene 20.04 21.97 20.69 7.02 21.83 51.91 531.79 770.29 591.86 750.92 6.38 819.76 1130.47 888.86 1077."
        },
        {
            "title": "3 Dataset",
            "content": "In this section, we introduce the M4-RAG dataset for training CLaMP 3 and the WikiMT-X benchmark for evaluation. We start with data sources, followed by the metadata curation process. Then, we summarize dataset statistics like scale and diversity. Finally, we elaborate on the details of the WikiMT-X benchmark."
        },
        {
            "title": "3.1 Data Sources",
            "content": "The training data for CLaMP 3 is built from both symbolic and audio music datasets, ensuring rich and diverse foundation for multimodal learning. The symbolic music data is sourced from WebMusicText (WebMT) (Wu et al., 2023a) with 1.4 million ABC notation files and the Million MIDI Dataset (MMD) (Zeng et al., 2021) with 1.5 million MIDI files. Since symbolic music formats use discrete symbols to represent music, they can be converted into one another, albeit with some information loss. To fully utilize the data, these datasets were unified by converting MMD to ABC and WebMT to MIDI. This process yields 3 million symbolic music files, offering diverse and comprehensive training coverage. The audio data comes from the MERT training dataset (Li et al., 2024), comprising 160 thousand hours of audio from 1.8 million tracks collected online. As CLaMP 3 directly utilizes MERT-extracted features, the training data exclusively consists of these precomputed features, leading to substantial savings in both computational resources and time."
        },
        {
            "title": "3.2 Metadata Curation",
            "content": "Music titles often serve as unique identifiers, enabling the retrieval of rich and detailed descriptions from diverse online sources. When paired with artist names, they further refine searches, pinpointing specific versions or performances and reducing ambiguities caused by covers or adaptations. This distinctive property makes music titles reliable basis for generating annotations, even in the absence of paired music-text datasets. To leverage this, we curated M4-RAG (Millionscale Multilingual Music Metadata), dataset comprising 2.31 million metadata entries. The curation process involved several key steps: Title Filtering: Entries without titles were excluded, as titles are essential for retrieving meaningful information from the web. Web Search: Google searches were conducted using titles and, where available, artist names. For each entry, the top 10 search results were collected to ensure diverse and reliable sources. RAG: Using Qwen2.5-72B (Yang et al., 2024), we generated annotations from the retrieved documents and basic metadata (titles and artist names). We refined the metadata when discrepancies were found. The annotations covered the fields in Table 1 under Annotations, with an additional Boolean field indicating if the source material had sufficient information for generating meaningful annotations. Quality Filtering: Entries were discarded if flagged by the Boolean field for insufficient information, if their format failed to meet the standards outlined in Table 1, or if any fields were left empty. Postprocessing: To address inconsistencies in the generated annotations, Region fields were mapped to recognized countries, while Description fields were refined using Qwen to remove identifiable details such as titles and lyrics. Language consistency across long-form fields (Background, Analysis, Description, Scene) was verified with fastText (Joulin et al., 2017). Entries with inconsistent languages or languages unsupported by either XLM-R or Qwen were removed, and valid detected languages were recorded in the Language field. Multilingual Translation: To enhance linguistic diversity, random language supported by both XLM-R and Qwendifferent from the originalwas selected for each entry, and long-form annotations were translated into it using Qwen."
        },
        {
            "title": "Prompt and examples of generated annotations",
            "content": "are provided in Appendix A. Figure 3: Language distribution of original and translated entries in M4-RAG, covering 27 languages. Figure 4: Country-wise distribution of music tracks in M4-RAG, spanning 194 countries."
        },
        {
            "title": "3.3 Dataset Statistics",
            "content": "Through metadata curation, we obtained M4-RAG, which consists of 2.31 million entries. It includes 0.58 million ABC-text pairs from WebMT, 0.17 million MIDI-text pairs from MMD, and 1.56 million audio-text pairs from the MERT training data. Each metadata entry includes both short-form annotations, such as genres and tags, and detailed long-form descriptions. As summarized in Table 1, the long-form descriptions account for the majority of the dataset, providing extensive semantic details from multiple perspectives. M4-RAG spans 27 languages, with the original metadata predominantly in English, as shown in Fig. 3. To address this imbalance, translations were added to the long-form descriptions, greatly boosting non-English data. This was particularly impactful for low-resource languages, such as Malay and Burmese, where most data depends on translations, greatly enhancing their representation. In terms of geographic coverage, M4-RAG incorporates music from 194 countries. Fig. 4 illustrates contributions from both major music-producing nations and less-represented regions. This global reach ensures the dataset reflects diverse range of musical traditions and styles from across the world. Figure 5: Genre distribution of the WikiMT-X dataset."
        },
        {
            "title": "3.4 Benchmark Dataset",
            "content": "WikiMT-X (WikiMusicText-eXtended) extends WikiMT (Wu et al., 2023a), focusing on 20thcentury Western music with 1,000 entries, each with sheet music, audio, and detailed metadata. The original WikiMT dataset had the following drawbacks: 1) the text was sourced from Wikipedia, mainly focused on background information with limited semantic diversity; 2) the absence of audio data severely restricted the evaluation scope; and 3) the genre labels were obtained through keyword matching, resulting in relatively low accuracy and reducing the reliability of the dataset. To address these deficiencies, WikiMT-X made the following improvements: We used llama-3.1-sonar-large-128k-online2 (Dubey et al., 2024), feeding it sheet music with titles, artist names, and lyrics. It retrieved relevant web pages and summarized them into background, analysis, description, and scene. We manually matched sheet music with audio recordings retrieved from YouTube and removed 10 identified duplicates. We reorganized genre categories based on data distribution and re-annotated labels. These enhancements make WikiMT-X useful for multimodal MIR research tasks, assessing models capabilities in handling text annotations of diverse semantic types, and classifying music across modalities using genre labels. Appendix provides t-SNE visualizations of CLaMP 3 embeddings on WikiMT-X, showing modality, language, and semantic distributions in the shared representation space. In addition, Table 11 presents the genre classification results of CLaMP 3 and baseline models across different musical modalities and text annotations. 2https://www.perplexity.ai Table 2: Results for English text-to-music retrieval on several benchmarks: WikiMT and MidiCaps have 1,010 pairs, Song Describer Dataset (SDD) has 706 audio and 1,106 captions, and MusicCaps-Remake (MC-R) contains 2,777 pairs. MC-R prevents data leakage by using full-length audio and rewritten captions from AudioSets evaluation set. Model CLaMP CLaMP 2 CLaMP 3c2 sa CLaMP 3saas Model CLAP TTMR++ CLaMP 3c2 sa CLaMP 3saas Symbolic Benchmarks WikiMT-X (Sheet Music) WikiMT MidiCaps Background Analysis Description 0.2561 0.3438 0.4498 0. 0.1236 0.2695 0.2826 0.1798 0.2122 0.3024 0.4028 0.3301 0.1345 0.2374 0.3382 0.2758 0.0306 0.0418 0.0835 0.1274 Audio Benchmarks WikiMT-X (Audio) SDD 0.1310 0.1437 0.1612 0.1985 MC-R 0.0657 0.1248 0.0959 0.1177 Background Analysis Description 0.0598 0.1119 0.1180 0.2017 0.0429 0.0833 0.1206 0.1711 0.0318 0.0584 0.0639 0.0988 Scene 0.0426 0.0838 0.1512 0. Scene 0.0218 0.0301 0.0619 0."
        },
        {
            "title": "4.2 English Text-to-Music Retrieval",
            "content": "This section evaluates CLaMP 3 on retrieval tasks, comparing it to state-of-the-art baselines. We present results for the two best-performing CLaMP 3 variantsone for symbolic music and one for audio. full retrieval comparison of all variants can be found in Appendix C, and classification results are available in Appendix D."
        },
        {
            "title": "4.1 Settings",
            "content": "Both symbolic music and audio alignments were trained for up to 100 epochs on 8 NVIDIA H800 GPUs. Symbolic music alignment required 4 days with learning rate of 5e-5 and batch size of 1024. Audio alignment took 1 day with learning rate of 1e-5 and batch size of 2048. M4-RAG was divided into 99% for training and 1% for validation. During training, metadata information was randomly selected to form text inputs. Mixed-precision (Micikevicius et al., 2018), AdamW optimizer (Loshchilov and Hutter, 2019), and 1,000-step warm-up (Goyal et al., 2017) were used to enhance efficiency. Following the training strategy in Sec. 2.1, we explored various modality alignment orders for symbolic and audio modalities, and present the two top-performing variants below: CLaMP 3saas: Optimized for audio, this model follows the full multi-stage alignment: symbolic audio audio symbolic. CLaMP 3c2 sa : Optimized for symbolic, this model starts from CLaMP 2-initialized text and symbolic encoders, followed by two stages: the text encoder is jointly trained with the symbolic encoder, then frozen to align with the audio encoder. We evaluated retrieval performance using Mean Reciprocal Rank (MRR), which measures the inverse of the rank of the paired item, across all tasks. For symbolic music retrieval, we compared CLaMP 3 with CLaMP 2 (Wu et al., 2024) and CLaMP (Wu et al., 2023a) on WikiMT (using ABC notation) and MidiCaps (Melechovsky et al., 2024) (using MIDI). For audio retrieval, we evaluated CLaMP 3 against state-of-the-art models CLAP (Wu et al., 2023b) and TTMR++ (Doh et al., 2024) on the Song Describer Dataset (SDD) (Manco et al., 2023) and MusicCaps-Remake (MCR) (Agostinelli et al., 2023), which addresses data leakage by using full-length audio and rewritten captions (see Appendix E) from AudioSets evaluation set (Gemmeke et al., 2017). In addition, we tested all models on WikiMT-X to evaluate their performance across varying semantic perspectives. As shown in Table 2, CLaMP 3 achieved significant improvements over its predecessors and baseline models across both symbolic and audio retrieval tasks. For symbolic music retrieval, CLaMP 3c2 sa achieved MRR scores of 0.4498 on WikiMT and 0.2826 on MidiCaps, clearly outperforming both CLaMP 2 and CLaMP, despite using only half the training data. This improvement can be attributed to the high-quality, richly annotated M4RAG dataset. Similarly, CLaMP 3saas, though optimized for audio retrieval, exceeded CLaMP by notable margin on symbolic benchmarks and performed comparably to CLaMP 2 on WikiMT. These results demonstrate that our multi-stage training approach effectively preserves performance on modalities that were not explicitly optimized. Table 3: Results for multilingual text-to-music retrieval on translated WikiMT-X background annotations. Languages marked with asterisks were not included in the M4-RAG training data. The BLEU scores below each language are calculated by back-translating the text with the SeamlessM4T model and comparing it to the original English text. Model ABC Notation CLaMP 2 CLaMP 3c2 sa CLaMP 3saas MIDI CLaMP 2 CLaMP 3c2 sa CLaMP 3saas Audio CLaMP 3c2 sa CLaMP 3saas ru 49.69 fr 55.50 es 62.82 ar 53.38 zh 39.58 fi* 39. el* 55.55 ta* 40.07 kk* 36.57 am* 56.08 0.2668 0.3614 0.2918 0.2968 0.3949 0. 0.2934 0.3921 0.3239 0.2298 0.3155 0.2789 0.1646 0.2373 0.2358 0.2795 0.3524 0.2919 0.2410 0.3226 0.2681 0.0915 0.1415 0. 0.2543 0.3397 0.2703 0.1237 0.1871 0.1139 0.1271 0.1921 0.1165 0.1414 0.2101 0.1319 0.1452 0.2137 0.1330 0.1113 0.1681 0. 0.0749 0.1316 0.0937 0.1438 0.2019 0.1245 0.1087 0.1702 0.1143 0.0466 0.0804 0.0601 0.1079 0.1765 0.1104 0.0616 0.1039 0. 0.1068 0.1788 0.1150 0.1980 0.1202 0.1962 0.0981 0.1665 0.0877 0.1459 0.1112 0. 0.1014 0.1736 0.0720 0.0945 0.1005 0.1561 0.0681 0.0675 Beyond symbolic music retrieval, CLaMP 3 also achieved notable performances in audio retrieval. Both variantsCLaMP 3c2 sa and CLaMP 3saasconsistently outperformed CLAP, with CLaMP 3saas standing out. It achieved the highest MRR of 0.1985 on SDD, marking substantial improvement over TTMR++ (0.1437) and CLAP (0.1310). While TTMR++ performed well on MC-R (0.1248), its results on the original MusicCaps dataset are abnormally higher (see Table 12), likely because it was trained on half of MusicCaps original music-text pairs. This training overlap suggests that indirect data leakage affects its performance, even when evaluated on MC-R. CLaMP 3s strong performance extends to WikiMT-X, with both variants outperforming baselines across all four semantic categories. In Background and Analysis, where texts provide rich cultural or technical details, CLaMP 3c2 sa and CLaMP 3saas excelled, achieving MRRs of 0.4028 and 0.3382 (sheet music) and 0.2017 and 0.1711 (audio). Description and Scene, however, are much harder to retrieve because they are less specific and semantically sparse. Description excludes explicit identifiers like titles or artist names, while Scene focuses on abstract, visualized scenario depictions (rather than the music itself), both of which make retrieval more difficult. Even so, CLaMP 3 performed notably better, with CLaMP 3saas scoring 0.0988 (Description) and 0.0963 (Scene) in audio, compared to TTMR++ (0.0584, 0.0301). This improvement stems from M4-RAGs diverse annotations, which better equip CLaMP 3 to retrieve abstract, semantically sparse texts compared to baseline models trained on less diverse data."
        },
        {
            "title": "4.3 Multilingual Text-to-Music Retrieval",
            "content": "Currently, no non-English music-text benchmarks exist, making multilingual evaluation challenging. To address this, we used SeamlessM4T (Barrault et al., 2023) to translate WikiMT-X background annotations into multiple languages. To account for translation noise, BLEU scores (Papineni et al., 2002) were calculated by comparing original texts with back-translations. The translated annotations were then used for retrieval of matching ABC notation, MIDI (from ABC), and audio files. We carefully selected ten languages to ensure diversity in linguistic families, scripts, regions, and resource levels. Five UN official languages were chosen from those included in M4-RAG as they represent different cultures and regions with global significance. The other five, marked with asterisks in Table 3, come from different linguistic families with distinct scripts and minimal vocabulary overlap, specifically to test CLaMP 3s generalization to languages unseen in music-text alignment. To the best of our knowledge, apart from CLaMP 3, CLaMP 2 is the only multilingual MIR model, but it is limited to symbolic music. No baselines exist for multilingual audio retrieval, as models like CLAP and TTMR++ are restricted to English. CLaMP 3s two variants differ in their language exposure. CLaMP 3c2 sa initializes its text and symbolic music encoders from CLaMP 2, which was pre-trained on symbolic-text alignment across all XLM-R-supported languages, giving it prior exIn contrast, posure to all languages in Table 3. CLaMP 3saas has never aligned music data with the languages marked with asterisks, demonstrating true cross-modal generalization in its performance. Table 4: Results for emergent cross-modal retrieval on WikiMT-X pairings across different musical modalities. S: Sheet Music (ABC notation), P: Performance Signals (MIDI, converted from ABC), A: Audio recordings. Model SP SA PS PA AS AP CLaMP 2 0.5138 CLaMP 3c2 0.4547 0.0543 0.5293 0.0313 0.0492 0.0383 sa CLaMP 3saas 0.3262 0.0578 0.3146 0.0397 0.0410 0.0303 0.4480 - - - - Table 3 shows that CLaMP 3 demonstrates strong cross-lingual generalization in both symbolic music and audio retrieval tasks. For symbolic music retrieval, CLaMP 3c2 sa clearly outperforms CLaMP 2 on all languages, including those not in M4-RAG, showing that full language coverage during training is not necessary for improved multilingual retrieval. Meanwhile, CLaMP 3saas, without any prior alignment between these languages and music or specific optimization for symbolic music tasks, matches CLaMP 2s performance on MIDI and surpasses it on ABC notation. This indicates that CLaMP 3saas achieves true cross-lingual generalization on unseen languages. it outperformed CLaMP 3c2 In audio retrieval, CLaMP 3saas performed well on languages it had never seen during alignment. sa on For instance, Finnish (0.1770 vs. 0.1112), Greek (0.1736 vs. 0.1014), and Kazakh (0.1561 vs. 0.1005), even though CLaMP 3c2 sa had indirect exposure to these languages during CLaMP 2 pre-training. Notably, even for its weakest unseen language, Amharic (0.0675), CLaMP 3saas outperformed CLAPs performance on English text (0.0598). This suggests that prior exposure to language is not necessary for achieving strong audio retrieval performance. The ability to retrieve languages beyond the training data stems from XLM-Rs cross-lingual semantics and the universal representations of CLaMP 3s music encoders. This enables the model to handle low-resource languages and even generalize to unseen ones, enhancing its inclusivity and versatility for global MIR."
        },
        {
            "title": "4.4 Emergent Cross-Modal Retrieval",
            "content": "Emergent cross-modal retrieval assesses models ability to align and retrieve musical content across modalities without explicit alignment training, showcasing its capacity to generalize to unaligned modalities. Table 4 reports results for all possible retrieval directions between ABC notation, MIDI, and audio data. CLaMP 3 significantly advances cross-modal retrieval by supporting both symbolic and audio modalities, addressing key limitation of CLaMP 2. While CLaMP 2 excels in symbolic tasks (SP: 0.5138, PS: 0.4480) without explicit alignment between ABC and MIDI, it cannot retrieve between symbolic and audio modalities. In contrast, CLaMP 3c2 sa not only achieves stateof-the-art performance on symbolic music tasks like PS (0.5293) but also enables emergent retrieval between symbolic music and audio. Similarly, CLaMP 3saas, optimized for audio retrieval, achieves meaningful results on new tasks such as SA (0.0578) and PA (0.0397), demonstrating its ability to unify symbolic and audio modalities in shared representation space. While audio retrieval is inherently more challenging due to the continuous nature of audio signals, all directions achieve MRR scores well above the random baseline of 0.0075. Nonetheless, further optimization is required to reduce the performance gap between symbolic and audio retrieval."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we introduced CLaMP 3, unified MIR framework that aligns sheet music, performance signals, audio, and multilingual text using contrastive learning. CLaMP 3 demonstrates strong cross-modal and cross-lingual generalization, effectively handling unaligned modalities and unseen languages during training. To address the lack of high-quality datasets, we curated M4-RAG, collection of 2.31 million music-text pairs spanning 27 languages and 194 countries. We also released WikiMT-X, the first benchmark combining text, sheet music, and audio for comprehensive evaluation. Our experiments show that CLaMP 3 achieves state-of-the-art performance in both symbolic and audio retrieval, excels in multilingual tasks, and enables retrieval across unaligned musical modalities. These results demonstrate its flexibility and the effectiveness of its shared representation space. To conclude, CLaMP 3 sets new standard in multimodal and multilingual MIR, demonstrating robust cross-modal and cross-lingual generalization. By releasing the CLaMP 3 model, M4-RAG dataset, and WikiMT-X benchmark, we provide resources to support future research in MIR and music generation across languages and modalities."
        },
        {
            "title": "Correspondence",
            "content": "Although CLaMP 3 attains state-of-the-art performance across modalities and languages, showing cross-modal and cross-lingual generalization, this work has several limitations that need to be addressed for further advancements in MIR. First, while contrastive learning has advanced multimodal information retrieval, it struggles to capture the temporal dynamics of music. This is because such models typically use single global representation to store the entire semantic content of piece of music, making them insensitive to temporal dynamics. For example, in Beethovens Symphony No. 5, the iconic four-note motif develops throughout the piece, yet current systems often miss this context. Addressing this requires moving beyond contrastive learning to incorporate temporal modeling, enabling systems to better capture nuances and deliver more context-aware and accurate retrieval. Second, although Table 3 indicates that while CLaMP 3 can generalize to languages beyond the multilingual text-tomusic-text alignment, music retrieval evaluation in it heavily relies on translation models due to the lack of native multilingual benchmarks. The translation quality varies significantly across languages, which introduces noise and reduces the reliability of evaluations. Developing native multilingual benchmarks is the primary and almost indispensable solution to achieve more accurate and fair assessments of model performance. Finally, as shown in Table 4, the alignment between audio and symbolic modalities, though showing emergent capabilities with performance far above random, remains relatively weak. Addressing this limitation requires collecting paired data for supervised alignment and leveraging text as bridging modality to further enhance connections between different musical modalities."
        },
        {
            "title": "Authors",
            "content": "Shangda Wu1, shangda@mail.ccom.edu.cn Zhancheng Guo1, 23a053@mail.ccom.edu.cn Ruibin Yuan2, ryuanab@connect.ust.hk Junyan Jiang3, 4, jj2731@nyu.edu Seungheon Doh5, seungheondoh@kaist.ac.kr Gus Xia3, 4, Gus.Xia@mbzuai.ac.ae Juhan Nam5, juhan.nam@kaist.ac.kr Xiaobing Li1, lxiaobing@ccom.edu.cn Feng Yu1, yufengai@ccom.edu.cn Maosong Sun1, 6, sms@tsinghua.edu.cn"
        },
        {
            "title": "Affiliations",
            "content": "1Central Conservatory of Music 2Hong Kong University of Science and Technology 3New York University Shanghai 4Mohamed bin Zayed University of Artificial Intelligence 5Korea Advanced Institute of Science and Technology 6Tsinghua University"
        },
        {
            "title": "Acknowlegdements",
            "content": "This work was supported by the following funding sources: Special Program of National Natural Science Foundation of China (Grant No. T2341003), Advanced Discipline Construction Project of Beijing Universities, Major Program of National Social Science Fund of China (Grant No. 21ZD19), and the National Culture and Tourism Technological Innovation Engineering Project (Research and Application of 3D Music). In addition, we would like to express our gratitude for the use of icons from flaticon3 in Fig. 1 and Fig. 2. We also thank Yusong Wu from the University of Montreal for helping us understand details of CLAP."
        },
        {
            "title": "References",
            "content": "Andrea Agostinelli, Timo Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. 2023. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325. Jisheng Bai, Haohe Liu, Mou Wang, Dongyuan Shi, Wenwu Wang, Mark Plumbley, Woon-Seng Gan, and Jianfeng Chen. 2024. Audiosetcaps: An enriched audio-caption dataset using automated generation pipeline with large audio and language models. arXiv preprint arXiv:2411.18953. Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023. Seamlessm4t-massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596. Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. 2019. The mtg-jamendo dataset for automatic music tagging. ICML. 3https://www.flaticon.com Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2024. Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 12061210. IEEE. Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March 5-9, 2017, pages 776780. IEEE. Yi-Hui Chou, Chen, Chin-Jui Chang, Joann Ching, Yi-Hsuan Yang, et al. 2021. Midibert-piano: largescale pre-training for symbolic music understanding. arXiv preprint arXiv:2107.05223. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 84408451. Association for Computational Linguistics. Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez. 2023. Simple and controllable music genIn Advances in Neural Information Proeration. cessing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Seungheon Doh, Keunwoo Choi, Jongpil Lee, and Juhan Nam. 2023a. Lp-musiccaps: Llm-based pseudo music captioning. In Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023, pages 409416. Seungheon Doh, Minhee Lee, Dasaem Jeong, and Juhan Nam. 2024. Enriching music descriptions with finetuned-llm and metadata for text-to-music retrieval. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024, pages 826830. IEEE. Seungheon Doh, Minz Won, Keunwoo Choi, and Juhan Nam. 2023b. Toward universal text-to-music retrieval. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15. IEEE. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind one embedding space to bind them all. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1518015190. IEEE. Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677. Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. 2022. Mulan: joint embedding of music audio and natural language. In Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, pages 559566. Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomás Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers, pages 427 431. Association for Computational Linguistics. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger B. Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Zili Wang, Yike Guo, and Jie Fu. 2024. MERT: acoustic music understanding model with large-scale self-supervised training. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Lucas Ferreira and Jim Whitehead. 2019. Learning to generate music with sentiment. In Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands, November 4-8, 2019, pages 384390. Ilya Loshchilov and Frank Hutter. 2019. Decoupled In 7th International weight decay regularization. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023b. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023, pages 15. IEEE. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger B. Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, and Jie Fu. 2023. MARBLE: music audio representation benchmark for universal evaluation. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. 2021. Musicbert: Symbolic music understanding with large-scale pre-training. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 791800. Association for Computational Linguistics. Haina Zhu, Yizhi Zhou, Hangting Chen, Jianwei Yu, Ziyang Ma, Rongzhi Gu, Wei Tan, and Xie Chen. 2025. Muq: Self-supervised music representation learning with mel residual vector quantization. arXiv preprint arXiv:2501.01108. Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, et al. 2023. The song describer dataset: corpus of audio captions for music-and-language evaluation. arXiv preprint arXiv:2311.10057. Jan Melechovsky, Abhinaba Roy, and Dorien Herremans. 2024. Midicapsa large-scale midi dataset with text captions. arXiv preprint arXiv:2406.02255. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed precision training. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311318. ACL. Jan Retkowski, Jakub Stepniak, and Mateusz Modrzejewski. 2024. Frechet music distance: metric for generative symbolic music evaluation. arXiv preprint arXiv:2412.07948. Kihyuk Sohn. 2016. Improved deep metric learning with multi-class n-pair loss objective. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 18491857. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Shangda Wu, Yashan Wang, Ruibin Yuan, Zhancheng Guo, Xu Tan, Ge Zhang, Monan Zhou, Jing Chen, Xuefeng Mu, Yuejie Gao, et al. 2024. Clamp 2: Multimodal music information retrieval across 101 languages using large language models. arXiv preprint arXiv:2410.13267. Shangda Wu, Dingyao Yu, Xu Tan, and Maosong Sun. 2023a. Clamp: Contrastive language-music pretraining for cross-modal symbolic music information retrieval. In Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023, pages 157165."
        },
        {
            "title": "A Prompt and Examples",
            "content": "Figure 6: The metadata generation prompt was used for constructing the M4-RAG dataset. This prompt outlines the required JSON structure for describing music metadata comprehensively, including fields for title, artists, region, language, genres, tags, background context, musical analysis, general description, and visual scene. Detailed instructions and formatting requirements are provided to ensure high-quality and consistent metadata extraction from search results. Based on our experience, we recommend adding the requirement to the prompt that Region and Language be output in accordance with ISO standards, which can reduce the need for post-processing. Figure 7: Metadata examples from the M4-RAG and WikiMT-X datasets. The top section shows an entry for Mairis Wedding from the M4-RAG dataset, including detailed multilingual metadata in English and Vietnamese, and an associated audio recording identified by YouTube ID. The bottom section presents an entry for Deed Do from the WikiMT-X dataset, which includes YouTube ID linking to an audio recording, genre label (Jazz, one of eight predefined categories), four types of long-form text annotations, and lead sheet in ABC notation. t-SNE Visualizations on WikiMT-X We apply t-SNE (t-distributed Stochastic Neighbor Embedding) to the WikiMT-X dataset to visualize how CLaMP 3 organizes data into shared representation space. The projections illustrate the models ability to align data across modalities, languages, and semantic categories. Fig. 8a includes features from Text (background annotations), ABC notation, MIDI, and Audio. Each modality forms distinct cluster, reflecting the inherent differences in how information is encoded. Notably, modalities closer to Text tend to perform better, aligning with the trend in Table 3, suggesting correlation between embedding proximity and cross-modal effectiveness. Additionally, all musical modalities display mirrored symmetry around the Text cluster, indicating that Text may serve as semantic anchor. This symmetry suggests CLaMP 3 aligns modalities relative to Text, balancing modality-specific features while preserving semantic consistency. Fig. 8b focuses on background annotations in four languagesEnglish, Spanish, Chinese, and Amharicselected to represent varying retrieval performance levels. Despite their linguistic differences, these languages largely overlap, indicating strong cross-lingual alignment. English and Spanish cluster closely, reflecting both their shared linguistic roots. Chinese shows moderate overlap with English, suggesting that CLaMP 3 effectively bridges typologically distant languages. However, Amharic, low-resource and unseen language, forms more isolated clusters, indicating the challenges of aligning low-resource languages. (a) Modality (b) Language 8c Fig. four shows semantic categoriesBackground, Analysis, Description, and Sceneshowing how CLaMP 3 handles different content types. Background, Analysis, and Description often converge, reflecting the overlap in explanatory texts as they cover related musical concepts. In contrast, Scene forms distinct clusters, likely because it focuses on visual depictions, leading to more consistent semantic patterns tied to specific imagery rather than music. Across all three visualizations, genre boundaries remain clear despite differences in modality, language, or semantic category. This shows that CLaMP 3 effectively aligns multimodal and multilingual data while preserving genre-specific distinctions, demonstrating the models strong representational capabilities. (c) Semantics Figure 8: t-SNE visualization of the WikiMT-X dataset, illustrating the distribution of samples based on three distinct factors: (a) Modality, (b) Language, and (c) Semantics. The representations are extracted using CLaMP 3saas. Each point represents data sample, colored according to its genre. Table 5: Results for English text-to-music retrieval on several benchmarks: WikiMT and MidiCaps have 1,010 pairs, Song Describer Dataset (SDD) has 706 audio and 1,106 captions, and MusicCaps-Remake (MC-R) contains 2,777 pairs. MC-R prevents data leakage by using full-length audio and rewritten captions from AudioSets evaluation set. Model CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas Model CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas Symbolic Benchmarks WikiMT-X (Sheet Music) WikiMT MidiCaps Background Analysis Description 0.1973 0.3789 0.4498 0.2993 0.3555 0.3631 0.0788 0.1322 0.2826 0.0884 0.1798 0.2688 0.2108 0.3591 0.4028 0.2919 0.3301 0.3295 0.1660 0.3088 0.3382 0.2507 0.2758 0.2957 0.1049 0.1316 0.0835 0.1459 0.1274 0. Audio Benchmarks WikiMT-X (Audio) SDD 0.1977 0.1607 0.1612 0.2003 0.1985 0.2115 MC-R 0.1117 0.0937 0.0959 0.1045 0.1177 0. Background Analysis Description 0.1602 0.1718 0.1180 0.1597 0.2017 0.1583 0.1375 0.1586 0.1206 0.1522 0.1711 0.1530 0.0854 0.0997 0.0639 0.1020 0.0988 0. Scene 0.1056 0.1643 0.1512 0.1464 0.1512 0.1395 Scene 0.0819 0.0871 0.0619 0.0873 0.0963 0.0885 Table 6: Results for multilingual text-to-music retrieval on translated WikiMT-X background annotations. Languages marked with asterisks were not included in the M4-RAG training data. The BLEU scores below each language are calculated by back-translating the text with the SeamlessM4T model and comparing it to the original English text. Model ABC Notation CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas MIDI CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas Audio CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas ru 49.69 fr 55.50 es 62.82 ar 53.38 zh 39.58 fi* 39. el* 55.55 ta* 40.07 kk* 36.57 am* 56.08 0.1750 0.3262 0.3614 0.2648 0.2918 0.2954 0.0418 0.1174 0.1921 0.0565 0.1165 0. 0.1267 0.1619 0.1068 0.1426 0.1788 0.1331 0.1931 0.3544 0.3949 0.2810 0.3214 0.3171 0.0416 0.1284 0.2101 0.0582 0.1319 0.1645 0.1515 0.1717 0.1150 0.1580 0.1980 0.1566 0.1964 0.3536 0.3921 0.2817 0.3239 0.3225 0.0432 0.1316 0.2137 0.0620 0.1330 0. 0.1525 0.1714 0.1202 0.1588 0.1962 0.1554 0.1594 0.3072 0.3155 0.2450 0.2789 0.2773 0.0404 0.1132 0.1681 0.0582 0.1141 0.1408 0.1210 0.1529 0.0981 0.1370 0.1665 0.1304 0.1559 0.2459 0.2373 0.2271 0.2358 0.2144 0.0332 0.0890 0.1316 0.0517 0.0937 0. 0.1089 0.1414 0.0877 0.1202 0.1459 0.1208 0.1828 0.3163 0.3524 0.2644 0.2919 0.2990 0.0456 0.1217 0.2019 0.0620 0.1245 0.1560 0.1430 0.1585 0.1112 0.1468 0.1770 0.1550 0.1641 0.2879 0.3226 0.2415 0.2681 0.2721 0.0449 0.1112 0.1702 0.0585 0.1143 0. 0.1428 0.1544 0.1014 0.1431 0.1736 0.1460 0.0997 0.1336 0.1415 0.1432 0.1246 0.1348 0.0297 0.0623 0.0804 0.0394 0.0601 0.0653 0.0610 0.0991 0.0720 0.0795 0.0945 0.0901 0.1575 0.2894 0.3397 0.2561 0.2703 0.2750 0.0398 0.1117 0.1765 0.0595 0.1104 0. 0.1043 0.1456 0.1005 0.1276 0.1561 0.1340 0.0876 0.1317 0.1871 0.1300 0.1139 0.1690 0.0267 0.0540 0.1039 0.0354 0.0544 0.0793 0.0559 0.0774 0.0681 0.0617 0.0675 0.0874 Table 7: Results for emergent cross-modal retrieval on WikiMT-X pairings across different musical modalities. S: Sheet Music (ABC notation), P: Performance Signals (MIDI, converted from ABC), A: Audio recordings. Model SP SA PS PA AS AP CLaMP 3as 0.1637 0.0557 0.1477 0.0248 0.0456 0.0237 CLaMP 3sa 0.3205 0.0739 0.3054 0.0397 0.0479 0.0237 CLaMP 3c2 0.4547 0.0543 0.5293 0.0313 0.0492 0.0383 sa CLaMP 3assa 0.1911 0.0619 0.1646 0.0299 0.0513 0.0264 CLaMP 3saas 0.3262 0.0578 0.3146 0.0397 0.0410 0.0303 CLaMP 3c2 saas 0.3909 0.0688 0.4375 0.0467 0.0558 0.0431 Performance of CLaMP 3 Variants straightforward way to train CLaMP 3 would be to align symbolic music, audio, and text all at once. However, early experiments showed that this led to unstable training. The text encoder struggled because symbolic and audio data had very different distributions (Fig.8a) and pulled it in opposite directions, making alignment ineffective. To solve this, we adopted multi-stage alignment strategy (Sec. 2.1) that gradually integrates each modality, ensuring stable and effective alignment. To explore the best way to align modalities, we tested different training orders, leading to several model variants. The main difference among them is how and when the text encoder is aligned with symbolic music and audio encoders: CLaMP 3as: two-stage alignment where text is first aligned with audio, then the text encoder is frozen while aligning with symbolic music. CLaMP 3sa: The reverse of CLaMP 3as, first aligning text with symbolic music, then freezing the text encoder while aligning with audio. CLaMP 3c2 sa : Same as CLaMP 3sa, but starting with pre-trained text and symbolic encoders from CLaMP 2. CLaMP 3assa: four-stage alignment: audio symbolic symbolic audio, with the text encoder frozen in the second and fourth stages to maintain stability. CLaMP 3saas: four-stage alignment: symbolic audio audio symbolic, also freezing the text encoder in the second and fourth stages. CLaMP 3c saas: Same as CLaMP 3saas, but initialized with pre-trained text and symbolic encoders from CLaMP 2. We evaluate these six variants across all experiments in Sec. 4 to assess their effectiveness in different retrieval tasks. Table 5 shows that aligning text with symbolic music before audio improves generalization in English text-to-music retrieval. CLaMP 3sa outperforms CLaMP 3as in symbolic retrieval without compromising audio performance. Four-stage models outperform two-stage models in audio retrieval, emphasizing the importance of iterative alignment. Among them, CLaMP 3saas achieves the best balance between symbolic and audio retrieval. Leveraging CLaMP 2s weight initialization enhances symbolic retrieval, as seen in CLaMP 3c2 sa leading symbolic tasks. However, it does not consistently improve audio retrieval, likely because CLaMP 2 was trained only on symbolic music, limiting its text encoders adaptability to audio alignment. Table 6 demonstrates the impact of pretraining and training order on multilingual textto-music retrieval. In symbolic retrieval, using CLaMP 2s pre-trained text-symbolic encoders provides clear advantage, with CLaMP 3c2 sa achieving the highest scores across most languages. This suggests that pre-training helps build strong shared representation space, especially for MIDI, where M4-RAGs limited native data weakens overall performance. However, pre-training is not always decisive, as some non-pretrained models surpass pre-trained variants in certain languages for ABC retrieval. In contrast, audio retrieval is consistently strongest with CLaMP 3saas, even in unseen languages, suggesting that training order plays more crucial role in cross-lingual generalization. Table 7 evaluates emergent cross-modal retrieval, where no direct supervised alignment exists among musical modalities. CLaMP 3c2 sa achieves the best symbolic retrieval (SP), showing that CLaMP 2 pre-training strengthens symbolic-text alignment, which indirectly benefits symbolic retrieval. For symbolic-audio retrieval, CLaMP 3c2 saas performs best, leading in PA (0.0467), AS (0.0558), and AP (0.0431). It consistently outperforms CLaMP 3saas, suggesting that pre-training provides stronger shared representation space, leading to better cross-modal generalization between unpaired modalities. These results show the importance of both training order and pre-training in MIR. Multi-stage alignment stabilizes training, while training order plays key role, particularly in audio retrieval and cross-lingual generalization. Pre-training with CLaMP 2 strengthens symbolic retrieval and improves cross-modal generalization, but its benefits are limited for audio retrieval. Table 8: Symbolic classification performance for ABC notation and MIDI was assessed across three datasets: WikiMT (1,010 pieces, 8 genres), VGMIDI (204 pieces, 4 emotions), and Pianist8 (411 pieces, 8 composers). Model Modality WikiMT VGMIDI Pianist F1-macro Accuracy F1-macro Accuracy F1-macro Accuracy M3 CLaMP CLaMP 2 CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas M3 CLaMP 2 CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas ABC ABC ABC ABC ABC ABC ABC ABC ABC MIDI MIDI MIDI MIDI MIDI MIDI MIDI MIDI 0.2349 0.3452 0.3990 0.3135 0.3225 0.3316 0.3102 0.3177 0.3568 0.2621 0.2898 0.3361 0.2614 0.3073 0.2882 0.2721 0. 0.4010 0.4267 0.4653 0.4307 0.4455 0.4356 0.4455 0.4356 0.4257 0.4257 0.4455 0.4653 0.4010 0.4455 0.4406 0.4158 0.4208 0.6016 0.6453 0.7449 0.6638 0.7725 0.6845 0.4990 0.7969 0.6694 0.5399 0.5246 0.5600 0.6864 0.6223 0.5001 0.5723 0.5474 0.6341 0.6866 0.8049 0.7073 0.8049 0.7317 0.6341 0.8049 0.7561 0.6098 0.6585 0.5854 0.7073 0.7073 0.6098 0.6341 0. 0.7395 0.7067 0.8025 0.6872 0.7403 0.7722 0.6796 0.7716 0.7891 0.9199 0.8927 0.8186 0.8461 0.8696 0.8076 0.7834 0.8565 0.7590 0.7152 0.8072 0.6867 0.7590 0.7711 0.6988 0.7952 0.7952 0.9157 0.8916 0.8313 0.8554 0.8675 0.8193 0.7952 0.8554 Table 9: Audio classification performance is evaluated on multiple benchmarks included in MARBLE: MTT (25,860 clips, 50 tags), GS (7,035 clips, 24 keys), GTZAN (1,000 clips, 10 genres), EMO (744 clips, valence/arousal regression), Nsynth (305,979 clips, 11 instrument categories, 88 pitches), and VocalSet (7,506 clips, 17 singing techniques, 20 singers). Model MTT Tagging GS GTZAN Key Genre EMO Emotion Nsynth Instrument Nsynth VocalSet VocalSet Tech Pitch Singer ROC AP Acc Acc R2V R2A Acc Acc Acc Acc MERTmean 0.9068 0.3915 0.6475 CLAP 0.9066 0.3897 0.1596 TTMR++ 0.9082 0.3922 0.1672 CLaMP 3as 0.9097 0.3888 0.4935 CLaMP 3sa 0.9084 0.3863 0.2533 CLaMP 3c2 0.9092 0.3924 0.2545 sa CLaMP 3assa 0.9098 0.3935 0.1498 CLaMP 3saas 0.9109 0.3941 0.5377 CLaMP 3c2 saas 0.9095 0.3938 0. 0.6689 0.8207 0.8551 0.8379 0.8448 0.8551 0.8793 0.8655 0.8138 0.5185 0.7501 0.5408 0.7025 0.5599 0.7116 0.5944 0.7413 0.6031 0.6949 0.5477 0.6876 0.5921 0.7327 0.5907 0.7004 0.5368 0.6589 0.6963 0.7817 0.6735 0.6445 0.6338 0.6147 0.6411 0.6377 0.6562 0.9152 0.5146 0.5012 0.8601 0.8647 0.8574 0.8742 0.8689 0.8732 0.7219 0.6868 0.6342 0.6780 0.7061 0.6710 0.6842 0.7053 0.6798 0.8961 0.6327 0.5352 0.8491 0.8419 0.8007 0.8555 0.8441 0. Table 10: Audio classification performance on the MTG-Jamendo dataset (55,000+ tracks) was evaluated across four tasks: instrument classification (41 tags), mood/theme classification (59 tags), genre classification (95 tags), and top-50 multi-label classification. Model MERTmean CLAP TTMR++ CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas Instrument Mood/Theme Genre Top50 ROC 0.7421 0.7480 0.7806 0.7895 0.7780 0.7832 0.7911 0.7872 0.7803 AP 0.1764 0.1812 0.2111 0.2254 0.2112 0.2168 0.2269 0.2208 0.2145 ROC 0.7598 0.7601 0.7705 0.7814 0.7823 0.7796 0.7828 0.7835 0.7825 AP 0.1383 0.1323 0.1477 0.1476 0.1533 0.1475 0.1486 0.1547 0.1522 ROC 0.8672 0.8544 0.8742 0.8750 0.8713 0.8679 0.8763 0.8703 0.8734 AP 0.1818 0.1716 0.2030 0.2114 0.2008 0.2046 0.2109 0.2076 0.2092 ROC 0.8280 0.8197 0.8340 0.8321 0.8276 0.8220 0.8290 0.8242 0.8296 AP 0.2837 0.2773 0.3049 0.3068 0.3011 0.2964 0.3041 0.3021 0."
        },
        {
            "title": "D Music Classification",
            "content": "D.2 Audio Music Classification This section evaluates CLaMP 3 variants and baselines via linear probing, assessing their ability to classify musical attributes in symbolic and audio music, as well as musical modalities and text annotations in WikiMT-X. D.1 Symbolic Music Classification Table 8 presents symbolic music classification results for ABC notation and MIDI across three benchmarks: WikiMT (Wu et al., 2023a) consists of 1,010 lead sheets in ABC notation sourced from Wikifonia4, labeled into 8 genre categories based on corresponding Wikipedia entries. VGMIDI (Ferreira and Whitehead, 2019) contains 204 MIDI transcriptions of video game soundtracks, annotated with 4 emotion labels derived from valence and arousal levels. Pianist8 (Chou et al., 2021) includes 411 piano performances, transcribed from audio to performance MIDI, and labeled with their respective composers across eight categories. To enable evaluation in both formats, all datasets were converted between ABC and MIDI. Despite improved text alignment, CLaMP 3 does not surpass CLaMP 2 in sheet music classification. This is likely because CLaMP 3 was trained on only half as much symbolic data. While stronger textual supervision benefits retrieval, it does not fully offset the reduced symbolic training for classification. However, CLaMP 3 still outperforms M3the symbolic music encoder it was initialized fromon most benchmarks, suggesting that contrastive text supervision enhances the semantic salience of extracted features. These results indicate that retrieval and classification improvements are relatively independent. In text-to-music retrieval (Table 2, Table 3), CLaMP 3especially CLaMP 3c2 sa significantly outperforms CLaMP 2, yet this advantage does not extend to classification. possible explanation is that retrieval requires rich representations and effective interaction between text and music encoders, while classification depends solely on an encoders ability to extract features relevant to predefined labels. Thus, while higher-quality text annotations enhance retrieval, they do not necessarily improve symbolic music classification. 4http://www.synthzone.com/files/Wikifonia/ Wikifonia.zip To evaluate the audio classification performance of CLaMP 3 variants and baselines, we conduct linear probing on MARBLE (Yuan et al., 2023) and MTG-Jamendo (Bogdanov et al., 2019). MARBLE is comprehensive benchmark collection for music representation evaluation. We assess models on 8 tasks covering different aspects of audio understanding. MTG-Jamendo is largescale benchmark with over 55,000 music tracks annotated for multiple classification tasks. It focuses on high-level musical attributes, making it well-suited for evaluating models ability to capture semantic meaning in music. We also assess the self-supervised model MERT, CLaMP 3s audio feature extractor, averaging embeddings to one per 5-second clip across layers and time steps. Table 9 shows the strengths of contrastive and self-supervised models in audio classification. CLaMP 3 variants excel in high-level tasks, like genre classification (GTZAN) and tagging (MTT), where capturing abstract musical meaning is crucial. MERT, however, performs better in low-level tasks such as key detection (GS) and pitch classification (Nsynth), where fine spectral detail is more important. Contrastive models generally struggle with short-duration audio (e.g., 4-second clips in Nsynth) because their focus on aligning longer segments with text limits their ability to capture fine acoustic details. These results suggest contrastive learning is better for semantic tasks, while selfsupervised models are more effective for detailed acoustic analysis, particularly for short clips. Table 10 shows that contrastive models, particularly CLaMP 3 variants, consistently outperform MERT across all MTG-Jamendo tasks. Notably, CLaMP 3 models achieve the highest scores in most tasks, demonstrating how diverse and highquality text annotations help contrastive models learn and capture complex musical semantics. In summary, contrastive models perform well in high-level classification tasks but struggle with short clips and fine-grained acoustic details. Their effectiveness heavily depends on the text annotations used during training. For instance, CLAP achieves strong results in instrument classification (Nsynth) because its training data is dominated by instrument and genre descriptions. However, it performs poorly in key detection (GS), where such annotations offer little relevant information. Table 11: Classification performance on WikiMT-X (1,000 entries, 8 genres) across different musical modalities and text annotations. Model ABC MIDI Audio Background Analysis Description Scene Accuracy CLaMP CLaMP 2 CLAP TTMR++ CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas F1-macro CLaMP CLaMP 2 CLAP TTMR++ CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas 0.7000 0.6800 - - 0.6850 0.7000 0.6850 0.7000 0.7150 0.6750 0.5252 0.5287 - - 0.5431 0.5345 0.5428 0.5499 0.5720 0. - 0.6350 - - 0.6100 0.6650 0.6350 0.6300 0.6800 0.6300 - 0.3784 - - 0.4005 0.5108 0.4171 0.3976 0.4967 0.4313 - - 0.6450 0.7150 0.7050 0.6850 0.6850 0.7200 0.7050 0.6850 - - 0.3943 0.4714 0.4755 0.4881 0.4589 0.5130 0.4995 0.4432 0.8050 0.7900 0.6950 0.7400 0.8200 0.8000 0.7850 0.8650 0.8400 0.8300 0.6835 0.6617 0.5913 0.6914 0.7424 0.7917 0.6626 0.8486 0.8123 0. 0.7900 0.8150 0.6800 0.7600 0.8350 0.8600 0.8550 0.8650 0.8550 0.8500 0.6486 0.6832 0.5491 0.6694 0.7933 0.8199 0.7439 0.8277 0.8225 0.8054 0.6900 0.7250 0.6500 0.6700 0.7800 0.7700 0.7750 0.7700 0.7800 0.7700 0.6079 0.6333 0.4921 0.6254 0.7639 0.7372 0.7318 0.6878 0.7484 0.7082 0.6250 0.6150 0.5550 0.5950 0.6550 0.6500 0.6500 0.6850 0.6650 0.6850 0.4447 0.3710 0.3100 0.4246 0.4780 0.4527 0.4260 0.5207 0.4742 0. D.3 Classification on WikiMT-X Table 11 presents classification results across different musical modalities (ABC, MIDI, Audio) and text annotations (Background, Analysis, Description, Scene) on WikiMT-X. Compared to the WikiMT results in Table 8, all models show substantial gains in genre classification accuracy and F1-macro for ABC and MIDI. This confirms that reannotating genre labels significantly reduced label noise, leading to more reliable classification. The improvements suggest that earlier inconsistencies in genre annotations were major limiting factor in classification performance. The reorganized label taxonomy and refined annotations in WikiMT-X provide more structured and consistent genre framework, making it more reliable benchmark for music classification. Across different musical modalities, the bestperforming models for ABC, MIDI, and Audio achieve comparable classification results. This suggests that genre-related features are well-preserved regardless of musical representation. Fig. 8a further supports this observation, showing clear genre boundaries across all modalities, indicating CLaMP 3 models can effectively extract genre information from both representations, reinforcing the idea that genre characteristics are consistently encoded in musical data. clear distinction emerges between text and music classification: models perform significantly better on text annotations (Background, Analysis, Description) than on music data. This is likely because text often contains explicit genre-related cues, making classification more direct. For example, descriptions like syncopated piano chords and walking bass strongly suggest jazz. In contrast, classifying music requires models to infer genre from intricate relationships between harmony, rhythm, and timbre. However, Scene classification behaves differently from other text-based categoriesit describes environmental settings rather than musical attributes, making its classification challenge more similar to music than text. Models trained solely on audio-text alignment (i.e., CLAP, TTMR++) perform worse in text classification, likely due to the limited diversity of annotations in large-scale audio-text datasets, which often list only instruments and genres. In contrast, symbolic-text datasets provide richer semantics, including background context and musicological analysis. CLaMP 3as is an exceptionthough its text encoder was fully updated during audio alignment, it achieves much stronger text classification than models like CLAP and TTMR++. This is likely due to M4-RAGs well-curated and diverse annotations, which offer broader and more expressive linguistic representation of musical content. Table 12: Results for English text-to-music retrieval on MusicCaps, reflecting data leakage in baseline models. Evaluations are conducted on both the full set and the AudioSet evaluation set. R/O denotes the use of rewritten or original captions, while F/C indicates retrieval using full tracks or clips. Model CLAP TTMR++ CLaMP 3as CLaMP 3sa CLaMP 3c2 sa CLaMP 3assa CLaMP 3saas CLaMP 3c2 saas Full Set (5,521 pairs) Eval Set (2,858 pairs) RF RC OF OC RF RC OF OC 0.0536 0.1410 0.0874 0.0741 0.0729 0.0830 0.0890 0.0973 0.0743 0.2315 0.0642 0.0591 0.0609 0.0592 0.0705 0.0737 0.0640 0.1757 0.0696 0.0530 0.0619 0.0743 0.0652 0.0762 0.0894 0.3155 0.0536 0.0431 0.0504 0.0530 0.0523 0. 0.0657 0.1248 0.1119 0.0934 0.0961 0.1045 0.1177 0.1180 0.0886 0.1341 0.0830 0.0735 0.0832 0.0784 0.0889 0.0933 0.0774 0.1219 0.0917 0.0661 0.0822 0.0897 0.0890 0.0961 0.1113 0.1382 0.0699 0.0572 0.0651 0.0723 0.0682 0."
        },
        {
            "title": "E Data Leakage of MusicCaps",
            "content": "MusicCaps, widely used text-to-music retrieval benchmark, includes 5,521 music-text pairs with 10-second audio clips. As subset of AudioSet, many models are trained on overlapping data, raising concerns about reliability, as they may memorize seen examples rather than learning true retrieval patterns. Table 12 shows text-to-music retrieval results on MusicCaps, examining data leakage in baseline models. We evaluate performance on the full dataset (Full Set) and the AudioSet evaluation subset (Eval Set), while also assessing the effects of caption rewording (Original vs. Rewritten) and audio length (Clip vs. Full Track). Leakage varies across models: TTMR++ is the most affected, having been trained on MusicCaps pairs from the training set of AudioSet, exposing it to half the benchmark; CLAP, trained on the full AudioSet, has seen all MusicCaps audio; in contrast, CLaMP 3 has minimal exposure, with only 150 audio recordings appearing in M4-RAG. To mitigate leakage effects, we introduce rewritten captions generated using Qwen, ensuring semantic consistency while incorporating structured aspect listsdetailed annotations of key musical attributes such as instrumentation, mood, and rhythm. Additionally, we conduct retrieval on both 10-second clips and full-length tracks, forming four evaluation settings: RF: Rewritten captions with full tracks. RC: Rewritten captions with clips. OF: Original captions with full tracks. OC: Original captions with clips. Table 12 reveals clear data leakage. TTMR++ is the only model that performs worse on the evaluation set than on the full benchmark, despite the evaluation set containing fewer retrieval candidates, which should naturally lead to higher MRR scores. This suggests severe overfitting to seen MusicCaps training data. Additionally, both TTMR++ and CLAP show performance drops with rewritten captions and full-length tracks. For TTMR++, this suggests that these modifications help reduce leakage effects, though not entirely. For CLAP, the decline is likely due to rewritten captions incorporating more detailed semantic information from aspect lists, which may shift retrieval behavior. In contrast, all CLaMP 3 variants show improved performance with rewritten captions, likely due to M4-RAGs use of Qwen, making them more attuned to its text patterns. They also gain an advantage in full-track retrieval. While baseline models rely on 10-second clips and average embeddings across segments, CLaMP 3 processes up to 640 seconds of audio, enabling it to capture relationships across an entire track. In contrast, baselines extract semantics from isolated clips, restricting their ability to utilize long-form audio context effectively. These results raise broader concerns about benchmark reliability in text-to-music retrieval. Other benchmarks also face leakage risksSDD, for instance, comes from MTG-Jamendo, which was included in CLAPs training data. In contrast, WikiMT-X, manually curated for this study, mitigates leakage by sourcing audio from the web rather than existing datasets. However, since this audio remains publicly accessible, large-scale models may still have exposure. To further reduce leakage, future benchmarks should prioritize private or newly recorded datasets for unbiased evaluation."
        }
    ],
    "affiliations": []
}