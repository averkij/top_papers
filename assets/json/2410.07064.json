{
    "paper_title": "Data Selection via Optimal Control for Language Models",
    "authors": [
        "Yuxian Gu",
        "Li Dong",
        "Hongning Wang",
        "Yaru Hao",
        "Qingxiu Dong",
        "Furu Wei",
        "Minlie Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection."
        },
        {
            "title": "Start",
            "content": "Yuxian Gu1,2, Li Dong2, Hongning Wang1, Yaru Hao2, Qingxiu Dong2, Furu Wei2, Minlie Huang1 1The CoAI Group, Tsinghua University 2Microsoft Research https://aka.ms/GeneralAI"
        },
        {
            "title": "Abstract",
            "content": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs capabilities for downstream usage. We formulate data selection as generalized Optimal Control problem, which can be solved theoretically by Pontryagins Maximum Principle (PMP), yielding set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to 400B models trained on 10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection. 4 2 0 2 9 ] . [ 1 4 6 0 7 0 . 0 1 4 2 : r (a) Scaling of Training Computation (1.7B) (b) Scaling of Model Size Figure 1: Scaling curves of average accuracy on 9 widely-used downstream tasks with respect to computation (a) and model sizes (b). We select pre-training corpora from the CommonCrawl and pre-train LMs on the selected data. PDS is compared with the Redpajama data cleaning pipeline [71]. The computation curves are calculated based on the training of 1.7B LM. guyx21@mails.tsinghua.edu.cn Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "With the thriving of language models (LMs; 38, 10), the role of data selection for pre-training becomes increasingly important, which aims at identifying valuable pre-training instances to accelerate model learning or improve downstream performance [4]. This focus enables researchers to explore the limit of LMs in the face of increasing training data demands [12, 60, 67]. It also helps to reduce the computational costs during pre-training [66], and addresses the potential limitations caused by available Internet data [74, 59]. Without doubt, pre-training data selection is highly valuable for both research and industry sectors. Unlike previous works relying primarily on manually crafted heuristics [70, 79], we connect data selection with classical Optimal Control theory [47], where control variables in dynamic system are optimized to achieve desired objectives. This mathematical formulation allows fine-grained white-box analysis of how the control variables drive dynamic system from one state to another. In particular, by treating data selection as the control variables (i.e., whether data point is included in pre-training), the LM pre-training process as the dynamic system, and the LMs downstream performance as the objective, we leverage Pontryagins Maximum Principle (PMP; 63) to derive the necessary conditions for optimal data selection in theory. These results offer rigorous, theory-driven alternative to the ad-hoc trial-and-error practices that currently dominate LM pre-training. Based on our theoretical results, we introduce PMP-based Data Selection (PDS), framework that selects high-quality pre-training data at scale, by solving the equation system induced by the PMP conditions. Balancing effectiveness and efficiency, PDS first solves the equation system for the optimal data selection on proxy dataset (e.g., 0.2B tokens), assigning quality score to each instance based on its impact on downstream tasks. After that, data scorer (e.g., 125M) is trained to predict the quality scores and then infers scores on the target corpus (e.g., 50B tokens). Finally, the predicted scores guide data selection for pre-training LMs with various sizes (e.g., 1.7B) from scratch. Unlike previous pre-training data selection methods based on deduplication [70, 1], pattern matching [79], or single checkpoint performance [22, 43], which are agnostic to the pre-training process of LMs, PDS exploits the highly dynamic nature of LM pre-training through the theoretical optimal control perspective. On the other hand, compared to methods that incorporate signals from the LM training process in an online fashion [81, 75], PDS operates offline, before the training begins, which avoids additional training-time computation overhead and allows for training LMs with arbitrary configurations while performing PDS only once. Furthermore, PDS only filters the training corpus, leaving highly optimized pre-training pipelines largely intact. Most importantly, PDS enjoys strong theoretical basis, opening up the black box of understanding the impact of individual data points on LM pre-training. In our experiments, we select data from the CommonCrawl2 with PDS and pre-train LMs with 160M, 470M, 1B, and 1.7B parameters from scratch. We observe around 2 times speed-up in pre-training on the 1.7B LM and constant improvement in downstream tasks and language modeling performance across all model sizes compared to state-of-the-art baselines. Extrapolating these results using the Scaling Law [44, 40], we show that the benefits remain consistent for 400B LMs trained on 15T tokens. Besides, PDS enhances data utilization in data-constrained setting, reducing the pre-training data demand by 1.8 times, which is critical advantage as the LM community is running out of data [74]. We also conduct extensive analysis and ablation studies on the key factors of PDS to facilitate further research on data selection."
        },
        {
            "title": "2 Method",
            "content": "2.1 Problem Formulation We study an LM parameterized with θ RN , pre-trained from scratch on dataset = {xn}D n=1, over total steps. Data selection [79, 22] aims at identifying subset from D, such that LMs trained on achieve better downstream performance, measured by lower downstream loss J(θ). The pre-training process renders J(θ) as function of D, which can be fully characterized by (cid:3) data quality score vector γ = (cid:2)γ1, γ2, , γD in D-dimensional simplex , where = 2https://commoncrawl.org/ 2 (cid:110) (cid:12) (cid:80)D [γ1, γ2, , γD](cid:12) (cid:111) 3. higher quality score in γ indicates the corresponding instance is more helpful to reduce J(θ), and thus the LM should learn more from the instance. This results in the following general pre-training loss, defined as the weighted sum of the per-instance loss by γ: n=1 γn = 1 and γn 0 for 1 L(θ, γ) = (cid:88) n= γnl(xn, θ), (1) where l(xn, θ) = log pθ(xn). The goal of data selection is thus to find γ that reduces the downstream loss J(θ), and then select instances with the highest scores in γ. For simplicity, we assume that the LM is trained using Gradient Decent (GD) for 0 < , with the derivation under the Adam optimizer [45] provided in Appendix C: θt+1 = θt ηL(θt, γ), (2) where θt represents the model parameters at the time step during GD and η is the learning rate. Optimization Problem. Motivated by the literature on learned optimizers [56], we optimize γ by minimizing the area under the curve (AUC; 18) of J(θt), which is approximated by the cumulative sum of J(θt) over the pre-training process: min γ (cid:88) t=1 J(θt), s.t. θt+1 = θt ηL(θt, γ), γ U. (3) Intuitively, lower AUC corresponds to faster convergence of the loss and improved final downstream performance. Unlike evaluating J(θt) at individual time steps, the AUC captures the overall LM training dynamics. As shown in Appendix A, minimizing the AUC essentially enhances the constants in the LMs Scaling Laws [44], leading to substantial improvements in LM learning. 2.2 Data Selection as Optimal Control We recognize the optimization problem in Eq. (3) is analogous to discrete time optimal control problem [47], where J() is the cost function, the model parameters θ are the state variables evolving according to Eq. (2), and the data quality scores γ are the control variables to be optimized within . This perspective provides convenient framework to formulate the pre-training data selection problem from more theoretical perspective. Theoretically Optimal Solution for Data Selection. Optimal control problems can be solved by powerful tool known as Pontryagins Maximum Principle (PMP; 63), which provides set of necessary conditions for the optimal control variables and their corresponding state variables (See Appendix for its formal expression). However, standard PMP conditions allow the optimal control to vary over time, whereas in Eq. (3), the control variables γ are constrained to be time-invariant due to the offline nature of data selection in our setting. This typically makes the optimization problem more challenging due to the shrinking of feasible region. In the following, we present the PMP conditions for data selection under this constraint: Theorem 2.1 (PMP Conditions for Data Selection). Let γ solve the problem in Eq. (3), and θ RN such that denote the LM parameters trained with γ. For 0 < , there exists vector λ (4) t+1 = θ θ = λ λ ηL(θ t+1 + J(θ , γ), θ 0 = θ0, ) η2L(θ , γ)λ = J(θ ), t+1, λ (cid:35) λ t+1 l(xn, θ ) , γ U, (5) (6) γ = arg max γ (cid:88) γn (cid:34)T 1 (cid:88) n= t=0 where 2L(θ , γ) denotes the Hessian matrix of L(θ, γ) with respect to θ evaluated at θ = θ . 3Only the relative data quality is meaningful for data selection. Therefore, we ensure the sum of the quality scores to 1 to avoid the impact of their individual scales. t+1 RN defines target vector aligning with Figure 2: An illustration of Theorem 2.1. Left: λ the optimization direction towards optimal data selection, as in Eq. (5). Right: data quality scores are positively correlated with how close the gradient direction of each instance is to the target direction, calculated as the dot-product between λ ) for = n, m, k, as in Eq. (6). t+1 and li,t = l(xi, θ We prove Theorem 2.1 in Appendix using the standard PMP conditions and the Lagrange multiplier method, and provide an illustration for this theorem in Figure 2. By inspecting the PMP conditions for data selection, we can see that Eq. (4) ensures the LM parameters, trained with the optimal data quality scores, continue to evolve via GD. As illustrated in Figure 2 (Left), Eq. (5) defines λ , target vector suggesting the ideal gradient direction formed only by high-quality data points. In particular, λ aggregates information about the downstream loss J(θt) with respect to current training step and the LMs training dynamics 2L(θ summarizes the dynamics of LM pre-training (i.e., from future to the current). Since γ , Eq. (6) essentially suggests that xn with higher (cid:80) ) value should obtain larger score in γ, as shown in Figure 2 (Right). This indicates that the instances whose gradients align closely with the target vector λ should be selected. Note that the PMP conditions for data selection form complete equation system, where θ , and γ are the solutions. In principle, by solving Eqs. (4)-(6) simultaneously, we can derive the optimal data quality scores, forming the foundation of our data selection framework PDS. , γ), from to t. As result, λ l(xn, θ , λ λ t+1 2.3 PDS: Data Selection based on PMP PDS selects pre-training data by solving the PMP conditions defined in Eqs. (4)-(6), and consists of three key components, as illustrated in Figure 3. To balance effectiveness and efficiency, PDS first uniformly samples proxy dataset Dprx from the pre-training corpus D. Algorithm 1, derived from the PMP conditions, is then applied to Dprx to compute data quality scores for each instance (Section 2.3.1). Then, data scorer, typically small LM, is fine-tuned to predict the quality scores based on the instances in Dprx. The learnt data scorer is subsequently applied to infer quality scores on the entire pre-training corpus (Section 2.3.2). Finally, the instances with large scores are selected to form high-quality corpus D, which is used to pre-train LMs with any size (Section 2.3.3). 2.3.1 Data Quality Scores from PMP Algorithm 1 solves the PMP conditions for data selection iteratively and returns the data quality scores γ. We elaborate on the details of the algorithm and provide an efficient implementation. obtained in the outer iterations. Overview: Algorithm 1 solves bi-level optimization problem, consisting of an outer loop to based on the current γ. γ is first uniformly and λ compute γ and two inner loops to compute θ initialized and then updated for To epochs, based on θ and λ Inner loops: In each iteration of the outer loop, we run the forward inner loop to compute θ according to Eq. (4) from = 0 to = 1, equivalent to training the LM with GD using the current data quality scores to re-weight the per-instance losses. After that, λ is computed from = 1 to = 0 with Eq. (5) in the reverse inner loop, based on θ Update of γ: γ is supposed to be updated according to Eq. (6) with θ computed in the inner loops. Eq. (6) indicates that the new γ should be set as one-hot vector, where the element corresponding to the highest (cid:80)T 1 l(xn, θ ) value is set to 1 and the others are set to 0. However, this hard update is unstable, as it causes training the LM with only one example in the upcoming outer loop iteration. Therefore, Algorithm 1 adopts soft update, which increases γ by value proportional to (cid:80)T 1 t+1l(xn, θt) and projects the updated γ back into . obtained from the forward inner loop. and λ t=0 λ t=0 λ t+1 4 Figure 3: The PDS framework. We compute data quality scores γ on proxy dataset Dprx using Algorithm 1, which is derived from the Pontryagins Maximum Principle [63] (Section 2.3.1). After that, the data scorer learns to predict quality scores from instances, which then infers scores for large corpus (Section 2.3.2). Finally, high-quality corpus is selected based on the inferred scores to pre-train an LM (Section 2.3.3). Algorithm 1 PMP-Solver Input: LM learning rate η. Outer loop learning rate α. Outer loop epochs To. Training data before selection D. Downstream loss J(θ). Training steps . Proj[] that projects point in RD to . Model initialization θ0. Output: Data quality scores γ. (cid:104) 1 , 1 γ = (cid:2)γ1, γ2, , γD repeat To times (cid:3) , , 1 (cid:105) ; for = 0, 1, , 1 do θt+1 θt ηL(θt, γ) end for λT J(θT ) for = 1, 2, , 1 do λt λt+1 + J(θt) η2L(θt, γ)λt+1 end for for = 1, 2, , do γn γn + α (cid:80)T t=0 λ t+1l(xn, θt) end for γ Proj [γ] end and return γ Outer loop Forward inner loop Corresponding to Eq. (4) Reverse inner loop Corresponding to Eq. (5) Corresponding to Eq. (6) Efficient Implementation. Running Algorithm 1 on Dprx based on the learning of large LM remains computationally intensive, as the inner loops involve training the LM for all steps with GD and computing the Hessian matrix. Therefore, we limit the outer loop to just one epoch and employ stochastic gradient descent (SGD) with small batch size in the inner loops, which is based on small proxy LM with prx parameters (N prx ) to be trained for prx steps (T prx ). To recover any lost long-range training dynamics, we run Algorithm 1 multiple times by setting θ0 to the checkpoints at different large-scale pre-training stages of the proxy LM and then average the obtained data quality scores on Dprx. Specifically, we first train the proxy LM for steps and save checkpoints steps. Then, the quality scores are given by 0 , , θ(M ) (cid:104) 0 , θ(2) θ(1) in every (cid:105) 0 γ = 1 (cid:88) m=1 PMP-Solver (cid:16) = Dprx, = prx, θ0 = θ(m) 0 (cid:17) , To = 1 , (7) where PMP-Solver refers to Algorithm 1. We also incorporate several practical optimization techniques to further reduce the computational overhead, as described in Appendix E.1. 2.3.2 Data Scorer We fine-tune small LM as the data scorer to fit the data quality scores γ on Dprx. Specifically, each instance in Dprx is encoded by averaging the output hidden states of the data scorer. The representation of each instance is then passed through linear head, outputting scalar. The linear 5 head and the LM are trained together to fit γ on Dprx with the Mean Square Error loss: ϕ, w, = arg min ϕ,w,b Dprx (cid:88) (wh(xprx , ϕ) + γ n)2, n= (8) is the nth instance in Dprx, ϕ is the parameters of the data scorer, and h(, ) Rd is the where xprx average output hidden states of an LM along the sequence length, with representing the hidden state size. Rd, are the parameters of the linear head. After fine-tuning, we infer the data quality scores of the instances in with the data scorer, where the quality score for xn is given by γ(xn) = wh(xn, ϕ) + b. 2.3.3 Data Selection We use the output scores from the data scorer to estimate the value of the instances in to select the final pre-training corpus for the large LM. Given the importance of data diversity in pre-training LMs, we adopt Gumbel-Top-K to introduce randomness into the selection process: = Top-K {γ(xn) τ log( log(un)) xn D, 1 D}, (9) where u1, u2, , uD are independently sampled from Uniform(0, 1) and τ is hyper-parameter to control the strength of the Gumbel noise. The size of the selected data is managed by data selection ratio r, with = rD in our experiments. 2.4 Discussion Effectiveness of PDS. Compared to existing offline data selection methods that curate the pretraining corpus before the LM training starts using pattern information [79], deduplication [70, 1], or single-step checkpoints [22], PDS incorporates long-range training dynamics into data selection, as reflected by the target vector λ in Eq. (5). This can be critical for selecting high-quality instances, given the highly dynamic nature of LM pre-training. Although we run Algorithm 1 in proxy environment and transfer the quality scores to the large-scale setting via the data scorer, many previous works [80, 81] have shown that data quality information is learnable and transferable across model sizes. Different LMs also share many common training dynamics [69]. Efficiency and Flexibility of PDS. Unlike recent online data selection approaches to incorporate LM training dynamics [81, 75], PDS selects the pre-training corpus offline. This allows PDS to be run only once and used for pre-training multiple LMs of any sizes, without incurring additional computational overhead. The FLOPs needed by PDS in proxy environment are also negligible compared to the demands of large-scale pre-training, as shown in complexity analysis in Section 3.3. Besides, the offline nature of PDS makes it flexible to be integrated into optimized pre-training pipelines [15] by simply replacing the data sources."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup Data. We use the CommonCrawl split from Redpajama [71] as to exclude the influence of domain weights [80]. During pre-processing, multiple documents may be merged into single pre-training instance with 1,024 tokens. For the downstream loss J(θ), we compute the LMs loss on the training split of LIMA [84], high-quality dataset consisting of 1,030 diverse instruction-response pairs that cover wide range of downstream scenarios. Our evaluation is conducted on various downstream datasets other than LIMA to avoid over-fitting. Model. We adopt the same model architecture as Mistral [42] and pre-train LMs with 160M, 470M, 1B, and 1.7B parameters. Model configurations are detailed in Table 6. PDS. To compute the data quality scores from PMP, we adopt 160M proxy LM. Dprx consists of 160K instances uniformly sampled from D. We first pre-train the proxy LM on for 50K steps and then select checkpoints at [10K, 20K, 30K, 40K, 50K] steps. Initialized from these checkpoints, 6 HS LAMB Wino. OBQA ARC-e ARC-c PIQA SciQ BoolQ Avg. Conventional RHO-Loss DSIR IF-Score PDS Conventional RHO-Loss DSIR IF-Score PDS 32.2 32.2 32.8 32.2 33.5 36.7 36.6 36.4 36.6 37. 34.9 35.3 35.7 35.7 38.2 41.4 42.4 42.6 41.8 44.6 Model Size = 160M 25.6 28.1 26.6 27.4 28.4 40.9 40.5 41.2 40.8 42.3 Model Size = 470M 30.4 29.4 29.8 29.6 29.8 44.8 43.7 46.0 44.7 46.5 22.5 24.1 23.8 22.6 24.1 25.2 25.2 24.7 25.1 25.8 51.4 53.2 52.5 51.1 51.4 52.4 53.0 51.7 53.4 52. 58.3 58.5 57.8 57.6 59.2 61.0 60.4 61.0 60.8 61.8 65.5 63.1 68.7 64.1 68.8 70.6 72.8 72.0 68.8 73.8 57.6 53.0 54.7 55.8 58.7 60.4 59.8 55.8 58.7 61. 43.2 43.1 43.8 43.0 45.0 47.0 47.0 46.7 46.6 48.2 Table 1: Results on the downstream evaluation datasets in OLMo [31]. We report the accuracy scores and the average scores across the datasets. The best scores of each model size are boldfaced. PDS achieves the best performance in most cases compared to the baselines. the proxy LM undergoes inner loops with η = 0.008 over prx = 100 steps with mini-batch size of 256. γ is updated for one outer loop epoch with α = 1. For the data scorer, we fine-tune 125M Fairseq-Dense model [5] along with the linear head, using the objective in Eq. (8). The training details for the data scorer can be found in Appendix E.2. For Data Selection, we set δ = 0.1, = 0.4, with further hyper-parameter exploration provided in Appendix G.3. Pre-Training. We pre-train all LMs for 100k steps, using batch size of 512 and max input length of 1,024, resulting in roughly 50B tokens. In Section 3.2, we select 50B-token dataset from CC corpus containing 125B tokens to assess how different data selection methods improve LM learning given sufficiently large D. In Section 3.3 (Data-Constrained Setting), we also analyze the effectiveness of PDS when is limited in size. See Appendix E.3 for more pre-training details. Evaluation. We evaluate the LMs 0-shot accuracy on the downstream test datasets used in OLMo [31] and their 0/5-shot accuracy on MMLU [39]. We also report the LMs language modeling loss on subset of DCLM [48], high-quality corpus curated with complex pipelines and human heuristics, to verify that models trained on retain diversity and long-tail knowledge coverage. Further evaluation details are in Appendix E.4. Baselines. We compare PDS with conventional pre-training and 3 offline data selection methods: Conventional: conventionally pre-training LM on 50B tokens uniformly sampled from D, also referring to Redpajama in Figure 1. RHO-Loss [58]: selecting data with high reducible losses, as in Eq. (51). DSIR [79]: selecting data with high n-gram overlap with instances in LIMA. IF-Score [43]: selecting data with high influence scores, as in Eq. (52). 3.2 Main Results PDS Improves Downstream Performance. We present the evaluation results of the pre-trained LMs on the OLMo evaluation datasets and MMLU in Table 1 and Table 2, respectively. As shown, PDS outperforms the baselines on most datasets, achieving the best overall performance across models with 160M and 470M parameters. Figure 1(b) shows the scaling curves of average accuracy on the OLMo evaluation sets with respect to the model sizes, which range from 160M to 1.7B, demonstrating that the performance improvement remains consistent as the model scales up. These results indicate that the data quality scores obtained in proxy environment generalize well to various model sizes and downstream tasks. 7 Figure 4: Test losses on the DCLM corpus [48] for 160M, 470M, 1B and 1.7B LMs. N"
        },
        {
            "title": "PDS",
            "content": "GPT-3 [12] Llama [72] Llama 2 [73] Llama 3.1 [21] 175B 300B 1.0T 6.7B 2.0T 70B 15T 405B 2.882 2.942 2.877 2.851 2.872 2.896 2.855 2.838 Table 3: Test loss extrapolation using the Scaling Law [40]. We predict the test loss when the LM size and the trained tokens meet that of GPT-3 175B, Llama 6.7B, Llama 2 70B, and Llama 3.1 405B. The improvements of PDS remain consistent for these LMs. PDS Enhances Language Modeling. Besides downstream tasks, we show that PDS also enhances language modeling on high-quality pre-training corpus. Figure 4 shows the test losses on the DCLM subset for conventionally pre-trained LMs and PDS-trained LMs with 160M, 470M, 1B, and 1.7B parameters. We can see that PDS-trained LMs constantly achieve better performance across various model sizes. In Table 3, we extrapolate the test losses using the Scaling Law [40], showing that the improvements with PDS persist in pre-training recent large LMs, such as GPT-3 [12] and Llama family [72, 73, 21]. Details of the extrapolation are provided in Appendix G.2. The DCLM corpus is curated using complex pipeline based on human heuristics and is verified to be diverse and comprehensive in its coverage of human knowledge. Algorithm 1 offers principled alternative to the complex pipeline for curating pre-training corpus. Method 0-shot 5-shot 160M 470M Conventional RHO-Loss DSIR IF-Score PDS Conventional RHO-Loss DSIR IF-Score PDS 27.2 27.3 27.4 27.4 27.4 27.6 28.4 28.0 28.4 28.9 26.7 27.1 27.3 27.1 27.7 28.5 28.5 28.2 28.8 29. Table 2: The 0/5-shot accuracy scores on MMLU. The best scores of each model size are boldfaced. PDS Accelerates LM Learning. In Figure 1(a), we plot the average accuracy scores on the OLMo evaluation datasets with respect to the training FLOPs of the 1.7B model. PDS achieves 2.0 times acceleration in terms of training-time computation compared to conventional pre-training. Similar trends are observed for other model sizes (Figure 8) and DCLM losses (Figure 4). 3.3 Analysis Data-Constrained Setting. In Section 3.2, we assume the original pre-training data is unlimited to ensure that the PDS-selected corpus contains 50B tokens, the same as in conventional pre-training. In practice, when the data before selection is limited, the LM has to be trained on the PDS-selected data for multiple epochs. In Figure 5, we restrict to contain 50B tokens and apply PDS with selection ratios [0.125, 0.25, 0.5], corresponding to training the LM on the selected data for [8, 4, 2] epochs, respectively. We can see that selecting 1/4 data with PDS and training for 4 epochs achieves the lowest test losses, consistent with the findings of Muennighoff et al. [59]. This suggests that PDS improves data utilization as the high-quality web-crawled corpora run out [74]. We extrapolate the 8 Figure 5: Test losses on DCLM corpus [48] in the data-constrained setting. We select data with PDS for different selection ratios and train the model for multiple epochs to reach the same token number budgets. Figure 6: Comparison between exact and efficient implementation to solve the data quality scores in simulated setting. The effectiveness is measured by J(θt). The efficient implementation saves computation and preserves most of the performance of the exact solution. Complexity Proxy γ-solver O(N prxD + 4M prxDprx) O(3N scoreDprx + scoreD) Data Scorer Data Selection O(D) PDS Pre-Training O(N D) FLOPs (1020) Actual Time 0.49 0.063 0.0 5.1 15.2 Hours 1.50 Hours 10.2 Minutes 144 Hours Table 4: Asymptotic complexity, GPU FLOPs, and actually spent time of different PDS steps and 1.7B model pre-training. prx and Dprx are the sizes of the proxy LM and proxy data. score is the size of the data scorer. We elaborate on the details of how the complexity and FLOPs are computed in Appendix F. PDS costs little computational overhead compared to pre-training large LMs. loss curve of conventional pre-training with the Scaling Law suggested by [59], and find that it requires another 42B tokens to achieve the performance of PDS (r = 0.25, 4 Eps), which means PDS reduces the pre-training data requirement by 1.8 times. Efficient Implementation for Solving Data Quality Scores. In Section 2.3.1, we introduce efficient implementations to solve the data quality scores by running Algorithm 1 for single outer epoch, leveraging small proxy LM trained for limited number of steps. In Figure 6, we use simulated setting where γ can be obtained exactly from Algorithm 1 within an affordable computational budget and compare its performance (measured by J(θt)) and overhead (measured in FLOPs) with our efficient implementation. Details of this simulated setting can be found in Appendix E.6. The results show that the efficient implementation significantly reduces computational overhead while preserving most of the performance of the exact solution. Complexity Analysis. We compare the computational complexity of PDS with pre-training in Table 4. The overhead of running PDS to select data is only about 1/9 of that of pre-training 1.7B model. More importantly, since PDS is an offline method, the selected corpus can be used to pre-train any number of LMs without additional computational cost. In addition, the offline nature of PDS allows it to seamlessly integrate into recent highly optimized pre-training pipelines [15], requiring only replacement of the data source without altering the pre-training process. 3.4 Ablation Studies on PMP-Solver Training Dynamics Information. Incorporating the LMs training dynamics into data selection is key distinction between PDS and other offline data selection approaches. While IF-Score also uses the gradient information of well-trained LMs to estimate data importance, we find that the long-range training dynamics in early training stages are more valuable. Table 5 shows the results when different types of learning information are considered. PDS (50K) refers to using the LM checkpoint at 50K as θ(m) in Eq. (7). PDS (T prx = 1) means running the inner loops as in Eq. (4) and Eq. (5) for only 0 9 Conventional IF-Score PDS (50K) PDS (T prx=1) PDS (50K-100K) PDS (10K-50K, ours) Corr. Acc. - 0.32 0.51 0.54 0.48 0.52 43.2 43.0 44.0 44.6 43.4 45. Table 5: Data selection based on different kinds of learning information. We report the LMs zero-shot accuracy on the OLMo evaluation tasks (Acc.) and the Spearman Correlation of the data scorer (Corr.). Figure 7: LM performance on the OLMo evaluation tasks (Average Accuracy) and data scorer performance (Spearman Correlation) when different proxy model and proxy data sizes are adopted. one step, excluding long-range information. PDS (50K-100K) refers to setting θ(m) to checkpoints at later training stages. Comparing our choice with IF-Score, PDS (50K), and PDS (T prx = 1), we conclude that the long-range training dynamics is more valuable than single-step gradient, although it may be slightly harder for the data scorer to fit. Our choice also outperforms PDS (50K-100K), indicating that the early-stage training dynamics are more beneficial than those from later stages. An explanation is that LMs conduct large-range parameter searching during early training and converge to local optimums in the late stages. Therefore, early-stage information helps the LM choose better local optimums, which can always be achieved by later annealing. 0 Proxy Model and Proxy Data. In Figure 7, we explore how the sizes of the proxy LM and the proxy dataset affect the performance of the data scorer and the pre-trained LM. As the size of the proxy LM increases, the LMs downstream performance increases, but the data scorers performance decreases. We notice that when using the 8.7M model, the LMs performance (44.0) is close to that of DSIR (43.8), which selects data based on n-gram matching. This implies that small LMs estimate data quality primarily through shallow patterns that are easy to learn. More complex LM learning information is encoded in the data quality scores computed based on larger proxy LMs, making it harder for the data scorer to fit, but this can be mitigated by increasing the size of Dprx."
        },
        {
            "title": "4 Related Work",
            "content": "Data Selection for Language Models. Many works have explored data selection approaches to accelerate LM learning or improve downstream performance [4]. Some curate data prior to LM training, which we refer to offline methods, including domain-mixing [80, 25, 26], data pruning [55, 70, 1], sample-wise data selection [78, 20, 79, 35], and data programming [65, 33, 34]. Other works dynamically select data during LM training by adjusting domain-mixing weights [77, 13, 3] or more fine-grained reweighting strategies [24, 30, 36, 68]. This work studies data selection from its general principles, theoretically deriving optimal selection and designing scalable algorithms to implement it. Optimal Control in Deep Learning. The principles of optimal control [47] have been shown to be effective in deep learning [7, 53], by treating the forward pass of multi-layer neural network as control process where the hidden vectors are state parameters and the model parameters are control variables to optimize. With the help of Pontryagins Maximum Principle [63], some works design optimization algorithms with better convergence rates [50, 83] or broader application scenarios [49], and others provide theoretical foundations of neural networks for better interpretation [37, 28]. Unlike these works, we adopt Optimal Control in novel and orthogonal way, by treating the models learning pass as the control process."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we investigate selecting pre-training data of LMs from both theoretical and practical perspectives. We first formulate data selection as an Optimal Control problem to derive set of 10 necessary conditions for optimal data selection using Pontryagins Maximum Principle (PMP), which establishes theoretical principles for LM pre-training. Then, building on these theoretical results, we introduce PDS, practical framework that solves the PMP conditions in practice based on long-range LM training dynamics. Extensive experiments show that PDS selects high-quality pre-training corpora that accelerate LM learning and boost LM performance across various model sizes and downstream tasks. We find that PDS also improves data utilization in data-constrained settings, which mitigates the pre-training data exhaustion problem."
        },
        {
            "title": "References",
            "content": "[1] A. K. M. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos. SemDeDup: Dataefficient learning at web-scale through semantic deduplication. In ICLR 2023 Workshop, 2023. URL https://openreview.net/forum?id=4vlGm9gv6c. [2] N. Agarwal, B. Bullins, and E. Hazan. Second-order stochastic optimization for machine learning in linear time. JMLR, 2017. URL https://www.jmlr.org/papers/volume18/ 16-491/16-491.pdf. [3] A. Albalak, L. Pan, C. Raffel, and W. Y. Wang. Efficient online data mixing for language model pre-training. In NeurIPS workshop, 2023. URL https://openreview.net/forum? id=9Tze4oy4lw. [4] A. Albalak, Y. Elazar, S. M. Xie, S. Longpre, N. Lambert, X. Wang, et al. survey on data selection for language models. TMLR, 2024. URL https://openreview.net/forum?id= XfHWcNTSHp. [5] M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. In Proceedings EMNLP, 2022. URL https://aclanthology.org/2022.emnlp-main.804/. [6] H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. CMS Books in Mathematics. Springer, 2011. URL https://doi.org/10. 1007/978-1-4419-9467-7. [7] M. Benning, E. Celledoni, M. J. Ehrhardt, B. Owren, and C.-B. Schönlieb. Deep learning as optimal control problems: Models and numerical methods. arXiv preprint arXiv:1904.05657, 2019. URL https://arxiv.org/abs/1904.05657. [8] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. URL https://arxiv.org/abs/2401.02954. [9] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of AAAI, 2020. URL https://ojs.aaai.org/index.php/ AAAI/article/view/6239/6095. [10] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL https://arxiv.org/abs/2108.07258. [11] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. [12] T. Brown, B. Mann, N. Ryder, M. Subbiah, et al. Language models are few-shot learners. In Proceedings of NeurIPS, 2020. URL https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. [13] M. Chen, N. Roberts, K. Bhatia, J. Wang, C. Zhang, F. Sala, and C. Ré. Skill-it! datadriven skills framework for understanding and training language models. In Proceedings of NeurIPS, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/70b8505ac79e3e131756f793cd80eb8d-Paper-Conference.pdf. 11 [14] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. URL https://arxiv.org/abs/1604.06174. [15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, et al. PaLM: Scaling language modeling with pathways. JMLR, 2023. URL http://jmlr.org/papers/v24/ 22-1144.html. [16] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of NAACL-HLT, 2019. URL https://aclanthology.org/N19-1300. [17] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. URL https://arxiv.org/abs/1803.05457. [18] C. Cortes and M. Mohri. AUC optimization vs. error rate minimization. In Proceedings of NeurIPS, 2003. URL https://proceedings.neurips.cc/paper/2003/hash/ 6ef80bb237adf4b6f77d0700e1255907-Abstract.html. [19] M. Dagréou, P. Ablin, S. Vaiter, and T. Moreau. How to compute hessian-vector products? In ICLR Blogposts 2024, 2024. URL https://iclr-blogposts.github.io/2024/blog/ bench-hvp/. [20] Q. Du, C. Zong, and J. Zhang. Mods: Model-oriented data selection for instruction tuning. arXiv preprint arXiv:2311.15653, 2023. URL https://arxiv.org/abs/2311.15653. [21] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. [22] L. Engstrom, A. Feldmann, and A. Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. URL https://arxiv.org/abs/2401.12926. [23] L. C. Evans. An introduction to mathematical optimal control theory. University of California, 2005. URL https://math.berkeley.edu/evans/control.course.pdf. [24] S. Fan and M. Jaggi. Irreducible curriculum for language model pretraining. arXiv preprint arXiv:2310.15389, 2023. URL https://arxiv.org/abs/2310.15389. [25] S. Fan, M. Pagliardini, and M. Jaggi. DOGE: Domain reweighting with generalization esIn Second Agent Learning in Open-Endedness Workshop, 2023. URL https: timation. //openreview.net/forum?id=qiKqsqwYXm. [26] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. URL https://arxiv.org/abs/2101.00027. [27] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, et al. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/ records/12608602. [28] B. Geshkovski and E. Zuazua. Turnpike in optimal control of pdes, resnets, and beyond. Acta Numer., 31:135263, 2022. doi: 10.1017/S0962492922000046. URL https://doi.org/10. 1017/S0962492922000046. [29] A. Gokaslan, V. Cohen, E. Pavlick, and S. Tellex. Openwebtext corpus, 2019. URL http: //Skylion007.github.io/OpenWebTextCorpus. [30] D. Grangier, P. Ablin, and A. Hannun. Adaptive training distributions with scalable online bilevel optimization. arXiv preprint arXiv:2311.11973, 2023. URL https://arxiv.org/ abs/2311.11973. [31] D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. H. Jha, H. Ivison, I. Magnusson, Y. Wang, et al. OLMo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. URL https://arxiv.org/abs/2402.00838. 12 [32] R. Grosse, J. Bae, C. Anil, N. Elhage, A. Tamkin, A. Tajdini, B. Steiner, D. Li, E. Durmus, E. Perez, et al. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023. URL https://arxiv.org/abs/2308.03296. [33] Y. Gu, X. Han, Z. Liu, and M. Huang. PPT: Pre-trained prompt tuning for few-shor learning. In Proceedings of ACL, 2022. URL https://arxiv.org/abs/2109.04332. [34] Y. Gu, P. Ke, X. Zhu, and M. Huang. Learning instructions with unlabeled data for zero-shot cross-task generalization. In Proceedings of EMNLP, 2022. URL https://arxiv.org/abs/ 2210.09175. [35] Y. Gu, L. Dong, F. Wei, and M. Huang. Pre-training to learn in context. In Proceedings of ACL, 2023. URL https://aclanthology.org/2023.acl-long.267/. [36] Y. Gu, L. Dong, Y. Hao, Q. Dong, M. Huang, and F. Wei. Towards optimal learning of language models. arXiv preprint arXiv:2402.17759, 2024. URL https://arxiv.org/abs/2402. 17759. [37] J. Han, Q. Li, et al. mean-field optimal control formulation of deep learning. Research in the Mathematical Sciences, 2019. URL https://link.springer.com/article/10.1007/ s40687-018-0172-y. [38] X. Han, Z. Zhang, N. Ding, Y. Gu, et al. Pre-trained models: Past, present and future. AI Open, 2:225250, 2021. doi: 10.1016/J.AIOPEN.2021.08.002. URL https://doi.org/10.1016/ j.aiopen.2021.08.002. [39] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring In Proceedings of ICLR, 2021. URL https: massive multitask language understanding. //openreview.net/forum?id=d7KBjmI3GmQ. [40] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, et al. Training compute-optimal large language models. In Proceedings of NeurIPS, 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf. [41] P. J. Huber. Robust Estimation of Location Parameter. Springer New York, 1992. doi: 10.1007/ 978-1-4612-4380-9_35. URL https://doi.org/10.1007/978-1-4612-4380-9_35. [42] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. URL https://arxiv.org/abs/2310.06825. [43] J. Kaddour, O. Key, P. Nawrot, P. Minervini, and M. J. Kusner. No train no gain: ReIn Procedvisiting efficient training algorithms for transformer-based language models. ings of NeurIPS, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 51f3d6252706100325ddc435ba0ade0e-Abstract-Conference.html. [44] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. [45] D. P. Kingma and J. Ba. Adam: method for stochastic optimization. In Proceedings of ICLR, 2015. URL https://openreview.net/forum?id=8gmWwjFyLj. [46] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Proceedings of KR, 2012. URL https://dl.acm.org/doi/10.5555/3031843.3031909. [47] F. L. Lewis, D. Vrabie, and V. L. Syrmos. Optimal control. John Wiley & Sons, 2012. [48] J. Li, A. Fang, G. Smyrnis, M. Ivgi, M. Jordan, S. Gadre, H. Bansal, E. Guha, S. Keh, K. Arora, et al. DataComp-LM: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. URL https://arxiv.org/abs/2406.11794. 13 [49] Q. Li and S. Hao. An optimal control approach to deep learning and applications to discreteweight neural networks. In Proceedings of ICML, 2018. URL http://proceedings.mlr. press/v80/li18b.html. [50] Q. Li, L. Chen, C. Tai, and W. E. Maximum principle based algorithms for deep learning. JMLR, 2017. URL http://jmlr.org/papers/v18/17-653.html. [51] Z. Lin, Z. Gou, Y. Gong, X. Liu, Y. Shen, R. Xu, C. Lin, Y. Yang, J. Jiao, N. Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. URL https://arxiv.org/abs/2404.07965. [52] D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 1989. URL https://link.springer.com/article/10. 1007/BF01589116. [53] G.-H. Liu and E. A. Theodorou. Deep learning theory review: An optimal control and dynamical systems perspective. arXiv preprint arXiv:1908.10920, 2019. URL https://arxiv.org/ abs/1908.10920. [54] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In Proceedings of ICLR, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. [55] M. Marion, A. Üstün, L. Pozzobon, A. Wang, M. Fadaee, and S. Hooker. When less is more: Investigating data pruning for pretraining LLMs at scale. In NeurIPS Workshop on Attributing Model Behavior at Scale, 2023. URL https://openreview.net/forum?id=XUIYn3jo5T. [56] L. Metz, N. Maheswaranathan, C. D. Freeman, B. Poole, and J. Sohl-Dickstein. Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves. arXiv preprint arXiv:2009.11243, 2020. URL https://arxiv.org/abs/2009. 11243. [57] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of EMNLP, 2018. URL https://api.semanticscholar.org/CorpusID:52183757. [58] S. Mindermann, J. M. Brauner, M. Razzak, M. Sharma, A. Kirsch, W. Xu, B. Höltgen, A. N. Gomez, A. Morisot, S. Farquhar, and Y. Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, editors, Proceedings of ICML, 2022. URL https://proceedings. mlr.press/v162/mindermann22a.html. [59] N. Muennighoff, A. M. Rush, B. Barak, T. L. Scao, N. Tazi, A. Piktus, S. Pyysalo, T. Wolf, and C. Raffel. Scaling data-constrained language models. In Proceedings of NeurIPS, 2023. URL https://openreview.net/forum?id=j5BuTrEj35. [60] OpenAI. GPT-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774. [61] D. Paperno, G. Kruszewski, A. Lazaridou, N.-Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of ACL, 2016. URL https://aclanthology.org/P16-1144.pdf. [62] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, et al. PyTorch: An imperative style, high-performance deep learning library. In Proceedings of NeurIPS, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html. [63] L. S. Pontryagin. Mathematical theory of optimal processes. Routledge, 2018. URL https: //doi.org/10.1201/9780203749319. [64] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of SC, 2020. [65] A. J. Ratner, C. D. Sa, S. Wu, D. Selsam, and C. Ré. Data programming: Creating large training sets, quickly. In Proceedigns of NeurIPS, 2016. URL https://proceedings.neurips.cc/ paper/2016/hash/6709e8d64a5f47269ed5cea9f625f7ab-Abstract.html. 14 [66] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. Morcos. Beyond neuIn Proceedings ral scaling laws: of NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 7b75da9b61eda40fa35453ee5d077df6-Abstract-Conference.html. law scaling via data pruning. beating power [67] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. URL https://arxiv.org/abs/2312.11805. [68] M. Thakkar, T. Bolukbasi, S. Ganapathy, S. Vashishth, S. Chandar, and P. Talukdar. Selfinfluence guided data reweighting for language model pre-training. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of EMNLP, 2023. URL https://aclanthology.org/2023. emnlp-main.125/. [69] K. Tirumala, A. Markosyan, L. Zettlemoyer, and A. Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. In Proceedings of NeurIPS, 2022. URL https://openreview.net/forum?id=u3vEuRr08MT. [70] K. Tirumala, D. Simig, A. Aghajanyan, and A. Morcos. improving LLM pretraining via document de-duplication and diversification. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Proceedings of NeurIPS, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ a8f8cbd7f7a5fb2c837e578c75e5b615-Abstract-Datasets_and_Benchmarks.html. D4: [71] Together. Redpajama: an open dataset for training large language models, October 2023. URL https://github.com/togethercomputer/RedPajama-Data. [72] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. URL https://arxiv.org/abs/2302.13971. [73] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. URL https://arxiv.org/abs/2307.09288. [74] P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint arXiv:2211.04325, 2022. URL https://arxiv.org/abs/2211.04325. [75] J. T. Wang, P. Mittal, D. Song, and R. Jia. Data shapley in one training run. arXiv preprint arXiv:2406.11011, 2024. URL https://arxiv.org/abs/2406.11011. [76] J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text (ACL 2017), 2017. URL https://aclanthology.org/W17-4413. [77] M. Xia, T. Gao, Z. Zeng, and D. Chen. Sheared llama: Accelerating language model pre-training via structured pruning. In Proceedings of ICLR, 2024. URL https://openreview.net/ pdf?id=6s77hjBNfS. [78] M. Xia, S. Malladi, S. Gururangan, S. Arora, and D. Chen. LESS: Selecting influential data for targeted instruction tuning. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=PG5fV50maR. [79] S. M. Xie, S. Santurkar, T. Ma, and P. Liang. Data selection for language models via importance resampling. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Proceedings of NeurIPS, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/6b9aa8f418bde2840d5f4ab7a02f663b-Abstract-Conference.html. [80] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu, P. S. Liang, Q. V. Le, T. Ma, and A. W. Yu. DoReMi: Optimizing data mixtures speeds up language model pretraining. In Proceedings of NeurIPS, 2024. URL https://openreview.net/forum?id=lXuByUeHhd. 15 [81] Z. Yu, S. Das, and C. Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. arXiv preprint arXiv:2406.06046, 2024. URL https://arxiv.org/ abs/2406.06046. [82] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of ACL, 2019. URL https://aclanthology.org/ P19-1472/. [83] D. Zhang, T. Zhang, Y. Lu, Z. Zhu, once: Accelerating adversarial of NeurIPS, 2019. 812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html. You only propagate In Proceedings URL https://proceedings.neurips.cc/paper/2019/hash/ training via maximal principle. and B. Dong. [84] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu, et al. Lima: Less is more for alignment. In Proceedings of NeurIPS, 2024. URL https://openreview.net/ forum?id=KBMOKmX2he."
        },
        {
            "title": "A Connection Between AUC and Scaling Law Constants",
            "content": "We show that the area under the loss curve (AUC) is directly connected to the scaling law [44] constants, and minimizing AUC essentially improves the scaling properties of LMs. As suggested by existing literature [44, 40], the test losses of LMs follow power-law with respect to the training steps after warmup stage: L(t) = tc + Lirre, > T0, (10) where and are scaling law constants, Lirre is the irreducible loss related to the noise in the test set, and T0 is the end steps of the warmup stage. Lirre is invariant to optimizing pre-training data selection strategies. Therefore, we care about the reducible loss, whose constants depend on the data quality scores γ: We then consider the AUC of the reducible loss for sufficiently large : Lre(t) = L(t) Lirre = C(γ) tc(γ) , > T0. AUC(γ) = (cid:90) t=T C(γ) tc(γ) dt = C(γ) 1 c(γ) (T 1c(γ) 1c(γ) 0 ). (11) (12) For c(γ) < 1, when is sufficiently large, AUC(γ) C(γ) 1c(γ) 1c(γ). Minimizing AUC causes B(γ) to decrease and β(γ) to increase4, improving the scaling properties of LMs. For c(γ) > 1, AUC(γ) C(γ) , which also exhibit trend of decreasing C(γ) and increasing c(γ) when c(γ)1 AUC is minimized. 1 c(γ)1 0 Proof of Theorem 2. To prove Theorem 2.1, we first describe the standard discrete-time Pontryagins Maximum Principle (PMP; 63) in Optimal Control [47] for time-variant control variables: Theorem B.1 (PMP). Consider the following optimization problem in discrete dynamical system: min γt 1 (cid:88) t=0 (θt, γt) + J(θT ), (13) s.t. θt+1 = (θt, γt), γt U, where the state variable θt RN , the control variable γt RD, and : RN (cid:55) R, : RN (cid:55) RN are continuous in RN D. Let γ denote the corresponding state variable. For 0 < , there exists co-state vector λ be the solution to this problem, and θ RN such that t+1 = λH(θ θ = θH(θ λ γ = arg min γt , λ , λ H(θ t+1, γ t+1, γ , λ 0 = θ0, = J(θT ), ), θ ), λ t+1, γt), γt U, where : RN RN RD (cid:55) is the Hamiltonian function defined by H(θ, λ, γ) = (θ, γ) + λf (θ, γ). (14) (15) (16) (17) Proof of Theorem B.1 can be found in various textbooks in Optimal Control [47, 23]. In our context, we interpret θt as the model parameters, J() as the downstream loss function, as the D-dimensional simplex as defined in Section 2.1 and as the GD operation where the data weights γt changes with respect to the training steps t: θt+1 = θt η (cid:88) n=1 γn,tl(xn, θt) = θt ηL(θt, γt). (18) 4f (c) = 1 1c 1c is increasing with respect to when 1 < ln , which is easily satisfied for sufficiently large . 17 In this way, the Hamilton function is H(θ, λ, γ) = (θ, γ) + λ [θ ηL(θ, γ)] . (19) The key challenge to prove Theorem 2.1 is that the control variables are constrained to be invariant of the training step in data selection, as discussed in Section 2.1 and 2.2, and introducing more constraint usually makes an optimization problem more complex. Formally, the requirement of invariant data weights can be expressed by 1 equations: Therefore, the optimization of data selection, as in Eq. (3), is equivalent to the following problem: γ1 = γ0, γ2 = γ0, , γT 1 = γ0. (20) min γt (cid:88) t=1 J(θt), s.t. θt+1 = θt ηL(θt, γt), γt U, γ0 = γ1 = = γT 1. (21) We adopt the method of Lagrange multipliers to solve Eq. (21) by considering the following optimization problem: min γt 1 (cid:88) t=0 J(θt) + 1 (cid:88) (cid:88) t=1 n=1 µn,t(γn,t γn,0) + J(θT ), (22) s.t. θt+1 = θt ηL(θt, γt), γt U, where (µn,t)1nD,0tT 1 are Lagrange multipliers. Note that we split J(θT ) out and add J(θ0) to the sum of J(θt), which does not affect the minimization. In this way, when (θt, γt) takes the following form: (θt, γt) = J(θ0) J(θt) + 1 (cid:88) (cid:88) t=1 n=1 µn,tγn,0, if = 0 (cid:88) n=1 µn,tγn,t, if 1 1 , (23) we can apply Theorem B.1 to Eq. (21). By substituting Eq. (19), Eq. (20), and Eq. (23) into Eq. (14) and Eq. (15), we have: θ = θ = λ λ ηL(θ t+1 + J(θ which prove Eq. (4) and Eq. (5) in Theorem 2.1 when we set γ Eq. (23) into Eq. (16), we have , γ 0 ), θ ) η2L(θ 0 = θ0, , γ 0 )λ t+1, λ = J(θT ), 0 = γ. By substituting Eq. (19) and (24) (25) γ = arg max γ0 arg max γt (cid:88) n=1 (cid:88) n=1 (cid:34) λ γn,0 l(xn, θ 0) + (cid:35) µn,t , if = 0 1 (cid:88) t=1 (cid:104) γn,t λ t+1 l(xn, θ ) µn,t (cid:105) , if 1 (26) Considering the time-invariant constraint in Eq. (20), we set γ 0 = γ 1 = = γ 1 = γ and get γ = arg max γ γ = arg max γ (cid:88) n=1 (cid:88) n=1 (cid:34) γn ηλ l(xn, θ 0) + (cid:35) µn,t , if = 1 (cid:88) t=1 (cid:104) γn ηλ t+ l(xn, θ ) µn,t (cid:105) , if 1 1 , (27) 18 which forms complete equation system containing equations and unknown variables (T 1 number of µt = (cid:2)µ1,t, µ2,t, , µD,t (cid:3) plus one γ), which has the solution: µn,t = ηλ t+ l(xn, θ ) η"
        },
        {
            "title": "Sn\nT",
            "content": "γ = arg max γ (cid:88) n=1 γnη"
        },
        {
            "title": "Sn\nT",
            "content": ", , 1 D, 0 1, (28) (29) where Sn = (cid:80)T 1 is proved by combining Eq. (24), Eq. (25), and Eq. (29). l(xn, θ t=0 λ t+1 ). Note that Eq. (29) is equivalent to Eq. (6). So far, Theorem 2."
        },
        {
            "title": "C Derivation for Adam",
            "content": "We provide our formulation and derivation for Adam [45] in this section. For 0 1, the parameter update rules of Adam is given by mt+1 = β1mt + (1 β1)gt, vt+1 = β2vt + (1 β2)g2 , (cid:99)mt+1 = mt+1/(1 βt+1 ), (cid:98)vt+1 = vt+1/(1 βt+1 ), θt+1 = θt η(cid:99)mt+1/((cid:112) 1 2 (cid:98)vt+1 + ϵ), (30) where β1, β2, ϵ, η are hyper-parameters and gt = L(θt, γt, D). We set m0 = 0 and v0 = 0. To formulate the LM training with Adam as an Optimal Control [47] problem, we treat the vector Θt = [θt, mt, vt] R3N as the state variable. Let denote the state-transition function from Θt to Θt+1: and thus represents the following relations: θt+1 = (Θt, γ), θt+1 =θt η 1 βt+1 (cid:113) β1mt + (1 β1)gt (β2vt + (1 β2)g2 )/(1 βt+1 2 mt+1 = vt+1 = , β1mt + (1 β1)gt 1 βt+1 β2vt + (1 β2)g2 1 βt+1 1 . , ) + ϵ (31) (32) (33) (34) Similar to GD, we can still define the Hamiltonian function of the problem and obtain the following theorem with Pontryagins Maximum Principle (PMP; 63): Theorem C.1 (PMP Data Selection for Adam). Let γ solve the problem in Eq. (3), and Θ the state variable corresponding to γ. Then, there exists co-state vector Λ denote R3N such that Θ t+1 = ΛH(Θ = ΘH(Θ Λ t+1, γ), Θ t+1, γ), Λ 0 = [θ0, 0, 0] , = [J(θT ), 0, 0] , γ = arg min γ H(Θ , Λ t+1, γ), γ U, , Λ , Λ 1 (cid:88) t=0 where : R3N R3N RD (cid:55) is the Hamiltonian function defined by H(Θ, λ, γ) = J(θ) + ΛF (Θ, γ). (35) (36) (37) (38) Similar to the derivation for GD, Eq. (35) controls the state transition, equivalent to Eq. (31). To simplify the further derivation of Eq. (36) and Eq. (37), we assume vt+1 0, which is reasonable gt 19 Algorithm 2 PMP Solver for Adam Input: LM learning rate η. Outer loop learning rate α. Outer loop epochs To Training data D. Downstream loss function J(). Training steps . Function Proj[] that projects point in RD to the D-simplex. Output: Data quality score γ γ = (cid:2)γ1, γ2, , γD repeat To times (cid:3) (cid:104) 1 , 1 , , 1 for = 0, 1, , 1 do Θt+1 ΛH(Θt, Λt+1, γ) end for ΛT = [J(θT ), 0, 0] for = 1, 2, , 1 do Λt ΘH(Θt, Λt+1, γ) end for for = 1, 2, , do γn γn + αγn H(Θt, Λt+1, γ) end for γ Proj [γ] end and return γ (cid:105) ; Θ0 (cid:104) (cid:105) θ(k) 0 , 0, 0 Outer loop Forward inner loop Eq. (35), expanded to Eq. (32-33) Reverse inner loop Eq. (36), expanded to Eq. (39-42) Eq. (37), expanded to Eq. (43) because vt+1 is an exponential moving average of g2 than 1 in practice. Therefore, we have vt+1 θt 0, vt+1 γt and the weight 1 β2 is usually much smaller 0 and thus Eq. (36) can be written to Λ = (cid:104) Λ(1) , Λ(2) , Λ(3) (cid:105) , Λ(1) J(θ ) + Λ(1) t+1 (1 β1)η 1 βt+1 1 2(θ , γ) (cid:112) Λ(1) t+1 (cid:98)vt+1 + ϵ + Λ(2) = (1 β1) 1 βt+ 1 ηβ1 1 βt+1 1 Λ(3) = ηβ2 1 βt+ 2 (cid:112) 2(θ , γ)Λ(2) t+1, (cid:112) + Λ(1) t+1 (cid:98)vt+1 + ϵ (cid:99)mt+1Λ(1) (cid:16)(cid:112) t+1 (cid:98)vt+1 (cid:98)vt+1 + ϵ β1 1 βt+1 Λ(2) t+1, (cid:17)2 + β2 1 βt+1 2 Λ(3) t+1. (39) (40) (41) (42) To achieve the minimum in Eq. (37), we need to compute the gradient of with respect to γt: γnH(Θ , Λ t+1, γ) = η(1 β1) 1 βt+1 1 Λ(1) t+1 l(xn, θt) (cid:98)vt+1 + ϵ (cid:112) + 1 β1 1 βt+1 1 Λ(2) t+1 l(xn, θt) (43) In this way, we can use Algorithm 2 to solve for the data quality scores on the proxy dataset Dprx, just like Algorithm 1 in Section 2.3.1. Then, we train data scorer with the solved data weights γ as in Section 2.3.2 and conduct data selection based on the scores inferred by the data scorer on as in Section 2.3.3. In pilot experiments, we find that computing data quality scores based on Adam does not show substantial improvement over that based on GD. Given that Algorithm 2 requires twice as much computation and 3 times as much GPU memory as Algorithm 1, we adopt PMP condition for data selection based on GD (Theorem 2.1) to build PDS in our main paper. 20 Algorithm 1 as Proximal Gradient Decent We provide another view of Algorithm 1 as using Proximal Gradient Decent method [6]. Specifically, we can optimize Eq. (3) with the following rules: A(γ) = (cid:88) t=1 J(θt), γ Proj [γ αγA] , (44) (45) where A(γ) denotes the cost function in Eq. (3), α is the learning rate and Proj[] projects point in RD to the D-simplex. γA = is the gradient of A(γ) with respect to the data weights γ. Now, we compute each element of γA with the chain rule: , , γD (cid:104) γ1 , dγ2 (cid:105) γn = = = (cid:88) t=1 (cid:88) t=1 (cid:88) t=1 J(θt) γn J(θt) θt γn J(θt) (cid:88) t=1 θt θt θt γn = η (cid:88) t=1 J(θt) (cid:88) t=1 θt θt l(xn, θt1) (cid:88) t1 (cid:88) = η t=0 (cid:34) (cid:88) t= 1 (cid:88) t=0 = η t=t+1 J(θt) θt θt+1 l(xn, θt) J(θt) θt θt+1 (cid:35) l(xn, θt), where θt θt+1 satisfies θt θt = θt+1 θt θt θt+1 = (cid:2)I η2L(θt, γt)(cid:3) θt θt+1 , according to Eq. (2). In the following, we show that: λt+1 = (cid:88) t=t+1 θt θt+1 J(θt), (46) (47) (48) where λt+1 is the co-state vector in Algorithm 1. Let λ λ t+1 = (cid:80)T = J(θT ) and the following difference equation for λ t: t=t+1 λ = (cid:88) t=t θt θt J(θt) =J(θt) + =J(θt) + (cid:88) t=t+1 (cid:88) θt θt J(θt) (cid:2)I η2L(θt, γ)(cid:3) θt θt+1 =J(θt) + λ t=t+1 t+1 η2L(θt, γ)λ t+1. 21 θt θt+ J(θt), we then have (49) J(θt)"
        },
        {
            "title": "Model Size",
            "content": "160M 470M 1B 1.7B dmodel 768 1,024 1,536 2,048 dFFN nlayers nhead dhead 3,072 4,096 6,144 8,192 12 24 24 24 12 16 16 16 64 64 96 128 learning rate 6 104 3 104 2.5 104 2 10 Table 6: Model configurations and corresponding learning rates. Given that λ at = , we have λ t+1 satisfies the same difference equation as λt+1 in Algorithm 1 and has the same value t+1 = λt+1. Therefore, the gradient in Eq. (46) can be written as γn = ηλ t+1l(xn, θt). (50) Combining Eq. (45) and Eq. (50), we recover the update rules of γn,t in Algorithm 1 by setting α = α/η."
        },
        {
            "title": "E Implementation Details",
            "content": "E.1 Solving Data Quality Scores Algorithm 1 needs to compute the Hessian matrix, as in Eq. (5), and per-instance vector-gradient product, as in Eq. (6). We adopt the Jacobian-Vector-Product5 in PyTorch [62] to efficiently implement these operations. There is still large room for this implementation, such as adopting other deep learning frameworks [19, 11] or using lower-complexity algorithms [75]. Algorithm 1 requires storing all the LM checkpoints θt from = 0 to = 1 in the forward inner loop for computing λt in the reverse inner loop. To reduce the single-device GPU memory use, we implement an activation partition algorithm inspired by ZeRO-2 [64], where θt in one inner-loop pass are stored in different GPU devices. We also adopt strategy inspired by activation checkpointing [14]. E.2 Training Data Scorer We fine-tune the Fairseq-Dense-125M model [5] on the solved data weights γ to obtain the data scorer. as in Eq. (8), we adopt linear transformation on the mean pooling of the instances representations along the sequence length. The size of the hidden state is 768. We optimize Eq. (8) with the AdamW [54] optimizer for 5 epochs, using learning rate 1 104 and batch size 512. We split 10% samples from Dprx for validation and select the checkpoint achieving the highest Spearman correlation score on the validation set to infer data quality scores in D. E.3 Pre-Training We pre-train all our models for about 50B tokens with batch size of 512 and max input length of 1,024. We use the AdamW [54] optimizer and cosine learning rate scheduler, which warmups the learning rates for 2K steps and then decays it to 10% of the maximal value. We follow [12] to set the maximal learning rates as listed in Table 6, together with the model configurations. E.4 Evaluation We evaluate our trained models on MMLU [39] and the evaluation sets used in OLMo [31], including Hellaswag (HS; 82), Winograde (Wino.; 46), LAMBADA (LAMB; 61), OpenbookQA (OBQA; 57), ARC-easy/challenge (ARC-e/c; 17), PIQA [9], SciQ [76], and BoolQ [16]. We adopt the LMevaluation-harness library [27]6 to conduct the zero-shot evaluation, which formulates the tasks as the multiple-choice problems and chooses the answer by comparing the answer-length-normed loss across candidates (acc_norm). We also compute the test loss of our trained models on the DCLM 5https://pytorch.org/docs/stable/func.api.html 6https://github.com/t1101675/lm-harness/ 22 corpus [48], 10K subset uniformly sampled from the high-quality pre-training corpus in Li et al. [48]. E.5 Baselines Reducible Holdout Loss Selection (RHO-Loss; 58, 51). RHO-Loss selects data with high reducible holdout loss defined as follows: RHO-Lossn,t = l(xn, θt) l(xn, θdown), where θdown is the model parameters trained on the downstream tasks, which is LIMA [84] in our experiments. We first split 10% of LIMA for validation and choose θdown with the lowest validation loss. Then, we train θt while selecting xn with top α 100% RHO-Lossn,t to compute gradients. We tried α {0.1, 0.3, 0.5, 0.7} and find that α = 0.5 performs the best. (51) Data Selection via Importance Resampling (DSIR; 79). DSIR selects pre-training data based on the n-gram feature overlap between the instances in the downstream dataset (LIMA [84] in our experiments) and the large-scale corpus. Pre-training instances whose features have high probabilities under the feature distribution of the downstream dataset will obtain higher sampling weights. We use the official implementation of DSIR7. Influence Score (IF-Score 43, 32). We adopt the influence function [43, 32] to measure the quality of using the downstream loss J(θ) as the target: IF-Score(x) = l(x, θT ) (cid:2)2L(θT )(cid:3) (52) J(θT ), (cid:80) where θT is the LM parameters trained for steps and L(θT ) = 1 xD l(x, θT ). We adopt linear-time iterative algorithm to compute the inverse-Hessian-vector-product [2]. To reduce the computational cost, we adopt similar efficient implementation as PDS by computing the IF-Scores on small proxy data based on small proxy LM and then transferring these scores to large pre-training corpus with fine-tuned data scorer. We select examples with the top 40% scores inferred by the data scorer on the large pre-training corpus. E.6 Simulated Setting for Experiments in Table 6 To exactly run Algorithm 1 with feasible computational overhead, we adopt 12M LM from the Mistral [42] family, with small vocabulary. We uniformly sample 4,096 instances as D, with max sequence length of 256, and construct 16K vocabulary from and LIMA. We run the inner loops for 5K steps and the outer loop for 5 epochs to get the PDS (exact) line in Figure 6. For PDS (Efficient), we adopt an 8.7M model as the proxy LM and set = 5, prx = 100. We run the inner loop using SGD with batch size of 128. The outer loop epoch number is set to 1."
        },
        {
            "title": "F Complexity Analysis",
            "content": "Following Hoffmann et al. [40], for an LM with parameters to be trained on tokens, we assume the computational FLOPs of forward and backward pass are 2N and 4N D, respectively. We compute the FLOPs and asymptotic complexities of different stages in PDS as follows: Solving Data Quality Scores: According to Section 2.3.1, we first pre-train proxy LM on which consumes 6N prxD FLOPs. Then, we perform Algorithm 1 times on Dprx based on the proxy LM. The forward inner loop in Algorithm 1 consumes 6N prxDprx FLOPs. The reverse inner loop can be treated as the backward propagation of the forward inner loop as discussed in Appendix D, which consumes 2 6N prxDprx FLOPs. The update of γ results in one forward and backward pass of the proxy LM on Dprx, which consumes 6N prxDprx FLOPs. In summary, the asymptotic complexity of solving data quality scores is O(N prxD + 4M prxDprx). Data Scorer: The data scorer with score is trained on Dprx and used to infer data quality scores on D. Therefore, the computation overhead is 6N scoreDprx +2N scoreD and the asymptotic complexity is O(3N scoreDprx + scoreD). 7https://github.com/p-lambda/dsir 23 (a) Model Size = 160M (b) Model Size = 470M (c) Mode Size = 1B Figure 8: Scaling curves of average accuracy on the OLMo [31] evaluation datasets with respect to computation for 160M, 470M, and 1B sizes. α β Conventional PDS 8.09 102 6.21 103 7.50 105 1.76 105 0.397 0. 0.651 0.585 2.829 2.829 Table 7: Scaling law constants by fitting the test losses on the DCLM corpus. Data Selection: Selecting pre-training corpus requires iterating over D, whose asymptotic complexity is O(D). This process can be done on CPUs and does not require GPU FLOPs. Pre-Training: Pre-training an LM with parameters requires 6N FLOPs, whose asymptotic complexity is O(N D)."
        },
        {
            "title": "G More Results",
            "content": "G.1 Scaling Curves of Computation for Other Model Sizes We plot scaling curves of computation for 160M, 470M, and 1B models in Figure 8. PDS-selected data accelerates the model learning across model sizes. G.2 Test Loss Extrapolation with the Scaling Law We extrapolate the test losses on the DCLM corpus [48] of the conventionally trained and PDS-trained LMs with the Scaling Law [40, 44]. Following Hoffmann et al. [40], we consider the scaling law with the following form: L(N, D) = + α + Dβ , (53) where is the model parameters, is the trained tokens, and A, B, E, α, β are constants. We obtain these constants by minimizing the Huber loss [41]: min a,b,e,α,β (cid:88) (Ni,Di,Li) Huberδ(LSE(a α log Ni, β log Di, e) log Li), (54) where LSE() is the log-sum-exp operation. The loss is summed over all (Ni, Di, Li), which is obtained by the test losses of 160M, 470M, 1B, and 1.7B LM during training from 0B to 50B tokens. We record the losses every 2.5B tokens, resulting in total 4 50B/2.5B = 80 tuples like (Ni, Di, Li). After solving a, b, and from Eq. (54) , we have = exp(a), = exp(b), and = exp(e). Hoffmann et al. [40] optimizes Eq. (54) using the LBFGS algorithm [52]. However, we find this algorithm sensitive to the initialization of the parameters to be optimized. Therefore, we apply two-stage optimization. Specifically, we first fit the following data scaling curves for =160M, 470M, 1B, and 1.7B with non-linear least squares from scipy.optimize.curve_fit8, which is 8https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit. html 24 Figure 9: Effect of the data selection ratio α. We report the average accuracy on the OLMo evaluation datasets for α [0.3, 0.4, 0.5, 0.6, 1.0]. Figure 10: Effect of the strength τ in GumbleTop-K. We report the average accuracy on the OLMo evaluation datasets for τ [0.0, 0.1, 0.3, 0.5]. much more robust to the initialization: L(D) = E(N ) + B0(N ) Dβ0(N ) , (55) where E(N ), B0(N ) and β0(N ) are the fitted parameters. Then, we fit the following model size scaling curve: = E0 + A0 α0 . (56) We use the constants from Eq. (56) and the average constants from Eq. (55) to compute the initialization for the LBFGS algorithm: a0 = log A0, B0(160M) + B0(470M) + B0(1B) + B0(1.7B) 4 , b0 = log α0 = α0, β0 = β0(160M) + β0(470M) + β0(1B) + β0(1.7B) 4 , e0 = log E0, (57) where a0, b0, α0, β0, e0 are the parameter initialization for the LFBGS algorithm to optimize Eq. (54). We set δ = 1 103 and learning rate to 0.05 when running LFBGS and obtain the constants in Table 7. We use these constants and Eq. (53) to compute the predicted loss in Table 3. In Section 3.3 (Data-Constrained Setting), to compute the expected token demand of conventional pre-training to match the performance of PDS (r = 0.25, 4 Eps.), we solve for using the constants in Table 3 and use 4 as the token demand, indicating the LM can be trained for 4 epochs as suggested by Muennighoff et al. [59]. G.3 Ablation Studies Data Selection Ratio. In Figure 9, we investigate how the data selection ratio affects the performance of PDS when the original training corpus is sufficiently large (in Section 3.3, we explore the data selection ratio in the data-constrained setting.). lower data selection ratio results in better final model performance. However, to ensure that the selected data contains enough training tokens, larger original corpus is needed for lower α. Therefore, we keep α = 0.4 in our main experiments to balance effectiveness and data demand. Gumbel Noise in Data Selection In Figure 10, we explore the effect of the strength τ in GumbleTop-K used for data selection in Eq. (9). We can see that τ = 0.1 achieves the best performance, verifying the benefits of increasing the diversity of the pre-training corpus. Too large τ value makes PDS degenerate to random selection, causing the performance to decrease. Acc. J(θ) Conventional Choice of J(θ). The desired data plays an important role in determining the quality of the selected data. We test different downstream datasets to compute J(θ) in PDS and report the model performance in Table 8. The comparison between the results of using LAMBADA and LIMA shows the importance of the downstream data diversity. Instances in LAMBADA mostly come from stories, while LIMA is composed of instruction-response pairs that cover various tasks, which yields better overall performance. When comparing LIMA, OpenWebText [29], and CC, we conclude that data quality is another major concern. Although OpenWebText has been shown to have better scaling factors [8] and used as the target set [12], replacing it with higher quality LIMA further improves performance. Compared to diversity and quality, large sizes of downstream datasets seem less important, because LIMA performs the best with the least instance number. Table 8: Effect of using different downstream datasets to compute J(θ). We report average accuracy on the OLMo evaluation datasets. LAMB. CC OWT 43.7 43.0 44.1 LIMA PDS 43.2 45.0 -"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "The CoAI Group, Tsinghua University"
    ]
}