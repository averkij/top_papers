{
    "paper_title": "Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression",
    "authors": [
        "Zheng Chen",
        "Mingde Zhou",
        "Jinpei Guo",
        "Jiale Yuan",
        "Yifei Ji",
        "Yulun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20$\\times$. Code is released at: https://github.com/zhengchen1999/SODEC."
        },
        {
            "title": "Start",
            "content": "Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression Zheng Chen1*, Mingde Zhou1*, Jinpei Guo1,2, Jiale Yuan1, Yifei Ji1, Yulun Zhang1 1Shanghai Jiao Tong University, 2Carnegie Mellon University 5 2 0 2 7 ] . [ 1 9 7 9 4 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, novel single-step diffusion image compression model. We argue that in image compression, sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20. Code is released at: https://github.com/zhengchen1999/SODEC. Introduction The rising cost of data storage and transmission underscores the importance of image compression. Traditional codecs such as JPEG2000 (Taubman, Marcellin, and Rabbani 2002) and VVC (Bross et al. 2021) perform reliably at medium to high bitrates. However, when the bitrate drops to low levels (e.g., <0.1 bpp), they tend to produce block artifacts, blurring, and structural distortions. Achieving balance between distortion and perceptual quality under low bitrate constraints remains challenging problem. In recent years, learning-based image compression models built upon variational autoencoders (VAEs) (Kingma, Welling et al. 2019) have surpassed traditional methods in the rate-distortion trade-off (Balle et al. 2018; Cheng et al. 2020; Van Den Oord, Vinyals et al. 2017). Benefiting from advances in probabilistic modeling, such as hyperpriors (Balle et al. 2018; Minnen, Balle, and Toderici 2018), these approaches typically excel in distortion-oriented metrics like PSNR and MS-SSIM. Moreover, to better align *These authors contributed equally. Corresponding author: Yulun Zhang, yulun100@gmail.com Figure 1: LPIPS-bitrate-latency comparison on DIV2K-Val. Decoding time is measured on 512512 images using one A6000 GPU. Our method achieves the best perceptual quality (i.e., LPIPS). Meanwhile, compared to the multi-step diffusion-based method DiffEIC (Li et al. 2024), our method offers 38 speedup in decoding time. with human perception, subsequent works further incorporate perception-oriented objectives, leading to more comprehensive rate-distortion-perception framework (Blau and Michaeli 2019; Mentzer et al. 2020; Muckley et al. 2023; Agustsson et al. 2023; He et al. 2022b). These methods achieve more realistic reconstruction by employing distortion and perceptual losses to enhance realism. However, VAE-based methods struggle to reconstruct details when operating at extremely low bitrates, resulting in poor perceptual quality. In other words, while the reconstructions may appear technically correct, they often lack realism. In contrast, diffusion models (Ho, Jain, and Abbeel 2020) have recently demonstrated remarkable performance in the rate-perception trade-off, due to their powerful generative priors. Specifically, in diffusion-based methods, the encoder produces compact latent representation, while decoding is reformulated as multi-step conditional denoising process (Theis et al. 2022; Lei et al. 2023b). Guided by conditional signals derived from the bitstream, the diffusion model iteratively refines noisy latent (Yang and Mandt 2023; Vonderfecht and Liu 2025; Careil et al. 2024; Ghouse et al. 2023; Relic et al. 2024). Thus, diffusion-based models can synthesize highly realistic textures and details, even under extreme compression. Moreover, some approaches integrate global (e.g., text prompts) or local (e.g., quantized features) guidance to constrain the generative process (Pan, Zhou, and Tian 2022; Careil et al. 2023; Li et al. 2024). These conditions guide the model to generate more plausible details, yielding more realistic reconstructions. However, such models face two critical challenges: (1) High latency. The multi-step denoising process incurs substantial decoding latency and computational cost. This limits their applicability in real-time or resource-constrained scenarios. (2) Low fidelity. The generative nature of diffusion models makes them heavily reliant on pre-trained priors rather than the input itself. This leads to reconstructions that deviate from the original content, compromising fidelity. To address these challenges, we propose SODEC (steering one-step diffusion model with fidelity-rich decoder), novel image compression model designed for low-bitrate scenarios. Our SODEC is designed around efficient decoding and high-fidelity guidance. (1) Single-step decoding. To mitigate the high latency of multi-step diffusion, we replace the iterative denoising process with single-step process. Benefits to the informative latent representations produced by the pre-trained VAE-based compression model, singlestep decoding is sufficient to realize high-quality reconstruction. (2) Fidelity guidance module. To compensate for the potential fidelity loss, we employ pre-trained VAE-based compression model to produce high-fidelity preliminary reconstruction. This reconstruction serves as explicit visual guidance to the diffusion model, encouraging outputs faithful to the source image. (3) Rate annealing training strategy. To ensure effective training at extremely low bitrates, we adopt three-stage optimization. The model is first pretrained at higher bitrates to learn informative representations. Then, we gradually anneal the model to the target bitrate, selectively preserving essential information. Benefits to above designs, SODEC achieves impressive performance in terms of rate-distortion-perception tradeoff. Furthermore, due to the single-step and lightweight conditioning, our SODEC achieves excellent decoding efficiency. As shown in Fig. 1, compared to multi-step diffusion paradigms (e.g., PerCo (Careil et al. 2023), DiffEIC (Li et al. 2024)), SODEC delivers over 20 speedup. Our contributions are summarized as follows: We propose SODEC, single-step diffusion image compression model that significantly accelerates decoding while preserving high perceptual-fidelity quality. We introduce the fidelity guidance module, diffusion guidance mechanism conditioned on high-fidelity reconstruction, effectively improving content fidelity. We develop the rate annealing training strategy, threestage optimization scheme that enables the model to retain critical information at extremely low bitrates. SODEC achieves state-of-the-art performance in the rate-distortion-perception trade-off, while delivering significantly improved decoding efficiency."
        },
        {
            "title": "Related Work",
            "content": "VAE-based Compression Model Compressing images at extremely low bitrates is challenge where traditional methods like JPEG2000 (Taubman, Marcellin, and Rabbani 2002) and VVC (Bross et al. 2021) often produce severe blurring and artifacts. Recently, learned compression based on Variational Autoencoders (VAEs) (Kingma and Welling 2014) has surpassed traditional codecs in rate-distortion performance (Balle et al. 2018; Cheng et al. 2020; Wang et al. 2022; Minnen and largely due to innovaSingh 2020; He et al. 2022a), tions like the hyperprior model. This architecture is refined with sophisticated context models and quantization strategies, such as the hierarchical prior model (Minnen, Balle, and Toderici 2018) and VQ-VAE (Van Den Oord, Vinyals et al. 2017), achieving state-of-the-art performance on distortion-oriented metrics like PSNR and MS-SSIM. Subsequently, to enhance visual realism, perception-oriented models (Tschannen, Agustsson, and Lucic 2018; Blau and Michaeli 2019; Agustsson et al. 2019; Mentzer et al. 2020; Muckley et al. 2023) are introduced to optimize the ratedistortion-perception. However, these models still tend to produce artifacts and lack detail at extremely low bitrates. Diffusion-based Compression Model Diffusion models (Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2020; Rombach et al. 2022) excel at highquality image synthesis by framing generation as an efficient, latent-space noise prediction task. Recent compression works adapt these models for image compression by treating it as conditional denoising problem (Saharia et al. 2021; Xia et al. 2025; Liu et al. 2024). Typically, an encoder transforms the source image into compact latent representation that conditions the reverse diffusion process, enabling reconstructions with high perceptual quality. This paradigm is demonstrated by foundational methods like CDC (Yang and Mandt 2023), which conditions on learned latent. More sophisticated strategies, e.g., DiffC (Vonderfecht and Liu 2025), use reverse-channel coding to steer pre-trained diffusion model without fine-tuning. Moreover, diffusion-based compression models employ various guidance signals to enhance reconstruction quality. For example, some approaches compress images into purely semantic space (Lei et al. 2023a; Bachard, Bordin, and Maugey 2024; Pan, Zhou, and Tian 2022). For instance, Pan et al. (Pan, Zhou, and Tian 2022) encode an image into textual embedding that subsequently guides pre-trained text-to-image model. Other works (Careil et al. 2023; Guo et al. 2025; Li et al. 2024) utilize more sophisticated conditioning. For example, PerCo applies both pre-extracted text prompts for global context and quantized visual features for local details. In contrast, DiffEIC derives its guidance internally, extracting global context vector from the hyperprior and injecting it into the diffusion process. However, these methods share two primary limitations: (1) the substantial latency from their multi-step diffusion process, and (2) the tendency to sacrifice fidelity for perceptual realism due to the diffusion prior. Figure 2: Overview of SODEC. (a) VAE compression module: pre-trained VAE-based compression model is used to generate the informative latent representation. (b) One-step diffusion model: The latent is mapped to the diffusion space via the transformation module, followed by single-step denoising to produce the reconstructed output. (c) Fidelity guidance module (FGM): high-fidelity preliminary reconstruction is generated using the VAE-based compression model. Then, the pre-trained ViT is used to extract visual features as the guidance for the diffusion model. Methodology In this section, we provide an overview of our proposed model, SODEC, as illustrated in Fig. 2. The section begins with the VAE Compression Module. Subsequently, we elaborate on the core component of SODEC: the one-step diffusion model and the fidelity guidance module. Finally, we detail our rate annealing training strategy. SODEC Overview The overview of SODEC is illustrated in Fig. 2. The framework begins with VAE Compression Module that downsamples raw image R(HW 3) for 16 times to compact latent representation R(H/16W/16C), where is the latent channels (usually 220). After the entropy coding, restored ˆy and ˆz are passed into transformation module Ts and converted into content variable ˆyt R(64644) suitable for diffusion process. Then, we apply the one-step diffusion model to speed up the denoising time compared to the previous multi-step diffusion model (Careil et al. 2023; Li et al. 2024). The one-step diffusion model will then be used to generate the denoised content variable ˆy0. Simultaneously, we utilize pre-trained fidelity-rich decoder Da and further fine-tune it for high fidelity. To achieve this goal, we introduce an alignment loss Lalign that consists of pixel-wise loss that constrains Da to consistently produce high-fidelity images. After Da decodes the latent representation ˆy into the raw image ˆxf , we use the pre-trained ViT model (Liu et al. 2021) to capture the high-fidelity feature information. Then we linearly project it into the embedding space, getting the condition guidance cg. To achieve the best performance, we also introduce the rate annealing training strategy. This strategy first pretrains complete VAE model with higher bitrate than our final aim. This VAE model comprises rich representation in the latent space. Then, we lift the rate penalty by applying larger trade-off parameter λ in the loss function. Thus, the model can distill from the rich representation and selectively discard non-essential information. This strategy is proven to achieve better performance than directly training."
        },
        {
            "title": "VAE Compression Module",
            "content": "The proposed SODEC employs VAE-based compression backbone to efficiently encode the input image into bitstream. This module is comprised of the encoder E, hyperencoder Ha, and probability model P. Given an input image x, the encoder produces compact latent representation y=E(x). Hyperencoder Ha then extracts the hyper-latent z=Ha(y). Next, both of these representations and are quantized into ˆy=Q(y), ˆz=Q(z), where Q() represents the quantization operation. Finally, the learned probability model conditions on the quantized hyperprior ˆz to predict the parameters (µ, σ) of Gaussian distribution, which models the probability of latent representation ˆy for efficient entropy coding. For the compression model, we pre-train HiFiC (Mentzer et al. 2020) and use its learned weights to initialize our compression backbone E, Ha, and P. In addition, we use the pre-trained VAE decoder to initialize the decoder Da in the fidelity guidance module. We apply Da to generate the highfidelity preliminary reconstruction ˆxf : ˆxf = Da(Q(E(x))). (1) This model is optimized using the rate-distortion function: LEG = Expx [λ r(ˆy, ˆz) + d(x, ˆxf )] , (2) where r() denotes the rate and λ is the hyperparameter to control rate penalty, and d(x, ˆxf ) represents the distortion: d(x, ˆxf ) = kM MSE(x, ˆxf ) + kP dP (x, ˆxf ), (3) where kM and kP are hyperparameters. We choose LPIPS for the perception distortion dp (in all the subsequent training, we also choose LPIPS by default). It is worth noting that, in this pre-training stage, we adopt smaller λ (i.e.smaller rate penalty) to train stronger VAE encoder-decoder pair with higher bitrates. This is beneficial for our subsequent training. More details will be shown in the Rate Annealing Training Strategy section. As shown in Fig. 3, the preliminary reconstruction ˆxf is highly faithful to the original image, although it may lack perceptual richness. Conversely, while the diffusion model excels at synthesizing realistic textures, it lacks explicit knowledge of the source image. Therefore, we can apply the high-fidelity reconstruction ˆxf as strong conditional guide, to steer the diffusion generator to reconstruct details that are plausible and consistent with the original content. Thus, we can achieve both good fidelity and perception results. Specifically, the module first utilizes pre-trained fidelityrich decoder, Da, to generate the high-fidelity preliminary reconstruction ˆxf from the compressed latent ˆy: ˆxf = Da(ˆy), (6) where Da comes from the pre-trained HiFiC encoderdecoder pair, as shown in Eq. (2). Subsequently, pre-trained ViT Transformer (Dosovitskiy et al. 2021), denoted as the feature extractor F, is employed to capture deep visual features from this intermediate image. These features are then mapped into the conditioning space of the diffusion model by projection network Fp to produce the final guidance condition cg: cg = Fp(F(ˆxf )), (7) where the resulting condition cgRLD consists of sequence of embedding vectors of dimension D. In our model, and are chosen as 77 and 1024. This high-fidelity guidance cg, which encapsulates rich high-fidelity structural information from the source, is then injected into the diffusion denoising model ϵθ through crossattention to steer the diffusion process. Table 2 in the ablation study demonstrates that this guidance mechanism can effectively steer the generative process, ensuring the final output is both perceptually realistic and highly faithful to the original content."
        },
        {
            "title": "Rate Annealing Training Strategy",
            "content": "We propose three-stage training strategy for our SODEC, illustrated in Fig. 2. This idea is based on the motivation that selecting from rich representation and discarding nonessential information is easier than recreating detailed information. Thus, we decide to first train VAE model with higher bitrates and then increase the rate penalty to force the model to discard and choose the most useful information. Stage 1: High-Bitrate VAE Pre-training. Our strategy begins by pre-training HiFiC (Mentzer et al. 2020) model, which serves as the core compression component. In this stage, the model is trained end-to-end on the ratei.e., LEG = distortion function as shown in Eq. (2), Expx [λ r(y) + d(x, ˆxf )]. We intentionally use small value for the Lagrange multiplier λ to place lower penalty on the bitrate. This encourages the model to learn rich and comprehensive latent representation by prioritizing highfidelity reconstructions. This pre-training phase is conducted on the HiFiC. After this stage, we obtain the high-bitrate version of networks E, Ha, P, and Da. Figure 3: Fidelity comparison (i.e., MS-SSIM) on DIV2KVal. We compare MS-SSIM (with GT) under different bitrates for the fidelity reconstruction and the diffusion outputs with (w/) and without (w/o) the fidelity guidance module (FGM). The use of FGM improves reconstruction fidelity. One-Step Diffusion Model Given ˆy and ˆz from the bitstream, we propose transformation module to convert them into content variable ˆyt, which is suitable for diffusion denoising. First, we use hyper synthesis network Hs to extract global information from the hyperprior ˆz, where w=Hs(ˆz). Then, we then merge and ˆy and convert them into content variables ˆyt=Ts(ˆy, w), where Ts denotes the transformation module. Here, ˆyt is conceptually analogous to noisy latent at the timestep from the forward process: ˆyt = αt ˆy0 + 1 αtϵ. (4) For standard diffusion models, they perform multi-step diffusion process to predict clear version of noisy latent. However, these processes are extremely slow and are the most time-consuming steps during the image reconstruction process. Thus, to speed up the diffusion process, we introduce the one-step diffusion model, based on Stable Diffusion 2.1 (Rombach et al. 2022). In this diffusion model, noise estimator with UNet architecture ϵθ is used to predict the clear, denoised version of the content variable ˆy0: ˆyt 1 αtϵθ(ˆyt, t, cg) ˆy0 = , (5) αt where cg is the condition guidance. We describe the details of cg in the next section. Finally, pre-trained diffusion decoder Dm reconstructs the output image ˆx from the denoised content variable ˆy0, where ˆx=Dm(ˆy0). In SODEC, we set the timestep as 999. Meanwhile, to adapt the diffusion model to image compression tasks, we adopt LoRA (Hu et al. 2022) to fine-tune the diffusion model. Fidelity Guidance Module The powerful generative prior of diffusion models enables the synthesis of high-perceptual-quality images. However, it often comes at the cost of reconstruction fidelity. To address this limitation, we propose the fidelity guidance module that injects high-fidelity information into the diffusion process. Stage 2: Diffusion Path Warm-up. In the second stage, we transfer the learned weights of the VAE components (E, Ha, P, Da) into our SODEC architecture, as shown in Fig. 2. The entire VAE encoding module (E, Ha, P) is frozen. The gradient flow is shown as follows: ˆxf = Da(sg(Q(E(x)))), = Hs(sg(ˆz)), (8) where sg denotes the stop-gradient operation, which cuts off the backpropagation of the gradient for this path. Training is focused exclusively on the diffusion-based generator and path. Specifically, we freeze the well pretrained model ViT and diffusion decoder Dm and fine-tune the UNet in diffusion using LoRA. Moreover, we train the following networks with full parameter updating: hyper synthesis network Hs, transformation module Ts, fidelity guidance decoder Da, and linear projection network Fp. The optimization objective for this stage only includes distortion loss between the output ˆx and the original image x: = Expx [d(x, ˆx)] , (9) where d() is the same as Eq. (3). Particularly, we do not apply rate penalty nor an alignment loss Lalign, because the VAE module is frozen and the latent representation ˆy is not distorted. This training stage aims to teach the onestep diffusion generator to effectively map the fixed latent representations to high-quality reconstructions. Stage 3: Joint Training with Rate Annealing. In this stage, we perform end-to-end optimization of the entire framework. The pre-trained ViT and the final VAE decoder Dm remain frozen, while all other networks are trained with full parameters, except the U-Net, which continues to be fine-tuned via LoRA. As the VAE encoder is updated, the latent representation ˆy can become distorted. To ensure that the fidelity decoder Da continues to produce high-fidelity reconstructions, we introduce an alignment loss, Lalign. From experiments, we find the MSE loss to be most effective: Lalign = (cid:2)x ˆxf 2 2 (cid:3) , where ˆxf = Da(ˆy). (10) The experimental details of Lalign are provided in the ablation. Then, the training objective becomes: Loverall = d(x, ˆx) + λ r(ˆy, ˆz) + α Lalign. (11) This objective is to fully leverage the generative power of the diffusion model under the guidance of fidelity-rich features to achieve an optimal rate-distortion-perception trade-off. Finally, the model is fine-tuned with GAN-based objective Lg to enhance the synthesis of rich details while maintaining fidelity. Therefore, the overall loss for this final finetuning stage can be written as: Lfinetune = d(x, ˆx) + λ r(ˆy, ˆz) + α Lalign + β Lg, (12) where the hyperparameter β is used to control the penalty of the GAN loss. Detailed training hyperparameter settings are provided in the implementation details of the main paper and the supplementary material."
        },
        {
            "title": "Experiments",
            "content": "Experimental Settings Datasets. Our SODEC model is trained using random 512512 patches extracted from the LSDIR dataset. To evaluate performance, we benchmark SODEC on three standard datasets: Kodak (Eastman Kodak Company 1999), DIV2K Validation dataset (denoted as DIV2K-Val), and CLIC2020 test set (Toderici et al. 2020). We center-crop all images in the validation datasets to 512512 resolution to facilitate consistent and fair comparison. Metrics. Finally, the compression rate is measured in bits per pixel (bpp). For reconstruction fidelity, we report the PSNR and the MS-SSIM (Wang, Simoncelli, and Bovik 2003). To measure perceptual similarity to the ground truth, we employ LPIPS (Zhang et al. 2018) and DISTS (Ding et al. 2020). Furthermore, to evaluate the realism of the generated images in reference-free setting, we adopt the noreference metrics NIQE (Mittal, Soundararajan, and Bovik 2012) and CLIPIQA (Wang, Chan, and Loy 2023). The compression rate is measured in bits per pixel (bpp). choose Implementation Details. We the HiFiC model (Mentzer et al. 2020) without discriminator as the VAE compression module. We utilize Stable Diffusion 2.1 (Rombach et al. 2022) and set the timestep as 999 to perform one-step diffusion. We set the batch size to 2 and use the AdamW optimizer with β1=0.9 and β2=0.999. We conduct our experiments on 2 NVIDIA RTX A6000 GPUs. More settings are provided in the supplementary material."
        },
        {
            "title": "Main Results",
            "content": "We conduct extensive experiments to validate the effectiveness of our one-step diffusion model, SODEC, in the ultralow bitrate regime. To provide comprehensive analysis, we benchmark our method against several state-of-the-art generative compression models, covering dominant VAE-based, generative tokenizer paradigms, and multi-step diffusion. Specifically, we compare against MS-ILLM (Muckley et al. 2023) and HiFiC (Mentzer et al. 2020), which are leading VAE-based methods. For multi-step diffusion approaches, we compare with CDC (Yang and Mandt 2023). We also include the current diffusion-based models: PerCo (Korber et al. 2024) and DiffEIC (Li et al. 2024). Quantitative Evaluation. As shown in the visualized results in Fig. 4, our proposed SODEC establishes new stateof-the-art across all evaluated metrics. Our model achieves superior perceptual quality, outperforming other diffusionbased compression models like PerCo (Korber et al. 2024) and DiffEIC (Li et al. 2024). Moreover, our SODEC also excels in reconstruction fidelity (e.g., MS-SSIM). Qualitative Evaluation. We present visual comparisons on three datasets in Fig. 5. SODEC achieves reconstructions closer to the original images. In contrast, existing methods often suffer from missing details or content inconsistencies under extreme compression. More visual comparisons are provided in the supplementary material. o - K 2 I 0 2 0 2 C Figure 4: Quantitative comparison with state-of-the-art methods on the Kodak, DIV2K-Val, and CLIC2020 datasets. Model Total Time (ms) Enc. Time (ms) Dec. Time (ms) bpp Guidance Strategy MS-SSIM LPIPS bpp HiFiC MS-ILLM PerCo DiffEIC SODEC 9.3 9.3 6,242.2 7,827.5 232.9 5.4 54. 1,540.0 266.4 5.0 3.9 84.4 4,702.2 7,561.1 227.9 0.0310 0. 0.0313 0.0391 0.0314 Table 1: Inference efficiency comparison on the DIV2K-Val dataset. Total, encoding, and decoding times are measured on one A6000 GPU with the 512512 image. Inference Efficiency. Moreover, we compare the inference time in Tab. 1. The runtime is tested on one A6000 CPU with the 512512 image. Our single-step diffusion model, SODEC, offers substantial advantage in latency. Compared to the multi-step diffusion-based method, PerCo (Korber et al. 2024), our SODEC is 26 faster."
        },
        {
            "title": "Ablation Study",
            "content": "We conduct our ablation study on LSDIR (train) and DIV2K-Val (test). By default, the models are trained for 50K steps in the pre-training process, and 40K steps in the SODEC end-to-end training process for fair comparison. (i) No Guidance (ii) Text Prompt Guidance (iii) Hyperprior Guidance (iv) Aux. Fidelity Guidance (ours) 0.8212 0.8185 0.8258 0.8481 0.3625 0.3631 0. 0.0424 0.0412 0.0385 0.3351 0.0368 Table 2: Ablation on the fidelity guidance module. Fidelity Guidance Module. We conduct an ablation study to validate the effectiveness of our proposed fidelity guidance module. We compare three settings: (i) no explicit guidance; (ii) text prompt guidance (used by PerCo); (iii) semantic features guidance extracted from the hyperprior (used by DiffEIC); and (iv) our fidelity guidance module. As shown in Tab. 2, the baseline model without guidance has poor performance. While using text prompts (case ii) or guidance from the hyperprior (case iii) yields some gains, its impact on reconstruction fidelity is limited. In contrast, our proposed fidelity guidance module leads to substantial improvement in reconstruction accuracy. Crucially, this significant gain in fidelity is achieved with almost no degradation in perceptual quality as measured by LPIPS. This demonstrates that our guidance mechanism achieves superior balance between realism and fidelity. Dataset Original HiFiC CDC MS-ILLM PerCo DiffEIC SODEC (ours) Kodak bpp 0.0461 0. 0.0399 0.0512 0.0402 0.0364 DIV2K-Val bpp 0.0420 0.0505 0.0432 0.0535 0.0457 0. CLIC2020 bpp 0.0400 0.0472 0.0421 0. 0.0462 0.0380 Figure 5: Qualitative comparison with state-of-the-art methods on the Kodak, DIV2K-Val, and CLIC2020 datasets. Alignment Loss Config. MS-SSIM LPIPS bpp (i) No Alignment Loss (ii) MSE + LPIPS (iii) Merged into Main Loss (iv) MSE only (ours) 0.7490 0.7481 0.7984 0.7948 0.4210 0.3961 0. 0.3827 0.0203 0.0199 0.0232 0.0227 Table 3: Ablation on the setting of alignment loss (Lalign). Training Strategy MS-SSIM LPIPS bpp (i) Frozen VAE Module (ii) Joint Training (Matched bpp) (iii) Low-to-High bpp Curriculum (iv) Rate Annealing (ours) 0.8512 0.8621 0.8643 0. 0.3761 0.3750 0.3451 0.0695 0.0678 0.0593 0.3113 0.0604 Table 4: Ablation study on different training strategies. Alignment Loss. To ensure the preliminary reconstruction ˆxf remains high-fidelity even as the latent representation ˆy gets distorted during fine-tuning, we introduce an alignment loss Lalign to constrain it. We investigate four distinct formulations for this fidelity-preservation mechanism: (i) no alignment loss, where decoder Da receives no direct gradient supervision; (ii) composite loss of perceptual (LPIPS) and distortion (MSE); (iii) no separate Lalign term (Lalign=0); and (iv) distortion-only (MSE) loss. As summarized in Tab. 3, our results validate the need for an explicit alignment loss, as its omission (case i) significantly degrades performance. While composite loss (case ii) provides no significant improvement in fidelity, merging the constraint into the main loss (case iii) enhances fidelity but at the expense of perceptual quality. In contrast, dedicated, distortion-only alignment loss (case iv) substantially boosts fidelity over the composite loss (case ii) with negligible impact on perception compared with case (ii). Rate Annealing Training Strategy. To validate the efficacy of our proposed rate annealing training strategy, we conduct comparative analysis of four distinct training schemes: (i) training with the entire VAE compression module frozen, thereby excluding it from the optimization process; (ii) apply joint training approach, but we manually tune the Lagrange multiplier λ to ensure the final bitrate is close to the original values; (iii) low-to-high bpp curriculum, where the rate penalty is progressively relaxed; and (iv) our proposed high-to-low bpp Rate Annealing strategy. The results are presented in Tab. 4. It is evident that our rate annealing training strategy significantly outperforms all other training schemes. For given reconstruction quality, our method achieves an average bitrate saving of over 30%. Conversely, at an equivalent bitrate, our proposed method provides substantially better reconstruction quality. This demonstrates the effectiveness of our approach, which allows the model to first learn rich feature representation in less constrained, high-bitrate regime before distilling it into more efficient, low-bitrate representation. Conclusion In this paper, we address the challenges of high latency and poor fidelity in existing diffusion-based compression models. We propose SODEC, novel model that demonstrates the effectiveness of single-step diffusion for image compression. We introduce the fidelity guidance module to improve reconstruction fidelity. The module provides explicit structural guidance through high-fidelity preliminary reconstruction. Furthermore, we introduce the rate annealing training strategy that enables effective optimization at extremely low bitrates. Extensive experiments demonstrate that our SODEC achieves excellent rate-distortion-perception performance. Compared with multi-step diffusion approaches, SODEC offers more than 20 decoding speedup. References Agustsson, E.; Minnen, D.; Toderici, G.; and Mentzer, F. 2023. Multi-realism image compression with conditional generator. In CVPR. 1 Agustsson, E.; Tschannen, M.; Mentzer, F.; Timofte, R.; and Gool, L. V. 2019. Generative adversarial networks for extreme learned image compression. In ICCV. 2 Bachard, T.; Bordin, T.; and Maugey, T. 2024. CoCliCo: Extremely low bitrate image compression based on CLIP semantic and tiny color map. In PCS. 2 Balle, J.; Minnen, D.; Singh, S.; Hwang, S. J.; and Johnston, N. 2018. Variational image compression with scale hyperprior. arXiv preprint arXiv:1802.01436. 1, 2 Blau, Y.; and Michaeli, T. 2019. Rethinking lossy compression: The rate-distortion-perception tradeoff. In ICML. 1, 2 Bross, B.; Wang, Y.-K.; Ye, Y.; Liu, S.; Chen, J.; Sullivan, G. J.; and Ohm, J.-R. 2021. Overview of the versatile video coding (VVC) standard and its applications. TCSVT. 1, 2 Careil, M.; Muckley, M. J.; Verbeek, J.; and Lathuili`ere, S. 2023. Towards image compression with perfect realism at ultra-low bitrates. In ICLRns. 2, 3 Careil, M.; Muckley, M. J.; Verbeek, J.; and Lathuili`ere, S. 2024. Towards Image Compression with Perfect Realism at Ultra-Low Bitrates. In ICLR. 2 Cheng, Z.; Sun, H.; Takeuchi, M.; and Katto, J. 2020. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In CVPR. 1, 2 Ding, K.; Ma, K.; Wang, S.; and Simoncelli, E. P. 2020. Image quality assessment: Unifying structure and texture similarity. TPAMI. 5 Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR. 4 Eastman Kodak Company. 1999. Kodak Lossless True Color Image Suite. http://r0k.us/graphics/kodak/. Accessed: 202405-28. 5 Ghouse, N. F.; Petersen, J.; Wiggers, A.; Xu, T.; and Sauti`ere, G. 2023. Residual Diffusion Model for High Perceptual Quality Codec Augmentation. arXiv preprint arXiv:2301.05489. 2 Guo, J.; Ji, Y.; Chen, Z.; Liu, K.; Liu, M.; Rao, W.; Li, W.; Guo, Y.; and Zhang, Y. 2025. OSCAR: One-Step DifarXiv preprint fusion Codec Across Multiple Bit-rates. arXiv:2505.16091. 2 He, D.; Yang, Z.; Peng, W.; Ma, R.; Qin, H.; and Wang, Y. 2022a. Elic: Efficient learned image compression with unevenly grouped space-channel contextual adaptive coding. In CVPR. 2 He, D.; Yang, Z.; Yu, H.; Xu, T.; Luo, J.; Chen, Y.; Gao, C.; Shi, X.; Qin, H.; and Wang, Y. 2022b. Po-elic: Perceptionoriented efficient learned image coding. In CVPR. 1 Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. In NeurIPS. 1, 2 Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W.; et al. 2022. Lora: Low-rank adaptation of large language models. ICLR. 4 Kingma, D. P.; and Welling, M. 2014. Auto-encoding variational bayes. In ICLR. 2 Kingma, D. P.; Welling, M.; et al. 2019. An introduction to variational autoencoders. Foundations and Trends in Machine Learning. 1 Korber, N.; Kromer, E.; Siebert, A.; Hauke, S.; MuellerGritschneder, D.; and Schuller, B. 2024. Perco (sd): Open perceptual compression. arXiv preprint arXiv:2409.20255. 5, 6 Lei, E.; Uslu, Y. B.; Hassani, H.; and Bidokhti, S. S. 2023a. Text+ sketch: Image compression at ultra low rates. In ICMLW. 2 Lei, E.; Uslu, Y. B.; Hassani, H.; and Saeedi Bidokhti, S. 2023b. Text + Sketch: Image Compression at Ultra Low Rates. In ICMLW. 1 Li, Z.; Zhou, Y.; Wei, H.; Ge, C.; and Jiang, J. 2024. Towards extreme image compression with latent feature guidance and diffusion prior. TCSVT. 1, 2, 3, 5 Liu, L.; Zhou, Y.; Liu, Y.; Ma, S.; and Gao, W. 2024. Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models. In CVPR. 2 Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV. 3 Mentzer, F.; Toderici, G. D.; Tschannen, M.; and Agustsson, E. 2020. High-fidelity generative image compression. In NeurIPS. 1, 2, 3, 4, 5 Minnen, D.; Balle, J.; and Toderici, G. D. 2018. Joint autoregressive and hierarchical priors for learned image compression. In NeurIPS. 1, 2 Minnen, D.; and Singh, S. 2020. Channel-wise autoregressive entropy models for learned image compression. In ICIP. 2 Mittal, A.; Soundararajan, R.; and Bovik, A. C. 2012. Making completely blind image quality analyzer. SPL. 5 Muckley, M. J.; El-Nouby, A.; Ullrich, K.; Jegou, H.; and Improving statistical fidelity for neural Verbeek, J. 2023. image compression with implicit local likelihood models. In ICML. 1, 2, 5 Pan, Z.; Zhou, X.; and Tian, H. 2022. Extreme generative image compression by learning text embedding from diffusion models. arXiv preprint arXiv:2211.07793. 2 Relic, L.; Azevedo, R.; Gross, M.; and Schroers, C. 2024. Lossy image compression with foundation diffusion models. In ECCV. 2 Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. 2, 4, 5 Saharia, C.; Ho, J.; Chan, W.; Salimans, T.; Fleet, D. J.; and Norouzi, M. 2021. Image Super-Resolution via Iterative Refinement. arXiv preprint arXiv:2104.07636. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. 2 Taubman, D. S.; Marcellin, M. W.; and Rabbani, M. 2002. JPEG2000: Image compression fundamentals, standards and practice. Journal of Electronic Imaging. 1, 2 Theis, L.; Salimans, T.; Hoffman, M. D.; and Mentzer, F. 2022. Lossy compression with gaussian diffusion. arXiv preprint arXiv:2206.08889. 1 Toderici, G.; Theis, L.; Johnston, N.; Agustsson, E.; Mentzer, F.; Balle, J.; Shi, W.; and Timofte, R. 2020. CLIC 2020: Challenge on Learned Image Compression. In CVPRW. 5 Tschannen, M.; Agustsson, E.; and Lucic, M. 2018. Deep generative models for distribution-preserving lossy compression. NeurIPS. 2 Van Den Oord, A.; Vinyals, O.; et al. 2017. Neural discrete representation learning. In NeurIPS. 1, 2 Vonderfecht, J.; and Liu, F. 2025. sion with pretrained diffusion models. arXiv:2501.09815. 2 Wang, D.; Yang, W.; Hu, Y.; and Liu, J. 2022. Neural datadependent transform for learned image compression. In CVPR. 2 Wang, J.; Chan, K. C.; and Loy, C. C. 2023. Exploring clip for assessing the look and feel of images. In AAAI. 5 Wang, Z.; Simoncelli, E. P.; and Bovik, A. C. 2003. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems & Computers. 5 Xia, Y.; Zhou, Y.; Wang, J.; An, B.; Wang, H.; Wang, Y.; and Chen, B. 2025. DiffPC: Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement. In ICLR. 2 Yang, R.; and Mandt, S. 2023. Lossy image compression with conditional diffusion models. In NeurIPS. 1, 2, 5 Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as perceptual metric. In CVPR. 5 Lossy compresarXiv preprint"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Shanghai Jiao Tong University"
    ]
}