{
    "paper_title": "Agent models: Internalizing Chain-of-Action Generation into Reasoning models",
    "authors": [
        "Yuxiang Zhang",
        "Yuqi Yang",
        "Jiangming Shu",
        "Xinyan Wen",
        "Jitao Sang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position \\emph{Large Agent Models (LAMs)} that internalize the generation of \\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 8 5 6 0 . 3 0 5 2 : r AGENT MODELS: INTERNALIZING CHAIN-OF-ACTION GENERATION INTO REASONING MODELS Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen & Jitao Sang School of Computer Science and Technology Beijing Jiaotong University Beijing, China {yuxiangzhang, yqyang, jiangmingshu, xinyanwen, jtsang}@bjtu.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce realenvironment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ ADaM-BJTU/AutoCoA."
        },
        {
            "title": "INTRODUCTION",
            "content": "OpenAI has outlined five progressive stages on the path to Artificial General Intelligence (AGI). The first stage, characterized as Chatbot, is exemplified by Large Language Models (LLMs) like GPT3.5 and GPT-4 OpenAI (2023). The second stage, termed Reasoner, introduces Large Reasoning Models (LRMs) such as o1 OpenAI (2024) and o3. Recently, OpenAI released Operator OpenAI (2025a) and Deep Research OpenAI (2025b), signaling the arrival of the third stage: Agent. These systems reportedly combine reasoning with autonomous tool usage, enabling independent execution of multi-round workflows by interacting with the real-world environment. It is believed that the technology behind Operator and Deep Research is not merely integrating existing LLMs or LRMs with agentic workflows (e.g., ReAct Yao et al. (2022), Reflexion Shinn et al. (2023)). Instead, it represents further upgrade in model capabilities: the new models are capable of long-term planning, tool manipulation, and environmental interaction. Taking Deep Research as an example, most currently available open-source versions, such as Open Deep Research Team (2025) by Hugging Face, primarily rely on agentic workflows to orchestrate task planning and tool usage. The switching between thought and action is triggered by preset workflows, making it passive behavior. In contrast, OpenAIs Deep Research enhances the multi-turn tool usage capability of reasoning models. The switching between thought and action is based on the models inherent behaviors, where the model actively decides when and how to take action 1. This allows for Corresponding author. 1 Equal contribution. 1 In an agentic workflow employing reasoning models, the agent also triggers actions based on its own behavior during reasoning, but this switching is only based on the preceding context. In contrast, the model behind OpenAIs Deep Research has undergone end-to-end fine-tuning on the basis of reasoning models for tool usage capabilities. It considers not only the preceding context but also the subsequent context, explicitly trained with task completion as the optimization objective. 1 Figure 1: The path from Large Language Model (LLM) to Large Reasoning Model (LRM) and the forthcoming Large Agent Model (LAM). longer chain of thought-action loops, enabling the completion of more complex knowledge search tasks. We believe this type of tool-augmented reasoning model deserves distinct name, and we propose calling it Large Agent Models (LAMs). Task planning and tool usage are two core capabilities of an Agent. The reasoning model internalizes the capability of task planning, i.e., generating Chain-of-Thought (CoT). By further enhancing and internalizing the capability of tool usage, i.e., generating Chain-of-Action (CoA), the reasoning model evolves into an Agent model. It is helpful to review the development of reasoning models to understand the relationship between agentic workflow and agent model. As illustrated in Fig. 1, reasoner needs to add the ability to generate Chain-of-Thought (CoT) on top of the System-1 LLM. One approach is through prompting, leveraging in-context learning to force the model to output multi-step reasoning, commonly referred as CoT prompting Wei et al. (2022). Another approach is learning-based Zhang et al. (2024a); Zeng et al. (2024), whether through SFT alone, combination of SFT followed by RL, or direct RL. This results in reasoning models like o1 and DeepSeek-R1. Unlike CoT prompting relying on external guidance to elicit multi-step reasoning, reasoning models learn the logics between thinking steps and internalize the CoT generation capability as active model behavior. Similarly, agents need to move beyond CoT reasoning to CoA generation, paradigm dynamically interleaving thought and action. While agentic workflow relies on prompting-based methods to simulate this interplay, agent model learns the coupled CoT and CoA patterns into its behavior, resulting in stronger logical coherence and seamless transitions between thought and action. The model initiates tool usage only when necessary and makes context-aware decisions on how to execute it. Harnessing tools to embody in digital/physical environments, agent models effectively bridge the gap between reasoning and action. In fact, this paradigm shift from prompting to learning aligns with AIs decades-long evolution from handcrafted rules to data-driven learning: from expert systems to classical machine learning and then to deep learning. Patterns learned from data exhibit better generalization and greater flexibility. We have already witnessed that reasoning models like OpenAI o1 and DeepSeek R1 have marginalized the role of explicit prompting, as they can better understand user intent and proactively make plans. Similarly, the shift from agentic workflows to agent models, which internalize CoA generation, is expected to gradually replace the static and scripted workflows. In this report, we introduce the recent attempt to enhance the multi-turn tool usage capability of reasoning models, that is, to interleave CoA generation within the CoT trajectories. Our preliminary study focuses on small-scale reasoning models, using search action as testbed for open-domain question answering (QA) tasks. Initial results demonstrate that internalizing CoA generation substantially improves task completion rates compared to agentic workflows. Future work will involve experimental validation on larger models, more tool types, and wider range of open-ended tasks."
        },
        {
            "title": "2 AGENT MODEL",
            "content": "2.1 DEFINITION AND FORMALIZATION Agent models are enhanced with the ability to use tools. This requires it to extend beyond its own reasoning behaviors and interact with the external environment. Fig. 2 compares the interaction 2 Figure 2: Interaction paradigm: Chatbot vs. Reasoner vs. Agent. paradigms from chatbots to agent models. Both Chatbot and Reasoner focus exclusively on binary interactions between human and model. The difference lies in that reasoner-layer models engage in slow, deliberate reasoning before responding. While, the third layer, Agent, requires the model to think and act simultaneously, which takes the external environment into consideration. This creates ternary structure of human-model-environment. Action, in this case, refers to interacting with the environment through tool usage to obtain feedback. After multiple rounds of thought, action, and observation, the final response is generated. This applies to both Operator and Deep Research: Operators environment consists of operating systems and applications, and Deep Research currently operates within the environment of webbased information and data resources. We formally define Agent Model as follows: Definition 1. An Agent Model is generative model built upon reasoning model, enhanced through end-to-end task-oriented tool-augmented training. It produces sequences of interleaved reasoning (Chain-of-Thought) and action (Chain-of-Action) steps, where each action invokes tool 2 to interact with the external environment. Observations from these interactions guide subsequent reasoning and actions until the task is completed. The inference of agent model can be formalized as partially observable Markov decision process (POMDP), where the state is defined as [s0, tc, x1:n], comprising: (1) s0: the initial environment state (e.g., the state of digital or physical system), (2) tc: the task context (e.g., user prompt), (3) x1:n: the sequence generated so far, combining tokens emitted by the model and responses from the environment. Actions in this POMDP are the generation of the next token xn+1 πθ( s0, tc, x1:n), where πθ is the policy parameterized by θ. At each step, the policy πθ can generate one of the following types of tokens: think: Initiates or continues reasoning step, contributing to the Chain-of-Thought (CoT), which represents internal deliberation or planning. Following this token, reasoning sequence = [r1, r2, . . . , rk] is generated, where each ri is token in the reasoning process. action: Invokes tool to interact with the environment, contributing to the Chain-ofAction (CoA). This token signals the transition from thought to action, deciding when to action. Following this token, the model internally generates or acquires from environment: Action type: specifies the tool from tool library τ (e.g., search), Action parameters: p, defines how to execute the tool, deciding how to action (e.g., detailed search query), Response by action invocation: fetches the environments response = τ (p), analogous to observations in ReAct. Note that this response is not generated by policy πθ 2 Tools can be broadly categorized into two types: those for accessing external resources, such as realtime information and specialized databases via web browsers and API interfaces, and those for professional processing, such as complex computation and data analysis softwares. 3 but returned by interacting with the environment. will appended to the trajectory, providing context for subsequent turns. answer: Outputs the final response, indicating task completion or termination due to exceeding predefined budget. Following this token, an answer sequence = [a1, a2, . . . , am] is generated, representing the task solution. The inference pipeline of agent model is shown in Algorithm 1, outlining the step-by-step process by which the agent model generates its trajectory. The algorithm iteratively generates tokens based on the current state, deciding whether to engage in further reasoning, invoke tool, or conclude with an answer. This pipeline ensures that the model alternates between internal reasoning (CoT) and external actions (CoA), allowing it to dynamically adapt its strategy based on environmental feedback. The loop continues until the task is completed or predefined limit on reasoning-action iterations/contextual length is reached. We can see that, unlike agentic workflows that rely on external prompts or scripts to manage interactions, the agent model internalizes the logic of tool usage as part of its learned behavior, enabling autonomous and context-aware decision-making for tool invocation. Algorithm 1 Agent Model Inference Pipeline Require: Policy model πθ, tool library , Task context tc, max action steps , max contextual length Ensure: Final answer end while else if xn = action then while next token / {action, answer} do πθ( x1:n) x1:n+1 x1:n r, + Generate token: xn+1 πθ( x1:n) x1:n+1 x1:n xn+1, + 1 if xn = think then 1: Initialize: 0, #(tc), x1:n tc, 2: while < and < and answer not in x1:n do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end if 22: 23: end while 24: if answer / x1:n then 25: 26: 27: end if 28: return + 1 Generate πθ( x1:n), = #(c) x1:n+k x1:n τ , + Select tool: τ [c] Generate πθ( x1:n), = #(p) x1:n+k x1:n p, + Execute: τ (p), = #(p) x1:n+k x1:n o, + Generate πθ( x1:n), + 1 return x1:n x1:n answer Generate πθ( x1:n) else if xn = answer then Generate CoT Generate CoA Generate action type map action type tokens to specific tool Generate action parameter Invoke tool and acquire response Generate answer sequence Force termination and answer 2.2 CHALLENGES The development of agent model requires end-to-end training on top of reasoning models to enhance its capabilities in tool usage and interaction with the external environment. This presents two fundamental challenges. 4 Challenge 1: Balancing Reasoning and Action. Reasoning models excel at generating Chains-ofThought (CoT). Fine-tuning these models to enhance action execution capabilities poses common problem: the risk of forgetting the models CoT-generation ability Zhang et al. (2024b). More critically, the key to interleaving CoT with Chain-of-Action (CoA) lies in teaching the model to trigger actions at appropriate times within the reasoning process. This ensures that reasoning and action complement each other effectively to jointly accomplish the task. Challenge 2: Managing External Environment Interaction. Interacting with the external environment through tool invocations may result in high training costs, low efficiency and even risks. Additionally, the dynamic nature of the external environment complicates the training process. For instance, during online reinforcement learning (RL), training delays and dynamic environmental changes will lead to instability in training, as the model needs to adapt to the changing environment. OpenAIs Deep Research exemplifies the practical challenges in agent model development. As officially claimed OpenAI (2025b), Deep Researchs agent model is built by end-to-end reinforcement fine-tuning atop the flagship reasoning model o3. The main action here is web search, and the environment consists of publicly available information on the internet. Regarding Challenge 1, the model needs to autonomously determine during the reasoning process when its internal knowledge is insufficient, and when it needs to obtain information from the external environment. This requires the model to have good understanding of its knowledge limitations and the ability to trigger actions at the right time. As for Challenge 2, the cost of calling search engines and the dynamic nature of the network environment pose significant difficulties. The time and resources required for search calls can lead to high training costs and low efficiency. Moreover, the dynamic nature of the network environment makes it challenging for the model to adapt and respond in timely manner. In the next Section, we will use an open-domain QA task similar to Deep Research as the testbed, with the tool restricted to web search, to explore possible solutions."
        },
        {
            "title": "3 LEARN TO GENERATE CHAIN-OF-ACTION",
            "content": "3.1 OVERVIEW We introduce an agent model training solution, Automatic generation of Chain-of-Action (AutoCoA), which consists of two stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), to internalize the Chain-of-Action (CoA) generation capability into reasoning models. To address the challenge of reasoning-action balance, we mix pure CoT data (w/o tool usage) in both SFT and RL stages. This ensures that the model does not forget its reasoning capabilities and can adaptively decide whether to use action on tasks of different difficulties. Meanwhile, at the beginning of SFT, we add additional substage of injecting the tool-usage ability at step level, referred to as CoT+A. Specifically, we generate contrastive data pairs by modifying reasoning prefix in erroneous samples: one variant enables tool usage while the other does not. Successful task completion with tool invocation serves as positive example, whereas failure without action forms negative counterpart. Contrastive loss is then used to train the model on when to trigger actions. To address the challenge of managing environment interaction, we explore the possibility of enabling the agent model to simulate the environment by incorporating an internal world model. Specifically, at the end of SFT, we added sub-stage to train the policy model to simulate tool calls and generate the corresponding observations. During the RL stage, the model first generates its own observations based on the policy model and explores potential trajectories through extensive rollouts for fast convergence. Then, only limited number of tool calls are made to interact with the real environment, ensuring adaptation to real-world conditions. The overall AutoCoA framework consists of the following stages: SFT Stage CoT + A: focusing on step-level action triggered by interleaving action tokens action within CoT sequences at critical reasoning junctures. Using contrastive loss, trajectories 5 with necessary actions (e.g., invoking search to resolve knowledge gaps) are compared against those without, teaching the model to autonomously decide when to action. CoT + CoA (w/ observation mask): the main training phase at the trajectory-level. During loss calculation for the policy model, the environment response is excluded. This teaches the model how to action, including determining the action type and parameters. CoT + CoA (w/o observation mask): training the model to predict environment responses (e.g., simulated search results) alongside actions. Joint optimization of action generation and observation prediction equips the policy with an implicit world model. This preconditions the model for the subsequent RL stage. RL Stage CoT + CoA (simulated environment): after rolling out an action, the policy model itself continues to generate the observation leveraging the learned internal world model from SFT. This allows for extensive sampling to explore diverse action sequences, and rapid converging to feasible solution space. CoT + CoA (real environment): leveraging the improved exploration capabilities from the previous sub-stage, the model interacts with the real environment to receive observations. This substage helps the model adapt to the real-world environment and ensures robust deployment. The AutoCoA framework enables progressive capability injectionfrom step-level action triggering to trajectory-level CoA optimizing, and from simulated exploration to real environment interaction. In the following subsections, we introduce in detail the implementation of each sub-stage. 3.2 SUPERVISED FINE-TUNING 3.2.1 DATA SYNTHESIS In this section, we use the training set of HotpotQA Yang et al. (2018) as our base dataset. HotpotQA is multi-hop question-answering dataset containing questions and answers that require multi-step reasoning. We randomly selected 20,000 samples to construct our dataset D. Formally, each data point contains the following elements: Question q: natural language question requiring multi-step reasoning; Answer a: the correct answer to the question. To support the training requirements of the AutoCoA framework in the Supervised Fine-Tuning (SFT) stage, we designed three data synthesis methods to generate CoT data, CoA data, and CoT+A data, respectively. We used the DeepSeek-R1-Distill-Qwen-32B Guo et al. (2025) model for data generation. Regarding the environment for tool invocation, we set up local wiki search engine based on FlashRAG Jin et al. (2024). The integration of this tool enables the model to simulate the process of obtaining additional information through external resources during data generation, thereby training the model to intelligently trigger external tool calls when facing complex reasoning tasks, further enhancing its performance. CoT data: We randomly selected 10,000 samples from dataset as the base data. Using the DeepSeek-R1-Distill-Qwen-32B model with CoT prompt designed to elicit step-by-step reasoning, we generated pure chain-of-thought processes without permitting external tool usage. The resulting CoT data captures the models sequential reasoning, encompassing problem decomposition, analysis of known information, and derivation of the final answer, all independent of external tools. These data are designed to maintain the models basic reasoning abilities, ensuring it can handle questions that do not require external knowledge supplementation. Formally, we define the CoT data set as DCoT = {(q, rCoT , a)(q, a) D}, where rCoT denotes the sequence of reasoning steps generated by the model. CoA data: We selected another 10,000 samples from dataset for construction. Using CoA prompt that instructs the model to invoke search tools when encountering knowledge gaps, we enabled tool-assisted reasoning. The model triggers the action token to call search tool, retrieves necessary information, and resumes reasoning based on the results. The generated CoA data captures the full chain: the question q, the reasoning process rCoA (including the action trigger action, observation from the environment, and subsequent reasoning), and the final answer a. This data structure enables the model to learn when and how to trigger actions, as well as how to continue reasoning based on the returned observations. Formally, we define the set of CoA data as DCoA = {(q, rCoA, a)(q, a) D}, where rCoA represents the interleaved sequence of reasoning, action, observation, and further reasoning. CoT+A data: The CoT+A sample construction comprises two pairs of positive and negative examples. From the CoT and CoA data, we identified samples where either method yielded incorrect answers. For these, we generated contrastive pairs by altering the reasoning trajectory at specific point: either inserting an action trigger action to enable tool usage or removing it to enforce internal reasoning. Each pair shares the same initial prefix but diverges on whether tool usage is permitted thereafter, leading to scenarios where either CoT is incorrect but CoA is correct or CoA is incorrect but CoT is correct. The former scenario highlights cases in which the model must leverage external tools to arrive at the correct solution, whereas the latter shows that indiscriminate tool usage can result in errors. We train these contrastive pairs using contrastive loss functions to help the model learn when to trigger actions and when to rely solely on its internal reasoning. For the training process, we initially selected 1,500 CoT+A contrastive data points as the training set for SFT Stage 1, thereby injecting the capability to determine when to trigger action. Subsequently, for SFT Stages 2 and 3, we combined 5,000 CoA data points with 1,000 CoT data points to instill the capability of how to execute action. Including 1,000 CoT samples ensures the model retains its ability to solve tasks without external tools, preventing over-reliance on actions and balancing reasoning with tool usage. This mixed strategy ensures that the model can adaptively decide whether to utilize external tools based on task complexity and knowledge requirements, thereby maintaining stable performance across diverse types of questions. 3.2.2 SFT-STAGE1: COT+A In this subsection, we describe our training approach for teaching reasoning models to decide when to switch from internal reasoning to external actiona process that can be framed as contrastive learning over sequence patterns. During this learning process, the policy model πθ distinguishes between two distinctly different sequence output modes: one mode is initiated by the thinking-type token think and ultimately ends with an action-type token action, triggering interaction with the external environment; the other mode includes only the models internal reasoning behaviors initiated by the think token. Both modes share the same prefix context. We denote by xchosen = [x1, x2, . . . , xn] the sequences for which we intend to boost the probability under the policy model, and by xrejected the sequences whose probabilities we aim to reduce. In most cases, we consider sequences containing the action token as xchosen. However, to avoid reinforcing model biases and to help the model correctly learn when to act, we also include some sequences containing only the think token and ultimately arriving at the correct answer as part of xchosen. Given context c, we compute the log logprobability of each target output sequence. After normalization, the result can be calculated as: (x c) = 1 x (cid:88) i=1 log πθ(xi c, x<i) where denotes the length of the sequence, and πθ() represents the probability output by the current model for the given sequence. The contrastive loss function used at this stage is thus formulated as: Lcontra = log σ [P (xchosen c) (xrejected c)] where σ is the sigmoid function. This loss encourages the model to assign higher probabilities to chosen sequences than to rejected sequences. During training, we observed phenomenon: both chosen and rejected sequence probabilities tend to decrease simultaneously, potentially leading to model collapse. To prevent this, we incorporate an auxiliary supervised fine-tuning loss: 7 This auxiliary loss encourages the model to maintain high probabilities for chosen sequences while differentiating them from rejected sequences. The final loss combines these components: Laux = (xchosen c) = Lcontra + α Laux where α is coefficient controlling the contribution of the auxiliary loss."
        },
        {
            "title": "3.2.3 SFT-STAGE2: COT + COA (W/ OBSERVATION MASK)",
            "content": "After the previous stage, the policy model πθ has learned to decide at the individual step level when to trigger actions for external assistance; however, it has not yet captured complete decisionmaking trajectories. Therefore, at this stage,we further train the model via supervised fine-tuning on synthetic complete decision-making trajectories that integrate both CoT and CoA components. This enhances the models capability to couple multiple reasoning and action steps, teaching it behavioral paradigm of alternating reasoning with action, while preserving its original reasoning capabilities as much as possible. Additionally, to avoid interference from external feedback during learning, we mask out loss contributions from tokens representing external feedback. Given the task context tc and the complete decision-making trajectory = [x1, x2, . . . , xn], where each xi {think, action, answer}, the loss function at this stage is computed as follows: = 1 x (cid:88) i= Ixi=o [ log πθ(xi tc, x<i) ] Here, Ixi=o indicates tokens that do not correspond to external feedback, and we only consider the numerical terms associated with these tokens in the loss calculation. 3.2.4 SFT-STAGE3: COT + COA (W/O OBSERVATION MASK) In this stage, we modify the training process by including the loss for external environment feedback tokens in the complete trajectories. During training, the model is tasked with optimizing both its internal reasoning and action outputs while also accurately predicting external feedback. This joint training effectively serves as implicit learning of an internal world model, thereby offering more robust initialization for the subsequent reinforcement learning stage. 3.3 REINFORCEMENT LEARNING The reinforcement learning stage in the AutoCoA framework aims to internalize the Chain-of-Action generation capability into the reasoning model, enhancing its multi-turn tool usage capacity. Building on the Supervised Fine-Tuning stage, RL employs Group Relative Policy Optimization (GRPO) to refine the models ability to interleave CoT and CoA sequences. The approach utilizes precise matching rule reward system, ensuring that the model not only reaches the correct answer but also adheres to predefined formatting requirements. 3.3.1 REWARD FUNCTION The reward function in RL is strictly based on two criteria: exact match with the ground truth and adherence to the required format. The model receives rewards only when the generated output exactly matches the expected result. Additionally, format penalty is introduced to enforce the correct structure of the output sequence. Exact matching reward: positive reward is awarded only when the final output is an exact match with the ground truth. This approach encourages the model to maintain high precision in generating responses. Format penalty: penalty is imposed if the models output does not adhere to the required format, such as missing critical tags like think. This mechanism ensures the model generates well-structured CoT and CoA sequences."
        },
        {
            "title": "3.3.2 POLICY OPTIMIZATION USING GRPO",
            "content": "The model is optimized using the Group Relative Policy Optimization (GRPO) algorithm, which is adapted to the generative nature of CoT and CoA sequences. GRPO evaluates group of potential actions at each step and optimizes the policy based on the relative performance of these actions within the group. For each state, GRPO samples set of actions and computes the group-relative advantage: At = rt"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 rt,k Where rt is the reward of the selected action, and is the size of the action group. This advantage function guides the model to prefer actions that outperform others in the same context. The objective function for GRPO is: J(θ) = Eτ πθ (cid:35) log πθ(atst)At (cid:34) (cid:88) t=0 This method ensures that policy updates are driven by the relative quality of actions, promoting stability and efficiency during training. 3.3.3 RL-STAGE1: COT + COA (SIMULATED ENVIRONMENT) In the first RL stage, the model interacts with simulated environment using its internal world model developed during the SFT phase. This simulated environment acts as controlled, low-cost, and highly efficient sandbox where the model can freely explore diverse range of action strategies. During this stage, the model generates hypothetical tool interactions and simulates the responses using its learned internal world model. GRPO is utilized to explore wide array of potential CoA strategies through group-based sampling and rule-based rewards. Through iterative refinement, the model learns to balance reasoning and action, improving its ability to decide when and how to invoke external tools. 3.3.4 RL-STAGE2: COT + COA (REAL ENVIRONMENT) In the second RL stage, the agent model transitions to the real environment, where it interacts with actual tools to obtain real-world feedback. This phase focuses on adapting the strategies initially learned from the simulated environment to dynamic and unpredictable real-world scenarios. Compared to the previous stage, in this stage, the model executes real tool invocations (e.g., web searches) and replaces the simulated responses with actual feedback obtained from these tools. This staged approach enables the agent model to maintain its internalized CoA generation capability, ensuring strong performance in both controlled and open-ended task environments."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETTINGS The evaluations in this experiment encompass range of open-domain QA tasks, including 2 singlehop QA datasets and 3 multi-hop QA datasets. Single-hop QA datasets: Natural Questions (NQ) Kwiatkowski et al. (2019) consists of real-world Google search queries with answers sourced from Wikipedia articles, providing benchmark for answering straightforward questions. 9 TriviaQA Joshi et al. (2017) is large-scale dataset derived from trivia websites and competitions, characterized by complex entity relationships and challenging question formulations. Multi-hop QA datasets: HotpotQA Yang et al. (2018) is the pioneering large-scale dataset that requires reasoning across multiple Wikipedia paragraphs to derive answers. 2WikiMultihopQA (2WIKI) Ho et al. (2020) enhances multi-hop question answering by offering explicit reasoning paths. MuSiQue Trivedi et al. (2022) presents 2-4 hop questions constructed by combining elements from five existing single-hop datasets, testing the ability to integrate multiple information sources. Bamboogle Press et al. (2022) introduces set of complex questions that major search engines, including Google, often answer incorrectly, offering robust evaluation of models compositional reasoning across diverse domains. In this experiment, we use two evaluation metrics: Exact Match accuracy (EM) : Counts as correct only when the models prediction is exactly the same as the ground truth. This metric strictly evaluates the models precision. LLM-based accuracy (LLM): Uses LLM (Qwen2.5-14B-instruct) to determine if the models prediction semantically matches the ground truth, allowing for more diverse correct answers. We selected small-scale reasoning model, R1-Distill-Qwen-7B, for AutoCoA training. For parameter optimization, we utilized LLaMA-Factory Zheng et al. (2024) as the framework for Supervised Fine-Tuning and verl Sheng et al. (2024) as the framework for Reinforcement Learning. All RL training was conducted on single node with Nvidia H20 GPUs, utilizing sequence parallelism. 4.2 COMPARED METHODS We conducted comprehensive evaluation of our AutoCoA framework by comparing it with various baseline methods across different categories. This section provides brief introduction to these methods. Model only (w/o action) To establish baseline for the inherent knowledge capacity of foundation models without retrieval augmentation, we evaluate several reasoning-focused models in zero-shot setting: R1-Distill-Qwen-7B : the initial policy model used for AutoCoA training. Deepseek-R1 : The original 670-B large reasoning model with enhanced reasoning capabilities, representing state-of-the-art performance without retrieval augmentation. Agent workflow We implemented traditional agent workflows (ReAct) on different types of base models to evaluate the effectiveness of action frameworks across model categories: Qwen-7B-Base(ReAct) : base language model configured with ReAct prompting techniques to generate reasoning and action sequences. This represents standard baseline for agentic workflows without specialized reasoning capabilities. R1-Distill-Qwen-7B(ReAct) : reasoning-enhanced model configured with ReAct prompting. This approach combines enhanced reasoning capabilities with structured action framework. Both approaches follow standard ReAct protocols where external tool calls are explicitly structured through prompting rather than being internalized as model capabilities. Agent model We evaluated several variants of our agent model to understand the impact of each training stage: 10 SFT-stage1 : The model after completing only the CoT+A stage, which focuses on teaching when to action using contrastive learning at the step level. This stage used 1.5K CoT+A contrastive data points. SFT-stage2 : The model after completing only the CoT+CoA(w/ observation mask) stage, which focuses on trajectory-level action patterns without observation mask prediction. This stage used mix of 5K CoA and 1K CoT data points. SFT-stage1&2 : combined approach where the model undergoes both SFT-Stage 1 and SFT-Stage 2 sequentially, benefiting from both step-level action triggering and trajectorylevel pattern learning. SFT-stage1&2&3 : The complete SFT pipeline, including the CoT+CoA(w/o observation mask) stage that enables the model to predict environment responses alongside actions, effectively learning an implicit world model. This stage used the same data mix as SFTstage2. We compared the performance of different RL settings and methods. All training was conducted with 96 optimization steps on the same 4,608 problems: RL-stage2: In this setting, we utilize in-context learning to guide model without SFT through CoA-based reasoning. An additional 50% optimization steps were added for this training configuration. SFT-stage1&2&3 + RL-stage1: The model is trained exclusively in the simulated environment, using SFT-stage1&2&3 as the initial policy model. SFT-stage1&2&3 + RL-stage1&2: The model is first trained for the initial 80 steps (5/6) in the simulated environment, then switched to the real environment for the rest 1/6. The initial policy model is set to SFT-stage1&2&3. SFT-stage1&2 +RL-stage2: The model is trained exclusively in the real environment, using SFT-stage1&2 as the initial policy model. 4.3 RESULTS The main results are summarized in Table 1. Observations include: Agentic workflow vs. agent model. After CoA learning, all AutoCoA variants substantially outperform the initial policy model with the ReAct workflow. This validates the effectiveness of end-to-end optimization for task-oriented when and how to act during reasoning. Agent model: SFT-stage1&2 vs. SFT-stage2. Separating CoA fine-tuning into when-toact (stage1) and how-to-act (stage2) sub-stages enhances the models ability to learn CoA patterns, enabling better transitions between thought and action. Agent model: RL-stage2: Compared to the initial policy, direct RL shows some improvement but is constrained by the initial policys baseline capacity, resulting in marginal gains. Direct RL application shows marginal improvements over the initial policy, constrained by the policys baseline capacity. Agent model: SFT-stage1&2&3 + RL-stage1 vs. SFT-stage1&2&3 +RL-stage1&2. After extensive (5/6) simulated environment training (RL-stage1), small portion (1/6) of real-world interaction (RL-stage2) improves adaptation to dynamic environments, further boosting accuracy. Agent model: SFT-stage1&2 + RL-stage2 vs. SFT-stage1&2&3 +RL-stage1&2. Compared to full real-world interaction, initial simulated environment training sacrifices some performance. However, it significantly reduces interaction costs. Achieving acceptable results with only 1/6 real interactions demonstrates the feasibility of internal world modeling. Future work is needed to improve world model fidelity and leverage the saved costs for advanced training paradigms. 11 Table 1: Performance comparison between different QA methods. Bold indicates the highest value, while underline indicates the highest value among the different settings of AutoCoA. Method NQ TriviaQA HotpotQA 2WIKI MuSiQue Bamboogle EM LLM EM LLM EM LLM EM LLM EM LLM EM LLM EM LLM Single-hop QA Multi-hop QA Average Model only (w/o action) R1-Distill-Qwen-7B Deepseek-R1* Agentic workflow Qwen-7B-Base (ReAct) R1-Distill-Qwen-7B (ReAct) Agent model: SFT SFT-stage1 SFT-stage2 SFT-stage1&2 SFT-stage1&2&3 Agent model: RL RL-stage2 SFT-stage1&2&3+RL-stage1 SFT-stage1&2&3+RL-stage1&2 SFT-stage1&2+RL-stage2 5.9 39.0 9.8 54.2 17.3 78.2 21.1 85. 11.7 44.8 14.1 53.0 23.3 52.4 25.9 55.8 1.1 21.2 2.5 27. 11.7 56.8 13.1 63.2 11.8 49.5 14.4 60.3 18.0 15.1 37.8 20. 35.2 25.7 48.8 30.1 17.2 15.1 28.4 18.6 24.0 21.3 30.8 22. 4.2 1.9 8.4 3.9 19.2 12.3 24.8 14.7 19.6 15.2 29.8 18. 22.7 25.0 25.0 23.8 22.8 26.0 28.6 28.4 32.2 33.1 34.3 33.1 30.0 33.4 37.0 34.6 40.0 42.6 45.6 44.7 37.5 47.2 48.6 52. 47.8 48.2 51.3 51.2 43.2 53.8 54.0 56.6 21.2 31.7 33.0 30.1 23.2 31.4 33.2 35.6 28.3 39.2 40.8 36.5 28.2 40.0 41.2 42. 26.4 43.8 48.1 49.2 33.2 48.6 49.6 48.4 29.6 48.4 52.4 52.0 33.8 51.4 53.6 51.2 4.8 8.5 9.0 10.5 5.6 9.6 12.6 12. 8.2 12.1 12.4 14.7 8.2 12.4 16.2 15.2 20.2 24.3 30.4 28.8 20.0 27.2 28.0 26.4 22.8 27.2 33.3 32.0 23.2 32.0 31.2 30. 22.7 29.5 32.0 31.3 23.7 31.7 33.4 33.9 29.1 35.6 38.5 37.7 27.8 37.2 38.9 38.5 * The results reflect R1s knowledge memorization rather than action capabilities, due to its large parameter size and the outdated knowledge in QA questions. On new questions, R1s accuracy is very low. See Appendix.A for details. 1 (a) Ratio of correctly completed tasks. (b) Task success rate. Figure 3: Analysis of hong-horizon execution capabilities. 4.3.1 IMPROVING LONG-HORIZON TASK EXECUTION To evaluate the role of CoA learning in tackling tasks that require long-horizon reasoning and action, we analyzed model performance before and after CoA learning as the number of executed actions (#action) increased. Fig. 3(a) illustrates the distribution of correctly completed tasks across different #action. Without CoA learning, the initial policy model primarily succeeded on tasks requiring only 1 or 2 actions. After CoA learning, agent models demonstrated the ability to support longer thought/action rounds, enabling them to complete more complex tasks. We further examined task success rates by #action (Fig. 3(b)). The results reveal consistent trend with Fig. 3(a): Initial policy with workflow shows declining success rates as #action increases, whereas agent models maintain relatively high accuracy at 5 actions. This demonstrates that taskoriented end-to-end training enables agent models to learn improved interleaving patterns between thought and action, with strong consistency in long-context scenarios as the number of actions increases."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "We propose AutoCoA, an agent model learning approach to enhance the multi-turn tool use capability on top of reasoning models. Through end-to-end fine-tuning, the derived agent model demonstrates superior ability to determine when and how to act in pursuit of the ultimate goal, outperforming traditional agentic workflows in goal-oriented decision-making. Methodologically, this report represents an initial exploration into learning CoA (Chain-of-Action). It serves as preliminary attempt, with wide range of potential approaches yet to be explored, including pure reinforcement learning (RL), improved action-inclusive sampling, and loss design for trajectories that interleave CoT (Chain-of-Thought) and CoA. 13 From task perspective, the next steps are extending the application scope: (1) Agent model for open-ended tasks, exploring methods for implementing agent models capable of handling openended tasks with rubrics for grading; (2) AgentRFT (Reinforcement Fine-Tuning), building on our prior work with OpenRFT Zhang et al. (2024b) to tailor the agent foundation model to meet specific task objectives and leverage specialized tools within particular scenarios."
        },
        {
            "title": "5.1 AGENT ROADMAP",
            "content": "OpenAI has long pursued the vision of seamless environmental interaction. Operator stems from its early key project Computer Use, and Deep Research continues its goal since Plugins and GPTs. However, the earlier attempts were constrained by the limited capabilities of models at the time. Today, with advancements in language modeling and logical reasoning, Deep Research has tackled highly complex tasks using only basic tools like web search and document parsing. This demonstrates universal recipe for agent development: once models reasoning capacity surpasses certain threshold, providing it with few simple tools and verifiable task objective within controlled environment enables what the Claude 3.7 official blog describes as action scaling. The fact that OpenAI has began to assess model capabilities via replaceable human expert hours is particularly noteworthy. Even if model only possesses an average level of human intelligence and tool usage ability, combined with the efficiency of AI in information acquisition and processing, the enhancement to productivity would be explosive. Operator and Deep Research exemplify OpenAIs agent paradigmcombining execution (Operator as the hands) and reasoning (Deep Research as the brain) to achieve dynamic programming, autonomous tool usage, asynchronous processing, and automatic task completion. We categorize their functionalities as Computer Use and Knowledge Research, respectively, and envision threephase roadmap for agent development  (Fig. 4)  . 5.1.1 SHORT-TERM: APPLICATIONS By enhancing reasoning foundation models with basic tool-usage capabilities to create agent foundation models, practical applications are enabled through methods like Reinforcement Fine-Tuning (RFT). Below are the key scenario applications for this stage: Agentic RPA natural evolution of traditional Robotic Process Automation (RPA), Agentic RPA integrates learning and reasoning to automate enterprise workflows (e.g., ERP/CRM systems). Unlike rule-based RPA, which struggles with unstructured data or dynamic environments, Agentic RPA adapts to evolving business needs, can tackle more complex and flexible tasks. Personal assistant Focused on mobile-centric, personalized interactions, this agent leverages behavioral logs and personal knowledge bases to act as users digital twin. Future extensions include proactive behavior (predicting needs rather than passive responses) and multimodal context awareness (perceiving and interpreting environmental cues to refine task execution). Domain analyst Deep Research currently faces two main limitations: restricted open-domain data access and shallow expert analysis. For specialized fields such as law, medicine, and research, addressing these gaps requires fine-tuning on proprietary data and domain knowledge. This may enable the leap into actionable insights, supporting real-world analytical consulting and decision-making. Coding agent In addition to applying computer use and knowledge research capabilities, another critical shortterm direction is automating software engineering, as exemplified by projects like Devin and Manus. Coding agents treat programming not as an end goal but as tool-use mechanism (e.g., automating API integrations). Whether deployed internally or as standalone products, coding agents represent foundational infrastructure for scaling agent capabilities. 14 Figure 4: Agent roadmap."
        },
        {
            "title": "5.1.2 MID-TERM: EXPANDING CAPABILITIES",
            "content": "Agent evolution in the mid-term will focus on two parallel directions: (1) Action executor, enhancing task execution by accessing broader range of I/O interfaces, evolving into more capable hands and feet. (2) Insight seeker, deepening information processing through expanded tool integration, becoming more comprehensive brain. Action executor Beyond integrating more software application APIs, agent task execution will extend from software operations to the physical world, requiring enhanced interfaces for both input perception and output execution. IoT device interfaces are typical inputs. By connecting to IoT devices in smart homes and factories, Agent can better sense the environment, and understand human behavior and needs for automated task execution. Output execution interfaces will involve remote operation and control for physical-world automation. Insight seeker At the Deep Research release, OpenAI noted that search is just the first step, and hopes to integrate more tools in the future to let the Agent autonomously discover new knowledge. Possible tools include information acquisition (data sources, unstructured information parsing) and information processing (general visualization, database analysis, and domain-specific tools). Currently, knowledge research agents mainly focus on discovery and information integration within the language space. Innovation and insight require more input. The current input is provided by human feedback in the human-machine collaboration paradigm. In the future, inputs could come from new findings through tool experiments or feedback from multi-modal intelligence. At that time, information processing will no longer start with explicit user requests nor take the form of simple Q&A. Instead, the role will shift: we will provide agents with data, experimental materials, and simulation environments, allowing them to autonomously explore, analyze, deduce, and generate new insights. With the continued development of action executors and coding agents, the insight seeker could further evolve toward the fourth stage: Innovator. 5.1.3 LONG-TERM: AGENT OS Web pages and apps have served as the application carriers for internet and mobile internet technologies, respectively, while agents are now emerging as the carriers for artificial intelligence technologies. From computational perspective, agent models function as the CPU in traditional computer architecture, handling core computation and task orchestration. Tools function as software programs, executing specific operations, and short-term and long-term memory correspond to RAM and hard drives. Together, these components form the basis of what could be termed an agent OS. However, building an agent OS presents significant challenges. Much like the von Neumann architecture relies on the unified storage of programs and data alongside complete instruction set, agent OS requires unified representations for multimodal I/O, tools, and memory, as well as an instruction 15 set tailored for agents. This poses difficulties for current Transformer models, which depend on token-based sequential representations, and for natural language-based instruction systems. Furthermore, data representation and organizational structures need to be adapted to suit agent models. GUIs like web pages and apps, designed for human interaction, are not the most efficient for agents. Agent protocol is required to define how agents interact with tools, vector databases, and other agents, enabling operations at more foundational level to fully harness their strengths. As the agent ecosystem matures, multi-agent collaboration will become pivotal. Agents will dynamically coordinate in complex projectssharing intermediate results, delegating subtasks, and selforganizing like human team. This evolution from individual competence to collective intelligence, marked by enhanced autonomy and cooperative problem-solving, may signal the emergence of the fifth stage: Organization."
        },
        {
            "title": "REFERENCES",
            "content": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Pan. Mintqa: multi-hop question answering benchmark for evaluating llms on new and tail knowledge. arXiv preprint arXiv:2412.17032, 2024. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: modular toolkit for efficient retrieval-augmented generation research. arXiv preprint arXiv:2405.13576, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. OpenAI. Gpt-4 technical report. Technical report, OpenAI, 2023. URL https://cdn.openai. com/papers/gpt-4.pdf. OpenAI. Openai o1 system card. Technical report, OpenAI, 2024. URL https://cdn. openai.com/o1-system-card-20241205.pdf. OpenAI. Introducing operator, January 2025a. URL https://openai.com/index/ introducing-operator/. OpenAI. Introducing deep research, February 2025b. URL https://openai.com/index/ introducing-deep-research/. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. ReflexIn Advances in Neural Information ion: Language agents with verbal reinforcement learning. Processing Systems (NeurIPS), volume 36, pp. 86348652, 2023. 16 Hugging Face Team. Open-source deepresearch freeing our search agents. Hugging Face Blog, 2025. URL https://huggingface.co/blog/open-deep-research. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. Jason Wei, Barret Zoph, Sainbayar Sukhbaatar, Jaime Pineda, Tom Hashimoto, Henry Ma, Quoc Le, and David Dohan. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 1520815221, 2022. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, and Karthik Narasimhan. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. URL https://arxiv.org/abs/2210.03629. Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, and Xipeng Qiu. Scaling of search and learning: roadmap to reproduce o1 from reinforcement learning perspective. arXiv preprint arXiv:2412.14135, 2024. Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding, 2024a. Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao, and Jitao Sang. Openrft: Adapting reasoning foundation model for domain-specific tasks via reinforcement fine-tuning, 2024b. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A APPENDIX",
            "content": "Table 2: Performance Comparison across Temporal Knowledge Domains Dataset Model DeepSeek-R1Distill-Qwen-7B DeepSeek-R1 Recent (Unknown) Popular& Old (Known) Main Question Sub-question Main Question Sub-question EM LLM EM LLM EM LLM EM LLM 0. 0.052 0.017 0.039 0.052 0.073 0. 0.267 0.078 0.098 0.079 0.119 0. 0.646 0.899 0.944 A.1 DATASETS This work evaluates how model knowledge memorization impacts multi-hop reasoning accuracy. The experiments are structured as follows: 1. Unknown knowledge evaluation: test set of 192 three-hop questions (576 sub-questions) was constructed using time-sensitive corpora sourced from 97 sports-related reports published by CCTV News between January 16, 2025, and March 7, 2025. This dataset ensures that the models were not exposed to the data during training, allowing for an evaluation of their ability to handle unseen temporal knowledge. 2. Known knowledge evaluation: For comparison, we used the MINTQA-POP benchmark dataset, proposed by He et al. (2024). We use subset of 192 three-hop popular questions (576 subquestions) was selected through stratified sampling. This dataset covers high-frequency knowledge domains familiar to the models from wiki datasets, enabling an assessment of model performance on known knowledge. A.2 EXPERIMENTAL RESULTS The results highlight the models distinct performance across the two evaluation scenarios: 1. Performance on questions requiring known knowledge: R1 achieved 61.5% (EM) and 64.6% (LLM) on MINTQA-POP, vastly outperforming the distilled 7B models 5.2% (EM) and 7.3% (LLM). This demonstrates R1s reliance on memorized patterns rather than generalized reasoningit excels on tasks involving familiar knowledge domains. 2. Performance on questions requiring unknown knowledge: Both models collapsed on unseen temporal data: R1 dropped to 7.8% (EM) and 9.8% (LLM), and R1-Distill-Qwen-7B dropped to 2.6% and 5.2% (LLM). The drastic performance drop highlights fundamental knowledge cutoff dependence. As noted in Section 4.3, R1s superior performance stems from memorizing outdated wiki factswhen tested on temporal questions requiring very recent knowledge, its accuracy declines significantly, underscoring the necessity of augmenting internal knowledge through tool usage from external environments."
        }
    ],
    "affiliations": [
        "School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China"
    ]
}