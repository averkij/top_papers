{
    "paper_title": "Better Embeddings with Coupled Adam",
    "authors": [
        "Felix Stollenwerk",
        "Tobias Stollenwerk"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets."
        },
        {
            "title": "Start",
            "content": "Felix Stollenwerk* AI Sweden Tobias Stollenwerk Forschungszentrum Jülich 5 2 0 2 3 1 ] . [ 2 1 4 4 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is cause of anisotropic embeddings, and suggest modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets."
        },
        {
            "title": "Introduction",
            "content": "Anisotropic Embeddings Large Language Models (LLMs) take sequence of tokens as input and predict the next token. An embedding matrix is used to map the input tokens to the hidden space of the model, while an unembedding matrix provides the inverse mapping to the output token space. Although the two matrices can in principle be different, it is common practice to apply weight tying (Press and Wolf, 2017) and use the transpose of the embedding matrix for unembedding. During training, the model learns an embedding vector in hidden space for each token in the vocabulary. However, it is observed that those embedding vectors are clustered in small subspace away from the origin (Gao et al., 2019). This anisotropy limits the semantic usefulness of the embeddings and, in turn, the expressiveness and generalizability of the model. Multiple attempts have been made to both explain the root cause of the problem and alleviate it (more on this in Sec. 7). In particular, Bis et al. (2021) have shown that the problem can be traced back to mere shift of the mean embedding vector away from the origin. With the mean embedding vector as reference point, the embeddings feature *Corresponding author: felix.stollenwerk@ai.se near-perfect isotropy. However, the role of the employed optimization algorithm has, to the best of our knowledge, not yet been investigated. Optimization Algorithms Optimization algorithms are an indispensable ingredient in the training of neural networks generally and LLMs in particular. While SGD is the foundational optimization technique, Adam (Kingma and Ba, 2014) is the most widely used optimization techniques for LLMs due to its superior performance and robustness. While it provides multiple conceptional advantages over SGD, see e.g. Ruder (2017) for detailed discussion, the one that is particularly striking with regard to word embeddings is that Adam is well-suited for sparse data. More concretely, this means that using Adam, the embedding update vectors for rare words are scaled up in comparison to those of more frequent words. This is relevant in the context of LLMs as word frequencies in the training data are typically very skewed and may differ by several orders of magnitude. Formally, this is captured by the unigram probability distribution (cid:101)p [0, 1]V , which for given dataset and tokenizer is defined by (cid:101)pi (cid:101)pi(d, t) = ni nj (cid:80) , (1) where {1, . . . , } is the vocabulary index and ni is the total number of occurrences of the i-th token in the tokenized dataset. visualization of an example unigram probability distribution can be found in App. A. Our Contributions In this work, we combine the research areas of anisotropic embeddings and optimization algorithms and provide the following contributions: We show that the Adam optimizer plays crucial role in causing anisotropic embeddings. 1 We suggest Coupled Adam, an easy-toimplement yet efficient adjustment of the original Adam optimization algorithm, which is specifically designed for embedding parameters in order to alleviate the anisotropy problem. We demonstrate that our method not only significantly improves the quality of word embeddings, but also has beneficial effect on upstream and downstream performance for sufficiently large datasets."
        },
        {
            "title": "Embeddings",
            "content": "We study the collective shift of the embeddings (that underlies the anisotropy problem), by analyzing their vector updates based on the optimization algorithms SGD and Adam. Weight tying is assumed, but only contributions from the output layer are considered, following Bis et al. (2021). Our results apply to all model architectures with standard language modeling head."
        },
        {
            "title": "The equations for the standard language modeling\nhead read",
            "content": "L = log (pt) exp (lt) j=1 exp (lj) (cid:80)V pt = li = ei , (2) (3) (4) where R0 is the loss for next token prediction, and pt [0, 1] is the predicted probability of the li and ei RH denote true token V. the logits and embeddings for each token V, respectively. RH is the final hidden state provided by the model for single token. Note that the operation in Eq. (4) is the dot product of two vectors in RH . Backward propagation yields the following gradients with respect to the input vectors ei and of Eq. (4): gi := ei = (δit pi) (5) This result was first reported using different notation in Bis et al. (2021), and is rederived in App. for the readers convenience. 2 Figure 1: Toy example of hidden state vector (shown in blue) and three embedding vectors ei (shown in red) in = 2 dimensions. The gray vectors represent the embedding update vectors, for the SGD (dark) and the Adam (light) optimizer. The update vector of the true token is aligned with h, while the others point in the opposite direction, see Eq. (5). Note that the sum of embedding update vectors vanishes for SGD, while this is not necessarily the case for Adam, cf. Eqs. (11) and (16)."
        },
        {
            "title": "2.2 Vanishing Sum of Embedding Gradients",
            "content": "Optimization algorithms for neural networks usually update the model parameters iteratively, using an additive update vector that points in direction opposite to the gradient of the loss with respect to the parameters. In the case of embedding vectors, this can be expressed by = e(τ 1) e(τ ) + u(τ ) , (6) with , (7) g(τ ) u(τ ) is the update vector for e(τ ) where u(τ ) at time step τ . Eq. (5) implies that the embedding vector et of the true token is updated in direction +h, while the update vectors ui for all the other embedding vectors ei with = are proportional to h, see Fig. 1. This circumstance is referred to in the literature as the \"common enemy effect\" (Bis et al., 2021), and regarded as the cause of the representation degeneration problem. However, as we will see in the following sections, this explanation is incomplete, as it does not take into account the scaling of the gradients with the predicted probabilities pi, see Eq. (5). The basis for our argumentation is the observation that the sum of embedding gradients vanishes, as the following simple calculation shows: (cid:88) i=1 g(τ ) (5) = (cid:88) (cid:16) it p(τ ) δ(τ ) (cid:17) h(τ ) i=1 (cid:32) = 1 (cid:33) p(τ ) (cid:88) i= h(τ ) = 0 (8) Next, we will study how Eq. (8) translates to the sum (cid:80)V of embedding update vectors, as well as the mean embedding vector i=1 u(τ ) µ(τ ) ="
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) i=1 e(τ ) (9) Since the exact definition of the embedding update vector ui, i.e. the proportionality factor in Eq. (7), depends on the optimization algorithm, we discuss SGD and Adam separately. 2."
        },
        {
            "title": "Invariant Mean Embedding with SGD",
            "content": "We consider the application of the SGD optimization algorithm on the embedding vectors1. At each training step, an embedding vector is simply updated by adding the associated negative gradient gi, multiplied by global learning rate η. Hence, Eq. (7) becomes = η g(τ ) u(τ ) (10) (initial embeddings), L(ei) (objective), 0 (2nd moment) Input: η (lr), e(0) β1, β2 (betas), (number of time steps) Initialize: m(0) 0 (1st moment), v(0) Output: e(T ) (final embeddings) 1: for τ = 1 . . . do 2: 3: 4: for = 1 . . . do ) i /(cid:0)1 βτ 1 (cid:1) /(cid:0)1 βτ g(τ ) ei L(τ )(e(τ 1) m(τ ) β1m(τ 1) v(τ ) β2v(τ 1) (cid:98)m(τ ) m(τ ) (cid:98)v(τ ) v(τ ) if coupled then (cid:80)V (cid:98)ν(τ ) 1 for = 1 . . . do if coupled then (cid:98)v(τ ) (cid:98)ν(τ ) e(τ ) e(τ 1) i=1 (cid:98)v(τ ) η (cid:113) 2 (τ ) (cid:98)m (τ ) +ϵ (cid:98)v 5: 6: 7: 8: 9: 10: 11: 12: 13: + (1 β1)g(τ ) (cid:16) g(τ ) + (1 β2) (cid:1) (cid:17)2 14: return e(T ) Algorithm 1: Pseudocode for the Adam algorithm and our extension, the Coupled Adam algorithm (highlighted), applied to the embedding vectors ei. Note that weight decay is not applied. Together with Eq. (8), this implies that the sum of embedding update vectors vanishes at any time step τ : is given by = η(τ ) u(τ ) (cid:98)m(τ ) , (13) (cid:88) i=1 u(τ ) (10) = η (cid:88) i=1 g(τ ) (8) = 0 (11) Consequently, the mean embedding vector will stay invariant during the training process: µ(τ ) µ(τ 1) (9,6) ="
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) i=1 u(τ ) (11) = 0 (12) This holds even though the different embeddings ei will be individually updated in different directions with different magnitudes. Moreover, all of the above is true also in the case of SGD with momentum, which follows from linearity and mathematical induction. Eq. (12) has far-reaching implications with regard to the anisotropy problem. It entails that the embedding vectors do not collectively shift away from the origin if SGD (with or without momentum) is used."
        },
        {
            "title": "2.4 Shifted Mean Embedding with Adam",
            "content": "In this section, we analyze the behavior of the mean embedding during optimization with Adam (Kingma and Ba, 2014), see Algorithm 1. The update vector Eq. (7) for the Adam algorithm 1Details are given in App. C. where we have introduced an i-dependent effective learning rate η(τ ) := (cid:113) η (cid:98)v(τ ) + ϵ (14) and (cid:98)v(τ ) Note that (cid:98)m(τ ) denote the exponentially averaged first and second moments, respectively, defined according to lines 4-7 in Algorithm 1. The i-dependent learning rate serves the purpose of individually normalizing the update vectors for different parameters in the Adam optimizer. However, it also has an unwanted effect specifically on the embedding vectors. While we know from Eq. (8) and Algorithm 1 (lines 4,6) that the unweighted sum over the first moments vanishes, (cid:80)V = 0, this is not true for the weighted sum, i=1 (cid:98)m(τ ) (cid:88) i=1 (cid:98)m(τ ) η(τ ) = 0 , (15) = η(τ ) unless η(τ ) for all i, V. Hence, the sum of embedding update vectors does not vanish in general, (cid:88) i= u(τ ) (13) = (cid:88) i=1 η(τ ) (cid:98)m(τ ) (15) = 0 (16) 3 This, in turn, causes the mean embedding to change during training, The first factor introduces global scale to all update vectors simultaneously: µ(τ ) µ(τ 1) (9,6) ="
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) i=1 (16) = 0 , u(τ ) (17) which is in stark contrast to the case of SGD (cf. Eq. (12)). We have thus identified that an i-dependency of the second moment (cid:98)v(τ ) of the Adam optimizer leads to the observed collective shift of the embedding vectors away from the origin. Next, we will show that the second moment indeed depends on i. More concretely, we will argue that its expectation value is proportional to the unigram probabilitity2 (see Eq. (1)), [(cid:98)vi] (cid:101)pi (18) In App. D.1, Eq. (18) is derived using minimal assumptions and experimental input. Here, we restrict ourselves to confirming the relationship in purely experimental manner. [(cid:98)vi] is estimated directly by measuring (cid:98)vi multiple times during training, using different models. We then perform linear fits of [(cid:98)vi] as function of (cid:101)pi. Indeed, the fits yield high coefficient of determination, on average R2 = 0.85(7), and proportionality constant of [(cid:98)vi] (cid:101)pi Details about the exact procedure and plots showing the data and linear fits can be found in App. D.2. 10 := (19)"
        },
        {
            "title": "3 Coupled Adam",
            "content": "In the previous section, we have identified the individual scales of the second moments vi for different embedding vectors ei as the root cause of the anisotropy problem. This implies that solution to the problem is to enforce that the second moments are the same for every i. The question arises whether and how this can be done in the best way, without harming the performance of the model. To answer this, we note that the normalization of the embedding update vector by the Adam second moment can be split into two parts: [(cid:98)vi] (19) = (cid:101)pi ="
        },
        {
            "title": "A\nV",
            "content": "((cid:101)piV ) (20) 2Note that from here until Eq. (23), the time index (τ ) is dropped for the sake of readability. 4 (19)"
        },
        {
            "title": "A\nV",
            "content": "104 5 104 = 2 109 , (21) where the numbers correspond to our experiments from the previous section with 50000. The second factor scales the update vectors individually. It is one on average:"
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) i=1 ((cid:101)piV ) = 1 (22) Our goal is to retain the first, global factor and get rid of the second, individual factor. The canonical way to do this is to simply take the average of the second moment over the vocabulary items i:"
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) i=1 [(cid:98)vi] (20,22) ="
        },
        {
            "title": "A\nV",
            "content": "(23) In practice, the exponentially averaged second moments (cid:98)v(τ ) as they appear in Eq. (14) are replaced by their average: (cid:98)ν(τ ) :="
        },
        {
            "title": "1\nV",
            "content": "V (cid:88) i=1 (cid:98)v(τ ) (24) We call the resulting algorithm Coupled Adam, as it couples the second moments of the embedding vectors via Eq. (24). It is displayed in Algorithm 1. Evidently, with Coupled Adam, the effective learning rate in Eq. (14) that enters the update vector in Eq. (13) becomes independent of i. Hence, like SGD but unlike standard Adam, the sum of embedding updates vanishes. However, like standard Adam but unlike SGD, Coupled Adam uses second moment to normalize the embedding update vectors."
        },
        {
            "title": "4 Experiments",
            "content": "Two types of experiments are conducted to study the impact of coupling the second moments of the embedding update vectors. First, set of small-scale experiments (Sec. 4.1) with models and datasets of varying sizes up to 1B parameters and 20B tokens, respectively. Afterwards, we perform few large-scale experiments (Sec. 4.2) to verify that the usefulness of our method extrapolates to the realm of large language models with more than 1B parameters trained on at least the corresponding compute-optimal (Hoffmann et al., 2022) amount of data. In order to verify the generalizability of our method, the smalland large-scale experiments involve different datasets, training frameworks and dense transformer model architectures. An overview of the model and dataset sizes employed in our experiments is given in App. E.1. For each combination, two models are trained: one using standard Adam and one using Coupled Adam for the embeddings, see Eq. (24). Both variants use standard Adam for all non-embedding parameters. The various metrics we employ to assess both the general model performance and the quality of the model embeddings will be discussed in Sec. 4.3."
        },
        {
            "title": "4.1 Small-scale Experiments",
            "content": "Our small-scale experiments use the OpenWebText Corpus (Gokaslan and Cohen, 2019) and the GPT2 tokenizer (Radford et al., 2019). The model architecture also follows GPT-2, while the hyperparameter setup is taken from GPT-3 (Brown et al., 2020), see App. E.2 for further details. An implementation based on nanoGPT (Karpathy, 2022) is used. We define grid (D, ) with dataset sizes {5B, 10B, 20B} and model sizes {125M, 355M, 760M}, and repeat each experiment = 3 times with different seeds in order to estimate uncertainties and assess statistical significance."
        },
        {
            "title": "4.2 Large-scale Experiments",
            "content": "For our large-scale experiments, we use the SlimPajama dataset (Soboleva et al., 2023) and the GPT2 tokenizer. state-of-the-art dense transformer model architecture akin to (Touvron et al., 2023) is chosen, including e.g. RoPE embeddings (Su et al., 2023) and the SwiGLU activation function (Shazeer, 2020). Details can be found in App. E.2. The experiments are conducted using Modalities (Lübbering et al., 2024) as the training framework. We consider two model sizes, 1.3B and 2.6B. In order to cover the two common scenarios of computeoptimal training and overtraining, we conduct two sets of experiments: Firstly, we use near computeoptimal dataset sizes, 26B and 52B tokens, respectively. Secondly, we increase the number of tokens by factor 4, resulting in 105B and 210B tokens, respectively. Each large-scale experiment is performed = 1 times."
        },
        {
            "title": "4.3 Evaluation",
            "content": "ated using the Language Model Evaluation Harness (Gao et al., 2023) on the following tasks: ARC easy and challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), RACE (Lai et al., 2017), TruthfulQA (Lin et al., 2022) and WinoGrande (Sakaguchi et al., 2020). More concretely, the considered metric is the average accuracy, which we will denote by Acc. To assess the quality of the embeddings, we first compute their isotropy, defined as (Arora et al., 2016; Mu et al., 2018) Iso(E) := mincX Z(c) maxcX Z(c) , (25) where RHV is the embedding matrix, Z(c) = (cid:80)V i=1 exp(cT ei) is the partition function and = {c} is the set of eigenvectors RH of EET RHH . Secondly, the 2-norm µ of the mean embedding, see Eq. (9), and the average 2-norm of the embeddings ei = 1 i=1 ei as well as their ratio (cid:80)V µr := µ/ei (26) are determined. In addition, we evaluate the models on embedding benchmarks for word similarity and relatedness, to assess how well they represent semantic meaning. Following Bis et al. (2021), we consider the benchmarks SimLex999 (Hill et al., 2015), MEN (Bruni et al., 2014), WordSim353 (Finkelstein et al., 2001) and Stanford Rare Words (Luong et al., 2013). Each dataset provides pairs of words labeled with ground truth score that represents the words semantic similarity. We derive model scores from the cosine similarity of the corresponding embedding vectors, and report the Pearson correlation of the two scores averaged over the datasets, which we denote by r. Finally, some additional important properties of the embedding matrix are investigated. We study the correlation between the length of an embedding vector and the unigram probability, ρ := 100 corr(cid:0)(ei)V i=1, (cid:101)p(cid:1) , (27) to measure how well the former represents the latter. Furthermore, the condition number κ, defined as the ratio of the smallest and largest singular values of the embedding matrix, is determined in percent: κ := 100 mini Σii maxi Σii (28) Upstream performance is measured in terms of test loss, while downstream performance is evaluHere, = ΣV denotes the singular value decomposition of the embedding matrix. 5 Adam () Acc () Iso () µ () µr () () ρ () κ () 5B 10B 20B 125M 355M 760M 125M 355M 760M 125M 355M 760M Standard Coupled Standard Coupled Standard Coupled Standard Coupled Standard Coupled Standard Coupled Standard Coupled Standard Coupled Standard Coupled 3.14 (0) 3.12 (1) 2.95 (0) 2.93 (0) 2.85 (0) 2.86 (0) 3.07 (0) 3.03 (0) 2.86 (0) 2.83 (0) 2.75 (0) 2.74 (1) 3.03 (0) 2.97 (0) 2.79 (0) 2.75 (0) 2.68 (1) 2.65 (0) 0.340 (2) 0.339 (2) 0.352 (3) 0.350 (4) 0.360 (3) 0.357 (3) 0.343 (3) 0.343 (1) 0.359 (2) 0.365 (2) 0.375 (2) 0.372 (3) 0.346 (1) 0.350 (1) 0.366 (4) 0.372 (6) 0.385 (3) 0.392 (2) 0.31 (2) 0.94 (1) 0.44 (2) 0.98 (0) 0.43 (1) 0.97 (0) 0.21 (3) 0.91 (1) 0.35 (2) 0.96 (0) 0.38 (3) 0.96 (0) 0.10 (3) 0.83 (1) 0.25 (2) 0.95 (2) 0.28 (2) 0.94 (2) 1.10 (6) 0.02 (0) 0.81 (2) 0.01 (0) 0.84 (1) 0.01 (0) 1.58 (5) 0.05 (0) 1.01 (4) 0.02 (0) 0.97 (4) 0.02 (0) 2.14 (7) 0.11 (0) 1.32 (8) 0.04 (0) 1.21 (8) 0.03 (0) 0.67 (3) 0.01 (0) 0.67 (1) 0.01 (0) 0.63 (0) 0.01 (0) 0.75 (0) 0.02 (0) 0.74 (2) 0.01 (0) 0.66 (2) 0.01 (0) 0.82 (2) 0.03 (0) 0.82 (2) 0.02 (0) 0.73 (3) 0.01 (0) 15 (3) 55 (0) 16 (2) 56 (1) 14 (3) 55 (1) 9 (2) 57 (2) 10 (3) 57 (1) 11 (2) 57 (0) 5 (1) 57 (2) 5 (2) 57 (1) 3 (4) 58 (0) -54 (3) 87 (1) -47 (0) 86 (0) -49 (2) 85 (1) -64 (5) 82 (0) -55 (3) 83 (1) -56 (1) 84 (0) -66 (2) 77 (0) -65 (4) 78 (0) -64 (2) 81 (0) 0.6 (1) 4.8 (2) 0.8 (0) 6.8 (4) 0.7 (0) 6.9 (3) 0.4 (0) 3.6 (8) 0.5 (0) 5.3 (2) 0.5 (0) 6.2 (0) 0.3 (0) 1.7 (5) 0.3 (0) 4.1 (3) 0.3 (0) 4.4 (2) Table 1: Results of our small-scale experiments. and denote the dataset and model size, respectively. is the test loss, and the column Acc represents the accuracy averaged over the downstream tasks listed in Sec. 4.3. The other evaluation metrics are defined in the same section, see Eqs. (25)-(28). The arrow in parentheses indicates whether higher or lower value is desirable. Every training was conducted = 3 times with different seeds, and the numbers represent the (rounded) averages and standard deviations in the following shorthand notation format: 0.123 (4) 0.123 0.004. For each combination (D, ) and each metric, the respective better value is highlighted in bold if the (unrounded) difference is significant according to Students t-test with one-sided confidence level of α = 95% (see App. for details). Plots for and Acc are shown in App. G.1."
        },
        {
            "title": "5.1 Small-scale Experiments",
            "content": "the small-scale experiments The results of (Sec. 4.1) are shown in Tab. 1. We find that both upstream and downstream performance are better with Coupled Adam if the dataset size is sufficiently large. In fact, the improvement appears to increase monotonically with the dataset size D. In addition, the embedding-specific metrics benefit greatly from Coupled Adam. In particular, the isotropy reaches values above 0.90 (with single exception), while and κ are hugely improved as well. The mean embedding is evidently close to the origin. Finally, Coupled Adam leads to significantly stronger (positive) correlation ρ between the length of an embedding vector and its associated unigram probability."
        },
        {
            "title": "5.2 Large-scale Experiments",
            "content": "The results of the large-scale experiments (Sec. 4.2) are shown in Tab. 2. We observe very similar patterns as for the small-scale experiments. Although upstream and downstream performance are worse with Coupled Adam for compute-optimal dataset sizes, they are better if 4 times larger datasets are used. Note that for the small-scale experiments, the upstream and downstream performance were found to be better already for compute-optimal dataset sizes. We attribute this to the fact that the batch size for the large-scale experiments is five times larger (cf. App. E.2), which results in fewer optimization steps for the same dataset size. Regarding the embedding-specific metrics, we again find significant and consistent improvements throughout all experiments. However, we do observe certain shift of the mean embedding vector away from the origin, even if Coupled Adam is used. The shift becomes more pronounced as the model and dataset sizes increase, and is also reflected in reduced isotropy. As we shall see in the following section, it comes along with optimal model performance though. An obvious hypothesis in light of our analysis in Sec. 2 is that the residual shift of the mean embeddings is due to weight tying. This is supported by the results of Machina and Mercer (2024), who find improved isotropy for models without weight tying. We leave it for future work to verify the hypothesis."
        },
        {
            "title": "6 Ablations",
            "content": "We perform some additional experiments to shed further light on how Coupled Adam works. model size of = 125M and the dataset sizes {5B, 10B, 20B} from the small-scale experiments (Sec. 4.1) are used, and each experiment is repeated = 3 times with different seeds. 6 Adam () Acc () Iso () µ () µr () () ρ () κ () 26B 1.3B 52B 2.6B 105B 1.3B 210B 2.6B Standard Coupled Standard Coupled Standard Coupled Standard Coupled 2.433 2.448 2.257 2.273 2.278 2.277 2.131 2.129 0.402 0. 0.451 0.441 0.446 0.447 0.490 0.492 0.50 0.96 0.51 0.86 0.40 0. 0.34 0.65 0.67 0.05 0.68 0.10 0.87 0.23 0.96 0.49 0.45 0. 0.40 0.06 0.43 0.09 0.41 0.14 53 66 55 66 52 54 67 -41 74 -44 74 -52 71 -52 69 1.2 4. 1.0 3.0 0.6 2.2 0.5 1.5 Table 2: Results of our large-scale experiments. See the caption of Tab. 1 for an explanation of the column names. For each combination (D, ) and each metric, the respective better value is highlighted in bold."
        },
        {
            "title": "6.1 Scaled Coupled Adam",
            "content": "While coupling the second moment of the embedding gradients using the average in Eq. (24) is the canonical choice, one could also use multiple of the average. We conduct additional experiments where the coupled second moment is scaled by powers of 2: (29) (cid:98)ν(τ ) 2n (cid:98)ν(τ ) , with scaling exponents {z 5 5}. Note that using scaling exponent = 0 is equivalent to using different effective learning rate for the embeddings than for all the other parameters, via Eqs. (24) and (14). In particular, smaller scaling exponent corresponds to smaller effective learning rate and vice versa. The results for = 20B are shown in Tab. 3, and the dependency of the loss on the scaling exponent for that very dataset size is visualized in Fig. 2. Results for other dataset sizes and plots for the other evaluation metrics can be found in App. G.2. Our data shows that the loss reaches minimum close to = 0, with rather weak dependence on the scaling exponent in its vicinity. Nevertheless, for the smallest and largest scaling exponents studied, we find that the loss gets significantly worse. Regarding downstream performance, we see indications of similar pattern, although the statistical uncertainties are too large to draw definite conclusions. The semantic usefulness of the embedding vectors as measured by seems to suffer from scaling exponent < 0. For the isotropy and the mean embedding, we observe the opposite behavior. They benefit from smaller scaling exponent and the associated smaller embedding updates, with the effect being more pronounced the larger the training dataset size D. However, this also negatively affects the model performance. Hence, we conclude that, at least within the range of our experiments, the optimal setting is to have the same learning rate for the embedding parameters as for all the other model parameters, as implied by = 0 and Eq. (24)."
        },
        {
            "title": "6.2 SGD",
            "content": "We train several models using SGD with momentum γ = 0.9 as the optimizer for the embeddings. Since Adam via the inverse square root of its second moment effectively scales the learning rate up by factor comprising orders of magnitude (see Eq. (21)), we explicitly multiply the learning rate in SGD by factor of comparable size3. hyperparameter search using {100, 200, 300, 400, 500, 600} is performed to search for the optimum with respect to upstream performance (loss), see App. G.3 for details. It is found at = 300 for {5B, 10B} and = 400 for = 20B. The respective optimal model is compared to its counterpart trained with Coupled Adam in Tab. 4. The results show that, although SGD is advantageous with respect to isotropy, the mean embedding shift and the condition number, Coupled Adam consistently achieves better results on all upstream and downstream task metrics, while having one less hyperparameter to fine-tune."
        },
        {
            "title": "7 Related Work",
            "content": "Gao et al. (2019) first described the anisotropy issue, which they referred to as representation degeneration problem, and suggested cosine regularization as mitigation strategy. Alternative techniques to address the problem have been developed, including adversarial noise (Wang et al., 2019), spectrum control (Wang et al., 2020) and Laplacian regularization (Zhang et al., 2020). Bis et al. (2021) have shown that the anisotropy of embeddings can for the most part be traced back to common shift of the embeddings in dominant direction. They called this phenomenon common enemy effect, and 3Note that the difference between momentum in SGD and the first moment in Adam also plays role here. 7 -5 -4 -3 -2 - 0 1 2 3 4 5 () Acc () Iso () µ () µr () () ρ () κ () 2.99 (0) 2.99 (0) 2.98 (0) 2.98 (0) 2.98 (0) 0.349 (2) 0.348 (5) 0.352 (5) 0.352 (1) 0.348 (3) 0.97 (0) 0.97 (1) 0.95 (1) 0.94 (2) 0.87 (2) 0.01 (0) 0.02 (0) 0.03 (0) 0.04 (0) 0.07 (0) 0.01 (0) 0.01 (0) 0.02 (0) 0.02 (0) 0.02 (0) 55 (1) 55 (1) 57 (2) 57 (1) 57 (1) 76 (3) 78 (3) 78 (2) 78 (2) 79 (2) 6.0 (6) 4.4 (1) 2.7 (5) 2.3 (4) 2.1 (3) 2.97 (0) 0.350 (1) 0.83 (1) 0.11 (0) 0.03 (0) 57 (2) 77 (0) 1.7 (5) 2.97 (0) 2.98 (0) 2.97 (0) 2.97 (1) 2.98 (0) 0.351 (4) 0.353 (2) 0.352 (0) 0.352 (1) 0.349 (4) 0.66 (6) 0.47 (6) 0.27 (4) 0.09 (0) 0.01 (1) 0.20 (1) 0.34 (1) 0.54 (2) 0.83 (2) 1.32 (2) 0.03 (0) 0.04 (0) 0.05 (0) 0.05 (0) 0.06 (0) 56 (0) 58 (1) 58 (2) 58 (1) 57 (1) 78 (2) 78 (2) 78 (1) 77 (1) 75 (1) 1.6 (5) 1.6 (9) 2.0 (7) 2.3 (2) 2.1 (6) Table 3: Results of our experiments with Scaled Coupled Adam, for = 125M and = 20B. Values are highlighted in bold if they are significantly better than all the other values in the same column, see the caption of Tab. 1 for more details. Figure 2: Dependency of the loss on the scaling exponent n, see Eq. (29), for = 125M and = 20B. The plot shows the difference to the loss obtained for = 0. 5B 125M 10B 125M 20B 125M Optimizer () Acc () Iso () µ () µr () () ρ () κ () SGD (300) Coupled Adam SGD (300) Coupled Adam SGD (400) Coupled Adam 3.17 (0) 3.12 (1) 3.07 (1) 3.03 (0) 3.00 (0) 2.97 (0) 0.333 (3) 0.339 (2) 0.341 (4) 0.343 (1) 0.346 (5) 0.350 (1) 0.99 (0) 0.94 (1) 0.99 (0) 0.91 (1) 0.98 (1) 0.83 (1) 0.00 (0) 0.02 (0) 0.01 (0) 0.05 (0) 0.01 (0) 0.11 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.02 (0) 0.02 (0) 0.03 (0) 45 (1) 55 (0) 49 (1) 57 (2) 54 (0) 57 (2) 71 (1) 87 (1) 71 (4) 82 (0) 76 (5) 77 (0) 15.5 (4) 4.8 (2) 12.2 (2) 3.6 (8) 7.4 (1.1) 1.7 (5) Table 4: Comparison of models whose embeddings were trained with SGD and Coupled Adam. The SGD models were obtained after hyperparameter search for the learning rate. The associated factor is specified in parentheses in the Optimizer column. Bold values indicate better results with statistical significance, see the caption of Tab. 1 for more details. provided semi-quantitative explanation (Eq. (5)), which we developed further in the present work by including the optimizer in the analysis. In Yu et al. (2022), Adaptive Gradient Gating is proposed, based on the empirical observation that it is the gradients for embeddings of rare tokens that cause anisotropy. Our analysis conforms to this finding and attributes it to massive up-scaling of the gradients for rare embeddings with Adam, cf. Fig. 1. Machina and Mercer (2024) have demonstrated that large Pythia models (Biderman et al., 2023) show improved isotropy compared to similar models, and attribute this to the absence of weight tying. This is in accordance with our analysis of the unembedding gradients in conjunction with Adam, Sec. 2. While all the previously mentioned papers use average cosine similarity (Ethayarajh, 2019) or Iso from Eq. (25) to quantify the geometry of embedding vectors, Rudman et al. (2022) deviate from this. Their notion of isotropy is based solely on the embeddings covariance matrix and embodied by the metric IsoScore. In particular, IsoScore is mean-agnostic, while Iso strongly correlates with the mean embedding (see e.g. Tab. 1). Finally, concurrent to our work, Zhao et al. (2024) have investigated the importance of using the second moment in Adam with regard to performance and stability. They found that simplified variants of Adam that use the same effective learning rate either for the whole embedding matrix (Adalayer) or each embedding vector (Adalayer*) are slightly worse than Adam but better than SGD. Adalayer* is similar to Coupled Adam, but corresponds to the second moment averaged over hidden space instead of vocabulary space."
        },
        {
            "title": "8 Conclusions",
            "content": "Our work addresses the well-known anisotropy problem for LLM embeddings. We have advanced the theoretical understanding of the phenomenon by showing that it is combination of the common enemy effect and the individual second moments in Adam that causes collective shift of the embedding vectors away from the origin. To mitigate the problem, we have introduced Coupled Adam, which enforces the same effective learning rate for every embedding vector, and thus suppresses the collective shift of the embeddings. We have found that Coupled Adam consistently improves embedding-specific metrics across all experiments, while also achieving better downstream and upstream performance for large datasets, as they are typically used in LLM training. The code to reproduce our results is available at github.com/flxst/coupled-adam ."
        },
        {
            "title": "9 Limitations",
            "content": "Although our method is generally applicable to all common LLM architectures, as they share the same language modeling head and embeddings, only dense decoders were used in our experiments. In addition, only models with up to = 2.6B parameters have been tested. The cosine decay learning rate schedule was applied throughout all experiments (App. E.2). Alternatives such as an infinite learning rate schedule are not incorporated in our study. Furthermore, as mentioned at the end of Sec. 5, we have not explicitly verified that the slight residual shift of the mean embedding, which is observed even for Coupled Adam, is caused by weight tying. Finally, we have used straightforward implementation of Coupled Adam, closely following Algorithm 1. More sophisticated implementations might lead to increased efficiency and further improvements; we leave it for future work to investigate this."
        },
        {
            "title": "Acknowledgements",
            "content": "Our computational experiments used around 20000 GPU hours. They were partly run on the EuroHPC supercomputers MeluXina and MareNostrum5 in conjunction with the grants EHPC-DEV-2023D10032 and EHPC-EXT-2023E02-038."
        },
        {
            "title": "References",
            "content": "Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2016. latent variable model approach to PMI-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385399. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 23972430. PMLR. Daniel Bis, Maksim Podkorytov, and Xiuwen Liu. 2021. Too much in common: Shifting of embeddings in transformer language models and its implications. In North American Chapter of the Association for Computational Linguistics (NAACL). Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. J. Artif. Int. Res., 49(1):147. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Kawin Ethayarajh. 2019. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5565, Hong Kong, China. Association for Computational Linguistics. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. volume 20, pages 406414. Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and TieYan Liu. 2019. Representation degeneration problem in training natural language generation models. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. framework for few-shot language model evaluation. Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus. Felix Hill, Roi Reichart, and Anna Korhonen. 2015. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665695. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models. 9 Andrej Karpathy. 2022. NanoGPT. https://github. com/karpathy/nanoGPT. Diederik Kingma and Jimmy Ba. 2014. Adam: International method for stochastic optimization. Conference on Learning Representations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785 794, Copenhagen, Denmark. Association for Computational Linguistics. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland. Association for Computational Linguistics. Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104113, Sofia, Bulgaria. Association for Computational Linguistics. Max Lübbering, Mehdi Ali, Felix Stollenwerk, Michael Fromm, Alexander Arno Weber, and Richard Rutmann. 2024. Modalities: pytorch-native framework for distributed and reproducible founhttps://github.com/ dation model Modalities/modalities. training. Anemily Machina and Robert Mercer. 2024. Anisotropy is not inherent to transformers. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 48924907, Mexico City, Mexico. Association for Computational Linguistics. Jiaqi Mu, Suma Bhat, and Pramod Viswanath. 2018. All-but-the-top: Simple and effective postprocessing for word representations. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany. Association for Computational Linguistics. Ofir Press and Lior Wolf. 2017. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 157163, Valencia, Spain. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Sebastian Ruder. 2017. An overview of gradient descent optimization algorithms. William Rudman, Nate Gillman, Taylor Rayne, and Carsten Eickhoff. 2022. IsoScore: Measuring the uniformity of embedding space utilization. In Findings of the Association for Computational Linguistics: ACL 2022, pages 33253339, Dublin, Ireland. Association for Computational Linguistics. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):87328740. Noam Shazeer. 2020. Glu variants improve transformer. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Dilin Wang, Chengyue Gong, and Qiang Liu. 2019. Improving neural language modeling via adversarial training. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6555 6565. PMLR. Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. 2020. Improving neural language generation with spectrum control. In International Conference on Learning Representations. 10 Sangwon Yu, Jongyoon Song, Heeseung Kim, Seongmin Lee, Woo-Jong Ryu, and Sungroh Yoon. 2022. Rare tokens degenerate all tokens: Improving neural text generation via adaptive gradient gating for rare token embeddings. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2945, Dublin, Ireland. Association for Computational Linguistics. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Zhong Zhang, Chongming Gao, Cong Xu, Rui Miao, Qinli Yang, and Junming Shao. 2020. Revisiting representation degeneration problem in language modeling. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 518527, Online. Association for Computational Linguistics. Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham Kakade. 2024. Deconstructing what makes good optimizer for language models."
        },
        {
            "title": "A Unigram Probability Distribution",
            "content": "Fig. 3 shows the unigram probability distribution for the example of the OpenWebText Corpus dataset and the GPT-2 tokenizer. Figure 3: Logarithm log( (cid:101)pi) of the unigram probability distribution for the OpenWebText Corpus and the GPT-2 tokenizer. The maximum probability is maxi (cid:101)pi 0.037 or maxi log( (cid:101)pi) 3.30. The minimum probability (not shown) is mini (cid:101)pi = 0 or mini log( (cid:101)pi) = ."
        },
        {
            "title": "B Embedding Gradients",
            "content": "We explicitly derive Eq. (5), which we recall here for convenience: gi := ei"
        },
        {
            "title": "The chain rule yields",
            "content": "= (δit pi) (5) ei = (cid:88) k= pt pt lk lk ei , (30) where the individual factors can directly be obtained from Eqs. (2)-(4): = 1 pt = δkt exp (lt) Σ exp (lt) exp (lk) Σ = δktpt ptpk = pt(δkt pk) = δkih (31) (32) (33) pt pt lk lk ei Note that in the first line of Eq. (32), we use the abbreviation Σ = Inserting Eqs. (31), (32) and (33) into Eq. (30) directly leads to Eq. (5): (cid:17) j=1 exp (lj) (cid:16) (cid:80)V . ei = = (cid:88) k=1 (cid:88) k=1 1 pt pt(δkt pk) δkih (δkt pk) δkih = (δit pi) h"
        },
        {
            "title": "C SGD Algorithm",
            "content": "For completeness and comparison to (Coupled) Adam as displayed in Algorithm 1, we summarize the SGD algorithm in Algorithm 2. (initial embeddings), L(ei) (objective), γ for = 1 . . . do Input: η (lr), e(0) (momentum), (number of time steps) Output: e(T ) (final embeddings) 1: for τ = 1 . . . do 2: 3: 4: 5: 6: g(τ ) b(τ ) 7: e(τ 1) e(τ ) 8: 9: return e(T ) g(τ ) ei L(τ )(e(τ 1) if > 1 then γb(τ 1) b(τ ) ηb(τ ) + g(τ ) else ) i Algorithm 2: Pseudocode for the SGD algorithm with optional momentum, applied to the embedding vectors ei."
        },
        {
            "title": "Adam",
            "content": "Using the unigram probability (cid:101)pi = p(i = t), this can also be written as In this appendix, the validity of [(cid:98)vi] (cid:101)pi (18) is verified. Due to the linearity of lines 5 and 7 in Algorithm 2, it suffices to show that the squared gradient has the property in question: (cid:2)g2 (cid:3) (cid:101)pi (34) We do this in two different ways. First, we derive Eq. (34) using semi-theoretical approach with minimal experimental input. Afterwards, we confirm the relationship in purely experimental manner. D.1 Semi-theoretical Derivation Here, we derive an expression for the expectation value of the squared gradient in terms of simple observables (Theorem 2). Subsequently, the dependency of those observables on (cid:101)pi is determined experimentally. Together, this will yield the proportionality expressed by Eq. (34). We begin our reasoning with lemma. Lemma 1 (Expectation Value Decomposition). The expectation value of the squared gradient can be decomposed into conditional expectation values as follows: (cid:2)g2 (cid:3) = (cid:101)pi (cid:2)g2 (cid:12) (cid:12) = t(cid:3) + (1 (cid:101)pi) (cid:2)g2 (cid:12) (cid:12) = t(cid:3) (35) Proof. Our starting point is the definition of the expectation value for the continuous random variable g2 : (cid:90) (cid:2)g2 (cid:3) = g2 p(gi) dgi , (36) where denotes the probability distribution of gi. Since the vocabulary item can only be either the true token or not, we can decompose into sum of joint probability distributions (using the law of total probabilities), each of which can be expressed in terms of conditional probabilities like so: p(gi) = p(gi, = t) + p(gi, = t) = p(gi = t) p(i = t) + p(gi = t) p(i = t) (37) p(gi) = (cid:101)pi p(gi = t) + (1 (cid:101)pi) p(gi = t) (38) If we insert Eq. (38) back into Eq. (36), the expectation value becomes (cid:3) = (cid:101)pi g2 p(gi = t) dgi (cid:2)g2 (cid:90) (cid:90) + (1 (cid:101)pi) g2 p(gi = t) dgi , (39) which by definition of the (conditional) expectation value, Eq. (36), is equivalent to Eq. (35). Theorem 2 (Expectation Value Squared Gradient). Given that the squared hidden state vector h2 is independent of pi and whether is the true token or not, the expectation value of the squared gradient g2 is given by (cid:3) = (cid:2)g2 + (1 (cid:101)pi) (i=t) (cid:101)pi (i=t) (cid:105) (cid:104) , i with := (cid:2)h2(cid:3) (i=t) (i=t) := (cid:2)(1 pi)2 (cid:12) (cid:12) := (cid:2)p2 (cid:12) = t(cid:3) (cid:12) = t(cid:3) (40) (41) (42) (43) Proof. We start from Lemma 1 and the square of the gradient, g2 (5) = (δit pi)2 h2 (44) Note that squared variables of vectors in RH always denote the elementwise (Hadamard) product, e.g. (cid:104) (45) 0 , (cid:2)g2 (cid:2)g2 gi gi RH g2 with strictly non-negative elements. Using Eq. (44), the expectation values on the right side of Eq. (35) can be expressed as (cid:12) (cid:12) = t(cid:3) = (cid:12) (cid:12) = t(cid:3) = (cid:2)p (1 pi)2 h2 (cid:12) h2 (cid:12) (cid:12) = t(cid:3) Given our assumptions regarding h2, its expectation value can be factored out: (cid:2)g2 (cid:2)g2 (cid:12) (cid:12) = t(cid:3) = (i=t) (cid:12) (cid:12) = t(cid:3) = (i=t) (cid:105) (cid:12) = (49) (48) (47) (46) Inserting Eqs. (48) and (49) into Eq. (35) yields Eq. (40). 12 (cid:3) , RH 0 and (cid:101)pi, (i=t) Note that Eq. (40) is vector equation, with (cid:2)g2 , (i=t) R0. It states that the expectation value of g2 factorizes into global constant that is i-independent, and factor that is i-dependent. The latter is specific combination of the unigram probability (cid:101)pi, determined by the data, and the conditional expecand (i=t) tation values (i=t) , determined by the model. Experimental Input Regarding the unigram probability, we know that 1. (cid:101)pi 1. This is the case for virtually all natural language datasets with common vocabulary size of > 10000, according to Zipfs law. The conditional expectation values (i=t) and (i=t) can be empirically estimated by applying training data to different checkpoints. We consider the three small-scale experiments of Sec. 4.1 with {125M, 355M, 760M} and = 20B, and take ten equidistant checkpoints after {2B, 4B, . . . , 20B} seen tokens for each of them. We then continue pseudo-training on 20 batches ( 2k samples or 2M tokens, see Tab. 5) of data using zero learning rate, and measure the conditional probabilities in Eqs. (42, 43) from which our target quantities can be estimated. Subsequently, linear fits of the form (i=t) (i=t) = A(i=t) (cid:101)pi = A(i=t) (cid:101)pi , (50) (51) with fit parameters A(i=t) and A(i=t) are performed. R2 is used to assess the quality of the fits. In addition, the mutual information between the response and the explanatory variable is computed. Since we observe only very weak dependence of the results for R2 and on and D, we specify the mean and standard deviation over all experiments for them. Our findings are: 2. (i=t) is independent of (cid:101)pi. The linear fits yield R2 = 0.003(1), and the mutual information is = 0.14(2). (i=t) ; (cid:101)pi (cid:17) (cid:16) 3. (i=t) is proportional to (cid:101)pi. The linear fits yield R2 = 0.92(1), and the mutual information is = 0.50(2). (i=t) ; (cid:101)pi (cid:17) (cid:16) The three empirical results above, together with Theorem 2, immediately lead to Eq. (34). D.2 Experimental Confirmation We reuse the experiments from the previous section to measure the second moment (cid:98)vi directly, in order to estimate [(cid:98)vi]. Again, linear fits of the form [(cid:98)vi] = (cid:101)pi are performed and the mutual information is computed. We find 4. [(cid:98)vi] is proportional to (cid:101)pi. (52) The linear fits yield R2 = 0.85(7), and the mutual information is (E [(cid:98)vi] ; (cid:101)pi) = 1.18(9). The results for = 125M and = = 20B are depicted in Fig. 4, as an example. Figure 4: Experimental results for [(cid:98)vi] (vertical axis) vs. (cid:101)pi (horizontal axis) for = 125M and = = 20B. The blue line shows the linear fit with R2 = 0.91. Note that while R2 and are again virtually independent of and D, the fit parameter is not. Instead, it seems to increase with D, as shown in Fig. 5. However, as stated in Eq. (19), the order Figure 5: Experimental results for the linear fit parameter as function of and D. of magnitude is 104 throughout our experiments."
        },
        {
            "title": "E Experimental Details",
            "content": "E.1 Model and Dataset Sizes The model sizes and dataset sizes employed in our experiments are depicted in Fig. 6. lr heads layers emb. dim. 124M 6.0e-4 350M 3.0e-4 760M 2.5e-4 2.0e-4 1.3B 1.6e-4 2.6B 12 16 16 32 32 12 24 24 24 32 768 1024 1536 2048 2560 Table 6: Model-size dependent hyperparameter used in our experiments. denotes the model size in terms of parameters, while lr corresponds to the maximum learning rate."
        },
        {
            "title": "Significance",
            "content": "For the error analysis, two separate random variables, X0 and X1, are considered. The symbol represents one of the metrics discussed in Sec. 4.3, while 0 and 1 stand for two approaches that are to be compared, like standard Adam and Coupled Adam, for instance. For each of the two random variables = {0, 1}, we conduct and evaluate training runs with different seeds, yielding results {X (1) , . . . , (S) } (53) While it is desirable to have large sample size S, it is prohibitively expensive for large model and dataset sizes to repeat training runs. We use = (54) except for the large-scale experiments (Sec. 4.2), where we restrict ourselves to ="
        },
        {
            "title": "We are interested in the difference",
            "content": "d = X1 X0 (55) (56) For = 1, it can be computed straight forwardly. However, no statement about the statistical uncertainty or significance of can be made. In the case of = 3, we apply one-sided Students t-test with confidence level of α = 95% (57) First, the sample means Xi ="
        },
        {
            "title": "1\nS",
            "content": "S (cid:88) s=1 (s) (58) and the corrected sample standard deviations ˆσ2 ="
        },
        {
            "title": "1\nS − 1",
            "content": "S (cid:88) (cid:16) s=1 (s) Xi (cid:17) (59) Figure 6: Overview of the dataset (horizontal axis) and model sizes (vertical axis) involved in our small-scale (blue, green and orange circles) and large-scale (red squares) experiments. The dashed, black line shows = D/20, which is approximately the compute-optimal trajectory according to Hoffmann et al. (2022). E.2 Training Hyperparameters In Tab. 5, we list the general hyperparameters used in our small-scale (Sec. 4.1) and large-scale (Sec. 4.2) experiments. During warm-up, the learnDescription Small-scale Large-scale optimizer β1 β2 ϵ weight decay gradient clipping dropout weight tying vocab size learning rate schedule layer normalization precision hidden activation positional embedding sequence length batch size (samples) batch size (tokens) warmup training framework training parallelism AdamW 0.9 0.95 1e-8 0.1 1.0 0.0 true 50304 cosine decay LayerNorm BF16 GeLU absolute (learned) 1024 96 100k 100 steps nanoGPT DDP SwiGLU RoPE 2048 256 500k 1% of steps Modalities FSDP Table 5: General hyperparameters used in our two sets of experiments. ing rate is increased from zero to the maximum learning rate. This is followed by cosine decay which reduces the learning rate to 10% of the maximum at the end of training. Note that weight decay is applied only to linear layers, not layer norms or embeddings. Tab. 6 shows the hyperparameters related to model size, following GPT-3 (Brown et al., 2020). 14 G.2 Scaled Coupled Adam Tab. 3 of Sec. 6.1 shows the results of varying the scaling exponent (see Eq. (29)) for = 20B. The dependency of the loss is visualized in Fig. 2. Here, in Fig. 8, we extend the visualization of the results to {5B, 10B, 20B} and the other evaluation metrics. G.3 SGD In Tab. 4 of Sec. 6.2, we showed results for SGD using the best hyperparameter . Detailed results of the corresponding hyperparameter searches can be found in Tab. 7. for the two samples {0, 1} are estimated. The sample means from Eq. (58) are combined to an estimate for their difference, = X1 X0 (60) and the sample standard deviations from Eq. (59) are propagated to the sample standard deviation of via Gaussian error propagation: (cid:115)(cid:18) X ˆσd = (cid:19)2 ˆσ0 + (cid:18) X1 (cid:19) ˆσ1 (cid:113) (56) = 0 + ˆσ2 ˆσ2 1 (61) Students t-distribution for the chosen confidence level α (see Eq. (57)) and the ν = 1 (54) = 2 degrees of freedom yields tα,ν = 2.92 (62) (63) With S, σd and tα,ν from Eqs. (54), (61) and (63) as ingredients, the one-sided confidence threshold for the difference can be computed as dsignificance = tα,ν ˆσd (64) Hence, the estimate from Eq. (60) is considered statistically significant improvement of approach = 1 over approach = 0 if < dsignificance (65) for metrics where smaller values are desirable (e.g. L), and > dsignificance (66) for metrics where larger values are better (e.g. Acc)."
        },
        {
            "title": "G Additional Results",
            "content": "G.1 Small-scale Experiments In Fig. 7, we visualize the results of our smallscale experiments (Sec. 5.1) for the loss and the average downstream task accuracy Acc, as listed in Tab. 1. 15 Figure 7: Difference in loss (left) and average downstream task accuracy (right) between Coupled Adam and standard Adam, for the different dataset sizes (horizontal axis) and model sizes (colors) of the small-scale experiments. The vertical bars indicate the one-sided 95% confidence interval for the difference to be significant. In order to avoid overlaps, the data points for = 125M and = 760M are slightly shifted to the left and right, respectively. Optimizer () Acc () Iso () µ () µr () () ρ () κ () 5B 125M SGD (100) SGD (200) SGD (300) SGD (400) SGD (500) SGD (600) Standard Adam Coupled Adam 3.23 (0) 3.19 (1) 3.17 (0) 3.18 (2) 3.19 (2) 3.24 (2) 3.14 (0) 3.12 (1) 0.325 (1) 0.327 (2) 0.333 (3) 0.332 (3) 0.330 (6) 0.326 (3) 0.340 (2) 0.339 (2) 1.00 (0) 1.00 (0) 0.99 (0) 0.99 (0) 0.99 (0) 0.99 (0) 0.31 (2) 0.94 (1) 0.00 (0) 0.00 (0) 0.00 (0) 0.01 (0) 0.01 (0) 0.01 (0) 1.10 (6) 0.02 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.67 (3) 0.01 (0) 33 (1) 41 (1) 45 (1) 48 (3) 49 (1) 48 (2) 15 (3) 55 (0) 59 (1) 66 (1) 71 (1) 75 (1) 77 (1) 79 (1) -54 (3) 87 (1) 25.0 (1.1) 19.2 (9) 15.5 (4) 14.2 (5) 13.2 (2) 11.6 (2) 0.6 (1) 4.8 (2) N Optimizer () Acc () Iso () µ () µr () () ρ () κ () 10B 125M SGD (100) SGD (200) SGD (300) SGD (400) SGD (500) SGD (600) Standard Adam Coupled Adam 3.13 (1) 3.09 (0) 3.07 (1) 3.07 (0) 3.09 (0) 3.11 (1) 3.07 (0) 3.03 (0) 0.337 (4) 0.339 (4) 0.341 (4) 0.340 (3) 0.338 (3) 0.341 (4) 0.343 (3) 0.343 (1) 1.00 (0) 0.99 (0) 0.99 (0) 0.99 (1) 0.99 (0) 0.98 (1) 0.21 (3) 0.91 (1) 0.00 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 1.58 (5) 0.05 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.75 (0) 0.02 (0) 41 (1) 48 (1) 49 (1) 51 (2) 52 (1) 53 (0) 9 (2) 57 (2) 58 (4) 65 (4) 71 (4) 76 (3) 79 (3) 81 (1) -64 (5) 82 (0) 22.2 (7) 16.1 (3) 12.2 (2) 10.7 (8) 10.3 (2) 9.4 (7) 0.4 (0) 3.6 (8) N"
        },
        {
            "title": "Optimizer",
            "content": "L () Acc () Iso () µ () µr () () ρ () κ () 20B 125M SGD (100) SGD (200) SGD (300) SGD (400) SGD (500) SGD (600)"
        },
        {
            "title": "Standard Adam\nCoupled Adam",
            "content": "3.05 (1) 3.02 (0) 3.01 (1) 3.00 (0) 3.01 (1) 3.04 (3) 3.03 (0) 2.97 (0) 0.343 (2) 0.345 (1) 0.350 (1) 0.346 (5) 0.348 (4) 0.348 (5) 0.346 (1) 0.350 (1) 0.99 (0) 0.99 (0) 0.98 (1) 0.98 (1) 0.98 (1) 0.95 (3) 0.10 (3) 0.83 (1) 0.01 (0) 0.01 (0) 0.01 (0) 0.01 (0) 0.02 (0) 0.02 (0) 2.14 (7) 0.11 (0) 0.01 (0) 0.01 (0) 0.02 (0) 0.02 (0) 0.02 (0) 0.02 (0) 0.82 (2) 0.03 (0) 50 (1) 52 (1) 53 (2) 54 (0) 55 (1) 52 (5) 5 (1) 57 (2) 57 (7) 64 (7) 70 (6) 76 (5) 78 (5) 81 (5) -66 (2) 77 (0) 18.4 (4) 12.7 (7) 9.0 (2) 7.4 (1.1) 7.5 (1.6) 6.8 (2.8) 0.3 (0) 1.7 (5) Table 7: Results of our experiments with SGD. Values are highlighted in bold if they are significantly better than all the other values in the same column. Figure 8: Dependency of different metrics on the scaling exponent n, see Eq. (29). From top to bottom: loss (upstream performance), average accuracy (downstream performance), isotropy, mean embedding norm ratio and r. Each plot shows the difference to the respective metric obtained for = 0. The arrows indicate whether larger () or smaller () values are desirable."
        }
    ],
    "affiliations": [
        "AI Sweden",
        "Forschungszentrum Jülich"
    ]
}