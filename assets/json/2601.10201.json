{
    "paper_title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "authors": [
        "Jiarui Yao",
        "Ruida Wang",
        "Tong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized."
        },
        {
            "title": "Start",
            "content": "PRL: Process Reward Learning Improves LLMs Reasoning Ability and Broadens the Reasoning Boundary Jiarui Yao*, Ruida Wang*, Tong Zhang University of Illinois Urbana-Champaign {jiarui14, ruidaw, tozhang}@illinois.edu 6 2 0 2 5 1 ] . [ 1 1 0 2 0 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Improving the reasoning abilities of Large Language Models (LLMs) has been continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus KL-divergence penalty term between the policy model and reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ metric. Extensive experiments 1 show the effectiveness of PRL could be verified and generalized."
        },
        {
            "title": "Introduction",
            "content": "The reasoning capabilities of Large Language Models (LLMs) have witnessed remarkable growth, transitioning from simple pattern matching, prompting or in-context learning (Wei et al., *Equal contribution. 1Code available at github repo. 1 2022) to solving complex challenges, such as mathematical, coding, and logical problems. While pre-training establishes strong foundation, posttraining techniques particularly Reinforcement Learning (RL) have become indispensable for aligning and enhancing these models with complex reasoning tasks (Xu et al., 2025a). Recent advancements, such as DeepSeek-R1 (Guo et al., 2025), have demonstrated that RL with verifiable rewards can significantly incentivize reasoning capabilities, sparking wave of research into optimizing LLMs for domains like mathematics, coding, and scientific reasoning etc. Despite this progress, significant bottleneck remains in the nature of the supervision signals. Most prevailing RL frameworks rely on outcome rewards, which are sparse signals provided only at the end of complete reasoning trajectory. While effective for simple tasks, outcome supervision often fails to provide the fine-grained guidance necessary for multi-step reasoning, where single intermediate error can derail the entire solution. To address this, recent works have pivoted toward Process Reward Models (PRMs), which assign credit to intermediate steps. However, existing implementations often suffer from severe efficiency and theoretical drawbacks. Many rely on computationally expensive inference-time techniques like Monte Carlo Tree Search (MCTS) or require training separate, heavy reward models to estimate step-wise value. Furthermore, the design of these process signals is frequently heuristic, lacking rigorous theoretical connection to the global optimization objective. In this paper, we propose Process Reward Learning (PRL), novel framework that bridges the gap between outcome objectives and process supervision without the computational overhead of MCTS or separate reward model training (Yang et al., 2024a; Luo et al., 2024). We theoretically demonstrate that the standard entropy-regularized reinforcement learning objective can be naturally decomposed into intermediate steps. By deriving the relationship between the optimal policy and the reference model, we formulate rigorous definition of process rewards that are intrinsically aligned with the global outcome. Our approach offers direct and efficient way to assign credit to intermediate reasoning steps, effectively turning sparse outcome rewards into dense process supervision signals. This guides exploration more effectively during RL optimization while maintaining mathematical equivalence to the original objective of reward maximization with KL-divergence penalty. Our contributions could be summarized as follows: Theoretical Framework: We derive PRL from the original optimization goal without modification, showing that optimal process rewards can be formulated as the decomposition of the entropy-regularized outcome objective. This provides solid theoretical backing often missing in heuristic PRM designs. Algorithmic Efficiency: Unlike methods dependent on MCTS or external value networks, PRL integrates process supervision directly into the policy gradient workflow, significantly enhancing training efficiency. Empirical Performance: We conduct extensive experiments on standard math reasoning benchmarks (including MATH500, Minerva Math, and Olympiad Bench, etc.) using different base models, including Qwen2.5-Math and Llama-3.2 series, at several scales. Our results demonstrate that PRL not only improves average performance (average@N) but also \"broadens the reasoning boundary\" of LLMs, as evidenced by significant gains in pass@N metrics compared to strong baselines like REINFORCE, RAFT (Dong et al., 2023; Xiong et al., 2025) and GRPO (Guo et al., 2025; Shao et al., 2024)."
        },
        {
            "title": "2 Related Work",
            "content": "Along with LLMs demonstration of great potential of accomplishing both daily and professional tasks, the research to better enhance LLMs abilities through various aspects keeps growing, in which RL plays more and more crucial role. As pointed out in Yao (2025), the successful application of RL in LLMs post-training is largely attributed to LLMs themselves, which provide strong enough prior knowledge for exploration afterwards. Along with the pre-training of LLMs providing better and better base models, post-training techniques, especially RL, come into the stage and align LLMs well across general downstream domains. Reinforcement Learning for LLMs Reinforcement Learning (RL) has been flourishing research area for long time, especially along with its successful deployment in game simulations, embodied AI, and robotics. Well-known RL algorithms, especially PPO (Schulman et al., 2017), have become standard practice for policy model training. After the appearance of LLMs, RL is also utilized for post-training in RLHF (Ouyang et al., 2022; Bai et al., 2022). As DeepSeek-R1 (Guo et al., 2025) reveals how to train reasoning models based on RL with verifiable reward (RLVR) to the public, there have been tons of following works applying such spirit on all kinds of domains, including but not limited to search and information retrieval (Jin et al., 2025; Chen et al., 2025a), multimodal reasoning (Tian et al., 2025), embodied AI (Wang et al., 2025), and GUI agents (Luo et al., 2025). New algorithms that fit the characteristics of LLMs like GRPO (Guo et al., 2025; Shao et al., 2024), GSPO (Zheng et al., 2025). Besides, there are several open-source RL training frameworks (Hu et al., 2024; Sheng et al., 2024; Fu et al., 2025; Zhu et al., 2025) that provide great environment setup for algorithmic research. LLMs Reasoning The reasoning ability of LLMs has been hot topic in recent years. Wei et al. (2022) finds that using Chain-of-Thought (CoT) prompting to explicitly require LLMs to reason step by step boosts the models performance by large margin. Diverse prompting manners (Yao et al., 2023; Xu et al., 2025b; Zhou et al., 2022) appear to fully take advantage of the in-context learning ability of LLMs. Later, besides test-time enhancement, improving the inherent reasoning ability of LLMs through better training emerges (Hao et al., 2024; Liu et al., 2025; Arora and Zanette, 2025; Kumar et al., 2025). To facilitate LLM RL training with better efficiency, (Yao et al., 2025; Chen et al., 2025b; Zhang et al., 2025) propose different methods like curriculum learning, gradient variance estimation, etc., to reweight the training data. Relevant works analyzing the roles of data, different algorithms and commonly used tricks like 2 Figure 1: PRL workflow demonstration. For each prompt and response trajectory (x, a) with = [a1, a2, , aL], we could split the reasoning response into several intermediate steps (by fixed length, newline symbol, etc.) and calculate the process reward as the entropy ratio between the current policy πω and reference policy π0. The final process reward for each step is the combination of ① the entropy ratio (cid:80)p π0(aj x,a(j1)) and ② the final outcome reward r(x, a). j=1 log πω(aj x,a(j1)) clipping, importance sampling also unveil the key underlying factors that affect the training stability and efficiency of RL training. Particularly for math reasoning, DAPO (Yu et al., 2025) summarizes recent progress and sets up several common practices like clipping higher and data filtering for training reasoning models. Reasoning models are widely adopted in more general domains like questionanswering (QA) (Chen et al., 2024; Ranaldi and Freitas, 2024), coding (Yang et al., 2024b), and agentic tasks (Yang et al., 2024c; Ferrag et al., 2025). Process Reward Modeling Ever since the release of OpenAI o1-series models (Jaech et al., 2024), the research community has shown an enthusiasm to look for the underlying mechanism for the success of the reasoning models like o1. Among all the explorations, process reward models (PRMs) (Khalifa et al., 2025) become strong candidate as they align with human intuitions at first glance. Most PRMs will decompose the single reasoning trajectory into several intermediate steps, and assign fine-grained rewards to them one by one. Famous works like Cui et al. (2025) design process reward as the log likelihood ratio between the current policy model and reference model, similar to the reward shaping in Direct Policy Optimization (DPO (Rafailov et al., 2023)). Zhang et al. (2024b) considers an entropy regularized RL objective and utilizes MCTS to estimate the process rewards at each step. Other works using MCTS for PRM estimation include Zhang et al. (2024a); Park et al. (2025)."
        },
        {
            "title": "3.1 Problem Setting and Formulation",
            "content": "We consider the training of reasoning model using RL under alphabet Σ, which implicitly leads to space of text Σ. Let Σ be prompt, = [a1, . . . , aL] Σ be the response of an LLM to prompt x, where a1, . . . , aL Σ are the CoT steps (or any reasoning steps) to generate the response in reasoning model. Assume we have deterministic reward function : Σ [0, 1] to evaluate the quality of text sentence, i.e., r([x, a]) [0, 1] assessing the quality of the response to prompt x. We aim to train policy model (an LLM essentially) π : Σ Σ [0, 1], which represents the probability of some given text under the condition of another given text. In particular, π(ax) is the 3 probability of generating text Σ under the given text Σ Our goal is to train policy π(ax) that generates high-quality responses while staying close to the original response provided by π0 x, which is reference model, and usually as the original policy model. The entropy-regularized formulation is: (cid:20) Q(π) = Ex Eaπ(x)r(x, a) (cid:21) KL(ππ0) , 1 η where KL(ππ0) = (cid:82) π(x) log π(x) π0(x) dx is the KLdivergence between two distributions formed by π and π0. Usually, the KL-divergence penalty term is used for restricting the policy model π from deviating too much from the reference model π0. Based on the above assumption and problem formulation, we could derive the following theorem about the closed-form solution for the optimal policy π. Theorem 3.1. Given reference (normally the original) policy π0 : Σ Σ [0, 1], and the optimial policy maximizing the entropy-regularized reward, π = arg maxπ{Q(π)}, we have: [aℓ+1, . . . , aL] Σ be the completion of the partial response. Given the policy model π, we define the entropy-regularized process reward as ℓ (x, a(ℓ)) = a(ℓ)π(x,a(ℓ)) (cid:20) r(x, a) 1 η (cid:88) j=ℓ+ ln π(ajx, a(j1)) π0(ajx, a(j1)) (cid:21) . From the fact that all paths should give identical value (under the optimal policy), we could obtain the following theorem. Theorem 3.3. Given x, a(ℓ) Σ where represents the prompt and a(ℓ) represents partial answer, no matter what [a(ℓ+1), a(ℓ+2), , a(L)] = aℓ is, the following equality always holds: ℓ (x, a(ℓ)) = r(x, a) 1 η (cid:88) j=ℓ+1 ln π(ajx, a(j1)) π0(ajx, a(j1)) , Proof of this theorem can be found in Appendix A.3. More generally, we have that for arbitrary ℓ < L: π(ax) π0(ax)eηr(x,a). ℓ (x, a(ℓ)) = r p(x, a(p)) So we know the distribution from the optimal policy only depends on the reference model and the reward function. The full proof of the theorem is delayed to Appendix A.1. This further leads to the following corollary. Corollary 3.2. (x, a) Σ Σ, there exists constant such that r(x, a) 1 η ln π(ax) π0(ax) = C. The detailed proof of this corollary is postponed in the Appendix A.2. We will exploit this corollary in our RL algorithm to optimize π from process reward perspective."
        },
        {
            "title": "3.2 Optimal Process Reward",
            "content": "To formulate process rewards, we consider the whole reasoning process with tokens, = [a1, . . . , aL], 1 η (cid:88) j=ℓ+ ln π(ajx, a(j1)) π0(ajx, a(j1)) , where we define L(x, a) = r(x, a)."
        },
        {
            "title": "3.3 Process Reward and Policy Learning\nWe could generalize the optimal reward function r∗\ninto a learnable one as well. We assume a learnable\nreward model ru([x, a(ℓ)]) : Σ∗ → [0, 1] with u\nas the learnable model parameter, and a learnable\npolicy πw with w as the learnable parameter.",
            "content": "We generate reasoning trajectory given prompt using πw, and for any pair ℓ < (and we may choose multiple such pairs), we update the reward model by running SGD with respect to the following objective function (ru([x, a(ℓ)]) yℓ)2, where the target is set by (cid:18) yℓ = stopgrad ru([x, a(p)]) where ai is token belonging to Σ. Then we try to assign reward ℓ (x, a(ℓ)) to the partial answers a(ℓ) = [a1, . . . , aℓ] Σ. Let a(ℓ) = 1 η (cid:88) j=ℓ+1 ln πw(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:19) . 4 Model MATH500 Minerva Math Olympiad Bench AMC23 AIME24 5 Avg Qwen2.5-Math-1.5B + RAFT + GRPO + PRL Qwen-2.5-Math-7B + RAFT + GRPO + PRL Llama-3.2-1B-Instruct + GRPO + PRL Llama-3.2-3B-Instruct + GRPO + PRL 81.60 87.40 88.00 89.40 82.00 91.80 92.60 93. 45.20 57.80 60.60 67.80 76.80 74.00 35.66 42.65 44.85 45.59 31.03 51.84 52.21 52.57 8.46 17.65 17.65 28.31 36.03 36. 48.15 52.00 56.44 58.07 51.56 62.22 65.33 65.19 12.89 22.81 20.15 28.30 39.56 41.33 65.00 77.50 70.00 85.00 72.50 82.50 85.00 85. 20.00 30.00 30.00 45.00 55.00 67.50 20.00 33.33 20.00 30.00 40.00 46.67 46.67 43.33 6.67 13.33 6.67 23.33 16.67 16. 56.82 62.23 64.40 66.31 58.24 70.34 72.12 72.38 22.81 33.42 33.03 41.66 51.16 51.42 Table 1: Performance (%) comparison across several math reasoning benchmarks, measured by pass @ 8. Note that we always have ru([x, a]) = r([x, a]) for the whole response a. We update πw by running SGD with respect to the following objective function 2024) to alleviate. In the GRPO application, we could turn the reward function into the advantage function first, and then calculate the process advantage as (cid:88) j=1 ρj ln πw(aj[x, a(j1)]), where we set ρℓ = stopgrad ru([x, a(p)]) (cid:18) (cid:18) ρℓ = stopgrad A([x, a]) (cid:88) j=ℓ 1 η ln πw(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:19) , (cid:88) j=ℓ+1 1 η ln πw(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:19) , for any < L. We may also set = and a(p) = a(L), and then update policy without learning ru(). Now this is similar to policy gradient for entropy regularized RL optimization: (cid:18) ρℓ = stopgrad r([x, a]) (cid:88) j=ℓ 1 η ln πw(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:19) . (3.1) This will be the simplest method once we have rule-based reward function, i.e., the optimal reward function r([x, a]) = r([x, a]). But it may lead to higher variance, and one may combine this with other advantage estimation algorithms like PPO (Schulman et al., 2017) and GRPO (Shao et al., where A([x, a]) is the normalized reward in group, calculated by subtracting the mean and dividing by the standard deviation of the rewards in the same group."
        },
        {
            "title": "The detailed optimization proof can be found in",
            "content": "Appendix A.4. In the actual implementation, we also integrate the importance sampling and clipping tricks, as several papers (Schulman et al., 2017; Yu et al., 2025) pointed out the importance of these tricks in helping stabilize the training process. And the final reward function becomes L(ω) = (cid:40) min ρj (cid:88) j=1 πω(aj[x, a(j1)]) πold(aj[x, a(j1)]) , (cid:40) ρj clip πω(aj[x, a(j1)]) πold(aj[x, a(j1)]) , 1 ε (cid:41) (cid:41) βDKL(πωπ0), 5 and AMC23. The sizes of the above benchmarks are summarized in Table 2. In our evaluation, we use Math-Verify 2 to systematically verify the results. Environment and Computation For the RL training, we modify the verl (Sheng et al., 2024) code repo and implement PRL based on it. For model training, we use 4 Nvidia H100 GPUs. For the hyperparameters, we use batch size of 128, with learning rate of 1e-6, prompt length 1024 and response length 3072. For detailed hyperparameters, please refer to Table 6. 4.2 Quantitive Results First we summarize the average performance of PRL with different baselines in Table 3. The performance metric is average @ 8, where for each problem in the evaluation dataset, we sample eight times and calculate the average pass rate across the rollouts. Note that the average results are size-weighted average across all datasets. From Table 3, we can see that PRL achieves consistently better performance across different benchmarks with different base models. This validates our hypothesis that for reasoning tasks, especially under challenging scenarios, process supervision is important compared with pure outcome-based reward signal. The summation of the log ratio between the current policy model πω and the reference model provides fine-grained guidance for the exploration in RL optimization."
        },
        {
            "title": "Benchmark",
            "content": "MATH500 Minerva Math Olympiad Bench AIME24 AMC"
        },
        {
            "title": "Size",
            "content": "500 272 675 30 40 Table 2: The size of evaluation benchmarks used in experiments. We also compare the performance of the reasoning boundary of different models by the metric pass @ (n = 8 in our evaluation due to inference resource limitation), where we rollout times for the same prompt, and the final score is one if at least one of the rollouts is correct. The detailed results are in Table 1. From the results, we could see PRL not only increases the average performance by 2https://github.com/huggingface/Math-Verify 6 Figure 2: The training dynamics of KL loss and entropy loss with Qwen2.5-Math-7B as the base model under different configurations. where (cid:18) ρj = stopgrad r([x, a]) (cid:88) j=ℓ 1 η ln πw(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:19) , and πold is the policy from the last step. The pseudo code for PRL algorithm pipeline could be referred in Algorithm 1."
        },
        {
            "title": "4 Experiments and Results",
            "content": "We conduct our main experiments on math reasoning tasks, subdomain of LLMs reasoning with rich related literature and open-source base models, with public large-scale datasets."
        },
        {
            "title": "4.1 Experiment Setup",
            "content": "Models and Data In the line of research to improve the math reasoning ability of LLMs, Qwen and Llama have been commonly selected. In consideration of the limit on computation resources, we choose Qwen2.5-Math-1.5B and Qwen2.5Math-7B, with Llama-3.2-1B-Instruct and Llama3.2-3B-Instruct as the base models. Such choices help alleviate the bias across model scale and architectures. As for the training data, we use NuminaMath (Li et al., 2024) and randomly sample subset with approximately 150k samples to constitute the training dataset. Each problem is associated with verifiable reward, and therefore, the correctness of outputs from LLMs could be easily checked by rule-based program. For the evaluation benchmarks, we use MATH500 (Hendrycks et al., 2021), Minverva Math (Lewkowycz et al., 2022), Olympiad Bench (He et al., 2024), AIME24 Algorithm 1 Process Reward Learning (PRL) Input: Dataset D, initial policy model πinitial, reference model π0, reward function r(), KL regularization coefficient η, clipping parameter ϵ, learning rate α Output: Optimized Policy πω 1: Initialize πω πinitial 2: while training not converged do 3: Sample batch of prompts {xi}n for each prompt xi do i=1 Generate trajectories = [a1, a2, . . . , aL] via πω(xi) Compute outcome reward = r(xi, a) Compute advantage A(xi, a) = Rmean for each time step = 1 to do std Can be rule-based or learnable reward Calculate log-ratio (KL term): kt = ln πω(atxi,a(t1)) π0(atxi,a(t1)) Calculate Future KL Penalty Sum: 1 η kj Compute Process Advantage ρt: ρt stopgrad (A(xi, a) St) St = (cid:80)L j=t end for Calculate rt(ω) = πω(atxi, a(t1))/πωold(atxi, a(t1)) and loss Li(ω): Li(ω) = 1 t=1 min (rt(ω)ρt, clip(rt(ω), 1 ϵlow, 1 + ϵhigh)ρt) (cid:80)L 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for L(ω) = 1 Update parameters: ω ω αωL(ω) Li(ω) (cid:80) 20: 21: end while concentrating the output quality, but also broadens the reasoning boundary by increasing the pass @ metric. This also originates from the property that all reasoning trajectories should lead to exactly the same reward under the optimal reward function in Corollary 3.2."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We first compare the different ways in which we split the intermediate steps would impact the performance of PRL. Though for reasoning models with structured outputs, we could separate the intermediate steps by newline symbols (n or nn), we find that splitting steps by fixed length also works well. The results could be found in Table 4. It could be inferred that different ways of splitting could cause performance fluctuation, while finding balanced way in our experiments, splitting intermediate steps with fixed length of 256 could achieve the most performance boost for PRL. The training dynamics (entropy loss of the policy model πω and KL divergence between πω and reference model π0) with base model Qwen2.5Math-7B is also displayed in Figure 2. We compare different algorithms, including RAFT, GRPO, and PRL, under different configurations. It could be seen that with fixed length of 256 PRL achieves the best balance between deviating from the reference policy π0 and maintaining relatively high entropy loss for exploration. The order of calculating the advantage and the process reward matters as well. To be more specific, we could first calculate the advantage value as GRPO for prompt-response pairs {(x, ai)} in the sample group A(x, ai) = R(x, a) µ(ai) σ(ai) , and substitute the original reward in Equation 3.1 with advantage. On the other hand, we could also first calculate the process reward with the original outcome reward, and then compute the advantage using the process reward ρℓ(x, a) as R(x, a). Since there is coefficient η that we could tune as hyperparameter for the summation of log ratios, and usually this is large number from 100 to 300, the order of calculating the advantage and the process reward does not matter too much, and thus both of them achieve similar results, slightly better 7 Model MATH500 Minerva Math Olympiad Bench AMC23 AIME24 5 Avg Qwen2.5-Math-1.5B + RAFT + GRPO + PRL Qwen-2.5-Math-7B + RAFT + GRPO + PRL Llama-3.2-1B-Instruct + GRPO + PRL Llama-3.2-3B-Instruct + GRPO + PRL 38.53 62.92 67.50 71.00 41.62 74.58 80.72 81. 13.48 26.30 28.65 34.95 48.23 47.50 11.76 23.12 25.55 28.63 12.87 33.18 36.76 37.78 2.21 6.16 5.01 9.97 16.96 16. 18.85 28.17 31.35 33.57 19.24 37.30 44.20 44.93 2.46 6.06 6.76 8.06 17.35 16.93 24.38 37.5 44.69 45.94 28.12 59.06 63.12 63. 2.81 9.38 14.38 16.25 22.81 27.50 3.75 9.17 6.67 9.17 9.58 20.00 22.08 22.50 1.25 0.00 1.67 3.33 5.83 6. 23.91 38.59 42.09 44.86 25.52 49.08 54.96 55.71 6.03 12.72 13.76 17.39 27.37 27.06 Table 3: Performance (%) comparison across several math reasoning benchmarks, measured by average @ 8."
        },
        {
            "title": "Model",
            "content": "Qwen2.5-Math-1.5B + GRPO + PRL (nn) + PRL (fixed length 256) Qwen2.5-Math-7B + GRPO + PRL (fixed length 16) + PRL (fixed length 64) + PRL (fixed length 256)"
        },
        {
            "title": "5 Avg",
            "content": "56.82 64.40 66.31 65.86 58.24 72.12 71.19 70.27 72.38 Table 4: Performance comparison among different intermediate steps splitting manners, with metric pass @ 8. Here, PRL (nn) means we use two adjacent newline symbols to split intermediate steps, while PRL (fixed length 256) means that we use fixed length of 256 tokens to constitute the intermediate steps. than GRPO under metric pass @ 8 as shown in Table 5."
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "The reasoning ability of LLMs has been improved by large margin through post-training techniques like RL, while most of the training is performed based on an outcome-based reward function (model). Among the existing works providing process supervision signals, computationally intensive estimations for the intermediate rewards like MCTS or additional training for another reward model, are usually inevitable. The PRL algorithm"
        },
        {
            "title": "Model",
            "content": "Llama-3.2-3B-Instruct + GRPO + PRL (advantage first) + PRL (process reward first)"
        },
        {
            "title": "5 Avg",
            "content": "41.66 51.16 51.42 51.42 Table 5: Performance comparison between different orders of calculating the process reward and advantage values, with metric pass @ 8. PRL (advantage first) means we first calculate the advantage only using the outcome rewards, while PRL (process reward first) means we first calculate the process rewards and then advantages based on them. provides such fine-grained process supervision by integrating the summation of the log ratio between the current policy and reference policy, removing the need to do expensive calculations of extra training. Our experimental results demonstrate the effectiveness of PRL, showing performance improvement on both average @ and pass @ metrics. Besides, we provide detailed and rigorous theoretical foundation for PRL, from the motivation to the reward formulation, forming complete optimization framework. Note that the purpose of providing process supervision during the reasoning process is mainly for better guidance on intermediate steps and eliciting more diverse reasoning paths, as all reasoning trajectories should lead to the same process reward under the optimal configuration. While currently PRL has demonstrated the potential to improve both the reasoning quality and boundary, we could further make step towards encouraging the exploration of LLMs reasoning by adding an extra exploration term. We leave the exploration in such direction for future research."
        },
        {
            "title": "Limitations",
            "content": "The current experiment results are mainly based on relatively small-scale open-source models, Qwen 2.5 Math (Yang et al., 2024a) and Llama-3.2 (Dubey et al., 2024) with 1B to 7B parameters due to computation resources limitation, while scaling up to larger models with 10B 100B parameters remains to be further explored. In addition, though our analysis for PRM is based on an entropy-regalized RL objective, there may be different ways to derive the process reward formulation, leading to different reward shaping. The way of splitting the intermediate steps could also be tuned, by fixing the step length, partitioning by newline symbols, or decided by another model. We hope these could shed light on future research."
        },
        {
            "title": "Acknowledgments",
            "content": "This research used the DeltaAI (NSF OAC2320345) and Delta (NSF OAC-2005572) advanced computing and data resources, supported by the National Science Foundation and the State of Illinois."
        },
        {
            "title": "References",
            "content": "Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, and 1 others. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025a. Research: Learning to reason with search for llms via reinforcement learning. Preprint, arXiv:2503.19470. Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. 2025b. Self-evolving arXiv preprint curriculum for llm reasoning. arXiv:2505.14970. Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. 2024. Autoprm: Automating procedural supervision for multi-step reasoning via controllable question decomposition. arXiv preprint arXiv:2402.11452. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Yuchen Zhang, Jiacheng Chen, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, and 1 others. 2025. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. 2025. From llm reasoning to autonomous ai agents: comprehensive review. arXiv preprint arXiv:2504.19678. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. 2025. Areal: large-scale asynchronous reinforcement learning system for language reasoning. Preprint, arXiv:2505.24298. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828 3850. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. Openrlhf: An easyto-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143. 9 Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. 2025. Process reward models that think. arXiv preprint arXiv:2504.16828. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. 2025. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, and 1 others. 2024. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. 2025. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, and 1 others. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. 2025. Gui-r1: generalist r1-style vision-language arXiv preprint action model for gui agents. arXiv:2504.10458. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Sungjin Park, Xiao Liu, Yeyun Gong, and Edward Choi. 2025. Ensembling large language models with process reward-guided tree search for better complex reasoning. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1025610277. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741. Leonardo Ranaldi and Andre Freitas. 2024. Self-refine instruction-tuning for aligning reasoning in language models. arXiv preprint arXiv:2405.00402. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, and Ziwei Liu. 2025. Ego-r1: Chain-of-tool-thought for ultra-long egocentric video reasoning. arXiv preprint arXiv:2506.13654. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li FeiFei, Lijuan Wang, Yejin Choi, and Manling Li. 2025. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. Preprint, arXiv:2504.20073. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, and 1 others. 2025. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343. 10 Hanning Zhang, Pengcheng Wang, Shizhe Diao, Yong Lin, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, and Tong Zhang. 2024b. Entropyregularized process reward model. arXiv preprint arXiv:2412.11006. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, and 1 others. 2025. Group sequence policy optimization. arXiv preprint arXiv:2507.18071. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. 2022. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066. Zilin Zhu, Chengxing Xie, Xin Lv, and slime Contributors. 2025. slime: An llm post-training framework for rl scaling. https://github.com/THUDM/slime. GitHub repository. Corresponding author: Xin Lv. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, and 1 others. 2025a. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025b. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, and 1 others. 2024a. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Terry Yue Zhuo, and Taolue Chen. 2024b. Chainof-thought in neural code generation: From and for lightweight language models. IEEE Transactions on Software Engineering. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024c. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:50528 50652. Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, and Tong Zhang. 2025. Optimizing chain-of-thought reasoners via gradient variance minimization in rejection sampling and rl. arXiv preprint arXiv:2505.02391. Shunyu Yao. 2025. The second half. https:// ysymyth.github.io/The-Second-Half/. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816. Enci Zhang, Xingang Yan, Wei Lin, Tianxiang Zhang, and Lu Qianchun. 2025. Learning like humans: Advancing llm reasoning capabilities via adaptive difficulty curriculum learning and expert-guided selfreformulation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 66306644."
        },
        {
            "title": "A Missing Proofs",
            "content": "In this section, we supplement the detailed proofs for the theorems mentioned in the main body. A.1 Proof of Theorem 3.1 Consider for fixed prompt Σ, and our optimization objective is: {Eaπ(x)[r(x, a)] (cid:40) (cid:18) π(yx) r([x, y]) 1 η KL(ππ0)} max π(x) = max π(x) (cid:88) yΣ 1 η ln (cid:18) π(yx) π0(yx) (cid:19)(cid:19) (cid:41) , under the condition (cid:80) Thus, we could construct the Laplacian of the above optimization problem, yΣ π(yx) = 1. L(π(x), λ) = π(yx) (cid:18) r([x, y]) 1 η ln (cid:18) π(yx) π0(yx) (cid:19)(cid:19) (cid:88) yΣ (cid:16) 1 λ (cid:88) (cid:17) π(yx) . yΣ Setting the partial derivative of L(π, λ) with respect to π equal to 0, π(yx) L(π, λ) = 0, we have π(yx) (cid:88) π(yx) (cid:18) r([x, y]) yΣ 1 η ln (cid:18) π(yx) π0(yx) (cid:19)(cid:19) λ 1 (cid:88) yΣ π(yx) = 0 r([x, y]) 1 η (cid:18) ln (cid:19) (cid:18) π(ax) π0(ax (cid:19) + 1 λ = 0 η r([x, y]) η λ 1 = ln (π(ax)) ln (π0(ax)) ln π(ax) = ln π0(ax) + η r([x, y]) η λ 1 π(ax) = π0(ax) exp (η r([x, y]) ηλ 1) π0(ax) exp(ηr([x, a]). Therefore we complete the proof. A.2 Proof of Corollary 3.2 Based on Theorem 3.1, we have π(ax) π0(ax)eηr(x,a). So there exists constant R, such that, π(ax) = π0(ax)eηr(x,a). After transposing and simplifying terms, we can obtain r(x, a) 1 η ln π(ax) π0(ax) = eC = R. 12 A.3 Proof of Theorem 3.3 From Corollary 3.2, under optimal policy π and optimal reward r: = r([x, a]) 1 η ln π(ax) π0(ax) . Given basic properties of language modeling, we have: ln π(ax) π0(ax) = ln Therefore, we have: (cid:81)L (cid:81)L j=1 π(aj[x, a(j1)]) j=1 π0(aj[x, a(j1)]) = (cid:88) j= ln π(aj[x, a(j1)]) π0(aj[x, a(j1)]) . = r([x, a]) (cid:32) 1 η ℓ (cid:88) j=1 ln π(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:33) 1 η (cid:88) (cid:32) ln j=ℓ+1 (cid:33) π(aj[x, a(j1)]) π0(aj[x, a(j1)]) Therefore, ℓ ([x, a(ℓ)]) = a(ℓ)π([x,a(ℓ)]) r([x, a]) + 1 η ℓ (cid:88) j=1 ln = a(ℓ)π([x,a(ℓ)]) = C, (cid:33) (cid:88) (cid:32) ln 1 η j=ℓ+1 (cid:32) π(aj[x, a(j1)]) π0(aj[x, a(j1)]) (cid:33) π(aj[x, a(j1)]) π0(aj[x, a(j1)]) for some constant C, where the final equation holds due to that a(ℓ) is fixed given the conditional ℓ ([x, a(ℓ)]) should be constant no matter what distribution π([x, a(ℓ)]). Thus, the process reward the future path is under the optimal policy π. Therefore, given the whole reasoning trajectory = [a1, , aL], the universal equality holds: ℓ (x, a(ℓ)) = r(x, a) 1 η (cid:88) j=ℓ+1 ln π(ajx, a(j1)) π0(ajx, a(j1)) . A.4 Proof for optimization For fixed prompt x, the objective function for optimizing the policy model is: (ω) = Eaπω(x) (cid:20) r(x, a) 1 η ln (cid:18) πω(ax) π0(ax) (cid:19)(cid:21) . Therefore, taking the gradient of the objective function with respect to ω, we have: ωJ (ω) = (cid:88) aΣ We set the following: ωπω(ax)r([x, a]) 1 η (cid:88) aΣ (cid:34) ω πω(ax) (cid:88) k=1 ln πω(ak[x, a(k1)]) πω(ak[x, a(k1)]) (cid:35) . (A) = (cid:80) aΣ ωπω(ax)r([x, a]) (B) = 1 η (cid:80) aΣ ω (cid:104) πω(ax) (cid:80)L k=1 ln πω(ak[x,a(k1)]) πω(ak[x,a(k1)]) (cid:105) From basic knowledge of calculus, we know that ωπω(a) = πω(a)ω ln πω(a) and ω k=1 ω ln πω(akx, a(k1)). k=1 ln πω(akx,a(k1)) π0(akx,a(k1)) = (cid:80)L (cid:80)L 13 Therefore, (A) = (cid:88) aΣ πω(ax)ω ln(πω(ax)) r([x, a]). For the second term (B), we have (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) η (B) = (cid:88) aΣ (cid:88) = aΣ (cid:88) = aΣ (cid:88) = aΣ (cid:88) = aΣ (cid:88) = aΣ ω(πω(a)) (cid:88) k=1 ln πω(ak[x, a(k1)]) π0(ak[x, a(k1)]) + πω(a)ω (cid:88) k=1 ln πω(ak[x, a(k1)]) π0(ak[x, a(k1)]) (cid:35) πω(ax)(ω ln πω(ax)) πω(ax)(ω ln πω(ax)) πω(ax)(ω ln πω(ax)) πω(ax)(ω ln πω(ax)) πω(ax)(ω ln πω(ax)) (cid:88) k=1 (cid:88) k=1 (cid:88) k= (cid:88) k=1 (cid:88) k=1 ln πω(akx, a(k1)) π0(akx, a(k1)) ln πω(akx, a(k1)) π0(akx, a(k1)) ln πω(akx, a(k1)) π0(akx, a(k1)) ln πω(akx, a(k1)) π0(akx, a(k1)) ln πω(akx, a(k1)) π0(akx, a(k1)) + πω(ax)ω (cid:35) ln(πω(akx, a(k1)) (cid:88) k= (cid:35) + πω(a)ω ln π(ax) (cid:35) + ωπω(a) + ω (cid:88) aΣ πω(a) (cid:35) (cid:35) . Combining the above two results, we have ωJ(ω) = (cid:88) aΣ πω(ax)(ω ln πω(ax)) r([x, a]) (cid:34) ln πω(akx, a(k1)) π0(akx, a(k1)) (cid:35) 1 η (cid:88) k=1 (cid:34) πω(ax) ω (cid:88) j=1 (cid:88) = aΣ ln πω(aj[x, a(j1)]) r([x, a]) 1 η (cid:88) k=1 ln πω(akx, a(k1)) π0(akx, a(k1)) (cid:35) = (cid:88) (cid:88) aΣ j=1 πω(ax)ω ln πω(ajx, a(j1)) r([x, a]) (cid:34) 1 η (cid:88) k=1 ln πω(akx, a(k1)) π0(akx, a(k1)) (cid:35) (cid:34) (cid:32) = Eaπω(x) ω ln πω(aj[x, a(j1)]) r([x, a]) 1 η (cid:88) k=1 ln πω(ak ) π0(ak ) (cid:33)(cid:35) . To stabilize the training process, we can turn the reward term ρj presented in the main body of paper. (cid:16) r([x, a]) 1 η (cid:80)L k=1 ln πω(ak ) π0(ak ) (cid:17) into any process swer it correctly in all rollouts, while another one finetuned with PRL makes it correct successfully."
        },
        {
            "title": "B More Experiments Details",
            "content": "We present the hyperparameters used in our experiments in Table 6. We mainly use Nvidia H100 GPUs as the computation resources, and the total training time for one experiment run is about 15 hours on 4 H100 server."
        },
        {
            "title": "C Case Study",
            "content": "We provide concrete example for problem in C.1 from Olympiad Bench (He et al., 2024), where the base model finetuned with GRPO does not an-"
        },
        {
            "title": "Parameter",
            "content": "GRPO group size batch size mini batch size max prompt length max response length learning rate KL loss coefficient entropy loss coefficient PRL step length PRL η"
        },
        {
            "title": "Value",
            "content": "5 128 256 1024 3072 1e 6 0.0 0.0 256 100300 Table 6: Full hyperparameters list. 15 C.1: Case Study Problem cubic sequence is sequence of integers given by an = n3 + bn2 + cn + d, where b, and are integer constants and ranges over all integers, including negative integers. Determine the possible values of a2015 a2016 for cubic sequence satisfying the condition in part (a). Wrong answer 1 Given cubic sequence an = n3 +bn2 +cn+d, we need to find the possible values of a2015 a2016. First, lets define a2015 and a2016 in terms of the constants b, c, and d: a2015 = 20153 + 20152 + 2015 + a2016 = 20163 + 20162 + 2016 + Now, lets compute their product: a2015 a2016 = (20153 + 20152 + 2015 + d) (20163 + 20162 + 2016 + d) This product is bit complex, but we can simplify it by expanding the expression and grouping terms. a2015 a2016 = (20153 + 20152 + 2015 + d)(20163 + 20162 + 2016 + d) = (20153 + 20152 + 2015 + d)(20163 + 20162 + 2016 + d) = (20153 20163 + 20153 20162 + 20153 2016 + 20153 + 20152 20163 +b2 20152 20162 + 20152 2016 + 20152 + 2015 20163 + 20164 +c 2015 2016 + 2015 + 20163 + 20162 + 2016 + d2) This expansion is quite complex, so lets simplify by noting that the leading term of the product is 20153 20163, which is much larger term. The other terms involve lower powers of b, c, and d, which are much smaller than 20153 20163. Since we are asked to find the possible values of a2015 a2016 under the condition given, we can focus on the leading terms and simplify the expression. The leading term of the expanded product is 20153 20163. Since the sequence involves integer constants and ranges over all integers, the product will be an integer. Therefore, the possible values of a2015 a2016 are determined by the leading term 20153 20163, and since the other terms are much smaller compared to this leading term, they do not affect the overall magnitude. The final answer is: 20153 20163 Wrong answer 2 Given that an = n3 + bn2 + cn + is cubic sequence, we need to determine the possible values of a2015 a2016. First, lets express a2015 and a2016 in terms of n: a2015 = 20153 + 20152 + 2015 + a2016 = 20163 + 20162 + 2016 + Now we need to find the product a2015 a2016. To simplify the expressions, lets introduce the following variables: = 20153 + 20152 + 2015 + 16 = 20163 + 20162 + 2016 + We can rewrite in more manageable form by noting that 2016 = 2015 + 1: = (2015 + 1)3 + (2015 + 1)2 + (2015 + 1) + = 20153 + 3 20152 + 3 2015 + 1 + (20152 + 2 2015 + 1) + (2015 + 1) + = 20153+b20152+c2015+d+320152+32015+1+b20152+2b2015+b+c2015+c+d = + 3 20152 + 3 2015 + 1 + 2 2015 + + + = + 2 2015 + 3 20152 + + 3 2015 + + + 1 Since = 20153 + 20152 + 2015 + d, we can substitute into the above equation: = + 2 2015 + 3 20152 + + 3 2015 + + + 1 = + 2 2015 + 3 20152 + + 3 2015 + + + 1 = + 2 2015 + 3 20152 + + 3 2015 + + + Finally, we need to find the product a2015 a2016 = B: = (A) (A + 2 2015 + 3 20152 + + 3 2015 + + + 1) = A2 + (2 2015 + 3 20152 + + 3 2015 + + + 1) Since can take any value depending on the specific values of b, c, and d, and can be any integer, the possible values of can be any integer. Therefore, the possible values of a2015 a2016 are all integers. The final answer is All integers . Correct answer To solve this problem, we need to understand the concept of the \"weight\" of binary number. The weight of binary number is the count of 1s in its binary representation. For example, the binary number 1011 has weight of 3 because it has three 1s. We are asked to find the difference between the number of positive integers at most 22017 with even weight and the number of positive integers at most 22017 with odd weight. First, observe that any positive integer at most 22017 can be represented in binary with at most 2017 bits. The total number of such binary numbers is 22017, but since we are only considering positive integers, there are 22017 1 such numbers. (We subtract 1 because 0 does not have weight.) Now, consider the function that counts the number of 1s in the binary representation of number. This function can be thought of as random walk where each bit flip changes the weight by 1. The key insight here is that for any given n, the number of 1s in its binary representation (i.e., its weight) is equally likely to be either even or odd, because each bit flip changes the weight by 1 and there are an equal number of even and odd steps. Therefore, out of the 22017 1 numbers, approximately half will have even weight and half will have odd weight. Hence, the number of positive integers at most 22017 with even weight is the same as the number of positive integers with odd weight. So, the difference between these two counts is 0. The final answer is:"
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}