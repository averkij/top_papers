{
    "paper_title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities",
    "authors": [
        "Chuangtao Ma",
        "Yongrui Chen",
        "Tianxing Wu",
        "Arijit Khan",
        "Haofen Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art advances in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 9 0 0 2 . 5 0 5 2 : r Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities Chuangtao Ma1, Yongrui Chen2, Tianxing Wu2, Arijit Khan1, Haofen Wang3 1Aalborg University, Denmark, 2Southeast University, China, 3Tongji University, China {chuma, arijitk}@cs.aau.dk, {yongruichen, tianxingwu}@seu.edu.cn, haofenwang@tongji.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable performance on questionanswering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KGs role when integrating with LLMs. We systematically survey state-of-theart advances in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities."
        },
        {
            "title": "Introduction",
            "content": "Question answering (QA) plays fundamental role in artificial intelligence, natural language processing, information retrieval, and data management areas since it has wide range of applications such as text generation, chatbots, dialog generation, web search, entity linking, natural language query, factchecking, etc. The pre-trained language models (PLMs) and recent LLMs have shown superior performance in several QA tasks such as KBQA (Knowledge bases QA), KGQA (Knowledge graph QA), CDQA (Closed domain QA), etc. However, PLM and LLM-based methods are incapable of handling complex QA scenarios due to the following limitations. (1) Limited complex reasoning capability: LLMs encapsulate very limited reasoning capability since they have been pre-trained with the 1 tasks of predicting the next word in text sequence. (2) Lack of up-to-date and domain-specific knowledge: LLMs are incapable of generating accurate and up-to-date responses for domain-specific QA because they have been pre-trained on the data and knowledge with cutoff date. (3) Tendency to generate hallucinated content: LLMs usually generate hallucinated content due to the lack of factual verification and logical consistency checking. Challenges. Retrieval augmented generation (RAG) (Mao et al., 2021) is proposed for opendomain QA by retrieving the relevant contexts from large documents, and several techniques, such as graph neural networks (GNNs) (Li et al., 2025b), have been investigated to enhance the retrieval coverage from passages. Although RAG-based QA can generate better responses in comparison to NoRAGbased QA, it still has limited reasoning capacity and understanding of user interactions during complex QA. Complex QA usually involves knowledge interactions and fusion among data across modalities and sources, and an excellent understanding of complex queries and user interactions, whereas the RAG-based QA suffers lot from the following technical challenges. (1) Knowledge conflicts: Conflicts occur due to the fusion of inconsistent and overlapping knowledge between LLMs and external sources in RAG-based QA that may further tend to generate inconsistent answers. (2) Poor relevance and quality of retrieved context: The accuracy of the generated answers in RAG-based QA largely depends on the relevance and quality of the retrieved context, where irrelevant context leads to incorrect results. (3) Lack of iterative and multi-hop reasoning: RAG-based QA struggles to generate accurate and explainable answers for questions requiring global and summarized contexts due to lack of iterative. The emergence of synthesizing LLMs+KGs provides unique opportunity to address the above challenges and limitations of LLMs for knowledgeintensive tasks, e.g., complex QA (Ma et al., 2025a). The graph retrieval augmented generation (GraphRAG) (Zhang et al., 2025; Peng et al., 2024; Han et al., 2024) and knowledge graph re- (Santrieval augmented generation (KG-RAG) martin, 2024; Yang et al., 2024) have demonstrated the strengths of unifying LLMs with KGs for complex QA. These GraphRAG and KG-RAG based QA approaches introduce several modules, such as knowledge integration and fusion, reasoning guidelines, and knowledge validation and refinement, to mitigate the above challenges. Motivation. In recent years, rapidly increasing number of works on synthesizing LLMs and KGs for QA have been conducted to achieve complex QA under open-domain and long-context settings. This survey aims to address the aforementioned limitations by outlining the recent progress in integrating LLMs with KGs for complex QA, summarizing technical advances, and identifying open challenges and future research opportunities. Our survey differs from the existing surveys (details in Appendix A.1), it provides comprehensive overview of the recent advancements of QA from the perspective of the roles of KGs when synthesizing LLMs and KGs for complex QA. Taxonmy. We categorize the methodology of synthesizing LLMs and KGs and complex QA from different perspectives, and structured taxonomy (details in Appendix B) is given in Figure 1. Notably, these categories, divided from different perspectives, are non-exclusive, where work might be classified into multiple categories from different perspectives. The online resources of this survey can be accessed via Github.1 Contributions. Considering the popularity and mainstream adoption of both LLMs and KGs, and their wide applications in QA, our survey is timely. The contributions of this work are summarized be- (1) We introduce the structured taxonomy low. that categorizes state-of-the-art (SOTA) works on synthesizing LLMs+KGs for QA. (2) We review the SOTA works on synthesizing LLMs and KGs for QA in various categories and discuss the recent advanced topics in this field. (3) We create the alignments between various approaches in synthesizing LLMs and KGs and complex QA and highlight how these approaches address the specific challenges of different complex QA; (4) We discuss the current challenges and research oppor1https://github.com/machuangtao/LLM-KG4QA/ tunities on synthesizing LLMs and KGs for QA."
        },
        {
            "title": "2 Complex QA",
            "content": "The methodology in synthesizing LLMs and KGs for QA has been exploited as follows."
        },
        {
            "title": "2.1 Multi-document QA",
            "content": "Multi-document QA refers to the QA over contexts from multiple documents, while efficiently and effectively retrieving the relevant knowledge from multiple contexts is the main technical challenge. To reduce the retrieval latency and improve the quality of the retrieved context for multidocument QA, KGP (Wang et al., 2024d) introduces an LLM-based graph traversal agent for retrieving relevant knowledge from KG. Similarly, CuriousLLM (Yang and Zhu, 2025) integrates knowledge graph prompting, reasoning-infused LLM agent, and graph traversal agent to augment LLMs for multi-document QA. VisDom (Suri et al., 2024) introduces novel multimodal RAG for multi-document question answering by integrating and fusing the multi-modal knowledge and leveraging the (Chain-of-thought) CoT-based reasoning."
        },
        {
            "title": "2.2 Multi-modal QA",
            "content": "Multi-modal QA refers to the QA over multi-modal data, and visual QA (VQA) is one of the typical multi-modal QA. To retrieve the most relevant knowledge from the external KG for enhancing VQA, MMJG (Wang et al., 2022) introduces an adaptive knowledge selection to jointly select knowledge from visual and text knowledge based on the knowledge-aware attention and multi-modal guidance. To effectively retrieve the evidence from multi-modal data, RAMQA (Bai et al., 2025) enhances multi-modal retrieval-augmented QA by integrating learning-to-rank with training of generative models via multi-task learning. KVQA (Dong et al., 2024b) integrates LLMs with multimodal knowledge by using two-stage prompting and pseudo-siamese graph medium fusion to balance intra-modal and inter-modal reasoning."
        },
        {
            "title": "2.3 Multi-hop QA",
            "content": "Multi-hop QA differs from simple QA, it usually involves multi-step reasoning to generate the final answers. The basic idea is to decompose the multihop questions into multiple single-hop questions, then generate the answers for each single-hop question, and finally integrate them (Linders and Tomczak, 2025). For instance, GraphLLM (Qiao et al., 2 Multi-doc QA 2.1 KGP (Wang et al., 2024d); CuriousLLM (Yang and Zhu, 2025); VisDom (Suri et al., 2024) Multi-modal QA 2.2 MMJG (Wang et al., 2022); RAMQA (Bai et al., 2025); KVQA (Dong et al., 2024b) Muti-hop QA 2.3 GraphLLM (Qiao et al., 2024); HOLMES (Panda et al., 2024); GMeLLo (Chen et al., 2024a) Complex QA 2 Muti-run and Conversational QA 2.4 CoRnNetA (Liu et al., 2024b); LLMs-Graph-Reasoning (Chen et al., 2024a); SELF-multiRAG (Roy et al., 2024) Explainable QA 2. RoHT (Zhang et al., 2023); EXPLAIGNN (Christmann et al., 2023); RID (Feng et al., 2025) Temporal QA 2.6 TimeR4 (Qian et al., 2024); GenTKGQA (Gao et al., 2024); KG-IRAG (Yang et al., 2025) KGs as Background Knowledge 3.1 Knowledge Integration and Fusion 3.1.1 InfuserKI (Wang et al., 2024a); KEFF (Zhao et al., 2025a); KnowLA (Luo et al., 2024b); KGAdapter (Tian et al., 2024); GAIL (Zhang et al., 2024d) Retrieval Augmented Generation 3.1.2 PLRTQA (Alawwad et al., 2025); GNN-Ret (Li et al., 2025b); GRAG (Hu et al., 2024b); LEGO-GraphRAG (Cao et al., 2024); KG-RAG (Xu et al., 2024); KG2RAG (Zhu et al., 2025) Offline KG Guidelines 3.2.1 SR (Zhang et al., 2022); InfuserKI (Wang et al., 2024a); KnowLA (Luo et al., 2024b); KGAdapter (Tian et al., 2024); GAIL (Zhang et al., 2024d); KELDaR (Li et al., 2024b) KGs as Reasoning Guidelines 3.2 Online KG Guidelines 3.2. Oreo (Hu et al., 2022); KBIGER (Du et al., 2022); LLM-ARK (Huang, 2023); ToG (Sun et al., 2024a); ToG-2 (Ma et al., 2025b); KG-CoT (Zhao et al., 2024b) Agent-based KG Guidelines 3.2.3 KG-Agent (Jiang et al., 2024); ODA (Sun et al., 2024b); GREASELM (Zhang et al., 2021) KGs as Refiners and Validators 3.3 Advancements 3.4 KG-Driven Filtering and Validation 3.3. ACT-Selection (Salnikov et al., 2023); Q-KGR (Zhang et al., 2024c); KG-Rank (Yang et al., 2024); KGR (Guan et al., 2024) KG-Augmented Output Refinement 3.3.2 Hybrid Method 3.4.1 EFSUM (Ko et al., 2024); InteractiveKBQA (Xiong et al., 2024); LPKG (Wang et al., 2024b) KG-RAG (Sanmartin, 2024); LongRAG (Zhao et al., 2024a); KG-Rank (Yang et al., 2024); FRAG (Zhao, 2024); KGQA (Ji et al., 2024) Optimization 3.4. PG-RAG (Liang et al., 2024b); KGP (Wang et al., 2024d); SPOKE KG-RAG (Soman et al., 2024) o p e l K M e e g e L Figure 1: Structured Taxonomy of Synthesizing LLMs and KGs for QA. 2024) leverages LLMs to decompose the multihop question into several simple sub-questions and retrieve the sub-graphs via GNNs and LLMs to generate the answers for sub-questions based on graph reasoning. HOLME (Panda et al., 2024) utilizes context-aware retrieved and pruned hyperrelational KG that is constructed based on the entity-document graph to enhance LLMs for generating the answers of multi-hop QA. To enable accurate fact retrieval and reasoning of LLMs for multi-hop QA, GMeLLo (Chen et al., 2024a) effectively integrates the explicit knowledge of KGs with the linguistic knowledge of LLMs by introducing the fact triple extraction, relation chain extraction, and query and answer generation."
        },
        {
            "title": "2.4 Multi-run and Conversational QA",
            "content": "The challenge of multi-run and conversational QA lies in how to make the language model (LM) easily understand the questions and intermediate interactions. To make user interactions easily understood by machines, CoRnNetA (Liu et al., 2024b) introduces LLM-based question reformulations, reinforcement learning agents, and soft reward mechanisms to improve the interpretation of multi-turn interactions with KGs. The conversational QA involves several multi-run QA to refine and get accurate answers through multiple rounds of interactions. The knowledge aggregation module and graph reasoning are introduced for joint reasoning between the graph and LLMs (Jain and Lapata, 2024) to address the challenges of understanding the question and context for conversational QA. To improve the contextual understanding and the answer quality for conversational QA, SELF-multiRAG (Roy et al., 2024) leverages LLMs to retrieve from the summarized conversational history and reuse the retrieved knowledge for augmentation."
        },
        {
            "title": "2.5 Explainable QA",
            "content": "Explainable QA (XQA) aims to provide explanations for the generated answers based on the reasoning over the factual KGs. To effectively integrate the multiple sources of knowledge for XQA, RoHT (Zhang et al., 2023) introduces two-stage XQA method that implements the probabilistic reasoning based on the constructed Hierarchical Question Decomposition Tree (HQDT) from the aggregated knowledge. To trace the provenance and improve the explainability of the answers, EXPLAIGNN (Christmann et al., 2023) constructs heterogeneous graph from retrieved KB knowledge and user explanations and generates explanatory evidence based on GNN with question-level attention. RID (Feng et al., 2025) directly integrates the unsupervised retrieval with LLMs based on rein3 forcement learning-driven knowledge distillation."
        },
        {
            "title": "2.6 Tempral QA",
            "content": "The challenges of temporal QA lie in fully understanding the implicit time constraints and effectively incorporating them with temporal knowledge for temporal reasoning. To improve the accuracy of LLMs in answering temporal questions, TimeR4 (Qian et al., 2024) introduces RetrieveRetriev-Rerank pipeline to augment the temporal reasoning of LLMs by temporal knowledgebased fine-tuning. Similarly, GenTKGQA (Gao et al., 2024) introduces temporal GNN and virtual knowledge indicators to capture temporal knowledge embeddings, and further dynamically integrates retrieved subgraphs into LLMs for temporal reasoning. To facilitate reasoning of LLMs with KGs, KG-IRAG (Yang et al., 2025) enables LLMs to incrementally retrieve knowledge and evaluate its sufficiency for augmenting the capabilities of LLMs in answering time-sensitive and event-based queries involving temporal dependencies."
        },
        {
            "title": "3 Approaches and Alignments",
            "content": "The strengths and limitations of the approach in synthesizing LLMs and KGs for QA are summarized (details in Table 2), and the research progress on the alignment of these approaches with the complex QA are discussed (details in Table 7)."
        },
        {
            "title": "3.1 KGs as Background Knowledge",
            "content": "KGs usually play the role of background knowledge when synthesizing LLMs for complex QA, where knowledge fusion and RAG are the main technical paradigms (comparison in Table 3)."
        },
        {
            "title": "3.1.1 Knowledge Integration and Fusion",
            "content": "Knowledge integration and fusion aim to enhance language models (LMs) by integrating unknown knowledge into LMs for QA. As shown in Figure 2, Figure 2: Knowledge Integration and Fusion. the KGs and text are aligned via local subgraph extraction and entity linking, and then fed into the cross-model encoder to bidirectionally fuse text and KG to jointly train the language models for complex QA tasks (Yasunaga et al., 2022; Zhang 4 et al., 2021). To address knowledge forgetting and noisy knowledge during knowledge integration, InfuserKI (Wang et al., 2024a) and KEFF (Zhao et al., 2025a) introduce the adaptive selection and knowledge enhancement filter, respectively, which selects the new knowledge and integrates it with LLMs. Fine-tuning LLMs with text and knowledge graphs can improve their performance of LLMs on specified tasks. For instance, KG-Adapter (Tian et al., 2024) improves parameter-efficient fine-tuning of LLMs by introducing knowledge adaptation layer to LLMs. GAIL (Zhang et al., 2024d) fine-tunes LLMs for lightweight KGQA models based on retrieved SPARQL-question pairs from KGs."
        },
        {
            "title": "3.1.2 Retrieval Augmented Generation",
            "content": "RAG serves as retrieval and augmentation mechanism, as shown in Figure 3. It first retrieves relevant knowledge from the text chunks based on vector-similarity retrieval, and then augments the LLMs by integrating the retrieved context with LLMs (Alawwad et al., 2025; Li et al., 2025b). However, the mainstream RAG methods retrieve Figure 3: Retrieval Augmented Generation. the relevant knowledge from the textual chunks, which ignores the structured information and interrelations of these textual chunks. To mitigate this limitation, Graph RAG (Hu et al., 2024b) and KGRAG (Sanmartin, 2024; Xu et al., 2024; Linders and Tomczak, 2025) are proposed. Instead of retrieving the knowledge from textual chunks, Graph RAG directly retrieves the relevant knowledge from graph data. GRAG (Hu et al., 2024b) retrieves the top-k relevant subgraphs from the textual graph and then integrates the retrieved subgraphs with the query by aggregating and aligning the graph embeddings with text vectors based on GNNs. LEGO-GraphRAG (Cao et al., 2024) decomposes the retrieval process into subgraph-extraction, pathfiltering, and path-refinement modules, thereby improving the reasoning capabilities of LLMs with retrieved knowledge. KG2RAG (Zhu et al., 2025) retrieves the relevant subgraph from KG and expanding the textual chunks with the retrieved KG for augmenting the generation."
        },
        {
            "title": "3.1.3 Aligning with Complex QA",
            "content": "In the approaches of synthesizing LLMs and KGs for QA where KG plays the role of background knowledge, the retrieved relevant knowledge from the factual knowledge graph can reconcile knowledge conflicts from the multiple documents for multiple-doc QA (Wang et al., 2024d). The crossmodal reasoning can facilitate the cross-modal interaction and alignment for multi-modal QA (Suri et al., 2024). Additionally, the question decomposition of multi-hop QA can be augmented by fusing the knowledge from LLMs and KGs, which further facilitates iterative reasoning for generating the accurate final question (Saleh et al., 2024; Cao et al., 2024). The RAG (Roy et al., 2024) and KG-RAG (Sanmartin, 2024) can also improve the capabilities of LLMs in understanding the users interactions for generating accurate answers for multi-run and conversational QA. However, the key technical challenge behind this methodology is how to retrieve the relevant knowledge from large-scale KGs and then effectively fuse with LLMs without inducing knowledge conflicts?"
        },
        {
            "title": "3.2 KGs as Reasoning Guidelines",
            "content": "KGs can provide reasoning guidelines for LLMs to access precise knowledge from factual evidence based on reasoning. Recent methods (comparison in Table 4) for integrating the reasoning of KG and LLMs can be classified into three categories."
        },
        {
            "title": "3.2.1 Offline KG Guidelines",
            "content": "In offline KG guidelines, KG supplies potential paths or subgraphs before the reasoning process by the LLM, from which the LLM selects the most relevant path for reasoning. For instance, SR (Zhang et al., 2022) trains subgraph retriever that operates independently from the downstream reasoning process, while Keqing (Wang et al., 2023) decomposes complex questions using predefined templates, retrieves candidate entities and triples from KG. EtD (Liu et al., 2024a) initially uses GNN to identify promising candidates and extracts relevant fine-grained knowledge pertinent to the questions, and then creates knowledge-enhanced multiplechoice prompt to guide LLM for generating the final answer. Recent studies have been exploring the application of novel formats of guidelines. GCR (Luo et al., 2024a) first converts KG into KG-Trie and then develops graph-constrained decoding and lightweight LLM to generate multiple reasoning paths and candidate answers. KELDaR (Li et al., 2024b) introduces the question decomposition and atomic retrieval modules to extract implicit information and retrieves the relevant subgraphs from KG to augment LLM for QA."
        },
        {
            "title": "3.2.2 Online KG Guidelines",
            "content": "This paradigm emphasizes that the guidance of the KG directly involves the LLMs reasoning process, rather than merely relying on existing static knowledge. For example, Oreo (Hu et al., 2022) uses contextualized random walk across KG and conducts single step of reasoning through the specific layers. KBIGER (Du et al., 2022) considers the (k-1)-th reasoning graph to construct the k-th instruction for reasoning and corrects erroneous predictions of intermediate entities. LLMARK (Huang, 2023) approaches reasoning tasks as sequential decision-making processes and employs Proximal Policy Optimization (PPO) for optimization. ToG (Sun et al., 2024a) allows LLMs to iteratively perform beam search over KGs, by which the most promising reasoning paths and the most likely reasoning outcomes are generated. In contrast, ToG-2 (Ma et al., 2025b) utilizes entities as intermediaries to guide LLMs toward precise answers based on iterative retrieval between documents and KGs. KG-CoT (Zhao et al., 2024b) leverages external KGs to generate reasoning paths for joint reasoning of LLMs and KGs to enhance the reasoning capabilities of LLMs for QA."
        },
        {
            "title": "3.2.3 Agent-based KG Guidelines",
            "content": "KGs can also be integrated into the reasoning process of LLMs as component within an Agent system, as shown in Figure 4. This integration alFigure 4: KGs as Reasoning Guidelines. lows the Agent to leverage structured knowledge for augmenting the decision-making and problemsolving capabilities of LLMs. KG-Agent (Jiang et al., 2024) is multifunctional toolbox integrating LLMs with KG-based executor and knowledge 5 memory system that autonomously selects tools and updates the memory to enhance the reasoning of LLMs over KGs. ODA (Sun et al., 2024b) incorporates KG reasoning capabilities through global observation approach, which improves reasoning abilities by employing cyclical paradigm of observation, action, and reflection. GREASELM (Zhang et al., 2021) effectively integrates encoded representations from LMs and GNNs by introducing several modality interaction layers to seamlessly blend structured knowledge with language contexts."
        },
        {
            "title": "3.2.4 Aligning with Complex QA",
            "content": "The approaches incorporating the KGs with LLMs can enable multi-hop and iterative reasoning over the factual graph of KGs and further augment the reasoning capability of LLMs for complex QA. The challenges of knowledge retrieval, knowledge conflicts, interactions across modalities, and complex question decomposition in multi-document QA, multi-modal QA, and multi-hop QA can be easily mitigated based on joint reasoning over the factual KGs and LLMs (Suri et al., 2024; Liang et al., 2025; Qiao et al., 2024). Moreover, the joint reasoning over the factual KGs and LLMs provides the logical inference chains and anchor for LLMs to generate explainable answers with clear evidence from factual KGs (Zhao et al., 2024b). Although the joint reasoning over the factual KGs and LLMs can address the challenges of complex QA well, joint KGLLM reasoning remains inefficient because large-scale graph traversal is computationally intensive and time-consuming. Moreover, the reasoning capabilities of KGs mainly depend on the completeness and knowledge coverage of KGs, where the incomplete, inconsistent, and outdated knowledge from KGs might induce noise or conflicts. The main challenge lies in how to improve the reasoning efficiency over the large-scale graph and reasoning capabilities under incomplete KG?"
        },
        {
            "title": "3.3 KGs as Refiners and Validators",
            "content": "The factual evidence from KGs enables LLMs to refine and verify the intermediate answers, as shown in Figure 5. In these methods (comparison in Table 5), KGs act as refiner and validator."
        },
        {
            "title": "3.3.1 KG-Driven Filtering and Validation",
            "content": "KGs enhance the accuracy and reliability of LLM outputs by filtering and validating candidate answers with structured and verified information. For instance, ACT-Selection (Salnikov et al., 2023) filFigure 5: KGs as Refiners and Validators. ters and re-ranks answer candidates based on their types extracted from Wikidata. Q-KGR (Zhang et al., 2024c) improves the reasoning capabilities of LLMs by filtering out the irrelevant knowledge based on the ranking of the relevance score between the question and knowledge. KGs can improve the factual accuracy of generated answers, as demonstrated by KG-Rank (Yang et al., 2024), which integrates medical KGs with re-ranking techniques to enhance the credibility of generated responses. Moreover, KGR (Guan et al., 2024) autonomously extracts and validates factual statements, significantly boosting performance on factual QA."
        },
        {
            "title": "3.3.2 KG-Augmented Output Refinement",
            "content": "Integrating KGs with LLMs is essential to refining the outputs of LLMs for greater clarity and accuracy. EFSUM (Ko et al., 2024) employs LLM as fact summarizer to generate relevant summaries from KGs, thereby improving performance in zeroshot QA. InteractiveKBQA (Xiong et al., 2024) enables iterative knowledge interactions, allowing LLMs to generate logical forms and refine outputs based on user feedback. LPKG (Wang et al., 2024b) fine-tunes LLMs with KG-derived planning data to enhance the planning capabilities of LLMs for sophisticated reasoning in complex QA."
        },
        {
            "title": "3.3.3 Aligning with Complex QA",
            "content": "The approaches that leverage the retrieved factual evidence from KGs for refinement and validation are designed to augment the capability of LLMs in understanding user interactions and verifying the intermediate reasoning for multi-hop QA (Chen et al., 2024a) and multi-run and conversational QA (Xiong et al., 2024), respectively. However, the common case is that the knowledge from KGs is not available or not enough to verify the intermediate results, since the factual knowledge from KGs might be incomplete (Zhou et al., 2025). In particular, the knowledge conflicts between the intermediate answer and KG facts might induce irrelevant re6 sults due to the poorly verified intermediate results. Meanwhile, the refinement and validation of results largely depend on the correctness, timeliness, and completeness of factual knowledge in KGs. Thereby, the main challenges behind this methodology are how to handle the knowledge conflicts between the intermediate answer and KG facts and incrementally update the KGs to ensure factual knowledge in KGs is up-to-date and correct?"
        },
        {
            "title": "3.4 Advancements",
            "content": "The advanced methods can be categorized into two subcategories: hybrid methods and optimization."
        },
        {
            "title": "3.4.1 Hybrid Method",
            "content": "In addition to acting as single role, the approaches (comparison in Table 6) where KGs serve multiple roles when synthesizing with LLMs for QA have been investigated. For instance, KG-RAG (Sanmartin, 2024) introduces the Chain-of-Explorations (CoE) to rank and obtain the top-k nodes or relationships based on vector similarity, by which the most relevant knowledge is selected and then fed into LLMs for the final answer. LongRAG (Zhao et al., 2024a) retrieves the top-k most relevant chunks based on hybrid retriever and analyzes their relevance with the query by introducing CoT-guided filter. Furthermore, the KG-augmented prompting is introduced to LLMs for augmenting the generation of the final answer. In KGRank (Yang et al., 2024), multiple ranking methodsare introduced to refine the retrieved triples for augmenting the reasoning with the most relevant knowledge. FRAG (Zhao, 2024) introduces reasoning-aware and flexible-retrieval modules to retrieve reasoning paths from KGs, thereby guiding and augmenting LLM for efficient reasoning and answer generation. KGQA (Ji et al., 2024) combines the CoT-based prompting with graph retrieval to improve the retrieval quality and the multi-hop reasoning capability of LLMs for QA."
        },
        {
            "title": "3.4.2 Optimization",
            "content": "To mitigate the low efficiency and high computing costs of existing methods in unifying LLMs and KGs for complex QA, several optimization techniques (extension in Appendix D) are proposed to improve the efficiency of synthesizing LLMs and KGs. (1) Index-based optimization. It aims to accelerate the process of learning embeddings and vector storage for given questions and knowledge context. For instance, PG-RAG (Liang et al., 2024b) proposes dynamic and adaptable knowledge retrieval indexes based on LLMs that can effectively handle complex queries and improve the overall performance of RAG systems in QA tasks. (2) Prompt-based optimization. It mainly enhances the quality of the prompts based on prompt engineering. For example, KGP (Wang et al., 2024d) proposes KG prompting approach to enhance the prompt for LLMs and optimize the knowledge retrievalby introducing the KG construction module and LLM-based graph traversal agent. (3) Costbased optimization. It aims to minimize computation costs by reducing the number of calls to LLMs and accelerating knowledge retrieval. In particular, SPOKE KG-RAG (Soman et al., 2024) proposes token-based optimized KG-RAG framework that integrates explicit and implicit knowledge from KG with LLM to enhance LLMs for cost-effective QA."
        },
        {
            "title": "3.4.3 Aligning with Complex QA",
            "content": "The hybrid method that involves the multiple roles of KG when synthesizing LLMs and KGs can address the limitations and challenges of complex QA well. The combination of knowledge fusion, RAG, CoT-based reasoning, and ranking-based refinement is capable of accelerating the complex question decomposition (Wang et al., 2023) for multi-hop QA, enhancing the context understanding for multi-run and conversation QA (Roy et al., 2024), facilitating the interactions across modalities for multi-modal QA (Dong et al., 2024b), and improving the explainability of the generated answers (Christmann et al., 2023). However, the remaining challenge is how to achieve efficient vector indexing and search over large-scale KGs and make balance between the cost and performance?"
        },
        {
            "title": "4 Evaluation and Application",
            "content": "We summarize the metrics & dataset and showcase the related applications and demos."
        },
        {
            "title": "4.1 Evaluation and Benchmark Dataset",
            "content": "The metrics and benchmark datasets are summarized and compared (details in Appendix E.1). Metrics. The metrics for evaluating the approach in synthesizing LLMs with KGs for QA are summarized: (1) Answer Quality (AnsQ); (2)Retrieval Quality (RetQ); (3) Reasoning Quality (ReaQ). Benchmark Dataset. The recent benchmark datasets are summarized and compared (details in Table 8) with the focus on Answer Quality, Retrieval Quality, and Reasoning Quality."
        },
        {
            "title": "4.2 Applications",
            "content": "We showcase the industrial and scientific applications and demos (details in Appendix E.2) in synthesizing LLMs with KGs for QA."
        },
        {
            "title": "5 Open Challenges and Opportunities",
            "content": "We summarize the challenges by highlighting the opportunities and discussing the future direction. Scaling to both Effectiveness and Efficiency. LLM+KG systems retrieve the facts and perform multi-hop reasoning under tight latency and memory budgets. Three bottlenecks are emerging: (1) Structure-aware retrieval: Vanilla dense or sparse retrieval treats KG as an unordered triples, thereby discarding topological cues that are vital for pruning the search space (Tian et al., 2025). Hierarchical graph partitioning, dynamic neighbourhood expansion, and learned path-prior proposal networks are promising ways to expose structure to the retriever while keeping the index sub-linear. (2) Amortized reasoning: Current prompting pipelines repeatedly query the KG for every beam or CoT step. Caching subgraphs, reusing intermediate embeddings, and exploiting incremental-compute friendly hardware can mitigate the quadratic blow-up of iterative reasoning. (3) Lightweight answer validation: Most guardrails rely on large LLMs, where probabilistic logic programs, or bloom filter sketches could provide on-device verification with O(1) additional parameters. An opportunity is to design the retriever and validator that uncertainty estimates from the former guide selective execution of the latter. Knowledge Alignment and Dynamic Integration. Once KG snapshot is injected into an LLM, it starts to become outdated. Just like real-world KGs usually involve adding new entities, deleting relations, and resolving contradictions. Future work should: (1) Quantify alignment: We lack metrics that score not only semantic overlap but also structural compatibility between parametric knowledge in the LLM and symbolic knowledge in the KG. Contrastive probing with synthetic counterfactuals or topology-aware alignment losses may fill this gap. (2) Facilitate real-time updates: Parameter-efficient tuning (e.g. LoRA modules keyed by graph deltas) and retrieval-time patching (streaming KGs with temporal indices) are early steps toward stream-time knowledge alignment. (3) Detect and resolve conflicts: Bayesian trust networks, source-aware knowledge distillation, and multi-agent debate protocols can estimate and reconcile confidence scores across modalities and sources. Incorporating these into the decoding objective is an open challenge with high pay-off. Explainable and Fairness-Aware QA. The scale of LLMs poses challenges to explainability and fairness in QA. While integrating KGs offers path toward interpretable reasoning, it also introduces computational challenges and fairness concerns. Future work may consider the following directions: (1) Reasoning over subgraphs: Retrieving subgraphs from large-scale KGs is computationally expensive and often results in overly complex or incomprehensible explanations. Structure-aware retrieval and reranking methods should be employed to identify subgraphs consistent with the gold paths. Furthermore, CoT-based prompting can be used to guide LLMs in generating explicit reasoning steps grounded in the retrieved subgraphs. (2) Fairnessaware knowledge retrieval: LLMs can capture social biases from training data, but KGs may contain incomplete or biased knowledge. As result, the fairness concerns remain in RAG (Wu et al., 2024b). Incorporating fairness-aware techniques into KG retrieval (e.g., reranking based on bias detection) and integrating them with counterfactual prompting can mitigate bias. (3) Multi-turn QA: Single-turn QA restricts the exploration of diverse perspectives and limits the exploration of reasoning processes. Developing multi-turn QA with retrieval strategies that can dynamically detect and adjust for bias and improve further the explainability and fairness through multi-turn interactions."
        },
        {
            "title": "6 Conclusion",
            "content": "This survey has systematically examined the synthesis of LLMs and KGs in QA, presenting novel taxonomy that categorizes methodologies based on QA types and the role of KGs. Our analysis highlights the strengths and limitations of current approaches, emphasizing the potential opportunities of leveraging the KGs to augment LLMs to overcome challenges such as limited reasoning capabilities, outdated knowledge, and hallucinations in complex QA scenarios. Despite significant advancements, several remaining challenges include efficient reasoning, knowledge alignment, and explainable QA. Future research should focus on developing adaptive frameworks that dynamically integrate up-to-date knowledge with LLMs, as well as establishing efficient methods for scaling reasoning, explainability, and fairness."
        },
        {
            "title": "Limitations",
            "content": "This survey covers the taxonomy and summary of the recent advancements in synthesizing LLMs and KGs for QA and the challenges of complex QA, however, we are aware that this survey may miss some newly released works due to the rapid expansion of works on this topic. Moreover, the survey mainly highlights the alignments between the recent methodologies of incorporating LLMs and KGs for QA and the challenges of the various complex QA tasks, while these taxonomies from different perspectives are non-exclusive, and the overlap between the two taxonomies may arise. Furthermore, this survey underemphasizes the quantitative and experimental evaluation of different methodologies due to the various implementation details, the diversity of the benchmark datasets, and nonstandardized evaluation metrics. References Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali Alkhathlan, and Amani Jamal. 2025. Enhancing textbook question answering task with large language models and retrieval augmented generation. Pattern Recognit., 162:111332. Yang Bai, Christan Earl Grant, and Daisy Zhe Wang. 2025. RAMQA: unified framework for retrieval-augmented multi-modal question answering. arXiv:2501.13297. Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2023. Prompting is programming: query language for large language models. Proc. ACM Program. Lang., 7:19461969. Yuzheng Cai, Zhenyue Guo, Yiwen Pei, Wanrui Bian, and Weiguo Zheng. 2024. SimGRAG: Leveraging similar subgraphs for knowledge graphs driven arXiv preprint retrieval-augmented generation. arXiv:2412.15272. Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, and Kevin Zhou. 2024. LEGO-GraphRAG: Modularizing graph-based retrieval-augmented generation for design space exploration. arXiv:2411.05844. Ruirui Chen, Weifeng Jiang, Chengwei Qin, Ishaan Rawal, Cheston Tan, Dongkyu Choi, Bo Xiong, and Bo Ai. 2024a. LLM-based multi-hop question answering with knowledge graph integration in evolving environments. In EMNLP, pages 1443814451. Weijie Chen, Ting Bai, Jinbo Su, Jian Luan, Wei Liu, and Chuan Shi. 2024b. KG-Retriever: Efficient knowledge indexing for retrieval-augmented large language models. arXiv:2412.05547. Zichen Chen, Jianda Chen, Ambuj Singh, and Misha Sra. 2024c. XplainLLM: knowledge-augmented dataset for reliable grounded explanations in LLMs. In EMNLP, pages 75787596. Philipp Christmann, Rishiraj Saha Roy, and Gerhard Weikum. 2023. Explainable conversational question answering over heterogeneous sources via iterative graph neural networks. In SIGIR, pages 643653. Philipp Christmann and Gerhard Weikum. 2024. RAGbased question answering over heterogeneous data and text. arXiv:2412.07420. Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco. 2023. Complex QA and language models hybrid architectures, survey. arXiv:2302.09051. Junnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, and Xiao Huang. 2024a. Costefficient knowledge-based question answering with large language models. In NeurIPS. Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, and Xiao Huang. 2024b. Modalityaware integration with large language models for knowledge-based visual question answering. In ACL, pages 24172429. Haowei Du, Quzhe Huang, Chen Zhang, and Dongyan Zhao. 2022. Knowledge-enhanced iterative instruction generation and reasoning for knowledge base question answering. In NLPCC, pages 431444. Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated evaluation of retrieval augmented generation. In EACL, pages 150158. Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. 2025. MiniRAG: Towards extremely simple retrieval-augmented generation. arXiv:2501.06713. Jianzhou Feng, Qin Wang, Huaxiao Qiu, and Lirong Liu. 2025. Retrieval in decoder benefits generative models for explainable complex question answering. Neural Networks, 181:106833. Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, and Dongsheng Li. 2024. Two-stage generative question answering on temporal knowledge graph using large language models. In ACL Findings, pages 67196734. Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang, and Xin Wang. 2024. PokeMQA: Programmable knowledge editing for multi-hop question answering. In ACL, pages 8069 8083. Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. 2024. Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting. In AAAI, pages 1812618134. 9 Willis Guo, Armin Toroghi, and Scott Sanner. 2024. CR-LT-KGQA: knowledge graph question answering dataset requiring commonsense reasoning and long-tail knowledge. arXiv:2403.01395. Vivek Gupta, Pranshu Kandoi, Mahek Vora, Shuo Zhang, Yujie He, Ridho Reinanda, and Vivek Srikumar. 2023. TempTabQA: Temporal question answerIn EMNLP, pages ing for semi-structured tables. 24312453. Yixin Ji, Kaixin Wu, Juntao Li, Wei Chen, Mingjie Zhong, Xu Jia, and Min Zhang. 2024. Retrieval and reasoning on KGs: Integrate knowledge graphs into large language models for complex question answering. In EMNLP, pages 75987610. Zhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, and Gerhard Weikum. 2021. Complex temporal question answering on knowledge graphs. In CIKM, pages 792802. Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan Rossi, Subhabrata Mukherjee, Xianfeng Tang, and 1 others. 2024. Retrieval-augmented generation with graphs (GraphRAG). arXiv:2501.00309. Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong Wen. 2024. Kg-Agent: An efficient autonomous agent framework for complex reasoning over knowledge graph. arXiv:2402.11163. Jie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Pan. 2024. MINTQA: multi-hop question answering benchmark for evaluating LLMs on new and tail knowledge. arXiv:2412.17032. Aidan Hogan, Xin Luna Dong, Denny Vrandeˇcic, and Gerhard Weikum. 2025. Large language models, knowledge graphs and search engines: crossroads for answering users questions. arXiv:2501.06699. Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong Wu, and Jeff Pan. 2024a. Benchmarking large language models in complex question answering attribution using knowledge graphs. arXiv:2401.14640. Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024b. GRAG: Graph retrieval-augmented generation. arXiv:2405.16506. Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou Sun. 2022. Empowering language models with knowledge graph reasoning for question answering. In EMNLP, pages 95629581. Pengcheng Huang, Zhenghao Liu, Yukun Yan, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, and Chenyan Xiong. 2025. PIPKAG: Mitigating knowledge conflicts in knowledgeaugmented generation via parametric pruning. arXiv preprint arXiv:2502.15543. Yuxuan Huang. 2023. Evaluating and enhancing large language models for conversational reasoning on knowledge graphs. arXiv:2312.11282. Nourhan Ibrahim, Samar Aboulela, Ahmed Ibrahim, and Rasha Kashef. 2024. survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges. Discov. Artif. Intell., 4(1):76. Parag Jain and Mirella Lapata. 2024. Integrating large language models with graph-based reasoning for conversational question answering. arXiv:2407.09506. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Appl. Sci., 11(14):6421. Amanda Kau, Xuzeng He, Aishwarya Nambissan, Aland Astudillo, Hui Yin, and Amir Aryani. 2024. Combining knowledge graphs and large language models. arXiv:2407.06564. Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, and Dongha Lee. 2024. Evidence-focused fact summarization for knowledge-augmented zeroshot question answering. In EMNLP, pages 10636 10651. Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 2023. BioASQQA: manually curated corpus for biomedical question answering. Scientific Data, 10(1):170. Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, and Arman Cohan. 2024a. M3SciQA: multi-modal multi-document scientific QA benchmark for evaluating foundation models. In EMNLP, pages 1541915446. Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, and Yongjian Cui. 2025a. CoT-RAG: Integrating chain of thought and retrieval-augmented generation to enhance reasoning in large language models. arXiv preprint arXiv:2504.13534. Yading Li, Dandan Song, Changzhi Zhou, Yuhang Tian, Hao Wang, Ziyi Yang, and Shuhao Zhang. 2024b. framework of knowledge graph-enhanced large language model based on question decomposition In EMNLP Findings, pages and atomic retrieval. 1147211485. Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, and Rui Wang. 2025b. Graph neural network enhanced retrieval for question answering of large language models. In NAACL, pages 6612 6633. 10 Jianxin Liang, Xiaojun Meng, Huishuai Zhang, Yueqian Wang, Jiansheng Wei, and Dongyan Zhao. 2025. ReasVQA: Advancing VideoQA with imperfect reasoning process. arXiv:2501.13536. Jannat Ara Meem, Muhammad Shihab Rashid, Yue Dong, and Vagelis Hristidis. 2024. PAT-questions: self-updating benchmark for present-anchored temporal question-answering. In ACL, pages 1312913148. Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, and 1 others. 2024a. KAG: Boosting LLMs in professional domains via knowledge augmented generation. arXiv preprint arXiv:2409.13731. Xun Liang, Simin Niu, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, and Chenyang Xi. 2024b. Empowering large language models to set up knowledge retrieval indexer via self-learning. arXiv:2405.16933. Jasper Linders and Jakub Tomczak. 2025. Knowledge graph-extended retrieval augmented generarXiv preprint ation for question answering. arXiv:2504.08893. Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. 2024a. Explore then determine: GNN-LLM synergy framework for reasoning over knowledge graph. arXiv:2406.01145. Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, and Hanghang Tong. 2024b. Conversational question answering with language models generated reformulations over knowledge graph. In ACL, pages 839850. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS, 35:25072521. Linhao Luo, Zicheng Zhao, Chen Gong, Gholamreza Haffari, and Shirui Pan. 2024a. Graph-constrained reasoning: Faithful reasoning on knowledge graphs with large language models. arXiv:2410.13080. Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and Wei Hu. 2024b. KnowLA: Enhancing parameterefficient finetuning with knowledgeable adaptation. In NAACL, pages 71467159. Chuangtao Ma, Yongrui Chen, Tianxing Wu, Arijit Khan, and Haofen Wang. 2025a. Unifying large language models and knowledge graphs for question answering: Recent advances and opportunities. In EDBT, pages 11741177. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, and Jian Guo. 2025b. Think-on-Graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. In ICLR. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-augmented retrieval for opendomain question answering. In ACLIJCNLP, pages 40894100. Lars-Peter Meyer, Johannes Frey, Kurt Junghanns, Felix Brei, Kirill Bulert, Sabine Gründer-Fahrer, and Michael Martin. 2023. Developing scalable benchmark for assessing large language models in knowledge graph engineering. In SEMANTICS. Linyong Nan, Weining Fang, Aylin Rasteh, Pouya Lahabi, Weijin Zou, Yilun Zhao, and Arman Cohan. 2024. OMG-QA: Building open-domain multimodal generative question answering systems. In EMNLP, pages 10011015. Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. 2023. Large language models and knowledge graphs: Opportunities and challenges. Trans. Graph Data Knowl., 1(1):138. Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying large language models and knowledge graphs: roadmap. IEEE Trans. Knowl. Data Eng., 36(7):35803599. Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, and Prathosh P. 2024. HOLMES: Hyper-relational knowledge graphs for multi-hop question answering using LLMs. In ACL, pages 1326313282. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: survey. arXiv:2408.08921. Aleksandr Perevalov, Andreas Both, and Axel-Cyrille Ngonga Ngomo. 2024. Multilingual question answering systems for knowledge graphsa survey. Semantic Web, 15(5):20892124. Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui, Li Zhang, and Kehui Song. 2024. TimeR4: Time-aware retrieval-augmented large language models for temporal knowledge graph question answering. In EMNLP, pages 69426952. Zijian Qiao, Nan Li, Chenxi Huang, Gangliang Wang, Shenglin Liang, Hui Lin, and Qinglang Guo. 2024. GraphLLM: general framework for multi-hop question answering over knowledge graphs using large language models. In NLPCC, pages 136148. Springer. Nirmal Roy, Leonardo Ribeiro, Rexhina Blloshmi, and Kevin Small. 2024. Learning when to retrieve, what to rewrite, and how to respond in conversational QA. In EMNLP, pages 1060410625. 11 Ahmmad OM Saleh, Gökhan Tür, and Yucel Saygin. 2024. SG-RAG: Multi-hop question answering with large language models through knowledge graphs. In ICNLSP, pages 439448. Mikhail Salnikov, Maria Lysyuk, Pavel Braslavski, Anton Razzhigaev, Valentin Malykh, and Alexander Panchenko. 2023. Answer candidate type selection: Text-to-text language model for closed book question answering meets knowledge graphs. In KONVENS, pages 155164. Diego Sanmartin. 2024. KG-RAG: Bridging the gap between knowledge and creativity. arXiv:2405.12035. Juan Sequeda, Dean Allemang, and Bryon Jacob. 2024. benchmark to understand the role of knowledge graphs on large language models accuracy for question answering on enterprise SQL databases. In GRADES-NDA@SIGMOD/PODS, pages 112. Zhengliang Shi, Lingyong Yan, Weiwei Sun, Yue Feng, Pengjie Ren, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, Maarten de Rijke, and Zhaochun Ren. 2025. Direct retrieval-augmented optimization: Synergizing knowledge selection and language models. arXiv preprint arXiv:2505.03075. Moein Shirdel, Joel Rorseth, Parke Godfrey, Lukasz Golab, Divesh Srivastava, and Jarek Szlichta. 2025. AprèsCoT: Explaining LLM answers with knowledge graphs and chain of thought. In EDBT, pages 11421145. Karthik Soman, Peter Rose, John Morris, Rabia Akbas, Brett Smith, Braian Peetoom, Catalina Villouta-Reyes, Gabriel Cerono, Yongmei Shi, Angela Rizk-Jackson, and 1 others. 2024. Biomedical knowledge graph-optimized prompt generation for large language models. Bioinformatics, 40(9):btae560. Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, and Xiuying Chen. 2025. Injecting domain-specific knowledge into large language models: comprehensive survey. arXiv preprint arXiv:2502.10708. Daniel Steinigen, Roman Teucher, Timm Heine Ruland, Max Rudat, Nicolas Flores-Herr, Peter Fischer, Nikola Milosevic, Christopher Schymura, and Angelo Ziletti. 2024. Fact Finderenhancing domain expertise of large language models by inarXiv preprint corporating knowledge graphs. arXiv:2408.03010. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, HeungYeung Shum, and Jian Guo. 2024a. Think-on-Graph: Deep and responsible reasoning of large language model with knowledge graph. In ICLR. Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. 2024b. ODA: Observation-driven agent for integratIn ACL, pages ing LLMs and knowledge graphs. 74177431. Manan Suri, Puneet Mathur, Franck Dernoncourt, Kanika Goswami, Ryan Rossi, and Dinesh Manocha. 2024. VisDoM: Multi-document QA with visually rich elements using multimodal retrievalaugmented generation. arXiv:2412.10704. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In NAACL, pages 41494158. Dehao Tao, Feng Huang, Yongfeng Huang, and Minghu Jiang. 2024. Clue-Guided Path Exploration: Optimizing knowledge graph retrieval with large language models to address the information black box challenge. arXiv:2401.13444. Wen tau Yih, Matthew Richardson, Chris Meek, MingWei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In ACL, pages 201206. Shiyu Tian, Yangyang Luo, Tianze Xu, Caixia Yuan, Huixing Jiang, Chen Wei, and Xiaojie Wang. 2024. KG-Adapter: Enabling knowledge graph integration in large language models through parameter-efficient fine-tuning. In ACL, pages 38133828. Shiyu Tian, Shuyue Xing, Xingrui Li, Yangyang Luo, Caixia Yuan, Wei Chen, Huixing Jiang, and Xiaojie Wang. 2025. systematic exploration of knowledge graph alignment with large language models in retrieval augmented generation. In AAAI, pages 2529125299. Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun Wang, Lei Feng, and Bo An. 2023. keqing: knowledge-based question answering is nature chain-of-thought mentor of LLM. arXiv:2401.00426. Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, and Haifeng Chen. 2024a. Infuserki: Enhancing large language models with knowledge graphs via infuser-guided knowledge integration. In EMNLP, pages 36753688. Miao Su, ZiXuan Li, Zhuo Chen, Long Bai, Xiaolong Jin, and Jiafeng Guo. 2024. Temporal knowledge graph question answering: survey. arXiv:2406.14191. Jianfeng Wang, Anda Zhang, Huifang Du, Haofen Wang, and Wenqiang Zhang. 2022. Knowledgeenhanced visual question answering with multimodal joint guidance. In IJCKG, pages 115120. Yuan Sui and Bryan Hooi. 2024. Can knowledge graphs make large language models more trustworthy? An empirical study over open-ended question answering. arXiv:2410.08085. Jinyu Wang, Jingjing Fu, Rui Wang, Lei Song, and Jiang Bian. 2025. PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation. arXiv preprint arXiv:2501.11551. 12 Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, and Huajun Chen. 2024b. Learning to plan for retrievalaugmented large language models from knowledge graphs. In EMNLP, pages 78137835. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. 2024c. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. In EMNLP, pages 56275646. Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024d. Knowledge graph prompting for multi-document question answering. In AAAI, volume 38, pages 1920619214. Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, and Kang Liu. 2023. MenatQA: new dataset for testing the temporal comprehension and reasoning abilities of large language models. In EMNLP, pages 14341447. Min Wu, Xinglu Yi, Hui Yu, Yu Liu, and Yujue Wang. 2022. Nebula graph: An open source distributed graph database. arXiv preprint arXiv:2206.07278. Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis Ioannidis, Karthik Subbian, James Zou, and Jure Leskovec. 2024a. Stark: Benchmarking LLM retrieval on textual and relational knowledge bases. NeurIPS, 37:127129127153. Xuyang Wu, Shuowei Li, Hsin-Tai Wu, Zhiqiang Tao, and Yi Fang. 2024b. Does RAG introduce unfairness in LLMs? evaluating fairness in retrievalarXiv preprint augmented generation systems. arXiv:2409.19804. Yurong Wu, Yan Gao, Bin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, and Linjun Yang. 2024c. StraGo: Harnessing strategic guidance for prompt optimization. In EMNLP, pages 1004310061. Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-augmented generation with knowledge graphs for customer service question answering. In SIGIR, pages 29052909. Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yuhe Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, and Irene Li. 2024. KG-Rank: Enhancing large language models for medical QA with knowledge In BioNLP Workgraphs and ranking techniques. shop@ACL, pages 155166. Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, and Flora Salim. 2025. KG-IRAG: knowledge graph-based iterative retrieval-augmented generation framework for temporal reasoning. arXiv preprint arXiv:2503.14234. Zukang Yang and Zixuan Zhu. 2025. CuriousLLM: Elevating multi-document QA with reasoning-infused knowledge graph prompting. In NAACL, pages 274 286. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher Manning, Percy Liang, and Jure Leskovec. 2022. Deep bidirectional language-knowledge graph pretraining. In NeurIPS, pages 3730937323. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of retrievalaugmented generation: survey. In CCF Conference on Big Data, pages 102120. Hong Qing Yu and Frank McQuade. 2025. RAG-KGIL: multi-agent hybrid framework for reducing hallucinations and enhancing llm reasoning through rag and incremental knowledge graph learning integration. arXiv preprint arXiv:2503.13514. Gongbo Zhang, Zihan Xu, Qiao Jin, Fangyi Chen, Yilu Fang, Yi Liu, Justin Rousseau, Ziyang Xu, Zhiyong Lu, Chunhua Weng, and Yifan Peng. 2024a. mapreduce approach to effectively utilize long context information in retrieval augmented language models. arXiv:2412.15271. Guanming Xiong, Junwei Bao, and Wen Zhao. 2024. Interactive-KBQA: Multi-turn interactions for knowledge base question answering with large language models. In ACL, pages 1056110582. Haozhen Zhang, Tao Feng, and Jiaxuan You. 2024b. Graph of records: Boosting retrieval augmented generation for long-context summarization with graphs. arXiv:2410.11001. Chuan Xu, Qiaosheng Chen, Yutong Feng, and Cheng Gong. 2025a. mmRAG: modular benchmark for retrieval-augmented generation over text, taarXiv preprint bles, and knowledge graphs. arXiv:2505.11180. Jiajie Zhang, Shulin Cao, Tingjian Zhang, Xin Lv, Juanzi Li, Lei Hou, Jiaxin Shi, and Qi Tian. 2023. Reasoning over hierarchical question decomposition In ACL, tree for explainable question answering. pages 1455614570. Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, and Lichao Sun. 2025b. NodeRAG: Structuring graph-based rag with heterogeneous nodes. arXiv preprint arXiv:2504.11544. Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. 2022. Subgraph retrieval enhanced model for multi-hop knowledge base question answering. In ACL. 13 Dongzhuoran Zhou, Yuqicheng Zhu, Yuan He, Jiaoyan Chen, Evgeny Kharlamov, and Steffen Staab. 2025. Evaluating knowledge graph based retrieval augmented generation methods under knowledge incompleteness. arXiv preprint arXiv:2504.05163. Andrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. 2024. FanOutQA: multi-hop, multi-document question answering benchmark for large language models. In ACL, pages 1837. Xiangrong Zhu, Yuexiang Xie, Yi Liu, Yaliang Li, and Wei Hu. 2025. Knowledge graph-guided arXiv preprint retrieval augmented generation. arXiv:2502.06864. Michael Zhang and Eunsol Choi. 2021. SituatedQA: Incorporating extra-linguistic contexts into QA. In EMNLP, pages 73717387. Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Junnan Dong, Hao Chen, Yi Chang, and Xiao Huang. 2025. survey of graph retrieval-augmented generation for customized large language models. arXiv:2501.13958. Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher Manning, and Jure Leskovec. 2021. GreaseLM: Graph reasoning enhanced language models for question answering. In ICLR. Yu Zhang, Kehai Chen, Xuefeng Bai, Zhao Kang, Quanjiang Guo, and Min Zhang. 2024c. Question-guided knowledge graph re-scoring and injection for knowledge graph question answering. In EMNLP, pages 89728985. Zhiqiang Zhang, Liqiang Wen, and Wen Zhao. 2024d. GAIL fine-tuned LLM enhanced framework for low-resource knowledge graph question answering. In CIKM, pages 33003309. Dongfang Zhao. 2024. FRAG: Toward federated vector database management for collaborative and secure retrieval-augmented generation. arXiv:2410.13272. Qi Zhao, Qi Song, Tian Xie, Haiyue Zhang, Hongyu Yang, and Xiangyang Li. 2025a. Improving pretrained language models with knowledge enhancement and filtering framework. In NAACL Findings, pages 38603871. Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, and Jie Tang. 2024a. LongRAG: dual-perspective retrieval-augmented generation paradigm for long-context question answering. In EMNLP, pages 2260022632. Ruilin Zhao, Feng Zhao, Long Wang, Xianzhi Wang, and Guandong Xu. 2024b. KG-CoT: Chain-ofthought prompting of large language models over knowledge graphs for knowledge-aware question answering. In IJCAI, pages 66426650. Xuejiao Zhao, Siyan Liu, Su-Yin Yang, and Chunyan Miao. 2025b. MedRAG: Enhancing retrievalaugmented generation with knowledge graph-elicited reasoning for healthcare copilot. In Proceedings of the ACM on Web Conference 2025, pages 44424457. Shangshang Zheng, He Bai, Yizhe Zhang, Yi Su, Xiaochuan Niu, and Navdeep Jaitly. 2024a. KGLens: Towards efficient and effective knowledge probing of large language models with knowledge graphs. arXiv:2312.11539. Xinxin Zheng, Feihu Che, Jinyang Wu, Shuai Zhang, Shuai Nie, Kang Liu, and Jianhua Tao. 2024b. KSLLM: Knowledge selection of large language models with evidence document for question answering. arXiv:2404.15660."
        },
        {
            "title": "A Related Survey and Paper Selection",
            "content": "A.1 Related Survey Previous surveys have drawn the roadmap of unifying LLMs+KGs (Pan et al., 2024), have discussed the opportunities and challenges (Pan et al., 2023) of leveraging LLMs for knowledge extraction, ontologies and KGs constructions, have summarized the integration and synthesis paradigms of LLMs and KGs (Kau et al., 2024; Ibrahim et al., 2024), and gave an overview of the knowledge injection methods between LLMs and domain-specific knowledge (Song et al., 2025). Additionally, the existing surveys give an overview of multilingual knowledge graph question answering (Perevalov et al., 2024), review temporal knowledge graph QA (Su et al., 2024), complex QA (Daull et al., 2023), and discuss search engines, KGs and LLMs from the perspective of user information seeking for QA (Hogan et al., 2025). The comparative analysis of these surveys across LLMs, KGs, LLMs+KGs, GraphRAG, and QA is given in Table 1. We summarize that the previous surveys on synthesizing LLMs and KGs for QA show limitations (1) The scope in terms of the scope and tasks. of the surveys is dedicated for general knowledgeintensive tasks, such as knowledge extraction, entity linking, KG construction and completion, and text generation, etc; (2) the QA task in the surveys is limited to the close-domain QA over the knowledge graph; (3) the survey on integrating LLMs, KGs, and search engines for answering users questions is from the perspective of user-centric information needs. A.2 Paper Selection We first retrieve research papers since 2021 by using Google Scholar and PaSa2 using the phrases knowledge graph and language model for question answering\" and KG and LLM for QA\", and extend the search scope of the benchmark dataset paper to 2019, and then screen and select them based on their relevancy and publication venue quality."
        },
        {
            "title": "B Taxonomy",
            "content": "B.1 Complex QA We divide the complex QA into the following categories according to the technical challenges. Multi-document QA. Multi-document QA (Multi-doc QA) retrieves and synthesizes rele2https://pasa-agent.ai/ vant information from various sources to provide comprehensive answer. Multi-modal QA. It refers to the QA over data and knowledge involving multiple modalities such as text, audio, images, video, etc. Muti-hop QA. It usually involves decomposing the complex question and generating the final answers based on multi-step and iterative reasoning over factual KG. Multi-run and conversational QA. It involves user interactions to determine the final answer since some complex questions can not be answered in single turn. Explainable QA. XQA (Explainable QA) not only gives the answers to the question but also provides explanations for the given answers. Temporal QA. It refers to the questions with temporal intent over the temporal KGs that have entities, relations, and associated temporal conditions. B.2 Approaches of Synthesizing LLMs and"
        },
        {
            "title": "KGs for QA",
            "content": "We categorize the methodology of synthesizing LLMs and KGs for QA based on the role of KGs. KG as Background Knowledge. When KGs are used as background knowledge to enhance LLMs for QA, the questions are parsed to identify the relevant subgraphs from KGs, then they are integrated with LLMs based on knowledge integration and GraphRAG. KGs as Reasoning Guidelines. KGs can serve as reasoning guidelines for LLMs for QA tasks by providing structured real-world facts. This factual knowledge and their reliable reasoning paths can improve the explainability of the generated answers. KGs as Refiners and Validators. KGs can be treated as refiners and validators for LLMs in QA tasks, where LLMs can verify initial answers with factual knowledge and filter out inaccurate responses by integrating KG to ensure that the final responses are precise. Hybrid Methods. The hybrid methods involve KGs multiple roles, namely, background knowledge, reasoning guidelines, refiner and validator, when synthesizing LLMs and KGs for QA. We summarize the strengths and limitations of each method of synthesizing LLMs and KGs for QA in Table 2 according to our previous taxonomy."
        },
        {
            "title": "Survey",
            "content": "KGs LLMs LLMs+KGs Graph RAG QA (cid:34) Unifying LLMs and KGs: Roadmap (Pan et al., 2024) LLMs+KGs: Opportunities and Challenges (Pan et al., 2023) (cid:34) (cid:34) Combing KGs and LLMs (Kau et al., 2024) (cid:34) mQAKG (Perevalov et al., 2024) (cid:34) TKGQA (Su et al., 2024) (cid:34) LLMs, KGs, and SEs (Hogan et al., 2025) (cid:34) Complex QA (Daull et al., 2023) (cid:34) Knowledge Injection (Song et al., 2025) [This work] (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:35) (cid:34) (cid:35) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:35) (cid:35) (cid:35) (cid:35) (cid:34) (cid:35) (cid:35) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) Table 1: Comparsion of Existing Survey Across LLMs, KGs, LLMs+KGs, GraphRAG, and QA. mKGQA: multilingual Question Answering for Knowledge Graphs, TKGQA: Temporal Knowledge Graph QA. (cid:34) Covered or Dicussed, (cid:35) Not Covered or Dicussed."
        },
        {
            "title": "C Summary and Alignment",
            "content": "C.1 Summary Tables of Approaches The detailed summarization and comparison of the different categories of approaches in terms of main techniques, language model (LM), knowledge graph (KG), dataset, QA types, and evaluation metrics are given in the following tables. KG as Background Knowledge. Table 3 gives summary and comparison of approaches in the category of KG as background knowledge. KG as Reasoning Guidelines. Table 4 gives summary and comparison of approaches in the category of KG as reasoning guidelines. KG as Refiners and Validators. Table 5 gives summary and comparison of approaches in the category KG as refiners and validators. Hybrid Approach. Table 6 gives summary and comparison of hybrid approaches. C.2 Alignment of Approaches to Complex QA The alignment of the existing approaches in synthesizing LLMs and KGs to diverse complex QA is presented in Table 7."
        },
        {
            "title": "D Extended Optimization",
            "content": "It The extended optimization techniques are summarized and discussed as follows. Index-based Optimization. is timeconsuming and complex task to create indexes from long-range facts and retrieve the relevant knowledge from large-scale graphs. To address this issue, GoR (Zhang et al., 2024b) leverages GNN and BERT score-based objectives to optimize node embeddings during graph indexing. KGRetriever (Chen et al., 2024b) leverages hierarchical index graph to enhance knowledge correlations and improve information retrieval for efficient knowledge indexing. NodeRAG (Xu et al., 2025b) integrates the heterogeneous graphs and fine-grained retrieval with RAG by optimizing the indexing of graph structures. DRO (Shi et al., 2025) proposes direct retrieval-agumentation method to directly estimate the distribution of document permutations from the select model for jointly learning the selection model and generative LM model. Prompt-based Optimization. To facilitate the deep fusion between the retrieved knowledge from KGs with the internal knowledge of LLMs, several prompt-based optimization approaches are proposed. For instance, StraGo (Wu et al., 2024c) enhances the quality and stability of the prompts based on the insights and strategic guidance learned from the historical prompts by using in-context learning. Meanwhile, several ranking strategies have been exploited to retrieve the top relevant knowledge from diverse knowledge bases, and the most relevant contexts are further fed to LLMs together with the prompts. For example, KGRank (Yang et al., 2024) leverages re-ranking techniques based on the score measuring relevance and redundancy to rank the top-k triples from KGs and then combine them with the prompt to generate the answers for QA. Similarly, KS-LLM (Zheng et al., 2024b) introduces the evidence sentence selection module to retrieve the most relevant evidence sentences based on the ranking of the Euclidean distance between the triples and each evidence sentence. Instead of direct ranking of context-based relevance, BriefContext (Zhang et al., 2024a) introduces the preflight check to predict the relevance ranking between the user query and the retrieved documents, which is divided into multiple"
        },
        {
            "title": "KG as Background Knowledge Broad Coverage\nKG as Reasoning Guidelines\nKG as Refiners and Validator\nHybrid Approach",
            "content": "Multi-hop Capabilities Hallucination Reduction Validation Latency Limitation Mitigation"
        },
        {
            "title": "High Domain Coverage",
            "content": "High Accuracy & Recency Dynamic Adaptation Table 2: Comparison of Approaches with Different Roles of KGs. Methods Techniques LM(s) KG(s) Dataset(s) QA Type Metric(s) InfuserKI (Wang et al., 2024a) Knowledge-based Fine Llama-2-7B Tuning KnowLA (Luo et al., 2024b) Knowledgeable Adaptation Llama2-7B, Alpaca2 UMLS, Movie KG (MetaQA) WordNet, ConceptNet, Wikidata KG-Adapter (Tian et al., 2024) GAIL (Zhang et al., 2024d) Parameter Efficient Fine Tuning & Joint Reasoning GAIL Fine-tuning Llama-2-7B-base, Zephyr-7B-alpha ConceptNet, Freebase Llama-2-7B, BERTa Freebase GRAG (Hu et al., 2024b) Textual Graph RAG Llma-2-7B LEGO-GraphRAG (Cao et al., 2024) KG2RAG (Zhu et al., 2025) KG-RAG (Xu et al., 2024) Modular Graph RAG Graph-guided Chunks Expansion Vector-based Subgraph Retrieval Qwen2-72B, Sentence Transformer Llama-3-8B GPT-4 ExplaWebQSP, Graphs Freebase Inherent Dataset KGs Self-constructed KGs PubMedQA, MetaQA1HopQA CSQA, SIQA, BBH, WQSP, TriviaQA OBQA, CSQA, WQSP, CWQ WQSP, CWQ, GrailQA GraphQA, WQSP WQSP, CWQ, GrailQA HotpotQA KGQA NR, RR, F1 MCQA, CBQA, TruthfulQA Acc, CE Score, BLEU, ROUGE KGQA, MCQA, OBQA, CWQ KGQA Acc, Hits@1 EM, F1, Hits@1 KGQA F1, Hits@1, Acc KBQA, CWQ R, F1, Hits@1 Multi-hop QA F1, P, Curated Dataset KGQA MRR, Recall@K, NDCG@K, BLEU, ROUGE, METEOR Table 3: Summary and Comparison of Methods in the Category of KG as Background Knowledge. NR: Newlylearned Rate, RR: Remembering Rate, CE Score: Cross Entropy Score, EM: Exact Match, MRR: Mean Reciprocal Rank, BLEU: Bilingual Evaluation Understudy, ROUGE: Recall-Oriented Understudy, NDCG: Normalized Discounted Cumulative Gain, METEOR: Metric for Evaluation of Translation with Explicit Ordering. chunks for multiple RAG subtasks based on the map-reduce strategy. To leverage the multi-source knowledge for RAG-based QA, QUASAR (Christmann and Weikum, 2024) enhances RAG-based QA by effectively integrating unstructured text, structured tables, and knowledge graphs for evidence retrieval and re-ranking and filtering the relevant evidence from the retrieved evidence. Cost-based Optimization. To implement efficient and effective knowledge probing of LLMs, GLens (Zheng et al., 2024a) initially leverages the Thompson sampling strategy to measure the alignment between KGs and LLMs for addressing the knowledge blind spots of LLMs, and then designs graph-guided question generator to convert KGs to text and designs sampling strategy on the parameterized KG structure for accelerating KG traversal. Similarly, Coke (Dong et al., 2024a) minimizes the calls of LLMs for KGQA by introducing clusterlevel Thompson sampling to formulate the accuracy expectation and an optimized context-aware policy to distinguish the expert model based on question semantics. LMQL (Beurer-Kellner et al., 2023) minimizes the call of LLMs by generating an efficient inference procedure based on the LMP (Language Model Programming) constraints and control flow. It differs from the above work that CGPE (Tao et al., 2024) optimizes the knowledge retrieval based on clue-guided path exploration and information matching from knowledge bases to enhance the capabilities of LLMs for unfamiliar questions and reduce the costs of LLMs. To summarize, even if several optimizations and ranking strategies have been recently investigated to reduce the costs of graph retrieval, graph reasoning, and the length of the context of LLMs, however, the relevant subgraphs extraction, graph reasoning, and vector-based retrieval remain computationally costly task."
        },
        {
            "title": "E Evaluations and Applications",
            "content": "The details of evaluation metrics, benchmark datasets, industrial and scientific applications, and demos of synthesizing LLMs and KGs for QA are summarized and compared as follows. E.1 Evaluations Metrics. The evaluation metrics in synthesizing LLMs with KGs for QA are: (1) the metrics measuring the answer quality, BERTScore (Peng et al., 2024), answer relevance (AR), hallucination (HAL) (Yang et al., 2025), accuracy matching, human-verficed completeness (Yu and McQuade, 2025); (2) the metrics measuring the retrieval qual17 RoBERTa-base GPT-3.5-Turbo, GPT-4Turbo KG(s) Dataset KGs Dataset KGs Dataset(s) QA Type Metric(s) Inherent WQSP, CWQ KBQA Hits@1, F1 Inherent WQSP, CWQ Methods Techniques LM(s) SR (Zhang et al., 2022) KELDaR (Li et al., 2024b) Oreo (Hu et al., 2022) KBIGER (Du et al., 2022) ToG (Sun et al., 2024a) Trainable Subgraph Retriever, Fine-tuning Question Decomposition Tree, Atomic KG Retrieval Knowledge Interaction and Injection, KG Random Walk Instruction Interative Reasoning Beam-search base Retrieval, LLM Agent RoBERTA-base, base T5Dataset KGs, Wikidata Inherent PreLSTM based trained Model GPPT-3.5-Turbo, GPT4, Llama-2-70B-Chat Sentence Transformer Inherent Dataset KGs, Freebase Dataset KGs, Wikidata Inherent Freebase, ToG-2 (Ma et al., 2025b) Hibrid Knowledge-guided Context Retrieval RAG, GPT-3.5-Turbo, GPT4o, Llama-3-8B, Qwen2-7B Wikipedia, Wikidata KGQA, Multihop QA CBQA, OBQA, Multi-hop QA EM Acc Multi-hop KBQA KBQA, OpenDomain QA Hits@1, Hits@1 Acc, EM, R, F1 & Multi-hop KBQA Document QA, Domainspecific QA KBQA, Multihop QA KGQA, ODQA Hits@1, F1 Acc, Hit@K KBQA Hits@1, Acc WQ, NQ, TrivWQSP, iaQA, CWQ, HotpotQA WQSP, CWQ, GrailQA CWQ, WQSP, GrailQA, QALD10-en, WQ WQSP, QALD10-en, AdvHotpotQA, HotpotQA, ToG-FinQA WQSP, CWQ, SQ, WQ WQSP, CWQ, GrailQA QALD10-en KG-CoT (Zhao et al., 2024b) KG-Agent (Jiang et al., 2024) ODA (Sun et al., 2024b) GREASELM (Zhang et al., 2021) CoT-based KG and LLM Joint Reasoning KG-Agent based Instruction Tuning ODA-based Knowledge Graph Retrieval Mint-based KG and LM Cross-modal Fusion and Pretraing GPT-4, GPT-3.5-Turbo, Llama-7B, Llama-13B Dvinci-003, GPT-4, Llama-2-7B GPT-4, GPT-3.5 Inherent Dataset KGs, Freebase NQ-Wiki, TQ-Wiki, WQ-Freebas Wikidata RoBERTA-Large, AristoRoBERTA, SapBERT, PubmedBERT ConcepNet, UMLS, DrugBank CQA, OBQA, MedQAt Multiplechoice QA Acc Table 4: Summary and Comparison of Methods in the Category of KG as Reasoning Guidelines. EM: Exact Match, Acc: Accuracy, R: Recall. ity of RAG, context relevance (Es et al., 2024), faithfulness score (FS) (Yang et al., 2024), precision, context recall (Yu et al., 2024; Huang et al., 2025), mean reciprocal rank (MRR) (Xu et al., 2024), normalized discounted cumulative gain (NDCG) (Xu et al., 2024); (3) the metrics measuring the reasoning quality for multi-hop QA, Hop-Acc (Gu et al., 2024), reasoning accuracy (RA) (Li et al., 2025a). Benchmark Dataset. The recent benchmark datasets are summarized and compared in Table 8 with the focus on the following evaluations. (1) Answer Quality (AnsQ): the correctness of the generated answer with supported evidence and retrieved context in comparison to the ground-truth answer; (2) Retrieval Quality (RetQ): the relevance of the provided or retrieved context in comparison to the human-validated relevant context; (3) Reasoning Quality (ReaQ): the correctness of generated reasoning chains and intermediate steps that explain how the final answer is derived. E.2 Applications We showcase the applications and demos in synthesizing LLMs with KGs for QA. KAG (Liang et al., 2024a) (by Antgroup)3 is domain-knowledge augmented generation framework that leverages KGs and vector retrieval to bi-directionally enhance LLMs for knowledgeintensive tasks, such as QA. 3https://github.com/OpenSPG/KAG PIKE-RAG (Wang et al., 2025) (by Microsoft)4 is specialized knowledge and rationale augmented generation system with focus on extracting, understanding, and applying domain-specific knowledge to guide LLMs toward accurate responses. GraphRAG-QA (by NebulaGraph)5 is an industrial demo of GraphRAG integrating several query engines for augmenting QA, NLP2Cypher-based KG query engine (Wu et al., 2022), vector RAG query engine, and Graph vector RAG query engine. MedRAG (Zhao et al., 2025b) (by Nanyang Technological University et al.)6 is KG-elicited reasoning enhanced RAG-based healthcare copilot that generates diagnoses and treatment recommendations based on the input patient manifestations. Fact Finder (Steinigen et al., 2024) (by Fraunhofer IAIS and Bayer)7 is QA system that augments LLMs with the query-based KG retrieval from medical KG to improve the correctness and completeness of the answers. AprèsCoT (Shirdel et al., 2025) (by University of Waterloo)8 is visualization tool for understanding and explaining the answers generated by LLMs with CoT-based prompting and visualized path exploration and inference over KGs. 4https://github.com/microsoft/PIKE-RAG 5https://github.com/wey-gu/demo-kg-build 6https://github.com/SNOWTEAM2023/MedRAG 7https://github.com/chrschy/fact-finder 8http://lg-research-2.uwaterloo.ca:8050/ aprescot 18 Methods Techniques LM(s) ACT-Selection (Salnikov et al., 2023) ACT-based Answer Selection and Ranking T5-Large-SSM KG(s) Wikidata Q-KGR (Zhang et al., 2024c) KG-Rank (Yang et al., 2024) Question-guided KG Re-scoring, FNN-based Knowledge Injection Similarity and MMR based Ranking FLAN-T5-XL, RoBERTa-Large, Llama-2-7B GPT-4, Llama-2-13B Llama-2-7B, UMLS, DBpedia ConceptNet OBQA, PIQA KGR (Guan et al., 2024) EFSUM (Ko et al., 2024) InteractiveKBQA (Xiong et al., 2024) LPKG (Wang et al., 2024b) TruthfulRefine-then-Retrieve, Knowledge ness Verfication KG Fact Summarization, KG Helpfulness and Faithfulness Filter Multi-turn Interaction for Observation and Thinking Planning LLM Tuning, Inference, and Execution Llama-2-7B, Llama-2-7B, GPT-4, Vanilla Transformer GPT-3.5-Turbo, FlanT5-XL, Llma-2-7BChat GPT-4-Turbo, Mistral7B, Llama-2-13B GPT-3.5-Turbo, CodeQwen1.5-7BChat, Instruct Llama-3-8BCKG, PrimeKG Inherent (Freebase, Dataset KGs Wikidata) Freebase, Wikidata, Movie KG Dataset KGs Wikidata15K Inherent (Wikidata), Dataset(s) QA Type Metric(s) SQ, Mintaka RuBQ, KGQA, CBQA, Multi-lingual KGQA KGQA Hit@ Acc LiveQA, ExpertQA-Bio, ExpertQAMed, MedQA MedQuAD Domainspecific QA Domainspecific QA ROUGE-L, BERTScore, MoverScore, BLEURT Truthfulness Score WQSP, Mintaka KGQA, MultiAcc hop QA KBQA, Domainspecific QA KGQA, Multihop QA F1, Hits@1, EM, Acc EM, P, WQSP, CWQ, Pro, KQA MetaQA HotpotQA, 2WikiMQA, Bamboogle, MuSiQue, CLQA-Wiki Table 5: Summary and Comparison of Methods in the Category of KG as Refiners and Validators. Acc: Accuracy, EM: Exact Match, BLEURT: Bilingual Evaluation Understudy with Representations from Transformers. Methods Techniques LM(s) LongRAG (Zhao et al., 2024a) SimGRAG (Cai et al., 2024) Domain-specific FineTuning for RAG & CoTguided Filter Instruction Fine-Tuning for RAG with Filter ChatGLM3-6B, Qwen1.5-7B, Vicunav1.5-7B, Llama-3-8B, GPT-3.5-Turbo, GLM-4 Llama-3-8B-Instruct, Gemma-2-27B-it KG(s)"
        },
        {
            "title": "Wikidata",
            "content": "Wikipedia, PubMed Dataset(s) QA Type Metric(s) HotpotQA, 2WikiMQA, MusiQue PubMedQA, BioASQm MedQA, MedMCQA, LiveQA, MedicationQA WQAP, CWQ KBQA, Multihop QA F1 Domainspecific Multi-choice QA QA, Acc, MAUVE, EM, F1 Rouge-L,"
        },
        {
            "title": "KGQA",
            "content": "Hits@1, F1 KGQA (Ji et al., 2024) KG-IRAG (Yang et al., 2025) PIP-KAG (Huang et al., 2025) RAG-KG-IL (Yu and McQuade, 2025) CoT-RAG (Li et al., 2025a) KG-related Instruction Tuning with CoT Reasoning Incremental Retrieval and Iterative Reasoning Parameteric Pruning for KAG Agent-based Incremental Learning and Knowledge Dynamic Update KG-driven CoT Generation and Knowledgeaware with RAG Pseudo-program KGs Llama-2-7B-Chat, BGE-1.5-en-base"
        },
        {
            "title": "Inherent",
            "content": "Llama-3-8B-Instruct, GPT-3.5-Turbo, GPT4o-mini, GPT-4o Llma-3-8B-Instruct GPT-4o Self-constructed KGs"
        },
        {
            "title": "Temporal QA",
            "content": "EM, F1, HR, HAL"
        },
        {
            "title": "Inherent",
            "content": "Dataset KGs Self-constructed KGs"
        },
        {
            "title": "Curated Dataset\nfrom NHS",
            "content": "KGQA, Multihop QA Domainspecific QA EM, ConR, MR AM, HVC ERNIE-Speed-128K, GPT-4o-mini Self-curated Pseudo-Program KGs HotpotQA, CSQA, SIQA KGQA, Multihop QA RA, Robustness Table 6: Summary and Comparison of Methods in the Category of Hybrid Approach. EM: Exact Match, HR: Hit Rate, HAL: Hallucination, ConR: Context Recall, MR: Memorization Ratio, AM: Accuracy Matching, HVC: Human-verified Completeness, RA: Reasoning Accuracy."
        },
        {
            "title": "Approach",
            "content": "Multi-doc QA Multi-modal QA Multi-hop QA Multi-run QA XQA Temporal QA"
        },
        {
            "title": "KG as Background Knowledge\nKG as Reasoning Guidelines\nKG as Refiners and Validator\nHybrid Methods",
            "content": "(cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:34) (cid:34) (cid:34) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) Table 7: Research Progress on Alignment of Existing Approaches of synthesizing LLMs and KGs with Complex QA. (cid:34) Fully investigated. Partially investigated. (cid:35) Not yet investigated."
        },
        {
            "title": "AnsQ RetQ ReaQ Brief Description",
            "content": "WebQSP (tau Yih et al., 2016)"
        },
        {
            "title": "KBQA",
            "content": "BioASQ-QA (Krithara et al., 2023)"
        },
        {
            "title": "KBQA",
            "content": "CAQA (Hu et al., 2024a)"
        },
        {
            "title": "KBQA",
            "content": "CR-LT KGQA (Guo et al., 2024)"
        },
        {
            "title": "KGQA",
            "content": "EXAQT (Jia et al., 2021)"
        },
        {
            "title": "KGQA",
            "content": "(cid:34) (cid:34) (cid:34) (cid:34) CommonsenseQA (Talmor et al., 2019) Multi-choice QA (cid:34) MedQA (Jin et al., 2021) Multi-choice QA (cid:34) MINTQA (He et al., 2024) Multi-hop QA FanOutQA (Zhu et al., 2024) Multi-Hop QA M3SciQA (Li et al., 2024a) Multi-modal QA OMG-QA (Nan et al., 2024) Multi-modal QA ScienceQA (Lu et al., 2022) Multi-modal QA TempTabQA (Gupta et al., 2023)"
        },
        {
            "title": "Temporal QA",
            "content": "PATQA (Meem et al., 2024) MenatQA (Wei et al., 2023)"
        },
        {
            "title": "Temporal QA\nTemporal QA",
            "content": "(cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) Loong (Wang et al., 2024c) Long-context QA (cid:34) SituatedQA (Zhang and Choi, 2021) Open-retrieval QA (cid:34) ChatData (Sequeda et al., 2024) LLM-KG QA LLM-KG-Bench (Meyer et al., 2023) XplainLLM (Chen et al., 2024c) LLM-KG QA LLM-KG QA OKGQA (Sui and Hooi, 2024) LLM-KG QA LiHua-World (Fan et al., 2025) LLM-KG QA STaRK (Wu et al., 2024a) LLM-KG QA CoConflictQA (Huang et al., 2025) LLM-KG QA mmRAG (Xu et al., 2025a) LLM-KG QA (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:35) (cid:34) (cid:34) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:35) (cid:35) (cid:35) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) for queries Contains SPARQL knowledge-based QA. Includes exact and ideal answers of question over multi-doc summarization. Evaluates complex reasoning and attribution. Long-tail entities and commonsense reasoning. Supports temporal QA with multiple entities, predicates, and conditions. Contains questions with the correct answer and four distractor answers. Medical multi-choice QA dataset with multilingual medical examination text Supports LLMs evaluation on new and tail knowledge. Contains multi-hop question over multidocument. QA over the context across multiple document. Evaluates the retrieval and reasoning of QA across modalities. Supports multi-choice questions across disciplines in science topics. Supports temporal reasoning over semistructured tables. Present-anchored temporal QA Evaluates the temporal reasoning capability of LLMs QA over multi-doc with relevant document for the final answer. Includes correct question with temporal or geographical context. LLMs+KGs QA over the enterprise SQL database. LLMs in knowledge graph engineering. Focuses on QA explainability and reasoning Evaluates LLMs+KGs for open-ended QA. Evaluates capability of LLMs on multihop QA in the scenario of RAG. Evaluates the performance of LLMsdriven RAG for QA. Evaluates contextual faithfulness for QA in the scenario of KAG. Evaluates multi-modal RAG including QA dataset across text, tables, and KGs. Table 8: Comparison of LLMs+KGs for QA Benchmark Dataset. (cid:34) The dataset was primarily designed to support this evaluation. The dataset can be adapted for this evaluation, but this evaluation is not their main focus. (cid:35) The dataset does not support this evaluation."
        }
    ],
    "affiliations": [
        "Aalborg University, Denmark",
        "Southeast University, China",
        "Tongji University, China"
    ]
}