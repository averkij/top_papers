{
    "paper_title": "DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain",
    "authors": [
        "Walter Hernandez Cruz",
        "Peter Devine",
        "Nikhil Vadgama",
        "Paolo Tasca",
        "Jiahua Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution. We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation. We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 5 4 0 2 2 . 2 0 6 2 : r DLT-Corpus: Large-Scale Text Collection for the Distributed Ledger Technology Domain Walter Hernandez Cruz1,3, Peter Devine2, Nikhil Vadgama1,3, Paolo Tasca1,3, Jiahua Xu1,3 1Centre for Blockchain Technologies, University College London 2School of Informatics, University of Edinburgh 3Exponential Science {walter.hernandez.18,nikhil.vadgama,p.tasca,jiahua.xu}@ucl.ac.uk,pdevine2@ed.ac.uk Abstract We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language underexplored despite the sectors $3 trillion market capitalization and rapid technological evolution. We demonstrate DLT-Corpus utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in virtuous cycle where research precedes and enables economic growth that funds further innovation. We publicly release the full DLT-Corpus; LedgerBERT, domainadapted model achieving 23% improvement over BERT-base on DLT-specific Named Entity Recognition (NER) task; and all associated tools and code. CCS Concepts Computing methodologies Language resources; Natural language processing; Information systems Data mining. Keywords Distributed Ledger Technology, Blockchain, Text Corpus, Corpus Construction, Text Mining, Sentiment Analysis, Innovation Diffusion, Patent Analysis, Cryptocurrency, Natural Language Processing"
        },
        {
            "title": "1 Introduction\nThe DLT field lacks a comprehensive text corpus. While, at the\ntime of this writing, the ecosystem has grown to ‚àº$3 trillion in\nmarket capitalization [15] and introduced new concepts such as\nstablecoins, Decentralized Exchanges (DEXs), and Automated Mar-\nket Makers (AMMs) [36], NLP research within the DLT domain\nremains constrained by narrow, task-specific datasets that overlook\nsubstantial textual resources in scientific publications, patents, and\ntechnical documentation.",
            "content": "Current DLT datasets focus primarily on cryptocurrency price prediction [31, 53, 68], trading [23, 47, 52, 53], and smart contracts [19, 43, 72, 83]. These resources support specific NLP downstream tasks, such as NER [36], Question Answering (QA) [67], sentiment analysis [5, 65], but fail to capture the substantial textual resources available in scientific literature, patent filings, and technical documentation. This gap limits the development of practical NLP applications: Retrieval-Augmented Generation (RAG) systems that reduce Large Language Model (LLM) hallucinations [67], patent landscape monitoring, protocol documentation analysis, and technology trend detection, to cite few examples. Similarly, [2, 11, 29, 41, 46, 58, 78] demonstrate that small language models trained on domain-specific corpora outperform general-purpose LLMs on specialized tasks while remaining computationally efficient [11, 41, 46, 78], highlighting the need for comprehensive DLT text resources. We introduce DLT-Corpus1, the largest domain-specific text collection for DLT research: 2.98 billion tokens from 22.12 million documents spanning open-access scientific literature2, USPTO patent filings3, and Twitter/X posts4 collected before 2023 platform restrictions [21]. This corpus integrates technical specifications, economic mechanisms, community discourse, and governance frameworks, providing foundation for NLP systems tailored to the DLT domain. We demonstrate the corpus utility through two analyses. First, we track how technologies diffuse from scientific literature to patents to social media, finding that concepts such as stablecoins, AMMs, and DEXs consistently originate in research before reaching commercial and consumer communities. Second, we examine correlations between market dynamics and innovation activity, finding that scientific publications lead market expansion by two years (ùúå = 0.95, ùëù < 0.001), while social media sentiment remains bullish (i.e., extremely optimistic) even during crypto winters. In summary, our contributions are: DLT-Corpus5: 2.98 billion tokens from 22.12 million documents (37,440 scientific publications, 49,023 patents, 22M social media posts) with rich metadata enabling cross-disciplinary research. Innovation diffusion analysis: Evidence that Distributed Ledger Technologies (DLTs) follow traditional technology transfer patterns, with research preceding market expansion and creating virtuous funding cycle. 1https://huggingface.co/collections/ExponentialScience/dlt-corpus 2https://huggingface.co/datasets/ExponentialScience/DLT-Scientific-Literature 3https://huggingface.co/datasets/ExponentialScience/DLT-Patents 4https://huggingface.co/datasets/ExponentialScience/DLT-Tweets 5https://huggingface.co/collections/ExponentialScience/dlt-corpus Sentiment analysis dataset6: 23,301 cryptocurrency news headlines and brief descriptions with crowdsourced annotations from active community members, addressing the need for domain-specific labeled data. LedgerBERT7: domain-adapted language model achieving 23% improvement over BERT-base on DLT-specific NER task, developed through continued pre-training of SciBERT [12]. DLT-Corpus, sentiment analysis dataset, models, and code8 are publicly available to support reproducibility and future research."
        },
        {
            "title": "3 Related Work",
            "content": "Existing DLT text resources are fragmented and narrow. The DLT domain combines technical specifications [8, 16, 55], economic mechanisms [13, 50, 54], and social dynamics [20, 36], with rapid terminological evolution introducing concepts like AMMs, DEXs, stablecoins, Non-Fungible Tokens (NFTs), and Maximal Extractable Value (MEV). Yet existing datasets focus narrowly on cryptocurrency markets: price prediction [31, 45, 53, 68], trading [47, 57], fraud detection [24, 48], and sentiment analysis [5, 30, 65]. These datasets primarily extract social media content from Twitter/X, Telegram, and Reddit [42, 45, 68], but despite some studies collecting millions of tweets, this data is rarely publicly available. Other work relies on transactional data [25, 72] and smart contracts [19, 43, 83], overlooking textual resources in scientific publications, patents, and technical documentation. General-purpose corpora such as RefinedWeb [59], CommonCrawl, and C4 [63] contain some cryptocurrency content but lack domain specificity. DLT-Corpus addresses this gap by integrating scientific literature, USPTO patents, and Twitter/X data collected before 2023 API restrictions [21], enabling both crossdomain analysis and domain-specific NLP applications. Innovation diffusion in DLT lacks integrated analysis. Prior work on DLT domain analysis remains fragmented: NER applied to scientific literature [36] or patents [82], news-based studies [60], taxonomies [9, 73], and systematic reviews [28, 81]. Our work provides the first integrated analysis spanning scientific publications, patents, and community discourse (6), revealing how innovation emerges and diffuses across research, commercial, and user communities. Hernandez Cruz et al."
        },
        {
            "title": "4 Datasets\nWe introduce two datasets:",
            "content": "DLT-Corpus9: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), patents (49,023 filings), and social media (22M posts). This unstructured text corpus captures technical specifications, economic mechanisms, and community discourse across the DLT domain. Sentiment analysis dataset10: 23,301 cryptocurrency news headlines (and brief description of them) with crowdsourced annotations from active community members, enabling sentiment analysis research. We describe the construction of each dataset below. See Appendix for complete documentation and data dictionaries (Table 6, Table 7, Table 8, Table 9)."
        },
        {
            "title": "4.1 DLT-Corpus\nWe construct the DLT-Corpus by aggregating text from three com-\nplementary sources that capture different aspects of the DLT ecosys-\ntem: academic and industry publications provide formal technical\nknowledge, patent filings reveal innovation trajectories, and so-\ncial media reflect community discourse and market dynamics. This\nmulti-source approach allows comprehensive coverage of technical\nterminology and evolving community language.",
            "content": "Legal compliance. We exclusively use open-access scientific literature and, whenever available, include in the metadata for each publication its license  (Table 6)  information to ensure redistribution rights for research and commercial applications. Patent data derives from public USPTO records. Social media content was collected before Twitter/Xs 2023 Terms and Conditions (T&Cs) changes that restricted API access on 18 May 202311, preserving valuable snapshot of community discourse. Then, the DLT-Corpus could facilitate academic research and industry use by mitigating legal barriers, which have become persistent challenge in the NLP domain [17], especially for industry applications with commercial benefits. Rich metadata. Each subset includes structured metadata enabling research beyond language modeling: publication venues, authors, and references for scientific literature  (Table 6)  . Inventor, assignee, and filing information for patents  (Table 7)  . Timestamps and sentiment labels (based on 5.2) for social media  (Table 8)  . Therefore, the metadata included for each source of the DLT-Corpus could support cross-disciplinary investigations spanning innovation diffusion, technology forecasting, collaborative network analysis, computational social science, and other areas of research reflecting the increasingly interdisciplinary nature of the DLT field [36]. For industry practitioners, this metadata could enable practical applications, including patent landscape monitoring, R&D trend detection, competitor analysis, etc. Corpus statistics. Fig. 1 summarizes the corpus: 2.98 billion tokens across 22.12 million documents. Document lengths vary by source: social media averages 51 tokens, scientific literature 15.1k tokens, 6https://huggingface.co/datasets/ExponentialScience/DLT-Sentiment-News 7https://huggingface.co/ExponentialScience/LedgerBERT 8https://github.com/dlt-science/DLT-Corpus 9https://huggingface.co/collections/ExponentialScience/dlt-corpus 10https://huggingface.co/datasets/ExponentialScience/DLT-Sentiment-News 11https://x.com/en/tos/previous/version_18 DLT-Corpus: Large-Scale Text Collection for the Distributed Ledger Technology Domain (a) Tokens distribution (b) Document count (c) Average document length (d) Temporal evolution (e) DLT keywords density by corpus Figure 1: Overview of DLT-Corpus composition and patents 26.4k tokens. Token distribution: patents 43.5%, social media 37.6%, scientific literature 18.9% (Fig. 1a). 4.1.1 Scientific literature. Collection. We retrieved PDFs and metadata (authors, title, year, venue, references, licensing)12 from Semantic Scholar13 using domainspecific queries (e.g., Distributed Ledger Technology, Blockchain, Hashgraph, DAG, Consensus Mechanisms, Distributed Computing, Distributed Storage, Distributed Databases), producing 144,843 initial documents. Processing. We parsed PDFs to Markdown using PyMuPDF4LLM,14 filtered for English-only content using FastText [40], and removed outliers that were either too short (< 500 tokens) or too long (> 40k tokens), retaining 112,911 documents. Domain filtering. To ensure relevance, we fine-tuned BERT-basecased15 on the NER dataset from [36] and predicted domain-specific entities in each document. We filtered documents based on prediction quality by calculating: (1) the number of predicted entities per document, (2) the maximum prediction score, and (3) the median prediction score across all entities. Documents were retained if they had maximum prediction score above 0.995 or median prediction score at or above the scientific literature subset-wide median, ensuring high confidence in domain-specific content while maintaining reasonable coverage. This filtering step, combined with duplicate removal, reduced the dataset to 38,010 documents. Then, manual review remove 570 marginally relevant papers, obtaining 37,440 publications. The removed papers were false positives retrieved because terms like distributed, consensus, protocol, and network appear in non-DLT contexts. Manual inspection revealed these papers clustered around biomedical domains: Alzheimers disease research (neurofibrillary tangles, tau proteins, amyloid, dementia), infectious disease epidemiology (COVID-19, tuberculosis), oncology, and healthcare systems because they share terminology with DLT but focus on distributed biological processes, clinical consensus protocols, or decentralized healthcare delivery rather than Distributed Ledger Technology (DLT). The scientific literature subset16 spans 1978mid 2025 and represents, to our knowledge, the first large-scale scientific corpus for the DLT domain. 12Copyright licensing information included when available via Semantic Scholar. 13https://www.semanticscholar.org/ 14https://pypi.org/project/pymupdf4llm/ 15https://huggingface.co/google-bert/bert-base-cased 16https://huggingface.co/datasets/ExponentialScience/DLT-Scientific-Literature"
        },
        {
            "title": "4.1.2 Patents. We collected patent text and metadata (e.g., patent\nnumber, title, publication date, inventor, assignee, applicant, data-\nbase) from USPTO‚Äôs US-PGPUB and USPAT databases using search\nterms ‚ÄúDistributed Ledger Technology‚Äù and ‚Äúblockchain‚Äù in titles,\nabstracts, and full text. We focus on US patents because USPTO‚Äôs\nterms state that patent text is ‚Äútypically not subject to copyright\nrestrictions‚Äù17 facilitating NLP research and commercial use.",
            "content": "The patents subset18 contains 49,023 documents spanning 1990 mid 2025 and represents, to our knowledge, the first compiled patent dataset for the DLT domain. Social media. We aggregated Twitter/X data from academic 4.1.3 [26, 38, 56] and industry sources,19,20,21,22,23 all collected before Twitter/Xs 2023 API restrictions that affected researcher access [21]. Initial aggregation produced 28,775,339 posts. After removing empty posts (407) and deduplication (25,417,108 unique), we filtered for English using Lingua,24 resulting in 22,033,090 posts. Each post includes timestamp and sentiment labels (bullish, bearish, neutral) from LedgerBERT (5). The social media subset25 spans 2013mid 2023 and represents, to our knowledge, the largest publicly available snapshot of DLT community discourse on Twitter/X."
        },
        {
            "title": "4.1.4 Corpus quality assessment. To quantify domain specificity\nand quality, we compare DLT-Corpus vocabulary distributions\nagainst two general-purpose corpora: RefinedWeb [59] (600B to-\nkens) and C4 [63] (156B tokens). We curated 361 keywords from the\nDLT taxonomy of [36] and [73], covering consensus mechanisms,\ncryptographic primitives, smart contracts, token standards, Decen-\ntralized Finance (DeFi) concepts, and major platforms. We sampled\napproximately 60M tokens from each corpus and computed key-\nword density (occurrences per 1,000 words), document coverage\n(percentage of documents containing at least one keyword), and\nJensen-Shannon divergence to measure distributional differences\n[51].",
            "content": "DLT-Corpus exhibits 8.7 times higher keyword density than general corpora (see Table 1): 43.96 keywords per 1,000 words (averaging across subsets) versus 5.05 in RefinedWeb and C4 (see 17https://www.uspto.gov/terms-use-uspto-websites 18https://huggingface.co/datasets/ExponentialScience/DLT-Patents 19https://www.kaggle.com/datasets/leoth9/crypto-tweets 20https://www.kaggle.com/datasets/kaushiksuresh147/bitcoin-tweets 21https://www.kaggle.com/datasets/tleonel/crypto-tweets-80k-in-eng-aug-2022 22https://www.kaggle.com/datasets/rezasemyari/crypto-sentiment-tweets 23https://www.kaggle.com/datasets/hiraddolatzadeh/bitcoin-tweets-2021-2022/data 24https://github.com/pemistahl/lingua-py 25https://huggingface.co/datasets/ExponentialScience/DLT-Tweets Table 1: Comparison of DLT-Corpus with RefinedWeb [59] and C4 [63]"
        },
        {
            "title": "Type",
            "content": "Density Doc. Cov. JS Div. Social Media Patents Scientific Lit."
        },
        {
            "title": "Domain\nDomain\nDomain",
            "content": "DLT-Corpus avg. Domain RefinedWeb C"
        },
        {
            "title": "General\nGeneral",
            "content": "86.92 25.27 19.68 43.96 4.96 5.14 96.3% 99.8% 100.0% 98.7% 55.6% 51.9% 0.44 0.45 0.39 0.43 0.10 0.10 -Keyword density: vocabulary analysis comparing DLT-Corpus subsets against general-purpose web corpora using 361 domain keywords. Keyword density measures occurrences per 1,000 words. -Document coverage: indicates the percentage of documents containing at least one domain keyword -Jensen-Shannon (JS) divergence: measures vocabulary distribution difference versus general corpora (higher values indicate greater difference); for general corpora, the value shows the baseline divergence between RefinedWeb and C4. also Fig. 1e). Document coverage reaches 98.7% in DLT-Corpus compared to 53.8% in general corpora. Jensen-Shannon divergence between DLT-Corpus and general corpora ranges from 0.39 to 0.45, while divergence between RefinedWeb and C4 is only 0.10. This difference confirms that DLT-Corpus has fundamentally distinct vocabulary distributions from general web text. Higher keyword density matters for language model learning because language models rely on word frequency during training to acquire vocabulary [18]. Domain-adaptive pretraining on corpora with concentrated terminology exposure leads to performance gains on downstream tasks [32], as models encounter domain-specific terms more frequently and learn their contextual usage patterns [18]. The 8.7 higher density in DLT-Corpus compared to general web text provides substantially more learning signal per token for DLT terminology."
        },
        {
            "title": "4.2 Sentiment analysis\nTo support sentiment analysis in the DLT domain, we constructed\na labeled dataset26 from CryptoPanic,27 a cryptocurrency news\nplatform where active community members vote on articles.",
            "content": "Annotation. Users vote on news headlines and brief descriptions of them across three dimensions: market direction (bullish/bearish), content quality (important/lol28), and engagement (liked/disliked). We normalize vote percentages by total engagement, filter by median minimum votes, and use 25th/75th percentiles as classification boundaries: below 25th = negative, above 75th = positive, between = neutral. This percentile-based approach mitigates popularity bias from raw counts. Crowdsourcing advantages. Our approach leverages collective intelligence [69] from domain experts (active crypto users), avoiding the pitfalls of an LLM-based annotation approach, which, if we had followed it, could have introduced systematic biases and statistical manipulation [10]. 26https://huggingface.co/datasets/ExponentialScience/DLT-Sentiment-News 27Data collected via CryptoPanics free API, MarchMay 2025. T&Cs at collection time contained no restrictions on academic research. 28lol (Laughing Out Loud) indicates humorous headlines. Hernandez Cruz et al. Table 2: Performance on in-domain DLT entity recognition from scientific literature. NER scores represent F1-average across 5-fold cross-validation with strict entity-level matching (exact boundary and type agreement required)."
        },
        {
            "title": "Model",
            "content": "Parameters NER F1 General Baselines BERT-base [22] MosaicBERT-base [62] ModernBERT-base [74] Domain-Adapted Baselines SciBERT [12] BioClinical ModernBERT [70] LedgerBERT (This work) LedgerBERT Improvement over SciBERT Improvement over BERT-base 110M 109M 149M 110M 150M 110M 0.243 0.076 0.209 0.289 0.192 0.299 +3.5% +23.05% Statistics. The dataset contains 23,301 examples (1.85M tokens, 79.51 tokens/example average) spanning 2021mid 2025."
        },
        {
            "title": "5 Domain-adapted language model\nTo demonstrate the practical utility of DLT-Corpus for language\nmodel development, we train LedgerBERT29, a domain-adapted\nencoder for DLT-specific NLP tasks. We evaluate LedgerBERT on\ntwo tasks: in-domain NER (where improvement validates corpus\nquality) and out-of-domain sentiment analysis (where maintained\nperformance validates generalization).",
            "content": "Training. We use continued pre-training rather than training from scratch, following evidence that domain adaptation of existing models outperforms full pre-training [32, 79]. We initialize from SciBERT [12], which captures multidisciplinary scientific content and likely includes some DLT-related material, making it stronger starting point than general-purpose BERT. Hyperparameters. We experimented with different hyperparameter configurations and selected final values based on model convergence and validation loss. We train for 3 epochs with learning rate 5 105 (linear decay), Masked Language Model (MLM) probability 0.15, warmup ratio 0.10, batch size 12, sequence length 512, weight decay 0.01, and Stable AdamW optimizer [77] with bfloat16 precision. Compute. Training on one NVIDIA H100 GPU required approximately 68.7 GPU-hours."
        },
        {
            "title": "5.1 Primary evaluation: in-domain NER\nNER serves as the primary evaluation of corpus quality because\nperformance directly reflects how well the model learned domain-\nspecific terminology. We use the DLT-focused NER dataset from\n[36], targeting entities such as consensus mechanisms (Proof of\nStake (PoS), Proof of Work (PoW)), platforms (Ethereum, Hedera),\nand technical concepts (Merkle tree, private key). This dataset de-\nrives from scientific literature, matching our corpus composition.",
            "content": "29https://huggingface.co/ExponentialScience/LedgerBERT DLT-Corpus: Large-Scale Text Collection for the Distributed Ledger Technology Domain Table 3: Performance on out-of-domain sentiment analysis from cryptocurrency news articles titles and brief descriptions. Table 4: Correlation between cryptocurrencies market capitalization and document volumes  (Fig. 2)  ."
        },
        {
            "title": "Market\nDirection",
            "content": "Content Char."
        },
        {
            "title": "Engagement\nQuality",
            "content": "BERT-base DistilBERT-base SciBERT LedgerBERT vs. SciBERT 0.577 0.570 0.591 0.590 -0.2% 0.514 0.508 0.514 0.502 -2.3% 0.539 0.544 0.532 0.537 +0.9% Note: News articles are absent from DLT-Corpus due to copyright restrictions on collecting, using, and redistributing journalistic content. Results demonstrate preserved general capabilities despite domain-specific training. Fine-tuning. We experimented with learning rates and number of epochs, selecting final hyperparameters based on convergence behavior and cross-validation performance. We fine-tune for 20 epochs with learning rate 1 105, 500 warmup steps, batch size 16 (gradient accumulation 2 for effective batch 32), and 5-fold crossvalidation grouped by paper to prevent data leakage. Results. Table 2 reports F1 scores using strict entity-level matching (exact boundary and type required). LedgerBERT achieves 0.299 F1, improving over SciBERT (0.289 F1) by 3.5% relative and over BERT-base (0.243 F1) by 23%. The progression of BERT-base SciBERT LedgerBERT demonstrates the cumulative value of domain-specific pre-training: general scientific knowledge (SciBERT) provides foundation, and DLT-Corpus adds specialized terminology."
        },
        {
            "title": "5.2 Generalization test: out-of-domain",
            "content": "sentiment analysis To verify that domain-specific training does not degrade general capabilities, we evaluate on sentiment analysis of cryptocurrency news, which is task representing out-of-domain generalization because news articles are absent from DLT-Corpus due to copyright restrictions.30 Fine-tuning. We selected hyperparameters through experimentation, monitoring convergence on the validation set. We fine-tune LedgerBERT31 for 3 epochs with learning rate 2 105, 500 warmup steps, batch size 8, and 90/10 train/test split. Results. Table 3 shows LedgerBERT performs comparably to SciBERT across all sentiment dimensions (within 0.2% on market direction, the primary metric). This result is important: domainspecific training preserved general language understanding despite the corpus emphasizing scientific literature, patents, and social media posts rather than sentiment-bearing news. Interpretation. The NER improvement (23% over BERT-base) combined with maintained sentiment performance demonstrates that DLT-Corpus enables domain specialization without catastrophic"
        },
        {
            "title": "Scientific literature\nPatents\nSocial media",
            "content": "2013-2024 2013-2024 2013-2023 Spearmans ùúå 0.76 0.96 0.98 ùëù-value <0.004 <0.001 <0.001 Table 5: Lagged correlations (Spearmans ùúå) between document volumes and market capitalization. Lag (years)"
        },
        {
            "title": "Document volumes lead market",
            "content": "5 4 3 2 1 0.86** 0.90** 0.93*** 0.95*** 0.94*** 0.77 0.82* 0.95*** 0.88** 0.90***"
        },
        {
            "title": "Concurrent",
            "content": "0.86** 0.90** 0.93*** 0.95*** 0.95*** 0 0.76** 0.98*** 0.97***"
        },
        {
            "title": "Market leads document volumes",
            "content": "1 2 3 4 5 0.79** 0.68* 0.47 0.31 0.36 0.92*** 0.85** 0.86** 0.89** 0.60 0.95*** 0.96*** 0.97*** 0.95*** 0.93*** ùëù < 0.05, ùëù < 0.01, ùëù < 0.001 forgetting, which means the model gains DLT-specific knowledge while retaining general capabilities for out-of-domain tasks."
        },
        {
            "title": "6 Analysis\nWe demonstrate the utility of DLT-Corpus through two analyses: (1)\ncorrelations between market dynamics and document production,\nand (2) technology diffusion patterns across communities. These\nanalyses serve as examples of research enabled by the corpus.",
            "content": "Market-document correlations. Fig. 2 shows document growth across all corpus subsets correlating with cryptocurrency market capitalization. We quantify these relationships using Spearmans rank correlation. Table 4 reports Spearmans rank correlations between annual market capitalization and document volumes (20132023 for social media, 20132024 for patents and publications).33 All three document types show strong positive correlations: scientific literature (ùúå = 0.76, ùëù < 0.004), patents (ùúå = 0.96, ùëù < 0.001), and social media (ùúå = 0.98, ùëù < 0.001). Lagged correlations reveal temporal structure. To test whether research drives market expansion or responds to it, we compute lagged correlations  (Table 5)  . The scientific literature subset spans 19782025 (earliest: [66]), enabling meaningful lag analysis. 30For example, see ongoing copyright lawsuits by news organizations against OpenAI [14, 61] and Microsoft [61] over alleged copyright violations of news articles, which motivated this exclusion. 31https://huggingface.co/ExponentialScience/LedgerBERT-Market-Sentiment 32Median market capitalization per year of the cryptocurrency market using aggregated data from CoinGecko. 33Systematic market data became available around 2013 as cryptocurrencies developed sufficient liquidity [55]. Hernandez Cruz et al. Figure 2: Yearly growth of global cryptocurrency market capitalization32and documents in the DLTCorpus. (a) Stablecoins (b) DEX (c) AMM Figure 3: Mentions per year for selected technologies in the DLT-Corpus. The y-axis represents the relative frequency of mentions normalized by the total volume of documents in each source per year. Scientific publications show asymmetric temporal patterns: correlations remain strong when publications lead the market (negative lags), but decay rapidly when the market leads publications and losing significance beyond two years (ùúå = 0.47, ùëù = 0.21 at three years). This asymmetry indicates research precedes market expansion. Social media exhibits the opposite pattern: strongest correlations when the market leads by three years (ùúå = 0.97, ùëù < 0.001), weaker when social media leads (ùúå = 0.77, ùëù = 0.07 at five years). Patents show symmetric patterns with peak concurrent correlation (ùúå = 0.98, ùëù < 0.001), significant whether patents lead (ùúå = 0.95, ùëù < 0.001 at three years) or lag (ùúå = 0.86, ùëù = 0.007 at three years). Additionally, the three corpus subsets reflect distinct communities with different incentives: researchers seeking knowledge dissemination (scientific literature), industry practitioners protecting commercial innovations (patents), and users engaging with the market (social media). Therefore, at more granular level, considering the incentives that the three audiences reflected in the DLT-Corpus have, we investigate two fundamental questions about innovation dynamics in the DLT ecosystem: (1) Where do technological concepts originate, and how do they diffuse across communities? (2) How does market sentiment relate to research and commercial innovation activity? These analyses provide insights into the relationship between public discourse, scientific inquiry, and commercial innovation in the DLT domain. Most importantly, these analyses serve as introductory demonstrations of use cases for the type of analyses that can be carried out with the DLT-Corpus."
        },
        {
            "title": "6.1 Technology diffusion across communities\nWe track when key DLT concepts first appear within the commu-\nnity of users, academic and industry researchers, or the business-\nfocused community. The DLT-Corpus enables this analysis through\ntimestamped documents from scientific literature (¬ß4.1.1), patents\n(¬ß4.1.2), and social media (¬ß4.1.3).",
            "content": "Technology selection. We focus on three economically significant technologies: (1) stablecoins, given recent regulatory frameworks (US GENIUS Act,34 Europes MiCAR35) and their role bridging traditional and decentralized finance [37, 49]. (2) DEXs, holding approximately $174B in Total Value Locked (TVL)36. (3) AMMs, the mechanism enabling decentralized token swapping [35, 80]. Findings. Stablecoins (Fig. 2a), AMMs (Fig. 2c), and DEXs (Fig. 2b) consistently originate in scientific literature, with researchers maintaining sustained interest over time. This pattern aligns with traditional technology transfer models where research precedes commercial application and consumer adoption [3, 4]. Cryptocurrency mentions vs. technology mentions. We contrast technology diffusion with cryptocurrency mentions to distinguish innovation interest from speculative interest. We analyze Bitcoin, Ethereum, and XRP because they represent the three largest nonstablecoins by market capitalization37). We include Hedera because it uses Hashgraph [8], which differs from the blockchain architecture used by most cryptocurrencies. At the time of writing, Hedera is the only non-blockchain DLT in the top 20 by market capitalization. Bitcoin (Fig. 4a) shows high user interest but declining patents and plateauing publications representing characteristics of mature consumer asset. Ethereum (Fig. 4b) exhibits growing publications and patents alongside user interest, reflecting continued innovation through smart contracts and DeFi.38 Similar to Ethereum, Hedera (Fig. 4d) attracts primarily academic interest with limited user engagement, consistent with early-stage technology transfer. XRP (Fig. 4c) shows sharply declining user engagement around 2020, which coincides with major lawsuit in the United States (US) involving this digital asset [71]. Interestingly, patents and academic and industry research continue growing for XRP before plateauing and slowly picking up again. The contrast between cryptocurrency mentions  (Fig. 4)  and technology mentions  (Fig. 3)  reveals the speculative focus of users on digital assets while researchers and industry practitioners focus on technologies. Then, this raises the question: to what degree does market sentiment affect researchers? 34https://www.congress.gov/bill/119th-congress/senate-bill/394/text 35https://www.esma.europa.eu/esmas-activities/digital-finance-and-innovation/mar kets-crypto-assets-regulation-mica 36https://defillama.com/ 37https://coinmarketcap.com/ 38https://defillama.com/chains DLT-Corpus: Large-Scale Text Collection for the Distributed Ledger Technology Domain (a) Bitcoin (b) Ethereum (c) XRP (d) Hedera Figure 4: Proportion of mentions per year for selected cryptocurrencies in the DLT-Corpus. The y-axis represents the relative frequency of mentions normalized by the total volume of documents in each source per year."
        },
        {
            "title": "6.2 Market sentiment and innovation activity\nThe DLT ecosystem exhibits a unique characteristic: public mar-\nkets provide real-time feedback on technological developments\nthrough cryptocurrency prices and trading activity. This raises an\nintriguing question about whether market dynamics influence the\npace and direction of innovation. Do periods of market enthusiasm\ncorrelate with increased research output and patent activity? Or\ndo academic and commercial innovation proceed independently of\nmarket sentiment?",
            "content": "Method. We examine the relationship between social media sentiment and the number of patent filings and scientific publications over time. We classified social media posts according to market sentiment (bullish, bearish, or neutral), using the finetuned LedgerBERT (5, 4.2), and aggregated them over yearly intervals. Findings. We observe that even during periods of crypto winter (e.g., 2018 to 201939,40), the user community is overwhelmingly bullish  (Fig. 5)  . Additionally, bearish sentiment peaks in 2022 while 2023 shows bullish sentiment rapidly growing as the market recovers. Comparing Fig. 1d with Fig. 5 reveals that patents and scientific publications follow trajectories largely independent of short-term market sentiment. Instead, innovation activity grows alongside the overall market expansion (6.1)."
        },
        {
            "title": "7 Discussion",
            "content": "Divergent community interests. The community of users seems to focus on cryptocurrencies as investments, while researchers appear to concentrate on underlying technologies. New Distributed Ledger Technologies (DLTs) and concepts first appear in the scientific literature before spreading to patents and the user community (6.1), following traditional technology transfer model [3, 4]. This pattern suggests that engaging with recently published scientific literature can help identify emerging technologies in the DLT field before they become mainstream, potentially creating opportunities for early commercial innovation. Cryptocurrencies trajectory diverge. Analysis of specific cryptocurrencies reveals how different factors shape their evolution 39https://www.kraken.com/learn/crypto-bull-bear-markets 40https://finance.yahoo.com/news/contrasting-2022-market-crash-2018174800300.html Figure 5: Market sentiment in social media and yearly growth of global cryptocurrency market capitalization. across communities  (Fig. 4)  . Bitcoin shows declining patent activity and plateauing scientific publications despite sustained interest from the user community, suggesting it is maturing into consumerfocused digital asset. Ethereum exhibits different pattern, with growing academic publications and patents alongside user interest, reflecting its continued role in driving innovation through smart contracts and DeFi applications. XRP shows how external events shape community behavior. For example, XRPs user engagement dropped sharply around 2020 during its legal challenges [71], while research activity continued. Hedera attracts primarily academic interest and limited user engagement, suggesting that new DLT architectures, like Hashgraph [8] powering Hedera, can sustain scientific interest without immediate market enthusiasm. However, given that the DLT-Corpus indicates the DLT field may follow traditional technology transfer model, Hedera may be in the early stages, with mainstream popularity among users coming later. These patterns show that technology innovation, regulation, and market speculation each shape the DLT ecosystem independently. Research creates economic value through virtuous cycle. 6 suggests that research establishes the foundation for the DLT field [36] that precedes market expansion, while commercial innovation and community discourse respond strongly to market conditions. Then, as cryptocurrency markets grew, increased capital likely funded industry research, leading to more patent filings and heightened community engagement. This creates virtuous cycle in which foundational research generates innovations that commercial actors and the broader community adopt, develop, and speculate on during periods of market growth, thereby channeling more funding into future research. This pattern benefits the DLT field by maintaining stable research foundation while market-driven activity accelerates technology adoption and deployment."
        },
        {
            "title": "8 Conclusions\nWe introduce DLT-Corpus, a dataset comprising 2.98 billion tokens\nfrom 22.12 million documents across scientific literature (including\nacademic publications and industry whitepapers), USPTO patents,\nand social media. Our analysis, serving as introductory demonstra-\ntions of the utility of the DLT-Corpus, reveals that technologies and\nconcepts typically originate in scientific literature before reaching\npatents and social media, following traditional technology transfer\npatterns. While social media sentiment remains overwhelmingly\nbullish, even during crypto winters, scientific and patent activity\ngrow independently of market fluctuations, instead tracking overall\nmarket expansion.",
            "content": "We release DLT-Corpus41 along with sentiment analysis dataset42 from crowdsourced annotations, the LedgerBERT language model, and the code used to support reproducibility, future research in domain-specific NLP, and innovation diffusion analysis for the DLT field."
        },
        {
            "title": "9 Limitations",
            "content": "Language coverage. We focus exclusively on English-language data derived from open-access scientific literature, patents, and social media. However, English is the dominant language for web content43 and nearly all scientific publications [1, 7]. Domain relevance filtering. Although we manually revised the filtered scientific literature subset, removing 570 papers (4.1.1), there is still possibility that marginally relevant DLT papers may remain in the dataset. Data accessibility and legal compliance trade-offs. We prioritize data accessibility and legal compliance (see 10) in constructing the DLT-Corpus, which may limit subset sizes and collection from other types of data sources, like news (5.2), but reduces legal barriers for academic and commercial use of the DLT-Corpus. This emphasis addresses growing industry concerns about regulatory and copyright risks in Artificial Intelligence (AI) development. S&P 500 companies have acknowledged in hundreds of corporate filings and executive transcripts that legal and regulatory risks are primary concerns in their AI adoption [34], which is likely amplified by ongoing copyright lawsuits against OpenAI [14, 61], Microsoft [61], Anthropic [39], and Meta [44] over alleged copyright violations from data collected and used for training their LLMs. 41https://huggingface.co/collections/ExponentialScience/dlt-corpus68e44e40d4e7a3bd7a224402 42https://huggingface.co/datasets/ExponentialScience/DLT-Sentiment-News 43https://www.statista.com/statistics/262946/most-common-languages-on-theinternet/ Hernandez Cruz et al."
        },
        {
            "title": "10 Ethics\nThe sentiment analysis dataset and DLT-Corpus could enable mar-\nket manipulation or coordinated trading strategies. We acknowl-\nedge this risk but note that such tools are already widely available,\nand our contribution primarily advances research transparency.",
            "content": "All data in DLT-Corpus derives from publicly available sources. For the scientific literature subset, we exclusively use open-access publications with documented licensing (via Semantic Scholar44) to ensure redistribution rights. Patent data is collected from USPTO, which explicitly states in its Terms of Use that patent text is typically not subject to copyright restrictions45. The social media subset aggregates previously published academic and industry datasets (see 4.1.3) collected before Twitter/X implemented significant API access restrictions and pricing changes in 2023 [21]. By posting publicly, users granted Twitter licenses to make their content available to other companies, organizations or individuals for distribution based on Twitter/Xs T&Cs at the time.46,47 Additionally, our research complies with General Data Protection Regulation (GDPR) principles, particularly Article 8948 academic research exemptions for processing publicly available data.49 Similarly, following GDPR guidelines (Article 5(1)(c)50), we apply data minimization51 by excluding usernames from social media posts, retaining only text content and timestamps. While Twitters T&Cs permitted the collection of public content, including username, we recognize that users privacy expectations may have changed since the original posting, and username removal reduces the risk of cross-platform tracking or other potential harms without compromising the utility of our dataset for the research purposes outlined in this work."
        },
        {
            "title": "11 Acknowledgements\nWe extend our gratitude to Max Bartolo for his valuable and con-\nstructive feedback for our dataset evaluation and language model\ntraining.",
            "content": "References [1] 2023. Scientific publishing has language problem. Nature Human Behaviour 2023 7:7 7, 7 (7 2023), 10191020. doi:10.1038/s41562-023-01679-6 [2] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart√≠n Bl√°zquez, Guilherme Penedo, Lewis Tunstall, Andr√©s Marafioti, Hynek Kydl√≠ƒçek, Agust√≠n Piqueres Lajar√≠n, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl√©mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. 2025. SmolLM2: When Smol Goes Big Data-Centric Training of Small Language Model. (2 2025). https://arxiv.org/abs/2502.02737v1 [3] Fernand Amesse and P. Cohendet. 2001. Technology transfer revisited from the perspective of the knowledge-based economy. Research Policy 30, 9 (12 2001), 14591478. doi:10.1016/S0048-7333(01)00162-7 [4] Linara Axanova. 2012. U.S. Academic Technology Transfer Models: Traditional, Experimental And Hypothetical. (2012). http://lesnouvelles.lesi.org/lesnouvell es2012/lesnouvellesPDFJune2012/Axanova.pdf 44https://www.semanticscholar.org/ 45https://www.uspto.gov/terms-use-uspto-websites 46https://x.com/en/tos/previous/version_17 47https://x.com/en/tos/previous/version_18 48https://gdpr.eu/article-89-processing-for-archiving-purposes-scientific-orhistorical-research-purposes-or-statistical-purposes/ 49https://gdpr-text.com/read/article-85/ 50https://gdpr.eu/article-5-how-to-process-personal-data/ 51https://europa.eu/youreurope/business/dealing-with-customers/dataprotection/data-protection-gdpr/index_en.htm DLT-Corpus: Large-Scale Text Collection for the Distributed Ledger Technology Domain [5] Nur Azmina, Mohamad Zamani, Jasy Liew, Suet Yan, and Ahmad Muhyiddin Yusof. 2022. XLNET-GRU Sentiment Regression Model for Cryptocurrency News in English and Malay. 3642 pages. https://aclanthology.org/2022.fnp-1.5/ [6] Adam Back, Matt Corallo, Luke Dashjr, Mark Friedenbach, Gregory Maxwell, Andrew Miller, Andrew Poelstra, Jorge Tim√≥n, and Pieter Wuille. 2014. Enabling Blockchain Innovations with Pegged Sidechains. (2014). [7] Anees Bahji, Laura Acion, Anne Marie Laslett, and Bryon Adinoff. 2023. Exclusion of the non-English-speaking world from the scientific literature: Recommendations for change for addiction journals and publishers. Nordic Studies on Alcohol and Drugs 40, 1 (2 2023), 613. doi:10.1177/14550725221102227 [8] Leemon Baird and Atul Luykx. 2020. The Hashgraph Protocol: Efficient Asynchronous BFT for High-Throughput Distributed Ledgers. 2020 International Conference on Omni-Layer Intelligent Systems, COINS 2020 (8 2020). doi:10.1109/COINS49042.2020.9191430 [9] Mark C. Ballandies, Marcus M. Dapp, and Evangelos Pournaras. 2022. Decrypting distributed ledger designtaxonomy, classification and blockchain community evaluation. Cluster Computing 25, 3 (6 2022), 18171838. doi:10.1007/S10586021-03256-W/FIGURES/12 Joachim Baumann, Paul R√∂ttger, Aleksandra Urman, Albert Wendsj√∂, Flor Miriam Plaza-del Arco, Johannes B. Gruber, and Dirk Hovy. 2025. Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation. (9 2025). https://arxiv.org/abs/2509.08825v1 [10] [12] [11] Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. 2025. Small Language Models are the Future of Agentic AI. (6 2025). https://arxiv.org/abs/2506.02153v1 Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: Pretrained Language Model for Scientific Text. EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference (2019), 36153620. doi:10.18653/V1/D19-1371 [13] Bruno Biais, Philip Bond, Jonathan Chiu, Rod Garratt, Niklas Haeusle, Shiyang Huang, Huisu Jang, Stephen Karolyi, Leonid Kogan, Jiasun Li, Tao Li, Evgeny Lyandres, Urban Jermann, Jonathan Payne, Julien Prat, Daniel Rabetti, Qihong Ruan, Fahad Saleh, Ville Savolainen, Donghwa Shin, Endong Yang, Jingjie Zhang, Shunming Zhang, Lin William Cong, Zhiheng He, and Ke Tang. 2025. The Tokenomics of Staking. (4 2025). doi:10.3386/W33640 [14] Blake Brittain. 2025. Judge explains order for New York Times in OpenAI copyright case Reuters. https://www.reuters.com/legal/litigation/judgeexplains-order-new-york-times-openai-copyright-case-2025-04-04/ [15] Eric Budish. 2025. Trust at Scale: The Economic Limits of Cryptocurrencies and Blockchains. The Quarterly Journal of Economics 140, 1 (1 2025), 162. doi:10.1093/QJE/QJAE [16] Vitalik Buterin. 2014. Ethereum: Next-Generation Smart Contract and Decentralized Application Platform. (2014). https://ethereum.org/content/whitepape r/whitepaper-pdf/Ethereum_Whitepaper_-_Buterin_2014.pdf [17] Richard Eckart De Castilho, Giulia Dore, Thomas Margoni, Penny Labropoulou, and Iryna Gurevych. 2018. Legal Perspective on Training Models for Natural Language Processing. https://aclanthology.org/L18-1202/ [18] Tyler A. Chang and Benjamin K. Bergen. 2022. Word Acquisition in Neural Language Models. Transactions of the Association for Computational Linguistics 10 (1 2022), 116. https://aclanthology.org/2022.tacl-1.1/ [19] E. Chen, N. Roche, Y.-H. Tseng, W. Hernandez, J. Shangguan, and A. Moore. 2023. Conversion of Legal Agreements into Smart Legal Contracts using NLP. In ACM Web Conference 2023 - Companion of the World Wide Web Conference, WWW 2023. doi:10.1145/3543873.3587554 [20] Lin William Cong, Yuanyu Qu, and Guojun Wang. 2025. Blockchains for environmental monitoring: theory and empirical evidence from China. Review of Finance 29, 5 (9 2025), 13031336. doi:10.1093/ROF/RFAF033 [21] Brittany I. Davidson, Darja Wischerath, Daniel Racek, Douglas A. Parry, Emily Godwin, Joanne Hinds, Dirk van der Linden, Jonathan F. Roscoe, Laura Ayravainen, and Alicia G. Cork. 2023. Platform-controlled social media APIs threaten open science. Nature Human Behaviour 2023 7:12 7, 12 (11 2023), 20542057. doi:10.1038/s41562-023-01750-2 Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Google, and Language. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North (2019), 41714186. doi:10.18653/V1/N19- [22] [23] Wenzhi Ding, Chen Lin, Yichen Luo, and Jiahua Xu. 2025. Decompose Market Manipulation Strategies: Evidence from On-chain Meme Coin Market. (9 2025). doi:10.2139/SSRN.5953738 [24] Honglin Fu, Yebo Feng, Cong Wu, and Jiahua Xu. 2025. textsc{Perseus}: Tracing the Masterminds Behind Cryptocurrency Pump-and-Dump Schemes. (3 2025). https://arxiv.org/abs/2503.01686v1 [25] Yu Gai, Liyi Zhou, Kaihua Qin, Dawn Song, and Arthur Gervais. 2023. Blockchain Large Language Models. (4 2023). https://arxiv.org/abs/2304.12749v2 [26] Amish Garg, Tanav Shah, Vinay Kumar Jain, and Raksha Sharma. 2021. CrypTop12: Dataset for Cryptocurrency Price Movement Prediction from Tweets and Historical Prices. Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021 (2021), 379384. doi:10.1109/ICMLA529 53.2021.00065 [27] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum√© Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (11 2021), 8692. doi:10.1145/3458723 [28] Anjee Gorkhali, Ling Li, and Asim Shrestha. 2020. Blockchain: literature review. Journal of Management Analytics 7, 3 (7 2020), 321343. doi:10.1080/23270012.2 020.1801529 [29] David Grangier, Angelos Katharopoulos, Pierre Ablin, and Awni Hannun Apple. 2024. Need Small Specialized Language Model? Plan Early! (2 2024). https: //arxiv.org/abs/2402.01093v2 [30] Dominique Gu√©gan and Thomas Renault. 2021. Does investor sentiment on social media provide robust information for Bitcoin returns predictability? Finance Research Letters 38 (1 2021), 101494. doi:10.1016/J.FRL.2020.101494 [31] Vincent Gurgul, Stefan Lessmann, and Wolfgang Karl H√§rdle. 2025. Deep learning and NLP in cryptocurrency forecasting: Integrating financial, blockchain, and social media data. International Journal of Forecasting (3 2025). doi:10.1016/J.IJ FORECAST.2025.02. [32] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Dont Stop Pretraining: Adapt Language Models to Domains and Tasks. Proceedings of the Annual Meeting of the Association for Computational Linguistics (2020), 83428360. doi:10.18653/V1/2020.ACLMAIN.740 [33] Eric Harris-Braun, Arthur Brock, and Paul Daoust. [n. d.]. Holochain Distributed Coordination by Scaled Consent, not Global Consensus. ([n. d.]). doi:10.1145/32 2186.322188 [34] Melissa Heikkila, Chris Cook, and Clara Murray. 2025. Americas top companies keep talking about AI but cant explain the upsides. https://www.ft.com/con tent/e93e56df-dd9b-40c1-b77a-dba1ca01e473 [35] Walter Hernandez Cruz, Firas Dahi, Yebo Feng, Jiahua Xu, Aanchal Malhotra, and Paolo Tasca. 2025. AMM-based DEX on the XRP Ledger. 2025 IEEE International Conference on Blockchain and Cryptocurrency (ICBC) (6 2025), 110. doi:10.1109/ ICBC64466.2025.11114626 [36] Walter Hernandez Cruz, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, and Jiahua Xu. 2025. Evolution of ESG-focused DLT research: An NLP analysis of the literature. Quantitative Science Studies 6 (8 2025), 124. doi:10.1162/QSS.A.7 [37] Walter Hernandez Cruz, Jiahua Xu, Paolo Tasca, and Carlo Campajola. 2024. No Questions Asked: Effects of Transparency on Stablecoin Liquidity During the Collapse of Silicon Valley Bank. (2024). https://arxiv.org/abs/2407.11716 [38] Kia Jahanbin, Mohammad Ali Zare Chahooki, and Fereshte Rahmanian. 2023. Database of influencers tweets in cryptocurrency (2021-2023). 2 (2023). doi:10.1 7632/8FBDHH72GS. [39] Lily Jamali. 2025. AI firm Anthropic agrees to pay authors $1.5bn for pirating work - BBC News. https://www.bbc.co.uk/news/articles/c5y4jpg922qo [40] Armand Joulin, √âdouard Grave, Piotr Bojanowski, and Tom√°≈° Mikolov. 2017. Bag of Tricks for Efficient Text Classification. 427431 pages. https://aclanthology.o rg/E17-2068/ [42] [41] Martin Juan, Jos√© Bucher, and Marco Martini. 2024. Fine-Tuned Small LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification. (6 2024). https://arxiv.org/abs/2406.08660v2 Inwon Kang, Maruf Ahmed Mridul, Abraham Sanders, Yao Ma, Thilanka Munasinghe, Aparna Gupta, and Oshani Seneviratne. 2024. Deciphering Crypto Twitter. Proceedings of the 16th ACM Web Science Conference, WebSci 2024 (5 2024), 331342. doi:10.1145/3614419.3644026 Jaehyun Kim, Thi Thu Huong Le, Sangmyeong Lee, and Howon Kim. 2024. Ethereum Smart Contracts Vulnerabilities Detection Leveraging Fine-Tuning DistilBERT. International Conference on Platform Technology and Service (2024), 133138. doi:10.1109/PLATCON63925.2024.10830749 [43] [44] Kate Knibbs. 2025. Meta Secretly Trained Its AI on Notorious Piracy Database, Newly Unredacted Court Docs Reveal WIRED. https://www.wired.com/story/ new-documents-unredacted-meta-copyright-ai-lawsuit/ [45] Olivier Kraaijeveld and Johannes De Smedt. 2020. The predictive power of public Twitter sentiment for forecasting cryptocurrency prices. Journal of International Financial Markets, Institutions and Money 65 (3 2020), 101188. doi:10.1016/J.INTF IN.2020. [46] Xianzhi Li, Samuel Chan, Xiaodan Zhu, Yulong Pei, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023. Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? Study on Several Typical Tasks. EMNLP 2023 - 2023 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Industry Track (2023), 408422. doi:10.18653/V1/2023.EMNLP-INDUSTRY.39 [47] Yuan Li, Bingqiao Luo, Qian Wang, Nuo Chen, Xu Liu, and Bingsheng He. 2024. CryptoTrade: Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading. EMNLP 2024 - 2024 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference (2024), 10941106. doi:10.18653/V1/20 24.EMNLP-MAIN.63 Hernandez Cruz et al. [71] [70] Thomas Sounack, Joshua Davis, Brigitte Durieux, Antoine Chaffin, Tom J. Pollard, Eric Lehman, Alistair E. W. Johnson, Matthew McDermott, Tristan Naumann, and Charlotta Lindvall. 2025. BioClinical ModernBERT: State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP. https: //arxiv.org/abs/2506.10896v1 Jonathan Stempel. 2025. SEC ends lawsuit against Ripple, company to pay $125 million fine Reuters. https://www.reuters.com/legal/government/sec-endslawsuit-against-ripple-company-pay-125-million-fine-2025-08-08/ Jianguo Sun, Yifan Jia, Yanbin Wang, Ye Tian, and Sheng Zhang. 2025. Ethereum fraud detection via joint transaction language model and graph representation learning. Information Fusion 120 (8 2025), 103074. doi:10.1016/J.INFFUS.2025.10 3074 (6 2025). [72] [73] Paolo Tasca and Claudio J. Tessone. 2017. Taxonomy of Blockchain Technologies. Principles of Identification and Classification. Ledger 4 (5 2017), 139. doi:10.5 195/LEDGER.2019.140 [74] Benjamin Warner, Antoine Chaffin, Benjamin Clavi√©, Orion Weller, Oskar Hallstr√∂m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2025. Smarter, Better, Faster, Longer: Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference. 1 (8 2025), 25262547. doi:10.18653/V1/2025.ACL-LONG.127 [75] Mark D. Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan Willem Boiten, Luiz Bonino da Silva Santos, Philip E. Bourne, Jildau Bouwman, Anthony J. Brookes, Tim Clark, Merc√® Crosas, Ingrid Dillo, Olivier Dumon, Scott Edmunds, Chris T. Evelo, Richard Finkers, Alejandra Gonzalez-Beltran, Alasdair J.G. Gray, Paul Groth, Carole Goble, Jeffrey S. Grethe, Jaap Heringa, Peter A.C. Hoen, Rob Hooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J. Lusher, Maryann E. Martone, Albert Mons, Abel L. Packer, Bengt Persson, Philippe Rocca-Serra, Marco Roos, Rene van Schaik, Susanna Assunta Sansone, Erik Schultes, Thierry Sengstag, Ted Slater, George Strawn, Morris A. Swertz, Mark Thompson, Johan Van Der Lei, Erik Van Mulligen, Jan Velterop, Andra Waagmeester, Peter Wittenburg, Katherine Wolstencroft, Jun Zhao, and Barend Mons. 2016. The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 2016 3:1 3, 1 (3 2016), 160018. doi:10.1038/sdata.2016.18 [76] Gavin Wood. 2016. Polkadot: Vision for Heterogeneous Multi-Chain Framework. (2016). [77] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. 2023. Stable and low-precision training for large-scale vision-language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems (NIPS 23). Curran Associates Inc., Red Hook, NY, USA. [78] Yang Xiao, Mohan Jiang, Jie Sun, Keyu Li, Jifan Lin, Yumin Zhuang, Ji Zeng, Shijie Xia, Qishuo Hua, Xuefeng Li, Xiaojie Cai, Tongyu Wang, Yue Zhang, Liming Liu, Xia Wu, Jinlong Hou, Yuan Cheng, Wenjie Li, Xiang Wang, Dequan Wang, and Pengfei Liu. 2025. LIMI: Less is More for Agency. (9 2025). https: //arxiv.org/abs/2509.17567v2 [79] Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2024. Efficient Continual Pretraining for Building Domain Specific Large Language Models. Findings of the Association for Computational Linguistics ACL 2024 (2024), 1018410201. doi:10.18653/V1/2024.FINDINGS-ACL.606 Jiahua Xu, Krzysztof Paruch, Simon Cousaert, and Yebo Feng. 2023. SoK: Decentralized Exchanges (DEX) with Automated Market Maker (AMM) Protocols. Comput. Surveys 55, 11 (11 2023), 150. doi:10.1145/3570639 [80] [81] Min Xu, Xingtong Chen, and Gang Kou. 2019. systematic review of blockchain. Financial Innovation 5, 1 (12 2019), 114. doi:10.1186/S40854-019-0147-Z [82] Guangguang Yang, Sen Niu, Bingrong Dai, Bofeng Zhang, Chao Li, and Yangjing Jiang. 2024. Named entity recognition method of blockchain patent text based on deep learning. Other Conferences (7 2024), 143. doi:10.1117/12.3031134 [83] Zhiju Yang, Gaoyuan Man, and Songqing Yue. 2023. Automated Smart Contract Vulnerability Detection using Fine-Tuned Large Language Models. ACM International Conference Proceeding Series (12 2023), 1923. doi:10.1145/3651655.3651658 [48] Zichao Li. 2025. Knowledge-Grounded Detection of Cryptocurrency Scams with Retrieval-Augmented LMs. (8 2025), 4048. doi:10.18653/V1/2025.KNOWLLM-1.4 [49] Gordon Liao and John Caramichael. 2022. Stablecoins: Growth Potential and Impact on Banking. International Finance Discussion Paper 2022, 1334 (2022), 126. doi:10.17016/ifdp.2022.1334 [50] Yuen C. Lo and Francesca Medda. 2020. Assets on the blockchain: An empirical study of Tokenomics. Information Economics and Policy 53 (12 2020), 100881. doi:10.1016/J.INFOECOPOL.2020.100881 Jinghui Lu, Maeve Henchion, and Brian Mac Namee. 2020. Diverging Divergences: Examining Variants of Jensen Shannon Divergence for Corpus Comparison Tasks. (2020), 1116. [51] [52] Yichen Luo, Yebo Feng, Jiahua Xu, and Yang Liu. 2026. Resisting Manipulative Bots in Meme Coin Copy Trading: Multi-Agent Approach with Chain-ofThought Reasoning. Proceedings of the ACM Web Conference 2026 (WWW 26), April 13√¢fi17, 2026, Dubai, United Arab Emirates 1 (1 2026). doi:10.1145/3774904. 3792635 [53] Sean McNally, Jason Roche, and Simon Caton. 2018. Predicting the Price of Bitcoin Using Machine Learning. International Euromicro Conference on Parallel, Distributed and Network-Based Processing (6 2018), 339343. doi:10.1109/PDP201 8.2018.00060 [54] Roberto Moncada, Enrico Ferro, Maurizio Fiaschetti, and Francesca Medda. 2024. Blockchain Tokens, Price Volatility, and Active User Base: An Empirical Analysis Based on Tokenomics. International Journal of Financial Studies 2024, Vol. 12, Page 107 12, 4 (10 2024), 107. doi:10.3390/IJFS12040107 [55] Satoshi Nakamoto. 2008. Bitcoin: peer-to-Peer Electronic Cash System. 552 557 pages. https://bitcoin.org/bitcoin.pdf [56] Leonardo Nizzoli, Serena Tardelli, Marco Avvenuti, Stefano Cresci, Maurizio Tesconi, and Emilio Ferrara. 2020. Charting the Landscape of Online Cryptocurrency Manipulation. IEEE Access 8 (2020), 113230113245. doi:10.1109/ACCESS .2020.3003370 [57] Anna Paula Pawlicka Maule and Kristen Marie Johnson. 2021. Cryptocurrency Day Trading and Framing Prediction in Microblog Discourse. Proceedings of the 3rd Workshop on Economics and Natural Language Processing, ECONLP 2021 (2021), 8292. doi:10.18653/V1/2021.ECONLP-1.11 [58] Branislav Pecher, Ivan Srba, and Maria Bielikova. 2025. Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance. (11 2025), 165184. doi:10.18653 /V1/2025.EMNLP-MAIN.9 [59] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aim√©e Cojocaru, Hamza Alobeidli, Alessandro Cappelli, B. Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only. Neural Information Processing Systems (2023). [60] Arif Perdana, Alastair Robb, Vivek Balachandran, and Fiona Rohde. 2021. Distributed ledger technology: Its evolutionary path and the road ahead. Information & Management 58, 3 (4 2021), 103316. doi:10.1016/J.IM.2020.103316 [62] [61] Audrey Pope. 2024. NYT v. OpenAI: The Timess About-Face - Harvard Law Review. https://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timessabout-face/ Jacob Portes, Alex Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. 2023. MosaicBERT: Bidirectional Encoder Optimized for Fast Pretraining. Advances in Neural Information Processing Systems 36 (12 2023). https://arxiv.org/abs/2312.17482v2 [63] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of machine learning research (2019). [64] Mayank Raikwar, Nikita Polyanskii, and Sebastian Muller. 2024. SoK: DAGbased Consensus Protocols. 2024 IEEE International Conference on Blockchain and Cryptocurrency, ICBC 2024 (2024). doi:10.1109/ICBC59979.2024.10634358 [65] Pornpanit Rasivisuth, Maurizio Fiaschetti, and Francesca Medda. 2024. An investigation of sentiment analysis of information disclosure during Initial Coin Offering (ICO) on the token return. International Review of Financial Analysis 95 (10 2024), 103437. doi:10.1016/J.IRFA.2024.103437 [66] R. L. Rivest, A. Shamir, and L. Adleman. 1978. method for obtaining digital signatures and public-key cryptosystems. Commun. ACM 21, 2 (2 1978), 120126. doi:10.1145/359340.359342 [67] Sougata Sarkar, Aditya Badwal, Amartya Roy, Koustav Rudra, and Kripabandhu Ghosh. 2025. CryptOpiQA: new Opinion and Question Answering dataset on Cryptocurrency. 1110711120 pages. doi:10.5281/zenodo.14469000 [68] Pavlo Seroyizhko, Zhanel Zhexenova, Muhammad Zohaib Shafiq, Fabio Merizzi, Andrea Galassi, and Federico Ruggeri. 2022. Sentiment and Emotion Annotated Dataset for Bitcoin Price Forecasting Based on Reddit Posts. FinNLP 2022 - 4th Workshop on Financial Technology and Natural Language Processing, Proceedings of the Workshop (2022), 203210. doi:10.18653/V1/2022.FINNLP-1. [69] Rion Snow, Brendan Oconnor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and Fast But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks. 254263 pages. https://aclanthology.org/D08-1027/ DLT-Corpus: Large-Scale Text Collection for the Distributed Ledger Technology Domain Table 6: Fields in Scientific Literature dataset Table 7: Fields in Patents dataset Field Description Field Description paperId title abstract text url year publicationDate venue publicationVenue publicationTypes authors references fieldsOfStudy s2FieldsOfStudy isOpenAccess openAccessPdf lang lang_conf total_tokens Semantic Scholar identifier Publication title Publication abstract Full text in Markdown format URL to the source document Publication year Full publication date Venue name Publication venue (journal, conference, etc.) Type of publication (e.g., JournalArticle) Author list with metadata Cited works Academic disciplines Semantic Scholars field classifications Open access flag Link to open access PDF if available Language code Language detection confidence Total number of tokens Datasets documentation We provide standardized documentation following the Datasheet for Datasets framework [27] and align with the FAIR Guiding Principles [75] to ensure that our datasets are Findable, Accessible, Interoperable, and Reusable for human researchers and computational agents. A.1 DLT-Corpus Motivation Purpose: DLT-Corpus52 was created to address the lack of largescale, domain-specific text corpora for NLP and other type of research in the Distributed Ledger Technology (DLT) field. Creators: Walter Hernandez Cruz, Peter Devine, Nikhil Vadgama, Paolo Tasca, Jiahua Xu Composition Content: 2.98 billion tokens across three subsets: Scientific literature: 37,440 documents, 564M tokens Patents: 49,023 documents, 1,296M tokens Social media: 22.03M documents, 1,120M tokens Temporal Coverage: Scientific: 1978-2025 Patents: 1990-2025 Social media: 2013-mid Language: English Missing Data: Social media posts after 2023 due to platform access restrictions. Confidentiality: No private or confidential data. All sources are publicly accessible. Social media usernames removed to protect privacy (see 10). Data Fields: Scientific Literature. Table 6 describes the fields in the scientific literature subset. DLT-Patents. Table 7 describes the fields in the patents subset. 52https://huggingface.co/collections/ExponentialScience/dlt-corpus Patent Number Document ID Title text Date Published Filing Date Application Number Family ID Inventor Assignee Applicant Name Primary Examiner Assistant Examiner CPCI CPCA OR XREF Notes Notes/Tagged Relevancy Database total_tokens USPTO patent number (e.g., US10123456B2) Unique document identifier Patent title Full patent text Publication date Application filing date USPTO application number Patent family identifier Inventor names Patent owner Applicant information USPTO primary examiner USPTO assistant examiner Cooperative Patent Classification Invention codes Cooperative Patent Classification Additional codes Other references Related patent references Additional notes Tagged annotations Relevancy score or classification Source database (USPGPUB or USPAT) Total number of tokens in the document Tweets. Table 8 describes the fields in the social media subset. Table 8: Fields in Tweets dataset"
        },
        {
            "title": "Description",
            "content": "tweet timestamp year language sentiment_class sentiment_label sentiment_score confidence_level total_tokens Tweet text Post creation time Post year Detected language Sentiment category Sentiment label Sentiment score Prediction confidence for sentiment label Token count Collection Scientific Literature53: Collected from Semantic Scholar API using domain-specific queries, filtered for domain relevance using finetuned BERT model (4.1.1). Patents54: Retrieved from USPTO public databases (USPGPUB, USPAT) using keyword searches (4.1.2). Social Media55: Aggregated from previously published academic datasets and publicly available industry sources, all collected before Twitter/Xs 2023 API restrictions (4.1.3). Preprocessing Scientific Literature: PDF parsing to Markdown, language detection, length filtering, domain relevance filtering (see 4.1.1 for more details). Patents: Text extraction, formatting standardization (e.g., fix encoding errors). Social Media: Username removal, duplicate detection, language filtering (see 4.1.3 for more details). 53https://huggingface.co/datasets/ExponentialScience/DLT-Scientific-Literature 54https://huggingface.co/datasets/ExponentialScience/DLT-Patents 55https://huggingface.co/datasets/ExponentialScience/DLT-Tweets Uses Intended Use: NLP research, language model development, innovation studies, text mining in DLT domain, and other social and computational linguistic studies. Unsuitable Uses: Identification of specific individuals, creating investment advice systems without proper disclaimers, and applications requiring post-2023 social media data. Impact: May enable market manipulation if misused. Researchers should implement appropriate safeguards. Distribution Access: https://huggingface.co/collections/ExponentialScience/dltcorpus-68e44e40d4e7a3bd7a224402 License: Scientific literature: Mixed open-access licenses (CC-BY, CC-BY-SA, CC0, and other permissive licenses). Individual license information is included in metadata where available. Patents: Public domain under USPTOs T&Cs. Patent text is typically not subject to copyright restrictions per USPTOs T&Cs56. Social media: Released under CC-BY-NC 4.0 for research purposes. Collected before changes in Twitter / Xs T&Cs in 202357,58, permitting academic research use [21]. Maintenance Updates: Currently static snapshot. Future versions may expand scientific literature and patents, but would likely not include post2023 social media. A.2 Sentiment Analysis Dataset Motivation Purpose: The DLT Sentiment Analysis Dataset59 was created to support sentiment analysis research in the DLT domain, addressing the lack of high-quality labeled data that captures domain-specific sentiment expressed by cryptocurrency community members. Creators: Walter Hernandez Cruz, Peter Devine, Nikhil Vadgama, Paolo Tasca, Jiahua Xu Composition Content: 23,301 examples with 1.85M tokens (average 79.51 tokens per example). Labels: Three sentiment dimensions with three categories each: Market direction: bullish, bearish, neutral Content quality: important, lol, neutral Engagement: liked, disliked, neutral Temporal Coverage: January 2021 to May 2025 Language: English Missing Data: None. Confidentiality: No private or confidential data. All content derived from publicly available cryptocurrency news articles headlines (and brief descriptions) voted on by CryptoPanic users. 56https://www.uspto.gov/terms-use-uspto-websites 57https://x.com/en/tos/previous/version_18 58https://x.com/en/tos/previous/version_17 59https://huggingface.co/datasets/ExponentialScience/DLT-Sentiment-News Hernandez Cruz et al. Table 9: Fields in Sentiment Analysis dataset Field Description title description text timestamp url source_url market_direction engagement_quality content_characteristics vote_counts total_votes total_tokens Article headline Article summary Combined title and description Publication time Original article link CryptoPanic news link Bullish/bearish/neutral label Liked/disliked/neutral label Important/lol/neutral label Votes per category Total vote count Token count Data Fields: Table 9 describes the fields in the sentiment analysis dataset. Collection Source: CryptoPanic platform, where cryptocurrency community users vote on news articles headlines and their brief descriptions across multiple sentiment categories. Annotation Method: Crowdsourced voting by active cryptocurrency users, providing domain expertise. Vote percentages normalized by total engagement, filtered using median minimum votes, with 25th and 75th percentiles as classification boundaries (4.2). Preprocessing Label Assignment: Percentile-based classification to mitigate popularity bias. Articles below the 25th percentile are labeled negative, above the 75th percentile are labeled positive, and those between are labeled neutral for each dimension (see 4.2 for more details). Quality Control: Minimum vote threshold applied to exclude articles with insufficient community engagement (4.2). Uses Intended Use: Sentiment analysis research, domain-specific model evaluation, market sentiment studies in DLT domain. Unsuitable Uses: Investment decision systems without proper disclaimers, identifying individual voters, and applications requiring real-time sentiment. Impact: May enable market manipulation if misused. Researchers should implement appropriate safeguards and ethical guidelines. Distribution Access: https://huggingface.co/datasets/ExponentialScience/DLTSentiment-News License: CC-BY-NC 4.0 for research purposes. Derived from publicly available CryptoPanic data with crowdsourced community annotations. Data collected via CryptoPanics free API between March and May 2025. To the best of our knowledge, the T&Cs at the time of collection (cryptopanic.com/terms/) contained no restrictions on academic research use or redistribution. Maintenance Updates: Currently static snapshot. Future versions may expand temporal coverage or add additional sentiment dimensions."
        }
    ],
    "affiliations": [
        "Centre for Blockchain Technologies, University College London",
        "Exponential Science",
        "School of Informatics, University of Edinburgh"
    ]
}