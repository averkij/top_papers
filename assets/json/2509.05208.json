{
    "paper_title": "Symbolic Graphics Programming with Large Language Models",
    "authors": [
        "Yamei Chen",
        "Haoquan Zhang",
        "Yangyi Huang",
        "Zeju Qiu",
        "Kaipeng Zhang",
        "Yandong Wen",
        "Weiyang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from a natural-language description. This task also serves as a lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable vector graphics (SVGs). We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards approach, where a format-validity gate ensures renderable SVG, and a cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers a precise and interpretable lens on cross-modal grounding."
        },
        {
            "title": "Start",
            "content": "Yamei Chen1,2,* Haoquan Zhang1,3,* Yangyi Huang1,2 Zeju Qiu4 Kaipeng Zhang3 Yandong Wen2 Weiyang Liu1, 1The Chinese University of Hong Kong 2Westlake University 3Shanghai Artificial Intelligence Laboratory 4Max Planck Institute for Intelligent Systems SphereLab.ai/SGP-Gen 5 2 0 S 5 ] . [ 1 8 0 2 5 0 . 9 0 5 2 : r Figure 1: Qualitative results of symbolic graphics programming. We use reinforcement learning with customized verifiable reward to finetune Qwen-2.5-7B. As training progresses, we can observe that the model acquires better compositional drawing ability, producing semantically accurate symbolic graphics programs."
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel at program synthesis, yet their ability to produce symbolic graphics programs (SGPs) that render into precise visual content remains underexplored. We study symbolic graphics programming, where the goal is to generate an SGP from natural-language description. This task also serves as lens into how LLMs understand the visual world by prompting them to generate images rendered from SGPs. Among various SGPs, our paper sticks to scalable *Equal contributions 1 vector graphics (SVGs), as they are widely used and can be easily rendered into images. We begin by examining the extent to which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, comprehensive benchmark covering object fidelity, scene fidelity, and compositionality (attribute binding, spatial relations, numeracy). On SGP-GenBench, we discover that frontier proprietary models substantially outperform open-source models, and performance correlates well with general coding capabilities. Motivated by this gap, we are interested in how to improve LLMs ability to generate SGPs. We propose reinforcement learning (RL) with verifiable rewards approach, where format-validity gate ensures renderable SVG, and cross-modal reward aligns text and the rendered image via strong vision encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to Qwen-2.5-7B, our method substantially improves SVG generation quality and semantics, achieving performance on par with frontier systems. We further analyze training dynamics, showing that RL induces (i) finer decomposition of objects into controllable primitives and (ii) contextual details that improve scene coherence. Our results demonstrate that symbolic graphics programming offers precise and interpretable lens on cross-modal grounding, while reinforcement learning with cross-modal rewards provides scalable way for injecting visual knowledge into LLMs."
        },
        {
            "title": "Introduction",
            "content": "Accurately describing complex scene using natural language is inherently difficult. Natural language often leaves room for ambiguity, lacking the precise spatial, geometric, and structural details needed to fully capture visual scene. While such flexibility is advantageous for everyday communication, it poses significant challenges for tasks that demand unambiguous, executable specifications of visual content. Symbolic graphics programs (SGPs) offer promising alternative, as they encode scenes as structured, formal representations that can be deterministically rendered into graphics content like images or 3D objects. By bridging the gap between abstract linguistic descriptions and concrete visual representations, SGPs provide means to represent scenes with both precision and compositionality. Motivated by advances in program synthesis with large language models (LLMs) [AON+21, NPH+22], we study their ability to perform symbolic graphics programming, which generates SGPs given natural language description. Given that LLMs are pretrained on large corpora of code, we expect them to be capable of understanding SGPs as specialized class of programs. [QLF+25, ZCZL24a] have shown that LLMs possess semantic understanding of SGPs. Building on these findings, we extend the research question to whether LLMs can generate SGPs. Unlike question answering over SGPs in [QLF+25], SGP generation demands more precise understanding of the correspondence between semantics and programs. Moreover, this task serves as lens into how well LLMs can both understand and synthesize the visual world by producing graphics objects (e.g., images) rendered from SGPs. Scalable Vector Graphics (SVGs) are representative form of SGPs and are widely available on the internet. As renderable programs, they bridge the visual and linguistic domains, framing SGP generation as semantic grounding task from natural language prompts to formal code. Because the outputs of SVGs can be directly visualized for semantic correctness, SVGs provide an ideal testbed for studying symbolic graphics programming with LLMs. In this work, we restrict the format of SGPs to SVGs, though our methodology can naturally extend to other SGP formats. We start with the first research question: To what extent can LLMs generate SGPs effectively? To investigate this, we introduce SGP-GenBench, large-scale benchmark designed to evaluate LLM performance across three dimensions: object-level accuracy, scene-level semantics, and compositional consistency. SGP-GenBench enables systematic comparison across models and provides diagnostic insights into their symbolic graphics programming capabilities. Using this benchmark, we conduct extensive evaluations of both proprietary and open-source models. Our results indicate that proprietary reasoningenhanced models consistently outperform non-reasoning variants, with performance strongly correlated with coding proficiency. Additionally, open-source LLMs, in contrast to proprietary ones, remain substantially less effective in generating valid and semantically aligned SVGs. 2 Figure 2: (a) Symbolic controllability of SGPs: to generate regular octagon, SGPs can deliver precise representation and fine-grained controllability, in contrast to the result from Qwen-Image [WLZ+25]. (b) Procedural generation: this example illustrates the procedural generation of park scene, where items are represented at different steps. The observation that open-source models still struggle to generate usable SGPs raises the second research question: How can we improve their SGP generation ability? To this end, we propose reinforcement learning (RL) approach that leverages similarity scores between visual encoder outputs and input text descriptions as the verifiable reward signals. This approach enables LLMs to progressively improve both the quality and semantic alignment of their SVG generation. Experiments show that our method can substantially enhance symbolic graphics programming: open-source LLMs that initially produced unrecognizable SVGs are trained to achieve performance comparable to state-of-the-art proprietary models. In addition, we analyze the results of the RL-trained model to better understand how its SVG generation evolves during training. Through comprehensive case studies and statistical analysis, we identify two key behaviors: (1) the model learns to generate longer and higher-quality SGPs by decomposing complex objects into simpler, more controllable elements, and (2) it produces additional, semantically related visual subjects that well align with the prompt. These findings suggest that our RL-trained model exhibits emergent behaviors beyond those directly optimized for. Our contributions are summarized below: We introduce SGP-GenBench, large-scale benchmark that comprehensively evaluates LLMs ability to generate SGPs across object-level accuracy, scene-level coherence, and compositional consistency. We enhance LLMs symbolic graphics programming with rule-based reinforcement learning, leveraging similarity between visual encoder outputs and input text descriptions as the verifiable reward. We provide an in-depth analysis of RL-trained models, showing that they exhibit emergent behaviors such as decomposing complex concepts into simpler elements and generating additional, semantically relevant objects."
        },
        {
            "title": "2 Symbolic Graphics Programming as Visual Synthesis",
            "content": "Symbolic graphics programming is the task of generating symbolic graphics programs from natural language instructions. Since symbolic graphics program can be deterministically rendered into unique graphical object (e.g., an SVG rendered as an image), the task can be regarded as form of visual synthesis. However, unlike conventional text-to-image generation, which relies on latent representations and pixel-based output, symbolic graphics programming operates by translating natural language into 3 formal language (i.e., from prompt to code). This distinction highlights its unique nature: visual generation through structured, interpretable, symbolic representations rather than through latent embeddings. As one of the most widely used visual representations on the internet, SVG serves as natural bridge between vision and language, making SVG generation semantic grounding task from prompt to code. The ability to render outputs provides an immediate means of verifying whether the generated program produces the intended result. For these reasons, we adopt SVG as the target representation in the experiments and analyses that follow. The reason why symbolic graphics programming is interesting can be understood through two defining properties of SGPs: SGPs are parametric, enabling precise expression. symbolic graphics program is inherently parametric: geometry is defined by numeric coordinates, control points, radii, angles, and affine transforms, while appearance is governed by discrete attributes and continuous values (e.g., stroke width, opacity). This parameterization provides precise and scalable control over positions, sizes, alignments, symmetries, and occlusions, allowing models to specify not only what to draw but also how to draw it with fine-grained accuracy. Since exact expression depends on precise parameter values, large language models that yield strong symbolic reasoning can generate accurate geometric graphics, which is capability that remains difficult for many text-to-image (T2I) systems. As illustrated in Figure 2(a), prompting an LLM and state-of-the-art T2I model (Qwen-Image [WLZ+25]) to produce regular octagon showed clear difference: the LLM generated SVG with correct vertex coordinates that rendered exactly as intended, whereas the T2I model failed to produce clean and text-aligned polygon. SGPs are procedural, enabling hierarchical semantic composition. symbolic graphics program also provides procedural description, whereby complex scenes are constructed from predefined primitives. In formats such as SVG, this procedural nature is expressed through hierarchical rendering, where later elements occlude earlier ones. This design enables distinct visual concepts to be assigned to separate layers, facilitating operations such as adding, removing, or duplicating elements, as well as reordering them, without disrupting the semantics encoded in individual components. As shown in Figure 2(b), we illustrate city-park scene by cumulatively stacking elements layer by layer, demonstrating SVGs strong compositional flexibility. More broadly, the complexity of the generated SGPs (e.g., program length, number of primitives, or nesting depth) can serve as characterization of the visual complexity underlying natural language scene description. Intuitively, simple prompts often map to concise programs with few elements, while richer and more detailed descriptions require longer programs with multiple objects, relations, and layered attributes. This provides structured and quantifiable lens to analyze scene complexity: program statistics can be directly correlated with the semantic richness of the input text and the perceptual intricacy of the target image. Beyond evaluation, such complexity measures can be used to guide curriculum learning, assess model scalability, or even benchmark the compositional reasoning ability of LLMs."
        },
        {
            "title": "Programming of LLMs",
            "content": "In this section, we introduce SGP-GenBench, large-scale benchmark designed to evaluate the symbolic graphics programming capabilities of LLMs. The three data components of the benchmark are detailed in Section 3.1, and the evaluation metrics are described in Section 3.2. Figure 3 gives an overview and some examples of our SGP-GenBench."
        },
        {
            "title": "3.1 Construction of SGP-GenBench",
            "content": "We present SGP-GenBench, consisting of three complementary components to comprehensively evaluate and benchmark the SGP generation capabilities of large language models: 4 Figure 3: Overview of the proposed SGP-GenBench and some examples. Scene generation capability on COCO-val, which contains 80 diverse object categories with rich descriptive captions depicting complex scenes with multiple objects and interactions, serving as the comprehensive scene component of our SGP-GenBench. The original validation set from the official 2017 split of MS-COCO [LMB+14] contains 5,000 images, from which we randomly sampled 1,024 examples for our evaluations to ensure computational efficiency while maintaining statistical significance. Object generation capability on SGP, validation set comprising 930 examples from our internetcollected SGP-Object-val dataset with captions generated by prompting gemini-2.5-flash-preview, primarily focusing on single object generation tasks to evaluate the models ability to render individual objects with high fidelity. Compositional generation capability on SGP-CompBench, which evaluates three key compositional aspects inspired by T2I-CompBench [HSX+23]: attribute binding (color, shape, texture), spatial relationships (2D, 3D, implicit relations), and numeracy (accurate generation of 3-10 objects). The benchmark contains 3,200 prompts for comprehensive evaluation. To ensure models are tested on compositionality of generation rather than object quality of generation, we used 80 common objects as item candidates in our prompt generation. See Table 6 for comprehensive list of the objects."
        },
        {
            "title": "3.2 Evaluation Metrics",
            "content": "We adopt two categories of evaluation metrics. The first assesses the semantic fidelity of objects and scenes, while the second evaluates the compositional quality of the generated outputs. To evaluate the semantic fidelity of objects and scenes, we report CLIP-Score [RKH+21, ZMKB23] (cross-modal cosine similarity between caption and image embeddings), DINO-Score [ODM+24] (cosine similarity of visual features to reference) and VQA-Score [HLK+23, LXT+22] (visual question answering accuracy on generated rasters), HPS v2 [WHS+23] (predicted human preference). For detailed introduction of each metric, refer to Section B.2. 5 For compositional quality, we design prompts for each task and ask the judge model to assess the compositionality of the model; the full prompts are listed in Section B.1. Each sub-task is scored out of 100 by asking judge model whether the generated SVG meets the prompt along specific dimension. Evaluation prompts for attribute bindings (color, shape, texture) and spacial relations (2D, 3D, implicit) are direct. For numeracy, we assess generation quality in three ways: accuracy of the total number (total), recognizability of all items (item), and correctness of the count for each distinct item (CPI: count per item). The overall numeracy score is the weighted sum of these three components, using weights 0.2, 0.2, and 0.6, respectively."
        },
        {
            "title": "3.3 Summary of Benchmark Results",
            "content": "We summarize the main findings from SGP-GenBench. The benchmark results are shown in two tables (Tables 1 and 2), and complete results along with more detailed analysis are provided in Section 5.2. SGP-GenBench reflects general model abilities. The ranking of models on our benchmark aligns well with their perceived general capabilities, especially in code generation. For example, Claude 3.7 Sonnet Thinking generally outperforms o3, which in turn surpasses Gemini 2.5 Pro Preview, followed by open-source systems like DeepSeek-R1 and Qwen-2.5-7B. This consistent ordering suggests that SVG generation is reliable indicator of broader model competence. Closed-source models remain strongest. Frontier systems achieve the best results not only on compositional reasoning tasks such as attribute binding and numeracy, where Claude 3.7 Sonnet Thinking reaches 90.5 on color binding and 89.4 on numeracy, but also on scene and object fidelity, where Gemini 2.5 Pro Preview attains the top DINO object score of 0.653 and strong VQA scene performance of 0.554. Our RL-trained model substantially narrows the gap. The RL post-trained Qwen-2.5-7B raises its overall compositional score from 8.8 to 60.8, outperforming all other open-source counterparts such as DeepSeek-R1 and QwQ-32B. It also achieves the best VQA score across all models at 0.596, slightly higher than Claude 3.7 Sonnet Thinking, demonstrating that reinforcement learning enables open-source models to approach the closed-source frontier."
        },
        {
            "title": "4 Eliciting Symbolic Graphic Programming of LLMs via Reinforcement",
            "content": "Learning with Cross-Modality Alignment Reward We introduce our problem formulation and reward design in this section, with schematic illustration of the method in Figure 4. Implementation details (tricks for stabilizing training and preventing reward hacking) are deferred to Section D."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "We start by formulating the symbolic graphics programming task as rule-based RL problem. Task. Let denote captions and denote valid SVG programs = (s1, . . . , sT ) of length over vocabulary . For caption we draw πθ( c), render it with deterministic renderer ˆx = R(s) RHW 3. We cast generation as single-episode Markov decision process whose state at step is the pair (c, s1:t1); the action is the next token st <eos>; the transition deterministically appends st; the process terminates when <eos> is emitted or the sequence reaches length Tmax; and scalar reward r(s, c, x) is issued once upon termination. Objective. We optimize the policy parameters θ to maximize the expected reward under the data 6 distribution µ: J(θ) = (c,x)µ sπθ( c) (cid:2)r(s, c, x)(cid:3), (4.1) where each data entry consists of caption and optionally reference image x. Policy update (GRPO). We adopt GRPO [SWZ+24], critic-free variant of PPO [SWD+17]. For each caption we sample SVG programs {si}G i=1 . With clip range ϵ and reward Ri = r(si, c, x), JGRPO(θ) = (c,x)µ,{si}G i=1πθ(c) where ˆAi,t and ri,t are defined as"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 si si (cid:88) (cid:16) (cid:16) min t=1 ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:17)(cid:17) . (4.2) Ri mean (cid:16) (cid:16) std {Ri}G i=1 {Ri}G i=1 (cid:17) (cid:17) , ri,t(θ) = πθ(si,t (c, si,1:t1)) πθold(si,t (c, si,1:t1)) . (4.3) ˆAi,t ="
        },
        {
            "title": "4.2 Reward Design",
            "content": "For every trajectory (caption c, generated SVG program s) we assign scalar reward that factorizes into an outer format gate and an inner perceptual term: r(s, c, x) = rfmt(s) (cid:16) λText rText(s, c) + λImage rImage(s, x) (cid:17) . (4.4) The binary term rfmt {0, 1} (defined below) guarantees that only syntactically valid, renderable code propagates perceptual rewards. Throughout, we keep λText = 1 unless stated otherwise. When reference image is unavailable, we set λImage = 0 without changing Equation (4.4)."
        },
        {
            "title": "4.2.1 Format–Validity Reward",
            "content": "We set format-validity reward as binary reward: rfmt(s) = 1, 0, if passes both checks below, otherwise. (4.5) ThinkAnswer structure check. Each LLM response must follow the prompt template <THINK> . . . </THINK> <ANSWER> . . . </ANSWER>, where the ANSWER block contains SVG code and the THINK block contains reasoning process. We apply lightweight regular expression to verify the presence and order of these tags. Renderer check. The extracted SVG string is rendered through CairoSVG in Python. Successful conversion to raster image without exceptions constitutes pass."
        },
        {
            "title": "4.2.2 Text-Image Alignment Reward\n(cid:1), such as CLIP [RKH+21], SigLIP\nWe employ a generic language-image contrastive model E = (cid:0)ftext, fimg\n[ZMKB23], or any successor trained with a contrastive objective. Given the caption c and the rendered\nimage ˆx = R(s), we obtain unit-normalized embeddings",
            "content": "t = ftext(c) ftext(c)2 , = fimg(ˆx) fimg(ˆx)2 . (4.6) 7 Figure 4: An illustration of the RL pipeline. Given text description, we sample group of SVG codes from the model and render them as images. Each SVG code is scored by the alignment between the rendered image and the text description. The advantages are calculated based on the scores, and used for updating the model. The raw similarity cos(t, v) = tv [1, 1] is linearly rescaled to the interval [0, 1]: rText(s, c) = 1 2 (cid:0)cos(t, v) + 1(cid:1). (4.7) No additional learnable parameters are introduced, maintaining the simplicity of the signal. Because it relies solely on the caption and the model-generated image, rText is defined for all prompts, serving as the main supervisory signal in open domain settings where no reference image is provided. 4.2.3 Image-Image Alignment Reward The image-image term rImage is computed only when reference image is available alongside the caption. We extract global (image-level) features with self-supervised vision encoder FDINO [ODM+24]: zgt = FDINO(x), zgen = FDINO(ˆx). (4.8) Then the similarity is measured by the cosine of the two embeddings and then linearly scaled to [0, 1]: which is used as the visual-fidelity component of the overall reward. rImage(s, x) = (cid:0)cos(zgen, zgt) + 1(cid:1), 1 2 (4.9)"
        },
        {
            "title": "4.3 Discussions and Intriguing Insights",
            "content": "In this section, we discuss why applying reinforcement learning with verifiable rewards to symbolic graphics programming is conceptually compelling. Distilling knowledge from vision foundation models. Our reward function is defined by external vision foundation models (e.g., DINO, CLIP), which provide strong semantic and geometric supervision signals. Through reinforcement learning, the LLM gradually aligns its generations with the representations and judgments of these vision models. This process can be viewed as form of implicit distillation, where the LLM internalizes the visual priors, spatial reasoning, and semantic grounding capabilities embedded in large vision models. Beyond improving native visual understanding, such distillation enhances cross-modal alignment, enabling the LLM to better reason about text-SGP-image correspondences, capture fine-grained visual semantics, and ground abstract descriptions in concrete procedural structures. Training without ground truth SGPs. Unlike supervised finetuning (SFT), our RL approach does not require paired image-program annotations or ground truth SGPs. Instead, it can operate directly on 8 Figure 5: Qualitative comparison of SVGs generated by frontier LLMs and our RL-trained model. Our model achieves results comparable in quality to state-of-the-art commercial models, while generating graphics that are more natural and detailed. raw images, using external vision foundation models to provide feedback signals. This removes the costly and often infeasible requirement of constructing large-scale datasets of image-program pairs, which are difficult to collect and scale beyond narrow domains. By learning directly from images, the model can generalize to more diverse and open-ended visual inputs, while the reward mechanism ensures that the generated programs remain semantically faithful to the underlying visual content. This paradigm enables scalable training at internet scale where explicit program annotations are unavailable. Alignment between linguistic world and visual world. Our RL approach, guided by rewards from vision foundation models, can be viewed as an effective alignment between the LLMs linguistic understanding (expressed through symbolic graphics programs, SGPs) and the visual knowledge embedded in powerful vision models. In this process, the LLM learns not only to map natural language into structured symbolic programs, but also to ensure that these programs are consistent with the perceptual judgments of vision models. This dual alignment anchors abstract linguistic semantics to concrete visual evidence, narrowing the gap between how language describes scene and how vision perceives it. Such grounding improves the reliability of the generated SGPs and enhances cross-modal reasoning."
        },
        {
            "title": "5.1 Training Data",
            "content": "We train on balanced mixture of two sources, COCO 2017 captions and MMSVG-Illustration-40k, yielding 95,026 training examples. From the official MS-COCO 2017 split (118,287 images, each with five human-written captions), we randomly sample 47,513 image-caption pairs. From the MMSVG-2MIllustration corpus [YCC+25], we select 47,513 SVGs after filtering out text-centric items (e.g., letter, Chinese character, text). COCO provides broad, human-authored captions of real-world images, while MMSVG provides detailed 9 Model CLIP DINO VQA Sce. Obj. Avg. Sce. Obj. Avg. Sce. Obj. Avg. Sce. HPS Obj. Avg. Frontier closed-source LLMs GPT-4o-mini GPT-4o o1-mini o1 o3-mini o3 o4-mini Gemini 2.0 Flash Gemini 2.5 Flash Preview Gemini 2.5 Pro Preview Claude 3.5 Sonnet Claude 3.7 Sonnet Claude 3.7 Thinking Open-source LLMs QwQ-32B DeepSeek-R1 Qwen-2.5-7B Qwen-2.5-7B w/ RL (ours) 0.207 0. 0.221 0.220 0.231 0.253 0.246 0. 0.222 0.256 0.240 0.262 0.262 0.219 0. 0.155 0.258 0.278 0.284 0.289 0.285 0. 0.283 0.296 0.275 0.286 0.302 0.293 0. 0.305 0.272 0.278 0.213 0.286 0.243 0. 0.255 0.252 0.262 0.268 0.271 0. 0.254 0.279 0.266 0.284 0.284 0.245 0. 0.184 0.272 0.021 0.031 0.028 0.031 0. 0.067 0.052 0.023 0.033 0.088 0. 0.088 0.090 0.031 0.042 0.008 0.102 0.573 0. 0.603 0.607 0.608 0.595 0.629 0. 0.297 0.316 0.315 0.319 0.322 0. 0.340 0.306 0.603 0.653 0.318 0.371 0.624 0. 0.642 0.340 0.368 0.366 0.295 0. 0.330 0.354 0.379 0.521 0.469 0. 0.349 0.554 0.428 0.465 0.497 0. 0.520 0.520 0.482 0.536 0.468 0. 0.572 0.528 0.380 0.417 0.419 0. 0.450 0.502 0.503 0.381 0.424 0. 0.478 0.581 0.594 0.567 0.574 0.574 0.584 0.118 0. 0.121 0.122 0.128 0.153 0.143 0. 0.125 0.154 0.174 0.182 0.185 0. 0.187 0.180 0.193 0.176 0.183 0. 0.146 0.153 0.153 0.153 0.158 0. 0.168 0.146 0.154 0.177 0.140 0.165 0.190 0. 0.165 0.183 0.164 0.199 0.181 0.549 0.594 0. 0.566 0.290 0.318 0.204 0.334 0.334 0. 0.265 0.632 0.456 0.508 0.385 0.560 0.395 0. 0.325 0.596 0.123 0.134 0.103 0.150 0.172 0.180 0. 0.177 0.147 0.157 0.125 0.164 Table 1: Performance on scene (COCO-val) and object (SGP-Object-val) generation. Bold marks the best per group. Our RL-tuned model substantially improves over its base (Qwen-2.5-7B) and is competitive with frontier closed-source models. captions of vector-graphics. The mixture of the two datasets balances between scene-level and object-level captions, and removing text-related images avoids shortcuts via text rendering."
        },
        {
            "title": "5.2 Main Results on SGP-GenBench",
            "content": "We begin by comparing our RL-tuned 7B model with frontier openand closed-source LLMs on scene and object generation, reporting both quantitative metrics and qualitative examples in Section 5.2.1. We then turn to compositional evaluation on SGP-CompBench in Section 5.2.2. Together, these evaluations provide complete picture of both fidelity and structured reasoning in SVG generation."
        },
        {
            "title": "5.2.1 Fidelity on Object and Scene Generation",
            "content": "Table 1 compares our model against frontier closedand open-source LLMs. We report CLIP-Score, DINO-Score, VQA-Score, and HPS v2. Claude 3.7 Sonnet and Thinking leads CLIP and HPS, while Gemini 2.5 Pro Preview tops DINO; however, our Qwen-2.5-7B w/ RL attains the best overall VQA score 0.596, exceeding all models. RL lifts the 7B base strongly across metrics, pushing it into the frontier performance band. Across splits, objects generally score higher than scenes on CLIP and HPS, and DINO on scene generation remains low for all due to the photo-vector domain gap. Overall, RL closes much of the gap to proprietary models, yielding superior task-grounded faithfulness while keeping aesthetic quality competitive. For qualitative evaluation, we compare our model with other frontier LLMs by examining SVGs generated from four prompts selected from COCO-val and SGP-Object-val, as shown in Figure 5. The results demonstrate that our model usually produce images with enhanced detail fidelity for scene generation. For instance, in response to the prompt \"A person on surfboard riding on small wave\", while competing models render only the basic elements (water, surfboard, and athlete), our model incorporates an additional light blue wave layer that accurately represents the foam characteristic of 10 Model Attribute Binding Color Shape Texture Avg. Spatial Relation 2D 3D Implicit Avg. Total Numeracy Item CPI Overall Avg. Frontier closed-source LLMs GPT-4o o1 o3 o4-mini Gemini 2.5 Flash Preview Gemini 2.5 Pro Preview Claude 3.7 Sonnet Claude 3.7 Sonnet Thinking Open-source LLMs QwQ-32B DeepSeek-R1 Qwen-2.5-7B Qwen-2.5-7B - RL (Ours) 62. 70.8 88.9 82.4 63.6 88.1 89.3 90. 54.3 72.6 7.1 84.3 48.7 25.2 73. 62.1 45.0 65.7 82.8 85.6 51.0 62. 10.0 71.3 34.3 53.0 71.7 69.6 56. 74.9 77.3 82.4 31.4 48.4 1.7 46.0 48. 49.6 78.1 71.4 55.2 76.2 83.1 86. 45.6 61.2 6.3 67.2 49.7 54.6 81.6 71. 46.0 77.4 75.9 80.2 43.6 59.3 5. 55.7 37.3 39.4 62.0 57.9 38. 59.1 59.4 74.4 33.5 43.8 5.8 53.9 49. 46.4 84.5 76.5 57.1 80.0 73.7 86. 46.0 58.2 8.1 61.7 45.4 46.8 76. 68.5 47.3 72.2 85.9 66.4 91. 90.3 82.8 94.7 25.5 20.1 59. 52.9 34.5 68.0 51.1 41.7 81. 76.1 62.0 83.8 69.7 80.3 91.4 94.9 85.5 65.5 78.9 91. 41.0 53.7 6.4 57.1 79.9 83.5 42.6 63. 21.1 35.4 5.8 47.5 51.4 60.4 10.7 57. 52.7 42.0 78.8 74.3 59.8 82. 82.5 89.4 50.9 57.4 16.1 56.8 48.3 46. 77.5 71.0 53.4 76.2 77.9 84.8 45. 57.4 8.8 60.8 Table 2: Compositional generation results on SGP-CompBench, broken down into attribute binding (color binding, shape binding and texture binding), relation (2D relation, 3D relation and implicit relation), and numeracy (total count, item existence and count per item (CPI)). Average scores are provided for each category and overall. See Table 8 for more performances of more models. actual surfing conditions. Similarly, for the motorcycle example, our model generates significantly more detailed components, including distinctive red elements representing the \"tail lights\"."
        },
        {
            "title": "5.2.2 Compositional Generation",
            "content": "Following the T2I CompBench [HDS+25] protocol, in Table 2, we evaluate the compositional capabilities of our model using the SGP-CompBench benchmark. This evaluation is divided into three main aspects: attribute binding (including color, shape, and texture), spatial relationship (covering 2D, 3D, and implicit relations), and numeracy (assessing the models ability to generate images with object counts ranging from 3 to 10). Detailed evaluation setup can be found in Section B.1. Attribute binding. All models perform significantly better on color and shape binding compared to texture binding. Our RL-tuned model mirrors this pattern. While narrowing the gap with frontier closedsource models, it performs strongly on color/shape binding but still lags on texture binding, reflecting the inherent difficulty of encoding textures in SVG. This disparity reflects the inherent characteristics of SVG representations. Colors are simple parameters (e.g., fill=\"red\", stroke=\"#00FF00\") and shapes can be controlled through geometric primitives and their attributes, whereas textures such as the highlights and reflections of metallic apple demand extra drawing operations beyond parameter tweaks. Since patterns that cannot be directly assigned through parameters are difficult to express in SVG, the performance drop is therefore expected. Spatial relationship. Models generally handle 2D and implicit relations better than 3D ones. Precisely placing elements suffices for 2D, and implicit cues (e.g., watch, wear) tolerate loose layouts. But 3D scenes require ordering code so that later shapes occlude earlier ones, which is harder under SVGs top-to-bottom rendering, hence lower scores. Numeracy. All models demonstrate strong capabilities in overall counts, with Claude 3.7 Thinking achieving an impressive 94.9% accuracy, yet stumble on Count-Per-Item: they may draw seven objects but not the asked three apples and four bananas. CPI rises alongside general generation quality, implying that recognizable objects are prerequisite for correct numeracy recognition. We also observe that the Item score fluctuates more than the CPI score. The Item score reflects whether specific object exists, while the CPI measures the accuracy of the predicted quantity under the assumption that the object"
        },
        {
            "title": "Encoder",
            "content": "VQA SGP"
        },
        {
            "title": "COCO",
            "content": "Avg."
        },
        {
            "title": "COCO",
            "content": "Diversity SGP Avg."
        },
        {
            "title": "COCO",
            "content": "Text-Image Encoders CLIP ViT-B/32 CLIP ViT-L/14 SigLIP Base/16-384 SigLIP Large/16-384 0. 0.575 0.632 0.628 0.554 0.567 0.560 0.549 0. 0.571 0.596 0.589 0.110 0.135 0.184 0. Vision-Only Encoders (with SigLIP Base/16-384) w/o Vision Encoder DINOv2-ViT-S/14 DINOv2-ViT-B/14 DINOv2-ViT-L/14 DINOv2-ViT-G/ 0.632 0.630 0.632 0.632 0.627 0.560 0. 0.558 0.561 0.561 0.596 0.595 0.595 0. 0.594 0.184 0.208 0.174 0.176 0. 0.157 0.178 0.194 0.193 0.194 0.165 0. 0.145 0.138 0.134 0.157 0.189 0.203 0. 0.187 0.157 0.161 0.152 0.157 0. 0.164 0.150 0.164 0.168 0.173 0.167 0. HPS SGP 0.187 0.182 0.185 0.177 0. 0.182 0.182 0.188 0.182 Avg. 0.172 0. 0.175 0.164 0.175 0.175 0.178 0.177 0. Table 3: Ablation of embedding models on COCO-val and SGP-Object-val. Bold numbers mark the best results for each column. is present. Since the CPI explicitly conditions the judge on object existence, it naturally emphasizes numerical accuracy. Thus, the larger swings in Item scores suggest the model still struggles with generating semantically correct objects, whereas the more stable and higher CPI values indicate it can count reliably once recognition is established."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "To disentangle the factors behind our performance gains, we systematically ablate three core components of the RL pipeline: (i) the reward stack (Section 5.3.1), (ii) the presence of explicit Chain-of-Thought prompting (Section 5.3.2), and (iii) the choice of RL algorithm (Section E.1). Each ablation reveals how design choices affect quality and diversity. For the computation details of the diversity, see Section B.2."
        },
        {
            "title": "5.3.1 Effect of Different Embedding Models",
            "content": "Our reward pipeline relies on (i) text-image encoders to score semantic alignment and (ii) vision-only encoders to judge visual similarity (see Section 4.2). To quantify the impact of each choice, we compare the influence of different embedding models. For text-image encoder ablations, we train only with textimage similarity reward, while for vision-only encoder ablations we add both text-image and image-image similarity rewards with equal coefficients (1.0) and the text-image encoder is fixed to SigLIP Base/16-384. Each setting is trained to step 750 for comparability. As shown in Table 3, three observations emerge from the ablation. SigLIP yields stronger grounding than CLIP. Replacing CLIP with SigLIP consistently boosts factual grounding (VQA) and diversity of generated results, the gap is especially significant on the natural-image COCO-val split but narrows on the synthetic-caption SGP-Object-val set, suggesting that SigLIPs trainingon more diverse and semantically rich image-text databetter captures realworld semantics, whereas abstract shape-caption alignment benefits less from this advantage. Embedding model size does not strongly correlate with performance. Within each family, larger encoders do not always guarantee higher VQA: SigLIP Large/16-384 raises Diversity yet loses few VQA points relative to the smaller Base, and CLIP ViT-L/14 only marginally beats ViT-B/32. Vision-only reward gives little improvements on VQA but improves HPS. Adding lightweight S/14 reward atop SigLIP gives no VQA gain, whereas L/14 yields slightly better VQA; the extra-large G/14 gives diminishing VQA score. In general adding vision-only rewards tends to reduce diversity but 12 Variant CLIP DINO VQA COCO SGP Avg. COCO SGP Avg. COCO SGP HPS Avg. COCO SGP Diversity Avg. COCO SGP Avg. w/ CoT 0.258 w/o CoT 0.259 0.286 0.272 0.102 0.566 0.334 0.632 0.560 0. 0.164 0.185 0.175 0.282 0.271 0. 0.555 0.327 0.621 0.548 0.585 0.172 0.186 0. 0.184 0.177 0.194 0.189 0.269 0.223 Table 4: Ablation of Chain-of-Thought prompting on COCO-val and SGP-Object-val. Performance differences across metrics are marginal, indicating CoT is not essential for quantitative performance. (a) Best-of-N metrics vs. log (b) (Best-of-N ) metrics vs. log Figure 6: Analysis of the Best-of-N performance of RL checkpoints. (a): Best-of-N curves for the SigLIP-Base score. (b): Gain over the first checkpoint; the horizontal intercept indicates the value of required to match RL-trained later checkpoints. Each curve corresponds to checkpoint at 30, 120, 210, 300, 630, and 900 RL steps. The y-axis shows the text-image similarity computed with SigLIP Base/16-384. improves alignment with human preference. Because additional vision encoders yield only marginal gains, we report all final results using fixed reward stack of SigLIP Base/16-384."
        },
        {
            "title": "5.3.2 Effect of CoT Prompting",
            "content": "We test whether explicit Chain-of-Thought prompting is necessary for performance or primarily beneficial for interpretability. We train the two prompting variants on Qwen2.5-7B for 750 RL steps under an identical reward stack: (1) With CoT : model outputs self-explanation before the SVG; and (2) Without CoT : model outputs the SVG directly. As shown in Table 4, the ablation confirms that explicit CoT prompting is not prerequisite for strong quantitative performance: across CLIP, DINO, VQA, HPS and Diversity the two variants differ only slightly. SVG generation itself already induce implicit planning, so the program itself functions as the reasoning trace, and the contribution of verbal reasoning is negligible."
        },
        {
            "title": "6 Additional Analysis of the RL-tuned Model",
            "content": "In this section, we analyze how reinforcement learning with verifiers (RLVR) changes the models SVG drawing behavior and capability. First, we plot the improvement of Best-of-N sampling performance as training progresses in Section 6.1. To better understand the source of this improvement, we analyze the behavioral patterns that emerge during RLVR training in Section 6.2. We discuss an unexpected color-choice bias of different embedding models in Section 6.3. 13 (a) Evolution of elements count (b) Evolution of code length Figure 7: Training dynamics of code complexity. Error bars reflect variability across prompts and sampling replicates. Figure 8: Although both express the concept of motorcycle, the early-stage model at training step 30 only divides the concept into four levels, with poor semantic representation and inaccurate relative positioning across levels. By step 900, the later model splits the concept into eight parts, achieving accurate semantics and precise positional encoding at every level."
        },
        {
            "title": "6.1 Reinforcement Learning vs. Best-of-N Sampling",
            "content": "Compared to measuring performance by sampling once, Best-of-N metrics report the best score among parallel samples and can better measure the models potential to complete task. [YCL+25] has shown that in the math domain the improvement of RL training can be offset by Best-of-N sampling with moderate parallel samples N. We thus want to verify if RL can improve the model capability in non-trivial way. In Figure 6(a) we show that RL shifts the entire Best-of-N curve upward for every checkpoint, although for larger N, the gap shrinks. To determine where curves intersect, we subtract the baseline curve (checkpoint 30) from later curves in Figure 6(b) and linearly fit each curve to locate the where advantage vanishes. All RL checkpoints intersect with the baseline between 106 and 108 samples, which is three to six orders of magnitude larger than the 102-103 reported in [YCL+25]. This indicates that for the task of symbolic graphics programming, approximately million candidates would be required to match the benefits RL brings within 1K steps of training, demonstrating that RL delivers qualitative improvements that naive Best-of-N decoding cannot realistically achieve within practical compute budgets. 14 Figure 9: Examples of optional details generated without explicit prompting. (a) Given only group of people sit at table with cake, the model adds sprinkles on the cake. (b) Given only beach-related prompt, the model introduces waves, sand, and surfer. These unrequested elements are consistent with the scene and enhance its naturalness and completeness. (a) Comment-to-element ratio vs. training step (b) Optional comment ratio vs. training step Figure 10: Quantitative evolution of generated code structure over training."
        },
        {
            "title": "6.2 How SGP Generation Capabilities Evolve during Training",
            "content": "Figure 7 shows that both the number of SVG elements and the total code length increase steadily over training. This trend indicates that the policy progressively adopts richer scene structure and longer programs to draw according to the prompts. Closer inspection of the generated code reveals two consistent behaviors that explain these curves. Strategy 1: Decomposition into basic components. The model typically emits short comment followed by several lines of SVG that implement that sub-concept. Early in training, single comment often precedes block of code. As training progresses, the policy decomposes complex objects into multiple parts that are easier to draw and to place precisely on the canvas, yielding more accurate geometry and spatial relationships (Figure 8). Throughout, the model relies on limited set of elements to draw(e.g., rect, circle, line, path); see Section E.3 for distribution of used drawing primitives, suggesting that improved drawings arise from better composition of fixed toolbox rather than expanding it. Strategy 2: Contextual optional details. Beyond literal prompt fulfillment, the policy increasingly introduces plausible, unrequested elements that improve coherence and realisme.g., sprinkles on cake 15 Caption Image SVG excerpt Caption Image SVG excerpt CLIP reward SigLIP reward Man laughing standing next to his motorcycle with his bicycle attached to it. People walking by blue train next to mountain. <rect x=\"0\" y=\"200\" width=\"400\" height=\"100\" fill=\"green\" stroke=\"none\"/> Man laughing standing next to his motorcycle with his bicycle attached to it. <rect x=\"210\" y=\"240\" width=\"30\" height=\"15\" fill=\"royalblue\" stroke=\"black\" stroke-width=\"2\"/> People walking by blue train next to mountain. <rect x=\"10\" y=\"140\" width=\"280\" height=\"45\" fill=\"#A0AAAA\"/> <polygon points=\"30,50 60 ,20 90,50 110,75 130,60 160, 45 190,55 210,70 240, 50 260,80 290,100\" fill=\"#99B09E\" /> Table 5: Color choices under different reward models. CLIP prefers canonical colors like red and blue, while SigLIP prefers low-saturation colors like #948E8F. for people at table with cake, or waves, sands, and surfer in beach scenes (Figure 9). These decoration details are consistent with the scene and contribute to perceived completeness. We quantify these behaviors in Figure 10. The comment-to-element ratio increases with training, reflecting finer-grained decomposition as shown in Figure 10(a), while in Figure 10(b) the fraction of comments annotated (optional) rises, indicating more frequent addition of contextual details."
        },
        {
            "title": "6.3 Color Preferences Under Different Reward Models",
            "content": "An unexpected stylistic discrepancy emerges when we compare different text-image encoders. Under CLIP reward the policy gravitates toward canonical color wordsfill=\"red\", fill=\"blue\"whereas the SigLIP reward favors delicate, low-saturation hexadecimal colors such as fill=\"#948E8F\". We place the two behaviors side by side in Table 5. Qualitatively, SigLIP-rewarded outputs appear less saturated and more diversed. This suggests that SigLIP encourages finer color matching, encouraging the model to move beyond the basic palette."
        },
        {
            "title": "7 Related Work",
            "content": "Text-to-SVG Generation. Early pioneering models for vector graphics generation [CDAT20, EJBF22, LHES19, RGLM21] were limited to producing relatively simple or domain-specific images. Additionally, since these approaches typically required curating custom SVG datasets and training models from scratch, they often suffered from poor generalization and did not support text-guided generation. These critical limitations were overcome with the introduction of diffusion-based pipelines, which enabled powerful textto-SVG generation. Diffusion-based pipelines such as VectorFusion [JXA23] and SVGDreamer [XZW+24] optimize vector primitives by back-propagating pixel-space losses from text-conditioned diffusion models. Moving beyond pure diffusion, LLM-centric approaches [WSML23, XHL+25] design SVG-aware encoding: Chat2SVG [WSL25] lets an LLM emit coarse template that diffusion stage refines, NeuralSVG [PAR+25] learns an implicit MLP scene representation trained with score-distillation, while StarVector [RPA+25] frames SVG code as sequence in multi-modal transformer. General-purpose chat LLMs can also generate valid SVG, yet their performance relies on their enormous scalemotivating our RL-based method that achieve comparable proficiency with substantially smaller models. Reinforcement-Learning Post-Training for LLMs. RLHF [ZSW+19, OWJ+22] popularized by InstructGPT [OWJ+22] finetunes policies with PPO [SWD+17] against reward model fitted to human preferences. RLAIF [LPM+24, BKK+22] replaces these labels with AI-generated preferences. complementary line, RLVR, uses hard verifiers instead of subjective scores: DeepSeek-R1 [GYZ+25] shows that purely rule-based reward from mathematical verifiers elicit strong reasoning. follow-up works apply RLVR in coding [LWG+22, GZL+23, GZC+25] and agentic tool-use [QLI+24, WYL+25]. Our work 16 inherits RLVR and RLAIFs label efficiency with foundation models providing rewards, extending RLVRs simple rule-based scenario to reward that reflects complex human preference and perception. Vision and Multi-modal Foundation Models. Our rewards are calculated by angular distance of vectors in the embedding space of foundation encoders. DINOv2 [ODM+24] extracts rich visual features, letting us judge visual similarity of an SVG render and reference image. CLIP [RKH+21] aligns images and text in shared space, while SigLIP [ZMKB23] refines CLIPs contrastive objective with pairwise sigmoid loss. Together these encoders eliminate task-specific labeling and keep our RL loop lightweight. Benchmarking LLMs on Vector Graphics Processing. Evaluating Large Language Models (LLMs) for vector graphics processing is nascent but growing research area. While several benchmarks have been proposed, they often fall short in assessing the complexity of real-world SVG generation. For instance, BBH [SSS+22] utilizes SVG primitives to assess LLMs understanding of basic geometric shapes. SGP-Bench [QLF+25] focuses on LLMs ability to semantically understand symbolic graphics programs, using SVG code as an indicator of 2D modality comprehension, but does not evaluate LLMs code generation capabilities. SVGEditBench [NM24, NM25] proposes benchmark for assessing LLMs ability to modify SVG code, though the required modifications remain largely simple. While VGBench [ZCZL24b] broadens the scope by evaluating both LLMs understanding and generation of vector graphics, its SVG generation benchmarks are confined to simple SVG icons, lacking the intricate complexity characteristic of professional vector designs. In contrast, our proposed SGP-GenBench directly addresses this gap by providing benchmark specifically designed to evaluate LLMs ability to generate complex vector graphics, offering significantly more challenging and comprehensive assessment than existing methods."
        },
        {
            "title": "8 Concluding Remarks",
            "content": "In this paper, we address two key questions: (1) What is the current quality of symbolic graphics program generation by large language models? and (2) How can it be improved? To this end, we first introduce SGP-GenBench, benchmark that evaluates LLMs ability to generate SGPs along three dimensions: object-level accuracy, scene-level coherence, and compositional consistency. Second, we propose posttraining approach that finetunes models with rule-based reinforcement learning, where the similarity between the rendered image and the input text serves as verifiable reward. Experimental results demonstrate that the finetuned model generates more accurate and detailed SGPs while also acquiring effective generation strategies. Looking ahead, promising directions include developing adaptive curricula, analyzing the evolution of models internal processes, and exploring whether enhanced drawing skills can transfer to broader reasoning tasks."
        },
        {
            "title": "References",
            "content": "[AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 2 [BKK+22] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 16 [CDAT20] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. Deepsvg: hierarchical generative network for vector graphics animation. volume 33, 2020. 16 [EJBF22] Valeria Efimova, Ivan Jarsky, Ilya Bizyaev, and Andrey Filchenkov. Conditional vector graphics generation for music cover images. arXiv preprint arXiv:2205.07301, 2022. 16 [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 16 [GZC+25] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. In ICML, 2025. 16 [GZL+23] Philip Gorinski, Matthieu Zimmer, Gerasimos Lampouras, Derrick Goh Xin Deik, and Ignacio Iacobacci. Automatic unit test data generation and actor-critic reinforcement learning for code synthesis. In EMNLP, 2023. 16 [HDS+25] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 11 [HLK+23] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In ICCV, 2023. 5, 26 [HSX+23] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. In NeurIPS, volume 36, 2023. [JXA23] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models. In CVPR, 2023. 16 [LHES19] Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. learned representation for scalable vector graphics. In ICCV, 2019. 16 [LMB+14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV. Springer, 2014. 5 [LPM+24] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: scaling reinforcement learning from human feedback with ai feedback. In ICML, 2024. [LWG+22] Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS, volume 35, 2022. 16 [LXT+22] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. In EMNLP, 2022. 5 [NM24] Kunato Nishina and Yusuke Matsui. Svgeditbench: benchmark dataset for quantitative assessment of llms svg editing capabilities. In CVPR, 2024. 17 [NM25] Kunato Nishina and Yusuke Matsui. Svgeditbench v2: benchmark for instruction-based svg editing. arXiv preprint arXiv:2502.19453, 2025. [NPH+22] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022. 2 18 [ODM+24] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 5, 8, 17, 26 [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, volume 35, 2022. 16 [PAR+25] Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, and Daniel Cohen-Or. Neuralsvg: An implicit representation for text-to-vector generation. arXiv preprint arXiv:2501.03992, 2025. 16 [QLF+25] Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Xiao, Katherine Collins, Joshua Tenenbaum, Adrian Weller, Michael Black, and Bernhard Schölkopf. Can large language models understand symbolic graphics programs? In ICLR, 2025. 2, [QLI+24] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. 16 [RGLM21] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy Mitra. Im2vec: Synthesizing vector graphics without vector supervision. In CVPR, 2021. 16 [RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 5, 7, 17, 26 [RPA+25] Juan Rodriguez, Abhay Puri, Shubham Agarwal, Issam Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images and text. In CVPR, 2025. 16 [SSS+22] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [SWD+17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 7, 16 [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 7 [WHS+23] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 5, 26 19 [WLZ+25] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3, [WSL25] Ronghuan Wu, Wanchao Su, and Jing Liao. Chat2svg: Vector graphics generation with large language models and image diffusion models. In CVPR, 2025. 16 [WSML23] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Iconshop: Text-guided vector icon synthesis with autoregressive transformers. ACM Transactions on Graphics (TOG), 42(6), 2023. 16 [WSZ+23] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-toimage models with human preference. arXiv preprint arXiv:2303.14420, 1(3), 2023. 26 [WYL+25] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025. [XHL+25] Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. Empowering llms to understand and generate complex vector graphics. In CVPR, 2025. 16 [XLW+23] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 26 [XZW+24] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. Svgdreamer: Text guided svg generation with diffusion model. In CVPR, 2024. 16 [YCC+25] Yiying Yang, Wei Cheng, Sijin Chen, Xianfang Zeng, Fukun Yin, Jiaxu Zhang, Liao Wang, Gang Yu, Xingjun Ma, and Yu-Gang Jiang. Omnisvg: unified scalable vector graphics generation model. arXiv preprint arXiv:2504.06263, 2025. [YCL+25] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 14 [YZZ+25] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 29 [ZCZL24a] Bocheng Zou, Mu Cai, Jianrui Zhang, and Yong Jae Lee. Vgbench: Evaluating large language models on vector graphics understanding and generation. arXiv preprint arXiv:2407.10972, 2024. 2 [ZCZL24b] Bocheng Zou, Mu Cai, Jianrui Zhang, and Yong Jae Lee. Vgbench: Evaluating large language models on vector graphics understanding and generation. arXiv preprint arXiv:2407.10972, 2024. 17 [ZMKB23] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 5, 7, [ZSW+19] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Data Curation",
            "content": "A.1 SGP-Single-9k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 SGP-CompBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Evaluation Details",
            "content": "B.1 SGP-CompBench Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Metrics Details for Scene and Object Evaluation . . . . . . . . . . . . . . . . . . . . . . 22 22 22 23 26 More Results on SGP-CompBench"
        },
        {
            "title": "27\nC.1 Model Performance on CompBench Throughout Training . . . . . . . . . . . . . . . . . . 27",
            "content": "C.2 Full Results on SGP-CompBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Training Details and Experimental Settings",
            "content": "D.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Preventing Entropy Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Prohibition of Text Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Additional Experiments and Analysis",
            "content": "E.1 Comparison between GRPO and PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Effect of TrainingData Mixture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Element Type Distribution Shifts During Training . . . . . . . . . . . . . . . . . . . . . 29 29 29 29 29 29 30 E.4 Model Behaviors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 E.5 Evolution of Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Data Curation",
            "content": "A.1 SGP-Single-9k We first identified the main categories needed for our collection, then determined the appropriate subcategories. We gathered SVG data by crawling categorized content from searchable SVG repositories such as SVGRepo. After deduplication, we used vision-language model (Gemini-2.0-Flash) to generate captions for each image. Our quality checks confirmed this model was well-suited for the captioning task. Our collection resulted in 9,268 SVG samples. We reserved 10% for the evaluation set, with some subcategories receiving higher sampling rates to ensure diversity. This yielded 930 samples for evaluation purposes. The remaining data was initially intended for training. However, considering that MMSVG might already contain similar samples, we decided not to use this data for training. We plan to release this dataset to the research community in the future. To prevent models from exploiting SVGs text rendering capabilities to directly output text and artificially inflate evaluation scores, we implemented strict filtering mechanisms. Specifically, we removed all SVG samples containing text-related tags such as <text>, <tspan>, and <textPath> from our dataset. Additionally, we filtered out prompts containing words related to text elements, including: text, word, letter, character, symbol, number, digit, font, script, write, written, writing, typography, label, caption, title, name, sign, signature, logo, slogan, spell, phrase, quote, and message. This ensures that models must express content through drawing graphical elements rather than simply rendering text, allowing for more accurate assessment of true drawing capabilities."
        },
        {
            "title": "Prompt Used for Text Detection",
            "content": "Prompt: Based on the following image description, analyze whether the image likely contains any text, letters, words, numbers, characters, symbols, or other textual elements. Image Description: \"description\" First provide your reasoning about whether the description suggests text is present in the image. Then provide your determination with simple \"Contains text: [Yes/No]\" on new line. Example output: The description mentions \"a logo with the company name written below\", which clearly indicates there is text in the image. Contains text: Yes Another example: The description only mentions \"a red circle with blue background\" with no mention of any text, letters, or symbols. Contains text: No A.2 SGP-CompBench We followed the methodology of T2ICompBench, utilizing the same relation and binding vocabulary, but substituted the object names with the 80 most frequently occurring object nouns from COCO. The primary difference in our approach lies in the evaluation process. Due to the significant distribution gap between SVG images and natural photographs, we were unable to employ trained BLIP models or detection models to assist with scoring. Instead, we relied exclusively on LLMs as judge models for evaluation purposes. For our SGP-CompBench evaluation, we employed two complementary approaches to generate test prompts:(1)LLM-Generated Prompts. For both binding and relation tasks, we used large language model to create 600 prompts for each category. This approach ensured linguistic diversity and natural phrasing in our test set.(2)Template-Generated Prompts. For numeracy tasks, we utilized code templates to systematically generate prompts covering object counts from 3 to 10. For each count value, 22 we created 100 prompts, resulting in 800 numeracy test cases. This methodical approach allowed us to comprehensively evaluate the models ability to handle different quantities.to The combined approach of LLM-generated and template-generated prompts provided robust and diverse evaluation framework, enabling thorough assessment of our models compositional generation capabilities across different aspects of sgp generation."
        },
        {
            "title": "B Evaluation Details",
            "content": "B.1 SGP-CompBench Evaluation Details This section provides the detailed evaluation protocol and prompt design for SGP-CompBench, including the standardized instructions for SVG generation and the quantitative evaluation criteria. For the numeracy evaluation, we employ three-step assessment process: verifying the accuracy of total object count (Total), confirming the presence of all required objects (Item Presence), and validating the correct count per specific item (Count Per Item, CPI). These three metrics are weighted at 0.2, 0.2, and 0.6 respectively to calculate the final numeracy score. To obtain quantitative results, we adopt the Model-as-a-Judge (MAJ) framework, where visionlanguage model is prompted to assess whether the generated image satisfies the requirements specified in the caption for each subtask. For each aspect, we design clear and specific prompts tailored to the corresponding evaluation criterion. The model outputs score ranging from 0 to 100. B.1."
        },
        {
            "title": "80 Common Objects List",
            "content": "We list here the 80 common objects in Table 6. ID Category 1 person 6 bus 11 fire hydrant 16 cat 21 elephant 26 umbrella 31 skis 36 baseball glove 41 wine glass 46 bowl 51 broccoli 56 cake 61 dining table 66 remote 71 toaster 76 vase ID Category 2 bicycle 7 train 12 stop sign 17 dog 22 bear 27 handbag 32 snowboard 37 skateboard 42 cup 47 banana 52 carrot 57 chair 62 toilet 67 keyboard 72 sink 77 scissors"
        },
        {
            "title": "ID Category",
            "content": "3 car 8 truck 13 parking meter 18 horse 23 zebra 28 tie 33 sports ball 38 surfboard 43 fork 48 apple 53 hot dog 58 couch 63 tv 68 cell phone 73 refrigerator 78 teddy bear ID Category 4 motorcycle 9 boat 14 bench 19 sheep 24 giraffe 29 suitcase 34 kite 39 tennis racket 44 knife 49 sandwich 54 pizza 59 potted plant 64 laptop 69 microwave 74 book 79 hair drier ID Category 5 airplane 10 traffic light 15 bird 20 cow 25 backpack 30 frisbee 35 baseball bat 40 bottle 45 spoon 50 orange 55 donut 60 bed 65 mouse 70 oven 75 clock 80 toothbrush Table 6: The 80 common objects for SGP-CompBench. B.1.2 SVG Generation Instruction For the SVG generation task in SGP-CompBench, the following standardized instruction is used to prompt the model:"
        },
        {
            "title": "SVG Generation Instruction",
            "content": "You are an expert in generating SVG code. Your task is to carefully analyze the description and produce only the corresponding SVG code. Do not generate any images or explanationsoutput strictly the SVG code that fulfills the following description. Description: [description] B.1.3 Evaluation Prompts and Scoring Criteria To quantitatively assess the generated SVGs, we design specific evaluation prompts and scoring rubrics for different aspects: attribute binding, relation, and numeracy. The following are the evaluation prompts and their corresponding scoring criteria. We used Gemini-2.5-Flash-Preview as our judge model used in evaluation on SGP-CompBench, for the superior vision-language understanding capability of the model. We used Gemini-2.5-Flash-Preview as our judge model used in evaluation on SGP-CompBench, for the superior vision-language understanding capability of the model."
        },
        {
            "title": "Prompt Used for Attribute Binding Evaluation",
            "content": "Prompt: Evaluate whether the image matches the following prompt: [PROMPT] Scoring criteria: 100: All items are recognizable and the binding between items and their attributes is correct. 50: All items are recognizable, but the binding between items and their attributes is incorrect or unclear. 30: Items are not recognizable, but the attribute binding appears correct. 0: Items are not recognizable and the binding between items and their attributes is incorrect. Response format: REASONING: [your reasoning] SCORE: [score]"
        },
        {
            "title": "Prompt Used for Relation Evaluation",
            "content": "Prompt: Evaluate whether the image matches the following prompt: [PROMPT] Scoring criteria: 100: The items are clear and the relation between items is correct. 50: The items are not clear, but the relation between items is correct. 30: The items are clear, but the relation between items is incorrect. 0: The items are not clear and the relation between items is incorrect. Response format: REASONING: [your reasoning] SCORE: [score] 24 Prompt Used for Numeracy Evaluation (Total Count) Prompt: Evaluate whether the image contains exactly [TOTAL_COUNT] distinct items in total (they do not need to be recognizable, but should be clearly individual objects). Scoring criteria: 100: All items in the image are clearly individual objects, and the total count is correct. 50: All items are clearly individual objects, but the total count is incorrect. 30: Some items are clearly individual objects, and the total count is incorrect. 0: The items are not clearly individual objects and the total count is incorrect. Response format: REASONING: [your really brief reasoning] SCORE: [score] Prompt Used for Numeracy Evaluation (Item Presence) Prompt: Check whether the image contains the following items: [ITEM LIST]. Scoring criteria: 100: The image contains all the items listed above. 50: The image contains most of the items listed above. 30: The image contains some of the items listed above. 0: The image does not contain any of the items listed above. Response format: REASONING: [your really brief reasoning] SCORE: [score] 25 Prompt Used for Numeracy Evaluation (Count Per Instance) Prompt: Evaluate whether the image contains exactly [COUNT] distinct [NOUN] in total. Scoring criteria: 100: The image contains exactly [COUNT] distinct [NOUN], and they are clearly individual objects. 50: The image does not contain all the [COUNT] distinct [NOUN], but the count is close to [COUNT]. 30: The image does not contain all the [COUNT] distinct [NOUN], but the count is far from [COUNT]. 0: The image does not contain any of the [COUNT] distinct [NOUN]. Response format: REASONING: [your really brief reasoning] SCORE: [score] B.2 Metrics Details for Scene and Object Evaluation CLIP-Score. For each generated image we compute the cosine similarity between its vision embedding and the caption embedding using two CLIP [RKH+21] models (ViT-B/32, ViT-L/14). The final score is the arithmetic mean over both models. DINO-Score. Cosine similarity between the CLS tokens of the generated image and the reference image, averaged across four DINOv2 [ODM+24] variants (DINOv2-ViT-S/14, DINOv2-ViT-B/14, DINOv2-ViT-L/14, DINOv2-ViT-G/14 ). Diversity. For each prompt we sample SVGs, extract DINOv2 [ODM+24] features (all four models), compute the pairwise cosine similarities, and report 1 mean(similarity) as diversity score, averaged across the four encoders. VQA-Score. Following [HLK+23], we generate set of questionanswer pairs about the content of each image, then ask visionlanguage model (VLM) to answer based on the generated raster. The score is the fraction of correct answers, averaged over all prompts. Human Preference Score (HPSv2). Human Preference Scores [WHS+23, WSZ+23, XLW+23] are widely used to evaluate text-to-image models. These scores predict how likely humans would prefer an image based on large-scale ranking data. We use HPSv2 [WHS+23] to measure the perceptual quality of our rendered SVG images. 26 More Results on SGP-CompBench C.1 Model Performance on CompBench Throughout Training As reinforcement learning training progresses, our model demonstrates significant improvement trends on the CompBench benchmark. Table 7 illustrates the performance changes across different training checkpoints (from step 30 to step 780)."
        },
        {
            "title": "Step",
            "content": "Attribute Binding Color Shape Texture BindAvg Relation 2D 3D Implicit RelAvg Total Numeracy Item CPI NumAvg Avg 060 090 120 150 180 240 270 300 330 360 420 450 480 510 540 600 630 660 690 720 780 8.3 0.0 57.4 62.4 63. 61.2 73.3 73.7 76.5 74.2 81. 81.0 82.3 82.1 82.5 80.8 80. 80.6 83.4 80.7 85.0 84.9 82. 84.3 85.5 84.7 0.0 9.9 50. 50.6 51.6 50.1 62.2 47.5 63. 65.5 68.5 63.9 66.5 68.3 67. 70.1 71.2 66.3 64.5 65.7 71. 69.5 69.5 69.7 70.3 70.9 0. 0.0 32.9 21.4 23.9 21.1 28. 58.5 42.5 24.0 28.0 47.0 55. 46.5 55.6 54.4 44.1 45.9 33. 39.0 57.4 41.6 35.0 39.6 54. 35.7 3.0 3.3 47.0 44.8 46. 44.1 54.2 59.7 62.1 54.6 59. 63.9 68.0 65.6 68.5 68.4 65. 64.3 60.3 61.8 71.4 65.3 62. 64.5 70.2 63.8 9.8 0.0 26. 40.3 32.6 28.8 42.2 44.5 54. 39.9 44.1 45.2 41.3 49.2 46. 44.0 47.0 53.1 47.6 49.2 51. 47.0 54.2 44.5 53.7 45.1 13. 0.0 31.4 41.9 39.8 37.1 44. 46.3 46.3 43.0 42.5 46.5 45. 47.7 49.1 52.4 54.1 53.4 53. 56.3 52.0 52.2 45.3 52.2 56. 52.9 10.2 0.0 17.1 23.4 30. 62.6 52.7 40.6 62.1 57.0 60. 65.0 50.9 52.0 63.8 63.2 53. 56.6 46.7 63.5 58.0 42.7 56. 52.1 59.8 64.3 11.0 0.0 25. 35.2 34.3 42.9 46.4 43.8 54. 47.0 49.0 52.2 45.8 49.6 53. 53.2 51.5 54.4 49.2 56.3 53. 47.3 52.0 49.6 56.6 54.1 14. 4.9 29.4 30.1 42.4 31.1 43. 57.3 56.8 54.8 56.8 61.0 61. 62.7 60.3 58.4 56.6 58.6 59. 55.9 56.9 57.0 55.1 56.0 55. 55.5 1.9 2.1 12.2 18.6 22. 23.1 24.9 39.5 39.2 41.4 45. 46.5 44.1 47.5 45.7 48.3 47. 48.6 46.8 47.9 45.1 48.9 44. 46.7 46.7 44.4 8.9 6.4 28. 34.4 39.9 41.3 42.3 56.2 58. 59.8 63.1 64.2 59.9 62.6 61. 63.0 61.0 59.0 57.6 54.4 53. 56.5 54.5 57.1 57.6 52.4 8. 5.2 25.3 30.4 37.0 35.6 39. 49.7 51.8 55.1 58.3 60.0 57. 59.6 57.8 59.1 57.4 56.8 55. 53.4 52.5 55.1 52.7 54.8 55. 51.4 7.4 2.5 33.4 37.6 39. 41.5 47.6 50.7 56.7 51.9 55. 58.6 56.9 58.1 60.0 60.4 58. 58.8 55.0 57.6 60.1 56.0 56. 56.5 61.3 57.1 Table 7: Compositional generation results on SGPCompBench during training, broken down into attribute binding (color, shape, and texture), relation (2D, 3D, and implicit), and numeracy (total count, item existence, and CPI). Average scores are provided for each category and overall. 27 C.2 Full Results on SGP-CompBench We show the full results on SPG-CompBench in Table 8. Model Attribute Binding Color Shape Texture Avg. Relation Numeracy 2D 3D Implicit Avg. Total Item CPI Avg. Avg Frontier open-source LLMs QwQ-32B DeepSeek-R1 54. 72.6 Frontier closed-source LLMs GPT-4o-mini GPT-4o o1-mini o3-mini o3 o4-mini Gemini 2.0 Flash Gemini 2.5 Flash Preview Gemini 2.5 Pro Preview Claude 3.5 Sonnet Claude 3.7 Sonnet Claude 3.7 Sonnet Thinking 60.8 62.2 60. 70.8 60.5 88.9 82.4 58.7 63. 88.1 75.3 89.3 90.5 51.0 62. 52.1 48.7 47.5 25.2 46.7 73. 62.1 49.5 45.0 65.7 71.2 82. 85.6 31.4 48.4 39.0 34.3 46. 53.0 55.1 71.7 69.6 37.7 56. 74.9 57.1 77.3 82.4 45.6 61. 43.6 33.5 59.3 43.8 50.6 48.4 51.4 49. 54.1 78.1 71.4 48.6 55.2 76. 67.9 83.1 86.2 39.1 36.1 49.7 37.3 43.8 30. 54.6 39.4 64.7 43.9 81.6 62.0 71.0 57.9 43.7 31.8 46.0 38. 77.4 59.1 62.0 50.4 75.9 59.4 80.2 74.4 Our open-source baseline and RL-tuned models Qwen-2.5-7B Qwen-2.5-7B w/ RL (750) Qwen-2.5-7B w/ RL (900) 7.1 84.3 86.3 10.0 71.3 74. 1.7 46.0 60.7 6.3 67.2 74. 5.2 5.8 55.7 53.9 50.4 51.1 46.0 58. 42.9 49.2 46.3 46.4 61.6 84. 76.5 40.6 57.1 80.0 65.0 73. 86.4 8.1 61.7 62.4 41.0 53. 79.9 83.5 21.1 35.4 51.4 60. 50.9 57.4 39.4 45.4 40.3 46. 56.8 76.0 68.5 38.7 47.3 72. 59.1 69.7 80.3 6.4 57.1 54. 80.3 85.9 89.3 66.4 90.8 91. 90.3 85.9 82.8 94.7 87.1 91. 94.9 42.6 63.4 57.5 19.5 25. 28.1 20.1 34.5 59.8 52.9 24. 34.5 68.0 44.5 65.5 78.9 5. 47.5 46.6 37.4 51.1 58.6 41. 66.4 81.1 76.1 52.1 62.0 83. 75.0 85.5 91.4 10.7 57.6 55. 45.7 52.7 58.2 42.0 64.7 78. 74.3 54.2 59.8 82.3 71.3 82. 89.4 16.1 56.8 53.8 45.2 57. 44.3 48.3 48.9 46.7 57.7 77. 71.0 47.1 53.4 76.2 65.4 77. 84.8 8.8 60.8 61.7 Table 8: Compositional generation results on SGPCompBench, broken down into attribute binding (color binding, shape binding and texture binding), relation (2D relation, 3D relation and implicit realtion), and numeracy (total count, item existenece and count per item (CPI)). Average scores are provided for each category and overall."
        },
        {
            "title": "D Training Details and Experimental Settings",
            "content": "D.1 Experimental Setup All experiments run on single node with eight NVIDIA H100 GPUs (80 GB each) in BF16 mixed precision; optimisation uses AdamW with constant learning rate of 1 106, global batch of 128 captions (8 GPUs 16 micro-batches), and gradient-norm clipping at 1.0. We run the RL algorithms without reference model. The implementation builds on the open-source oat-zero1 framework. For inference, throughout training and evaluation we set temperature 1.0 and top_p 1.0. For inference of baseline models, we set temperature 0.7, keep the other parameters as default, with an exception of Qwen2.5-7B setting temperature 1.0 and top_p 1.0. We run all our experiments on Nvidia 8xH100 GPU node. D.2 Preventing Entropy Collapse Early experiments with symmetric PPO clip range of clip_range = 0.2 drove the policys token-level entropy to near zero, producing degenerate, highly repetitive SVGs. Following [YZZ+25], we adopt asymmetric clipping: clip_high = 0.28, clip_low = 0.20. The wider positive bound allows larger updates when the advantage > 0, fostering exploration of new token sequences, while the tighter negative bound still prevents destructive policy shifts for < 0. This simple change restores healthy entropy trajectory without sacrificing stability or requiring additional entropy bonuses. D.3 Prohibition of Text Elements Because CLIP rewards can be gamed by rendering the caption verbatim, we extend the FormatValidity reward (Section 4.2.1) with strict ban on SVG text-rendering tags. Concretely, the parser rejects any output containing <text>, <tspan> and <textPath>. Violation sets rfmt(s) = 0, nullifying downstream perceptual rewards and hence providing strong learning signal against this exploit. This prohibition is applied consistently during training and evaluation to ensure fair assessment of the models genuine drawing ability."
        },
        {
            "title": "E Additional Experiments and Analysis",
            "content": "E.1 Comparison between GRPO and PPO We compare GRPO, critic-free variant of PPO, against standard PPO on identical settings, with same backbone, reward stack, and data split, to isolate the effect of algorithm choice. For both setting we train for 1 020 steps on the identical Qwen2.5-3B backbone. The results are presented in Table 9. In general, GRPO outperforms PPO in all metrics, while PPO yields higher diversity. E.2 Effect of TrainingData Mixture To disentangle the influence of training corpora from that of reward models, we fix the reward stack to SigLIP Base/16-384 and omit the DINOv2 model and vary only the ratio of COCO captions to MMSVG-Illustration-40k captions: Baseline (50 % COCO / 50 % MMSVG) the mix used in all previous experiments; 1https://github.com/sail-sg/oat-zero 29 Algorithm GRPO PPO CLIP VQA COCO SGP Avg. COCO SGP Avg. COCO SGP Avg. COCO SGP Avg. COCO SGP Avg. Diversity DINO HPS 0.259 0.283 0.271 0.089 0.555 0.322 0.596 0.522 0.559 0.159 0.177 0.168 0.242 0. 0.259 0.083 0.532 0.308 0.511 0. 0.511 0.157 0.142 0.150 0.178 0. 0.188 0.249 0.245 0.247 Table 9: Comparison of GRPO and PPO after 1,020 steps on COCO-val and SGP-Object-val. GRPO achieves better alignment and semantic accuracy. Figure 11: Evolution of SVG element type distribution throughout training. The numbers behind each legend denotes the average numbers of this element per SVG code across steps. 100 % COCO pure natural-image captions; 100 % MMSVG pure synthetic SVG captions. We evaluate on the validation splits (COCOVal, SGPSingle-9k-val) and report CLIP, DINO, VQA, and Diversity. We train each setting to step 750 for fair comparison. Discussion. We summarize patterns from Table 10. Models trained on single corpus score highest on their own validation set but drop sharply on the opposite set. Training in the 50 / 50 mixture sacrifices slightly in each domain, yet produces more balanced overall performance. Since natural-image captions (COCO) emphasize rich scene semantics, whereas SVG captions (MMSVG) emphasize explicit geometry, combining them supplies mutually reinforcing signals that single-domain training lacks. We thus conclude that the gains from the mixed data suggest that expanding captionstyle coverage is promising route to further improvements. It also shows that the cross-domain generalization is limited. In other words, breadth of data is important for SVG-generation tasks."
        },
        {
            "title": "Train Mix",
            "content": "CLIP DINO VQA Diversity 100% COCO MMSVG COCO SGP Avg. COCO SGP Avg. COCO SGP Avg. COCO SGP Avg. 0.298 0.664 0.529 0.597 0.174 0.240 0.207 0.189 0.596 0.440 0.563 0.502 0.230 0.194 0. 0.265 0.278 0.272 0.110 0.485 0.286 0.272 0.102 0.258 0.228 0.287 0.258 0.566 0.334 0.632 0.050 0.570 0.310 100% 0.560 0. 0.194 50% 50% 0% 0% Table 10: Comparison of trainingdata mixtures. Bold numbers denote the best value in each column. E.3 Element Type Distribution Shifts During Training Figure 11 reveals the evolution of SVG element usage throughout training. We observe that dominant elements such as <rect>, <circle>, <line>, <ellipse>, <polygon>, and <path> maintain their prevalence, while various less frequently used elements show gradual decline in usage. 30 E.4 Model Behaviors We found the RL-trained policy exhibits camera-like framing strategy: it frequently places shapes at absolute coordinates that extend well beyond the declared viewBox. Because any path segment outside the viewBox is silently clipped by the SVG renderer, the model effectively uses the viewBox as virtual viewfinder, cropping out the surplus geometry and revealing only the portion that aligns with the caption (Figure 12). This behavior emerges without any explicit instructionsuggesting the optimizer discovered that overshooting object boundaries is cheaper (in token entropy) than fine-tuning every coordinate to fit perfectly inside the frame. The result is visually coherent images with cleaner edges and fewer near-miss truncation artefacts. Figure 12: Qualitative examples from the model that draws full elephant whose body extends outside the viewBox (right). The renderer clips the excess, yielding neatly framed composition (left). E.5 Evolution of Chain-of-Thought Unlike token-level supervised fine-tuning, reinforcement learning encourages the model to verbalize why its drawing matches the caption. As shown in Table 11, early in training the policy already learns to reason about how to decompose the scene into different objects and further decompose the objects into components. Later on the optimization steps nudge it toward richer details: detailed description of some key components, relations between objects, activities of persons, and even lighting and shading. These CoTs act as an internal checklist that aligns the SVG with the caption."
        },
        {
            "title": "Step",
            "content": "30 60 120 300 750 Chain-of-Thought To create an SVG image of person driving plated motorcycle on track with people watching, we need to include several elements such as track, motorcycle, person, and some onlookers. The motorcycle will have wheels, body, and plates. The track can be represented as curve or straight lines depending on the desired style. People can be drawn as simple stick figures or more detailed figures with bodies, heads, and arms. We will use various SVG shapes like circles for wheels, rectangles for bodies, and lines for limbs. To create an SVG image of person driving plated motorcycle on track with people watching, we need to consider individual components such as the track, the ground beneath it, the motorcycle with its features (like wheels, body, seat, handlebars, and headlight), the driver inside the motorcycle, spectators watching the race. Different shapes and colors will be used to depict each component accurately. In order to create an SVG image for person driving plated motorcycle on track with people watching, we need to consider the shapes and components involved for the motorcycle, the body, wheels, seat, rider, track, and crowd. For the track, we need to consider the boundary and the grooves. The crowd needs appropriate head shapes, bodies, and legs. To generate an image of person driving plated motorcycle on track with people watching with the use of SVG, should consider defining shapes, colors, and proportions for each element. The track will have curves and banking to accurately portray the course. The motorcycle should have features like wheels, handlebars, frame, body, and tires. The person should consist of head, torso, and limbs. The spectators should each have distinct shapes for heads, bodies, and faces. The lighting and shading should be taken into account for more realistic representation. To generate an image showing person driving plated motorcycle on track with spectators watching, should incorporate geometric forms depicting road and track, motorcycle, human figures, spectators. must incorporate some recognizable components such as frame, seat, wheels, body, handlebars, rider, and motorcycle. should reproduce or construct recognizable sightseeing components on or around racetrack. should include features that illustrate or emulate spectators watching. should create recognizable, clutchable geometries and recognizable features. Table 11: Evolution of our RL-trained models Chain-of-Thought on the caption Person driving plated motorcycle on track with people watching."
        }
    ],
    "affiliations": [
        "Max Planck Institute for Intelligent Systems",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "Westlake University"
    ]
}