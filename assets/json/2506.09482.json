{
    "paper_title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression",
    "authors": [
        "Dingcheng Zhen",
        "Qian Qiao",
        "Tan Yu",
        "Kangxi Wu",
        "Ziwei Zhang",
        "Siyuan Liu",
        "Shunshun Yin",
        "Ming Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs a diffusion model to estimate the distribution of image samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves a Frechet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster inference latency compared to state-of-the-art methods based on AR Transformer and x112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce a novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up a new frontier in the field of image generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 2 2 8 4 9 0 . 6 0 5 2 : r Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression Dingcheng Zhen, Qian Qiao, Tan Yu, Kangxi Wu Ziwei Zhang, Siyuan Liu, Shunshun Yin, Ming Tao Soul AI {dingchengzhen, qiaoqian, yutan, wukangxi, zhangziwei, siyuanliu, yinshunshun, ming} @soulapp.cn https://github.com/TransDiff/TransDiff Figure 1: Generated samples from TransDiff trained on ImageNet. Top: 512 512 and 256 256 samples. Middle: effect of semantic feature diversity in TransDiff on image quality(left to right: increasing diversity.). Bottom: results of semantic features fusion from images of different classes. (The first two columns show images from two classes; the third shows the fused result.)"
        },
        {
            "title": "Abstract",
            "content": "We introduce TransDiff, the first image generation model that marries Autoregressive (AR) Transformer with diffusion models. In this joint modeling framework, TransDiff encodes labels and images into high-level semantic features and employs diffusion model to estimate the distribution of image samples. On the ImageNet 256 256 benchmark, TransDiff significantly outperforms other image generation models based on standalone AR Transformer or diffusion models. Specifically, TransDiff achieves Fréchet Inception Distance (FID) of 1.61 and an Inception Score (IS) of 293.4, and further provides 2 faster inference latency compared Equal contribution. Corresponding author, Project Leader Preprint. Under review. to state-of-the-art methods based on AR Transformer and 112 faster inference compared to diffusion-only models. Furthermore, building on the TransDiff model, we introduce novel image generation paradigm called Multi-Reference Autoregression (MRAR), which performs autoregressive generation by predicting the next image. MRAR enables the model to reference multiple previously generated images, thereby facilitating the learning of more diverse representations and improving the quality of generated images in subsequent iterations. By applying MRAR, the performance of TransDiff is improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open up new frontier in the field of image generation."
        },
        {
            "title": "Introduction",
            "content": "In recent years, image generation models have made significant progress driven by transformers [49, 12, 48, 37] and diffusion models [16, 10, 31, 32, 39, 7, 21]. Currently, the two main approaches are AR Transformer and diffusion models. AR Transformer use tokenized image representations that convert images from pixel space into discrete visual tokens through vector quantization (VQ) [48] for AR image generation. In contrast, diffusion models employ progressive denoising process, starting from random Gaussian noise and ultimately generating images. Although these two approaches have achieved remarkable success in image generation, they face fundamental limitations inherent in their design. On the one hand, key challenge for AR Transformer image generation is the O(n2) computational complexity of causal attention, which becomes especially prohibitive for high-resolution images. To mitigate this, most current AR-based methods convert raw pixels into latent space representatione.g., 32 32 instead of the original 256 256 3 image space. This compressed representation enables efficient model training and inference. However, the high-rate compression approach, while mitigating the complexity of O(n2) through discrete representations [12, 48, 37] learned in the latent space, can compromise image quality by reducing detail fidelity. The method essentially achieves computational efficiency at the cost of information loss, as it quantizes continuous features and trains on discrete tokens. On the other hand, diffusion models can represent arbitrary probability distributions, enabling the generation of high-quality, photorealistic images. However, their iterative denoising process inherently results in slower inference speeds compared to AR approaches. Moreover, diffusion models only utilize frozen LM [35] to represent text condtition or employ an n-layer MLP to encode class condtition, which limits their representational capacity. natural question arises: Can we marry the strengths of AR Transformers and diffusion models into unified architecture? We propose TransDiff, novel approach that integrates the advantages of both AR Transformers and diffusion models within unified training framework. Specifically, TransDiff first employs an AR Transformer to encode inputs into high-level semantic features, then uses diffusion model (i.e., the \"diffusion-based decoder\") to decode these features into images. Through joint training, the AR Transformer learns to extract high-level semantics while the diffusionbased decoder develops the capability to interpret these semantic features precisely (e.g., Fig. 1). Current AR models can be broadly categorized into two types: token-level AR models [36, 4, 8, 48, 47] and scale-level AR models [45, 27, 30, 53]. Token-level models generate images by predicting one token at each step, with the model only observing an incomplete set of image tokens during generation. In contrast, scale-level models generate images by progressively predicting the next scale, where the model refers to coarse and blurry image version at each step. In both cases, the predictions rely on incomplete or imprecise information, which limits the overall quality and diversity of the generated images. To address these limitations, we propose novel autoregressive generation paradigmMultiReference AutoRegression (MRAR)where the model predicts complete image at each step. This approach enables the model to leverage more comprehensive and complete reference information during generation, resulting in more diverse and higher-quality image outputs (e.g., Fig. 2). We validate the effectiveness of TransDiff with MRAR as its autoregressive strategy on the challenging ImageNet [9] benchmark. This approach achieves 1.42 Fréchet Inception Distance (FID) score, outperforming diffusion models with the equal parameter count (FID 1.58). While maintaining 2 Figure 2: Compare of Token-Level AR, Scale-Level AR and MRAR. inference speed comparable to state-of-the-art AR Transformer models, it significantly outperforms diffusion models in inference efficiency."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion Models for Image Generation Diffusion models gradually remove noise from images to restore them to clear images. Early works used U-Net as the denoiser [17, 40, 16, 28, 43]. Such as, CDM [17] uses cascaded U-net diffusion models to generate high-fidelity images. And, LDM [40] performs the diffusion processes in the latent space of pretrained autoencoders and significantly reduces the computational cost. Recently, with the significant progress of the transformer architecture in the field of NLP, growing number of works have begun using transformers as denoisers for image generation [33, 11, 25, 21], such as, DiT [33] is the first work to use the transformer architecture instead of the previous U-Net architecture and to explore the scalability of diffusion transformers. And, Esser et al. [11] improve rectified flow models with perceptually-biased noise sampling and introduces novel dual-modality transformer. Despite the significant progress of diffusion models in the field of image generation, they often require higher resource consumption and longer inference times to achieve better results. In contrast, our approach consumes fewer resources and reduces inference time, while also achieving superior performance compared to diffusion models with the same number of parameters. 2.2 Autogressive Models for Image Generation Autoregressive models generate images by regressively predicting the next token [22, 5, 38, 52, 12] or next scale [45, 27, 30, 53]. For next token prediction, RQ-Transformer [22] utilizes ResidualQuantized VAE (RQ-VAE) for autoregressive image generation, resulting in lower computational costs and faster sampling speeds. Maskgit [5] utilizes bidirectional transformer decoder that learns to predict randomly masked image tokens by attending to tokens in all directions, significantly enhancing the efficiency of autoregressive decoding. For next scale prediction, VAR [45] proposes coarse-to-fine \"next-scale prediction\" approach to redefine autoregressive learning for images, enabling GPT-style AR models to surpass diffusion transformers in image generation for the first time. DiMR [27] incrementally enhances features at various scales, enabling gradual improvement in visual detail from coarse to fine levels. Different from all the above studies, this paper proposes Multi-Reference AutoRegression (MRAR), which is novel autoregressive generation paradigm. It predicts complete image each step, allowing the model to refer to more comprehensive and complete information during prediction, thereby generating higher-quality and more diverse images."
        },
        {
            "title": "3 Approach",
            "content": "3.1 TransDiff We introduce TransDiff, which first unifies two dominant methods in contemporary image generation: the AR Transformer and the diffusion model. In brief, we employ the AR Transformer to encode input into high-level semantic features serving as conditioning signal, and utilize diffusion decoder [32] to generate the image. This section begins with brief overview of these two methodologies, followed by detailed explanation of how we systematically integrate them into unified framework. 3 Figure 3: Our Transdiff approach overview. Wherein (a) and (c) are the Vanilla Autoregressive Transformer and the Vanilla Diffusion Model, respectively. (b) is our TransDiff approach. (d) and (e) provide detailed explanation of 1-step AR and MRAR. AR Transformer. Conventional autoregressive image generation methods typically rely on pretrained tokenizer to quantize images into discrete tokens, which are then used to reconstruct the original image. However, the limited capacity of the VQ model imposes fundamental constraint on the reconstruction quality . This bottleneck arises from the inherent trade-off between compression ratio and information retention: smaller compression ratio increases computational complexity, while larger compression ratio leads to significant loss of information during quantization. Unlike common AR Transformer models [34, 3], which take discrete tokens as input, we let = (x1, x2, . . . , xN ) denote sequence of continuous features, where xn Rld . The AR Transformer models assume that the probability of each xn depends exclusively on its preceding tokens (x1, x2, . . . , xn1). This assumption of unidirectional dependency enables the joint probability of the entire sequence to be factorized as product of conditional probabilities: p(x) = (cid:89) n=1 p(xn x1, x2, . . . , xn1) The loss function of AR Transformer models is typically defined as: LAR = min 1 (cid:88) n=0 LF(xn, AR Transformer(x0, x1, ..., xn1)) where LF denotes the loss function(e.g., cross-entropy loss). (1) (2) Diffusion Decoder. We adopt DiT [32] as our diffusion decoder. In addition, we replace the original DDPM [16] objective with Flow Matching [26]. Flow Matching learns transformation from latent variables ϵ (0, 1) to samples drawn from the data distribution q, formulated through an ordinary differential equation (ODE): dxt dt = ψθ(xt, t, c) (3) Here, ψθ represents learnable velocity field parameterized by the model weights θ. [0, 1] denotes the continuous time variable and xt refers to the data point at time t. is conditioning input. To improve optimization efficiency, we adopt rectified flow [28], which assumes that the trajectory connecting data sample and noise sample ϵ in latent space follows straight-line path: xt = (1 t) + ϵ (4) 4 where ϵ (0, I) and [0, 1]. Then, the loss of the diffusion decoder can be expressed as: Ldif = min EtU nif orm([0,1]),xP,cARTransformer (cid:2)(ϵ x) ψθ(xt, t, c)2(cid:3) (5) where represents the distribution of the training dataset, and refers to condition of diffusion decoder generated by AR Transformer Encoder, based on both the image class and the known image. Marrying AR Transformer with Diffusion Decoder. In this paper, we introduce TransDiff, which, to the best of our knowledge, is the first framework that enable joint training of the AR Transformer and the diffusion model(i.e., diffusion decoder). While AR Transformer-based image generation methods use VQ-VAE to map images into discrete tokens, our approach employs VAE to project images into continuous latent space. Subsequently, our AR Transformer encodes the obtained image latent into high-level semantic features. = AR Transformer(input) (6) where input R(hw)d denotes the the continuous latent of image, R(h/f w/f )(df ) denotes the condition for diffusion decoder, where denotes the compression ratio. Then, these features serve as conditioning inputs to the diffusion decoder to generate the image. Detailed as follows: output = Diff(c, ϵ) (7) where Diff denotes the Diffusion Deocder, ϵ (0, I) and output R(hw)d denotes the the continuous latent of output image. Finally, by combining the loss of AR Transformer model (i.e.,Eq. 2 )and the diffusion decoder(i.e., Eq. 5), the overall loss of our approach can be defined as: Lall = min 1 (cid:88) n=0 EtUniform([0,1]),xP (cid:34) (cid:13) (cid:13) (cid:13)(ϵ xn) ψθ(xt n, t, AR Transformer(x0, x1, . . . , xn1)) 2(cid:35) (8) (cid:13) (cid:13) (cid:13) where xn denotes the n-th known input latent for AR Transformer, and xt sample of xn at time step t. represents the diffusion 3.2 Multi-Reference Autoregressive (MRAR) In the autoregressive component of our approach, we introduce novel autoregressive generation paradigm, Multi-Reference Autoregressive (MRAR), which diverges from traditional autoregressive generation paradigm. Unlike the standard one-step autoregressive (1-step AR) method, which generates predictions sequentially from the previous output, MRAR incorporates multiple references from the entire sequence of previous outputs. Our experimental results demonstrate that MRAR consistently outperforms the one-step autoregressive strategy, offering significant improvements in performance. Based on these observations, we adopt MRAR as the preferred autoregressive strategy in our model, positioning it as new paradigm for autoregressive modeling. 3.2.1 1-Step AR As shown in the Figure 3, the goal of 1-Step AR encode high-level semantic features through single forward pass of the AR transformer, conditioned on the class label, which is then decoded by the Diffusion Decoder to generate the output. Specifically, the input can be formalized as follows: Mask = Concat([m, m, ..., m]) input = Concat([C, Mask]) (9) where, denotes the class embedding of image, and represents the mask embedding. And R64d, Mask R(hw)d. 5 Then, we follow the forward propagation and loss computation of TransDiff: = AR Transformer(input) output = Diff(c, ϵ) (10) Furthermore, as shown in the Figure 3 (d), in 1-Step AR, the attention mask of the AR Transformer in 1-Step AR is bidirectional attention matrix filled with zeros. The inference process of 1-Step AR is shown in the Algorithm 1. 3.2.2 Multi-Reference Autoregressive (MRAR) In the 1-Step AR setting, we observe that in the image features (as defined in the Eq. 10) of category, the more diverse the features relevant to that category, the better the quality of the generated images (A detailed experimental analysis is provided in the Sec. 4.2.2). Based on the above observations, we propose MRAR, which learns more comprehensive features by referencing multiple past generated images. This allows it to generate higher-quality images through iterations(A detailed experimental analysis is provided in Sec. ). The detailed process is shown in the Figure 3 (e). Therefore, building on the training of 1-Step AR, we further fine-tune the model using MRAR. Specifically, the input can be formalized as follows: input = Concat([C, Mask, ximg0 , ximg1, ..., ximgn1 ]) (11) i=1 R(h/f w/f )(df ) denotes the latent of the i-th image under the same label, }n where {ximgi and is the compression ratio. Mask R(h/f w/f )(df ) represents the mask embedding. Then, we follow the forward propagation and loss computation of TransDiff: = AR Transformer(input) = Concat([cimg0, cimg1 , cimg2, ..., cimgn ]) output = Diff(c, ϵ) = Concat([oimg0, oimg1 , oimg2, ..., oimgn ]) (12) i=0 R(h/f w/f )(df ) and {oimgi }n i=0 R(hw)d denote the condition features }n where {cimgi and the final output of the i-th image, respectively. Moreover, the final generated image is denoted as R(hw)d. As illustrated in the Figure 3 (e), the attention mask in the AR Transformer of oimgn MRAR is implemented as causal attention matrix. The inference process of MRAR is shown in the Algorithm 2."
        },
        {
            "title": "4 Experiments",
            "content": "We demonstrate through series of experiments that TransDiff is fast method capable of fitting image distributions. Furthermore, we validate that the novel MRAR image generation paradigm effectively enriches TransDiffs feature representations, which enhances the diversity of generated images. Specifically, we investigate the following questions: Can TransDiff beat diffusion-only and transformer-only approaches?  (Table 1)  Does the AR Transformer in TransDiff extract high-level semantic features, and can the diffusion decoder interpret these features to generate high-quality images? (Figure 4a, Figure 1) Is the MRAR paradigm superior to Token-Level AR and Scale-Level AR paradigms? ( Table 4, Figure 4b) 4.1 Setup Datasets and Metrics. We conduct comprehensive experiments on the ImageNet [9] at 256 256 and 512 512 resolution, evaluating our method through quantitative metrics including Fréchet Inception Distance (FID) [15], Inception Score (IS) [41], Precision-Recall analysis and computational efficiency (inference time). The sampling method in the Appendix is used in the inference stage. Implementation Details. The Implementation Details are shown in Appendix B. 6 4.2 Results and Analysis 4.2.1 TransDiff beats diffusion-only and AR Transformer-only approaches TransDiff is unified architecture that integrates AR Transformer with diffusion model. Its core design involves generating an image representation through single forward pass of AR Transformer, followed by diffusion decoder to accomplish the final synthesis. In contrast, the diffusion-only approach relies entirely on multi-step diffusion process to incrementally denoise and generate samples, while the AR Transformer-only approach sequentially constructs content through an autoregressive mechanism, generating pixels tokens one-by-one. In the Table 1, we compare the performance of three architectures. Under similar parameter settings, TransDiff-L with MRAR achieve the lowest FID of 1.49 and TransDiff with 1 step AR achieve competitive remarkable speed of 0.2s/image. Table 1: Quantitative comparison on ImageNet 256 256 with existing diffusion-only and transformer-only approaches. Type Diffusion Diffusion AR AR Model Params FID IS DiT-XL/2 675M MDTv2-XL 676M RAR-L 461M 600M VAR-d20 2.27 1.58 1.70 2. 1.69 1.49 278.2 314.7 290.5 302.6 282.0 282.2 Hybrid Hybrid TransDiff-L 1Step 683M TransDiff-L MRAR 683M Table 2: Quantitative comparison on ImageNet 256 256 between Diffusion Decoder and Diffusion Loss. It is important to note that our approach has the capability for one-step inference, whereas MAR does not. Model MAR-L MAR-L TransDiff-L 1Step TransDiff-L MRAR Steps FID IS 1 256 1 4 336.58 1.78 1.69 1. 1.3 296.0 282.0 282.2 Time 0.07s 1.1s 0.2s 0.8s Diffusion Decoder vs. Diffusion Loss. In the AR transformer component, we adopt similar architecture to MAR [24]; however, we employ Dit[33] decoder to model the distribution of image samples, which fundamentally differs from the optimization objective of the MAR model based on the diffusion loss function. To investigate the effects of the two diffusion processes, we keep the AR Transformer unchanged and perform comparative evaluation of the differences in FID, IS, and inference time (Time) for single image across varying inference steps between the Diffusion Loss (MAR) and the Diffusion Decoder (TransDiff). In the Table 2, our framework, which incorporates diffusion decoder, demonstrates superior performance by reducing the FID from 1.78 to 1.49 and achieving faster inference. At the same time, it is important to note that our approach has the capability for one-step inference, whereas MAR does not. State-of-the-art image generation. We evaluate TransDiff models of different sizes (Base, Large, and Huge) on the ImageNet 256 256 conditional generation benchmark, and systematically compare them with state-of-the-art image generation model families. As quantitatively demonstrated in the Table 3, TransDiff achieve exceptional performance metrics characterized by extraordinarily lowest FID (e.g., 1.42) and competitive IS (e.g., 301.2). These results comprehensively validate the architectural innovation of TransDiff in simultaneously advancing both generation quality and computational efficiency, surpassing existing diffusion-based models as well as autoregressive/masked autoregressive frameworks. Additionally, these advantages hold true on the 512 512 synthesis benchmark(lowest FID of 2.51)), which is provided in the Appendix D. 4.2.2 AR Transformers Representation We use the AR Transformer to extract high-level semantic features from images, which exhibit substantial representational diversity and contribute to improved performance in image generation tasks. To validate our hypothesis that more diverse image semantic features lead to higher-quality generated images, we first introduce novel and independent metric for evaluating representational diversity. Then, we conduct comprehensive quantitative and qualitative experiments to support our conclusions. Metric for Evaluating the Diversity of Image Semantic Features. In this paper, we quantify feature diversity by computing cosine similarity matrix constructed from the 256 L2-normalized Table 3: Conduct comprehensive comparative analysis of various model architectures on the class-conditional ImageNet 256256. Metrics include Fréchet inception distance (FID), inception score (IS), precision (Pre.), recall (Rec.), and per-image generation time (Time). Columns \"w/o CFG\" and \"w/ CFG\" denote results with and without classifier-free guidance. w/ CFG w/o CFG Type Model #Params Time FID IS FID IS Pre. Rec. GAN GAN GAN Mask. Mask. Mask. Mask. Diffusion Diffusion Diffusion Diffusion AR AR AR AR AR AR AR AR AR MAR MAR MAR TransDiff TransDiff TransDiff TransDiff TransDiff TransDiff BigGAN [2] GigaGAN [18] StyleGan-XL [42] MaskGIT [6] MAGE [23] MAGVIT-v2 [51] MaskBit [50] LDM-4 [39] DiT-XL/2 [32] DiffT [14] MDTv2-XL/2 [13] VQGAN [12] VQGAN [12] LlamaGen-3B [44] VAR-d16 [45] VAR-d20 [45] VAR-d30 [45] RAR-L [52] RAR-XL [52] RAR-XXL [52] MAR-B [24] MAR-L [24] MAR-H [24] TransDiff-B, 1 Step AR TransDiff-B, MRAR TransDiff-L, 1 Step AR TransDiff-L, MRAR TransDiff-H, 1 Step AR TransDiff-H, MRAR 112M 569M 166M 227M 230M 307M 305M 400M 675M - 676M 227M 1.4B 3.1B 310M 600M 2.0B 461M 955M 1.5B 208M 479M 943M 290M 290M 683M 683M 1.3B 1.3B 6.95 3.45 2. 6.18 6.93 3.65 - 10.56 9.62 - 5.06 18.65 15.78 9.95 - - - - - - 3.48 2.60 2.35 5.09 5.56 3.05 2.95 2.23 2. 224.5 225.5 265.1 182.1 195.8 200.5 - 103.5 121.5 - 155.6 80.4 74.3 112.87 - - - - - - 192.4 221.4 227.8 153.5 148.1 185.7 192.7 210.1 200. - - - - - 1.78 1.51 3.60 2.27 1.73 1.58 - - 2.18 3.30 2.57 1.92 1.70 1.50 1.48 2.31 1.78 1.55 2.47 2.25 1.69 1.49 1.61 1. - - - - - 319.4 328.6 247.7 278.2 276.5 314.7 - - 263.3 274.4 302.6 323.1 290.5 306.9 326.0 281.7 296.0 303.7 244.2 244.3 282.0 282.2 293.4 301. - - - - - - - 0.87 0.83 0.80 0.79 - - 0.81 0.84 0.83 0.82 0.82 0.80 0.80 0.82 0.81 0. 0.81 0.80 0.81 0.82 0.81 0.81 - - - - - - - 0.48 0.57 0.62 0.65 - - 0.58 0.51 0.56 0.59 0.60 0.62 0.63 0.57 0.60 0. 0.56 0.57 0.60 0.60 0.61 0.62 - - 0.3 0.5 - - - - 45 - - 19 24 - 0.4 0.5 1 15.0 8.3 6.4 - - - 0.1 0.4 0.2 0.8 0.4 1.6 image representations. Specifically, given the feature matrix = [a1; a2; . . . ; a256] R256d, where each row ai is an L2-normalized semantic feature vector, the cosine similarity between any two features ai and aj is computed as: The full cosine similarity matrix is then obtained via: CosSim(ai, aj) = ai aj = AT = a1 a1 a2 a1 ... a1 a2 a2 a2 ... a256 a1 a256 a2 . . . a1 a256 a2 a256 ... a256 To evaluate feature diversity while excluding self-similarities, we define: Cosine Similarity = SL1 8 (13) (14) (15) (a) (b) Figure 4: (a):As training steps increase, the FID score consistently decreases while cosine similarity among generated samples drops, indicating improved image quality and diversity. (b): Human evaluation results among our TransDiff and other SoTA methods of Token-Level AR and Scale-Level AR. Figure 5: Results of semantic features fusion from images of different classes. (The first two columns show images from two classes; the third shows the fused result.) smaller cosine similarity score indicates that the generated image representations are more diverse in the semantic space. Quantitative Analysis of Image Semantic Features. As shown in the Figure 4a, the results demonstrate that as the number of training steps increases, the FID score consistently decreases while image diversity simultaneously improves, indicating enhanced image quality. This correlation suggests that prolonged training not only refines feature representations but also promotes greater diversity in generated samples, leading to higher-quality image generation. Qualitative Analysis of Image Semantic Features The qualitative analysis is presented in the Figure 1 . In the Middle, the results generated by the diffusion decoder using 32, 64, 128, and 256 image semantic features are shown. It can be observed that as the number of image features increases, the generated images exhibit richer details and improved visual quality, which further validates our conclusions. In addition, Bottom of the Figure 5 reveals an interesting phenomenon. When we fuse semantic features (128 features from each) from images of different classes (e.g., Macaw and Tiger), generated images exhibit characteristics of both original classes (e.g., \"Tiger-striped macaw\"). This observation demonstrates that features obtained through our AR Transformer indeed capture high-level semantic features of images. 4.2.3 MRAR beats Token AR and Scale-Level AR paradigms Based on the experimental analysis in the Section 4.2.2, we observe that higher diversity measure corresponds to lower FID score, indicating better image quality. Therefore, we propose the MRAR paradigm (illustrated in the Figure 3 (e)) to enable the incorporation of more comprehensive feature representations. 9 Table 4: MRAR vs. scale AR. Compared to the baseline, MRAR achieves higher diversity (lower diversity measure), resulting in decrease in FID from 1.69 to 1.49. In contrast, Scale-Level AR significantly reduces the diversity (higher diversity measure) and yields higher FID. Model TransDiff-L, 1 step AR TransDiff-L, Scale AR TransDiff-L, MRAR Measure FID 1.69 2.78 1.49 0.44 0.64 0.39 IS 282.0 279.8 282. Figure 6: Correlation between the number of references and FID score. Human Evaluation on MRAR, Token-Level AR and Scale-Level AR. To verify the superiority of our MRAR paradigm compared to token-level (e.g., LlamaGen-3B and RAR-XXL) and scale-level AR paradigms (e.g., VAR-d30), we conducted human evaluation. Sixty participants (66.7% aged 24 30, 33.3% aged 30 40; 50% male, 50% female; 83.3% with AIGC model experience) evaluate each image using 5-point Likert scale across three dimensions: subject diversity, background diversity, and image quality. total of 200 images are presented in random order to minimize bias and better capture subjective perceptions of visual diversity and quality. As shown in the Figure 4b, participants consistently rated the MRAR paradigm higher than the token-level and scale-level AR paradigms in all three dimensions, which justifies its superiority in image generation. Ablation Study on MRAR vs. Scale-Level AR. To further validate the superior performance of our proposed TransDiff model with the MRAR paradigm, we fine-tuned the model using both the Scale-Level AR and MRAR paradigms based on 1-Step AR training. The experimental results in the Table 4 show that, compared to the original 1-Step AR, the MRAR paradigm achieves higher measure, indicating greater image feature diversity, along with lower FID score, which signifies better image generation quality. In contrast, the Scale-Level AR paradigm shows lower measure, indicating reduced image feature diversity, and higher FID score, indicating poorer image generation quality. This demonstrates that the MRAR paradigm, by referencing more complete and comprehensive images, is able to obtain more diverse semantic features, leading to improved generation results. Properties of MRAR. To further explore the extended properties of the MRAR paradigm, we directly extend TransDiff-L into multi-reference input framework. Through comprehensive experiments, we systematically evaluate model performance under different reference configurations (64, 16, 4, and 0), with particular focus on analyzing the correlation between the number of references and overall image quality (i.e., fid value). The Figure 6 shows the optimal number of references (i.e., 4)."
        },
        {
            "title": "5 Conclusion and Limitations\nConclusion: In this paper, we propose TransDiff, demonstrating through strong experiments the sig-\nnificant potential of marrying AR Transformers with diffusion models, without being constrained by\ndiscrete representations. Notably, our proposed MRAR paradigm showcases excellent performance,\nopening up a new direction in autoregressive image generation. We hope this new paradigm can\ninspire future research in both image and video generation.",
            "content": "Training Data Constraints: Primarily trained on ImageNet, the model lacks the scale and diversity of commercial proprietary datasets. This limits its ability to generate high-resolution, photorealistic images with complex semantic details, where visual quality is notably inferior to industrial models. Future work should explore larger, more diverse datasets or domain adaptation techniques to address this discrepancy. Computational Resource Limitations: Experiments were restricted by available GPU resources, preventing investigation of larger model architectures (e.g., deeper transformers, multi-scale frameworks) or higher-resolution image generation, which could enhance quality. Hardware memory constraints were primary factor."
        },
        {
            "title": "References",
            "content": "[1] Vlad Bally and Denis Talay. The euler scheme for stochastic differential equations: error analysis with malliavin calculus. Mathematics and computers in simulation, 38(1-3):3541, 1995. [2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [4] Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan: Towards high-resolution image generation with efficient vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73687377, 2023. [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked In IEEE/CVF Conference on Computer Vision and Pattern generative image transformer. Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1130511315. IEEE, 2022. [6] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [13] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. [14] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. In European Conference on Computer Vision, pages 3755. Springer, 2024. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 11 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [17] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:47:147:33, 2022. [18] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1012410134, 2023. [19] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. [20] P.E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Stochastic Modelling and Applied Probability. Springer Berlin Heidelberg, 2013. [21] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [22] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 11513 11522. IEEE, 2022. [23] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21422152, 2023. [24] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [25] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. CoRR, abs/2405.08748, 2024. [26] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [27] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models and time-dependent layer normalization. Advances in Neural Information Processing Systems, 37:133879133907, 2024. [28] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [29] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [30] Xiaoxiao Ma, Mohan Zhou, Tao Liang, Yalong Bai, Tiejun Zhao, Biye Li, Huaian Chen, and Yi Jin. Star: Scale-wise text-conditioned autoregressive image generation. arXiv preprint arXiv:2406.10797, 2024. [31] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. 12 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 41724182. IEEE, 2023. [34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [36] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [37] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [38] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan L. Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. CoRR, abs/2502.20388, 2025. [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1067410685. IEEE, 2022. [41] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [42] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. [43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [45] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [46] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [47] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. 13 [48] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [50] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and LiangChieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024. [51] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [52] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. CoRR, abs/2411.00776, 2024. [53] Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling. arXiv preprint arXiv:2408.01181, 2024. 14 Inference Process of 1 Step AR and MRAR The inference process of 1 Step AR is shown as the Algorithm 1: Algorithm 1 1-Step Auto Regression inference process Input: class token [C] and mask token Maskimg Output:the generated image Rimg corresponding to the class token [C] 1: input = Concat([C, Maskimg]) 2: = AR Transformer(input) 3: Rimg = Diff(c, ϵ) The inference process of MRAR is shown as the Algorithm 2: else input = Concat([C, Maskimg]) Algorithm 2 Multi-Reference Auto Regression inference process Input: class token [S] and mask token Maskimg Output:the generated image Rimg corresponding to the class token [S] 1: for = 0 to do if == 0 then 2: 3: 4: 5: 6: 7: 8: 9: end for 10: = Concat([cimg0 , cimg1 , cimg2, ..., cimgn]) 11: output = Diff(c, ϵ) = Concat([oimg0, oimg1, oimg2 , ..., oimgn ]) 12: Rimg = oimgn end if cimgi = AR Transformer(input) ximgi = cimgi input = Concat([C, Maskimg, ximg0, ..., ximgi1 ]) Implementation Details. Our AR Transfomer architecture follows the Transformer [49] implementation and the diffusion model architecture follows the DiT [32]. TransDiff is available in three model sizes: Huge(AR Transformer has 40 blocks and width of 1280 and diffusion decoder has 20 blocks and width of 1280),Large(AR Transformer has 32 blocks and width of 1024 and diffusion decoder has 16 blocks and width of 1024), and Base(AR Transformer has 24 blocks and width of 768 and diffusion decoder has 12 blocks and width of 768). TransDiff with 1 Step AR is trained with model sizes ranging from 290M to 1.3B parameters, on 32 8 Nvidia A800 GPU machines with batch size of 2048 for 800 epochs, using the AdamW optimizer [29] with learning rate of 8.0e 4 and weight decay of 0.02, along with the exponential moving average (EMA) [19] strategy. And after fine-tuning with the MRAR paradigm for 40 epochs using separate learning rate of 5.0e 5, we obtain TransDiff with MRAR. The EulerMaruyama Method Our diffusion module inherits the EulerMaruyama method for image generation from noise through stochastic sampling. However, in contrast to EulerMaruyama, we introduce two scaling factors, s1 and s2, which respectively modulate the drift term and diffusion term. These parameters primarily function to optimize the generation process by balancing numerical stability with output quality through strategic term scaling. The following analytical breakdown elaborates on their operational mechanisms. 15 The reverse process of diffusion models is typically implemented by solving Stochastic Differential Equation (SDE), governed by: (cid:20) vθ(x, t) dx = 1 2 (cid:21) σ(t)2x log p(xt) dt + σ(t)dW (16) where vθ(x, t) denotes the noise predicted by the velocity model, σ(t) represents the diffusion coefficient, governing the intensity of noise injection, dW indicates the increment of Wiener process (Brownian motion), characterizing stochastic noise. We formally define the drift term as dcur = vθ(x, t) 1 (cid:112) 2 σ(t)2sθ(x, t) and the diffusion term as σ(t) [20]. Through the EulerMaruyama Method, the reverse process of the diffusion model can be discretized as: xnext = xcur + dcur dt + (cid:112) σ(t) (17) In Eq 17, the weights of the drift and diffusion terms are intrinsically determined by the model architecture. However, practical implementations of the vanilla EulerMaruyama Method [1] may encounter two critical challenges: Numerical instability and Non-trivial trade-offs. To address these challenges, we introduce two scaling factors (s1 and s2) into the vanilla EulerMaruyama framework, reformulating the reverse diffusion process as: xnext = xcur + s1 dcur dt + s2 (cid:112) σ(t) (18) The s1 factor modulates the drift term intensity by scaling the directional update step size dcur t. value below 1.0 suppresses excessive drift updates to enhance numerical stability, whereas values exceeding 1.0 accelerate generative convergence at potential costs to fine-grained detail preservation. The s2 factor regulates the diffusion term strength through scaled noise injection (cid:112) σ(t) . Reducing this factor below 1.0 diminishes stochastic perturbations to improve output sharpness, while increasing it beyond 1.0 promotes sample diversity through amplified noise trade-off that may introduce artifacts at higher scaling magnitudes."
        },
        {
            "title": "D More Quantitative Results",
            "content": "Type Diffusion Diffusion AR VAR Table 5: ImageNet 512 512 conditional generation. IS 101.0 240.8 290.5 303.2 286.6 #params FID 23.24 3.04 26.52 2.63 2.51 Model ADM [10] DiT-XL/2 [33] VQGAN [12] VAR-d36-s [46] 554M 657M 227M 2.3B 683M Time - 81s 25s 1s 1s TransDiff TransDiff-L, MRAR We also report our FID, IS on ImageNet 512 512 benchmark, as detailed in Tab 5."
        },
        {
            "title": "E More Qualitative Results",
            "content": "We also provide more visualization examples in Figure 7, 8 and 9. As presented, Transdiff-H MRAR produces high-quality, diverse, and content-rich image samples. Furthermore, as illustrated in Figure 7, compared to VAR and MAR under similar parameter settings, Transdiff produces more realistic textures, sharper facial features, and consistent lighting. This highlights its strong potential for complex semantic tasks like human face synthesis. 16 Figure 7: Model comparison on ImageNet 256 256 benchmark. 17 Figure 8: Qualitative Results. More 256256 class-conditional generation results from TransDiff-H, MRAR on ImageNet. Figure 9: Qualitative Results. More 512512 class-conditional generation results from TransDiff-L, MRAR on ImageNet."
        }
    ],
    "affiliations": [
        "Soul AI"
    ]
}