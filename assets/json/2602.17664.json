{
    "paper_title": "Sink-Aware Pruning for Diffusion Language Models",
    "authors": [
        "Aidar Myrzakhan",
        "Tianyi Li",
        "Bowei Guo",
        "Shengkun Tang",
        "Zhiqiang Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning."
        },
        {
            "title": "Start",
            "content": "Sink-Aware Pruning for Diffusion Language Models Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen VILA Lab, MBZUAI 6 2 0 2 9 1 ] . [ 1 4 6 6 7 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attentionsink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose Sink-Aware Pruning, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/ VILA-Lab/Sink-Aware-Pruning."
        },
        {
            "title": "Introduction",
            "content": "Diffusion Language Models (DLMs) (Li et al., 2025; Nie et al., 2025; Ye et al., 2025; Zhu et al., 2025) generate text via iterative denoising over multiple timesteps, repeatedly updating the full token sequence (or latent representation) until it converges. This contrasts with autoregressive (AR) LLMs (Radford et al., 2018, 2019), which generate tokens one-by-one with single forward pass per new token. While DLMs recently can achieve attractive generation properties, their iterative inference substantially increases compute and memory cost, making many acceleration, especially pruning, critical for practical deployment. Most existing pruning recipes, however, are adapted from AR Transformers and implicitly assume that attention behaviors (and their important tokens) transfer unchanged to diffusion-style generation. 1 Figure 1: Illustration of attention sink behaviors in Diffusion Language Models. Ours Sink-Aware Pruning reduces sink variance by downscaling unstable sinks. key AR-specific phenomenon is the existence of attention sink tokens: small set of positions (often early tokens such as BOS, system prompts, or few prefix tokens) that consistently attract disproportionately large attention mass across layers and heads. In AR models, these sinks behave as stable global anchors that help propagate conditioning information and stabilize residual-stream dynamics across the long, causal computation graph in autoregressive-based attention. Consequently, many AR pruning and token-dropping methods explicitly preserve sinks (or protect early/prefix tokens) to avoid catastrophic quality degradation. This practice has become de facto heuristic: when pruning attention structure, do not remove sink positions because they are assumed to be universally important in AR models. We revisit this assumption for DLMs and find it does not hold. Because DLMs update all tokens at each diffusion timestep, the models attention organization evolves throughout the denoising trajectory: early steps must resolve global structure under high noise, while later steps refine local synFigure 2: Attention sink heatmap dynamics across generation steps for AR LLM (LLaMA-3-8B) and DLM (LLaDA). For each model, we show 3 different generation stages (25, 50, and 75% of the total generation steps) and plot the attention mass received by each token position (y-axis) across all heads/layers (x-axis). In LLaMA, the sink position (deep-blue vertical band) is stable across steps, while in LLaDA, the sink position shifts significantly across diffusion steps, indicating higher sink variance. The step in AR model refers to the generation process. tax and semantics under lower noise. To quantify sink stability, we compute attention statistics at every generation step for every token: for AR models, step corresponds to each newly generated token; for DLMs, step corresponds to each diffusion timestep (with attention computed over the whole sequence each time). We then track the sink position (e.g., the token index receiving maximal aggregated attention, or the top-k sink indices) across the entire generation process and define sink variance as the degree to which these sink positions shift over steps, as shown in Fig. 2. This analysis (as shown in Fig. 1 and Fig. 4a) reveals clear divergence: sink-token variance in DLMs is substantially larger than in AR LLMs. In AR generation, sink positions tend to be highly persistent: once sink emerges (typically near the prefix), it remains sink across subsequent token emissions and across many heads / layers. In DLMs, by contrast, the identity and location of the dominant sink frequently changes across diffusion timesteps, reflecting the models shifting needs as denoising progresses. Many sinks in DLMs are therefore transient: they may attract attention at some timesteps (often under high noise or during global structure formation) but lose prominence later, and different sinks may take over as the sequence becomes more refined. This implies that always keep sinks is not diffusion-invariant principle, rather, it is property of causal, prefix-conditioned AR computation. Motivated by this, we propose Sink-Aware Pruning: diffusion-specific pruning strategy that distinguishes stable anchors from ephemeral sinks, and prunes accordingly. Instead of hard-coding sink preservation, we (i) estimate sink variance over the denoising trajectory, (ii) identify unstable sink candidates whose sink positions fluctuate significantly across diffusion timesteps, and (iii) prune these unstable sinks to reduce attention / weight cost while preserving informative pathways. Crucially, the method is generation-paradigm aware: for AR LLMs, where sink variance is low and sinks are structurally important, thus prior work keeps sinks, for DLMs in this work, where sink variance is high, we allow and encourage sink pruning. Empirically, Sink-Aware Pruning improves the quality-efficiency trade-off over strong prior pruning baselines under matched compute. Across multiple DLM settings, pruning transient sinks reduces redundant or global attention that does not persist across timesteps, enabling more aggressive pruning without the failure modes observed when AR-centric sink-preservation heuristics are naively applied to DLMs. The results support our central claim: attention sinks are not universally must-keep tokens, their utility depends on the generation dynamics. By explicitly measuring sink-position variance and tailoring pruning decisions to diffusion timesteps, Sink-Aware Pruning provides principled and effective route to accelerating DLM inference without retraining. In summary, our contributions are threefold: We introduce sink-position variance to track how attention-sink indices shift over the full generation trajectory, and show it is much higher in diffusion LLMs than in autoregressive LLMs when computing attention at every AR step / diffusion timestep for each token. We demonstrate that the AR heuristic always keep attention sinks does not transfer to diffusion generation: sinks are stable anchors in AR models but are often transient in DLMs, so they can be pruned. We propose Sink-Aware Pruning, which prunes unstable sinks in DLMs (while conventional AR LLMs preserve sinks) with timestep-aware retention of salient attentions, and outperforms strong prior pruning baselines under the same compute."
        },
        {
            "title": "2.1 Diffusion Language Models",
            "content": "Diffusion Language Models (DLMs) (Li et al., 2025; Nie et al., 2025; Ye et al., 2025) have recently emerged as promising non-autoregressive alternative to conventional autoregressive language models. Inspired by diffusion models in continuous domains (Ho et al., 2020; Rombach et al., 2022), DLMs formulate text generation as an iterative denoising process, enabling parallel generation and naturally incorporating bidirectional context. Existing work broadly categorizes DLMs into continuous-space and discrete-space formulations. Continuous DLMs operate on token embeddings or logits and apply Gaussian or flowbased diffusion processes (Gong et al., 2022; Li et al., 2022), while discrete DLMs define diffusion directly over categorical token spaces using masking or structured transition operators (Austin et al., 2021; He et al., 2023; Sahoo et al., 2024). Recent advances demonstrate that large-scale discrete DLMs can achieve performance comparable to strong autoregressive baselines while offering substantial inference speedups through parallel decoding (Wu et al., 2025; Ni et al., 2025). Moreover, hybrid autoregressive-diffusion approaches further balance generation quality and efficiency by combining block-wise autoregression with intra-block diffusion refinement (Arriola et al., 2025; Cheng et al., 2025)."
        },
        {
            "title": "2.2 Attention Sink in LLMs",
            "content": "Initially identified in LLMs by Xiao et al. (2023), attention sinks are small set of tokens (often early in the context) that absorb disproportionate attention across many layers / heads despite limited semantic value (Gu et al., 2024; Barbero et al., 2025). Xiao et al. (2023) attribute sinks to Softmax normalization: when query has no strong match, attention mass must still be assigned somewhere, and globally visible early tokens become natural dumping ground for redundant attention. Recent work shows sinks also appear in masked DLMs but with different behavior (Rulli et al., Studying large masked DLMs (e.g., 2025). LLaDA (Nie et al., 2025), Dream (Ye et al., 2025), MMaDA (Yang et al., 2025a)), they find sink locations are step-dependent, emerging, shifting, or vanishing as denoising progresses. Moreover, unlike autoregressive models where removing sinks can severely hurt performance, masked DLMs are relatively robust: masking top sinks during generation causes only minor drops. This suggests bidirectional iterative denoising provides alternative aggregation pathways, making sink dynamics central to long-context efficiency and stability across decoding paradigms."
        },
        {
            "title": "2.3 LLM Pruning",
            "content": "Network pruning has long been studied as model compression technique that reduces inference cost by removing redundant parameters based on importance criteria (LeCun et al., 1989; Hassibi et al., 1993; Han et al., 2015a; Cheng et al., 2024). Existing approaches are commonly categorized into unstructured and structured pruning. While unstructured pruning offers fine-grained flexibility, structured pruning removes entire neurons, channels, or matrix rows/columns, making it more amenable to hardware acceleration but often at the cost of higher 3 Figure 3: Overview of Sink-Aware Pruning. Given input activations, we compute per-token attention mass aggregated across all layers and heads (Step 1), identify sink tokens via threshold-based criterion, and derive soft down-weighting factor ω = 1 s. The original activation is then suppressed at sink positions to produce new activation (Step 2), which is substituted into existing pruning criteria, Wanda or SparseGPT, to compute sink-aware importance scores (Step 3). Final pruning decisions are made based on the updated scores (Step 4). accuracy degradation (Han et al., 2015b; Liu et al., 2017; Molchanov et al., 2019). As large language models continue to scale, applying traditional pruning pipelines becomes increasingly challenging due to the prohibitive cost of retraining. This has motivated shift toward post-training pruning methods, which aim to identify and remove unimportant weights directly from pretrained models using lightweight importance metrics, without full retraining (Sun et al., 2023; Frantar and Alistarh, 2023; Das et al., 2023; Yang et al., 2025b). These approaches have demonstrated promising results for compressing LLMs with minimal performance loss, and highlight pruning as practical tool for improving inference efficiency in large-scale models (Ma et al., 2023). Sink-Aware Pruning for DLMs"
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Wanda. Wanda (Sun et al., 2023) assigns each weight an output-wise importance score that combines its magnitude with the norm of the corresponding input activation. For linear layer with weights RCoutCin and input activations RN Cin collected from calibration set, the importance score of weight Wij is defined as: Sij = Wij Xj2. (1) where Xj RN denotes the j-th input feature across samples. Pruning is performed independently for each output neuron by comparing scores within each row group Gi = {Wi,1, . . . , Wi,Cin} and removing the lowest s% weights per output. The activation norms Xj2 are estimated without gradient or Hessian computation, making Wanda computationally efficient for large models. SparseGPT. SparseGPT (Frantar and Alistarh, 2023) formulates the post-training pruning process as layer-wise reconstruction problem using second-order information. For layer with original weights and layer inputs X, the pruning objective is defined as: (M (cid:102)W )X2 , (2) min M,(cid:102)W where is binary pruning mask and (cid:102)W denotes the reconstructed weights. Given fixed mask Mi for the i-th output row, the optimal remaining weights admit closed-form linear regression solution: wi Mi = (cid:16) XMiX Mi (cid:17)1 XMi (wMiXMi) , (3) Based on second-order approximation, the loss increase induced by removing scalar weight wm is estimated as: εm = w2 [H 1]mm . (4) 4 where = XX +λI is the empirical Hessian approximation. SparseGPT iteratively prunes weights with small εm and updates the remaining weights to reduce reconstruction error, enabling high sparsity without retraining."
        },
        {
            "title": "3.2 Sink-Aware Pruning",
            "content": "Variance Statistics in DLMs and AR models. To characterize how attention sinks differ between diffusion and autoregressive language models, we introduce two complementary variance statistics. Let A(t) RN denote the attention matrix at generation step (a diffusion timestep for DLMs; token emission for AR models), where is the sequence length. We define the incoming attention mass at position and step as: mt(i) = (cid:88) j=1 A(t) j,i , (5) which measures how much total attention all other tokens direct toward position i. Positions receiving substantially more incoming attention than the average act as attention sinks at that step. We capture two dimensions of sink behavior. Spatial variance measures how unevenly attention concentrates across positions when averaged over the full trajectory. We compute m(i) = 1 t=1 mt(i) and take (cid:80)T σ2 spatial = Vari( m(i)) . (6) large value indicates that few positions dominate the attention landscape on average, but does not reveal whether those positions stay fixed over time. Temporal variance captures how much the sink location shifts across steps. At each step t, we compute the attention-weighted centroid over the current sink set St (defined below): ct = (cid:80) iSt (cid:80) iSt mt(i) mt(i) , σ2 temporal = Vart(ct). (7) near-zero temporal variance means the sink remains locked to the same region throughout generation, while large value indicates substantial migration. As shown in Fig. 4, these two statistics reveal clear contrast. AR models exhibit high spatial variance but near-zero temporal variance: their sinks are concentrated on small, fixed set of early positions and remain stationary across all generation lower steps. DLMs show the opposite pattern: (a) Variance of the sink across generation/denoising steps. (b) Variance of total attention received by each token across all the generation/denoising steps. Figure 4: Attention sink variance for diffusion LMs (LLaDA, Dream) and autoregressive LMs (Llama 3.1, Qwen 2.5). spatial variance (attention is more distributed) but temporal variance that is orders of magnitude larger. The sink trajectory  (Fig. 5)  provides further intuition: for AR models, the sink stays flat, while for DLMs it drifts progressively from earlier to later positions as denoising advances, with wide step-to-step variability. This drift reflects how early diffusion steps focus on global structure under high noise, while later steps shift attention toward local, token-level refinement. In short, AR sinks are spatially concentrated and temporally stable, while DLM sinks are spatially diffuse and temporally transient. Pruning Metric. We now formalize the notion of attention sinks used in our pruning criterion. We begin by computing the attention mass received by each token position. At denoising step t, the cumulative attention score of token in head of layer ℓ is: A(t,ℓ,h) ="
        },
        {
            "title": "1\nS",
            "content": "S (cid:88) i=1 A(t,ℓ,h) i,j , (8) (cid:80) This masked activation replaces in existing pruning criteria. For Wanda base method, we substitute (cid:101)X into Eq. (1) yields the sink-aware score (cid:101)Sij = Wij (cid:101)X j2, and pruning proceeds as before. For SparseGPT as the base approach, the empirical Hessian is computed from sink-masked inputs, (cid:101)H = 1 (cid:101)Xn + λI, where is the calibration dataset, with the pruning and reconstruction procedure following Eqs. (2)(4) identically. By suppressing sink tokens in the Hessian, the reconstruction allocates capacity toward faithfully reproducing the layers output on semantically meaningful positions, rather than preserving behavior on tokens whose outsized activations would otherwise dominate the second-order statistics. (cid:101)X Figure 5: Sink position across generation/denoising steps for diffusion and AR LMs. Shaded regions denote std across runs. i,j where A(t,ℓ,h) is the attention weight from query position to key position and is the sequence length. We aggregate across all layers and heads to obtain position-level attention mass: mt(j) = (cid:88) (cid:88) ℓ=1 h=1 A(t,ℓ,h) ,"
        },
        {
            "title": "4 Experiments",
            "content": "(9)"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "A token is considered sink token at step if its attention mass significantly exceeds that of other positions, i.e., mt(j) >"
        },
        {
            "title": "1\nS − 1",
            "content": "(cid:88) k=j mt(k) + ϵ, (10) where ϵ > 0 controls the detection sensitivity. To obtain differentiable relaxation, we convert this criterion into soft sink score via sigmoid: ϕt(j) = σ (mt(j) mt(k) ϵ ,"
        },
        {
            "title": "1\nS − 1",
            "content": "(cid:88) k=j (11) We evaluate (11) on noised calibration inputs at uniformly spaced set of timesteps {1, . . . , } and average to obtain step-invariant sink estimate: ϕ(k) = 1 (cid:88) tT ϕt(k). (12) Sink-Aware Importance Reweighting. Given the soft sink score ϕ(j), we define per-position weight ωj = 1 ϕ(j), (13) which is near zero for sink tokens and near one elsewhere. We construct sink-masked activation (cid:101)X by scaling each row of the original activation RSCin by ωj, effectively suppressing the contribution of sink positions: (cid:101)Xj,: = ωj Xj,:, = 1, . . . , S. (14) 6 Models and Benchmarks. We evaluate our sinkaware pruning method on multiple widely adopted pretrained diffusion language models (DLMs), including LLaDA (Nie et al., 2025), Dream (Ye et al., 2025), and LLaDA-1.5 (Zhu et al., 2025), as well as the multimodal unified DLM MMaDA (Yang et al., 2025a). For text generation and reasoning tasks, we assess the pruned models on diverse set of standard language model benchmarks: MMLU (Hendrycks et al., 2020) (5-shot), ARC-C (Clark et al., 2018) (0-shot), PIQA (Bisk et al., 2020) (0-shot), WinoGrande (Sakaguchi et al., 2021) (0-shot), HellaSwag (Zellers et al., 2019) (0-shot), RACE (Lai et al., 2017) (0shot), GSM8K (Cobbe et al., 2021) (5-shot), and GPQA (Rein et al., 2024) (5-shot). Baselines and Implementation Details. We compare our method against several widely used pruning baselines, including Wanda (Sun et al., 2023), SparseGPT (Frantar and Alistarh, 2023), and magnitude-based pruning. All baselines are implemented based on the open-source codebases of Wanda and SparseGPT for fair comparison. For sink-aware pruning, we derive importance scores from attention maps aggregated across all layers and heads, and apply the resulting pruning masks uniformly to all pruned layers, including both feed-forward and attention layers. Importance scores are computed using calibration set drawn from WikiText-2 (Merity et al., 2016), consisting of 128 randomly sampled sequences each truncated to length of 2048 tokens. The same calibration"
        },
        {
            "title": "Sparsity Method",
            "content": "Avg. MMLU ARC-C PIQA WinoG GSM8K HellaSwag GPQA RACE"
        },
        {
            "title": "Dense Base",
            "content": "57.93 65.97 43.00 74.10 69.30 0. 0.50 0.75 Wanda 57.43 Sink-Aware 57."
        },
        {
            "title": "57.23\nSparseGPT\nSink-Aware 57.68",
            "content": "Wanda 52.70 Sink-Aware 53.18 SparseGPT 52.34 Sink-Aware 52.36 Wanda 29.99 Sink-Aware 30."
        },
        {
            "title": "32.57\nSparseGPT\nSink-Aware 32.57",
            "content": "65.20 65.41 65.16 65.33 61.43 62.16 60.97 60.79 24.76 24.01 28.60 28. 43.94 43.52 43.09 43.09 39.08 41.38 39.68 39.59 18.52 18.77 20.99 21. 75.30 74.97 74.43 74.37 72.63 73.18 72.20 72.95 56.69 59.96 61.75 60. 68.59 68.59 67.56 69.53 64.56 65.27 64.64 65.82 47.43 49.17 50.04 51. 69.29 66.03 68.16 67.17 68.58 57.01 55.88 53.53 52.11 0.99 1. 1.52 1.90 72.70 71.95 72.30 72.10 71.98 67.52 67.18 66.90 67. 45.25 46.85 48.20 48.70 30.40 38.70 29.85 29.70 30.25 30. 27.15 27.95 27.70 27.48 22.85 23.10 23.90 23.55 38.55 38.32 38.10 38. 32.20 32.45 33.10 32.82 23.45 24.10 25.55 25.20 Table 1: Pruning results on LLaDA across 8 benchmarks. Sparsity Method Avg. MMLU ARC-C PIQA WinoG GSM8K HellaSwag GPQA RACE Dense Base 60.94 68.25 55.20 74. 67.72 0.50 0.75 51.58 Wanda Sink-Aware 51.68 SparseGPT 54.40 Sink-Aware 54.58 30.38 Wanda Sink-Aware 30. SparseGPT 31.72 Sink-Aware 31.83 60.90 60.84 64.13 64.46 23.96 24.10 26.92 28.02 41.64 42. 44.88 44.97 20.65 21.08 22.18 21.25 68.12 68.82 70.18 70.18 53.43 52. 54.52 54.73 60.38 59.75 63.54 64.09 52.17 52.25 51.30 51.93 67. 49.13 49.66 52.84 53.15 1.06 1.14 1.44 1.59 73.30 65.40 65. 68.72 68.35 45.12 46.05 48.20 47.90 36.60 44.70 30.85 30. 32.48 32.90 23.50 23.72 24.30 24.65 36.20 35.85 38.42 38.58 23.15 22. 24.90 24.55 Table 2: Pruning results on Dream across 8 benchmarks. data is used for all baselines to ensure controlled and fair comparison. We evaluate both unstructured and structured pruning settings. For unstructured pruning, we consider sparsity ratios of 25%, 50%, and 75%. For structured pruning, following prior work, we evaluate multiple pruning levels corresponding to progressively removing structured components of the model. All experiments are conducted under identical configurations across models and benchmarks, and all results are reported using the same evaluation protocols."
        },
        {
            "title": "4.2 Pruning Results",
            "content": "Unstructured Pruning. Tables 15 report unstructured pruning results across all models and benchmarks. Sink-aware pruning consistently matches or surpasses the corresponding baselines at every sparsity level, demonstrating that accounting for the transient, denoising-step-dependent nature of attention sinks in DLMs yields more faithful compressed models than criteria designed for static AR-style sinks. The gains are most pronounced at moderate-to-high sparsity (50%75%), where aggressive compression amplifies the cost of naively removing weights that serve unstable but functionally critical sink positions. At lower sparsity (25%), the advantage narrows and is occasionally marginal, consistent with our observation that sink instability is not equally pronounced across all layers and model families, when the sparsity budget is generous, even standard importance criteria can avoid the most harmful removals. At 75% sparsity, all methods degrade substantially, yet sink-aware pruning remains among the top-performing approaches, suggesting that discounting transient sinks provides more principled lower bound on model utility under extreme compression. Importantly, the improvements are consistent across both Wanda and SparseGPT backbones, indicating that sink-aware masking acts as general-purpose correction that complements existing importance scoring rather than being tied to specific pruning criterion. Structured Pruning. Table 4 presents structured pruning results on LLaDA. Sink-aware pruning 7 Sparsity Method Avg. MMLU ARC-C PIQA WinoG GSM8K HellaSwag GPQA RACE Dense Base 58.59 64.07 50.43 73.61 68. 0.25 0.50 0.75 Wanda 58.09 Sink-Aware 58.22 58.44 SparseGPT Sink-Aware 58.47 Wanda 53.78 Sink-Aware 54. 53.91 SparseGPT Sink-Aware 54.10 Wanda 31.03 Sink-Aware 32.89 SparseGPT Sink-Aware 33.94 33.63 63.30 63.41 63.62 63. 59.04 58.67 59.20 59.33 25.87 27.20 29.85 29.08 51.37 51.54 52.05 52. 47.27 48.63 48.46 49.15 19.62 24.91 26.88 25.77 74.81 74.97 74.97 74. 72.69 73.39 72.63 73.12 59.09 61.37 61.21 61.86 68.19 68.27 68.51 68. 66.85 66.93 65.19 65.43 49.09 51.93 52.09 51.38 65.73 62.47 62. 63.08 63.23 51.63 52.01 52.24 52.69 1.14 1.29 1.67 1.52 74. 73.68 73.55 73.92 73.78 69.55 69.38 69.20 69.15 46.80 48.30 49.85 49. 31.75 39.87 31.20 31.30 31.55 31.62 28.35 28.48 28.92 28. 23.15 23.80 24.05 24.25 39.72 40.05 39.85 39.80 34.82 34.65 35.45 35. 23.50 24.35 25.90 25.65 Table 3: Pruning results on LLaDA1.5 across 8 benchmarks. Figure 6: Delta vs. baseline on average accuracy. Average accuracy change (Avg, in percentage points) from applying Sink-Aware pruning on top of each baseline (WANDA / SPARSEGPT) at different sparsity levels. Bars report = Avg(Sink-Aware) Avg(baseline); positive values indicate improved performance retention after pruning. Results are shown for LLaDA, Dream, and LLaDA1.5 over 8 benchmarks. Model PIQA WinoG ARC-E ARC-C"
        },
        {
            "title": "4.3 Visualization and Analysis",
            "content": "LLaDA (Base) 0.7942 0.7388 0.7420 0.4437 Structured Pruning 0.3 0.6834 LLaDA-structure Sink-Aware (Ours) 0. Structured Pruning 0.5 LLaDA-structure 0.5898 Sink-Aware (Ours) 0.6037 0.6630 0.6740 0.6907 0.7175 0.3780 0.3820 0.5572 0.5724 0.4853 0. 0.2039 0.2362 Table 4: Structured pruning results on LLaDA. consistently outperforms the structured baseline, with the margin growing at higher pruning ratios (0.30.5). This trend is intuitive: structured pruning removes entire heads or layers at once, making each decision considerably more disruptive than removing individual weights, and therefore more sensitive to whether sink-critical components are retained. The growing improvement at higher ratios further reinforces our broader observation that sink-aware pruning is most valuable precisely when compression is aggressive, and the cost of misidentifying important structures is highest. Fig. 6 reveals clear and consistent pattern: Sink-Aware pruning reliably improves over strong baselines across models and sparsity levels, with gains that grow as compression becomes more aggressive. The most notable improvements appear at 75% sparsity, where preserving structurally critical parameters has the greatest impact, most evidently on LLaDA-1.5 under Wanda. At lower sparsity levels (25% to 50%), gains are smaller but remain broadly positive, suggesting that the sink-aware signal usefully complements existing importance criteria even in less demanding regimes. Fig. 7 and Fig. 8 show how Sink-Aware pruning alters head-level pruning decisions under 50% unstructured sparsity setting on LLaDA-8B. The result suggests partial alignment between sink strength patterns and mask disagreement for Wanda, whereas the correspondence appears weaker for SparseGPT. The sparsity difference Figure 7: Sink strength and per-head mask disagreement (XOR) between baseline and sink-aware pruning. The left subfigure shows the sink strength of the original LLaDA-8B model, measured as the average attention each head allocates to sink tokens. The center and right show the fraction of weights per head whose pruning decision differs (XOR) between the baseline and our method, using Wanda (center) and SparseGPT (right), respectively. Figure 8: Per-head sparsity difference between baseline and sink-aware pruning. The left and right subfigures show the signed per-head sparsity difference for Wanda and SparseGPT, respectively. Red indicates that the sink-aware variant prunes more aggressively in given head, while blue indicates it preserves more weights. plot also indicates that Sink-Aware Pruning introduces smaller head-level pruning ratio changes for SparseGPT compared to Wanda. trade-off: sink-aware pruning is most beneficial when compression is aggressive or when diffusiontime attention dynamics are highly non-stationary. Discussion. Our results suggest that sink behavior is generation-paradigm dependent. In AR models, sinks are typically stable prefix anchors, in DLMs, sink positions shift across denoising steps and are often transient. This explains why directly inheriting AR-style always keep sinks heuristics can be suboptimal for diffusion pruning. By discounting unstable sinks, our Sink-Aware Pruning preserves larger fraction of semantically useful parameters under the same sparsity budget, especially at moderate-to-high pruning levels. We also observe that improvements are not uniform across all settings. On some modelbenchmark pairs (particularly in low-sparsity regimes), sink-aware variants are close to or slightly below strong baselines, indicating that sink instability is not equally pronounced in every layer, task, or model family. This points to practical"
        },
        {
            "title": "5 Conclusion",
            "content": "We revisited pruning for Diffusion Language Models from the perspective of attention-sink dynamics. Our analysis shows that sink positions in DLMs are substantially less stable than in AR models, which challenges the common sink-preservation heuristic inherited from AR pruning. Based on this observation, we proposed Sink-Aware Pruning, diffusion-oriented strategy that discounts unstable sinks when estimating pruning importance. Across multiple DLM families, benchmarks, and sparsity levels, the proposed method consistently improves or matches strong baselines while yielding better quality-efficiency trade-off without retraining. These findings highlight that pruning rules should be aligned with generation dynamics rather than directly transferred across paradigms."
        },
        {
            "title": "Limitations",
            "content": "Our current study has several limitations. First, sink statistics are estimated from fixed calibration distribution, and distribution shift may reduce reliability. Second, we mainly evaluate post-training pruning without recovery finetuning. Combining sinkaware scoring with lightweight post-pruning adaptation may further improve robustness. Moreover, while we include one multimodal DLM, broader validation on larger multimodal and long-context settings is still needed. Future work can explore layer-wise timestep-adaptive sink policies and joint optimization with quantization to further improve the quality-efficiency frontier."
        },
        {
            "title": "References",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. 2025. Block Interpolating between autoregressive diffusion: arXiv preprint and diffusion language models. arXiv:2503.09573. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993. Federico Barbero, Alvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veliˇckovic, and Razvan Pascanu. 2025. Why do arXiv preprint llms attend to the first token? arXiv:2504.02732. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, and 1 others. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439. Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. 2024. survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence. Shuang Cheng, Yihan Bian, Dawei Liu, Linfeng Zhang, Qian Yao, Zhongbo Tian, Wenhai Wang, Qipeng Guo, Kai Chen, Biqing Qi, and 1 others. 2025. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Rocktim Jyoti Das, Mingjie Sun, Liqun Ma, and Zhiqiang Shen. 2023. Beyond size: How gradients shape pruning decisions in large language models. arXiv preprint arXiv:2311.04902. Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pages 1032310337. PMLR. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. The language model evaluation harness. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. 2022. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. 2024. When attention sink emerges in language models: An empirical view. arXiv preprint arXiv:2410.10781. Song Han, Huizi Mao, and William Dally. 2015a. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149. Song Han, Jeff Pool, John Tran, and William Dally. 2015b. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28. Babak Hassibi, David Stork, and Gregory Wolff. 1993. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks, pages 293299. IEEE. Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuan-Jing Huang, and Xipeng Qiu. 2023. Diffusionbert: Improving generative masked language models with diffusion models. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: Long papers), pages 45214534. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. 10 Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840 6851. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785 794, Copenhagen, Denmark. Association for Computational Linguistics. Yann LeCun, John Denker, and Sara Solla. 1989. Optimal brain damage. Advances in neural information processing systems, 2. Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. 2025. survey on diffusion language models. arXiv preprint arXiv:2508.10875. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusionlm improves controllable text generation. Advances in neural information processing systems, 35:4328 4343. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. 2017. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 27362744. Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Importance estimaFrosio, and Jan Kautz. 2019. tion for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1126411272. Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, and Michael Qizhe Shieh. 2025. Diffusion language models are super data learners. arXiv preprint arXiv:2511.03276. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Large language diffusion models. arXiv preprint arXiv:2502.09992. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, and 1 others. 2018. Improving language understanding by generative pre-training. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First conference on language modeling. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Maximo Eduardo Rulli, Simone Petruzzi, Edoardo Michielon, Fabrizio Silvestri, Simone Scardapane, Attention sinks and Alessio Devoto. 2025. arXiv preprint in diffusion language models. arXiv:2510.15731. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. 2024. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. 2023. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. 2025. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. 2025a. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809. Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus Müller, Jonas Kübler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, Sravan Babu Bodapati, and 1 others. 2025b. Wanda++: Pruning large language models via regional gradients. arXiv preprint arXiv:2503.04992. 11 Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th annual meeting of the association for computational linguistics, pages 47914800. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and 1 others. 2025. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223."
        },
        {
            "title": "A Results on Additional Models",
            "content": "Table 5 reports pruning results on MMaDA, multimodal unified diffusion language model, complementing the main-paper results on LLaDA, Dream, and LLaDA-1.5."
        },
        {
            "title": "B Evaluation Details",
            "content": "PIQA, WinoGrande, loglikelihood-based evaluations (MMLU, For ARC-C, HellaSwag, RACE, and GPQA), we build on the official model implementations provided in the LLaDA1 and Dream2 GitHub repositories. For generation-based tasks (GSM8K), we leverage Fast-DLLM (Wu et al., 2025) to enable fast inference. All evaluations are conducted using the lm-evaluation-harness (Gao et al., 2024) framework under identical configurations across all models and baselines. For generation tasks, we use generation length of 256 tokens, block length of 256, and 256 denoising steps. For loglikelihood-based benchmarks, we follow the Monte Carlo estimation protocol with 128 samples across all tasks, except for MMLU where we use single sample following the evaluation convention established in the official LLaDA codebase."
        },
        {
            "title": "C Model Architecture Details",
            "content": "All models use decoder-only Transformer backbone as the denoising network (mask predictor) in 1https://github.com/ML-GSAI/LLaDA 2https://github.com/DreamLM/Dream discrete diffusion. For LLaDA, the architecture follows LLaMA-style design with RMSNorm, SwiGLU feed-forward blocks, and RoPE positional encoding (Nie et al., 2025). Dream-7B adopts the same base configuration as Qwen2.5-7B (Ye et al., 2025; Yang et al., 2024). MMaDA-8B follows the LLaDA configuration for text generation and is initialized from an LLaDA-8B checkpoint (Yang et al., 2025a). Table 6 shows the total number of trainable parameters, the number of Transformer layers L, and the model width dmodel."
        },
        {
            "title": "Params",
            "content": "#Layers dmodel LLaDA-8B LLaDA-1.5-8B Dream-7B MMaDA-8B 8.02B 8.02B 7B 8.02B 32 32 28 32 4096 4096 3584 Table 6: Backbone configurations."
        },
        {
            "title": "D Benchmark Descriptions",
            "content": "We evaluate text generation and reasoning using diverse suite of established benchmarks: MMLU (Hendrycks et al., 2020) evaluates broad factual knowledge and reasoning across 57 academic subjects using multiple-choice questions, and is commonly reported in fewshot settings. ARC-C (Clark et al., 2018) (AI2 Reasoning Challenge, Challenge split) consists of multiple-choice grade-school science questions designed to require non-trivial reasoning beyond surface pattern matching."
        },
        {
            "title": "Sparsity Method",
            "content": "Avg. MMLU ARC-C PIQA WinoG GSM8K HellaSwag GPQA RACE"
        },
        {
            "title": "Dense Base",
            "content": "35.63 33.69 24.32 60.50 50.83 0. 0.50 0."
        },
        {
            "title": "34.05\nSparseGPT\nSink-Aware 33.77",
            "content": "Wanda 29.85 Sink-Aware 30."
        },
        {
            "title": "30.26\nSparseGPT\nSink-Aware 30.19",
            "content": "32.89 32.99 33.30 33.30 31.14 30.27 31.33 31.24 23.93 24.82 24.01 24. 23.98 24.15 24.57 24.83 23.12 22.95 23.81 23.72 19.37 20.65 20.39 18. 60.28 60.17 60.39 60.34 59.79 59.90 59.36 59.41 54.79 55.33 55.77 55. 50.59 49.64 50.91 50.67 51.14 49.57 50.04 48.86 50.12 49.64 49.01 50. 9.48 8.72 8.87 9.10 9.17 5.38 5.53 5.84 5.76 0.76 0. 1.06 1.14 52.50 52.25 51.80 52.05 52.10 49.72 49.45 50.25 49. 42.55 42.90 43.48 43.15 25.20 28.50 24.80 24.72 25.15 25. 24.18 24.08 24.55 24.30 23.10 23.55 23.40 23.28 28.12 27.90 28.35 28. 26.80 26.55 27.20 26.95 24.15 24.40 24.95 24.70 Table 5: Pruning results on MMaDA across 8 benchmarks. PIQA (Bisk et al., 2020) tests physical commonsense reasoning via multiple-choice questions about everyday situations, selecting the more plausible solution among two candidate answers. WinoGrande (Sakaguchi et al., 2021) measures commonsense coreference and pronoun resolution using adversarially filtered, multiple-choice Winograd-style sentence pairs. HellaSwag (Zellers et al., 2019) assesses commonsense inference by selecting the most plausible continuation of given context from multiple candidate endings. RACE (Lai et al., 2017) is reading comprehension benchmark derived from English examinations, requiring multi-sentence reasoning over passages with multiple-choice questions. GSM8K (Cobbe et al., 2021) evaluates multistep mathematical reasoning on grade-school word problems; performance is typically measured by exact match on the final numeric answer. GPQA (Rein et al., 2024) is challenging, domain-expert-written benchmark of graduate-level multiple-choice questions in biology, physics, and chemistry, designed to be difficult to solve via simple lookup."
        }
    ],
    "affiliations": [
        "VILA Lab, MBZUAI"
    ]
}