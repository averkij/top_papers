{
    "paper_title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models",
    "authors": [
        "Annemette Brok Pirchert",
        "Jacob Nielsen",
        "Mogens Henrik From",
        "Lukas Galke Poech",
        "Peter Schneider-Kamp"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available."
        },
        {
            "title": "Start",
            "content": "FlexMoRE: Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models Annemette Brok Pirchert1 , Jacob Nielsen1,2 , Mogens Henrik From1,2 , Lukas Galke Poech1 and Peter Schneider-Kamp1 1University of Southern Denmark 2Ordbogen A/S ampirchert@gmail.com {jacn,from,galke,petersk}@imada.sdu.dk 6 2 0 2 9 ] . [ 1 8 1 8 8 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 20 to 214 resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available."
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) often benefit from access to\ndomain-specific data in specialized settings. In many practi-\ncal applications, such data is subject to privacy, legal, or pro-\nprietary constraints that limit centralized collection or shar-\ning. At the same time, maintaining multiple domain-adapted\nmodels can be costly in terms of hardware, storage, and train-\ning resources.",
            "content": "Such constraints commonly arise in domains such as healthcare, law, and enterprise systems, where regulations like the American HIPAA and the European GDPR restrict data movement and usage [Xie et al., 2025; Pahune et al., 2025]. In these settings, centralized training is often cumbersome or directly impossible. This motivates training approaches and model architectures that can incorporate domain-specific expertise without direct data sharing. However, combining independently trained models is often non-trivial, as many existing architectures assume joint optimization, shared parameters, or fixed model composition. Existing approaches address only parts of this problem. Mixture-of-Experts (MoE) architectures scale model capacity via sparse routing but typically rely on centrally trained full-size experts with large parameter count and high memory requirements. [Cao et al., 2024; Mu and Lin, 2025; Zhao et al., 2025]. Pathway Language Models (PaLM) [Chowdhery et al., 2023; Anil et al., 2023] orchestrate model across many accelerators, with models that can generalize over different domains and tasks while being highly efficient. Pathways enable data parallelism at pod (node) level, making it possible to orchestrate data in separate training nodes. Parameter-efficient fine-tuning methods such as LoRA [Hu et al., 2022] and Mixture-of-Adapters (MoA) [Cao et al., 2025; Wang et al., 2022] reduce adaptation cost by introducing low-rank, additive modules into shared backbone. These approaches do not support independently trained experts or inference-time opt-in and opt-out [Hu et al., 2023], though. Recently, FlexOlmo [Shi et al., 2025] enabled decentralized expert training and inference-time composition without data sharing by training domain-specific experts alongside public and frozen base model. However, major drawback is that FlexOlmo relies on full-size experts, which in practice limits scalability due to high accelerator memory requirements. In this work, we introduce FlexMoRE, which builds on the FlexOlmo framework for decentralized expert composition but significantly reduces the parameter count and accelerator memory footprint of individual experts. FlexMoRE supports independently trained full-size and low-rank experts within the same MoE routing framework. The low-rank experts might be either trained as adapters from scratch alongside the public base model, or, they can be derived via post-hoc lowrank factorization of fully-finetuned experts relative to base expert. The latter we designate as Post-hoc Low-Rank Adaptation (PHLoRA) experts [Vasani et al., 2025]. In our Flexthis through improved routing strategies that aim to stabilize training and improve expert utilization [Liu et al., 2024; Zhou et al., 2022]. Other studies further show that MoE architectures can be made parameter-efficient by restricting updates to lightweight experts, achieving performance comparable to full fine-tuning while modifying only small fraction of parameters [Zadouri et al., 2023]. System-level optimizations reduce the runtime cost of executing MoE models through memory and inference improvements [Rajbhandari et al., 2022; Cao et al., 2024; Wang et al., 2025; Zhao et al., 2025] but assume fixed, centrally trained pool of experts and shared data access. Low-Rank Adapters and Mixtures Parameter-efficient fine-tuning methods adapt large language models by introducing lightweight trainable modules while keeping the backbone frozen. Low-Rank Adaptation (LoRA) [Hu et al., 2022] is widely adopted approach that substantially reduces training cost and the number of trainable parameters while maintaining strong performance. Multiple LoRA adapters can be treated as expert models that can selectively be routed to [Hu et al., 2023]. Several recent works extend LoRA using MoE designs, exploring different routing and expert allocation strategies such as dynamic routing, layer-wise expert placement, and sparse expert activation [Kunwar et al., 2025; Cao et al., 2025; Ji and Song, 2025; Zhuang et al., 2025; Li et al., 2024; Zou et al., 2025]. Despite these advances, most MoELoRA approaches assume centralized training and joint optimization over shared adapter pool, limiting their applicability in settings with restricted data sharing and federatedly trained experts. Decentralized Expert Composition under Data Governance Constraints FlexOlmo introduces language model architecture designed to support flexible data usage under strict governance constraints, commonly arising from regulations such as HIPAA, GDPR, as well as data sovereignty requirements [Shi et al., 2025; Pahune et al., 2025]. In many real-world settings, these constraints effectively preclude centralized access to sensitive data. As result, decentralized training paradigms such as federated learning have gained attention, as they avoid centralized access to raw data [Abishek et al., 2025]. MoE variants have also been explored in federated contexts as means of balancing data heterogeneity, privacy, and model performance [Yi et al., 2024]. Unlike most conventional MoE architectures, which are typically trained via joint optimization over shared or centrally accessible datasets, FlexOlmo enables experts to be trained independently on closed or private data and composed only at inference time. domain-informed routing mechanism allows experts to be selectively included or excluded during inference, enabling fine-grained control over which data sources contribute to given prediction. Summary Existing work on MoE, parameter-efficient finetuning, and decentralized training addresses orthogonal aspects of model scaling and adaptation. LoRA-based methods reduce adaptation cost within shared backbones, while FlexOlmo enables inference-time composition of independently trained dense experts under data governance constraints. With FlexMoRE, we now explore whether FlexOlmo-style Figure 1: FlexMoRE follows standard MoE architecture, similarity FlexOlmo, utilizing the domain-informed router, but routing to one or more group(s) with base expert and rank-heterogeneous experts. MoRE architecture, low-rank experts are implemented as independent MoE experts relative to full-size base expert. In the case of FlexOlmo, we can conviently use the public base model as the full-size base expert. In this paper, we show that decentralized training and flexible inference-time composition extend to experts parameterized via low-rank approximations rather than full-size experts. In sum, our contributions are: flexible mixture of rank-heterogeneous experts architecture, called FlexMoRE, in which full-size experts can be deliberately combined with low-rank experts.1 Empirical support confirming the effectiveness of this architecture obtained through deriving post-hoc LoRA experts from the existing FlexOlmo model, showing without the need for training that we can retain and even improve the performance across most benchmarks at one third of the memory requirements. comprehensive study of using different LoRA ranks for the experts and an in-depth analysis of rank sensitivity showing that reasoning-heavy tasks require higher ranks than knowledge-heavy tasks."
        },
        {
            "title": "2 Related Work\nMixture of Experts (MoE) MoE architectures enable con-\nditional computation by sparsely routing inputs to a subset\nof specialized experts, allowing model capacity to scale with-\nout proportional increases in per-token computation [Fedus et\nal., 2022; Mu and Lin, 2025]. MoE architectures are widely\nused to improve efficiency and specialization in large lan-\nguage models. A central challenge in MoE systems is ex-\npert routing, as suboptimal routing can lead to under-utilized\nPrior works have addressed\nor over-specialized experts.",
            "content": "1Code will be made available for reproducibility and reuse. decentralized expert composition remains effective with lowrank rather than full-size experts."
        },
        {
            "title": "3.1 The FlexMoRE architecture\nthe feedforward module\nIn a standard MoE architecture,\n(FFN) in each transformer block is replaced by a routing\nmodule and n FFN experts. At inference time, a limited selec-\ntion of experts is selected per layer to facilitate a forward pass.\nExperts are typically trained jointly in MoE models. Flex-\nOlmo [Shi et al., 2025] introduced the possibility of training\nexperts independently from each other: Each expert is trained\nonly with the globally shared public expert. This ensures that\nthe individual routing modules are anchored against the same\nbase model.",
            "content": "In our proposed FlexMoRE architecture, we additionally consider low-rank experts. We allow that low-rank adapters can be integrated as experts and that the low-rank adapters can be attached to an arbitrary full-size expert. In FlexOlmo setting, natural choice for this full-size expert is the public model. This is because the public expert already needs to be consulted for individual expert training. Therefore, the public expert also forms an ideal base for the low-rank adapters. In theory, FlexMoRE could be composed of an arbitrary mix of at least one full-size and multiple low-rank experts. Each low-rank expert needs to track which other expert (either full-size, or low-rank) it uses as base. To ensure this relationship is well-founded, we require that at some point, full-size expert is reached. Crucially, the low-rank experts do not need to have the same rank. Given the full-size expert Mbase, it can have low-rank adapters, each with different rank ri, that attach to it mri . We denote such combination as := (Mbase, {mri }), where mi denotes an expert with index and rank ri. One can see that one could compose multiple combinations of such full-rank plus low-rank experts M0, . . . Mj. For the sake of simplicity, however, in this paper, we will only consider cases with single full-sized expert and sets of low-rank experts of (potentially varying) ranks ri. Just as in FlexOlmo, FlexMoRE depends on the domaininformed router function mapping the input vector to distribution over expert modules: (x) = Wrx, Wr R(n+1)h. This also means we inherit the decomposing of Wr into individual expert-specific router embeddings, having each row representing specific full-size or low-rank mri expert. Routing to any low-rank expert also triggers its corresponding full-size base expert, to which we then apply the low-rank adapter mri ."
        },
        {
            "title": "3.2 Deriving Experts through Adapter Extraction\nWe consider the weights of a public base expert (layer in-\ndices omitted for simplicity): Expert 0 (public) : Wbase ∈\nRdout×din and the respective expert i < N : Expert i (domain) :",
            "content": "Wi Rdoutdin . Each trained expert requires identical amounts of memory and computational resources, even when data might not necessitate the entire offered capacity. We hypothesize that some domains require less capacity than others. More specifiaclly, we expect that low rank approximation of an expert En trained on domain-specific dataset Ddomain is sufficient to perform comparably to full-size expert. We integrate such experts as follows. First, we calculate the difference between the public model W0 and the domain expert Wi: = WiW0. This represents the contribution of the domain-specific expert, which also implies that each expert remains anchored to their corresponding base expert. Next, we compute the truncated singular value decomposition: = UΣV, where Rdoutdout , Σ Rdoutdin, and Rdindin , denoting the left-singular vector, singular value matrix and left-singular vector, respectively. We obtain the low-rank approximation by utilizing only the first values, truncating the SVD, n, obtaining Ur, Σr and . Ur = U[:, : r] Rdoutr Σr = diag(σ1, . . . , σr) Rrr = V[: r, :] Rrdin (1) (2) (3) This exactly reduces the shape of our expert, defined by the the best rank approximation. The original matrix can be reconstructed as follows: (r) = UrΣrV (cid:101) = (Ur Σr)V (4) Thies yields rank approximation of the original matrix. We employ our rank-tuned expert by adding it to its corresponding base expert (e.g. the public base model): (cid:102)W(r) (r) = W0 + (cid:101) With Equation 5, we formulate an approximation of given Expert with an approximation error introduced by truncating the decomposition at rank r. (5) Our architectural design, therefore, allows us to derive lowrank experts from the full-sized experts via PHLoRA [Vasani et al., 2025]. Here, we obtain SVD components and that we need to derive low-rank expert by splitting the Σr symmetrically between the Ur and Vr components: (cid:112) (cid:112) = Ur Σr, = Vr Σr (6) This allows us to easily integrate with existing LoRA libraries such as the PEFT Library. This implies the MoE module can compute the output given an input as follows: = ΣiTopk(f (x))softmax(f (x)i)(Mn + BnAn) (7) where and is the components from low rank expert and denotes the router function that computes the probabilities from x. Low-rank adapters could also be trained from scratch in fashion similar to the full-finetuning performed in FlexOlmo."
        },
        {
            "title": "3.3 Rank Sensitivity Analysis\nA crucial objective of this research is to understand the re-\nlationship between expert rank and task performance. To",
            "content": "quantify this, we estimate rank sensitivity using linear regression between log2 of the expert rank and task performance. For each model family and evaluation group, we fit s(r) = α + β log2 r, where s(r) denotes the evaluation score at rank while β measures sensitivity to rank increases. Positive values of β indicate that increasing rank consistently improves performance, while values near zero or negative indicate diminishing returns. We apply this procedure to both the full combined mixture-of-experts model and the individual experts. To identify the rank at which performance peaks, we further define the typical peak rank for expert and evaluation group as e,g = arg maxr{20,...,214} se,g(r), computed directly from the observed scores without regression, smoothing, or normalization. In the case of ties, the lowest rank achieving the maximum score is selected."
        },
        {
            "title": "4.1 Evaluation Datasets\nWe evaluate all models on a benchmark suite aligned with\nthat used in FlexOlmo [Shi et al., 2025], enabling com-\nparability. Our evaluation spans a diverse collection of\nestablished benchmarks grouped into general-purpose and\ndomain-specific evaluations, covering a total of 120 tasks.",
            "content": "General-purpose evaluation includes: MC9, collection of nine multiple-choice reasoning benchmarks (ARC-Easy, ARC-Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], CommonsenseQA (CSQA) [Reddy et al., 2019], HellaSwag [Zellers et al., 2019], OpenBookQA [Mihaylov et al., 2018], PIQA [Bisk et al., 2019], SocialIQA [Sap et al., 2019], and WinoGrande [Sakaguchi et al., 2019]); GEN5, consisting of five generative question answering tasks (CoQA) [Reddy et al., 2019], SQuAD [Rajpurkar et al., 2016], Natural Questions [Kwiatkowski et al., 2019], TriviaQA [Joshi et al., 2017], and DROP [Dua et al., 2019]); AGIEval, suite of college-level academic reasoning tasks [Zhong et al., 2023]; and BBH, collection of challenging multi-step reasoning tasks from BIG-Bench [Suzgun et al., 2022]. Domain-specific evaluation includes MMLU [Hendrycks et al., 2021] and MMLU-Pro [Wang et al., 2024], which both assess broad academic knowledge across multiple disciplines. In total, we consider 120 individual evaluation tasks. (cid:80)"
        },
        {
            "title": "4.2 Evaluation Procedure and Baselines\nMeasures We summarize downstream task performance by\nan aggregate score ‘Avg’, which follows the same evalua-\ntion protocol as FlexOlmo. Specifically, Avg is computed as\nthe unweighted mean over evaluation group means: Avg =\n1\n, where G is the set of our six\n|G|\nconsidered evaluation groups (MC9, GEN5, AGIEval, BBH,\nMMLU, and MMLU-Pro), Tg is the set of tasks within group\ng, and st is the task-level score.\nTested configurations We evaluate the following configu-\nrations: (iii) The individual low-rank experts derived from",
            "content": "(cid:16) 1 Tg tTg (cid:80) gG st (cid:17) the six available FlexOlmo experts (Code, Creative Writing, Math, News, Academic, Reddit), which we evaluate as mixture-of-experts models with two experts. The Educational Text expert is not publicly available and has, thus, not been part of our experimental setup. (ii) homogeneous FlexMoRE models, with one full-sized expert (the public model) and the remaining experts as low-rank adapters of identical rank. We evaluate ranks from 20 to 211, as ranks greater than 211 increase rather than decrease the total number of model parameters. (iii) heterogeneous FlexMoRE, where the low-rank adapters can be of different ranks. To determine the bestperforming ranks per expert, we consider their performance on the MC9 eval group or the average Avg over all six groups. For MC9 as the reference eval group, we obtain rank 26 for Code, 27 for Creative Writing, 211 for Math, 20 for News, 23 for Academic, and 27 for Reddit. When we consider performance on all eval groups, we obtain rank 29 for Code, 24 for Creative Writing, 211 for Math and Academic, 26 for News, and 29 for Reddit. Note that this evaluation procedure differs from the one in FlexOlmo [Shi et al., 2025]: All our expert evaluations are conducted in 2x7B setup with the expert alongside the public base experts. FlexOlmo instead evaluates single experts in isolation without the public base expert. Our approach ensures that the evaluation corresponds to both how the experts have been trained and how they are going to be used. For comparability to the full-size experts, we also re-evaluate the six available FlexOlmo experts in the exact same way. Baselines Our main baseline for the final MoE model is FlexOlmo with full-size experts, under varying numbers of active experts (2, 4, 7), reflecting choices reported in [Shi et al., 2025]. Our baselines for the evaluation of individual lowrank experts are the corresponding experts in their full-size version. Relative improvement against the respective baseline is quantified as: [%] = 100 AvgmodelAvgbaseline . Avgbaseline"
        },
        {
            "title": "5.1 Single-expert Results\nOur main finding is that low-rank experts can be competitive\nand even outperform their full-size baseline. Table 1 shows\nthe performance of the low-rank experts (r ≤ 211) against\nthe FlexOlmo baseline experts (evaluated as mixture of the\npublic base model and the respective expert). We observe\nconsistent performance gains for low-rank experts, ranging\nfrom 0.73% to 7.13% for the Code, Creative Writing, Aca-\ndemic, and Reddit experts at ranks 29, 24, 26, 211, and 29,\nrespectively. In contrast, the Math expert exhibits a perfor-\nmance decrease of 1.34%, even when evaluated against the\naverage at the highest meaningful rank (211). Full results for\nall experts on all benchmarks across all tested ranks can be\nfound in Appendix A. Crucially, no single LoRA rank is con-\nsistently best across all experts. Some evaluation groups ex-\nhibit flat performance across a wide range of ranks, whereas",
            "content": "2x7B-1T Expert Code (baseline) Code (r = 29) Creative Writing (baseline) Creative Writing (r = 24) Math (baseline) Math (r = 211) News (baseline) News (r = 26) Academic (baseline) Academic (r = 211) Reddit (baseline) Reddit (r = 29) Total Expert MC9 GEN5 AGIEval BBH MMLU MMLU-Pro Avg (%) 11.63B 8.04B 11.63B 7.32B 11.63B 10.27B 11.63B 7.39B 11.63B 10.27B 11.63B 8.04B 0.6757 4.33B 743M 0.6866 0.6709 4.33B 23.3M 0.6854 4.33B 2.97B 0.6836 0.6832 0.6602 4.33B 92.9M 0. 4.33B 2.97B 0.6732 0.6710 0.6171 4.33B 743M 0.6768 0.4668 0.4946 0.4998 0.5074 0.4862 0. 0.5058 0.5078 0.5000 0.5104 0.4108 0.5018 0.3909 0.3824 0.3965 0.3959 0.4138 0. 0.3691 0.4004 0.3944 0.3928 0.3668 0.3983 0.3831 0.3704 0.3424 0.3545 0.4589 0. 0.3554 0.3646 0.3622 0.3653 0.3668 0.3499 0.5381 0.5464 0.5378 0.5601 0.5648 0. 0.5387 0.5578 0.5479 0.5486 0.5358 0.5441 0.2443 0.2492 0.2478 0.2595 0.2615 0. 0.2446 0.2624 0.2422 0.2515 0.2350 0.2422 0.4498 0.4549 0.4492 0.4605 0.4781 0. 0.4457 0.4631 0.4533 0.4566 0.4221 0.4522 +1.14 +2.50 -1. +3.92 +0.73 +7.13 Table 1: Best post-hoc LoRA experts with rank = 2k, {0, . . . , 11} compared to their full-size baselines. Model size is reported as number of total parameters of the 2x7B mixture and for the expert only. All scores are reported as mean performance, and relative improvements ( Avg / (%)) are computed with respect to cl corresponding baseline model. others favor higher-rank specialization. To select the expert ranks for composing our FlexMoRE MoE models, we follow two strategies, one relying on experts MC9 performance as proxy and one relying on experts average performance across all benchmarks. Details can be found in Appendix A."
        },
        {
            "title": "5.2 Results of Mixture-of-Experts Models",
            "content": "Table 2 reports the results of the MoE models (i.e., the public base model plus six experts) across the six eval groups. The experts rank in our heterogeneous are either defined by individual experts performance on MC9 or by average performance over all six benchmarks. We compare the heterogeneous model to homogeneous FlexMoRE model with fixed rank across all experts and to the FlexOlmo baseline. The heterogeneous experts outperforms both the FlexOlmo baseline and the homogeneous FlexMoRE models on average. Even more interestingly, the MC9calibrated ranks perform better than the ranks calibrated across all six eval groups, performing consistently better on average performance Avg. The MC9-calibrated rank-heterogeneous FlexMoRE models are globally best. In Figure 2 we report the 7-experts FlexMoRE with 2, 4, and 7 active experts per token. It is evident that while homogeneous rank tuning can outperforms the FlexOlmo baseline with the right choice of rank, the heterogeneous FlexMoRE models, consistently outperform both the baseline and all rank-homogeneous FlexMoRE models. Across all configurations, the FlexMoRE model comprising experts selected according to their performance on MC9 consistently outperforms selection based on the average performance across all six eval groups. This effect is most pronounced for configurations with very few (a2) or many (a7) active experts, while the difference is less pronounced but still non-negligible for a4 as shown in Table 2 and in Figure 2. Despite incorporating more task-specific information, the FlexMoRE model based on Avg favors capacity-heavy specialists whose performance gains occur at higher LoRA ranks, resulting in reduced effectiveness under fixed rank and routing budgets. In contrast, the FlexMoRE model based on MC9 performance implicitly prioritizes rank-efficient experts that deliver strong marginal gains at low to moderate ranks. This yield superior performance across eval groups. We provide further supporting material in Appendix A. Task Diversity and Rank Efficiency Domain-specialized experts perform best on the benchmarks aligned with their expected domains (e.g., Math on BBH, News on MMLU-Pro), typically peaking at higher ranks. This complementarity motivates heterogeneous post-hoc LoRA as principled mechanism for uneven capacity allocation, achieving strong overall performance while substantially reducing parameter count and accelerator memory footprint compared to homogeneous rank tuning. Restricting rank calibration to unique experts yields qualitatively similar trends, confirming that results are not driven by multiple ranks of the same expert. Generally, heterogeneous FlexMoRE models consistently outperform the homogeneous variants as well as the FlexOlmo baseline. All these gains come with considerable reductions in memory and computational requirements. For the single experts in our best-performing heterogeneous model based on MC9 performance, the memory requirements are reduced by 31.39% for the Math expert, 95.71% for the Creative Writing and Reddit experts, 97.85% for the Code expert, 99.73% for the Academic expert, and 99.96% for the News expert when compared to full expert."
        },
        {
            "title": "5.3 Results of Rank Sensitivity Analysis\nFigure 3 shows the rank–performance trends and benchmark-\nspecific peak ranks. To quantify rank sensitivity, we fit a lin-\near regression between log2 rank and performance for each\nevaluation group (see Section 3.3). As shown in Figure 3,\nreasoning-oriented benchmarks such as BBH exhibit consis-\ntently positive rank sensitivity in FlexMoRE 7x7B models,\nwith slopes up to 0.0104 and strong Pearson correlations\n(r ≈ 0.94–0.96 for FlexMoRE a2 and a4), indicating a coher-",
            "content": "7x7B-1T Model Params MC9 GEN5 AGIEval BBH MMLU MMLU-Pro Avg. (%) FlexOlmo-a2 (baseline) FlexMoRE-a2 (homogen. = 29) FlexMoRE-a2 (heterogen. all) FlexMoRE-a2 (heterogen. MC9) FlexOlmo-a4 Baseline FlexMoRE-a4 homogen. (r = 210) FlexMoRE-a4 heterogen. (All) FlexMoRE-a4 heterogen. (MC9) FlexOlmo-a7 Baseline FlexMoRE-a7 homogen. (r = 210) FlexMoRE-a7 heterogen. (All) FlexMoRE-a7 heterogen. (MC9) 33.27B 11.75B 14.84B 10.75B 33.27B 16.21B 14.84B 10.75B 33.27B 16.21B 14.84B 10.75B 0.6257 0.6722 0.6776 0. 0.6709 0.6819 0.6896 0.6936 0.6550 0.6733 0.6862 0.6894 0.4508 0.4886 0.4780 0.4868 0.4548 0.4784 0.4846 0.4896 0.3614 0.4776 0.4834 0.4896 0.3842 0.3976 0.4169 0. 0.3999 0.3920 0.4121 0.4140 0.3745 0.3680 0.4042 0.4140 0.4469 0.3897 0.4195 0.4269 0.4014 0.3887 0.4124 0.4255 0.3560 0.3662 0.3861 0.4255 0.5297 0.5483 0.5475 0. 0.5518 0.5463 0.5557 0.5516 0.5097 0.5209 0.5398 0.5516 0.2415 0.2501 0.2537 0.2543 0.2486 0.2487 0.2533 0.2564 0.2212 0.2265 0.2420 0.2564 0.4465 0.4577 0.4655 0. 0.4546 0.4560 0.4679 0.4718 0.4130 0.4388 0.4569 0.4711 +2.53 +4.27 +5.49 +0.32 +2.94 +3.79 +6.25 +10.65 +14.08 Table 2: Results for the Mixture of Experts models with a2, a4, and a7 active experts. The FlexOlmo baseline is full-sized model without any low-rank adapters. FlexMoRE homogeneous is the configuration with one full-size expert and all remaining six experts as low-rank adapters of the same rank (the ranks = 2k, {0, . . . , 11}, utilizing the rank that performs best on average across all benchmarks. In FlexMoRE heterogenous (All), we selected the rank for each expert based on performance across all six benchmarks. Finally, FlexMoRE heterogenous (MC9) is our best-performing proposed model, where we select the rank for each expert based on its performance on MC9. All scores are reported as mean performance. Relative improvements (%) are computed against the corresponding baseline model. Figure 2: Unweighted average performance of FlexMoRE models with 2, 4, and 7 active experts across six benchmarks. The solid curve shows performance under homogeneous post-hoc LoRA rank tuning. Dashed horizontal lines correspond to heterogeneous FlexMoRE compositions, with the dotted line indicating experts selected based on performance across all benchmarks (All) and the dashed line experts selected using MC9 (MC9). The FlexOlmo baseline is shown for reference. ent and monotonic benefit from increased rank. In contrast, knowledge-oriented benchmarks such as GEN5 and MC9 frequently show weak or negative rank sensitivity (e.g., GEN5 slopes down to 0.0042 with 0.86), suggesting diminishing returns or overfitting at higher ranks. Detailed results on the rank sensitivity regression analysis can be found in Appendix B. Aggregating results across evaluation groups, FlexMoRE models exhibit positive typical rank sensitivity, with the strongest median effect observed at an intermediate number of active experts (FlexMoRE a4, median slope 0.0030), followed by diminished gains for larger expert counts (FlexMoRE a7, median 0.0008). In contrast, the most evaluated individually show near-zero or negative median rank sensitivity, that performance is unaffected by low-ranking. The exception is the math expert which benefits from higher ranks (median slope 0.0018). Details can be found in Appendix B. Typical Peak Rank Per Task For each evaluation group, we summarize the distribution of log2 e,g across experts using the median and interquartile range. Results show that peak performance typically occurs at moderate ranks rather than at the maximum tested rank as shown in Figure 3. With respect to average performance, the median peak occurs at log2 = 9 with an interquartile range of [6.75, 10.5], corresponding approximately to ranks 27210. Knowledgeoriented benchmarks peak substantially earlier (e.g., MMLU: median log2 = 2), whereas reasoning-heavy benchmarks peak later (e.g., BBH: median log2 = 11.5). Expert-level Heterogeneity Aggregated rank trends mask substantial heterogeneity across individual experts. To isolate expert-specific effects, we repeat the regression analysis independently for each expert using the aggregated average score across evaluation groups. While some experts (most notably the Math expert) exhibit strong and stable positive relationship between rank and performance, several experts show flat or negative slopes. This heterogeneity explains the weak average rank sensitivity observed for expert-only models and motivates per-expert analysis of rank effects (see Appendix B). Figure 3: Typical log2 LoRA rank at which experts achieve peak performance. For each expert and evaluation group g, the peak rank is computed directly from the observed scores after sorting by rank and resolving ties by selecting the lowest rank. Peak performance typically occurs at moderate ranks: for the aggregated average (Avg), the median peak is at log2 = 9 with IQR [6.75, 10.50] (i.e., ranks 27210). Knowledge-oriented benchmarks peak earlier (e.g., MMLU: median log2 = 2, IQR [1.25, 8.00]; GEN5: median log2 = 5, IQR [4.25, 5.75]), while reasoning-heavy benchmarks peak at substantially higher ranks (e.g., BBH: median log2 = 11.5, IQR [7.25, 12.00])."
        },
        {
            "title": "6 Discussion\nOur experiments demonstrated that the optimal rank of an ex-\npert is task-specific. We show that MoEs with full size ex-\nperts are suboptimal and limited in their scaling by the scarce\naccelerator memory. We extended FlexOlmo to FlexMoRE,\nintroducing low rank experts that enable researchers and prac-\ntitioners to scale and collaborate on expert models without\nhaving to train full-size experts. These contributions come\nwithout compromise to overall model performance and con-\ntribute to the democratization of LLM development.\nTask-dependency of optimal ranks\nIn Section 5.1 we in-\nvestigated the optimal rank for each expert across tasks. We\ndemonstrated that five out of six experts can be post-hoc\nlow-ranked yielding superior downstream task performance.\nThe math expert proves sensitive to lower rank and exhibits\na slight decrease in performance. Our analysis suggests\nthat knowledge-heavy datasets thrive in lower ranks while\nreasoning-based tasks require higher ranks. This is not en-\ntirely surprising as reasoning tasks generally are more com-\nplex and known to require more capacity.\nRank heterogeneity and expert contribution Our expert-\nlevel analysis shows that peak performance occurs at widely\nvarying LoRA ranks across domains. Crucially, high peak\nperformance does not imply high contribution within a mix-\nture. Instead, contribution is governed by the interaction be-\ntween rank efficiency, routing frequency, and task coverage.\nThis explains why experts that dominate individual bench-\nmarks at high ranks do not necessarily improve combined\nmodels, while experts with modest peak scores but early gains\ncontribute disproportionately to overall performance.\nGlobal rank trends and implications\nJointly, these results\nshow that LoRA ranks matter but that their impact is task-\ndependent and complex. While increasing rank can improve\nperformance, gains saturate well before the maximum tested\nrank for most experts and tasks. There is no single optimal\nrank across benchmarks; instead, effective rank allocation de-\npends strongly on task characteristics. In practice, ranks be-\nyond approximately log2 r ≈ 9–10 yield diminishing returns\nfor most experts, suggesting that LoRA ranks should be allo-\ncated selectively rather than scaled uniformly. This point is\nheavily underscored by our results from rank-heterogenous\nmodels, which consistently outperform homogeneous ones\nregardless of the rank of the homogeneous model.",
            "content": "Expert Rank Selection We have studied two possible approaches to determine the optimal rank of each expert in FlexMoRE model. In the first option, we use the MC9 benchmark as proxy. The second option takes the global average performance of an expert across all six benchmarks. Note that in both cases, the selection happens based on the performance of individual experts, not on the performance of the entire mixture. Interestingly, the MC9 rank selection strategy turns out favorably, indicating that multiple-choice tasks may be sufficient proxy for expert performance, while other benchmarks may dilute the selection process as they, for instance, lead to higher than necessary ranks. In the final mixture, high-rank experts may counterbalance low-rank experts. Limitations Our work comes with several limitations. First, we did not apply router-tuning after post-hoc LoRA extraction. The effect of router-tuning was marginal in FlexOlmo, and the clear expectation is that the performance would only be further improved with router tuning. Second, while our architecture allows having multiple full-size experts, transitive chains of low rank experts, and LoRA training from scratch, in this paper, we limited ourselves to post-hoc LoRA extraction from pre-trained FlexOLMo models. This has allowed us to conduct comprehensive set of experiments to determine rank-sensitivity under fixed conditions."
        },
        {
            "title": "References",
            "content": "[Abishek et al., 2025] Alok Abishek, Lisa Erickson, and Tushar Bandopadhyay. Data and ai governance: Promoting equity, ethics, and fairness in large language models. MIT Science Policy Review, 6:139146, August 2025. [Anil et al., 2023] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng arXiv preprint Chen, et al. Palm 2 technical report. arXiv:2305.10403, 2023. [Bisk et al., 2019] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. [Cao et al., 2024] Shiyi Cao, Shu Liu, Tyler Griggs, Peter Schafhalter, Xiaoxuan Liu, Ying Sheng, Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica. Moe-lightning: Highthroughput moe inference on memory-constrained gpus, 2024. [Cao et al., 2025] Jie Cao, Tianwei Lin, Hongyang He, Rolan Yan, Wenqiao Zhang, Juncheng Li, Dongping Zhang, Siliang Tang, and Yueting Zhuang. Moa: Heterogeneous mixture of adapters for parameter-efficient finetuning of large language models, 2025. [Chowdhery et al., 2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling lanJournal of Machine guage modeling with pathways. Learning Research, 24(240):1113, 2023. [Clark et al., 2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. [Clark et al., 2019] Christopher Clark, Kenton Lee, MingWei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. [Dua et al., 2019] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019. [Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. [Hu et al., 2022] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [Hu et al., 2023] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023. [Ji and Song, 2025] Shihao Ji and Zihui Song. L-moe: Endto-end training of lightweight mixture of low-rank adaptation experts, 2025. [Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, 2017. [Kunwar et al., 2025] Pradip Kunwar, Minh N. Vu, Maanak Gupta, Mahmoud Abdelsalam, and Manish Bhattarai. Ttlora moe: Unifying parameter-efficient fine-tuning and sparse mixture-of-experts, 2025. [Kwiatkowski et al., 2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452 466, 2019. [Li et al., 2024] Dengchun Li, Yingzi Ma, Naizheng Wang, Zhengmao Ye, Zhiyuan Cheng, Yinghao Tang, Yan Zhang, Lei Duan, Jie Zuo, Cal Yang, and Mingjie Tang. Mixlora: Enhancing large language models fine-tuning with lorabased mixture of experts, 2024. [Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv preprint et al. Deepseek-v3 technical report. arXiv:2412.19437, 2024. [Mihaylov et al., 2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. [Mu and Lin, 2025] Siyuan Mu and Sen Lin. comprehensive survey of mixture-of-experts: Algorithms, theory, and applications, 2025. [Pahune et al., 2025] Saurabh Zahid Akhtar, Venkatesh Mandapati, and Kamran Siddique. The importance of ai data governance in large language models. Big Data and Cognitive Computing, 9(6), 2025. Pahune, [Rajbhandari et al., 2022] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale, 2022. [Rajpurkar et al., 2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text, 2016. [Yi et al., 2024] Liping Yi, Han Yu, Chao Ren, Heng Zhang, Gang Wang, Xiaoguang Liu, and Xiaoxiao Li. pfedmoe: Data-level personalization with mixture of experts for model-heterogeneous personalized federated learning, 2024. [Zadouri et al., 2023] Ted Zadouri, Ahmet Ustun, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning, 2023. [Zellers et al., 2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. [Zhao et al., 2025] Yushu Zhao, Zheng Wang, and Minjia Zhang. Puzzlemoe: Efficient compression of large mixture-of-experts models via sparse expert merging and bit-packed inference, 2025. [Zhong et al., 2023] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023. [Zhou et al., 2022] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022. [Zhuang et al., 2025] Yuan Zhuang, Yi Shen, Yuexin Bian, Qing Su, Shihao Ji, Yuanyuan Shi, and Fei Miao. Ldmole: Learnable dynamic routing for mixture of lora experts, 2025. [Zou et al., 2025] Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, and Xiangyang Ji. Flylora: Boosting task decoupling and parameter efficiency via implicit rank-wise mixture-of-experts, 2025. [Reddy et al., 2019] Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: conversational question answering challenge, 2019. [Sakaguchi et al., 2019] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [Sap et al., 2019] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions, 2019. [Shi et al., 2025] Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, Daogao Liu, Margaret Li, Dirk Groeneveld, Mike Lewis, Wen tau Yih, Luca Soldaini, Kyle Lo, Noah A. Smith, Luke Zettlemoyer, Pang Wei Koh, Hannaneh Hajishirzi, Ali Farhadi, and Sewon Min. Flexolmo: Open language models for flexible data use, 2025. [Singh et al., 2025] Raul Singh, Nicol`o Brunello, Vincenzo Scotti, and Mark Carman. L1ra: Dynamic rank assignment in lora fine-tuning. In Proceedings of the 8th International Conference on Natural Language and Speech Processing (ICNLSP-2025), pages 360373, 2025. Suzgun, [Suzgun et al., 2022] Mirac Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022."
        },
        {
            "title": "Nathan",
            "content": "[Vasani et al., 2025] Bhoomit Vasani, Jack FitzGerald, Anjie Fang, and Sushmit Vaish. Phlora: data-free post-hoc lowrank adapter extraction from full-rank checkpoint, 2025. [Wang et al., 2022] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 57445760, 2022. [Wang et al., 2024] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max W.F. Ku, Kai Wang, Alex Zhuang, Rongqi Richard Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. ArXiv, abs/2406.01574, 2024. [Wang et al., 2025] Haodong Wang, Qihua Zhou, Zicong Hong, and Song Guo. D2moe: Dual routing and dynamic scheduling for efficient on-device moe-based llm serving. In Proceedings of the 31st Annual International Conference on Mobile Computing and Networking, ACM MOBICOM 25, page 574588. ACM, November 2025. [Xie et al., 2025] Luyuan Xie, Tianyu Luan, Wenyuan Cai, Guochen Yan, Zhaoyu Chen, Nan Xi, Yuejian Fang, Qingni Shen, Zhonghai Wu, and Junsong Yuan. dflmoe: Decentralized federated learning via mixture of experts for medical data analysis, 2025. Top-k Experts Benchmark Performance"
        },
        {
            "title": "Analysis",
            "content": "Figure 4 provides the results for all experts on all benchmarks across all tested ranks. To contextualize expert selection behavior across benchmarks, we report two complementary analyses. First, we list the top-k ranked expert models per benchmark, where each entry corresponds to specific expertrank pair. Second, we report top-k analysis restricted to unique experts, where for each expert the best-performing LoRA rank (rank 211) is selected. In both cases, rankings are computed independently per benchmark, with ties resolved by selecting the lowest LoRA rank. Table 3 reports the top-k expert models per benchmark without enforcing uniqueness across experts. This view highlights which expertrank combinations achieve the highest scores on each benchmark, allowing multiple ranks of the same expert to appear. The analysis reflects peak standalone performance under the rank constraint, independent of mixture composition. Table 4 reports complementary analysis restricted to unique experts. For each benchmark, each expert appears at most once, using its best-performing LoRA rank within the rank constraint. This view isolates expert-level performance independent of rank multiplicity and facilitates comparison across domains. Sensitivity Analysis & Typical Peak Rank B.1 Rank Sensitivity Analysis To characterize how performance varies with LoRA rank, we analyze rank sensitivity using simple linear regression between evaluation score and log2 LoRA rank. For each model family and evaluation group, we fit s(r) = α + β log2 where s(r) denotes the observed score at rank r. The coefficient β provides coarse summary of how performance changes with increasing rank: positive values indicate consistent gains, while values near zero or negative suggest diminishing returns. This analysis is applied to both individual experts and combined mixture-of-experts models to enable comparison across settings. Detailed results are provided in Tables 5, 6, 7 and 8. B.2 Typical Peak Rank Per Task Because linear trends do not capture where performance saturates, we additionally report the rank at which peak For each expert and evalperformance is observed. uation group g, e,g = arg maxr{20,...,214} se,g(r) computed directly from the observed scores without regression or smoothing. In the case of ties, the lowest rank achieving the maximum score is selected. We summarize peak-rank behavior using the median and interquartile range across experts. Results provided in Table 9. the peak rank is defined as Figure 4: Performance across all six benchmarks illustrating expert specialization and rank sensitivity. AGIEval emphasizes knowledgeintensive and academic-style tasks, where the experts like perform strongly. BBH focuses on structured reasoning, favoring the Code and Math experts. GEN5 captures open-ended and generative abilities, where the Creative Writing and Reddit experts are most competitive. MC9 evaluates mixed-task performance and favors rank-efficient generalist experts with broad cross-domain utility. Benchmark Ranking Model LoRA rank MC9 MC9 MC9 MC9 MC9 MC9 MC9 GEN GEN5 GEN5 GEN5 GEN5 GEN5 GEN5 AGIEval AGIEval AGIEval AGIEval AGIEval AGIEval AGIEval BBH BBH BBH BBH BBH BBH BBH MMLU MMLU MMLU MMLU MMLU MMLU MMLU MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 Flex-news-2x7B-1T-r1 Flex-code-2x7B-1T-r64 Flex-creative-2x7B-1T-r128 Flex-news-2x7B-1T-r2 Flex-news-2x7B-1T-r4 Flex-news-2x7B-1T-r8 Flex-pes2o-2x7B-1T-r1024 Flex-pes2o-2x7B-1T-r512 Flex-reddit-2x7B-1T-r32 Flex-pes2o-2x7B-1T-r2048 Flex-pes2o-2x7B-1T-r256 Flex-reddit-2x7B-1T-r8 Flex-math-2x7B-1T-r2048 Flex-news-2x7B-1T-r32 Flex-news-2x7B-1T-r2 Flex-news-2x7B-1T-r1 Flex-news-2x7B-1T-r128 Flex-creative-2x7B-1T-r Flex-math-2x7B-1T-r2048 Flex-math-2x7B-1T-r1024 Flex-math-2x7B-1T-r512 Flex-code-2x7B-1T-r2048 Flex-code-2x7B-1T-r256 Flex-code-2x7B-1T-r1024 Flex-creative-2x7B-1T-r4 Flex-creative-2x7B-1T-r16 Flex-creative-2x7B-1T-r2 Flex-news-2x7B-1T-r2 Flex-creative-2x7B-1T-r1 Flex-creative-2x7B-1T-r32 Flex-news-2x7B-1T-r1 Flex-news-2x7B-1T-r8 Flex-news-2x7B-1T-r64 Flex-news-2x7B-1T-r32 Flex-news-2x7B-1T-r4 Flex-news-2x7B-1T-r16 1 64 128 2 4 8 1024 512 32 2048 256 8 2048 32 2 1 128 2048 1024 512 2048 256 1024 4 16 2 2 1 32 1 8 64 32 4 16 Score 0.6910 0.6889 0.6886 0.6882 0.6878 0.6876 0.5132 0.5126 0.5104 0.5104 0.5092 0. 0.4072 0.4048 0.4034 0.4032 0.4027 0.4020 0.4289 0.4026 0.3839 0.3776 0.3730 0.3728 0.5605 0.5601 0.5601 0.5596 0.5590 0.5589 0.2636 0.2628 0.2624 0.2623 0.2618 0.2612 Table 3: Top-6 ranked expert models per benchmark, restricted to LoRA ranks 211. Ties are resolved by selecting the lowest rank. Expert mapping: news (News), code (Code), creative (Creative Writing), pes2o (Academic), reddit (Reddit) and math (Math) Benchmark Ranking Model LoRA rank MC9 MC9 MC9 MC9 MC9 MC9 MC9 GEN GEN5 GEN5 GEN5 GEN5 GEN5 GEN5 AGIEval AGIEval AGIEval AGIEval AGIEval AGIEval AGIEval BBH BBH BBH BBH BBH BBH BBH MMLU MMLU MMLU MMLU MMLU MMLU MMLU MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro MMLU-Pro 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 Flex-news-2x7B-1T Flex-code-2x7B-1T Flex-creative-2x7B-1T Flex-math-2x7B-1T Flex-reddit-2x7B-1T Flex-pes2o-2x7B-1T Flex-pes2o-2x7B-1T Flex-reddit-2x7B-1T Flex-news-2x7B-1T Flex-creative-2x7B-1T Flex-math-2x7B-1T Flex-code-2x7B-1T Flex-math-2x7B-1T Flex-news-2x7B-1T Flex-creative-2x7B-1T Flex-reddit-2x7B-1T Flex-pes2o-2x7B-1T Flex-code-2x7B-1T Flex-math-2x7B-1T Flex-code-2x7B-1T Flex-pes2o-2x7B-1T Flex-news-2x7B-1T Flex-reddit-2x7B-1T Flex-creative-2x7B-1T Flex-creative-2x7B-1T Flex-news-2x7B-1T Flex-math-2x7B-1T Flex-code-2x7B-1T Flex-pes2o-2x7B-1T Flex-reddit-2x7B-1T Flex-news-2x7B-1T Flex-math-2x7B-1T Flex-creative-2x7B-1T Flex-pes2o-2x7B-1T Flex-code-2x7B-1T Flex-reddit-2x7B-1T 1 64 128 2048 128 8 1024 32 64 16 32 4 2048 32 512 512 1024 2048 2048 2048 64 2048 4 4 2 2048 4 2 1024 1 2048 16 1024 4 256 Score 0.6910 0.6889 0.6886 0.6832 0.6814 0.6803 0.5132 0.5104 0.5078 0.5074 0.5050 0. 0.4072 0.4048 0.4020 0.3983 0.3949 0.3903 0.4289 0.3776 0.3653 0.3646 0.3613 0.3578 0.5605 0.5596 0.5557 0.5516 0.5513 0.5474 0.2636 0.2598 0.2595 0.2523 0.2498 0.2457 Table 4: Top-6 (all) unique experts per benchmark. For each expert, the best-performing LoRA rank (rank 211) is selected, with ties broken by lowest rank. For MC9, all unique experts achieve performance within 1.1 percentage points of the maximum score, despite requiring LoRA ranks that vary by over three orders of magnitude. Expert mapping: news (News), code (Code), creative (Creative Writing), pes2o (Academic), reddit (Reddit) and math (Math)"
        },
        {
            "title": "Model Group median",
            "content": "min max groups FlexMoRE 7x7B-IT FlexMoRE-a2 FlexMoRE-a4 FlexMoRE-a7 0.0003 0.0030 0.0008 -0.0042 -0.0031 -0.0083 0.0104 0.0074 0. Individual Experts 2x7B-IT Flex-math Flex-pes2o Flex-code Flex-reddit Flex-creative Flex-news 0.0018 0.0001 -0.0000 -0.0003 -0.0010 -0.0016 -0.0010 -0.0003 -0.0027 -0.0065 -0.0018 -0.0026 0.0108 0.0013 0.0030 0.0017 0.0002 0."
        },
        {
            "title": "Experts\nAll experts",
            "content": "-0.0002 -0.0018 0.0026 6 6 6 6 6 6 6 6 6 Table 5: Median rank sensitivity (slope per log2 rank) across six benchmarks, computed via linear regression using at least four rank points per task (rank 20 - 214). Positive values indicate that increasing LoRA rank consistently improves performance. Each row summarizes the distribution of rank sensitivity across evaluation groups, where the median reflects the typical effect and the min/max capture task-dependent variability. FlexMoRE models exhibit positive rank sensitivity, with the strongest effect at an intermediate number of active experts (a4), indicating diminishing returns at higher expert counts. In contrast, expert-only models show near-zero or negative rank sensitivity, suggesting limited benefit from increased LoRA rank."
        },
        {
            "title": "Model Group Benchmark",
            "content": "Slope per log2 rank Pearson points AGIEval Experts AGIEval FlexMoRE-a2 AGIEval FlexMoRE-a4 AGIEval FlexMoRE-a7 AGIEval BBH Experts BBH FlexMoRE-a2 BBH FlexMoRE-a4 BBH FlexMoRE-a7 BBH GEN5 Experts GEN5 FlexMoRE-a2 GEN5 FlexMoRE-a4 GEN5 FlexMoRE-a7 GEN5 MC9 Experts MC9 FlexMoRE-a2 MC9 FlexMoRE-a4 MC9 FlexMoRE-a7 MC MMLU MMLU Experts FlexMoRE-a2 MMLU FlexMoRE-a4 MMLU FlexMoRE-a7 MMLU MMLU-Pro Experts MMLU-Pro FlexMoRE-a2 MMLU-Pro FlexMoRE-a4 MMLU-Pro FlexMoRE-a7 MMLU-Pro 0.0001 0.0006 0.0026 0.0004 0.0026 0.0104 0.0074 0.0036 -0.0018 -0.0042 -0.0031 -0.0083 -0.0010 -0.0025 0.0029 0. -0.0005 -0.0001 0.0031 0.0004 0.0000 0.0010 0.0031 0.0012 0.0385 0.3527 0.9105 0.2844 0.4765 0.9433 0.9608 0.9081 -0.4110 -0.8594 -0.8410 -0.7460 -0.3110 -0.5318 0.8473 0. -0.2409 -0.0642 0.8983 0.2032 0.0074 0.6395 0.8800 0.5407 90 15 15 15 90 15 15 15 90 15 15 15 90 15 15 90 15 15 15 90 15 15 15 Table 6: Per evaluation-group rank sensitivity (slope per log2 rank), Pearson r, and number of evaluated ranks. Results are computed for ranks 20 - 214. Rank sensitivity varies substantially across evaluation groups and model families. Reasoning-heavy benchmarks such as BBH exhibit strong and consistent positive rank sensitivity in FlexMoRE models (r 0.9), indicating that increased LoRA rank provides effective additional capacity. In contrast, expert-only models show weak or near-zero sensitivity across groups, suggesting that rank effects are largely noise-dominated without active-expert routing. Several knowledge-oriented benchmarks (e.g., GEN5, MC9) exhibit negative rank sensitivity, consistent with diminishing returns or overfitting at higher ranks."
        },
        {
            "title": "Expert",
            "content": "Slope per log2 rank Pearson points Code Flex-code-2x7B-1T Creative Writing Flex-creative-2x7B-1T Math Flex-math-2x7B-1T News Flex-news-2x7B-1T Academic Flex-pes2o-2x7B-1T Reddit Flex-reddit-2x7B-1T -0.0000 -0.0240 -0. -0.8510 0.0028 0.9550 -0.0013 -0.8590 0. 0.4480 -0.0014 -0.5660 15 15 15 15 15 Table 7: Per-expert rank sensitivity (slope per log2 rank) computed by linear regression between LoRA rank (20214) and the aggregated average evaluation score (Avg mean). Across experts, rank sensitivity exhibits substantial heterogeneity (median 0.0004, 25th/75th percentiles [0.0012, 0.0002], minimum 0.0014, maximum 0.0028). Only the math-specialized expert shows strong and consistent positive rank effect, while most experts exhibit weak or negative rank sensitivity."
        },
        {
            "title": "Benchmark",
            "content": "Slope per log2 rank Pearson points Code Flex-code-2x7B-1T Flex-code-2x7B-1T Flex-code-2x7B-1T Flex-code-2x7B-1T Flex-code-2x7B-1T Flex-code-2x7B-1T MC9 GEN5 AGIEval BBH MMLU MMLU-Pro Creative Writing Flex-creative-2x7B-1T MC9 Flex-creative-2x7B-1T GEN5 Flex-creative-2x7B-1T AGIEval Flex-creative-2x7B-1T BBH Flex-creative-2x7B-1T MMLU Flex-creative-2x7B-1T MMLU-Pro Math Flex-math-2x7B-1T Flex-math-2x7B-1T Flex-math-2x7B-1T Flex-math-2x7B-1T Flex-math-2x7B-1T Flex-math-2x7B-1T News Flex-news-2x7B-1T Flex-news-2x7B-1T Flex-news-2x7B-1T Flex-news-2x7B-1T Flex-news-2x7B-1T Flex-news-2x7B-1T Academic Flex-pes2o-2x7B-1T Flex-pes2o-2x7B-1T Flex-pes2o-2x7B-1T Flex-pes2o-2x7B-1T Flex-pes2o-2x7B-1T Flex-pes2o-2x7B-1T Reddit Flex-reddit-2x7B-1T Flex-reddit-2x7B-1T Flex-reddit-2x7B-1T Flex-reddit-2x7B-1T Flex-reddit-2x7B-1T Flex-reddit-2x7B-1T MC9 GEN5 AGIEval BBH MMLU MMLU-Pro MC9 GEN5 AGIEval BBH MMLU MMLU-Pro MC9 GEN5 AGIEval BBH MMLU MMLU-Pro MC9 GEN5 AGIEval BBH MMLU MMLU-Pro 0.0000 -0.0027 0.0007 0.0030 -0.0011 -0.0001 -0.0011 -0.0004 0.0002 -0.0011 -0.0018 -0.0010 0.0010 -0.0010 0.0024 0.0108 0.0016 0.0021 -0.0023 0.0001 -0.0026 -0.0001 -0.0017 -0.0015 -0.0003 -0.0000 0.0006 0.0013 0.0000 0. -0.0033 -0.0065 -0.0008 0.0017 0.0001 0.0003 0.0260 -0.8400 0.5620 0.9800 -0.9140 -0.1160 -0.7580 -0.7370 0.3230 -0.8230 -0.9000 -0.8840 0.8220 -0.6550 0.9280 0.9230 0.8920 0.9260 -0.9260 0.1730 -0.8600 -0.1040 -0.8990 -0.8960 -0.2790 -0.0580 0.5030 0.8160 0.0300 0. -0.6510 -0.7490 -0.3500 0.8690 0.1190 0.2750 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 Table 8: Per-expert, per-evaluation-Benchmark rank sensitivity (slope per log2 rank) computed via linear regression between LoRA rank (20214) and each evaluation Benchmarks mean score. Slopes quantify the direction and magnitude of rank effects for each experttask combination. Reasoning-oriented benchmarks (e.g., BBH, AGIEval) exhibit consistently positive rank sensitivity for the Math expert, while knowledge-centric benchmarks (e.g., GEN5, MC9) often show weak or negative sensitivity across experts. Across all experttask pairs, the distribution of slopes spans from negative to strongly positive values, with the median near zero and wide minmax range, highlighting strong experttask interactions and non-uniform rank effects. Benchmark Median Q25 Q75 MMLU GEN5 MMLU-Pro MC9 AGIEval BBH Avg. 2.00 5.00 6.00 6.50 9.50 11.50 9.00 1.25 4.25 2.50 3.75 9.00 7.25 6.75 8.00 5.75 9.50 7.00 11.50 12.00 10. 6 6 6 6 6 6 6 Table 9: Typical log2 LoRA rank at which experts achieve peak performance. For each expert and evaluation Benchmark g, the peak rank is defined as e,g = arg maxr{20,...,214} se,g(r), computed directly from the observed scores after sorting by rank and resolving ties by selecting the lowest rank (no regression, smoothing, or normalization). The table reports the median and interquartile range (25th75th percentiles) of log2 e,g across experts. Peak performance typically occurs at moderate ranks: for the aggregated average (Avg), the median peak is at log2 = 9 with IQR [6.75, 10.50] (i.e., ranks 27210). Knowledge-oriented benchmarks peak earlier (e.g., MMLU: median log2 = 2, IQR [1.25, 8.00]; GEN5: median log2 = 5, IQR [4.25, 5.75]), while reasoning-heavy benchmarks peak at substantially higher ranks (e.g., BBH: median log2 = 11.5, IQR [7.25, 12.00])."
        }
    ],
    "affiliations": [
        "Ordbogen A/S",
        "University of Southern Denmark"
    ]
}