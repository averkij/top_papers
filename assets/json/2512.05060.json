{
    "paper_title": "4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer",
    "authors": [
        "Xianfeng Wu",
        "Yajing Bai",
        "Minghan Li",
        "Xianzu Wu",
        "Xueqi Zhao",
        "Zhongyuan Lai",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 6 0 5 0 . 2 1 5 2 : r 4DLANGVGGT: 4D LANGUAGE-VISUAL GEOMETRY GROUNDED TRANSFORMER Xianfeng Wu1,3,4, Yajing Bai1,3, Minghan Li2, Xianzu Wu1,5, Xueqi Zhao1,6, Zhongyuan Lai1, Wenyu Liu3 & Xinggang Wang3 1State Key Laboratory of Precision Blasting, Jianghan University 2Harvard AI and Robotics Lab, Harvard University 3School of EIC, Huazhong University of Science and Technology 4Department of Computing, The Hong Kong Polytechnic University 5Department of Computer Science, Hong Kong Baptist University 6School of Mathematics and Statistics, Hubei University of Education Figure 1: Qualitative comparison of language feature visualizations by our method and 4DLangSplat (Li et al., 2025c). The top part shows semantic visualizations learned by both methods, where our approach not only captures finer details (upper-right zoomed regions) but also demonstrates higher sensitivity to temporal state changes of objects (lower-right highlighted examples). The bottom part illustrates our methods learned semantic features projected onto 3D point clouds, providing more interpretable view of spatiotemporal semantics in dynamic scenes."
        },
        {
            "title": "ABSTRACT",
            "content": "Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction Equal contributions; Corresponding author: xgwang@hust.edu.cn. 1 primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in 4DLangVGGT Repository."
        },
        {
            "title": "1\nScene understanding (Peng et al., 2023) has become a core capability in modern applications such as\nhuman–robot interaction (Fang et al., 2024), AR/VR content creation (Schieber et al., 2025), and in-\ntelligent surveillance (Yuan et al., 2024). While recent advances in 3D visual–language learning (Ma\net al., 2025; Fan et al., 2025) have shown strong performance in static settings, they remain limited\nwhen extended to dynamic 4D scenarios, where both geometry and semantics evolve continuously\nover time. Unlike static environments (Li et al., 2025a;b; Qin et al., 2024), real-world scenes de-\nmand temporal consistency, semantic continuity, and cross-frame alignment to handle open-ended\nand time-sensitive queries. Directly applying 3D methods often leads to semantic drift and unstable\nalignment, highlighting a critical gap that motivates research in robust 4D vision–language mod-\nels (Cai et al., 2025; Ge et al., 2025).",
            "content": "Recent research (Li et al., 2025c) has begun to explore extending scene representations toward language-guided 4D fields. However, most existing approaches remain heavily reliant on Gaussian Splatting pipelines. While Gaussian Splatting has shown promising performance in controlled settings, its fundamental drawback lies in the need for explicit per-scene optimization. This requirement introduces several critical limitations: the computational cost becomes prohibitively high, scalability across diverse videos is severely restricted, and separate models must be maintained for different environments, making large-scale deployment impractical. More importantly, the reliance on per-scene training fundamentally undermines the feasibility of real-time applications, where efficiency and generalization are indispensable requirements. These constraints underscore the pressing need for new solutions that move beyond scene-specific pipelines. To alleviate the scalability issues caused by per-scene optimization, we turn to the paradigm of feedforward 4D geometric reconstruction (Zhuo et al., 2025; Wang et al., 2024; 2025b). Methods such as StreamVGGT (Zhuo et al., 2025) demonstrate strong real-time performance and generalization by enabling efficient reconstruction without scene-specific optimization. However, these approaches focus solely on geometry and motion, lacking semantic or language alignment, and are therefore insufficient for supporting open-vocabulary 4D understanding. This gap highlights the need for next-generation framework that jointly models geometry and semantics within unified architecture. To address these limitations, we propose 4DLangVGGT, Transformer-based feed-forward framework that unifies dynamic geometric reconstruction and visual-language alignment within single architecture. The framework integrates two key components: 4D Visual Geometry Transformer, which captures spatio-temporal geometric representations of dynamic scenes, and Semantic Bridging Decoder (SBD), which maps scene-aware features into language-aligned semantic space to bridge the gap between geometric perception and semantic prediction. Through this design, the model achieves both high structural fidelity and semantic consistency, as shown in Fig. 1, while inheriting the deployment efficiency and strong generalization capabilities of feed-forward approaches. More importantly, to the best of our knowledge, our proposed 4DLangVGGT is the first 2 unified language field model that can be jointly trained across multiple dynamic scenes and directly applied during inference, eliminating the need for costly per-scene optimization and thereby significantly enhancing the practicality of deployment in large-scale, real-world systems. Experiments show that our method not only generalizes well but also achieves state-of-the-art results across multiple benchmarks, yielding up to 2% improvements under per-scene training and around 1% gains under training across scenes. Our main contributions are as follows: We propose 4DLangVGGT, the first Transformer-based feed-forward framework that unifies 4D geometric reconstruction with visual-language alignment in single network. We introduce SBD, which maps dynamic, scene-aware features into language-aligned semantic space, effectively bridging the gap between geometric perception and semantic prediction. Unlike prior scene-specific methods, our model can be jointly trained across multiple dynamic scenes (6 scenes in HyperNeRF and 6 scenes in Neu3D) and directly applied at inference without per-scene optimization, making large-scale real-world deployment feasible."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Static 3D Scene Understanding. Language grounding in 3D has been studied with NeRF-based and Gaussian-based representations. NeRF-based methods such as LERF (Kerr et al., 2023) and OV-NeRF (Liao et al., 2024) enabled open-vocabulary querying but suffered from slow volumetric rendering. To improve efficiency, LangSplat (Qin et al., 2024) adopted 3D Gaussian Splatting (Kerbl et al., 2023) with hierarchical semantics, achieving orders-of-magnitude faster rendering, while extensions like GaussianGrasper (Zheng et al., 2024) demonstrated applications in robotics. Multimodal fusion approaches such as LangSurf (Li et al., 2024) further enhanced cross-modal alignment. Nonetheless, existing methods remain limited to static scenes and do not generalize to dynamic environments. Dynamic 4D Scene Understanding. Bridging natural language and dynamic 4D scene understanding has emerged as critical research direction, with core efforts focused on tight integration of linguistic semantics into time-varying geometric representations. 4DLangSplat (Li et al., 2025c) uses object-wise video captions and status deformable network to supervise 4D Gaussian Splatting field that supports both time-sensitive and time-agnostic open-vocabulary queries; 4-LEGS (Fiebelman et al., 2025) lifts spatio-temporal video features into 4D Gaussian representation to localize text prompts in space and time, allowing interactive video editing. They both depend on Gaussian Splatting, which needs scene-specific optimization. Collectively, these works represent important advances but do not yet satisfy all desiderata of efficient inference, cross-scene generalization, and tightly aligned semantics with evolving geometry. Feed-forward Scene Reconstruction. Feed-forward frameworks provide scalable alternative to NeRFand GS-based reconstruction by leveraging pretrained encoders or end-to-end architectures. Works such as DUST3R (Wang et al., 2024), VGGT (Wang et al., 2025a), and StreamVGGT (Zhuo et al., 2025) enable efficient 3D and 4D reconstruction, while methods like SplatterImage (Szymanowicz et al., 2024), Flash3D (Szymanowicz et al., 2025), and Niagara (Wu et al., 2025) emphasize efficiency and scalability. However, these approaches focus solely on geometric reconstruction, leaving open the challenge of unifying feed-forward reconstruction with language grounding for generalizable 4D semantic understanding."
        },
        {
            "title": "3 PRELIMINARIES: VGGT & STREAMVGGT",
            "content": "Visual Geometry Grounded Transformer (VGGT) (Wang et al., 2025a) is feed-forward Transformer for 3D scene reconstruction that achieves fast and accurate results in single pass. Given one or more scene views, it directly predicts key 3D attributes such as camera parameters, depth maps, point maps, and 3D point tracks. The processing flow of VGGT can be summarized in three stages. First, the image encoder DINO (Caron et al., 2021; Oquab et al., 2023), denoted as E, transforms the input sequence {It}T t=1 into image tokens {Ft}T t=1, with an additional camera token {Ct}T t=1 appended to each image. These tokens are then fed into the Alternating-Attention transformer layers D, which alternate between frame-level and cross-frame self-attention to refine the 3 Figure 2: Overview of 4DLangVGGT. The framework integrates geometry encoder, semantic bridging decoder, and multi-objective training strategy to achieve language-aware 4D fields with geometric fidelity and semantic alignment. representations and produce two outputs: updated camera tokens and geometry tokens {Gt}T t=1. Finally, the multi-head predictor, comprising the camera head Hcam and the DPT (Ranftl et al., 2021) head HDPT, decodes the corresponding tokens to yield camera parameters Oc and dense geometric predictions Og , thereby completing the end-to-end mapping from images to 3D attributes. StreamVGGT extends VGGT to the streaming setting by employing causal temporal attention for sequential inference, where each incoming frame is processed incrementally. During inference, only cache memory of past tokens needs to be maintained: Mt = Mt1 [Ct, Ft], enabling real-time efficiency while preserving temporal consistency. The overall process can be formulated as follows: Ft = E(It); [Ct, Gt] = ( [Ct, Ft] Mt1 ) ; Oc = Hcam(Ct), Og = HDPT(Gt). (1) These definitions and formulations provide the necessary background for introducing our method."
        },
        {
            "title": "4 METHODOLOGY: 4DLANGVGGT",
            "content": "We introduce 4DLangVGGT, unified framework for building language-aware 4D fields that maintain geometric fidelity while ensuring semantic alignment. As illustrated in Figure 2, the framework comprises three main components: (i) StreamVGGT-based geometry encoder that generates spatio-temporal geometric representations (Sec. 4.1), (ii) Semantic Bridging Decoder (SBD) that maps geometry tokens into language-aligned semantic space (Sec. 4.2), and (iii) multi-objective training strategy that jointly optimizes semantic alignment and appearance reconstruction (Sec. 4.3). Together, these components provide robust foundation for 4D perception that is both structurally faithful and semantically interpretable. 4.1 STREAMVGGT-BASED GEOMETRY ENCODER As mentioned in the preliminaries and Eq. (1), the StreamVGGT aggregator alternates between spatial attention and causal temporal attention, producing geometry tokens {Gt}T t=1 that encode both fine-grained 3D geometry structure and temporal dynamics. In our framework, we adopt this architecture but keep it frozen during training. The reason is twofold: (i) StreamVGGT has already been pre-trained on large-scale video data for geometry reconstruction, providing strong spatiotemporal representations that generalize well to diverse scenes; and (ii) freezing this part avoids redundant optimization and reduces computational cost, allowing the training process to focus on semantic alignment rather than relearning geometry from scratch. In our framework, we leverage both geometry tokens and camera tokens. The geometry tokens Gt ensure geometry-centered representations that serve as the foundation for semantic alignment. 4 Meanwhile, the camera tokens Ct are retained mainly for inference. They remain frozen during training, but at inference time they enable the model to exploit camera intrinsics and extrinsics to map features back into the 4D point cloud space, ensuring that semantic information is properly injected and aligned at the point cloud level. The StreamVGGT-based encoder provides strong, geometry-centered foundation for our framework, enabling reliable spatio-temporal representations to support subsequent semantic alignment."
        },
        {
            "title": "4.2 SEMANTIC BRIDGING DECODER (SBD)",
            "content": "While geometry tokens capture the geometry structural and temporal dynamics of scene, they remain agnostic to semantics and cannot directly align with natural language queries. To address this gap, we propose the Semantic Bridging Decoder (SBD), whose goal is to establish robust mapping between geometry representations and language semantics, thereby unifying geometric fidelity and semantic alignment. Geometry-to-Contextual Representation Transformation. The input geometry tokens Gtt = 1T are first processed by contextual-aware Dense Prediction Transformer (DPT) (Ranftl et al., 2021). DPT combines the spatial sensitivity of local convolutional operations with the global modeling capability of Transformers, thereby capturing long-range dependencies across both spatial and temporal dimensions. This new introduced DPT, denoted as Hlang DPT, which employs stacked self-attention layers to transform geometry tokens into contextually enriched feature representations, significantly enhancing their semantic discriminability. Ht = Hlang DPT(Gt), [1, , ], (2) where Ht Rhwc is referred to as the unified 4D feature representation. Here, and denote the spatial resolution of the token map, while is the feature dimension after transformation. Importantly, this module remains trainable during optimization, allowing it to be continuously refined for semantic tasks. Dual-head Semantic and Reconstruction Decoding. Once the contextual-geometry features Ht are obtained, they are passed through two independent prediction heads that project them into complementary semantic and visual subspaces for dual supervision: ˆSt = fLang(Ht), ˆIt = σ(fRGB(Ht)) , where fLang maps the features into d-dimensional semantic embedding space for language alignment, yielding ˆSt Rhwd, which serves as the predicted semantic representation at time t. Meanwhile, fRGB projects the features back into the image space to reconstruct RGB frames, producing ˆIt RHW 3, which represents the reconstructed videos at time and thereby enforces perceptual consistency. Here, and denote the spatial resolution of video frames. [1, , ], (3) 4.3 MULTI-OBJECTIVE TRAINING Semantic Loss. During training, we employ two complementary types of semantic supervision: time-agnostic semantic supervision and time-sensitive semantic supervision. The former provides static, object-level constraints, while the latter captures temporally evolving semantics, and together they enhance the models ability to achieve robust semantic alignment. For each video, we first use Segment Anything Model (SAM) (Kirillov et al., 2023) and DEVA (Cheng et al., 2023) to generate its object-level masks {Mi,t}N,T Time-agnostic semantic supervision. Each mask region is passed through CLIP (Radford et al., 2021a) to obtain its object-specific embedding, which is then assigned to all pixels within the mask region, yielding region-aligned semantic feature map: i,t=1,1, where denotes the object index with objects in total. i,t = fCLIP(It Mi,t), SCLIP eCLIP = (cid:88)N i=1 eCLIP i,t Mi,t, [1, , ] (4) where the CLIP embedding is denoted as eCLIP takes the value 1 if pixel belongs to the object and 0 otherwise. i,t R11d, and the object mask Mi,t Rhw1 Time-sensitive semantic supervision. Using SAM masks across frames, we feed the video-level regions corresponding to each object into multimodal large language model (fMLLM) to generate detailed and temporally consistent descriptions. These descriptions are then encoded by large 5 language model (fLLM) to obtain the corresponding semantic embeddings, which will be assigned to all pixels within the mask, producing dynamic semantic ground truth: {edyn i,t }T t=1 = fLLM (cid:0)fMLLM({It Mi,t}T t=1)(cid:1) , Sdyn = (cid:88)N i=1 edyn i,t Mi,t. (5) Final semantic supervision. The semantic maps ˆSt predicted by the Semantic Head in Eq. (3) are aligned with ground truth St {SCLIP } using combination of L1 regression and cosine similarity. , Sdyn t Llang = (cid:88)T t=1 λ1 ˆSt St1 + λ2 (cid:16) (cid:17) 1 cos( ˆSt, St) , St {SCLIP , Sdyn }. (6) Here, λ1 and λ2 are loss weights. This dual supervision scheme enables the model to learn both static object-level semantics and temporally dynamic semantics, thereby improving alignment in dynamic scenes. Reconstruction Loss. To ensure perceptual fidelity, the reconstructed RGB frames are supervised using hybrid L1L2 objective: (cid:88)T λimg ˆIt It1 + (1 λimg) ˆIt It2 2, Lrgb = (7) t=1 where ˆIt is the frame reconstructed by the RGB Head in Eq. (3), It is the ground-truth input frame. λimg [0, 1] controls the trade-off between the structural accuracy (L1) and the pixel-level smoothness (L2). Final Joint Objective. To jointly preserve semantic alignment and visual fidelity, we employ dual-supervision scheme. The overall training objective is defined as = αLlang + βLrgb, (8) where α, β 0 control their relative contributions."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP Training Data. We conducted training and evaluation on the HyperNeRF (Park et al., 2021) and Neu3D (Li et al., 2022) datasets. We utilized the semantic segmentation annotation dataset for dynamic scenes provided by 4DLangSplat Li et al. (2025c). For feature extraction, the OpenCLIP ViT-B/16 model was used to obtain CLIP features, while the Qwen2.5-VL-7B-Instruct Bai et al. (2025) model was employed to extract dynamic semantics. Following 4DLangSplat, the e5-mistral7b model was applied to process time-varying captions and generate embeddings, and separate autoencoders were trained to compress the CLIP Radford et al. (2021b) features to 3 dimensions and the dynamic semantics features to 6 dimensions. Implementation Details. The aggregator module of StreamVGGT (Zhuo et al., 2025) was used to extract geometric features from input video sequences, with maximum of 128 past frames retained to preserve temporal dependencies while controlling memory usage. Following StreamVGGT (Zhuo et al., 2025) and VGGT (Wang et al., 2025a), input frames were resized to 518 pixels; however, we instead cropped them to the nearest multiple of 14 to better approximate the original resolution. Training employed batch size of 8 and an initial learning rate of 4 105, and all experiments were conducted on four NVIDIA GeForce RTX 3090 GPUs (24 GB). Baselines. Following the evaluation protocol of 4DLangSplat, we benchmark our approach against representative methods under both time-agnostic and time-sensitive query settings. Our primary baselines are LangSplat (Qin et al., 2024), which introduces language-driven Gaussian splatting for static scene understanding, and 4DLangSplat (Li et al., 2025c), which extends this paradigm to dynamic scenes by incorporating temporal modeling. In the time-agnostic setting, we further compare against Feature-3DGS (Zhou et al., 2024), feature distillation framework that compresses high-dimensional representations into compact 3D Gaussians, and Gaussian Grouping (Ye et al., 2024), which leverages semantic segmentation to cluster and render scene elements. For the timesensitive setting, we include the deformable CLIP from 4DLangSplat, which integrates deformable 6 Table 1: Quantitative comparison of time-agnostic language queries on the HyperNeRF dataset. Method Per-scene LangSplat (Qin et al., 2024) Feature-3DGS (Zhou et al., 2024) Gaussian Grouping (Ye et al., 2024) 4DLangSplat (Li et al., 2025c) 4DLangVGGT (Ours) 4DLangVGGT (Ours) americano chick-chicken split-cookie torchocolate Average mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc 72.08 34.65 61.77 83.48 86.45 97.61 62.96 71.31 98.77 98.95 75.98 47.21 34.65 86.50 90.70 97.86 87.22 75.52 98.81 99.03 76.54 47.03 72.71 90.04 90.15 97.32 68.25 96.56 98.67 98. 69.55 24.71 58.95 71.79 72.77 98.09 64.58 85.52 98.10 98.32 73.54 38.40 57.02 82.95 85.02 97.72 70.75 82.22 98.59 98.77 82.46 98. 86.91 98.88 91.44 98.87 75.15 98. 83.99 98.67 Table 2: Quantitative comparison of time-sensitive language queries on the HyperNeRF dataset. Method Per-scene LangSplat (Qin et al., 2024) Deformable CLIP (Li et al., 2025c) Non-Status Field (Li et al., 2025c) 4DLangSplat (Li et al., 2025c) 4DLangVGGT (Ours) 4DLangVGGT (Ours) americano chick-chicken split-cookie espresso Average Acc vIoU Acc vIoU Acc vIoU Acc vIoU Acc vIoU 53.26 23.16 45.19 52.17 39.96 60.57 83.65 94.56 59.59 89.42 66.07 96.73 98.01 66.78 89.96 18.20 42.77 86.28 90.62 93.56 73.58 89.62 91.50 95.28 95. 44.03 33.08 44.85 75.28 78.46 78.60 83.14 81.89 82.44 83.44 16.15 20.86 47.95 49.20 51.56 54.01 22.65 61.80 44.72 87.58 68.57 90.83 72.26 73.06 90.86 90.03 67.77 97. 93.44 95.76 84.02 82.86 52.06 91. 74.74 Gaussian fields with static CLIP embeddings to assess cross-modal alignment, and Non-Status Field, which removes temporal state modeling to isolate its contribution. The definitions of the evaluation metrics, together with additional implementation details and analysis, are provided in the appendix for completeness. 5.2 MAIN RESULTS We evaluate two training regimes to examine both cross-scene applicability and per-scene performance. The first regime trains single model on multiple videos and applies this shared model for inference across different scenes (multi-video single model). The second regime adopts the per-scene protocol used in 4DLangSplat, i.e., training one model per scene. This per-scene setting is included to align with existing Gaussian splatting methods and to provide fair comparison of our methods performance. 5.2.1 HYPERNERF DATASET We evaluate on HyperNeRF under two modes: time-agnostic and time-sensitive language queries, assessing both spatial grounding accuracy and temporal dynamics. Time-Agnostic Language Queries. As shown in Table 1, under the per-scene setting (training one model per scene with the same protocol as 4DLangSplat), training and testing sets are not fully disjoint, and the results reflect performance under the same distribution. In this setting, 4DLangVGGT consistently surpasses all baselines, outperforming 4DLangSplat by 3% mIoU and 0.18% mAcc on average. Under the multi-video single-model setting (a single model across multiple scenes without retraining), our method also outperforms 4DLangSplat, gaining about 1% mIoU and 0.08% mAcc, showing strong cross-scene generalization with shared training weights. Time-Sensitive Language Queries. Table 2 evaluates methods with time-sensitive queries, which require both spatial localization and temporal identification (e.g., glass contains darker brown In the per-scene setting, 4DLangVGGT outperforms all baselines, exceeding liquid in Fig. 3). 4DLangSplat by 0.03% Acc and 0.8% vIoU. Under the multi-video single-model setting, our model further improves temporal accuracy, surpassing per-scene models by 0.58% Acc and 1.68% vIoU. These results demonstrate that 4DLangVGGT more reliably captures object dynamics and semantic state changes, highlighting its strength in languagevision alignment and spatiotemporal consistency for dynamic 4D environments. 5.2.2 NEU3D DATASET On the Neu3D dataset, which mainly consists of long-range videos where object dynamics are not prominent, we focus on time-agnostic language queries. 7 Table 3: Quantitative comparisons of time-agnostic language queries on the Neu3D dataset. Method Per-scene coffee martini cook spinach cut roasted beef Average mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%) Feature-3DGS (Zhou et al., 2024) Gaussian Grouping (Ye et al., 2024) LangSplat (Qin et al., 2024) 4DLangSplat (Li et al., 2025c) 4DLangVGGT (Ours) 4DLangVGGT (Ours) 30.23 71.37 67.97 85.16 87.59 85. 84.74 97.34 98.47 99.23 99.40 99.35 41.50 46.45 78.29 85.09 86.93 85.25 95.59 93.79 98.60 99.38 99.52 99. 31.66 54.70 36.53 85.32 87.72 86.17 91.07 93.25 97.04 99.28 99.32 99.30 34.46 57.51 60.93 85.19 87.41 85. 90.47 94.79 98.04 99.30 99.41 99.37 Figure 3: Qualitative results of time-sensitive language queries between 4DLangSplat and our 4DLangVGGT. Our 4DLangVGGT provides more accurate grounding compared to 4DLangSplat. Time-Agnostic Language Queries. As shown in Table 3, our method (4DLangVGGT) achieves the best overall performance across all evaluated scenes, with an average of 87.41% mIoU and 99.41% mAcc, outperforming all baselines. Notably, compared to the second-best method 4DLangSplat, our approach yields consistent improvements in both mIoU and mAcc, demonstrating stronger spatial semantic grounding under this setting. In addition, we evaluate the more challenging multi-video single-model setting, where single model is jointly trained on multiple scenes and directly applied for inference without per-scene retraining. Even under this stricter condition, our model maintains strong performance, achieving 85.64% mIoU and 99.37% mAcc, which is close to the per-scene results. This highlights the efficiency and cross-scene generalization ability of our framework. 5.2.3 VISUALIZATION Table 4: Ablation study of the RGB Head for reconstruction on Hypernerf dataset. To qualitatively assess the learned 4D semantic fields, we visualize both time-agnostic and timesensitive query results in Fig. 3 and Fig. 4, respectively. For time-agnostic queries, our method produces sharper and more consistent masks than baseline methods, particularly in scenes with complex geometry or occlusions. For timesensitive queries (as shown in Fig. 3), our framework can accurately capture critical semantic transitions, such as the moment when an object changes state or when an action begins (e.g., glasses contain darker brown liquid). In contrast, 4DLangSplat often struggles to detect such fine-grained changes, frequently producing temporally inconsistent masks or missing key state boundaries. These visualizations provide intuitive evidence mIoU(%) mAcc(%) Acc(%) Time-sensitive query Time-agnostic query 98.67 97.68 74.74 70. 91.44 88.52 83.99 78.36 RGB Head vIoU(%) Figure 4: Comparison of time-agnostic query masks. The results demonstrate that our method consistently extracts accurate object masks in both intact and fragmented cookie scenarios, whereas 4DLangSplat exhibits degraded performance when handling fragmented cases. that our method achieves superior semantic alignment with both spatial structures and temporal dynamics. 5.3 ABLATION STUDY Ablation Study on the RGB Head. To investigate the contribution of the RGB reconstruction head in the Semantic Bridging Decoder (SBD), we conducted an ablation experiment in which the RGB Head was removed. The results, summarized in Table 4, demonstrate that removing the RGB Head leads to noticeable drop (around 5% in IoU, 1-2% in Acc) in both time-agnostic and time-sensitive performance . This shows that the auxiliary reconstruction branch is essential for preserving appearance-level cues, which in turn strengthens semantic alignment and yields more accurate grounding. Ablation Study on the Architecture of Heads. We further compare different architectures for the Semantic and RGB Heads in Table 5. The UNet design achieves consistently better results than simple MLP (improving by +0.95% mIoU, +1.16% mAcc, +2.06% Acc, and +2.15% vIoU). the benefit of UNets These gains highlight hierarchical features for capturing fine-grained structures, leading to stronger spatialtemporal grounding than shallow alternatives."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Table 5: Ablation study of the different architectures for the RGB Head and Semantic Head. Architecture of Heads UNet MLP Time-agnostic query Time-sensitive query mIoU(%) mAcc(%) Acc(%) vIoU(%) 83.99 83.04 98.67 97.51 91.44 89.38 74.74 72.59 In this work, we introduced 4DLangVGGT, feed-forward framework that unifies geometry-aware 4D perception with language grounding for dynamic scene understanding. By leveraging the Semantic Bridging Decoder (SBD), the auxiliary RGB head and the joint supervision loss, our method 9 effectively bridges low-level geometric cues and high-level semantic alignment, leading to more faithful and robust predictions. Extensive experiments on HyperNeRF and Neu3D demonstrate that 4DLangVGGT achieves strong performance and generalization without per-scene optimization, outperforming Gaussian-splatting baselines in both scalability and efficiency. These results highlight the potential of our framework as step toward scalable, language-aware 4D semantic fields, paving the way for future extensions to larger-scale datasets and richer multimodal supervision."
        },
        {
            "title": "REFERENCES",
            "content": "Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron: large scale dataset of object-centric videos in the wild with pose annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7822 7831, 2021. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Xudong Cai, Shuo Wang, Peng Wang, Yongcai Wang, Zhaoxin Fan, Wanting Li, Tianbao Zhang, Jianrong Tao, Yeying Jin, and Deying Li. Mem4d: Decoupling static and dynamic memory for dynamic scene reconstruction. arXiv preprint arXiv:2508.07908, 2025. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In ICCV, 2023. Yong Xien Chng, Henry Zheng, Yizeng Han, Xuchong Qiu, and Gao Huang. Mask GroundIn 2024 IEEE/CVF Conference on Computer Viing for Referring Image Segmentation . sion and Pattern Recognition (CVPR), pp. 2656326573, Los Alamitos, CA, USA, June 2024. doi: 10.1109/CVPR52733.2024.02509. URL https://doi. IEEE Computer Society. ieeecomputersociety.org/10.1109/CVPR52733.2024.02509. Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. Irving Fang, Yuzhong Chen, Yifan Wang, Jianghan Zhang, Qiushi Zhang, Jiali Xu, Xibo He, Weibo Gao, Hao Su, Yiming Li, et al. Egopat3dv2: Predicting 3d action target from 2d egocentric vision for human-robot interaction. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 30363043. IEEE, 2024. Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, and Hadar Averbuch-Elor. 4legs: 4d language embedded gaussian splatting. In Computer Graphics Forum, pp. e70085. Wiley Online Library, 2025. Luzhou Ge, Xiangyu Zhu, Zhuo Yang, and Xuesong Li. Dynamicgsg: Dynamic 3d gaussian scene graphs for environment adaptation. arXiv preprint arXiv:2502.15309, 2025. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1972919739, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. 10 Hao Li, Roy Qin, Zhengyu Zou, Diqi He, Bohan Li, Bingquan Dai, Dingewn Zhang, and Junwei Han. Langsurf: Language-embedded surface gaussians for 3d scene understanding. arXiv preprint arXiv:2412.17635, 2024. Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, and Junwei Liang. Seeground: See and ground for zero-shot open-vocabulary 3d visual grounding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 37073717, 2025a. Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 55215531, 2022. Wanhua Li, Yujie Zhao, Minghan Qin, Yang Liu, Yuanhao Cai, Chuang Gan, and Hanspeter Pfister. Langsplatv2: High-dimensional 3d language gaussian splatting with 450+ fps. arXiv preprint arXiv:2507.07136, 2025b. Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, and Hanspeter Pfister. 4d langsplat: 4d language gaussian splatting via multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 22001 22011, 2025c. Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, and Qing Li. Ov-nerf: Open-vocabulary neural radiance fields with vision and language foundation models for 3d semantic understanding. IEEE Transactions on Circuits and Systems for Video Technology, 2024. Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1724917260, 2025. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. ACM Trans. Graph., 40(6), dec 2021. Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 815824, 2023. Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2005120060, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021a. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021b. Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1217912188, 2021. Hannah Schieber, Jacob Young, Tobias Langlotz, Stefanie Zollmann, and Daniel Roth. Semanticscontrolled gaussian splatting for outdoor scene reconstruction and rendering in virtual reality. In 2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR), pp. 318328. IEEE, 2025. 11 Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1020810217, 2024. Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reIn 2025 International Conference on 3D Vision (3DV), pp. construction from single image. 670681. IEEE, 2025. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025a. Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1051010522, 2025b. Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. Xianzu Wu, Zhenxin Ai, Harry Yang, Ser-Nam Lim, Jun Liu, and Huan Wang. Niagara: Normalintegrated geometric affine field for scene reconstruction from single view. arXiv preprint arXiv:2503.12553, 2025. Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In European conference on computer vision, pp. 162179. Springer, 2024. Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Chen Chen, Jian Jin, and Zhenzhen Jiao. Towards surveillance video-and-language understanding: New dataset baselines and challenges. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22052 22061, 2024. Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong, Zengmao Wang, Lina Liu, et al. Gaussiangrasper: 3d language gaussian splatting for open-vocabulary robotic grasping. IEEE Robotics and Automation Letters, 2024. Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2167621685, 2024. Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 12 Figure 5: 4D inference pipeline of 4DLangVGGT. Input video frames are processed by StreamVGGT to obtain geometry tokens. The Semantic Bridging Decoder predicts both RGB reconstructions and semantic embeddings, while the geometry decoder estimates depth maps and camera poses. Inverse-projection lifts them into 3D point cloud, onto which the predicted RGB and semantics are colorized, yielding 3D frames and 3D semantic maps."
        },
        {
            "title": "A MORE EXPERIMENTAL SETTINGS AND DETAILS",
            "content": "A.1 IMPLEMENTATION DETAILS Input Resolution. StreamVGGT requires input resolutions to be multiples of 14. Therefore, we centrally crop all frames to the nearest multiple of 14, ensuring compatibility with the architecture while preserving the main scene content. Semantic Features. We extract feature embeddings from CLIP (512 dimensions) and E5 (4096 dimensions). To reduce dimensionality, two separate autoencoders are trained to compress these embeddings into 3-dimensions and 6-dimensions latent spaces, respectively. Training Hyperparameters. For the semantic loss Llang (Eq. 6), we set λ1 = 0.2 and λ2 = 0.01. For the reconstruction loss Lrgb (Eq. 7), we set λimg = 0.5. We use the AdamW optimizer with an initial learning rate of 4 105, weight decay of 1 104, and gradient clipping at 1.0. warmup strategy of 20 epochs is applied, followed by either constant or cosine decay scheduling. The geometry encoder (StreamVGGT) is kept frozen, while the Semantic Bridging Decoder are trained. Metric. We adopt four standard metrics to evaluate both time-agnostic and time-sensitive querying. For the time-agnostic setting, mean accuracy (mAcc) measures the proportion of correctly predicted pixels, while mean intersection-over-union (mIoU) evaluates the overlap between predicted and ground-truth masks. For the time-sensitive setting, accuracy (Acc) reflects the ratio of correctly identified frames, and video-level IoU (vIoU) assesses spatial alignment within the predicted temporal segments. Together, these metrics provide balanced evaluation of spatial precision and temporal consistency. A.2 INFERENCE IN 4D At inference time, our framework takes sequence of video frames as input and produces both geometry-aware reconstructions and semantic fields. The process is illustrated in Fig. 5. First, the StreamVGGT encoder extracts spatio-temporal geometry tokens that capture the underlying 3D structure and temporal dynamics. These geometry tokens are fed into two parallel branches: 1. Our semantic bridging decoder predicts per-frame semantic embeddings aligned with natural language, while the RGB head reconstructs frames to ensure perceptual fidelity. 2. The depth head of StreamVGGT estimates dense depth maps, and the camera head of StreamVGGT predicts camera poses. Using these outputs, we perform inverse-projection to lift 2D pixels into 3D point cloud. 13 Figure 6: Additional qualitative comparison of language feature visualizations by our method and 4DLangSplat (Li et al., 2025c) like Fig. 1. Finally, the predicted RGB values and semantic embeddings are colorized onto the 3D point map. This produces both 3D frames (geometry with RGB appearance) and 3D semantic maps (geometry with open-vocabulary semantics), enabling unified 4D language field representation."
        },
        {
            "title": "B MORE QUALITATIVE RESULTS",
            "content": "As shown in the visualizations in Fig. 6. The top part shows semantic visualizations learned by both methods, where our approach not only captures finer details (upper-right zoomed regions) but also demonstrates higher sensitivity to temporal state changes of objects (lower-right highlighted examples). The bottom part illustrates our methods learned semantic features projected onto 3D point clouds, providing more interpretable view of spatiotemporal semantics in dynamic scenes."
        },
        {
            "title": "C GENERALIZATION EXPERIMENT",
            "content": "C.1 CROSS-DATASET GENERALIZATION EXPERIMENTS As shown in the visualizations in Fig. 7, our method maintains stable reconstruction quality and produces coherent, artifact-free renderings on unseen datasets such as Objectron (Ahmadyan et al., 2021), even under substantial domain shifts. These results further demonstrate that our approach generalizes well beyond its training distribution. C.2 CROSS-QUERY GENERALIZATION EXPERIMENTS We conducted query-level generalization study, with results provided in Table 6. In this experiment, the original evaluation queries were replaced with semantically similar yet syntactically 14 Figure 7: Additional robustness experiment. We train our method on the HyperNeRF dataset and evaluate it on videos from Objectron datasets to demonstrate cross-dataset generalization and visual robustness. Table 6: Generalization experiments across different queries. We evaluated the performance of different time-sensitive queries on 4DLangSplat and 4DLangVGGT to explore their generalization capabilities across diverse queries. Query americano chick-chicken Raw query Paraphrased query Raw query Paraphrased query 4DLangSplat 4DLangVGGT 66.07 67.77 51.34 64.82 -14.73 -2.95 90.62 93.44 83.26 90.36 -7.36 -3. different expressions to test the models robustness to linguistic variations. The results show that our model remains stable under such query changes and exhibits better cross-query generalization compared to 4DLangSplat. The modified queries used in this experiment are listed below: Query for americano Raw query #1. Glasses contain light-colored liquid. Raw query #2. Glasses contain dark brown liquid. Paraphrased query #1. Glasses are filled with light-colored liquid. Paraphrased query #2. Glasses hold deep brown-colored liquid. Query for chick-chicken Raw query #1. Closed chicken container. Raw query #2. Opened chicken container. Paraphrased query #1. chicken container thats sealed shut. Paraphrased query #2. container of chicken that is open. Table 7: Ablation study of the DPT layer. DPT layer Time-agnostic query Time-sensitive query mIoU(%) mAcc(%) vIoU(%) Acc(%) 80.36 83.99 + 3. 96.49 98.67 + 2.18 72.15 74.74 + 2.59 89.37 91.44 + 2."
        },
        {
            "title": "D ADDITIONAL ABLATION STUDY",
            "content": "As shown in Table 7, introducing the DPT layer yields clear improvements across both time-agnostic and time-sensitive evaluations: +3.63% mIoU and +2.18% mAcc for time-agnostic queries, and +2.59% vIoU and +2.07% Acc for time-sensitive queries. These results demonstrate that DPTs contextual modeling significantly enhances semantic discrimination while improving both spatial and temporal alignment."
        },
        {
            "title": "E LIMITATION AND FUTURE WORKS",
            "content": "While our work introduces 4DLangVGGT, the first unified Transformer-based framework for 4D language fields, there are still several limitations to address. First, our experimental validation is limited to HyperNeRF and Neu3D, which contain only small number of dynamic scenes. Although these benchmarks are standard in prior literature, they do not fully reflect the scale and diversity of real-world environments. Consequently, the generalization of our framework to more complex and large-scale settings remains to be thoroughly explored. In future work, we plan to scale up our approach to substantially larger and more diverse datasets, aiming to rigorously evaluate both efficiency and robustness under real-world conditions. We will explore improving the fine-grained and precision of dynamic semantic supervision, which is inspired by recent Mask Grounding approaches (Chng et al., 2024), which demonstrate the effectiveness of achieving fine-grained alignment between linguistic expressions and localized visual regions. Furthermore, we envision the development of domain-specific large model for 4D language fields, which can serve as foundation model for embodied AI, AR/VR, and open-vocabulary dynamic scene understanding. Such model would unify semantic reasoning and geometric perception at scale, potentially enabling new applications that go beyond current scene-level benchmarks."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Hong Kong Baptist University",
        "Department of Computing, The Hong Kong Polytechnic University",
        "Harvard AI and Robotics Lab, Harvard University",
        "School of EIC, Huazhong University of Science and Technology",
        "School of Mathematics and Statistics, Hubei University of Education",
        "State Key Laboratory of Precision Blasting, Jianghan University"
    ]
}