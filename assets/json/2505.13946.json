{
    "paper_title": "Visual Instruction Bottleneck Tuning",
    "authors": [
        "Changdae Oh",
        "Jiatong Li",
        "Shawn Im",
        "Yixuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the robustness of MLLMs under distribution shifts, from a representation learning perspective. Inspired by the information bottleneck (IB) principle, we derive a variational lower bound of the IB for MLLMs and devise a practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide a theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of three MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLM's robustness under shifts by pursuing the learning of a minimal sufficient representation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 4 9 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Visual Instruction Bottleneck Tuning",
            "content": "Changdae Oh Jiatong Li Shawn Im Yixuan Li Department of Computer Sciences, University of WisconsinMadison {changdae,sharonli}@cs.wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "Despite widespread adoption, multimodal large language models (MLLMs) suffer performance degradation when encountering unfamiliar queries under distribution shifts. Existing methods to improve MLLM generalization typically require either more instruction data or larger advanced model architectures, both of which incur non-trivial human labor or computational costs. In this work, we take an alternative approach to enhance the robustness of MLLMs under distribution shifts, from representation learning perspective. Inspired by information bottleneck (IB) principle, we derive variational lower bound of the IB for MLLMs and devise practical implementation, Visual Instruction Bottleneck Tuning (Vittle). We then provide theoretical justification of Vittle by revealing its connection to an information-theoretic robustness metric of MLLM. Empirical validation of three MLLMs on open-ended and closed-form question answering and object hallucination detection tasks over 45 datasets, including 30 shift scenarios, demonstrates that Vittle consistently improves the MLLMs robustness under shifts by pursuing the learning of minimal sufficient representation."
        },
        {
            "title": "Introduction",
            "content": "In intensive races on the track of frontier-level AI models, we have observed unprecedented achievements through the form of general-purpose chat assistant known as multimodal large language models (MLLMs) [1, 2, 3] that combine visual encoder with large language model. Their universal yet flexible question-answering interface enables MLLMs to easily permeate our lives from general problem-solving [4, 5] to practical applications [6, 7, 8, 9]. While these models may achieve human-like or even surpass human-level performance on certain tasks, critical gap remains in their robustnessparticularly in handling input variations that humans process effortlessly. Human intelligence thrives on the ability to distill large amount of sensory and cognitive inputs into concise abstract representations, process akin to conceptual compression [10, 11]. By prioritizing sparse salient features while discarding redundancy, humans can shape robust prototypical representation of complex data instances that captures proper level of invariance to low-level superficial features for generalization, yet maintains sensitivity to high-level abstract features for discrimination [12, 13, 14]. Unfortunately, there are consistent reports implying that the current MLLMs still lag far behind this desired trade-off between invariance and sensitivity [15, 16, 17, 18]. Specifically, MLLMs fail to produce relevant responses under query distribution shifts. That is, they are vulnerable to processing subtly perturbed samples and long-tail samples [18]. This limitation partially stems from the difficulty of acquiring diverse high-quality multimodal instruction data at scale. When trained via standard maximum likelihood estimation on this relatively limited amount of instruction data, MLLM tends to fit to data-specific patterns and result in brittle solution [19, 20, 21]. To enhance generalization, existing efforts typically fall into two categories (1) datacentric approaches, which collect more instruction data [22, 23, 24] and processes input in finer granularity [25, 26], and (2) model-centric approaches, which scale up the underlying model using Preprint. Under review. Figure 1: Illustration of distribution shifts for an MLLM (a) and performance degeneration and embedding shifts of the MLLM (b). An MLLM (LLaVA-v1.5-7B) receives arbitrary queries that might be visually and/or textually perturbed by unexpected noise. These distribution shifts result in performance drops, as shown in the middle bar plot. visualization of intermediate layer representations of the MLLM on LLaVA-Bench-COCO and its variants indicates that MLLM fails to learn proper level of invariance to generalize multimodal queries in the representation space. more expressive or specialized backbones [27, 28, 29, 30]. However, both data scaling and model scaling are resource-intensiverequiring significant annotation or computational cost. In this work, we propose new approach from representation-centric view to improve the robustness of MLLMs under distribution shifts. Rather than scaling data or model, we introduce lightweight, theoretically grounded module that enhances the internal representations of MLLMs via the information bottleneck (IB) principle. While the IB framework has been explored in small-scale or classification settings [31, 32, 33, 34, 35], integrating it to autoregressive multimodal instruction tuning poses unique challenges due to the complexity of modeling mutual information across highdimensional, sequential, and heterogeneous modalities. We overcome these barriers by formulating novel variational lower bound of the IB objective specifically tailored to the multimodal and sequential nature of MLLMs. We further instantiate this formulation as modular and scalable implementationVisual Instruction Bottleneck Tuning (Vittle), which inserts one simple bottleneck layer within the LLM backbone. Vittle pursues minimal sufficient representations [36] that try to preserve only response-relevant information while discarding non-essential residual features. To our knowledge, this is the first work to investigate the IB framework for end-to-end instruction tuning of multimodal LLMs, offering model-agnostic pathway toward building more robust AI systems. We conduct an extensive evaluation of Vittle across wide spectrum of multimodal benchmarks to assess its robustness and generalization under distribution shift. Our experiments span 30 distribution shifts covering diverse forms of perturbation (in both vision and language) and long-tail distributions. Through these evaluations, we demonstrate that Vittle consistently improves robustness over standard instruction tuning baselines, without sacrificing performance on standard benchmarks and canonical tasks. Notably, we find that the bottlenecked representations induced by Vittle lead to enhanced invariance in the latent space, aligning semantically similar inputs more closelyeven under input shiftswhile reducing overfitting to modality-specific artifacts. We also show that Vittle is compatible with different MLLMs, offering robustness gains while maintaining similar inference-time cost. These results underscore the practical benefit and theoretical promise of information-regularized representation learning for robust multimodal instruction tuning. Contributions: (1) We propose new representation-centric framework for improving the robustness of MLLMs under distribution shifts, grounded in the information bottleneck principle. (2) We explore the IB-based end-to-end learning objective of an MLLM for the first time by inducing new variational lower bound of IB for MLLM and devising practical instantiation, Vittle, supported by theoretical analysis. (3) Through experiments on 30 diverse types of distribution shifts, we thoroughly validate the robustness of MLLMs on open-ended/closed-form QA and object hallucination detection tasks and show advantages of compressive representation induced by pursuing the IB principle."
        },
        {
            "title": "2 Background, Related Work, and Motivation\nMultimodal large language models (MLLMs). Recent advances in MLLMs integrate a pre-trained\nlanguage model with a vision encoder through visual instruction tuning [37, 38]. To be specific,\nlet X = (Xv, Xt) denote a multimodal input query consisting of visual and textual input, e.g.,\nan image and a corresponding instruction or a question given that image, and Y denote a desired",
            "content": "2 response given the input query. An MLLM fθ with parameter θ is trained to produce the desired response given an input query with conditional autoregressive language modeling objective, i.e., arg minθ EX,Y [(cid:80)M m=1 log fθ(YmXv, Xt, Y<m)] for sequence of -length responses, where the visual input Xv go through visual encoder and projector modules to be converted as sequence of tokens that have the same dimension as text embeddings and can be processed by an LLM backbone1. After being trained, these models process wide array of multimodal instructions to solve arbitrary visual question answering tasks [39]. Robustness problem in MLLMs. Despite their impressive performance on standard benchmarks and their growing deployment in real-world applications [7, 40, 41], MLLMs remain vulnerable to input perturbations [42, 43, 44]. For example, MLLMs undergo systematic performance drop [18] when they encounter samples of superficial perturbations (e.g., varying brightness of image and typo in text) illustrated in Figure 1 (a). As shown in the bar plot of Figure 1 (b), LLaVA-v1.5-7B model undergoes severe performance degradation on LLaVA-Bench-COCO (LB-COCO; [37]) under the perturbations from visual input, textual input, and their joint (V, T, and Pert.). We posit that these vulnerabilities arise from the way MLLMs structure their internal representation space. In particular, inputs affected by perturbations are often embedded far from their intact (clean) counterparts, reflecting distribution shift in the representation space that leads to poor generalization from an information-theoretic perspective [18]. The right side of Figure 1 (b) illustrates this phenomenon: using LLaVA-v1.5, we visualize representations of LB-COCO alongside its challenging variant, where the image and text inputs are perturbed. In this setting, semantically equivalent examples are mapped to distinct and distant regions in the latent space, suggesting lack of invariance to superficial input variations, which is crucial for robustness to distribution shifts. Motivated by this, our work aims to enhance the robustness of MLLMs by explicitly regularizing their internal representations, encouraging them to retain task-relevant information while discarding input-specific noisethereby finding good balance between invariance to low-level superficial features and sensitivity to high-level abstract features for better generalization. Information bottleneck principle. The information bottleneck framework provides principled approach to measure the quality of representations that are maximally predictive of target variable while compressing redundant information from an input variable [45, 46]. Numerous works have explored the use of IB training objective [31], across computer vision [47, 48], natural language processing [34, 35], graph learning [33, 49], and time-series modeling [50]. These efforts are supported by theoretical insights suggesting that optimizing for the IB objective can reduce generalization error [32, 51]. However, most prior work focused on classification settings [34, 35] and/or relatively small-scale models [31, 52, 34]. Although recent study explored IB for MLLMs [53], the authors adopted IB training on lightweight projector module while keeping the LLM backbone frozen. In contrast, our work is the first to investigate the IB framework for end-to-end training of large-scale autoregressive multimodal language models. Beyond shallow adaptations, we directly modify the internal structure of the LLM to promote IB-consistent behavior throughout the training process. We focus specifically on instruction tuning for MLLMswhich have become increasingly central to modern AI ecosystems but remain largely unexplored from the perspective of IB-based learning."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary: Information Bottleneck As Learning Objective Let be multimodal input query (e.g., image-text pair), the desired output, and = (X) an intermediate representation extracted by the MLLM encoder (). The Information Bottleneck principle aims to learn representations that are maximally informative about the output while being minimally informative about the input X. Formally, this is expressed as the optimization objective: max IBf (X, ) := I(Z, ) (cid:124) (cid:123)(cid:122) (cid:125) acquiring information from desired output β I(Z, X) (cid:124) (cid:123)(cid:122) (cid:125) compressing input-specific redundant information , (1) where I(, ) denotes mutual information and β is the trade-off coefficient. Minimizing I(Z, X) encourages removing redundant or input-specific variations, while maximizing I(Z, ) ensures that the representation retains task-relevant signals necessary to predict the desired output. 1For simplicity, we will omit the visual encoder and projector in our learning objective at following sections. In other words, the IB objective promotes representations that discard non-essential features tied to the input modality, while preserving those critical for solving the task. This property is particularly desirable for robust instruction tuning, where diverse multimodal inputs must be mapped to consistent, meaningful outputs under varied conditions (e.g., visual and textual perturbations). Despite its appeal, integrating the IB objective into MLLM training is highly non-trivial due to the intractability of mutual information estimation and the complexity of autoregressive and multimodal architectures. 3.2 Variational Inference for Information Bottleneck in MLLMs Directly optimizing the IB objective is generally intractable, as it involves mutual information terms over unknown data distributions. In this work, we introduce tractable variational bound on the IB objective, specifically tailored to the autoregressive and multimodal structure of MLLMs. We outline the key steps below and provide full derivations in the Appendix C. We begin with the mutual information term I(Z, X). Given the sequential nature of MLLMs, we decompose both the input = (Xv, Xt) and the latent representation = (Zv, Zt) into visual and textual components. We can then derive the following upper bound for I(Z, X): I(Z, X) = Ex,z[log p(zx) p(z) ] Ex,z[log = Exv ,xt [Eztxv ,xt [Ezv xv [log p(zx) r(z) p(zvxv) r(zv) ] = Exv ,xt,zv ,zt [log p(ztxv, xt)p(zvxv) r(zv)r(zt) ] ]]] + Exv ,xt [Ezv xv [Eztxv ,xt [log p(ztxv, xt) r(zt) ]]] = Exv [DKL(p(zvxv)r(zv))] + Exv ,xt [DKL(p(ztxv, xt)r(zt))], (2) where the first inequality holds given the non-negativity of Kullback-Leibler divergence (KLD), DKL(r(z)p(z)), and p(zvxv, xt) = p(zvxv) due to causal attention in MLLM. We introduce r(z) = r(zv, zt) = r(zv)r(zt) as factorizable variational approximation of the true prior p(z). Next, for the output-relevant term I(Z, ), we have the lower bound: I(Z, ) = Ey,z (cid:20) log (cid:21) p(yz) p(y) Ex,y,z [log q(yz)] Ey[log p(y)] Ex,y (cid:2)Ezx [log q(yz)](cid:3) , (3) where we replace the true posterior p(yz) with variational approximation q(yz) that will be parameterized by model (will be elucidated in Section 3.3). Finally, combining the lower bound of I(Z, ) and the upper bound of I(Z, X) yields variational lower bound for the IB objective as follows, (cid:2)Ezx[log q(yz)](cid:3) IB(X, ) Ex,y β (Exv [DKL(p(zvxv)r(zv))] + Exv ,xt [DKL(p(ztxv, xt)r(zt))]), (4) In the next section, we elaborate on how we can implement this variational lower bound for MLLM instruction tuning in practice. 3.3 Vittle: Practical Implementation of Visual Instruction Bottleneck Tuning By using Monte Carlo approximation of expectations over data, Eq. (4) can be expressed as follows, Lβ = 1 N (cid:88) i=1 zxi [log q(yiz)] β (DKL(p(zvxi v)r(zv)) + DKL(p(ztxi v, xi t)r(zt))). (5) To compute this empirical estimate of the IB lower bound, we need to model the posterior distributions, p(zvxv) and p(ztxv, xt), and prior distributions r(zv) and r(zt), of the MLLMs inner representation Z. While in principle these distributions can take arbitrary forms, multivariate Gaussian distributions have been widely adopted in variational inference and probabilistic embedding literature [54, 55, 56, 31, 57, 58] due to their mathematical tractability and empirical effectiveness. By following this common standard, we set the posteriors and priors as Gaussian with diagonal covariance for d-dimensional variable, and will elucidate how exactly they are defined below. Posterior distributions. As illustrated in Figure 2, we parameterize the posteriors p(zvxv) and p(ztxv, xt) using simple MLP blocks. Specifically, we introduce two non-linear projections, : Rd R2d, which map each d-dimensional token embedding to the posterior Gausgϕv , gϕt sian parameter vectors µ Rd and σ2 Rd + for the vision and language modalities, respectively. Given an intermediate l-th layer representation (zv, zt) = fθl (xv, xt), we define: I), p(ztxv, xt) = (zt; µt, σ2 p(zvxv) = (zv; µv, σ I), 4 Figure 2: Vittle architecture. We insert learnable bottleneck layer gϕ = {gϕv , gϕt} on top of blocks of LLM backbone (i.e., LLM-stem fθl ) to estimate posterior distributions of token embeddings. After obtaining sample per token {zv, zt} from posteriors, we interpolate it with pre-bottlenecked token representation {zv, zt} and pass it through the remaining LLM blocks (i.e., LLM-head fθl+). v] = gϕv (fθl (xv)) and [µt, σ2 where [µv, σ2 ] = gϕt(fθl (xv, xt)), with the mean and variance parameters split along output dimensions of MLP. These MLPs are applied position-wise in the same manner as Transformers feed-forward layers [59], producing token-wise variational posteriors. Now, we can sample from the posterior distributions of MLLM representation by zv p(zvxv) and zt p(ztxv, xt). Then, to strike balance between invariance and sensitivity, we interpolate the original representation (pre-bottleneck) with its bottlenecked counterpart as ˆz = (1 α)z + αz. These representations are fed into the remaining layers to compute the predictive distribution over outputs, i.e., q(yz) := fθl+(yˆzv, ˆzt). While direct sampling introduces non-differentiability, we can enable the gradient flow using the reparameterization trick [55] to sample via = µ + σ ϵ with ϵ (0, I) where µ and σ are the outputs of the bottleneck MLP module given input x. Prior distributions. We consider two instantiations of the prior distribution for both Zv and Zt: (1) fixed standard Gaussian (0, I), which is input-independent and enforces strong isotropy, and (2) learnable Gaussian (µψ, σ2 ψ are two learnable vectors shared across samples. Each prior affects the formation of representations differentlythe fixed prior imposes stronger regularization and robustness, while the learnable prior introduces additional flexibility by allowing the model to adapt to the instruction tuning distribution. We name the former Vittle (F) and the latter Vittle (L), and validate them altogether for all the evaluations in Section 4. ψ I), where µψ and σ2 Overall objective and implementation. The first term of Lβ(Eq. (5)) can be easily computed through the standard cross-entropy, and our Gaussian instantiation of posterior and priors allows us to derive closed-form expressions of KLD terms that can be computed from simple arithmetic between µ and σ2 parameters (See Appendix A.2). We set β = 0.1 where is the hidden dimension of the MLLM, to normalize the KL regularization terms relative to the size of the latent dimension. The interpolation coefficient α in ˆz = (1 α)z + αz increases progressively following cosine schedule up to 0.5. During inference, we consistently use an averaged representation ˆz = (z + z)/2. The target layer to apply the bottleneck module can differ between visual and textual tokens, but we set = 24 for both modalities among 32 layers in 7B-size LLM, i.e., top 25% layer, by default for simplicity (See Appendix B.1 for the ablation study). Figure 2 depicts the architecture overview. 3.4 Theoretical Justification The learning objective of Vittle has an attractive theoretical interpretation that can support the improvement in robustness of Vittle. In this section, we first introduce recently proposed information-theoretic measure of MLLMs robustness under distribution shifts, effective mutual information difference (EMID [18]), and show how Vittle can contribute to improving EMID. Definition 3.1 (EMID). Let PΘ : be an MLLM with parameters Θ that produces an output response YΘ given an input instruction X. For joint distributions PXY and QXY , effective mutual information difference of PΘ over and is defined as below, EMID(PXY , QXY ; PΘ) := [I(PXYΘ) I(PXY )] [I(QXYΘ) I(QXY )]. (6) 5 where I() denotes mutual information that measures the relevance between input instruction and response. higher value of EMID indicates that MLLM PΘ undergoes performance degeneration in the distribution (test data) compared to (training data), so we want to achieve lower value of it to ensure robustness. We now derive an upper bound for EMID (See Appendix for the proof). Proposition 3.2 (EMID upper bound). Let PΘ be an MLLM that maps = {Xv, Xt} to = {Zv, Zt}, and then sequentially maps to YΘ. Given joint distributions PXY = PX PY and QXY = QX QY (resp. PZY and QZY ), by assuming consistent conditionals over ZvZt, ZtZv, and between and Q, we have an upper bound for EMID(PXY , QXY ; PΘ) as below, ˆH(cid:0)D 1 2 JS(PZv QZv ) + 1 JS(PZt QZt ) + (cid:112)XZ (cid:1) + H(PYΘ ) H(PY ) + H(QYΘ ) H(QY ), (7) 1 2 where and JS indicate the entropy and square root of Jensen-Shannon divergence (JSD), respectively, XZ := EzP [DKL(PXzMXz)] + EzQ[DKL(QXzMXz)] with mixture distribution , and ˆH := maxxX [H(QY x) + H(PYΘ)]. As we consider an optimization problem of = +Q Θ, the terms, H(PY ), H(QY ), and max H(QY ), can be ignored from Eq. 7. We can also ignore (cid:112)XZ term because it cannot directly affect YΘ given the Markov assumption YΘ. Implication. Vittle maximizes the variational lower bound of IB, which consists of (1) minimizing standard negative log-likelihood term representing an expected risk, and (2) minimizing KLD terms to enforce posterior distributions close to prior distributions. By pursuing (1), MLLM PΘ seeks solution Θ that minimizes the expected risk and reduces its output entropy H(PYΘ) and H(QYΘ) [60, 61, 62]. Besides, it also reduces JSD between representation distributions PZ and QZ by promoting all posterior samples to be laid near the pre-defined priors. In summary, reduced entropy and JSD terms induce lower EMID, which means that Vittle tries to achieve minimal difference between effective mutual information over training and evaluation distributions, while adapting to the in-distribution training set. We show that Vittle indeed reduces JSD and EMID under distribution shifts in Table 4, and demonstrate in Section 4.2 that Vittles nice theoretical property is translated into consistent robustness gains under 30 distribution shift scenarios while maintaining in-distribution task performance."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Setup Model and implementation detail. We adopt LLaVA-v1.5 [63] as our main baseline MLLM, where we set CLIP ViT-L/14-336px [64] as vision encoder, Vicuna-v1.5-7B [65] as an LLM, and two-layer MLP as projector. We follow the standard two-stage training of LLaVA [37], and replicate stage-1 for image-text alignment with the same configuration and dataset (LLaVA-pretrain-558k) of LLaVA-v1.5 [63]. Then, on the LLaVA-mix-665k, we apply our Vittle objective. To validate the scalability and broad applicability, we also consider LLaVA-v1.5-13B and Prism-7B [66]. Refer to Appendix for details and Appendix for LLaVA-v1.5-13B and Prism-7B results, respectively. Task, metric, and datasets. We evaluate instruction-tuned MLLMs with three representative tasks: (1) open-ended question answering, (2) object hallucination detection, and (3) closed-form question answering. All are formatted as question answering (QA) with single image input, where we use the average relative preference score measured by GPT-4o LLM judge [67] with three repeated runs for open-ended QA, while using exact matching accuracy for hallucination detection and closed-form QA. For open-ended QA tasks, we adopt four datasets: LB-COCO [37] as clean and typical dataset, and LLaVA-Bench in-the-wild (LB-Wild), LLaVA-Bench-Wilder (LB-Wilder), and WildVisionBench (WV-Bench) as long-tail datasets. Then, we apply 27 types of image and text perturbations on LB-COCO samples2 to yield 28 variants of perturbed LB-COCO (one of clean and nine of visual, textual, and joint perturbations, respectively). For object hallucination detection tasks, we adopt POPE [68] as clean and typical dataset. Then, we generate nine variants of perturbed POPE with visual perturbations. Here, we consider the LB-COCO and POPE as in-distribution (ID) datasets because they are generated from MS-COCO samples that construct majorities of the instruction tuning set of modern MLLMs, including LLaVA. For closed-form QA, we adopt four representative datasets: ScienceQA [69], MMMU [70], MME [4], and MMStar [71]. In summary, we experiment with 45 datasets (31 of open-ended, 10 of object hallucination detection, and 4 of closed-form tasks). 2See Appendix A.3 for comprehensive summary of all perturbations and their generation processes. 6 Figure 3: Object hallucination detection performance on POPE variants. We enumerate the hallucination detection accuracy of each method on nine versions of perturbed samples, and observe consistent gains by Vittle (highlighted by green numbers of relative improvement from baseline). Figure 4: Open-ended QA performance on LB-COCO variants. We enumerate the relative preference score of responses from each model on 18 version of perturbed samples, and observe consistent gains by Vittle (especially for the Vittle (F)) on most of the textual (top), and joint (bottom) perturbations (results on visual perturbations are deferred to Appendix B). 4.2 Results Vittle improves robustness under input perturbations. We first evaluate Vittle on object hallucination detection tasks with nine variants of POPE perturbed by visual corruptions in Figure 3. Although MLLMs trained with standard objective and Vittle similarly suffer from perturbations, two instantiations of Vittle consistently outperform the standard objective. Interestingly, Vittle outperforms the baseline even in clean POPE (See Appendix B). We speculate that Vittles information control prevents the reliance on partial feature of single modality [72], e.g., textual feature, which is common source of hallucination. Next, we present the validation on the open-ended QA task with 18 types of input perturbations, which are applied to visual and textual input independently or simultaneously in Figure 4. As we can see, Vittle greatly enhances performance in various perturbation datasets highlighted by green numbers that indicate the relative improvements of Vittle (F) compared to the baseline. Among the two variants of Vittle, Vittle (F) showcases better generalization under perturbations than Vittle (L), suggesting the benefits of conservative zero-centered isotropic prior distribution to address variety of subtle input perturbations. Next, we further explore Vittles robustness by evaluating varying perturbation severity. To be specific, we generate perturbations on three different degrees that determine how significantly the image or text would be changed. In Figure 5, we see that Vittle achieves better performance in general, where the margin becomes larger under severe perturbations. In summary, we observe consistent gain by Vittle on the perturbed input setting across two tasks, which indicates that Vittle enhances the robustness to distribution shifts by pursuing the minimality of data representation. Figure 5: Evaluation under varying perturbation severity. Vittle achieves better performance, especially on severe perturbations. 7 Vittle improves generalization to long-tail distributions. Not only subtle perturbations on input, but long-tail samples are also commonly encountered in many MLLM applications. In Table 1, we validate Vittle on three long-tail QA tasks constructed with real-world user queries. We see that Vittle also excels in generalizing long-tailed samples compared to the baseline. Interestingly, Vittle (L)learnable priorexhibits better performance compared with Vittle (F). We speculate that learnable prior IB guides the model to learn better sensitivity for high-level abstractions as well as an invariance to low-level noise by allowing additional flexibility to shape data-driven priors, yielding superior performance on tasks that require in-depth understanding of irregular queries. Table 1: Performance comparison on long-tail open-ended QA tasks those contain queries that are quite different from typical training samples in terms of visual content and textual semantics. Table 2: Performance comparison on general benchmark datasets. These four multi-choice QA datasets require higher level of multimodal understanding across multiple domains. Method Baseline Vittle (L) Vittle (F) LB-Wild LB-Wilder WV-Bench 51.6 54.6 52.2 156.9 168.8 166.1 60. 60.4 59.7 Method Baseline Vittle (L) Vittle (F) SciQA MMMU MME MMStar Avg. 64. 64.7 65.4 35.6 35.3 34.5 69.7 70.5 70.1 33. 33.7 33.5 50.9 51.1 50.9 Vittle preserves competitive performance on general benchmarks. Although the main focus of Vittle is to improve the models robustness under distribution shifts, securing the rich multimodal understanding capability and knowledge to diverse disciplines is also crucial as an essence of MLLM. To validate the multimodal understanding with an advanced level of knowledge, we evaluate each method on four representative closed-form QA benchmark datasets covering various fields. In Table 2, we observe that Vittle shows competitive performance with the standard approach, which implies that Vittle can also be used as general-purpose learning objective. Table 3: Comparison with weight-space compression methods. We compare Vittle with the LoRA and weight decay (WD) methods on LB-COCO and its perturbed variants. Method Baseline LoRA WD Vittle (L) Vittle (F) Clean Pert. Pert. Pert. 77. 73.4 74.1 76.7 76.1 73.4 70.4 72.1 73.9 74.2 72.2 62.7 73.0 73.0 74.1 62. 39.7 59.5 62.7 64.4 Figure 6: Comparison with other objective functions. We report the average performance for all perturbations in POPE and LB-COCO. Comparison with alternative learning approaches. Note that the regularization forced by Vittle works on the representation space to penalize the amount of information encoded in the data representations. One of the natural alternatives is to regularize the model weight directly. In Table 3, we compare Vittle with LoRA [73] and the weight decay method (WD) as instantiations of weight-space regularization, and the results suggest that explicit regularization on weight-space does not ensure good balance between adaptability on in-distribution and robustness to distribution shifts. The other line of alternatives is information maximization during visual instruction tuning [74, 75], which is the exact opposite of Vittles design principle. We compare two recent methods of this, ROSS [74] and LIT [75], with Vittle on LB-COCO and POPE under perturbations. As shown in Figure 6, although these approaches are effective in improving object hallucination detection performances, they fail to achieve competitive performance on the open-ended QA task (See Appendix for more results). This implies the non-trivial challenge of devising general learning objective for MLLMs that can consistently improve robustness across diverse tasks, where we can see the promise of Vittle towards broadly applicable robust instruction tuning. Qualitative analysis. Figure 7 shows responses of clean queries and their visually or textually perturbed counterparts. Although the query before and after each perturbation conveys the same meaning and intention, LLaVA-v1.5 reveals volatility in its responses, whereas Vittle shows stable behavior by providing consistent responses, i.e., generating exactly the same response in the case of visual perturbation while keep focusing on the same object in case of textual perturbation. 8 Figure 7: Case study on LB-COCO under perturbations. Although LLaVA-v1.5 produces reasonable response for clean samples, the response and its quality vary under perturbations. Meanwhile, Vittle maintains the consistency for the responses. Representation analysis. We next see how Vittle shapes the representation space and how it affects robustness. In Table 4, we measure the average value of empirical JSD and EMID discussed in Section 3.4 over 27 perturbed variants of LB-COCO. Both JSD and EMID are computed between two distributions, clean and one of its perturbed versions, and then averaged over 27 clean-perturbed pairs (See Appendix A.4 for details). As our hypothesis, Vittle reduces distributional gaps, e.g., achieving smaller JSDs, between clean and perturbed samples in its representation space, thereby achieving smaller EMID value that indicates better robustness. In Figure 8 (top), we further show PCA visualizations (in the same axis scale) for representations of LLaVA and Vittle on clean and image-text perturbed LB-COCO. We see that Vittle embeds the clean and semantically equivalent perturbed samples more closely. Moreover, the bottom panel shows that Vittle induces smaller cosine distances between clean and perturbed pairs in terms of the histogram and the average value in parentheses. These results indicate that our learning objective is indeed effective in structuring better representation space that drives robustness. Table 4: JSD and EMID evaluation on 27 LB-COCO variants. Method Baseline Vittle (L) Vittle (F) JSD () EMID () 0.068 0.048 0.047 0.026 0.021 0.025 Figure 8: PCA and pair-wise cosine distance of representations. Cost analysis. Vittle introduces lightweight bottleneck layer inside of LLM that slightly increases the total number of trainable parameters (by 1.5%). One may thus wonder how Vittles training and inference time is compared with bottleneck-free baseline. In Table 5, we show the wall-clock training (per iter and total), and inference time per sample. Although Vittle increases the training time up to 20% compared with baseline, its inference time is almost identical to the original model, which is reasonable amount of cost overhead given significant gains in terms of robustness. Table 5: Runtime comparison. Method Tr./it.(s) Baseline Vittle Te.(s) 0.1048 0. Tr.(h) 11.06 13.36 7.363 9."
        },
        {
            "title": "5 Conclusion",
            "content": "This work provided the first investigation on the promise of information bottleneck from the context of MLLM instruction tuning to ensure robustness of MLLM under distribution shifts. We proposed new theoretically-grounded visual instruction tuning method, Vittle, that injects bottleneck layer inside the LLM to induce posterior samples of internal representations that encode useful information to produce valid responses while discarding other residual information from input queries. With negligible additional cost, Vittle is easily optimized with variational lower bound of IB and shows consistent gains in robustness in 30 types of distribution shifts while also achieving strong performance on standard benchmarks, indicating that Vittle promotes good balance between invariance and sensitivity during representation learning."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "The authors thank Seongheon Park and Sean Xuefeng Du for their valuable suggestions and discussions that shaped the draft. Research is supported in part by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, Schmidt Sciences Foundation, and Alfred P. Sloan Fellowship."
        },
        {
            "title": "References",
            "content": "[1] xAI. Grok 3 Beta The Age of Reasoning Agents. https://x.ai/news/grok-3, February 2025. [2] OpenAI. OpenAI o3 and o4-mini System Card. https://openai.com/index/ o3-o4-mini-system-card/, April 2025. [3] Google Cloud. Gemini 2.5 Pro. https://cloud.google.com/vertex-ai/generative-ai/docs/ models/gemini/2-5-pro?hl=ko, May 2025. [4] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. [5] Hao Yang, Yanyan Zhao, Yang Wu, Shilong Wang, Tian Zheng, Hongbo Zhang, Zongyang Ma, Wanxiang Che, and Bing Qin. Large language models meet text-centric multimodal sentiment analysis: survey. arXiv preprint arXiv:2406.08068, 2024. [6] Rawan AlSaad, Alaa Abd-Alrazaq, Sabri Boughorbel, Arfan Ahmed, Max-Antoine Renault, Rafat Damseh, and Javaid Sheikh. Multimodal large language models in health care: Applications, challenges, and future outlook. J. Med. Internet Res., 26:e59505, 2024. [7] Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Zhengliang Liu, Zihao Wu, Peng Shu, Jie Tian, Tianze Yang, Shaochen Xu, et al. Large language models for manufacturing. arXiv preprint arXiv:2410.21418, 2024. [8] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1359013618, 2024. [9] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng Cao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo Cao, Ziran Wang, and Chao Zheng. survey on multimodal large In Proceedings of the IEEE/CVF Winter Conference on language models for autonomous driving. Applications of Computer Vision (WACV) Workshops, pages 958979, January 2024. [10] Mark Turner. The art of compression. The artful mind: Cognitive science and the riddle of human creativity, pages 93114, 2006. [11] Eddie Gray and David Tall. Abstraction as natural process of mental compression. Mathematics Education Research Journal, 19(2):2340, 2007. [12] George Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81, 1956. [13] Eleanor Rosch. Cognitive representations of semantic categories. Journal of experimental psychology: General, 104(3):192, 1975. [14] Li Zhaoping. Vision: looking and seeing through our brains information bottleneck. arXiv preprint arXiv:2503.18804, 2025. [15] Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui. On the out-of-distribution generalization of multimodal large language models. arXiv preprint arXiv:2402.06599, 2024. [16] Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, and Kun Zhang. How well does gpt-4v (ision) adapt to distribution shifts? preliminary investigation. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. 10 [17] Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, and Tae-Hyun Oh. Beaf: Observing before-after changes to evaluate hallucination in vision-language models. In European Conference on Computer Vision, pages 232248. Springer, 2025. [18] Changdae Oh, Zhen Fang, Shawn Im, Xuefeng Du, and Yixuan Li. Understanding multimodal llms under distribution shifts: An information-theoretic approach. In International Conference on Machine Learning, 2025. [19] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. [20] Wenqian Ye, Guangtao Zheng, Yunsheng Ma, Xu Cao, Bolin Lai, James Matthew Rehg, and Aidong Zhang. MM-spubench: Towards better understanding of spurious biases in multimodal LLMs. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. [21] Yiming Liang, Tianyu Zheng, Xinrun Du, Ge Zhang, Jiaheng Liu, Xingwei Qu, Wenqiang Zu, Xingrun Xing, Chujie Zheng, Lei Ma, et al. Aligning instruction tuning with pre-training. arXiv preprint arXiv:2501.09368, 2025. [22] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087, 2023. [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [24] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. [25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [26] Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, et al. Long-vita: Scaling large multi-modal models to 1 million tokens with leading short-context accuray. arXiv preprint arXiv:2502.05177, 2025. [27] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [28] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [29] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, and Guilin Liu. Eagle: Exploring the design space for multimodal LLMs with mixture of encoders. In The Thirteenth International Conference on Learning Representations, 2025. [30] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [31] Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. In International Conference on Learning Representations, 2017. [32] Matias Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of the information bottleneck in representation learning. In 2018 IEEE International Symposium on Information Theory (ISIT), pages 15801584, 2018. [33] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. Advances in Neural Information Processing Systems, 33:2043720448, 2020. [34] Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. Variational information bottleneck for effective low-resource fine-tuning. arXiv preprint arXiv:2106.05469, 2021. 11 [35] Yawei Li, David Rügamer, Bernd Bischl, and Mina Rezaei. Calibrating LLMs with information-theoretic evidential deep learning. In The Thirteenth International Conference on Learning Representations, 2025. [36] Thomas Cover. Elements of information theory. John Wiley & Sons, 1999. [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [38] Wenliang Dai, Junnan Li, DONGXU LI, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36:4925049267, 2023. [39] Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, et al. Vhelm: holistic evaluation of vision language models. Advances in Neural Information Processing Systems, 37:140632140666, 2024. [40] Xiang Li, Congcong Wen, Yuan Hu, Zhenghang Yuan, and Xiao Xiang Zhu. Vision-language models in remote sensing: Current progress and future trends. IEEE Geoscience and Remote Sensing Magazine, 2024. [41] Mubashar Raza, Zarmina Jahangir, Muhammad Bilal Riaz, Muhammad Jasim Saeed, and Muhammad Awais Sattar. Industrial applications of large language models. Scientific Reports, 15(1):13755, 2025. [42] Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, and Mu Li. Benchmarking robustness of multimodal image-text models under distribution shift. Journal of Datacentric Machine Learning Research, 2024. [43] Xuanming Cui, Alejandro Aparcedo, Young Kyun Jang, and Ser-Nam Lim. On the robustness of large multimodal models against image adversarial attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2462524634, 2024. [44] Aayush Atul Verma, Amir Saeidi, Shamanthak Hegde, Ajay Therala, Fenil Denish Bardoliya, Nagaraju Machavarapu, Shri Ajay Kumar Ravindhiran, Srija Malyala, Agneet Chatterjee, Yezhou Yang, et al. Evaluating multimodal large language models across distribution shifts and augmentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53145324, 2024. [45] Naftali Tishby, Fernando Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. [46] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pages 15. Ieee, 2015. [47] Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang. Significance-aware information bottleneck for domain adaptive semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 67786787, 2019. [48] Marco Federici, Anjan Dutta, Patrick Forré, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. In International Conference on Learning Representations, 2020. [49] Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In International Conference on Machine Learning, pages 1552415543. PMLR, 2022. [50] Zichuan Liu, Tianchun Wang, Jimeng Shi, Xu Zheng, Zhuomin Chen, Lei Song, Wenqian Dong, Jayantha Obeysekera, Farhad Shirani, and Dongsheng Luo. Timex++: Learning time-series explanations with information bottleneck. In International Conference on Machine Learning, pages 3206232082. PMLR, 2024. [51] Kenji Kawaguchi, Zhun Deng, Xu Ji, and Jiaoyang Huang. How does information bottleneck help deep learning? In International Conference on Machine Learning, pages 1604916096. PMLR, 2023. [52] Zifeng Wang, Tong Jian, Aria Masoomi, Stratis Ioannidis, and Jennifer Dy. Revisiting hilbert-schmidt information bottleneck for adversarial robustness. Advances in Neural Information Processing Systems, 34:586597, 2021. [53] Jiaqi Bai, Hongcheng Guo, Zhongyuan Peng, Jian Yang, Zhoujun Li, Mohan Li, and Zhihong Tian. Mitigating hallucinations in large vision-language models by adaptively constraining information flow. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2344223450, 2025. 12 [54] Alex Graves. Practical variational inference for neural networks. Advances in neural information processing systems, 24, 2011. [55] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR), 2014. [56] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe and. Variational inference: review for statisticians. Journal of the American Statistical Association, 112(518):859877, 2017. [57] Seong Joon Oh, Andrew C. Gallagher, Kevin P. Murphy, Florian Schroff, Jiyan Pan, and Joseph Roth. In International Conference on Learning Modeling uncertainty with hedged instance embeddings. Representations, 2019. [58] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. Probabilistic embeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 84158424, 2021. [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [60] Bingbing Wen, Chenjun Xu, Robert Wolfe, Lucy Lu Wang, Bill Howe, et al. Mitigating overconfidence in large language models: behavioral lens on confidence estimation and calibration. In NeurIPS 2024 Workshop on Behavioral Machine Learning, 2024. [61] Haoyan Yang, Yixuan Wang, Xingyin Xu, Hanyuan Zhang, and Yirong Bian. Can we trust llms? mitigate overconfidence bias in llms through knowledge transfer. arXiv preprint arXiv:2405.16856, 2024. [62] Tobias Groot and Matias Valdenegro-Toro. Overconfidence is key: Verbalized uncertainty evaluation in large language and vision-language models. In Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024), pages 145171, 2024. [63] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [65] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. [66] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [68] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, 2023. [69] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [70] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [71] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 13 [72] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision, pages 1834, 2024. [73] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [74] Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. In The Thirteenth International Conference on Learning Representations, 2025. [75] Zhihan Zhou, Feng Hong, Jiaan Luo, Jiangchao Yao, Dongsheng Li, Bo Han, Ya Zhang, and Yanfeng Wang. Learning to instruct for visual instruction tuning. arXiv preprint arXiv:2503.22215, 2025. [76] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [77] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, Melbourne, Australia, July 2018. Association for Computational Linguistics. [78] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. [79] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training In International conference on machine for unified vision-language understanding and generation. learning, pages 1288812900. PMLR, 2022. [80] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [81] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [82] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [83] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [84] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Featured Certification. [85] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [86] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [87] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024. [88] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating vision-language models in the wild with human preferences. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 14 [89] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: contrastive log-ratio upper bound of mutual information. In International conference on machine learning, pages 17791788. PMLR, 2020. [90] Conneau. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019. [91] Jhoan Hoyos-Osorio and Luis Sanchez-Giraldo. The representation jensen-shannon divergence. arXiv preprint arXiv:2305.16446, 2023. [92] Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. arXiv preprint arXiv:2502.02013, 2025. [93] Evan Hernandez and Jacob Andreas. The low-dimensional linear geometry of contextualized word representations. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 8293, 2021. [94] Zhuo-Yang Song, Zeyu Li, Qing-Hong Cao, Ming-xing Luo, and Hua Xing Zhu. Bridging the dimensional chasm: Uncover layer-wise dimensional reduction in transformers through token correlation. arXiv preprint arXiv:2503.22547, 2025. [95] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):28972905, 2018. [96] Changjian Shui, Qi Chen, Jun Wen, Fan Zhou, Christian Gagné, and Boyu Wang. novel domain adaptation theory with jensenshannon divergence. Knowledge-Based Systems, 257:109808, 2022. [97] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via compression approach. In International conference on machine learning, pages 254263. PMLR, 2018. [98] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Unpublished notes, 1986. [99] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Occams razor. Information Processing Letters, 24(6):377380, 1987. [100] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465471, 1978. [101] Geoffrey Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory, pages 513, 1993. [102] Peter Grünwald. Minimum description length tutorial. Advances in minimum description length: Theory and applications, 5:180, 2005. [103] Milad Sefidgaran, Abdellatif Zaidi, and Piotr Krasnowski. Minimum description length and generalization guarantees for representation learning. Advances in Neural Information Processing Systems, 36:1489 1525, 2023. [104] Horace Barlow et al. Possible principles underlying the transformation of sensory messages. Sensory communication, 1(01):217233, 1961. [105] Andrew Gordon Wilson. Deep learning is not so mysterious or different. arXiv preprint arXiv:2503.02113, 2025. [106] Marc Finzi, Gregory Benton, and Andrew Wilson. Residual pathway priors for soft equivariance constraints. Advances in Neural Information Processing Systems, 34:3003730049, 2021. [107] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort In International Conference on Learning pretrained features and underperform out-of-distribution. Representations, 2022. [108] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning In Proceedings of the IEEE/CVF conference on computer vision and pattern of zero-shot models. recognition, pages 79597971, 2022. 15 [109] Yoonho Lee, Annie Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. In The Eleventh International Conference on Learning Representations, 2023. [110] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1933819347, 2023. [111] Junjiao Tian, Zecheng He, Xiaoliang Dai, Chih-Yao Ma, Yen-Cheng Liu, and Zsolt Kira. Trainable In Proceedings of the IEEE/CVF Conference on projected gradient method for robust fine-tuning. Computer Vision and Pattern Recognition, pages 78367845, 2023. [112] Changdae Oh, Mijoo Kim, Hyesu Lim, Junhyeok Park, Euiseog Jeong, Zhi-Qi Cheng, and Kyungwoo Song. Towards calibrated robust fine-tuning of vision-language models. Advances in Neural Information Processing Systems, 37, 2024. [113] Jaedong Hwang, Brian Cheung, Zhang-Wei Hong, Akhilan Boopathy, Pulkit Agrawal, and Ila Fiete. Imagenet-rib benchmark: Large pre-training datasets dont guarantee robustness after fine-tuning. arXiv preprint arXiv:2410.21582, 2024. [114] Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, and Dongyoon Han. Dawin: Training-free dynamic weight interpolation for robust adaptation. In The Thirteenth International Conference on Learning Representations, 2025. [115] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. [116] Wei Han, Hui Chen, and Soujanya Poria. Towards robust instruction tuning on multimodal large language models. arXiv preprint arXiv:2402.14492, 2024. [117] Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, and Yixuan Li. Position: Challenges and future directions of data-centric ai alignment. 2025. [118] Yingjun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees GM Snoek, and Ling Shao. Learning to learn with variational information bottleneck for domain generalization. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 200216. Springer, 2020. [119] Bo Li, Yifei Shen, Yezhen Wang, Wenzhen Zhu, Dongsheng Li, Kurt Keutzer, and Han Zhao. Invariant information bottleneck for domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 73997407, 2022. [120] Jiao Zhang, Xu-Yao Zhang, Chuang Wang, and Cheng-Lin Liu. Deep representation learning for domain generalization with information bottleneck principle. Pattern Recognition, 143:109737, 2023. [121] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405, 2023. [122] Sheng Liu, Haotian Ye, and James Zou. Reducing hallucinations in large vision-language models via latent space steering. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "Contents",
            "content": "A Extended Experiment Setup and Implementation Detail A.1 Model and Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Vittle Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Downstream Task Benchmark Construction . . . . . . . . . . . . . . . . . . . . . A.4 Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Results B.1 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Full Results of Pair-wise Cosine Distance . . . . . . . . . . . . . . . . . . . . . . B.3 Full Results with LLaVA-v1.5-7B and LLaVA-v1.5-13B . . . . . . . . . . . . . . B.4 Applicability to Other MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . Derivation of Variational Bound for IB in MLLM Missing Proof D.1 Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 New Upper Bound for Effective Mutual Information Difference . . . . . . . . . Extended Literature Review Limitation and Future Work Impact Statement 17 17 19 22 23 23 24 24 28 29 29 29"
        },
        {
            "title": "A Extended Experiment Setup and Implementation Detail",
            "content": "A.1 Model and Training In this work, we consider LLaVA-v1.5 [63] as our target multimodal large language model (MLLM) with CLIP ViT-L/14-336px [64] and Vicuna-v1.5-7B [65] as visual encoder and LLM backbone, respectively, and two-layer MLP as projector (modality connector that maps features of the visual encoder into the text embedding space). Although all of the results presented in the main body of the paper were produced with LLaVA-v1.5-7B, we also experimented with LLaVA-v1.5-13B (with Vicua-v1.5-13B as the LLM backbone) to validate the scalability of our method, and consider Prism-7B [66] as an additional MLLM architecture to validate the broad applicability of Vittle. For fair comparison, all models are trained on the LLaVA-pretrain-558k and LLaVA-mix-665k datasets, consisting of mixture of LAION [76], CC [77], SBU [78] datasets with BLIP captions [79] and mixture of LLaVA-instruct-158K and academic-task-oriented (V)QA datasets, respectively. Training configurations such as optimizer, learning rate, and batch size are summarized in Table 6 and Table 7. All training runs are conducted with eight A100-80GB GPUs with DeepSpeed ZeRO library. The shortest run takes roughly 11 hours, whereas the longest run takes about 14 hours. Now, we elaborate on the overall workflow of LLaVA and Prism below. 17 Table 6: Hyperparameter list of Vittle training. We adopt exactly the same configurations with LLaVA-v1.5 [63] for Stage 1 and 2. Table 7: Hyperparameter list of PrismVittle training. We adopt exactly the same configurations with Prism-DINOSigLIPControlled-7B [66] single stage training. Config Stage1 Stage2 Global batch size Batch size per GPU Learning rate Learning rate schedule Warmup ratio Weight decay Epoch Optimizer Precision 128 16 2e-5 256 32 1e-3 Cosine decay w/ linear warmup 0.03 0.0 1 AdamW bf16 Config Value Global batch size Batch size per GPU Learning rate Learning rate schedule Warmup ratio Weight decay Epoch Optimizer Precision 128 16 2e-5 cosine decay w/ linear-warmup 0.03 0.1 1 AdamW bf16 LLaVA is built with pre-trained visual encoder that takes visual inputs, pre-trained LLM backbone that takes text instructions, and lightweight projector that maps features produced by the visual encoder into the text embedding space of LLM backbone so that the visually-grounded multimodal instruction input query can be processed by the LLM backbone. LLaVA undergoes two-stage training: (1) The first stage takes into account modality alignment, where the projector is trained on image and corresponding instruction or caption with conditional language modeling loss implemented by aggregating cross-entropy losses across response tokens, while the visual encoder and LLM backbone are frozen. (2) The second stage stands for the instruction tuning, where the projector and LLM backbone are jointly trained on multimodal instruction samples with the same conditional language modeling loss while the visual encoder is still frozen. This two-stage training has been considered standard approach for developing MLLMs and is widely adopted [80, 81, 28]. Prism has model architecture similar to LLaVA, but provides some valuable insight into the design of the MLLM training recipe, and we note two remarkable design choices of Prism that distinguish it from LLaVA: (1) incorporating multiple visual encoders rather than hosting single visual encoder, and (2) reducing the two-stage alignment-then-instruction tuning into single-stage instruction tuning. Note that different self-supervised visual representation learning induces features that have different strengths, and several works reveal the benefits of ensembling multiple different visual encoders to leverage complementary advantages [82, 66, 29]. Prism incorporates SigLIP [83] and DINOv2 [84] to enjoy both robust global feature and fine-grained local feature. Meanwhile, Karamcheti et al. [66] showed that the simplified single-stage training strategy can be cost-effective alternative to the standard two-stage training. To train these MLLMs, we consider five baseline approaches: (1) the standard full LLM finetuning with conditional language modeling loss, (2) parameter-efficient LoRA [73] fine-tuning with the conditional language modeling loss, (3) conditional language modeling loss with weight decay regularization, (4) reconstructive visual instruction tuning (ROSS) [74], and (5) learning to instruct (LIT) [75]. For the LoRA-based training configuration, we use the same one provided by the official LLaVA-v1.5 repository3, and for the weight decay regularization, we select the regularization magnitude parameter among {0.1, 0.01, 0.001} based on the POPE evaluation result. We now elucidate two competitive baseline methods, ROSS and LIT, in the following paragraphs. It is worth noting that these methods are designed to encode more (visual) information into the representation space, which is opposite to our Vittles design motivation that pursues minimal sufficient representation for improving robustness to distribution shifts. Reconstructive visual instruction tuning (ROSS) follows the two-stage training of LLaVA, but tries to reconstruct the visual inputs from the LLM backbone by adopting regression or denoising learning objective in addition to language modeling loss during its second stage. By doing so, ROSS guides the MLLM to learn much richer visual understanding, which is usually lacking in modern MLLMs [82, 72]. The reconstruction target can be raw RGB pixel value or the latent representation from an external visual encoder such as VQGAN [85] or VAE [55], and ROSS requires an additional trainable module to reconstruct visual content, which is discarded during inference. We follow the training recipe from the official code repository4 to replicate ROSS-D-7B with the same visual encoder and LLM backbone to LLaVA-v1.5. For fair comparison with LLaVA and Vittle, we 3https://github.com/haotian-liu/LLaVA 4https://github.com/Haochen-Wang409/ross 18 train the ROSS with the same dataset (that of LLaVA-v1.5) for both training stages, while the original ROSS model was trained on slightly larger dataset in the second stage. Learning to Instruct (LIT) also focuses on the visual shortcomings of current MLLM and tries to improve the visual understanding capability of MLLM by incorporating an additional loss term that incentivizes the encoding of additional visual information. To be specific, while the cross-entropy loss in LLaVAs conditional language modeling objective is aggregated through the response tokens only, LIT introduces an extra cross-entropy loss term, which is aggregated over the instruction (question) tokens only, thereby enforcing MLLM to learn to predict proper textual instruction given an image. As LIT uses the same visual and language backbone model and training dataset as LLaVA-v1.5, we use the pre-trained checkpoint of LIT from Hugging Face5 for evaluation. In Figure 6, we observe that while ROSS and LIT are somewhat effective in improving performance on object hallucination detection tasks with the aid of enhanced visual understanding capability, they significantly underperform Vittle and even the original LLaVA on the open-ended QA task under distribution shifts. This implies that pursuing more information encoding during visual instruction tuning may not result in better robustness to distribution shifts, but aiming to learn minimal sufficient representation via Vittle can be promising solution for this (See Table 11 for details). A.2 Vittle Implementation Details This section provides additional details on implementing Vittle through Python-style pseudo code in Figure 9 and text below. Following the standard two-stage LLaVA training recipe, we freeze the visual encoder and LLM backbone during the first stage and only train the projector module. In the second stage, Vittle inserts bottleneck layer gϕ, consisting of two of the twolayer MLPs {gϕv , gϕt} for visual and textual modalities, inside the LLM backbone to estimate the distributional parameters (mean and diagonal covariance) of the posterior Gaussian distributions for each visual and textual token. Each bottleneck module is constructed with {nn.Linear(d,d), nn.GELU(), nn.Linear(d,2*d)} where denotes the hidden dimension of the LLM backbone, and this results in slightly increased number of model parameters (up to 1.5% from the baseline). We use these estimated distribution parameters to sample representation from this posterior via = µ + σ ϵ where ϵ (0, I). Then, for given bottleneck layer index and for the maximum length of visual Mv and textual input tokens Mt, the bottleneck layer gϕ takes sequence of token representations = {zv,1, ..., zv,Mv , zt,1, ..., zt,Mt} produced from the layer to build informationpenalized representations ˆz = {ˆzv,1, ..., ˆzv,Mv , zt,1, ..., zt,Mt}, where ˆz = (1 α)z + αgϕ(z). Here, we use an interpolated representation between the original pre-bottleneck representation and the post-bottleneck representation gϕ(z) with an interpolation coefficient α that progressively grows from 0 to 0.5 by cosine schedule during training. We observe that solely using the post-bottleneck representation induces diverging language modeling loss at the later steps of training, and speculate that it is hard to generate valid response with the information-penalized representation only. Then, we jointly train the LLM backbone, the projector, and this bottleneck layer together during the second stage of training with the objective function 5. As we assume diagonal covariance Gaussian for the prior and posterior distributions, KullbackLeibler divergence (KLD) between the prior and posterior can be easily expressed as below, DKL(q, p) = 1 2 (cid:88) j=1 (cid:0) log σ2 p[j] σ2 [j] 1 + (µp[j] µq[j])2 σ2 p[j] + (cid:1) σ2 [j] σ2 p[j] (8) where µ and σ denote d-dimensional distributional parameter vectors and [j] indicates j-th element from the vectors. Vittle has two important hyperparameters: (1) target layer index for bottleneck application, and (2) posterior KLD regularization strength parameter β. After tuning across {24, 28, 31} and β { 0.01 } where denotes the latent dimension of the LLM backbone, we set = 24 and β = 0.1/d based on the average performance of POPE and LB-COCO clean datasets. The interpolation coefficient α can also be tuned, but we found that increasing α beyond 0.5 hinders stable training and observing increased language modeling loss at the later parts of training progress. Figure 12 and Table 10 present the results of hyperparameter ablation study. , 0.05 , 0. , 0.2 , 1.0 5https://huggingface.co/zhihanzhou/LIT-LLaVA-1.5-Vicuna-7B/tree/main 19 def reparam ( mu , logvar ): std = ( logvar / 2). exp () batch_size , seq_len , hidden_dim = mu . shape = torch . randn ( batch_size , seq_len , hidden_dim ) return mu + std * def forward ( self , input_embeds , img_seq_len , , ** kwargs ): ... hidden_states = input_embeds for l_idx , llm_layer in enumerate ( self . llm . layers ): layer_outputs = llm_layer ( hidden_states , ** kwargs ) hidden_states = layer_outputs [0] if l_idx == self . bot leneck_layer_idx : # posterior inference v_params = self . g_v ( hidden_states [: ,: i_seq_len ,:]) t_params = self . g_t ( hidden_states [: , i_seq_len : ,:]) v_mean = v_params [: ,: ,: self . h_dim ] v_logvar = v_params [: ,: , self . h_dim :] t_mean = t_params [: ,: ,: self . h_dim ] t_logvar = t_params [: ,: , self . h_dim :] v_post = reparam ( v_mean , v_logvar ) t_post = reparam ( t_mean , t_logvar ) z_post = torch . cat (( v_post , t_post )) # interpolation between original and bottlenecked hidden_states = (1 - ) * hidden_states + * z_post ... Listing 1: Forward pass of Vittle def normalized_kld ( mu , logvar , modality = None ): if modality is None : # vittle ( ) - fixed prior (0 , ) kl_loss = -0.5 * (1+ logvar - mu **2 - logvar . exp ()). mean () else : # vittle ( ) - learnable prior mu_pr , logvar_pr = self . l_prior [ modality ] logvar_d = logvar - logvar_pr scaled_mu_d = ( mu - mu_pr ). pow (2)/ logvar_pr . exp () var_ratio = logvar . exp ()/ logvar_pr . exp () kl_loss = -0.5 * (1+ logvar_d - scaled_mu_d - var_ratio ). mean () return kl_loss def loss ( self , logits , labels , v_mean , v_logvar , t_mean , t_logvar ): lm_loss = self . llm . loss_function ( logits , labels ) if self . learnable_prior : flag_v , flag_t = \" \" , \" \" else : flag_v , flag_t = None , None kld_v = self . normalized_kld ( v_mean , v_logvar , flag_v ) kld_t = self . normalized_kld ( t_mean , t_logvar , flag_t ) return lm_loss + self . beta * ( kld_v + kld_t ) Listing 2: Training objective of Vittle Figure 9: PyTorch-style pseudo code for the forward pass and training objective of Vittle 20 A.3 Downstream Task Benchmark Construction Open-ended QA task. One of the most representative applications of an MLLM is the generation of free-form responses given multimodal instruction queries. We consider LLaVA-Bench COCO (LB-COCO; [37]) as typical in-distribution (ID) open-ended QA dataset, which is constructed from MS-COCO images [86] with GPT-generated text queries that have 90 pairs of image and text. We then generate 27 variants of this LB-COCO by applying nine types of image perturbations, nine types of text perturbations, and nine types of image-text joint perturbations, to benchmark MLLMs robustness under various distribution shifts (which will be elaborated at the end of this section). Meanwhile, we also consider three datasets LLaVA-Bench in-the-wild (LB-Wild; [37]), LLaVA-Bench Wilder (LB-Wilder; [87]), and WildVision-Bench (WV-Bench; [88]) constructed by real-world web users image-text paired queries of 60, 128, and 500 samples, respectively, to validate models capability to address long-tailed queries in practice. This results in 31 different open-ended QA datasets in total: clean and 27 perturbed LB-COCO variants, and three long-tail datasets. Object hallucination detection task. Meanwhile, one of the most crucial evaluation aspects of an MLLM is the degree of hallucination of its output. representative benchmark for this is the POPE dataset [68], where the model is tasked to answer in binary {Yes, No} form given question about the objects existence given an image. The POPE dataset was also created from the MS-COCO source images with 9,000 corresponding questions, and we consider this dataset as an ID dataset. As we did for the LB-COCO dataset, we generated 9 variants of POPE by applying nine types of visual perturbations to images. Textual perturbations were not considered here because the text query of this dataset is relatively short, so perturbing the core object word token can distort the desired semantics of the question. In summary, we conducted validation on 10 different POPE variants. Closed-form QA task. There are numerous closed-form QA datasets that assess the internal knowledge of MLLMs from various perspectives. In this paper, we consider four representative datasets: ScienceQA [69], MMMU [70], MME [4], and MMStar [71], which are designed to validate multimodal knowledge and understanding capability across various domains. Distribution shift simulation. The goal of this study is to improve the robustness of MLLM under distribution shifts. We mainly focus on subtle perturbations on image and text, which is worth-noting problem given the fact that current MLLMs undergo systematic performance degradation under perturbations. We consider nine visual perturbations listed in Table 8, nine textual perturbations listed in Table 9, and nine image-text joint perturbations: {zoom_blur, frost, gaussian_noise} {arabic, greek, hindi}. The translations for Arabic, Greek, and Hindi languages from English are conducted by OpenAI GPT-4o with prompt: \"Please translate {SOURCE} sentence provided by the user into {TARGET}.\", and all the remaining perturbations are generated MMRobustness source code6. The actual examples of each visual textual perturbation are presented in Figure 11 and Figure 10. Table 8: List of visual perturbations. We consider nine visual perturbations from four categories: (1) Blur, (2) Digital, (3) Weather, and (4) Noise, to validate the robustness of MLLMs under diverse types of visual perturbations. Table 9: List of textual perturbations. We consider nine textual perturbations from three categories: (1) character-level, (2) word-level, and (3) sentence-level, to validate the robustness of MLLMs under diverse types of textual perturbations. Name Category Defocus Blur Zoom Blur Contrast Brightness Fog Frost Gaussian Noise Shot Noise Speckle Noise Blur Blur Digital Weather Weather Weather Noise Noise Noise Name Char Typo Char Delete Char Insert Word Swap Word Delete Word Insert Category Character-level Perturbation Character-level Perturbation Character-level Perturbation Word-level Perturbation Word-level Perturbation Word-level Perturbation Arabic Translation Greek Translation Hindi Translation Sentence-level Perturbation Sentence-level Perturbation Sentence-level Perturbation 6https://github.com/Jielin-Qiu/MM_Robustness Figure 10: Examples of visual perturbations. Figure 11: Examples of textual perturbations. A.4 Evaluation Details Open-ended QA task. Compared to multi-choice closed-form QA tasks that have unique groundtruth answer per question, open-ended free-form generation-style QA tasks do not provide single ground-truth answer. We follow the current standard evaluation paradigm, (M)LLM-as-a-Judge, that uses an external (usually more powerful) MLLM to gauge the quality of our target MLLM of interest via prompting. To be specific, for given input query x, reference answer y, MLLM fθ : Y, and the judge model : Z+, relative preference score is defined as, Ex,y[ r(x,fθ(x)) ]. r(x,y) For all of our open-ended QA evaluations, we used the same system prompt template provided by LLaVA authors7, and we also adopted the MS-COCO annotation8-based GPT-4 response9 and the gpt_answer10 released by LLaVA-NeXT authors as reference answers for LB-COCO variants and LB-Wilder, respectively. For LB-Wild and WV-Bench, we generated reference answers with GPT-4o. 7https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/rule.json 8https://github.com/haotian-liu/LLaVA/blob/main/llava/eval/table/caps_boxes_ coco2014_val_80.jsonl 9https://github.com/haotian-liu/LLaVA/blob/main/playground/data/coco2014_val_qa_ eval/qa90_gpt4_answer.jsonl 10https://huggingface.co/datasets/lmms-lab/LLaVA-Bench-Wilder 22 Object hallucination detection and closed-form QA task. In contrast to open-ended tasks, all object hallucination detection and closed-form QA tasks provide single ground truth answer as form of discrete labels such as {Yes, No} and {A, B, C, D, ...}. For the multi-choice QA datasets, MMMU, MMStar, and ScienceQA, we attached subfix prompt: \"Answer in character from the given choices directly.\" at the end of each question for answer formatting, while using the original question text for YES-or-NO datasets, MME and POPE, without formatting prompt. We measured the exact matching accuracy Ex,y[I(θ(x) = y)] for these tasks. Effective Mutual Information Difference (EMID) and Jensen-Shannon Divergence (JSD). In addition to the evaluation with traditional metrics, we also consider the EMID and JSD-based evaluation, which was recently proposed as an information-theoretic approach to measure the robustness of MLLMs [18]. To compute the empirical estimates of MI, which is required for EMID computation, we use the CLUB estimator [89] and reproduce the training and inference process of [18] by adopting image and text embeddings for the input image and text from CLIP-ViT-B/32 [64] and XLM-RoBERTa-Base [90] to replace Xv and Xt, and also the text embeddings of XLM-RoBERTaBase for responses and YΘ. To compute empirical estimates of JSD, we adopted the representation JSD estimator [91] on top of the CLIP-ViT-B/32 and XLM-RoBERTa-Base, too. Representation analysis. Inspired by recent work that reveals the importance of intermediate layer representation of the LLM backbone [92], we use the last input token embedding of the 24th layer (out of 32 layers in 7B LLM backbone) for all experiments carried out in the representation space (Figure 1 (b) right, Figure 8, Figure 13, and the JSD computation in Table 4)."
        },
        {
            "title": "B Additional Results",
            "content": "B.1 Ablation Study We first investigate two important hyperparameters for Vittle: (1) bottleneck layer index and (2) KLD regularization strength β, where we determined those parameter values based on the average performance on the clean POPE and LB-COCO, while not observing performance on perturbation datasets for fair model selection. We then further explore the impact of the interpolation coefficient α, which plays role in controlling the balance between the original representation and the bottleneck representation. Note that we could not conduct such an extensive search due to the computational burden of training 7B 13B scale models, so the hyperparameter values found here may not be optimal, and Vittle can achieve better results with further hyperparameter tuning. Figure 12: Ablation study for the bottleneck layer index (left) and KLD regularization magnitude parameter β (right). For bottleneck target layer ablation (Figure 12 left), we swept across {8, 16, 20, 24, 28, 31} out of 32 layers of the 7B-size LLM backbone. However, applying the bottleneck on the early layer failed to make the language modeling loss converge, so we only provided results for 24, 28, and 31 layers. We observed that intermediate layers (L24 and L28) achieve better results than the penultimate layer (L31), and L24 shows better results on POPE while L28 outperforms L24 on LB-COCO. In conclusion, applying the bottleneck to too early parts hinders shaping some shallow syntactic features that will be actively used at later parts of the layers [93], whereas applying it to too late parts hurts output-specific alignment or formatting [94], which guide us to decide intermediate layer, i.e., 24th, 23 as default choice. This is in line with recent finding that the intermediate layer of LLM matters more than the early or later layers by showing that the quality measurements of the intermediate layer representations have stronger correlation with performance in downstream tasks [92]. Although we can search different layer indices for visual and textual tokens, we leave this to future work. For KLD regularization strength parameter ablation (Figure 12 right), we swept across {0.01, 0.05, 0.1, 0.2, 1.0}, and found that in the POPE dataset, strong regularization results in better performance, whereas it is not the case for LB-COCO. We choose 0.1/d as our default, which induces balanced clean-data performance on these two tasks. Table 10: Ablation study for the representation interpolation coefficient α of the bottleneck layer. We observe that using the bottlenecked representation beyond the half portion of the total hinders the convergence of the language modeling loss."
        },
        {
            "title": "Alpha",
            "content": "POPE POPE Shifts Avg. LB-COCO LB-COCO Shifts Avg."
        },
        {
            "title": "Baseline",
            "content": "86.98 87.22 87.34 87.71 0.1 0.25 0.5 0.75 1 84.12 84.20 84.47 84.90 77. 77.9 75.6 76.7 Failed to converge Failed to converge 72.3 73.1 73.1 73.0 We also explore the effect of the representation interpolation parameter α [0, 1], which can be interpreted as gating mechanism to control the information flow. As α approaches one, the later parts of the LLM backbone (LLM head in our notation) mainly use the information-penalized representation, while if α becomes smaller, the model strongly relies on the original representation. In Table 10, we observe that using too large values of α results in diverging language modeling loss, indicating that using strongly penalized representation only cannot predict proper response tokens in sequence. Meanwhile, the larger value of α induces better POPE performance, whereas the trend is inconsistent in the LB-COCO data set, which is consistent with the observations from the previous ablation study in β. B.2 Full Results of Pair-wise Cosine Distance We speculate that the performance degradation of MLLMs under perturbations originates from the representation discrepancy between clean and perturbed samples. That is, in the ideal case, clean sample and its semantically equivalent perturbed sample should be closely mapped in the representation space, but current MLLMs did not shape the representation space in that way (see Figure 1 and Figure 8). In Figure 13, we provide the histograms of representation space pair-wise cosine distance between clean and perturbed examples in 27 types of perturbations. As we can see, Vittle (F) consistently mitigates the representation gap by reducing the pair-wise distance over diverse types of perturbations. B.3 Full Results with LLaVA-v1.5-7B and LLaVA-v1.5-13B Table 11 summarizes the overall results of our perturbation benchmarks on object hallucination detection (POPE) and open-ended QA tasks (LB-COCO). We note two findings here: (1) weightspace regularization methods, such as LoRA and WD failed to achieve reasonable performance; (2) although information maximization-based instruction tuning methods, such as ROSS and LIT, somewhat improve performance on POPE and its perturbation datasets, they greatly underperform Vittle, indicating non-trivial challenge to design versatile instruction tuning objective that can improve MLLMs on broad tasks. Meanwhile, we explore whether Vittle can be effective for much larger model, e.g., 13B-scale model. Table 12 shows that Vittle achieves consistent performance gains in object hallucination detection and open-ended QA tasks under distribution shifts, implying the scalability of our method. 24 Figure 13: Pair-wise cosine distance of intermediate representations between clean LB-COCO and 27 versions of perturbed LB-COCO datasets. Vittle (F) consistently reduces the representation gap between the clean samples and their semantically equivalent perturbed ones. Table 11: Comparison with alternative training approaches. We compare Vittle with weightspace regularization methods, LoRA [73] and weight decay (WD), and two recent visual instruction tuning learning objectives, ROSS [74] and LIT [75] on LLaVA-v1.5-7B model. Evaluations are conducted on POPE, its nine visually perturbed variants (POPE Pert.), LB-COCO, and its nine {visually/textually/jointly} perturbed variants, where we mark the best one as bold and the second best one as underlined. Method POPE POPE Pert. LB-COCO LB-COCO Pert. LB-COCO Pert. LB-COCO Pert. Baseline LoRA WD ROSS LIT Vittle (L) Vittle (F) 86. 83.33 87.22 87.79 87.38 87.71 87.81 84.12 80.23 83.97 84.67 84.21 84.91 84.99 77. 73.4 74.1 74.4 77.5 76.7 76.1 73.4 70.4 72.1 72.0 72.1 73.9 74.2 72. 62.7 73.0 71.3 72.9 73.0 74.1 62.3 39.7 59.5 60.0 58.9 62.7 64.4 Table 12: Vittle on LLaVA-v1.5-13B model. We compare Vittle with the standard learning objective on LLaVA-v1.5-13B model that uses Vicuna-v1.5-13B as an LLM backbone. We set the bottleneck layer index = 36, interpolation coefficient α = 0.5, and bottleneck KLD regularization strength β = 0.1 . Vittle outperforms baseline on perturbed datasets while showing rivaling performance on the clean dataset. Method POPE POPE Pert. LB-COCO LB-COCO Pert. LB-COCO Pert. LB-COCO Pert. Baseline Vittle (L) Vittle (F) 87.14 87.22 87.32 84.02 84.85 84.65 76.9 76.6 76.8 73.5 74.5 74. 73.8 74.0 73.9 64.6 65.4 65.3 B.4 Applicability to Other MLLMs We now investigate Vittles effectiveness on another recent MLLM, Prism-7B, beyond LLaVA. As noted in Section A.1, Prism has quite different design principle than LLaVA with respect to the visual encoder and the training strategy, so it is suitable for investigating the versatility of Vittle between models. Table 13 shows summarized results on our perturbation benchmarks11. In object hallucination detection tasks, Vittle outperforms the standard cross-entropy only training baseline on clean and perturbed datasets. In open-ended QA tasks, Vittle consistently boosts performance in perturbation scenarios with large margins while maintaining performance on the clean dataset. The results of the perturbation-specific performance comparison are provided in Figure 14. Table 13: Vittle on Prism-7B model. We compare Vittle (F) with the standard learning objective under the Prism-7B model training regime that adopts two visual encoders (DINOv2 and SigLIP) and the single-stage training rather than two-stage training with single CLIP visual encoder. Vittle significantly improves perturbation-robustness compared with naive learning objective. Method POPE POPE Pert. LB-COCO LB-COCO Pert. LB-COCO Pert. LB-COCO Pert. Baseline Vittle (F) 87.54 88.11 85.29 85.52 79.4 79.0 75.3 76.2 71.9 75. 53.8 63.2 11Due to resource constraints, we only explore Vittle (F) one of our prior distribution instantiations. 26 Figure 14: Object hallucination detection performance on POPE perturbation datasets (top), and Open-ended QA performance on LB-COCO perturbation datasets (three below) of Prism7B. We enumerate the accuracy for the object hallucination detection task and relative preference score for the open-ended QA task of each method on perturbed datasets, where we observe consistent performance gains by Vittle."
        },
        {
            "title": "C Derivation of Variational Bound for IB in MLLM",
            "content": "Here we provide full derivation for the variational lower bound for IB. The derivation skeleton was mainly inspired by existing works [95, 31]. We begin with the mutual information term I(Z, X). Given the sequential nature of MLLM, we decompose both the input = (Xv, Xt) and the latent representation = (Zv, Zt) into visual and textual components. We can then derive the following upper bound for I(Z, X): (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) I(Z, X) = = = = (cid:90) dxdz = p(x, z) log p(zx) p(z) dxdz dxdz DKL(p(z)r(z)) p(x, z) log p(x, z) log p(x, z) log p(x, z) p(x)p(z) p(zx) r(z) p(zx) r(z) dxdz p(xv, xt, zv, zt) log dxvdxtdzvdzt p(ztxv, xt)p(zvxv) r(zv)r(zt) (cid:90) (cid:90) (cid:90) p(xv, xt) p(ztxv, xt) p(zvxv) log dxvdxtdzvdzt (cid:90) + p(xv, xt) (cid:90) p(zvxv) p(ztxv, xt) log dxvdxtdzvdzt p(zvxv) r(zv) p(ztxv, xt) r(zt) = Exv [DKL(p(zvxv)r(zv))] + Exv,xt[DKL(p(ztxv, xt)r(zt))], (9) where the first inequality holds given the non-negativity of DKL[r(z), p(z)] and p(zvxv, xt) = p(zvxv) due to causal attention in MLLM. Here, we introduce r(z) = r(zv, zt) = r(zv)r(zt) as factorizable variational approximation of the true prior for the latent representation p(z). Next, for the output-relevant term I(Z, ), we have the lower bound: (cid:90) (cid:90) (cid:90) (cid:90) I(Z, ) = = = p(y, z) log p(y, z) p(y)p(z) (cid:90) dydz = p(y, z) log p(yz) p(y) dydz p(y, z) log q(yz)dydz + DKL(p(yz)q(yz)) (cid:90) p(y) log p(y)dy p(y, z) log q(yz)dydz p(x, y, z) log q(yz)dxdydz = (cid:90) p(x)p(yx)p(zx) log q(yz)dxdydz, = Ex,yEzx [log q(yz)] . (10) where p(x, y, z) = p(x)p(zx)p(yx) given the Markov chain assumption Z, and p(zx, y) = p(zx) holds given that the representation can not directly depend on , and the entropy term of y, i.e., (cid:82) p(y) log p(y)dy = H(Y ), is ruled out due to its independence for optimization problem. Here, we replace the intractable p(yz) with variational approximation q(yz) that will be parameterized by model. Finally, combining the lower bound of I(Z, ) and the upper bound of I(Z, X) yields variational lower bound for the IB objective as follows, (cid:2)Ezx[log q(yz)](cid:3) IB(X, ) Ex,y β (Exv [DKL(p(zvxv)r(zv))] + Exv ,xt [DKL(p(ztxv, xt)r(zt))]). (11)"
        },
        {
            "title": "D Missing Proof",
            "content": "D.1 Preliminary We start by providing definition of Mutual Information (MI) below. Definition D.1 (Mutual Information (MI)). For joint distribution PXY over Y, the mutual information with respect to PXY is defined as, I(PXY ) := Ex,yPXY [log PXY (x, y) PX (x)PY (y) ]. (12) If is an instruction and is corresponding response, we regard I(PXY ) as relevance between the instruction and the response that can be seen as possible quantification of instruction following capability of MLLMs. Effective MI is defined based on the MI as follows: Definition D.2 (Effective Mutual Information (EMI) [18]). Given the joint distribution PXY and MLLM PΘ parameterized with Θ, the effective mutual information between the input and model response is defined as, EMI(PXY ; PΘ) := I(PXYΘ) I(PXY ), (13) where PXYΘ denotes the joint distribution between the input and the output of the model YΘ. Although the vanilla MI can also be used as metric to evaluate models output response by I(PXYΘ), the scale of it varies depending on the target data distribution which is undesired when our interest is to compare performance of model across multiple domains which can be addressed by EMI. Recall that we are ultimately interested in the performance difference of MLLMs across two different datasets, and this can be captured by the EMI difference (EMID) as follows: Definition D.3 (EMID). Let PΘ : be an MLLM with parameters Θ that produces an output response YΘ given an input instruction X. For joint distributions PXY and QXY , effective mutual information difference of PΘ over and is defined as below, EMID(PXY , QXY ; PΘ) := [I(PXYΘ) I(PXY )] [I(QXYΘ) I(QXY )]. (14) By setting as an instruction tuning distribution (training data) and as an arbitrary test time distribution (evaluation data), we prefer model that has smaller EMID value, which indicates better robustness under distribution shifts between and Q. Now, based on the original theorem provided by Oh et al. [18], we are ready to derive new upper bound for EMID tailored to our representation-centric visual instruction tuning setup. D.2 New Upper Bound for Effective Mutual Information Difference We first review Lemma 1 of Shui et al. [96] and its adapted version, conditional entropy bound [18] as follows, Lemma D.4 (Lemma 1 from Shui et al. [96]). Let be the real-valued integrable random variable, and denoting two distributions on common space by and such that is absolutely continuous w.r.t. . If for any function and λ such that EP [exp(λ(f (z) EP (f (z))))] < , then we have: λ(EzQ[f (z)] EzP [f (z)]) DKL(QP ) + log EzP [exp(λ(f (z) EzP [f (z)]))] Lemma D.5 (Conditional entropy bound [18]). Let (x) := maxxX H(QY x), given the marginal distributions PX and QX , and conditional distributions PY and QY , according to Lemma D.4, we have conditional upper bound: := H(QY x) and ˆH(QY x) i) ExP [H(QY x)] ExQ[H(QY x)] ˆH(QY x)(cid:112)2DJS(PX QX ). Similarly, given the marginal distribution PX and QX , and an MLLM PΘ, let (x) := H(PΘ(x)) and ˆH(PΘ) := maxxX H(PΘ(x)), then, according to Lemma D.4, we have another conditional upper bound: ii) ExQ[H(PΘ(x)] ExP [H(PΘ(x))] ˆH(PΘ)(cid:112)2DJS(PX QX ). 29 Next, we should also need to formulate the relationship between JSD in the input space and JSD in the representation space, which is done through Lemma D.6. Lemma D.6. Let : be an encoder that maps an input to representation Z, for the input distributions PX and QX and -induced representation distribution PZ and QZ, we have an inequality below, (cid:112)2DJS(PX QX ) (cid:112)2DJS(PZQZ)+ EzP [DKL(PXzMXz)] + EzQ[DKL(QXzMXz)] (cid:113) (15) where MXz := PXz + QXz 2 . Proof. We start from the definition of JSD, DJS(PX QX ) = 1 2 DKL(PX MX ) + 1 2 DKL(QX MX ), MX = PX + QX . By applying the chain rule of KLD under deterministic map = (X), we know that, DKL(PXZMXZ) = DKL(PZMZ) + (cid:90) PZ(z)DKL(PXzMXz)dz = DKL(PX MX ) + DKL(PX MX ) (cid:90) (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40) PX (x)DKL(PZxMZx)dx Then, we have, DJS(PX QX ) = DJS(PZQZ) + 1 2 (EzP [DKL(PXzMXz)] + EzQ[DKL(QXzMXz)]), which results in ineq. (15) by applying the triangular inequality after multiplying 2 on both sides. Now we derive new upper bound for EMID, which is defined over the representation space rather than the previous one defined over the input space [18] in Proposition D.7. Proposition D.7 (EMID upper bound). Let PΘ be an MLLM that maps = {Xv, Xt} to = {Zv, Zt}, and then sequentially maps to YΘ. Given joint distributions PXY = PX PY and QXY = QX QY , by assuming consistent conditional distributions over ZvZt, ZtZv, and between and Q, we have an upper bound for EMID(PXY , QXY ; PΘ) as follow, ˆH(cid:0)D 1 JS(PZv QZv ) + 1 2 JS(PZt QZt ) + (cid:112)XZ (cid:1) + H(PYΘ ) H(PY ) + H(QYΘ ) H(QY ), (16) 1 2 where and JS indicate the entropy and square root of Jensen-Shannon divergence (JSD), respectively, XZ := EzP [DKL(PXzMXz)] + EzQ[DKL(QXzMXz)] with mixture distribution = +Q , and ˆH := maxxX [H(QY x) + H(PYΘ)]. 2 Proof. Given the entropy-based definition of the mutual information, I(PXY ) := H(PY ) ExP [H(PY x)], let PYΘ = ExP [PΘ(x)] and QYΘ = ExQ[PΘ(x)], then, EMID can be expressed as follows, EMID(PXY , QXY ; PΘ) = EMI(PXY ; PΘ) EMI(QXY ; PΘ) = (H(PYΘ) ExP [H(PΘ(x))] H(PY ) + H(PY )) (H(QYΘ) ExQ[H(PΘ(x))] H(QY ) + H(QY )) (H(PY ) H(QY )) + (cid:0)ExQ[H(PΘ(x))] ExP [H(PΘ(x))](cid:1) + H(PYΘ ) H(PY ) + H(QY ) H(QYΘ) (H(PY ) H(QY )) + (cid:0)ExQ[H(PΘ(x))] ExP [H(PΘ(x))](cid:1) (A) + H(PYΘ) H(PY ) + H(QY ) H(QYΘ). 30 (B) (17) Moreover, we have the following inequality for H(PY ) proposed by [18], H(PY ) H(QY ) 4ExP [D 1 4 JS(PY xQY x)] + ExP [H(QY x)] + ExQ[H(QY x)] (18) By plugging inequalities in Lemma D.5 and ineq. (18) into the ineq. (17) to replace the terms (A) and (B), and given the consistent conditional distribution assumption for X, i.e., PY = QY , we have much simpler upper bound as follows, EMID(PXY , QXY ; PΘ) ˆH(cid:112)2DJS(PX QX ) + H(PYΘ) H(PY ) + H(QY ) H(QYΘ), where ˆH := maxxX [H(QY x) + H(PYΘ )]. Then, we can further replace the term DJS(PX QX ) by using Lemma D.6 to get bound defined by representation divergence as below, EMID(PXY , QXY ; PΘ) ˆH((cid:112)2DJS(PZ QZ ) + + H(PYΘ ) H(PY ) + H(QY ) H(QYΘ ). (cid:113) EzP [DKL(PXzMXz)] + EzQ[DKL(QXzMXz)]) (19) Meanwhile, the chain rule of KLD and the definition of JSD with our consistent conditional distributions for ZvZt and ZtZv, one can easily show that, 2DJS(PZvZtQZvZt) = DKL(PZvZtMZvZt) + DKL(QZvZtMZvZt) = DJS(PZv QZv ) + DJS(PZtQZt) (20) Plugging Eq. 20 into ineq. (19) and applying the triangular inequality complete the proof."
        },
        {
            "title": "E Extended Literature Review",
            "content": "Compression for generalization. There is rich history in the machine learning field that connects compression of the model or its inner representation to generalization [97], from the classical learning theory with Occams razor [98, 99] and Minimal Description Length [100, 101, 102] to IB principle [45, 46], by suggesting models that provide minimal and simplest representation of data generalize better [32, 101, 51, 103] analogy to human perception [12, 104, 14]. Recently, Wilson [105] proposed new generalization bound for contemporary large-scale models where the compressibility of learning algorithm plays key role in better generalization. According to that discussion, even the maximally flexible billion-scale model can have small effective dimensionality (indicating the higher compressibility) by embracing soft inductive biases [106], such as, regularization term, to the learning problem. On top of these, IB-objective of Vittle can be understood as soft inductive bias to seek minimal sufficient representation that helps generalization for the challenging queries. Robustness of fine-tuned foundation models. Although large-scale pre-trained models have appealing generalization capability across diverse data instances from different domains, their finetuned counterparts usually hurts that strong generalization capability while being adapting on taskspecific in-distribution samples [107, 108]. This undesirable performance compromise between adaptation to in-distribution samples and generalization to samples from broad domains has spurred the community to work on robust fine-tuning of foundation models [107, 108, 109, 110, 111, 112, 113]. This line of work addresses the adaptation-robustness trade-off by (1) introducing regularization term [111, 112], (2) tweaking the training procedure [107, 109], or (3) merging multiple models in the weight space [108, 114]. However, almost all of the existing robust fine-tuning literature has focused on discriminative model, such as CLIP [64], under classification setups. Although there are few works on robust instruction tuning of MLLMs [115, 116], they do not specifically focus on improving robustness under diverse types of distribution shifts and propose data-centric approach, i.e., expanding instruction tuning datasets in terms of quantity or diversity, that requires external MLLM-based data generation process and/or careful post-processing from humans. In this work, we take representation-centric approach that modifies the learning objective of visual instruction tuning to efficiently enhance the robustness of MLLM under diverse distribution shifts (27 types in total)."
        },
        {
            "title": "F Limitation and Future Work",
            "content": "One of the potential concerns with IB is its reliance on the quality of , i.e., gold response to given instruction, which is usually generated by another (M)LLM. As disclosed by Yeh et al. [117], the existing datasets for supervised fine-tuning are quite noisy, and we cannot ensure the advantage of IB on this noisy annotation setup. Moreover, IB alone does not guarantee the generalization of samples from completely different domains and may require additional domain information [118, 119, 120]. Investigating the potential of noisy annotation setups and domain generalization setups can be interesting future research problems. Meanwhile, well-organized representation space by IB can be helpful for representation engineering or steering methods [121, 122] that are also worth exploring for future work."
        },
        {
            "title": "G Impact Statement",
            "content": "Multimodal large language models (MLLMs) today have many societal applications. This work tackles the robustness of MLLMs to distribution shifts between training and test time data. We observed consistent improvement of our proposal Vittle in various types of visual and textual shifts, allowing users to trust the model more than before to safely use AI in variety of environments. Moreover, although we focused on the robustness perspective in this work, improved invariancesensitivity trade-off also benefits the fairness-discriminativeness trade-off, which is another crucial desideratum towards reliable AI. Meanwhile, even though its robustness to distribution shifts was improved, there are still potential misuse cases with MLLMs that can affect humanity by producing systematically biased outputs, given the existence of some adversarial data providers or attackers."
        }
    ],
    "affiliations": [
        "Department of Computer Sciences, University of Wisconsin-Madison"
    ]
}