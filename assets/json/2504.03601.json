{
    "paper_title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
    "authors": [
        "Akshara Prabhakar",
        "Zuxin Liu",
        "Weiran Yao",
        "Jianguo Zhang",
        "Ming Zhu",
        "Shiyu Wang",
        "Zhiwei Liu",
        "Tulika Awalgaonkar",
        "Haolin Chen",
        "Thai Hoang",
        "Juan Carlos Niebles",
        "Shelby Heinecke",
        "Huan Wang",
        "Silvio Savarese",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 1 0 6 3 0 . 4 0 5 2 : r APIGen-MT: Agentic PIpeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay Akshara Prabhakar Zuxin Liu Weiran Yao Jianguo Zhang Ming Zhu Shiyu Wang Zhiwei Liu Tulika Awalgaonkar Haolin Chen Thai Hoang Juan Carlos Niebles Shelby Heinecke Huan Wang Silvio Savarese Caiming Xiong Salesforce AI Research {akshara.prabhakar,zuxin.liu}@salesforce.com"
        },
        {
            "title": "Abstract",
            "content": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated humanagent interplay. We train family of modelsthe xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on τ -bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields highquality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Model Website & Dataset https://huggingface.co/Salesforce/xLAM-2 https://apigen-mt.github.io"
        },
        {
            "title": "Introduction",
            "content": "The growth of Large Language Model (LLM) agents has been accelerating at an unprecedented rate, driven by advancements in AI capabilities and increasing demand across various industries [19, 1, 9, 5, 28, 52, 21, 6, 23]. Their role has evolved beyond simple conversational chatbots to AI agents capable of executing real-world tasks, such as managing financial transactions, scheduling appointments, and handling customer service requests. These applications demand not only linguistic fluency but also precise execution, reliability, and adherence to domain-specific policies. Realistic enterprise use cases involve having an assistant (also referred to as agent in this document) that is capable of fluently conversing with humans of different personalities, incrementally understanding their intent, extracting the background details needed, accurately invoke APIs, and operate over complex business logic structure. Despite their potential, building robust and reliable AI agents presents significant challenges [47]. Recent benchmarks reveal that even advanced LLMs struggle with multi-turn interactions, particularly Equal Contribution Corresponding Authors Preprint. Figure 1: Comparative performance of larger xLAM-2-fc-r models (8B-70B, trained with APIGen-MT data) against state-of-the-art baselines on function-calling (BFCL v3 [43]) and agentic (τ -bench [47]) capabilities. when required to perform complex function calls, track long-term dependencies, or request missing information [48, 26, 44, 43, 18]. Although framework design and prompt engineering have shown promise, the underlying model capabilities remain the primary bottleneck, largely due to two fundamental obstacles: (1) the scarcity of high-quality agent interaction data in public pretraining corpora, and (2) the prohibitive cost and time required to manually collect and label such data, especially for domain-specific applications requiring specialized knowledge. Several approaches have attempted to address these challenges. APIGen [24] introduced techniques for generating single-turn function calling data, while [39] explored methods for knowledge distillation in agent training. However, these approaches primarily focus on single-turn interactions, failing to capture the complexity of real-world agent usage, where multiple turns are often required. Other efforts like [49, 48, 10], while incorporating multi-turn aspects, lack human-agent interplaycrucial for realistic data generation. The verification and synthesis of high-quality multi-turn trajectories containing both linguistic diversity and grounded actions remains largely unsolved, creating significant barrier to advancing agent capabilities. To address these limitations, we introduce APIGen-MT, an agentic data synthesis pipeline for generating high-quality multi-turn agent data. It operates in two main steps: first, data agent generates detailed and verified task \"blueprint\", and second, this blueprint guides the generation of realistic multi-turn interactions through simulated agent-human interplay (Subsection 4.2). The blueprint generation includes sampling relevant APIs, policies, domain data, and user personas to create grounded general tasks configurations, and using reverse task recombination (SubSubsection 4.1.3) to enhance complexity. These blueprints are validated through format/execution checks and an LLM committee review using reflection-based mechanism [37]. Subsequently, the validated blueprint seeds simulated interaction between human LM and an agent (e.g., gpt-4o), producing complete interaction trajectory with dialogue, actions, and environment feedback for training. The main contributions of our work are summarized as follows: We propose APIGen-MT, an agentic data synthesis pipeline that leverages environment execution feedback and review committee to ensure the high-quality of generated multi-turn agent data. We develop two-phase framework that first creates detailed task blueprints with verifiable groundtruth actions, then transforms these blueprints into realistic multi-turn conversational agent trajectories with tool-usage through simulated human-agent interplay. We train series of models across multiple architectures and scales (Llama 3.1/3.2 and Qwen 2.5 at 1B to 70B parameters), demonstrating superior performance on two popular agentic benchmarks: τ -bench and BFCL, surpassing many frontier models including gpt-4o (Figure 1). We open-source our high-quality synthetic data and trained models, i.e., the xLAM-2-fc-r series, to advance research in AI agent space."
        },
        {
            "title": "2 Related Work",
            "content": "Tool-Use Agents. Tool-use capabilities enhance LLMs by enabling interaction with external tools, extending their reasoning and functionality [42, 31, 22]. Function-calling frameworks allow LLMs to 2 parse queries, select tools, and interpret results, but often require predefined tools, limiting adaptability [24, 41]. Efforts were made to address this by creating reusable tools from scratch on the fly [8], built upon by ToolMaker [42] which leverages tools from existing code repositories. Others compose workflows or learn from demonstrations [31, 34]. Recently, several works have adopted specialized approaches for agent trainingcritique-informed planning [12], fine-tuning on selective steps [46], teasing apart reasoning from format following (Agent-FLAN) [11], and autonomously invoking tools without explicit post-training (ToRL) [20]. Interactive Conversational Benchmarks. Evaluating LLM agents in multi-turn settings requires specialized benchmarks. MultiChallenge [38] and ToolDial [36] assess agents on context maintenance and tool-augmented dialogue. InterCode [45] and CRMArena [18] evaluate iterative problem-solving and customer management. ToolSandbox [26] provides stateful, interactive benchmark for tool use. User simulations have become essential in these benchmarks, offering systematic, realistic interactions [47, 26, 29]. Our work complements these efforts by generating synthetic multi-turn conversations to train and evaluate agents in such realistic settings. Synthetic Data Generation. The scarcity of high-quality training data drives synthetic data generation. Multi-agent frameworks like MAG-V [35], AgentInstruct [27], and MATRIX [40] create realistic datasets by simulating agent interactions. Other approaches utilize instruction composition [17, 10] and multi-turn planning to produce complex dialogues [53]. Related to our effort in generating multi-turn training data, BUTTON [10] generates synthetic compositional instruction tuning data by combining 2-3 atomic tasks and conducting trajectory collection via multi-agent setup. However, this involves construction of APIs based on the task generated and lacks systematic quality control and filtering during task composition limiting data verification. MAGNET [48] proposed graph-based method to generate function signature paths which are iteratively transformed to sequence of queries and function calls. While many of these prior approaches have been tested mainly on reasoning or single-turn interaction scenarios, our framework, APIGen-MT, advances this line of work, being applicable to any existing environment by generating high-quality multi-turn data for realistic agent-human interactions, focusing on reliable tool selection and parameter generation. By systematically preparing the context, we first generate tasks adhering to any domain constraints and corresponding executable groundtruth function calls in an agentic fashion with iterative refinement via feedback loops. Further, the simulated agent-human interplay mechanism allows us to generate verifiable long interaction trajectories."
        },
        {
            "title": "3 APIGen-MT Method for Synthesizing High-Quality Multi-Turn Data",
            "content": "In this section, we present APIGen-MT, an agentic pipeline for generating multi-turn data through simulated agent-human interplay. We first formalize the multi-turn interaction problem and then describe our two-phase framework for generating high-quality, verifiable multi-turn data. 3.1 Multi-Turn Interaction Problem Formulation Multi-turn interactions between an AI assistant and human user present unique challenges that go beyond single-turn exchanges. We formalize this interaction as Partially Observable Markov Decision Process (POMDP) defined by the tuple (U, S, A, O, , R), where represents the instruction space containing possible user intents; denotes the state space of the environment and conversation history; = {tool_call, response} is the action space available to the assistant; = OE OH is the observation space comprising observations from the environment (OE) and response from the human (OH ); : is the transition function; and is the reward function evaluating interaction success. The AI assistant must engage in multi-turn conversation with the human user to incrementally understand their intent and solve it through appropriate interactions with the environment while adhering to any domain rules. At turn t, the assistant predicts an action at based on the interaction history and understanding of thus far. When at is tool_call compliant with the rules, it triggers state transition (st , oE), where oE OE is the tool output (typically in structured format like JSON). When at is response to the human, it causes state transition (st , oH ), where oH OH is the humans follow-up message. Importantly, the environment state st+1 remains latent to both the assistant and the human. The interaction completes , response) (st+1 E, tool_call) (st+ 3 Figure 2: Overview of the APIGen-MT framework. Phase 1 generates task configurations and groundtruth actions through an agentic process with feedback loops. Phase 2 collects human-agent-environment interaction trajectories by simulating realistic conversations between human user and test agent in an executable environment. when the human sends terminating message or the maximum number of turns is reached. The reward R(SE, a) is calculated based on the cumulative state change in the environment SE and the sequence of responses = {ai ai response to human} provided by the assistant throughout the episode. The assistants objective is to maximize this reward. 3.2 APIGen-MT Framework Overview Generating high-quality multi-turn data that captures the complexities of agent-human interactions presents significant challenges. Directly synthesizing multi-turn conversations in one shot is difficult for two key reasons: (1) single error or hallucination in any intermediate step can lead to complete failure, and (2) the content of each turn depends on previous function calls and their outputs, creating complex dependencies that are difficult to maintain consistently. To address these challenges, we introduce APIGen-MT, two-phase framework for generating verifiable and diverse multi-turn data (Figure 2). Our approach extends the APIGen framework [24] by adding an agentic feedback loop and simulated human-agent interplay to generate realistic multi-turn conversations. The core insight of our approach is to separate the task generation process into two distinct phases: first creating detailed \"blueprint\" of the task (Phase 1), and then using this blueprint to guide the generation of realistic multi-turn interactions that fill in the conversational details (Phase 2). This separation allows us to ensure both the correctness of the underlying task structure and the naturalness of the resulting conversations. 3.2.1 Phase 1: Task Configuration and Groundtruth Generation The initial phase of APIGen-MT focuses on systematically generating well-defined task configurations, each comprising user instruction (q), corresponding sequence of verifiable groundtruth actions (agt), and the expected final outputs (ogt). This phase establishes solid, verifiable foundation for each interaction scenario before the complexities of conversational dynamics are introduced. As 4 depicted in Figure 2, this is achieved through an agentic workflow incorporating multi-stage validation and refinement loops. More specifically, it has the following steps: 1. Context Preparation: Relevant information such as available APIs, domain-specific rules or policies, and reference data is assembled. This context grounds the subsequent generation step in the specific constraints and capabilities of the target environment. 2. LLM-based Data Generator: An LLM utilizes the prepared context to propose initial task configurations. Each configuration consists of: detailed user instruction describing the high-level intent. sequence of groundtruth actions agt required to fulfill the intent. Expected final outputs ogt to be provided to the user. 3. Format & Execution Checker: Proposed configurations undergo automated technical validation. This component performs multiple checks: Verifies the structural correctness of generated actions (e.g., valid API call formats) and outputs. Confirms the executability of each action in agt within simulated target environment (checking API names, arguments, types). 4. Review Committee: Configurations passing rule-based checks proceed to semantic evaluation by committee of multiple LLM reviewers. This committee assesses quality aspects like the coherence between and agt, completeness, and overall task sensibility. We use majority voting to achieve more stable assessment. 5. Feedback Generation and Refinement: If task fails at either the validation (Step 3) or review (Step 4) stage, Feedback Generator aggregates failure reasons and reviews, reflects upon them, and produces summarized improvement plan. This plan guides the Data Generator (Step 2) in refining the task proposal in subsequent iteration. Successfully validated tasks exit this loop. This agentic design with feedback loops is crucial for generating high-quality tasks efficiently. By incorporating reflection and improvement based on validation results, the system can learn from failures and progressively generate better tasks. 3.2.2 Phase 2: Human-Agent-Environment Interaction Trajectory Collection Building upon the validated task configurations q, agt, ogt from Phase 1, the second phase generates realistic multi-turn interaction data by simulating dynamic conversations between an LLM-based human user and test agent operating within an executable environment. Guided by the task instruction and often specific persona, the simulated human naturally reveals information or subgoals incrementally, while the agent interprets the evolving context, interacts with the environment via API calls when needed, and responds coherently. Importantly, the simulated user is unaware of the underlying environment and available APIs mimicking real-world user. The simulation produces complete interaction trajectories that capture dialogue turns, agent actions, and environment responses. Each trajectory is validated by comparing its outcome against the groundtruth actions (agt) and expected outputs (ogt) from Phase 1. Only those trajectories that verifiably achieve the task using both state-based and output-based checks are accepted into the dataset, ensuring that interactions are both dynamically plausible and grounded in correct solution. This two-phase design offers several benefits. First, it provides verifiability by grounding interaction data in pre-validated task configurations. Second, it enhances realism by focusing the simulation on natural turn-by-turn dynamics without the simultaneous burden of task solution generation. Lastly, the modular approach isolates issues in task design from those in conversational modeling, facilitating debugging and scalability across diverse interaction patterns. In essence, by integrating agentic generation of verifiable task \"blueprint\" with realistic simulation of conversational dynamics, APIGen-MT produces high-quality, multi-turn interaction data that balances structural correctness with the naturalness required for training agent models."
        },
        {
            "title": "4 A Case Study of APIGen-MT on τ -bench",
            "content": "This section details the instantiation of the APIGen-MT framework (Subsection 3.2) with τ -bench [47]. Generating high-quality, multi-turn interaction data with nuanced human-agent dynamics presents challenges, as direct conversation simulation often leads to inconsistencies or task deviations. 5 Figure 3: Realization of APIGen-MT framework for τ -bench. We first generate realistic task instances by random walk down the API graph and sampling. Next the tasks are validated following multi-stage pipeline. Instances which fail are sent back to the Generator to be refined based on the validation feedback. Finally, trajectories are generated by simulated human user that interacts with test agent by supplying the query details in turn-wise manner. Trajectories which pass stateand outputbased evaluations are collected. Therefore, our two-phase approach addresses this by first synthesizing detail task configurations that define the users high-level intent (q), groundtruth actions (agt), and the expected final outputs (ogt). By establishing this verifiable \"blueprint\" first (Phase 1), we can then more reliably simulate the fine-grained, turn-by-turn interaction dynamics between human and an agent within the executable environment (Phase 2), ensuring the collected trajectories are both realistic and grounded in verifiable solution path. τ -bench, with its realistic domains, executable APIs, and specific policies, provides an ideal testbed for this methodology. Figure 3 illustrates this specific implementation. 4.1 Phase 1 Implementation: Task Configuration Generation and Validation 4.1.1 API Dependency Graph and Context Samplers Generating realistic tasks for τ -bench requires navigating its specific APIs, policies, and data structures. We implemented the following techniques for task generation and validation. API Graph Modeling. We model the available APIs in each τ -bench domain as directed graph, where nodes represent APIs and edges represent dependencies between them. An edge exists from API to API if Bs input arguments can depend on As output and the co-occurrence of this tool-call pair is permitted under domain policies. This graph-based approach enables us to generate realistic task sequences by performing random walks through the API dependency graph. Specialized Context Samplers. To ensure task diversity, realism, and grounding, we utilize several domain-specific samplers that provide context to the LLM-based task generator. API Sampler: We distinguish between state-exploring (read) APIs and state-changing (write) APIs which can modify the environment states. The generator focuses on sampling the necessary write APIs to form the core of agt, allowing flexibility in how read APIs might be used during the subsequent interaction phase. This approach encourages exploration while ensuring that specific state-changing actions are included in the groundtruth. Policy Sampler: For each τ -bench domain, we extract and sample from the domain-specific policies and rules. These policies are incorporated into the task generation process to ensure compliance of real-world use cases. Task complexity is influenced by the number of write calls and the associated policy constraints. Domain Data Sampler: To ground tasks in realistic domain data without exceeding context limits, we sample domain-specific data with additional metadata (e.g., cost, time, attributes). This metadata enhances coverage and enables more creative and diverse task scenarios. Persona Sampler: We incorporate user persona descriptions from PersonaHub [16] to inform the user intent and inject realistic human qualities and situational context, enhancing diversity for subsequent Phase 2 human-agent interaction simulation. Example Sampler: We provide few-shot examples of well-formed tasks relevant to the sampled APIs, guiding the generator on structure and format. 6 For each task generation iteration, we randomly vary the sampling frequency for each sampler to enhance diversity and prevent repetitive scenarios. The sampled information is compiled into prompt instructing the LLM generator to produce <thought> (its reasoning process), the user <instruction> (q), the corresponding groundtruth <actions> (agt), and the expected final <outputs> (ogt). 4.1.2 Multi-Stage Validation for τ -bench We implement rigorous three-stage validation process for the τ -bench environment: Stage 1: Action Validation. Format Check: Verifies the presence and basic structure of required task components tool calls in (<thought>, <instruction>, <actions>, <outputs>) and ensures all <actions> are valid JSON and outputs in <outputs> are strings. Execution Check: Simulates each action in agt within the τ -bench environment, validating API names, argument names, and data types. The cumulative effect on the environment state (SE) is captured as diff_patch, similar to git diff. Policy Compliance Check: Leverages the executable nature of τ -bench by translating domain policies into Python unit tests. These tests run against the simulated execution trace of agt to detect violations, especially those arising from interactions between multiple actions (e.g., action is invalid given the state change caused by prior action A). Failures yield detailed feedback on the specific policy violation. Stage 2: Alignment Validation. Tasks successfully passing Stage 1s action validation are then assessed for semantic alignment. Specifically, we evaluate whether the groundtruth actions (agt), as reflected by their environmental effects summarized in the diff_patch, accurately and comprehensively fulfill the users intent expressed in the instruction (q). To mitigate the potential biases and inconsistencies of single evaluator, we employ committee of diverse LLM judges [52, 7]. These judges review each task based on systematic rubric with metrics such as Correctness, Completeness, Satisfaction, and Creativity (refer Figure 9 in Appendix for details). Each judge provides scores and qualitative feedback. We utilize majority voting strategy across the committees judgments to determine the final assessment for each metric and the overall task quality. This approach yields more stable and reliable evaluation results compared to single-judge assessments. Stage 3: Final Semantic Review & Refinement. Based on the aggregated scores from the committee (determined via majority voting), tasks achieving an average score above predefined threshold are accepted and added to the pool of validated task configurations. Tasks that fail this review trigger the feedback loop mechanism. Consolidated feedback, summarizing the points raised by the committee majority, is sent back to the LLM task generator. This initiates reflection process [37], guiding the generator to revise the task in the subsequent iteration to address the identified shortcomings. 4.1.3 Reverse Task Recombination for Complex Task Construction While the iterative refinement process improves task quality and efficiency, directly generating complex, long-horizon tasks involving multiple steps remains challenging. Validation failures can occur due to subtle policy conflicts or difficulties in ensuring perfect alignment across many steps. To overcome this and systematically construct more complicated scenarios, we implement Reverse Task Recombination, technique that leverages the principle of compositionality [10, 17], similar to modular design in software engineering. The core idea is to build complex tasks from simpler, independently validated \"building blocks\": 1. Select Validated Tasks: Identify multiple simpler tasks (T1, T2, ...) that have successfully passed all validation stages (Stages 1-3) and are associated with the same user persona. 2. Concatenate Components: Combine their respective groundtruth actions (acombined = agt,1 agt,2 ...) and expected outputs (ocombined = ogt,1 ogt,2 ..., where denotes action sequence concatenation and denotes output aggregation). 7 3. Re-Check Policy Compliance: Rerun the Policy Check on acombined to ensure that the cumulative action sequence remains logically sound and adheres to the domain rules as combinations could cause conflicting actions to appear together, for e.g., returning and canceling the same order. 4. Synthesize Combined Instruction: Instruct the LLM generator to create new, coherent, overarching user instruction (qcombined) that logically integrates the goals and steps represented by acombined and ocombined. This new instruction should frame the combined actions as single, more complex user request."
        },
        {
            "title": "Submit",
            "content": "5. Re-Validate Semantics: the newly formed complex task Tcombined = {qcombined, acombined, ocombined} for validation starting from Stage 2 (Alignment Validation). Stage 1 (Action Validation) can be safely skipped for acombined because each constituent action sequence (agt,1, agt,2, ...) has already been individually checked for format and execution within its original context, and policy compliance in the current context. Stage 3 (Final Semantic Review) proceeds based on the outcome of Stage 2 for the combined task. This method allows for the scalable generation of complex, multi-step tasks with greater reliability, as it builds upon verified components while focusing the validation effort on the semantic coherence and alignment of the combined whole. 4.2 Phase 2: Simulated Human-Agent Interplay and Trajectory Collection Building on the verified tasks from Phase 1which include detailed user intent q, groundtruth actions agt, and expected outputs ogtwe simulate multi-turn interaction trajectories between an agent (A) and human user (H) modeled by an LLM. Guided by the instruction and an associated persona, the simulated human incrementally reveals task details to mimic realistic interactions. The agent, instantiated as gpt-4o with its function-calling mode, interprets the evolving intent and executes the necessary actions to complete the task. Trajectory Collection. We employ rejection sampling to ensure that only trajectories achieving the task goal (r = 1) are retained. Success is determined by comparing the final environment state to agt and the agents final responses to ogt. For enhanced data coverage, each task is attempted up to three times, and the union of all unique successful trajectories is compiled into an offline dataset suitable for downstream applications such as behavioral cloning. Stabilizing Simulated Human. critical challenge in this phase is maintaining the stability and fidelity of the simulated human. Over multiple conversational turns, the human LLM may drift from the original instruction or be unduly influenced by the agents responses [30], introducing variability that hinders reliable evaluation [47]. To address this, we adopt Best-of-N (N=4) sampling strategy in combination with self-critique mechanism for the human LLMs responses (see Figure 12 in Appendix for details), allowing it to adhere to the task instruction more accurately and not be mislead by the test agent responses. Its effectiveness was validated on the τ -bench test set, where improved consistency in agent performance evaluation across multiple trials was observed  (Table 3)  . 4.3 Data Collection & Statistics Data Collection Procedure. We source APIs implemented as Python functions from τ -bench. Among these, we have 15 read and 13 write APIs across both domains. τ -bench is accompanied with detailed policies and domain rules in two settings - Retail and Airline which we use as guideline policies. We utilize gpt-4o and DeepSeek V3 and R1 models in the task generation, validation and agent-human interplay stages to collect training data. The prompts used in every stage are provided in Appendix B. We set the maximum number of reflection-based feedback turns to 3 for retail and 5 for airline respectively. Statistics. summary of the data collection is shown in Figure 4. Figure 5 shows that we can efficiently collect long trajectories requiring strong model like gpt-4o to take an average 12 turns to complete the task using APIGen-MT. Our agentic pipeline involving review committee and iterative refinement via reflection provides 2.5x boost to the task collection success rate to attain 70%. Our implementation demonstrates that the APIGen-MT framework can successfully generate highquality multi-turn data for complex domains with strict policy constraints. The two-phase approach Metric Task Config. S.R. (Phase 1) Task Config. S.R. w/o Agentic Feedback Trajectory Sim. S.R. (Phase 2) Total Validated Trajectories Min. Turns per Trajectory Max. Turns per Trajectory Avg. Tool Calls per Trajectory Avg. User Turns per Trajectory Value 70% 28% 67% 3,820 1 29 7 Figure 4: Statistics for the dataset generated using APIGen-MT. Success rates (S.R.) are reported for the task configuration (w. and w/o agentic feedback in Phase 1) and trajectory simulation (Phase 2) stages. Figure 5: Density distribution of assistant and user turns in collected trajectories. with agentic feedback loops and simulated human-agent interplay proves effective in creating diverse, realistic, and verifiable datasets for training and evaluating conversational agents."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Training Details. We perform filtered Behavioral Cloning (BC) using the collected trajectories with Llama 3.1/3.2 Instruct models [15] and Qwen 2.5 Instruct models [32]. The collected trajectories are split at every assistant response and we train to predict only the assistant response tokens by masking the prompt and other messages. To enhance the dataset diversity, we also jointly train our xLAM-2-fc-r models with function-calling data from [24] and other domains of agentic data from [50, 51]. We utilize the LLama-Factory library [54] and perform full-finetuning using DeepSpeed ZeRO [33] stage 3, Flash Attention 2 [14] in bfloat16 precision with AdamW optimizer [25] and train for at most 3 epochs on NVIDIA H200 node. Benchmarks. We evaluate on two challenging benchmarks designed specifically for assessing agent capabilities (1) BFCL v3 [43], leading benchmark for tool-use evaluation, specifically designed to assess LLMs function calling capabilities and (2) τ -bench [47], comprehensive benchmark for evaluating AI agents in realistic scenarios. More details are in Appendix A. Both are particularly well-suited for evaluating the effectiveness of our APIGen-MT approach, as they focus on multi-turn interactions and tool use capabilities, which are central to our data generation methodology. 5.2 Experiment Results We compare the performance of our trained models (xLAM-2-fc-r) against state-of-the-art proprietary models such as gpt models (o1, gpt-4o); claude models (claude-3.5-haiku, claude-3.5-sonnet, claude-3.5-sonnet (new), and claude-3.7-sonnet), and open-source LLMs including DeepSeek v3, and the baselines Llama 70B and Qwen 32B. BFCL v3 Results. On the BFCL v3 benchmark, our models demonstrate exceptional performance. As shown in Table 1, xLAM-2-70b-fc-r and xLAM-2-32b-fc-r achieve the top 2 positions on the leaderboard with overall accuracies of 78.19% and 75.83% respectively, surpassing all proprietary and open-source models. The most striking advantage appears in multi-turn scenarios, where our models excel across all parameter scales. xLAM-2-70b-fc-r achieves 75.12% multi-turn accuracy, while our smaller models show remarkable capabilities with xLAM-2-8b-fc-r at 69.25%, xLAM-2-3b-fc-r at 56.00%, and even xLAM-2-1b-fc-r at 43.12% - all substantially outperforming o1 (36%) and gpt-4o in function-calling mode (41%). Additionally, our models demonstrate strong hallucination detection, with xLAM-2-3b-fc-r achieving 94.44% on relevance detection, matching the best score in this category. 9 Table 1: Performance comparison of different models on BFCL leaderboard (as of date 04/03/2025). The rank is based on the overall accuracy, which is weighted average of different evaluation categories. FC\" stands for function-calling mode in contrast to using customized prompt\" to extract the function calls. See the benchmark [43] for details. Rank Overall Acc Model Single-Turn Multi-Turn Hallucination Non-live (AST) Non-live (Exec) Live (AST) Overall Acc Relevance Irrelevance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 . . . 34 35 36 37 38 78.19 75.83 74.31 72.83 72.08 69.94 69.58 68.39 67.98 67.88 67.87 67.72 65.12 65.11 64.1 64.1 . . . 58.93 58.9 58.90 58.55 58.45 58.42 xLAM-2-70b-fc-r (FC) xLAM-2-32b-fc-r (FC) watt-tool-70b (FC) xLAM-2-8b-fc-r (FC) GPT-4o-2024-11-20 (Prompt) GPT-4.5-Preview-02-27 (FC) GPT-4o-2024-11-20 (FC) ToolACE-2-8B (FC) watt-tool-8B (FC) GPT-4-2024-04-09 (FC) o1-2024-12-17 (Prompt) BitAgent-8B o3-mini-25-01-31 (Prompt) xLAM-2-3b-fc-r (FC) CoALM-405B GPT-4o-mini-24-07-18 (FC) ... Gemini-2-Flash-Thinking Qwen2.5-14B-Instruct (FC) xLAM-2-1b-fc-r (FC) DeepSeek-V3 (FC) mistral-large-2407 (FC) ToolACE-8B (FC) 88.48 89.50 84.06 84.35 88.1 86.12 87.42 87.58 86.56 84.73 85.67 86.92 86.15 82.94 90.58 85.21 87.4 85.42 76.23 89.17 86.81 87.54 85.98 86.48 89.39 85.59 89.38 83.98 89.2 87.11 89.34 85.21 87.45 89.52 89.46 81.88 89.07 83.57 87.07 84.86 74.86 92.32 84.38 89.21 72.63 73.79 77.74 66.73 79.83 79.34 79.65 80.05 76.5 80.5 80.63 76.14 79.08 58.69 74.5 74.41 . . . 75.97 76.68 59.88 68.41 69.88 78.59 75.12 66.38 58.75 69.25 47.62 45.25 41 36.88 39.12 38.12 36 38.5 28.75 56.00 28.75 34.12 14.5 15.88 43.12 18.62 23.75 7.75 66.67 83.33 94.44 83.33 83.33 66.67 83.33 72.22 83.33 72.22 72.22 83.33 72.22 94.44 100 83.33 77.78 55.56 88.89 88.89 72.22 83.33 78.74 76.25 76.32 64.11 83.76 83.64 83.15 90.11 83.15 83.81 87.78 82.38 82.96 57.94 71.79 74. 72.75 77.69 56.87 59.36 52.85 87.88 τ -bench Results. Table 2 shows performance under the default naive user simulation setting on τ -bench. Our xLAM-2-70b-fc-r model achieves an overall success rate of 56.2%, significantly outperforming the base Llama 3.1 70B Instruct model (38.2%) and other open-source models like DeepSeek v3 (40.6%). Notably, our model even outperforms proprietary models such as gpt-4o (52.9%) and approaches the performance of more recent models like Claude 3.5 Sonnet (new) (60.1%). Table 2: Success Rate (pass@1) of various open-source and proprietary models on the Retail and Airline settings of τ -bench (averaged across at least 5 trials). The xLAM-2-fc-r models are trained on the data generated using APIGen-MT. Overall indicates the average score across both domains. 1 indicates results from [13]; 2 indicates results from [2]; 3 indicate results from [3]; 4 indicates from [4]. Note. We evaluate only with the benchmarks think tool and no prompt optimizations. Model τ -Retail τ -Airline Overall Open-Source Models Qwen 2.5 32B Instruct Llama 3.1 70B Instruct DeepSeek v31 xLAM-2-70b-fc-r xLAM-2-32b-fc-r xLAM-2-8b-fc-r xLAM-2-3b-fc-r xLAM-2-1b-fc-r 24.4 50.4 58.3 67.1 64.3 58.2 44.4 22.5 Proprietary Models Gemini 1.5 pro1 gpt-4o-2024-11-20 o13 Claude 3.5 Haiku2 Calude 3.5 Sonnet2 Claude 3.5 Sonnet (new)3 Claude 3.7 Sonnet4 Claude 3.7 Sonnet + optimized prompt 4 54.9 62.8 73.5 51.0 62.6 71.5 78.3 81. 10 25.0 26.0 22.8 45.2 45.0 35.2 32.0 21.0 25.2 43.0 54.2 22.8 36.0 48.8 41.2 58.4 24.7 38.2 40.6 56.2 54.6 46.7 38.2 21.8 40.1 52.9 63.9 36.9 49.3 60.1 59.8 69.8 particularly impressive result is that our smaller models, such as xLAM-2-32b-fc-r and xLAM-2-8b-fc-r, achieve remarkable performance (54.6% and 46.7% respectively), outperforming much larger baseline models. This suggests that our synthetic data generation approach enables efficient knowledge transfer, allowing smaller models to achieve competitive performance with significantly fewer parameters. These results across both benchmarks demonstrate that our APIGen-MT approach for generating synthetic multi-turn data through simulated agent-human interplay is highly effective. Models trained on this data consistently outperform open-source baselines and on par with proprietary models, with particularly strong performance in multi-turn scenarios. Importantly, our approach enables smaller models to achieve competitive or superior performance compared to much larger models, highlighting the efficiency and effectiveness of our data generation methodology. 5.3 Consistency & Stability Experiments We plot the passˆk curves [47] in Figure 6 on τ -bench in the default naive user LM setting. passˆk is defined as the chance of all i.i.d. task trials being successful, averaged across all tasks. As increases, we see less drop in success rate (SR) for our models. Notably on the more complex airline domain, xLAM-2-70b-fc-r has higher passˆ5 score than Claude, despite having slightly lower passˆ1 suggesting higher reliability and consistency across multiple trials. This is critical property for deployment in real-world applications, where consistent performance is essential. Figure 6: Passˆk curves measuring the probability that all 5 independent trials succeed for given task, averaged across all tasks for τ -retail (left) and τ -airline (right) domains. Higher value indicates consistency of the models. Next, we adopt the BoN user LM setting (introduced in Subsection 4.2) to assess its effectiveness in producing more stable results across trials. Although this enhancement is applied to the user LM, Table 3 highlights the improved success rate and reduced variance in models utilizing the BoN user simulation. This suggests that enhancing the user simulation strategy with simple self-critiquing mechanism can not only increase stability but also improve agent performance. Table 3: The Success Rate (SR) measured across 5 trials on the Retail domain of τ -bench using gpt-4o and xLAM-2-70b-fc-r as the test assistants. The average success rate is higher with lower variance using BoN based user simulation, indicative of more stable evaluation. Model (User LM setting) t1 t2 t3 t4 SR Average SR Variance gpt-4o (Naive) gpt-4o (BoN) xLAM-2-70b-fc-r (Naive) xLAM-2-70b-fc-r (BoN) 61.7 65.2 69.6 66.9 57.4 69.6 65.2 71.3 65.2 67.0 62.6 68. 65.2 66.1 68.7 66.9 64.4 67.0 69.6 70.4 62.8 67.0 67.1 68.8 11.1 2.6 9.7 4.0 5.4 In-Depth Analysis of Model Behavior To better understand the behavior of our trained models, we perform an in-depth investigation of the tasks solved by xLAM-2-70b-fc-r and state-of-the-art model Claude 3.5 Sonnet (new) on τ -bench. We categorize tasks into short, medium and long based on the number of turns required by 11 Claude 3.5 to solve each task across union of 8 trials. This categorization is derived by calculating the 33rd and 66th percentiles of the number of turns. From Figure 7 we observe that particularly on the long task category, the success rate for xLAM-2-70b-fc-r is much higher than gpt-4o but lags behind Claude. Further, we assess the efficiency of the agent by measuring the number of interactions needed with the simulated user for the agent to fully comprehend the intent and successfully complete the task. The plot reveals that xLAM-2-70b-fc-r is at par with gpt-4o but requires more interactions compared to Claude, which can be attributed to its method of retrieving user details in stages, necessitating more turns. These observations suggest potential areas for improvement in future iterations."
        },
        {
            "title": "6 Discussion",
            "content": "Figure 7: Performance and efficiency comparisons of xLAM-2-70b-fc-r with frontier models on τ -bench. Conclusion. We introduced APIGen-MT, two-phase framework for generating high-quality multiturn agent data through simulated human-agent interactions. By decoupling the creation of detailed task blueprints from the simulation of conversational trajectories, our approach ensures both structural correctness and natural dialogue dynamics. Experiments on τ -bench and BFCL v3 demonstrate that models trained on our synthetic data outperform existing baselines, with even smaller models showing competitive performance in multi-turn scenarios. Moreover, our stabilization techniques yield more consistent and reliable agent behavior. By open-sourcing our synthetic data and trained models, we aim to foster further advances in AI agent development. Limitations and future directions. Despite its advantages, APIGen-MT has limitations that present opportunities for future research. First, while our Best-of-N sampling and self-critique mechanisms reduce human user simulation variance, some stochasticity in human behavior remains; more deterministic simulation methods or refined filtering metrics could further stabilize the process. Second, our current approach discards failed trajectories in the second phase, yet these cases may offer valuable insights; future work could leverage such failures as additional contrastive signal during model training. Third, the multi-stage validation process, though effective, incurs computational overhead; developing more efficient validation or adaptive sampling strategies could improve scalability. Finally, extending our approach to additional domains and incorporating self-improvement through reinforcement learning are promising directions for future work."
        },
        {
            "title": "References",
            "content": "[1] S. Agashe, J. Han, S. Gan, J. Yang, A. Li, and X. E. Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. [2] Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ 3-5-models-and-computer-use. [3] Anthropic. Claude 3.7 sonnet, 2025. URL https://www.anthropic.com/news/ claude-3-7-sonnet. [4] Anthropic. Claude think tool, 2025. URL https://www.anthropic.com/engineering/ claude-think-tool. [5] A. Antoniades, A. Örwall, K. Zhang, Y. Xie, A. Goyal, and W. Wang. Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement. arXiv preprint arXiv:2410.20285, 2024. [6] D. Bahdanau, N. Gontier, G. Huang, E. Kamalloo, R. Pardinas, A. Piché, T. Scholak, O. Shliazhko, J. P. Tremblay, K. Ghanem, et al. Tapeagents: holistic framework for agent development and optimization. arXiv preprint arXiv:2412.08445, 2024. 12 [7] Z. Bi, K. Han, C. Liu, Y. Tang, and Y. Wang. Forest-of-thought: Scaling test-time compute for enhancing llm reasoning. arXiv preprint arXiv:2412.09078, 2024. [8] T. Cai, X. Wang, T. Ma, X. Chen, and D. Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023. [9] CAMEL-AI.org. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. https://github.com/camel-ai/owl, 2025. Accessed: 202503-07. [10] M. Chen, sunhaoze, T. Li, F. Yang, H. Liang, KeerLu, B. CUI, W. Zhang, Z. Zhou, and weipeng chen. Facilitating multi-turn function calling for LLMs via compositional instruction tuning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=owP2mymrTD. [11] Z. Chen, K. Liu, Q. Wang, W. Zhang, J. Liu, D. Lin, K. Chen, and F. Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024. [12] Z. Chen, M. Li, Y. Huang, Y. Du, M. Fang, and T. Zhou. Atlas: Agent tuning via learning critical steps. arXiv preprint arXiv:2503.02197, 2025. [13] S. Cognition. Apt-1 blog, 2025. URL https://www.scaledcognition.com/blog/apt-1. [14] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL https://arxiv.org/abs/2307.08691. [15] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] T. Ge, X. Chan, X. Wang, D. Yu, H. Mi, and D. Yu. Scaling synthetic data creation with 1,000,000,000 personas, 2024. URL https://arxiv.org/abs/2406.20094. [17] S. A. Hayati, T. Jung, T. Bodding-Long, S. Kar, A. Sethy, J.-K. Kim, and D. Kang. Chainof-instructions: Compositional instruction tuning on large language models. arXiv preprint arXiv:2402.11532, 2024. [18] K.-H. Huang, A. Prabhakar, S. Dhawan, Y. Mao, H. Wang, S. Savarese, C. Xiong, P. Laban, and C.-S. Wu. Crmarena: Understanding the capacity of llm agents to perform professional crm tasks in realistic environments, 2025. URL https://arxiv.org/abs/2411.02305. [19] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [20] X. Li, H. Zou, and P. Liu. Torl: Scaling tool-integrated rl, 2025. URL https://arxiv.org/ abs/2503.23383. [21] Y. Li, Y. Li, X. Wang, Y. Jiang, Z. Zhang, X. Zheng, H. Wang, H.-T. Zheng, P. Xie, P. S. Yu, et al. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. arXiv preprint arXiv:2411.02937, 2024. [22] W. Liu, X. Huang, X. Zeng, X. Hao, S. Yu, D. Li, S. Wang, W. Gan, Z. Liu, Y. Yu, et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2024. [23] Z. Liu, J. Zhang, K. Asadi, Y. Liu, D. Zhao, S. Sabach, and R. Fakoor. Tail: Task-specific adapters for imitation learning with large pretrained models. arXiv preprint arXiv:2310.05905, 2023. [24] Z. Liu, T. Hoang, J. Zhang, M. Zhu, T. Lan, J. Tan, W. Yao, Z. Liu, Y. Feng, R. RN, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482, 2024. [25] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 13 [26] J. Lu, T. Holleis, Y. Zhang, B. Aumayer, F. Nan, F. Bai, S. Ma, S. Ma, M. Li, G. Yin, et al. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682, 2024. [27] A. Mitra, S. Patel, T. Chakrabarty, and C. Baral. Agentinstruct: An agentic framework for generating high-quality synthetic instruction data. arXiv preprint arXiv:2402.12360, 2024. [28] J. Pan, X. Wang, G. Neubig, N. Jaitly, H. Ji, A. Suhr, and Y. Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. [29] J. Pan, R. Shar, J. Pfau, A. Talwalkar, H. He, and V. Chen. When benchmarks talk: Re-evaluating code llms with interactive feedback, 2025. URL https://arxiv.org/abs/2502.18413. [30] J. S. Park, J. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [31] Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, X. Zhou, Y. Huang, C. Xiao, et al. Tool learning with foundation models. ACM Computing Surveys, 57(4):140, 2024. [32] Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [33] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 20, page 35053506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/ 3394486.3406703. [34] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [35] S. Sengupta, K. Curtis, A. Mallipeddi, A. Mathur, J. Ross, and L. Gou. Mag-v: multi-agent framework for synthetic data generation and verification. arXiv preprint arXiv:2412.04494, 2024. [36] J. Shim, G. Seo, C. Lim, and Y. Jo. Tooldial: Multi-turn dialogue generation method for tool-augmented language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=J1J5eGJsKZ. [37] N. Shinn, F. Cassano, A. Gopinath, K. R. Narasimhan, and S. Yao. Reflexion: language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=vAElhFcKW6. [38] V. Sirdeshmukh, K. Deshpande, J. Mols, L. Jin, E.-Y. Cardona, D. Lee, J. Kritz, W. Primack, S. Yue, and C. Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. arXiv preprint arXiv:2501.17399, 2025. [39] H. Su, R. Sun, J. Yoon, P. Yin, T. Yu, and S. Ö. Arık. Learn-by-interact: data-centric framework for self-adaptive agents in realistic environments. arXiv preprint arXiv:2501.10893, 2025. [40] S. Tang, X. Pang, Z. Liu, B. Tang, R. Ye, X. Dong, Y. Wang, and S. Chen. Synthesizing post-training data for llms through multi-agent simulation. arXiv preprint arXiv:2410.14251, 2024. [41] J. Wang, J. Zhou, M. Wen, X. Mo, H. Zhang, Q. Lin, C. Jin, X. Wang, W. Zhang, and Q. Peng. Hammerbench: Fine-grained function-calling evaluation in real mobile device scenarios. arXiv preprint arXiv:2412.16516, 2024. 14 [42] G. Wölflein, D. Ferber, D. Truhn, O. Arandjelovic, and J. N. Kather. Llm agents making agent tools. arXiv preprint arXiv:2502.11705, 2025. [43] F. Yan, H. Mao, C. C.-J. Ji, T. Zhang, S. G. Patil, I. Stoica, and J. E. Gonzalez. Berkeley function calling leaderboard. 2024. [44] J. Yang, A. Prabhakar, S. Yao, K. Pei, and K. R. Narasimhan. Language agents as hackers: Evaluating cybersecurity skills with capture the flag. In Multi-Agent Security Workshop @ NeurIPS23, 2023. URL https://openreview.net/forum?id=KOZwk7BFc3. [45] J. Yang, A. Prabhakar, K. Narasimhan, and S. Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. Advances in Neural Information Processing Systems, 36, 2024. [46] R. Yang, F. Ye, J. Li, S. Yuan, Y. Zhang, Z. Tu, X. Li, and D. Yang. The lighthouse of language: Enhancing llm agents via critique-guided improvement. arXiv preprint arXiv:2503.16024, 2025. [47] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan. Tau-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. [48] F. Yin, Z. Wang, I.-H. Hsu, J. Yan, K. Jiang, Y. Chen, J. Gu, L. T. Le, K.-W. Chang, C.-Y. Lee, H. Palangi, and T. Pfister. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation, 2025. URL https://arxiv.org/abs/2503.07826. [49] Y. Zeng, X. Ding, Y. Wang, W. Liu, W. Ning, Y. Hou, X. Huang, B. Qin, and T. Liu. Boosting tool use of large language models via iterative reinforced fine-tuning. arXiv preprint arXiv:2501.09766, 2025. [50] J. Zhang, T. Lan, M. Zhu, Z. Liu, T. Hoang, S. Kokane, W. Yao, J. Tan, A. Prabhakar, H. Chen, et al. xlam: family of large action models to empower ai agent systems. arXiv preprint arXiv:2409.03215, 2024. [51] J. Zhang, T. Hoang, M. Zhu, Z. Liu, S. Wang, T. Awalgaonkar, A. Prabhakar, H. Chen, W. Yao, Z. Liu, et al. Actionstudio: lightweight framework for data and training of action models. arXiv preprint arXiv:2503.22673, 2025. [52] K. Zhang, W. Yao, Z. Liu, Y. Feng, Z. Liu, R. Rithesh, T. Lan, L. Li, R. Lou, J. Xu, et al. Diversity empowers intelligence: Integrating expertise of software engineering agents. In The Thirteenth International Conference on Learning Representations, 2024. [53] Y. Zhang, J. Lu, and N. Jaitly. Probing the multi-turn planning capabilities of LLMs via 20 question games. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14951516, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.82. URL https://aclanthology.org/2024.acl-long. 82/. [54] Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/ 2403.13372."
        },
        {
            "title": "A Benchmarks Description",
            "content": "BFCL v3: It introduces comprehensive evaluation across single-turn, multi-turn, and multistep function calling scenarios. BFCL v3 evaluates models on their ability to understand user requests, select appropriate functions, generate valid parameters, and interpret function outputs across multiple interaction turns. The benchmark uses weighted average of different evaluation categories to provide an overall accuracy score. 15 τ -bench: It measures an agents ability to interact with simulated human users (powered by language models) and programmatic APIs while following domain-specific policies. τ -bench emulates dynamic conversations across multiple domains, including retail and airline customer service, requiring agents to maintain context across turns, understand user intents, and follow complex domain-specific rules. The benchmark emphasizes the importance of multi-turn interactions and policy adherence in real-world applications."
        },
        {
            "title": "B Prompts",
            "content": "The prompts used across the various stages of APIGen-MT implemented for τ -bench are shown here Task Configuration Generation (Figure 8), Alignment Validation (Figure 9), Final Semantic Review (Figure 10), Trajectory Collection (Figure 11), Stabilized Human Simulation (Figure 12). Task Configuration Generation Prompt ## Instructions Generate task instruction that mimics realistic human users and their intentions, such as with different personality and goals. The task instruction should be followed by actions which is list of the tool_calls to be taken to solve this task and outputs which is list of the answers to specific information requests made by the user. Think step by step to come up with the action(s) and the corresponding tool_call(s) translating this thought that would be necessary to fulfill the users request or solve their intentions. Focus on common retail scenarios following the provided task instruction guidelines. ## Guidelines for Generating Task Instruction (q) {task_rules + domain_rules} ### User Data {sampled_user_details} ### Order Data {sampled_orders} ## Guidelines for generating Groundtruth Actions (agt) 1. The main focus is to generate actions that can modify the underlying database. 2. For actions that do not modify the database like specific information requests, scan the provided User Data directly and append only the answer in outputs (ogt). Do not make separate tool calls for this in actions. Include multiple tool calls when the scenario requires multiple steps or modifications. 3. 4. Provide precise tool calls with all necessary parameters for each action. 5. Ensure all actions adhere to retail policies and common sense practices. ## Tools The available tool combination in Python format is as follows: {sampled_tools} ## Output Format Generate your response according to the following format. Enclose the thought process within <thought></thought> tags, and the final structured response within <answer></answer> tags. The structured response should be in strict JSON format, without any additional comments or explanations. ## Example Tasks {example} Do not directly copy instruction and the action patterns from the examples. Ground the generation from the above provided data. Generate the task now. Figure 8: Task configuration generation prompt for retail domain of τ -bench. You are an AI judge and your goal is to judge the quality and validity of the provided task object based on the guidelines, following the rubric. Task Alignment Validation Prompt ## Guidelines The task object contains an intent (q) from user, actions (agt), and outputs (ogt). The actions correspond to the tool_calls made by an AI assistant to satisfy the instruction. description of the tools available to the AI assistant is provided. The diff_patch is the difference in the database state after the tool_calls are made. It should only reflect changes corresponding to the intent. There should be no extraneous changes. If the diff_patch is empty, it means that the tool_calls did not change the database state, which is possible if the instruction was to provide information only. Perform brief reflection on the task based on the below Rubrics. Think step-by-step to generate score of 0 or 1 for each of these criteria (1 means follows criterion and 0 means does not) ## Rubric Correctness: Do the actions (agt) accurately implement the instruction (q)? Completeness: Is the instruction (q) sufficiently detailed, and is it fully addressed by the actions? (Includes rule-based checks). Satisfaction: Do the expected outputs (ogt) fulfill any explicit or implicit information requests within the instruction (q)? Creativity: Does the task represent non-trivial, plausible, and potentially interesting scenario within the domain? ## Task Object {task} ## Tools in Python format {tools} ## Diff Patch {diff_patch} ## Output format <scores> {{ \"reflection\": str, <a brief high-level review of the task> \"correctness\": int, <0/1>, \"completeness\": int, <0/1>, \"satisfaction\": int, <0/1>, \"creativity\": int, <0/1>, \"total\": int, <total score out of 4> \"correction\": str, <brief explanation and suggested correction (if needed)> }} </scores> Figure 9: Task alignment validation prompt for τ -bench. This is sent to each LM in the review committee to get their scores, following which we employ majority voting. You are responsible for analyzing and summarizing feedback from multiple AI judges. Your primary goal is to provide clear, actionable feedback that will help the generator LLM improve its future outputs. You do not evaluate the task directly; instead, you review and grounding the existing feedback from the AI judges. Final Semantic Review Prompt ## Review Process Begin by analyzing individual reflections and scores from each judge. Summarize common points of agreement or disagreement. Offer concise summary of actionable feedback to be sent back to the data generator, which aims to improve the next round of data quality. ### Diff Patch {diff_patch} ### Generated Task Data {task} ### AI Judges Feedback {reviews} ## Output Format Generate your response according to the following format. Enclose the thought process within <thought></thought> tags, and the final summary of actionable feedback within <summary></summary> tags. Figure 10: Final semantic review prompt for τ -bench. You are detail-oriented user interacting with an AI agent. Trajectory Collection Prompt ## Intent {intent} ## Rules Generate one line at time to simulate the users message. Do not give away all the intent at once. Only provide the information that is necessary for the current step. Do not hallucinate information that is not provided in the intent. If the intent goal is satisfied, generate ###STOP### to end the conversation. Do not repeat the exact intent in the conversation. Instead, use your own words to convey the same information. Try to make the conversation as natural as possible and stick to the personalities in the intent. Figure 11: Trajectory collection prompt for τ -bench. You are fair judge and an expert in following details. BoN User LM Setting Prompt human is interacting with retail assistant to get help on solving their task. You are provided with the description of the human and the task the human wants to accomplish (wrapped with <description></description>), and candidate response (wrapped with <response></response>) the human wants to give the assistant. Please help the human evaluate this candidate response, give an integer score (ranging from 0 to 10) to indicate the correctness of the response, higher score means better quality. 1. If the response includes specific item / order / personal details, and they correctly match the task description you should give full score of 10. If there is some change in details, give corresponding lower score (more incorrect details gets lower score). 2. The response can include any normal conversation otherwise (e.g. asking details, saying ###STOP###) etc. which are all correct responses. 3. Additionally, if the candidate_response keeps the conversation flowing by describing the task clearly / gives information properly then give high score and if not (e.g. \"I dont remember\" or unhelpful response) should get corresponding lower score. <description> {description} </description> <response> {response} </response> After scoring using the mentioned guideline, tell me your score, wrap it in <score></score> tags. Figure 12: Best-of-N (BoN) User LM setting prompt used in the retail domain of τ -bench."
        }
    ],
    "affiliations": [
        "Salesforce AI Research"
    ]
}