{
    "paper_title": "Executable Knowledge Graphs for Replicating AI Research",
    "authors": [
        "Yujie Luo",
        "Zhuoyun Yu",
        "Xuehai Wang",
        "Yuqi Zhu",
        "Ningyu Zhang",
        "Lanning Wei",
        "Lun Du",
        "Da Zheng",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code will released at https://github.com/zjunlp/xKG."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 5 9 7 7 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Executable Knowledge Graphs for Replicating AI Research",
            "content": "Yujie Luo* , Zhuoyun Yu* , Xuehai Wang, Yuqi Zhu, Ningyu Zhang , Lanning Wei, Lun Du, Da Zheng , Huajun Chen Zhejiang University Ant Group Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph {luo.yj,zhangningyu}@zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Replicating AI research is crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (XKG), modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as general and extensible solution for automated AI research replication1."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of AI has dramatically accelerated scientific progress, producing thousands of new publications each year (Zhao et al., 2023). However, reproducing these results remains major bottleneck: many papers omit critical implementation details, code repositories are incomplete or unavailable, and essential background knowledge is scattered across diverse sources (Zhao et al., 2025; Seo et al., 2025; Zhou et al., 2025; Edwards et al., 2025; Zhu et al., 2025; Huang et al., 2025; Zhu et al., 2025; Kon et al., 2025; Yan et al., 2025). While humans perform the tedious pipeline of reading papers, inspecting code, and collecting back- * Equal Contribution. Corresponding Authors. 1https://github.com/zjunlp/xKG. ground materials to reproduce results, enabling machines to perform the same workflow reliably remains an open challenge (Chen et al., 2025). Why Executable Knowledge Graphs? Existing attempts (Tang et al., 2025; Ou et al., 2025) to convert papers into knowledge bases show promising signs but often stop at shallow scaffolding rather than delivering rigorous, reproducible implementations. Three key issues limit agent-driven reproduction: (1) most approaches fail to extract deeper technical insights hidden in cited references and background literature; (2) they overlook practical signals embedded in concrete code implementations; and (3) the lack of structured, unified representation prevents effective retrieval, composition, and reuse of scientific concepts and their executable components (Hua et al., 2025). To address these gaps, we propose the Executable Knowledge Graph (XKG), novel knowledge representation that fuses textual paper knowledge with its corresponding executable code snippets. Unlike conventional KG, XKG captures both conceptual relations and runnable components, enabling agents to retrieve, reason about, and assemble the precise artifacts needed for faithful reproduction. We evaluate XKG by integrating it into three distinct agent frameworksBasicAgent, IterativeAgent, and PaperCoder. Our experiments on PaperBench (Starace et al., 2025) demonstrate consistent and significant performance gains. The design of XKG is modular and extensible, facilitating its adoption and expansion across diverse research domains."
        },
        {
            "title": "2.1 Preliminary",
            "content": "We define the paper reproduction task as generating an executable code repository from paper , modeled as = A(P ), where is an agent. The primary benchmark for this task evaluates the Figure 1: XKG is constructed automatically from arXiv papers and GitHub repositories (Examples at Appendix D). functional correctness of against an evaluation rubric . final Replication Score, = E(R, ), quantifies the weighted proportion of criteria met. Search in Burns et al. (2022), see Figure 5) from papers, as well as associate these techniques with code snippets, yielding more precise knowledge."
        },
        {
            "title": "2.3 Executable Knowledge Graph",
            "content": "We model XKG as hierarchical, multi-relational graph XKG = (N , E), which is composed of various node types and edge types defined as: = NP NT NC = Estruct Eimpl (1) (2) We first define the three types of nodes to capture knowledge at different granularities: Paper Node (np): Represents paper as tuple np = (Mp, {nt}i, {nc}j), containing metadata Mp (e.g., abstracts, references, etc.), technique nodes {nt}i, and code nodes {nc}j. Technique Node (nt): self-contained acat}k) with its defit}k, ranging from demic concept nt = (Dt, {n nition Dt and sub-nodes {n complete framework to reusable component. Code Node (nc): An executable unit nc = (σ, τ, δ) comprising code implementation σ, test script τ , and documentation δ. These nodes are then linked by the following two primary types of edges: Structural Edge (estruct): An edge (nt,i, nt,j) indicates an architectural dependency between technique nodes. Implementation Edge (eimpl): directed edge (nt, nc) linking technique node to its code implementation. Through the above design, it becomes possible to link specific techniques (e.g., Contrast-Consistent"
        },
        {
            "title": "2.3.2 Hierarchical Graph Construction\nBased on the corpus obtained above, we then pro-\nceed to construct the XKG, including the following\nthree automated steps:",
            "content": "Step 1: Technique Extraction. We first use o4-mini to deconstruct the papers methodology Method Model vanilla +XKG vanilla +XKG vanilla +XKG vanilla +XKG vanilla +XKG vanilla +XKG MU-DPO TTA-FP One-SBI CFG FRE Average BasicAgent o3-mini 12.96 37.22+24.26 22.63 27.26+4.63 40.55 39.141.41 DS-R1 33.05 39.14+6.09 18.24 20.82+2.58 17.22 24.49+7.27 20.82 22.86+2.04 31.56 33.97+2. 14.82 14.670.15 17.08 21.38+4.30 17.89 24.57+6.68 27.89 31.62+3.73 IterativeAgent o3-mini 22.22 43.70+21.48 21.38 36.28+14.90 28.77 23.914.86 31.09 26.574.52 DS-R1 16.20 47.40+31.20 31.19 31.78+0.59 31.28 29.152.13 35.30 38.44+3.14 24.60 31.91+7.31 19.35 26.50+7.15 21.32 31.89+10.57 27.02 35.22+8. PaperCoder o3-mini 23.15 46.48+23.33 45.70 53.99+8.29 DS-R1 43.24 49.26+6.02 52.48 52.080.40 43.26 59.19+15.93 51.18 73.03+21.85 61.12 60.680.44 50.37 63.13+12.76 39.84 50.36+10.52 42.31 53.21+10.90 52.23 60.34+8.11 62.37 59.532.84 Table 1: Main results on PaperBench Code-Dev. We evaluate on the official lite subset of PaperBench, consisting of five papers: MU-DPO, TTA-FP, One-SBI, CFG, and FRE (details in Table 3). Results are reported using the Replication Score (%) metric with o3-mini as evaluator. All scores are shown as best@3 to mitigate task stochasticity and tool-related failures. into preliminary hierarchical tree of Technique Nodes NT linked by Structural Edges estruct. Subsequently, we utilize RAG2 (treating the paper as document) to enrich each node by retrieving relevant text from the paper, which is then synthesized into comprehensive definition Dt. This step yields set of detailed yet unverified techniques that may contain noise. Step 2: Code Modularization. For each technique nt, its definition is used as query to retrieve relevant code snippets, following the similar RAG-based procedure (treating the code as document) as in Step 1. We then employ o4-mini to synthesize these snippets into candidate Code Node nc, which includes the implementation σ, test script τ , and accompanying documentation δ. This candidate node is then organized in modular fashion and subjected to an iterative self-debugging loop to verify the executability of each module, ultimately producing set of fully executable Code Nodes Nc along with their associated Implementation Edges eimpl. Step 3: Knowledge Filtering. We formalize simple yet powerful verification principle: technique nt is considered valuable only if it can be grounded in executable code. Therefore, any technique for which Step 2 failed to retrieve relevant code snippets is pruned from the XKG. This filtering process ensures that only techniques with proven, practical value populate the final XKG, eliminating the noise and overly granular nodes introduced in Step 1. Finally, we construct XKG from 42 curated papers, totaling 591,145 tokens. We aim to automate 2We employ text-embedding-3-small throughout all stages of xKG construction. this process to enable knowledge scaling. 2.4 Using Executable Knowledge Graphs In practical reproduction workflow, LLM agent can use XKG at two critical stages. For high-level planning, the agent fetches the target papers Paper Node (without all Code Nodes) to grasp its core techniques and overall structure. During lowlevel implementation, the agent queries XKG for semantically relevant (Technique, Code) pairs to aid in specific functionalities. These two steps can be supplied either as callable tools for ReActstyle agents or as pluggable components of fixedworkflow agents. Crucially, to combat knowledge noise, all retrieved candidates are processed by final LLM-based Verifier (o4-mini). This verifier acts as critical quality gate, filtering, reranking, and refining the results to ensure that the retrieved knowledge is highly relevant and implementable."
        },
        {
            "title": "3.1 Settings",
            "content": "We evaluate XKG on the lite collection of PaperBench Code-Dev using structured rubric (Starace et al., 2025), weighted tree of binary criteria whose leaves are aggregated by an o3-minibased evaluator into single score. We integrate XKG into BasicAgent (a ReAct-style agent), IterativeAgent (adds self-improvement loop), both with one-hour runtime limit, and PaperCoder(a repository-level reproduction agent with fixed workflow). See Appendix for more details."
        },
        {
            "title": "3.2 Main Results",
            "content": "As shown in Table 1, XKG achieves substantial performance gains across diverse agent frameworks and LLM backbones. On the general ReAct-style IterativeAgent with DeepSeek-R1, XKG delivers performance improvement of 8.20%. The effectiveness of XKG is further highlighted by the 10.90% improvement achieved with PaperCoder powered with o3-mini. Notably, the impact of XKG is also highly paper-dependent. While BasicAgent with o3-mini achieves remarkable 24.26% performance gain on MU-DPO, the same configuration yields only 2.58% improvement on One-SBI and even 0.15% drop on the FRE task. This striking contrast reveals critical dependency on the target paper (details in Appendix B). 3.3 Further Analysis Method XKG(Full) w/o Paper Node w/o Code Node w/o Technique Node Score (%) Drop () 53.21 51.08 48.65 52.16 - 2.13 4.56 1. Table 2: Ablation study on the core components of XKG. Performance is averaged over all 5 papers. Code-based structured knowledge aids AI research replication. As shown in Table 2, our ablation study conducted on PaperCoder framework with o3-mini setup, reveals that removing any component degrades performance. The most significant drop occurs when removing Code Nodes, decreasing the score by 4.56% (53.21% 48.65%), suggesting that LLM agents benefit immensely from fine-grained knowledge, with executable code being the most critical component. Ablating Paper Nodes yields substantial degradation of 2.13%, highlighting the value of high-level structural overview of the target task. In contrast, omitting Technique Nodes results in modest 1.05% drop, since the function of each technique is already implicitly captured by the Code Nodes, rendering the explicit description redundant. Successful reproduction hinges on retrieved code quality. Building on the above findings, we conduct further analysis into how the quality of Code Nodes within XKG influences performance. Using PaperCoder with o3-mini on two high-gain papers, MU-DPO and TTA-FP, we compare XKG with four configurations, each repeated three times to mitigate stochasticity (Figure 2): w/o Code, without access to any code nodes; + Raw Figure 2: Further study on Code Node quality. Figure 3: Case Study on MU-DPO. comparison of performance with and without XKG on IterativeAgent. Code, which incorporates code nodes with raw retrieved snippets; + Rewrite, using LLM-rewritten executable nodes but omitting the verification step. As illustrated in Figure 2, our full approach not only achieves the highest mean score but also exhibits low variance. Notably, even incorporating raw code snippets (+ Raw Code) improves performance, validating that our method effectively localizes necessary code. critical insight emerges from the + Rewrite setting, which underperforms even the raw snippet baseline. We attribute this to misleading guidance phenomenon: well-formatted but contextually irrelevant code can cause the agent to blindly adopt the retrieved snippets, deviating from the target papers specific implementation. xKG Transforms Agents from Scaffolding to Implementation. To understand the mechanism behind the performance gains, we conduct case study on the MU-DPO paper (Figure 3). We notice that XKG enriches information granularity, allowing agents to generate critical details accurately, and improves modular implementation capability, enabling agents to reuse verified code for functionally correct implementations, as illustrated by the case colors in Figure 3."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduce XKG, which improves AI research replication. We aim for xKG to serve as an AIfor-Research knowledge base, reducing noise from web retrieval and improving efficiency."
        },
        {
            "title": "Limitations",
            "content": "This work has several limitations. First, the PaperBench task exhibits high variance and is costly to evaluate. Although we report results across multiple papers and conduct experiments, due to funding constraints, we only perform experiments on the lite collection of PaperBench Code-Dev. Second, for emerging domains, there may be no available reference papers at all, which limits the applicability of our approach to scenarios where some baseline references exist. Finally, while the code-based knowledge organization we propose may have the potential to transfer to similar tasks, exploring this remains future work (Nathani et al., 2025; Chan et al., 2024; Toledo et al., 2025; Jia et al., 2025; Miao et al., 2025). During our work, we found another project with similar name, ExeKG (Zheng et al., 2022b,a; Zhou et al., 2022). However, our approach differs fundamentally in the organization of the knowledge base we adopt much simpler structure of nodes and edges. Moreover, the problems addressed are entirely distinct: our focus is on paper replication tasks. We hold deep respect for the pioneering efforts of the ExeKG authors."
        },
        {
            "title": "References",
            "content": "Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. 2024. Mle-bench: Evaluating machine learning arXiv agents on machine learning engineering. preprint arXiv:2410.07095. Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, et al. 2025. Ai4research: survey of artificial intelligence for scientific research. arXiv preprint arXiv:2507.01903. Nicholas Edwards, Yukyung Lee, Yujun Audrey Mao, Yulu Qin, Sebastian Schuster, and Najoung Kim. 2025. Rexbench: Can coding agents autonomously implement ai research extensions? arXiv preprint arXiv:2506.22598. Kevin Frans, Seohong Park, Pieter Abbeel, and Sergey Levine. 2024. Unsupervised zero-shot reinforcement learning via functional reward encodings. In Fortyfirst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Manuel Glöckler, Michael Deistler, Christian Dietrich Weilbach, Frank Wood, and Jakob H. Macke. 2024. In FortyAll-in-one simulation-based inference. first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Tianyu Hua, Harper Hua, Violet Xiang, Benjamin Klieger, Sang Truong, Weixin Liang, Fan-Yun Sun, and Nick Haber. 2025. Researchcodebench: Benchmarking llms on implementing novel machine learning research code. arXiv preprint arXiv:2506.02314. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Huichi Zhou, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, et al. 2025. Deep research agents: systematic examination and roadmap. arXiv preprint arXiv:2506.18096. Hangyi Jia, Yuxi Qian, Hanwen Tong, Xinhui Wu, Lin Chen, and Feng Wei. 2025. Towards adaptive ml benchmarks: Web-agent-driven construction, domain expansion, and metric optimization. arXiv preprint arXiv:2509.09321. Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, et al. 2025. Exp-bench: Can ai conduct ai research experiments? arXiv preprint arXiv:2505.24785. Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, and Rada Mihalcea. 2024. mechanistic understanding of alignment algorithms: case study on DPO and toxicity. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Jiacheng Miao, Joe Davis, Jonathan Pritchard, and James Zou. 2025. Paper2agent: Reimagining research papers as interactive and reliable ai agents. arXiv preprint arXiv:2509.06917. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, et al. 2025. Mlgym: new framework and benchmark for advancing ai research agents. arXiv preprint arXiv:2502.14499. Shuaicheng Niu, Chunyan Miao, Guohao Chen, Pengcheng Wu, and Peilin Zhao. 2024. Test-time model adaptation with only forward passes. In Fortyfirst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. data analytics. In Proceedings of the 31st ACM international conference on information & knowledge management, pages 50645068. Dongzhuoran Zhou, Baifan Zhou, Zhuoxun Zheng, Zhipeng Tan, Egor Kostylev, and Evgeny Kharlamov. 2022. Towards executable knowledge graph translation. In ISWC (Posters/Demos/Industry). Mingyang Zhou, Quanming Yao, Lun Du, Lanning Wei, and Da Zheng. 2025. Reflective paper-to-code reproduction enabled by fine-grained verification. arXiv preprint arXiv:2508.16671. Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, and Yue Zhang. 2025. Ai scientists fail without strong implementation capability. arXiv preprint arXiv:2506.01372. Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025. Automind: Adaptive knowledgeable agent for automated data science. arXiv preprint arXiv:2506.10974. Guillaume Sanchez, Alexander Spangher, Honglu Fan, Elad Levi, and Stella Biderman. 2024. Stay on topic with classifier-free guidance. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. 2025. Paper2code: Automating code generation from scientific papers in machine learning. arXiv preprint arXiv:2504.17192. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt ThompPaperbench: Evaluating ais son, et al. 2025. arXiv preprint ability to replicate ai research. arXiv:2504.01848. Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. 2025. Ai-researcher: Autonomous scientific innovation. CoRR, abs/2505.18705. Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis AudranReiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, et al. 2025. Ai research agents for machine learning: Search, exploration, and generalization in mle-bench. arXiv preprint arXiv:2507.02554. Shuo Yan, Ruochen Li, Ziming Luo, Zimu Wang, Daoyang Li, Liqiang Jing, Kaiyu He, Peilin Wu, George Michalopoulos, Yue Zhang, et al. 2025. Lmrbench: Evaluating llm agents ability on reproducing language modeling research. arXiv preprint arXiv:2506.17335. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Xuanle Zhao, Zilin Sang, Yuxuan Li, Qi Shi, Weilun Zhao, Shuo Wang, Duzhen Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2025. Autoreproduce: Automatic ai experiment reproduction with paper lineage. arXiv preprint arXiv:2505.20662. Zhuoxun Zheng, Baifan Zhou, Dongzhuoran Zhou, Ahmet Soylu, and Evgeny Kharlamov. 2022a. Executable knowledge graph for transparent machine In Prolearning in welding monitoring at bosch. ceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 51025103. Zhuoxun Zheng, Baifan Zhou, Dongzhuoran Zhou, Ahmet Soylu, and Evgeny Kharlamov. 2022b. Exekg: Executable knowledge graph system for user-friendly ponents, where excelling at core techniques highlighted by XKG leads to the neglect of secondary objectives. Figure 4: Average performance gain per paper. More fundamentally, this performance disparity is tied to the papers research archetype. analytical papers, such as MU-DPO(Lee et al., 2024), which synthesize and refine existing techniques, benefit substantially as their components are wellrepresented in XKG. Conversely, methodological papers like One-SBI(Glöckler et al., 2024), which introduce fundamentally novel architectures, find less directly applicable knowledge, as their core innovations have limited precedent in the corpus. This outcome is logical, as the performance bottleneck shifts from knowledge argumentation to the intrinsic innovative capacity of the base LLM itself."
        },
        {
            "title": "A Experimental Setup",
            "content": "A.1 Benchmarks The original PaperBench benchmark (Starace et al., 2025) is designed to evaluate the ability of AI agents to reproduce state-of-the-art AI research from scratch. The full benchmark includes 20 recent papers from top-tier machine learning conferences (e.g., ICML 2024), where agents must understand each paper, develop complete codebase, and replicate its empirical results. As full-scale evaluation is both computationally expensive and time-consuming, the authors introduced lightweight variant, PaperBench Code-Dev, which focuses solely on code developmentassessing implementation correctness without requiring code execution or result verification. In our study, we adopt the pre-defined lite subset of PaperBench Code-Dev provided in the official repository, spanning diverse AI domains including machine learning (ML), reinforcement learning (RL), and natural language processing (NLP). Furthermore, since PaperBench shows that BasicAgent and IterativeAgent gain little performance improvement after one hour, we cap their execution time at one hour for efficiency and cost reasons. Evaluation follows structured hierarchical rubric co-developed with the original authors, and an LLM-based(o3-mini) evaluator aggregates final scores using weighted binary criteria tree. More specific details about the papers and their evaluation nodes are listed in Table 3. A.2 Configuration Details The configuration of our XKG framework comprises both hyperparameters and prompts. The hyperparameters are managed via central config.yaml file, which is organized into modules for Code-RAG, Paper-RAG, and Knowledge Graph Retrieval. We summarize the key parameters for each module in Tables 4-7. In addition, the specific prompts designed in our system are detailed in Appendix C."
        },
        {
            "title": "B Further Analysis on Target Paper",
            "content": "As illustrated in Figure 4, the effectiveness of XKG is highly contingent on the target paper, with performance occasionally degrading. Bad cases stem from two primary failure modes: (1) Over-reliance on retrieved code, where the agent prioritizes generic snippets over the papers unique implementation details; and (2) Over-focus on core comOur Abbr. PaperBench Name CodeDev Nodes FRE (Frans et al., 2024) TTA-FP (Niu et al., 2024) MU-DPO (Lee et al., 2024) One-SBI (Glöckler et al., 2024) CFG (Sanchez et al., 2024) fre test-time-model-adaptation mechanistic-understanding all-in-one stay-on-topic-with-classifier-free-guidance 306 86 36 92 70 Table 3: Abbreviations and statistics for the PaperBench tasks evaluated in this work. We assign brief abbreviation to each paper for easier reference. The \"CodeDev Nodes\" column specifies the number of nodes to evaluation for each reproduction task. Hyperparameter Code-RAG Module code.embedder.model Value Description text-embedding-3-small code.text_splitter.chunk_size code.text_splitter.chunk_overlap code.retriever.faiss.top_k code.retriever.llm.top_files code.exec_check_code 350 100 10 5 False The embedding model used for code chunk vectorization. The size of each text chunk when splitting code files. The number of overlapping characters between adjacent chunks. Number of initial candidate chunks retrieved via FAISS vector search. Number of top files selected by the LLM reranker for detailed analysis. boolean flag to enable or disable the executionbased verification of generated code. Table 4: Hyperparameters for the Code-RAG module in XKG. Hyperparameter Paper-RAG Module paper.rag Value True paper.embedder.model text-embedding-3-small paper.text_splitter.chunk_size paper.retriever.faiss.top_k 350 5 Description boolean flag to enable or disable the entire Paper-RAG process. The embedding model used for paper text vectorization. The size of each text chunk when splitting the paper content. Number of relevant text excerpts retrieved from the paper via FAISS. Table 5: Hyperparameters for the Paper-RAG module in XKG. Hyperparameter Value Description Knowledge Graph Retrieval retrieve.embedding_model all-MiniLM-L6-v2 retrieve.technique_similarity retrieve.paper_similarity 0.6 0.6 The sentence-transformer model used for calculating similarity between techniques. The minimum similarity score required for technique to be retrieved from the KG. The minimum similarity score required for paper to be retrieved from the KG. Table 6: Hyperparameters for Knowledge Graph retrieval. Hyperparameter Value Description Global & Model Profile Configuration log_level kg_path DEBUG storage/kg max_prompt_code_bytes 52100 model paper_model code_model DeepSeek-V o4-mini o4-mini Sets the verbosity of logging. The directory where the constructed Knowledge Graph is stored. The maximum size in bytes for code content included in prompt to the LLM. The primary foundation model for the agents core reasoning tasks. specialized model used specifically for extracting and rewriting techniques from papers. specialized model used for rewriting and debugging code. Table 7: Common global settings and an example model profile (basic-deepseek-v3). Specific models can be defined for different sub-tasks, allowing for flexible and optimized model selection."
        },
        {
            "title": "C Prompts",
            "content": "In this section, we showcase some of the key prompts used in the full pipeline of our system, which serve as reference. The prompts are organized by their functional role in the pipeline: paper parsing, code repository parsing, and knowledge graph construction. C.1 Paper Parsing Prompts Prompt for Extracting References from .bbl File # Task You are provided with .bbl file {bbl}. Please extract the titles of all the references in the .bbl file. # Output 1. Output the extracted reference titles in the form of string list. 2. If no reference is available, please return None. Please wrap your final answer between two ``` in the end."
        },
        {
            "title": "Prompt for Extracting Paper Contributions",
            "content": "# Task You are provided with the paper titled {title}. Here are the main sections of the paper: {sections}. Furthermore, key equations from the paper are provided to help you understand its specific algorithms: {equations}. Your task is to analyze the provided research paper and identify its Core Components. For each Component, you must provide clear, concise, and implementable definition. # INSTRUCTIONS 1. Identify Core Components: Read the paper to identify its primary components. componnet is not limited to single algorithm; it can be novel methodology, reusable techniques, key insight/finding, open-source datasets/benchmarks, etc. 2. Categorize Each Component: Assign one of the following types to each component you identify: Methodology: novel, end-to-end procedure proposed by the paper for solving problem. This can be an entire algorithm or model architecture design that addresses specific research challenge. It must correspond to systematic and complete end-to-end code implementation. When composed of multiple atomic sub-techniques, represent using the \"components\" field. Ensure the methodology can be implemented standalone, instead of generic theoretical definition or high-level outline of framework. Technique: self-contained and algorithmically implementable component, applied within the papers Methodology or Experiment Process. It is either novel module from this work, or traceable technique from prior research. When composed of multiple atomic sub-techniques, represent using the \"components\" field. Ensure each technique can be implemented standalone, requiring NO integration with other modules to constitute single code module. Exclude theoretical points and experimental tricks not directly tied to code implementation. Move them to the \"Finding\" category. Finding: significant empirical or theoretical insight which can refer to an intriguing experimental finding, powerful theoretical proof, or promising research direction. Resource: PUBLICLY available dataset or benchmark originally constructed in this paper. 3. Define and Detail: For each component, provide detailed definition adhering to the following rules: Fidelity: All definitions must originate strictly from the provided paper. Do not invent details. Atomicity & Modularity: Each component, whether high-level or component, should be defined as distinct, self-contained unit. Explain its inputs, core logic, and outputs. Reproducibility: Retain as much original detail as possible. The definition should be comprehensive enough for an engineer or researcher to understand and implement it. Structure: If Methodology or Technique is composed of smaller Techniques, represent this hierarchical relationship using nested bullet points. This is crucial for understanding how the parts form the whole. Dont list techniques individually if theyre already part of larger technique/methodology. # OUTPUT FORMAT Organize the extracted techniques into list of dictionaries, with the final answer wrapped between two ``` markers. The keys for each dictionary are described below: 1. name: str, the name of the component, expressed as concise and standardized academic term, intended to precisely capture its core identity while facilitating efficient indexing and retrieval from other literature. 2. type: str, One of Methodology, Technique, Finding, or Resource. 3. description: str, detailed, self-contained explanation of the component, focusing on what it is, how it works, and its purpose. For implementable items, describe the whole process without missing any critical steps and implementation details. For insights, describe the core discovery. Maximize the retention of description and implementation details from the original text. 4. components: List[dict], Optional, If the component is complex Methodology or Techinque composed of multiple smaller techniques, this field lists its key sub-techniques. Each sub-technique listed here must also be defined separately as complete technique object following this same JSON schema (with name, type and description as dictionary keys), allowing for hierarchical and recursive decomposition. ATTENTION: Only Methodology and Technique can have Technique as its components!!! Now please think and reason carefully, and wrap your final answer between two ``` in the end. C.2 Code Repository Parsing Prompts Prompt for Generating Code Repository Overview # Task Analyze this GitHub repository {name} and create structured overview of it. # Input 1. The complete file tree of the project: {file_tree} 2. The README file of the project: {readme} # Output Create detailed overview of the project, including: 1.Overview (general information about the project) 2.System Architecture (how the system is designed) 3.Core Features (key functionality) Organize the overview in clear and structured markdown format. Please wrap your final answer between two ``` in the end."
        },
        {
            "title": "Prompt for Finding Associated Paper from Code",
            "content": "# Task Analyze this GitHub repository {name}, and determine whether this repository is directly associated with specific academic paper. # Input The README file of the project: {readme} # Output 1. If you can find clear evidence that this repository is the official or direct code implementation of specific academic paper, return the full title of the paper as string. 2. If there is no sufficient evidence to identify directly corresponding paper (e.g., only general descriptions, multiple papers, or no paper mentioned), return None. Please wrap your final answer between two ``` in the end. C.3 Knowledge Graph Construction Prompts Prompt for Rewriting Techniques Description # Task Your task is to refine and enhance the description of technical concept extracted from research paper {paper}. The goal is to produce clear, concise, and comprehensive description that accurately captures the essence of the technique. # Input 1. Technical Concept from the paper {paper}: {technique} 2. Relevant Excerpt of this Technique: {excerpt} # Output Return precise and comprehensive description, presented as single, continuous paragraph written in comprehensive, academic style. Avoid using bullet points, numbered lists, or other form of itemization. 1. Ensure the technique precisely matches the original description. DO NOT alter, expand, or reduce the scope of the technique. Ignore other related techniques and only FOCUS ON this technique. 2. Strictly adhering to the original description, augment its implementation details based on the provided excerpts. All formulas, parameter configurations, and implementation details must be extracted from the given excerpts, ensuring strict adherence to them. Avoid any summarization, inference, or omission. 3. If the excerpts offer no new information, leave the description unchanged. Your response MUST be based solely on the original description and provided excerpts. The inclusion of ANY external information or fabricated details is strictly forbidden!!! 4. Ensure that the provided description is precise, complete, and possesses sufficient detail to correspond to specific implementation. Now please think and reason carefully, and wrap your final answer between two ``` in the end."
        },
        {
            "title": "Prompt for Identifying Relevant Code Snippets",
            "content": "# Task Your task is to analyze list of code files retrieved from GitHub repository, and identify which files are directly relevant to the implementation of specific technical concept defined in an academic paper {paper}. # Input 1. Technical Concept Definition from the paper {paper}: {technique} 2. Overview of the Code repository: {overview} 3. Relevant Code Files: {file_snippets} # Output Return list of filenames formatted as [\"xx\", \"xx\", ...], sorted in descending order of relevance of the technical concept. 1. Exclude any file not DIRECTLY correspond to the concrete implementation and configurarion of this technique (e.g., tests, documentation, other technique implementation). 2. Confirm that direct implementation exists within your provided file list. If no such implementation can be found, return None. 3. Return the nitrogen list even if theres only one file. Now please think and reason carefully, and wrap your final answer between two ``` in the end."
        },
        {
            "title": "Prompt for Reranking Retrieved Techniques",
            "content": "# Task Your task is to analyze list of technique implementations retrieved from the knowledge base, and identify which techniques are directly relevant to the implementation of specific technical concept. # Input 1. Technical Concept Definition: {technique} 2. Relevant Technique implementations: {relevant_techniques} # Output Return list of (technique_name, apply_guidance) tuples formatted as [(\"\", \"\"), (\"\",\"\"), ...], sorted in descending order of relevance to the technical concept. The guidance should be short explanation of how the technique applies to the current scenario and what modifications are needed for adaptation. Use clear and definite wording, avoiding parentheses. 1. Exclude any techniques not relevant to the concrete implementation of this technique. 2. Ensure the returned technique name exactly matches the original one. 3. For technologies with identical core definitions, keep the one whose application is most relevant. 4. If no such technique can be found, return None. 5. Return the nitrogen list even if theres only one relevant technique. Now please think and reason carefully, and wrap your final answer between two ``` in the end. Prompt for Rewriting Code for Leaf Technique # Task Your task is to transform collection of disparate source code snippets, which are the official implementation of technique component from research paper {paper}, into single, self-contained, and executable code block. The final code block must be clean, well-documented, and easy for others to understand and run. # Input 1. Abstract of the paper {paper}: {abstract} 2. Technical Concept Definition from the paper {paper}: {technique} 3. Relevant Code Files: {file_snippets} # Workflow 1. Analyze: Understand the techniques inputs, outputs and workflow from the paper. Focus ONLY on THIS technique, ignoring the mentioned context and related techniques. 2. Isolate & Extract: Based on the description of the technique, determine what is its PRECISE role and functionality, and extract ONLY the code you identified as belonging to {technique}. Other mentioned associated techniques MUST BE IGNORED AND EXCLUDED. 3. Refactor: Integrate the extracted code by removing hard-coded values, isolating the core algorithm, and standardizing it with proper documentation and type hints. 4. Assemble & Test: Build the final script and add an test block as runnable example. Ensure accuracy and conciseness, avoiding unnecessary output. 5. Documentation: Write brief and concise documentation of the code logic, configurable options, and usage in 5-10 sentences. # Requirements 1. Dependency Management: Ensure all necessary imports and dependencies are included at the beginning of the code block. 2. Fidelity to the Original Technique: Strictly follow the description of the given technique to organize the code. ONLY focus on the implementation that DIRECTLY corresponds to THIS technique!!! (e.g., if the technique is loss function definition, implement only the code for its calculation. Ignore all other parts of the algorithms implementation, even if provided in the code snippets.) 3. Code Encapsulation and Documentation: Encapsulate the core logic of the technique into one or more functions/classes. Every function and class method must include comprehensive docstring explaining its purpose, parameters, and return values. All function arguments and return values must have clear type hints. Preserve original parameters and comments from the source code. 4. Reproducibility and Testing: main execution block, starting with the comment # TEST BLOCK, is required at the end of the file, which serves as practical usage example and test case. The test case should use parameters from the code repository or paper. If missing, create and state your own defaults. 5. Fidelity to the Original Logic: You must strictly adhere to the algorithmic logic present in the provided code snippets. Your role is to refactor and structure, not to re-implement or invent new logic. Minimal, necessary modifications are permitted (e.g., renaming variables for clarity, adapting function signatures for dependency injection), but the core computational steps must remain identical to the original authors implementation. 6. Documentation of Usage Scenarios: Provide concise and fluent document of the code modules core logic, configurable options, and usage. Limit the description to 5-10 clear and coherent sentences. # Output 1. Implement the technique standalone without relying on external, undefined components. Return an executable code block and corresponding documentation, each wrapped between two ``` . Example: [... Reasoning Steps ...] ```python [... Core Implementation of the technique ...] [... Ignore other relevant techniques ...] # TEST BLOCK [... Example Usage ...] ``` The brief documentation of the code: ``` [...Brief Documentation ...] ``` 2. Verify that the generated code does not exceed the scope of the techniques definition. If the technique requires integration with other modules to constitute single code module, return None. If no direct implementation of the technique is found in the given code snippets, also return None. Now, please proceed with the task, following the workflow and adhering to all requirements. Generate the final code block and documentation wrapped between two ``` separately at the end. Prompt for Rewriting Code for Composite Technique # Task Your task is to transform collection of disparate source code snippets, which are the official implementation of technique component from research paper paper, into single, self-contained, and executable code block. The final code block must be clean, well-documented, and easy for others to understand and run. # Input Abstract of the paper {paper}: {abstract} Technical Concept Definition from the paper {paper}: {technique} Sub-techniques and Associated Code: {sub_techniques} Relevant Code Files: {file_snippets} # Workflow Analyze: Understand the techniques inputs, outputs and workflow from the paper. Locate: Fully reuse the code of the provided sub-techniques. For any uncovered parts, locate the relevant implementation logic from the given code snippets. Refactor: Integrate the extracted code by removing hard-coded values, isolating the core algorithm, and standardizing it with proper documentation and type hints. Assemble & Test: Build the final script and add an test block as runnable example. Ensure accuracy and conciseness, avoiding unnecessary output. Documentation: Write brief and concise documentation of the code logic, configurable options, and usage in 5-10 sentences. # Requirements Dependency Management: Ensure all necessary imports and dependencies are included at the beginning of the code block. Fidelity to the Original Technique: Strictly follow the description of the given technique to organize the code. ONLY focus on the implementation that DIRECTLY corresponds to THIS technique!!! (e.g., if the technique is loss function definition, implement only the code for its calculation. Ignore all other parts of the algorithm like model definition or training loop). Return None if no direct implementation is found. Code Encapsulation and Documentation: Encapsulate the core logic of the technique into one or more functions/classes. Every function and class method must include comprehensive docstring explaining its purpose, parameters, and return values. All function arguments and return values must have clear type hints. Preserve original parameters and comments from the source code. Reproducibility and Testing: main execution block, start with the comment # TEST BLOCK, is required at the end of the file, which serves as practical usage example and test case. The test case should use parameters from the code repository or paper. If missing, create and state your own defaults. Fidelity to the Original Logic: You must strictly adhere to the algorithmic logic present in the provided code snippets. Your role is to refactor and structure, not to re-implement or invent new logic. Minimal, necessary modifications are permitted (e.g., renaming variables for clarity, adapting function signatures for dependency injection), but the core computational steps must remain identical to the original authors implementation. Documentation of Usage Scenarios: Provide concise and fluent document of the code modules core logic, configurable options, and usage. Limit the description to 5-10 clear and coherent sentences. # Output 1. Implement the technique standalone without relying on external, undefined components. Return an executable code block and corresponding documentation, each wrapped between two ```. Example: [... Reasoning Steps ...] ```python [... Core Implementation of the technique ...] [... Ignore other relevant techniques ...] # TEST BLOCK [... Example Usage ...] ``` The brief documentation of the code: ``` [...Brief Documentation ...] ``` 2. Verify that the generated code does not exceed the scope of the techniques definition. If the technique requires integration with other modules to constitute single code module, return None. If no direct implementation of the technique is found in the given code snippets, also return None. Now, please proceed with the task, following the workflow and adhering to all requirements. Generate the final code block and documentation wrapped between two ``` separately at the end."
        },
        {
            "title": "Prompt for Verifying Rewritten Code",
            "content": "# Task Your task is to determine if the given code block strictly follows the provided technique description and relevant code files. # Input Technical Concept Definition from the paper {paper}: {technique} Relevant Code Files: {file_snippets} Implemented Code Block: {code} # Output 1.Return False if the implementation is unrelated to the technique. 2.Return False if the implementation contains core logic cannot be located in the given relevant code files. 3.Return False if the implementation contains logics not covered in the technique description (e.g., the technique defines submodule, but the code implements the full algorithm). 4.Return True if the code implements exactly what is specified in the technique description without adding any unnecessary features beyond the technical concept, and strictly follows the implementation in the given code files. Now please think and reason carefully, provide detailed analysis process for the above criteria, and wrap your final answer between two ``` in the end. Prompt for Decomposing Task into Techniques # Task Your task is to decompose complex academic task into its automic fundamental techniques based on its description. # Input Academic Task Definition: {description} # Output Return list of (name, description) tuples in the format [(\"...\", \"...\"), (\"...\", \"...\")], sorted by their importance to the task composition in descending order. Use clear and definite wording, avoiding parentheses. Each tuple must represent distinct, fundamental academic concept that is reusable and traceable in other literature. Each tuple is explicitly mentioned or directly relevant to the target task. Avoid overly broad or vague techniques; each should have clear, specific code implementation. Avoid trivial techniques like Cosine Similarity that require no literature review. If the tasks implementation does not involve any specific academic concepts (e.g., purely engineering, configuration, or organizational task), simply return None. Now please think and reason carefully, and wrap your final answer between two ``` in the end."
        },
        {
            "title": "D Running Examples of xKG",
            "content": "Figure 5: An example of XKG data storage. Paper Nodes are stored as JSON files, with technique and code nodes embedded as structured dictionaries, where key-value pairs are used to create one-to-one mapping representing the implementation relationship."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University",
        "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
    ]
}