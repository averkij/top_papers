{
    "paper_title": "Behind Maya: Building a Multilingual Vision Language Model",
    "authors": [
        "Nahid Alam",
        "Karthik Reddy Kanjula",
        "Surya Guthikonda",
        "Timothy Chung",
        "Bala Krishna S Vegesna",
        "Abhipsha Das",
        "Anthony Susevski",
        "Ryan Sze-Yin Chan",
        "S M Iftekhar Uddin",
        "Shayekh Bin Islam",
        "Roshan Santhosh",
        "Snegha A",
        "Drishti Sharma",
        "Chen Liu",
        "Isha Chaturvedi",
        "Genta Indra Winata",
        "Ashvanth. S",
        "Snehanshu Mukherjee",
        "Alham Fikri Aji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 0 1 9 8 0 . 5 0 5 2 : r Behind Maya: Building Multilingual Vision Language Model Nahid Alam1,2, Bala Krishna Vegesna5, Iftekhar Uddin2, Drishti Sharma2, Karthik Reddy Kanjula2, Abhipsha Das2, Shayekh Bin Islam7,2, Surya Guthikonda3,2, Anthony Susevski2, Roshan Santhosh8, Timothy Chung4,2, Ryan Sze-Yin Chan6, Snegha A9, Chen Liu10, Isha Chaturvedi2, Genta Indra Winata11, Ashvanth.S2, Snehanshu Mukherjee12, Alham Fikri Aji"
        },
        {
            "title": "Abstract",
            "content": "In recent times, we have seen rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) multilingual imagetext pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) multilingual imagetext model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya. 1. Introduction Recent progress in Large Language Models (LLMs) and vision encoders like CLIP [28] and SigLIP [40] has greatly advanced Vision-Language Models (VLMs). Models such as Flamingo [2], LLaVA [19, 20], KOSMOS [25, 27], Florence-2 [37], and Molmo [9] excel at image captioning, VQA, and reasoning. Qwen2-VLs M-RoPE [32, 36] and PaLIs joint modality and cross-lingual scaling [6, 7] further improve multimodal understanding. We acknowledge that although Aya Vision [8] showcases multilingual capabilities in 23 languages, it was released subsequent to the development of Maya. In general though, VLMs still underperform in lowresource languages, struggling with cultural context and localized visual concepts [15]. key reason is the lack of high-quality multilingual multimodal data. Most pretraining datasetsCOCO [18], Flickr30K [38], LAION [30], Visual Genome [16], and LLaVA [20]are English-centric, others [10, 35] remain limited in scale and cultural coverage. To address these challenges, we introduce Maya, an open-source multilingual VLM that extends multimodal capabilities to eight languages. Our key contributions: 1. novel multilingual image-text pretraining dataset consisting of 550K samples for future development of multilingual VLMs, 2. new multilingual VLM that demonstrates improved performance in understanding cultural and linguistic nuances compared to PALO-7B [22] on LLaVA-BenchIn-The-Wild [20], offering multilingual alternative to LLaVA [20]. 2. Dataset Creation and Filtering 2.1. Dataset Creation Methodology Recently, PALO [22] and Pangea [39] have created multilingual image-text dataset to build multilingual multimodal models. However, these multilingual datasets often suffer from data quality issues and distribution biases across languages. For example, in the PALO dataset, the distribution varies signiﬁcantly between English and other languages [10, 13, 22]. To address these limitations, we present novel pre-training dataset tailored for LLaVAs architecture that is both multilingual and optimized for diverse language representation. Our dataset introduces rigorous processes for toxicity analysis, distribution balance, and quality control, ensuring consistent and reliable performance in multiple languages and modalities, as shown in Figure 1. We expand the original English LLaVA dataset, which contains 550K samples, to include seven additional languagesChinese, French, Spanish, Russian, Hindi, Japanese, and Arabicyielding total of 4.4 million samples, equally distributed across all eight languages. Our approach encompasses three components: 1) parallel dataset creation using hybrid translation method, 2) prompt engineering optimization, and 3) scalable generation of pre-training datasets. This pipeline integrates multiple language models, such as [3, 24, 33], alongside specialized multilingual models like Aya 35B [4], to ensure quality, cross-lingual data suitable for multilingual applications. Figure 1. Pretrain Dataset Preparation Process 2.1.1. Multilingual LLaVA Pretrain Dataset Our Multilingual Pretrain dataset builds on the LLaVA dataset [20], using image-text pairs and the corresponding GPT responses. Our approach started with sampling to select 30 diverse samples per language, guided by Length Analysis (LA), Flesch Reading Ease (FRE), and FleschKincaid Grade Level (FKGL) metrics [34] to obtain representative GPT response in English. cascaded translation and veriﬁcation process ensures quality: initial translation using Google Translate, followed by back-translation and ﬁnal human review with the help of [1, 3, 33] to generate the prompt engineering dataset in 8 languages. 2.1.2. Prompt Engineering and Evaluation During prompt engineering, we evaluate prompts per language using BLEU score-based process. We construct prompt evaluation dataset following Figure 1, translating six sample prompts into seven languages using Aya 35B. These translations are compared with reference translations of the prompt engineering dataset using BLEU [26] and N-gram scores [5, 31]. In Arabic, Chinese, French, Hindi, Japanese, Russian and Spanish, Preamble 6 consistently produces the highest BLEU scores per Ngram (typically 0.40.5), showing clear improvement from Type 5. Figure 2 shows Preamble 6 with the largest area in 1to 4-grams, indicating better ﬁdelity to the phrase level and consistent performance across languages. We adopt Preamble 6 as our ﬁnal prompt template, shown in Listing 1, and integrate it into our translation framework. 2.1.3. Translation Framework Design Our translation framework uses the best preamble identiﬁed in the prompt engineering evaluation step. The prompt includes 1) standardized input-output formatting to maintain uniformity across languages, 2) example-driven instruction sets tailored for complex translations, and 3) integrated validation triggers to ensure quality control. Using Aya 35B translation capabilities, our framework achieves more than 0.47 average BLEU scores in seven languages. Listing 1. Translation Instructions ## Instructions You are an expert in translations. Your job is to translate the input to Japanese in the given chat. Ensure that: {Specific Things to Consider while Translating} Note: {Extra Constraints on Output Generation} ## Examples ### Example 1 Input: {Input Sentence} Expected Output: {Translated Sentence} Figure 2. Radar chart of N-gram averages by preamble Multilingual Language Response Xa Multilingual Language Model fφ Projection Multilingual Vision Encoder Zv Hv Xv Image Hq Xq Multilingual Language Instruction Figure 3. Maya Architecture adapted from LLaVA [21] 2.1.4. Scalable Dataset Generation For large-scale dataset generation, we implement batch processing pipeline integrated with Aya 35B API as shown in Figure 1. The extracted GPT values from LLaVA Pretrain Dataset are passed through the Aya 35B batch-optimized API calling with intermediate translation checkpointing. The pipeline does necessary error handling and comprehensive logging for quality tracking. We implement version control for intermediate translations and maintain detailed debug logs, ensuring reproducibility and enabling systematic error analysis. The translation pipeline enables efﬁcient processing of the 550K samples while maintaining translation quality. 3. Multilingual Multimodal Instruction Tuning 3.1. Model Architecture We draw inspiration from LLaVA 1.5 [19] for model architecture. We employ the Aya-23 8B [4] model as our LLM fφ because of its multilingual capability. Aya-23 has 8 billion parameters, an 8K context window, and is trained across 23 languages. Our dataset, however, includes 8 of these 23 languages, aligning with our objective of optimizing Maya for diverse yet focused linguistic range. For the vision encoder, we opted for SigLIP1 [40] rather than CLIP [28], which is traditionally used in LLaVA. This choice is motivated by SigLIPs strong performance, multilingual adaptability, and capacity for variable-length patch sizes. Unlike CLIP, SigLIP supports scalable positional embeddings, enabling it to accept inputs of varying dimensions through positional embedding interpolation. This ﬂexibility makes SigLIP particularly suitable for our purpose. For each input image Xv, we get the visual features from SigLIP, Zv = g(Xv). trainable projection matrix then converts the image features Zv into language features Hv. 3.2. Pretraining For image-text alignment, we used projection matrix that brings image features Xv closer to language features. This projection matrix is simple 2-layer MLP with GELU 1siglip-base-patch16-256-multilingual from activation [12], as in LLaVA 1.5 [19]. Although we experimented with 4and 8-layer MLPs, the 2-layer conﬁguration consistently achieved the lowest training loss. Advanced alignment techniques, such as gated soft-attention in Flamingo [2], Q-Former in BLIP-2 [17], or pooling from MM1 [23] as alternatives to the projection layer, are set aside for future work. For each image Xv, we used the single-turn conversation data (X 1 , 1 a, . . . , , ) where is the total number of turns from LLaVA [20]. We pretrained Maya on the translated dataset in eight languages. Image inputs were cropped to 256x256 for compatibility with the SigLIP encoder. For training, we used 8xH100 GPUs with 80GB DRAM, per-device batch size of 32 and global batch size of 256. learning rate of 1e-3 and cosine scheduler were applied during training. The pretraining process only trains the projection matrix and takes about 20 hours. 3.3. Finetuning We instruction-ﬁnetuned our pretrained Maya model using the PALO 150K instruction-tuning dataset [22]. During ﬁnetuning, we observed that Low Rank Adaptation (LoRA) [14] produced suboptimal results, particularly when both adapter matrices and were updated with the same learning rate [11]. Based on these ﬁndings, we opted against using LoRA and instead implemented full ﬁnetuning on 8xH100 GPUs. Our ﬁnetuning conﬁguration used perdevice batch size of 16 and global batch size of 128. Finetuning process took about 48 hours. We kept both the vision encoder and the language encoders frozen during the training process. We did pretraining and ﬁnetuning for both versions of our dataset: the pretraining dataset translated into 7 languages. 4. Results We evaluate Maya on the PALO multilingual benchmark [22], as shown in Table 1. Although Maya was pretrained on only eight languages, it was ﬁnetuned on the PALO instruction dataset covering ten languages, and is thus evaluated on all ten. Maya outperforms all 7B models and is competitive with 13B models, surpassing LLaVA-13B on average and trailing PALO-13B by small margin. Among the eight common languages, Maya outperforms PALO-7B in ﬁve. We attribute this to Mayas multilingual pretraining, in contrast to PALOs English-only pretraining. Notably, Maya outperforms both PALO and LLaVA in Arabic for both size classes, likely due to its handling of root-based morphology and effective prompt design. We also evaluate Maya on English only dataset across various benchmarks as shown in Table 2. https://huggingface.co/google/siglip-base-patch16-256-multilingual Model English Chinese French Spanish Russian Japanese Arabic Hindi Bengali Urdu Avg. Maya (8B) LLaVA-7B PALO-7B LLaVA-13B PALO-13B 61.5 67.9 64.2 -6.4 69.5 65.5 -8. 61.7 55.7 55.7 +6.0 62.9 62.1 -1.2 61.0 62.4 58.3 -1. 67.5 66.4 -6.5 60.4 64.5 61.0 -4.1 64.6 65.9 -5. 62.2 55.3 57.4 +4.8 62.3 62.4 -0.2 63.7 59.2 57.5 +6. 65.3 60.6 -1.6 63.4 38.9 57.8 +5.6 37.2 56.9 +6. 64.0 29.4 57.6 +6.4 27.8 66.8 -2.8 50.8 13.9 51.7 -0. 20.4 53.5 -2.7 55 21.8 55.3 -0.3 22.1 59.6 -4. 60.4 46.9 57.7 +2.7 49.9 61.9 -1.5 Table 1. comparison of LLaVA and PALO with Maya on eight languages adapted from LLaVA-Bench (In-the-Wild). Values underlined indicate best performance within size class and values in bold indicate best performance across all models tested. We provide performance differences between Maya and the best competing model within the size classes where red indicates where Maya is performing better and blue indicates where Maya is performing worse than the best in the size class. Avg. represents the average over all the languages."
        },
        {
            "title": "VizWiz",
            "content": "ScienceQA TextVQA POPE-random MMBench MM-VeT MME (P+C) 57.79% 34.92% 70.27% 47.01% 85.30% 71.10% 29.8 72.45% Table 2. Accuracy of Maya models on English Language across multiple benchmarks. Abbreviations: P+C = Perception + Cognition. Figure 4. Example image from LLaVA-Bench (In-the-Wild) [20]. Figure 5 shows VQA outputs in various languages for Figure 4. The Bengali response is the most detailed, identifying both meat and the wooden table. Spanish, French, and Hindi mention the meat but miss the table. Chinese and Japanese outputs are similar to English in detail. Figure 5. Maya output for prompt (with image from Figure 4): Please describe the food in {language} in 1 sentence. 5. Conclusion though some traces may remain. Maya enables high-quality multilingual, multimodal AI content generation, addressing gaps in low-resource languages. We curate data to minimize harmful content, Future work includes reﬁning cross-modal alignment with alternative projection layers, unfreezing decoder layers for ﬁnetuning, and expanding pretraining to Bengali and Urdu. We also plan to scale the instruction-tuning dataset to 665K examples, improve translation via languagespeciﬁc preambles, and benchmark on datasets like PangeaBench [39] and CVQA [29] for broader, robust evaluation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 1, 3 [3] Anthropic. Introducing the next generation of Claude, 2024. 1, 2 [4] Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, et al. Aya 23: Open Weight Releases to Further Multilingual Progress. arXiv preprint arXiv:2405.15032, 2024. 1, 3 [5] Eric Brill, Radu Florian, John C. Henderson, and Lidia Mangu. Beyond n-grams: Can linguistic sophistication imIn COLING 1998 Volume 1: prove language modeling? The 17th International Conference on Computational Linguistics, 1998. [6] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: JointlyScaled Multilingual Language-Image Model. arXiv preprint arXiv:2209.06794, 2022. 1 [7] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. PaLI-X: On Scaling up Multilingual Vision and Language Model. arXiv preprint arXiv:2305.18565, 2023. 1 [8] Cohere. Aya Vision, 2025. 1 [9] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and PixMo: Open Weights and Open Data for State-of-theArt Multimodal Models. arXiv preprint arXiv:2409.17146, 2024. 1 [10] Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30K: Multilingual English-German image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 7074, Berlin, Germany, 2016. Association for Computational Linguistics. 1 [11] Souﬁane Hayou, Nikhil Ghosh, and Bin Yu. LoRA+: Efﬁcient Low Rank Adaptation of Large Models. arXiv preprint arXiv:2402.12354, 2024. 3 [12] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint arXiv:1606.08415, 2016. 3 [13] Musashi Hinck, Carolin Holtermann, Matthew Lyle Olson, Florian Schneider, Sungduk Yu, Anahita Bhiwandiwalla, Anne Lauscher, Shaoyen Tseng, and Vasudev Lal. Why do LLaVA Vision-Language Models Reply to Images in English? arXiv preprint arXiv:2407.02333, 2024. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685, 2021. 3 [15] Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The State and Fate of Linguistic Diversity and Inclusion in the NLP World. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online, 2020. Association for Computational Linguistics. 1 [16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International journal of computer vision, 123:3273, 2017. 1 [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [18] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 1 [19] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning, 2023. 1, [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning, 2023. 1, 2, 3, 4 [21] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge, 2024. 3 [22] Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao Anwer, Tim Baldwin, Michael Felsberg, and Fahad Khan. PALO: Polyglot Large Multimodal Model for 5B People. arXiv preprint arXiv:2402.14818, 2024. 1, 3 [23] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training. arXiv preprint arXiv:2403.09611, 2024. 3 [24] OpenAI. Hello GPT-4o, 2024. 1 [25] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-G: Generating Images in Context with Multimodal Large Language Models. ArXiv, abs/2310.02992, 2023. 1 [26] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [27] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding thy, and Graham Neubig. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. arXiv preprint arXiv:2410.16153, 2024. 1, 4 [40] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image PreIn Proceedings of the IEEE/CVF International Training. Conference on Computer Vision, pages 1197511986, 2023. 1, 3 Multimodal Large Language Models to the World. arXiv preprint arXiv:2306.14824, 2023. 1 [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 3 [29] David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. CVQA: Culturally-diverse Multilingual arXiv preprint Visual Question Answering Benchmark. arXiv:2406.05967, 2024. [30] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1 [31] Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. 2 [32] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding, 2021. 1 [33] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2023. 1, 2 [34] Textstat Contributors. Textstat: An Easy to Use Library to Calculate Statistics from Text, 2024. Python package for calculating readability statistics. 2 [35] Ashish Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522, 2022. 1 [36] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024. [37] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing uniﬁed representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4818 4829, 2024. 1 [38] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 1 [39] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoor6. Appendix 6.1. Author Afﬁliations 1 Cisco Meraki 2 Cohere Labs Community 3 Indiana University Bloomington 4 Imperial College London 5 Georgia Institute of Technology 6 The Alan Turing Institute 7 Bangladesh University of Engineering and Technology 8 University of Pennsylvania 9 IIT Bombay 10 TU Darmstadt 11 Capital One 12 IIT Dhanbad 13 MBZUAI 6.2. Acknowledgment We cannot express our gratitude enough to the Cohere Labs Community for bringing the open research community together which was instrumental to building Maya. The generous API credit for multilingual image-text translation from Cohere Labs Community has direct impact on where the model exists today."
        }
    ],
    "affiliations": []
}