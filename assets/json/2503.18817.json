{
    "paper_title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations",
    "authors": [
        "Jeonghyeon Kim",
        "Sangheum Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\\\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 1 8 8 1 . 3 0 5 2 : r Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations Jeonghyeon Kim Sangheum Hwang* Seoul National University of Science and Technology {mawjdgus, shwang}@ seoultech.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multimodal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multimodal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of naıve fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with posthoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy. 1. Introduction In open-world deployments, detecting out-of-distribution (OoD) data is crucial for ensuring the reliability and *Corresponding Author safety of machine learning models. Without robust outof-distribution detection (OoDD), models may produce erroneous decisions that could lead to severe consequences in various applications such as medical diagnosis and autonomous driving. Therefore, it is imperative that models not only accurately classify in-distribution (ID) data but also effectively identify and respond to OoD inputs. Recently, the emergence of vision-language models (VLMs) like CLIP [38] has expanded OoDD research to include multi-modal approaches that leverage textual information [5, 7, 14, 24, 31, 34, 46]. These multi-modal approaches enable zero-shot (ZS) OoDD without fine-tuning encoders. For example, MCM [31] calculates an OoD score as the maximum value of the scaled softmax applied to the cosine similarities between an image embedding and the ID text embeddings. NegLabel [14] not only uses ID texts but also additional textual information, negative labels distinct from ID texts, as auxiliary information for OoDD. While both methods utilize ID texts, NegLabel demonstrates superior OoDD performance compared to MCM. This enhanced performance is due to its ability to fully leverage textual information and effectively utilize the knowledge embedded in pretrained VLMs. Both methods are frequently employed as post-hoc OoD scoring functions due to their efficiency and effectiveness as ZS approaches. Despite the notable ZS OoDD performance, these approaches can be suboptimal for downstream datasets. As shown in Fig. 1b, even when utilizing textual information, ZS methods underperform in ID accuracy compared to single-modal finetuning (SMFT) methods (e.g., LP, FFT, LP-FT), which rely solely on visual input data. While ZS OoDD research has primarily focused on enhancing OoDD performance, it often overlooks the importance of maintaining high ID accuracy, which is crucial for the overall reliability of the model. Only few recent studies [17, 30] have focused on the impact of multi-modal fine-tuning (MMFT) and prompt learning (PL) from comprehensive perspective. These studies have shown the potential of fine-tuning methods in enhancing both ID accuracy and OoDD performance. Based on these findings, we explore the potential perfor- (a) Illustration of CMA (b) MOS benchmark: ID ACC vs. 1-FPR95 Figure 1. (a) illustrates the hyperspherical embedding space and the corresponding cosine similarity values between the dog image and photo of < label > text embeddings. Initially, the embedding space shows bipartite separation between images and texts (top) [26, 35, 39]. Through CMA, ID images and texts are brought closer together while maintaining clear separation from OoD texts (bottom). This alignment enhances the discriminability of ID data from negative concepts (i.e., OoD labels), thereby improving OoDD performance. In (b), uncolored shapes represent MCM, while colored shapes denote NegLabel. The arrows indicate the effect of NegLabel compared to MCM, demonstrating that our method enhances its effectiveness. Points closer to the top right indicate better ID accuracy and OoDD performance. mance improvements achievable by integrating MMFT or PL methods with effective OoD scoring functions. However, our experiments and analysis (see Sections 4.2 and 5) reveal that existing fine-tuning methods are incapable of effectively leveraging the pretrained textual knowledge embedded in VLMs, despite showing significantly better ID accuracy compared to ZS methods. We examine the underlying cause of this limitation and attribute it to the modality gap [26, 35, 39] within the ID embeddings in the multimodal representation space, where image and text embeddings are unexpectedly separated on hypersphere as illustrated in Figs. 1a (top) and 2a. The impact of the modality gap is minimal for classification tasks because selecting the highest similarity is relatively straightforward as shown in the upper right of Fig. 1a. However, when we fully leverage the textual information for OoDD, this gap may hinder the distinction between ID texts and negative concept labels, potentially leading to decrease in OoDD performance. We discuss this challenge further in Section 5. To tackle this issue, we propose novel MMFT method using regularization term called cross-modal alignment (CMA). Our methodology aims to fine-tune the image and text encoders to improve both OoDD performance and ID accuracy, forming an embedding space similar to the bottom hypersphere in Fig. 1a. The key idea behind CMA involves two main components: 1) alignment between the image and text modalities of the ID data to effectively separate negative text embeddings, and 2) correspondence between matching ID image-text pairs to maintain ID accuracy. Our approach is designed to bring image-text pairs of ID data closer together while effectively separating ID images and texts from negative concepts. To implement these ideas, we employ contrastive loss for each modality with an additional CMA regularization loss. We show that our method is equivalent to modeling and optimizing joint energy-based model with generative term in the hyperspherical embedding space, as detailed in Section 3. Using the widely adopted MOS benchmark [12] for VLM-based OoDD and the recent OpenOOD v1.5 benchmark datasets [49], which considers both Near-OoD and Far-OoD scenarios, we demonstrate that our method achieves superior performance in the ImageNet-1k dataset OoD setting. We also analyze the impact of reducing the ID modality gap on the hypersphere from the perspectives of uniformity and alignment. Our main contributions are summarized as follows: We introduce novel MMFT method called cross-modal alignment (CMA), which effectively aligns image-text embeddings on hypersphere. This alignment enhances both ID accuracy and OoDD performance by leveraging the pretrained textual knowledge in VLMs. We demonstrate that minimizing our objective is equivalent to maximizing the log-likelihood of joint energybased model on hyperspherical representation space. Our approach shows state-of-the-art performance on the MOS and standard OpenOOD v1.5 benchmarks. Also, we provide an in-depth analysis of how the hyperspherical embedding space built by CMA enhances OoDD. 2. Related works Out-of-distribution detection (OoDD) enables models to distinguish between ID and OoD samples while maintaining accuracy on ID data. variety of OoDD techniques have been proposed for visual inputs [4, 10, 11, 16, 18 20, 27, 32, 33, 40, 41, 45]. Recently, OoDD research has entered new paradigm with the emergence of VLMs, such as CLIP [38], which leverage the textual information for OoDD [7, 14, 31, 37, 46, 51, 52]. Among these, several ZS OoDD methods [5, 7, 14, 31, 46] have been introduced. ZOC [5] performs OoDD by generating OoD candidate labels using text description generator, while CLIPN [46] utilizes an additional text encoder capable of interpreting negative prompts. Certain ZS OoDD methods, such as MCM [31] and NegLabel [14], have demonstrated remarkable performance as post-hoc OoDD scoring functions. Another approach, PL [24, 34], involves training only single or few layers to learn prompts suitable for the task. LoCoOp [34] employs an OoD regularization technique that treats segments of an images local features, such as ID-irrelevant elements like backgrounds, as OoD features during the training process. NegPrompt [24] develops set of negative prompts, each representing negative aspect of given class label, to clearly delineate the boundaries between ID and OoD images. Previous research has primarily focused on leveraging the capabilities of VLMs by using textual information. However, the aforementioned methods, which either freeze the encoder, partially utilize it, or train additional layers, may be suboptimal in fully leveraging the potential of pretrained VLMs. Recent works [17, 30] have improved OoDD performance through MMFT and PL methods, such as FLYP [8] and CoOp [52]. Building on these studies, we investigate fine-tuning strategies to maximize the use of VLMs knowledge. We emphasize the potential of MMFT, which directly engages with the multi-modal representation space through fine-tuning all parameters, in contrast to ZS or PL methods. We focus on reducing the modality gap [13, 26, 29, 35, 39], which can hinder distinguishability between ID and OoD texts. While certain MMFT methods, such as m2-mix [35], address this gap, they are mainly aimed at robust fine-tuning for downstream tasks. In contrast, our method, CMA, is specifically designed to reduce the modality gap in ID embeddings, enabling more effective utilization of VLMs pretrained knowledge for OoDD. 3. Methods 3.1. Preliminaries and Motivation CLIP uses contrastive learning approach to learn multimodal representations in hyperspherical space by leveraging large-scale image and text data. The contrastive learning objective in CLIP trains both an image encoder () and text encoder g() simultaneously to ensure that matching image-text pairs are close in shared representation space, while non-matching pairs are distant. Given batch consisting of pairs of images and their corresponding text descriptions (i.e., positive texts) {(Ik, Tk)}B k=1, the pairs of embeddings {(ik, tk)}B k=1 are computed. Note that the dot product ik tk is the cosine similarity between ik and tk since () and g() produce L2normalized unit vectors (i.e., projected onto hypersphere). Therefore, CLIP loss can be formulated as follows: k=1 = {(f (Ik), g(Tk))}B LCLIP = 1 2B where (cid:88) (Lk image + Lk text), k=1 Lk image = log exp((ik tk/τ )) j=1 exp((ik tj)/τ ) (cid:80)B , Lk text = log exp((ik tk/τ )) j=1 exp((ij tk)/τ ) (cid:80)B . (1) (2) (3) Here, τ is temperature parameter that scales the similarities and is set to learnable parameter in CLIP. The learned representations are commonly used for transfer learning, and recently, FLYP [8] has shown that employing the CLIP loss for fine-tuning is highly effective for both ID accuracy and OoD generalization. image = (ik tk)/τ + log (cid:80)B The contrastive loss is fundamentally focused on disIt is known that the numerator of this loss crimination. corresponds to the alignment term, while the denominator corresponds to the uniformity term [44, 47]. However, interpreting the denominator of the CLIP loss, which applies to multi-modal contrastive learning, as uniformity term can be misleading. Specifically, Lk image can be decomposed as Lk j=1 exp((ik tj)/τ ). When an image is given, the first term increases the similarity with the positive text, while the second term decreases the similarity with all texts, including the positive text. Consequently, instead of uniformly distributing the text embeddings on the hypersphere, they form clusters distinct from the image embeddings. This tendency is further reinforced when the temperature parameter is sufficiently small, as even slight increase in similarity with the positive text compared to other texts can significantly reduce the loss. The same applies when text is given. Therefore, the CLIP loss primarily encourages contrastive learning across modalities rather than within each modality, leading to distinct clusters for each modalitys embeddings as discussed in [26] and [35]. Given that CLIP is trained on vast amount of image and text data, it demonstrates remarkable OoDD performance even in ZS setting [14, 31, 46]. Among the ZS methods leveraging CLIPs rich multi-modal representations, NegLabel [14] is particularly effective. This approach identifies labels that are distant from ID texts and uses these negative labels for OoDD. Specifically, when an image is given, NegLabel determines whether the image is OoD by comparing the distances between the image and the ID texts with those between the image and the negative labels. Fine-tuning methods like FLYP enhance discrimination for ID classes, thereby achieving strong OoDD performance. However, they do not effectively utilize the pretrained negative concepts as NegLabel does. This limitation arises because these methods retain the embedding tendencies of the original CLIP loss, resulting in clustered embeddings. Consequently, the text embeddings still form clusters, which limits the improvement of separability from negative labels. 3.2. Cross-Modal Alignment To effectively utilize pretrained multi-modal representations for OoDD through fine-tuning, it is essential to ensure that the image-text pairs corresponding to ID become closer, while also sufficiently separating the ID images and texts from negative concepts (i.e., negative labels). To obtain representations that satisfy these properties, we propose cross-modal alignment (CMA) loss for each modality that mitigates the separation effect between the image and text modalities of the ID data, which can be computed as: exp (ik tj/τ ) , (4) Lk imageCMA = log Lk textCMA = log (cid:88) j=1 (cid:88) j=1 The proposed CMA losses work by globally increasing the similarity between ID image and text pairs, causing the ID modalities to be well-aligned on the hypersphere. As result, the separability from the pretrained negative concepts can be enhanced, allowing for more effective OoDD. By introducing hyperparameter λ that controls the alignment strength, the final loss can be written as: LCMA = LCLIP + λ 2B (cid:88) k=1 (Lk imageCMA + Lk textCMA ). (6) Equivalently, Lk (1 λ) log (cid:80)B the same manner. image in Eq. 2 is revised to (ik tk)/τ + j=1 exp((ik tj)/τ ), and Lk text is revised in OoD score. To fully exploit the pretrained negative concepts, we follow an OoD scoring scheme of NegLabel. Given text embeddings of ID classes {tk}C k=1 and negative text embeddings {tk}M k=1 obtained by the NegMining algorithm [14], the OoD score for an image is obtained by: S(I) = (cid:80)C k=1 exp (i tk/τ ) + (cid:80)M k=1 exp (i tk/τ ) (cid:80)C k=1 exp (cid:0)i tk/τ (cid:1) , (7) where represents the image embedding of from the image encoder (). Relation to EBMs. The proposed CMA objective is closely related to energy-based models (EBMs) [23]. We demonstrate that minimizing our objective in Eq. 6 is equivalent to maximizing the log-likelihood of joint EBM in hyperspherical embedding space. This equivalence allows us to extend the EBM framework to the multi-modal domain and demonstrates its effectiveness for OoDD tasks. Specifically, our CMA objective regulates the importance of the generative term (i.e., the log marginal density) in the EBM framework through the hyperparameter λ. By defining the similarity between image embeddings and text embeddings as the negative energy Eθ(i, t) = (i t), the joint distribution of images and texts can be expressed as Gibbs-Boltzmann distribution as qθ(i, t) = exp(Eθ(i, t)/τ )/Z(θ) where is the partition function, (cid:82) i.e., Z(θ) = (cid:82) exp(Eθ(i, t)/τ ) dt di which is generally intractable. Note that θ represents learnable parameter set of () and g(). Given data distribution p, θ can be estimated by maximizing the expected log-likelihood of qθ under p: + 1 Ep [log qθ(it) + log qθ(t)] . (8) Hereafter, we will consider only the first term on the righthand side as the same analogy applies to the second term. In EBMs, Ep [log qθ(ti)] and Ep [log qθ(i)] are interpreted as discriminative term and generative term, respectively. Given samples from p, the discriminative term can be estimated by the contrastive loss function as described in [15], and therefore, the empirical form of it corresponds to Lk image in Eq. 2. Therefore, the contrastive loss can be equated to constrained form of EBMs, which does not consider the generative term. The generative term can be written as Ep [log qθ(i)] = Ep[Eθ(i)] log Z(θ), and the empirical estimation is generally intractable due to Z(θ). However, we can ignore the partition function since our embeddings and are in the hyperspherical space. The von Mises-Fisher (vMF) distribution is probability distribution on the hypersphere in Rp for the p-dimensional unit vectors. Given be the mean direction of vMF, the probability density function for the image embeddings is exp (ij tk/τ ) . (5) max θ Ep [log qθ(i, t)] = max θ 1 2 Ep [log qθ(ti) + log qθ(i)] defined as qθ(it) = Cp(1/τ ) exp (Eθ(i, t)/τ ) , where Cp(1/τ ) denotes the normalization factor. Thus, the log marginal density of and its empirical estimate can be written as: to perform well in both classification and OoD generalization [8]. m2-mix aims to mitigate the modality gap on the hypersphere by utilizing hard negative embeddings obtained through the mixing of image and text embeddings. log qθ(i) = log (cid:90) qθ(it)qθ(t) dt qθ(it) log log (cid:88) tB (cid:88) tB exp (Eθ(i, t)/τ ) + (11) (9) (10) where constant = log Cp(1/τ ). It should be noted that the first term on the right-hand side corresponds to Lk imageCMA : maximizing log qθ(i) is equivalent to minimizing Lk imageCMA . Consequently, while the contrastive loss focuses on solely discrimination (i.e., the log conditional likelihood log qθ(ti)), our method also considers generation (i.e., the log marginal density log qθ(i)). 4. Experiments 4.1. Experimental Setup Datasets. We evaluate our method in comparison to various previous approaches using the ImageNet-1k OoDD benchmarks, MOS [12], and OpenOOD v1.5 [49]. Following the MOS, we use ImageNet-1k as the ID dataset along with semantically diverse OoD datasets such as iNaturalist [42], SUN [48], Places [50], and Textures [2]. Additionally, we employ the standard OpenOOD v1.5 benchmarks to assess performance in both Nearand Far-OoD scenarios, also using ImageNet-1k as the ID dataset. For Near-OoD, datasets such as SSB-hard [43] and NINCO [1], which are semantically more challenging to distinguish from the ID dataset, are included. For Far-OOD, we utilize datasets such as iNaturalist, Textures, and OpenImage-O [22]. Baselines. We compare the performance of our method for OoDD against baselines categorized into four main training strategies1: (1) ZS, (2) PL, (3) SMFT, and (4) MMFT. For comprehensive assessment, we compare both OoDD performance and ID accuracy. For ZS methods, we employ MCM [31], NegLabel [14] and CLIPN [46]. In PL, we consider CoOp [52] and LoCoOp [34]. SMFT methods, common approach in visual transfer learning, involve training the pretrained visual encoder on the ID dataset alongside classification layer. We employ linear-probing (LP), full-fine-tuning (FFT), and linear-probing then fine-tuning (LP-FT) [21]. For MMFT, we use FLYP [8] and m2-mix [35]. FLYP mimics the CLIP pre-training scheme for fine-tuning and has been proven 1For SMFT and MMFT, results are obtained using the entire dataset. For PL, results are reproduced using the default 16-shot setting. Performance across various shot settings is reported in Appendix B.1. OoD scoring functions. For PL and MMFT, we utilize both MCM and NegLabel2 as OoD scores. For SMFT, which cannot utilize textual information, we adopt different OoD scoring methods. In this case, we employ commonly used post-hoc methods that serve as standard baselines in visual OoDD research, such as MSP [9], ODIN [25], and Energy [27]. Evaluation metrics. For evaluation, we use the following metrics to provide comprehensive assessment of performance: (1) the false positive rate of OoD images when the true positive rate of in-distribution images is at 95% (FPR95), (2) the area under the receiver operating characteristic curve (AUROC), and (3) ID accuracy. Implementation details. For our experiments3, we use the pretrained CLIP-B/16 model, which is widely adopted in prior research [14, 31, 34, 46, 52]. In our method, we perform hyperparameter sweeps over learning rates from the set {1e-4, 1e-5, 1e-6} and the hyperparameter λ for alignment strengths {1e-1, 1e-2, 1e-3}, using batch size of 512.4 These settings are based on Goyal et al. [8]. We apply early stopping based on the accuracy of the ID validation set. Additional experimental details can be found in Appendix A. 4.2. Experimental Results Tables 1 and 2 show the FPR95 and AUROC performances on the MOS and OpenOOD v1.5 benchmark datasets, respectively, while Table 3 presents the ID accuracy of each method. These results demonstrate the effectiveness of our proposed method in both OoDD and ID classification. We observe that SMFT methods, which rely solely on the visual encoder, are less effective for OoDD compared to other approaches leveraging textual information. When utilizing multi-modal foundation models like CLIP, it becomes clear that textual information plays crucial role in OoDD. For the MOS and OpenOoD v1.5 Far-OoD benchmarks in Tables 1 and 2, ZS NegLabel improves FPR95 by 15.66%, 17.93% and AUROC by 3.15%, 3.42% over ZS MCM. Similarly, both PL and MMFT benefit significantly from NegLabel. Notably, our proposed method surpasses the existing state-of-the-art performance, achieving 2Using the same NegMining approach as NegLabel. The results reported in Tables 1 and 2 are based on experiments conducted without the grouping strategy, which does not affect the performance superiority. For more details, see Appendix B.2. 3Our code is available at https://github.com/ma-kjh/CMA-OoDD. 4Ablation studies on various prompts and the hyperparameter λ are reported in Appendices B.3 and B.4. The performance superiority remains unchanged regardless of the prompts used. Table 1. Comparison of OoDD performance on the MOS benchmark dataset. and denote that higher and lower values are better, respectively. The highest values are highlighted in bold, and the second-highest values are underlined. Methods Zero-Shot (ZS) MCM CLIPN NegLabel Prompt Learning (PL) CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel iNaturalist Places FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC Textures Average SUN 32.28 94.40 39.33 92.28 44.94 89.83 57.98 85.99 43.63 90.63 23.94 95.27 26.17 93.93 33.45 92.28 40.83 90.93 31.10 93.10 2.30 99.37 23.23 95.14 39.85 90.98 46.49 89.64 27.97 93.78 26.37 94.49 35.23 92.59 43.29 89.67 41.47 90.62 36.59 91.84 4.95 98.90 25.76 94.59 30.07 93.33 44.35 89.59 26.28 94.10 23.08 95.46 33.39 93.25 40.74 90.52 40.75 91.14 34.49 92.59 99.25 46.63 90.58 55.44 87.65 46.03 89.85 37.83 91.83 3.19 Single-modal Fine-tuning (SMFT) LPMSP LPODIN LPEnergy FFTMSP FFTODIN FFTEnergy LP-FTMSP LP-FTODIN LP-FTEnergy Multi-modal Fine-tuning (MMFT) FLYPMCM FLYPNegLabel m2-mixMCM m2-mixNegLabel CMAMCM CMANegLabel 41.61 90.87 67.03 81.07 66.03 81.25 62.82 81.46 59.37 83.66 28.46 94.90 51.13 89.17 51.13 88.39 53.83 86.40 46.14 89.72 31.09 94.48 58.42 86.95 65.90 83.68 75.48 78.23 57.72 85.83 51.66 87.87 70.55 79.86 71.77 79.42 70.82 78.42 66.20 81.39 19.05 96.38 39.24 90.75 46.93 88.52 52.89 85.93 39.53 90.40 24.46 95.62 58.98 84.77 59.92 83.89 55.60 84.45 49.74 87.18 44.02 89.81 66.60 80.78 69.20 80.06 65.59 79.44 61.35 82.52 27.22 94.28 46.65 87.80 52.26 86.00 53.58 84.18 44.93 88.06 29.16 93.93 48.27 87.47 53.24 85.68 54.57 83.81 46.31 87.72 24.86 94.35 39.81 90.58 47.92 87.16 41.19 89.34 38.44 90.36 99.31 23.48 94.82 37.23 90.86 41.70 89.27 26.39 93.57 3.16 22.41 95.61 39.18 91.85 47.07 88.72 43.44 90.13 38.02 91.58 99.43 23.03 94.86 35.55 91.21 36.65 90.68 24.40 94.05 2.39 22.95 95.65 40.01 91.78 48.83 88.41 44.93 89.87 39.18 91.43 99.62 16.84 96.36 27.65 93.11 33.58 91.64 19.93 95.13 1.65 an FPR95 of 19.93% and an AUROC of 95.13% on the MOS benchmark. Our method also demonstrates strong performance compared to baselines for the OpenOOD v1.5 Nearand Far-OoD scenarios, as shown in Table 2, though it shows only moderate results on the SSB-hard dataset. In this case, FFTEnergy achieves better results than other methods. This observation suggests that while OoDD methods leveraging textual information are generally effective in Far-OoD settings, the advantage becomes less pronounced in semantically similar datasets, e.g., Near-OoD scenarios.5 Moreover, the results of m2-mix in Tables 1 and 2 indicate that mitigating the modality gap improves OoDD performance by effectively leveraging the pretrained knowledge. In particular, our CMA shows that reducing the modality gap within ID embeddings is more effective for enhancing the distinguishability between ID and OoD texts. While each method using textual information shows performance improvements when provided with negative texts (i.e., NegLabel) rather than relying solely on ID texts, the 5To further investigate the behavior in Near-OoD scenarios, we perform additional experiments on ImageNet-1k splits [36], as described in Appendix B.5. PL and MMFT methods do not consistently deliver superior OoDD performance compared to ZS. In Table 1, the results reveal that CoOpNegLabel outperforms ZS NegLabel on the Places and Textures datasets, yet demonstrates inferior performance on the iNaturalist and SUN datasets. LoCoOpNegLabel also shows much lower performance compared to ZS NegLabel on the SUN and Places datasets, despite being specialized for OoDD. For FLYPNegLabel, we observe that its AUROC performance is lower than that of ZS NegLabel across all datasets, trailing by an average of 0.21%. These results suggest that existing PL or MMFT methods do not enhance the distinction between ID and negative texts as effectively as ZS NegLabel. This observation demonstrates that not considering the use of pretrained knowledge can lead to performance degradation, even when the methods are fine-tuned for specific downstream datasets. In contrast, our method CMANegLabel effectively leverages these negative concepts to perform OoDD, which we further analyze from the perspective of modality gap using various metrics in Section 5. Table 3 presents ID accuracy on ImageNet-1k, demonstrating that our proposed CMA performs well in both Table 2. Comparison of OoDD performance on the OpenOOD v1.5 benchmark dataset. The terms Near-OoD and Far-OoD refer to the average OoDD performance across datasets in each respective scenario. Methods Zero-Shot (ZS) MCM NegLabel Prompt Learning (PL) CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel Single-modal Fine-tuning (SMFT) LPMSP LPODIN LPEnergy FFTMSP FFTODIN FFTEnergy LP-FTMSP LP-FTODIN LP-FTEnergy Multi-modal Fine-tuning (MMFT) FLYPMCM FLYPNegLabel m2-mixMCM m2-mixNegLabel CMAMCM CMANegLabel SSB-hard iNaturalist FPR95AUROCFPR95AUROCFPR95AUROCFPR95AUROCFPR95AUROCFPR95AUROCFPR95AUROC Textures Openimage-O Far-OoD Near-OoD NINCO 88.32 65.05 79.23 74.44 83.78 69.74 32.25 94.41 55.62 87.11 44.34 91.23 44.07 90.92 81.87 71.32 69.82 77.09 75.85 74.21 2.32 99.36 44.98 90.56 31.10 93.10 26.14 94.34 86.10 67.72 77.24 74.59 81.67 71.16 26.31 94.50 38.38 91.92 37.64 92.08 34.11 92.83 68.20 78.72 57.71 84.20 62.96 81.46 4.96 98.90 42.74 90.28 23.18 95.25 23.63 94.81 87.38 66.23 77.04 73.46 82.21 69.84 22.98 95.48 37.89 92.27 37.44 92.09 32.77 93.28 76.03 74.42 66.51 80.96 71.27 77.69 3.70 99.16 43.87 90.59 29.29 93.65 25.62 94.47 85.77 68.15 73.94 79.64 79.86 73.89 41.37 90.90 60.02 84.11 52.91 87.14 51.43 87.38 85.15 71.79 75.08 81.91 80.12 76.85 28.45 94.92 50.64 88.68 41.93 91.86 40.34 91.82 87.93 68.80 79.40 78.14 83.67 73.47 31.09 94.48 73.84 80.60 58.20 88.08 54.38 87.72 81.00 72.77 69.43 81.54 75.21 77.16 51.66 87.85 68.74 80.46 56.45 86.31 58.95 84.88 65.15 82.94 56.71 87.16 60.93 85.05 19.06 96.37 49.88 87.53 25.18 94.83 31.37 92.91 54.21 85.91 55.47 87.18 54.84 86.54 24.55 95.62 52.97 86.41 29.32 93.95 35.61 91.99 80.02 72.56 69.16 80.78 74.59 76.67 43.96 89.82 63.24 81.49 50.43 87.77 52.54 86.36 73.52 77.83 64.13 82.38 68.82 80.10 27.34 94.28 50.85 86.34 32.84 93.05 37.01 91.22 75.81 76.48 65.08 82.05 70.44 79.27 29.22 93.93 51.84 86.04 34.36 92.85 38.47 90.94 73.89 75.13 61.88 79.93 67.89 77.53 24.73 94.36 37.48 90.42 29.66 93.20 30.62 92.66 67.56 80.59 59.76 83.33 63.66 81.96 3.14 99.31 37.77 91.04 22.87 94.22 21.26 94.86 77.63 77.78 66.17 83.43 71.90 80.60 22.94 95.80 43.10 90.85 29.63 94.27 31.89 93.64 66.57 79.72 54.12 84.93 60.35 82.32 2.34 99.43 32.48 92.33 17.80 95.64 17.54 95.80 78.28 74.95 65.64 82.18 71.96 78.56 23.03 95.65 41.47 91.18 29.25 94.35 31.25 93.72 62.80 81.71 49.70 87.21 56.25 84.46 1.65 99.62 29.55 93.21 14.65 96.57 15.29 96.47 Table 3. ID Accuracy on ImageNet-1k Methods ID Acc. Zero-Shot (ZS) MCM & NegLabel Prompt Learning (PL) CoOp LoCoOp Single-modal Fine-tuning (SMFT) LP FFT LP-FT Multi-modal Fine-tuning (MMFT) FLYP m2-mix CMA 66. 71.95 71.72 79.22 81.44 81.48 82.58 82.67 82.64 OoDD and ID classification, exceeding the performance of many other methods. Our results indicate that fine-tuning methods such as SMFT and MMFT achieve significantly better ID accuracy compared to ZS or PL methods. Notably, Tables 1 and 2 highlight the advantages of utilizing textual information for OoDD, with ZS, PL, and MMFT outperforming SMFT. Therefore, fine-tuning the encoders is effective for improving ID accuracy, while leveraging textual information is particularly beneficial for OoDD. Consequently, our proposed fine-tuning method, CMA, designed to fully leverage pretrained textual knowledge, achieves superior overall performance in both ID classification and OoDD. 5. Analysis of Hyperspherical Embeddings In this section, we aim to provide insights into how the embedding space shapes and leverages the pretrained knowledge of VLMs to achieve effective performance in OoDD. As shown in Figs. 1a (top) and 2a, the embedding space of CLIP is bipartite, with images and texts being separated. This phenomenon is referred to as the modality gap [26, 35]. It is important to note that effectively utilizing textual information for OoDD requires ID images and ID texts to be closer together in the embedding space than OoD texts. However, the modality gap can hinder this ideal scenario. Reducing the modality gap between ID images and ID texts is expected to enhance the distinguishability between ID and OoD texts, as illustrated in Fig. 1a (bottom). To quantify and analyze the modality gap, we compute the uniformity and alignment measures using ImageNet1k validation data. These measures are computed as follows [35]: Uniformity := log Ep (cid:2)exp (cid:0)2ei ej2 (cid:1)(cid:3) , (12) Alignment := Ep (cid:20) ii ti2 2 min j=i ii tj2 (cid:21) . (13) Embeddings ei and ej can represent either ID image embeddings ii, ij or ID text embeddings ti, tj, or any combination thereof. We explore variations in the input combinations of Eqs. 12 and 13 to quantitatively assess different aspects of uniformity and alignment in the embedding space. i=1 {tj}n We consider four specific variants of the uniformity metric and two of the alignment metric. To assess the spread among all ID embeddings, we calculate the uniformity value (denoted as Uni-All) as in Eq. 12, where ei, ej {ii}n j=1. Uni-I and Uni-T represent uniformity measured only among ID image embeddings (i.e., ei, ej i=1) and ID text embeddings (i.e., ei, ej {ti}n {ii}n i=1), respectively. We also compute the uniformity of cross-modal embeddings (Uni-CM) across all ID image-text pairs, where ei {ii}n j=1, and Uni-CMM, which uses only matching ID image-text pairs from Uni-CM, i.e., (ei, ej) {(ii, ti)}n i=1. Both metrics serve as indicators of the modality gap. When the Uni-CM value is comparable to Uni-All, Uni-T, and Uni-I, and the Uni-CMM value is lower than these metrics, this indicates reduced modality gap. i=1 and ej {tj}n The uniformity metrics quantify only the distance within the ID embedding space and do not capture the critical distance to OoD texts. Therefore, we measure alignment metrics from two different perspectives using Eq. 13. The first term represents the distance between the matching ID image and text embeddings. Align-ID and Align-OoD are calculated by setting tj in the second term (i.e., minimum distance) to ID and OoD text embeddings, respectively. Given an ID image, Align-ID indicates the degree to which the matching ID text embedding is distinguishable from other ID text embeddings, while Align-OoD represents the distinguishability of the matching ID text embedding from other OoD text embeddings. Table 4. Summary of uniformity and alignment values Uniformity Alignment Methods Uni-All Uni-I Uni-T Uni-CM Uni-CMM Align-ID Align-OoD 1.594 0.874 1.320 2.346 ZS CoOp 1.702 0.874 1.701 2.343 LoCoOp 1.419 0.874 0.904 2.290 1.821 1.395 1.215 2.437 FLYP m2-mix 1.208 0.947 1.441 1.584 0.862 0.719 0.993 1.323 CMA 2.141 2.073 2.080 2.124 1.014 0.725 0.023 0.032 0.024 0.052 0.104 0.114 0.035 0.069 0.066 0.068 0.130 0.138 In Table 4, we observe that CMA achieves the smallest Uni-CM and Uni-CMM values, as well as the highest Align-ID and Align-OoD values among all methods. These results demonstrate CMAs effectiveness in both OoDD performance and ID accuracy. Notably, Uni-CMM is much smaller than Uni-T and is close to Uni-I, highlighting CMAs strength in aligning cross-modal embeddings of ID data, as illustrated in Figs. 1a (bottom) and 2. In contrast, other methods show larger modality gaps. For example, Uni-CM in ZS is approximately 2.7 and 1.8 times larger than Uni-I and Uni-T, respectively. Although Uni-CMM is smaller than Uni-CM, it remains significantly larger than intra-modal distances, indicating that matching image-text pairs are still considerably more distant than embeddings within each modality. This supports the scenario depicted in Fig. 1a (top). Moreover, FLYP shows UniCM and Uni-CMM values that exceed those of Uni-I and Uni-T, despite fine-tuning both encoders. This finding explains why FLYPNegLabel performs worse than ZS NegLabel on the MOS benchmark datasets, even though it achieves higher Align-ID and Align-OoD values than ZS. Similar patterns are observed in PL methods like CoOpNegLabel and LoCoOpNegLabel. This analysis shows that larger modality gap can impair the distinguishability between ID and OoD texts, even when ID image-text pairs are more aligned. Meanwhile, m2-mix achieves performance close to CMAs results. Although not as effective as CMA, m2-mix shows improved OoDD performance and reduction in the modality gap. These results suggest that, even though it was developed for different purposes, addressing the modality gap has significant impact on effectively utilizing OoD text. (a) Zero-Shot (b) FLYP (c) m2-mix (d) CMA Figure 2. Visualization of DOSNES [28] on ImageNet-1k validation dataset and the MOS benchmark dataset. Blue and orange represent ID image and ID text embeddings, respectively, while green and red represent OoD image and OoD text embeddings. Additional visualizations are provided in the Appendix C. 6. Conclusion In this paper, we introduce cross-modal alignment (CMA), novel multi-modal fine-tuning (MMFT) method that achieves state-of-the-art performance in both OoDD and ID accuracy. We establish theoretical connection between CMA and EBM by incorporating the generative term into contrastive learning. Our experimental results show how the CMA regularizer enhances the hyperspherical structure of the embedding space, reduces the modality gap, and strengthens alignment, leading to better OoD detection and ID classification. We plan to further explore the effectiveness of using auxiliary negative labels in MMFT training."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by the National Research Foundation of Korea (NRF) under Grant [RS-2024-00352184] and [RS-2024-00354675] funded by the Ministry of Science and ICT (MSIT)."
        },
        {
            "title": "References",
            "content": "[1] Julian Bitterwolf, Maximilian Muller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. In International Conference on Machine Learning. PMLR, 2023. 5, 11 [2] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2014. 5, 11 [3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image In Proceedings of the IEEE/CVF Conference on database. Computer Vision and Pattern Recognition, 2009. 11 [4] Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting out-ofdistribution objects. Advances in Neural Information Processing Systems, 2022. 3 [5] Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pre-trained model clip. In Proceedings of the AAAI conference on artificial intelligence, 2022. 1, 3 [6] Christiane Fellbaum. WordNet: An electronic lexical database. MIT press, 1998. 11 [7] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. Advances in Neural Information Processing Systems, 2021. 1, 3 [8] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3, 5, 11 [9] Dan Hendrycks and Kevin Gimpel. baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2016. 5 [10] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. In International Conference on Machine Learning. PMLR, 2022. 3 [11] Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 3 [12] Rui Huang and Yixuan Li. Mos: Towards scaling out-ofdistribution detection for large semantic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2, 5, [13] Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, and Trishul Chilimbi. Understanding and constructing latent modality In Prostructures in multi-modal representation learning. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3 [14] Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han. Negative label guided ood detection with pretrained vision-language models. In International Conference on Learning Representations, 2024. 1, 3, 4, 5, 11, 13 [15] Beomsu Kim and Jong Chul Ye. Energy-based contrastive learning of visual representations. Advances in Neural Information Processing Systems, 2022. 4 [16] Jihyo Kim, Jiin Koo, and Sangheum Hwang. unified benchmark for the unknown detection capability of deep neural networks. Expert Systems with Applications, 229: 120461, 2023. 3 [17] Jeonghyeon Kim, Jihyo Kim, and Sangheum Hwang. Comparison of out-of-distribution detection performance of clipbased fine-tuning methods. In International Conference on Electronics, Information, and Communication, 2024. 1, 3 [18] Jihyo Kim, Seulbi Lee, and Sangheum Hwang. Reflexive guidance: Improving OoDD in vision-language models via self-guided image-adaptive concept generation. arXiv preprint arXiv:2410.14975, 2025. [19] Polina Kirichenko, Pavel Izmailov, and Andrew Wilson. Why normalizing flows fail to detect out-of-distribution data. Advances in Neural Information Processing Systems, 2020. [20] Jiin Koo, Sungjoon Choi, and Sangheum Hwang. Generalized outlier exposure: Towards trustworthy out-ofdistribution detector without sacrificing accuracy. Neurocomputing, 577:127371, 2024. 3 [21] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2021. 5, 11 [22] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Journal of Computer Vision, 2020. 5 [23] Yann LeCun, Sumit Chopra, Raia Hadsell, Ranzato, and Fujie Huang. tutorial on energy-based learning. Predicting structured data, 2006. 4 [24] Tianqi Li, Guansong Pang, Xiao Bai, Wenjun Miao, and Jin Zheng. Learning transferable negative prompts for outof-distribution detection. arXiv preprint arXiv:2404.03248, 2024. 1, 3 [25] Shiyu Liang, Yixuan Li, and Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations, 2018. [26] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 2022. 2, 3, 7 [27] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems, 2020. 3, 5 [28] Yao Lu, Jukka Corander, and Zhirong Yang. Doubly stochastic neighbor embedding on spheres. Pattern Recognition Letters, 2019. 8 [29] Xiang Ma, Xuemei Li, Lexin Fang, and Caiming Zhang. Bridging the modality gap: Dimension information alignment and sparse spatial constraint for image-text matching. In Proceedings of the 32nd ACM International Conference on Multimedia, 2024. 3 [30] Yifei Ming and Yixuan Li. How does fine-tuning impact outof-distribution detection for vision-language models? International Journal of Computer Vision, 2024. 1, [31] Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-distribution detection with vision-language representations. Advances in Neural Information Processing Systems, 2022. 1, 3, 5, 11 [32] Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings for out-of-distribution detection? In International Conference on Learning Representations, 2022. 3 [33] Atsuyuki Miyai, Qing Yu, Go Irie, and Kiyoharu Aizawa. Can pre-trained networks detect familiar out-of-distribution data? arXiv preprint arXiv:2310.00847, 2023. 3 [34] Atsuyuki Miyai, Qing Yu, Go Irie, and Kiyoharu Aizawa. Locoop: Few-shot out-of-distribution detection via prompt learning. Advances in Neural Information Processing Systems, 2024. 1, 3, 5, 11, 13 [35] Changdae Oh, Junhyuk So, Hoyoon Byun, YongTaek Lim, Minchul Shin, Jong-June Jeon, and Kyungwoo Song. Geodesic multi-modal mixup for robust fine-tuning. Advances in Neural Information Processing Systems, 2024. 2, 3, 5, 7, 8, 11 [36] Andres Palechor, Annesha Bhoumik, and Manuel Gunther. Large-scale open-set classification protocols for imagenet. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 4251, 2023. 6, 18 [37] Sangha Park, Jisoo Mok, Dahuin Jung, Saehyung Lee, and Sungroh Yoon. On the powerfulness of textual outlier exposure for visual ood detection. Advances in Neural Information Processing Systems, 2024. [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PMLR, 2021. 1, 3 [39] Peiyang Shi, Michael Welle, Marten Bjorkman, and Danica Kragic. Towards understanding the modality gap in clip. In ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls, 2023. 2, 3 [40] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Outof-distribution detection with deep nearest neighbors. In International Conference on Machine Learning. PMLR, 2022. 3 [41] Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Nonparametric outlier synthesis. In International Conference on Learning Representations, 2022. 3 [42] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. 5, [43] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: good closed-set classifier is all you need. In International Conference on Learning Representations, 2021. 5, 11 [44] Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 3 [45] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 3, 11 [46] Hualiang Wang, Yi Li, Huifeng Yao, and Xiaomeng Li. Clipn for zero-shot ood detection: Teaching clip to say no. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 1, 3, 5, 11 [47] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on In International Conference on Machine the hypersphere. Learning. PMLR, 2020. 3 [48] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene In IEEE computer socirecognition from abbey to zoo. ety conference on Computer Vision and Pattern Recognition. IEEE, 2010. 5, [49] Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, et al. Openood v1. 5: Enhanced benchmark for out-of-distribution detection. In NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models, 2023. 2, 5, 11 [50] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 2017. 5, 11 [51] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 3 [52] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 2022. 3, 5, 11, 13 Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Detailed Experimental Settings A.1. Datasets MOS Benchmark Datasets. Our experiments utilize the MOS benchmark dataset [12], which has been used in prior studies [14, 31, 34, 46]. The MOS benchmark includes ImageNet-1k [3] as the in-distribution (ID) dataset The outwith validation set with 50,000 images. of-distribution (OoD) dataset consists of four datasets: SUN [48], Places [50], Textures [2], and iNaturalist [42], with no overlap with ImageNet-1k. The test OoD datasets include 10,000 images each from iNaturalist, SUN, Places, and 5,640 images from Textures. OpenOOD v1.5 Benchmark Datasets. OpenOOD v1.5 [49] includes six benchmarks: four standard OoDD benchmarks and two full-spectrum OoDD benchmarks. For our experiments, we utilize the standard OoDD benchmark with ImageNet-1k, which consists of 45,000 images for testing and 5,000 images for validation. This benchmark dataset includes two scenarios: Near-OoD and Far-OoD scenarios. The Near-OoD scenario includes SSB-hard [43] (49,000 images across 980 categories) and NINCO [1] (5,879 images), while the Far-OoD scenario contains iNaturalist (10,000 images), Textures (5,640 images), and OpenImage-O [45] (1,763 images). A.2. Implementation details Our proposed method, Cross-Modal Alignment (CMA), was implemented using Python 3.9.18 and PyTorch 1.12.0+cu116. All experiments were conducted on 8 NVIDIA A6000 GPUs, each with 48GB of memory, running on Ubuntu 20.04.6 LTS. We conduct experiments across zero-shot (ZS), prompt learning (PL), single-modal fine-tuning (SMFT), and multimodal fine-tuning (MMFT), following the default configurations of each baseline for fair comparison. For ZS, we use the post-hoc methods MCM [31] and NegLabel [14], which leverage text information without additional training. These OoD scoring methods are also employed for PL and MMFT experiments. As in previous works [14, 31], we use the prompts photo of [label] for MCM in the ZS setting and The nice [label] for NegLabel with temperature scaling set to 1 and 0.01, respectively. NegLabel [14] highlights that word choice in prompt engineering significantly impacts OoD detection performance: negative prompts degrade performance, positive prompts improve it, and neutral prompts require careful tuning. Among these, the The nice [label] template provides optimal results in NegLabel. However, in MMFT methods such as FLYP [8], template choice does not lead to significant accuracy variations. To explore this further, we conduct an ablation study using three different prompts, finding that the prompt choice does not significantly affect OoDD performance in MMFT. Detailed results are provided in Appendix B.3. Unlike MCM, NegLabel requires additional hyperparameters due to its use of negative texts. Specifically, negative labels are mined from large word corpora like WordNet [6]. We follow the NegMining from Jiang et al. [14], which extracts = 10, 000 negative labels with percentile η = 0.05 from WordNet. The extracted texts are then used for OoD scoring, as described in Eq. 7. Additionally, the results in Tables 1 and 2 do not incorporate the grouping strategy. Detailed results on the grouping strategy are provided in Appendix B.2. For CoOp [52] and LoCoOp [34], we also adopt the settings proposed in the original papers. However, PL employs few-shot fine-tuning with 16-shot setting, making direct comparisons with SMFT and MMFT potentially unfair. To address this, we not only follow the original settings but also evaluate with increasing shot counts, up to the fullshot setting where all ID data are utilized, as detailed in Appendix B.1. In the 16-shot setting, we report the averaged results from three repeated experiments with seeds 0, 1, and 2 using the original codebase [34, 52]. For other shot settings, we do not perform repeated experiments. In SMFT, we use LP, FFT, and LP-FT [21], each of which exclusively utilizes the visual encoder without relying on textual information. Following Goyal et al. [8], we perform hyperparameter sweep with learning rates {1e-4, 1e-5, 1e-6} and weight decay values {0.0, 0.1}. Models are trained for 10 epochs, selecting the best based on indistribution (ID) accuracy, as OoD data is not directly available for validation in real-world OoDD settings. Through this procedure, the learning rates are set to 1e-4 for LP and 1e-5 for FFT and LP-FT, with weight decay of 0.0. For MMFT, we compare FLYP [8] and m2-mix [35] with our proposed approach, using the same hyperparameter sweep as SMFT. Our method explores learning rates {1e-4, 1e-5, 1e-6}, weight decay values {0.0, 0.1}, and alignment strengths (i.e., λ) {1e-1, 1e-2, 1e-3}, with batch size of 512. Early stopping is based on the accuracy of the ID validation set. In m2-mix, the mixup weight is controlled using the λ value instead of alignment strength. In summary, Fig. 3 presents snippet of code to illustrate our proposed method. the CLIP image and text encoders extract corresponding embeddings, which are projected into the same dimension and undergo L2 normalization, projecting them onto shared hyperspherical space. For CLIP and FLYP, contrastive learning optimizes cosine similarity by increasing it for matching imagetext pairs while reducing it for non-matching pairs. Our method extends this approach by calculating CMA text and CMA img to derive the total loss. When the alignment strength parameter λ is set to 0, the training process is equivalent to FLYP. # extract image and text embeddings img_emb, text_emb, scale = model(images, texts) # joint hypersphirical embeddings img_emb /= img_emb.norm(dim=-1, keepdim=True) text_emb /= text_emb.norm(dim=-1, keepdim=True) # scaled cosine similarity logits = (scale) * (img_emb @ text_emb.T) # clip symmetric loss function gt = torch.arange(bs) img_loss = Cross_Entropy_Loss(logits, gt, axis=0) text_loss = Cross_Entropy_loss(logits, gt, axis=1) # cross-modal-alignment regularization CMA_img = -torch.logsumexp(logits.per_img,dim=1) CMA_text = -torch.logsumexp(logits.per_text,dim=1) # total CMA loss total_loss = (img_loss + args.lam * CMA_img.mean())/2 + (text_loss + args.lam * CMA_text.mean())/2 Figure 3. Pytorch-like pseudo-code of CMA Algorithm 1 NegMining (proposed in NegLabel [14]) Input: Candidate labels c, ID labels Y, Text encoder text Output: Negative labels ei = text(prompt(yi)) 1: // Calculate text embeddings 2: for yi do 3: 4: end for 5: for yi do 6: 7: 8: 9: end for 10: // Choose negative labels from top-k distances. 11: = topk([d1, d2, . . . , dC], c, ) ei = text(prompt(yi)) // Measure candidate-ID label distance. di = percentileη({ cos(ei, ek)}K k=1) B.3. The impact of prompts on OoDD performance To evaluate the impact of prompts on performance, we conduct an ablation study using three prompts: photo of [label], The nice [label], and no prompt, as shown in Table 9. These prompts are derived from the ablation study of NegLabel [14]. Models are trained and evaluated with the same prompts. Our results indicate that altering the prompt does not lead to significant changes in performance. Specifically, in MCM, the performance difference across prompts does not exceed 1% in terms of average AUROC and FPR95. While positive prompts demonstrate slightly better OoDD performance, the differences are not significant enough to affect its performance superiority. These results show that the choice of prompt during MMFT has negligible impact on OoDD performance. B.4. The impact of λ values on OoDD performance We select the λ value based on ID accuracy, as actual OoD data is not available for evaluation. To determine the optimal value of λ, we compare different λ values {1e-1, 1e-2, 1e-3, 5e-4}, and 1e-3 yields the highest ID accuracy, which is also aligned with the best OoDD performance, as shown in Table 10. Notably, an increase in alignment strength does not consistently improve ID accuracy or OoDD performance, highlighting the need for careful tuning. In our experiments, CMA demonstrates strong OoDD performance when the λ value is optimized for ID accuracy, even without access to OoD data. B. Additional Ablations B.1. PL results with various-shot settings In Tables 1 and 2 of the main paper, we present PL results based on the default 16-shot settings from CoOp [52] and LoCoOp [34]. To ensure fair comparison, we extend these implementations to consider additional shot settings, including full-shot, and compare them with SMFT and MMFT, which employ full-shot configurations. Specifically, we conduct experiments using 256, 512, 1024, and full-shot settings, as shown in Tables 5, 6, and 7. All settings use batch size of 512, consistent with the SMFT and MMFT configurations. Our observations reveal that increasing the number of shots generally improves both OoDD performance and ID accuracy, suggesting that prompt learning benefits from more training data. However, full-shot settings do not always yield better results; in some benchmarks, performance at full-shot is even worse than ZS. Additionally, the observed improvements are not sufficient to outperform other baselines. For CoOpNegLabel, the highest ID accuracy of 74.07% is achieved in the 1024-shot setting, as shown in Table 7. In contrast, the best OoDD performance is observed in the 256-shot setting on the MOS benchmark and the 512-shot setting on the OpenOOD v1.5 benchmark. These results indicate that while increasing the number of shots from 16 to full-shot provides incremental gains, determining an optimal setting remains difficult. Nonetheless, our approach consistently outperforms CoOp and LoCoOp across all shot settings. B.2. The effect of grouping strategy The NegMining algorithm expands textual information by selecting words maximally distant from ID texts, thereby reducing the risk of high similarity between ID images and negative labels, as described in Algorithm 1. However, increasing the number of negative labels raises the variance in OoD scores, which can lead to more false positives. To address this, NegLabel [14] has proposed grouping strategy that divides the negative labels into multiple groups to balance the benefits of additional information with the risk of false positives. We report the performance of the grouping strategy proposed by NegLabel at = 100 in Table 8. Applying the grouping strategy improves OoDD performance as shown in the table. To highlight the inherent capabilities of CMA, we do not apply additional performance-enhancing techniques, such as the grouping strategy, in our main experiments. Nevertheless, our method achieves state-of-the-art performance without the grouping strategy and shows further improvements when it is applied. Table 5. OoDD performance across different shot settings (16-, 256-, 512-, 1024-, and full-shot) for CoOp and LoCoOp on the MOS benchmark iNaturalist Places FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC Textures Average SUN Methods Zero-Shot MCM NegLabel 16-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 3.19 256-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 4.37 512-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 4.85 1024-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 3.80 full-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 4.48 32.28 94.40 39.33 92.28 44.94 89.83 57.98 85.99 43.63 90.63 99.37 23.23 95.14 39.85 90.98 46.49 89.64 27.97 93.78 2. 26.37 94.49 35.23 92.59 43.29 89.67 41.47 90.62 36.59 91.84 4.95 98.90 25.76 94.59 30.07 93.33 44.35 89.59 26.28 94.10 23.08 95.46 33.39 93.25 40.74 90.52 40.75 91.14 34.49 92.59 99.25 46.63 90.58 55.44 87.65 46.03 89.85 37.83 91.83 28.26 94.14 34.69 92.83 42.05 90.15 41.67 90.53 36.67 91.91 99.00 29.34 94.41 29.07 94.23 34.25 93.01 24.23 95.16 4.28 18.80 96.12 34.46 92.92 42.04 90.32 39.77 91.55 33.77 92.73 99.09 49.39 90.53 64.01 85.82 51.45 88.95 42.31 91.10 24.78 94.80 33.63 92.89 40.61 90.46 39.45 91.17 34.62 92.33 99.14 34.54 93.30 30.80 93.79 31.01 93.64 24.98 94.97 3.59 22.00 95.50 30.06 93.80 36.27 91.37 40.89 91.38 32.30 93.02 98.93 40.15 92.29 58.99 86.76 60.78 85.40 41.19 90.84 22.83 95.15 33.60 92.80 40.96 90.46 39.40 91.33 34.20 92.44 98.93 33.76 93.65 30.19 94.26 31.73 93.51 25.05 95.09 4.54 22.10 95.27 32.58 93.58 38.50 91.16 39.52 91.52 33.18 92.88 99.10 41.17 91.97 56.59 87.95 56.93 87.43 39.62 91.61 23.88 94.98 35.74 92.49 41.72 90.13 38.93 91.14 29.10 92.19 98.87 32.80 93.72 32.23 93.80 32.81 93.16 25.75 94.89 5.14 20.25 96.01 32.72 93.25 38.82 90.87 39.96 91.39 32.94 92.88 99.02 43.44 91.03 66.05 83.16 53.51 88.34 41.87 90.39 Table 6. OoDD performance across different shot settings (16-, 256-, 512-, 1024-, and full-shot) for CoOp and LoCoOp on the OpenOOD v1.5 benchmark SSB-hard iNaturalist FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC Openimage-O Near-OoD Far-OoD Textures NINCO 89.45 64.11 82.70 69.82 86.08 66.97 61.94 87.62 54.26 87.71 53.80 88.60 56.67 87.98 99.36 44.98 90.56 31.10 93.10 26.14 94.34 81.87 71.32 69.82 77.09 75.85 74.21 2.32 Methods Zero-Shot MCM NegLabel 16-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 76.03 74.42 66.51 80.96 71.27 77.69 256-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 68.88 81.54 70.86 79.89 69.87 80.72 512-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 67.23 81.48 70.40 78.99 68.82 80.23 1024-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 65.79 82.29 71.85 77.62 68.82 79.95 full-shot CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel 69.13 80.92 72.60 77.84 70.86 79.38 86.10 67.72 77.24 74.59 81.67 71.16 26.31 94.50 38.38 91.92 37.64 92.08 34.11 92.83 68.20 78.72 57.71 84.20 62.96 81.46 98.90 42.74 90.28 23.18 95.25 23.63 94.81 87.38 66.23 77.04 73.46 82.21 69.84 22.98 95.48 37.89 92.27 37.44 92.09 32.77 93.28 99.16 43.87 90.59 29.29 93.65 25.62 94.47 4. 3.70 86.87 68.10 76.04 74.52 81.45 71.31 28.23 94.16 38.11 91.93 37.27 92.22 34.54 92.77 63.67 81.66 52.80 86.29 58.23 83.98 98.99 32.51 93.65 20.76 95.96 19.19 96.20 86.63 66.46 76.07 73.77 81.35 70.11 18.78 96.14 36.55 92.70 34.53 92.76 29.95 93.86 99.08 50.31 89.83 29.87 93.97 28.19 94.29 4.29 4.40 85.89 68.85 76.06 74.85 80.98 71.85 24.75 94.81 35.96 92.49 35.28 92.57 32.00 93.29 99.14 29.28 94.22 18.90 96.34 17.26 96.57 68.48 79.54 50.01 87.31 59.25 83.43 86.40 66.30 75.30 73.94 80.85 70.12 21.93 95.51 37.89 92.55 34.44 92.77 31.42 93.61 98.92 59.40 86.31 30.01 93.91 31.42 93.05 4. 3.60 86.04 68.76 75.94 75.58 80.99 72.17 22.78 95.17 36.11 92.56 34.69 92.71 31.19 93.48 68.92 79.90 50.16 87.23 59.54 83.57 98.93 30.07 94.10 19.83 96.20 18.15 96.41 86.37 66.53 75.78 74.48 81.07 70.51 21.94 95.28 36.55 92.55 35.59 92.46 31.36 93.43 99.09 55.83 88.11 29.45 93.84 29.71 93.68 4.56 3.84 86.13 68.92 75.96 75.17 81.04 72.04 23.84 94.99 35.45 92.41 34.70 92.77 31.33 93.39 63.73 82.39 52.27 86.90 58.00 84.64 98.87 31.10 93.73 19.02 96.42 18.43 96.34 85.89 67.35 74.84 75.03 80.36 71.19 20.17 96.02 37.09 92.52 33.62 92.95 30.29 93.83 99.01 51.92 89.25 30.11 93.68 28.84 93.98 4. 5.16 Table 7. ID accuracy across different shot settings (16-, 256-, 512-, 1024-, and full-shot) for CoOp and LoCoOp on ImageNet-1k Methods 16-shot CoOp LoCoOp 256-shot CoOp LoCoOp 512-shot CoOp LoCoOp 1024-shot CoOp LoCoOp full-shot CoOp LoCoOp Acc. 71.95 71.72 72.96 72. 73.61 73.12 74.07 73.28 73.97 73.44 Table 8. The effect of the grouping strategy (n = 100) on the MOS benchmark. The symbol represents the result with the grouping strategy. Methods Zero-Shot (ZS) MCM NegLabel NegLabel Multi-modal Fine-tuning (MMFT) FLYPMCM FLYPNegLabel FLYPNegLabel m2-mixMCM m2-mixNegLabel m2-mixNegLabel CMAMCM (Ours) CMANegLabel (Ours) (Ours) CMANegLabel iNaturalist Places FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC Textures Average SUN 32.28 94.40 39.33 92.28 44.94 89.83 57.98 85.99 43.63 90.63 99.37 23.23 95.14 39.85 90.98 46.49 89.64 27.97 93.78 2.30 1.55 99.58 17.96 95.82 33.53 91.97 44.34 89.86 24.35 94.31 24.86 94.35 39.81 90.58 47.92 87.16 41.19 89.34 38.44 90.36 99.31 23.48 94.82 37.23 90.86 41.70 89.27 26.39 93.57 3.16 2.41 99.45 20.38 95.40 32.64 91.66 38.49 89.83 23.48 94.08 22.41 95.61 39.18 91.85 47.07 88.72 43.44 90.13 38.02 91.58 99.43 23.03 94.86 35.55 91.21 36.65 90.68 24.40 94.05 2.39 99.53 20.13 95.41 31.91 91.96 34.22 91.17 22.03 94.52 1.85 22.95 95.65 40.01 91.78 48.83 88.41 44.93 89.87 39.18 91.43 99.62 16.84 96.36 27.65 93.11 33.58 91.64 19.93 95.13 1.65 99.66 16.11 96.55 26.52 93.48 33.09 91.90 19.27 95.40 1. Table 9. Comparison with different prompt settings (e.g., positive, neutral, and no prompts) for FLYP and CMA on the MOS benchmark Methods < class > FLYPMCM FLYPNegLabel CMAMCM (Ours) CMANegLabel (Ours) photo of < class > FLYPMCM FLYPNegLabel CMAMCM (Ours) CMANegLabel (Ours) The nice < class > FLYPMCM FLYPNegLabel CMAMCM (Ours) CMANegLabel (Ours) iNaturalist"
        },
        {
            "title": "Average",
            "content": "FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC 25.42 3.64 21.85 2.15 26.13 4.51 22.07 1. 24.86 3.16 22.95 1.65 94.06 99.23 95.73 99.55 94.04 99.06 95.80 99.55 94.35 99.31 95.65 99.62 38.93 22.61 39.53 18.57 39.04 29.23 38.82 20. 39.81 23.48 40.01 16.84 90.84 95.22 91.85 96.15 90.64 93.99 91.91 96.03 90.58 94.82 91.78 96.36 47.29 35.70 48.18 28.98 47.63 42.58 47.70 32. 47.92 37.23 48.83 27.65 87.30 91.49 88.56 93.05 87.02 89.46 88.62 92.50 87.16 90.86 88.41 93.11 40.60 43.44 45.62 34.01 41.12 43.83 44.08 35. 41.19 41.70 44.93 33.58 89.45 88.55 89.78 91.86 89.90 88.03 89.94 91.07 89.34 89.27 89.87 91.64 38.06 26.35 38.80 20.93 38.48 30.04 38.17 22. 38.44 26.39 39.18 19.93 90.41 93.62 91.48 95.15 90.40 92.63 91.57 94.79 90.36 93.57 91.43 95.13 Table 10. Comparison of different λ values for CMA on the MOS benchmark iNaturalist Places FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC"
        },
        {
            "title": "SUN",
            "content": "Methods λ = 0.1 CMAMCM (Ours) CMANegLabel (Ours) 5.61 λ = 0.01 CMAMCM (Ours) CMANegLabel (Ours) 2.89 λ = 0.001 CMAMCM (Ours) CMANegLabel (Ours) 1.65 λ = 0.0005 CMAMCM (Ours) CMANegLabel (Ours) 1.85 26.20 94.96 53.54 86.56 57.73 83.45 49.73 88.00 46.80 88.24 98.91 32.57 93.75 40.97 90.44 47.30 89.68 31.61 93.19 22.70 95.86 43.95 90.64 52.63 87.33 45.80 90.09 41.27 90.98 99.41 20.24 95.81 31.11 92.78 39.15 91.30 23.35 94.83 22.95 95.65 40.01 91.78 48.83 88.41 44.93 89.87 39.18 91.43 99.62 16.84 96.36 27.65 93.11 33.58 91.64 19.93 95.13 22.20 95.73 38.72 91.96 47.59 88.17 42.13 90.16 37.66 91.50 99.61 17.70 96.08 29.50 92.50 31.54 92.12 20.15 95."
        },
        {
            "title": "Acc",
            "content": "81.12 81.96 82.64 82.56 B.5. Additional Near-OoD experiments To thoroughly evaluate performance in Near-OoD scenarios, we conduct experiments on challenging ImageNet-1k splits from [36]. Specifically, we adopt the P1, P2, and P3 protocols in [36]. Each of these includes known, negative, and unknown classes. For example, in P1, the known classes consist of 116 fine-grained dog breeds from ImageNet. The unknown classes include 166 non-animal categories that are semantically distant from the known classes. Additionally, 67 four-legged animal classes are designated as negative classes, which are semantically closer to the known classes but remain distinct. The negative classes are originally intended to aid the model in distinguishing known classes from unknown classes during training. For our Near-OoD experiments, we treat negative classes, along with unknown classes, as OoD since simple zero-shot (ZS) method using NegLabel yields near-perfect performance on P1 and P2, making it difficult to evaluate the benefits of MMFT approaches. As shown in Table 12, ZS NegLabel achieves AUROC scores of 99.96% and 99.42% for P1 and P2, respectively. These results indicate that the unknown classes can be effectively distinguished using pre-trained textual information. Since the model already separates the unknown set too well, it becomes challenging to evaluate the contribution of textual information in MMFT. To address this, we construct more challenging splits 2, and 3 by designating additional negative datasets as OoD (i.e., negative classes + unknown classes), as described in Table 11. We perform hyperparameter search based on FPR95 using the validation sets of known and negative classes. Note that negative classes are used only as validation/test datasets, and are not included in training. 1, As shown in Table 13, our method achieves the highest AUROC scores, maintaining robust OoDD performance even under challenging conditions, while also achieving the highest accuracy among all compared methods. However, we observe that although AUROC remains higher than that of FFTEnergy (i.e., SMFT), which does not utilize textual information, the average FPR95 is comparable. To gain deeper understanding, we analyze each protocol in sequence. Starting with 1, we observe that FFTEnergy underperforms in both FPR95 and AUROC compared to methods that utilize textual information through NegLabel. This can be attributed to the fact that in 1, the semantic distance between unknown/negative classes and known classes is sufficiently large, allowing textual information such as negative concept labels to effectively distinguish them. This observation aligns with prior findings on the effectiveness of textual information in Far-OoD scenario. Next, in 2, we observe that all NegLabel-based methods, except for our CMANegLabel, underperform compared to FFTEnergy. This indicates that CMA effectively reduces the modality gap, thereby improving the utilization of textual information. Similarly, in 3, while our method performs worse than SMFT in terms of FPR95, it achieves higher AUROC score. These findings indicate that FFTEnergy, which relies solely on visual features, can effectively distinguish between subclasses within broader category (e.g., various types of Hunting Dog in 2) solely based on visual cues. In contrast, existing NegLabel-based approaches struggle to separate ID and OoD classes when they belong to the same or semantically related categories, likely due to the modality gap. Our method addresses this challenge by mitigating the modality gap, thereby improving detection performance in Near-OoD scenarios. ID (Known) OOD (Negative+Unknown) 1 All dog classes 29055 / 5800 Half of hunting dog classes 7224 / 1500 Other 4-legged animal classes Non-animal classes 17420 / 11650 (3350+8300) Half of hunting dog classes Other 4-legged animal classes 7949 / 4300 (1550+2750) 3 Mix of common classes 38633 / Mix of common classes 24549 / 13050 (4850+8200) Table 11. More challenging ImageNet-1k splits. The numbers represent the number of validation/test samples. C. Additional Visualization To achieve more intuitive visualization, we use PCA with 1,000 image prototypes and class prototypes from ImageNet-1k, along with 1,000 random OoD images (from the MOS benchmark datasets) and negative texts. As shown in Fig. 4, methods such as ZS and FLYP exhibit clear modality gap between ID image and ID text embeddings (orange and blue points). This gap is also observed in OoD image and text embeddings, as illustrated in Figs. 5 and 6. Our findings indicate that eliminating this modality gap among ID embeddings is essential for fully leveraging textual information, such as negative concept texts, as discussed in Section 5. In CMA, which addresses this modality gap, the orange and blue points are clustered closer together, as are the red and green points. Table 12. ZS OoDD Performance on splits P1, P2, and P3 (ID = Known, OoD = Unknown) Methods Zero-Shot (ZS) MCM NegLabel P1 P2 P3 Average FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC 14.27 96.96 56.07 88.89 35.11 89.64 35.15 91.83 99.42 29.41 90.10 11.00 96.49 0.23 99.96 3.35 Table 13. Comparison of OoDD performance on our splits 1, 2, and 3 (ID = Known, OoD = Negative + Unknown) Methods Zero-Shot (ZS) MCM NegLabel Prompt-Learning CoOpMCM CoOpNegLabel LoCoOpMCM LoCoOpNegLabel Single-modal Fine-tuning (SMFT) FFTMSP FFTODIN FFTEnergy Multi-modal Fine-tuning (MMFT) FLYPMCM FLYPNegLabel CMAMCM CMANegLabel 1 2 3 Average FPR95 AUROC FPR95 AUROC FPR95 AUROC FPR95 AUROC Acc 29.18 93.07 64.30 85.34 52.41 83.32 48.63 87.24 99.74 24.09 93.34 48.50 82.90 24.59 91.99 1.17 73.43 24.83 93.91 63.03 84.75 52.78 83.64 46.88 87.43 1.20 99.73 26.75 91.65 48.82 83.72 25.59 91.70 22.47 94.87 62.95 84.70 52.25 83.20 45.89 87.59 99.78 25.25 91.85 60.13 79.47 28.76 90.37 0.90 75.10 74.95 19.18 95.45 61.44 87.04 56.46 93.78 45.69 89.98 3.24 2. 99.30 25.88 93.78 38.00 89.55 22.37 94.21 84.74 99.41 18.19 94.52 39.92 88.40 20.24 94.11 9.14 2.33 9.21 2.29 98.16 42.67 89.94 42.05 87.11 31.29 91.74 99.42 19.35 94.45 41.76 86.79 21.15 93.56 98.16 43.02 89.95 41.26 89.43 31.16 92.51 99.42 18.07 94.76 40.97 89.71 20.44 94.63 85.30 85.55 (a) ZS (b) FLYP (c) m2-mix (d) CMA Figure 4. Visualization of image and text embeddings using PCA on ImageNet-1k. Orange and blue points represent ID image and ID text embeddings, respectively. (a) Zero-Shot (b) FLYP (c) m2-mix (d) CMA Figure 5. Visualization of image and text embeddings using PCA on ImageNet-1k and negative texts. Orange and blue points represent ID image and ID text embeddings, respectively, while red points denote negative text embeddings. (a) Zero-Shot (b) FLYP (c) m2-mix (d) CMA Figure 6. Visualization of image and text embeddings using PCA on ImageNet-1k, MOS benchmark datasets, and negative texts. Orange and blue points represent ID image and ID text embeddings, respectively, while green and red points denote OoD image and negative text embeddings."
        }
    ],
    "affiliations": [
        "Seoul National University of Science and Technology"
    ]
}