{
    "paper_title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents",
    "authors": [
        "Yuhan Guo",
        "Cong Guo",
        "Aiwen Sun",
        "Hongliang He",
        "Xinyu Yang",
        "Yue Lu",
        "Yingji Zhang",
        "Xuntao Guo",
        "Dong Zhang",
        "Jianzhuang Liu",
        "Jiang Duan",
        "Yijia Xiao",
        "Liangjian Wen",
        "Hai-Ming Xu",
        "Yong Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner"
        },
        {
            "title": "Start",
            "content": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents Yuhan Guo1,2, Cong Guo1, Aiwen Sun3, Hongliang He5, Xinyu Yang4, Yue Lu4, Yingji Zhang7, Xuntao Guo6, Dong Zhang4, Jianzhuang Liu11, Jiang Duan1, Yijia Xiao8, Liangjian Wen1, Hai-Ming Xu9, Yong Dai10 1Southwestern University of Finance and Economics, 2Shanghai Jiao Tong University, 3Central South University, 4Hithink Research, 5Westlake University, 6Harbin Institute of Technology, 7University of Manchester, 8University of California, Los Angeles, 9University of Adelaide, 10Fudan University, 11Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences https://Gnonymous.github.io/Web-CogReasoner 5 2 0 2 ] . [ 1 8 5 8 1 0 . 8 0 5 2 : r Figure 1: Visualization of the Web-CogKnowledge Framework along with the experimental results. These authors contributed equally to this work. Project Leader. Corresponding authors."
        },
        {
            "title": "Abstract",
            "content": "Multimodal large-scale models have significantly advanced the development of web agents, enabling them to perceive and interact with the digital environment in manner analogous to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning1. Therefore, we decompose web agents capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural domains. In this framework, knowledge content learning corresponds to the agents processes of Memorizing and Understanding, which rely on the former two types knowledge respectively, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, structured resource curated from 14 real-world websites, designed to systematically instill the core knowledge necessary for web agent. This dataset serves as the agents conceptual groundingthe \"nouns\" upon which comprehension is builtas well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the WebCogReasoner. Extensive experimentation reveals its significant superiority over existing models, particularly in its capacity for generalization to unseen tasks where its structured knowledge proves decisive. To facilitate rigorous and systematic evaluation, we introduce the Web-CogBench, comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner."
        },
        {
            "title": "Introduction",
            "content": "The advent of large-scale models marks landmark advancement in artificial intelligence. The development of Large Multimodal Models (LMMs), in particular, has vastly expanded the horizons for AI applications. In recent years, AI agents have emerged as primary vehicle for deploying these models, significantly extending their capabilities. Much like humans, these agents can leverage diverse array of tools to enhance their skills, leading to remarkable achievements in domains such as code generation [17, 18], image [16, 4] and video synthesis [1, 25], and academic research. Notably, the application of web agents to surfing the internet has also seen substantial progress. The evolution of web agents has transitioned from early rule-based systems to modern approaches that widely leverage Large Language Models (LLMs) or Languge Vision Models (LVMs) [31, 26, 35, 29]. These LLM-powered agents typically process HTML or Accessibility Tree inputs, formulating tasks and observations into natural language prompts for the model to reason upon and decide the next action. With the integration of LVMs, web agents have gained perceptual abilities akin to human vision, enabling them to comprehend and reason about multimodal content on web pages. As whole, web agents can be broadly categorized into three types: (1) Text-only agents [37, 21], which suffer from information loss due to their inability to capture visual cues; (2) Vision-only agents [28], which rely solely on visual information and thus lose critical structured data; and (3) Hybrid agents [19, 13], which utilize both visual and textual information. LLMs and LMMs are pre-trained on vast amounts of general-domain knowledge, which is undeniably crucial for their foundational capabilities. However, as with any specialized field, this general knowledge is often insufficient, creating performance bottleneck that caps the potential of the three aforementioned model types. While many existing works have attempted knowledge enhancement or injection, these methods often lack systematic or theoretically grounded approach. To address this, we draw inspiration from highly successful paradigm in human education: Blooms Taxonomy [27, 8]. This framework suggests that effective learning can be deconstructed into two key stages: 1Drawing inspiration from Blooms educational philosophy, cornerstone of modern pedagogy. 2 knowledge content learning and cognitive processes. Accordingly, we propose that training web agent should follow similar two-stage methodology. The first stage is dedicated to acquiring knowledge of varying difficulties. The second stage then focuses on the analysis, application, and ultimately, the creative synthesis of that knowledge. This mirrors the human learning trajectory: we first accumulate knowledge through education (Stage 1), and then based on that foundation of knowledge and experience, we learn to apply, innovate, and create (Stage 2). Therefore, in this paper, we introduce training paradigm for our model grounded in Blooms Taxonomy, bifurcated into two sequential stages: Knowledge Content Learning and Cognitive Processes. The initial stage, Knowledge Content Learning, is dedicated to building multi-layered knowledge foundation. This foundation consists of three distinct types: Factual knowledge, which includes basic, foundational concepts; Conceptual knowledge, which maps the interrelationships between these facts; and Procedural knowledge, which represents the logical frameworks needed to synthesize factual and conceptual understanding for effective reasoning. To curate this knowledge, we construct Web-CogDataset, collecting metadata from 14 prominent websites and engineered 12 granular tasks designed to systematically instill each knowledge type. In essence, this stage teaches the agent foundational web knowledge and how to apply it to familiar tasks. The subsequent stage, Cognitive Processes, focuses on cultivating the agents self-exploring capability. This advanced capability empowers the agent to build upon its learned foundation, enabling it to \"learn how to learn\" by creatively exploring its own \"self-knowledge\" to tackle previously unencountered tasks and challenges. To operationalize this, we developed knowledge-guided reasoning templates and utilized imitation learning to train the model. This process explicitly teaches the agent the cognitive faculties required for autonomous learning and creative problem-solving in novel scenarios. To validate our approach, we conducted rigorous evaluations on suite of general-purpose benchmarks as well as our own. Our findings reveal that the agent trained with our methodology consistently and significantly outperforms state-of-the-art baselines. Crucially, and in alignment with our hypothesis, the performance gap widens dramatically on knowledge-intensive tasks. This demonstrates that, analogous to human learning, the structured acquisition of knowledge is key determinant of an agents ability to excel in complex, domain-specific scenarios. In summary, our contributions are threefold: 1. Drawing inspiration from Blooms taxonomy and established human educational paradigms, we propose the Web-CogKnowledge Framework systematic, two-stage training methodology designed to enhance the cognitive capabilities of web agents. Built upon this framework, we develop Web-CogReasoner. Rigorous benchmarking demonstrates that agents trained under our framework achieve significant performance improvement over current state-of-the-art models. 2. We construct the Web-CogDataset, structured curriculum consisting of 12 fine-grained and progressively challenging tasks. These tasks are meticulously designed to incrementally build the agents web knowledge, cognition capability, and higher-order reasoning. 3. To enable comprehensive and robust evaluation, we introduce Web-CogBench, novel benchmark specifically designed to assess whether web agent possesses the requisite prior knowledge and cognitive capabilities for effective web navigation. This benchmark will be released publicly to foster further research in this area."
        },
        {
            "title": "2 Related Work",
            "content": "Web Agent Early studies on web page understanding have concentrated on leveraging structured text. The work [11] defined crucial HTML understanding tasks, such as semantic classification, description generation, and autonomous web navigation, and trained models to comprehend the structure and content of HTML. AutoWebGLM[20] introduced an HTML simplification algorithm. Through curriculum learning, it first trained the model using collected web page data to recognize web page structures and understand the functions of different interactive components, like text boxes and buttons. Subsequently, the model learned to execute simple web tasks and decompose complex ones into subtasks, enabling the complete execution of web tasks from simple to complex vision-based Web Page Understanding. Recent research has explored the use of visual information for web page understanding. SeeClick[7] designed specialized pre-training tasks, including text-to-point and text-to-bounding box localization 3 tasks, as well as point-to-text and bbox-to-text generation tasks. These tasks facilitated the models learning of the correlation between the positions of web elements and their textual descriptions, enhancing the models ability to localize and understand web elements. CogAgent[14] introduced high-resolution cross-module, enabling the model to recognize tiny elements and text in GUIs, and constructed large-scale GUI annotated dataset for GUI grounding tasks, achieving excellent performance in multiple visual question answering (VQA) benchmarks and GUI navigation task datasets. OmniParser[30] integrated three typical visually-situated text parsing taskstext spotting, key information extraction, and table recognitionwithin unified framework, providing more efficient solution for GUI Agents to handle complex tasks. Unlike many existing agent frameworks that rely on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS[28] adopted an end-to-end architectural design, directly perceiving screenshots, executing reasoning processes, and autonomously generating valid actions. UGround[10] developed universal visual grounding model using GUI dataset with 10 million elements, demonstrating outstanding performance in desktop tasks and mobile tasks. Multimodal approaches have emerged as powerful way to comprehensively understand web pages. WebVoyager[12] utilized the GPT-4V-ACT tool to overlay bounding boxes and numerical labels of interactive elements on web page screenshots. Additionally, it complemented web page visual information with the Accessibility Tree, helping the model gain more comprehensive understanding of web page content. SeeAct[36] directly harnessed the powerful visual and language understanding capabilities of GPT-4V and explored various grounding strategies to map the text plans generated by the model into specific web operations. Tuning-free Attention-driven Grounding (TAG) [32] method further accomplishes this task without the need for additional fine-tuning by leveraging the inherent attention patterns in pretrained MLLMs. Web Agent Evaluation To evaluate GUI agentscapabilities in web interactions, benchmarks are broadly categorized into web browsing tasks and web understanding assessments. For web browsing, offline benchmarks like Mind2Web[9], Multimodal-Mind2Web, AutoWebBench[20] and WebVLNv1[6] evaluate modelsability to perform multi-step web tasks (e.g., booking flights, filling dynamic forms) using static datasets that include webpage content along with corresponding interaction trajectories. In contrast to offline benchmarks, online environments such as Mini-WoB++[22], Webshop[33], and WebArena[37] enable real-time interaction testing. Mini-WoB++[22] focuses on low-level UI operations (e.g., clicking buttons, handling pop-ups) within simplified webpages. By comparison, Webshop[33] and WebArena[37] replicate complex, real-world taskssuch as ecommerce transactions and Wikipedia editingto evaluate task completion performance. Building on this, VisualWebArena[19] incorporates multimodal inputs to assess models ability to handle vision-augmented interactions in dynamic web environments. To evaluate deeper web understanding, benchmarks are designed to target specific cognitive abilities. WEBQA[5] assesses models capacity for open-domain, multi-hop reasoning. In contrast, ScreenQA[15] and the ScreenAI[2] suite focus on screen-centric comprehension. ScreenQA[15] evaluates models ability to recognize UI elements and interpret context by answering questions based on screenshots. ScreenAI[15] further extends this with three progressively challenging subtasks: Screen Annotation (SA), which involves spatial layout tagging; ScreenQA Short, which tests concise fact extraction; and Complex ScreenQA, which requires multi-step reasoning across diverse screen layouts. Together, these tasks assess fine-grained layout understanding, semantic interpretation, and practical problem-solving in visually dense interfaces."
        },
        {
            "title": "3 Web-CogKnowledge Framework",
            "content": "3.1 Blooms Taxonomy The Blooms Taxonomy 2 (Anderson & Krathwohl, 2001) presents two-dimensional pedagogical design, i.e., knowledge content learning and cognitive process dimension, which demonstrates \"shallow-to-deep\" instructional methodology. This approach posits that effective learning is not monolithic but is structured progression through distinct types of knowledge, beginning with foundational elements and culminating in high-level self-awareness. This educational strategy ensures that learners first establish solid base of facts and concepts before advancing to the execution of complex procedures and, ultimately, the application of reflective metacognitive strategies. 2https://fctl.ucf.edu/teaching-resources/course-design/blooms-taxonomy/. 4 This pedagogical progression from simple to complex can be formally outlined through four hierarchical types of knowledge: Factual Knowledge, Conceptual Knowledge, Procedural Knowledge and Metacognitive Knowledge. More details can be found in Appendix A.1.1. 3.2 Web-CogKnowledge Motivated by Blooms taxonomy, we propose hierarchical web knowledge framework that structures web knowledge according to its taxonomy, collects corresponding knowledge at each level, and trains the model correspondingly. We refer to this knowledge as Web-CogKnowledge. This WebCogKnowledg decomposes knowledge into three levels: 1. Factual knowledge: concrete information extracted from web content. Such as identifying the attributes of individual web elements and predicting the immediate, direct consequences of single interaction; 2. Conceptual knowledge: semantic relationships and abstract patterns underlying webpage content and structure. Such as inferring the function of interface components, comprehending the overall purpose and structure of webpage, and interpreting its multimodal content; 3. Procedural knowledge: actionable know-how for accomplishing specific tasks through interaction, including planning, decision-making, and sequential execution. Such as executing goal-oriented action sequences, inferring user intent from observed behaviors, and handling unexpected interruptions to complete complex tasks; This taxonomy aligns each knowledge type with corresponding cognitive competency required for web-based reasoning and interaction. Figure 2: Statistics of Selected Websites by Category. To fully leverage the potential of Web-CogKnowledge, leading to the ultimate realization of the Web-CogKnowledge Framework. We first collect multimodal metadata from 14 real-world websites (see Figure 2) and then design Web-CogDataset with diverse set of web tasks for Web-Agent training in Section 3.3. And finally construct Web-CogBench in Section 3.4, built from meticulously chosen subsets. Specifically, we curate the metacognitive knowledge and Web-CogReasoner learning and training process in Section 4 due to its importance and complexity. More details about data collection and cleaning process can be found in Appendix A.1.3. 3.3 Web-CogDataset By selectively crawling metadata from the aforementioned websites and leveraging the hierarchical structure of Web-CogKnowledge, we define series of tasks corresponding to different levels of knowledge granularity. This process results in the construction of comprehensive and hierarchically structured dataset, referred to as Web-CogDataset. The task definitions, organized according to the knowledge hierarchy, are introduced below. Detailed statistics and illustrative examples for Web-Dataset are provided in Table 1 and Figure 3. For additional implementation details, please refer to the Appendix A.2. Factual Web Knowledge Tasks Overview The Factual Web Knowledge tasks are designed to establish the agents core perceptual abilities by operationalizing our definition of factual knowledge. 5 Table 1: Statistics of the Web-CogDataset. Knowledge Task Statistics"
        },
        {
            "title": "Procedural Web Knowledge",
            "content": "Element Attribute Recognition Sub-elements Prediction Page Change Prediction Next Page Prediction Source Element Prediction Element Understanding WebPage Understanding Caption & QA Users Intention Prediction Popup Close Single-Step Web Task Noisy Multi-Steps Web Task 37K 1K 24K 18K 1K 40K 7K 15K 2K 1K 17K 7K 81K 62K 27K These five tasks train the agent to identify the attributes of individual web elements and predict the immediate, direct consequences of single interaction. By focusing on extracting concrete, visually grounded informationrather than abstract reasoningthis task category ensures the agent builds foundational understanding of the webs basic cause-and-effect mechanics. This layer of knowledge is the essential prerequisite for any subsequent conceptual or procedural reasoning. Element Attribute Recognition: Given screenshot with highlighted interactive element, the model predicts its semantic role (e.g., button, link) and accessible name (e.g., \"Submit\", \"Search\"), relying solely on visual cues. Sub-elements Prediction: The model infers the dynamic hierarchical structure of web interface by predicting the set of sub-elements that appear upon interacting with highlighted parent element. Page Change Prediction: The model anticipates the visual changes that occur after clicking on specific element, formulating an open-ended response that describes the expected dynamic behavior of the page. Next Page Prediction: The model predicts the subsequent page that results from interacting with specific element on the current page. To enhance generalization, we designed two types of tasks: multiple-choice questions and open-ended responses. Source Element Prediction: Given two screenshotsthe current page and the resulting target page. Then the model identifies which of the visually marked elements on the current page leads to the target, simulating visual cause-and-effect reasoning. Conceptual Web Knowledge Tasks Overview Building upon the factual foundation, the Conceptual Web Knowledge tasks are designed to move the agent from perception to comprehension. These tasks train the agent to synthesize discrete facts into meaningful patterns and understand the semantic relationships underlying webpages content and structure. Rather than merely identifying elements, the agent learns to infer their functions, comprehend the webpages overall purpose and organization, and interpret its multimodal content. This layer of conceptual understanding is critical, as it enables the agent to grasp the \"why\" behind the interface, transforming it from collection of static elements into coherent, interactive system. Element Understanding: For specific interactive element, the model generates an openended paragraph that comprehensively describes the elements Visible Traits (e.g., text, shape, styling), its On-page Location (e.g., header, sidebar, main content), and its likely User-facing Function (e.g., playing video, navigating to new page), relying solely on visual context. WebPage Understanding: Given full-page screenshot, the model generates comprehensive overview describing the webpages Layout Organization (e.g., header, event information, 6 Figure 3: Illustration of Web-CogDataset. seating chart, filter panel), Key Element Analysis (e.g., element attribute, description, function, Interaction, expected outcome), and Summary of the WebPage. This enables thorough understanding of the webpages structure and functionality. Caption & QA: The model generates captions and answers questions about embedded images and WebPages, assessing its visual reasoning and holistic understanding of multimodal webpage information (e.g., layout, text). Contributes to further enhancing the models understanding of WebPages, while preserving its open conversational abilities with web content. The orignal data is drawn from the Multi-UI [23] dataset. Procedural Web Knowledge Tasks Overview With both factual and conceptual knowledge established, the Procedural Web Knowledge tasks are designed to transition the agent from understanding to action. These tasks train the agent to formulate and execute goal-oriented plans by leveraging its actionable know-how for accomplishing specific tasks through interaction. The agent learns to decompose complex user goals into sequential steps, handle unexpected interruptions like popups, and orchestrate series of interactions to achieve desired outcome. This final knowledge layer is paramount, as it empowers the agent to apply its understanding strategically, converting passive knowledge into active, successful task completion. Users Intention Prediction: The model infers high-level user intent from sequence of webpage screenshots representing an interaction trajectory, requiring visual understanding and temporal reasoning. The task is built on the MultiModal-Mind2Web [9] dataset, mapping screenshot sequences to natural language instructions. 7 Popup Close: The model identifies and dismisses popups (e.g., notification modals, login forms) on synthesized webpage screenshots, using dataset of 51 popup components from JS Design 3 overlaid on OpenWebVoyager [13] webpages, with combinatorial augmentation of closing strategies for diverse training. Single-Step Web Task: The model executes goal-directed web interactions to perform atomic web actions, using annotated screenshots from the MultiUI [23] dataset, with tasks validated for accuracy. Noisy Multi-Steps Web Task: The model perform complex, goal-oriented interactions across multiple steps, based on unified OpenWebVoyager [13] trajectories, with standardized action space for consistent evaluation. To test robustness, some tasks include UI interruptions (e.g., popups) that must be dismissed before resuming the task. Table 2: Detailed statistics of Web-CogBench. Task Congition Metric #Num Element Attribute Recognition Next Page Prediction Source Element Prediction Element Understanding WebPage Understanding Users Intention Prediction Popup Close Single Step Exploration Memorizing Understanding Exploring ROUGE-L Accuracy Accuracy LVM Judge LVM Judge LVM Judge Accuracy Accuracy Total - - 249 93 32 200 77 105 58 876 3.4 Web-CogBench To rigorously evaluate the cognitive capabilities fostered by our knowledge-centric framework, we introduce Web-CogBench. While our training dataset is structured by knowledge types (Factual, Conceptual, Procedural), Web-CogBench assesses agent performance through the lens of three corresponding cognitive abilities: Memorizing, Understanding, and Exploring. This benchmark is curated from representative subset of our Web-CogDataset and is designed to measure how well an agent translates its learned knowledge into practical skills within complex web contexts. Detailed statistics of Web-CogBench are provided in Table 2. The three evaluation dimensions directly mirror our hierarchical knowledge framework: Memorizing Assessing the agents ability to recall and recognize concrete information, directly corresponding to the acquisition of Factual Knowledge. It evaluates whether the agent can accurately identify the attributes of web elements and the state of webpage. Understanding Measuring the agents capacity for semantic interpretation, aligning with the mastery of Conceptual Knowledge. It tests whether the agent can move beyond mere identification to comprehend the function of elements and the contextual relationships within page. Exploring Evaluating the agents ability to plan and execute goal-oriented actions, reflecting the application of Procedural Knowledge. It assesses whether the agent can strategically navigate the web, handle interruptions, and complete multi-step tasks to fulfill user goals. 3https://js.design/."
        },
        {
            "title": "4 Web-CogReasoner",
            "content": "4.1 Problem Setup We model the interaction between the environment and Web-CogReasoner as partially observable Markov decision process (POMDP): = (S, A, O, K, T, R), where denotes the webpage status, is the action space (See complete operations in Table 1). is the observation space, and represents the internal knowledge of Web-CogReasoner. : is the transition function. At each decision step t, the agent observes screenshot pt of the current webpage along with its corresponding accessibility tree (AX Tree) xt as the observation,i.e., at step 1, the model receives the initial observation o1 = (p1, x1). Let θ denotes the parameters of the LVMs, The decision policy πθ governs how the agent selects actions. Following the ReAct paradigm [34], the policy outputs at the initial step are given by:(h1, a1) = πθ(K, I, Q, o1). Where is the system prompt, is the users task query. The environment then transitions to the next state s2 using the transition function based on the executed action. At each subsequent time step t, the agent incorporates all historical observations, thoughts, and actions, leverage its internal knowledge to make the next decision: (ht, at) = πθ(K, I, Q, o1, h1, a1, . . . , ot) Finally, the overall performance is assessed using binary reward function R, which returns 1 only if the agent successfully completes the user-specified task at the final step, and 0 otherwise. 4.2 Web-CogReasoner Framework We introduce Web-CogReasoner, knowledge-driven reasoning system built on LVMs and trained on the Web-CogDataset in Section3.3. It solves complex web tasks by performing chain-of-thought reasoning grounded in knowledge acquired during training. We define web agent actions set based on common user behaviors observed during web browsing. At each interaction step, the model receives screenshot of the current webpage along with its corresponding AX Tree. The model integrates visual, structural representations with other contextual cues and history trajectories to predict the next action. Once an action is predicted, it is executed by an automated web agent. This process iterates until the task is completed or reach the maximum step limit. The following section details the design of our agent and reasoning framework: Observation Space To effectively understand and interact with complex web interfaces, our web agent takes as input both webpage screenshots and corresponding AX Tree. The screenshot provides rich visual context essential for grounding visual reasoning, while the AX Tree offers structured semantic representation of page elements. Specifically, the AX Tree encodes key properties such as role (indicating the type of an element), name (how the element should be referred to), description (additional descriptive content), and state (e.g., whether checkbox is checked or dropdown is expanded). Action Space To enable web agents to operate in real-world environments, we define an action space  (Table 3)  grounded in common user interactions during web browsing. Each action at operates on target element identified by unique ID from the current observation xt. For agents using AX Tree representations, this ID refers to node within the tree. Knowledge-driven CoT Reasoning To instantiate the hierarchical Web-CogKnowledge Framework in reasoning-capable agent, we introduce the Web-CogReasoner, structured cognitive web-agent that generates Knowledge-driven Chain-of-Thought (CoT) to guide decision-making in web environments. As illustrated in Figure 4, this CoT is not monolithic or opaque text block; rather, it is scaffolded reasoning template, where each segment is explicitly grounded in distinct type of web knowledge. This design ensures interpretability, traceability, and faithful knowledge alignment throughout the reasoning process. The CoT construction follows layered progression, with each stage corresponding to cognitive function defined in the Web-CogKnowledge taxonomy: Factual Knowledge (highlighted in blue) constitutes the foundation of reasoning by anchoring the model in observable page facts. This phase populates the Webpage Layout 9 Figure 4: An overview of the Web-CogReasoner framework, illustrating the Knowledge-driven Chain of Thought (CoT) process. The agent initiates with Task and observes the webpage (Observe Space). The Knowledge-driven COT Reasoning module then systematically leverages different types of Web-CogKnowledgeFactual Knowledge (blue), Conceptual Knowledge (green), and Procedural Knowledge (yellow). This structured reasoning guides the Planning phase, which decomposes the task and formulates step-by-step strategy. The process culminates in concrete Action to be executed on the webpage. 10 Table 3: Action Space of Web-CogReasoner for Web Interaction. Instruction Description click [id] type [id] [content] scroll [id or WINDOW] [up/down] dbclick [id] Click an element Input specified content into an element Scroll an element or the page up/down Double-click an element go_back go_forward stop [content] Restart Wait"
        },
        {
            "title": "Navigate to the previous webpage\nNavigate to the next webpage\nSubmit the final answer\nRestart the current task\nWait for one second before proceeding",
            "content": "Description and Key Element Analysis sections. The agent extracts concrete interface features such as DOM structure, ARIA attributes (e.g., role=search), accessible names, and current page states. At this stage, the agent is essentially answering the question: What is on the page? Conceptual Knowledge (highlighted in green) builds upon the factual layer by introducing semantic understanding. Within the same sections, the model interprets the purpose of key UI components and predicts their behavior under user interaction. For instance, it explains the functional intent of the search bar (allows users to input location) and anticipates changes to the page layout (the list of stores will update after input). This phase reflects the agents capacity to comprehend: What does this element mean? and How does it contribute to the task? Procedural Knowledge (highlighted in yellow) informs goal-oriented planning and task decomposition. It is primarily instantiated in the Task Recap, Task Decomposition, Step-byStep Reasoning, and Final Action Summary sections. The Reasoner infers the users intent from the task prompt, predicts the next sub-goal for interaction (e.g., Users will type in their desired location), and generates sequential plan: 1. Enter the ZIP code 90028 into the search bar; 2. Review the updated list of Apple Stores; 3. (Fallback) Click the Complete store list if results are missing. This layer translates semantic understanding into executable strategieseffectively answering How should the task be accomplished? By modularizing the CoT along these knowledge axes, Web-CogReasoner achieves structured planning grounded in cognitively coherent framework. Unlike conventional end-to-end models that produce opaque or hallucinated justifications, each reasoning step here is explicitly traceable to factual evidence, semantic inference, or procedural logic. This alignment reduces ungrounded output and enhances interpretability. As shown in Figure 4, the color-coded CoT segments (yellow, purple, blue) illustrate how the agent transitions from perception to comprehension to action. This workflow encapsulates the chain: Task Prompt Knowledge-driven CoT Reasoning Planning Action Through this process, the Web-CogReasoner becomes practical embodiment of Web-CogKnowledge, leveraging its hierarchical structure to support robust, transparent, and goal-directed web reasoning. 4.3 Training We employ multi-stage Imitation Learning strategy to train our model on the Web-CogDataset, utilizing Qwen2.5-VL-7B [3] as the base model. Each stage is aligned with distinct layer of the Web-CogKnowledge Framework: (1) the first knowledge content learning focuses on acquiring Factual Knowledge and Conceptual Knowledge, enabling the model to interpret web content and semantics; (2) the second cognitive process emphasizes Procedural Knowledge, training the model to plan and execute multi-step web interactions. To accommodate the increased complexity of the final stage, which involves multi-image inputs and extended reasoning, we configure training with maximum sequence length of 8K and batch size of 1 with gradient accumulation of 16 steps. All experiments are conducted on cluster of 8 NVIDIA A800 80GB GPUs."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Experimental Setup 5.1.1 Models and Baselines To comprehensively evaluate Web-CogReasoner, we compare its performance against several representative and state-of-the-art baselines. Our proposed model, Web-CogReasoner, is built upon the Qwen2.5-VL-7B[3] large multimodal model (LMM) as its foundational architecture. As detailed in Section 4, Web-CogReasoner extends Qwen2.5-VL by integrating knowledge-induced cognitive reasoning framework, which leverages structured knowledge base (described in Section 3) to guide chain-of-thought (CoT) reasoning for more robust and generalizable web navigation. Specifically, we performed supervised fine-tuning on Qwen2.5-VL using our curated datasets to imbue it with the proposed knowledge-driven capabilities. We benchmark Web-CogReasoner against several prominent models. We include the original Qwen2.5-VL-7B [3], the foundational LMM for our model, evaluated in zero-shot/few-shot setting to quantify the performance gain from our enhancements. We also compare against OpenWebVoyager [13], recent state-of-the-art approach for web navigation that constructs an end-to-end web agent through iterative interaction and optimization. We consider two variants: OpenWebVoyagerIL, which serves as our primary fair comparison baseline, and OpenWebVoyagerMax, an enhanced version obtained by re-sampling and re-training on high-error tasks through self-improvement. The inclusion of penWebVoyagerMax further accentuates the performance advantage of our model. In addition, we include UI-TARS-7B-SFT[28], an end-to-end open-source agent that perceives screenshots directly and performs human-like interactions without relying on commercial models. Furthermore, we include two powerful general-purpose large models, Claude Sonnet 4 4 and Gemini 2.5 pro 5, accessing them via their APIs to assess their inherent multimodal reasoning capabilities for web navigation tasks. These baselines collectively represent diverse spectrum of current web agent methodologies, including foundational LLMs, interactive LLM-based agents, and powerful general-purpose LLM/LMMs leveraging prompting strategies. This allows for thorough and multifaceted evaluation of WebCogReasoners effectiveness and its unique advantages conferred by knowledge-induced cognitive reasoning. 5.1.2 Datasets Training Dataset In the training phase, we utilized comprehensive cognitive datasetWebCogDataset,which is meticulously constructed as detailed in Section 3.3. This dataset is specifically designed to imbue Web-CogReasoner with knowledge-induced cognitive reasoning capabilities. It encompasses rich variety of samples aligned with Factual, Conceptual, and Procedural knowledge dimensions, including diverse web page screenshots, corresponding accessibility trees, and detailed interaction trajectories with associated reasoning chains. Its comprehensive and structured nature is crucial for enabling the model to learn and perform sophisticated web navigation tasks. Evaluation Datasets To rigorously evaluate Web-CogReasoners performance and generalization capabilities, we employ multi-faceted evaluation strategy utilizing various prominent datasets. We utilize the following datasets for evaluation: Web-CogBench: As elaborated in Section 3.4, this custom-built benchmark is meticulously designed to assess distinct knowledge-driven cognitive abilities: Memorizing, Understand4https://www.anthropic.com/news/claude-4. 5https://deepmind.google/models/gemini/pro/. ing, and Executing. Aallowing for granular evaluation of how our knowledge-induced reasoning framework contributes to these specific competencies. VisualWebBench [24]: human-curated multimodal benchmark covering 1.5K examples across 139 real websites and 87 sub-domains, designed to evaluate MLLMss grounding and reasoning abilities in complex, web-based scenarios. WebVoyager [13]: This dataset comprises tasks on 15 websites that were seen during training and includes total of 643 online web task queries. It primarily assesses the agents ability to perform tasks on familiar web environments. Online Multimodal-Mind2Web [9]: This includes two complementary subsets. The Cross-Task Test Set contains 112 queries from 33 websites seen during training, evaluating generalization to new tasks within known structures. The Cross-Website Test Set comprises 106 queries from 6 unseen websites across familiar domains (Entertainment, Shopping, and Travel), assessing the agents ability to transfer domain knowledge to novel layouts. 5.2 Evaluation Metrics To provide comprehensive and quantitative assessment of our models performance, we employ set of specific metrics tailored to the nature of our evaluation datasets. Web-CogBench Metrics For our Web-CogBench, which evaluates distinct cognitive abilities, we use variety of metrics depending on the task format: Accuracy: For tasks with definitive correct answer, such as multiple-choice questions (Next Page Prediction) or single-element identification (Source Element Prediction, Popup Close, Single Step Exploration), we calculate the standard accuracy. An answer is considered correct only if it exactly matches the ground truth. ROUGE-L: For generative tasks that produce open-ended text, such as Element Attribute Recognition, we use ROUGE-L to measure the F1-score of the longest common subsequence between the predicted text and the ground truth. This metric effectively evaluates content overlap for short-form text generation. LVM Judge: For tasks requiring qualitative assessment of generated content, such as Element Understanding and WebPage Understanding, simple string match is insufficient. Therefore, we employ powerful Large Vision Model (LVM) as an automated judge. The LVM is prompted to evaluate the quality, relevance, and accuracy of the models generated description against the ground truth, returning numerical score on 5-point scale. This approach provides scalable and consistent method for evaluating complex, open-ended visual reasoning. Online Web Tasks Metric For online evaluation on benchmarks like WebVoyager and Multimodal-Mind2Web, the primary metric is the Task Success Rate (SR). task is considered success if and only if the agent successfully completes all required sub-steps and achieves the final goal specified in the instruction within the maximum allowed number of steps. The final success rate is the percentage of successfully completed tasks over the total number of tasks in the test set. This metric provides clear and direct measure of the agents practical utility and effectiveness in real-world, dynamic environments. 5.3 Main Result To validate the effectiveness of our proposed Web-CogReasoner and its Web-CogKnowledge Framework inspired by Blooms taxonomy, we conduct comprehensive set of experiments across several challenging benchmarks. We evaluate our models performance in three key areas: fine-grained cognitive capabilities, visual understanding, and practical generalization on live online tasks. Our model is compared against leading proprietary agents, such as Claude Sonnet 4 and Gemini 2.5 Pro, as well as state-of-the-art open-source models. Results on Web-CogBench We first evaluate Web-CogReasoner on our proposed Web-CogBench, benchmark specifically curated to assess the foundational knowledge and cognitive reasoning 13 Table 4: Performance evaluation on the Web-CogBench benchmark. Model Claude Sonnet 4 Gemini 2.5 Pro Qwen2.5-VL-7B UI-TARs-7B-SFT Web-CogReasoner (Ours) Element Attribute Rec Next Page Pre Source Element Pre Element Understanding 79.7 79.8 53.2 63.5 91.4 93.5 94.6 83.9 88.0 93. 62.5 84.4 65.6 31.3 87.5 62.8 62.6 60.0 48.0 69. WebPage Understanding User Intent Pre Popup Close Single Step Exp Overall Claude Sonnet 4 Gemini 2.5 Pro Qwen2.5-VL-7B UI-TARs-7B-SFT Web-CogReasoner (Ours) 54.3 73.5 62.0 48.0 79.0 64.7 51. 51.9 32.4 61.4 100 96.6 91.4 25.9 98.3 96.8 98. 90.3 33.9 95.2 76.8 80.2 69.8 46.4 84.4 capabilities of web agents. It encompasses twelve diverse tasks ranging from element recognition to user intent inference, in alignment with our two-stage cognitive framework. As presented in Table 4, Web-CogReasoner achieves state-of-the-art performance with an overall accuracy of 82.9%, surpassing both commercial and open-source baselines. Notably, it outperforms the powerful proprietary model Gemini 2.5 Pro (77.6%) and the strong open-source model Qwen2.5VL-7B (66.9%), with substantial gains in high-level reasoning tasks such as Source Element Prediction (87.5% vs. 84.4%) and WebPage Understanding (79.0% vs. 73.5%). These results confirm the effectiveness of our design, which couples structured knowledge acquisition (factual, conceptual, procedural) with Knowledge-driven Chain-of-Thought (CoT) reasoning to form solid cognitive foundation for agents. Importantly, we find that strong performance on Web-CogBench is highly indicative of overall agent effectiveness across web tasks. For example, Gemini performs best on Web-CogBench among baseline models and also exhibits the best overall task completion rate in live-agent evaluations, in table 6 and table 7. However, we further observe that foundational model capabilitiesparticularly in visual perceptionplay critical role in enabling such high-level cognition. Agents with weaker visual recognition skills exhibit reduced performance even when endowed with strong reasoning mechanisms, as reflected in additional evaluation results from VisualWebBench, in table 5, especially in perception-oriented tasks. In summary, Web-CogBench serves not only as direct measure of an agents reasoning capacity, but also implicitly requires strong perceptual grounding. Excelling on Web-CogBench implies that an agent possesses both the knowledge reasoning ability and the necessary perceptual competencemaking it highly reliable proxy for real-world web exploration effectiveness. Table 5: Performance evaluation on the VisualWebBench benchmark. Tasks are grouped into Perception-Oriented and Reasoning-Oriented categories. Model Claude Sonnet 4 Gemini 2.5 Pro Qwen2.5-VL-7B UI-TARs-7B-SFT Web-CogReasoner (Ours) Perception-Oriented Tasks WebQA HeadOCR OCR Perception Avg Reasoning-Oriented Tasks Element Ground Action Prediction Action Ground Reasoning Avg Overall Avg 81.1 91. 77.5 91.8 96.4 96.1 96.8 86.8 91.8 96.1 96.3 90. 68.0 85.4 88.4 91.2 92.9 77.4 89.7 93.6 85.9 86. 76.0 86.0 86.3 73.3 74.9 70.8 71.3 67.2 72.6 70. 71.7 78.7 72.6 96.2 95.1 81.4 97.2 97.0 80.7 80. 74.6 82.4 79.0 14 Results on VisualWebBench To evaluate the models visual understanding capabilities, we conduct experiments on the VisualWebBench benchmark [24]. The results, summarized in Table 5, expose the limitations of visually-focused benchmarks and motivate the development of our proposed Web-CogBench. Our Web-CogReasoner achieves the highest average score of 86.3%, marginally outperforming UITARs (86.0%). While this may suggest comparable performance at first glance, closer examination reveals stark contrast: UI-TARs performs significantly worse on Web-CogBench, with an overall score of only 48.2% (see Table 4). This discrepancy underscores critical insight excelling in tasks such as OCR and element localization does not imply competence in cognitive reasoning. High scores on visual benchmarks like VisualWebBench can obscure deeper limitations in understanding and decision-making. In contrast, Web-CogReasoner demonstrates consistent strength across both visual and reasoning benchmarks, confirming that our framework effectively integrates visual perception with advanced cognitive reasoning. This dual capability is essential for building robust and reliable web agents. Results on Online Web Tasks Beyond offline benchmarks, we evaluated Web-CogReasoners practical utility and generalization ability on live, online web tasks using the WebVoyager [13] and Online Multimodal-Mind2Web [9] datasets. These experiments test the agents ability to apply its knowledge and reasoning to complete complex, multi-step and live tasks on unseen websites and scenarios, which is the ultimate goal for any autonomous web agent. Notably, we did not include UI-TARs-7B-SFT [28] in the online evaluation due to the absence of publicly available online inference scripts. Therefore, we focus our comparison on the OpenWebVoyager [13] series, which are specifically designed for web task execution. Table 6: Task success rates on the WebVoyager test set. We compare our model, Web-CogReasoner, against several baselines across 14 websites. The \"Overall\" score is the average success rate. Agent Allrecipes Amazon Apple ArXiv GitHub Booking ESPN Coursera Claude Sonnet 4 Gemini 2.5 Pro Qwen2.5-VL-7B OpenWebVoyagerIL OpenWebVoyagerMax Web-CogReasoner (Ours) 26.7% 60.0% 0.0% 17.8% 22.2% 87.8% 63.4% 0.0% 12.2% 29.3% 48.8% 69.8% 68.3% 62.8% 67.4% 68.3% 2.3% 9.1% 45.5% 56.8% 0.0% 0.0% 4.7% 20.9% 14.0% 14.6% 32.6% 20.9% 26.8% 0.0% 0.0% 9.1% 9.1% 11.4% 11.4% 83.3% 73.8% 2.3% 31.0% 42.9% 26.7% 31.7% 32.6% 34.9% 29.3% 2.3% 15.9% 54.8% Cambridge Dictionary Google Flights Google Map Huggingface Wolfram Alpha Overall Claude Sonnet 4 Gemini 2.5 Pro Qwen2.5-VL-7B OpenWebVoyagerIL OpenWebVoyagerMax Web-CogReasoner (Ours) 37.2% 76.7% 11.6% 37.2% 34.9% 4.8% 4.8% 80.5% 75.6% 2.4% 0.0% 9.5% 22.0% 21.4% 29.3% 48.8% 58.1% 7.0% 20.9% 32.6% 37.2% 82.6% 82.6% 2.2% 26.1% 37.0% 47.7% 54.9% 2.2% 18.1% 26.2% 39.1% 30.2% 14.3% 55.8% 9.5% 39.0% BBC News 23.8% 52.3% 0.0% 9.5% 14.3% On the WebVoyager test set  (Table 6)  , Web-CogReasoner establishes new state-of-the-art for opensource agents with an overall success rate of 30.2%. This represents significant improvement over the previous best open-source model, OpenWebVoyagerMax (26.2%). Our model shows particularly strong performance on knowledge-intensive websites such as Cambridge Dictionary (55.8%) and Coursera (54.8%), reinforcing the benefits of our knowledge-centric design. While performance gap still exists when compared to leading proprietary models like Gemini (54.9%), our results demonstrate substantial step forward in closing this divide for the open-source community. The Mind2Web dataset  (Table 7)  serves as crucial benchmark for evaluating generalization to unseen tasks (cross-task) and unseen websites (cross-web). Our model, Web-CogReasoner, records overall scores of 17.0% and 10.1% in these respective settings. For fair interpretation of these results, it is essential to consider the training methodologies of the open-source baselines. OpenWebVoyagerIL acts as the standard baseline for generalization. However, OpenWebVoyagerMax involved an additional training stage where websites with high error rates were 15 Table 7: Performance comparison on the Mind2Web Online dataset under cross-task and crosswebsites Agent Cross-task (Unseen Task) Cross-web (Unseen Websites) Entertainment Shopping Travel Overall Entertainment Shopping Travel Overall Claude Sonnet 4 Gemini 2.5 Pro OpenWebVoyagerMax Qwen2.5-VL-7B OpenWebVoyagerIL Web-CogReasoner (Ours) 44.9% 46.9% 22.4% 2.2% 8.2% 16.3% 35.3% 35.3% 29.4% 40.0% 40.2% 28.3% 37.5% 15.2% 20.5% 1.0% 0.0% 0.0% 5.9% 6.3% 4.3% 23.5% 15.2% 17.0% 45.5% 42.4% 3.0% 3.0% 3.0% 12.1% 6.7% 10.0% 8.7% 0.0% 5.8% 7.7% 14.0% 21.7% 23.3% 25.5% 23.3% 11.7% 1.0% 0.0% 6.6% 4.7% 9.3% 10.1% re-sampled and used for further fine-tuning. This specialized training strategy provided it with an advantage on the more challenging scenarios that are prevalent in the test distribution, even without direct access to the test data. Consequently, comparisons with OpenWebVoyagerMax may not fully reflect genuine zero-shot generalization capabilities. Therefore, the most appropriate direct comparison is with OpenWebVoyagerIL. In this regard, our model significantly outperforms the baseline, showcasing substantial leap in performance (17.0% vs. 6.3% in cross-task). More remarkably, our models performance remains highly competitive even when compared to the fine-tuned OpenWebVoyagerMax (20.5% and 11.7%). The small performance gap between our genuinely generalized model and the specially tuned one strongly validates the robustness and superior generalization power of our approach, which builds on strong foundation of knowledge and cognitive reasoning. 5.4 Ablation Study Table 8: Ablation study of our Web-CogReasoner on the Web-CogBench. We evaluate the impact of each knowledge stage by progressively training the base model. \"Base Model\" is the original Qwen2.5-VL-7B. Each subsequent stage adds new layer of knowledge training. Model Configuration Memorizing Understanding Exploring Overall Qwen2.5-VL-7B (Base Model) 67.6 61.0 77. + Factual Knowledge (S1) + Conceptual Knowledge (S2) + Procedural Knowledge (S3) 85.5 (+16.9) 88.1 90.8 64.2 75.5 (+11.3) 74.1 60.1 65.8 85.0 (+7.1) 69.8 72.1 78.3 84. To empirically validate the effectiveness of our hierarchical, Blooms Taxonomy-inspired training methodology, we conduct thorough ablation study. Our goal is to isolate and quantify the contribution of each distinct knowledge layer: Factual, Conceptual, and Procedural. We progressively build our Web-CogReasoner agent, starting from the base Qwen2.5-VL-7B model and sequentially adding each knowledge stage. The performance at each step is evaluated on our Web-CogBench, as its cognitive dimensions (Memorizing, Understanding, Exploring) directly align with our knowledge framework. The results are presented in Table 8. Impact of Factual Knowledge The first stage of our curriculum introduces Factual Knowledge, teaching the agent the \"what\" of web elements. As shown in Table 8, this initial stage provides substantial performance boost, raising the overall score from 66.9% to 72.1%. Critically, the improvement is most pronounced in the Memorizing dimension, which surges from 68.6% to 85.5%. This demonstrates that by learning to perceive and identify concrete, factual information (e.g., element roles and names), the agent builds solid perceptual foundation, which is the first and essential step towards more complex reasoning. Impact of Conceptual Knowledge Building upon this factual base, the second stage integrates Conceptual Knowledge, which focuses on the \"how things relate.\" This layer enables the agent to understand the relationships between elements and the underlying semantic structure of webpage. The results show another significant leap in performance, with the overall score climbing to 78.3%. The most notable gain occurs in the Understanding dimension, which improves by 11.3%. This 16 confirms our hypothesis that once an agent can perceive facts, it must then learn to comprehend their context and purpose to effectively interpret webpages functionality. Impact of Procedural Knowledge The final stage incorporates Procedural Knowledge, teaching the agent \"how to act\" to achieve goal. This layer completes the Web-CogReasoners cognitive architecture, enabling it to formulate and execute plans. This stage delivers the final and most critical improvements for task completion, pushing the overall score to our final state-of-the-art result of 85.0%. The most dramatic increase is observed in the Exploring dimension, which skyrockets by 7.1%. This finding strongly indicates that goal-oriented, procedural training is paramount for converting passive understanding into active, successful task execution. In summary, this staged analysis provides compelling empirical evidence for our core thesis. Each knowledge layer, from Factual to Conceptual to Procedural, is an indispensable component that progressively builds the agents cognitive capabilities, validating our structured, curriculum-based approach to training highly capable web agents. 5.5 Qualitative Analysis Beyond the quantitative improvements shown in our ablation study, qualitative analysis of the agents behavior at each stage offers deeper insights into how our curriculum shapes its cognitive abilities. We examine the agents performance on representative task: \"Find and add laptop under $1000 to the shopping cart on an e-commerce website.\" Base Model (Qwen2.5-VL-7B) Without any specialized training, the base model struggles to formulate coherent plan. Its reasoning is often generic and untethered from the specific UI. It might correctly identify \"search bar\" but fails to execute meaningful action, or hallucinates actions that are not possible. For instance, its thought process might be: \"I should search for laptop,\" but its action is an ungrounded click \"Categories\" because it lacks the procedural knowledge to connect intent to multi-step sequence of actions. Stage 1 Agent (+ Factual Knowledge) After training on Factual Knowledge, the agents perceptual abilities are significantly enhanced. It can now accurately identify and label key elements with their correct attributes. Its thought process becomes grounded in the facts of the page: \"I see search bar [ID: 25] with the name Search products. see button [ID: 28] with the name Search.\" However, it still struggles with planning. It understands \"what\" is on the page but not \"why\" or \"how\" to use it. It might correctly type \"laptop\" into the search bar but then get stuck, not understanding that the next logical step is to click the search button to submit the query. Stage 2 Agent (+ Conceptual Knowledge) With the addition of Conceptual Knowledge, the agent begins to understand the relationships between elements and their purpose. Its reasoning graduates from simple identification to semantic interpretation. The thought process now reflects this understanding: \"The search bar [ID: 25] is for inputting queries. The search button [ID: 28] is functionally linked to it and will trigger the search. This group of elements forms search component.\" This allows it to reliably complete the search and navigate to the results page. However, on the results page, it may still struggle with complex procedural logic, such as applying price filter. Full Model (Web-CogReasoner, + Procedural Knowledge) The final agent, equipped with Procedural Knowledge, demonstrates goal-oriented planning and execution. It seamlessly translates the high-level task into concrete action sequence. Its thought process is now strategic plan: \"Goal: Add laptop under $1000 to cart. Step 1: Type laptop into search bar [ID: 25]. Step 2: Click search button [ID: 28]. Step 3: On the results page, locate the Price Range filter. Step 4: Input 1000 into the max price field [ID: 57]. Step 5: Identify suitable product from the filtered list and click its Add to Cart button [ID: 83].\" This demonstrates complete cognitive loop from perception and understanding to successful action, validating the necessity of the final procedural training stage."
        },
        {
            "title": "References",
            "content": "[1] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [2] Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. Screenai: vision-language model for ui and infographics understanding. arXiv preprint arXiv:2402.04615, 2024. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Fengxiang Bie, Yibo Yang, Zhongzhu Zhou, Adam Ghanem, Minjia Zhang, Zhewei Yao, Xiaoxia Wu, Connor Holmes, Pareesa Golnari, David Clifton, et al. Renaissance: survey into ai text-to-image generation in the era of large model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [5] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1649516504, 2022. [6] Qi Chen, Dileepa Pitawela, Chongyang Zhao, Gengze Zhou, Hsiang-Ting Chen, and Qi Wu. Webvln: Vision-and-language navigation on websites. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 11651173, 2024. [7] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [8] Jack Conklin. taxonomy for learning, teaching, and assessing: revision of blooms taxonomy of educational objectives complete edition, 2005. [9] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [10] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [11] Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large language models. arXiv preprint arXiv:2210.03945, 2022. [12] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [13] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, and Dong Yu. Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. arXiv preprint arXiv:2410.19609, 2024. [14] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [15] Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, Victor Carbune, Jason Lin, Maria Wang, Srinivas Sunkara, Yun Zhu, and Jindong Chen. Screenqa: Large-scale question-answer pairs over mobile app screenshots. arXiv preprint arXiv:2209.08199, 2022. 18 [16] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [17] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [18] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. [19] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [20] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: large language modelbased web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 52955306, 2024. [21] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [22] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. [23] Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. Harnessing webpage uis for text-rich visual understanding. arXiv preprint arXiv:2410.13824, 2024. [24] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024. [25] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [26] Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip Yu, et al. survey of webagents: Towards next-generation ai agents for web automation with large foundation models. arXiv preprint arXiv:2503.23350, 2025. [27] CHRISTOPHER Ormell. Blooms taxonomy and the objectives of education. Educational Research, 17(1):318, 1974. [28] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [29] Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: conceptual taxonomy, applications and challenge. arXiv preprint arXiv:2505.10468, 2025. [30] Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1564115653, 2024. [31] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [32] Hai-Ming Xu, Qi Chen, Lei Wang, and Lingqiao Liu. Attention-driven gui grounding: Leveraging pretrained multimodal large language models without fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 88518859, 2025. [33] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. [34] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [35] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. [36] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [37] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Blooms Taxonomy and Web Cognitive A.1.1 Blooms Tax 1. Factual Knowledge: The foundational layer, encompassing the basic, discrete elements of discipline that student must know, such as essential terminology and specific, isolated details. 2. Conceptual Knowledge: The synthesis of factual elements into coherent, organized structure. This level focuses on the interrelationships between basic elements, including knowledge of classifications, principles, generalizations, theories, and models. 3. Procedural Knowledge: The knowledge of how to perform task or inquiry. This involves an understanding of specific skills, algorithms, techniques, and methods, representing shift from \"knowing-what\" to \"knowing-how.\" A.1.2 Web data introduction Cognitive science typically classifies human knowledge into three categoriesfactual, conceptual, and proceduralwhich correspond to different stages of cognitive development: perceiving, understanding, and executing. This taxonomy captures the natural trajectory of human learning: starting with the perception of concrete facts and data (factual knowledge), progressing toward abstract comprehension of concepts and relationships (conceptual knowledge), and ultimately acquiring the ability to carry out complex, goal-oriented behaviors through practiced routines and strategies (procedural knowledge). An illustrative example is shown in Figure5). Figure 5: How people handle unfamiliar web pages. People learn factual, conceptual, and procedural knowledge to memorize, understand, and explore the web, ultimately completing specific tasks. A.1.3 Data Sourcing To collect comprehensive metadata from web pages, we developed data collection tool based on Playwright. This tool performs deep traversal and interaction by systematically clicking on all elements within each page. We define each round of interaction (i.e., one click) as layer, and using this iterative approach, we collected Layer 1 to Layer 6 data from 14 different websites (for complete website information, refer to Table 2 ) Data precessing For each web element, we capture its standalone screenshot, as well as screenshots taken before clicking (both with and without red bounding box), after hovering, and after clicking. Table 9: Web elementss meta-data. Data Description css allcss ourterhtml location role name elements CSS selectors CSS selector sequence of preceding elements elements outerhtml elements boundingbox elements role elements name See Figure 6 for an example. We also collect the following metadata: CSS, allCSS, outerHTML, and location. Additionally, we extract semantic information from each element based on its outerHTML. If role attribute is explicitly defined, we use its value directly as the elements semantic role. Otherwise, we infer the role by mapping the tag name using the WAI-ARIA specification. Similarly, to determine the elements semantic name, we extract the value of the aria-label if present; otherwise, we get its textual content. See Table 9 for detailed metadata of the web elements. Figure 6: Example visual states of web element (USD) we captured. Shown are: the element highlighted in the full-page view (base_rect.png), the hover state (hover.png), and the click state (click.png). These screenshots illustrate how the elements visual context evolves through user interactions. After collecting both the visual and semantic metadata, we present the corresponding screenshots of each clickable element to Qwen-VL 72B. The model is instructed to: analyze the visual changes on the page after hovering over and clicking the target element; identify and list any sub-elements that appear upon interaction (e.g., when dropdown menu is triggered by clicking); infer and generalize the functional purpose of the element. For the functional purpose prediction, the model is additionally required to provide confidence score. If this score remains below 0.5 after three retries, the prediction is excluded from evaluation. A.2 Task definition A.2.1 Factual Web Knowledge Element Attribute Rec We define the Element Attribute Recognition task to assess models capability to infer the interactive semantics of web elements exclusively from visual input. Given 22 full-page screenshot with specific interactive element marked by red bounding box, the model is tasked with predicting two key attributes: the semantic role of the highlighted element (e.g., \"button\", \"link\", \"checkbox\"), the semantic name, which refers to the elements accessible textual description (e.g., \"Submit\", \"Search\", \"Next\"). The ground truth for both attributes is derived from the element-level metadata collected as described in Section A.1.3. This task simulates the human cognitive ability to interpret the function of web interface components through visual perception alone, without relying on HTML structure or programmatic representations. Sub-elements Pre We define the Element Sub-element Prediction task to evaluate models ability to infer the hierarchical structure of web interfacesspecifically, to identify the sub-elements that become visible upon interaction with given parent element, using only visual information. In each task instance, the model is presented with full-page screenshot in which specific interactive element is highlighted by red bounding box. The model is instructed to predict the set of subelements (e.g., menu items, dropdown options) that appear as direct result of interacting with the highlighted element, such as clicking or hovering.The ground truth annotations for sub-elements are derived from the element-level metadata collected during the dynamic interaction process, as detailed in SectionA.1.3. This task simulates the human cognitive process of understanding interactive dependencies in graphical interfacerecognizing not only that component is clickable, but also predicting its dynamic expansion behavior. Page Change Pre We define the Page Change Prediction task to evaluate models capability to infer the visual consequences of interacting with specific web element, relying solely on visual input. In this task, the model is presented with full-page screenshot in which target interactive element is highlighted by red bounding box. The model is required to in an open-ended format to predict the visual changes that are likely to occur on the page after the element is clicked.The ground truth for this task is obtained from the generated responses of Qwen-VL 72B, which were produced based on visual metadata, as detailed in SectionA.1.3. This task is designed to simulate the human cognitive ability to anticipate the dynamic behavior of web interfaces through perception alonewithout access to the underlying source code or prior knowledge of the page logic. Next Page Pre We define the Next Page Prediction task to evaluate models ability to forecast navigation outcomes. Given full-page screenshot with highlighted interactive element, the model must predict the subsequent page that would result from interacting with that element. To ensure generalization capability, we implement two evaluation formats: multiple-choice selection (choosing from 4-5 possible next pages) and open-ended generation (describing the expected next page). Ground truth is derived from actual navigation sequences recorded during web interactions. This task assesses the agents understanding of functional relationships between interface elements and destination pages. Source element Pre We define the Source Element Prediction task to assess models ability to identify which element on webpage leads to specific target page, using only visual input. The model is given two screenshots: one showing the current webpage with 4-10 candidate elements marked by bounding boxes, and another showing the resulting target page. Based on visual cues alone, the model should determine which candidate element would trigger the transition to the target page when interacted with. This task simulates the human ability to reason about visual cause-and-effect relationships in web navigation, without relying on code or prior knowledge of page logic. A.2.2 Understanding Web Knowledge Element Understanding This task requires the model to produce comprehensive, open-ended description of highlighted elements visual appearance, functional semantics, and placement on the webpage. Specifically, the output should cover: (1) Visual Traits (text, shape, iconography); (2) Location (e.g., top-right, footer); and (3) Function (e.g., navigates to user profile). This task simulates abstract comprehension from concrete element appearance. 23 WebPage Understanding In this task, the model must generate detailed and structured overview of the entire page. The response includes layout segmentation (e.g., header, sidebar, content area), key modules (e.g., search panel, product gallery), and summary of page purpose and interactivity. This facilitates understanding of page-wide structure and intent. Caption & QA We define the Caption & QA task to evaluate models capability to comprehend and reason over both image and non-image content embedded within webpages. This task comprises four subtasks: Embedded Image Captioning: Given full-page screenshot containing one or more embedded images, the model is required to generate detailed and semantically meaningful caption for each image, describing its visual content and its contextual relevance within the surrounding webpage layout. Embedded Image QA: Given question grounded in the content of an embedded image within webpage screenshot, the model must produce an accurate, context-aware answer using only visual information. These questions may refer to image content (e.g., \"What brand is shown in the ad?\") or its function in the UI. Webpage Captioning: The model is tasked with generating an open-ended description of the webpages content, layout, and interactive purpose, treating the entire screenshot as input. The generated caption should reflect both structural composition and the inferred user intent of the webpage. Webpage QA: Given full-page screenshot and natural language question referring to any aspect of the page (e.g., title, layout, purpose, textual content), the model must generate grounded and precise answer based on visual and spatial information. All four subtasks are derived from the Multi-UI [23] dataset, which provides rich annotations for webpage visual elements and user-facing semantics. Together, these subtasks measure models ability to perform grounded visual-language understanding at both local (element-level) and global (page-level) scales. A.2.3 Procedual Web Knowledge Users intention Pre Built on the MultiModal-Mind2Web datasetwhich provides natural language instructions, action trajectories, and aligned web page screenshotswe introduce novel multi-modal task: inferring the users high-level intent from sequence of visual observations. Unlike traditional imitation learning or instruction-following tasks, our setting requires the model to infer why trajectory occurred, rather than how to execute it. Solving this task demands both visual understanding and temporal reasoning. The details of this task are as follows: 1. Task Definition: Given sequence of web page screenshots p1, p2, . . . , pn representing users interaction trajectory, the objective is to predict the original user instruction that guided the sequence. Each screenshot pt corresponds to the visual observation at step of successful task execution. Formally, the model learns mapping: : {p1, p2, . . . . . . , pn} where is the natural language instruction. 2. Dataset Construction: We construct our dataset by processing the original MultiModalMind2Web corpus. For each task, we extract only the visual observationsi.e., the sequence of web page screenshots corresponding to each step in the execution trajectory. We then pair each screenshot sequence with the original natural language instruction as the supervision signal. Popup Close We curated collection of 51 popup components from JS Design website, encompassing diverse range of visual styles and functional categories, such as notification modals, alert dialogs, and login forms. This diversity ensures comprehensive coverage of real-world popup use cases. For background webpages, we utilized the OpenWebVoyager[13] dataset, which contains large number of authentic webpage screenshots with varied layouts and content, providing rich foundation for synthesizing realistic popup-injected webpages. To construct the training data for this task, we employed the following procedure: 1. Synthesizing Webpage Screenshots with Popups: We randomly overlaid popup images onto background webpage screenshots to simulate webpages containing popups. During synthesis, we introduced variability by randomly adjusting the popups size and position and modifying the brightness and sharpness of the background images, thereby enhancing visual diversity and realism. 2. Generating Popup AX Tree: Each popup image was processed using Qwen-VL 2.5 32B to generate an ARIA-compliant AX Tree. To simulate diverse structural configurations, we randomly modified the index values of popup AX Tree elements and inserted the popup AX Tree into different locations within the original webpages AX Tree, resulting in combined AX Tree that reflects realistic variations in webpage structure. 3. Generating Popup Closing Strategies: We then instructed Qwen-VL 2.5 32B to identify all possible methods for closing the popup, based on the popup image and its corresponding AX Tree. Recognizing that, in practical settings, any correct method is sufficient, we applied combinatorial augmentation to the methods. Specifically, we enumerated all non-empty subsets of the strategies, yielding total of 2n 1 distinct answer combinations. This expansion significantly broadens the training distribution and increases the models exposure to diverse correct solutions. 4. Constructing the Training Dataset: Using the synthesized webpage screenshots and the enriched AX Tree, we constructed dataset for training models on popup dismissal. Each data point comprises: Input: webpage screenshot with an embedded popup and the corresponding AX Tree; Output: valid methods for closing the popup. Single-Step Web Task We define the Single-Step Web Task to evaluate models ability to ground high-level user intentions in visual webpage elements. Each task instance includes full-page screenshot from real-world webpage, concise natural language instruction (e.g., \"Search for product\", \"Log into the system\"), and several candidate elements marked by red bounding boxes. The model must identify which element, if clicked, would successfully fulfill the given task. This setup simulates perceptual grounding of user intentmatching natural language goals to actionable UI targets based solely on visual cues. All samples are directly sourced from the Multi-UI [23] dataset, which provides rich, annotated webpage screenshots paired with task descriptions and labeled ground-truth targets. No trajectorylevel annotation or external instruction rewriting is involved. This task offers reliable benchmark for evaluating atomic web interaction capabilities in static, visually grounded setting. Noisy Multi Step Web Task To further enhance the original OpenWebVoyager [13] dataset, we incorporate Knowledge-driven Chain-of-Thought (CoT) Reasoning to improve the models stepwise understanding and execution, details see Section 4.2 In addition, to simulate realistic interruptions during multi-step web interactions, we propose the Noisy Multi-Step Web Task by augmenting interaction trajectories from OpenWebVoyager. Specifically, for each sample in our Popup Close dataset, popup window is injected at specific step (e.g., step t) of an existing task trajectory. This modification introduces prerequisite interaction: the agent must first detect and dismiss the popup before resuming progress toward the original task goal. By explicitly modeling such interruptive UI elements, this task formulation captures more realistic web interaction paradigm in which user flows are frequently obstructed. It also provides challenging benchmark for evaluating agents robustness to UI-level noise and their capacity for error recovery."
        }
    ],
    "affiliations": [
        "Central South University",
        "Fudan University",
        "Harbin Institute of Technology",
        "Hithink Research",
        "Shanghai Jiao Tong University",
        "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "Southwestern University of Finance and Economics",
        "University of Adelaide",
        "University of California, Los Angeles",
        "University of Manchester",
        "Westlake University"
    ]
}