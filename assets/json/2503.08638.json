{
    "paper_title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "authors": [
        "Ruibin Yuan",
        "Hanfeng Lin",
        "Shuyue Guo",
        "Ge Zhang",
        "Jiahao Pan",
        "Yongyi Zang",
        "Haohe Liu",
        "Yiming Liang",
        "Wenye Ma",
        "Xingjian Du",
        "Xinrun Du",
        "Zhen Ye",
        "Tianyu Zheng",
        "Yinghao Ma",
        "Minghao Liu",
        "Zeyue Tian",
        "Ziya Zhou",
        "Liumeng Xue",
        "Xingwei Qu",
        "Yizhi Li",
        "Shangda Wu",
        "Tianhao Shen",
        "Ziyang Ma",
        "Jun Zhan",
        "Chunhui Wang",
        "Yatian Wang",
        "Xiaowei Chi",
        "Xinyue Zhang",
        "Zhenzhu Yang",
        "Xiangzhou Wang",
        "Shansong Liu",
        "Lingrui Mei",
        "Peng Li",
        "Junjie Wang",
        "Jianwei Yu",
        "Guojian Pang",
        "Xu Li",
        "Zihao Wang",
        "Xiaohuan Zhou",
        "Lijun Yu",
        "Emmanouil Benetos",
        "Yong Chen",
        "Chenghua Lin",
        "Xie Chen",
        "Gus Xia",
        "Zhaoxiang Zhang",
        "Chao Zhang",
        "Wenhu Chen",
        "Xinyu Zhou",
        "Xipeng Qiu",
        "Roger Dannenberg",
        "Jiaheng Liu",
        "Jian Yang",
        "Wenhao Huang",
        "Wei Xue",
        "Xu Tan",
        "Yike Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We tackle the task of long-form music generation--particularly the challenging \\textbf{lyrics-to-song} problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation"
        },
        {
            "title": "Start",
            "content": "YuE: Scaling Open Foundation Models for Long-Form Music Generation"
        },
        {
            "title": "HKUST and MAP",
            "content": "(alphabetical order) GitHub: https://github.com/multimodal-art-projection/YuE Demo: https://map-yue.github.io/"
        },
        {
            "title": "Abstract",
            "content": "We tackle the task of long-form music generationparticularly the challenging lyrics-to-song problemby introducing YuE (‰πê), family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through: (1) track-decoupled nexttoken prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) multitask, multiphase pre-training recipe to In addition, we redesign the in-context learning technique for converge and generalize. music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuEs learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation 5 2 0 2 1 1 ] . e [ 1 8 3 6 8 0 . 3 0 5 2 : r Figure 1. The General Application of YuE. The YuE model takes meta information and lyrics of the generated song in text and arbitrary audio as condition. The model can control outputs in multiple dimensions such as genre, emotion and languages."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work and Prelimenaries 3 YuE . . . . . . . . . . 3.1 Overview . 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Stage-1: Music Language Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Track-Decoupled Next-Token Prediction . . . . . . . . . . . . . . . . . . . 3.2.2 Structural Progressive Conditioning . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Music In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Stage-2: Residual Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Tokenization and Audio Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . 4 Training and Inference Strategies 4. Scaling Up Stage-1 Pre-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Multitask Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Multiphase Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Stage-2 Pre-training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Test-time Strategies . . . . . . . 5 Experiments 5.1 Data & Training Setup . . 5.2 Evaluation Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Main Results . . . 6.1 Human Evaluation . . . 6.1.1 Overall Comparison with Proprietary Systems. 6.1.2 Detailed Comparison with Proprietary Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Automatic Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 Vocal Agility . 6.2.2 Duration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.3 Model Based Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.4 Correlation Between Automatic Metrics and Human Evaluation . . . . . . . . . . . . 7 Fine-tuning To More Languages 8 Analysis and Ablations 8.1 Comparison of Audio Tokenizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Impact of Source Separation Prior and Dual-NTP . . . . . . . . . . . . . . . . . . . 8.3 Ablation Analysis of Lyrics-following Capabilities with CoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4 Effect of Scaling . 8.5 Analysis of Test-time Tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Representation Quality 10 Emergent Abilities 11 Memorization Effect 12 Unsuccessful Attempts 13 Conclusion and Future Work 14 Ethics and Responsibility 15 Contributions and Acknowledgments 2 5 6 6 6 6 8 9 10 11 12 12 12 13 14 14 14 14 15 16 16 17 17 18 18 18 19 20 21 21 22 23 24 24 24 25 26 26 28 28 35 35 35 36 37 38 Subjective Evaluation A.1 Evaluation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Evaluation Dimensions and Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Conditional Evaluation Dimension and Definitions . . . Qwen2Audio-Instruct Tagging Prompt Multilingual Subjective Evaluation 15 English Prompts From GPT 3 1. Introduction Neural music generation represents transformative intersection of technology and artistic creativity, offering profound commercial and cultural implications. By leveraging advanced algorithms, it is revolutionizing the music industry, enabling applications in entertainment, therapy, and personalized composition [Ma et al., 2024]. Given the universal presence of music in human culture [Mehr et al., 2019], these advances have the potential to democratize music creation, making it more accessible to broader audience, while simultaneously reshaping traditional industry practices and fostering innovative approaches to musical expression. Among various music generation tasks, lyrics-to-song audio generation, which involves creating full songs with vocals, accompaniment, from lyrics and control signals, is one of the most challenging. Despite its significance, no open-source system can achieve this at scale. While proprietary systems like Suno, and Udio1 have demonstrated impressive results, the lack of opensource alternatives limits accessibility, reproducibility, and innovation. Open-source ecosystems are crucial for advancing AI-driven music generation, enabling collaborative research and laying the foundation for AI models that can understand, compose, and innovate in the arts. Most existing academic systems for AI-driven music audio generation are constrained to short, sub-30-second clips and treat singing voice synthesis [Liu et al., 2022, Chen et al., 2020, Zhang et al., 2022] and instrumental generation [Copet et al., 2023b, Chen et al., 2023, Agostinelli et al., 2023] separately [Li et al., 2024]. While recent efforts have started addressing full-song lyrics-tomusic generation, they remain limited in effectivenessproducing short, low-quality output with poor musical coherence. The difficulty of this task arises from several key challenges: 1) Long-range dependencies: Music exhibits complex temporal structures spanning several minutes, making it difficult for models to maintain coherence over extended durations. 2) Signal complexity: Unlike speech or environmental sounds, music is inherently polyphonic, requiring precise coordination between multiple instrumental and vocal components. 3) Linguistic distortion: Singing alters phonemes, durations, and prosody in ways that differ significantly from spoken language, complicating the alignment between lyrics and melody. 4) Data scarcity: The lack of large-scale, high-quality paired datasets of lyrics, vocals, and accompaniment limits model training and generalization capabilities. In this paper, we introduce YuE, the first2 family of open foundation models designed to push the boundaries of long-form lyrics-to-song generation. Built upon the LLaMA2 [Touvron et al., 2023b, Zhang et al., 2024] architecture and trained on trillions of tokens, YuE generates highquality music up to five minutes long while maintaining lyrical alignment, musical coherence, and engaging vocal melodies. By leveraging innovative pre-training and inference techniques, YuE addresses key challenges of lyrics-to-song generation and outperforms several proprietary systems in musicality, expressiveness, and controllability. We further examine subjective correlations with various automatic metrics. Interestingly, some traditional metrics (e.g., CLAP-score [Wu et al., 2023a]) fail to align with human preferences, while metrics like CLaMP3-score [Wu et al., 2025] and vocal range correlate strongly with subjective scores, suggesting the need for new, music-specific metrics that better capture listeners perceptual judgments (Section 6). We further investigate potential memorization effects by thoroughly examining whether the model reproduces training data verbatim, and demonstrate that YuE largely avoids copying despite strong in-context conditioning (Section 11). Our main contributions include: 1) Track-Decoupled Next-Token Prediction: dual-token strategy that separately models different audio tracks (vocals, accompaniment) at the frame level, resilient to challenging low vocal-to-accompaniment ratio scenarios like metal (Section 3.2.1). 2) Structural Progressive Conditioning: progressive conditioning strategy for long-form music generation, enabling song-level lyrics following and structure control (Section 3.2.2). 1https://suno.com/, https://www.udio.com/ 2As of its release on Jan. 28, 2025, YuE familiy is the first publicly available, open-source lyrics-to-song model capable of full-song generation with quality on par with commercial systems. 4 3) Redesigned In-Context Learning for Music: novel ICL framework enabling advanced style transfer, voice cloning, and bidirectional content creation (Section 3.2.3). 4) Multitask Multiphase Pre-training: training strategy that converges and generalizes on in-the-wild data (Section 4.1). 5) Strong Performance: YuE demonstrates strong results in musicality, vocal agility, and generation duration compared to proprietary systems, supports multilingual lyrics following (Section 7), while also excelling in music understanding tasks on representation learning benchmark MARBLE (Section 9). 2. Related Work and Prelimenaries Music Generation and Singing Voice Synthesis. Early music generation approaches primarily focused on MIDI-based methods [Huang et al., 2018, Payne, 2022], while recent models generate raw audio conditioned on tags or text [Dhariwal et al., 2020a, Agostinelli et al., 2023, Liu et al., 2023, Huang et al., 2023, Copet et al., 2023a, Chen et al., 2024, Evans et al., 2024]. However, most existing audio methods are limited to instrumental music with short durations (around 30 seconds) due to computational constraints. Although some efforts incorporate vocals, they typically lack coherent lyrical semantics [Agostinelli et al., 2023, Dhariwal et al., 2020a]. Concurrently, deep learning has significantly advanced singing voice synthesis (SVS), leveraging techniques like GANs, diffusion models, and variational autoencoders for high-quality vocal synthesis [Chen et al., 2020, Liu et al., 2022, Zhang et al., 2022, Hong et al., 2023], and enabling nuanced control via language prompts or discrete tokens [Donahue et al., 2023, Wang et al., 2024, Wu et al., 2024]. Nevertheless, these SVS systems mostly generate pure vocals with explicit melodic guidance. In contrast, our work proposes novel approach capable of autonomously generating coherent and semantically meaningful vocals alongside instrumental accompaniments, supporting significantly extended song contexts of up to five minutes, thus substantially advancing automated music production. Song Generation. Despite recent progress in music generation research, academic models still face significant limitations. Previous or concurrent work, such as Jukebox [Dhariwal et al., 2020b], MelodyLM [Li et al., 2024], SongCreator [Lei et al., 2024], SongGen [Liu et al., 2025] struggle to generate long-form music audio beyond 30 seconds while maintaining coherence and high-quality synthesis. These models often lack fully open-source implementations, making reproducibility and further improvements difficult. For instance, Jukebox utilizes multi-scale VQ-VAE for raw audio modeling but suffers from noticeable artifacts and limited controllability. Similarly, SongCreator [Lei et al., 2025] and SongGen [Liu et al., 2025] introduce innovative transformer-based architectures for text-to-song generation, yet their performance is inferior to commercial counterparts. In contrast, industry-developed systems such as Tiangong Music (Kunlun Ltd.), Seed Music (ByteDance)[Bai et al., 2024], Suno, Udio, and Hailuo Music (MiniMax) have demonstrated promising results in song-level audio generation, though their technical details remain undisclosed. Our work addresses these gaps by offering an open-source, songlevel generative model with full technical transparency, achieving performance on par with leading proprietary systems. Audio Tokenizers. Discrete modeling of audio often employs neural codec tokenizers, particularly Residual Vector Quantization GANs (RVQ-GANs) [Kumar et al., 2024], typically categorized into acoustic and semantic tokens [D√©fossez et al., 2024, Borsos et al., 2023]. Acoustic tokens, optimized for reconstruction, encode fine acoustic details, causing significant token shifts even with minor acoustic variations. Prior studies [Copet et al., 2023b] indicate these tokens require extensive training epochs; notably, we find acoustic tokens alone fail to converge efficiently on our dataset (Section 8.1). Conversely, semantic tokens, derived from self-supervised learning encoders [Schneider et al., 2019, Baevski et al., 2020, Chung et al., 2021, Baevski et al., 2022, Ma et al., 2023], produce semantically meaningful representations (e.g., phonemes, notes, genres) [Zhang et al., 2023, Yuan et al., 2024b, Wang et al., 2025]. Unlike previous work, we conduct extensive experiments on complex in-the-wild music datasets, perform qualitative comparisons, and report tokenizer convergence, demonstrating that fusing semantic information significantly enhance convergence. 3. YuE 3.1. Overview YuE is an autoregressive (AR) language model (LM)-based framework tailored for lyrics-to-song generation. As depicted in Figure 2, YuE comprises four main components: an audio tokenizer (with lightweight upsampler), text tokenizer, and two language models (LMs). The audio tokenizer converts waveforms into discrete tokens using semanticacoustic fused approach. The Stage-1 LM is track-decoupled, trained on text tokens and semantic-rich bottom-level audio tokens (codebook-0 from residual VQ-VAE), modeling lyricsto-song generation as an AR next-token prediction (NTP) task. In Stage-2, smaller LM predicts residual tokens from codebook-0 tokens to reconstruct audio. Both LMs follow the widely-adopted LLaMA2 architecture [Touvron et al., 2023a, Team, 2024]. Finally, lightweight vocoder upsamples Stage2s 16 kHz audio to 44.1 kHz output. 3.2. Stage-1: Music Language Modeling Music language modeling stage (MuLM), illustrated in Figure 4, enables music generation conditioned on diverse inputs (lyrics, tags, structures, reference audio). We introduce MuLMs core techniques: 1) track-decoupled next-token prediction (Section 3.2.1), 2) structural progressive generation (Section 3.2.2), and 3) music in-context learning (Section 3.2.3). 3.2.1. Track-Decoupled Next-Token Prediction Challenges of Standard NTP. Popular LM-based approaches for modeling long RVQ sequences typically adopt multi-stage design [Wang et al., 2023a, Agostinelli et al., 2023, Borsos et al., 2023], where the first stage commonly uses single codebook-0 token to represent each audio frame.3 Let x1:ùëá = (ùë•1, ùë•2, . . . , ùë•ùëá ) represent sequence of audio tokens, where each ùë•ùë° corresponds to one frame. In standard NTP framework, we factorize the joint probability of x1:ùëá as: Figure 2. Overview of YuE framework: two-stage lyrics-to-song generation with audio/text tokenizers and two language models. Stage-1: music language modeling. Stage-2: residual modeling. Blue: vocal tokens, orange: accompaniment tokens, and grey: residual tokens. ùëù(x1:ùëá ) = ùëá (cid:214) ùë°=1 ùëù(cid:0)ùë•ùë° ùë•<ùë°; ùúÉ(cid:1), (1) where ùúÉ is the model parameter. During inference (generation), the model predicts the next token ÀÜùë•ùë° which maximizes the conditional distribution: ÀÜùë•ùë° = arg max ùë•ùë° ùëù(cid:0)ùë•ùë° ùë•<ùë°; ùúÉ(cid:1). (2) This approach works well for tokens x1:ùëá representing purely vocal (text-to-speech, TTS) or instrumental (text-to-music, TTM) signals but struggles when encoding both vocals and accompaniment simultaneously due to differing dynamics, as in lyrics-to-song tasks combining TTS and TTM. We quantify Linguistic information Loss After Tokenization (LLAT) using delta word error rate (ŒîWER), defined as ŒîWER = WERrecon WERori, where WERrecon and WERori are estimated by 3We acknowledge single-stage methods such as MusicGen, which utilize delay or parallel decoding patterns to reduce sequence length. However, we observed that the parallel decoding pattern fails to converge on our dataset, while the delay pattern results in longer sequences compared to multi-stage approaches. 6 Figure 3. ŒîWER across different music genres for mixture / vocal-only tracks. ŒîWER LLAT. fine-tuned Whisper4 model on tokenizer-reconstructed5 and original mixture audio, respectively. Figure 3 illustrates the relationship between ŒîWER and music genre (hip-hop, pop, metal) using 1k sampled tracks. An upward trend is evident, with metal exhibiting the highest LLAT followed by pop and hip-hop, indicating greater modeling difficulty in acoustically dense genres. Vocal-only tracks consistently achieve lower ŒîWER compared to mixtures, indicating lower LLAT after source separation. Track-Decoupled Next-Token Prediction (Dual-NTP). The above observation suggests that the issue arises from forcing single token ùë•ùë° to represent two distinct signals: vocal and music. Accompaniment can overshadow the vocal track, degrading lyric intelligibility. To overcome these shortcomings, we propose method that explicitly incorporates source separation prior, splitting each time step into two tokens: one for vocal and one for accompaniment (see dotted token pairs in Figure 4). In the proposed method, each time step ùë° outputs two tokens: ùë£ùë° (vocal token) and ùëéùë° (accompaniment token). The models sequence of tokens thus becomes: (cid:0) ùë£1 , (cid:124)(cid:123)(cid:122)(cid:125) vocal , ùëé1 (cid:124)(cid:123)(cid:122)(cid:125) accomp. ùë£2 , (cid:124)(cid:123)(cid:122)(cid:125) vocal ùëé2 (cid:124)(cid:123)(cid:122)(cid:125) accomp. , . . . , ùë£ùëá , (cid:124)(cid:123)(cid:122)(cid:125) vocal (cid:1). ùëéùëá (cid:124)(cid:123)(cid:122)(cid:125) accomp. (3) To formally define this, let v1:ùëá = (ùë£1, ùë£2, . . . , ùë£ùëá ) and a1:ùëá = (ùëé1, ùëé2, . . . , ùëéùëá ). We factorize their joint probability as: ùëù(cid:0)v1:ùëá , a1:ùëá (cid:1) = (cid:16) ùëù ùë£ùë°, ùëéùë° (cid:12) (cid:12) (cid:12) ùë£<ùë°, ùëé<ùë°; ùúÉ (cid:17) . ùëá (cid:214) ùë°=1 At inference time, the next pair (cid:0) ÀÜùë£ùë°, ÀÜùëéùë°(cid:1) is chosen to maximize this joint conditional: (cid:0) ÀÜùë£ùë°, ÀÜùëéùë°(cid:1) = arg max (ùë£ùë°, ùëéùë° ) (cid:16) ùëù ùë£ùë°, ùëéùë° (cid:12) (cid:12) (cid:12) ùë£<ùë°, ùëé<ùë°; ùúÉ (cid:17) . Although this probability is written in joint form, it can be decomposed as: (cid:16) ùëù ùë£ùë°, ùëéùë° (cid:12) (cid:12) (cid:12) ùë£<ùë°, ùëé<ùë°; ùúÉ (cid:17) (cid:16) ùë£ùë° = ùëù (cid:12) (cid:12) (cid:12) ùë£<ùë°, ùëé<ùë°; ùúÉ (cid:17) (cid:16) ùëéùë° ùëù (cid:12) (cid:12) (cid:12) ùë£ùë°, ùëé<ùë°; ùúÉ (cid:17) , making it straightforward to implement in standard AR decoding frameworks. (4) (5) (6) Discussion. Existing work has explored modeling dual tracks using various approaches [Lei et al., 2024, Li et al., 2024], often requiring large modifications to the LM architecture or modeling the tracks sequentially. In contrast, our proposed method offers more effective solution with the following advantages: 1) Scalability: By preserving the existing LM architecture, we leverage well-established pretraining infrastructures and enable straightforward scalability. 4A Whisper V3 checkpoint fine-tuned on an internal song dataset with manual transcription. 5We use X-Codec as our tokenizer. See more discussion in Section 3.4 and 8.1. 7 Figure 4. The Stage-1 Framework of YuE. Dotted lines: Dual-NTP (Section 3.2.1). Text interleave: CoT (Section 3.2.2). Green tokens: ICL (Section 3.2.3). Multitask learning (Section 4.1). 2) Convergence: Empirically, Dual-NTP converges to lower training loss compared to standard NTP. Notably, it demonstrates robust lyric adherence even within challenging minority genres (e.g., metal music)6, illustrating its adaptability to heterogeneous data distributions. 3) Joint Modeling of Tracks: Our approach jointly contextualizes both tracks in single forward pass, avoiding track synchronization issues, and allowing coherent and natural musical planning. 4) Granular Modeling & Processing: The explicit segregation of vocal and accompaniment tokens enables independent modeling, allowing for the capture of finer nuances, particularly in instrumentally-dense segments. This also facilitates separate post-processing and mastering for each track. 3.2.2. Structural Progressive Conditioning Challenges of Full-song Generation. While typical TTS and TTM systems operate on less than 30 seconds of context [Liu et al., 2023, 2024b, Wang et al., 2023a, Borsos et al., 2023, Copet et al., 2023b], full-song modeling requires handling minutes-long contexts. Although some proprietary systems like Suno.ai and Udio achieved this, their methodologies remain undisclosed. We find that extending the LM context to full-song modeling is non-trivial. Simply scaling up the LM context length does not yield effective song-level lyrics-following capabilities and demands substantial computational resources. key challenge is the long-term decay property of commonly adopted Rotary Position Embedding (RoPE) [Su et al., 2024]. In autoregressive TTS and TTM systems, text conditioning applied at the start degrades as audio tokens extend further. Empirically, this degradation begins around 3K tokens and leads to complete failure beyond 6K tokens, even with 16K-token pre-trained contexts. Our mitigation attempts, such as increasing the RoPE base (10K to 100K) or curriculum learning with gradually increasing audio lengths, have been ineffective. See ablation in Section 8.3 for more details. 7 To address the long-term decay property issue, Structural Progressive Conditioning (CoT). we propose an elegant solution that leverages the inherent structural priors of music. Songs are typically composed of distinct segments, such as intro, verse, chorus, bridge, and outro [Nieto 6We encourage the readers to listen to our demo page https://map-yue.github.io/. 7We named it CoT to pay tribute to the concept of Chain-of-Thought prompting [Wei et al., 2022], as we adopt similar instructions and leverage intermediate conditioning tokens as guidance. However, our approach fundamentally differs from the original Chain-of-Thought prompting in implementation and application. We also acknowledge that there are CoT-like work proposed for audio language models [Du et al., 2024a, Ma et al., 2025, Wang et al., 2025]. 8 et al., 2020, Bruderer et al., 2009, Lerdahl and Jackendoff, 1996]. We use all-in-one [Kim and Nam, 2023] to automatically segment songs into musical sections, with most of the sections shorter than 30 seconds. song is by average segmented into 14 sessions. Within each structure section, text form segment labels, lyrics and audio are paired together. From full song perspective, structured text and audio tokens are interleaved (see lyrics2song token arrangement in Figure 4). Special tokens are incorporated to indicate the start and end of the audio. We describe single training example as concatenation of several components. In our setup, song is constructed as Dcot = Instruct Tag Lyrics (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:123)(cid:122) Prompt (cid:16) (cid:17) ùëÅ ùëñ=1 ùë†ùëñ < >. Specifically, denotes sequence concatenation. Instruct is the instruction, task prefix as follows: Generate music from the given lyrics segment by segment. Tag denotes the musical tags, which is the style control string. An example of Tag is as follows: [Genre] jazz male deep vocal romantic big band. Lyrics represents the raw lyric text provided before any segmented annotations. < > is an end-of-document token. Besides, each segment ùë†ùëñ is structured as follow: ùë†ùëñ = [ R _ _ M ] ùúèùëñ ‚Ñìùëñ < > ùúìùëñ < > [ _ _ M ]. ùúèùëñ {[intro], [verse], [chorus], [bridge], [outro]} is structure label, ‚Ñìùëñ representing the segments lyric content8, and ùúìùëñ denoting sequence of Dual-NTP audio tokens9. In summary, each document in CoT begins with an instruction, metadata, and raw lyrics, followed by series of annotated segments, and ends with the < > token. 3.2.3. Music In-Context Learning Deficiencies of Speech ICL. Previous work in TTS [Wang et al., 2023a, Du et al., 2024b] often defines speech ICL via continuation-based approach. The sequence is constructed as: ùëá ref (cid:124)(cid:123)(cid:122)(cid:125) reference text ùëáinput (cid:124)(cid:123)(cid:122)(cid:125) input text ùê¥ ref (cid:124)(cid:123)(cid:122)(cid:125) reference audio ùê¥gen (cid:124)(cid:123)(cid:122)(cid:125) generated audio While this framework can be suitable for speech-based tasks, there are three major issues when directly applying it to music: 1) Necessity of reference text. Requiring text transcript for the reference audio can be redundant in musical context, and lyrics may be unavailable or challenging to obtain. 2) Unidirectional assumption. Continuation is unidirectional and restricts the task generalization in scenarios requiring bidirectional creativity, e.g., writing an entire piece from short chorus snippet. 3) Entanglement. Continuation imposes strong constraints on the style and content of the generated audio. Given music often features structural repetition, the model may simply replicate the reference melody or even entire segments, raising copyright concerns. Moreover, this tight coupling between reference and generated segments diminishes the effectiveness of control prompts or tags designed to steer the creative process. 8Interestingly, replacing lyrics string with can enable instrumental music generation. 9We prepend tokenizer type special token, e.g. <xcodec>, at the beginning of audio token sequence. 9 Figure 5. The Stage-2 Framework of YuE. Re-designing ICL for Music. The aforementioned issues necessitate novel approach to ICL for music. We propose revised formulation of music ICL in two modes: single-track and dual-track. In single-track mode, the reference audio can be an accompaniment, vocal, or full mixture track. In dual-track mode, we incorporate both the separated vocal and accompaniment tracks in token-level interleaved manner, akin to Dual-NTP. Extending the ICL format from CoT data, we randomly sample 2040s segment from the reference track(s) and prepend its token sequence to the CoT data: Dicl = ùê¥ ref Dcot. We find that this form of ICL can be effectively activated with minimal computational overhead (2% of the total pre-training cost). However, ICL constitutes strong conditioning signal and can be considered as easy data. Our preliminary experiments reveal that incorporating ICL data too early encourages shortcut learning [Geirhos et al., 2020], where the model tends to directly copy the reference audio rather than composing novel music. This strong content entanglement even disrupts lyrical control. Once shortcut learning occurs, the models creative capabilities cannot be easily restored. Removing ICL data and continuing training on CoT alone fails to resolve the issuewithout reference audio, the model struggles to generate meaningful outputs, exhibiting poor musicality. To address this, we introduce delayed activation strategy. We introduce small amount of ICL data (10B tokens) only during the annealing phase, ensuring no ICL data is used beforehand. This strategy facilitates disentangled control between text and reference audio. For instance, using Japanese city pop track with female vocal as reference, the model can transform the lyrics into English while preserving the same vocalist and genre, or even generate male English rap version of the city pop track. 3.3. Stage-2: Residual Modeling As shown in Figure 5, after Stage-1 yields coarse semantic tokens (codebook-0), Stage-2 refines the audio with additional codebooks 1, 2, . . . , 7. Denote the total number of codebooks by ùêæ = 8 (indexed from 0 to 7). Although codebook-0 is already produced by Stage-1, we train Stage-2 to predict all codebooks {0, 1, . . . , 7} jointly in single autoregressive framework. This design ensures that the model has unified view of both the high-level structure (codebook-0) and the residual details (codebooks 17). 1:ùëá = (ùë• (0) Architecture Overview. Let In Stage-2, we introduce additional codebooks, collectively denoted by , . . . , ùë• (0) (0) ùëá ) be the Stage-1 codebook-0 tokens for ùëá frames. (1:7) 1:ùëá = (cid:0)ùë• (1) 1 , . . . , ùë• (7) ; . . . ; ùë• (1) ùëá , . . . , ùë• (7) ùëá (cid:1). For training, we treat the output space as (0:7) 1:ùëá , i.e., each timestep ùë° has tuple (cid:1). , . . . , ùë• (7) , ùë• (1) ùë° ùë° (0:7) ùë° = (cid:0)ùë• (0) ùë° 10 Although codebook-0 tokens are the same as those from Stage-1, they are included in the training target so the model learns to predict them as well, thus capturing complete frame-level dependencies across all codebooks. Aligned Autoregressive Factorization. We maintain strictly time-aligned factorization: (cid:17) (cid:16) ùëù (0:7) 1:ùëá = ùëá (cid:214) ùë°=1 (cid:16) ùëù (0:7) ùë° (0:7) <ùë° (cid:17) . (cid:12) (cid:12) (cid:12) (7) This ensures that at each frame ùë°, the model conditions on all previously generated tokens across all codebooks, while still maintaining frame alignment with codebook-0. Cross-Conditioning. During training, we organize the sequence as: (cid:2) ùë• (0) , . . . , ùë• (0) ùëá 1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) all codebook-0 first , ùë• (1) 1 , ùë• (0) , ùë• (1) , . . . , ùë• (7) 2 1 1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) , ùë• (0) 2 , . . . , ùë• (7) 2 (cid:123)(cid:122) blocks of 0-7 per frame , . . . , ùë• (0) , . . . , ùë• (7) ùëá ùëá (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) , ùë• (1) ùëá (cid:3). That is, the first segment is only the codebook-0 tokens, followed by repeated 8-token blocks {0, 1, . . . , 7} for each frame. We apply standard teacher forcing on this extended sequence and minimize LStage2 = log ùëù (cid:16) (0:7) ùë° (0:7) <ùë° (cid:17) . (cid:12) (cid:12) (cid:12) ùëá ùë°= By placing all codebook-0 tokens at the beginning, the model is guaranteed to see the entire semantic structure before it encounters any mixed (07) blocks. This allows the model to plan the later residuals by attending to complete semantic outline from Stage-1. (0) Inference. At test time, codebook-0 tokens 1:ùëá come from Stage-1 and are treated as fixed (i.e., clamped). Even though the model is trained to predict codebook-0 as part of the joint sequence, during inference we replace any predicted codebook-0 tokens with the Stage-1 output. Consequently, the only free outputs in the autoregressive generation are the residual codebooks (1:7) 1:ùëá . This ensures the sequence alignment. Implementation. Our model is 1B-parameter Transformer with an 8K-token context window, trained on consecutive 6-second single-track segments. It employs shared acoustic codebook space to model various audio types, including speech, vocals, instrumentals, and mixtures. 3.4. Tokenization and Audio Reconstruction Following the design space of Borsos et al. [2023], Wang et al. [2023b], the stage-1 LM models text tokens and semantic-rich codebook-0 tokens. After investigation, we realized that the vanilla text-to-speech (TTS) / text-to-music (TTM) method performs poorly on our task, where musicality and song-level lyrics-following capability are the two key challenges. Table 1. Special tokens and their descriptions. Token Description <EOD> <SOA> <EOA> <stage_1> <stage_2> <encodec32k> <xcodec> <semanticodec> <hificodec> End of document Start of audio End of audio Start of Stage 1 Start of Stage 2 Tokenizer type (Encodec 32k) Tokenizer type (XCodec) Tokenizer type (SemantiCodec) Tokenizer type (HiFiCodec) Text Tokenizer. In this work, the vocabulary of the LMs contains two sections: text and audio. For the text part, we reuse LLaMA tokenizer with size of 32000 unique BPE tokens. Instructions, genres, lyrics, structure annotations, and structure segment boundary signals are represented with text format and tokenized with BPE. Semantic-Acoustic Fused Codec. For the audio vocabulary, we experimented with several open-source music and universal neural codecs. Detailed ablations are provided in Section 5. Ultimately, we adopted semantic-acoustic fused strategy [D√©fossez et al., 2024, Zhang et al., 2023, Liu et al., 2024a, Ye et al., 2024]. Specifically, we utilized X-Codec [Ye et al., 2024] as our off-the-shelf audio tokenizer. We employed general-purpose version of X-Codec, trained on mixture of 200k hours of 16 kHz audio with ratio of music : speech : audio effects = 1 : 1 : 0.05. The X-Codec tokenizer fuses 100M-parameter HuBERT-based universal semantic representation into the codec latent space. It has 50Hz frame rate, consists of 12 RVQ layers, each with codebook size of 1024. For this study, we used only the first 8 layers, as including more layers did not yield noticeable quality improvements. Notably, codebook-0 alone captures rich semantic information such as melody and vocal content, which are critical for our task. Vocabulary Expansion and Special Tokens. We expand the SentencePiece tokenizer vocabulary to support multiple audio tokenizers and special tokens. Specifically, we include Encodec-32khz-music [D√©fossez et al., 2022, Copet et al., 2023b], HiFi-Codec-universal [Yang et al., 2023a,b], X-Codec-general [Ye et al., 2024], and Semanticodec-100tps [Liu et al., 2024a]. For special tokens, we introduce the following: <EOD> represents the end of document, <SOA> denotes the start of audio, and <EOA> signifies the end of audio. Additionally, stage indicators, <stage_1> and <stage_2>, mark the beginning of Stage 1 and Stage 2 tokens, respectively. Tokenizer type indicators specify the corresponding tokenizer types, which are inserted between <SOA> and the actual audio token IDs. Note that stage indicators are only used in residual modeling and positioned between <SOA> and the tokenizer type indicator. Light-weight Upsampling Module. To achieve better perceptual audio quality, we upsample the reconstructed 16kHz audio to 44.1kHz. For this, we utilize light-weight upsampling vocoder adapting Vocos [Siuzdak, 2023] to predict the higher-frequency components. To enhance the robustness of the upsampler, we apply codebook dropout randomly and introduce small amount of Gaussian noise during training. 4. Training and Inference Strategies 4.1. Scaling Up Stage-1 Pre-Training 4.1.1. Multitask Learning Essential Capabilities Table 2. Decomposition of Lyrics-to-Song Generation Capabilities 1) Modeling of Human Vocal 2) Modeling of Instrumental 3) Joint Modeling of Vocal and Instrumental 4) Aligning Cross-Modal/Same-Modal Controls (lyrics, style, structure, in-context learning) lyrics-to-song data are inherently Conditional scarce, as most available music data exist in an unconditional format. Our preliminary experiments show that large models tend to overfit to dominant learning signal10, making it difficult for them to adhere to control signals when pre-training is predominantly driven by unconditional data. To address this, we propose multitask pre-training approach that facilitates knowledge transfer from auxiliary tasks to enhance lyrics-to-song generation. We decompose the essential capabilities required for this task into the four key components listed in Table 2. These components serve as guiding principles for our multitask setup, which includes: Text-to-Speech. Establishing alignment between linguistic control and human vocalization necessitates the use of speech data paired with text transcripts. This task is essential for enabling lyric-following capabilities, as discussed in capabilities 4) and 1). Omitting this task results in ineffective lyric control when training on in-the-wild data. TTS data primarily consists of short-form speech, typically under 20 seconds, which is significantly shorter than music tracks. To mitigate this sequence length mismatch, text-speech pairs are sequentially concatenated to form full-context sequences. Additionally, the task instruction Generate speech: is prepended to transcripts with dropout rate of 50% to enhance robustness. 10See more discussion in Section 12. 12 While this task is beneficial, the proportion of TTS data used is crucial. Excessive TTS training biases the generated token space towards speech, effectively modeling rap music but degrading performance on other genres require singing11. Conversely, insufficient TTS training leads to poor adherence to lyrics. Striking an optimal balance is essential for achieving effective lyric control across diverse musical styles. Music Generation. The majority of our dataset consists of unconditional music. We annotate all tracks using Qwen2-Audio[Chu et al., 2024] to obtain open-vocabulary tags. Tags are in the style of MTG-Jamendo[Bogdanov et al., 2019], categorized by genre, instrument, and mood. The input to Qwen2-Audio is 30-second clip sampled from each track. Prompt is shown in appendix B. Furthermore, 40% of the tracks are separated into vocal-instrumental dual-track format using UVR12. We employ ensemble predictions13 from three models: htdemucs_ft, Kim_Vocal_1, and UVR-MDX-NET-Inst_3. The processed tracks are tokenized and arranged into either tag-conditioned NTP or DualNTP formats. Text instructions are prepended before the audio sequences to distinguish the two prediction objectives: Generate music based on the given tags. or Generate music in dual-track format based on the given tags. The tag condition consists of [genre] string followed by shuffled tags separated by spaces, inserted between the instruction and the audio sequence. While this is relatively challenging task, training on it improves musicality, facilitates the development of capabilities 1), 2), and 3), while enabling style control within capability 4). Lyrics-to-Song. Obtaining high-quality paired lyrics-audio data is challenging, as sources from web searches and platform-provided transcripts often contain noise, irrelevant text, misaligned timestamps, and version discrepancies. To address this, we implement heuristic filtering to remove irrelevant content and exclude overly short lyrics (less than 10 sentences), retaining only approximately 10% of matched tracks. Despite filtering, some inconsistencies remain. The CoT design addresses these issues by leveraging segment-level rather than sentence-level lyrics-audio alignment, thus reducing reliance on precise matches. Additionally, incorporating TTS auxiliary task further enhances model robustness against imperfect alignment. Manual quality inspection on over one hundred segments confirmed an approximate 80% match rate, defined by the audible presence of the majority of text in the audio. For ICL, we support single-track and dual-track modes. Reference token sequences (20s40s) are randomly selected from the mixed, vocal, accompaniment tracks, or combinations thereof, and are prepended directly to corresponding CoT samples. We introduce vocal tags during ICL, prompt shown in appendix B. 4.1.2. Multiphase Training Phase-1: Warm Up. In the first phase, we warm up the model with linear learning rate schedule from ùëôùëü = 0 to ùëôùëü = 3 104 over 280B tokens. Only English and Chinese data are used, as manual verification showed that these two languages dominate the dataset and exhibit relatively high quality. To save computational costs, we use context length of 8192 (approximately 163s for mix music data and 81s for dual-track data) and global batch size of 768 (around 6.29M tokens). This phase rapidly establishes basic musical generation capabilities. Phase-2: Constant Learning Rate. In this phase, we maintain constant learning rate of 3 104 and introduce additional in-the-wild, lower-quality datasets, including multilingual data. The total processed tokens reach 1T. When incorporating new data, we maintain 2:1 ratio of old to new data to prevent excessive distribution shifts. Phase-3: Context Extension. We retain the learning rate at 3 104 and extend the context length. Since music inherently involves long sequences, we simply increase the maximum 11Overfitting TTS data turns the model into rap machine. 12https://github.com/Anjok07/ultimatevocalremovergui 13We use the minimal signal of each track. 13 positional embedding and sequence length to 16384 without modifying the data composition. We remove the single-track unconditional data during this phase. This phase continues training for an additional 750B tokens, further enhancing the models ability to handle long-context dependencies across multiple languages. Phase-4: Annealing with Control Injection. This is the final phase of the Stage-1 LM training. The learning rate follows cosine schedule, gradually annealing to 3 105. At this stage, we completely remove speech and unconditional music data while introducing stronger control signals. The control signals include reference audio (ICL), gender tags, vocal timbre tags, and BPM control. However, BPM control was later removed due to its coupling with lyrics length, which degrades lyrics following. To improve training data quality, we constructed quality signals and selected approximately 20K hours of high-quality data. The quality signals include playback count, likes, comments, and dataset source quality ratings (based on manual inspection pass rates). We performed annealing experiments across multiple languages, including English, Chinese, Japanese, and Korean. During annealing, we applied CoT to ICL ratio of 2:1 to prevent excessive reliance on reference songs. Remarkably, with only 40B tokens (2% of the total compute budget), the model successfully enabled all control signals introduced in this stage. 4.2. Stage-2 Pre-training. We train Stage-2 LM with context length of 8192. This phase incorporates all speech, demixed music, and mixed music datasets. The compute budget is set to 2T tokens. The learning rate follows linear warm-up and cosine annealing schedule with maximum learning rate of 3 104. We find in preliminary experiments that scaling the Stage-2 LM from 0.5B to 1B parameters and increasing the dataset size leads to improvements in audio quality; therefore, we adopt 1B-parameter model for this stage. 4.3. Test-time Strategies Forced Decoding. In stage-1 LM decoding, only vocabulary tokens within the audio range are permitted until the <EOA> token is predicted. Subsequently, the prompt for the next segment is forcibly provided based on user input. In stage-2 LM, the codebook-0 tokens, predicted by the previous stage, are enforced at each frame. When decoding the corresponding residual token, only the vocabulary of the respective codebook is allowed. Sampling and Classifier-Free Guidance. The sampling parameters are set as follows: topùëò = 50, repetition penalty = 1.1, top-ùëù = 0.93, temperature = 1, and maximum new tokens = 3000. Classifier-free guidance is applied with scale of ùë† = 1.5 for the first segment and ùë† = 1.2 for subsequent segments 14 to improve the good-case rate. Given the conditional logprobability ‚Ñìùëê (ùëò) = log ùëùùúÉ(ùëò ùë•) for token ùëò given prompt ùë• and the unconditional log-probability ‚Ñìùë¢(ùëò) = log ùëùùúÉ(ùëò ), the CFG-adjusted log-score is computed as: cfg(ùëò) = ùë†(cid:2)‚Ñìùëê (ùëò) ‚Ñìùë¢(ùëò)(cid:3) + ‚Ñìùë¢(ùëò). ‚Ñì Music In-Context Learning. Using songs chorus section for in-context learning significantly enhances musicality and stability. Moreover, we find that dual-track ICL enables better audio quality than single-track ICL mode. Consequently, dual-track ICL mode is enabled by default unless specified otherwise. 5. Experiments 5.1. Data & Training Setup Data Setup. For conditional speech data (TTS), we leverage three widely used English and Chinese TTS datasetsWeNetSpeech (zh), LibriHeavy (en), and GigaSpeech (en)comprising total of 70k hours of data. For unconditional music data (music generation), we mine 650K hours 14A lower guidance scale in later segments promotes diversity. 14 of in-the-wild music recordings from the Internet. 10% of the music data has corresponding lyrics after filtering. After tokenization, Stage-1 comprises 13B conditional speech tokens, over 200B unconditional music tokens (both mixed and demixed), and 28B CoT music tokens. During annealing, high-quality subset of 10B CoT tokens is sampled and expanded fourfold, creating 40B ICL dataset. This dataset includes variants such as vocal-ICL, accompaniment-ICL, mix-ICL, and dual-ICL. Prior to annealing, the data mixture is set at Conditional : Unconditional = 3 : 1 and Music : Speech = 10 : 1. During annealing, only CoT and ICL data are used, maintaining ratio of CoT : ICL = 2 : 1. Training Setup. Our codebase is built upon Megatron-LM [Shoeybi et al., 2019], following the LLaMA2 architecture [Touvron et al., 2023a, Zhang et al., 2024]. Most of our Stage-1 experiments use 0.5B-scale model trained on 16 NVIDIA H800 GPUs, with typical token budget of 100B tokens. Under this budget, models usually produce valid outputs, show preliminary lyric-following capabilities, and exhibit basic musical discernment. For scaling experiments, we increase the token budget to 500B and scale models to 0.5B, 2B, and 7B parameters, trained respectively on 32, 96, and 512 NVIDIA H800 GPUs. We maintain global batch size of 768 when possible by adjusting micro-batch size, gradient accumulation steps, and tensor parallelism; when computational resources are constrained, we reduce the global batch size to 512 or 256. For optimization, we use the Adam optimizer with gradient clipping set to 1.0, weight decay 0.1, ùõΩ1 = 0.9, ùõΩ2 = 0.95, ùúñ = 108, and parameter initialization with standard deviation 0.02. Detailed training procedures are described in Section 4.1.2. 5.2. Evaluation Protocol Baselines. As of the writing of this paper, apart from YuE, no academic or open-source system provides usable long-form song generation capabilities, and known prior works exhibit limited performance [Dhariwal et al., 2020b]. Therefore, we selected five popular closed-source systems for benchmarking: Suno V415, Udio16, Hailuo17, and Tiangong18. It is important to note that due to the black-box nature of these closed-source models, our evaluation conducted in January 2025 reflects the comparative performance between YuE and these systems at that specific point in time. All systems support lyric-based inputs; however, their support for style control inputs varies significantly. Specifically, Tiangong does not support textual style prompts, so we used our own reference audio as style control. Hailuo provides 18 predefined style tags, thus we selected the tag closest to our desired style prompt and used the systems default built-in reference audio, as uploading custom references is not supported. Human Evaluation. We conducted human evaluation involving 40 researchers, including 12 experts in Speech/Music AI19 and 7 trained musicians. None of the evaluators participated in model training, ensuring objectivity. Following prior studies [Donahue et al., 2023, Qu et al., 2024, Yuan et al., 2024a], we adopted an A/B test format. In the main experiments in Section 6, each model generated 42 full-length songs based on diverse set of English prompts specifying genre, instruments, emotion, lyrics, and tempo. These prompts utilized real lyrics that were rewritten by GPT and paired with corresponding 30s chorus segments as reference audio. Similarly, for the multilingual experiment, we used 10 Chinese prompts and 10 Japanese/Korean prompts. Some multilingual prompts contained sentences with more than one language, e.g., EN-JA-KR mixes. For evaluation involving nonEnglish multilingual samples, we invited native speakers or language-major students proficient in the respective languages to conduct assessments. 15https://suno.com 16https://www.udio.com 17https://hailuoai.com/music 18https://www.tiangong.cn/music 19Worked on text-to-speech, text-to-music, singing voice synthesis. 15 Figure 6. Human evaluation comparing YuE to 4 proprietary systems. YuE matches two of it (Tiangong, Udio) and outperforms one (Hailuo). Left: Average human preference on all aspects (warmer colors / larger numbers indicate higher preference); Right: win-tie-loss on musicality. Evaluators blindly compared pairs of music pieces produced by two different systems according to several criteria: Overall Musicality, Vocal Quality (VocalQual), Accompaniment Quality (AccompQual), Music Arrangement (MusicArr), Melodic Attractiveness (MelodicAttrac), VocalAccompaniment Compatibility (VocalAccompComp), Song Structure Clarity (SongStruct), Lyrics Following Accuracy (LyricFollow)20, Genre Controllability (GenCtrl), Instrument and Vocal Configuration Controllability (InstrCtrl), Emotional Expressiveness (EmoCtrl), and Tempo and Rhythm Control (Tempo/RhyCtrl). For ablation studies in Section 8, unless otherwise specified, we utilize set of 15 GPT-generated English prompts (see Appendix D). Each study undergoes small-scale A/B testing, with inference performed twice per prompt, resulting in total of 30 samples per setting. Automatic Evaluation. We also report automatic evaluation metrics, including KullbackLeibler (KL) divergence for measuring distributional differences in generated audio features using audioldm_eval21, Frechet Audio Distance (FAD) [Kilgour et al., 2019] for assessing audio quality and realism (also via audioldm_eval), Audiobox-Aesthetic [Tjandra et al., 2025] for capturing perceived musical aesthetics (Production Quality (PQ), Production Complexity (PC), Content Enjoyment (CE), and Content Usefulness (CU)) using neural audio embedding model, CLAP score22 and CLaMP 3 score [Wu et al., 2025]23 to measure semantic alignment between text prompts and audio outputs, vocal agility quantifying song-level vocal range and flexibility (pitch estimated with RMVPE24, applying 40ms note filtering and human verification), and generation duration as practical measure of song-level audio modeling capability. 6. Main Results 6.1. Human Evaluation We report the generation result on English in section 6.1. 20We observe that Whisper transcription accuracy is insufficiently robust for reliable automated lyrics-following evaluation. Therefore, lyrics alignment with input prompts is manually evaluated by human raters in the main experiments. 21https://github.com/haoheliu/audioldm_eval 22https://github.com/Stability-AI/stable-audio-metrics 23https://github.com/sanderwood/clamp3 24https://github.com/yxlllc/RMVPE Figure 7. Normalized human preference on different music aspects. Left: scores across 6 musical aspects; Right: performance on 5 types of control. 6.1.1. Overall Comparison with Proprietary Systems. In human evaluation Figure 6, our model, YuE, demonstrates competitive performance relative to four proprietary systems in both average human preference25 and musicality. Specifically, YuE outperforms Hailuo by clear margin, achieves comparable results to Tiangong and Udio, but still trails behind Suno V4, which remains the state-of-the-art system. In detailed musicality comparisons, YuE shows balanced winloss ratios against Tiangong and Udio, decisively outperforms Hailuo, but underperforms compared to Suno V4. These results indicate that while proprietary products still lead in the best quality, YuE represents promising step toward high-quality open-source music generation. 6.1.2. Detailed Comparison with Proprietary Systems. Aspects of Musicality and Acoustic Quality. To evaluate the subjective musical qualities of YuE and comparative models, we conducted detailed A/B test on six dimensions: vocal (acoustic) quality, accompaniment (acoustic) quality, music arrangement, melodic attractiveness, vocalbacktrack matching, and song structure. We visualize the win rate with radar plot in Figure 7(L). Suno V4 consistently outperforms all other models across these aspects, thus we normalized the win rate by Suno to improve visual clarity. Among other models, YuE excels notably in music structure and music arrangement, highlighting its capability for coherent long-form composition capability. However, YuE shows clear deficiencies in vocal and accompaniment acoustic quality, likely due to limitations of its current audio tokenization method. While YuE achieves decent musicality and convergence, the semantic-fused tokenizer requires improvements in acoustic detail via, enhanced decoder or super-resolution backend. Controllability. Similarly, we evaluated the controllability of YuE and comparative models through A/B testing on five dimensions: genre control, instrument/vocal control, emotion control, tempo/rhythm control, and lyrics following. Given limitations of existing classifiers and transcription systems, user preference win rate was our primary evaluation metric, with results presented in Figure 7(R). Suno v4 consistently outperforms all models across controllability metrics. Among other models, YuE performs strongest in genre adherence, instrument/vocal consistency, and emotion, highlighting its effectiveness in generating stylistically coherent music aligned with textual prompts. YuE demonstrates moderate performance in emotion and tempo control, indicating the need for improved lyric alignment and tempo tagging systems due to considerable noise observed in the pseudo label on the training corpus provided by Qwen2Audio. 25Obtained by averaging win rate over all aspects. 17 Overall, these results affirm YuEs robust controllability capabilities. 6.2. Automatic Evaluation 6.2.1. Vocal Agility Figure 8. Song-level vocal range on different systems. Higher values indicates better vocal agility, e.g. range=12 means the vocal only span through an octave in given song. YuEs vocal range is among the top close-source systems. As shown in Figure 8, the distribution of song-level vocal ranges across different systems reveals notable variations in vocal agility. Higher values indicate greater vocal expressiveness. Among the models, YuE demonstrates one of the widest vocal ranges (medium = 27 semitones), closely matching top-performing closed-source systems like Suno V4. This suggests that YuE is capable of generating diverse and dynamic vocal performances. In contrast, models like Hailuo and Tiangong show more constrained vocal range (medium number around 20 semitones), indicating potential limitations in expressiveness. These findings highlight YuEs strength in producing vocally rich and varied song compositions. 6.2.2. Duration Figure 9. Duration range on different systems. YuE generates the longest audio. The distribution of generated song durations across different systems reveals substantial variation in length constraints as demonstrated by Figure 9. YuE produces the longest audio, with significantly wider duration range compared to all other models, demonstrating its ability 18 Table 3. Comparison of various music generation models across multiple metrics. Metric Distrib. Match Content Based Alignment KL FAD CE CU PC PQ CLAP CLaMP 3 Hailuo SunoV4 Tiangong Udio YuE 0.756 0.620 0.708 0.503 0. 2.080 1.544 2.547 1.222 1.624 7.350 7.474 7.421 7.112 7.115 7.737 7.813 7.766 7.520 7.543 6.793 6.601 6.060 6.626 6.280 8.132 8.120 8.220 7.803 7.894 0.265 0.265 0.244 0.310 0. 0.106 0.160 0.114 0.156 0.240 to generate full-length songs beyond typical AI-generated clips. SunoV4 and Tiangong also generate relatively long audio. In contrast, Hailuo Music show the most restricted durations, suggesting limitations in modeling long-term musical structure. These results highlight YuEs advantage in handling extended temporal dependencies, making it more suitable for full-song generation. 6.2.3. Model Based Evaluation Table 3 illustrates model based automatic evaluation results, including distribution metrics KL and FAD, aesthetics metrics proposed by meta, and audio-text alignment score such as CLAP score [Wu et al., 2023a] and CLaMP 3 score [Wu et al., 2025]. Distribution Matching Metrics. We report KL and FAD to evaluate how well generated audio matches the target distribution. YuE achieves the best performance on KL divergence (0.372), significantly outperforming others such as Udio (0.503) and SunoV4 (0.620). While Udio attains the lowest FAD (1.222), YuE remains competitive (1.624), demonstrating effective audio quality and distribution matching capabilities. Although distribution-based metrics can suffer from sample size biases, they remain valuable for comparative purposes, particularly when evaluating against closed-source systems where large-scale sampling is impractical. We refrain from adopting the traditional MusicCaps-based evaluation scheme since MusicCaps contains large amount of purely instrumental content, rendering it unsuitable as reference set for song generation tasks. Content Based Metrics. Scores above 7 across audiobox-aesthetic dimensions indicate strong overall performance. Specifically, YuE achieves competitive scoresPQ (7.894), PC (6.280), CE (7.115), and CU (7.543)which closely match state-of-the-art closed-source systems such as SunoV4 (CE 7.474, CU 7.813) and Tiangong (PQ 8.220). These results suggest YuE performs comparably in terms of perceived audio aesthetics and usability. Alignment Metrics. YuE attains the highest alignment score according to CLaMP 3 (0.240) [Wu et al., 2025], closely aligning with the human-evaluated control√Øndicators from the previous section. However, we observe notably lower alignment for YuE according to the CLAP score (0.118), which not only diverges from human evaluation trends but also directly contradicts the findings from CLaMP 3. These discrepancies highlight potential limitations of the CLAP score in accurately capturing human perceptions of controllability, possibly due to differences in pretraining data and modeling strategies. In contrast, CLaMP 3 appears to benefit from recent methodological improvements and broader, web-scale training resources, resulting in more reliable evaluation outcomes.26 26We use CLaMP3 as the CLaMP 3 score backend, which is more recent model compared to CLAP, showing improved results in representation quality and music retrieval tasks due to extensive web-scale pretraining. In contrast, CLAP may suffer from limited exposure to singing/musical content during its training, which could lead to discrepancies in evaluating certain music types. 19 6.2.4. Correlation Between Automatic Metrics and Human Evaluation Table 4. Pearson correlation between subjective metrics (Musicality, Average) and automatic metrics. Vocal Range strongly impacts Musicality and Average ratings. KL FAD CE CU PC PQ CLAP CLaMP 3 VocalRange Musicality -0.232 -0.199 Average -0.249 -0.351 0.368 0.357 0.320 0.303 -0.268 -0.128 0.112 0.054 -0.072 0. 0.333 0.264 0.857 0.858 Correlation with Musicality & Average Preference When considering musicality and average human preference  (Table 4)  , the Vocal Range metric stands out, correlating most strongly (above 0.85) with both subjective ratings. This highlights the central role of vocal expressiveness in shaping listeners holistic impressions of generated music. It suggests that the song-level vocal range (or vocal agility) could serve as relatively reliable proxy for musicality within the lyrics-to-song task, offering practical measure to capture and quantify vocal expressiveness. Table 5. Pearson correlation of alignment metrics vs. human preference on controllability. LyricFollow GenCtrl InstrCtrl EmoCtrl Tempo/RhyCtrl CLAP CLaMP 3 -0.25 0.42 0.01 0.37 -0.07 0.44 0.14 0.33 0.09 0. Alignment Metrics. The correlation results in Table 5 demonstrate that CLaMP 3 scores consistently correlate better with human evaluations of controllability compared to CLAP scores. This is particularly evident in tasks such as LyricFollow (0.42 vs. -0.25) and InstrCtrl (0.44 vs. -0.07). Interestingly, the genre-following capability measured by the CLaMP 3 backend [Wu et al., 2025] appears to be closely related to lyric-following performance, even though lyrics are not explicitly included in the computation of the CLaMP 3 score. This indicates correlation between genre controllability and lyric adherence in music generation models. Conversely, the weaker correlations observed with CLAP suggest limitations in its capacity to capture nuanced perceptual aspects, likely due to insufficient exposure to singing and music-specific content during pre-training. Table 6. Pearson correlation of KL and FAD on acoustic quality preference metrics. AccompQual VocalQual KL FAD 0.14 -0.15 0.23 -0. Distribution Matching Metrics. We employed the more advanced PaSST [Koutini et al., 2021] backbone instead of the conventional VGGish [Hershey et al., 2017] to evaluate distribution matching metrics. Despite its sophistication, the AudioSet pre-trained backbone may inherently suffer from out-of-distribution (OOD) issues when dealing with generative music, particularly with singing or vocal elements. Additionally, sample size bias may contribute significantly, as limited availability of extensive audio samples from closed-source generative systems hinders accurate distribution estimations. As shown in Table 6, both KL and FAD exhibit weak correlations with accompaniment (acoustic) quality (AccompQual) and vocal (acoustic) quality (VocalQual), suggesting that distribution-level metrics may not fully capture subtle subjective perceptions of acoustic fidelity in our case. However, as indicated in Table 4, these same metrics correlate more strongly with musicality and overall human preference27. This implies that while distribution matching may not always 27Both KL and FAD are negatively correlated, since lower values indicate better alignment. 20 reflect finer acoustic details, they sometimes reflect qualities relevant to perceived musicality and listener satisfaction. Table 7. Pearson correlation of content-based metrics vs. related preference metrics. AccompQual VocalQual SongStruct VAComp MelAttrac MusicArr CE CU PC PQ 0.56 0.50 -0.09 0.27 0.66 0.61 0.00 0.36 0.33 0.27 -0.24 0.05 0.35 0.29 -0.20 0. 0.30 0.25 0.00 -0.03 0.31 0.26 -0.16 0.02 In Table 7, CE exhibits the strongest correlations, particularly with Content-Based Metrics. subjective acoustic quality measures such as VocalQual (0.66) and AccompQual (0.56). This indicates that CE might be especially sensitive to acoustic characteristics perceived by listeners. By contrast, correlations with musicality-related aspectssuch as SongStruct (0.33), VAComp (0.35), MelAttrac (0.30), and MusicArr (0.31)are relatively lower, suggesting lesser sensitivity of CE to detailed musical attributes. Meanwhile, both PC and PQ show notably weaker or inconsistent correlations across these subjective metrics, implying limitations in their ability to capture musicality related perceptual elements. 7. Fine-tuning To More Languages Our results (detailed in Appendix C) demonstrate YuEs strong adaptability and effectiveness through fine-tuning to multiple languages (Chinese, Korean, Japanese) within 40B-token budget28. As shown in Table 8, YuE notably achieves the highest lyrics-following performance in Japanese (70%). In Chinese lyrics-following, YuE secures second-best performance (60%) behind Suno (73%), while in Korean lyrics-following, it ranks third (55%). These results highlight YuEs robust adaptability and suggest potential for further improvement with targeted fine-tuning. YuE also demonstrates competitive musicality, placing second in Chinese (62%) and Korean (55%), which indicates effective cross-lingual transfer of musical features. However, its gap relative to Suno in Chinese musicality highlights the need for more culturally-specific training. Overall, these findings underscore YuEs promising multilingual capability and the importance of addressing linguistic and cultural nuances in fine-tuning approaches. Table 8. Human preference rate for lyrics following and musicality across languages. Bold indicates the best-performing system, and boxed indicates the second-best. Model Chinese Korean Japanese Lyrics Music Lyrics Music Lyrics Music YuE Udio Suno V4 Hailuo Tiangong 60 36 73 30 62 46 88 15 39 55 62 75 37 20 55 62 50 60 22 70 31 60 56 32 52 51 80 31 35 8. Analysis and Ablations 8.1. Comparison of Audio Tokenizers In preliminary experiments on 130k-hour subset of diverse music data, we conducted qualitative analysis of four popular audio tokenizers, specifically focusing on acoustic tokens 28Fine-tuning was conducted by re-annealing from the last constant learning rate checkpoint using an enhanced mixture of target language data. 21 Table 9. Qualitative comparison of different codec types based on reconstruction quality, LM convergence, and invalid probability. Invalid probability refers to the likelihood of generating noise or silence segments during LM token synthesis."
        },
        {
            "title": "Reconstruction LM Converge",
            "content": "Invalid Prob. Acoustic Acoustic Semantic + Acoustic Semantic + Acoustic Encodec32k HiFiCodec Semanticodec X-Codec"
        },
        {
            "title": "All\nAll\nHigh\nLow",
            "content": "and fused semantic-acoustic tokens (see Table 9). Separate semantic and acoustic tokenizers would require retraining and thus were beyond the scope of this study, reserved for future work. Acoustic tokenizers, including Encodec32k and HiFiCodec, exhibited decent reconstruction quality. However, their learned tokens proved challenging for LMs to converge due to the complexity and variability inherent in our in-the-wild dataset. Training 0.5B LM with acoustic tokens consistently failed to converge, resulting primarily in invalid outputs characterized by noise or silence. Although prior studies indicated Encodec32k has been successfully applied to TTM [Copet et al., 2023b], even scaling the LM to 7B and extending training up to 1 trillion tokens on our data yielded only intermittent success, with outputs still dominated by noise. In contrast, tokenizers integrating semantic and acoustic features (Semanticodec, X-Codec) demonstrated significantly better convergence, largely due to the stable clustering provided by SSL encoders. This stability facilitated successful LM training at the 0.5B scale. However, the stable clustering slightly compromised acoustic dynamics, causing only fair reconstruction quality. We further identified critical alignment flaw in Semanticodec related to AudioMAEs patch-based mechanism, where misalignment of one token propagated errors throughout reconstruction. X-Codec, using Hubert-derived semantics, avoided this issue and maintained lower invalid generation probability. 8.2. Impact of Source Separation Prior and Dual-NTP We define metric called the Vocal-to-Accompaniment Ratio (VAR), to quantify the effect of track-wise energy distribution on linguistic information loss. Let ùë£(ùëõ) denote the vocal signal and ùëé(ùëõ) denote the accompaniment signal, over ùëõ = 1, 2, . . . , ùëÅ. We compute VAR (in dB) as follows: VAR = 10 log10 . (8) (cid:32) (cid:205)ùëÅ ùëõ=1 (cid:205)ùëÅ ùëõ=1 (cid:33) (cid:0)ùë£(ùëõ)(cid:1) 2 (cid:0)ùëé(ùëõ)(cid:1) where higher VAR values indicate greater prominence of vocals relative to accompaniment, while lower VAR suggests accompaniment dominance. Similar to Figure 3, Figure 10 illustrates the WER-VAR relationship for mixture and vocal tracks across 1K samples, including tokenizer reconstructions. Although original vocal and mixture tracks exhibit similar absolute WER (solid blue and orange lines), mixture track reconstruction significantly increases WER (solid vs. dotted blue lines), especially as VAR declines, widening the gap (ŒîWER). 20%+ ŒîWER is observed around -8.0 dB VAR. In contrast, vocal tracks maintain low WER and smaller ŒîWER (the worst case is 10%- around -8.0dB VAR), indicating resilience of source separation priors to VAR degradation and reconstruction information loss. Additionally, we perform an ablation study comparing Dual-NTP and standard NTP. Figure 11 presents training loss curves of two 0.5B LMs trained with identical data and computational budgets (20B tokens). Dual-NTP demonstrates substantial reduction in loss (approximately 0.4 lower) compared to standard NTP, confirming its efficiency and robustness. Together, these analyses underscore the effectiveness of incorporating source separation priors with Dual-NTP into song modeling task. 22 Figure 10. Comparison of WER-VAR plot for mixture and vocal tracks, including their tokenizer reconstructions, over 1K samples. Figure 11. Training Loss over Consumed Train Tokens for NTP and Dual-NTP. 8.3. Ablation Analysis of Lyrics-following Capabilities with CoT The analysis in Figure 12 examines an ablation setting involving 0.5B LM, which was initially pretrained on default mixture dataset29 comprising 500B tokens and subsequently finetuned on the corresponding lyrics data for an additional 200B tokens using the specified methods: Vanilla, Curriculum, and ABF, and our proposed CoT. Additionally, we include results from the YuE-7B checkpoint to illustrate the performance gains achievable through scaling. Vanilla refers to text prepend conditioning, where the model is trained with prepended lyrics as input for conditioning. Curriculum involves gradually increasing the text prepend data with progressively longer durations (e.g., 30s, 60s, 90s, etc.), aiming to improve the models ability to follow lyrics over time. ABF [Xiong et al., 2023] refers to adjusting the rope base frequency from 10k to 100k during finetuning to explore its effect on lyrics-following performance. Figure 12. WER over time. Both CoT and model scaling significantly enhance lyrics-following capability. The WER over time is estimated using fine-tuned Whisper model, with measurements recorded every 30 seconds up to 150 seconds. Overall, the proposed CoT method achieves consistently superior performance across all evaluated time intervals (30s to 150s). Scaling the model to 7B parameters demonstrates substantial improvements, reducing the WER from approximately 70% at 0.5B parameters to around 20%30. In contrast, Vanilla, Curriculum, and ABF methods exhibit substantially worse WER, indicating limited capability in maintaining lyrical coherence. Through manual inspection, we identified that the primary reason for failure in Vanilla and Curriculum was their tendency to generate instrumental preludes, causing the onset of singing to drift far from the original prepended 29A mixture of speech and music. Text transcripts are in prepend format. 30Note that 20% can be considered relatively low number. Refer to the GT WER-to-VAR plot in Figure 10. 23 lyrics condition, thus complicating accurate alignment. 8.4. Effect of Scaling We investigate the impact of model scaling on musicality and lyrics-following capabilities. We compared checkpoints at 0.5B, 2B, and 7B scales. While the 0.5B and 2B models were trained with limited budget of 500B tokens (in 16K context), the 7B model underwent complete scaling with significantly larger 1.75T token budget using the full training dataset. As illustrated in Figure 13, human evaluation demonstrates clear improvement trend in both musicality and lyrics-following as model scale and training budget increase. Notably, the 7B model exhibits substantial enhancements, indicating that increased parameter counts and extensive training significantly boost the models foundational creativity and compositional quality. These results confirm that scaling plays crucial role in achieving higher musicality and improved lyric adherence. 8.5. Analysis of Test-time Tricks Figure 13. Human preference overall win rates for Musicality and Lyrics-following across model scales (0.5B, 2B, and 7B) in pairwise A/B tests. Larger models consistently achieve higher preferences. Figure 14 presents human preference win rates for musicality obtained through A/B testing across different inference settings using YuE-7B checkpoints. Results clearly demonstrate that ICL-based methods outperform CoT-based methods significantly: ICL achieves win rate of 0.63 compared to only 0.21 for CoT. Incorporating CFG further enhances these methods; specifically, ICL+CFG obtains the highest win rate (0.79), substantially exceeding both ICL alone and the CoT-based configurations. This performance advantage stems from the strong conditioning ability of ICL, which restricts the decoded token space to musically favorable subspace guided by the provided humangenerated music prompt. CFG similarly strengthens this conditioning by amplifying the influence of the text condition on next-token logits, making generated outputs more closely aligned with the intended prompt-guided subspace and thus further improving musicality. Figure 14. Human preference win rates for Musicality across different test-time tricks. 9. Representation Quality YuE, fundamentally designed as generative model rather than explicitly for representation learning, is evaluated with MARBLE [Yuan et al., 2024b] here using its Stage-1 LM in an unconditional single-track setting. Notably, this mode serves primarily as an auxiliary task and 24 Table 10. Evaluation of YuE single-track unconditional mode on MARBLE. Including GTZAN genre classification, GS key recognition, MTG top 50 classes tagging, and EMO emotion regression. Dataset Task Metrics GTZAN GS MTG Key Genre Top50 AccRefined AP AUC R2V R2A EMO Emotion Acc MERT [2023] MusicFM [2024] MuQiter [2025] CLAP [2023b] CLaMP 3 [2025] YuE 78.6 83.8 85.6 82.1 86.6 83.4 65.6 63.9 65.0 16.0 53.8 67.0 29.9 - - 27.7 30.2 29. 83.4 - - 82.0 82.4 82.7 61.2 60.3 62.8 54.1 59.1 58.9 74.7 76.3 76.1 70.3 70.0 75.0 is disabled half way through the training. Moreover, it exclusively leverages discrete codes from codebook-0, implying significant reduction in available information compared to dedicated representation learning models. Despite these inherent limitations, YuE achieves state-of-the-art performance on the GS key recognition task (Acc=67.0%, see Table 10), demonstrating good sense of tonality and modality, which is essential for composing and singing in tune. Furthermore, its performance remains competitive with existing methods across other tasks, such as GTZAN genre classification, MTG tagging, and EMO emotion regression, underscoring YuEs robust general-purpose representation quality and learned musical skills. 10. Emergent Abilities We strongly encourage readers to visit our demo page for audio examples illustrating the capabilities described.31 Scaling up the model significantly enhances generation quality and unlocks novel abilities. Advanced Vocal Techniques. Beyond basic pop and rap vocals, our model spontaneously acquires diverse and expressive singing techniques, typically mastered only by gifted human vocalists through extensive training. These include vibrato, glissando, bel canto, death growl, mix voice, belting, riffs and runs, vocal fry, Beijing Opera, and Shanbei folk vocals. This indicates our Dual-NTP approach effectively captures subtle nuances in vocal performance. Spontaneous Performance. Our model spontaneously demonstrates musically expressive behaviors. For instance, in jazz performances, it naturally continues with scat singing after running out of lyrics; in cappella, it simultaneously generates multi-part harmonies with distinct vocalists handling melody and accompaniment; in folk music, it inserts contextually appropriate instrumental solos, such as harmonica interludes, during vocal pauses. World Music & Pattern Mixing. Our model effectively captures long-tail global music styles beyond mainstream western genres. For instance, it generates creative fusions such as Chinese gangsta rap accompanied by Japanese shamisen instrumentation and scales. It can also seamlessly blend distinct regional vocal styles, combining Chinese opera, Shanbei folk singing, and traditional Chinese vocals within single cohesive performance. Voice Cloning. Our model demonstrates high-fidelity voice cloning capabilities at inference time, successfully replicating distinct vocal identities. For example, we accurately reproduce 31https://map-yue.github.io/ the unique voices of Billie Eilish and Faye Wong (ÁéãËè≤) while generating entirely new lyrics and melodies. These cloned voices retain their signature timbral qualities, breathy textures, and emotional nuances, highlighting the models ability to capture and reproduce subtle vocal characteristics from limited reference data provided only at inference. Style Transfer. Our model shows versatile style transfer capabilities, enabling the generation of diverse and expressive vocal performances across different languages, genres, and timbres. YuE enables cross-lingual and genre adaptation while preserving the original lyrical and melodic structure. In one example, Japanese female J-pop vocal performance is transformed into an English male rap with the same city pop accompaniment. The model not only shifts the vocal characteristics but also adjusts prosody, phrasing, and expressiveness to ensure stylistic coherence, demonstrating its deep understanding of genre-specific vocal performance. Code Switching. The model naturally handles code-switching, smoothly transitioning between multiple languages or dialects within the same vocal performance, while preserving linguistic and stylistic consistency. 11. Memorization Effect Following previous literatures [Agostinelli et al., 2023, Yuan et al., 2024a], we investigate whether YuE, in its ICL modeconditioned on 30-second audio prompt and original lyricsreproduces significant portions of its training data. ICL is generally more prone to memorization, making this evaluation critical. We employ ByteCover2 [Du et al., 2022], state-of-the-art retrieval model optimized for melodysensitive similarity across entire songs.32 Specifically, we create two sets of ùëÅ = 1200 music samples: (Ref), comprising YuEs training examples, and (Gen), comprising corresponding samples generated by YuE in the ICL setting. We compute cosine similarity scores for each pair (ùëü, ùëî) with ùëü and ùëî G, analyzing the top 1% of scores since frequent high-similarity pairs would suggest substantial memorization. To contextualize these results, we compare them to real-world baselines from GTZAN (genrelevel similarities) and Covers80 (known melodic duplicates). Results are shown in Figure 15. The similarity distribution for Ref-Gen pairs is significantly lower than Covers80 and remains moderate even compared to GTZAN. While short repetitive motifs, particularly percussive loops, occasionally occur, overall results indicate that YuEs ICL mode does not engage in extensive copying. Instead, YuE recombines learned musical patterns creatively, demonstrating that the ICL mode effectively generates original content rather than memorizing entire training samples. 12. Unsuccessful Attempts During our initial scaling experiments, we encountered several challenges and setbacks. Here, we share these unsuccessful experiences to inform and inspire future research directions. Acoustic Tokens. As detailed in Section 8.1, LMs trained on acoustic tokens consistently exhibited convergence difficulties and yielded higher losses compared to semantic-enhanced tokens. We attribute these challenges primarily to inherent limitations of current acoustic token representations, typically derived from RVQ-GANs. Such tokens often prioritize compression efficiency over representational quality and typically have limited capacity. Consequently, models trained on these tokens may tend to adopt shortcuts, frequently resorting to direct information copying. Even when scaled substantially, these models achieve only marginal improvements [Hansen-Estruch et al., 2025, Xin et al., 2024, Parker et al., 2024]. We argue the lossy nature of discrete representations, limited semantic relevance [Zhang et al., 2023], and 32We do not use ByteCover3 [Du et al., 2023] as it specializes in shorter segments. 26 Figure 15. Box-plot comparison of cosine similarity across three scenarios: Covers80, Ref-Gen (our training vs. generated sets), and GTZAN. The black bar denotes the median, and the diamond denotes the mean. excessive focus on reconstruction tasks collectively contribute to the difficulties observed in fitting acoustic tokens. Unconditional Pre-train. We initially pre-trained large models to learn general representations for cross-modal alignment (text-to-vocal) via fine-tuning. At smaller scales (e.g., sub-billion parameters), models showed moderate success in learning basic mappings. However, at 7B parameters, unconditional pre-training became counterproductive: fine-tuning failed to establish effective cross-modal alignment. We hypothesize that larger models internalize overly generic priors, overshadowing the specific conditional mappings needed for alignment. This catastrophic inertia prevents large models from adapting effectively to lyrics-to-song tasks. Early Activation of ICL. We observed that early activation of ICL data led to poor musicality. Initially, the model began to excessively rely on the reference audio, resulting in overfitting and diminished musicality. After removing the reference audio later in the training process, the model continued to produce significant number of invalid outputs, such as silence or noise. This problem became more pronounced with scaling, where larger models struggled even more to recover from this shortcut learning. These results highlight the importance of carefully managing the timing of ICL data activation to avoid overfitting and preserve the models creativity. 13. Conclusion and Future Work We introduced YuE, an open-source foundation model family designed for long-form lyricsto-song generation. By combining large-scale data, track-decoupled next-token prediction, segment-wise conditioning strategy, and redesigned in-context learning framework, YuE can generate coherent, full-length songs with expressive vocals and detailed musical structure. Experimental results show that YuE matches or exceeds several commercial systems in musicality, controllability, and cross-lingual lyrics following, and it also achieves competitive music understanding results on standard benchmarks. These findings highlight the promise of open, large-scale music models in enabling controllable, high-quality song generation and in advancing broader research into music-aware AI systems. YuEs approach can be extended by improving acoustic fidelity and mixing, incorporating musical knowledge such as chord progressions and instrumentation theory, and integrating deeper prosodic and emotional controls. Multilingual and cross-cultural expansions hold significant potential, especially for underrepresented musical traditions. Beyond music creation, 27 YuE can benefit applications in music education, accessibility, and therapy, and can serve as an accessible platform for continued community-driven innovation in open music AI research. 14. Ethics and Responsibility Ensuring ethical and responsible AI-generated music is crucial for fostering transparency, accessibility, and fair contribution to the music industry. As suggested by Ma et al. [2024], to promote accountability, we advocate for the inclusion of AI-generated / AI-assisted tags in generated content, increasing transparency for both musicians and audiences. Additionally, our memorization-effect experiments in Section 11 demonstrate that our design maintains creativity without plagiarizing, even under strong training set conditioning. In contrast to closed-source commercial systems, our model leverages an exceptionally diverse training dataset, explicitly enriched with culturally diverse music content. This enables the model to innovate and create within niche musical styles effectively (see Section 10). As such, our model can serve as parameterized knowledge base, contributing to the preservation and expansion of human musical artistry and cultural heritage. 15. Contributions and Acknowledgments Core Contributors Ruibin Yuan, Lead, Pre-train, Data, Eval HKUST, Moonshot.ai, MAP, ryuanab@connect.ust.hk Hanfeng Lin, Pre-train, Data, Eval, Inference HKUST, MAP, hanfeng@ust.hk Shuyue Guo, Pre-train, Demo MAP Ge Zhang, Pre-train MAP, gezhang@umich.edu Jiahao Pan, Pre-train, Eval, Data HKUST, MAP, fengshicherish@gmail.com Contributors Yongyi Zang, Upsampler, Eval Independent Haohe Liu, Upsampler, Tokenizer, Demo University Of Surrey, MAP Yiming Liang, Eval Lead MAP Wenye Ma, Representation Learning MBZUAI, MAP Xingjian Du, Memorization Effect University of Rochester, MAP Xinrun Du, Pre-train MAP Zhen Ye, Tokenizer HKUST Tianyu Zheng, Pre-train MAP Yinghao Ma, Eval MAP, Queen Mary University of London Minghao Liu, Eval, Data 2077AI, MAP Zeyue Tian, Eval HKUST, MAP Ziya Zhou, Eval, Data HKUST, MAP Liumeng Xue, Eval, Data HKUST, MAP Xingwei Qu, Pre-train, Eval MAP Yizhi Li, Eval MAP, University of Manchester Shangda Wu, Eval Central Conservatory of Music, MAP Tianhao Shen, Eval, Inference MAP Ziyang Ma, Eval MAP, SJTU, NTU Jun Zhan, Eval Fudan University Chunhui Wang, Eval, Pre-train Geely Yatian Wang, Eval HKUST Xiaowei Chi, Eval HKUST Xinyue Zhang, Eval HKUST Zhenzhu Yang, Eval HKUST Xiangzhou Wang, Eval MAP Shansong Liu, Eval Meituan Lingrui Mei, Eval Meituan Peng Li, Eval HKUST 28 Junjie Wang, Eval Tsinghua University Jianwei Yu, Data, Inference Moonshot.ai Guojian Pang, Inference MAP Xu Li, Eval Xiaohongshu Zihao Wang, Data Zhejiang University, Carnegie Mellon University Wenhu Chen University of Waterloo, MAP Xinyu Zhou Moonshot.ai Xipeng Qiu Fudan University Roger Dannenberg Carnegie Mellon University, MAP Academic Advisors Xiaohuan Zhou MAP Lijun Yu Carnegie Mellon University Emmanouil Benetos Queen Mary University of London, MAP Yong Chen Geely Chenghua Lin University of Manchester, MAP Xie Chen Shanghai Jiao Tong University Gus Xia MBZUAI, MAP Zhaoxiang Zhang Chinese Academy of Sciences Chao Zhang Tsinghua University"
        },
        {
            "title": "References",
            "content": "Correspondence (Alphabetical Order) Jiaheng Liu Nanjing University, MAP, 13121221227@163.com Jian Yang MAP, jiaya@buaa.edu.cn Wenhao Huang MAP, rubio8741@gmail.com Wei Xue HKUST, weixue@ust.hk Xu Tan Moonshot.ai, MAP, tanxu2012@gmail.com Yike Guo HKUST, yikeguo@ust.hk A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, arXiv A. Roberts, M. Tagliasacchi, et al. MusicLM: Generating music from text. preprint:2301.11325, 2023. A. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449 12460, 2020. A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli. Data2Vec: general framework for self-supervised learning in speech, vision and language. In International Conference on Machine Learning, pages 12981312, 2022. Y. Bai, H. Chen, J. Chen, Z. Chen, Y. Deng, X. Dong, L. Hantrakul, W. Hao, Q. Huang, Z. Huang, et al. Seed-music: unified framework for high quality and controlled music generation. arXiv preprint arXiv:2409.09214, 2024. D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and X. Serra. The mtg-jamendo dataset for automatic music tagging. In Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019), Long Beach, CA, United States, 2019. URL http://hdl.handle.net/10230/42015. Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour. Audiolm: language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31: 25232533, 2023. doi: 10.1109/TASLP.2023.3288409. 29 M. J. Bruderer, M. F. McKinney, and A. Kohlrausch. The perception of structural boundaries in melody lines of western popular music. Musicae Scientiae, 13(2):273313, 2009. J. Chen, X. Tan, J. Luan, T. Qin, and T.-Y. Liu. Hifisinger: Towards high-fidelity neural singing voice synthesis. arXiv preprint:2009.01776, 2020. K. Chen, Y. Wu, H. Liu, M. Nezhurina, T. Berg-Kirkpatrick, and S. Dubnov. Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies. arXiv preprint arXiv:2308.01546, 2023. K. Chen, Y. Wu, H. Liu, M. Nezhurina, T. Berg-Kirkpatrick, and S. Dubnov. MusicLDM: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies. In International Conference on Acoustics, Speech and Signal Processing, pages 12061210. IEEE, 2024. Y. Chu, J. Xu, Q. Yang, H. Wei, X. Wei, Z. Guo, Y. Leng, Y. Lv, J. He, J. Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. W2V-Bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In IEEE Automatic Speech Recognition and Understanding Workshop, pages 244250. IEEE, 2021. J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D√©fossez. Simple and controllable music generation. arXiv preprint:2306.05284, 2023a. J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D√©fossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284, 2023b. A. D√©fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. A. D√©fossez, L. Mazar√©, M. Orsini, A. Royer, P. P√©rez, H. J√©gou, E. Grave, and N. Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020a. P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020b. C. Donahue, A. Caillon, A. Roberts, E. Manilow, P. Esling, A. Agostinelli, M. Verzetti, I. Simon, O. Pietquin, N. Zeghidour, et al. Singsong: Generating musical accompaniments from singing. arXiv preprint arXiv:2301.12662, 2023. X. Du, K. Chen, Z. Wang, B. Zhu, and Z. Ma. Bytecover2: Towards dimensionality reduction of latent embedding for efficient cover song identification. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 616620. IEEE, 2022. X. Du, Z. Wang, X. Liang, H. Liang, B. Zhu, and Z. Ma. Bytecover3: Accurate cover song identification on short queries. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. Y. Du, Z. Ma, Y. Yang, K. Deng, X. Chen, B. Yang, Y. Xiang, M. Liu, and B. Qin. Cot-st: Enhancing llm-based speech translation with multimodal chain-of-thought. arXiv preprint arXiv:2409.19510, 2024a. Z. Du, Q. Chen, S. Zhang, K. Hu, H. Lu, Y. Yang, H. Hu, S. Zheng, Y. Gu, Z. Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024b. Z. Evans, J. D. Parker, C. Carr, Z. Zukowski, J. Taylor, and J. Pons. Stable audio open. arXiv preprint:2407.14358, 2024. R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. P. Hansen-Estruch, D. Yan, C.-Y. Chung, O. Zohar, J. Wang, T. Hou, T. Xu, S. Vishwanath, P. Vajda, and X. Chen. Learnings from scaling visual tokenizers for reconstruction and generation. arXiv preprint arXiv:2501.09755, 2025. S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, et al. Cnn architectures for large-scale audio classification. In 2017 ieee international conference on acoustics, speech and signal processing (icassp), pages 131135. IEEE, 2017. Z. Hong, C. Cui, R. Huang, L. Zhang, J. Liu, J. He, and Z. Zhao. UniSinger: Unified end-to-end singing voice synthesis with cross-modality information matching. In ACM International Conference on Multimedia, pages 75697579, 2023. C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck. Music transformer. arXiv preprint arXiv:1809.04281, 2018. Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank, arXiv et al. Noise2Music: Text-conditioned music generation with diffusion models. preprint:2302.03917, 2023. K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi. Fr√©chet audio distance: reference-free metric for evaluating music enhancement algorithms. In Proc. Interspeech, 2019. T. Kim and J. Nam. All-in-one metrical and functional structure analysis with neighborhood attentions on demixed audio. In 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages 15. IEEE, 2023. K. Koutini, J. Schl√ºter, H. Eghbal-Zadeh, and G. Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36, 2024. S. Lei, Y. Zhou, B. Tang, M. W. Lam, F. Liu, H. Liu, J. Wu, S. Kang, Z. Wu, and H. Meng. Songcreator: Lyrics-based universal song generation. arXiv preprint arXiv:2409.06029, 2024. S. Lei, Y. Zhou, B. Tang, M. W. Lam, H. Liu, J. Wu, S. Kang, Z. Wu, H. Meng, et al. Songcreator: Lyrics-based universal song generation. Advances in Neural Information Processing Systems, 37: 8010780140, 2025. F. Lerdahl and R. S. Jackendoff. Generative Theory of Tonal Music, reissue, with new preface. MIT press, 1996. R. Li, Z. Hong, Y. Wang, L. Zhang, R. Huang, S. Zheng, and Z. Zhao. Accompanied singing voice synthesis with fully text-controlled melody. arXiv preprint arXiv:2407.02049, 2024. Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Lin, A. Ragni, E. Benetos, N. Gyenge, et al. MERT: Acoustic music understanding model with large-scale self-supervised training. arXiv preprint:2306.00107, 2023. H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. Proceedings of the International Conference on Machine Learning, 2023. H. Liu, X. Xu, Y. Yuan, M. Wu, W. Wang, and M. D. Plumbley. SemantiCodec: An ultra low bitrate semantic audio codec for general sound. IEEE Journal of Selected Topics in Signal Processing, 18 (8):14481461, 2024a. doi: 10.1109/JSTSP.2024.3506286. H. Liu, Y. Yuan, X. Liu, X. Mei, Q. Kong, Q. Tian, Y. Wang, W. Wang, Y. Wang, and M. D. Plumbley. AudioLDM 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024b. doi: 10.1109/TASLP.2024.3399607. J. Liu, C. Li, Y. Ren, F. Chen, and Z. Zhao. Diffsinger: Singing voice synthesis via shallow diffusion mechanism. In Proceedings of the AAAI conference on artificial intelligence, 2022. Z. Liu, S. Ding, Z. Zhang, X. Dong, P. Zhang, Y. Zang, Y. Cao, D. Lin, and J. Wang. Songgen: arXiv preprint single stage auto-regressive transformer for text-to-song generation. arXiv:2502.13128, 2025. Y. Ma, A. √òland, A. Ragni, B. M. Del Sette, C. Saitis, C. Donahue, C. Lin, C. Plachouras, E. Benetos, E. Shatri, et al. Foundation models for music: survey. arXiv preprint arXiv:2408.14340, 2024. Z. Ma, Z. Zheng, C. Tang, Y. Wang, and X. Chen. MT4SSL: Boosting self-supervised speech representation learning by integrating multiple targets. In Proceedings of Interspeech, 2023. Z. Ma, Z. Chen, Y. Wang, E. S. Chng, and X. Chen. Audio-CoT: Exploring chain-of-thought reasoning in large audio language model. arXiv preprint arXiv:2501.07246, 2025. S. A. Mehr, M. Singh, D. Knox, D. M. Ketter, D. Pickens-Jones, S. Atwood, C. Lucas, N. Jacoby, A. A. Egner, E. J. Hopkins, et al. Universality and diversity in human song. Science, 366(6468): eaax0868, 2019. O. Nieto, G. J. Mysore, C.-i. Wang, J. B. Smith, J. Schl√ºter, T. Grill, and B. McFee. Audio-based music structure analysis: Current trends, open challenges, and applications. Transactions of the International Society for Music Information Retrieval, 3(1), 2020. J. D. Parker, A. Smirnov, J. Pons, C. Carr, Z. Zukowski, Z. Evans, and X. Liu. Scaling transformers for low-bitrate high-quality speech coding. arXiv preprint arXiv:2411.19842, 2024. C. Payne. Musenet. https://openai.com/research/musenet, 2022. X. Qu, Y. Bai, Y. Ma, Z. Zhou, K. M. Lo, J. Liu, R. Yuan, L. Min, X. Liu, T. Zhang, et al. Mupt: generative symbolic music pretrained transformer. arXiv preprint arXiv:2404.06393, 2024. S. Schneider, A. Baevski, R. Collobert, and M. Auli. Wav2Vec: Unsupervised pre-training for speech recognition. INTERSPEECH, pages 34653469, 2019. M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. H. Siuzdak. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. arXiv preprint arXiv:2306.00814, 2023. J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568, 2024. T. L. Team. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3/. A. Tjandra, Y.-C. Wu, B. Guo, J. Hoffman, B. Ellis, A. Vyas, B. Shi, S. Chen, M. Le, N. Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 32 H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei. Neural codec language models are zero-shot text to speech synthesizers. arXiv, abs/2301.02111, 2023a. C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint:2301.02111, 2023b. X. Wang, M. Jiang, Z. Ma, Z. Zhang, S. Liu, L. Li, Z. Liang, Q. Zheng, R. Wang, X. Feng, et al. Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens. arXiv preprint arXiv:2503.01710, 2025. Y. Wang, R. Hu, R. Huang, Z. Hong, R. Li, W. Liu, F. You, T. Jin, and Z. Zhao. Prompt-Singer: Controllable singing-voice-synthesis with natural language prompt. arXiv preprint:2403.11780, 2024. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. M. Won, Y.-N. Hung, and D. Le. foundation model for music informatics. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12261230, 2024. doi: 10.1109/ICASSP48485.2024.10448314. S. Wu, Z. Guo, R. Yuan, J. Jiang, S. Doh, G. Xia, J. Nam, X. Li, F. Yu, and M. Sun. Clamp 3: Universal music information retrieval across unaligned modalities and unseen languages, 2025. URL https://arxiv.org/abs/2502.10362. Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023a. Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2023b. Y. Wu, J. Shi, Y. Tang, S. Yang, Q. Jin, et al. TokSing: Singing voice synthesis based on discrete tokens. arXiv preprint:2406.08416, 2024. D. Xin, X. Tan, S. Takamichi, and H. Saruwatari. Bigcodec: Pushing the limits of low-bitrate neural speech codec, 2024. URL https://arxiv.org/abs/2409.05377. W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y. Zou. Hifi-codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint arXiv:2305.02765, 2023a. D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu, et al. Uniaudio: An audio foundation model toward universal audio generation. arXiv preprint arXiv:2310.00704, 2023b. Z. Ye, P. Sun, J. Lei, H. Lin, X. Tan, Z. Dai, Q. Kong, J. Chen, J. Pan, Q. Liu, et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. arXiv preprint arXiv:2408.17175, 2024. 33 R. Yuan, H. Lin, Y. Wang, Z. Tian, S. Wu, T. Shen, G. Zhang, Y. Wu, C. Liu, Z. Zhou, et al. Chatmusician: Understanding and generating music intrinsically with llm. arXiv preprint arXiv:2402.16153, 2024a. R. Yuan, Y. Ma, Y. Li, G. Zhang, X. Chen, H. Yin, Y. Liu, J. Huang, Z. Tian, B. Deng, et al. Marble: Music audio representation benchmark for universal evaluation. Advances in Neural Information Processing Systems, 36, 2024b. G. Zhang, S. Qu, J. Liu, C. Zhang, C. Lin, C. L. Yu, D. Pan, E. Cheng, J. Liu, Q. Lin, et al. Mapneo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024. X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023. Y. Zhang, J. Cong, H. Xue, L. Xie, P. Zhu, and M. Bi. ViSinger: Variational inference with adversarial learning for end-to-end singing voice synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 72377241. IEEE, 2022. H. Zhu, Y. Zhou, H. Chen, J. Yu, Z. Ma, R. Gu, W. Tan, and X. Chen. Muq: Self-supervised music representation learning with mel residual vector quantization. arXiv preprint arXiv:2501.01108, 2025. 34 A. Subjective Evaluation A.1. Evaluation Methods In this subjective evaluation experiment, annotators were required to perform pairwise comparative evaluations of music generation outputs from multiple models. Each test unit comprised two distinct musical pieces generated by different models. Following complete playback of both samples, annotators conducted binary comparative selections (options: Superiority of A, Superiority of B, or Equivalence between and B) across predefined evaluation dimensions. Mandatory preference judgments were enforced for each dimensional criterion, with explicit instructions to minimize the frequency of selecting the equivalence option. The evaluation protocol incorporated double-blind procedure with randomized presentation order of audio pairs to mitigate potential ordering effects. Figure 16. Subjective evaluation platform. A.2. Evaluation Dimensions and Definitions 1) Overall Musicality Definition: The musical artistic value and professionalism demonstrated by the work as whole, reflecting whether it approaches the creative level of professional musicians or composers. Evaluation Criteria: Smoothness of the melody, complexity and rationality of the harmony, precision and rhythmic flow, and the artistic and creative qualities of the overall arrangement. 2) Vocal Quality Definition: The acoustic quality of vocal performance in the work. Evaluation Criteria: Pitch, rhythmic stability, naturalness of vocal timbre (resembling human singing), fullness and warmth of timbre, degree of mechanical or distorted sound, clarity of vocals, and richness in capturing delicate emotional expressions (e.g., variations in breath control, articulation precision, emotional conveyance). 3) Accompaniment Quality Definition: The acoustic quality of the instrumental accompaniment in the work. Evaluation Criteria: Realism and authenticity of instrumental timbres, dynamic variation and detail richness in instrumental expression (e.g., subtlety in guitar plucking or percussion dynamics). 4) Arrangement Complexity Definition: The layering, coherence, balance, and creativity of the musical arrangement in the work. Evaluation Criteria: Clarity of arrangement layers, coordination and interplay between instruments, balance of accompaniment within the overall audio track (e.g., appropriate vol35 ume and frequency distribution), fullness of low frequencies, brightness of high frequencies, diversity of arrangement elements (e.g., harmony, melodic lines, rhythm patterns across multiple dimensions), creativity, and variation and emotional progression between sections. 5) Melodic Memorability and Catchiness Definition: The memorability, accessibility, and resonance-inducing capability of the melody. Evaluation Criteria: Ease of memorization and singability, catchiness, emotional resonance, and repeated hooks or memorable elements, especially in the chorus. 6) Vocal-Accompaniment Matching Definition: The consistency and compatibility between vocal melodies and instrumental accompaniment in terms of musical style, modality, harmony, and rhythm. Evaluation Criteria: Compatibility of vocal melodies and accompaniment in modality, harmony, and rhythm, and absence of dissonance or conflict. 7) Song Structure Clarity Definition: The logical coherence and sectional distinctiveness of the overall song structure. Evaluation Criteria: Clarity of the songs structure (e.g., differentiation among verses, choruses, and interludes), naturalness of transitions between sections, and structural completeness. A.3. Conditional Evaluation Dimension and Definitions 8) Lyrics Following Definition: The accuracy of AI-generated vocals in performing the lyrics specified in the prompt. Evaluation Criteria: Accuracy of lyric delivery (whether the specified lyrics are correctly performed), clarity of pronunciation (whether the lyrics are intelligible), alignment of lyrics rhythm with the musical beat, and naturalness and correctness of multilingual lyric transitions and pronunciations. 9) Multilingual Lyrics Switching Naturalness and Correctness Definition: The fluency and accuracy of AI-generated vocals when performing lyrics in multiple languages, including the smoothness of transitions and the grammatical and pronunciation correctness of different languages. Evaluation Criteria: Fluency and naturalness of multilingual transitions: whether transitions between languages are smooth and seamless without abrupt changes or noticeable interruptions; accuracy of pronunciation for multilingual lyrics: whether the pronunciation in different languages is precise, clear, and adheres to the phonetic norms of each language, avoiding mispronunciations or accent deviations that could hinder understanding. 10) Genre Controllability Definition: The degree to which the generated music accurately reflects the musical genre specified in the prompt. Evaluation Criteria: Accuracy of musical genre characteristics (whether the generated music aligns with the features of the genre specified in the prompt, such as jazz, pop, classical, rock, etc.). 11) Instrument and Vocal Configuration Controllability Definition: The extent to which the generated music adheres to the instrument and vocal configuration specified in the prompt. Evaluation Criteria: Matching of instrument and vocal configuration (whether the generated music follows the specifications in the prompt, such as piano, guitar, male or female vocals, choir, etc.). 12) Emotional Expressiveness Definition: The accuracy and impact of emotional expression in the generated music, as specified in the prompt. Evaluation Criteria: Alignment of musical emotions with the emotional description in the prompt (e.g., passionate, sorrowful, cheerful). 13) Tempo and Rhythm Definition: The congruence of the musics tempo (BPM) and rhythm with the requirements 36 specified in the prompt. Evaluation Criteria: Consistency of generated music tempo (BPM) with the tempo specified in the prompt, and adherence to the required rhythmic patterns. B. Qwen2Audio-Instruct Tagging Prompt Music Tagging Prompt Analyze the provided audio and describe its features in valid JSON format with the following keys: Music_genre, Instrument, and Mood. If there are multiple entries for any key, represent them as list of strings. Example format: { \" Music_genre \": [\" Jazz \"] , \" Instrument \": [\" Saxophone \" , \" Piano \"] , \" Mood \": [\" Relaxed \"] } Vocal Tagging Prompt Analyze the provided audio and describe its vocal characteristics in valid JSON format with the following keys: gender, age, and vocal_timbre. If there are multiple entries for any key, represent them as list of strings. Example format: { \" gender \": [\" female \"] , \" age \": [\" adult \"] , \" vocal_timbre \": [\" bright \" , \" airy \"] } 37 C. Multilingual Subjective Evaluation (a) Chinese - Lyrics Following (b) Chinese - Musicality (c) Korean - Lyrics Following (d) Korean - Musicality (e) Japanese - Lyrics Following (f) Japanese - Musicality Figure 17. YuE vs. others across different languages on lyrics following and musicality. 38 D. 15 English Prompts From GPT ID: 1 [Genre] Rap [verse] Woke up in the morning, sun is shining bright Chasing all my dreams, gotta get my mind right City lights are fading, but my visions clear Got my team beside me, no room for fear Walking through the streets, beats inside my head Every step take, closer to the bread People passing by, they dont understand Building up my future with my own two hands [chorus] This is my life, and Im aiming for the top Never gonna quit, no, Im never gonna stop Through the highs and lows, Imma keep it real Living out my dreams with this mic and deal [verse] Late nights grinding, writing down these rhymes Clock is ticking fast, cant afford to waste time Haters gonna hate, but brush it off Turn the negativity into something strong Mama working hard, wanna make her proud Echoes of her prayers cutting through the crowd Friends turned strangers, but its all good Focused on my path like always knew would [chorus] This is my journey, and Im running this race Heart full of fire, you can see it in my face Obstacles ahead, but got no fear Victory is close, yeah, its almost here [bridge] They said couldnt do it, said Id never rise But now Im soaring high, reaching for the skies Lessons that learned made me who am Standing tall now, dont give damn [verse] Echoes in the alley, musics getting loud Feeling the adrenaline pumping through the crowd Spotlights on me, its my time to shine Living in the moment, everythings aligned Looking back now at the roads Ive crossed Every single battle, every line Ive tossed Made me stronger, wiser, ready for whats next Writing my own story, turning pages of the text [chorus] This is my song, and Im singing it proud Voices united, hear us shout out loud From the underground straight into the stars Carving out my name, leaving all these scars [outro] Yeah, this is for the dreamers, the ones who never quit Keep your head up high, and dont you ever submit Life is what you make it, so make it something great Step into your purpose, go and seize your fate 39 ID: 2 [Genre] Rock [verse] Standing on the corner, shadows in the night The citys heartbeat echoes under lights Hands deep in pockets, wandering alone Footsteps tracing paths to the unknown Suddenly he pauses, looks up to the sky Eyes reflect the questions passing by Whispers to the wind, words without sound Searching for the answers yet unfound [chorus] Lost within the chaos, seeking out sign In world of color, drawing blurred lines Moving forward, looking back, unsure of the way Trying to find place where he can stay [verse] He crosses empty streets, under neon glow Faces in the crowd, stories left untold Raises up his arms, reaching for the truth Grasping at the fragments of his youth Billboards and the banners flutter in the breeze The rhythm of the city brings him to his knees Heartbeat heavy, nowhere left to hide Feeling like hes lost amidst the tide [chorus] Lost within the chaos, seeking out sign In world of color, drawing blurred lines Moving forward, looking back, unsure of the way Trying to find place where he can stay [bridge] Doesnt want to leave, doesnt want to fight Caught between the darkness and the light No need for reason, nothing to prove Just soul in transit, with nothing to lose [outro] Doesnt want to leave, doesnt want to fight Chasing after shadows in the night He doesnt need the truth, doesnt need name Just looking for spark to fan the flame 40 ID: 3 [Genre] Pop [verse] Staring at the sunset, colors paint the sky Thoughts of you keep swirling, cant deny know let you down, made mistakes But Im here to mend the heart didnt break [chorus] Every road you take, Ill be one step behind Every dream you chase, Im reaching for the light You cant fight this feeling now wont back down Im the whisper in the wind, the shadow by your side The warmth you feel within when you cant hide You know you cant deny it now wont back down [verse] They might say Im foolish, chasing after you But they dont feel this love the way we do My heart beats only for you, cant you see? wont let you slip away from me [chorus] Every road you take, Ill be one step behind Every dream you chase, Im reaching for the light You cant fight this feeling now wont back down Im the whisper in the wind, the shadow by your side The warmth you feel within when you cant hide You know you cant deny it now wont back down [bridge] No, wont back down, wont turn around Until youre back where you belong Ill cross the oceans wide, stand by your side Together we are strong [outro] Every road you take, Ill be one step behind Every dream you chase, loves the tie that binds You cant fight this feeling now wont back down 41 ID: 4 [Genre] Jazz [verse] In the quiet of the evening, shadows start to fall Whispers of the night wind echo through the hall Lost within the silence, hear your gentle voice Guiding me back homeward, making my heart rejoice [chorus] Dont let this moment fade, hold me close tonight With you here beside me, everythings alright Cant imagine life alone, dont want to let you go Stay with me forever, let our love just flow [verse] Moonlight paints picture upon your lovely face Every glance between us fills the empty space Time stands still around us when youre in my arms Nothing else can matter, safe from any harm [chorus] Dont let this moment fade, hold me close tonight With you here beside me, everythings alright Cant imagine life alone, dont want to let you go Stay with me forever, let our love just flow [bridge] Every touch ignites fire, burning deep within Every smile you give to me makes my head spin Promise me youll stay awhile, dont ever say goodbye Together well chase every star across the sky [chorus] Dont let this moment fade, hold me close tonight With you here beside me, everythings alright Cant imagine life alone, dont want to let you go Stay with me forever, let our love just flow [outro] Stay with me forever, let our love just flow 42 ID: [Genre] Blues [verse] Late last night, the rain was pouring down Lonely footsteps echoed through the town Thinking bout the love that slipped away Wondering how let you go that day [chorus] Oh, my angel, where have you flown Left me here to face this world alone Im just fool, fool in love with you Cant deny this heartaches true [verse] Streetlights flicker, shadows on the wall Memories of you, recall Your laughter like song inside my head Without you here, my soul feels dead [chorus] Oh, my angel, wont you return In this fire of love, still burn Im just fool, fool in love with you Hoping someday youll feel it too [bridge] fell for you, and always knew That my world revolves around you hope and pray, both night and day That youll come back and choose to stay [chorus] Oh, my angel, where have you flown Left me here to face this world alone Im just fool, fool in love with you Waiting here, what else can do [outro] Im just fool, fool in love with you 43 ID: 6 [Genre] RnB_Soul [verse] Why dont we just find place to hide Leave all our worries and doubts behind When nothing in this world is as it seems Together we can live inside our dreams Theres no need to be afraid tonight In the love weve made, well find the light When were living in world of our own Its you and me, we never feel alone [chorus] They say its hard for man to let it show But with you, Im ready to let it all go Whatever we try, were gonna get there You take control, baby, dont care gotta keep on pushing when times get tough We keep on making better love [verse] Dont believe the things that others say Weve tried it all and found our way They should take look at you and me Learning how to love and set it free For every heartache, we take our time You teach me yours and Ill show you mine About the way that love is meant to be Together well rewrite our history [chorus] They say its hard for man to let it show But with you, Im ready to let it all go Whatever we try, were gonna get there You take control, baby, dont care gotta keep on pushing when times get tough We keep on making better love [bridge] Gotta take control and swallow my pride Every man has feelings deep inside You gotta find yourself before you can Be ready to love and understand Baby, know what youre thinking of We keep on making better love [outro] believe the love were makings gonna last forevermore Loving you feels so right, like never before Well be getting down tonight until the morning light We keep on making better love Better love, well be making Better love, no more faking They say its hard for man to let it show But with you here, Im ready to let go Whatever we try, were gonna get there You take control, baby, dont care gotta keep on pushing when times get tough We keep on making better love Better love (till fade out) ID: 7 [Genre] Ancient_Chinese_Style [verse] Beneath the moonlit sky so vast lone wanderer recalls the past Whispers of the bamboo leaves Echo tales the wind retrieves [chorus] Oh, the rivers flow, mountains high Journeying souls beneath the endless sky Threads of fate entwine our way Guiding us through night and day [verse] Lanterns glow with softest light Painting shadows in the night Silken robes and ancient songs Memories where hearts belong [chorus] Oh, the rivers flow, mountains high Journeying souls beneath the endless sky Threads of fate entwine our way Guiding us through night and day [bridge] Stars reflect in tranquil ponds Dreams unfold of times beyond Lotus blooms and cranes in flight Secrets held within the night [outro] As the sunrise paints the east Bringing hope and inner peace Footprints fade upon the shore But the spirit journeys evermore 45 ID: 8 [Genre] Folk [verse] Underneath the open sky so clear, We gather round with voices near. Through trials faced and stories told, Our spirits rise, our hearts unfold. [chorus] So lift the lanterns to the sky, Together we will soar and fly. Though shadows loom and doubts appear, Well keep the flame forever here. [verse] Remember all the paths weve crossed, The battles won, the moments lost. banner of hope we hold up high, symbol shining in our eyes. [chorus] So lift the lanterns to the sky, Together we will soar and fly. Though shadows loom and doubts appear, Well keep the flame forever here. [bridge] With hands united, we stand tall, Pledged to rise if we should fall. Through darkest nights and stormy seas, Our song will carry on the breeze. [chorus] So lift the lanterns to the sky, Together we will soar and fly. Though shadows loom and doubts appear, Well keep the flame forever here. [outro] Well keep the flame forever here. 46 ID: 9 [Genre] Dance [verse] Underneath the starlit sky, We come alive, you and I. City lights are shining bright, Dancing through the endless night. [chorus] Who are we? Lets break away, Feel the beat and let it play. Lost in music, hearts align, In this moment, we define. [verse] Shadows fade beneath the glow, Rhythms guide us where to go. Voices whisper in the crowd, Turn it up, well sing aloud. [chorus] Who are we? Lets break away, Feel the beat and let it play. Lost in music, hearts align, In this moment, we define. [bridge] Let the melody surround, Lift us off the solid ground. Every step and every move, In this dance we find our groove. [chorus] Who are we? Lets break away, Feel the beat and let it play. Lost in music, hearts align, In this moment, we define. [outro] Keep on dancing, feel the heat, Moving to the pounding beat. Who we are is here and now, Take my hand, well show them how. 47 ID: [Genre] Country [verse] Da-dum, da-da-da-da-da-da-da Da-dum, da-dum Walking down this lonesome road Thinking bout the love untold Why havent told you Ive whispered to the midnight stars Just how wonderful you are Why havent told you [chorus] Friends keep asking if Im fine just smile and say youre mine Might as well confess Cant keep this inside Maybe you feel the same way too Oh darling, if you do Why havent you told me Da-dum, da-da-da-da-da-da-da [verse] Ive sung it to the morning sun That with you, my lifes begun Why havent told you My hearts an open book today Waiting for the words to say Why havent told you [chorus] Friends keep asking whats the news just grin and think of you Time to take chance Let my feelings show Maybe you feel the same way too Oh darling, if you do Why havent you told me [bridge] Da-dum, da-da-da-da-da-da-da Da-dum, da-dum No more holding back these words Let them fly just like the birds [chorus] Now Im standing here tonight Hoping that got it right Might as well confess Cant keep this inside Maybe you feel the same way too Oh darling, if you do Lets not waste another day Why havent we told us [outro] Da-dum, da-da-da-da-da-da-da Da-dum, da-dum Now weve finally told us Our new lifes begun 48 ID: 11 [Genre] Rap [verse] Woke up in the morning, sun is shining bright Chasing all my dreams, gotta get my mind right City lights are fading, but my visions clear Got my team beside me, no room for fear Walking through the streets, beats inside my head Every step take, closer to the bread People passing by, they dont understand Building up my future with my own two hands [chorus] This is my life, and Im aiming for the top Never gonna quit, no, Im never gonna stop Through the highs and lows, Imma keep it real Living out my dreams with this mic and deal [verse] Late nights grinding, writing down these rhymes Clock is ticking fast, cant afford to waste time Haters gonna hate, but brush it off Turn the negativity into something strong Mama working hard, wanna make her proud Echoes of her prayers cutting through the crowd Friends turned strangers, but its all good Focused on my path like always knew would [chorus] This is my journey, and Im running this race Heart full of fire, you can see it in my face Obstacles ahead, but got no fear Victory is close, yeah, its almost here [bridge] They said couldnt do it, said Id never rise But now Im soaring high, reaching for the skies Lessons that learned made me who am Standing tall now, dont give damn [verse] Echoes in the alley, musics getting loud Feeling the adrenaline pumping through the crowd Spotlights on me, its my time to shine Living in the moment, everythings aligned Looking back now at the roads Ive crossed Every single battle, every line Ive tossed Made me stronger, wiser, ready for whats next Writing my own story, turning pages of the text [chorus] This is my song, and Im singing it proud Voices united, hear us shout out loud From the underground straight into the stars Carving out my name, leaving all these scars [outro] Yeah, this is for the dreamers, the ones who never quit Keep your head up high, and dont you ever submit Life is what you make it, so make it something great Step into your purpose, go and seize your fate 49 ID: 12 [Genre] Rock [verse] Standing on the corner, shadows in the night The citys heartbeat echoes under lights Hands deep in pockets, wandering alone Footsteps tracing paths to the unknown Suddenly he pauses, looks up to the sky Eyes reflect the questions passing by Whispers to the wind, words without sound Searching for the answers yet unfound [chorus] Lost within the chaos, seeking out sign In world of color, drawing blurred lines Moving forward, looking back, unsure of the way Trying to find place where he can stay [verse] He crosses empty streets, under neon glow Faces in the crowd, stories left untold Raises up his arms, reaching for the truth Grasping at the fragments of his youth Billboards and the banners flutter in the breeze The rhythm of the city brings him to his knees Heartbeat heavy, nowhere left to hide Feeling like hes lost amidst the tide [chorus] Lost within the chaos, seeking out sign In world of color, drawing blurred lines Moving forward, looking back, unsure of the way Trying to find place where he can stay [bridge] Doesnt want to leave, doesnt want to fight Caught between the darkness and the light No need for reason, nothing to prove Just soul in transit, with nothing to lose [outro] Doesnt want to leave, doesnt want to fight Chasing after shadows in the night He doesnt need the truth, doesnt need name Just looking for spark to fan the flame 50 ID: [Genre] Pop [verse] Staring at the sunset, colors paint the sky Thoughts of you keep swirling, cant deny know let you down, made mistakes But Im here to mend the heart didnt break [chorus] Every road you take, Ill be one step behind Every dream you chase, Im reaching for the light You cant fight this feeling now wont back down Im the whisper in the wind, the shadow by your side The warmth you feel within when you cant hide You know you cant deny it now wont back down [verse] They might say Im foolish, chasing after you But they dont feel this love the way we do My heart beats only for you, cant you see? wont let you slip away from me [chorus] Every road you take, Ill be one step behind Every dream you chase, Im reaching for the light You cant fight this feeling now wont back down Im the whisper in the wind, the shadow by your side The warmth you feel within when you cant hide You know you cant deny it now wont back down [bridge] No, wont back down, wont turn around Until youre back where you belong Ill cross the oceans wide, stand by your side Together we are strong [outro] Every road you take, Ill be one step behind Every dream you chase, loves the tie that binds You cant fight this feeling now wont back down 51 ID: 14 [Genre] Jazz [verse] In the quiet of the evening, shadows start to fall Whispers of the night wind echo through the hall Lost within the silence, hear your gentle voice Guiding me back homeward, making my heart rejoice [chorus] Dont let this moment fade, hold me close tonight With you here beside me, everythings alright Cant imagine life alone, dont want to let you go Stay with me forever, let our love just flow [verse] Moonlight paints picture upon your lovely face Every glance between us fills the empty space Time stands still around us when youre in my arms Nothing else can matter, safe from any harm [chorus] Dont let this moment fade, hold me close tonight With you here beside me, everythings alright Cant imagine life alone, dont want to let you go Stay with me forever, let our love just flow [bridge] Every touch ignites fire, burning deep within Every smile you give to me makes my head spin Promise me youll stay awhile, dont ever say goodbye Together well chase every star across the sky [chorus] Dont let this moment fade, hold me close tonight With you here beside me, everythings alright Cant imagine life alone, dont want to let you go Stay with me forever, let our love just flow [outro] Stay with me forever, let our love just flow ID: 15 [Genre] Blues [verse] Late last night, the rain was pouring down Lonely footsteps echoed through the town Thinking bout the love that slipped away Wondering how let you go that day [chorus] Oh, my angel, where have you flown Left me here to face this world alone Im just fool, fool in love with you Cant deny this heartaches true [verse] Streetlights flicker, shadows on the wall Memories of you, recall Your laughter like song inside my head Without you here, my soul feels dead [chorus] Oh, my angel, wont you return In this fire of love, still burn Im just fool, fool in love with you Hoping someday youll feel it too [bridge] fell for you, and always knew That my world revolves around you hope and pray, both night and day That youll come back and choose to stay [chorus] Oh, my angel, where have you flown Left me here to face this world alone Im just fool, fool in love with you Waiting here, what else can do [outro] Im just fool, fool in love with you"
        }
    ],
    "affiliations": []
}