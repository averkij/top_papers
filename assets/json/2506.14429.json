{
    "paper_title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
    "authors": [
        "Xiaoran Liu",
        "Zhigeng Liu",
        "Zengfeng Huang",
        "Qipeng Guo",
        "Ziwei He",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textbf{\\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textbf{\\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 9 2 4 4 1 . 6 0 5 2 : r SII-OpenMOSS LONGLLADA: UNLOCKING LONG CONTEXT CAPABILITIES IN DIFFUSION LLMS Xiaoran Liu1,2, Zhigeng Liu1, Zengfeng Huang1,2, Qipeng Guo2,3, Ziwei He2, Xipeng Qiu1,2 1School of Computer Science, Fudan University, 2Shanghai Innovation Institute, 3Shanghai AI Lab xrliu24@m.fudan.edu.cn, ziwei.he@sjtu.edu.cn, xpqiu@fudan.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Diffusion Models, or diffusion LLMs, have emerged as significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify unique characteristic of diffusion LLMs, unlike autoregressive LLMs, they maintain remarkably stable perplexity during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit distinct local perception phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, diffusion LLMs have become widely discussed topic in Natural Language Processing research (Nie et al., 2025; Ye et al., 2025). They are regarded as potential solution to key limitations (a) LLaDA-8B-Base (b) LLaMA3-8B-Base Figure 1: Comparison of perplexity and retrieval accuracy between the diffusion LLM, LLaDA-8B, and the auto-regressive LLM, LLaMA3-8B, both within and beyond pre-training context length. Corresponding Author. SII-OpenMOSS of traditional auto-regressive LLMs, including the reversal curse (Berglund et al., 2023), complex reasoning (Dziri et al., 2023), long-term planning, and maintaining coherence across extended contexts (Bachmann & Nagarajan, 2024; Ye et al., 2024; 2025). Significant research efforts have focused on validating their scalability (Nie et al., 2025; Ye et al., 2025), adapting them for multimodality (Yang et al., 2025; You et al., 2025; Yu et al., 2025), applying them to reasoning tasks (Zhao et al., 2025; Huang et al., 2025; Zhu et al., 2025), and optimizing their efficiency (Ma et al., 2025; Hu et al., 2025; Wu et al., 2025). However, the long-context capabilities of diffusion LLMs, specifically their performance and potential for length extrapolation, remain unexplored. We begin by systematically evaluating diffusion LLM LLaDA (Nie et al., 2025) against auto-regressive LLM LLaMA3 (Meta, 2024a) on perplexity and retrieval tasks, both within and beyond their pretrained context lengths (Figure 1). Notably, diffusion LLMs maintain stable perplexity and exhibit localized perception during direct length extrapolation. In stark contrast, auto-regressive LLMs suffer catastrophic perplexity surges and performance collapse when input length exceeds their maximum supported context window, 8k tokens. This divergence reveals fundamental architectural differences in long-context handling, raising critical questions: (1) What mechanisms enable diffusion LLMs extrapolation stability? (2) Can established length-extension techniques for auto-regressive LLMs be transferred to diffusion architectures? (3) How do diffusion LLMs perform on long-context benchmarks relative to auto-regressive baselines, and what unique capabilities or limitations emerge? In this work, we address these questions through comprehensive experiments and analysis. Besides the perplexity and retrieval experiment, we also benchmark Needle-In-A-Haystack (NIAH) performance for diffusion LLMs (LLaDA (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025), Dream-v0 (Ye et al., 2025)), quantitatively confirming their local perception bias during length extrapolation. We then analyze this phenomenon through Rotary Position Embedding (RoPE) theory, validating our interpretation with t-SNE visualizations. Building on these insights, we propose LongLLaDA, training-free method which successfully extends LLaDAs context window using NTK-based RoPE extrapolation (bloc97, 2023b), and verify preserved scaling laws (Liu et al., 2023b). Finally, we identify task-dependent capabilities where diffusion LLMs surpass or lag behind auto-regressive counterparts on long-context benchmarks. Our contributions are summarized as follows: First systematic analysis of diffusion LLMs long-context behavior, revealing their unique characteristics for stable perplexity and local perception during context extrapolation, with mechanistic explanation via RoPE dynamics. Effective context extension demonstrating NTK-based RoPE extrapolation and scaling laws transfer seamlessly to diffusion LLMs, achieving 6 context expansion (24k tokens) without further training. Capability benchmarking revealing diffusion LLMs match auto-regressive models on retrieval tasks, lag in aggregation, but excel at QA. We provide foundational insights for future long-context diffusion research."
        },
        {
            "title": "2 LONG-CONTEXT PHENOMENOLOGY OF DIFFUSION LLMS",
            "content": "We first evaluate the length extrapolation capabilities of diffusion LLMs, including LLaDA (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025), and Dream-v0 (Ye et al., 2025), compared with autoregressive LLMs such as LLaMA3 (Meta, 2024a), via Needle-In-A-Haystack (Gkamradt, 2023; Li et al., 2024), based on the experimental setup in Appendix B.1. All LLMs are required to generate at most 32 tokens, with diffusion LLMs using block size and sampling steps of 32. The results are shown in Figure 2. LLaMA3-8B-Base and LLaMA3-8B-Instruct maintain perfect retrieval accuracy within their pretrained 8k length, but suffer catastrophic performance degradation beyond this limit, failing to retrieve information at any depth. In contrast, LLaDA-8B-Base and LLaDA-8B-Instruct achieve 100% retrieval accuracy within 4k context. Surprisingly, when exceeding 4k, up to 24k, LLaDA still retrieves information from the nearest 4k window, demonstrating local perception like sliding window. This behavior remarkably differs from auto-regressive LLM extrapolation. Similar phenomena are observed in LLaDA-1.5 and Dream-v0, as illustrated in Appendix B.2. Different from auto-regressive LLMs, diffusion LLMs are influenced by sampling steps and strategies. For simplicity, we compare the impact of sampling steps on retrieval depth in NIAH. As shown in Figure 3, using the same input-output settings from previous experiments, we evaluate the LLaDA2 SII-OpenMOSS (a) LLaDA-8B-Base (b) LLaMA3-8B-Base (c) LLaDA-8B-Instruct (d) LLaMA3-8B-Instruct Figure 2: Results of Needle-In-A-Haystack tests (Gkamradt, 2023) on LLaDA-8B Series (Nie et al., 2025) and LLaMA3-8B Series (Meta, 2024b) under direct extrapolation. (a) LLaDA-8B-Base with = 1 (b) LLaDA-8B-Base with = 4 (c) LLaDA-8B-Base with = (d) LLaDA-8B-Base with = 16 Figure 3: NIAH Results of LLaDA-8B-Base (Nie et al., 2025) with different sampling steps s. 8B-Base with sampling step = 1, 4, 8, 16. Results show that at 1 or 4 steps, LLaDA-8B-Base fails to retrieve information beyond 8k length, and increasing to 8 or 16 can achieve retrieval depths of 25% at 16k and almost 10% at 24k context length. Similar results are observed on LLaDA-8B-Instruct and LLaDA-1.5 in Appendix B.2, demonstrating that the long-context performance of diffusion LLMs is influenced by sampling steps, but remains constrained by the maximum supported context length. 3 SII-OpenMOSS (a) LLaDA with Ttrain = 4k (b) LLaMA3 with Ttrain = 8k Figure 4: Comparison of trained position embedding interval between LLaDA-8B and LlaMA3-8B. The area within the dashed line represents trained relative position, while that beyond represents the relative position in length extrapolation, with unlearned position embedding values colored in gray."
        },
        {
            "title": "3 MECHANISTIC ANALYSIS",
            "content": "According to the preliminary knowledge in Appendix A, we attribute this phenomenon to diffusion LLMs being trained with richer positional information compared to auto-regressive LLMs. Critically, the bidirectional attention mechanisms in diffusion LLMs expose them to relative position rage of [1 Ttrain, Ttrain 1] during training, contrasting with the [0, Ttrain 1] range typical of auto-regressive models. This difference is evident in the RoPE mechanism. As visualized in Figure 4, for LLaDA (Ttrain = 4k) and LLaMA (Ttrain = 8k), we observe how the positional embeddings (sine/cosine components) behave within and beyond their maximum trained relative positions. High Frequencies: Both models perceive complete sinusoidal periods within their maximum trained relative distance, yielding comparable positional information encoding. Moderate Frequencies: LLaMA3s auto-regressive attention observes relative positions [0, 8191] when trained on 8192-token sequences. In contrast, LLaDAs bidirectional attention observes symmetric relative positions [4095, 4095] despite its shorter 4096-token training length. This symmetric coverage provides key advantage by fully capturing complete period of both the cosine and sine, enhancing its tolerance of direct length extrapolation. Low Frequencies: Both models exhibit limited extrapolation capability beyond their pretrained context windows. However, as visualized in Figure 4, the out-of-distribution (OOD) regions differ remarkably: LLaMA3 struggles to capture all negative position embeddings (gray region), representing half of the potential embedding space, while LLaDA significantly reduces the unlearned OOD spaces, resulting in enhanced robustness in length extrapolation. This results in relatively flattened perplexity growth curve, similar to auto-regressive RoPE-based LLMs with smaller base (Liu et al., 2023b; Men et al., 2024), as detailed in Appendix A. However, since the cosine function in RoPE, which primarily captures relative distances, is even, negative relative positions do not increase the LLMs maximum perceivable distance in the pre-training stage. Thus, diffusion LLM can only retrieve key information from limited relative positions within the training length, leading to the observed decay pattern in the NIAH evaluation. We validate this interpretation with the t-SNE visualization (Van der Maaten & Hinton, 2008; Zandieh et al., 2024) of QK states from the final layer of LLaMA3-8B-Base (Meta, 2024a) and LLaDA-8BBase (Nie et al., 2025), as shown in Figure 5. As shown in Figure 5a, for auto-regressive LLMs such as LLaMA3-8B-Base, the QK states within and beyond the maximum supported context length, 8k, present two different distribution clusters, and the manifold for QK states with RoPE also shows different trend when position embedding becomes OOD. Comparatively, regarding the clustering 4 SII-OpenMOSS (a) LLaDA-8B-Base (b) LLaMA3-8B-Base Figure 5: Visualization of the QK states from the final layer of LLaMA3-8B-Base (Meta, 2024a) and LLaDA-8B-Base (Nie et al., 2025) for sample from the GovReport subsets in LongBench (Bai et al., 2023). The visualization uses 2D t-SNE projection (Van der Maaten & Hinton, 2008), with each token represented as point in the image and the position index shown via color changing. feature for diffusion LLMs such as LLaDA-8B-Base, there is no distribution shift between QK states within and beyond 4k, and uniform manifold for QK states with RoPE. This demonstrates that diffusion LLM is more robust for the OOD position embeddings in length extrapolation. Therefore, unlike traditional auto-regressive LLMs that exhibit catastrophic performance degradation when exceeding their maximum supported context length, diffusion LLMs maintain stable outputs and demonstrate local perception in extended context."
        },
        {
            "title": "4 CONTEXT EXTENSION FOR DIFFUSION LLMS",
            "content": "Since the reason for the surprising phenomenon has been clarified, we now move on to the extrapolation methods for diffusion LLMs. Since the retrievable depth of diffusion LLMs remains constrained by the range of cosine values encountered during pre-training, we transfer the NTK-based extrapolation (bloc97, 2023b) and its scaling laws (Liu et al., 2023b) to diffusion LLMs, thus proposing the length extrapolation method for diffusion LLMs, LongLLaDA. As detailed in Appendix A. The scaling factor λ in training-free NTK scaling (bloc97, 2023b) for RoPE-based auto-regressive LLMs is decided by the extrapolation context length and critical dimension dextra calculated by rotary base β0 and pretrained context length Ttrain, as shown in Equation 1. (cid:24) 2 λ = 104 (cid:18) 2π Ttrain 2π dextra = 2 (cid:19)d/dextra logβ0 (1) (cid:25) , . Similarly, in LongLLaDA, based on Nie et al. (2025), the pretrained rotary base β0 = 500000, and the pre-training context length Ttrain is 4k. This yields critical dimension dextra = 64. Accordingly, the required scaling factor λ for extrapolation to 8k, 16k, 24k, and 32k is calculated as 4, 14, 31, and 55, respectively. The extrapolation results are illustrated in Figure 6 and Figure 7. When λ = 4, 14, LongLLaDA can effectively extrapolate diffusion LLMs to the corresponding context lengths, achieving near 100% recall across all depths within these ranges. As the context length increases beyond the extrapolation limit, the retrievable depth proportionally expands while maintaining the local-perception effect. The average depth score curves exhibit right shift across different context lengths. When λ = 31, lost-in-the-middle phenomenon (Liu et al., 2023a) similar to auto-regressive models emerges in intermediate depths, indicating that LongLLaDA approaches its practical extrapolation limit (bloc97, 2023b). When λ = 55, further extrapolation is unachievable. We also validate the effectiveness of LongLLaDA in LLaDA-1.5 (Zhu et al., 2025) and Dream-v0 (Ye et al., 2025) in Appendix B.2. Consequently, for RoPE-based diffusion LLMs, NTK extrapolation and its scaling law remain applicable during inference. 5 SII-OpenMOSS (a) LLaDA-8B-Base with λ = 4 (b) LLaDA-8B-Base with λ = 14 (c) LLaDA-8B-Base with λ = (d) LLaDA-8B-Base with λ = 55 Figure 6: NIAH Results of LLaDA-8B-Base (Nie et al., 2025) with different RoPE scaling factor. (a) LLaDA-8B-Instruct with λ = 4 (b) LLaDA-8B-Instruct with λ = 14 (c) LLaDA-8B-Instruct with λ = 31 (d) LLaDA-8B-Base with λ = Figure 7: NIAH Results of LLaDA-8B-Instruct (Nie et al., 2025) with different RoPE scaling factor."
        },
        {
            "title": "5 TASK-DRIVEN LONG-CONTEXT CAPABILITY ANALYSIS",
            "content": "Regarding the downstream long-context performance of diffusion LLMs and their difference from traditional auto-regressive LLMs, apart from the NIAH retrieval evaluation, we conduct comparative analyses across more benchmarks using LLaDA and LLaMA as examples. We first evaluate LLaDA8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025), and LLaMA3-8B (Meta, 2024a), including pretrained models and those employing NTK-based extrapolation during inference, with LongBench (Bai et al., 2023), in 4k and 8k context length, with the exceeding part being truncated from the middle. For the summary tasks, the output length is 512, while for the others, the output length is 64. We still 6 SII-OpenMOSS 4k 8k SD MD Sum ICL Syn Code Avg SD MD Sum ICL Syn Code Avg LLaDA-8B-Base + NTK λ = 4 15.1 18.4 32.0 42.0 54.7 59.6 34.1 13.9 13.1 30.8 40.7 56.0 57.4 32.4 14.7 19.1 31.5 40.9 52.4 63.0 34.0 15.2 18.6 31.0 41.4 53.8 59.2 33.6 LLaDA-8B-Instruct + NTK λ = 4 25.1 19.4 30.6 36.4 62.8 62.7 37.2 22.7 14.0 33.4 33.2 66.7 66.4 36.8 22.1 19.8 33.0 38.0 63.3 65.0 37.8 23.4 19.8 35.3 39.8 72.9 67.3 40.6 LLaDA-1.5 + NTK λ = 4 24.4 19.4 31.6 33.5 63.6 66.7 37.6 22.6 14.5 33.4 33.0 67.6 67.6 37.1 21.8 19.7 33.1 35.3 63.4 67.3 37.8 23.0 20.6 34.9 39.3 72.9 67.9 40. LLaMA3-8B-Base 17.2 18.7 25.0 41.7 47.6 66.5 33.6 18.2 18.3 26.1 44.5 49.6 69.4 35.1 LLaMA3-8B-Instruct 31.9 26.1 33.6 39.6 46.6 55.9 37.0 37.5 28.3 34.7 40.7 62.8 56.1 41.9 Table 1: Results of LLaDA-8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025) and LLaMA38B (Meta, 2024b) on LongBench (Bai et al., 2023) under 4k and 8k context length. Gray cells indicate that the evaluation context length exceeds the context length supported by the evaluated LLM. SD, MD, Sum, and Syn stand for Single-Doc QA, Multi-Doc QA, Summarization, and Synthetic tasks, while Avg is the average score of all subtasks weighted by the evaluation data number. 4k 8k 16k NIAH AGG QA Avg NIAH AGG QA Avg NIAH AGG QA Avg LLaDA-8B-Base + NTK λ = 4 + NTK λ = 14 + NTK λ = 31 LLaDA-8B-Instruct + NTK λ = 4 + NTK λ = 14 + NTK λ = 31 LLaDA-1.5 + NTK λ = 4 + NTK λ = 14 + NTK λ = 31 LLaMA3-8B-Base + NTK λ = 4 + NTK λ = LLaMA3-8B-Instruct + NTK λ = 4 + NTK λ = 13 99.7 99.5 99.8 100.0 99.3 99.8 100.0 100.0 98.7 99.8 100.0 100.0 99.8 99.9 99.5 99.6 99.8 99. 65.2 82.5 89.1 82.3 80.5 92.6 83.5 77.0 92.5 83.8 77.0 92.7 57.8 90.5 88.4 65.5 89.5 90.3 76.4 89.0 92.9 77.7 86.5 92.8 66.0 90.0 89.8 73.9 91.0 92.5 79.8 88.5 93.6 81.6 87.5 93.8 98.1 67.5 94.4 98.7 65.0 94.2 98.6 66.0 94.1 97.2 68.5 94.3 96.9 72.0 94.9 96.7 68.0 94.0 53.8 96.4 99.3 97. 52.3 95.9 97.3 98.8 53.9 96.3 99.9 98.9 99.6 99.8 99.1 98.2 99.6 99.3 45.2 41.0 49.8 61.0 73.0 84.7 68.9 64.0 86.8 75.2 62.5 87.1 44.3 48.0 49.8 56.6 89.0 85.8 66.2 89.5 88.9 73.5 88.5 91. 45.1 48.5 51.0 59.3 88.0 86.5 67.8 89.0 90.8 75.1 86.5 91.5 93.5 63.0 92.5 94.1 59.0 92.2 94.0 59.0 91.8 92.6 54.0 90.1 93.5 65.0 92.8 92.4 63.5 92.2 22.0 51.8 85.4 97.2 18.9 41.6 67.1 88.0 19.0 43.3 67.4 85. 0.0 97.0 93.8 0.0 95.0 95.3 36.0 19.5 1.9 17.1 53.5 44.1 48.1 54.0 72.0 51.8 41.0 78.0 12.0 47.0 21.6 31.0 73.0 44.0 53.8 84.5 66.7 62.2 82.0 81.1 14.2 46.0 22.1 32.2 73.0 45.3 51.6 84.0 66.3 58.2 81.5 78.7 0.0 0.0 0.0 86.6 54.5 88.1 90.3 56.0 87. 0.0 0.0 0.0 89.5 63.0 88.8 78.6 62.5 86.4 Table 2: Results of LLaDA-8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025) and LLaMA38B (Meta, 2024b) on RULER (Hsieh et al., 2024) under 4k, 8k and 16k context length. keep the sampling steps the same as the output length, and the block size to 64 for diffusion LLMs. The results are shown in Table 1. Still, LLaDA can give stable output and get decent performance beyond the maximum supported context length. Moreover, we find that in all task domains besides synthetic tasks, the difference between LLaDA Series and LLaMA3 Series is relatively limited compared with the difference within LLaMA3 Series. Only in the synthetic domain do LLaDA Series outperform LLaMA3 Series consistently. This inspires us to conduct an in-depth discussion of diffusion LLMs on the performance of the synthesis tasks compared with auto-regressive LLMs. We further the discussion with RULER benchmark (Hsieh et al., 2024), we compare LLaDA-8B (Nie et al., 2025), LLaDA-1.5 (Zhu et al., 2025), and LLaMA3-8B (Meta, 2024a), at context lengths of 4k, 8k, and 16k. We set the block size and sampling steps to 64 for diffusion LLMs. The results are shown in Table 2. First, consistent with the NIAH results, auto-regressive LLMs fail to produce valid outputs beyond their effective context length, while diffusion LLMs maintain measurable performance. Regarding task types, diffusion LLMs achieve comparable results to auto-regressive 7 SII-OpenMOSS LLMs on NIAH tasks, including Single-Key, Multi-Key, Multi-Query, and Multi-Value variants. However, diffusion LLMs show significantly inferior performance in aggregation tasks, including Variable Tracing and Frequent or Common Word Extraction, where auto-regressive LLMs typically perform well. Surprisingly, on QA tasks, including SQuAD and Hotpot, that challenge auto-regressive LLMs (Hsieh et al., 2024), diffusion LLMs demonstrate superior capability. These observations reveal the distinctive characteristics of diffusion LLMs in long-context tasks, that current diffusion LLMs, like LLaDA, demonstrate comparable performance to the auto-regressive LLMs, like LLaMA3, in most task types, but underperform in aggregation tasks, and outperform in QA tasks consistently."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Large Language Diffusion Models Recently, Large Language Diffusion Models, or diffusion LLMs, have become widely discussed topic in NLP research. After the theoretical simplification (Sahoo et al., 2024; Ou et al., 2024) and fine-tuning verification (Gong et al., 2024), researchers scale the size of diffusion LLMs to billions of parameters (Nie et al., 2024; 2025; Ye et al., 2025) and demonstrate that diffusion LLMs can achieve comparable results with more promising performance in the reversal curse (Berglund et al., 2023). These immediately attract the attention of many more researchers. Significant research efforts have focused on adapting diffusion LLMs for multimodality, such as MMaDA (Yang et al., 2025), LLaDA-V (You et al., 2025), and LaViDa (Li et al., 2025), applying them to reasoning tasks, such as d1 (Zhao et al., 2025), DCoLTHuang et al. (2025), and LLaDA-1.5 (Zhu et al., 2025), and optimizing their efficiency (Ma et al., 2025; Hu et al., 2025; Wu et al., 2025), including dKV-Cache (Ma et al., 2025), Dimple (Yu et al., 2025), dLLM-Cache (Liu et al.), FreeCache (Hu et al., 2025), Fast-dLLM (Wu et al., 2025), and so on. However, there is still no discussion on the long-context capability of diffusion LLMs. Length Extrapolation in LLM Length extrapolation, or length generalization, or context extension, is an important issue for LLMs (Press et al., 2022). The mainstream extrapolation research mainly focuses on adjusting position embedding, especially the widely used RoPE (Su et al., 2021). For example, Linear PI (Chen et al., 2023) first achieves LLMs length extrapolation by scaling position indices to the pre-training range with little fine-tuning. The NTK method (bloc97, 2023b;a; Peng et al., 2023) then scales the rotary base in RoPE (Su et al., 2021) to achieve plug-and-play length extrapolation. Subsequently, amplifying the rotary base and training on longer lengths has become the dominant approach for length extrapolation (Rozi`ere et al., 2023; Xiong et al., 2023; Liu et al., 2023b; Ding et al., 2024). In addition, ReRoPE (Su, 2023), ReAttention (Liu et al., 2024b), and DCA (An et al., 2024a;b) also achieve plug-and-play extrapolation by limiting the relative position. In this paper, we still focus on the length extrapolation via NTK scaling (bloc97, 2023b; Liu et al., 2023b) in the inference stage, and try to reveal and explain the similarities and differences in length extrapolation between diffusion-based and auto-regressive LLM."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we provide the first systematic analysis of long-context capabilities in diffusion LLMs. We demonstrate and analyze their characteristics for stable perplexity and local perception in direct context extrapolation from the perspective of the RoPE dynamic. Then, we propose LongLLaDA, which extends the context length in NTK scaling effectively without further training, and validate that the scaling laws still work for diffusion LLMs. Besides, we also show that diffusion LLMs match auto-regressive models on the average score of LongBench as well as the retrieval tasks, lag in aggregation tasks, but excel at QA in RULER evaluation. We hope our work can pave the foundation for future long-context research in diffusion LLMs."
        },
        {
            "title": "LIMITATION",
            "content": "Although we have conducted extensive experiments on diffusion LLMs, our results mainly focus on LLaDA Series and the inference stage. We will carry out the fine-tuning extrapolation experiments in the future to verify more conclusions in RoPE-based scaling theory for auto-regressive LLMs. Besides, we will also add more analyses focused on sampling strategy specialized in diffusion LLMs. 8 SII-OpenMOSS"
        },
        {
            "title": "REFERENCES",
            "content": "Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models. arXiv preprint arXiv:2402.17463, 2024a. Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of llms fall short? arXiv preprint arXiv:2410.18745, 2024b. Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. arXiv preprint arXiv:2403.06963, 2024. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on is fail to learn is a. arXiv preprint arXiv:2309.12288, 2023. bloc97. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023a. URL https://www.reddit.com/r/LocalLLaMA/comments/ 14mrgpr/dynamically_scaled_rope_further_increases/. bloc97. Ntk-aware scaled rope allows llama models to have extended (8k+) context URL size without any fine-tuning and minimal perplexity degradation., June 2023b. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. CoRR, abs/2306.15595, 2023. doi: 10.48550/ ARXIV.2306.15595. URL https://doi.org/10.48550/arXiv.2306.15595. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10. 48550/arXiv.2307.08691. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36:70293 70332, 2023. Gkamradt. Needle in haystack - pressure testing llms. https://github.com/gkamradt/ LLMTest_NeedleInAHaystack, 2023. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023. doi: 10. 48550/ARXIV.2308.16137. URL https://doi.org/10.48550/arXiv.2308.16137. 9 SII-OpenMOSS Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng. Can perplexity reflect large language models ability in long text understanding? arXiv preprint arXiv:2405.06105, 2024. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467, 2025. Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025. Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng Zhang. Massive values in self-attention modules are the key to contextual knowledge understanding. arXiv preprint arXiv:2502.01563, 2025. Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval and reasoning in 1 million context window? arXiv preprint arXiv:2407.11963, 2024. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023a. Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation. CoRR, abs/2310.05209, 2023b. doi: 10.48550/ARXIV.2310.05209. URL https://doi.org/10.48550/arXiv.2310.05209. Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Mianqiu Huang, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Beyond homogeneous attention: Memory-efficient llms via fourier-approximated kv cache. arXiv preprint arXiv:2506.11886, 2024a. Xiaoran Liu, Ruixiao Li, Qipeng Guo, Zhigeng Liu, Yuerong Song, Kai Lv, Hang Yan, Linlin Li, Qun Liu, and Xipeng Qiu. Reattention: Training-free infinite context with finite attention scope. arXiv preprint arXiv:2407.15176, 2024b. Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, et al. Thus spake long-context large language model. arXiv preprint arXiv:2502.17129, 2025. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen. Base of rope bounds context length. arXiv preprint arXiv:2405.14591, 2024. AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI., 2024a. AI Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI., 2024b. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. 10 SII-OpenMOSS Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URL https://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biIn The Tenth International Conference on Learning ases enables input length extrapolation. Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10. 48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Jianlin Su. Rerope: Rectified rotary position embeddings, July 2023. URL https://github. com/bojone/rerope. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/ 2104.09864. Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shiduo Zhang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, Yu-Gang Jiang, and Xipeng Qiu. Moss: An open conversational large language model. Machine Intelligence Research, 2024. ISSN 2731-5398. doi: 10.1007/s11633-024-1502-8. URL https://github.com/OpenMOSS/MOSS. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. arXiv preprint arXiv:2404.12224, 2024. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Videorope: What makes for good video rotary position embedding? arXiv preprint arXiv:2502.05173, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. 11 SII-OpenMOSS An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Beyond autoregression: Discrete diffusion for complex reasoning and planning. arXiv preprint arXiv:2410.14157, 2024. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025. Amir Zandieh, Insu Han, Vahab Mirrokni, and Amin Karbasi. Subgen: Token generation in sublinear time and memory. arXiv preprint arXiv:2402.06082, 2024. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. PRELIMINARY: ROPE EXTRAPOLATION IN AUTO-REGRESSIVE LLM Rotary Position Embedding (RoPE) (Su et al., 2021) employs trigonometric functions to encode (cid:105) , , q(d1) absolute positions in state qt = . By leveraging the properties of rotation matrices, RoPE encodes relative position in the attention matrix A, as shown in Equantion 2 and demonstrates superior performance, thus being widely adopted by many auto-regressive LLMs (Sun et al., 2024; Dubey et al., 2024; Yang et al., 2024). , , k(d1) k(0) and state ks = (cid:104) q(0) (cid:105) (cid:104) At,s = (qtRt) (ksRs) = qtRtsk = = d/21 (cid:88) n=0 d/21 (cid:88) n=0 (cid:34) q(2n) q(2n+1) (cid:16) q(2n) (cid:16) q(2n) (cid:35) (cid:20)cos θn(t s) sin θn(t s) cos θn(t s) (cid:17) sin θn(t s) + q(2n+1) k(2n) k(2n+1) (cid:21) (cid:34) k(2n) k(2n+1) (cid:35) . (2) k(2n+1) q(2n+1) k(2n) cos θn(t s) (cid:17) sin θn(t s) However, RoPE still faces the length extrapolation issue (Press et al., 2022). When RoPE-based autoregressive LLMs are tested beyond the pre-trained context length, the perplexity rises significantly, and downstream performance drops sharply. The underlying causes and corresponding solutions can be attributed to two key properties of trigonometric functions: periodicity and monotonicity. 12 SII-OpenMOSS Rule of Periodicity According to the design of RoPE, different dimensions of qt, ks use different rotary angles θn, with rotary base β0 = 10000 by default and the periods Tn for sin(θnt) and cos(θnt) increasing from low to high dimensions as shown in Equation 3. θn = β2n/d 0 , Tn = 2π β2n/d 0 , = 0, , d/2 1. (3) For lower dimensions, Tn is very short, compared with the pre-trained context length Ttrain, while for higher ones, Tn becomes significantly longer, exceeding Ttrain. Consequently, there exists critical dimension, dextra, as shown in Equation 4, within which sin(θnt) or cos(θnt) complete at least one full period within the pretrained length, whereas those beyond do not. dextra = 2 (cid:24) logβ0 (cid:25) . Ttrain 2π (4) Therefore, dimensions beyond dextra will encounter OOD position embedding when processing longer inputs and larger position indices in inference, leading to extrapolation issues (Liu et al., 2023b). To enable LLM to handle unseen position indices, NTK methods (bloc97, 2023b; Xiong et al., 2023) scale the rotary base by factor λ, reducing the rotary angle to achieve position interpolation. However, since different dimensions undergo different degrees of interpolation, the position embedding at the critical dimension will first become OOD. Thus, based on the scaled period of the critical dimension, the extrapolation upper bound Textra for NTK methods can be derived, as shown in Equation 5. Textra = 2π (λ β0)dextra/d. (5) Based on Equation 5, for an input length t, the rotary base scaling factor λ should be set as shown in Equation 6 to ensure no OOD position embeddings occur. Notably, this adjustment coefficient exhibits sup-linear, power-law increase with inference length (Liu et al., 2023b; 2025). λt = β1 (cid:18) 2π (cid:19)d/dextra . (6) It should be noted that while such interpolation could theoretically avoid extrapolation issues, it can only achieve 2 to 6 long-context extension during inference, as longer inputs lead to increased attention entropy, limiting further extrapolation (bloc97, 2023b; Han et al., 2023; Wang et al., 2024). Rule of Monotonicity Since the pre-trained position information of dimensions beyond the critical dimension limits the extrapolation capability of RoPE-based auto-regressive LLMs, if the rotary base is reduced, and each dimension can cover at least half or even full period, the perplexity curve of auto-regressive LLMs will be flattened (Liu et al., 2023b). However, this does not imply real length extrapolation. Subsequent studies (Men et al., 2024; Hu et al., 2024) find that such LLMs can only perceive local information in downstream evaluations and fail to retrieve long-context dependencies. Exposing LLM to periodic position information leads to downstream degeneration, manifesting sliding-window effect, which reveals another aspect of RoPE-based extrapolation, the impact of monotonicity. Although higher dimensions do not observe complete position information, they provide relatively complete monotonic interval, reflecting partial ordering in long-context scenarios. These dimensions exhibit larger activation values in long-context tasks (Jin et al., 2025), are more sensitive to modeling long-context dependencies (Liu et al., 2024a), and are better suited for capturing sequential relationships (Wei et al., 2025). Thus, solely optimizing for periodicity at the cost of losing monotonicity across all dimensions is wrong (Men et al., 2024; Liu et al., 2025)."
        },
        {
            "title": "B MORE EXPERIMENT RESULTS",
            "content": "B.1 SETUP Since we have clarified the RoPE-based extrapolation in auto-regressive LLM, we now turn our focus to that in diffusion LLM and try to answer the three questions raised in Section 1. We conduct experiments on the existing diffusion LLM series, including LLaDA-8B (Nie et al., 2025), LLaDA1.5 (Zhu et al., 2025), and Dream-v0 (Ye et al., 2025). By default, we set the number of sampling 13 SII-OpenMOSS (a) Pretrained LLaDA-1.5 (b) LLaDA-1.5 with λ = (c) LLaDA-1.5 with λ = 14 (d) LLaDA-1.5 with λ = 31 Figure 8: NIAH Results of LLaDA-1.5 (Zhu et al., 2025) with different λ. steps in diffusion LLM to 32 and keep the sampling strategy in the official code of LLaDA 1 and Dream 2. We use OpenCompass (Contributors, 2023) for validation. All experiments are performed with fixed random seed of 2025, FP16 precision, and accelerated with FlashAttention2 (Dao, 2023). B.2 MORE RESULTS OF NEEDLE-IN-A-HAYSTACK We report the NIAH results of LLaDA-8B-Instruct (Nie et al., 2025) and LLaDA (Zhu et al., 2025) with different sampling steps in Figure 11 and Figure 12 respectively, similar to Figure 3. In Figure 8, we report the NIAH results of pre-trained and NTK-scaled LLaDA-1.5 (Zhu et al., 2025). LLaDA-1.5 still supports 4k context length and has local perception in direct length extrapolation. We use the same scaling factors as LLaDA and achieve similar length extrapolation effects consistent with the prediction of the scaling law of RoPE-based length extrapolation (Liu et al., 2023b). In Figure 9 and Figure 10, we also report the NIAH performance of Dream-v0-7B-Base and Dreamv0-7B-Instruct. We find that both diffusion LLMs still have local perception in direct extrapolation, but only support context length around 2k. We recalculate the scaling factor λ for Dream-v0 in 4k, 8k, and 16k, respectively, based on Equation 1. Though the long-context capability of Dream-v0 Series is poor, we still find the NTK scaling works effectively in 4k context length. Besides, since Dream-v0 Series is trained not from scratch but initialized from auto-regressive LLM, Qwen2.5-7B (Ye et al., 2025), we do not compare them with auto-regressive LLMs in task characteristics. 1https://github.com/ML-GSAI/LLaDA 2https://github.com/HKUNLP/Dream 14 SII-OpenMOSS (a) Pretrained Dream-v0-7B-Base (b) Dream-v0-7B-Base with λ = (c) Dream-v0-7B-Base with λ = 25 (d) Dream-v0-7B-Base with λ = 126 Figure 9: NIAH Results of Dream-v0-7B-Base (Ye et al., 2025) with different λ. (a) Pretrained Dream-v0-7B-Instruct (b) Dream-v0-7B-Instruct with λ = 5 (c) Dream-v0-7B-Instruct with λ = (d) Dream-v0-7B-Instruct with λ = 126 Figure 10: NIAH Results of Dream-v0-7B-Instruct (Ye et al., 2025) with different λ. 15 SII-OpenMOSS (a) LLaDA-8B-Instruct with = 1 (b) LLaDA-8B-Instruct with = (c) LLaDA-8B-Instruct with = 8 (d) LLaDA-8B-Instruct with = 16 Figure 11: NIAH Results of LLaDA-8B-Instruct (Nie et al., 2025) with different sampling steps. (a) LLaDA-1.5 with = 1 (b) LLaDA-1.5 with = 4 (c) LLaDA-1.5 with = (d) LLaDA-1.5 with = 16 Figure 12: NIAH Results of LLaDA-1.5 (Zhu et al., 2025) with different sampling steps."
        }
    ],
    "affiliations": [
        "School of Computer Science, Fudan University",
        "Shanghai AI Lab",
        "Shanghai Innovation Institute"
    ]
}