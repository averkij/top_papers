{
    "paper_title": "MedINST: Meta Dataset of Biomedical Instructions",
    "authors": [
        "Wenhan Han",
        "Meng Fang",
        "Zihan Zhang",
        "Yu Yin",
        "Zirui Song",
        "Ling Chen",
        "Mykola Pechenizkiy",
        "Qingyu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The integration of large language model (LLM) techniques in the field of medical analysis has brought about significant advancements, yet the scarcity of large, diverse, and well-annotated datasets remains a major challenge. Medical data and tasks, which vary in format, size, and other parameters, require extensive preprocessing and standardization for effective use in training LLMs. To address these challenges, we introduce MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples, making it the most comprehensive biomedical instruction dataset to date. Using MedINST as the meta dataset, we curate MedINST32, a challenging benchmark with different task difficulties aiming to evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and evaluate on MedINST32, showcasing enhanced cross-task generalization."
        },
        {
            "title": "Start",
            "content": "MedINST: Meta Dataset of Biomedical Instructions Wenhan Han1, Meng Fang2,1, Zihan Zhang3, Yu Yin2, Zirui Song3, Ling Chen3, Mykola Pechenizkiy1, Qingyu Chen4 1Eindhoven University of Technology 2University of Liverpool 3University of Technology Sydney 4Yale University w.han@tue.nl, Meng.Fang@liverpool.ac.uk, qingyu.chen@yale.edu 4 2 0 2 7 1 ] . [ 1 8 5 4 3 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The integration of large language model (LLM) techniques in the field of medical analysis has brought about significant advancements, yet the scarcity of large, diverse, and well-annotated datasets remains major challenge. Medical data and tasks, which vary in format, size, and other parameters, require extensive preprocessing and standardization for effective use in training LLMs. To address these challenges, we introduce MEDINST, the Meta Dataset of Biomedical Instructions, novel multi-domain, multi-task instructional metadataset. MEDINST comprises 133 biomedical NLP tasks and over 7 million training samples, making it the most comprehensive biomedical instruction dataset to date. Using MEDINST as the meta dataset, we curate MEDINST32, challenging benchmark with different task difficulties aiming to evaluate LLMs generalization ability. We fine-tune several LLMs on MEDINST and evaluate on MEDINST32, showcasing enhanced cross-task generalization."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs), such as GPT-4 (OpenAI, 2024), LLaMA3 (Meta, 2024) and Mistral (Jiang et al., 2023) have demonstrated impressive performance across various open-domain NLP tasks. Rather than developing specialized, task-specific systems, there is an increasing focus on rapidly adapting LLMs to specific tasks through simple prompting techniques. Studies have demonstrated that such prompted LLMs can achieve and even outperforms the capabilities of specialized models in variety of NLP tasks (Radford et al.; Brown et al., 2020; Wei et al., 2022; Sanh et al., 2022). Due to the high cost of pre-training LLMs, instruction finetuning has become the standard method for adapting base LLMs to specific domains. Therefore, training domainspecific LLMs has largely shifted to data-centric approach. In recent years, the field of medical analysis has experienced transformative shift with the integration of large language model (LLM) techniques, fundamentally expanding the landscape of diagnostic and therapeutic strategies. The advancement of this field relies heavily on the availability of large, diverse, and well-annotated datasets, which are crucial for training robust and effective machine learning (ML) models. Although specialized biomedical models such as BioBERT (Lee et al., 2020), ClinicalXLNET (Huang et al., 2019), BioMTransformers (Alrowili and Shanker, 2021) and SciFive (Phan et al., 2021) have achieved success, they rely on task-specific modules and follow pre-train then fine-tune paradigm for specified tasks (Liu et al., 2021; Wang et al., 2023a). In this context, generalizing to unseen tasks is computationally expensive and time-consuming. Attempts exist such as In-BoXBART (Parmar et al., 2022) and BioMistral (Labrak et al., 2024) are finetuned with biomedical instructions. However, the data involved in training and evaluation are limited. Collecting raw medical data and converting it into format suitable for LLM applications is often complex and challenging. Medical data and tasks vary significantly in format, size, and other parameters, necessitating extensive preprocessing and standardization. This task becomes even more intricate when integrating multiple datasets from various domains into cohesive, standardized format. This raises the necessity of comprehensive biomedical instruction meta-dataset. To address the problem, we release MEDINST1, an instruction dataset collection includes 133 biomedical NLP tasks in 12 categories such as Named Entity Recognition (NER), Question1The code, models and data are available at https:// github.com/aialt/MedINST."
        },
        {
            "title": "Resource",
            "content": "Has task instructions? Has multi-task datasets? Has examples? Is public? Number of tasks Number of instructions Number of annotated task types Avg. task definition length (words) MEDINST (this work) 133 133 12 45.98 SUP-NATINST (Wang et al., 2022) BoX (Parmar et al., 2022) BLURB (Gu et al., 2021) (Biomedicine) 30 30 - 56. 32 32 9 - 13 - 6 - Table 1: Comparison of MEDINST to several datasets in biomedical field. Answering (QA), Relation Extraction (RE), etc. benchmark is set by curate test set from the entire collected dataset. In the experiment, multiple scales of LLMs are finetuned on our training data to demonstrate the generalization performance enhancement. Table 1 presents the comparison of MEDINST to relevant datasets in biomedical field. In summary, our contributions are: We release novel dataset MEDINST, biomedical instruction meta-dataset that involves 7M samples spanning 133 tasks among 12 categories. Using the meta dataset, we curate MEDINST32, challenging benchmark for evaluating the cross-task generalization ability of LLMs in the biomedical domain. We introduce instruction fine-tuned LLMs on MEDINST based on LLaMA-3 and conduct comprehensive evaluation and analysis across multiple baselines."
        },
        {
            "title": "2 Related Work",
            "content": "2."
        },
        {
            "title": "Instruction Finetuning",
            "content": "Instruction finetuning involves training models to follow specific instructions, often resulting in improved generalization and the ability to perform wider range of tasks (Wei et al., 2022). There are already numerous open-domain instruction datasets and finetuned models. NATURAL INSTRUCTIONS is curated from samples of different NLP datasets and the crowdsourcing instructions used to annotate them. FLAN 2021 (Wei et al., 2022) and 2022 (Longpre et al., 2023) provide extensive publicly available set of tasks and methods for instruction tuning. FLAN models are trained on the collection and exhibits strong generalization performance on variety tasks. The InstructGPT (Ouyang et al., 2022) model benefits in part from substantial dataset of prompts gathered through various synthetic data augmentation methods. However, this dataset is not publicly accessible. SUPERNATURAL INSTRUCTIONS (Wang et al., 2022) is established as benchmark of 1,616 diverse NLP tasks along with expert-written instructions. The collection covers 76 distinct task types, providing rigorous benchmarking of generalization performance of LLMs. The corresponding trained model Tk-INSTRUCT outperforms InstructGPT despite being an order of magnitude smaller. Self-Instruct (Wang et al., 2023b) provides new approach for instruction fine-tuning. It involves bootstrapping off the generations of pre-trained language models to improve the instruction-following performance of themselves. After the great success of ChatGPT (OpenAI, 2022), many efforts have been made to use data generated by ChatGPT to train their own large language models (LLMs). Alpaca (Taori et al., 2023) is finetuned from LLaMA (Touvron et al., 2023) on 52k instruction-following instances generated by Text-davinci-003. Compared to opendomain instruction datasets, instruction datasets in the biomedical field are relatively scarce. MedAlpaca (Han et al., 2023) utilizes data collection of 160k entries from reformatted medical NLP task and crawl of internet resources. ChatDoctor (Li et al., 2023) is trained using 100k patient-doctor dialogues from an online medical consultation platform. Similar to Alpaca, AlpaCare (Zhang et al., 2024) uses medical related instruction demonstrations generated by ChatGPT to train on LLaMA. By prompting ChatGPT to conduct self-chat, Baize (Xu et al., 2023) collect the dialogues to train specialized model for healthcare. Additionally, BioMistral (Labrak et al., 2024) and PMC-LLaMA (Wu et al., 2023) use medical-related corpora to pretrain their respective base models, followed by finetuning with an instruction dataset. All these models are only finetuned on limited number of tasks, making them prone to failure when confronted with new tasks. Our dataset focuses on biomedical domain, offering comprehensive instructionfollowing demonstrates spanning 133 tasks in 12 task categories, facilitating LLMs generalizing to unseen tasks. categories of tasks, where each may have several sub-categories. The categories are as follows:"
        },
        {
            "title": "2.2 Biomedical Benchmarks",
            "content": "Biomedical workshops, such as BioNLP (Kim et al., 2009) and BioCreative (Hirschman et al., 2005), often employ task-specific benchmark datasets. With the rise of LLMs, there are higher expectations for the comprehensive capabilities of medical models. As result, evaluating them on single task is no longer sufficient. BLUE (Biomedical Language Understanding Evaluation) (Peng et al., 2019) took the first step by constructing benchmark that includes 10 datasets covering 5 different task types. Building on this foundation, BLURB (Biomedical Language Understanding and Reasoning Benchmark) (Gu et al., 2021) expanded the dataset to 13, encompassing 7 different types. Instruction datasets exist for few and zero-shot evaluations. Agrawal et al. (2022) introduce 3 datasets for clinical information extraction by reannotating the CASI datset. SUPER-NATURAL INSTRUCTION (Wang et al., 2022) delivers 1600+ open-domain NLP tasks, among which 30 tasks are related to medicine and healthcare. Tailored for biomedicine, BoX (Parmar et al., 2022) provides 32 tasks in the scope of 9 categories. BigBIO (Fries et al., 2022) focuses on the process of constructing metadatasets, providing unified schema for 126 existing datasets across various tasks and offering tools for building new datasets. However, it does not contain instructions and the datasets are not in text generation format. Our dataset offers an extensive instruction benchmark including 32 tasks representing comprehensive evaluation of LLM performance in biomedical fields."
        },
        {
            "title": "3 MEDINST: Meta Dataset of\nBiomedical Instructions",
            "content": "We curate MEDINST by collecting 98 welladopted biomedical datasets from 12 task categories and reformulating them into 133 tasks. All tasks are regarded as text generation task and the data are formatted to instruction-following samples. The instructions are human annotated and tailored for each dataset/task. Figure 1 (a) depicts visualization of the dataset composition of MEDINST."
        },
        {
            "title": "3.1 Tasks",
            "content": "Figure 1 (b) shows the number of samples included in each task categories. We adopted 12 Named Entity Recognition (NER) NER is task in natural language processing that involves identifying and classifying key information entities. In the biomedical field, NER involves detecting and extract key entities such as diseases, drugs, genes, and other relevant biological terms within biomedical texts. In MEDINST, 56 NER datasets are collected, including the most commonly used BC5CDR (Li et al., 2016), JNLPBA (Collier et al., 2004), LINNAEUS (Gerner et al., 2010), etc. We have created unified instruction template for the NER task and made variations based on the specific requirements of each dataset. We have divided the NER task into two sub-categories, differing in output format. Sub-category 1 requires labeling each word in the input text using the BIO format, while Sub-category 2 requires directly outputting all detected entities that meet the criteria. In Subcategory 1, the input for each instance is single sentence, whereas in Sub-category 2, the input is an entire passage. This adds diversity to the NER task and creates different levels of difficulty, thereby enhancing the models stability in handling various output requirements and understanding longer texts. Named Entity Disambiguation (NED) The NED task involves determining the correct identity of named entities in text by linking them to specific entry in knowledge base. Most of the NER datasets contain annotations for entity disambiguation. The NED task has also been repurposed into two difficulty levels. The AskAPatient and TwADR datasets (Limsopatham and Collier, 2016) are used to create simpler tasks, where the input includes specified biomedical entity and its context, and the requirement is to output its identifiers in the corresponding database. Other dataset such as BioRelEx (Khachatrian et al., 2019), CPI (Döring et al., 2020), MedMentions (Mohan and Li, 2019), etc. have been reformatted into more challenging tasks, requiring the extraction of relevant biomedical entities from the given text and providing the corresponding identifiers for each entity. In additional, MeDAL dataset (Wen et al., 2020) has also been included in the NED task, which is medical text dataset curated for abbreviation disambiguation. We include total of 23 datasets in the NED task category. (a) Treemap. (b) Number of samples. Figure 1: MEDINST overview."
        },
        {
            "title": "TRANSL SUM TEXTPAIRCLASS ALL",
            "content": "t t #"
        },
        {
            "title": "MEDINST",
            "content": "MEDINST"
        },
        {
            "title": "Train\nDev\nTest",
            "content": "# Instruction/Task 56 30 37 43 19 13 49 24 11 9 21 9 23 21 10 12 19 9 2 19 13 8 10 10 6 9 13 10 2 11 8 2 7 10 7 1 9 6 9 8 5 8 5 5 3 3 7 1 1 6 - 3 5 4 5 3 2 2 5 3 1 1 2 - 3 2 1 1 1 - 1 2 1 - - 1 - - 1 163 88 87 131 64 32 133 Table 2: Dataset statistics across various categories. Coreference Resolution (COREF) COREF is the task of determining which words or phrases in text refer to the same entity. We used 13 datasets for this task category, most of which come from the BioNLP Shared Task. In addition, the MLEE (Pyysalo et al., 2012) and PDR (Kim et al., 2019) datasets have also been included. Question-Answering (QA) Multiple types of QA are collected, including yes/no, yes/no/maybe, In this category, 10 factoid, multi-choice, etc. datasets are employed and reformatted. For multiple-choice QA, we write out the full options in the output rather than assigning letters or numbers to each option. Textual Entailment (TE) Determining whether two texts contradict each other and whether statement aligns with the facts is crucial in the medical field. In this category, we re-format 6 factchecking datasets, FEVER (Thorne et al., 2018), HealthVer (Sarrouti et al., 2021), SciFact (Wadden Figure 2: Instruction and instance example. Relation Extraction (RE) RE involves identifying and categorizing the relationships between entities within given text. We utilize 24 datasets for RE task, including AnEM (Ohta et al., 2012), BioNLP 2011 REL (Pyysalo et al., 2011), etc. We simplified the task by listing all the possible relation types in the instruction for each dataset. The language model is prompted to extract all possible triples from the input text. et al., 2020), PubHealth (Kotonya and Toni, 2020), etc., into claim-evidence pairs. These datasets range from the general scientific domain to specific medical domains, such as COVID-19. Moreover, MEDIQA-RQE (Ben Abacha et al., 2019) is incorporated as question entailment task, i.e. determine whether the meaning of one question can be inferred from another question. As classic task in the TE category, the premise-hypothesis entailment task is represented by the SciTail dataset (Khot et al., 2018). Text Classification (TXTCLASS) The text classification task involves assigning predefined categories or labels to given piece of text based on its content. Although the input and output formats for text classification tasks are relatively fixed, the definitions and objectives of each task are highly diverse. Therefore, it is challenging to use template to standardize this type of instruction. To ensure the quality of the instructions, each task within this category is entirely crafted manually. We collect 5 datasets, SciCite (Cohan et al., 2019), Hallmarks-of-Cancer (Baker et al., 2016), BC7LitCovid (Chen et al., 2022), MedDialog (Zeng et al., 2020) and GEOKhoj-v12, for this category. Semantic Similarity (STS) The Semantic Similarity task aims at measuring how similar the meanings of two pieces of text are to each other. Originally regression task with similarity scores as outputs, we have redefined it as classification task by categorizing the similarity scores of all datasets into six integer levels from 0 to 5, where 0 indicates completely unrelated and 5 indicates highly similar. This task category includes 7 datasets, e.g. Bio-SimVerb, Bio-SimLex (Chiu et al., 2018) BIOSSES (Sogancıoglu et al., 2017), MQP (McCreery et al., 2020), etc. Event Extraction (EE) EE task requires identifying and categorizing events, such as biological processes or interactions, within biomedical texts. The Event Extraction (EE) task is typically complex, with events in documents often containing nested structures. To format the EE task as text generation task, we simplify it according to the BioNLP 2009 Core Event Detection subtask (Kim et al., 2009). We only detect events within given range of types and their primary arguments. Note that primary arguments must be biomedical entity within the text; we do not consider cases where primary arguments refer to another event. Translation (TRANSL) We have included the MuchMore (Buitelaar et al., 2003), ParaMed (Liu and Huang, 2021) and SciELO (Soares et al., 2018) datasets translated from German, Chinese, and Spanish into English. Text Pair Classification (TEXTPAIRCLASS) For this category, we employ sentiment analysis dataset, the Medical-Data3 , which analyzing the sentiment in text where drug is mentioned to determine whether the sentiment towards the drug is positive, negative, or neutral. Summarization (SUM) Summarization is also crucial for the application of LLMs in the biomedical field. In this category, we use the MeQSum (Ben Abacha and Demner-Fushman, 2019) and Multi-XScience (Lu et al., 2020) datasets. MeQSum presents patient questions, often in the form of lengthy texts, and the task requires capturing the main concern of these questions and providing concise rewrite. Multi-XScience is multidocument summarization task, which requires generating related work section for given article based on its abstract and the abstracts of some cited references."
        },
        {
            "title": "The complete dataset collection details are listed",
            "content": "in the Appendix E. 3."
        },
        {
            "title": "Instruction Construction",
            "content": "All instructions are written according to unified schema to ensure their quality. An instruction includes the following elements: Input Explanation The instruction first specifies the structure of the input. For example, for NER, the given input is typically sentence or passage; for QA tasks, the input can be question alone, question with context, or question with context and options. We describe the elements included in the input for each datasets task individually, avoiding the use of generalized descriptions. Task Definition The instructions include an explanation of the task and the specific actions the model needs to perform. The task definition is tailored to the content of each dataset and specifies any optional parameters. For example, the definition for the SciCite (Cohan et al., 2019) task is 2https://github.com/ElucidataInc/ GEOKhoj-datasets/tree/main/geokhoj_v1 3https://www.kaggle.com/datasets/arbazkhan971/ analyticvidhyadatasetsentiment \"Classify the intent of the citation within this context. Intents are: [background, method, result].\" avoiding vague instructions like \"Classify the text into [background, method, result].\" Output Format Here we specify the format of the output. In MedINST, we adopt formats corresponding to the complexity of the output content. For open text generation, the output is generally plain text; for classification tasks, multiple labels are separated by commas; for tasks like NER, where the output biomedical entities may contain various special characters, we enclose them in square brackets. For complex outputs, the instruction will provide template example of the output format. After drafting instructions according to the abovementioned elements, we further proofread them to make them more concise and aligned with natural human instructions, avoiding rigid, structured descriptions. Appendix presents the examples of instructions."
        },
        {
            "title": "3.3 MEDINST32 Benchmark Construction",
            "content": "Using the MEDINST as meta dataset, we carefully curate MEDINST32, challenging benchmark that covers 32 tasks with different difficulties to evaluate LLMs performance across various medical-related tasks comprehensively. Unlike previous works, the tasks selected for MEDINST32 encompass different difficulty levels, including knowledge difficulty and instruction difficulty. Specifically, knowledge difficulty assesses the models amount of biomedical knowledge, such as understanding levels of biomedical terms and their relationships, while instruction difficulty evaluates the models understanding and adherence to instructions. We divide difficulty into four categories and choose tasks from simplest (e.g., acronym completion) to hardest (e.g., RE, EE). Moreover, two positive examples are offered for each tasks. See more details in Appendix A."
        },
        {
            "title": "4.1 Setup",
            "content": "Problem Formulation. We combine the training sets to train multi-task biomedical models. Given an instruction Instt for task t, and the dataset (Xt, Yt), multi-task models learns map Mt : (Instt, x) y, where (x, y) (Xt, Yt). After learning set of maps M1, M2, ..., MT , the multi-task models can generalize to unseen tasks {T + 1, + 2, ..., + } and approximate the maps Mi, where Mi : (Insti, x) y, (x, y) (Xi, Yi). Training Data. Our goal is to test the generalization ability of LLMs on unseen tasks after instruction tuning multiple biomedical tasks. Once we have selected the 32 tasks in MEDINST32 (Sec. 3.3), we use the training set of the remaining tasks from MEDINST for multi-task fine-tuning. Since the MEDINST training set is too large and large number of training instances per task do not help generalization in instruction finetuning (Wang et al., 2022), we sample 100K samples to train our multitask biomedical LLMs, denoted as MI32. We select an equal number of samples from each task category to ensure balance across all tasks. Evaluation setup. Following Wang et al. (2022), we limit the test set for large size datasets aiming at efficient evaluation. We observe that models not fine-tuned on MedINST sometimes struggled to output according to the instructions, posing challenges for post-processing and metric calculation. To ensure fair comparison, we use fewshot prompts for baseline models during evaluation. Each test task is provided with two examples to help zero-shot models output in the standard format. Appendix details the implementation of training and evaluation. the Model. We fine-tune instruction-tuned LLaMA-3 (8B; Meta, 2024) and MMed-LLaMA-3 (8B; Qiu et al., 2024) on the aforementioned MI32 training set and derive LLaMA3-MI32 and MMedL3-MI32, respectively. Additionally, we fine-tune LLaMA-3 on the 100K samples from MEDINST, where the training sets of the datasets in MI32 are exposed to the model, to produce LLaMA3-MI, as an oracle model. Baselines. As direct comparison, we compare our LLaMA3-MI32 fine-tuned on MI32 with its base version, LLaMA3. Since MMed-LLaMA-3 is foundation model that has not been instruction fine-tuned, to make fair comparison, we use MMed-LLaMA-3-EnIns (Qiu et al., 2024), which is fine-tuned on the English medical instruction dataset from PMC-LLaMA (Wu et al., 2023). We denote it as MMedL3-EnIns. In addition, we compare BioMistral, an open-source LLM further pretrained on PubMed Central utilizing the instruction fine-tuned version of Mistral-7B (Jiang et al., 2023) and GPT-4o, an advanced variant of GPT4, excels in the biomedical domain with enhanced capabilities for understanding and generating complex medical and scientific texts. Metrics. Inspired by BLURB (Gu et al., 2021), we select appropriate metrics for each task in including Rouge-L, Entity F1 MEDINST32, (Entity-level F1), Label F1 (Label-level F1), MSE (Mean Squared Error) and EM (Exact Match). Entity-level F1 measures the overlap between the entities detected by the models and the ground truth, which is calculated by each data sample. Labellevel F1 is calculated from the entire dataset to measure the similarity between the models predictions and the labels."
        },
        {
            "title": "4.2 Results",
            "content": "Table 3 presents the evaluation results of our models and baselines on MEDINST32. As an oracle model, MMedL3-MI demonstrated excellent performance across various difficulty levels, outperforming GPT-4o in 25 tasks. This highlights the significant impact of the MEDINST dataset in enhancing the overall performance of models on biomedical tasks. The two zero-shot models, LLaMA3-MI32 and MMedL3-MI32, showed significant generalization improvements over their base models in most unseen tasks. They respectively outperformed GPT4o in 15 and 13 tasks. However, surprisingly, MMedL3-MI32, which used MMed-LLaMA-3 (further pretrained on biomedical corpora) as its base model, lagged behind LLaMA3-MI32 in 22 tasks. This indicates that using further pretraining to specialize general LLM to the biomedical domain may not be as effective as instruction finetuning, especially considering the substantial computational resources required for pretraining. This also underscores the necessity of building comprehensive biomedical instruction meta-dataset. MMedL3-EnIns was fine-tuned on 500K medical question-answering data, which includes training data from MedQA and PubMedQA that appeared in MEDINST32. Despite using few-shot prompting, its performance on MEDINST32 was still unsatisfactory. It even significantly lagged behind in QA tasks, especially in MedQA, achieving only 15.40 accuracy. This highlights the necessity of reformulating tasks to improve model generalization capabilities: training models to output in single format alone increases the risk of overfitting. (a) Performance with varying training data sizes. (b) Performance with varying model parameter sizes. Figure 3: Training sample and model parameter scale analysis."
        },
        {
            "title": "4.3 Ablation Analysis",
            "content": "We design experiments to explore the impact of the number of training samples and model parameters on finetuning performance. We employ the same strategy to sample 5K and 50K instances from the MI32 training set for training two additional MMedL3-MI32 models for comparison. Additionally, we trained both 4B and 14B versions of Phi-3 using the 50K dataset. In Figure 3, we calculate the average Rouge-L score for each task category to measure the performance of the models. In (a), it can be seen that as the number of training samples increases, the models overall performance improves. However, performance deteriorates with increased sample size in tasks such as summarization (SUM) and event extraction (EE). This is because as sampling expands, the proportion of smaller datasets decreases, leading to data imbalance, which causes uneven learning progress across different tasks. Part (b) demonstrates unexpected results regarding the scale of model parameters. Phi-3-14B performs less than the 4B version in three core tasks for the biomedical field: NER, RE, and EE. possible reason is that larger models require more data to be fully optimized and achieve generalization performance on unseen biomedical tasks. Specialized Category Dataset Difficulty Level Metric Model LLaMA3 BioMistral MMEDL3-EnIns GPT-4o LLaMA3-MI32 MMEDL3-MI32 LLaMA3-MI (Few Shot) Ours (Zero Shot) NCBI-disease BC5CDR AnEM BioNLP-2009 BioNLP-2011-GE BioNLP-2011-ID BioNLP-2011-REL BioNLP-2013-CG BioNLP-2013-GE BioNLP-2013-GRO BioNLP-2013-PC BioRED tmVar-v3 BioASQ-Task-B-yesno PubMedQA-labeled MedQA SciFact ManConCorpus CoVERt NER QA TE TXTCLASS Hallmarks-of-Cancer MedDialog NED RE COREF SUM EE STS TRANSL MeDAL tmVar-v3-NED AnEM-RE BC5CDR-RE BioInfer-RE AnEM-COREF MLEE-COREF Multi-XScience MLEE-EE BIOSSES ParaMed 2 2 3 2 2 3 2 3 2 4 3 3 3 1 2 2 2 2 1 2 1 2 4 4 4 1 1 2 4 1 2 Label-F1 Label-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F Label-F1 Label-F1 EM Label-F1 Label-F1 Label-F1 Entity-F1 Label-F1 EM Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F1 Entity-F Rouge-L Entity-F1 MSE 51.67 58.68 8.20 30.33 29.60 32.83 30.14 24.46 16.98 10.34 31.96 29.38 16.34 91.62 50.85 49.25 42.09 66.66 82. 45.33 91.34 21.6 0.18 2.56 4.28 18.74 34.52 54.17 13.28 0. 2.05 Rouge-L 47.51 24.00 33.86 3.66 22.24 20.97 18.45 22.73 10.49 15.49 4.14 19.45 16.45 8.96 67.57 23.73 24.51 36.33 29.92 47. 54.77 86.02 15.90 0.05 0.00 6.27 9.73 14.29 26.55 11.61 0. 4.15 50.49 30.59 28.77 1.72 19.71 14.40 21.19 20.26 8.63 13.29 2.91 19.57 16.33 0.39 91.82 48.28 15.40 33.69 51.83 55.87 11.93 56. 17.00 0.00 5.13 3.34 8.86 21.43 25.66 10.36 0.09 4. 46.49 47.57 75.11 37.44 57.83 57.43 68.59 59.01 57.59 43.74 37.79 68.75 60.73 42.08 93.52 56.11 81.93 92.61 60.09 93.76 42.40 98.77 59.40 7. 25.64 9.46 17.49 82.20 79.97 12.78 9.88 0.6 63. 78.55 81.28 32.03 76.06 76.29 51.80 75.93 56.36 71.59 12.48 62.94 74.01 58.46 93.10 53.81 47.68 85.85 68.20 91.15 44.01 96.72 28.90 2.84 0.20 14.21 28. 100.00 99.12 11.61 30.47 1.05 49.01 78.20 73.57 31.38 78.61 79.89 50.80 78.66 51.61 71.25 12.86 61.38 72.45 56. 86.19 53.65 45.72 84.14 68.57 93.49 32.65 77.67 30.00 0.78 1.54 13.69 26.23 100.00 98. 11.57 28.61 2.15 49.65 84.61 87.39 49.44 80.74 80.39 76.26 80.41 72.32 71.32 35.13 82.05 78.76 63.22 93.87 59.94 53. 95.06 69.14 96.93 45.84 100.00 36.60 1.10 16.24 27.93 32.83 100.00 95.72 14. 27.48 1.20 59.32 Table 3: Test results of various models on MEDINST32. indicates that the training sets of LLaMA3-MI includes the corresponding training sets of the datasets used by MEDINST32, whereas other models have not seen the MEDINST32 dataset. represents that lower score is better, while for other metrics, higher score is better. The best and second-best results for each row are highlighted in bold and underlined, respectively. For the baselines, we use few-shot prompt, providing two examples in the instruction. For the fine-tuned models, we use zero-shot prompt."
        },
        {
            "title": "MMLU",
            "content": "An CK CB CM MG PM Avg. BioMistral MMedL3 MMedL3-EnIns LLaMA3 MMedL3-MI (Ours) LLaMA3-MI (Ours) 48.89 65.19 68.15 67.41 64.44 68.15 66.42 70.19 64.91 76.60 67.92 75.47 63.19 72.22 71.52 80.56 71.53 75.00 58.38 55.49 59.53 67.63 58.96 67.63 70.00 74.00 76.00 82.00 74.00 83. 58.46 66.91 72.79 72.06 66.54 77.21 60.88 67.03 68.32 73.92 66.76 74.38 Table 4: Multiple-choice accuracy evaluation on MMLUMedicine, subset of MMLU benchmark. The subjects used are anatomy (An), clinical knowledge (CK), college biology (CB), college medicine (CM), medical genetics (MG) and professional medicine (PM). tasks such as Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction (EE) in the biomedical field are more susceptible to overfitting on small data sets compared to more general tasks like summarization (SUM)."
        },
        {
            "title": "4.4 Evaluation on Public English Benchmarks",
            "content": "The Massive Multitask Language Understanding (MMLU; Hendrycks et al., 2021) is benchmark that evaluates language models across various QA tasks and subjects. We train MMedL3-MI using the same 100K dataset that was used to train LLaMA3-MI. The models are tested on 6 medicalrelated subtasks of MMLU. Table 4 exhibits the result. As seen, LLaMA3-MI and MMedL3-MI perform similarly to the baseline model on MMLUMedicine. Additionally, note that LLaMA3-MI and MMedL3-MI are multitask models in the biomedical field, capable of handling various other, more challenging biomedical tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce an instruction metadataset MEDINST comprising 133 biomedical tasks across 12 task categories and challenging benchmark MEDINST32 for evaluating multitask biomedical models. Through various experiments, we train multiple biomedical models and demonstrate their strong generalization performance on biomedical tasks using our dataset. Due to resource constraints, we trained only on small subset and 8B models. Using the full dataset and larger models may lead to further improvements, which is left for future work. Our work lays the foundation for developing better-performing biomedical LLMs."
        },
        {
            "title": "Limitations",
            "content": "We identify our limitations as follows. First, due to computational resource constraints, we conducted our experiments with limited data and model sizes. We used the LoRA technique to finetune our model, which might limit the learning outcomes. Full-parameter finetuning could potentially yield better results. In future work, we will continue to explore ways to further enhance the performance of LLMs on biomedical-related tasks. Currently, the MedINST dataset only includes single-turn dialogues, which may limit the models ability to generalize to multi-turn dialogue tasks. Therefore, in the future, we plan to incorporate multi-turn instruction samples. Additionally, the current dataset is primarily in English, with other languages featured in the TRANSL tasks, so another direction for future work is to continue expanding the multilingual data."
        },
        {
            "title": "References",
            "content": "Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022. Large Language Models are Few-Shot Clinical Information Extractors. Preprint, arxiv:2205.12689. Sultan Alrowili and Vijay Shanker. 2021. BioMTransformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA. In Proceedings of the 20th Workshop on Biomedical Language Processing, pages 221227, Online. Association for Computational Linguistics. Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Högberg, Ulla Stenius, and Anna Korhonen. 2016. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics (Oxford, England), 32(3):432440. Asma Ben Abacha and Dina Demner-Fushman. 2019. On the Summarization of Consumer Health Questions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 22282234, Florence, Italy. Association for Computational Linguistics. Asma Ben Abacha, Chaitanya Shivade, and Dina Demner-Fushman. 2019. Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 370379, Florence, Italy. Association for Computational Linguistics. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. Preprint, arxiv:2005.14165. Paul Buitelaar, Thierry Declerck, Bogdan Sacaleanu, Špela Vintar, Diana Raileanu, and Claudia Crispi. 2003. Multi-Layered, XML-Based Approach to the Integration of Linguistic and Semantic Annotations. Qingyu Chen, Alexis Allot, Robert Leaman, Rezarta Islamaj, Jingcheng Du, Li Fang, Kai Wang, Shuo Xu, Yuefu Zhang, Parsa Bagherzadeh, Sabine Bergler, Aakash Bhatnagar, Nidhir Bhavsar, Yung-Chun Chang, Sheng-Jie Lin, Wentai Tang, Hongtong Zhang, Ilija Tavchioski, Senja Pollak, Shubo Tian, Jinfeng Zhang, Yulia Otmakhova, Antonio Jimeno Yepes, Hang Dong, Honghan Wu, Richard Dufour, Yanis Labrak, Niladri Chatterjee, Kushagri Tandon, Fréjus A. A. Laleye, Loïc Rakotoson, Emmanuele Chersoni, Jinghang Gu, Annemarie Friedrich, Subhash Chandra Pujari, Mariia Chizhikova, Naveen Sivadasan, Saipradeep Vg, and Zhiyong Lu. 2022. Multi-label classification for biomedical literature: An overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. Database: The Journal of Biological Databases and Curation, 2022:baac069. Billy Chiu, Sampo Pyysalo, Ivan Vulic, and Anna Korhonen. 2018. Bio-SimVerb and Bio-SimLex: Wide-coverage evaluation sets of word similarity in biomedicine. BMC Bioinformatics, 19(1):33. Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 35863596, Minneapolis, Minnesota. Association for Computational Linguistics. Nigel Collier, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Jin-Dong Kim. 2004. Introduction to the Bio-entity Recognition Task at JNLPBA. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications (NLPBA/BioNLP), pages 7378, Geneva, Switzerland. COLING. Kersten Döring, Ammar Qaseem, Michael Becer, Jianyu Li, Pankaj Mishra, Mingjie Gao, Pascal Kirchner, Florian Sauter, Kiran K. Telukunta, Aurélien F. A. Moumbock, Philippe Thomas, and Stefan Günther. 2020. Automated recognition of functional compound-protein relationships in literature. PloS One, 15(3):e0220925. Jason Alan Fries, Leon Weber, Natasha Seelam, Gabriel Altay, Debajyoti Datta, Samuele Garda, Myungsun Kang, Ruisi Su, Wojciech Kusa, Samuel Cahyawijaya, Fabio Barth, Simon Ott, Matthias Samwald, Stephen Bach, Stella Biderman, Mario Sänger, Bo Wang, Alison Callahan, Daniel León Periñán, Théo Gigant, Patrick Haller, Jenny Chim, Jose David Posada, John Michael Giorgi, Karthik Rangasai Sivaraman, Marc Pàmies, Marianna Nezhurina, Robert Martin, Michael Cullan, Moritz Freidank, Nathan Dahlberg, Shubhanshu Mishra, Shamik Bose, Nicholas Michio Broad, Yanis Labrak, Shlok S. Deshmukh, Sid Kiblawi, Ayush Singh, Minh Chien Vu, Trishala Neeraj, Jonas Golde, Albert Villanova del Moral, and Benjamin Beilharz. 2022. BigBIO: Framework for Data-Centric Biomedical Natural Language Processing. Preprint, arxiv:2206.15076. Martin Gerner, Goran Nenadic, and Casey M. Bergman. 2010. LINNAEUS: species name identification system for biomedical literature. BMC Bioinformatics, 11(1):85. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. Preprint, arxiv:2007.15779. Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno K. Bressem. 2023. MedAlpaca An Open-Source Collection of Medical Conversational AI Models and Training Data. Preprint, arxiv:2304.08247. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Lynette Hirschman, Alexander Yeh, Christian Blaschke, and Alfonso Valencia. 2005. Overview of BioCreAtIvE: Critical assessment of information extraction for biology. BMC Bioinformatics, 6(1):S1. Kexin Huang, Abhishek Singh, Sitong Chen, Edward T. Moseley, Chih-ying Deng, Naomi George, and Charlotta Lindvall. 2019. Clinical XLNet: Modeling Sequential Clinical Notes and Predicting Prolonged Mechanical Ventilation. Preprint, arxiv:1912.11975. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. Preprint, arxiv:2310.06825. Hrant Khachatrian, Lilit Nersisyan, Karen Hambardzumyan, Tigran Galstyan, Anna Hakobyan, Arsen Arakelyan, Andrey Rzhetsky, and Aram Galstyan. 2019. BioRelEx 1.0: Biological Relation Extraction In Proceedings of the 18th BioNLP Benchmark. Workshop and Shared Task, pages 176190, Florence, Italy. Association for Computational Linguistics. Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTaiL: Textual Entailment Dataset from Science Question Answering. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). Baeksoo Kim, Wonjun Choi, and Hyunju Lee. 2019. corpus of plantdisease relations in the biomedical domain. PLoS ONE, 14(8):e0221582. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Junichi Tsujii. 2009. Overview of BioNLP09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 19, Boulder, Colorado. Association for Computational Linguistics. Neema Kotonya and Francesca Toni. 2020. Explainable Automated Fact-Checking for Public Health Claims. Preprint, arxiv:2010.09926. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, PierreAntoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. BioMistral: Collection of OpenSource Pretrained Large Language Models for Medical Domains. Preprint, arxiv:2402.10373. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016. BioCreative CDR task corpus: resource for chemical disease relation extraction. Database: The Journal of Biological Databases and Curation, 2016:baw068. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. 2023. ChatDoctor: Medical Chat Model Fine-Tuned on Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. Nut Limsopatham and Nigel Collier. 2016. Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10141023, Berlin, Germany. Association for Computational Linguistics. Boxiang Liu and Liang Huang. 2021. ParaMed: parallel corpus for EnglishChinese translation in the biomedical domain. BMC Medical Informatics and Decision Making, 21(1):258. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, Prompt, and Predict: Systematic Survey of Prompting Methods in Natural Language Processing. Preprint, arxiv:2107.13586. Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, and Grégoire Altan-Bonnet. 2021. SciFive: textto-text transformer model for biomedical literature. Preprint, arxiv:2106.03598. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. Preprint, arxiv:2301.13688. Yao Lu, Yue Dong, and Laurent Charlin. 2020. MultiXScience: Large-scale Dataset for Extreme Multidocument Summarization of Scientific Articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 80688074, Online. Association for Computational Linguistics. Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier Amatriain. 2020. Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs. Preprint, arxiv:2008.13546. Meta. 2024. Introducing Meta Llama 3: The most capable openly available LLM to date. Sunil Mohan and Donghui Li. 2019. MedMentions: Large Biomedical Corpus Annotated with UMLS Concepts. Preprint, arxiv:1902.09476. Tomoko Ohta, Sampo Pyysalo, Junichi Tsujii, and Sophia Ananiadou. 2012. Open-domain Anatomical Entity Mention Detection. In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse, pages 2736, Jeju Island, Korea. Association for Computational Linguistics. OpenAI. 2022. Introducing chatgpt. OpenAI. 2024. GPT-4 Technical Report. Preprint, arxiv:2303.08774. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arxiv:2203.02155. Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, M. Hassan Murad, and Chitta Baral. 2022. InBoXBART: Get Instructions into Biomedical MultiTask Learning. Preprint, arxiv:2204.07600. Yifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 5865, Florence, Italy. Association for Computational Linguistics. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, HanCheol Cho, Junichi Tsujii, and Sophia Ananiadou. 2012. Event extraction across multiple levels of biological organization. Bioinformatics, 28(18):i575 i581. Sampo Pyysalo, Tomoko Ohta, and Junichi Tsujii. 2011. Overview of the Entity Relations (REL) supporting task of BioNLP Shared Task 2011. In Proceedings of BioNLP Shared Task 2011 Workshop, pages 8388, Portland, Oregon, USA. Association for Computational Linguistics. Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. Towards building multilingual language model for medicine. Preprint, arXiv:2402.13963. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multitask Prompted Training Enables Zero-Shot Task Generalization. Preprint, arxiv:2110.08207. Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, and Dina Demner-Fushman. 2021. Evidence-based In FindFact-Checking of Health-related Claims. ings of the Association for Computational Linguistics: EMNLP 2021, pages 34993512, Punta Cana, Dominican Republic. Association for Computational Linguistics. Felipe Soares, Viviane Moreira, and Karin Becker. 2018. Large Parallel Corpus of Full-Text Scientific Articles. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Gizem Sogancıoglu, Hakime Öztürk, and Arzucan Özgür. 2017. BIOSSES: semantic sentence similarity estimation system for the biomedical domain. Bioinformatics, 33(14):i49i58. Processing Workshop, pages 130135, Online. Association for Computational Linguistics. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. PMC-LLaMA: Towards Building Open-source Language Models for Medicine. Preprint, arxiv:2304.14454. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data. Preprint, arxiv:2304.01196. Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, and Pengtao Xie. 2020. MedDialog: Large-scale Medical Dialogue Datasets. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 92419250, Online. Association for Computational Linguistics. Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang and Linda Ruth Petzold. Chen, Zekun Li, 2024. AlpaCare:Instruction-tuned Large Language Models for Medical Application. Preprint, arxiv:2310.14558. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model."
        },
        {
            "title": "James",
            "content": "Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: large-scale dataset for Fact Extraction and VERification. https://arxiv.org/abs/1803.05355v3. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. Preprint, arxiv:2302.13971. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or Fiction: Verifying Scientific Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 75347550, Online. Association for Computational Linguistics. Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, and Jie fu. 2023a. Pretrained Language Models in Biomedical Domain: Systematic Survey. Preprint, arxiv:2110.05006. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-Instruct: Aligning Language Models with Self-Generated Instructions. Preprint, arxiv:2212.10560. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. Preprint, arxiv:2204.07705. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned Language Models Are Zero-Shot Learners. Preprint, arxiv:2109.01652. Zhi Wen, Xing Han Lu, and Siva Reddy. 2020. MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining. In Proceedings of the 3rd Clinical Natural Language"
        },
        {
            "title": "A Instruction Benchmark Construction",
            "content": "To comprehensively evaluate the models performance across various medical-related tasks, we selected 32 tasks from each task category in the MEDINST dataset to establish new biomedical benchmark, MEDINST32. The tasks selected for benchmarking encompass different levels of difficulty. This includes two aspects: knowledge difficulty and instruction difficulty. Knowledge difficulty assesses the amount of biomedical knowledge the model possesses, such as understanding categories of biomedical terms and their relationships. For basic-level assessment, we chose tasks like acronym completion (MeDAL). Intermediatelevel tasks include various NER, QA, TE, and TXTCLASS tasks. Finally, we included more challenging tasks like RE, EE, and tasks in NED that involve annotating identifiers. Instruction difficulty evaluates the models understanding and adherence to instructions. This dimension was not considered in previous benchmark datasets. For example, in multichoice QA tasks, previous works often labeled each option as A, B, C, etc., and the model only needed to respond with the corresponding label. In our QA task construction, we require the model to output the selected option as it is, which increases the task difficulty and reduces the chance of the model bypassing with simple letter responses. Additionally, we construct different instructions for similar tasks. For instance, in NER tasks, we developed two types of instructions: one requiring the model to repeat each word in the text in BIO format and label them one by one, and the other asking the model to directly extract all biomedical entity mentions and annotate their categories. For each task in MEDINST32, we provide two positive examples. For tasks that have training set, we select two examples from their training set. If task does not have training set, we find the most similar task from all the test set tasks in MEDINST and select from there. During selection, we strive to ensure that the two examples are diverse in content. For instance, in classification tasks, we choose examples with different labels. We remove all the datasets used in MEDINST32 from the MEDINST training set to create the training set for MEDINST32. We performed random sampling on portion of tasks with abundant data resources to control the number of test data in each category to be roughly consistent. This helps to reduce the computational resource consumption for evaluation. The sample sizes are shown in Table 5. For other datasets, we use the entire test set data. Dataset Name NCBI-disease BC5CDR BioNLP-2011-GE tmVar-v3 MeDAL ParaMed Multi-XScience Sample Size 100 100 100 100 1000 200 200 Table 5: Sampling sizes for evaluation. Overall, we provide more comprehensive and challenging biomedical instruction benchmark compared to previous works."
        },
        {
            "title": "B Instruction Examples",
            "content": "Table presents the instruction examples for each task categories. Each instruction contains three parts: input explanation, task definition, and output format, which clearly tell the LLM how to complete the task. For each task within category, the instruction can vary, thus requiring manual composition. However, for categories such as NED, RE, and EE tasks, the main body of the instruction is generic. We can efficiently edit the instruction by modifying some variable fields based on the metadata of each dataset, and these variable fields are highlighted in blue."
        },
        {
            "title": "C Implementation Details",
            "content": "Training For the baseline models, we used the LLaMA-3-8B-Instruct 4 and MMed-LLaMA-3-8B 5 models available on Hugging Face. Due to limited computational resources, we employed LowRank Adaptation (LoRA) for parameter-efficient fine-tuning (PEFT). The LoRA rank was set to 8, targeting all linear layers, including q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj. The learning rate was set at 1.0e4, with batch size of 4 and gradient accumulation steps of 4. We used cosine learning rate scheduler with 0.1 ratio of warmup. For training, we ran 5 epochs with 5K data, and 3 epochs for 50K and 4https://huggingface.co/unsloth/ llama-3-8b-Instruct 5https://huggingface.co/Henrychur/ MMed-Llama-3-8B QA Given question and context, select the correct answer from the provided options. TE Given pair of texts, consisting of claim and the evidence, determine whether the evidence supports, refutes, or is neutral regarding the claim. Respond with one of the following: Supports, Refutes, or Neutral. NER Given sentence, label each disease, disease class and symptom entity using the BIO format. In BIO format, indicates the beginning of an entity, indicates the inside of an entity, and indicates token not part of any entity. Label each word in the format: word [LABEL]. TXTCLASS You are provided with citation context. Classify the intent of the citation within this context. Intents are: [background, method, result]. NED You are provided with text. Your objective is to identify and extract all chemical and disease entities mentioned in the text, maintaining the order in which they appear. For each entity, provide its corresponding database identifier from MESH. The entities should be presented in the format: [entity1 <db_name/db_id>]. RE Given text, identify and extract specified relations between anatomical entities mentioned within it. The specified relation types are [frag, Part-of]. Relation explanation: frag: Frag relation marking coordination with ellipsis; Part-of: Part-of relation marking entity mention spanning prepositional phrase. Present each relation in format as follows: [<entity1> <relation> <entity2>]. COREF Given text and specified anatomical entity, identify and extract all co-references to that entity within the text. Present each co-reference entity in the following format: [co-reference entity]. STS Given two texts, evaluate their similarity and provide an integer score ranging from 0 to 5, where 0 indicates no similarity and 5 indicates high similarity. EE Given text, identify and extract the epecified types of bio-molecular events along with their primary arguments. The event type can be [Binding, Positive_regulation, Phosphorylation, Regulation, Transcription, Localization, Gene_expression, Protein_catabolism, Negative_regulation]. Present each event in the format as follows: [<type> <trigger> <theme entity>]. TRANSL Translate the text from Chinese to English. TEXTPAIRCLASS You are given drug name and piece of text. Analyze the sentiment in the text and determine whether the sentiment towards the drug is positive, negative, or neutral. Answer with Positive, Negative, or Neutral. SUM Writing the related-work section of paper based on its abstract and the articles it references. Table 6: Instruction examples for each task category. 100K datasets. The training was conducted on single 40GB A100 GPU. Query Template For the training and evaluation of all LLaMA-3 series models, we used the standard LLaMA-3 chat template. Table 7 shows an example. When constructing few-shot prompts, each example is treated as round of dialogue and added before the query that needs an answer. Unlike the approach where instructions are only given in the first round of dialogue, we included instructions in each example. This is because for some tasks without training set, we selected examples from the training sets of similar tasks, so the instructions in the examples may not completely match the instructions of the query. Table 8 demonstrates query of NER task."
        },
        {
            "title": "Tasks",
            "content": "We add additional metrics, BERT score and METEOR score, to evaluate the generated text on summarization and translation tasks. The evaluation results are presented in Table 9 and Table 10."
        },
        {
            "title": "E Dataset Collection",
            "content": "Table 11 lists all the dataset employed in MEDINST. Because single dataset might be reformulated into multiple tasks, we added suffixes to the names in the multi-task dataset. For example, BC5CDR appears in the NER, NED, and RE tasks. For the primary task, NER, we use the datasets original name, and for the other two tasks, we append the respective suffixes to the dataset name. <begin_of_text><start_header_id>system<end_header_id> You are helpful assistant.<eot_id><start_header_id>user<end_header_id> Given an utterance, determine if it is from doctor or patient. Do have covid 19?<eot_id><start_header_id>assistant<end_header_id> patient<eot_id> Table 7: LLaMA-3 prompt template. Example 1 Example"
        },
        {
            "title": "Query",
            "content": "Instrcution: You are provided with text. Your objective is to identify, extract and classify all gene and protein entities mentioned in the text, maintaining the order in which they appear. Types are [Gene, DomainMotif, FamilyName]. The entities should be presented in the following format: [entity <type>]. Cloning, expression and localization of an RNA helicase gene from human lymphoid cell ... ... cell line from diffuse large B-cell lymphoma. [RNA helicase <FamilyName>] [RNA helicase <FamilyName>] [p54 <Gene>] [RNA helicase <FamilyName>] [ME31B <Gene>] [ME31B <Gene>] Output: Input: Input: Instrcution: You are provided with text. Your objective is to identify, extract and classify all gene variant entities mentioned in the text, maintaining the order in which they appear. Types are [DNAMutation, SNP, ProteinMutation]. The entities should be presented in the following format: [entity <type>]. novel multidrug-resistance protein 2 gene mutation identifies ... ... heterozygous mutation was significantly associated with the presence of pruritus. [V1188E <ProteinMutation>] Output: Instrcution: You are provided with text. Your objective is to identify, extract and classify all gene variant entities mentioned in the text, maintaining the order in which they appear. Types are [OtherMutation, Species, DNAAllele, DNAMutation, CellLine, SNP, ProteinMutation, ProteinAllele, Gene, AcidChange]. The entities should be presented in the following format: [entity <type>]. novel single-nucleotide substitution, Glu 4 Lys ... ... Thus, our results suggest that Glu 4 Lys in the LTC4S might be associated with allergic diseases. Input: Table 8: Query example. Model LLaMA3 BioMistral MMEDL3-EnIns GPT-4o LLaMA3-MI32 (ours) MMEDL3-MI32 (ours) LLaMA3-MI (ours)"
        },
        {
            "title": "BERTScore METEOR Score",
            "content": "0.7467 0.7253 0.7314 0.8317 0.7951 0.7963 0.8203 0.1758 0.1152 0.1185 0.2333 0.1566 0.1220 0.1592 Table 9: SUM task: Multi-XScience results. Model LLaMA3 BioMistral MMEDL3-EnIns GPT-4o LLaMA3-MI32 (ours) MMEDL3-MI32 (ours) LLaMA3-MI (ours)"
        },
        {
            "title": "BERTScore METEOR Score",
            "content": "0.9000 0.9101 0.8888 0.9291 0.9115 0.9080 0.9379 0.3776 0.3670 0.3625 0.4661 0.3933 0.3781 0.6126 Table 10: TRANSL task: ParaMed results. Table 11: Dataset collection. Dataset BioASQ-Task-B-yesno BioASQ-Task-B-list BioASQ-Task-B-factoid BioASQ-Task-B-summary BiologyHowWhyCorpus BIOMRC Evidence-Inference-2.0 MedQA MedHop MEDIQA-QA PubMedQA-artificial PubMedQA-labeled SciQ FEVER HealthVer PubHealth SciFact ManConCorpus CoVERt MEDIQA-RQE SciTail NCBI-disease BC2GM CHEMDNER-BIO BC5CDR Linnaeus JNLPBA-DNA JNLPBA-RNA JNLPBA-CT JNLPBA-CL AnatEM AnEM BioInfer BioNLP-2009 BioNLP-2011-EPI BioNLP-2011-GE BioNLP-2011-ID BioNLP-2011-REL BioNLP-2013-CG BioNLP-2013-GE BioNLP-2013-GRO BioNLP-2013-PC BioNLP-2019-BB BioRED BioRelEx CellFinder CHEBI CHEMDNER"
        },
        {
            "title": "Task\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nQA\nTE\nTE\nTE\nTE\nTE\nTE\nTE\nTE\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER",
            "content": "Train 15,568 11,687 16,389 13,151 1,269 700,000 10,056 10,178 1,620 312 200,000 450 11,679 145,449 10,590 9,804 868 0 0 8,588 23,596 5,432 12,632 30,884 4,560 12,004 4,699 721 4,792 2,596 5,861 164 894 756 600 856 151 756 300 194 150 260 132 400 1,402 5 476 2,915 Dev 0 0 0 0 0 50,000 1,233 1,273 342 25 11,269 50 1,000 9,999 1,917 1,214 0 0 0 302 2,126 923 2,531 30,841 4,581 4,086 552 89 420 284 2,118 137 0 260 200 0 46 150 100 212 50 90 66 100 201 0 0 2,906 Continued on next page Test 813 1,000 724 824 0 62,707 1,222 1,272 0 150 0 500 1,000 9,999 1,823 1,233 1,189 2,775 212 230 1,304 942 5,065 26,561 4,797 7,181 622 102 1,422 377 3,830 30 206 150 0 338 117 260 200 256 100 175 0 100 0 5 0 2,477 Table 11 Continued from previous page Dataset ChemProt CHIA CPI DDI DrugProt EBM-NLP EU-ADR GENETAG PTM-Events GENIA-Term GNormPlus HPRD50 MedMentions miRNA MLEE NLM-Gene NLM-Chem OSIRIS PDR PICO-Annotation ProGene SCAI-Chemical SCAI-Disease SETH SPL-ADR tmVar-v1 tmVar-v2 tmVar-v3 Verspoor-2013 MedDialog SciCite Hallmarks-of-Cancer GEOKhoj-v1 BC7-LitCovid AskAPatient-NED BC5CDR-NED Bio-ID BioNLP-2019-BB-NED BioRED-NED BioRelEx-NED CPI-NED GNormPlus-NED Linnaeus-NED MeDAL MedMentions-NED miRNA-NED MuchMore-NED NCBI-disease-NED NLM-Gene-NED"
        },
        {
            "title": "Task\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nNER\nTXTCLASS\nTXTCLASS\nTXTCLASS\nTXTCLASS\nTXTCLASS\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED\nNED",
            "content": "Train 1,020 1,932 1,808 673 3,500 4,735 299 3,875 112 2,000 418 34 2,635 201 130 450 80 105 179 361 20,055 67 330 433 101 213 158 0 117 981 8,243 12,119 25,000 24,960 15,612 500 11,366 132 400 1,402 1,808 418 95 3,000,000 2,635 201 7,820 592 450 Dev 612 0 0 0 750 0 0 1,311 0 0 0 0 878 0 44 0 20 0 0 0 1,109 0 0 0 0 0 0 0 0 126 916 1,798 0 2,500 845 500 0 66 100 201 0 0 0 1,000,000 878 0 0 100 0 Continued on next page Test 800 0 0 279 0 187 0 2,567 0 0 261 9 879 100 87 100 50 0 0 0 2,414 0 0 0 0 101 0 493 0 122 1,861 3,547 5,000 6,239 867 500 0 0 100 0 0 261 0 1,000,000 879 100 0 100 100 Table 11 Continued from previous page Task Dataset NED NLM-Chem-NED NED OSIRIS-NED NED SPL-ADR-NED NED tmVar-v2-NED NED tmVar-v3-NED NED TwADR-L-NED RE AnEM-RE RE BC5CDR-RE RE BioInfer-RE RE BioNLP-2011-REL-RE RE BioNLP-2013-GE-RE RE BioNLP-2013-GRO-RE RE BioNLP-2019-BB-RE RE BioRED-RE RE BioRelEx-RE RE CHEBI-RE RE ChemProt-RE RE CHIA-RE RE CPI-RE RE DDI-RE RE DrugProt-RE RE EU-ADR-RE RE HPRD50-RE RE IEPA RE LLL05 RE MLEE-RE RE MuchMore-RE RE SETH-RE RE SPL-ADR-RE RE Verspoor-2013-RE COREF AnEM-COREF COREF BioNLP-2009-COREF COREF BioNLP-2011-EPI-COREF COREF BioNLP-2011-GE-COREF BioNLP-2011-ID-COREF COREF BioNLP-2011-REL-COREF COREF COREF BioNLP-2013-CG-COREF COREF BioNLP-2013-GE-COREF COREF BioNLP-2013-PC-COREF COREF BioRelEx-COREF COREF PTM-Events-COREF COREF MLEE-COREF COREF PDR-COREF STS Bio-SimVerb STS Bio-SimLex STS BIOSSES STS EHR-Rel STS MayoSRS STS MQP Train 80 105 101 158 0 4,816 22 500 642 378 40 149 121 395 1,263 415 767 1,876 1,246 510 2,433 253 28 114 77 32 7,734 212 96 114 10 536 440 571 170 535 466 53 455 1,143 25 198 19 1,000 988 64 3,741 101 3, Dev 20 0 0 0 0 115 5 500 0 92 41 48 59 97 178 0 443 0 0 0 542 0 0 0 0 11 0 0 0 0 2 110 168 0 31 110 176 41 128 167 0 57 0 0 0 16 0 0 0 Continued on next page Test 50 0 0 0 493 143 13 500 142 0 0 0 0 100 0 0 620 0 0 191 0 0 8 26 0 16 0 0 0 0 14 0 0 0 0 0 0 0 0 0 0 113 0 0 0 20 0 0 0 Table 11 Continued from previous page Dataset UMNSRS BioNLP-2009-EE BioNLP-2011-EPI-EE BioNLP-2011-GE-EE BioNLP-2011-ID-EE BioNLP-2013-CG-EE BioNLP-2013-GE-EE BioNLP-2013-PC-EE PTM-Events-EE MLEE-EE PDR-EE MuchMore-TRANSL ParaMed SciELO Medical-Data MeQSum Multi-XScience Train 1,153 695 383 765 110 299 149 257 111 127 167 6,374 62,127 3,006,699 Task STS EE EE EE EE EE EE EE EE EE EE TRANSL TRANSL TRANSL TEXTPAIRCLASS 5,279 1,000 SUM 30,369 SUM Dev 0 150 121 0 30 100 157 90 0 44 0 0 2,036 0 0 0 5,066 Test 0 0 0 0 0 0 0 0 0 87 0 0 2,102 0 0 0 5,"
        }
    ],
    "affiliations": [
        "Eindhoven University of Technology",
        "University of Liverpool",
        "University of Technology Sydney",
        "Yale University"
    ]
}