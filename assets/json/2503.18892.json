{
    "paper_title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild",
    "authors": [
        "Weihao Zeng",
        "Yuzhen Huang",
        "Qian Liu",
        "Wei Liu",
        "Keqing He",
        "Zejun Ma",
        "Junxian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools."
        },
        {
            "title": "Start",
            "content": "SimpleRL-Zoo SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild Weihao Zeng1 Yuzhen Huang1 Qian Liu2 Wei Liu1 Keqing He3 Zejun Ma2 1HKUST 3BUPT https://github.com/hkust-nlp/simpleRL-reason Junxian He1 2TikTok"
        },
        {
            "title": "Abstract",
            "content": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base modelsa paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategiessuch as adjusting format reward and controlling query difficultywe achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the aha moment). Notably, we observe the aha moment for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools. 5 2 0 2 4 2 ] . [ 1 2 9 8 8 1 . 3 0 5 2 : r Figure 1: Accuracy and response length across training iterations for different models, averaged on GSM8K, MATH500, Minerva Math, OlympiadBench, AIME24, and AMC23. Per-benchmark results are in Figure 12 (Appendix A). All training starts from base models. Equal Contribution. Correspondence to Weihao Zeng (wzengak@connect.ust.hk), Yuzhen Huang (yhuanghj@cse.ust.hk), and Junxian He (junxianh@cse.ust.hk). 1 SimpleRL-Zoo"
        },
        {
            "title": "Introduction",
            "content": "Large reasoning models, including OpenAI-o1 (Jaech et al., 2024), DeepSeek-R1 (DeepSeekAI et al., 2025a), and Kimi-k1.5 (Team et al., 2025), demonstrate remarkable abilities. These models excel at generating long Chains-of-Thought (CoT) (Wei et al., 2022) responses when solving complex tasks and exhibit advanced, reflection-like reasoning behaviors. Recently, DeepSeek-R1 (DeepSeek-AI et al., 2025a) has revealed that starting from pretrained models (i.e., base models), pure reinforcement learning (RL) with rule-based reward can lead to the spontaneous emergence of long CoT and self-reflection behaviors, called the aha moment. This RL training paradigm starting from base models is often referred to as zero RL training. While the success of zero RL training was initially demonstrated using DeepSeekV3 (DeepSeek-AI et al., 2025b), model with 671B parameters, it remained unclear whether such emergent phenomena persist in generally smaller and less capable open base models. Recent open-source efforts exploring zero-training approaches have predominantly centered on the Qwen2.5-series models (Zeng et al., 2025a; Yeo et al., 2025; Xie et al., 2025; Hu et al., 2025; Yu et al., 2025), which, even as base models, exhibit strong instruction-following capabilities and display notable cognitive behaviors such as backtracking and verification from the beginning, as we will detail in 2.5. Moreover, the analyses of model behavior in these studies remain largely superficial, focusing primarily on metrics such as response length and accuracy. These observations neither clearly establish whether the models reasoning behaviors actually change nor clarify the mechanisms underlying the emergence of effective reasoning, leaving significant gap in understanding. To provide more transparent understanding of zero RL training across different base models in the wild, this paper addresses the following key questions: (1) How do reasoning capabilities develop across various models during zero RL training? (2) Does an aha moment still occur for base models that initially lack strong instruction-following and self-verification abilities? (3) What are the critical factors for ensuring successful zero RL training across diverse base models? To this end, we perform zero RL training across diverse range of model series and sizes, including Mistral-7B (Jiang et al., 2023), Mistral-24B (Mistral AI, 2025), Llama3-8B (Dubey et al., 2024), DeepSeek-Math-7B (Shao et al., 2024), Qwen2.5-0.5B/1.5B/7B/14B/32B (Yang et al., 2024a), as well as Qwen2.5-Math-7B (Yang et al., 2024b). To maintain simplicity in the training recipe, our experiments rely exclusively on the training sets of GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets for rule-based reward modeling. It is worth noting that we adopt the same training hyperparameters to train all the models. Using GRPO (Shao et al., 2024) as the RL algorithm, combined with several critical factors that we identified, we obtain significant improvements in model accuracy across all base models, along with notable increase in response length for 9 out of the 10 models, with the exception of Qwen2.5-Math-7B. However, through careful monitoring of training dynamics and reasoning behaviors, we find that different base models exhibit distinct patterns during training. Also, certain specific factors require careful attention to ensure successful zero RL training. Below, we summarize our key findings. 1. Increased response length does not always correspond to an aha moment Interestingly, for most Qwen2.5 models, which form the foundation of most recent open-source efforts, we do not observe rise in the frequency of certain cognitive behaviors, such as self-reflection, despite the increase in response length. (2.5) 2. For the first time, we observe significant increase in the frequency of specific cognitive reasoning behaviors, such as verification, in small models outside the Qwen family, notably in the Llama3-8B and DeepSeek-Math-7B models. (2.5) 3. Enforcing rigid format reward (e.g., enclosing answers within boxes) (DeepSeekAI et al., 2025a) significantly penalizes exploration (Singh et al., 2023; Wang et al., 2024), particularly for base models that initially struggle with instruction following. This restriction lowers their performance ceiling and often induces overthinking behaviors (Chen et al., 2024). (3.1) 2 SimpleRL-Zoo 4. The difficulty level of the training data must align closely with the base models intrinsic exploration capabilities, otherwise zero RL will fail. (3.2) 5. In contrast to the observation in Shao et al. (2024), zero RL training lifts pass@k accuracy by 10-30 absolute points, strong evidence confirming zero RL training is not just reranking responses. (2.4) 6. We revisit the traditional training pipeline that performs SFT to learn to follow instructions before RL training. Specifically, we use conventional SFT datasets as cold start for RLa de facto approach prior to the release of DeepSeek-R1. While high-quality CoT data (Li et al., 2024) can rapidly enhance base models performance through imitation, we find that it significantly limits the models ability to explore freely during RL. This constraint diminishes post-RL performance and suppresses the emergence of advanced reasoning capabilities. (4)"
        },
        {
            "title": "2 On Emerging Reasoning in Zero RL Training",
            "content": "Existing works on studying zero RL training mostly focus on Qwen2.5-series models, and only track superficial metrics such as accuracy and response length (Zeng et al., 2025a; Hu et al., 2025; Yu et al., 2025). First, while Qwen2.5 models exhibit strong performance, they may not be representative of base models commonly encountered in the wild. This is because Qwen2.5 models incorporate substantial amount of synthetic data during pretraining and already display robust instruction-following abilities and certain reflective behaviors, as observed in our preliminary trials. Second, an increase in response length can result from various factors and does not necessarily imply an aha moment, the emergence of specific cognitive behaviors such as self-reflection. For instance, we observe that response length increases can sometimes be unhealthy, stemming from meaningless repetition. To address these gaps, this section investigates zero RL training across diverse range of base models spanning multiple families and sizes. By carefully monitoring training dynamics across variety of metrics beyond accuracy and response length, we aim to provide more comprehensive and transparent understanding of zero RL training for open base models in the wild. 2.1 Background: Zero RL Training In our study, we follow the zero RL training recipe in DeepSeek-AI et al. (2025a) using various open base models, employing the GRPO algorithm (Shao et al., 2024). GRPO optimizes computational efficiency by eliminating the need for separate value model; instead, it directly utilizes group-normalized rewards to estimate advantages. For query and set of responses = {o1, o2, . . . , oG} sampled from the old policy model πold, we adopt token-level, length-rectified GRPO objective to optimize the policy model π:1 JGRPO(θ) = 1 i=1 oi (cid:124) i=1 oi t=1 min (cid:2)ri,t(θ) ˆAi, clip (ri,t(θ); 1 ϵ, 1 + ϵ) ˆAi (cid:3) (cid:123)(cid:122) Clipped policy update (cid:125) β (cid:124) KL[πθ πref] (cid:125) (cid:123)(cid:122) KL penalty where ri,t(θ) = πθ(oi,t q, oi,<t) (oi,t q, oi,<t) πθold (1) where πref represents the reference model, and the term DKL introduces KL divergence constraint to limit how much the model can deviate from this reference. The advantage 1The original GRPO objective has length normalization term that introduces length biases. We remove the length normalization term similar to concurrent works (Yu et al., 2025; Liu et al., 2025) this length-rectified objective was the default implementation of GRPO in our adapted codebase, verl (Sheng et al., 2024). 3 SimpleRL-Zoo estimate ˆAi measures how much better the response oi is compared to the average response, which is computed using group of rewards {r1, r2, . . . , rG} for the responses in set O: ˆAi = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) (2) 2.2 Experimental Setup Dataset: To keep the training recipe simple, we select training data exclusively from the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets. For the MATH dataset, following prior studies (Lightman et al., 2023; Wang et al., 2023; Sun et al., 2024), we reserve the MATH500 subset as the test set, uniformly sample an additional 500 problems for validation, and combine the remaining 4,000 test problems with the original 7,500 training problems to form our training set. Each example in the MATH dataset is originally labeled with difficulty level ranging from 1 to 5. In our experiments, we find that data difficulty is critical for successful zero RL (3.2) and it is necessary to use data that aligns with the models capability. To investigate this phenomenon, we categorize the data into three difficulty levels: Easy (GSM8K and MATH lv.1), Medium (MATH lv.14), and Hard (MATH lv.35), with each category containing roughly 8,000 problems. For our main training runs, we use Easy for LLama-3.1-8B, Mistralv0.1-7B, and DeepSeek-Math-7B; Medium for Qwen2.5-0.5B; Hard for Mistral-Small-24B, Qwen-2.5-Math-7B, and Qwen-2.5-1.5B/7B/14B/32B, and we will report ablation study on data difficulty in 3.2. Reward: We use rule-based reward function that assigns rewards solely based on the correctness of the generated response: correct final answer receives reward of +1, while an incorrect one receives reward of 0. Recent studies (Luo et al., 2025; Chen et al., 2025) often incorporate format-based rules into reward calculation, encouraging the model to follow specific output formats. However, we find that this approach may hinder the models exploration and ultimately harm its performance particularly for the base models which struggle with following the format in the initial stage, as detailed in 3.1. Models: We conduct zero RL training experiments on Llama-3.1-8B (Dubey et al., 2024), DeepSeek-Math-7B (Shao et al., 2024), Mistral-v0.1-7B (Jiang et al., 2023), Mistral-Small-24bBase-2501 (Mistral AI, 2025), and Qwen-2.5 (0.5B, 1.5B, 7B, 14B, 32B) (Yang et al., 2024a). As we perform experiments for variety of models, under extremely simple settings with small, simple datasets and only correctness reward, we refer to our obtained models as SimpleRL-Zoo to represent simple training recipe for zoo of open base models. For models with weaker instruction-following capabilities (Llama-3.1-8B, Mistral-v0.1-7B, and Qwen-2.5-0.5B/1.5B), we employ simpler prompts (Chern et al., 2023) requiring only stepby-step reasoning. For models with stronger instruction-following abilities, we use more complex prompts (Yang et al., 2024a) that require the final answers to be placed in boxes. In our preliminary experiments, we observe that using complex prompts with models that have weak instruction-following capabilities often results in large amounts of irrelevant or nonsensical content being generated early in training, leading to instability. The content of simpler prompts and more complex prompts is shown in Figure 11 in Appendix. Benchmark: We evaluate performance on standard mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), MATH 500 (Hendrycks et al., 2021), Minerva Math (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024), as well as on competition-level benchmarks such as AIME 2024 and AMC 2023. Other Configurations: We train our models using the verl (Sheng et al., 2024) framework. Specifically, during training, we use prompt batch size of 1024, generate 8 rollouts per prompt, set maximum rollout length of 8,192 tokens, and train using mini-batch size of 256. It is worth noting that we use the same training hyperparameters to train all the models. During evaluation, we set the sampling temperature to 1.0 and allow maximum generation length of 16,384 tokens. For most benchmarks, we report pass@1 results. However, for the AIME 2024 benchmark specifically, we report both pass@1 and average accuracy computed 4 SimpleRL-Zoo over 32 samples (avg@32) due to limited data points. We provide detailed training and evaluation details in the Appendix B. 2.3 Evaluation Metrics During training, we monitor standard metrics such as accuracy and response length across benchmarks. As discussed before, however, we observe that response length as metric is quite superficial and cannot accurately reflect changes in the models reasoning behavior. Therefore, we adopt the following metrics additionally: Reasoning Behavior Ratio: To better understand the models reasoning patterns throughout the training process, we adopt the cognitive behavior framework proposed by Gandhi et al. (2025) and use GPT-4o (Hurst et al., 2024) to identify reasoning-related behaviors, including Backtracking, Verification, Subgoal Setting, and Enumeration. We report the ratio of responses that contain such cognitive behaviors. While some recent studies suggest tracking reflection behavior using related keywords (Yeo et al., 2025; Xie et al., 2025) as monitoring signals, we argue that these keywords only exhibit only weak correlation with high-level reasoning patterns like reflection and verification. As result, they fail to adequately capture the development of these reasoning processes. Further details can be found in Appendix F.1. Clip Ratio: In the early stages of training, the base model exhibits weak instructionfollowing ability and often fails to stop appropriately, resulting in irrelevant or excessively long outputs. After training collapses, the model may also generate repetitive or overly extended responses. Since the model has fixed maximum context length, such outputs may be truncated during both training and evaluation. To monitor this issue, we define the proportion of truncated outputs as the Clip Ratio. Average Stopped Length: Generations that are truncated often result from issues such as repetitive patterns or incomplete reasoning, which typically do not contribute to effective trajectories. To account for this factor, we introduce new metric to track the average length of responses that are stopped under normal conditions. It is more reliable metric to consider only valid responses, thereby eliminating the interference caused by unstopped responses. Pass@k Accuracy: We track the pass@k accuracy, which represents the percentage of questions for which at least one correct response is obtained when sampling responses per question. Pass@k serves as an indicator of the models exploration capabilities and is particularly relevant for RL, as it reflects the models ability to generate responses that can achieve positive reward. Previously, some researchers believed that RL training might merely reorder responses within the original model distribution, as evidenced by the lack of improvement in pass@k accuracy following RL training (Shao et al., 2024). 2.4 Main Results Zero RL Training Improves both Accuracy and Response Length Significantly: Figure 1 and Figure 12 in Appendix illustrate steady improvement in both response length and average accuracy across various benchmarks. Table 1 provides detailed breakdown of the results. Remarkably, even with only 8K training data for training, we observe significant performance gains across all benchmarks. For example, Qwen-32bs Pass@1 on AIME 24 surges from 10.0 to 36.7, and on MATH 500, it increases from 68.6 to 82.4. Despite the limited training data, consisting solely of GSM8K and MATH 500, we observe substantial performance gains on competition-level benchmarks such as AIME 2024 and AMC 2023. This highlights the impressive generalization abilities of zero RL training allowing the model to bridge the gap from easy to hard. In addition to the Qwen series models, we also significantly improve both performance and response length for other models that initially starts with low baselines. For instance, the 5 SimpleRL-Zoo Model GSM8K MATH Minerva Math Olympiad Bench AIME24 (Pass@1) AIME24 (Avg@32) AMC23 Avg. Mistral-v0.1-7B (cid:44) + SimpleRL-Zoo Llama-3.1-8B (cid:44) + SimpleRL-Zoo DeepSeek-Math-7B (cid:44) + SimpleRL-Zoo Mistral-Small-24B (cid:44) + SimpleRL-Zoo Qwen-2.5-0.5B (cid:44) + SimpleRL-Zoo Qwen-2.5-1.5B (cid:44) + SimpleRL-Zoo Qwen-2.5-7B (cid:44) + SimpleRL-Zoo Qwen-2.5-Math-7B (cid:44) + SimpleRL-Zoo Qwen-2.5-14B (cid:44) + SimpleRL-Zoo Qwen-2.5-32B (cid:44) + SimpleRL-Zoo 21.2 75.0 39.7 79.2 28.4 78.5 78.6 92.0 36.7 49.5 55.7 74.4 88.2 91.7 65.5 90.2 91.6 94.4 92.9 95.9 Llama, DeepSeek and Mistral Models 4.2 15.8 13.6 23.0 19.4 39.6 43.6 70.6 2.4 4.1 3.1 5.3 4.7 12.6 11.6 36.6 4.0 6.6 4.8 9.6 5.5 21.0 10.7 36. 15.8 34.4 29.6 59.0 64.6 78.2 63.6 80.2 65.4 80.2 68.6 82.4 Qwen Series Models 2.8 8.9 6.5 21.0 30.1 40.4 25.8 39.0 33.5 44.9 31.1 46.4 4.8 10.3 6.6 20.2 25.7 38.6 12.5 37.5 24.3 40.4 27.9 42.6 0.0 0.0 0.0 0.0 0.0 3.3 3.3 16.7 0.0 0.0 0.0 6.7 3.3 20.0 13.3 40.0 6.7 23.3 10.0 36.7 0.0 0.2 0.2 0.2 0.0 0.6 0.5 13. 0.3 0.7 0.1 4.2 0.3 15.6 8.6 24.0 3.4 14.2 4.5 27.2 0.0 10.0 2.5 15.0 10.0 20.0 17.5 45.0 12.5 22.5 12.5 35.0 30.0 62.5 42.5 70.0 37.5 57.6 45.0 67.5 5.3 18.6 10.6 22.0 11.3 29.2 27.6 49.6 12.1 20.9 18.5 36.1 40.3 55.2 37.2 59.5 43.2 56.8 45.9 61.9 Table 1: Detailed performance of various models across multiple benchmarks. The blue lines represent the models trained with our recipe. AIME is evaluated in two ways: Pass@1 (single run) and Avg@32 (average score from 32 runs). For AIME24 (Pass@1) and other benchmarks, baselines use greedy decoding, and models with SimpleRL-Zoo use temperature=1.0 and top-p=0.95. For AIME24 (Avg@32), we sample 32 responses per model with the same settings. Average scores are based on AIME (Pass@1) and other benchmarks. Figure 2: Pass@1 and Pass@8 accuracy over the training iterations of Mistral-Small-24B. The model is trained on the hard data (MATH levels 35) as described in 2.2. We evaluate its performance on three benchmarks: AIME24, AMC23, and Math500. The reported average score is the mean across these three benchmarks. DeepSeek-Math-7B model initially had performance score of around 10.0. After just 80 training iterations, its performance increases more than threefold, while its response length grows from around 300 to over 1200 tokens. Steady Improvement of Pass@k Accuracy: As shown in Figure 2, Mistral-Small-24B exhibits robust growth in pass@8 on MATH 500. Furthermore, as training progresses, the models pass@1 results eventually surpass the initial pass@8 results of the base model. Surprisingly, the gap between Pass@1 and Pass@8 does not diminish during training; instead, it widens as training progresses. By iteration 100, the two metrics differ by more than 30 absolute points on average. This suggests significant potential for further improvements in RL, as pass@8 represents the models ability to explore correct responses. Furthermore, Figure 3 shows that significant gap in pass@k performance persists between the base model and the model after RL training, even at higher values of k. Notably, after just 100 training iterations, the model achieves pass@1 performance comparable to the base models pass@16. 6 SimpleRL-Zoo Figure 4: Average clip ratio and stopped length across training iterations for different models. We assess the models every five steps on variety of math benchmarks, including GSM8K, MATH500, Minerva Math, and OlympiadBench, as well as competition-level benchmarks like AIME24 and AMC23. The red line indicates the clip ratio, while the blue line represents the stopped length. Per-benchmark results are in Figure 13 (Appendix A). This suggests that zero RL training not only adjusts the models output distribution to favor correct responses within the top candidates but also enhances the models internal reasoning abilities. Growth in Response Length May be Unhealthy: Response length does not always reflect genuine growth in reasoning. In some cases, unstable training can cause models to generate excessive repetitive content until they hit the context length limit, artificially inflating response length without improving reasoning depth. For example, Figure 4 shows that while most models maintain low clip ratio below 5% of the data when their average stopping length steadily increases, Mistral-7B-v0.1 exhibits high clip ratio and significant fluctuations in stopping length. Upon closer inspection of its responses, we find that the responses consist of incoherent, mixed-language gibberish, suggesting that its thinking process is not genuinely expanding. We note that such patterns would not be captured by response length as in Figure 1. These findings indicate that most models demonstrate meaningful and structured increase in response length. This raises an important question: What exactly do models learn as their thinking time increases? We answer this question next. Figure 3: Pass@k of Mistral-24B based on the average results from AIME24 and AMC23. 2.5 The Aha Moment Quantifying Emergence of Reasoning Behavior Figure 5 illustrates the reasoning behavior ratio on OlympiadBench during model training. By comparing Figure 5 with Figure 4, we observe that fluctuations in the reasoning behavior ratio effectively account for variations in the average stopped length. Interestingly, we find that different models exhibit entirely distinct trends in reasoning behavior changes. Smaller models, such as Qwen-2.5-0.5B and Qwen-2.5-1.5B, tend to prioritize learning the Subgoal Setting behavior, with its proportion increasing by approximately 45 times. Additionally, the proportions of Verification and Enumeration also show noticeable 7 SimpleRL-Zoo Figure 5: The change in reasoning behavior over the training iterations across all models. As described in 2.2, we use GPT-4o to extract and track shifts in reasoning behaviors on OlympiadBench. We focus on four reasoning-related behaviors: Backtracking, Verification, Subgoal Setting, and Enumeration. growth. In contrast, for other base models that inherently possess step-by-step reasoning capabilities, adjustments in Subgoal Setting during the RL training process are relatively minor. During training, we observe that DeepSeek-Math-7B, Llama-3.1-8B, and Mistral-Small-24B exhibit substantial increases in the proportions of Enumeration and Verification behaviors, rising from relatively low initial levels by approximately 3-4 times. This growth correlates closely with their changes in average stopped length, suggesting shift in reasoning patterns over time. For instance, in Mistral-Small-24B, reflection-oriented behaviors such as Verification and Backtracking increase dramatically from nearly 0% to approximately 50%, indicating the emergence of reflection behavior from scratch. This shift suggests that the model progressively internalizes verification mechanisms as part of its reasoning process, offering promising trajectory for further enhancement. In contrast, Qwen-2.5-7B and 32B demonstrate strong reasoning behaviors from the outset, with minimal changes throughout training. This stability aligns with their slow length adjustments (Figure 1) and suggests that Qwen models inherently possess robust reasoning capabilities. Rather than undergoing structural shift in their reasoning processes, they primarily benefit from small increases in thinking time, which yield significant performance improvements. Finally, we observe that Mistral-7B-v0.1 consistently exhibits low reasoning behaviors with no noticeable growth, further supporting our earlier analysis in 2.4. To intuitively illustrate the changes in reasoning behavior, we present examples of Mistral 24Bs reasoning before and after zero training in Figures 6. Additional examples and comprehensive case studies involving other models are available in the Appendix F.3. In Figure 6, we observe that unlike the base model, the zero training model actively attempts to verify if its initial solution is valid by substituting it back into the original expression. Upon recognizing that the first solution does not meet the necessary conditions, the model explicitly initiates backtracking approach, stating lets try another possibility, eventually arriving at the correct answer."
        },
        {
            "title": "3 Key Factors Shaping Zero Training",
            "content": "In this section, we identify key factors that influence stability and performance during zero RL training, particularly when dealing with early-stage or weaker models. First, we explore how an over-reliance on format rewards restricts exploration. Next, we analyze how 8 SimpleRL-Zoo Figure 6: comparison of Mistral-24Bs verification and backtraining behavior before and after zero training. Here, base solution represents the response of the Mistral-24B base model, while zero solution represents the response of the model after training. (a) Qwen-2.5-7B (b) Llama-3.1-8B Figure 7: Comparison of accuracy and response length with and without format rewards. (a) Mistral-7b-v0.1 (b) Qwen-2.5-7B Figure 8: Comparison of accuracy and response length across different data difficulty levels. We examine three levels of data: Easy (GSM8K and MATH lv.1), Medium (MATH lv.14), and Hard (MATH lv.35), with each category containing approximately 8,000 problems. data difficulty impacts exploratory behavior, illustrating how exposure to varying levels of difficulty shapes the exploration dynamics of base models. We also discuss the impact of some exploration-related hyperparameters in Appendix D. SimpleRL-Zoo 3.1 Over-Reliance on Format Rewards We find that enforcing strict formatting constraints, such as requiring the final answer to be enclosed in latex command boxed{}, can hinder models freely exploration and ultimately degrades performance. This is because many base models cannot follow the format constraint well in the initial stage, and imposing format reward will penalize many correct explorations. We compare two reward functions: one without format constraints, which rewards responses solely based on answer correctness (our default design in 2.2), and another that strictly enforces formatting by penalizing responses with reward of -1 if they fail to adhere to the required format. As illustrated in Figure 7, weaker models like Llama-3.1-8B struggle under strict formatting requirements, leading to rapid increase in response length early in training without performancec improvement. The model expends excessive effort on adhering to the format but fails to learn how to answer correctly, ultimately resulting in model collapse. Figure 7(a) further reveals that even stronger models, such as Qwen-2.5-7B, which initially comply with formatting constraints, suffer in later training stages. This includes both performance degradation and significant reduction in CoT length. Moreover, the strict formatting constraints limit the models upper performance potential, suggesting that rigid constraints stifle its ability to explore and refine solutions effectively. These findings highlight crucial insight: in zero RL training setting, rather than imposing rigid formatting rules, we should prioritize maintaining response verifiability while allowing sufficient flexibility for exploration. 3.2 Data Difficulty on Exploratory Behavior Base models exhibit varying performance and CoT behaviors when trained on different RL data. Figure 8 compare the performance of Mistral-7B and Qwen-2.5-7B across Easy (GSM8K, MATH Lv.1), Medium (MATH Lv.1-4), and Hard (MATH Lv.3-5) datasets. We observe clear trend: as data difficulty increases, Mistral-7Bs performance progressively deteriorates. When faced with high-difficulty data (Hard: MATH levels 3-5), the model struggles to generate responses that receive positive feedback from the reward system. This failure results in significant increase in response length without any corresponding improvement in accuracy, signaling breakdown in the training processoften referred to as training collapse. Figure 8 demonstrates that Qwen-2.5-7B exhibits pattern entirely opposite to Mistral-7B-v0.1. Specifically, as dataset difficulty decreases, both the models average accuracy and response length decline, with the effect being most pronounced on the simplest dataset, where even response length decreases. This finding aligns with our previous analysis of Qwen-2.5-7B in 2.5, reinforcing the notion that Qwen inherently possesses strong reasoning capabilities. To further improve its response length, training should incorporate more challenging datasets, such as competition-level problems, to encourage deeper reasoning and extended thinking time. The analysis highlights key insight: zero RL training data must align with the base models inherent reasoning capabilities, such as considering metrics like its Pass@K performance."
        },
        {
            "title": "4 How Traditional SFT Influences RL-Driven Reasoning Emergence",
            "content": "As base models may not follow instruction well and pose major challenge for zero RL training, one may wonder simple SFT stage as cold start may be helpful to learn to follow instructions well. In this section, we revisit the impact of traditional SFT methods (where the responses are not from long CoT models) as cold start on RL training performance and reasoning behaviornotably, this was the most commonly used post-training pipeline with RL following an SFT stage, before DeepSeek-R1. Specifically, we use subset of the NuminaMath (Li et al., 2024) dataset derived from GSM8K and MATH 2, containing 2We also conduct experiments using general SFT dataset beyond math-related ones, which can be found in Appendix and implies similar conclusion. 10 SimpleRL-Zoo Figure 9: Reasoning behavior ratio over RL training iterations after using different SFT steps as starting points. Base refers to the base Mistral-Small-24B model without any SFT, while Step 100 and Step 500 represent 100 and 500 steps of SFT on the base model, respectively. As described in 2.2, we use GPT-4o to extract and track shifts in reasoning behaviors on OlympiadBench. approximately 15K high-quality short CoT responses. We conduct SFT using Mistral 24B on this data and select models at 100 and 500 training steps as starting points for RL training. In Figure 10, we illustrate how model accuracy and output length evolve during RL training when different initial models are used. Our results indicate that starting from SFT models initially boosts performance significantly; however, these models encounter notable limitations in their maximum achievable accuracy and response length compared to starting from the base model during extended RL training. Crucially, we observe that these limitations become increasingly pronounced as the number of initial SFT steps grows. For instance, while the base model can attain pass@1 accuracy of approximately 49.6% during RL training, models initialized with 100 and 500 SFT steps achieve maximum accuracies of only about 47.3% and 40.3%, respectively. Figure 10: Accuracy and response length averaged on the six benchmarks over RL training iterations after running different SFT steps as starting points. Base refers to the base Mistral-Small-24B model without any SFT, while Step 100 and Step 500 represent 100 and 500 steps of SFT on the base model, respectively. To further investigate how initial SFT affects the emergence of reasoning behaviors, we analyze how often specific reasoning behaviors appeared during training at different starting points, as shown in Figure 9. Our analysis reveals that initial SFT negatively impacts the development of critical reasoning behaviors. Specifically, models with 100 SFT steps exhibit reduced upper limits in essential reasoning behaviors such as enumeration, verification, and backtracking, compared to the base model. Even more notably, models with 500 SFT steps experience significant declines in enumeration and verification behaviors in later training stages, highlighting detrimental long-term effect of extensive sft on reasoning capabilities. This prompts reconsideration of whether traditional SFT inherently restricts model exploration, perhaps highlighting the need for future cold-start strategies to prioritize exploration capacitywhether by incorporating long CoT data (DeepSeek-AI et al., 2025a; Yeo et al., 2025) or designing SFT techniques (Li et al., 2025) that strike balance between imitation and explorationto enable sustained improvements in model reasoning performance."
        },
        {
            "title": "5 Conclusion",
            "content": "Our paper demonstrates the effectiveness of zero RL training across diverse range of base models, yielding significant improvements in reasoning accuracy and response length. We provide strong evidence that zero-shot RL training is not merely reranking, but rather genuine enhancement. Furthermore, we identify key factors such as reward design, data 11 SimpleRL-Zoo difficulty, and models inherent abilities that shape the emergence of advanced reasoning behaviors. Our findings also indicate that starting RL training from models with traditional SFT may limit the development of advanced reasoning behaviors. Overall, our work highlights key factors for effective zero RL training and offers insights for future model improvements."
        },
        {
            "title": "References",
            "content": "Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025. Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu. Generative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. 12 SimpleRL-Zoo Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. 13 SimpleRL-Zoo Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, Zhi-Quan Luo, and Ruoyu Sun. Preserving diversity in supervised fine-tuning of large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=NQEe7B7bSw. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. https: //github.com/sail-sg/understand-r1-zero, 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Mistral AI. Mistral small 3, January 2025. URL https://mistral.ai/news/mistral-small-3. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. SimpleRL-Zoo Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025a. Notion Blog. Weihao Zeng, Yuzhen Huang, Lulu Zhao, Yijun Wang, Zifei Shan, and Junxian He. BSTar: Monitoring and balancing exploration and exploitation in self-taught reasoners. In The Thirteenth International Conference on Learning Representations, 2025b. URL https: //openreview.net/forum?id=P6dwZJpJ4m. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Yixin Cao, Yang Feng, and Deyi Xiong (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.38. URL https://aclanthology.org/2024.acl-demos.38/. 15 SimpleRL-Zoo Figure 11: Comparison between simple prompts and more complex prompts."
        },
        {
            "title": "A Detailed Result of SimpleRL",
            "content": "Following the setup described in Section 2.2, we perform zero training on various base models. The trained models are then evaluated on multiple benchmarks, including GSM8K, MATH 500, Minerva Math, OlympiadBench, AIME2024, and AMC2023. The average results across all these benchmarks are presented in Figures 1 and 4. In this section, we provide more detailed results. Figure 12 illustrates the trends in accuracy and response length, while Figure 13 shows the trends in clip ratio and stopped length."
        },
        {
            "title": "B Training and Evaluation Details",
            "content": "We typically use the same set of hyperparameters to train and evaluate all models in the SimpleRL-Zoo series in default main experiment setting. B.1 Training We use prompt batch size of 1,024 and generate 8 rollouts per prompt, with maximum rollout length of 8,192 tokens. Training is performed using mini-batch size of 256. The default sampling temperature is set to 1.0, and the clip ratio is 0.2. For models ranging from 0.5B to 14B parameters, we use KL loss coefficient of 1e-4. For models larger than 14B, the KL loss coefficient is set to 1e-3. B.2 Evaluation We build our evaluation script based on Yang et al. (2024b), using temperature of 1.0 and maximum generation length of 16K tokens. To ensure consistency, we adopt the same prompt template used during training. For most benchmarks, we report pass@1 results. However, for AIME 2024, which contains fewer problems, we report both pass@1 and average accuracy (avg@32), computed over 32 generated samples per problem. SimpleRL-Zoo Figure 12: detailed evaluation of accuracy and response length throughout the training steps for various models. The x-axis represents the training steps, with the purple line showing the accuracy trend and the yellow line depicting the response length. 17 SimpleRL-Zoo Figure 13: detailed evaluation of clip ratio and stopped length throughout the training steps for various models. The x-axis represents the training steps, with the red line showing the clip ratio trend and the blue line depicting the average stopped length. SimpleRL-Zoo Init Model GSM8K MATH 500 Minerva Math Olympiad Bench AIME24 (pass@1) AMC23 Avg. 0 Step 10 Step 20 Step 200 Step 1000 Step 2000 Step 4000 Step 92.0 93.0 92.6 90.3 88.9 89.8 87.7 70.6 69.4 65.2 59.0 48.8 49.0 52.0 36.8 39.7 34.2 31.6 27.6 23.2 23.5 36.6 32.3 30.7 23.3 20.7 18.1 17. 16.7 10.4 6.7 2.1 2.5 0.8 2.1 45.0 44.1 38.4 26.9 18.1 20.3 21.6 49.6 48.2 44.6 38.9 34.4 33.5 34.0 Table 2: Experimental results from multiple Mistral-Small-24B models, each fine-tuned with different number of SFT steps on general SFT dataset for RL. The number of steps refers to the number of SFT steps applied. The reported benchmarks reflect the performance metrics on various evaluation benchmarks, measured using the model that achieved the best average performance after 100 iterations of reinforcement learning training."
        },
        {
            "title": "C Impact of General SFT on the Performance of Reinforcement Learning",
            "content": "We also investigated the general SFT setting beyond math-related datasets. In this setup, we first conducted SFT on Mistral-Small-24B using the widely adopted OpenHermes-2.5 dataset.3 We implement with LLaMA-Factory (Zheng et al., 2024) and adopt common hyperparameters of SFT, including 512 examples per batch with constant learning rate of 1e-5. For consistency with our other experiments, we fine-tuned the model using the Qwen chat template. After SFT, we preserved multiple checkpoints at different training steps, and nearly 800 steps correspond to 1 epochs on the SFT dataset. We then performed reinforcement learning on these models using identical hyperparameters as in our zero-RL training experiments. Table 2 presents our findings, with performance reported as the best results achieved during RL training up to 100 iterations. The results demonstrate an inverse relationship between SFT steps and subsequent RL performance: models with more SFT steps showed diminished performance after RL training. While the average performance after 10 SFT steps remained comparable to the base model, it still exhibited some negative effects. More significantly, models with more than 20 steps showed substantially reduced RL potential. Therefore, we conclude that RL training produces the best performance gain when applied directly to the base model without any supervised fine-tuning, i.e., the zero RL training. Impact of Exploration-Related Hyperparameters In this section, we examine the effects of exploration-related hyperparameters on zerotraining. Drawing inspiration from Zeng et al. (2025b), we focus on two key factors: sampling size (the number of responses per query) and sampling temperature. Sampling Size: We examine how varying sampling sizes {1, 4, 8, 16, 32} influence the training process using the Mistral 24B model; these results are presented in Figure 14. Our analysis reveals clear trend: as increases, the models average performance notably improves, and variability in response lengths becomes significantly more stable. For example, after 100 training steps, the scenario with = 32 achieves an average accuracy approximately 6 points higher than that with = 8. Conversely, smaller sampling sizes (N = 1 and = 4) cause training instability and potential collapse, indicated by rapid growth in generated length without corresponding accuracy improvements. We hypothesize that larger sample sizes enable the model to explore broader and more diverse training space, which stabilizes advantage estimation and sustains continuous performance improvement. 3https://huggingface.co/datasets/teknium/OpenHermes-2. 19 SimpleRL-Zoo Figure 14: Comparison of accuracy and response length using different sampling numbers = 1, 4, 8, 32. The training data is the Hard part (MATH lv.35) with the same setting in main results, as described in 2.2. Sampling Temperature: We conduct research on Qwen-2.5-0.5B to analyze the impact of sampling temperature during both training and evaluation on model performance. The results, presented in Figure 18 , indicate that training with higher temperatures generally leads to better average performance. For instance, models trained with temperatures of 1.0 and 1.2 outperform those trained with 0.8 and 0.6. Additionally, we find that the optimal evaluation temperature depends on the training temperature. Specifically, models trained at higher temperatures require higher sampling temperatures during evaluation, as using greedy sampling often results in repetitive outputs. Conversely, models trained at lower temperatures perform best when evaluated with lower sampling temperatures. SimpleRL-Zoo For Qwen2.5-Math-7B In this section, we conduct experiments on Qwen2.5-Math-7B (Yang et al., 2024a) using the hard part data, as described in 2.2, which consists of only 8K examples from MATH lv3-5. We apply both the PPO and GRPO algorithms to train our base model, and the overall evaluation results across training steps are shown in Figure 15. The final performance and response length for both algorithms converge to similar values, with GRPO slightly outperforming PPO. While the performance continues to improve, the response length does not exhibit similar trend. Specifically, the stopping length for both algorithms remains relatively unchanged, and fluctuations in the average response length are primarily attributed to changes in the clip ratio. There are two main reasons for this behavior: First, the maximum context length for Qwen2.5-Math-7B is 4K, which is limited compared to other models with context lengths exceeding 8K, leading to high clip ratio. Second, as math-specific model, Qwen2.5-Math-7B already performs very well on MATH, the dataset we used for training, so it may not face enough challenge to further extend its response length. Therefore, we hypothesize that more challenging data might be needed to push this capable model further."
        },
        {
            "title": "F Reasoning Behavior Analysis",
            "content": "We apply Gandhi et al. (2025)s cognitive behavior framework to perform detailed analysis of how model reasoning behaviors change during zero training. We first describe our analysis setup, then compare reflection keyword tracking against this framework to monitor reflective behaviors. Finally, we use case studies to illustrate how the reasoning behaviors of various models evolve during training. F.1 Setup We use GPT4-o to identify and analyze the following key reasoning behaviors exhibited in the models responses, with the prompt shown in Figure 16: 20 SimpleRL-Zoo Figure 15: Comparison of accuracy and response length between PPO and GRPO on Qwen2.5-Math-7B. The base model is trained using 8K examples from MATH lv3-5, with the same settings described in 2.2. (1) Backtracking: The model actively identifies errors during response generation and explicitly revises previously used methods. (2) Verification: The model systematically checks intermediate results to ensure correctness. (3) Subgoal Setting: The model decomposes complex problems into smaller, manageable steps. (4) Enumeration: The model exhaustively considers multiple cases or possibilities to solve problems. Note that we replaced Backward Chaining with Enumeration, as the former was not relevant to our task. F.2 Comparison of Different Reasoning Behavior Tracking Methods Using DeepSeek Maths zero-training process as an example, we compare two different methods for monitoring reasoning behavior. The first method tracks the occurrence of specific keywords in the models responses, such as recheck, rethink, try again, wait, alternatively, retry, and however. The second method employs (Gandhi et al., 2025)s cognitive framework for evaluation. Figure 17 illustrates the observed changes in reasoning behavior according to these two approaches. During the training process, we observe that the proportion of specified keywords in the DeepSeek math models responses remains consistently low, exhibiting minimal variation. Conversely, reasoning behaviors identified by the cognitive framework demonstrate significant upward trend. To understand this intriguing discrepancy, we manually review the reasoning behaviors recorded by the cognitive framework. Our analysis reveals that many of these reasoning behaviors do not necessarily involve the predefined keywords. For instance, in Figure 19, the observed reasoning behaviors include Verification and Backtracking, neither of which contains the specified keywords. This indicates that keywords alone cannot effectively distinguish or capture the nuanced differences between such behaviors. Similarly, in Figure 20, the reasoning process involves implicit verification steps, including recalculating intermediate results such as the dot product and magnitudes before determining the cosine of the angle. Again, these subtle verification steps are not represented by the designated keywords. In Figure 22, the reasoning involves considering multiple possible scenarios or outcomes. This type of exploratory reasoning is also inadequately captured by keywordbased approaches. These examples collectively illustrate that relying solely on keyword presence is insufficient for accurately identifying and differentiating complex reasoning behaviors within model responses. 21 SimpleRL-Zoo Figure 16: Prompt for identifying and analyzing reasoning behaviors. F.3 Reasoning Behavior Variations Across Different Models We present cases illustrating notable improvements in model reasoning behavior during training (Figure 5). Specifically, these improvements are demonstrated in the following models: Mistral 24B (Figure 6 and Figure 23), Qwen 2.5-0.5B (Figure 24, Figure 25 and Figure 26), Qwen 2.5-1.5B (Figure 27 and Figure 28), DeepSeek-math-7B-base (Figure 19, Figure 20, Figure 21 and Figure 22), and Llama 3.1-8B (Figure 29 and Figure 30). 22 SimpleRL-Zoo Figure 17: Changes in reflection behavior identified by different methods. Figure 18: Impact of training and evaluation temperatures on Qwen2.5-0.5bs average final performance (x-axis: evaluation temp, y-axis: training temp). SimpleRL-Zoo Figure 19: comparison of DeepSeek-Math-7Bs Backtracking behavior before and after zero RL training. Here, base solution represents the response of the DeepSeek-Math-7B base model, while zero solution represents the response of the model after training. 24 SimpleRL-Zoo Figure 20: comparison of DeepSeek-Math-7Bs Verification behavior before and after zero RL training. Here, base solution represents the response of the DeepSeek-Math-7B base model, while zero solution represents the response of the model after training. Here involves implicit verification steps, including recalculating intermediate results such as the dot product and magnitudes before determining the cosine of the angle. SimpleRL-Zoo Figure 21: comparison of DeepSeek-Math-7Bs Verification behavior before and after zero RL training. Here, base solution represents the response of the DeepSeek-Math-7B base model, while zero solution represents the response of the model after training. This demonstrates more explicit verification, including key phrases like Lets check. 26 SimpleRL-Zoo Figure 22: comparison of DeepSeek-Math-7Bs Enumeration behavior before and after zero RL training. Here, base solution represents the response of the DeepSeek-Math-7B base model, while zero solution represents the response of the model after training. SimpleRL-Zoo Figure 23: comparison of Mistral-Small-24Bs Enumeration behavior before and after zero RL training. Here, base solution represents the response of the Mistral-Small-24B base model, while zero solution represents the response of the model after training. 28 SimpleRL-Zoo Figure 24: comparison of Qwen-2.5-0.5Bs Verification behavior before and after zero RL training. Here, base solution represents the response of the Qwen-2.5-0.5B base model, while zero solution represents the response of the model after training. SimpleRL-Zoo Figure 25: comparison of Qwen-2.5-0.5Bs Backtracking behavior before and after zero RL training. Here, base solution represents the response of the Qwen-2.5-0.5B base model, while zero solution represents the response of the model after training. 30 SimpleRL-Zoo Figure 26: comparison of Qwen-2.5-0.5Bs Enumeration behavior before and after zero RL training. Here, base solution represents the response of the Qwen-2.5-0.5-B base model, while zero solution represents the response of the model after training. SimpleRL-Zoo Figure 27: comparison of Qwen-2.5-1.5Bs Verification behavior before and after zero RL training. Here, base solution represents the response of the Qwen-2.5-1.5B base model, while zero solution represents the response of the model after training. 32 SimpleRL-Zoo Figure 28: comparison of Qwen-2.5-1.5Bs Enumeration behavior before and after zero RL training. Here, base solution represents the response of the Qwen-2.5-1.5B base model, while zero solution represents the response of the model after training. SimpleRL-Zoo Figure 29: comparison of Llama-3.1-8Bs Verification behavior before and after zero RL training. Here, base solution represents the response of the Llama-3.1-8B base model, while zero solution represents the response of the model after training. 34 SimpleRL-Zoo Figure 30: comparison of Llama-3.1-8Bs Subgoal Setting behavior before and after zero RL training. Here, base solution represents the response of the Llama-3.1-8B base model, while zero solution represents the response of the model after training."
        }
    ],
    "affiliations": [
        "BUPT",
        "HKUST",
        "TikTok"
    ]
}