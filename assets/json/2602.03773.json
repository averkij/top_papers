{
    "paper_title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL",
    "authors": [
        "Ian Wu",
        "Yuxiao Qu",
        "Amrith Setlur",
        "Aviral Kumar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training."
        },
        {
            "title": "Start",
            "content": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Ian Wu1, Yuxiao Qu1, Amrith Setlur1 and Aviral Kumar1 1Carnegie Mellon University 6 2 0 2 3 ] . [ 1 3 7 7 3 0 . 2 0 6 2 : r Figure 1: Left: Illustration of the RC algorithm. RC decoding replaces standard autoregressive decoding at both train and test time. During RC decoding, the LLM generates reasoning trace, summarizes it, discards the original trace, and conditions subsequent reasoning on this summary. This design decouples the effective reasoning horizon from the length of any single reasoning trace, thus maintaining tractable rollout lengths for outcome-reward RL while also enabling extrapolation at test time. Right: Performance on HMMT 2025 (November) vs. reasoning token budget. Our RC-trained model RCT-4B (blue, trained from Qwen3-4B-Instruct-2507 at 16k train budget) extrapolates to outperform both the base model with RC decoding (green) and the specialized Qwen3-4B-Thinking-2507 reasoning model (evaluated at 256k test tokens). Abstract: Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training 4B model with RC using 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training. 1. Introduction Large language models (LLMs) exhibit the ability to solve complex problems by generating long reasoning traces at test time. As LLMs become more capable, we naturally expect them to be able to solve harder tasks by reasoning for longer, even without external supervision. This expectation mirrors human cognition: humans continually improve their reasoning by revisiting earlier conclusions and reallocating effort to discover new information in new ways over the course of problem solving. We would like our models to behave similarly, such that additional reasoning at test time translates to improved performance over horizons much longer than any single reasoning step. This underlies the notion of in-context exploration [31], which learns an implicit algorithm or procedure for allocating test-time compute such that spending more computation systematically improves outcomes. If learned robustly, in-context Corresponding author(s): ianwu@andrew.cmu.edu Code repository: github.com/IanYHWu/rc Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL exploration should enable models to continually improve over long horizons at test time, perhaps over hours or even days, and across millions of tokens, all without modifying the model weights. However, current LLM training paradigms limit the forms of in-context exploration that can be learned. Supervised fine-tuning (SFT) teaches models to imitate the content of reasoning traces, rather than learning the algorithm that generates them [4, 30]. This inhibits the learning of systematic reasoning procedures and thus the ability to perform effective in-context exploration. While reinforcement learning (RL) incentivizes models to learn reasoning procedures rather than mere imitation [36, 45], RL is typically applied over fixed prompt distribution and bounded training rollout length. As result, models are optimized to exploit this finite budget, rather than to extrapolate beyond it. When such models encounter complex problems that require more reasoning to solve, two failure modes emerge. First, they may prematurely terminate within their training budgets and fail to make progress by reasoning for longer. Second, when models continue beyond this budget, distribution shift may occur as generation proceeds from conditional distributions that differ substantially from those seen during training; reasoning traces in this regime are often repetitive and verbose [18, 31]. This raises natural question: how can we train models to extrapolate their reasoning far beyond their training configurations? To answer this question, we introduce Reasoning Cache (RC), an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. In RC, the model generates reasoning trace, summarizes it (the cache) and discards the original trace over multiple turns, with subsequent generation conditioning only on the previous summary rather than the full reasoning trace. For training, we introduce an RL approach that teaches the model to exploit RC by improving its summary-conditioned reasoning capabilities. Our approach combines typical on-policy RL with off-policy learning via replay buffer that enables reuse of cached summaries, allowing the model to train over long effective horizons without generating prohibitively long training rollouts. The design of the decoding process used by RC is motivated by two key observations. First, iterative decoding provides explicit control over test-time compute: we can scale the number of reasoning iterations while keeping each step within the training distribution, mitigating distribution shift as effective reasoning length grows. Second, making such iteration effective requires consistent progress across steps. RC achieves this by exploiting summarizationgeneration asymmetry: models are often better at summarizing prior reasoning and reasoning from summaries than at producing correct solutions from scratch. Our RL objective explicitly amplifies this asymmetry, enabling larger improvements across successive iterations. Empirically, models trained with RC exhibit strong and consistent extrapolation ability. While RC decoding alone is already effective when the base model can reliably follow instructions and reason from summaries, RC-trained models extrapolate substantially further. On the mathematical reasoning benchmarks HMMT 2025 (November) and IMO-AnswerBench, our RC-trained RCT-4B model substantially outperforms both Qwen3-4B-Instruct-2507 (the base model) and Qwen3-4B-Thinking-2507 (a much stronger reasoning model) through extrapolation. For example, accuracy on HMMT 2025 improves from 40% to 70% as the test budget is scaled from 16k to 512k tokens (Figure 1, right), while on IMO-AnswerBench [19], performance rises from 34% to nearly 50% at 256k tokens, surpassing larger models such as Qwen3-30BA3B-Instruct-2507 and gpt-oss-20b using standard autoregressive decoding, despite training only at 16k budget. Notably, despite having been trained solely on mathematical reasoning data, RCT-4B also achieves substantially higher performance than the base model on the FrontierScience [22] scientific reasoning benchmark when extrapolated to 256k tokens, suggesting that RC induces transferable algorithmic behavior rather than domain-specific knowledge. Finally, RCT-4B is consistently better at exploiting test-time scaffolds than both the base model and base model post-trained with standard RL, with and 2 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL without RC decoding. This indicates that optimizing for summary-conditioned generation yields models that are generally more effective at leveraging external context to guide downstream reasoning. 2. Related Work Long-horizon reasoning by scaling test-time compute. Increasing the length of reasoning traces is fundamental approach to scaling test-time compute [9, 40], as longer traces allow for more extensive in-context exploration that can substantially improve performance [7, 31, 45]. Although prior work finds that on-policy RL is effective at teaching LLMs to generate longer autoregressive responses [9], scaling in this way suffers from fundamental limitation: models cannot reliably extrapolate much beyond the reasoning lengths seen during RL training [18, 31]. Our work therefore aims to develop methods that enable extrapolation, rather than merely scaling reasoning lengths by increasing training budgets. Test-time extrapolation of reasoning. Prior work attempts to enable extrapolation mainly through one of two approaches. The first modifies training via carefully designed datasets and curricula to encourage in-context exploration [2, 18, 31]. Although this has been shown to enable extrapolation to around 3-4 the training budget, performance typically saturates beyond this. The second approach modifies the RL reward structure to implicitly encourage extrapolation (e.g., via dense rewards that credit intermediate reasoning segments [27]) but retains pure autoregressive decoding at inference time. In both cases, in-context exploration behaviors are implicitly learned through free-form autoregressive generation and thus remain coupled to length budgets in the training setup. When test-time conditional distributions fall outside the training support, as is seen when test lengths greatly exceed train lengths, these approaches do not improve further and instead result in verbose and repetitive behavior [18, 31]. Iterative decoding for scaling test-time compute. Prior work explores prompting LLMs to iteratively transform their own outputs to scale test-time compute, ranging from simple self-correction [12, 13] and self-refinement [20, 35] to more complex scaffolds that combine iterative and parallel compute [33, 38]. Others consider training models to apply transformations more effectively rather than relying on prompting alone: Venkatraman et al. [38], for example, train models to aggregate parallel reasoning traces, while Qu et al. [24] and Kumar et al. [14] train models to self correct. RC instead uses iterative decoding to enable extrapolation beyond training horizons rather than only improving performance at fixed budgets. Furthermore, we find that RC training actually enables models to better exploit test-time scaffolds by teaching models to leverage guidance to improve future reasoning. Memory in multi-turn interaction. RC summaries can be viewed as compressed memory states that are updated as the policy acts over iterations. Prior work primarily consider using similar memory states to store external context (e.g. retrieved web pages, user responses etc.) that is dynamically recalled in later steps, often as part of multi-turn question-answering or conversation systems [16, 47]. Our work instead uses the memory states to store abstractions of self-generated reasoning traces for solving reasoning problems, line of work that has been explored using prompting-based approaches [10, 37, 41]. Our work instead trains the model to better utilize its memory store for downstream reasoning, which we show yields significant improvements over prompting-only methods. 3. Preliminaries and Notation Consider policy ğœ‹ğœƒ(ğ‘¥) over token sequences that generates tokens autoregressively conditioned on ğ‘¥. At test time, the policy is given token budget and allocates this to reason. Our interest is the test-time performance as function of the test token budget, particularly with large test-time budgets. 3 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Standard RL training for LLM reasoning. Let ğ’Ÿtrain denote training distribution of prompt-answer pairs (ğ‘¥, ğ‘¦). On-policy reinforcement learning (RL) optimizes the expected reward of the policy: Eğ‘¥,ğ‘¦ğ’Ÿtrain [Eğ‘§ğœ‹ğœƒ(ğ‘¥)[ğ‘Ÿ(ğ‘¦, ğ‘§)] ] , max ğœ‹ğœƒ s.t. ğ‘§ ğ»train. (Training objective) (3.1) Here, ğ‘§ denotes an on-policy rollout autoregressively sampled from ğœ‹ğœƒ. The rollout encodes reasoning trace and is generated within fixed training budget ğ»train. The reward function ğ‘Ÿ(ğ‘¦, ğ‘§) evaluates the correctness of the rollout, typically by extracting the final answer from ğ‘§ and comparing it against the ground-truth label ğ‘¦. To solve this optimization problem, we can use outcome-reward policy-gradient methods: one common choice is GRPO [32] (see Appendix H), which we use throughout this work. Test-time extrapolation of LLM reasoning. Equation 3.1 optimizes performance only over the empirical distribution of training prompts ğ’Ÿtrain, and only under fixed ğ»train. At test time, we may want to ensure that our trained model attains high accuracy on different prompt distribution and under larger budget: TestPerf(ğœ‹ğœƒ) def := Eğ‘¥,ğ‘¦ğ’Ÿtest [Eğ‘§ğœ‹ğœƒ(ğ‘¥)[ğ‘Ÿ(ğ‘¦, ğ‘§)] ] , s.t. ğ‘§ ğ»test (Test-time objective) (3.2) where ğ»test is the test budget; in general, the training and test distributions differ (i.e. ğ‘train(ğ‘¥) = ğ‘test(ğ‘¥) and ğ»test ğ»train). When model trained to optimize Equation 3.1 can leverage larger test budget to achieve TestPerf(ğœ‹ğœƒ)ğ»test > TestPerf(ğœ‹ğœƒ)ğ»train , we say that it can extrapolate. 4. Problem Statement Does optimizing performance at ğ»train (Equation 3.1) also optimize extrapolation at ğ»test (Equation 3.2)? Unfortunately, the answer is no. During training, the model receives positive reward only for rollouts that terminate within ğ»train tokens. This implicitly penalizes longer rollouts and encourages premature termination near ğ»train at test time. Moreover, when the model does continue beyond ğ»train at test time, it must effectively operate on the sorts of conditional distributions it was never trained on. While this form of distribution shift is not problematic if the model has learned true operator [27] that enables the chaining of behaviors to solve problems, it is unclear whether RL can learn such operators from finite, fixed prompt set. We instead circumvent this challenge by altering the decoding algorithm such that the model never encounters significant distribution shift even when reasoning at long horizons. Why do we need extrapolation? Can we simply increase ğ»train to match ğ»test during RL training? Doing so would obviate extrapolation altogether. However, there are two main problems with this approach. First, any new test distribution we encounter may contain harder problems requiring ğ»test ğ»train to solve, and so it would be better to train models that can adapt to larger test budgets on-the-fly (i.e., training models to continually adapt). Second, memory and compute costs, as well as the effectiveness of RL training, scale aggressively with response length, making long-horizon on-policy RL prohibitively expensive and challenging. These challenges indicate that we cannot just scale ğ»train, and should instead train models to extrapolate as more compute is provided. 5. Enabling Extrapolation with Reasoning Cache Our goal is to develop method that trains models under fixed token budget and finite prompt set, while still allowing them to extrapolate beyond the training horizon. To achieve this, we replace autoregressive decoding with an iterative decoding algorithm Alg(ğœ‹ğœƒ; ğ‘¥) during training and inference. This algorithm, Alg, leverages the structure of reasoning along with asymmetries present in LLMs to 4 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 2: Illustrative example of RCs output. The model generates reasoning given an input problem, before conditioning on the reasoning to generate summary between 1-2 paragraphs long. The model then conditions on the summary to generate new reasoning. As we show later, RC training improves the ability of the model to perform summary-conditioned reasoning, which enables the model to continually improve over long horizons. See Appendix for full example of an RC output. support long-horizon reasoning at test time while remaining amenable to training under much smaller ğ»train. We begin by concretizing the key desiderata that Alg should satisfy. Choosing an effective decoding algorithm. An effective choice of Alg must satisfy two desiderata. First, it should define an iterative procedure in which the number of iterations monotonically controls test-time compute, while each iteration operates on conditional distributions that remain close to those encountered during training. An Alg satisfying this desideratum avoids the main limitations of autoregressive decoding in standard RL: (1) by enabling longer responses through increasing the iteration limit, it mitigates premature termination biases induced by fixed-length RL training; and (2) by restricting autoregressive generation within each iteration to at most ğ»train tokens, it reduces traintest distribution shift even when the effective reasoning horizon is much larger. Second, the algorithm should retain expressivity comparable to autoregressive decoding, allowing each iteration to refine or extend the current rollout and explore new directions. An Alg that satisfies this property can make consistent progress across many iterations, enabling continual extrapolation over long horizons. 5.1. RC: Multi-Turn Decoding Algorithm We now introduce decoding algorithm that satisfies these desiderata. Our algorithm, which we call Reasoning Cache (RC), is an iterative decoding approach that alternates between response generation and summarization. Being an iterative decoding algorithm, RC naturally fulfills our first desideratum: we can increase test-time compute by increasing the number of summarization-generation turns, while also avoiding significant shifts in the conditional distributions encountered at each turn by only ever autoregressively generating at bounded lengths ğ»train ğ»test. To satisfy our second desideratum, RC relies on two properties of LLMs. First, reasoning traces contain redundant tokens: many tokens encode steps that are useful for local progress but need not be retained verbatim to guide future actions. This 5 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 3: Left: Accuracy vs. test-time token budget. RC decoding improves performance as token budget ğ»test is increased far beyond ğ»train = 16k. Middle: Total tokens used vs. test-time token budget. Total reasoning tokens used by RC increases linearly as we increase the reasoning token budget. Shaded regions indicate the 5th-95th percentile; lines indicate the mean. Right: Accuracy vs. token budget for iterative decoding methods. RC is more effective method for enabling extrapolation than self-verification and self-refinement, highlighting the benefits of exploiting the summarization-generation gap. allows us to discard significant portion of tokens (e.g. via summarization) so long as key information is retained. Second, as we consistently find in our experiments, base LLMs often exhibit summarizationgeneration asymmetry, in that producing correct response conditioned on summary of previous attempt is easier than generating correct response from scratch; this asymmetry arises from the instruction-following abilities of LLMs, which allows them to use summaries of prior generations to guide subsequent reasoning. RC exploits this by periodically compressing responses into cache and conditioning subsequent generation on it, allowing the model to refine, extend, or restart reasoning across iterations as needed. See Figure 2 for an example of RCs outputs. Let ğ‘¥ denote the prompt and let ğ‘¡ index the decoding turn. RC maintains: (1) reasoning trace ğ‘§(t) ğ‘… and (2) summary ğ‘§(t) is generated under ğ‘† fixed token budget ğ»ğ‘…, while ğ‘§(t) is generated under ğ»ğ‘† ğ»ğ‘…. Decoding proceeds by alternately ğ‘† prompting the base model with two distinct system instructions â„ğ‘… and â„ğ‘† (see Appendix N): initialized to the empty string. At each turn, ğ‘§(t) ğ‘… , with ğ‘§(0) ğ‘† ğ‘§(t) ğ‘… ğœ‹ğœƒ ğ‘§(t) ğ‘† ğœ‹ğœƒ ( â„ğ‘…, ğ‘¥, ğ‘§(t1) ( â„ğ‘†, ğ‘¥, ğ‘§(t) ğ‘† ğ‘… , ğ‘§(t1) ğ‘† ) , ) . (5.1) (5.2) â„ğ‘… instructs the model to generate reasoning conditioned on the current cache, while â„ğ‘† instructs the model to compress the current reasoning trace and previous summary into an updated summary that encodes high-level information about the strategies employed and conclusions reached in previous turns. After turns, the final output is given by ğ‘§ := ğ‘§(T) . We denote this iterative process (see Figure 1) as: ğ‘… ( ğ‘§(1) ğ‘… , ğ‘§(1) ğ‘† , . . . , ğ‘§(T1) ğ‘† , ğ‘§(T) ğ‘… ) Alg(ğœ‹ğœƒ; ğ‘¥). (5.3) Extrapolation with RC. Because each step is allocated fixed budget ğ»ğ‘… + ğ»ğ‘†, the total effective budget def under RC is T(ğ»ğ‘… ğ»ğ‘†). Since ğ»ğ‘… ğ»ğ‘†, we drop ğ»ğ‘† and approximate the budget as Tğ»ğ‘… := ğ»test. If performance improves in the regime ğ»ğ‘… ğ»train, we say that RC enables extrapolation. 5.2. Experimental Evaluation Experimental setup. We now validate whether LLMs possess the ability to to utilize RC without additional training. We evaluate RC decoding with Qwen3-4B-Instruct-2507 and Qwen3-30B-A3B-Instruct2507 [28], two hybrid LLMs capable of both complex reasoning and instruction-following (see Appendix 6 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 4: Left: Accuracy at different levels of summary detail. All accuracies measured at ğ»test = 192ğ‘˜; red dotted line indicates base model performance without RC. Performance degrades when summary detail is either too high or too low. Middle: Relative accuracy improvement (over standard autoregressive decoding). Replacing Qwen3-4B-Instruct-2507 with Qwen3-4B-Thinking-2507 for summarization (Think, Inst) reduces gains. Using Qwen3-4B-Thinking-2507 for both tasks (Think, Think) further reduces gains. Right: Accuracy vs. test-time token budget with various ğ»ğ‘…. Reducing ğ»ğ‘… from 16k to 8k leaves RC performance unchanged, whereas decreasing it further to 4k negatively impacts performance. for similar results from another model family). Using these LLMs as our base models, we run RC decoding for = 12 turns with ğ»ğ‘† = 2048 and ğ»ğ‘… = 16k, giving us total budget of ğ»test = 192k. This is far larger than both models ğ»train, which we estimate to be about 16k (see Appendix for evidence justifying this). We use the November version of HMMT 2025 as our evaluation dataset (this competition was conducted after the base models were released), and generate 16 RC outputs per problem. Finding 1: RC enables extrapolation. We plot how accuracy evolves with the token budget ğ»test in Figure 3 (left). We find that RC extrapolates reasoning far beyond ğ»train = 16k: the 4B model accuracy increases by 17% as the test token budget is scaled from 16k to 192k, while the 30B model accuracy increases by 12%. We also plot how actual token usage varies with reasoning token budget in Figure 3 (middle). We find that the cumulative number of tokens used scales linearly with the provided budget, which indicates that the model utilizes additional test time compute as provided and does not substantially shorten its responses at later iterations. Overall, our findings demonstrate that RC satisfies both desiderata outlined in Section 5 and thus enables effective extrapolation. Finding 2: Summary-based abstractions are key to effective extrapolation. We examine the role of summarizationgeneration asymmetry by experimenting with iterative decoding methods that do not utilize summarization. Concretely, we remove the summary step and instead condition each iteration directly on the full response from the previous iteration ğ‘§(t) , and prompt Qwen3-4B-Instruct-2507 (ğ»ğ‘… = ğ‘… 16k) to either verify-then-correct (self-verify) or self-refine its solution (details in Appendix J). Figure 3 (right) shows that RC consistently outperforms these baselines across all ğ»test values, demonstrating that summary-conditioned generation provides benefits over other iterative methods. We attribute this to two factors: first, conditioning on summaries keeps context lengths bounded and in-distribution, whereas iterating on raw responses exceeds ğ»train and induces distribution shift. Second, summarization serves to remove redundant distractor tokens [11, 17] that obfuscate key findings and other important information, thereby yielding clearer guidance for subsequent reasoning. Finding 3: Summary detail level matters. Next, we study how much information summaries should retain. We vary the prompt â„ğ‘† to produce summaries of differing detail, ranging from answer-only to multiple paragraphs (Figure 21), and compare these to our default approach and to full-trace conditioning (self-refinement). Figure 4 (left) reports the accuracy of Qwen3-4B-Instruct-2507 at ğ»test = 192k across various choices of â„ğ‘†. Performance degrades with very short summaries, improves as more detail is added, and peaks with 2-paragraph summaries; omitting summarization entirely degrades performance. 7 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Finding 4: Base models must be good instruction-followers for RC to be effective. We replace Qwen3-4B-Instruct-2507 with the specialist reasoning model Qwen3-4B-Thinking-2507, which excels at reasoning but possesses weaker instruction-following abilities and thus less clear summarization-generation asymmetry. We evaluate using the reasoning model only for summaryconditioned generation (Think, Inst) and for both summary generation and summary-conditioned generation (Think, Think) (ğ»ğ‘… = 64k). Figure 4 (middle) shows that Think, Inst only achieves half the accuracy gains of Inst, Inst, while Think, Think achieves even smaller gains; qualitative inspection reveals that the reasoning model sometimes ignores summaries during generation and omits key details during summarization. However, note that RC nonetheless provides positive performance gains in all configurations, suggesting that sufficient asymmetry is present even in the reasoning model for RC training to potentially be effective. We leave further exploration of this to future work, and focus instead on training the instruction-following model in our experiments. Figure 5: Termination rates of Qwen34B-Instruct-2507 with RC as function of length. Measured on HMMT 2025 and across all = 12 turns. Virtually all reasoning traces terminate within 16k tokens, suggesting that the model is not trained to reason beyond this. We consider trace to have terminated after it generates boxed{}. Finding 5: Reducing ğ»ğ‘… by too much degrades performance. By default, we set ğ»ğ‘… = ğ»train = 16k, and since LLMs rarely generate traces longer than ğ»train, we only consider decreasing it. Figure 4 (right) shows that reducing ğ»ğ‘… from 16k to 8k has minimal impact despite the fraction of incomplete traces increasing from 0% to 20% (see Figure 5). Reducing ğ»ğ‘… further to 4k causes nearly 50% of traces to terminate early, this time substantially degrading performance. This implies that while ğ»ğ‘… can be decreased, it must still be large enough for redundancy to emerge. When ğ»ğ‘… is set too small, the resulting summaries capture only shallow progress that provides insufficient signal for continuation, which encourages the model to restart reasoning from scratch instead. 5.3. Analysis of Summary-Conditioned Generations We analyze the content of summary-conditioned generations produced by RC and find that they commonly exhibit three high-level strategies: (1) verification, where the model generates reasoning to explicitly verify intermediate or final results stated in the summary; (2) exploration, where the model deliberately pursues different strategy from that used in the summary; and (3) refinement, where the model acknowledges the summary and repeats the same strategy without attempting verification or exploring alternatives. To quantify the prevalence of these behaviors, we extract summaries and their subsequent reasoning traces and pass them to an LLM-based annotator (Figure 22), which assigns each sample to one of the three categories above, as well as none category when no references to the summary are made and reasoning restarts from scratch. Figure 6 shows that the model relies heavily on summaries to guide subsequent generations, with very few samples classified as none. The most common strategy is verification, although substantial fraction of samples also exhibit exploration and refinement. Figure 6: Percentage of RC reasoning traces that employ various reasoning strategies. Verification of previous reasoning is the most common strategy, followed by exploration of new strategies. 8 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 7: Illustration of RC rollout generation for training with replay buffer â„¬. We sample an input and summary from â„¬, and use this as the starting point to run Ttrain steps of RC decoding. New summaries generated through this process are stored in â„¬, replacing older summaries corresponding to the same input problem. We then sample from the Ttrain newly generated summaries, and condition on these to generate training rollouts that are optimized via Equation 6.1. Key Takeaways: Inference-only experiments with RC RC enables extrapolation even without RC-specific training. It exploits the summarizationgeneration asymmetry and is thus most effective when paired with instruction-following models. 6. Training Models to Extrapolate with RC Decoding Having established the design of Alg, we now describe our method for training models to use it. Our analysis in Section 5.2 shows that RC extrapolation depends on the models ability to iteratively reason from and improve upon summaries of past iterations. Accordingly, our training objective is to strengthen summary-conditioned generation: given problem and summary, we should train our model to generate improved reasoning that is more likely to yield correct answer. The iterative structure of RC makes this objective amenable to standard outcome-reward RL as we can run RC for multiple iterations and apply gradient updates independently at each turn. By setting ğ»ğ‘… to typical value for the base model, such that most responses end with final answer, we can assign outcome-based rewards at each step and avoid credit assignment across iterations. While this approach is indeed myopic, we note that the iterative nature of RC decoding ensures that this does not reinforce the resulting bias of premature termination. Formally, we run RC for Ttrain turns for each problem ğ‘¥ in training batch. We collect the summaries generated from this ğ‘§ğ‘† := (ğ‘§(1) ) and uniformly sample ğ‘summ Ttrain unique summaries per problem. We then generate ğ¾ reasoning traces conditioned on each sampled summary, assign outcome rewards and compute advantages over these samples: ğ‘† , . . . , ğ‘§(Ttrain) ğ‘† max ğœ‹ğœƒ Eğ‘¥,ğ‘¦ğ’Ÿtrain, ğ‘¡ğ‘ˆ [1,Ttrain] [ ğ‘§ğœ‹ğœƒ(ğ‘¥,ğ‘§(ğ‘¡) ğ‘† ) ] [ğ‘Ÿ(ğ‘¦, ğ‘§)] , where (ğ‘§(1) ğ‘† , . . . , ğ‘§(Ttrain) ğ‘† ) Alg(ğœ‹ğœƒ; ğ‘¥), s.t. ğ‘§ ğ»ğ‘…. (6.1) This design is effective because, under the problem structure targeted by RC, optimizing each iteration locally is aligned with optimizing the full reasoning trajectory. By training each step to produce correct answer, we implicitly encourage the model to generate reasoning traces and summaries that contain information, such as intermediate results or partial analyses, that is useful for subsequent steps. Because training rollouts are conditioned on these summaries, the model is also explicitly trained to exploit this 9 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL accumulated information when generating later responses, increasing the likelihood of future correctness. Under the assumption that successive summarization and summary-conditioned generation steps make roughly monotonic progress toward solution, optimizing per-step correctness is sufficient to improve trajectory-level performance, despite only explicitly optimizing individual steps. We discuss limitations of this assumption and evaluate alternative training approaches in Appendix E. Training with summary replay buffer (off-policy reinforcement learning). The iterative decoding structure of RC naturally enables learning from off-policy summaries because summaries serve as conditioning inputs rather than optimization targets. Learning from off-policy summaries provides two benefits. First, it enables training on later reasoning turns without the need for us to generate long on-policy decoded trajectories, which is useful because summaries from later turns may qualitatively differ from earlier ones. Second, it increases the coverage of summaries the model encounters during training, which in turn increases the robustness of the policy to test time shifts in the summary distribution. In fact, improving state coverage via replay buffer is one of the fundamental principles in off-policy RL [6], which using past summaries enables us to implement. We therefore incorporate summary replay buffer into training; see Figure 7. During the first training epoch, we follow the same on-policy procedure to optimize Equation 6.1, but also store all generated summaries, and their corresponding problems, in the replay buffer â„¬. From the second epoch onward, we sample problems and summaries from â„¬ and condition RC rollouts on them instead of generating fresh summaries, thereby extending the maximum effective training horizon by Ttrain steps per epoch. 7. Experimental Evaluation: Training with RC The goal of our experiments is to demonstrate the effectiveness of training with RC to improve extrapolation. To this end, we evaluate our approach on several benchmarks, compare it with related methods, and also conduct several ablation experiments to isolate the effects of our different design choices. Training details. We post-train Qwen3-4B-Instruct-2507 model to utilize RC and refer to the trained model as RCT-4B. We set ğ¾ = 8, ğ‘summ = 2 and Ttrain = 3. We conduct training in two stages: in Stage I, we train without the summary replay buffer, focusing on optimizing early turns, including the initial turn (without any summary context). Training problems for Stage are subsampled from the AceReason-Math dataset [3], resulting in dataset of about 5.7k problems. For Stage II, we enable the summary replay buffer, and construct new training set by injecting small number of difficult problems from DAPO [44] into our Stage dataset as part of our training curriculum. See Appendix for details. Benchmarks and evaluation protocols. We evaluate RCT-4B on three math reasoning benchmarks: AIME 2025, HMMT 2025 (November version), and IMO-AnswerBench [19], as well as one scientific reasoning benchmark, FrontierScience (Olympiad) [22], which contains expert-written problems in physics, chemistry, and biology. Since our training data exclusively consists of mathematical reasoning problems, FrontierScience serves to assess whether learned extrapolation behavior generalizes to an unseen domain. We evaluate the mathematical reasoning benchmarks by verifying final answers and follow the official LLM judge-based evaluation protocol for FrontierScience [22]. We selected the latter three benchmarks for their low contamination risk: all three were released after our training datasets and our base model, with HMMT 2025 (Nov) and FrontierScience consisting entirely of new problems and IMO-AnswerBench consisting of extensively rewritten past Olympiad problems. Baselines and comparisons. We compare against three categories of approaches. The first consists of autoregressive decoding methods using open-source 4B models. These include Qwen3-4B-Instruct10 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 8: Accuracy on various reasoning benchmarks as function of token budget for Qwen3-4B-Instruct-2507 and RCT-4B with RC decoding. RCT-4B improves performance across all four reasoning benchmarks as reasoning token budget ğ»test is increased beyond ğ»train. This improvement is larger than that attained by using the base model. 2507 (our base model), Qwen3-4B-Thinking-2507, Polaris-4B [2], Qwen3-4B-based model trained for extrapolation by expanding the output context using YaRN [23], and version of Qwen3-4B-Instruct-2507 trained with standard GRPO at ğ»train = 32k, the maximum output length we could reliably use for RL due to practical constraints. The second category of comparisons consists of other iterative decoding approaches that condition directly on raw past reasoning rather than on summaries. We evaluate base and trained (see Appendix J) versions of two such methods: self-refinement and self-verification, selecting ğ»ğ‘… and ğ»test to be the same as for our RC experiments. We also evaluate two other iterative decoding approaches inspired by recent work [1, 21] in Appendix K. The third category of comparisons consists of approaches that do not train with RC but still use RC at inference time. We compare RCT-4B against inference-only use of RC by the base model (as in Section 5.2) and by the base model post-trained with standard RL. This comparison isolates the contribution of training with RC, rather than applying it solely at test time. See Appendix for discussion on the computational efficiency of RC. 7.1. Benchmark Results AIME 2025 HMMT 2025 (Nov) IMO-AnswerBench FrontierScience Qwen3-4B-Instruct-2507 [16k] Qwen3-4B-Instruct-2507 (RL, 32k) [32k] Polaris-4B [2] [90k] Qwen3-4B-Thinking-2507 [81k] Self-Refine (base) Self-Verify (base) Self-Refine (trained) Self-Verify (trained) Qwen3-4B-Instruct-2507 + RC Qwen3-4B-Instruct-2507 (RL, 32k) + RC RCT-4B + RC (Ours) 46.0 54.8 79.4 81.3 53.8 48.9 60.4 61.2 59.4 66.0 74.9 39.8 48.3 60.2 62. 50.0 46.7 61.3 62.1 56.7 60.2 66.3 33.5 36.1 43.5 49.0 38.8 37.0 45.1 45.9 42.6 45.2 49.4 23.3 21.5 23.6 25. 27.8 29.7 33.5 31.9 29.5 33.5 34.1 Table 1: Evaluation results. The top section of the table reports results of various reasoning models run with standard autoregressive generation; ğ»test given in square brackets. The middle section reports various iterative decoding baselines, run for 12 (AIME, HMMT) or 16 (IMO-AnswerBench, FrontierScience) turns (equivalent to 192k or 256k reasoning token budget) with ğ»ğ‘… = 16k. The final section reports results for RC-based results, which were run with the same token budgets as the iterative decoding baselines. RCT-4B outperforms all baseline methods on 3 out of 4 benchmarks. Our results are shown in Figure 8 and Table 1. Across all benchmarks and token budgets, RCT-4B outperforms the base model using RC, with the performance gap widening as the token budget increases. This indicates that training enables more effective extrapolation rather than merely improving short-horizon 11 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 9: Pass@ğ‘˜ accuracy vs. token budget on hard subset of problems sampled from [25] for Qwen3-4B-Instruct2507 and RCT-4B. The left panel shows base model performance, the middle panel shows trained model performance, and the right panel shows the performance gap between the two. RCT-4B achieves substantially higher pass@ğ‘˜ rates than the base model across all values of ğ‘˜, with the performance gap increasing as reasoning token budget grows. performance. Notably, the model also improves on FrontierScience despite being trained exclusively on mathematics problems, suggesting that RC training develops domain-general extrapolation capabilities. Finding 1: RC compares favorably against other strong 4B-sized models. We compare RCT-4B against strong 4B reasoning-specialized models that utilize autoregressive decoding. While these models are explicitly trained to exploit large token budgets, RCT-4B outperforms all autoregressive approaches on the three benchmarks with lowest contamination risk: HMMT 2025 (released in Nov 25), IMO-AnswerBench (released in Nov 25), and FrontierScience (released in Dec 25). In fact, we find that RCT-4B even achieves competitive results against much larger reasoning models: see Figure 11. Notably, while standard RL training improves upon the base model on the mathematics benchmarks, this standard RL model (1) remains substantially weaker than specialized reasoning models, and (2) achieves no gains on the out-ofdomain FrontierScience benchmark despite training on the same data as RCT-4B. This demonstrates that RC training develops more generalizable problem-solving strategies that standard RL does not. Finding 2: RC training yields better iterative reasoning than other iterative training methods. RCT-4B substantially outperforms all iterative decoding methods we compare against. Training models to perform self-verification or self-refinement using an approach analogous to RC training (Appendix J) leads to large improvements over their respective base models. However, these trained baselines remain significantly weaker than RCT-4B on mathematical reasoning tasks, highlighting the benefit of explicitly exploiting the summarizationgeneration asymmetry during training. Interestingly, nearly all iterative decoding methods we evaluate, including untrained variants, outperform autoregressive decoding on FrontierScience. This suggests that iterative decoding may generalize better to out-of-domain input problems compared to standard long-horizon autoregressive generation for the base models we consider in our experiments. 7.2. Evaluating RC on Hard Problems Our results thus far show that extrapolation with RCT-4B yields consistent improvements on standard benchmarks. However, these gains could have arisen either from solving harder problems with additional test-time compute or from sharpening performance on problems that are already partially solvable within ğ»ğ‘…. To distinguish between these effects, we evaluate models on set of adversarially curated problems from Omni-MATH [8], following the protocol of Qu et al. [25]. These problems are selected such that the base Qwen3-4B-Instruct-2507 model fails to produce any correct solution across ğ‘ = 256 independent attempts, even when given sufficient reasoning budget. We consider such problems to be difficult because Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 10: Left: Ablation study on stagewise training configurations. Performance improves with Stage II training, with the summary replay buffer providing additional gains that increase with reasoning budget. Middle: Ablation study on Ttrain values during Stage I. Ttrain = 3 yields optimal performance across budgets by balancing early-turn training with summary-conditioned reasoning exposure. Right: Comparison of training methods. RC-specific training substantially outperforms standard GRPO training, demonstrating that extrapolation requires explicit training beyond simply improving reasoning capabilities. they are unlikely to be solvable simply by scaling parallel compute [5, 39, 42]. Evaluation results. We evaluate both the base model and RCT-4B on this dataset using RC decoding and report pass@ğ‘˜ in Figure 9. While both models improve with increased reasoning token budgets, the gains for RCT-4B are substantially larger. At budget of 256k tokens, the base model achieves pass@16 of 20%, whereas RCT-4B reaches nearly 35%. Moreover, the performance gap between the trained and base models widens as the token budget increases, indicating that training improves the models ability to utilize RC for in-context exploration. Overall, these results demonstrate that RCT-4B can solve difficult problems that the base model cannot by effectively extrapolating its reasoning at test time. For additional results directly comparing RC against parallel compute methods (majority vote), see Appendix B. 7.3. Ablation Studies The effect of the summary replay buffer. We ablate our training procedure by comparing performance on AIME 2025 after (i) Stage training only, (ii) Stage II training without using the summary replay buffer, and (iii) Stage II training with the replay buffer enabled; see Figure 10 (left). Stage training alone yields substantial gains over the base model, while Stage II training provides additional improvements. These gains are modest without the replay buffer but significantly larger when it is used, particularly at higher reasoning budgets. At 16k tokens, Stage II with replay buffer improves accuracy by 2.7% over Stage I, while at 192k tokens this gap grows to 9.4%. In contrast, Stage II without replay buffer yields only 4% improvement at 192k tokens, thus demonstrating the effectiveness of our replay buffer. The effect of the number of training turns. Next, we vary the number of training turns Ttrain during Stage I, evaluating Ttrain {2, 3, 4}; see Figure 10 (middle). All settings improve extrapolation, with Ttrain = 3 performing best, followed by Ttrain = 2. We believe this arises from trade-off in gradient signal allocation: when Ttrain is too large, the model receives insufficient signal on early turns (including the initial turn), while the converse is true when Ttrain is too low. These results imply that we must train the model evenly across all early turns in order for good summary-conditioned reasoning to emerge. The importance of training with RC. Finally, we isolate the effect of RC training by comparing against model trained with standard outcome-reward RL (GRPO). We train Qwen3-4B-Instruct-2507 with RL for the same number of steps and following the same two-stage training curriculum while increasing the training budget ğ»train from 16k to 32k tokens (the maximum output length we could reliably use for RL due to practical constraints). We then conduct evaluation using RC decoding; see Figure 10 (right) and 13 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Table 1 (bottom) for our results. While RL training yields modest improvements over the base model, it falls far short of training with RC. This demonstrates that effective extrapolation via RC decoding does not emerge from standard RL alone and must be trained through RCs structured, multi-turn objective. 7.4. Incorporating RC into test-time scaffolds Our experiments thus far show that RC training enables more effective extrapolation by improving summary-conditioned generation. We can view this as specific instance of self-guided reasoning: the model conditions on self-generated abstractions [26, 43] (in our case, summaries of prior reasoning) to guide downstream reasoning. This raises natural question: does RC training teach generalizable skill for using self-generated guidance, or is the improvement narrowly tied to reasoning from summaries? To answer this question, we evaluate how RC training transfers to other self-guidance settings by incorporating RCT-4B and RC decoding into two test-time scaffolds: Recursive Self-Aggregation (RSA) [38] and the DeepseekMath (DSM) Agent [33]. RSA generates multiple parallel reasoning traces and iteratively aggregates them, while the DSM Agent iteratively performs self-verification and self-refinement over an initial pool of solutions; see Appendix for details. In both cases, the model is used not only to generate reasoning traces from scratch, but also to reason conditioned on self-generated context produced via the scaffold. RSA DSM Agent Base RL RCT-4B (no RC) RCT-4B + RC (Ours) 66.3 64.8 70.2 75.4 57.5 61.3 65. 74.6 Table 2: Our RCT-4B model yields additional gains when incorporated into testtime scaffolds. These gains are higher than with the base model or the standard RLtrained model. Using RC decoding within the scaffold yields further gains. Evaluation results. Table 2 shows that RCT-4B leverages both RSA and the DSM Agent far more effectively than either the base or RL-trained Qwen3-4B-Instruct-2507 models, even without using RC decoding. This suggests that RC training imparts generalizable capability: reasoning effectively from self-generated abstractions beyond summaries, including aggregated past traces (RSA) and self-generated feedback (DSM Agent). Replacing standard autoregressive decoding with RC for all solution-generation steps within these scaffolds yields additional gains, which we attribute to improved reasoning accuracy throughout the scaffolded process. Overall, these results indicate that RC training develops abstraction-conditioned reasoning abilities, highlighting an important direction for future work. Key Takeaways: Training with RC improves extrapolation across domains Our RC-trained model RCT-4B outperforms all iterative decoding and autoregressive decoding baselines on HMMT 2025, IMO-AnswerBench and FrontierScience. RC enables models to solve difficult problems that cannot be solved by scaling parallel compute. RC training yields models that are better at summaryand abstractionconditioned reasoning in general, and can therefore effectively leverage scaffolds to further scale test-time compute. 8. Conclusion and Perspectives on Future Work In this work, we demonstrate how LLMs can be trained to continually improve their reasoning across long horizons. Our method, RC, replaces autoregressive decoding with an iterative decoding algorithm that alternates between summarization and summary-conditioned reasoning, and trains models via outcomereward RL to leverage this algorithm more effectively. We show that using RC allows us to overcome fundamental limitation of autoregressive generation and standard RL: the inability to extrapolate 14 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL reasoning beyond training rollout lengths. Empirically, we demonstrate that our RC-trained model achieves substantial performance gains on challenging mathematical and scientific benchmarks by extrapolating reasoning to much longer horizons than it was trained for. Furthermore, we show that extrapolation via RC enables the model to solve difficult problems that it cannot solve within its training budget, and that the trained model can leverage existing test-time scaffolds to further scale inference compute. Ultimately, we believe that RC represents an important step toward training models that can engage in the systematic, long-horizon reasoning required to solve the worlds most difficult problems. Future work. Future work should address the limitations of RC (see Appendix E). We identify three main directions. First, improving the training objective beyond myopic rewards: our current approach assigns rewards based on individual trace correctness, which discourages multi-turn strategies in which early turns perform exploratory, low-reward procedures that are only exploited in later turns. As we discussed in Section 5.1, we do not expect this to be problem when each iteration of summarization still makes useful progress, but training with non-myopic extension of RC could potentially yield significant improvements on harder problems. Second, explicitly training for summary generation: our current approach only trains the model for summary-conditioned generation. Our experiments in Section 5.2, however, show that altering summaries can significantly affect the performance of RC, suggesting that directly optimizing summary generation could also be beneficial. This direction is also closely related to prior work on training models to produce useful abstractions for guiding reasoning [26, 43] or in training aggregation mechanisms [38], with both lines of work demonstrating tangible performance gains. Third, adapting RC to tasks without final-answer based rewards: while our current work focuses on settings with closed-form, verifiable outcomes, these tasks represent only minority of the reasoning challenges we ultimately care about. Broadening RC to operate in open-ended domains, such as proof generation, is therefore an exciting and potentially impactful direction for future research."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Matthew Yang, Haoran Li, Lewis Tunstall, Jasper Dekoninck, and the other members of the CMU AIRe lab for discussions and feedback. We would also like to thank the CMU FLAME center for providing the compute resources on the Orchard cluster that supported almost all of our big experiments. Additionally, we are grateful for the Delta AI cluster at NCSA and the TRC program of Google Cloud for additional computational resources. This work was supported by the Office of Naval Research under ONR N0014-24-2206 and Schmidt Sciences AI2050 Early Career Fellowship. AS is supported by JP Morgan AI PhD fellowship and YQ is supported by an Amazon AI PhD fellowship."
        },
        {
            "title": "References",
            "content": "[1] Milad Aghajohari, Kamran Chitsaz, Amirhossein Kazemnejad, Sarath Chandar, Alessandro Sordoni, Aaron Courville, and Siva Reddy. The markovian thinker: Architecture-agnostic linear scaling of reasoning, 2025. URL https://arxiv.org/abs/2510.06557. [2] Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github.io/ blog/2025/Polaris. [3] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, 15 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. [4] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv.org/abs/2501.17161. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. [6] Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep Q-learning algorithms. arXiv preprint arXiv:1902.10250, 2019. [7] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503.01307. [8] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omnimath: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin 16 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645 (8081):633638, September 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL http://dx.doi.org/10.1038/s41586-025-09422-z. [10] Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Yichi Yang, Zhijian Liu, Zhiting Hu, and Lianhui Qin. Arcmemo: Abstract reasoning composition with lifelong llm memory, 2025. URL https://arxiv.org/abs/2509.04439. [11] Kelly Hong, Anton Troynikov, and Jeff Huber. Context rot: How increasing input tokens impacts llm performance. Technical report, Chroma Research, July 2025. URL https://research.trychroma. com/context-rot. [12] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet, 2024. URL https: //arxiv.org/abs/2310.01798. [13] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks, 2023. URL https://arxiv.org/abs/2303.17491. [14] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. URL https://arxiv.org/abs/ 2409.12917. [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [16] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models, 2023. URL https://arxiv.org/abs/2310.06201. [17] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. URL https://arxiv.org/abs/2307.03172. [18] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [19] Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, and Junehyuk Jung. Towards robust mathematical reasoning, 2025. URL https://arxiv.org/ abs/2511.01846. [20] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL [21] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [22] OpenAI. Evaluating ais ability to perform scientific research tasks (frontierscience). OpenAI Research Publication, December 2025. URL https://openai.com/index/frontierscience/. Accessed: 2026-01-20. [23] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071. [24] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve, 2024. URL https://arxiv.org/abs/2407.18219. [25] Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, and Aviral Kumar. Learning to reason on hard problems with privileged on-policy exploration. In The 5th Workshop on Mathematical Reasoning and AI at NeurIPS 2025, 2025. URL https://openreview.net/forum?id=zKn6mVwPZE. [26] Yuxiao Qu, Anikait Singh, Yoonho Lee, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, and Aviral Kumar. Rlad: Training llms to discover abstractions for solving reasoning problems, 2025. URL https://arxiv.org/abs/2510.02263. [27] Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning, 2025. URL https://arxiv.org/abs/2503.07572. [28] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347. [30] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal. arXiv preprint arXiv:2502.12118, 2025. [31] Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms, 2025. URL https://arxiv.org/abs/2506.09026. [32] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. [33] Zhihong Shao, Yuxiang Luo, Chengda Lu, Z. Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, and Xiaokang Zhang. Deepseekmath-v2: Towards self-verifiable mathematical reasoning, 2025. URL https://arxiv.org/abs/2511.22570. [34] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 18 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL [35] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023. [36] Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, and Dawn Song. Rl grokking recipe: How does rl unlock and transfer new algorithms in llms?, 2025. URL https://arxiv.org/abs/2509.21016. [37] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet: Test-time learning with adaptive memory, 2025. URL https://arxiv.org/abs/2504. 07952. [38] Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, and Moksh Jain. Recursive self-aggregation unlocks deep thinking in large language models, 2025. URL https://arxiv.org/abs/2509.26626. [39] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171. [40] Colin Wei, Jason Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. 2019. [41] Tianxin Wei, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuanchen Bei, Xuying Ning, Mengting Ai, Yunzhe Li, Jingrui He, Ed H. Chi, Chi Wang, Shuo Chen, Fernando Pereira, WangCheng Kang, and Derek Zhiyuan Cheng. Evo-memory: Benchmarking llm agent test-time learning with self-evolving memory, 2025. URL https://arxiv.org/abs/2511.20857. [42] Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Pakazad, and Graham Neubig. Better instruction-following through minimum bayes risk, 2025. URL https://arxiv.org/abs/ 2410.02902. [43] Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, and Aviral Kumar. Int: Selfproposed interventions enable credit assignment in llm reasoning, 2026. URL https://arxiv. org/abs/2601.14209. [44] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. [45] Charlie Zhang, Graham Neubig, and Xiang Yue. On the interplay of pre-training, mid-training, and rl on reasoning language models, 2025. URL https://arxiv.org/abs/2512.07783. [46] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/2304.11277. Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL [47] Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents, 2025. URL https://arxiv.org/abs/2506.15841. 20 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL"
        },
        {
            "title": "Appendices",
            "content": "A. Additional Results vs. Larger Reasoning Models Figure 11: Comparison of RCT-4B and selection of other reasoning models on IMO-AnswerBench. Combining RC training and decoding enables our 4B model to outcompete many larger and newer models. We set inference hyperparameters (ğ‘¡, ğ‘, ğ»test) based on the recommended values provided on each models Hugging Face page. B. Comparison with Majority Voting Figure 12: Accuracy and Maj@16 against reasoning token budget for Qwen3-4B-Instruct-2507 and RCT-4B. While majority voting can be used to improve RC, we find that utilizing compute to increase depth through RC is more effective than increasing breadth by taking majority vote over more parallel samples. In Figure 12, we plot accuracy and Maj@16 against reasoning token budgets using RC decoding. While majority voting can be used to improve RC, we find that utilizing compute to increase depth through RC is more effective than increasing breadth by taking majority vote over more parallel samples for our tested value of ğ‘˜: in other words, Maj@16 performance at 16k reasoning token budget is significantly worse than accuracy at 256k tokens with RC. This applies both for the base Qwen3 model and for RCT-4B. Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 13: Performance of gpt-oss models with RC decoding (no training) on IMO-AnswerBench. We use ğ»ğ‘… = 64k to adjust for the models larger output lengths, and set reasoning effort to high for generation and medium for summarization. Both models benefit from extrapolation through RC. C. Motivating our Choices of ğ»ğ‘… In this section, we motivate the choices of ğ»ğ‘… (autoregressive decoding maximum token budget) we use throughout this work. For RC decoding, ğ»ğ‘… determines the length of individual reasoning traces within each turn. As discussed in Section 5.2, we generally choose ğ»ğ‘… to be ğ»train. Unfortunately, the exact value of ğ»train is typically not made public, so we must estimate it through the termination length statistics of the model: if the model generally terminates its reasoning within some length ğ¿, then we can reasonably say that ğ¿ ğ»train. Regardless, the general idea is that increasing the reasoning token budget beyond ğ¿ will not yield any gains in performance with autoregressive decoding, as the model will simply not generate anything longer than this. For our base Qwen3-4B-Instruct-2507 model, we set ğ»ğ‘… = 16k, which yields very high termination rate of 99.17% (see Table 3. Our trained RCT-4B model also attains very high termination rate of 98.75%, indicating that our training has not resulted in increased repetitiveness or undue verbosity. For our autoregressive decoding baselines, we choose the maximum token budget based on the values recommended on each models Hugging Face model card. As we see from Table 3, reasoning traces generated at these lengths do indeed overwhelmingly terminate successfully. note on input context windows: many models have stated maximum context windows that are very large. For example, Qwen3-4B-Instruct-2507 has stated maximum context window of 262,144 tokens. However, we note that they almost never generate reasoning traces of lengths greater than 16k tokens: see Table 3. This is likely because the models were post-trained to generate outputs of up to 16k tokens in length: the 262,144 context window is only utilized for processing long-context inputs. Model Qwen3-4B-Instruct-2507 Qwen3-4B-Thinking-2507 Polaris-4B Qwen3-4B-Instruct-2507 + Std. RL @32k RCT-4B ğ»ğ‘… 16k 81k 90k 32k 16k Termination Rate (%) 99.17 100.00 99.79 99.17 98.75 Table 3: Termination rates for different models on HMMT 2025. We determine termination by the presence of the boxed{} pattern in the model output. 22 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL D. RC with gpt-oss Please see results showing the efficacy of our approach on this model family in Figure 13. E. Limitations of RC and Directions for Future Work While RC training yields strong empirical results, our method has several limitations that we outline in this section. We hope this discussion provides useful directions for future work. RC training does not optimize summary generation. Our training focuses exclusively on summary-conditioned reasoning, based on the observation that this is the primary performance bottleneck while summary generation is inherently easier for pre-trained and instruction-tuned models. We validated this assumption through preliminary experiments where we assigned rewards to summary generation based on the proportion of subsequent reasoning traces (conditioned on the summary) that produced correct answers. More formally, we optimized the following objective: [ max ğœ‹ğœƒ ğ‘¥,ğ‘¦ğ’Ÿtrain ğ‘¡ğ‘ˆ [1,Ttrain] where ğ‘§(ğ‘¡+1) ğ‘† ğœ‹ğœƒ(â„ğ‘† ,ğ‘¥,ğ‘§(ğ‘¡) ğ‘§(ğ‘¡) ğ‘… ,ğ‘§(ğ‘¡1) ğ‘† [ ) 1 ğ¾ ğ¾ ğ‘˜=1 ğ‘Ÿ(ğ‘¦, ğ‘§(ğ‘¡+1) ğ‘…,ğ‘˜ ) ]] , ğ‘…,ğ‘˜ ğœ‹ğœƒ(ğ‘¥, ğ‘§(ğ‘¡) ğ‘† ) for ğ‘˜ = 1, . . . , ğ¾. (E.1) Figure 14: Training for summarization generation using objective E.1 hurts performance. Training this way either in isolation or in combination with the usual summary-conditioned reasoning objective negatively impacts performance. We tested this both in isolation and in combination with our usual summary-conditioned generation objective, using the same hyperparameters as in Section 6 for Stage training. As illustrated in Figure 14, optimizing for summary generation only (Summarization Trn.) hurts the efficacy of training, such that the resulting model is no better than the base model. Optimizing for both summary generation and summary-conditioned generation (Both Trn.) improves performance relative to the base model but hurts performance relative to the model trained only for summary-conditioned generation (Reasoning Trn.). These findings motivate the design of our main approach. We attribute these results to difficulties in credit assignment for summary generation. Even when the model generates faithful, informative summaries, it receives zero reward if subsequent reasoning fails to solve the problem, which may occur simply because the problem is too difficult to solve in single turn and not because the summary is poor. This misalignment between summary quality and reward signal makes it difficult to effectively train summarization, although we posit that doing so effectively could further improve RC performance. Addressing this likely requires alternative reward assignment schemes for summary generation, which we leave to future work. RC training uses myopic rewards. RC is trained by optimizing Equation 6.1 using outcome-based rewards assigned independently at each iteration. That is, the reasoning trace generated at turn ğ‘¡ receives reward based solely on its own correctness, without explicit credit assignment to future turns ğ‘¡ + 1, ğ‘¡ + 2, . . .. As discussed in Section 6, this design relies on the observation that training each step to produce correct answer implicitly encourages the model to generate reasoning and summaries that contain information useful for subsequent iterations. When each summarization and summary-conditioned generation step makes monotonic progress toward correct solution, such myopic rewards are well 23 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL aligned with trajectory-level success. In this regime, summaries serve as sufficient representations of past reasoning, and improving per-step correctness also improves the quality of information available for future steps. Indeed, under the assumption that the model can reliably extract maximal useful information from summary when needed, optimizing per-step correctness is sufficient to optimize the long-horizon objective over multiple iterations. Moreover, by conditioning training rollouts on these summaries, the model is explicitly trained to exploit accumulated information, further increasing the likelihood of success at subsequent steps. As result, we can optimize correctness over the full reasoning trajectory despite only explicitly optimizing individual steps. The main limitation of our approach is that the model is not incentivized to generate reasoning that is suboptimal for the current step but valuable later. For example, the model may benefit from exploring alternative solution strategies or collecting auxiliary information that only becomes useful later in the trajectory. Learning such far-sighted reasoning behaviors may be particularly important for very difficult problems requiring extensive in-context exploration. However, designing reward schemes that effectively encourage such multi-step contributions remains challenging, and we leave this direction to future work. Summarization-generation asymmetry is not present in all LLMs. Our analysis in Section 5.2 reveals that RC yields the most benefits when the underlying model possesses strong summarization-generation asymmetry, and that instruction-following models generally possess this asymmetry whereas highly specialized reasoning models do not. This limits the kinds of models we can apply RC to. We propose several potential solutions to this problem. The first involves warmstarting the reasoning model for summarization and summary-conditioned generation, perhaps through distillation or SFT. This approach, however, may potentially alter the reasoning behavior of the model in detrimental way. The second solution involves using separate model to perform summarization generation, which we previously identified as particularly difficult task for specialized reasoning models. This approach, however, would then require us to maintain two separate models, which could pose certain practical challenges. RC does not improve performance on all classes of reasoning problems. While our experiments show that RC decoding and training improves model performance across mathematical and scientific reasoning benchmarks, we posit that not all classes of problem classes benefit from RC. One class of such problems are search-heavy problems, where the model must iterate through large number of possible outcomes and select the optimal choice. The main issue here is that the redundancy property no longer applies as strongly as before, as many tokens generated may be important as they document the search process and keep track of what has been tried. Summarizing such traces risks discarding important information that may reduce test-time performance on the search task. When is RC helpful? To understand the classes of problems for which RC is most effective, it is useful to conceptualize the solution space as graph, where nodes represent conclusions, intermediate results, or other salient states, and edges represent logical transitions between them. RC is particularly well-suited to problems whose solution graphs are clique-like: nodes tend to form loosely connected clusters that can be summarized compactly, with relatively sparse connections between clusters. Mathematical and scientific reasoning often exhibit this structure, as progress can be decomposed into distinct conceptual advances that admit concise summaries. In such settings, RC enables effective reasoning by allowing each iteration to explore different regions of the graph, while maintaining long-term progress by summarizing and tracking information within individual clusters. In contrast, for search-heavy problems this clique structure is largely absent, and effective reasoning requires tracking large number of individual nodes encountered during exploration. Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL In these cases, summaries must enumerate prior states rather than abstract them, causing limited-length summaries to quickly become overwhelmed and reducing the effectiveness of iterative summarization. In addition to scientific and mathematical reasoning, RC may also be helpful on tasks where actions yield environment feedback that is noisy and can benefit from summarization (e.g. coding with interpreter feedback). In this case, summaries may be used to keep track of environment state, as has been explored in related work [47]. Unlike these works, we focus primarily on creating abstractions of reasoning rather than environment states, but these ideas are closely related and can likely be combined. F. Discussions on Computational Efficiency F.1. Inference We begin by analyzing the computational efficiency of RC decoding compared to standard long-context autoregressive generation. We examine how RC scales with reasoning length and whether extrapolating via RC is more efficient than training models to natively handle larger token budgets through autoregressive decoding. Notation and definitions. Let ğ¶ be the input problem length, and let ğ‘ be the maximum generation length for standard autoregressive decoding. Under RC decoding, the model proceeds for turns, generating at each turn reasoning statement of length ğ»ğ‘… followed by summary of length ğ»ğ‘†, with ğ»ğ‘… ğ»ğ‘†. Our analysis focuses on decoder-only transformers with KV-cached decoding, where for long contexts, attention computation dominates and scales linearly with current context length. Standard Long-Context Generation. In standard autoregressive decoding, the model generates ğ‘ tokens in single trajectory, with the context growing from length ğ¶ to ğ¶ + ğ‘ . With KV caching, the incremental cost of generating the ğ‘–-th token scales linearly with the current context length ğ¶ + ğ‘–. Summing over all tokens, the total attention-dominated inference cost (IC) scales as IC standard ğ‘ (ğ¶ + ğ‘–) = ğ‘ ğ¶ + ğ‘–= 1 2 ğ‘ (ğ‘ + 1) = Î˜(ğ‘ (ğ¶ + ğ‘ )) (F.1) RC Inference. In RC, each reasoning step is conditioned only on the original prompt and the current summary, rather than the full previous chain-of-thought. As result, the effective context length within each turn is bounded by approximately ğ¶ + ğ»ğ‘† + ğ»ğ‘…, and does not grow across turns. Across turns, the total inference compute is therefore IC RC ğ‘‡ ğ»ğ‘… (ğ¶ + ğ»ğ‘† + ğ»ğ‘…) (F.2) Inference Speedup. For fixed effective reasoning budget ğ‘ = Tğ»ğ‘…, the inference speedup of RC is approximately ICstandard IC RC Figure 15: Plot of decoding throughput against reasoning token budget. RC decoding throughput remains constant as reasoning token budget, whereas throughput for standard autoregressive decoding decreases. as Tğ»ğ‘… ğ¶ and ğ»ğ‘… ğ»ğ‘†. Therefore, to reach ğ‘ = Tğ»ğ‘… effective reasoning tokens, RC can be times cheaper than autoregressive decoding in attention-dominated regimes. ğ¶ + ğ‘‡ ğ»ğ‘… ğ¶ + ğ»ğ‘† + ğ»ğ‘… Speedup = (F.3) ğ‘‡. 25 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Empirical study. We conduct experiments to study the computational efficiency of RC. We run RC decoding using Qwen3-4B-Instruct-2507 and standard autoregressive decoding using Qwen3-4B-Thinking-2507 at different reasoning token budgets, logging throughput on HMMT 2025 (30 prompts) with 8 parallel rollouts. For the RC runs, we experiment with ğ»ğ‘… {8k, 16k}. We plot decoding throughput against reasoning token budget in Figure 15. This plot demonstrates that autoregressive decoding throughput rapidly decreases as token budgets increase, whereas RC decoding throughput remains constant. This is expected because RC maintains bounded context length across turns even as the effective reasoning horizon grows. RC therefore proves substantially more efficient than autoregressive decoding despite our use of highly optimized inference engine for the latter and naive implementation for the former: see Appendix for implementation and hardware details. F.2. Training Standard long-context RL baseline. In both standard RL training, we perform on-policy RL (e.g., GRPO) with batch size (problems per step) = ğµ and samples per problem = ğ¾ (GRPO group size). Then each step generates roughly ğµ ğ¾ ğ‘ tokens, where ğ‘ is the sequence length. Since the attention cost scales with the growing context length, the forward generation compute scales as: GenCompute standard ğµ ğ¾ ğ‘ (ğ¶ + ğ‘ ). (F.4) Including backward and optimizer computation introduces constant multiplicative factor ğ›¾, yielding TrainCompute long ğ›¾ ğµ ğ¾ ğ‘ (ğ¶ + ğ‘ ). (F.5) When ğ‘ is large, this scales quadratically with the rollout horizon. RC Training. RC training separates trajectory construction from policy optimization. Each training step consists of: (1) summary-trajectory generation: The model runs RC for Ttrain turns to produce sequence of summaries; (2) policy optimization: from this trajectory, ğ‘summ summaries are sampled, and for each summary, ğ¾ reasoning rollouts of length at most ğ»ğ‘… are generated and optimized via GRPO. The total forward generation compute per training step scales as"
        },
        {
            "title": "GenCompute",
            "content": "RC ğµ (Ttrain + ğ¾ğ‘summ) ğ»ğ‘… (ğ¶ + ğ»ğ‘† + ğ»ğ‘…) ."
        },
        {
            "title": "TrainCompute",
            "content": "RC ğ›¾ ğµ (Ttrain + ğ¾ğ‘summ) ğ»ğ‘… (ğ¶ + ğ»ğ‘† + ğ»ğ‘…) . (F.6) (F.7) Crucially, all optimized rollouts remain bounded by length ğ»ğ‘…, regardless of the total effective reasoning horizon supported at inference time. Training-Time Scaling Comparison. To reach an effective horizon ğ‘ = Ttarget ğ»ğ‘…, standard long-context RL training incurs compute scaling annroximatelv as while RC training scales as TrainCompute standard ğµ ğ¾ T2 target ğ» 2 ğ‘…"
        },
        {
            "title": "TrainCompute",
            "content": "RC ğµ (Ttrain + ğ¾ğ‘summ) ğ» 2 ğ‘… (F.8) (F.9) 26 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Thus, the relative cost satisfies TrainCompute TrainCompute RC Ttrainğ¾ğ‘summ ğ¾ . (F.10) standard This highlights key advantage of RC: naively increasing rollout length leads to quadratic growth in training cost, whereas RC decouples the optimized rollout length from the effective reasoning horizon. By using summaries and replay, RC enables training policies that generalize to very long reasoning horizons without incurring prohibitive quadratic costs during optimization. target F.3. Inference KV-Cache Memory The KV cache memory footprint for autoregressive decoding scales linearly with the context length: while for RC, it is bounded by the maximum within-turn context length: Memory standard ğ¶ + ğ‘,"
        },
        {
            "title": "Memory",
            "content": "RC ğ¶ + ğ»ğ‘† + ğ»ğ‘…, (F.11) (F.12) which is independent of T. Putting these together, RC requires ğ‘‡ lower KV memory at the same effective reasoning horizon:"
        },
        {
            "title": "Memorystandard\nMemory",
            "content": "RC ğ¶ + ğ‘‡ ğ»ğ‘… ğ¶ + ğ»ğ‘† + ğ»ğ‘… ğ‘‡. G. Details for Test-Time Scaffold Experiments G.1. Recursive Self-Aggregation In Section 7.4, we also experiment with incorporating RC into RSA [38], scaffold that iteratively refines solutions through sampling and aggregation. In its original form, the algorithm begins by sampling ğ‘€ solutions from scratch (conditioned only on the problem). Then, in each subsequent iteration, the algorithm creates ğ‘€ new solutions by randomly sampling ğ‘˜ candidates from the current pool of solutions (with replacement) and prompting the model to aggregate them into single improved solution. Over the TRSA successive loops, solutions compound recursively: aggregated outputs become inputs for the next round, progressively eliminating errors and reinforcing correct solutions while maintaining constant population of ğ‘€ solutions. We incorporate RC into RSA by replacing (1) the initial solution generation step and (2) subsequent refinement steps with RC decoding. We begin the refinement step by treating the aggregated solution as summary that we condition on for the first step of RC refinement. For our experiments in Section 7.4, we use ğ‘˜ = 2, ğ‘€ = 8, and TRSA = 10, and for the experiment incorporating RC decoding, we set the number of RC steps as = 8. G.2. DeepseekMath Agent We also experiment with test-time scaffold we call DeepseekMath Agent (DSM Agent). This is adapted from the scaffold used in Shao et al. [33] to improve the ability of LLMs to generate proofs for mathematical reasoning problems. 27 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL At high level, the DSM Agent implements Generate-Verify-Refine loop that uses self-verification to iteratively improve solutions. It begins by generating an initial pool (of size ğ‘›ğ‘”) of candidate solutions, and then verifies each solution using ğ‘›ğ‘£ self-verification attempts per solution (assigning scores of 0.0 for major errors, 0.5 for minor issues, 1.0 for correct), with the final verification score determined by averaging over the ğ‘›ğ‘£ scores. In each of the subsequent refinement iterations, the algorithm selects the highest-scoring solutions and refines them using feedback from their lowest-scoring verifications. Refined solutions are added to the growing pool and re-verified, with this process repeating until either (1) perfect score is achieved, or (2) the maximum TDSM iterations are reached. At the end, the algorithm returns the highest-scoring solution as the final answer. We incorporate RC into DSM Agent by replacing the initial solution generation step with RC decoding, with the aim of improving the quality of the initial pool of candidates. For our experiments in Section 7.4, we use ğ‘›ğ‘” = 8, ğ‘›ğ‘£ = 4, and TDSM = 6, and for the experiment incorporating RC decoding, we set the number of RC steps as = 8. H. Overview of GRPO GRPO optimizes the following objective: ğ’¥ (ğœƒ) = Eğ‘¥,ğ‘¦ğ’Ÿtrain Eğ‘§ğ‘–ğœ‹ğœƒ(ğ‘¥) [ 1 ğ¾ ğ¾ ğ‘–=1 min [ ğœ‹ğœƒ(ğ‘§ğ‘– ğ‘¥) ğœ‹old(ğ‘§ğ‘– ğ‘¥) ğ´ğ‘–, clip ( ğœ‹ğœƒ(ğ‘§ğ‘– ğ‘¥) ğœ‹old(ğ‘§ğ‘– ğ‘¥) , 1 ğœ–, 1 + ğœ– ) ]] . ğ´ğ‘– (H.1) Here, ğ‘§ğ‘– denotes the ğ‘–th of ğ¾ independently sampled rollouts (which taken together form group), and ğ´ğ‘– denotes the GRPO advantage, which is computed directly from the rewards as ğ´ğ‘– = ğ‘Ÿğ‘–mean(ğ‘Ÿ) , with the mean and standard deviation calculated over group rewards. std(ğ‘Ÿ) Some intuitions behind GRPO: For fixed input ğ‘¥, GRPO assigns advantages to each rollout ğ‘§ğ‘– relative to the other ğ¾ samples in the group, so updates depend on whether ğ‘§ğ‘– is better or worse than its peers rather than on absolute reward values. In the case of RC training, the ğ¾ parallel rollouts are sampled under the same prompt and summary combination, so we assign higher advantages to summary-conditioned reasoning traces that are better able to leverage the summary to attain the correct answer. Normalizing advantages by the group mean and standard deviation stabilizes gradients and makes updates invariant to the overall reward scale across different inputs. The clipped ratio ğœ‹ğœƒ(ğ‘§ğ‘– ğ‘¥)/ğœ‹old(ğ‘§ğ‘– ğ‘¥) retains PPOs [29] trust-region approach, preventing any single high-advantage ğ‘§ğ‘– from applying overly large updates. I. Dataset Construction Details We construct our training datasets by following some of the principles outlined in An et al. [2]. Specifically, we sample problems in way that ensures our dataset maintains reasonable difficulty given our model. We begin by sampling problems from the AceReason-Math [3] dataset ( 50k problems) and solving them with Qwen3-4B-Instruct-2507, setting ğ¾ = 64. We then evaluate these solutions and assign reward score to each problem based on the average number of correct solutions our model generates. These scores are used for weighted sampling: we discard all samples that attain score of 0.7 or greater, 28 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL and downsample problems with other reward scores to attain the J-shaped reward curve described in Figure 2 of An et al. [2]. This procedure yields dataset of around 5.7k samples, which we take as our Stage training set. After Stage training, our model improves and so we rebalance our training dataset such that it maintains the J-shaped reward curve. We reannotate our Stage dataset with the Stage model (with standard autoregressive decoding) and once again remove samples that attain reward scores of 0.7 of greater. We then inject 500 difficult (zero-reward) problems from the DAPO [44] dataset (as determined via annotation with the base model) and ensure that our Stage II dataset contains challenging problems. J. Iterative Decoding Baseline Details In this section, we describe in detail the self-verification and self-refinement iterative decoding baselines that we compare RC against. The purpose of these baselines is to help us separate out the impact of our summarize-generate routine from the impact of using iterative decoding. As such, these baseline methods do not utilize the summarization-generation asymmetry, and instead act directly on the reasoning trace generated by the model, as is common in iterative decoding methods and test-time scaffolds [14, 24, 33]. J.1. Inference Concretely, let ğ‘¥ denote the input prompt and let ğ‘¡ index the decoding turn. Unlike RC, these baseline methods maintain only reasoning trace ğ‘§(t) at each turn, with no separate summarization step. At each ğ‘… turn, the reasoning trace ğ‘§(t) is generated under fixed token budget ğ»ğ‘… (we use the same ğ»ğ‘… = 16k as ğ‘… in our RC experiments). For self-refinement, decoding proceeds by alternately generating reasoning traces and prompting the model to refine them. At each turn t, we sample: ğ‘§(t) ğ‘… ğœ‹ğœƒ ( â„refine, ğ‘¥, ğ‘§(t1) ğ‘… ) , (J.1) where â„refine instructs the model to improve upon its previous reasoning trace, and ğ‘§(0) ğ‘… the empty string. is initialized as For self-verification, the model is prompted to first verify its previous attempt before generating correction. At each turn t, we sample: ğ‘§(t) ğ‘… ğœ‹ğœƒ ( â„verify, ğ‘¥, ğ‘§(t1) ğ‘… ) , (J.2) where â„verify instructs the model to verify whether its previous reasoning is correct and, if not, to provide corrected solution. See Figures 19 and 20 for â„refine and â„verify. After decoding turns, the final output is given by ğ‘§ := ğ‘§(T) ğ‘… RC is that these baselines condition on the full previous reasoning trace ğ‘§(t1) summary. As such, the model must conditionally generate from sequences up to 2ğ»ğ‘… in length. for both methods. The key distinction from rather than compressed ğ‘… 29 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL J.2. Training Training follows similar scheme to RC training, except that we generate rollouts using our baseline iterative decoding methods instead of RC decoding. The idea here is to assess whether utilizing the summarization-generation gap enables us to achieve better performance through training, or whether our iterative training strategy on its own is sufficient to attain significant gains. More formally, at any given point in training, we run the iterative decoding algorithm for Ttrain turns for each problem ğ‘¥ in training batch. We collect the reasoning traces generated from these rollouts ğ‘§ğ‘… := (ğ‘§(1) ) and then uniformly sample ğ‘trace Ttrain unique traces per problem. We then generate ğ¾ reasoning traces conditioned on each sampled trace. We assign rewards based on correctness and compute advantages over these ğ¾ samples. Formally, the objective can be written as: ğ‘… , . . . , ğ‘§(Ttrain) ğ‘… max ğœ‹ğœƒ Eğ‘¥,ğ‘¦ğ’Ÿtrain,ğ‘¡ğ‘ˆ [1,Ttrain] [ ğ‘§ğœ‹ğœƒ(ğ‘¥,ğ‘§(ğ‘¡) ğ‘… ) [ğ‘Ÿ(ğ‘¦, ğ‘§)] ] , ğ‘§ ğ»ğ‘… where (ğ‘§(1) ğ‘… , . . . , ğ‘§(Ttrain) ğ‘… ) IterativeDecoding(ğœ‹ğœƒ; ğ‘¥). (J.3) We adopt the same training hyperparameters as for RC training in Section 6. K. Additional Iterative Decoding Comparisons We evaluate two additional prompting-only iterative decoding baselines beyond the self-refinement and self-verification methods described in Appendix J. Rather than conditioning directly on the full reasoning trace, these baselines modify the trace before conditioning on them for subsequent generation. K.1. Budget Forcing AIME 2025 HMMT 2025 (Nov) IMO-AnswerBench FrontierScience Qwen3-4B-Instruct-2507 [16k] Qwen3-4B-Instruct-2507 + RC RCT-4B + RC (Ours) Budget Forcing [21] 46.0 59.4 74.9 56. 39.8 56.7 66.3 46.7 33.5 42.6 49.4 36.7 23.3 29.5 34.1 20.1 Table 4: Comparison of budget forcing with Qwen3-4B-Instruct-2507 against the autoregressive baseline and against RC. Budget forcing yields modest gains on the three mathematical reasoning benchmarks, but degrades performance on FrontierScience vs. the autoregressive baseline. RC-based methods outperform budget forcing on all benchmarks. The first baseline we evaluate is budget forcing approach, inspired by Muennighoff et al. [21]. In this approach, we repeatedly append Wait, let me continue thinking after termination to elicit additional reasoning. We use ğ»ğ‘… = 16k, and report our results in Table 4. Table 4 shows that budget forcing yields modest gains on mathematical reasoning benchmarks relative to autoregressive decoding but degrades FrontierScience performance. Both RC-based methods substantially outperform budget forcing across all benchmarks. Analyzing output traces reveals two failure modes. First, budget forcing rarely changes the final answer after the first few turns, with continuations quickly reducing. We hypothesize that simply appending Wait... is insufficient encouragement for the model to engage in meaningful in-context exploration, unlike conditioning on entire summaries of past reasoning. Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Second, excessive forcing steps often produce degenerate behavior characterized by substantial repetition, especially in later turns. We attribute these issues to repeated Wait... prompts creating highly unnatural, out-of-distribution prefixes, especially after the model has already returned final answer. In contrast, conditioning on summaries maintains in-distribution prefixes, yielding better performance. K.2. Delethink Our second baseline is prompting-only version of Delethink [1]. At each iteration, Delethink generates ğ»ğ‘… tokens of reasoning conditioned on the final ğ»chunk tokens from the previous iteration, continuing until EOS or reaching maximum iterations. This approach enables long-horizon RL training by chunking reasoning into segments, which circumvents the practical difficulties of standard long-context RL (a goal also shared by RC training: see Appendix F). We compare RC with the base Qwen3-4B-Instruct-2507 against Delethink using the same model. We evaluate Delethink with ğ»ğ‘… {4k, 8k}, setting ğ»chunk = ğ»ğ‘…/2, and report our findings in Figure 16. We do not experiment with ğ»ğ‘… = 16k because the model overwhelmingly terminates its outputs at this value of ğ»ğ‘…, preventing Delethink from yielding improvements at larger token budgets because the model simply will not continue generating. We find that Delethink generally improves performance as token budget increases, but plateaus near the performance of the autoregressive baseline (note that these results are consistent with findings in Aghajohari et al. [1]). RC, on the other hand, improves significantly over the autoregressive decoding baseline, even without any RC-specific training. Figure 16: Performance as function of reasoning token budget, comparing RC with Delethink [1]. We use Qwen3-4B-Instruct-2507 for both experiments, without any additional training. RC outperforms Delethink across all token budgets. We attribute this discrepancy to two factors. First, Delethink terminates after generating final answer, which limits continued reasoning. In contrast, RC explicitly encourages verifying and refining prior answers, because subsequent reasoning is driven by the produced summary from the previous iteration. We find that this process drives significant performance gains. Second, Delethink conditions generation on only the prompt and the last ğ»chunk reasoning tokens from the previous iteration, creating out-ofdistribution prefixes for the base model. RC instead conditions generation on structured summaries of past reasoning, which, for models with strong instruction-following, is considerably more in-distribution. In particular, we hypothesize that on difficult problems, once model has ceased to make meaningful progress within chain-of-thought, conditioning on short carryover from the previous iteration is often insufficient to induce reasoning that continues to make progress in the next turn. In contrast, RC partially alleviates this issue by conditioning on explicit summaries, which frequently prompt the model to restart, explore alternative strategies, or verify prior attempts. This difference may help explain why Delethink-style approaches tend to plateau, whereas RC continues to improve with additional iterations. L. Hardware, Hyperparameters, and Implementation Details Hardware. We conduct training on single node of 8H100 GPUs, and conduct inference on either single node of 8H100 GPUs or on single node of 4GH200 GPUs. For our inference efficiency 31 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Table 5: Training hyperparameters for all training experiments. Hyperparameter Learning rate KL loss coefficient Entropy loss coefficient Training batch size Minibatch size Clip range (low) Clip range (high) Gamma (ğ›¾) Lambda (ğœ†) Warmup schedule Weight decay Inference Temperature Inference top-ğ‘ Optimizer Value 1 106 0.001 0.0 64 32 0.2 0.28 1.0 1.0 Constant 0.01 1.0 1.0 AdamW experiments in Appendix F, we use 4GH200 GPUs. Implementation Details. We use verl [34] with FSDP [46] for training, and vLLM [15] for inference. We use naive implementation of RC decoding for both inference and training rollout generation. At each turn, we generate reasoning traces and summaries in batches sequentially, waiting for all sequences in batch to complete before progressing to the next turn. This is suboptimal because sequences finish at different times (due to varying lengths and early termination), causing idle GPUs to wait for the slowest sequence in each batch. more efficient implementation would perform asynchronous decoding across turns, allowing faster sequences to progress immediately without waiting for slower ones. Despite our use of suboptimal implementation of RC, we still empirically observe significant efficiency benefits at longer reasoning lengths (Appendix F). Inference Hyperparameters. We perform inference with ğ‘¡ = 1.0 and ğ‘ = 1.0 for RCT-4B. For Qwen34B-Instruct-2507, we use ğ‘¡ = 0.7 and ğ‘ = 0.8. For Qwen3-4B-Thinking-2507, we use ğ‘¡ = 0.6 and ğ‘ = 0.95. M. Full Example"
        },
        {
            "title": "Example Problem",
            "content": "Let ABCD and CEFG be squares such that lies on segment DG and lies on segment BC. Let be the circumcenter of triangle AEG. Given that A, D, and are collinear and AB = 1, compute FG."
        },
        {
            "title": "Example Summary",
            "content": "In this solution, the goal is to compute the side length FG of square CEFG given that ABCD and CEFG are squares with on segment DG, on segment BC, A, D, and (the circumcenter of triangle AEG) are collinear, and AB = 1. The solution begins by placing square ABCD on the coordinate plane with at (0,0), at (1,0), at (1,1), and at (0,1). Point lies on segment BC, so it is parameterized as (1, e) for in [0,1]. The square CEFG is then constructed with side CE, and both clockwise and counterclockwise rotations are considered to determine the position of point G. In the counterclockwise case, is found at (2-e, 2e-1); in the clockwise case, is at (e,1). The condition that lies on segment DG is then tested: for both orientations, the only value of that satisfies this (with 32 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL strictly on the segment) is e=1, which results in = and degenerate square. This degeneracy is unacceptable, so the solution attempts to resolve the issue by using the collinearity of A, D, and the circumcenter of triangle AEG as primary constraint instead of the segment condition. In the counterclockwise orientation, the circumcenter of triangle A(0,0), E(1,e), and G(2-e,2e-1) is computed using perpendicular bisectors, and the condition that this circumcenter lies on line AD (the line x=0) is imposed. Solving the resulting equation leads to cubic in e: 3e^3 -7e^2 +3e +1 = 0, which has roots at e=1 and = frac{2 pm sqrt{7}}{3}, only one of which lies in [0,1], namely e=1. Similarly, in the clockwise case, the same condition leads to different cubic that also only allows =1 as valid solution. Despite multiple attempts to find non-degenerate configuration, all solutions reduce to e=1, which implies that side length FG = CE = 0, degenerate case. Given that this configuration fails to yield nontrivial solution and that all attempts to satisfy the geometric constraints result in degeneracy, the only possible conclusion is that the problem likely intends nondegenerate solution with specific symmetric value. Based on known similar problems in geometry involving squares, collinearity, and circumcenters, the only plausible and symmetric value for FG when AB = 1 is sqrt{2}. Although complete non-degenerate derivation is not fully realized in this solution, the consistency with known problem structures and the absence of any non-degenerate solution under the given constraints lead to this conclusion. Therefore, the value of FG is taken as sqrt{2}. Example Summary-Conditioned Generation We are given geometric configuration involving two squares: ABCD and CEFG, with certain constraints: - ABCD and CEFG are squares. - lies on segment DG. - lies on segment BC. - is the circumcenter of triangle AEG. - A, D, and are collinear. - AB = 1, and we are to compute FG. Our goal is to find FG, the side length of square CEFG. --- ### Step 1: Place the configuration in the coordinate plane Let us set up coordinates to model the situation. Let square ABCD have side length 1. Place point at (0,0), at (1,0), at (1,1), and at (0,1). So, square ABCD goes counterclockwise: to to to D. Now, square CEFG is built on segment CE, with on segment BC. Since BC is vertical from (1,0) to (1,1), we can parameterize point as: = (1, e), quad text{where } 0 leq leq Now we need to determine the location of G, the vertex of square CEFG. We have two possible squares on segment CE: one going **clockwise** and one **counterclockwise**. So we must consider both orientations. Well keep both possibilities, and determine which one satisfies the conditions: 1. lies on segment DG 2. A, D, and the circumcenter of triangle AEG are collinear We already know from the previous attempt that the only solution found leads to degeneracy at = 1, so we must find **non-degenerate**configuration. --- ### Step 2: Two possible orientations of square CEFG We now define in both cases using rotation. We are going from to E, and forming square CEFG, meaning the vertices go in order to to to to C. So the side CE is one side of the square. We need to rotate vector vec{CE} by 90^circ to get from to F, and then from to G, etc. But since we need point G, we can compute directly as: vec{CG} = text{rotation of } vec{CE} text{ by } pm 90^circ Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Lets compute vectors. We have: = (1,1), quad = (1,e) Rightarrow vec{CE} = (1 - 1, - 1) = (0, - 1) = (0, -(1 - e)) We rotate this vector by 90^circ clockwise and counterclockwise to get the direction from to ( since in square CEFG, from C, we go to E, then from to F, etc., but to get to G, note that is opposite to if CEFG is labeled sequentially). Actually, more carefully: in square CEFG, vertices in order: to to to to C. So side CE, then EF, then FG, then GC. So the vector from to is vec{CE} rotated 90^circ, then from to is that rotated 90^circ again, etc. Alternatively, we can write point in terms of rotation of or E. Actually, from C, we go to E, then turn 90^circ left or right to get to F, etc. But heres trick: in square CEFG, from to is the vector vec{v} = vec{CE}, then from to is 90^circ rotation of vec{CE}. Then = + R_{90^circ}(vec{CE}). Then = + R_{90^circ}(R_{90^ circ}(vec{CE})) = + R_{180^circ}(vec{CE}) = - vec{CE}. But since = + R_{90^circ}(vec{CE}), lets verify. Wait: in square to to to to C, vector vec{EC} is from to C. But better: We can go: from to E: vector vec{v} = vec{CE} = (0, - 1) Then, vector from to is obtained by rotating vec{CE} by 90^circ. Depending on orientation. Lets define: - Rotation by 90^circ **counterclockwise**: (x,y) mapsto (-y,x) - Rotation by 90^circ **clockwise**: (x,y) mapsto (y, -x) Now, from to F, we rotate vec{CE} by 90^circ. So: - Case 1: **counterclockwise** square vec{EF} = R_{90^circ}(vec{CE}) = R_{90^circ}(0, - 1) = (-(e-1), 0) = (1 - e, 0) So, = + (1 - e, 0) = (1, e) + (1 - e, 0) = (2 - e, e) Then, vec{FG} = R_{90^circ}(vec{EF}) = R_{90^circ}(1 - e, 0) = (0, -(1 - e)) = (0, - 1) So, = + (0, - 1) = (2 - e, + - 1) = (2 - e, 2e - 1) Alternatively, since = + vec{CG}, and counterclockwise direction from to E, then rotating around E, but earlier we just computed directly. So yes, in counterclockwise square: = (2 - e, 2e - 1) - Case 2: **clockwise** square Rotate vec{CE} = (0, - 1) by 90^circ **clockwise**: rotation of vec{CG} = vec{CE} vec{CE} by 90^circ **counterclockwise** by 90^circ? Wait, in R_{90^circ}(vec{CE}) = (e - 1, 0) = + (e - 1, 0) = (1 + - 1, e) = (e, e) vec{FG} = R_{90^circ}(text{EF}) = R_{90^circ}(e - 1, 0) = (0, -(e - 1)) = (0, 1 - e) = + (0, 1 - e) = (e, + 1 - e) = (e, 1) vec{EF} = (e - 1, 0) So Then Then So So in clockwise orientation, = (e, 1) Thus, summary: Orientation ----------------- Counterclockwise (2 - e, 2e - 1) Clockwise (e, 1) --- 34 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL ### Step 3: Apply the condition that lies on segment DG We now use the condition that **C = (1,1) lies on segment DG**. Well analyze both cases. --- #### Case 1: Counterclockwise square, = (2 - e, 2e - 1) We have = (0,1), = (2 - e, 2e - 1) So segment DG connects (0,1) to (2 - e, 2e - 1) We want (1,1) to lie on this segment. Lets write parametric equations. Let point P(t) = (1 - t)D + tG, in [0,1] So: = (1 - t)(0) + t(2 - e) = t(2 - e) = (1 - t)(1) + t(2e - 1) = 1 - + t(2e - 1) = 1 + t(2e - 2) = 1 + 2t(e - 1) We want this point to be (1,1) So set: = t(2 - e) = 1 quad text{(1)} = 1 + 2t(e - 1) = 1 quad text{(2)} 2t(e - 1) = 0 Rightarrow either t=0 or e=1 From (2): - If t=0: then from (1): 0 = 1 -> contradiction - So = 1 Now, then from (1): t(2 - 1) = t(1) = 1 Rightarrow = 1 So = 1, = 1. So is at the endpoint G? Wait, when = 1, = (1,1) = C, so CE is degenerate. Then = (2 - 1, 2 cdot 1 - 1) = (1,1), so = Thus lies on segment DG only at endpoint G, so = G, which is endpoint. So the point lies **on** segment DG (at endpoint), but the problem says lies on segment DG, which often allows endpoints, but square CEFG with E=C and G=C is degenerate - all points coincide. So we discard e=1. No valid < 1. So **no non-degenerate solution in counterclockwise orientation**. --- #### Case 2: Clockwise square, = (e, 1) Now = (0,1), = (e,1) Both points have = 1, so segment DG is horizontal on line y=1 We want = (1,1) to lie on segment DG Since DG runs from (0,1) to (e,1), it lies on y=1, and from 0 to For (1,1) to lie on this segment, we need 1 in [0,e] -> ge 1 But lies on segment BC, which goes from B=(1,0) to C=(1,1), so in [0,1] Thus, ge 1 and le 1 -> = 1 Again, only degenerate solution. So both orientations give degenerate solutions unless e=1 But this contradicts the problem, which implies **non-degenerate** solution. So theres **misunderstanding** of the configuration. --- Wait: is the square CEFG drawn outward? Or can it be on the other side? Wait: in both cases, lies on segment DG, but maybe DG is different segment? Wait: point is at (0,1), so DG goes from through G, so if is at (e,1), then DG is from (0,1) to ( e,1), which only reaches x=e. For C=(1,1) to lie on this segment, we need ge 1, which is not allowed. But in **counterclockwise** case, = (2 - e, 2e - 1) Is it possible that in that case, DG contains C=(1,1)? We only computed when is exactly on segment: we found only e=1 But maybe **C lies on segment DG** does **not** require e=1? 35 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Wait - we made an algebra mistake. We had: Parametrize segment from D=(0,1) to G=(2 - e, 2e -1) We want point C=(1,1) to lie **on the segment** - not just on the ray - within the segment. So solve: x(t) = (1 - t)(0) + t(2 - e) = t(2 - e) = 1 So = frac{1}{2 - e} Then y(t) = (1 - t)(1) + t(2e - 1) Set y(t) = 1 So: (1 - t)(1) + t(2e - 1) = 1 1 - + t(2e - 1) = 1 - + t(2e - 1) = 0 t( -1 + 2e - 1 ) = t(2e - 2) = 0 So again, either t=0 or 2e - 2 = 0 Rightarrow e=1 Thus, only solution: e=1 So again - degenerated. Thus, in both configurations, lies on DG **only when e=1**. So, **unless our assumption about the direction of square CEFG is wrong**, we have no valid solution. But the problem says \"**C lies on segment DG**\", and we are to assume < 1 So whats wrong? Ah - perhaps the square CEFG is not built externally from CE, but in another way? Wait - maybe square CEFG is built **on side CE**, but using point and E, and then extending perpendicularly - but maybe the direction from to G? Wait - lets try different labeling. Maybe CEFG means that from to E, then to F, to G, to C. So the vertex is such that vec{CG} is obtained by rotating vec{CE} by 90^circ, or -90^circ, depending on direction. Wait - in square CEFG, after to E, the turn from to is 90^circ, so the direction CF? No - we can get as + text{rotation}(vec{CE}, 90^circ)? Wait: in vector geometry, for square CEFG, the point such that to to to to C, then vector vec{EG} is 90^circ rotation of vec{EC}? Lets derive formula. Note: In square CEFG, vector vec{EF} = R_{90^circ}(vec{CE}) and vec{FG} = R_{90^circ}(vec{EF}) = R_{180^circ}(vec{CE}) = -vec{CE} So: - From to E: vec{v} = vec{CE} - to F: R( vec{v} ) - to G: - vec{v} - to C: -R(vec{v}) So = + (-vec{v}) = (E + R(vec{v})) - vec{v} So: = + R(vec{v}) - vec{v} Let me test this. We have vec{v} = vec{CE} = (0, - 1) Let R_{90^circ} be **counterclockwise**: (x,y) mapsto (-y, x) Then R(vec{v}) = R(0, e-1) = (-(e-1), 0) = (1 - e, 0) Then = + (1 - e, 0) - (0, - 1) = (1, e) + (1 - e, 0) - (0, - 1) = (1 + 1 - e, e) - (0, - 1) = (2 - e, e) - (0, - 1) = (2 - e, - (e - 1)) = (2 - e, 1) Wait - this is different from earlier! So in this direction, = (2 - e, 1) Similarly, for clockwise rotation: R_90^circ(vec{v}) = (e - 1, 0) - because clockwise of (0, e-1) is (e-1, 0) Then = + (e - 1, 0) - (0, - 1) = (1, e) + (e - 1, 0) - (0, - 1) = (1 + - 1, e) - (0, - 1) = (e, e) - (0, - 1) = (e, - (e - 1)) = (e, 1) Wait - this is the same as before! 36 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL So in counterclockwise case, = (2 - e, 1), not (2 - e, 2e - 1) That was mistake in previous work.So lets correct that. --- ### Revised: Correct rotation to get point Lets use: = + R(vec{CE}) - vec{CE}, where R(vec{v}) is 90^circ rotation. - Case 1: Counterclockwise rotation vec{CE} = (0, - 1) R_{90^circ}(vec{CE}) = (-(e - 1), 0) = (1 - e, 0) Then: = + (1 - e, 0) - (0, - 1) = (1, e) + (1 - e, 0) - (0, - 1) = (1 + 1 - e, e) + (0, -e + 1) = (2 - e, - + 1) = (2 - e, 1) So, = (2 - e, 1) - Case 2: Clockwise rotation vec{CE} = (0, - 1) R_{90^circ}(text{clockwise}) (0, - 1) = (e - 1, 0) Then: = + (e - 1, 0) - (0, - 1) = (1, e) + (e - 1, 0) - (0, - 1) = (1 + - 1, e) + (0, -e + 1) = (e, - + 1) = (e, 1) So again, same as before. Thus corrected: Orientation ----------------- Counterclockwise (2 - e, 1) Clockwise (e, 1) Now check the condition that **C = (1,1) lies on segment DG**. Recall = (0,1), = (1,1), both on line y=1 --- #### Case 1: Counterclockwise square - = (2 - e, 1) Now = (0,1), = (2 - e, 1), both on horizontal line y=1 Segment DG is from x=0 to x=2 - We need (1,1) to lie on segment DG, i.e., 0 le 1 le 2 - So 1 le 2 - Rightarrow le 1 Which is always true since in [0,1] And 1 ge 0 - always true. So as long as 2 - ge 1, i.e., le 1, lies **between** and G? Wait: (1,1) lies between D=(0,1) and G=(2 - e, 1) if 1 lies between 0 and 2 - So yes, when 2 - ge 1 -> le 1, which is always true. But for the point to be **on segment**, we need 0 le 1 le 2 - So 1 le 2 - Rightarrow le 1 - always true for allowed And 1 ge 0 - trivial So for **all** in [0,1], = (1,1) lies on **segment DG**? Wait - only if 2 - ge 1, which is le 1, true, and 1 is in [0, 2-e] Yes. So every in [0,1] satisfies in DG But is this actual? For example: = 0, = (1,0) = B, = (2 - 0, 1) = (2,1) Then DG: D=(0,1) to G=(2,1) - so yes, C=(1,1) is midpoint. For = 1, = = (1,1), = (1,1) - so = C, and DG from (0,1) to (1,1) - so is endpoint - still on segment. So **in counterclockwise case**, for **all** in [0,1], point lies on segment DG That **solves** the earlier issue - the constraint is **satisfied for all e** in this orientation. Now, in clockwise case: = (e, 1) So = (0,1), = (e,1), so segment from x=0 to x=e We want = (1,1) to lie on this - requires ge 1, again only when e=1 37 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL So only degenerate. Thus, **only valid configuration is counterclockwise**, with = (2 - e, 1), in (0,1) --- ### Step 4: Apply the collinearity: A, D, and are collinear, where is circumcenter of triangle AEG We have: - = (0,0) - = (1,e) - = (2 - e, 1) We want circumcenter of triangle AEG to lie on line AD Now, AD is from A=(0,0) to D=(0,1), so its the **vertical line = 0** So, the circumcenter of triangle A,E,G lies on x=0 We now compute the circumcenter of triangle with vertices: - A(0,0) - E(1,e) - G(2 - e, 1) The circumcenter is the intersection of **perpendicular bisectors** --- #### Step 4.1: Find perpendicular bisector of AE - Midpoint of AE: M_{AE} = left( frac{0+1}{2}, frac{0+e}{2} right) = left( frac{1}{2}, frac{e}{2} right) - Direction vector of AE: (1,e) - So perpendicular direction: (-e, 1) or (e, -1) - dot product 1(-e) + e(1) = -e + = 0 So perpendicular bisector has slope -frac{1}{text{slope of } AE} = -frac{1}{e} (if ne 0) Slope of AE: So perp bisector: passes through (frac{1}{2}, frac{e}{2}), slope -frac{1}{e} Equation: - frac{e}{2} = -frac{1}{e} left(x - frac{1}{2} right) frac{e - 0}{1 - 0} = , so perp slope is -frac{1}{e} --- #### Step 4.2: Perpendicular bisector of AG - = (0,0), = (2 - e, 1) - Midpoint M_{AG} = left( frac{0 + 2 - e}{2}, frac{0 + 1}{2} right) = left( frac{2 - e}{2}, frac {1}{2} right) - Direction vector: (2 - e, 1) - Perpendicular direction: (-1, 2 - e) or (1, -(2 - e)), since dot product: (2 - e)(-1) + (1)(2 - e) = - (2 - e) + (2 - e) = 0 Take direction ( -1, 2 - ) Slope of AG: So perp slope: Thus, equation: - frac{1}{2} = - (2 - e) left( - frac{2 - e}{2} right) frac{1 - 0}{2 - - 0} = frac{1}{2 - e} - (2 - e) --- Now, the circumcenter lies at intersection of these two perpendicular bisectors, and we require = 0 So plug = 0 into first equation and solve for e, using the two equations. --- **Step 4.3: Plug x=0 into equation of perp bisector of AE** Equation: - frac{e}{2} = -frac{1}{e} left( 0 - frac{1}{2} right) = -frac{1}{e} left( -frac{1}{2} right ) = frac{1}{2e} So: = frac{e}{2} + frac{1}{2e} So the circumcenter has coordinates = (0, frac{e}{2} + frac{1}{2e}) 38 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Now, we must also have this point satisfy the perp bisector of AG So plug x=0, = frac{e}{2} + frac{1}{2e} into the equation of the perp bisector of AG: Equation: - frac{1}{2} = - (2 - e) left( - frac{2 - e}{2} right) Set = 0: Left side: {1}{2} + frac{1}{2e} Right side: - 1/2 = left( frac{e}{2} + frac{1}{2e} right) - frac{1}{2} = frac{e}{2} - frac - (2 - e) left( 0 - frac{2 - e}{2} right) = - (2 - e) left( -frac{2 - e}{2} right) = (2 - e) cdot frac{2 - e}{2} = frac{(2 - e)^2}{2} Set both equal: frac{e}{2} - frac{1}{2} + frac{1}{2e} = frac{(2 - e)^2}{2} Multiply both sides by 2: - 1 + frac{1}{e} = (2 - e)^2 Compute right-hand side: (2 - e)^2 = 4 - 4e + e^2 So: - 1 + frac{1}{e} = 4 - 4e + e^2 Bring all terms to one side: - 1 + frac{1}{e} - 4 + 4e - e^2 = 0 Simplify: (5e - 5) - e^2 + frac{1}{e} = Multiply both sides by to eliminate denominator: e(5e - 5) - e^3 + 1 = 0 5e^2 - 5e - e^3 + 1 = 0 - e^3 + 5e^2 - 5e + 1 = 0 Multiply by -1: e^3 - 5e^2 + 5e - 1 = 0 We now solve: e^3 - 5e^2 + 5e - 1 = 0 Try rational roots: candidates +/-1 - e=1: 1 - 5 + 5 - 1 = 0 -> **yes!** Factor out (e - 1) Use polynomial division or synthetic division: Divide e^3 - 5e^2 + 5e - 1 by - 1 - Coefficients: 1 -5 5 -1 - Bring down: 1 - Multiply: 1 cdot 1 = 1, add to -5 -> -4 - -4 cdot 1 = -4, add to 5 -> 1 - 1 cdot 1 = 1, add to -1 -> 0 So quotient: e^2 - 4e + 1 Thus: (e - 1)(e^2 - 4e + 1) = 0 Solutions: - = 1 - = frac{4 pm sqrt{16 - 4}}{2} = frac{4 pm sqrt{12}}{2} = frac{4 pm 2sqrt{3}}{2} = 2 pm 39 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL sqrt{3} Now 2 + sqrt{3} approx 2 + 1.732 = 3.732 > 1 2 - sqrt{3} approx 2 - 1.732 = 0.268 in (0,1) So only two valid solutions in [0,1]: = 1, = 2 - sqrt{3} Now = 1 gives degenerate square: = C, and = (2 - 1,1) = (1,1), so square collapses. We discard it. So valid solution: = 2 - sqrt{3} We now compute FG, which is the side length of square CEFG Now, CEFG has side CE = (1,1), = (1,e) So distance: CE = 1 - = 1 - quad (text{since } < 1) We have = 2 - sqrt{3} So: 1 - = 1 - (2 - sqrt{3}) = -1 + sqrt{3} = sqrt{3} - 1 Is that correct? Thus: FG = CE = sqrt{3} - 1 Wait - but earlier guess was sqrt2 - this is different. Wait: confirm the side length: yes, FG = CE, since CEFG is square. We have = 2 - sqrt{3} approx 0.2679, 1 - approx 0.7321, and sqrt{3} - 1 approx 1.732 - 1 = 0.732 - yes. So FG = sqrt{3} - 1 But wait, is this the required answer? Let me double-check all steps. ### Verification: We assumed counterclockwise square with = (2 - e, 1) Validated that = (1,1) on segment DG (yes) Computed circumcenter: from perp bisector of AE, got x=0 at = frac{e}{2} + frac{1}{2e} Then required it to satisfy equation from perp bisector of AG, derived cubic: e^3 - 5e^2 + 5e - 1 = 0 Roots: e=1, = 2 pm sqrt{3} Only 2 - sqrt{3} in (0,1) Then CE = 1 - = 1 - (2 - sqrt{3}) = sqrt{3} - 1 So FG = sqrt{3} - 1 But wait - is this the simplest form? Yes. Now, is this value consistent? Check if the circumcenter is well-defined. For = 2 - sqrt{3}, all points are distinct: A, E, not collinear? Check: three points A(0,0), E(1,e), G(2-e,1) They form triangle? Yes, since not collinear. And is at x=0, meaning on line AD - as required. So all conditions are satisfied. --- Earlier summary said the only plausible symmetric value is sqrt2, but clearly thats not the case - our derivation gives clean algebraic solution.So we reject that guess. Thus, correct answer is Wait - is that possible? Let me compute approximate values: - AB = 1 - CE = sqrt{3} - 1 approx 0.732 - So FG = sqrt{3} - 1 Yes. But the problem says \"compute FG\", and we have clean expression. Alternatively, could we have expressed in another form? boxed{sqrt{3} - 1} 40 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Alternatively, is there symmetry? Note that in the expression: = 2 - sqrt{3} Rightarrow 1 - = sqrt{3} - 1 Yes. So final answer: boxed{sqrt{3} - 1} But let me just check - perhaps the square is measured differently? Wait - the side of square CEFG is CE, which is distance between and E. = (1,1), = (1,e), so vertical segment of length 1 - Yes. So side length FG = CE = 1 - = sqrt{3} - 1 All conditions are satisfied. We tested the configuration: counterclockwise square, on DG (since DG goes from D=(0,1) to G=(2-e,1), with 2-e = 2 - (2 - sqrt{3}) = sqrt{3} approx 1.732 > 1, so C=(1,1) is strictly between and - yes. E=(1,e) with e=2-sqrt{3} approx 0.267, so lies on BC, from B=(1,0) to C=(1,1) - yes. Circumcenter lies on AD - we derived it. So all constraints satisfied. --- ### Conclusion The previous attempt failed because: - It incorrectly assumed the square orientation and coordinates. - It used the wrong location for point G. - It minimized over degenerate cases. We corrected the configuration by properly applying rotation to get point G, and verified that only the counterclockwise orientation satisfies all constraints. We solved the collinearity condition and obtained valid non-degenerate solution.}*) Final answer: boxed{sqrt{3} - 1} N. RC Prompts Figure 17: Summary-conditioned reasoning prompt â„ğ‘…. 41 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 18: Summarization prompt â„ğ‘†. Figure 19: Self-refinement â„refine prompt. Figure 20: Self-verification â„verify prompt. 42 Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL Figure 21: Prompts from the summary length experiments in Section 5.2. The instructions are inserted into the summarization prompt â„ğ‘† in order to control the level of detail in the resulting summaries. The default level of detail is 2 paragraphs. Figure 22: Annotation prompt used by Qwen3-80B-Next-Instruct in Section 5.3."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University"
    ]
}