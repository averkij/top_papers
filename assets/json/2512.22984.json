{
    "paper_title": "Reverse Personalization",
    "authors": [
        "Han-Wei Kung",
        "Tuomas Varanka",
        "Nicu Sebe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 8 9 2 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Reverse Personalization",
            "content": "Han-Wei Kung1 Tuomas Varanka2 Nicu Sebe1 1University of Trento 2University of Oulu hanwei.kung@unitn.it Input LDFA [45] RiDDLE [51] Textual Inversion [22] Ours Identity anonymized Subject agnostic Attr. & scene kept Attr. controllable Yes Yes Poor No Yes Yes Poor No Yes No Good No Default Change age (older) Change race (Asian) Yes Yes Good Yes Figure 1. Our reverse personalization method removes identity-specific features while preserving both the original facial attributes and surrounding scenewithout requiring subject finetuning. It also provides intuitive user control over which attributes are retained or modified, enabling flexible and customizable anonymization for downstream applications."
        },
        {
            "title": "Abstract",
            "content": "Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identityspecific features rely either on the subject being wellrepresented in the pre-trained model or require model finetuning for specific identities. In this work, we analyze the identity generation process and introduce reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the models training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attributecontrollable anonymization. We demonstrate that our method achieves state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/ hanweikung/reverse-personalization. 1. Introduction Given an image of human face, how can we remove identity-specific features while preserving non-identity attributes? Recent advances in diffusion models have enabled the creation of realistic visual content [65, 68], including face synthesis [8, 32, 44, 63]. Several studies [25, 52, 78] have demonstrated the effectiveness of these models in generating faces from prompts and identities. Additionally, research [9, 28, 33, 60] has shown that manipulating attention weights on specific tokens within prompts can influence semantic alignment and provide fine-grained control. Building on this insight, we observe that adjusting attention weights on celebrity names within prompts controls the likeness of generated images. However, this approach presents one limitation. If the input image shows noncelebrity, their identity may not exist in the models learned feature space. In such cases, modifying attention weights has negligible effect, as the model lacks representation of the individual. To overcome this limitation, we first employ diffusion inversion techniques [33, 75], which map an input image into the latent space of pre-trained model. This enables us to synthesize facial images with specific traits using conditioning methods. Beyond text prompts, recent advances have introduced expressive conditioning techniques, such as face embeddings [77, 82, 85] and semantic masks [18, 32, 87]. Inspired by developments in personalization [22, 78, 85], we adopt identity-conditioned generation using face embeddings, allowing us to extract identity features from arbitrary input images. Our approach offers several key advantages. As an inversion-based method, it avoids model retraining, thus preserving the original generative capabilities of the diffusion model. This also ensures compatibility with other identity-conditioned generation methods and enables flexible control over high-level facial attributes. Importantly, our method supports image-only inputs, eliminating reliance on textual instructions. challenge then arises: how can we modulate how generated faces reflect the identity in the input image? By analyzing the generation process, we find that increasing the classifier-free guidance [30] scale amplifies identitydefining features. The model appears to first synthesize neutral face, then progressively injects characteristic traits. This insight led us to experiment with guidance scales, hypothesizing that they could suppress identity-specific features and produce reverse identity. Our experiments confirm this hypothesis, motivating the development of novel mechanism termed reverse personalization. Reverse personalization offers compelling benefits in face anonymization, where the goal is to protect personal identity while preserving utility. This capability is important across sectors such as healthcare [59, 84] and security [24, 41]. Anonymization techniques help address ethical concerns about surveillance and individual autonomy, while ensuring compliance with data privacy regulations including GDPR [2] and CCPA [1]. progress diffusion-based in GANanonymization, current methods face persistent challenges. Many struggle to strike balance between removing identity-specific features and preserving non-identity attributes and realism [12, 57]. Furthermore, research has shown that individuals willingness to disclose personal informationparticularly sensitive demographic attributes such as age, race, and genderis context-dependent [64]. individuals may For in workplace settings, instance, Despite and withhold such details to avoid bias or misunderstandings, especially in environments with status differences or limited trust [64]. Conversely, in healthcare contexts, disclosing demographic information can lead to improved outcomes by enabling more personalized and effective treatment [21]. However, existing face anonymization methods lack control over whether such attributes are retained or altered [47]. In contrast, our reverse personalization framework addresses this limitation, enabling users to flexibly control facial attributes in anonymized outputs. We analyze identity-conditioned generation and apply our findings to face anonymization. We demonstrate that reverse personalization removes identifiable facial features while maintaining realism and attribute consistency. We contribute: Conditional inversion: we present conditioning strategy that guides the inversion process to facilitate identity manipulation. Reverse personalization: we propose guidance mechanism that steers the generative process away from identity-defining features, enabling anonymization while maintaining realism. Our method achieves an optimal balance between identity obfuscation, attribute preservation, and visual quality. Attribute-controllable anonymization: our approach includes intuitive controls for adjusting facial features like age, gender, and ethnicity, making it easy to generate customizable, anonymized results. 2. Related Work Personalization. The field of personalized text-to-image generation [14, 22, 48, 70, 74] focuses on adapting diffusion models to synthesize images of specific subjects, styles, or concepts that are meaningful to individual users. This line of research enables models to learn visual concepts from few example images, allowing for tailored image generation beyond general models. Textual Inversion [22] optimizes token embeddings linked to placeholder token, allowing the model to incorporate new concepts into text-driven generation with few subject images. Similarly, DreamBooth [70] introduces fine-tuning approach that binds an identifier to specific subject. It leverages the semantic priors of pre-trained diffusion models while employing class-specific prior preservation loss to maintain visual consistency with the broader data distribution. Despite effectiveness, these methods suffer from computational costs, requiring several minutes to hours for finetuning. To overcome this limitation, parameter-efficient fine-tuning methods have emerged. HyperDreamBooth [71] introduces hypernetwork-based architecture that generates personalized weights from single image, offering speedupas much as 25 times faster than DreamBooth [70] 2 and 125 times faster than Textual Inversion [22]. JeDi [86] reduces computational demands by learning the joint distribution of multiple text-image pairs of common subject, enabling finetuning-free personalization. IP-Adapter [85] adopts an alternative strategy based on image prompt adaptation, allowing users to guide image generation using reference images while retaining controllable text prompts. These personalization techniques have unlocked variety of applications, including identity-preserving face generation [25, 52, 54, 73, 78, 80, 82], virtual try-on [81], and customized content creation [61]. However, while most efforts focus on preserving identity, style, or subject appearance, we repurpose these principles toward the opposite goalremoving identifiable facial traits. Our work extends the technical foundations of personalization to enable attribute-controllable face anonymization. Face anonymization. Face anonymization, or face deidentification, refers to concealing or altering facial traits to protect individual privacy in images. Traditional methods, such as blurring, pixelation, and masking, are simple and effective at preventing identification by humans. However, these approaches degrade the visual quality of images, destroying important contextual information like facial expressions, pose, and backgroundlimiting their usefulness for downstream computer vision tasks [12, 57]. In response, modern face anonymization techniques strive for balance between privacy protection and data utility. Rather than obscuring faces, these methods aim to prevent identity recognition while preserving identity-agnostic attributes crucial for tasks such as emotion analysis [37, 42] and estimating head pose [53] and gaze [79]. Among these, GAN-based methods [11, 15, 27, 56] have been explored. DP [36] and its successor DP2 [34] employ conditional GANs [58] to generate synthetic faces that maintain pose and background context. GANonymization [26] further advances this idea by focusing on preserving facial expressions during anonymization. Methods like FALCO [6] and RiDDLE [51] exploit the latent space of StyleGAN2 [40] to generate realistic anonymized faces while retaining nonidentity features. Driven by recent progress in generative modeling, diffusion models have emerged as promising for face anonymization. For example, LDFA [45] combines face detection with latent diffusion model to generate in-painted faces. FAMS [49] uses only reconstruction loss, eliminating dependence on identity losses derived from face recognition models or the use of facial landmarks and masks, which are prone to inaccuracies. Despite these advancements, challenges remain. Achieving an balance between anonymization and preservation of image utility is difficult [35]. Additionally, most methods offer limited controllability over which sensitive attributes are preserved or concealed during anonymization. To address the limitations, our reverse personalization framework leverages the precise reconstruction and generative power of diffusion models. Our method enables user-controlled anonymization by allowing selective preservation or removal of specific facial attributes, while maintaining realism and utility for identity-agnostic tasks. 3. Method 3.1. Preliminary DDPM Inversion. Denoising Diffusion Probabilistic Models (DDPMs) [31] define forward process that gradually adds Gaussian noise to data and learned reverse process that denoises it step by step. The reverse process reconstructs xt1 from noisy input xt as follows: xt1 = ˆµt(xt) + σtzt, zt (0, I), (1) where ˆµt(xt) is the predicted mean, σt is the variance schedule, and zt is standard Gaussian noise. To recover the noise used at each step, we compute: zt = xt1 ˆµt(xt) σt . (2) IP-Adapter. The IP-Adapter [85] is lightweight module that enables diffusion models to use image prompts by modifying their attention mechanisms. Specifically, it integrates visual information without requiring model retraining. The modified attention is computed as: Znew = Attention(Q, K, V) + λipa Attention(Q, K, V), (3) where Q, K, are the original queries, keys, and values, and K, are keys and values derived from the image prompt. The IP-Adapter [85] scale λipa controls the influence of the visual prompt. This mechanism facilitates flexible multimodal conditioning while preserving the pretrained models structure. 3.2. Motivation Given an image of person, our goal is to modify defining facial features while preserving non-identifying features and the surrounding context. Achieving balance between data utility and identity protection is central to face anonymization, which supports scientific progress [27] and privacy [62]. An intuitive approach leverages inversion techniques in diffusion models, which map real images into latent representations compatible with pretrained generator. One such method, Null-text Inversion [60], employs DDIM inversion [75] to extract sequence of latent codes. These are 3 Input Null-text Inversion [60] Textual Inversion [22] Ours 3.3. Reverse personalization To address the limitations of prompt-based reweighting and fine-tuning methods, we propose an approach to modify identity-specific facial features without relying on prompt reweighting or computationally intensive personalization. Our method leverages identity embeddings extracted from facial images and integrates them through identity-conditioned adapters to modulate the diffusion process. schematic overview of our approach is illustrated in Fig. 3. In standard diffusion model inversion, the denoising network uses prompt embeddings to guide the recovery of noise trajectories, ensuring that the reconstructed image matches the input prompt semantically and visually. This enables prompt-based image editing, where modifying the prompt can yield targeted changes in the output. However, in our reverse personalization settingwhere only one image is provided and the goal is to alter identity prompt substitution is not applicable. Instead, we introduce null-identity embeddings to guide the inversion process without identity-specific conditioning. We further improve inversion efficiency using DPMSolver++ [55], higher-order solver that surpasses DDPMbased methods in speed [9]. This also ensures perfect reconstruction of the input image, preserving structural and contextual details while enabling modification of identityspecific features. Our inversion process is formulated as: xt1 ˆµt(xt, xt+1, id) σt zt = , = T, . . . , 1, (4) where ˆµt(xt, xt+1, id) is the second-order estimate of the denoised sample, id denotes the null identity embedding, and σt is the noise variance at step t. Text-to-image diffusion models employ classifier-free guidance [30] to improve text fidelity by interpolating between conditional and an unconditional prediction. In contrast, we reinterpret classifier-free guidance [30] to control identity alignment. Specifically, we condition one forward pass on the identity embedding and leave the other unconditioned. Our reverse personalization guidance is defined as: ˆϵθ(xt, t, cid) = λcf ϵθ(xt, t, cid) + (1 λcf g) ϵθ(xt, t, id), (5) where λcf is the guidance scale, cid is the identity embedding, and id is the null identity. While standard personalization approaches use positive guidance scale to reinforce identity, we instead apply negative guidance scale to steer generation away from the provided identity. Our motivation comes from an observation: as the guidance scale increases, the model tends to enhance and exaggerate facial characteristics. This suggests 4 No prior knowledge required No fine-tuning needed No No Yes No Yes Yes Figure 2. Prompt-based attention reweighting methods such as Null-text Inversion [60] can modify facial identity only when the diffusion model knows the subject (e.g., well-known figures, such as Obama). Personalization methods like Textual Inversion [22] require fine-tuning with multiple reference images. Unlike these methods, our reverse personalization approach can modify facial identity without prior model knowledge or fine-tuning. then used to optimize the null-text embedding in classifierfree guidance [30], allowing the model to reconstruct the input image. This optimized embedding enables methods like Prompt-to-Prompt [28] to reweight attention maps during generation, offering fine-grained control. For instance, as shown in Fig. 2, attenuating the influence of the token Obama in the prompt photo of Obama alters identity-specific facial features while preserving nonidentity-relevant attributes like expression and the background context. However, this strategy has limitation: it relies on the models knowledge of the subjects identity. When the individual is poorly represented in the training datathe woman in Fig. 2adjusting prompt weights has little effect on the output. To address this, model personalization techniques, such as Textual Inversion [22], are applied. Textual Inversion [22] fine-tunes the models word embeddings on few images of concept (e.g., the women in Figs. 1 and 2) and associates with unique token S. This token can be used in prompts (e.g., photo of S) to trigger identity-aware generation. Although this approach moves beyond the models prior knowledge, it has challenges. It demands computational resources to fine-tune and may underperform when limited reference images are available, failing to capture the identity. (a) Conventional prompt-based reweighting method. (b) Our identity-based method. Figure 3. Our reverse personalization framework. Unlike prior prompt-based reweighting approaches that guide both inversion and generation using textual prompts, our method leverages identity information to condition these processes. The framework also supports intuitive user control, enabling selective retention or modification of semantic facial attributes. Generated Input Generated -8.0 0.773 -4.0 λcf 4.0 0. ID Distance 0.170 8.0 0.229 Increase Increase Figure 4. We visualize how varying the classifier-free guidance [30] scale affects identity preservation. Identity distance from the input increases with scaleespecially in the negative directionconfirming divergence from the input identity. that negating the scale may attenuate those features. We explore this hypothesis and visualize the results in Fig. 4. 3.4. Attribute-controllable anonymization Most anonymization methods do not allow users to choose which facial features to keep or change, limiting an important part of user control. However, research has shown that individuals privacy preferences are shaped by balance between perceived risks and potential benefits [64]. In some cases, retaining certain personal attributes can serve societal and individual goalsfrom improving healthcare [21] and workplace equity [10] to advancing research [4] and social justice [19]. To address this, our approach enables intuitive and consistent control over these attributes through natural language prompts. This is possible through our integration of image prompt adapters, which condition the diffusion model on visual identity cues without disrupting its interpretation of textual instructions. To control facial attributes while anonymizing an image, we first invert the image to its latent noise representation xT . During reverse diffusion process, we guide the model using new textual prompt ˆcattr that specifies the desired facial attributes (e.g., young woman or an elderly man). We modify the sampling equation by replacing the orig5 inal attribute prompt cattr with the updated prompt ˆcattr, while reusing the original noise trajectory zt. The sampling step becomes: ˆxt1 = ˆµt(ˆxt, ˆxt+1, cid, ˆcattr) + σtzt. (6) By substituting the previously estimated noise zt, we obtain: ˆxt1 = ˆµt(ˆxt, ˆxt+1, cid, ˆcattr) + xt1 ˆµt(xt, xt+1, id, cattr). (7) This formulation ensures that the anonymized output respects both the conditioning identity and the desired semantic attributes, allowing precise and interpretable control over the anonymization process. 4. Experiments We evaluate our method across benchmarks, comparing it with prior face anonymization approaches in terms of identity removal, attribute preservation, image quality, and controllability. We also present ablation studies to analyze key design choices. 4.1. Hyperparameter analysis 5 presents of Figure hyperparametersclassifier-free scale and IP-Adapter [85] scaleevaluated on CelebA-HQ [38] and FFHQ [39] datasets. analysis [30] quantitative guidance Figure 5a illustrates how the classifier-free guidance [30] scale influences re-identification rates, computed using two state-of-the-art face recognition models: SwinFace [66] and AdaFace [43]. When the guidance scale approaches zero, the generated faces closely resemble the original inputs, leading to higher re-identification rates. Conversely, as the guidance scale becomes more negative, the generated faces diverge further from the originals, resulting in lower reidentification rates. While reidentification rates, excessively negative values degrade reducing the guidance lowers scale Re-ID (%) Attribute preservation Image quality SwinFace AdaFace Expression Gaze Pose FID Face IQA CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ Ours NullFace [50] FAMS [49] FALCO [6] RiDDLE [51] LDFA [45] DP2 [34] 2.622 0.489 7.467 1.889 - 19.578 3.889 4.800 0.844 24.111 - 2.044 21.000 6. 0.783 0.157 3.309 0.179 - 11.495 0.901 2.029 0.358 13.912 - 0.512 11.369 1.927 9.119 10.025 10.012 10.206 - 8.653 9.931 9.353 9.856 8.847 - 10.049 10.392 10.141 0.152 0.165 0.165 0.263 - 0.265 0.263 0.177 0.187 0.176 - 0.214 0.346 0. 0.050 0.055 0.054 0.088 - 0.093 0.163 0.052 0.058 0.049 - 0.080 0.114 0.163 4.809 8.426 17.253 39.501 - 8.303 17.544 8.651 8.932 11.381 - 65.141 10.390 18.809 0.856 0.758 0.815 0.875 - 0.785 0.599 0.921 0.796 0.808 - 0.674 0.857 0. Table 1. Quantitative comparison of facial anonymization methods on CHQ (CelebA-HQ [38]) and FHQ (FFHQ [39]). The best result is marked in bold, and the second-best is underlined. (a) (c) (b) (d) Figure 5. Hyperparameter analysis of classifier-free guidance [30] scale and IP-Adapter [85] scale on CHQ (CelebA-HQ [38]) and FHQ (FFHQ [39]). image quality. As shown in Figs. 5b and 5c, overly negative guidance scales lead to higher Frechet Inception Distance (FID) [29] and reduced scores from face-specific image quality assessment (IQA) model [13]. Figure 5d examines the impact of the IP-Adapter [85] scale under fixed negative guidance setting. This parameter controls the strength of identity embeddings. IPAdapter [85] scale of 0.0 disables identity conditioning, leading to poor anonymization and high re-identification rates. As the scale increases, the model increasingly leverages the identity embeddings, enabling more effective modification of identity-specific features and drop in reidentification rates. 4.2. Comparison with existing methods ing CelebA-HQ [38] and FFHQ [39] datasets. We evaluated 4,500 subjects from each dataset. Our method was implemented on Stable Diffusion XL (SDXL) [65] for its superior image quality, with hyperparameters set to an IPAdapter [85] scale of 1.0 and classifier-free guidance [30] scale of 10.0. On an A100 GPU, generating 1024 1024 image took approximately 13 seconds. Evaluation was conducted across three dimensions: identity removal, attribute preservation, and image quality. Identity removal was measured by re-identification rates using SwinFace [66] and AdaFace [43]. To avoid bias, different face recognition model [16] was used to extract identity embeddings during generation. Attribute preservation was assessed via three metrics: expression difference using 3D face reconstruction model [17], pose difference using head pose estimation network [69], and gaze difference using gaze estimation model [3]. For image quality, we adopted the approach from previous studies [6, 15, 27, 56] by calculating FID [29]. Additionally, we utilized facespecific IQA model [13]. As summarized in Tab. 1, our method and NullFace [50] exhibit balanced performance without weaknesses. FAMS [49] and LDFA [45] suffer from high reidentification rates, indicating poor anonymization. Despite leveraging StyleGAN2 [40], known for producing photorealistic images, FALCO [6] and RiDDLE [51] exhibit highest FID [29] scoresdue to their inability to preserve background consistency, resulting in generated images that deviate from the originals. DP2 [34] struggles with pose preservation and image quality, likely because its inpainting strategy relies on pose estimation, which can be inaccurate, leading to generated faces misaligned with the original orientation. We also include privacyutility trade-off plots in the supplementary material (Sec. 6) to visualize the relationship between identity removal, attribute preservation, and image quality across methods. We compare our method against six state-of-the-art facial anonymization approaches: NullFace [50], FAMS [49], FALCO [6], RiDDLE [51], LDFA [45], and DP2 [34], usWhile NullFace [50] achieves the lowest re-identification rates, it underperforms in attribute preservation and image quality compared to our method. NullFace [50] samples 6 Input Ours NullFace [50] FAMS [49] FALCO [6] LDFA [45] DP2 [34] Figure 6. Qualitative comparison of anonymization results on CelebA-HQ [38]. It does this by supports attribute-guided anonymization. adapting StyleMC [46], which manipulates images along semantically meaningful directions in the GAN [23] latent space using CLIP-based [67] loss guided by textual prompts. While enabling attribute control improves DP2 [34]s accuracy in matching target attributes, its overall performance remains below that of our method. Age Sex (%) Race (%) CHQ FHQ CHQ FHQ CHQ FHQ Ours* Ours NullFace [50] FAMS [49] FALCO [6] RiDDLE [51] LDFA [45] DP2 [34]* DP2 [34] 3.744 4.931 5.461 6.293 5.016 - 4.426 3.890 4.822 4.314 5.854 6.017 6.598 - 6.144 5.888 4.468 5. 99.485 92.663 87.113 29.802 89.138 - 88.713 98.124 87.523 98.638 82.720 77.023 58.704 - 73.437 77.401 96.674 79.750 87.016 60.882 62.784 32.749 66.014 - 67.572 82.433 63.951 76.548 49.479 56.048 44.621 - 54.039 59.238 83.033 58.865 * w/ attribute control w/o attribute control Table 2. Accuracy of preserving high-level facial attributes (age, sex, race) in anonymized faces. In Tab. 2, our attribute-controlled anonymization achieves the highest accuracy in preserving all three attributes on CelebA-HQ [38], and two of the three attributes on FFHQ [39]. For the race attribute on FFHQ [39], DP2 [34] with attribute control slightly outperforms our method. Figure 8 presents qualitative examples that illustrate the effectiveness of attribute control, highlighting differences between preserving and altering these facial features. Input Ours NullFace [50] FAMS [49] RiDDLE [51] LDFA [45] DP2 [34] 4.4. Ablation study Figure 7. Qualitative comparison of anonymization results on FFHQ [39]. anonymized identities in the embedding space of an identity encoder, whereas our approach operates in the latent space of diffusion models. drawback of the former is that sampled embeddings may not always correspond to valid human faces, leading to lower Face IQA [13] scores. Figures 6 and 7 present qualitative comparisons on CelebA-HQ [38] and FFHQ [39], respectively. Additional examples are provided in the supplementary material (Sec. 11). 4.3. Attribute-controllable anonymization An advantage of our method is its ability to control highlevel facial attributes in generated images. To demonstrate this capability, we conduct experiments on controlling three types of basic demographic informationage, sex, and raceso that the generated anonymized faces match those of the original images. To assess key design choices, we conducted an ablation study with two alternative implementations. First, we replaced our DDPM [31] inversion with DDIM [75]. Second, we substituted the SDXL [65] model with InstantID [78], generation model designed for identity conditioning. Quantitative results are in Tab. 3. The DDIM inversion [75] underperforms across all metrics. This is primarily due to DDIMs inability to reconstruct the original image [33, 60], leading to degraded attribute preservation and image quality. Furthermore, DDIM [75] exhibits poor performance to large adjustments in the classifier-free guidance [30] scale, where excessive changes introduce artifacts and cause image failures. Consequently, this implementation suffers from higher re-identification rates due to its limited capacity to support aggressive anonymization. The InstantID-based implementation achieves lower reidentification rates than SDXL [65]. However, this results in reduced attribute preservation and higher FID [29]. This degradation is due to the ID embedding in InstantID, which entangles identity features with other facial attributes [78], making it difficult to preserve details unrelated to identity. Among existing methods, DP2 [34] is the only one that Qualitative comparisons in Fig. 9 further support these 7 Input Keep/Change age Input Keep/Change sex Input Keep/Change race (a) (b) (c) Figure 8. Qualitative comparison demonstrating the effect of attribute control on anonymized faces. Examples show the difference between preserving and altering age, sex, and race attributes. Re-ID (%) Attribute preservation Image quality SwinFace AdaFace Expression Gaze Pose FID Face IQA CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ CHQ FHQ Ours DDIM [75] InstantID [78] 2.622 36.889 0.489 4.800 39.533 0. 0.783 36.206 0.202 2.029 37.698 0.781 9.119 9.319 11.532 9.353 10.275 11.802 0.152 0.454 0.185 0.177 0.458 0. 0.050 0.154 0.057 0.052 0.171 0.054 4.809 47.194 30.334 8.651 38.873 18.347 0.856 0.708 0.889 0.921 0.733 0. Table 3. Ablation study comparing our method with alternatives using DDIM inversion [75] and InstantID [78] as the generation backbone. findings. The DDIM-based model fails to preserve facial attributes such as expression, gaze, and pose, and introduces artifacts. Meanwhile, the InstantID-based model tends to produce facial outputs appearing oversaturated and lacking realism. We also evaluated the attribute control of these alternative implementations, results in Tab. 4. Neither approach matches our SDXL-based implementation in preserving high-level facial attributes. Age Sex (%) Race (%) CHQ FHQ CHQ FHQ CHQ FHQ Ours* Ours DDIM [75] InstantID [78] * w/ attribute control 3.744 4.931 4.257 16.009 4.314 5.854 5.413 15.160 99.485 92.663 78.701 83.038 w/o attribute control 98.638 82.720 71.016 78.038 87.016 60.882 62.285 19. 76.548 49.479 55.889 28.517 Table 4. Attribute control accuracy (age, sex, race) for alternative implementations. Neither DDIM inversion [75] nor InstantID [78] achieves attribute consistency comparable to our SDXLbased method. Input Ours DDIM [75] InstantID [78] Figure 9. Qualitative results from the ablation study. Our method maintains both realism and accurate attribute preservation. 5. Conclusion We proposed reverse personalization framework for face anonymization that removes identity-specific features while preserving non-identity attributes. Our approach does not require the diffusion model to have prior exposure to the subject or any model fine-tuning, making it broadly applicable to arbitrary individuals. In addition to offering flexible anonymization, our method strikes an optimal balance among identity obfuscation, attribute preservation, and image quality."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by the EU Horizon project ELIAS - European Lighthouse of AI for Sustainability (No. 101120237) and the FIS project GUIDANCE (Debugging Computer Vision Models via Controlled Cross-modal Generation) (No. FIS2023-03251). We acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy), and thank the Finnish Foundation for Technology Promotion."
        },
        {
            "title": "References",
            "content": "[1] California Consumer Privacy Act (CCPA). https:// www.oag.ca.gov/privacy/ccpa/. 2 [2] General Data Protection Regulation (GDPR) Compliance Guidelines. https://gdpr.eu/. 2 [3] Ahmed Abdelrahman, Thorsten Hempel, Aly Khalifa, Ayoub Al-Hamadi, and Laslo Dinges. L2cs-net: Fine-grained gaze estimation in unconstrained environments. In 2023 8th International Conference on Frontiers of Signal Processing (ICFSP), pages 98102. IEEE, 2023. 6 [4] Claudia Acciai, Jesper Schneider, and Mathias Nielsen. Estimating social bias in data sharing behaviours: an open science experiment. Scientific Data, 10(1):233, 2023. 5 [5] Nouar AlDahoul, Talal Rahwan, and Yasir Zaki. Aigenerated faces influence gender stereotypes and racial homogenization. Scientific Reports, 15(1):14449, 2025. 2 [6] Simone Barattin, Christos Tzelepis, Ioannis Patras, and Nicu Sebe. Attribute-preserving face dataset anonymization via latent code optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80018010, 2023. 3, 6, 7, 1, 2, 4, [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [8] Fadi Boutros, Jonas Henry Grebe, Arjan Kuijper, and Naser Damer. Idiff-face: Synthetic-based face recognition through fizzy identity-conditioned diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1965019661, 2023. 1 [9] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 1, 4 [10] Irene Browne and Joya Misra. The intersection of gender and race in the labor market. Annual Review of Sociology, 29(1):487513, 2003. 5 [11] Zikui Cai, Zhongpai Gao, Benjamin Planche, Meng Zheng, Terrence Chen, Salman Asif, and Ziyan Wu. Disguise without disruption: Utility-preserving face de-identification. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 918926, 2024. 3 [12] Jingyi Cao, Xiangyi Chen, Bo Liu, Ming Ding, Rong Xie, Li Song, Zhu Li, and Wenjun Zhang. Face de-identification: arXiv State-of-the-art methods and comparative studies. preprint arXiv:2411.09863, 2024. 2, [13] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 2024. 6, 7, 1 [14] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems, 36:3028630305, 2023. 2 [15] Nicola DallAsen, Yiming Wang, Hao Tang, Luca Zanella, and Elisa Ricci. Graph-based generative face anonymisation with pose preservation. In International Conference on Image Analysis and Processing, pages 503515. Springer, 2022. 3, 6 [16] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF Conface recognition. ference on Computer Vision and Pattern Recognition, pages 46904699, 2019. 6 [17] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 00, 2019. 6 [18] Yingying Deng, Xiangyu He, Fan Tang, and Weiming Dong. Z-magic: Zero-shot multiple attributes guided image creator. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1839018400, 2025. [19] Frank Edwards, Hedwig Lee, and Michael Esposito. Risk of being killed by police use of force in the united states by age, raceethnicity, and sex. Proceedings of the National Academy of Sciences, 116(34):1679316798, 2019. 5 [20] Emilio Ferrara. Genai against humanity: Nefarious applications of generative artificial intelligence and large language models. Journal of Computational Social Science, 7(1):549 569, 2024. 2 [21] Amelia Fiske, Sarah Blacker, Lester Darryl Genevi`eve, Theresa Willem, Marie-Christine Fritzsche, Alena Buyx, Leo Anthony Celi, and Stuart McLennan. Weighing the benefits and risks of collecting race and ethnicity data in clinical settings for medical artificial intelligence. The Lancet Digital Health, 7(4):e286e294, 2025. 2, 5 [22] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 1, 2, 3, 4 [23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information Processing Systems, 27, 2014. 7 9 [24] Mislav Grgic, Kresimir Delac, and Sonja Grgic. Scface surveillance cameras face database. Multimedia Tools and Applications, 51:863879, 2011. [25] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Pulid: Pure and lightning id arXiv preprint Zhang, and Qian He. customization via contrastive alignment. arXiv:2404.16022, 2024. 1, 3 [26] Fabio Hellmann, Silvan Mertes, Mohamed Benouis, Alexander Hustinx, Tzung-Chien Hsieh, Cristina Conati, Peter Krawitz, and Elisabeth Andre. Ganonymization: ganbased face anonymization framework for preserving emoACM Transactions on Multimedia tional expressions. Computing, Communications and Applications, 21(1):127, 2024. 3 [27] Majed El Helou, Doruk Cetin, Petar Stamenkovic, and Fabio Zund. Vera: Versatile anonymization fit for clinical facial images. arXiv preprint arXiv:2312.02124, 2023. 3, 6 [28] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 1, 4 [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. 6, 7, 1 [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4, 5, 6, 7, [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 3, 7 [32] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation In Proceedings of the IEEE/CVF Conference and editing. on Computer Vision and Pattern Recognition, pages 6080 6090, 2023. 1, 2 [33] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12469 12478, 2024. 1, 2, 7 [34] Hakon Hukkelas and Frank Lindseth. Deepprivacy2: Towards realistic full-body anonymization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 13291338, 2023. 3, 6, 7, 1, 2, 4, 5, 8 [35] Hakon Hukkelas and Frank Lindseth. Does image In Proanonymization impact computer vision training? ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 140150, 2023. [36] Hakon Hukkelas, Rudolf Mester, and Frank Lindseth. Deepprivacy: generative adversarial network for face anonymization. In International Symposium on Visual Computing, pages 565578. Springer, 2019. 3, 2 [37] Jing Jiang and Weihong Deng. Disentangling identity and IEEE Transactions pose for facial expression recognition. on Affective Computing, 13(4):18681878, 2022. 3 [38] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 5, 6, 7, 1, 2, 3, 4 [39] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44014410, 2019. 5, 6, 7, 1, 2, 8 [40] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improvIn Proceedings of the ing the image quality of stylegan. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81108119, 2020. 3, [41] Furkan Kasım, Terrance Boult, Rensso Mora, Bernardo Biesseck, Rafael Ribeiro, Jan Schlueter, Tom aˇs Repak, Vareto, David Menotti, William Robson Schwartz, et al. Watchlist challenge: 3 rd open-set face detection and identification. In 2024 IEEE International Joint Conference on Biometrics (IJCB), pages 110. IEEE, 2024. 2 [42] Daeha Kim and Byung Cheol Song. Optimal transportbased identity matching for identity-invariant facial expression recognition. Advances in Neural Information Processing Systems, 35:1874918762, 2022. 3 [43] Minchul Kim, Anil Jain, and Xiaoming Liu. Adaface: In ProceedQuality adaptive margin for face recognition. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1875018759, 2022. 5, 6, 1 [44] Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. Dcface: Synthetic face generation with dual condition diffusion model. In Proceedings of the ieee/cvf Conference on Computer Vision and Pattern Recognition, pages 1271512725, 2023. 1 [45] Marvin Klemp, Kevin Rosch, Royden Wagner, Jannik Quehl, and Martin Lauer. Ldfa: Latent diffusion face anonymizaIn Proceedings of the tion for self-driving applications. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31993205, 2023. 1, 3, 6, 7, 2, 4, 5, 8 [46] Umut Kocasari, Alara Dirik, Mert Tiftikci, and Pinar Yanardag. Stylemc: Multi-channel based fast text-guided imIn Proceedings of the age generation and manipulation. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 895904, 2022. 7 [47] Zhenzhong Kuang, Xiaochen Yang, Yingjie Shen, Chao Hu, and Jun Yu. Facial identity anonymization via intrinIn Proceedings of sic and extrinsic attention distraction. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1240612415, 2024. [48] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 2 [49] Han-Wei Kung, Tuomas Varanka, Sanjay Saha, Terence Sim, and Nicu Sebe. Face anonymization made simple. In Proceedings of the Winter Conference on Applications of Com10 puter Vision (WACV), pages 10401050, 2025. 3, 6, 7, 1, 2, 4, 5, 8 Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1863018641, 2025. 3 [50] Han-Wei Kung, Tuomas Varanka, Terence Sim, and Nicu Sebe. Nullface: Training-free localized face anonymization. arXiv preprint arXiv:2503.08478, 2025. 6, 7, 1, 2, 3, 4, 5, 8 [51] Dongze Li, Wei Wang, Kang Zhao, Jing Dong, and Tieniu Tan. Riddle: Reversible and diversified de-identification with latent encryptor. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80938102, 2023. 1, 3, 6, 7, 2, [52] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. 1, 3 [53] Hai Liu, Tingting Liu, Zhaoli Zhang, Arun Kumar Sangaiah, Bing Yang, and Youfu Li. Arhpe: Asymmetric relationaware representation learning for head pose estimation in industrial humancomputer interaction. IEEE Transactions on Industrial Informatics, 18(10):71077117, 2022. 3 [54] Renshuai Liu, Bowen Ma, Wei Zhang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, and Xuan Cheng. Towards simultaneous and granular identity-expression control in personalized face generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21142123, 2024. 3 [55] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. 4 [56] Maxim Maximov, Ismail Elezi, and Laura Leal-Taixe. Ciagan: Conditional identity anonymization generative adverIn Proceedings of the IEEE/CVF Confersarial networks. ence on Computer Vision and Pattern Recognition, pages 54475456, 2020. 3, 6 [57] Blaˇz Meden, Peter Rot, Philipp Terhorst, Naser Damer, Arjan Kuijper, Walter Scheirer, Arun Ross, Peter Peer, and Vitomir ˇStruc. Privacyenhancing face biometrics: comprehensive survey. IEEE Transactions on Information Forensics and Security, 16:41474183, 2021. 2, [58] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 3 [59] Hiroyuki Mishima, Hisato Suzuki, Michiko Doi, Mutsuko Miyazaki, Satoshi Watanabe, Tadashi Matsumoto, Kanako Morifuji, Hiroyuki Moriuchi, Koh-ichiro Yoshiura, Tatsuro Kondoh, et al. Evaluation of face2gene using facial images of patients with congenital dysmorphic syndromes recruited in japan. Journal of Human Genetics, 64(8):789794, 2019. 2 [60] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 1, 3, 4, 7 [61] Jisu Nam, Soowon Son, Zhan Xu, Jing Shi, Difan Liu, Feng Liu, Seungryong Kim, and Yang Zhou. Visual persona: In Foundation model for full-body human customization. [62] Elaine Newton, Latanya Sweeney, and Bradley Malin. IEEE Preserving privacy by de-identifying face images. transactions on Knowledge and Data Engineering, 17(2): 232243, 2005. [63] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Jiankang Deng, Bernhard Kainz, and Stefanos Zafeiriou. Arc2face: foundation model for id-consistent human faces. In European Conference on Computer Vision, pages 241261. Springer, 2024. 1 [64] Katherine Phillips, Nancy Rothbard, and Tracy Dumas. To disclose or not to disclose? status distance and self-disclosure in diverse environments. Academy of Management Review, 34(4):710732, 2009. 2, 5 [65] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 6, 7, 2 [66] Lixiong Qin, Mei Wang, Chao Deng, Ke Wang, Xi Chen, Jiani Hu, and Weihong Deng. Swinface: multi-task transformer for face recognition, expression recognition, age estimation and attribute estimation. IEEE Transactions on Circuits and Systems for Video Technology, 34(4):22232234, 2023. 5, 6, 1 [67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 7 [68] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [69] Nataniel Ruiz, Eunji Chong, and James Rehg. Finegrained head pose estimation without keypoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 20742083, 2018. 6 [70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 2 [71] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65276536, 2024. 2 [72] Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. 1 11 image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 5, [86] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized textto-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67866795, 2024. 3 [87] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [73] Kaede Shiohara and Toshihiko Yamasaki. Face2diffusion for fast and editable face personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68506859, 2024. 3 [74] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan Essa, Michael Rubinstein, et al. Styledrop: Text-to-image synthesis of any style. Advances in Neural Information Processing Systems, 36:6686066889, 2023. 2 [75] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 3, 7, and Stefano Ermon. arXiv preprint [76] Rebecca Umbach, Nicola Henry, Gemma Faye Beard, and Colleen Berryessa. Non-consensual synthetic intimate imagery: Prevalence, attitudes, and knowledge in 10 countries. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 120, 2024. 2 [77] Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning text-toimage model on face. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. 2 [78] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 1, 2, 3, 7, 8 [79] Yaoming Wang, Yangzhou Jiang, Jin Li, Bingbing Ni, Wenrui Dai, Chenglin Li, Hongkai Xiong, and Teng Li. Contrastive regression for domain adaptation on gaze estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1937619385, 2022. 3 [80] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3 [81] Zhenyu Xie, Haoye Dong, Yufei Gao, Zehua Ma, and Xiaodan Liang. Dreamvton: Customizing 3d virtual try-on In Proceedings of the with personalized diffusion models. 32nd ACM International Conference on Multimedia, pages 1078410793, 2024. [82] Jianqing Xu, Shen Li, Jiaying Wu, Miao Xiong, Ailin Deng, Jiazhen Ji, Yuge Huang, Guodong Mu, Wenjie Id3: Identity-preserving-yetFeng, Shouhong Ding, et al. diversified diffusion models for synthetic face recognition. Advances in Neural Information Processing Systems, 37: 7777777798, 2024. 2, 3 [83] Haoxin Yang, Xuemiao Xu, Cheng Xu, Huaidong Zhang, Jing Qin, Yi Wang, Pheng-Ann Heng, and Shengfeng He. 2 face: High-fidelity reversible face anonymization via generative and geometric priors. IEEE Transactions on Information Forensics and Security, 2024. 2 [84] Yahan Yang, Junfeng Lyu, Ruixin Wang, Quan Wen, Lanqin Zhao, Wenben Chen, Shaowei Bi, Jie Meng, Keli Mao, Yu Xiao, et al. digital mask to safeguard patient privacy. Nature Medicine, 28(9):18831892, 2022. 2 [85] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-to-"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Balanced performance across privacy and 7. Privacy-utility trade-off across guidance utility scales In Fig. 10, we show five privacy-utility trade-off plots on CelebA-HQ [38] (CHQ) and FFHQ [39] (FHQ). The x-axis shows the re-identification rate, computed using AdaFace [43]. The y-axis shows utility, with lower values indicating better performance for expression, gaze, pose, and FID [29], and higher values for Face IQA [13]. Each method (NullFace [50], FAMS [49], FALCO [6], RiDDLE [51], LDFA [45], DP2 [34]) appears as point. gradient background highlights the lower-left (or upper-left for Face IQA [13]) corner as the optimal balance between privacy and utility. This visualization shows that our method consistently lies closest to the sweet spot, while others compromise either privacy or utility. Figure 11 shows the effect of negative classifier-free guidance [30] scales. Re-identification rates, computed with AdaFace [43] and SwinFace [66], are compared to utility measures: expression distance, gaze distance, pose distance, FID [29], and Face IQA [13]. Points represent different guidance scales (-20, -15, -10, -5). More negative scales reduce re-identification rates but increase attribute distances (expression, gaze, pose) and degrade image quality (higher FID [29], lower Face IQA [13]). This trade-off occurs because excessively negative guidance values impose the synthetic identity too strongly, altering non-identity attributes that should remain unchanged. Consistent with prior findings [72], excessive classifier-free guidance [30] leads to overall image degradation. (a) (c) (e) (b) (d) (f) Figure 10. Privacyutility trade-off plots. The gradient background highlights the optimal region (darker green) for balancing privacy and utility. 1 (a) (c) (e) (b) (d) (f) Figure 11. Privacyutility trade-off plots showing that increasingly negative guidance scales lower re-identification rates but also degrade image quality and distort non-identity attributes (expression, gaze, pose). 8. Challenging cases and limitations Input Anonymized Recovery test While prior methods struggle with extreme face angles or occlusions [36, 83], our approach performs reliably in these cases. The greater challenge occurs with extreme or uncommon expressions (see Fig. 12). In such cases, both our method and baselines struggle to preserve the original expression, likely due to limited training data for such expressions. Image quality is also bounded by the underlying diffusion model (SDXL [65] in our experiments). Using more advanced diffusion models could further improve quality. Our framework currently anonymizes single images. When applied to videos, it lacks temporal consistency. Extending the method to video anonymization using video diffusion models [7] is promising direction for future work. Input Ours NullFace [50] FAMS [49] RiDDLE [51] LDFA [45] DP2 [34] Figure 12. Examples where our method and baselines fail to preserve rare or extreme facial expressions. 9. Identity recovery test potential concern is whether the original identity can be recovered from an anonymized image by applying negative classifier-free guidance [30] scale to reverse anonymization. We argue that this is not feasible, as the outcome of our method depends on the interaction of multiple componentsincluding the model architecture, inversion process, and hyperparameter settingswhich prevent reversibility. We visualize recovery attempts in Fig. 13. The recovered images remain visibly different from the original inputs, aligning with the quantitative findings in Tab. 5, where re-identification rates remain low and comparable to anonymized images. These results demonstrate that the original identity cannot be restored through this approach. 10. Societal risks of AI-generated human faces AI-generated human faces pose societal risks. Now nearly indistinguishable from real ones, such faces enable convincing fake identities on social media, fostering manipulation, fraud, and disinformation [20]. These technologies also risk amplifying cultural, racial, and gender biases [5]. When trained on imbalanced datasets, face-generation models often reinforce societal prejudices, producing skewed and exFigure 13. Identity recovery attempts using negative classifier-free guidance [30] show that recovered images differ visually from the original inputs, demonstrating that the original identity cannot be restored. Re-ID (%) SwinFace AdaFace CelebA-HQ FFHQ CelebA-HQ FFHQ Anonymized Attempt to recover 2.556 2.284 4.724 4.117 0.768 0.566 2.028 1.430 Table 5. Identity recovery test by applying negative classifier-free guidance [30] to anonymized images. The low re-identification rates indicate that the original identity cannot be recovered through this method. clusionary representations. AI-generated faces are also used for sexual exploitation, such as creating non-consensual deepfake pornography [76] that targets and violates victims without consent. While AI-generated faces have promising applications, their misuse highlights the urgent need for coordinated action by policymakers and technologists to address these harms. 11. Additional qualitative results Qualitative comparisons between our method and six state-of-the-art approachesNullFace [50], FAMS [49], FALCO [6], RiDDLE [51], LDFA [45], and DP2 [34] are shown for CelebA-HQ [38] and FFHQ [39]. CelebAHQ [38] results are in Figs. 14 to 16, and FFHQ [39] results are in Figs. 17 to 19. Input Ours NullFace [50] FAMS [49] FALCO [6] LDFA [45] DP2 [34] Figure 14. Qualitative comparison of anonymization results on CelebA-HQ [38]. 3 Input Ours NullFace [50] FAMS [49] FALCO [6] LDFA [45] DP2 [34] Figure 15. Qualitative comparison of anonymization results on CelebA-HQ [38]. 4 Input Ours NullFace [50] FAMS [49] FALCO [6] LDFA [45] DP2 [34] Figure 16. Qualitative comparison of anonymization results on CelebA-HQ [38]. 5 Input Ours NullFace [50] FAMS [49] RiDDLE [51] LDFA [45] DP2 [34] Figure 17. Qualitative comparison of anonymization results on FFHQ [39]. 6 Input Ours NullFace [50] FAMS [49] RiDDLE [51] LDFA [45] DP2 [34] Figure 18. Qualitative comparison of anonymization results on FFHQ [39]. 7 Input Ours NullFace [50] FAMS [49] RiDDLE [51] LDFA [45] DP2 [34] Figure 19. Qualitative comparison of anonymization results on FFHQ [39]."
        }
    ],
    "affiliations": [
        "University of Oulu",
        "University of Trento"
    ]
}