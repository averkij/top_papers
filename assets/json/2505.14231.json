{
    "paper_title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning",
    "authors": [
        "Sule Bai",
        "Mingxing Li",
        "Yong Liu",
        "Jing Tang",
        "Haoji Zhang",
        "Lei Sun",
        "Xiangxiang Chu",
        "Yansong Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 1 3 2 4 1 . 5 0 5 2 : r UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning Sule Bai1,2,, Mingxing Li2, Yong Liu1, Jing Tang2, Haoji Zhang1, Lei Sun2, Xiangxiang Chu2, Yansong Tang1 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2AMAP, Alibaba Group {bsl23@mails.,tang.yansong@sz.}tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised finetuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed here."
        },
        {
            "title": "Introduction",
            "content": "Visual grounding is significant task that aims to recognize and localize target regions in images with the guidance of instructions. Conventional setting [65, 33] typically localizes objects based on predefined categories or explicit simple instructions (e.g., the blue shirt). It struggles to perform comprehension of implicit user instructions jointly with complex visual contexts. For example, handling nuanced queries like Which furniture in Image-2 can deal with the objects in Image-1? (as shown in Figure 1) requires advanced reasoning of user instructions across multiple images. Therefore, we focus on achieving universal visual grounding by unlocking broader spectrum of challenging scenarios in this work. To effectively tackle this universal and sophisticated visual grounding task, the ability to reason complex and implicit correspondence across diverse visual contexts is crucial. However, most previous works [65, 33, 50, 9] have focused on localizing targets within single-image scenarios * Work done during the internship at AMAP, Alibaba Group. Corresponding author Project lead Preprint. Under review. Figure 1: UniVG-R1 tackles wide range of visual grounding tasks with complex and implicit instructions. By combining GRPO training with cold-start initialization, it effectively reasons over instructions and visual inputs, significantly improving grounding performance. Our model achieves state-of-the-art results on MIG-Bench and exhibits superior zero-shot performance on four reasoning-guided grounding benchmarks with an average 23.4% improvement. with intuitive instructions, which demonstrates remarkable divergence from the requirements commonly observed in real-world applications. With the development of Large Language Models (LLMs), some works [19, 44, 4, 66] propose to leverage the powerful comprehension ability of LLMs to facilitate grounding task. Despite the great progress in understanding text instructions, these works are still limited to single image scenarios and fail to incorporate modeling of correlations across multiple images. Recently, Migician [22] introduces multi-image grounding benchmark encompassing diverse grounding tasks, thereby advancing foundational initiatives to bridge this research gap. However, Migician does not incorporate an explicit reasoning process during training, thereby falling short in terms of advanced reasoning capabilities, particularly in handling complex and implicit instructions across diverse images that are essential for universal visual grounding. Recognizing these limitations, we draw inspirations from the recent success of large reasoning models [14, 11, 42], such as DeepSeek-R1 [11], which employs rule-based reinforcement learning (RL) to significantly enhance large language model performance in solving challenging problems requiring in-depth reasoning. To this end, we explore the potential of the RL paradigm in this work and present UniVG-R1, powerful reasoning guided MLLM designed for universal grounding. Specifically, we initially conduct experiments using pure RL on recent advanced MLLMs (e.g., Qwen2-VL [45]), but find that it struggles to generate correct reasoning, leading to suboptimal performance. We ascribe this limitation to inherent constraints in the models intrinsic knowledge base when handling multi-image contexts, which critically hinders effective exploration of the reasoning space solely through RL. To address this limitation, we construct high-quality Chain-ofThought (CoT) [48] grounding dataset comprising 90k samples across diverse tasks, each annotated with reasoning chains and further validated by MLLMs to ensure correctness. Based on this dataset, we employ two-stage training protocol. The first stage utilizes cold-start supervised fine-tuning training, which directs the model towards correct reasoning pathways, then it is followed by Group Relative Policy Optimization (GRPO) training with an IoU-based verifiable reward functions, further incentivizing the models reasoning capabilities. Furthermore, we identify an inherent difficulty bias in the GRPO algorithm. Since GRPO computes the relative advantage within each group by normalization, it overlooks the varying difficulty among different samples. Consequently, easier samples receive policy gradient updates of magnitude similar to that of more challenging, lower-performing samples. This bias diminishes the training efficiency, especially as the proportion of easy samples increases during the RL training. To address 2 this issue, we propose simple online difficulty-aware weight adjustment strategy that dynamically scales the gradients of samples based on their difficulty, thereby encouraging more policy gradient updates from harder samples. We experiment with multiple difficulty metrics and empirically find that all variants consistently yield additional performance improvements. With the above designs modeling and consolidating reasoning abilities for diverse correspondence, our UniVG-R1 is capable of effectively addressing complex multimodal contexts, facilitating versatile and generalizable visual grounding applications in real-word scenarios. To demonstrate the effectiveness of our method, we conduct extensive evaluations on MIG-Bench [22], achieving state-of-the-art results with an average improvement of more than 9% on ten tasks. Furthermore, our model demonstrates superior generalizability, evidenced by significant zero-shot performance gains on group of benchmarks: +27.8% on LISA-Grounding [19], +15.9% on LLMSeg-Grounding [44], +20.3% on ReVOS-Grounding [54], and +25.3% on ReasonVOS [2]. In summary, we make the following contributions: (1) We propose UniVG-R1, reasoning guided MLLM for universal visual grounding, which employs GRPO training combined with cold-start initialization to effectively enhance reasoning capabilities across multimodal contexts. (2) high-quality CoT grounding dataset is introduced, encompassing diverse tasks, each meticulously annotated with detailed reasoning chains to facilitate advanced reasoning-based grounding. (3) We identify difficulty bias in GRPO training, and propose difficulty-aware weight adjustment strategy. Experiments validate that GRPO equipped with this strategy consistently enhance the model performance. (4) Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple grounding benchmarks, showcasing its versatility and generalizability."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Visual Grounding Visual grounding involves localizing visual element in an image based on specific linguistic query, which has broad applications across many tasks [57, 28, 27, 1, 61, 60, 68, 24, 46, 67]. RefCOCO/+/g [65, 33, 18, 36] is widely used benchmark for this task. Given an image and referring expression (e.g., the left apple), the model is required to identify the referred object. Early approaches [51, 50, 26, 53, 9, 21, 17, 41] leverage vision-language pre-trained models such as CLIP [37] to improve fine-grained understanding. With the rapid development of multimodal large language models (MLLMs) [25, 45, 20, 7], researchers have introduced more challenging datasets [19, 2, 44, 54], such as LISA-Grounding [19], which require models to comprehend complex instructions (e.g., find the food rich in vitamins in the image). series of works [47, 4, 23, 66, 63] have been proposed to address these tasks. Recently, Migician [22] introduces free-form multi-image grounding task, which requires models to perform multi-context understanding and grounding across ten different subtasks, including static difference, common object, and correspondence. However, existing methods lack advanced reasoning capabilities, resulting in suboptimal performance when dealing with complex multimodal contexts. In this work, we aim to enhance the models reasoning ability by introducing reasoning chains, thereby improving its performance in challenging scenarios. 2.2 Reasoning-Chain Guided Reinforcement Learning Reinforcement Learning (RL) has emerged as pivotal research direction for enhancing the complex reasoning capabilities of Large Language Models (LLMs) [11, 39, 14, 42, 6, 43, 55, 13, 16, 69, 62]. OpenAI-o1 [14] employs Reinforcement Learning from Human Feedback (RLHF) during fine-tuning, which significantly enhances the models reasoning ability and alignment with human preferences. The recent DeepSeek-R1 [11] employs GRPO [39], which, unlike traditional RL algorithms that rely on critic model, directly utilizes rule-based verifiable rewards to guide the models reasoning process. This approach significantly simplifies the training procedure and has proven highly effective in eliciting reasoning capabilities. Group policy gradient [6] (GPG) further simplifies the pipeline and performs better. This trend is gradually extending to MLLMs to further enhance their visual reasoning abilities [52, 29, 12, 34, 64, 56, 32, 5, 35, 8, 70, 10, 31]. Studies such as Visual-RFT [30] and VLM-R1 [40] show that, for single-image visual grounding tasks, directly applying GRPO with small number of samples can achieve improvements that surpass those of supervised fine-tuning. Vision-R1 [12] demonstrates the effectiveness of this approach in multimodal math benchmarks. In this work, we aim to extend this paradigm to the aforementioned universal grounding task. 3 Figure 2: We adopt two-stage training process. The first stage employs CoT-SFT, with the training data construction shown in (a). The second stage utilizes GRPO equipped with difficulty-aware weight adjustment strategy in (b). The GRPO training process is illustrated in (c), where the policy model generates multiple responses, and each is assigned distinct reward."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview In this section, we provide an overview of our proposed method, UniVG-R1. The task we address is practical and universal visual grounding problem, where the model is tasked with localizing objects based on implicit and complex instructions within multi-image context. Formally, given textual instruction , target image I, and several additional images , the model is expected to output bounding box B, defined as = M(T, I, ). Previous visual grounding methods typically rely on bounding box coordinate annotations or simple factual descriptions. After supervised fine-tuning, these models are restricted to such coordinates and lack explicit reasoning processes. However, the universal visual grounding task we address necessitates the model to comprehend complex instructions and additional visual inputs to perform localization. Motivated by the recent advancements in large reasoning models [14, 11, 42], we aim to introduce this paradigm into our approach. Our training process consists of two stages as shown in Figure 2. In the first stage, we construct high-quality dataset with Chain-of-Thought (CoT) annotations for supervised fine-tuning (SFT), enabling the model to learn structured reasoning trajectories. In the second stage, we employ rulebased reinforcement learning GRPO to guide the model in selecting correct reasoning chains, thereby further enhancing its reasoning capabilities. Additionally, we introduce difficulty-aware weight adjustment strategy to enhance the models performance during the GRPO training. 3.2 Cold Start Data Construction and Chain-of-Thought Supervised Fine-tuning Inspired by DeepSeek-R1-Zero [11], we initially explore the feasibility of training the model using pure reinforcement learning. However, experimental results in Section 4.3 show that under the same amount of data, the models performance is inferior to that achieved by supervised fine-tuning. We attribute this to the models limited grounding ability in multi-image scenarios, which makes it challenging to explore the reasoning space solely through reinforcement learning. Therefore, it is necessary to construct high-quality cold-start dataset in advance to guide the models learning and endow it with grounding-oriented cognitive capabilities. To this end, we randomly sample items from the MGrounding-630k dataset [22] and utilize the advanced multimodal large language model Qwen-VL-MAX [45] to generate chain-of-thought 4 reasoning processes. Specifically, as illustrated in Figure 2 (a), we provide the model with the question, bounding box coordinates, and predefined CoT prompt, prompting it to generate reasoning processes in the format: <think>thinking process here</think><answer>(x1, y1), (x2, y2)</answer>. For each item, we generate three reasoning chains and then use Qwen-VL-MAX to evaluate and select the best one as the final answer. Ultimately, we collect 76k samples and conduct further manual verification by randomly sampling 10% of the data for human evaluation, achieving final acceptance rate of 99.87%. Please refer to the supplementary materials for more details. This dataset is subsequently utilized for supervised fine-tuning on Qwen2-VL-7B, resulting in our stage-1 model. The trained model is capable of producing final bounding box predictions through coherent, step-by-step reasoning process. 3.3 Reinforcement Learning for Enhancing Reasoning Capability In the second stage, we employ rule-based reinforcement learning to enhance the models reasoning abilities. Specifically, we adopt the Group Relative Policy Optimization (GRPO) algorithm [39]. Unlike previous methods [38] that rely on an additional critic model, GRPO leverages direct verification function to assess the correctness of each answer. Given question q, the GRPO algorithm samples responses {o1, o2, . . . , oN } from the policy model πθold, and evaluates each response using rule-based verifiable reward function R(q, oi). For our task, we utilize two reward functions as described below: Accuracy Reward (racc): Given the ground truth bounding box coordinates BGT and the models predicted coordinates denoted as Bpred, we define the accuracy reward as IoU(Bpred, BGT ), where IoU denotes the Intersection over Union metric. This reward encourages the model to generate bounding boxes that closely match the ground truth. Format Reward (rf ormat): This reward ensures that the models response strictly adheres the model must output: <think>thinking process to the required format. here</think><answer>(x1, y1), (x2, y2)</answer>, and this reward returns value of 1 if the format is correct and 0 otherwise. The total reward for response oi is defined as ri = racc . To determine the relative quality of these responses, GRPO normalizes the rewards by computing their mean and standard deviation. The advantage for each response is then computed as: + rf ormat Specifically, Ai = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) . (1) where Ai represents the advantage of the candidate response oi relative to the other sampled responses within the group. GRPO encourages the model to generate responses with higher advantages by updating the policy πθ to maximize the following objective function: JGRP O(θ) = qP (Q),{oi}N i=1πθold (Oq) 1 N (cid:88) i=1 πθ(oiq) πθold(oiq) Ai βDKL(πθπref ) (2) DKL (πθπref ) = πref (oiq) πθ (oiq) log πref (oiq) πθ (oiq) 1 (3) where β is hyperparameter that controls the degree of the KL loss. During the second stage of training, we add the prompt First output the thinking process in <think> </think> tags and then output the bounding box in <answer> </answer> tags. to each question. The GRPO algorithm guides the model to select the correct reasoning chain from multiple sampled responses by assigning distinct advantages, thereby enhancing its reasoning capabilities, as shown in Figure 2 (c). 3.4 Difficulty-Aware Weight Adjustment Strategy During the stage 2 reinforcement learning process, we observe that most samples progressively become easier for the model, with the proportion of easy samples increasing and the proportion of 5 1 , racc 2 , . . . , racc hard samples steadily decreases. If we define mIoU = mean(racc ), where mIoU is the average accuracy reward of all responses for given sample. As shown in Figure 3, the proportion of easy samples (mIoU > 0.7) gradually increases, while the proportions of medium-difficulty samples (0.3 < mIoU < 0.7) and hard samples (mIoU < 0.3) both exhibit declining trend. Since the GRPO algorithm normalizes rewards to calculate the relative advantage within each group, easy samples (e.g., mIoU = 0.8) receives the same policy gradient update as hard samples (e.g., mIoU = 0.2). This leads to difficulty-bias issue. In particular, during the later stages of training, as easy samples become predominant, most updates are derived from these easier instances, making it difficult for the model to focus on hard samples. Figure 3: The proportion of easy, medium, and hard samples during GRPO training. To address this problem, we propose difficulty-aware weight adjustment strategy, which dynamically adjusts the weight of each sample based on its difficulty, as shown in Figure 2 (b). Specifically, we introduce difficulty coefficient ϕ mIoU to quantify the difficulty level of each sample, where the function ϕ is negatively correlated with mIoU. This coefficient dynamically adjusts the sample weights by computing the average accuracy reward of different responses for each sample. The detailed formula is provided below. JGRP O(θ) = qP (Q),{oi}G i=1πθold (Oq) (cid:34) 1 (cid:88) ϕ(mIoU ) i=1 πθ(oiq) πθold(oiq) (cid:35) Ai βDKL(πθπref ) (4) For the function ϕ, we explore various options. This strategy allows the model to pay more attention to difficult samples by assigning more weights to them during the GRPO training, thereby further enhancing its performance."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details Datasets. During training, there are two stages. In the first stage, we jointly utilize 76k CoT cold-start samples from the MGrounding-630k dataset (as mentioned in Section 3.2) and 14k samples from RefCOCO/+/g [65, 33]. In the second stage, we further mix 7k samples from MGrounding-630k with 3k samples from RefCOCO. For evaluation, we assess our model on the multi-image grounding benchmark MIG-Bench [22] and the RefCOCO/+/g dataset. Besides, the original MIG-bench dataset contains many incorrect annotations (see more details in the supplementary material), and we manually rectify them. The revised MIG-Bench will be released as well. Additionally, we evaluate the models zero-shot performance on several benchmarks, including LISA-Grounding [19], LLMSeg-Grounding [44], ReVOS Grounding [54], and ReasonVOS Grounding [2]. These datasets are originally designed for segmentation tasks, and we manually extract the corresponding bounding boxes. Among them, LISA and LLMSeg are single-image reasoning grounding tasks, while ReasonVOS and ReVOS are video reasoning grounding tasks. For videos, we uniformly sample 6 frames and require the model to perform grounding on one of these frames. Training Details. We conduct experiments on both Qwen2-VL-2B and Qwen2-VL-7B models. In the first stage, we use learning rate of 5e-6 and an accumulated total batch size of 24. In the second stage, the learning rate is set to 1e-6 with an accumulated total batch size of 16. The GRPO algorithm is configured with maximum completion length of 256 tokens and sampled 8 responses per input. Evaluation Metrics. We adopt the conventional Acc@0.5 metric for visual grounding tasks. This metric considers prediction correct if the Intersection over Union (IoU) with the ground truth exceeds 0.5. For all models, we utilize the official checkpoints and conduct evaluations under the same evaluation codes. Spontaneous Grounding Referential Grounding Models Difference Similarity Visual Reference Textual Visual+Textual AVG Static Robust Common OT MV Region Refer GG Reason Co-Re Qwen2-VL-72B [45] Mantis [15] LLaVA-OV-7B [20] Minicpm2.6 [58] mPLUG-Owl3 [59] InternVL2-8B [3] Qwen2-VL-7B [45] Migician [22] UniVG-R1 51.13 1.52 6.06 14.58 18.56 8.52 29.92 70. 71.97 43.61 0.00 3.19 2.13 6.38 19.15 36.17 45.74 58.51 73.74 3.31 3.43 14.34 34.93 38.40 43.07 72.76 93.13 24.54 12.18 0.18 9.82 8.55 19.82 14.55 67. 32.63 2.08 1.04 6.25 7.64 10.07 9.38 60.07 19.86 1.00 1.08 1.75 2.41 5.24 15.54 72.57 76.36 66.32 81.71 37.37 1.01 9.09 11.11 7.07 34.34 29.29 75. 82.83 67.83 10.02 15.43 10.02 22.85 39.79 63.51 84.12 88.04 50.51 0.00 6.93 2.97 9.09 26.80 44.33 52.58 62.89 17.94 0.85 0.85 2.56 5.98 7.69 16.24 33. 41.91 3.20 4.73 7.55 12.35 20.98 30.20 63.54 44.44 72.64 Table 1: Performance comparison on the revised MIG-Bench [22]. OT, MV, GG and Co-Re respectively means object tracking, multi-view grounding, group grounding and correspondence. Our UniVG-R1 achieves the best results across all tasks. The best results are shown in bold. Models Single image Multi images (video) LISA(val) [19] LISA(test) [19] LLMSeg [44] ReasonVOS [2] ReVOS [54] Qwen2-VL-7B [45] Migician [22] UniVG-R1 52.00 36.00 64.00 49.17 32.09 59.69 35.53 34.68 50. 9.83 33.41 58.73 23.55 39.70 60.03 AVG 34.02 35.18 58.61 Table 2: Zero-shot performance on several reasoning grounding benchmarks. 4.2 Main Results Performance on Migician. In Table 1, we present the performance comparison of our UniVGR1 with Qwen2-VL [45], Mantis [15], LLaVA-OV [20], MiniCPM2.6 [58], mPLUG-Owl3 [59], InternVL2 [3] and Migician [22] on the MIG-Bench. Our approach achieves new state-of-the-art results across all 10 subtasks, surpassing the previous leading model, Migician, by significant margin of 9.1%. Regarding the dataset size, Migician utilizes total of 1.2 million samples, including 630k from the multi-image grounding dataset, 130k from the RefCOCO dataset, and additional multimodal instruction-following data. In contrast, we only use curated dataset of 90k CoT samples for stage 1 and 10k for stage 2, totaling 100k samplesapproximately 8.3% of Migicians dataset size. Furthermore, our model significantly outperforms Qwen2-VL-72B by 75.12%, despite having much smaller parameter size. Zero-shot performance on reasoning grounding benchmarks. Moreover, Table 2 highlights our models robust zero-shot capabilities. UniVG-R1 consistently achieves superior results across all evaluated reasoning-guided grounding benchmarks, averaging 58.61% performance on both image and video tasks. While Migician demonstrates stronger performance than Qwen2-VL on video datasets, it lags behind on single-image datasets, particularly Lisa-Grounding. Overall, our model consistently delivers outstanding results on tasks requiring reasoning-chain guidance, excelling in both single-image and multi-image scenarios. Performance on RefCOCO. We also evaluate our model on the RefCOCO dataset, as shown in Table 3. We compare our UniVG-R1 with VisionLLM v2 [49], Shikra [4], InternVL2-8B [3], GroundingGPT [23], Griffon v2 [66], GroundingDINO-L [26], Qwen2-VL-7B [45], and Migician [22]. Our model achieves the best average performance of 88.20%. Notably, we outperform other models on RefCOCOg, which contains more complex reference instructions. This further validates our models capability to comprehend intricate instructions. Models RefCOCO RefCOCO+ RefCOCOg val testA testB val testA testB val VisionLLM v2 [49] Shikra [4] InternVL2-8B [3] GroundingGPT [23] Griffon v2 [66] GroundingDINO-L [26] Qwen2-VL-7B [45] Migician [22] 79.20 87.00 87.10 88.02 89.6 90.60 91.70 91.62 82.30 90.60 91.10 91.55 91.80 93.20 93.60 93.49 77.00 80.20 80.70 82.47 86.50 88.20 87.30 87.22 68.90 81.60 79.80 81.61 81.90 82.80 85.80 86. 75.80 87.40 87.90 87.18 85.50 89.00 90.50 91.06 61.80 72.10 71.40 73.18 76.20 75.90 79.50 79.93 73.30 82.30 82.70 81.67 85.00 86.10 87.30 88.06 test 74.80 82.20 82.70 81.99 86.00 87.00 87.80 87.80 AVG 74.14 82.97 82.94 83.57 85.30 86.60 87.96 88.16 UniVG-R1 91.64 93.11 87.16 85. 90.53 80.04 88.67 88.56 88.20 Table 3: The performance on Refcoco/+/g. 4.3 Ablation Study Training Stages. Inspired by DeepSeek-R1-Zero, we initially investigate the feasibility of training the model purely through reinforcement learning. As shown in Table 4, when training on 21k data 7 No. Methods Data Size Qwen2-VL-7B / 29.92 36.17 43.07 14.55 Static Robust Common OT Spontaneous Grounding Referential Grounding Difference Similarity Visual Reference Textual Visual+Textual AVG 1 2 3 4 6 7 8 Pure RL CoT-SFT SFT CoT-SFT CoT-SFT GRPO GRPO-Difficulty 21k 21k 90k 90k 10k 10k 10k 46.02 57.58 73.48 68.75 70.64 71.59 71.97 59.47 48.94 45.74 48. 52.13 53.19 58.51 88.59 90.18 89.69 90.55 89.69 93.01 93.13 MV 9. 48.96 58.68 62.85 61.46 stage 1 57.09 68.91 71.27 74.55 stage 75.45 77.09 76.36 60.42 64.24 66.32 Region Refer GG Reason Co-Re 15. 29.29 63.51 44.33 16.24 30.20 26.77 52. 86.62 80.88 76.64 80.96 81.71 78.79 80.81 77.78 78.79 78.79 81.82 82.83 84.95 84. 84.74 83.30 83.30 86.19 88.04 54.64 64.95 49.48 60.82 58.76 55.67 62.89 29.06 37. 31.62 41.89 57.43 64.50 67.30 69.00 41.88 42.74 44.44 68.77 70.65 72.64 Table 4: Ablation study of different stages. We finally adopt CoT-SFT in stage 1, and GRPO equipped with difficulty-aware weight adjustment strategy in stage 2. Methods Difficulty Function Spontaneous Grounding Referential Grounding Difference Similarity Visual Reference Textual Visual+Textual AVG Static Robust Common OT MV Region Refer GRPO / GRPO-Difficulty GRPO-Difficulty GRPO-Difficulty log(mIoU) (1.0 mIoU)2 exp(1mIoU) 71.59 72.92 72.73 71.97 53.19 54.26 53.19 58.51 93.01 92.64 93.12 93. 77.09 64.24 76.91 76.36 76.36 64.93 67.71 66.32 80.96 81.55 81.55 81. 81.82 83.84 81.82 82.83 Table 5: Ablation study of funtion ϕ. GG 86.19 87.01 85.57 88. Reason Co-Re 55.67 59.79 60.82 62.89 42.74 41.88 42.74 44.44 70. 71.57 71.56 72.64 samples, Pure RL (No. 2) underperforms CoT-SFT (No. 3) by 7.07% in average score. We attribute this discrepancy to the models inherent limitations in addressing grounding tasks within multi-image contexts, which makes exploring the reasoning space solely via reinforcement learning particularly challenging. Therefore, we adopt two-stage training approach. Stage 1: In this stage, we first examine the effect of data scaling. Increasing the CoT-SFT training dataset from 21k samples (No. 3) to 90k samples (No. 5) improves the average performance by 4.5%. We also compare standard SFT trained solely with coordinate annotations (No. 4) with CoT-SFT (No. 5), with both models trained on 90k samples. CoT-SFT achieves higher average performance (69.00%) compared to SFT (67.30%). This advantage is particularly evident in the Reason and Co-Re\" subtasks, which require strong reasoning abilities. Specifically, CoT-SFT surpasses SFT by 11.34% in Reason and 10.27% in Co-Re. This validates that the reasoning-guided approach enhances the models reasoning capabilities. After stage 1, CoT-SFT training endows the model with reasoning cognitive abilities. Stage 2: For Stage 2, all methods are fine-tuned on an additional 10k data samples based on the Stage 1 CoT-SFT model. We compare the performance of continued training with CoT-SFT (No. 6) against employing the GRPO algorithm (No. 7). GRPO improves the average performance by 1.88% over CoT-SFT. This gain is attributed to GRPOs mechanism of generating multiple responses and assigning different rewards, which guides the model to select the correct reasoning path and thus enhances its reasoning ability. Finally, we compare the standard GRPO algorithm (No. 7) with GRPO eqiupped with our difficulty-aware weight adjustment strategy, referred to as GRPO-Difficulty (No. 8). We observe that this strategy further yields approximately 2.0% improvement over the standard GRPO, demonstrating the effectiveness of the proposed method. Different difficulty functions. Regarding the difficulty-aware weight adjustment strategy proposed in Section 3.4, we investigate various formulations of the function ϕ to modulate sample difficulty. Specifically, we experiment with three distinct functions: log(mIoU), (1.0 mIoU)2, and exp(1mIoU). As shown in Table 5, among these, exp(1mIoU) yields the highest average performance of 72.62%. Therefore, we adopt this setting as the default in this work. Model size. We also investigate the impact of different model sizes in Table 6, presenting the performance of Qwen2-VL-2B. Although the 2B model ultimately underperforms compared to the 7B model, GRPO training significantly boosts its performance. We attribute this to the fact that the smaller 2B model may not fully develop its logical reasoning abilities after stage 1 training. As result, the GRPO algorithm, by guiding the model to select correct reasoning chains, brings about 8 Spontaneous Grounding Referential Grounding Methods Difference Similarity Visual Reference Textual Visual+Textual AVG Static Robust Common OT MV Region Refer GG Reason Co-Re Qwen2-VL-2B ①COT-SFT (90k) ②GRPO (10k) ③GRPO-Difficulty (10k) 15. 29.36 47.92 50.57 17.02 31.91 46.81 43.62 27. 62.09 85.64 88.34 2B model 13.45 7.29 34. 23.96 57.09 57.09 44.10 50.69 7.32 38.40 59.43 60. 19.19 65.66 75.76 72.73 57.53 74.23 81.03 82. 6.19 26.80 42.23 44.33 14.53 23.93 25.64 29. 18.58 41.09 56.57 57.96 Table 6: Performance on 2B model. Figure 4: Qualitative comparison of reasoning-guided grounding among Qwen2-VL-7B, Magician, and our UniVG-R1. Left: MIG-Bench. Right: LISA-Grounding. more substantial performance improvement for the smaller model. Our difficulty-aware weight adjustment strategy further amplifies this gain."
        },
        {
            "title": "5 Visualization",
            "content": "In Figure 4, we present qualitative comparison of our method with Qwen2-VL-7B and Migician. It is evident that UniVG-R1 effectively understands multi-context information across multiple images, as well as implicit instructions (e.g., identifying objects with similar functionality in the left image) and complex instructions (e.g., determining what can sweep away rain in the right image). Compared to other methods, UniVG-R1 provides more accurate results with explanations, demonstrating that our reasoning-guided approach enables the model to better comprehend and execute complex instructions."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose UniVG-R1, reasoning-guided MLLM designed for universal visual grounding tasks. UniVG-R1 effectively handles complex textual instructions across diverse multimodal contexts. To achieve this, we introduce two-stage training framework: (1) cold-start supervised fine-tuning stage leveraging high-quality CoT dataset to guide the model in learning structured reasoning trajectories, and (2) reinforcement learning stage using the GRPO algorithm to further enhance the models reasoning capabilities. Furthermore, we propose difficulty-aware weight adjustment strategy to address the difficulty bias in GRPO training, dynamically prioritizing harder samples to improve overall performance. Extensive experiments validate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on the multi-image grounding benchmark MIG-Bench with 9.1% improvement. Moreover, UniVG-R1 demonstrates strong generalization ability, attaining substantial zero-shot performance gains across multiple reasoning-guided grounding benchmarks. These results highlight the versatility and robustness of our UniVG-R1 in tackling complex, reasoning-guided multimodal grounding tasks."
        },
        {
            "title": "References",
            "content": "[1] Sule Bai, Yong Liu, Yifei Han, Haoji Zhang, and Yansong Tang. Self-calibrated clip for training-free open-vocabulary segmentation. arXiv preprint arXiv:2411.15869, 2024. [2] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. In NeurIPS, pages 68336859, 2024. [3] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. [4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [5] Rui Chen, Lei Sun, Jing Tang, Geng Li, and Xiangxiang Chu. Finger: Content aware finegrained evaluation with reasoning for ai-generated videos. arXiv preprint arXiv:2504.10358, 2025. [6] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. [7] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. [8] Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, and Yu Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv preprint arXiv:2503.07065, 2025. [9] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. In ICCV, pages 17691779, 2021. [10] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [13] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [14] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [15] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. [16] Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy Chen, Shafiq Joty, and Furu Wei. Preference optimization for reasoning with pseudo feedback. arXiv preprint arXiv:2411.16345, 2024. 10 [17] Lei Jin, Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Annan Shu, and Rongrong Ji. Refclip: universal teacher for weakly supervised referring expression comprehension. In CVPR, pages 26812690, 2023. [18] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, pages 787798, 2014. [19] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, pages 95799589, 2024. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [21] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR, pages 1096510975, 2022. [22] You Li, Heyu Huang, Chi Chen, Kaiyu Huang, Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, et al. Migician: Revealing the magic of free-form multi-image grounding in multimodal large language models. arXiv preprint arXiv:2501.05767, 2025. [23] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, YiQing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Vu Tu, et al. Groundinggpt: Language enhanced multi-modal grounding model. In ACL, pages 66576678, 2024. [24] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. arXiv preprint arXiv:2408.00754, 2024. [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, pages 3489234916, 2023. [26] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, pages 3855, 2024. [27] Yong Liu, Sule Bai, Guanbin Li, Yitong Wang, and Yansong Tang. Open-vocabulary segmentation with semantic-assisted calibration. In CVPR, pages 34913500, 2024. [28] Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, and Yansong Tang. Universal segmentation at arbitrary granularity with language instruction. In CVPR, pages 34593469, 2024. [29] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Segzero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. [30] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [31] Guanxing Lu, Ziwei Wang, Changliu Liu, Jiwen Lu, and Yansong Tang. Thinkbot: Embodied instruction following with thought chain reasoning. In ICLR, 2025. [32] Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek Wong, Xiaoyi Feng, and Maosong Sun. Deepperception: Advancing r1-like cognitive visual perception in mllms for knowledge-intensive visual grounding. arXiv preprint arXiv:2503.12797, 2025. [33] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 1120, 2016. [34] Meng, Du, Liu, Zhou, Lu, Fu, Shi, Wang, He, Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [35] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [36] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: survey of methods and datasets. TMM, pages 44264440, 2020. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [38] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [40] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [41] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: strong zero-shot baseline for referring expression comprehension. In ACL, pages 51985215, 2022. [42] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [43] Luong Quoc Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In ACL, pages 76017614, 2024. [44] Junchi Wang and Lei Ke. Llm-seg: Bridging image segmentation and large language model reasoning. In CVPR, pages 17651774, 2024. [45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [46] Yiqin Wang, Haoji Zhang, Jingqi Tian, and Yansong Tang. Ponder & press: Advancing visual gui agent towards general computer control. arXiv preprint arXiv:2412.01268, 2024. [47] Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, and Xiangxiang Chu. Lenna: Language enhanced reasoning detection assistant. In ICASSP, pages 15, 2025. [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, pages 2482424837, 2022. [49] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Zhe Chen, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. In NeurIPS, pages 6992569975, 2024. [50] Linhui Xiao, Xiaoshan Yang, Fang Peng, Yaowei Wang, and Changsheng Xu. Hivg: Hierarchical multimodal fine-grained modulation for visual grounding. In ACM MM, pages 54605469, 2024. [51] Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, and Changsheng Xu. Clipvg: Self-paced curriculum adapting of clip for visual grounding. TMM, pages 43344347, 2023. [52] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [53] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In CVPR, pages 1532515336, 2023. [54] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In ECCV, pages 98115, 2024. [55] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [56] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [57] Zhao Yang, Jiaqi Wang, Xubing Ye, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Language-aware vision transformer for referring segmentation. TPAMI, 2024. [58] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [59] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. [60] Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, and Yansong Tang. Atp-llava: Adaptive token pruning for large vision language models. arXiv preprint arXiv:2412.00447, 2024. [61] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, and Yansong Tang. Voco-llama: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. [62] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. [63] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. [64] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [65] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, pages 6985, 2016. [66] Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring. arXiv preprint arXiv:2403.09333, 2024. [67] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024. 13 [68] Shiyi Zhang, Sule Bai, Guangyi Chen, Lei Chen, Jiwen Lu, Junle Wang, and Yansong Tang. Narrative action evaluation with prompt-guided multimodal interaction. In CVPR, pages 18430 18439, 2024. [69] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. [70] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University"
    ]
}