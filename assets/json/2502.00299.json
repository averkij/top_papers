{
    "paper_title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
    "authors": [
        "Xiang Liu",
        "Zhenheng Tang",
        "Peijie Dong",
        "Zeyu Li",
        "Bo Li",
        "Xuming Hu",
        "Xiaowen Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods."
        },
        {
            "title": "Start",
            "content": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Xiang Liu 1 Zhenheng Tang 2 Peijie Dong 1 Zeyu Li 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1 5 2 0 2 1 ] . [ 1 9 9 2 0 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in chunk as basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and NeedleIn-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10% performance improvement under aggressive compression ratios compared to existing methods. 1. Introduction Large Language Models (LLMs) have become essential for addressing various downstream tasks of natural language processing (NLP), including summarization and question answering, which require the interpretation of long context from sources such as books, reports, and documents, often encompassing tens of thousands of tokens (Brown et al., 2020; Tay et al., 2022; Touvron et al., 2023). Recent advances in long-context technology within the field of 1The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China 2The Hong Kong University of Science and Technology, Hong Kong, China. Correspondence to: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu <xwchu@hkust-gz.edu.cn>. 1 machine learning (ML) systems (Dao, 2024; Jacobs et al., 2023; Xiao et al., 2024) have significantly enhanced computational throughputs and reduced latency of LLMs to process increasingly large input context lengths (Liu et al., 2024b; Young et al., 2024) with saving historical KV cache (key value attentions). However, the memory requirement of the KV cache in serving super-long contexts becomes new bottlneck (Zhang et al., 2023; Reid et al., 2024). For instance, the KV cache for single token in 7B-parameter model requires approximately 0.5 MB of GPU memory, resulting in 10,000-token prompt consuming around 5 GB of GPU memory. To address the substantial GPU memory consumption caused by KV caching, recent studies consider compressing the KV cache by pruning non-important discrete parts from the prompt tokens (Zhang et al., 2023; Li et al., 2024; Ge et al., 2023; Cai et al., 2024; Fu et al., 2024a; Yang et al., 2024b; Liu et al., 2024e; Tang et al., 2024). H2O (Zhang et al., 2023) and SnapKV (Li et al., 2024) have shown that retaining less than 50% of the discrete KV cache can significantly reduce GPU memory usage with minimal impact on performance. However, we identify that the previous KV cache compression methods (Zhang et al., 2023; Cai et al., 2024) measure token importance isolatedly, neglecting the dependency between different tokens in the real-world language characterics. For example, as shown in Figure 1, focusing on token-level importance might excessively focus on words about subjects turaco in the question while omitting crucial information about the objects (foods) in the documents, resulting the loss of essential semantic information. This motivates us to rethink the following question: How to avoid isolated token importance measurement and preserve the semantic information in KV cache? In light of this, we observe that the complete semantic information usually appear in continuous sequence (Fang & Xie, 2022). Thus, we introduce straightforward yet effective ChunkKV, grouping the tokens in chunk as basic compressing unit, which should be preserved or discarded as whole. Thus, it retains the most informative semantic chunks from the original KV cache. As shown in Figure 1, preserving chunk helps to catch the subject, predicate, and object. Furthermore, we investigate that the preserved ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 1: Illustration of the impact of the token discrete method and the chunk method on semantic preservation. The discrete method preserves words related to the question but often omits the subject. In contrast, the chunk method retains the subject of the words, maintaining more accurate semantic information. For the equation: is the score function, and is chunk of tokens. Table 1: Comparison of Methods on KV Cache Compression. Method KV Cache Compression Dynamic Policy Layer-Wise Policy Semantic Information Efficient Index Reuse StreamingLLM (Xiao et al., 2024) H2O (Zhang et al., 2023) SnapKV (Li et al., 2024) PyramidInfer (Yang et al., 2024b) PyramidKV (Cai et al., 2024) ChunkKV(Ours) KV cache indices by ChunkKV exhibit higher similarity compared to previous methods. Consequently, we develop technique called layer-wise index reuse, which reduces the additional computational time introduced by the KV cache compression method. As outlined in Table 1, recent highly relevant KV cache compression methods lack the ability to retain semantic information and efficiently reuse indices. To evaluate ChunkKVs performance, we conduct comprehensive experiments across multiple cutting-edge longcontext benchmarks: long-context tasks including LongBench (Bai et al., 2024) and Needle-In-A-HayStack (NIAH) (Kamradt, 2023), in-context learning tasks such as GSM8K (Cobbe et al., 2021) and JailbreakV (Luo et al., 2024). And also different models including DeepSeekR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8BInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al., 2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our experimental results demonstrate that ChunkKV surpasses existing KV cache compression methods in both efficiency and accuracy, primarily due to its ability to preserve essential information through selective chunk retention. These findings establish ChunkKV as simple yet effective approach to KV cache compression. We summarize our key contributions as follows: We identify the phenomenon in which discrete KV cache compression methods inadvertently prune the necessary semantic information. We propose ChunkKV, simple KV cache compression method that uses the fragmentation method that keeps the semantic information, and propose the layer-wise index reuse technique to reduce the additional computational time. We evaluate ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-AHayStack, as well as the GSM8K, many-shot GSM8K and JailbreakV in-context learning benchmark, and multi-step ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference reasoning (O1 and R1) LLMs, achieving state-of-the-art performance. 2. Related Work KV Cache Compression. KV cache compression technology has developed rapidly in the era of LLM, with methods mainly focused on evicting unimportant tokens. The compression process occurs before the attention blocks, optimizing both the prefilling time and GPU memory. Xiao et al. (2024) and Han et al. (2024) propose that initial and recent tokens consistently have high attention scores between different layers and attention heads. As result, retaining these tokens in the KV cache is more likely to preserve important information. Furthermore, FastGen (Ge et al., 2023) evicts tokens based on observed patterns. H2O (Zhang et al., 2023) and SnapKV (Li et al., 2024) employ dynamic KV cache compression methods, evaluating the importance of tokens based on attention scores and then evicting the less important ones. As inference scenarios become increasingly complex, dynamic KV cache compression methods demonstrate powerful performance. Recently, Yang et al. (2024b) and Cai et al. (2024) have closely examined the distributions of attention scores during the pre-filling stage of the Retrieval-Augmented Generation (RAG) task, discovering pyramidal KV cache compression pattern in different transformer layers. Although these KV cache compression methods have explored efficient GPU memory management while maintaining original performance, our study focuses more on the semantic information of the prompt. We find that chunks of the original KV cache are more important than discrete tokens. Chunking Method. The chunking methodology is widely used in the field of NLP due to its simplicity and effectiveness (Tjong Kim Sang & Veenstra, 1999). In the era of LLMs, chunking is primarily applied in data pre-processing. suggest grouping related trainFor example, Shi et al. ing data into chunks to achieve better convergence curves to pre-train LLMs. Fei et al. (2024) apply topic-based chunking method to improve the semantic compression of prompts. Furthermore, chunking plays an important role in the Retrieval-Augmented Generation (RAG) field (Yepes et al., 2024; Smith & Troynikov, 2024; Anthropic, 2024). It serves to divide documents into units of information with semantic content suitable for embedding-based retrieval and processing by LLMs. Layer-Wise Technique The layer-wise technique is widely used in the training and inference of large language models (LLMs). LISA (Pan et al., 2024a) is layer-wise sampling method based on observations of the training dynamics of Low-Rank Adaptation (LoRA)(Hu et al., 2022) across layers. LAMB(You et al., 2020) is layer-wise adaptive learning rate method that speeds up LLM training by stabilizing training convergence with large batch sizes. DoLa (Chuang et al., 2023) employs layer-wise contrasting to reduce hallucinations during LLM inference. 3. ChunkKV 3.1. Preliminary Study of KV Cache Compression With the increasing long-context capabilities of LLMs, the KV cache has become crucial for improving inference efficiency. However, it can consume significant GPU memory when handling long input contexts. The GPU memory cost of the KV cache for the decoding stage can be calculated as follows: MKV = 2 (1) where is the batch size, is the sequence length of prompt and decoded length, is the number of layers, is the number of attention heads, is the dimension of each attention head, and the first 2 accounts for the KV matrices, while the last 2 accounts for the precision when using float16. Table shows the configuration parameters for LLaMA3-8B-Instruct (Meta, 2024). With batch size = 1 and sequence length of prompt = 2048, the GPU memory cost of the KV cache is nearly 1 GB. If the batch size exceeds 24, the GPU memory cost of the KV cache will exceed the capacity of an RTX 4090 GPU. To address this issue, KV cache compression methods have been proposed, with the aim of retaining only minimal amount of KV cache while preserving as much information as possible. For more details on the LLM configuration parameters, refer to Appendix E. 3.2. Chunk Based KV Compression To address the limitations of existing KV cache compression methods, we propose ChunkKV, novel KV cache compression method that retains the most informative semantic chunks. The key idea behind ChunkKV is to group tokens in the KV cache into chunks that preserve more semantic information, such as chunk containing subject, verb and object. As illustrated in Figure 1, ChunkKV preserves the chunks of the KV cache that contain more semantic information. First, we define chunk as group of tokens that contain related semantic information. By retaining the most informative chunks from the original KV cache, ChunkKV can effectively reduce the memory usage of the KV cache while preserving essential information. The Algorithm 1 shows the pseudocode outline of ChunkKV. First, following H2O (Zhang et al., 2023) and SnapKV (Li et al., 2024), we set the observe window by computing the observation scores QTqw:Tq KT , where QTqw:Tq ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Algorithm 1 ChunkKV Algorithm 2 Layer-wise Index Reuse for ChunkKV Input: RTqd, RTkd, RTvd, observe window size w, chunk size c, compressed KV cache max length Lmax Output: Compressed KV cache K, Observe Window Calculation: QTqw:Tq KT {Attention scores for the observe window} (cid:6) Tk Chunk Attention Score Calculation: for = 1 to do Ai (cid:80)ic for each chunk} j=(i1)c+1 A:,j {Sum of observation scores (cid:7) {Calculate the number of chunks} (cid:5) end for Top-K Chunk Selection: (cid:4) Lmax Top_K_Indices indices of Top-k chunks based on Ai Compression: K, index_select(K, V, Top_K_Indices) Concatenation: concat(K concat(V K, 0:Lmaxw, KTkw:Tk ) 0:Lmaxw, VTvw:Tv ) is the observe window, is the Key matrix and the window size is usually set to {4, 8, 16, 32}. Next, the number of chunks is calculated as = (cid:6) Tk (cid:7), where Tk is the length of the Key matrix and is the chunk size. The observation scores for each chunk are then computed as Ai = (cid:80)ic j=(i1)c+1 A:,j for = 1, 2, . . . , C. Referring to previous works (Zhang et al., 2023; Li et al., 2024; Yang et al., 2024b; Cai et al., 2024), we still use the top-k algorithm as ChunkKVs sampling policy. For the top-k chunk selection, the top-k chunks are selected based on their observation scores, where = (cid:4) Lmax (cid:5), and Lmax is the maximum length of the compressed KV cache. The size of the last chunk will equal min(c, Lmax (k 1) c). The indices of the top-k chunks will keep the original sequence order. In the compression step, the key and value matrices are only retained based on the selected indices, resulting in the compressed KV cache. Finally, the observe window of the original KV cache will be concatenated to the compressed KV cache by replacing the last tokens to keep important information. The compressed KV cache is then used for subsequent attention computations. 3.3. Layer-Wise Index Reuse Furthermore, we investigated the preserved KV cache indices by ChunkKV and found that they exhibit higher similarity compared to previous methods. Figure 2 shows the layer-wise similarity heatmaps of SnapKV and ChunkKV. Each cell represents the similarity between the preserved 4 Input: Number of layers in LLMs Nlayers, number of reuse layers Nreuse Initialize: Dictionary to store indices Ireuse = {} for = 0 to (Nlayers 1) do if mod Nreuse == 0 then l, Il ChunkKV(Kl, Vl) l, K Ireuse[l] Il else Il Ireuse[ (cid:106) Nreuse (cid:107) Nreuse] end if V end for index_select(Kl, Il) index_select(Vl, Il) KV cache indices of two layers, with deeper colors indicating higher similarity. The results demonstrate that the KV cache indices preserved by ChunkKV are more similar to those in neighboring layers. As shown in Table 2, ChunkKV consistently achieves higher average Jaccard similarity between adjacent layers compared to SnapKV in different model architectures, indicating that the retained token index in ChunkKV is more similar to each other. For more detailed visualization, please refer to Appendix B.1.2. Table 2: Retained KV Cache Indices Similarity of Adjacent Layers for Different Models. Method H2O SnapKV ChunkKV LLaMA-3-8B 25.31% 27.95% 14.91% 16.50% Qwen2-7B 15.15% 15.78% Mistral-7B 57.74% 44.26% 52.16% Based on the above findings, we propose training-free layer-wise index reuse method to further reduce the additional cost of the KV cache compression time, which reuses compressed token indices across multiple layers. This procedure is formally described in Algorithm 2. The ChunkKV compression process returns the compressed KV cache and their respective token indices, denoted as Il. For layer-wise index reuse, we define grouping of layers such that all Nreuse layers share the same token indices for ChunkKV. Specifically, for group of layers {l, + 1, . . . , + Nreuse 1}, we perform ChunkKV on the first layer to obtain the token indices Il and reuse Il for the subsequent layers l+1, l+2, . . . , l+Nreuse1. The notation Kl[Il] and Vl[Il] indicates the selection of key and value caches based on the indices in Il. The efficiency analysis for layer-wise index reuse is provided in Appendix B.1.1. Theoretical Understanding. We provide theoretical understanding from the in-context learning (ICL) (Fang & Xie, 2022) to interpret why maintaining KV cache according to ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 2: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV (left) and ChunkKV (right) on LLaMA-3-8B-Instruct. continuous sequence in ChunkKV is better than according to sparse tokens. Informally speaking, the continuously chunklevel KV cache preserves the whole examples (semantic information) in ICL, thus reducing the requirement on distinguishability, i.e lower bound of KL divergence between the example and the question (Equation 4 in Condition 2). The complete analysis is provided in Appendix C. et al., 2022) increases the accuracy of the GSM8K of the PaLM model (Chowdhery et al., 2022) from 18% to 57% without additional training. In this section, we evaluate the performance of ChunkKV on the GSM8K, Many-Shot GSM8K (Agarwal et al., 2024), and JailbreakV (Luo et al., 2024) benchmarks. Table 3: GSM8K Performance Comparison. 4. Experiment Results In this section, we conduct experiments to evaluate the effectiveness of ChunkKV on KV cache compression in two benchmark fields, with chunk size set to 10 even for various model architectures. The first is the In-Context Learning benchmark, for which we select GSM8K (Cobbe et al., 2021) and Jailbreakv (Luo et al., 2024) to evaluate the performance of ChunkKV, furthermore we also include multi-step reasoning LLM DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025) to evaluate the performance of ChunkKV. The InContext Learning scenario is crucial capability for LLMs and has been adapted in many powerful technologies such as Chain-of-Thought (Wei et al., 2022; Diao et al., 2024; Pan et al., 2024b). The second is the Long-Context benchmark, which includes LongBench (Bai et al., 2024) and Needle-InA-HayStack (NIAH) (Kamradt, 2023), both widely used for assessing KV cache compression methods. All experiments were conducted three times, using the mean score to ensure robustness. 4.1. In-Context Learning The In-Context Learning (ICL) ability significantly enhances the impact of prompts on large language models (LLMs). For example, the Chain-of-Thought approach (Wei Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV (Ours) DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% 10% 51.6% 55.6% 57.6% 62.6% 65.7% LlaMa-3.1-8B-Instruct FullKV: 79.5% 30% 20% 10% 70.5% 63.8% 47.8% 72.2% 76.1% 64.0% 68.8% 45.0% 50.3% 77.1% 71.4% 48.2% 77.3% 77.6% 65.7% LlaMa-3-8B-Instruct FullKV: 76.8% 30% 70.6% 73.6% 70.2% 68.2% 74.6% Qwen2-7B-Instruct FullKV: 71.1% 30% 70.8% 61.2% 70.8% 64.7% 73.5% GSM8K In the in-context learning scenario, we evaluated multiple KV cache compression methods for GSM8K (Cobbe et al., 2021), which contains more than 1,000 arithmetic questions on LLaMA-3-8BInstruct, LLaMA-3.1-8B-Instruct (Meta, 2024), Qwen27B-Instruct (Yang et al., 2024a) and DeepSeek-R1-DistillLlama-8B (Guo et al., 2025). Follow the Agarwal et al. (2024), we consider many-shot GSM8K as long-context reasoning scenario, which is more challenging task than LongBench (Bai et al., 2024). The CoT prompt settings for this experiment are the same as those used by Wei et al. (2022), for many-shot GSM8K we set the number of shots to 50, which the prompt length is more than 4k tokens. 5 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Table 4: Many-Shot GSM8K Performance Comparison. Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV (Ours) DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% 10% 63.2% 54.2% 54.1% 59.2% 68.2% LlaMa-3.1-8B-Instruct FullKV: 82.4% 10% 74.3% 51.2% 68.2% 70.3% 79.3% For more details on the prompt settings, please refer to the APPENDIX G. Table 3 presents the performance comparison. The results demonstrate that ChunkKV outperforms other KV cache compression methods on different models and compression ratios. Table 4 presents the performance comparison of many-shot GSM8K, also ChunkKV outperforms other KV cache compression methods. The consistent superior performance of ChunkKV in both models underscores its effectiveness in maintaining crucial contextual information for complex arithmetic reasoning tasks. Jailbreak In this section, we evaluate the performance of ChunkKV on the JailbreakV benchmark (Luo et al., 2024). The prompt settings are the same as those used by Luo et al. (2024). Table 5 presents the performance comparison. The results demonstrate that ChunkKV outperforms other KV cache compression methods on different models and compression ratios. Which shows the effectiveness of ChunkKV in maintaining crucial contextual information for safety benchmark. Table 5: JailbreakV Performance Comparison. Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV (Ours) LlaMa-3.1-8B-Instruct FullKV: 88.9% 20% 10% 65.0% 53.1% 71.7% 88.0% 65.4% 84.3% 87.5% 85.5% 89.0% 87.9% 4.2. Long-Context Benchmark LongBench and NIAH are two widely used benchmarks for KV cache compression methods. Both benchmarks have context length that exceeds 10K. NIAH requires retrieval capability, while LongBench is meticulously designed benchmark suite that tests the capabilities of language models in handling extended documents and complex information sequences. LongBench We use LongBench (Bai et al., 2024) to assess the performance of ChunkKV on tasks involving longcontext inputs. For more details on LongBench, please refer to the APPENDIX F. We evaluated multiple KV cache eviction methods using the LongBench benchmark with LLaMA-3-8B-Instruct (Meta, 2024), Mistral-7B-InstructTable 6: KV cache compression methods on the LongBench benchmark. Results show performance gap compared to FullKV baseline (negative values indicate worse performance). Ratio StreamingLLM H2O SnapKV PyramidKV ChunkKV (Ours) LlaMa-3-8B-Instruct FullKV: 41.46 10% 20% 30% -13.80% -6.42% -2.36% -10.61% -3.16% -2.24% -8.85% -0.07% -5.38% -3.33% -2.00% -0.22% -2.29% -1.74% +0.31% Mistral-7B-Instruct-v0.3 FullKV: 48.08 10% -16.58% -9.30% -3.54% -3.52% -2.85% Qwen2-7B-Instruct FullKV: 40.71 10% -5.28% -0.64% -0.39% -0.98% +0.42% v0.3 (Jiang et al., 2023a), and Qwen2-7B-Instruct (Yang et al., 2024a), with KV cache compression ratio of 10%. The LongBench provides the Chinese subtask, and Qwen27B-Instruct also supports Chinese, so we tested Qwen2-7BInstruct with different KV cache compression methods on the Chinese subtasks. Tables 6 show that ChunkKV is capable of achieving on-par performance or even better than the full KV cache with less GPU memory consumption. This table presents the performance gap (in percentage) between each method and the FullKV baseline, where negative values indicate performance degradation compared to FullKV. The table is evaluated in the LongBench English subtask, where ChunkKV outperforms other compression methods overall. This suggests that ChunkKVs approach of retaining semantic chunks is more effective in preserving important information compared to other discrete token-based compression methods. For detailed results and Chinese subtask results, please refer to Appendix B.2 and B.5. Needle-In-A-HayStack We use Needle-In-A-HayStack (NIAH) (Kamradt, 2023) to evaluate LLMs long-context retrieval capability. NIAH assesses how well LLM extract hidden tricked information from extensive documents, and follow LLM-as-a-Judge (Zheng et al., 2023) we apply GPT4o-mini (OpenAI, 2023) to assess the accuracy of the retrieved information. We evaluated multiple KV cache eviction methods using NIAH with LLaMA-3-8B-Instruct and Mistral-7B-Instruct-v0.2, setting benchmark context lengths to 8k and 32k tokens. Table 7 provides statistical results for different compression methods. These findings clearly indicate the effectiveness of ChunkKV in managing varying token lengths and depth percentages, making it robust choice for KV cache management in LLMs. Figure 3 presents the NIAH benchmark results for LLaMA-3-8B-Instruct. The vertical axis represents the depth percentage, while the horizontal axis represents the token length, with shorter lengths on the left and longer lengths on the right. cell highlighted in green indiChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Table 8: Latency and throughput comparison between ChunkKV and FullKV under different input-output configurations. Percentages in parentheses indicate improvements over FullKV baseline. Method FullKV ChunkKV ChunkKV_reuse FullKV ChunkKV ChunkKV_reuse FullKV ChunkKV ChunkKV_reuse FullKV ChunkKV ChunkKV_reuse Sequence Length Performance Metrics Input Output Latency(s) Throughput(T/S) 4096 4096 4096 4096 4096 4096 8192 8192 8192 8192 8192 8192 1024 1024 4096 4096 4096 1024 1024 1024 4096 4096 4096 43.60 37.52 (13.9%) 37.35 (14.3%) 175.50 164.55 (6.2%) 162.85 (7.2%) 46.48 37.83 (18.6%) 36.85 (20.7%) 183.42 164.78 (10.2%) 162.15 (11.6%) 105.92 118.85 (12.2%) 124.09 (17.2%) 37.73 40.58 (7.6%) 41.12 (9.0%) 184.08 228.96 (24.4%) 232.99 (26.5%) 55.93 65.14 (16.5%) 66.05 (18.1%) The results in Table 8 shows that the layer-wise index reuse strategy (ChunkKV_reuse) further boosts performance, achieving up to 20.7% reduction in latency, and throughput improvements are particularly notable for longer input sequences, with ChunkKV_reuse delivering up to 26.5% improvement over FullKV. Table 7: NIAH Performance Comparison. KV cache Size StreamingLLM H2O SnapKV PyramidKV LlaMa-3.1-8B-Instruct FullKV: 74.6% 32.0% 28.0% 23.7% 21.5% 68.6% 71.2 % 61.7% 68.8% 47.9% 58.9% 41.0% 56.2% 72.6% 69.5% 65.1% 63.2% Mistral-7B-Instruct FullKV: 99.8% 44.3% 88.2% 91.6% 99.3% 99.8% ChunkKV (Ours) 74.5% 74.1% 73.8% 70.3% 512 256 128 96 128 cates that the method can retrieve the needle at that length and depth percentage. The detail visualization of the NIAH benchmark can be found in Appendix B.3. The visualization results demonstrate that ChunkKV outperforms other KV cache compression methods. (a) ChunkKV, accuracy 73.8% (b) PyramidKV, accuracy 65.1% (c) SnapKV, accuracy 58.9% (d) StreamingLLM, accuracy 23.7% Figure 3: NIAH benchmark for LLaMA3-8B-Instruct with KV cache size=128 under 8k context length. 4.3. Index Reuse This section will evaluate the performance of the layer-wise index reuse approach with ChunkKV from the two aspects of efficiency and performance. Measuring Efficiency. We evaluated the latency and throughput of ChunkKV compared to FullKV using LLaMA3-8B-Instruct on an A40 GPU. All experiments were conducted with reuse layer is 2, batch size set to 1 and inference was performed using Flash Attention 2, each experiment was repeated 10 times and the average latency and throughput were reported. Figure 4: Comparison with different index reuse layers on LongBench. Measuring Task Performance. This experiment evaluates the performance of the layer-wise index reuse approach by measuring the performance of the LongBench (Bai et al., 2024), the experiment settings are the same as LongBench in 4.2. And the number of index reuse layers is set from 1 to the number of layers in the model, where an index reuse layer of 1 corresponds to the normal ChunkKV without index reuse, and our method set reuse layer to 2. Figure 4 illustrates the performance of ChunkKV with varying index reuse layers on the LongBench benchmark. Generally, reuse layer set to 2 can achieve the minimal performance degradation across all models. For more experiments ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Table 9 and 10 show the performance of ChunkKV with different comperession ratios and different chunk sizes on the LongBench and NIAH. We conducted extensive experiments across different compression ratios and KV cache sizes to shows the effectiveness of ChunkKV and the chunk size is robust. Table 10: NIAH Performance with Different Chunk Sizes and KV Cache Sizes for LLaMA-3-8B-Instruct KV Cache Chunk Size Size 96 128 256 512 1 3 5 10 20 30 41.0 47.9 61.7 68.6 63.2 65.6 70.3 72.6 65.2 69.1 71.2 72.5 70.3 73.8 74.1 74. 67.2 72.3 73.2 74.3 65.3 72.0 72.3 74.0 53.1 71.2 71.1 72.6 From the chunk size ablation study, we can observe that across different tasks (LongBench and NIAH) and various compression settings, chunk size of 10 consistently delivers optimal or near-optimal performance. This empirical finding suggests that chunk size of 10 strikes good balance between preserving semantic information and compression efficiency, making it robust default choice for ChunkKV. Therefore, we adopt this chunk size setting throughout our experiments. 6. Conclusion We introduced ChunkKV, novel KV cache compression method that preserves semantic information by retaining more informative chunks. Through extensive experiments across multiple state-of-the-art LLMs (including DeepSeekR1, LLaMA-3, Qwen2, and Mistral) and diverse benchmarks (GSM8K, LongBench, NIAH, and JailbreakV), we demonstrate that ChunkKV consistently outperforms existing methods while using only fraction of the memory. Our comprehensive analysis shows that ChunkKVs chunkbased approach maintains crucial contextual information, leading to superior performance in complex reasoning tasks, long-context understanding, and safety evaluations. The methods effectiveness is particularly evident in challenging scenarios like many-shot GSM8K and multi-document QA tasks, where semantic coherence is crucial. Furthermore, our proposed layer-wise index reuse technique provides significant computational efficiency gains with minimal performance impact, achieving up to 20.7% latency reduction and 26.5% throughput improvement. These findings, supported by detailed quantitative analysis and ablation studies, establish ChunkKV as significant advancement in KV cache compression technology, offering an effective solution for deploying LLMs in resource-constrained environments while maintaining high-quality outputs. Figure 5: LongBench Performance Comparison with different chunk size under 10% compression rate. on index reuse, please refer to the APPENDIX B.1.3. Overall, these findings on efficiency and performance suggest that layer-wise index reuse can be an effective technique for optimizing the efficiency-performance trade-off in KV cache compression, with the potential for model-specific tuning to maximize benefits. 5. Ablation study 5.1. Chunk Size This section aims to investigate the impact of chunk size on the performance of ChunkKV. Different chunk sizes will lead to varying degrees of compression on the semantic information of the data. We set the experiemnt setting the same as in LongBench in Section 4.2. The chunk size is set from the range {1, 3, 5, 10, 20, 30}. Figure 5 shows the performance of the ChunkKV with different chunk size on the LongBench and NIAH benchmarks. The three colorful curves represent three LLMs with different chunk sizes, and the colorful dashed line is the corresponding FullKV performance. For more experiments on the size of the chunks with different compression ratios, refer to the Appendix B.4. Table 9: LongBench Performance with Different Chunk Sizes and Compression Ratios for LLaMA-3-8B-Instruct Compression Chunk Size Rate 10% 20% 30% 3 5 10 15 20 37.32 38.80 39.23 40.49 40.66 41.02 40.47 40.57 41.29 40.51 40.74 41.59 40.21 40.53 41.38 40.05 40.46 41. 39.57 40.04 41.02 From Figure 5, we can observe that the LongBench performance of ChunkKV is not significantly affected by the chunk size, with performance variations less than 1%. The three curves are closely aligned, indicating that chunk sizes in the range of {10, 20} exhibit better performance. 8 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"
        },
        {
            "title": "Impact Statement",
            "content": "Our study does not involve human subjects, data collection from individuals, or experiments on protected groups. The models and datasets used in this work are publicly available and widely used in the research community. We have made efforts to ensure our experimental design and reporting of results are fair, unbiased, and do not misrepresent the capabilities or limitations of the methods presented. In our work on KV cache compression for large language models, we acknowledge the potential broader impacts of improving efficiency in AI systems. While our method aims to reduce computational resources and potentially increase accessibility of these models, we recognize that more efficient language models could also lead to increased deployment and usage, which may have both positive and negative societal implications. We encourage further research and discussion on the responsible development and application of such technologies. We declare no conflicts of interest that could inappropriately influence our work. All experiments were conducted using publicly available resources, and our code will be made available to ensure reproducibility. We have made every effort to cite relevant prior work appropriately and to accurately represent our contributions in the context of existing research."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Rosias, L., Chan, S., Zhang, B., Anand, A., Abbas, Z., Nova, A., et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. ArXiv preprint, abs/2307.11088, 2023. URL https://arxiv.org/ abs/2307.11088. Anthropic. URL contextual-retrieval. Introducing contextual retrieval, 2024. https://www.anthropic.com/news/ Brandon, W., Mishra, M., Nrusimha, A., Panda, R., Reducing transformer key-value and Kelly, J. R. cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Cai, Z., Zhang, Y., Gao, B., Liu, Y., Liu, T., Lu, K., Xiong, W., Dong, Y., Chang, B., Hu, J., et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 38293846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL https:// aclanthology.org/2023.emnlp-main.232. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. Chuang, Y.-S., Xie, Y., Luo, H., Kim, Y., Glass, J., and He, P. Dola: Decoding by contrasting layers improves factuality in large language models. ArXiv preprint, abs/2309.03883, 2023. URL https://arxiv.org/ abs/2309.03883. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https: //arxiv.org/abs/2110.14168. Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. LongBench: bilingual, multitask benchmark for long context understanding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL https: //aclanthology.org/2024.acl-long.172. Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., and Gardner, M. dataset of information-seeking questions and answers anchored in research papers. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 45994610, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 9 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference 365. URL https://aclanthology.org/2021. naacl-main.365. Diao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang, T. Active prompting with chain-of-thought for large language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13301350, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.73. URL https: //aclanthology.org/2024.acl-long.73. Fabbri, A., Li, I., She, T., Li, S., and Radev, D. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. In Korhonen, A., Traum, D., and Màrquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 10741084, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/ P19-1102. URL https://aclanthology.org/ P19-1102. Fang, H. and Xie, P. An end-to-end contrastive selfsupervised learning framework for language understanding. Transactions of the Association for Computational Linguistics, 10:13241340, 2022. doi: 10.1162/tacl_ a_00521. URL https://aclanthology.org/ 2022.tacl-1.76/. Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., and Han, W. Extending context window of large language models via semantic compression. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 51695181, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 306. URL https://aclanthology.org/2024. findings-acl.306. Fu, Q., Cho, M., Merth, T., Mehta, S., Rastegari, M., and Najibi, M. LazyLLM: Dynamic token pruning for efIn Workshop on ficient long context LLM inference. Efficient Systems for Foundation Models II @ ICML2024, 2024a. URL https://openreview.net/forum? id=gGZD1dsJqZ. Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024b. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. Model tells you what to discard: Adaptive kv cache compression for llms. ArXiv preprint, abs/2310.01801, 2023. URL https://arxiv.org/abs/2310.01801. Gliwa, B., Mochol, I., Biesek, M., and Wawer, A. SAMSum corpus: human-annotated dialogue dataset for abstractive summarization. In Wang, L., Cheung, J. C. K., Carenini, G., and Liu, F. (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70 79, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409. Guo, D., Xu, C., Duan, N., Yin, J., and McAuley, J. J. Longcoder: long-range pre-trained language model for code completion. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 12098 12107. PMLR, 2023. URL https://proceedings. mlr.press/v202/guo23j.html. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Han, C., Wang, Q., Peng, H., Xiong, W., Chen, Y., Ji, H., and Wang, S. LM-infinite: Zero-shot extreme length generalization for large language models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39914008, Mexico City, Mexico, 2024. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.naacl-long.222. He, W., Liu, K., Liu, J., Lyu, Y., Zhao, S., Xiao, X., Liu, Y., Wang, Y., Wu, H., She, Q., Liu, X., Wu, T., and Wang, H. DuReader: Chinese machine reading comprehension dataset from real-world applications. In Choi, E., Seo, M., Chen, D., Jia, R., and Berant, J. (eds.), Proceedings of the Workshop on Machine Reading for Question Answering, pp. 3746, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2605. URL https://aclanthology.org/W18-2605. Ho, X., Duong Nguyen, A.-K., Sugawara, S., and Aizawa, A. Constructing multi-hop QA dataset In for comprehensive evaluation of reasoning steps. Scott, D., Bel, N., and Zong, C. (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main. 580. URL https://aclanthology.org/2020. coling-main.580. 10 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? ArXiv preprint, abs/2404.06654, 2024. URL https: //arxiv.org/abs/2404.06654. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptaIn The Tenth Internation of large language models. tional Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 14191436, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL https:// aclanthology.org/2021.naacl-main.112. Jacobs, S. A. et al. DeepSpeed Ulysses: System optimizations for enabling training of extreme long sequence Transformer models. ArXiv preprint, abs/2309.14509, URL https://arxiv.org/abs/2309. 2023. 14509. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023a. URL https: //arxiv.org/abs/2310.06825. Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. LLMLingua: Compressing prompts for accelerated inference of large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1335813376, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. URL https:// aclanthology.org/2023.emnlp-main.825. Jiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 11 pp. 16581677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.acl-long.91. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Barzilay, R. and Kan, M.-Y. (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https:// aclanthology.org/P17-1147. Kamradt, G. Needle testing LLMs. sure https://github.com/gkamradt/LLMTest_ NeedleInAHaystack/tree/main. In Haystack - presGithub, URL 2023. Kleijn and der Vaart, V. The bernstein-von-mises theorem under misspecification. Electronic Journal of Statistics, 6:354381, 2012. URL https://api. semanticscholar.org/CorpusID:85548207. Koˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. doi: 10.1162/tacl_a_00023. URL https:// aclanthology.org/Q18-1023. Li, D., Shao, R., et al. How long can open-source LLMs truly promise on context length?, 2023. URL https: //lmsys.org/blog/2023-06-29-longchat. Li, X. and Roth, D. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. URL https:// aclanthology.org/C02-1150. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. ArXiv preprint, abs/2404.14469, 2024. URL https://arxiv.org/abs/2404.14469. Liu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B. Minicache: Kv cache compression in depth dimension for large language models. arXiv preprint arXiv:2405.14366, 2024a. Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model on million-length video and language with ringattention. ArXiv preprint, abs/2402.08268, 2024b. URL https: //arxiv.org/abs/2402.08268. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024c. doi: 10.1162/tacl_a_00638. URL https:// aclanthology.org/2024.tacl-1.9. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. Liu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems. In The Twelfth International Conference on Learning Representations, 2024d. URL https://openreview.net/ forum?id=pPjZIOuQuF. Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024e. Luo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=GC4mXVfquq. Meta. Introducing meta llama 3: The most capable openly https://ai.meta.com/ available llm to date. blog/meta-llama-3/, 2024. Accessed: 2024-0607. Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. ArXiv preprint, abs/2305.16300, 2023. URL https: //arxiv.org/abs/2305.16300. OpenAI. Gpt-4o-mini: Advancing cost-efficient intelligence, 2023. Accessed: 2023-12-14. Pan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C., and Zhang, T. Lisa: Layerwise importance sampling for memory-efficient large language model fine-tuning. ArXiv preprint, abs/2403.17919, 2024a. URL https: //arxiv.org/abs/2403.17919. Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy, O. ZeroSCROLLS: zero-shot benchmark for In Bouamor, H., Pino, J., long text understanding. and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 7977 7989, Singapore, 2023. Association for Computational doi: 10.18653/v1/2023.findings-emnlp. Linguistics. 536. URL https://aclanthology.org/2023. findings-emnlp.536. Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V., Smith, N. A., Zettlemoyer, L., Yih, W.-t., and Lewis, M. In-context pretraining: Language modeling beyond document boundaries. In The Twelfth International Conference on Learning Representations. Smith, B. and Troynikov, A. Evaluating chunking Technical report, Chroma, URL https://research.trychroma. strategies for retrieval. 2024. com/evaluating-chunking. Steinwart, I. How to compare different loss functions and their risks. Constructive Approximation, 26:225287, 2007. URL https://api.semanticscholar. org/CorpusID:16660598. Sun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma, S., Zhang, Q., Wang, J., and Wei, F. You only cache once: Decoder-decoder architectures for language models. arXiv preprint arXiv:2405.05254, 2024. Tang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han, S. Quest: Query-aware sparsity for efficient long-context llm inference. ArXiv preprint, abs/2406.10774, 2024. URL https://arxiv.org/abs/2406.10774. Pan, R., Xing, S., Diao, S., Sun, W., Liu, X., Shum, K., Zhang, J., Pi, R., and Zhang, T. Plum: Prompt learning using metaheuristics. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 21772197, Bangkok, Thailand and virtual meeting, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.129. URL https://aclanthology. org/2024.findings-acl.129. Pires, B. Á. and Szepesvári, C. Multiclass classification calibration functions. arXiv preprint arXiv:1609.06385, 2016. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : benchmark for efficient transIn 9th International Conference on Learnformers. ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https: //openreview.net/forum?id=qVyeW-grC2k. Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D., Schuster, T., Zheng, H. S., Houlsby, N., and Metzler, D. Unifying language learning paradigms. ArXiv preprint, abs/2205.05131, 2022. URL https://arxiv.org/ abs/2205.05131. 12 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Tjong Kim Sang, E. F. and Veenstra, J. Representing text chunks. In Thompson, H. S. and Lascarides, A. (eds.), Ninth Conference of the European Chapter of the Association for Computational Linguistics, pp. 173179, Bergen, Norway, 1999. Association for Computational Linguistics. URL https://aclanthology.org/ E99-1023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. ArXiv preprint, abs/2307.09288, 2023. URL https://arxiv.org/abs/2307.09288. Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554, 2022. doi: 10. 1162/tacl_a_00475. URL https://aclanthology. org/2022.tacl-1.31. Wang, Q., Ding, L., Cao, Y., Tian, Z., Wang, S., Tao, D., and Guo, L. Recursively summarizing enables long-term dialogue memory in large language models. arXiv preprint arXiv:2308.15022, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wingate, D., Shoeybi, M., and Sorensen, T. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 56215634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp. 412. URL https://aclanthology.org/2022. findings-emnlp.412. Wu, H. and Tu, K. Layer-condensed kv cache for efficient inference of large language models, 2024. URL https: //arxiv.org/abs/2405.10637. Wu, H., Zhan, M., Tan, H., Hou, Z., Liang, D., and Song, L. VCSUM: versatile Chinese meeting summarization dataset. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 6065 6079, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl. 377. URL https://aclanthology.org/2023. findings-acl.377. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=NG7sS51zVF. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. ArXiv preprint, abs/2407.10671, 2024a. URL https://arxiv.org/abs/2407.10671. Yang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and PyramidInfer: Pyramid KV cache comZhao, H. pression for high-throughput LLM inference. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 32583270, Bangkok, Thailand and virtual meeting, 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 195. URL https://aclanthology.org/2024. findings-acl.195. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259. Yepes, A. J., You, Y., Milczek, J., Laverde, S., and Li, R. Financial report chunking for effective retrieval augmented generation, 2024. URL https://arxiv.org/abs/ 2402.05131. You, Y., Li, J., Reddi, S. J., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C. Large batch optimization for deep learning: Training BERT in 76 minutes. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum? id=Syx4wnEtvH. Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi: Open foundation models by 01. ai. ArXiv preprint, abs/2403.04652, 2024. URL https://arxiv.org/ abs/2403.04652. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K., Han, X., Thai, Z. L., Wang, S., Liu, Z., et al. -bench: Extending long context evaluation beyond 100k tokens. ArXiv preprint, abs/2402.13718, 2024. URL https: //arxiv.org/abs/2402.13718. 13 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A. H., Celikyilmaz, A., Liu, Y., Qiu, X., and Radev, D. QMSum: new benchmark for query-based multi-domain meeting summarization. In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 59055921, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 472. URL https://aclanthology.org/2021. naacl-main.472. Zhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y., Cotterell, R., and Sachan, M. Recurrentgpt: Interactive generation of (arbitrarily) long text, 2023. Zhou, Z., Tao, R., Zhu, J., Luo, Y., Wang, Z., and Han, B. Can language models perform robust reasoning in chainIn The of-thought prompting with noisy rationales? Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference"
        },
        {
            "title": "Appendix",
            "content": "A In-depth Analysis of ChunkKV vs. Discrete Token Methods A.1 Quantitative Analysis . A.2 Hypothetical Scenario . A.3 Comparative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Implications for Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experiments B.1 Layer-Wise Index Reuse . B.2 LongBench . . . . . . . B.3 Needle-In-A-Haystack . B.4 Chunk Size . . B.5 Multi-Lingual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theoretical Understanding Additional Related Work Statistics of Models Statistics of Datasets Prompt Limitations Licenses 16 16 17 17 18 18 25 26 29 29 33 33 34 34 35 15 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference A. In-depth Analysis of ChunkKV vs. Discrete Token Methods A.1. Quantitative Analysis To rigorously evaluate the effectiveness of ChunkKV compared to discrete token-based methods, we conducted systematic experiments using LLaMA-3-8B-Instruct model. We randomly selected 100 sequences from the each sub-category of LongBench dataset and analyzed two key metrics across different model layers: KV cache L1 loss and attention cosine similarity. For each sequence, we: 1. Computed the full KV cache and attention patterns without compression as ground truth. 2. Applied ChunkKV, SnapKV, and H2O compression methods with fixed 10% compression ratio, and the parameters of the three methods are set the same as in Table 14. 3. Measured the differences between compressed and uncompressed versions. Figure 6: Layer-wise comparison of L1 loss and attention cosine similarity between ChunkKV and discrete token-based methods in Single-Document QA sub-category of LongBench. Results Analysis As shown in Figure 6, ChunkKV demonstrates superior performance across both metrics: KV Cache L1 Loss: ChunkKV achieves consistently lower L1 loss compared to SnapKV and H2O, particularly in the early and middle layers (layers 5-25). This indicates better preservation of the original KV cache information through the semantic chunk-based approach. Attention Cosine Similarity: ChunkKV exhibits higher similarity scores across most layers, with notably strong performance in layers 0-5 and 20-30. This suggests better preservation of attention relationships between tokens, which is crucial for maintaining semantic understanding. To quantify these improvements, we calculated average metrics across all layers, as shown in Table 11. ChunkKV achieves both the lowest L1 loss and highest attention cosine similarity, outperforming both baseline methods. Significance of Results While the improvements may appear modest in absolute terms (approximately 2% in L1 loss and 1.5% in cosine similarity), their practical significance is substantial. These metrics reflect the models ability to maintain crucial semantic relationships and attention patterns, which are essential for complex reasoning tasks. The consistent improvements across different sequences demonstrate that preserving semantic chunks leads to better information retention than selecting individual tokens. The enhanced performance is particularly evident in the middle layers of the model, which are typically responsible for higher-level semantic processing. This provides concrete evidence for why ChunkKV achieves superior performance on downstream tasks compared to discrete token-based methods. 16 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Table 11: Detailed comparison of KV cache metrics across different task categories in LongBench. Method Single-Document Multi-Document QA QA Summarization Few-shot Learning Synthetic & Code ChunkKV SnapKV H2O ChunkKV SnapKV H2O KV Cache L1 Loss 0.8748 0.8933 0.8917 0.8770 0.8930 0.8913 Attention Score Cosine Similarity 0.3651 0.3594 0.3572 0.3841 0.3771 0. 0.8741 0.8921 0.8905 0.3567 0.3513 0.3491 0.8861 0.8917 0.8906 0.4330 0.4305 0.4284 0.8726 0.8938 0.8915 0.3805 0.3759 0. A.2. Hypothetical Scenario To provide deeper understanding of ChunkKVs effectiveness compared to discrete token-based methods, we present detailed analysis using hypothetical scenario. This analysis aims to illustrate the fundamental differences between these approaches and explain why ChunkKV is more effective at preserving semantic information in long contexts. Consider comprehensive document that contains detailed information on various animals, including their habitats, diets, and behaviors. user asks the question \"What do pandas eat in the wild?\" Both ChunkKV and discrete token-based methods would use this question to calculate observation scores for the document. However, their approaches to selecting and retaining information differ significantly. A.2.1. DISCRETE TOKEN-BASED METHOD discrete token-based method might identify and retain individual tokens with high relevance scores, such as: pandas\",eat\", bamboo\", wild\", diet\", food\" Although these tokens are relevant, they lack context and coherence. The method might discard other essential tokens that provide crucial context or complete the information. A.2.2. CHUNKKV METHOD In contrast, ChunkKV would identify and retain semantically meaningful chunks, such as: In the wild, pandas primarily eat bamboo shoots and leaves\" Their diet consists of 99% bamboo, but they occasionally consume other vegetation\" Wild pandas may also eat small rodents or birds when available\" By preserving these chunks, ChunkKV maintains not only the relevant keywords but also their contextual relationships and additional pertinent information. A.3. Comparative Analysis The advantages of ChunkKV become evident when we consider how these retained pieces of information would be used in subsequent processing: 1. Contextual Understanding: Discrete tokens require the model to reconstruct meaning from isolated words, which could lead to ambiguity. ChunkKV provides complete phrases or sentences, allowing for immediate and accurate comprehension. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference 2. Semantic Coherence: ChunkKV preserves the semantic relationships within chunk, crucial to understanding nuances such as the difference between primary and occasional food sources for pandas. 3. Information Density: single chunk can contain multiple relevant tokens in their proper context, potentially retaining more useful information within the same compressed cache size compared to discrete methods. 4. Reduced Ambiguity: Discrete methods might retain the token eat\" from various sentences about different animals. ChunkKV ensures that eat\" is preserved specifically in the context of pandas in the wild. 5. Temporal and Logical Flow: ChunkKV can maintain the sequence of ideas present in the original text, preserving any temporal or logical progression that may be crucial for understanding. A.4. Implications for Model Performance This analysis suggests several key implications for model performance: Improved Accuracy: By retaining contextually rich information, ChunkKV enables more accurate responses to queries, especially those requiring nuanced understanding. Enhanced Long-context Processing: Preservation of semantic chunks allows for better handling of long-range dependencies and complex reasoning tasks. Reduced Computational Overhead: Although both methods compress the KV cache, ChunkKVs approach may reduce the need for extensive context reconstruction, potentially improving inference efficiency. Versatility: The chunk-based approach is likely to be more effective across wide range of tasks and domains as it preserves the natural structure of language. This in-depth analysis demonstrates why ChunkKV is more effective in preserving semantic information in long contexts. By retaining coherent chunks of text, it provides language models with more contextually rich and semantically complete information, leading to improved performance in tasks that require deep understanding and accurate information retrieval from extensive documents. B. Additional Experiments B.1. Layer-Wise Index Reuse B.1.1. EFFICIENCY ANALYSIS The layer-wise index reuse method significantly reduces the computational complexity of ChunkKV. Without index reuse, ChunkKV would be applied to all Nlayers layers, resulting in total compression time of Nlayers Tcompress, where Tcompress is the time taken to compress one layer. With index reuse, ChunkKV is only applied to Nlayers layers, reducing the total time to Nreuse Nlayers Tcompress + (Nlayers Nlayers ) Tselect, where Tselect is the time taken to select indices, which is typically much smaller Nreuse Nreuse than Tcompress. This results in theoretical speedup factor of: Speedup = Nlayers Tcompress Tcompress + (Nlayers Nlayers Nreuse Nlayers Nreuse ) Tselect Assuming Tselect is negligible compared to Tcompress, this simplifies to approximately Nreuse. In practice, the actual speedup may vary depending on the specific implementation and hardware, but it can still lead to substantial time savings, especially for models with large number of layers. B.1.2. LAYER-WISE INDEX SIMILARITY This section details the experiment of layer-wise index reuse similarity described in Section 3.3. The inference prompt is randomly selected from the LongBench benchmark, and the preserved indices for H2O, SnapKV, and ChunkKV are saved in 18 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference the log file. For multi-head attention, only the indices of the first head are saved. PyramidKV, which has varying preserved index sizes across different layers, is not applicable for this experiment. Then we calculate the Jaccard similarity of the preserved indices of adjacent layers for different models. Table 12 shows the Jaccard similarity of the preserved indices of adjacent layers for different models. Table 12: Retained KV Cache Indices Similarity of Adjacent Layers for Different Models. Method H2O SnapKV ChunkKV LLaMA-3-8B-Instruct Qwen2-7B-Instruct Mistral-7B-Instruct 25.31% 27.95% 14.91% 16.50% 15.15% 15.78% 57.74% 44.26% 52.16% Figures 7-9 (LLaMA-3-8B-Instruct), 10-12 (Mistral-7B-Instruct), and 13-15 (Qwen2-7B-Instruct) display the heatmaps of layer-wise indices similarity of the preserved KV cache indices by H2O, SnapKV and ChunkKV on different models. The pattern of the layer-wise indices similarity heatmap is consistent across different models, aligning with our findings in Section 3.3. 19 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 7: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on LLaMA-3-8B-Instruct Figure 8: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV on LLaMA-3-8B-Instruct 20 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 9: Layer-wise similarity heatmaps of the preserved KV cache indices by ChunkKV on LLaMA-3-8B-Instruct Figure 10: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on Mistral-7B-Instruct 21 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 11: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV on Mistral-7B-Instruct Figure 12: Layer-wise similarity heatmaps of the preserved KV cache indices by ChunkKV on Mistral-7B-Instruct B.1.3. INDEX REUSE PERFORMANCE Figure 16 illustrates the performance of ChunkKV with varying index reuse layers on the GSM8K benchmark. The experiment reveals that math problems are more sensitive to index reuse layers compared to LongBench. Both LLaMA3-8B22 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 13: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on Qwen2-7B-Instruct Figure 14: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV on Qwen2-7B-Instruct Instruct and Qwen2-7B-Instruct exhibit significant performance degradation, with LLaMA3-8B-Instruct experiencing steeper decline after two layers of index reuse than Qwen2-7B-Instruct. This suggests that the Qwen2-7B-Instruct model may be more robust to index reuse. 23 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Figure 15: Layer-wise similarity heatmaps of the preserved KV cache indices by ChunkKV on Qwen2-7B-Instruct Figure 16: GSM8K Performance Comparison with different index reuse layers Table 13 shows the performance of ChunkKV with different numbers of index reuse layers in GSM8K. The number of index reuse layers is set from 1 to the number of layers in the model, where index reuse layer of 1 corresponds to the normal ChunkKV without index reuse, and 28/32 is the maximum number of layers for LLaMA-3-8B-Instruct and Qwen2-7B-Instruct. The significant performance drop of LLaMA-3-8B-Instruct raises another question: whether the KV cache compression method is more sensitive to the models mathematical reasoning ability. 24 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Table 13: Reusing Indexing Performance Comparison on GSM8K Model 1 3 5 8 10 20 28/ Number of Index Reuse Layers LLaMA-3-8B-Instruct Qwen2-7B-Instruct 74.5 71.2 74.6 71.2 65.9 73.0 44.1 69. 15.3 67.4 2.20 71.1 1.60 54.0 1.80 49.4 B.2. LongBench The Table 14 shows the average performance of KV cache compression methods in the LongBench English subtask categories. The ChunkKV achieves the best performance on the overall average, and the Multi-Document QA category, which supports that chunk method is more effective for semantic preservation. Table 14: Comprehensive performance comparison of KV cache compression methods across LongBench English subtasks. Results are shown for various models and tasks, highlighting the effectiveness of different compression techniques. Method Single-Document QA Multi-Document QA N rt a n - A o A 2 i e si R G Few-shot Learning Synthetic Code e R Q ri m A n C R c - Avg. Avg len 18,409 3,619 4, 9,151 4,887 11,214 8,734 2,113 5, 8,209 6,258 11,141 9,289 1,235 4,206 Summarization rt lti Q 10,614 FullKV 25.70 29.75 41.12 45.55 35.87 22. 25.63 23.03 26.21 73.00 90.56 41. 4.67 69.25 58.05 50.77 41.46 LlaMa-3-8B-Instruct, KV Size = Full StreamingLLM 20.62 13.09 22.10 24.80 17.32 31.80 H2O 25.08 22.02 37.95 SnapKV 25.58 20.77 35.85 PyramidKV 24.89 22.96 37.64 ChunkKV StreamingLLM 23.35 18.97 32.94 25.60 21.88 35.36 H2O 25.50 25.95 38.43 SnapKV PyramidKV 25.36 26.88 37.99 26.13 28.43 38.59 ChunkKV StreamingLLM 24.49 22.53 35.30 25.87 23.03 37.06 H2O 25.15 28.75 39.28 SnapKV PyramidKV 25.42 27.91 38.81 25.88 29.58 38.99 ChunkKV 36.31 40.84 43.36 43.80 43.27 42.39 42.06 44.12 44.21 44.46 44.33 43.71 43.57 44.15 43.94 LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% 28.01 33.28 35.08 33.03 36.45 15.61 18.90 20.29 21.45 20. 21.47 22.29 22.94 23.68 22.80 21.05 22.29 22.64 22.26 22.97 19.39 21.82 21.37 21.85 20.82 62.00 40.00 71.00 71.50 71.50 84.18 90.51 90.47 90.47 90.52 LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% 29.37 32.68 35.38 35.65 34.13 18.76 19.72 20.49 21.43 21.06 25.78 23.54 24.85 25.52 24.72 21.92 22.77 23.36 23.43 23.11 25.16 22.72 23.51 23.47 22.91 71.00 45.50 72.50 72.00 71. 88.85 90.57 90.52 90.56 90.56 LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% 32.81 33.68 36.16 36.28 34.16 19.00 20.93 21.58 21.72 21.70 27.12 24.56 25.56 26.50 26.50 22.19 23.14 23.19 23.10 23. 25.93 23.58 24.30 24.28 23.95 72.50 50.50 73.00 72.00 72.00 89.84 90.77 90.52 90.56 90.56 Mistral-7B-Instruct-v0.3, KV Size = Full 40.27 40.55 40.15 41.66 40.83 40.82 41.67 40.91 41.45 41. 41.75 41.96 41.70 41.87 42.47 4.62 69.10 58.84 55.26 5.79 69.50 58.04 55.26 5.66 69.25 58.69 56.50 5.84 69.25 58.52 55.91 5.93 69.00 60.49 57.48 5.04 69.00 56.46 51.12 5.51 69.25 54.97 50.95 5.23 69.25 56.74 51.75 5.26 69.50 56.55 50.93 5.09 69.00 58.17 52.51 5.41 69.00 60.40 55.13 4.91 69.25 59.38 55.39 4.96 69.25 60.27 55.74 4.67 69.50 60.09 55.19 5.34 69.25 61.68 56.35 35.74 37.06 40.15 40.08 40.51 38.80 37.79 40.53 40.63 40. 40.48 39.23 41.43 41.37 41.59 FullKV 29.07 41.58 52.88 49.37 39.01 28. 34.93 25.68 27.74 76.00 88.59 47. 6.00 98.50 61.41 62.39 48.08 Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% StreamingLLM 25.15 25.47 30.08 29.35 33.39 50.39 H2O 28.54 36.88 53.42 SnapKV PyramidKV 29.40 35.39 52.96 29.75 36.82 53.99 ChunkKV 44.39 49.58 50.15 49.93 50.33 32.49 36.76 38.17 38.67 38. 19.40 27.42 27.99 28.63 29.01 24.11 25.16 26.67 27.59 27.03 20.85 24.75 25.21 24.99 24.76 19.55 22.12 22.33 22.77 21.42 65.00 42.00 72.00 74.00 76.00 88.21 89.00 89.36 90.02 88. 44.83 47.04 45.44 46.07 46.49 4.50 79.50 59.48 58.82 5.50 98.50 57.58 59.24 5.50 99.00 59.79 61.63 4.00 98.50 58.54 60.88 5.00 98.00 59.98 61.47 40.11 43.61 46.38 46.39 46.71 Qwen2-7B-Instruct, KV Size = Full FullKV 25.11 42.64 44. 14.25 13.22 9.08 36.38 23.43 26. 77.00 89.99 44.88 6.75 75.92 60.17 61.84 40.71 Qwen2-7B-Instruct, KV Size Compression Ratio = 10% StreamingLLM 25.15 45.42 41.46 H2O 26.17 44.33 42.54 26.84 45.96 45.79 SnapKV 27.51 44.45 43.59 PyramidKV ChunkKV 26.48 44.19 45.04 13.66 12.81 14.27 13.35 15.94 11.95 12.46 13.35 13.13 12.60 8.72 9.15 9.91 9.12 10.52 32.79 33.24 32.62 32.28 32.38 21.49 22.69 22.70 22.60 22. 26.24 25.94 25.83 25.45 25.91 77.50 76.50 77.00 77.00 77.50 89.15 89.44 89.19 89.44 89.22 44.54 44.32 44.71 44.53 44.78 7.50 50.50 60.03 60.91 8.00 76.00 61.28 62.39 7.50 71.50 60.35 61.37 7.00 73.50 60.91 61.24 8.50 76.50 60.64 61.32 38.56 40.45 40.55 40.31 40. 25 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference B.3. Needle-In-A-Haystack Figure 17 and 18 visualizes the performance of ChunkKV on the NIAH benchmark for LLaMA-3-8B-Instruct and Mistral7B-Instruct with KV cache size of 128 under 8k and 32k context length. The performance of ChunkKV is consistently better as the context length increases. 26 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference (a) ChunkKV, accuracy 73.8% (b) PyramidKV, accuracy 65.1% (c) SnapKV, accuracy 58.9% (d) H2O, accuracy 47.9% Figure 17: NIAH benchmark for LLaMA-3-8B-Instruct with KV cache size=128 under 8k context length (e) StreamingLLM, accuracy 23.7% 27 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference (a) ChunkKV, accuracy 99.8% (b) PyramidKV, accuracy 99.3% (c) SnapKV, accuracy 91.6% (d) H2O, accuracy 88.2% Figure 18: NIAH benchmark for Mistral-7B-Instruct with KV cache size=128 under 32k context length (e) StreamingLLM, accuracy 44.3% 28 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference B.4. Chunk Size Table 15 shows the performance of ChunkKV with different chunk size on the LongBench benchmark. Table 15: LongBench Performance Comparison with different chunk sizes Model 3 5 10 30 Full KV Chunk Size LLaMA-3-8B-Instruct Mistral-7B-Instruct Qwen2-7B-Instruct 40.49 46.45 40.38 40.47 46.51 40. 40.51 46.71 40.66 40.05 46.42 40.88 39.57 45.98 40.73 41.46 48.08 40.71 Table 16 shows the performance of ChunkKV with different chunk size on the GSM8K benchmark. Figure 19 shows that the ChunkKV with different chunk sizes on GSM8K displays the same curve pattern as LongBench. The CoT prompt length for GSM8K is only 1K tokens, so the optimal chunk size range is smaller. Figure 19: GSM8K Performance Comparison with different chunk size Table 16: GSM8K Performance Comparison with different chunk sizes Model 3 5 10 Full KV Chunk Size LLaMA-3-8B-Instruct Qwen2-7B-Instruct 74.6 73.5 74.5 71.2 73.9 71. 63.2 71.7 76.8 71.1 B.5. Multi-Lingual Table 17 is the Chinese support model Qwen2-7B-Instruct evaluated on the LongBench Chinese subtask, where ChunkKV achieves better performance than other compression methods and the full KV cache performance. Both the English and Chinese results indicate that ChunkKV is promising approach for maintaining crucial information in the KV cache. 29 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Table 17: Performance comparison of Chinese subtask on LongBench for Qwen2-7B-Instruct. Single-Document QA Multi-Document QA Summarization Few-shot Learning Synthetic Method Avg len FullKV StreamingLLM H2O SnapKV PyramidKV ChunkKV MF-zh 6,701 39.17 38.05 37.99 44.25 36.57 45. DuReader 15,768 VCSum 15,380 LSHT 22, Avg. PR-zh 6,745 Qwen2-7B-Instruct, KV Size = Full 23.63 16. 43.50 70.50 38.60 Qwen2-7B-Instruct, KV Size Compression Ratio = 10% 23.24 19.58 20.27 20.56 20.15 15.92 16.16 16.24 16.15 16. 40.50 41.67 44.50 43.50 43.75 44.50 67.35 68.10 66.50 71.10 32.44 36.55 38.67 36.55 39.45 C. Theoretical Understanding In this section, we provide the theoretical interpretation from the perspective from the In-context learning (ICL) to further understand how ChunkKV outperforms token-level KV cache compression. Pretraining Data Distribution. Given set of concepts Θ and concept θ Θ, we define the pretraining data is sampled from p(o1, . . . , oT ) = (cid:82) θΘ p(o1, . . . , oT θ)p(θ)dθ (Fang & Xie, 2022). Each token is sampled from vocabulary O. For simplicity, we write o1:t = o1 . . . ot. Language Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Fang & Xie, 2022) usually utilize the next word prediction as the language modelling, which predicts the next token ot given the previous tokens o1 . . . ot1 for all = 1, . . . , . Formally, language modelling can be writen as the distribution (oto1:t1). And it is pretrained on huge corpus sampled from the pretraining distribution p(o1, . . . , ot+1) (Fang & Xie, 2022). Considering the large scale of the model size and dataset size, it can be assumed that the (o1 . . . ot+1) has been aligned with the p(o1 . . . ot+1) (Fang & Xie, 2022). Prompt Distribution. Following (Fang & Xie, 2022), prompt is composed of an input token sequence followed by an output token y. Then, the i-th training example 1 that can appear in any place in the whole prompt o1:T is defined as Oi consisting of an input xi = Oi [1 : 1] (the first 1 tokens) followed by the output yi = Oi [k] at the end, where the length is fixed for simplicity. The i-th training example is independently generated as follows: 1) Generate start hidden state hstart from prompt , θ). The test input start distribution pprompt; 2) Given hstart xtest = xn+1 is sampled similarly. Then, the prompt consists of sequence of training examples (Sn) followed by the example xtest: , generate the example sequence Oi = [xi, yi] from p(Oihstart [Sn, xtest] = [x1, y1, x2, y2, . . . , xn, yn, xtest] pprompt. (2) In-context learning setups and Assumptions. We follow other settings and assumptions in (Fang & Xie, 2022). With the greedy decoding (Fu et al., 2024b), sampling the next token from the language modeling (oto1:t1) becomes the predictor as = arg maxot (oto1:t1). Thus, for [Sn, xtest], the in-context learning predictor can be written as fn(xtest) := arg maxy p(ySn, xtest), which outputs the most likely prediction over the pretraining distribution conditioned on the prompt distribution. Its expected 0-1 error with examples is L0-1(fn) = Extest,ytestpprompt[1[fn(xtest) = ytest]]. We define pi θ(o) := p(O[i] = oO[1 : 1], θ) of the i-th token with previous tokens and the analogous distribution pi prompt := pprompt(O[i] = oO[1 : 1]) under the prompt distribution. Following (Fang & Xie, 2022), there is distinguishability condition formalizes when in-context learning occurs giving the concept θ. The distinguishability condition is dependent on KL divergence between the previous two distributions and the error terms 1Here, training example in prompts means happens during the prompt learning. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference ϵθ resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting θ(o) and pi pi Condition 1 (distinguishability (Fang & Xie, 2022)). The θ is distinguishable if for all θ Ω, θ = θ, prompt correspond to the concept θ and θ. (cid:88) i=1 KLi(θθ) > ϵθ, (3) where the KLi(θθ) := EO[1:i1]pprompt[KL(pi promptpi θ)]. Noises from KV Cache Compression. Naturally, because of the sparsified KV cache, some history tokens in o1:t1 at different layers lost its attention score calculation with respect to the next word prediction ot. We can regard this as the noise added onto the o1:t1. Thus, distincting θ from θ requires larger KL divergence. Following (Zhou et al., 2024), we provide the following second condition about the distinguishability with the KV cache sparsity. Condition 2 (distinguishability under sparsified KV cache). With the noise introduced by the sparsified KV cache of the sparse ratio r, the distribution mismatch between the prompt and the pretraining distribution that is approximated by LLM is enlarged, resulting in varied requirement with error term ξθ(r) for θ being distinguishable if for all θ Θ, θ = θ, (cid:88) i=1 KLi(θθ) > ϵθ + ξθ(r), where ξθ(r) r. (4) Lemma 1 (noisy-relaxed bound in (Fang & Xie, 2022; Zhou et al., 2024)). let denotes the set of θ which does not satisfy Condition 1. We assume that KL(pprompt(ytestxtest))p(ytestxtest, θ) is bounded for all θ and that θ minimizes the multi-class logistic risk as, LCE(θ) = Extestpprompt[pprompt(ytestxtest) log p(ytestxtest, θ)]. Extestpprompt[KL(pprompt(ytestxtest)p(ytestxtest, θ))] (ϵθ + ξθ(r)), θ B, If then lim L01(fn) inf L01(f ) + g1 (cid:18) (cid:19) (ϵθ) sup θB , (5) (6) (7) where g(ν) = 1 2 2016) for the multiclass logistic loss for ν [0, 1]. (cid:0)(1 ν) log(1 ν) + (1 + ν) log(1 + ν)(cid:1) is the calibration function (Steinwart, 2007; Pires & Szepesvári, Following (Kleijn & der Vaart, 2012; Fang & Xie, 2022), KL divergence is assumed to haver the 2nd-order Taylor expansion with the concept θ. Then, we have the following theorem and proof. Theorem 1. Assume that KL divergences have 2nd-order Taylor expansion around θ: (Fang & Xie, 2022; Zhou et al., 2024) Let the set of θ which does not satisfy Equation 3 in Condition 1 to be B. > 1, KLi(θθ) = 1 2 (θ θ)Ij,θ (θ θ) + O(θ θ3) (8) where Ij,θ is the Fisher information matrix of the j-th token distribution with respect to θ. Let γθ = maxj λmax(Ij,θ ) min jλmin(Ij,θ ) where λmax, λmin return the largest and smallest eigenvalues. Then for 2 and as , the 0-1 risk of the in-context learning predictor fn is bounded as lim L0-1(fn) inf L0-1(f ) + g1 (cid:18) (cid:18) γθ supθB(ϵθ + ξθ(r)) 1 (cid:19)(cid:19) (9) 31 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference Proof. (Fang & Xie, 2022) By the Taylor expansion on θ, we have for any θ in that (cid:88) j=2 KLi(θθ) 1 2 1 2 (cid:88) j=2 (θ θ)Ij,θ (θ θ) + (k 1)O(θ θ3) (k 1)λmin(Ij,θ )θ θ = θ θ2 (ϵθ + ξθ(r)) 1 2 (k 1)(minj λmin(Ij,θ )) . We can bound the last KL term (k-th token) with the above term: KLk(θθ) = (θ θ)Ik,θ (θ θ) + O(θ θ3) 1 2 1 2 (ϵθ + ξθ(r))(maxj λmax(Ij,θ ) + O(1)) (k 1) minj λmin(Ij,θ ) λmax(Ij,θ ))θ θ2 + O(θ θ2) (max . Rearranging above equation, and with KLk(θθ) = Extestpprompt[KL(pprompt(ytestxtest)p(ytestxtest, θ))], there is Extestpprompt[KL(pprompt(ytestxtest)p(ytestxtest, θ))] (ϵθ + ξθ(r))(maxj λmax(Ij,θ ) + O(1)) (k 1) minj λmin(Ij,θ ) Combining Equation 16 with Equation 6 into Lemma 1 completes the proof. (10) (11) (12) (13) (14) (15) (16) KV Cache Sparsification. Revisiting the Equation 4 in Condition 2, the ξθ(r) is enlarged with the sparsity ratio r. The higher compression ratio (means that more KV cache are discarded), the more noise ξθ(r). Then it leads to the higher bound of the limn L01(fn) in Equation 5 in Lemma 1. Next, we discuss how KV cache compression influences the Equation 4. Token-level Importance Measure. The token-level KV cache methods usually calculate the importance of different tokens. Then, the KV cache with indexes that have higher importance will be preserved. Such indexes are normaly choosed as the attention score. Considering that the case in Figure 1, where each token in the i-th training 2 example sequence (Oi = [xi, yi]) might be compressed, and tokens are sparsified concretely without out dependency to other tokens. However, in the generation process of the i-th training example, Oi = [xi, yi] is sampled from p(Oihstart θ(o) := p(O[j] = oO[1 : 1], θ) of the j-th token with previous tokens and the analogous distribution pj prompt := pprompt(O[j] = oO[1 : 1]). And the KL divergence is defined as KLj(θθ) := EO[1:j1]pprompt[KL(pj promptpj θ)], which means that in training example Oi = [xi, yi] = Oi[1 : k], each token Oi[j] has strong dependency with Oi[1 : 1], noises on previous any j-th token will influence the distinguishability on the following tokens, i.e. requiring larger {KLu(θθ)}u>j. On the other hand, the token-level sparsity enlarges the requirement on the distinguishability uniformly for each example Oi (the case in Figure 1), which uniformly loses the bound of L0-1(fn) as in Equation 9. , θ) and pj Chunk-level Importance Measure. Different from token-level importance measure, ChunkKV regards tokens in continuous window as basic unit that should be left or discarded as whole. The preserved window can be regarded as saving the complete Oi = [xi, yi] without noise. Thus, ChunkKV reduces the noise ξθ(r) for the preserved Oi, which lowers the bound of L0-1(fn). More intuitively, ChunkKV focus on reducing the noise on some complete training examples, but some other examples overall with low importance will be discarded. Then, the model identifies the xtest from those clean and more related training examples Oi and neglect those Oi with less importance. Note that here, we do not provide the rigorous proof on how KV cache sparsity enhances the requirement of the distinguishability and how different KLj(θθ) on Oi = [xi, yi] influences the bound L0-1(fn). We left this as the future work to analyze how KV cache sparsity influences the in-context learning. 2Here, training means prompt learning (Fang & Xie, 2022). 32 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference D. Additional Related Work KV cache sharing Recent work has explored various strategies for sharing KV caches across transformer layers. LayerCondensed KV Cache (LCKV) (Wu & Tu, 2024) computes KVs only for the top layer and pairs them with queries from all layers, while optionally retaining standard attention for few top and bottom layers to mitigate performance degradation. Similarly, You Only Cache Once (YOCO) (Sun et al., 2024) computes KVs exclusively for the top layer but pairs them with queries from only the top half of layers, employing efficient attention in the bottom layers to maintain constant cache size. In contrast, Cross-Layer Attention (CLA) (Brandon et al., 2024) divides layers into groups, pairing queries from all layers in each group with KVs from that groups bottom layer. MiniCache (Liu et al., 2024a) introduces novel method that merges layer-wise KV caches while enabling recovery during compute-in-place operations, optimizing KV cache size. These methods illustrate various trade-offs between computation, memory usage, and model performance when sharing KV caches across transformer layers. Long-Context Benchmarks The landscape of long-context model benchmarks has evolved to encompass wide range of tasks, with particular emphasis on retrieval and comprehension capabilities. Benchmarks for understanding have made significant strides, with -Bench (Zhang et al., 2024) pushing the boundaries by presenting challenges that involve more than 100,000 tokens. LongBench (Bai et al., 2024) has introduced bilingual evaluations, addressing tasks such as longdocument question answering, summarization, and code completion. Complementing these efforts, ZeroSCROLLS (Shaham et al., 2023) and L-Eval (An et al., 2023) have broadened the scope to include diverse array of practical natural language tasks, including query-driven summarization. In parallel, retrieval benchmarks have largely relied on synthetic datasets, offering researchers precise control over variables such as the length of input tokens. This approach minimizes the impact of disparate parametric knowledge resulting from varied training methodologies. significant body of recent work has concentrated on the development of synthetic tasks specifically for retrieval evaluation (Kamradt, 2023; Mohtashami & Jaggi, 2023; Li et al., 2023; Liu et al., 2024c; Hsieh et al., 2024). In addition, researchers have explored the potential of extended contexts in facilitating various forms of reasoning (Tay et al., 2021). This dual focus on synthetic retrieval tasks and comprehensive understanding benchmarks reflects the fields commitment to rigorously assessing the capabilities of long-context models across diverse linguistic challenges. Prompting Compression In the field of prompt compression, various designs effectively combine semantic information to compress natural language. Wingate et al. (2022) utilize soft prompts to encode more information with fewer tokens. Chevalier et al. (2023) present AutoCompressor, which uses soft prompts to compress the input sequence and extend the original length of the base model. Both Zhou et al. (2023) and Wang et al. (2023) recurrently apply LLMs to summarize input texts, maintaining long short-term memory for specific purposes such as story writing and dialogue generation. The LLMLingua series (Jiang et al., 2023b; 2024; Fei et al., 2024) explores the potential of compressing LLM prompts in long-context, reasoning, and RAG scenarios. Fei et al. (2024) use pre-trained language models to chunk the long context and summarize semantic information, compressing the original context. E. Statistics of Models Table 18 provides configuration parameters for LLMs that we evaluated in our experiments. Model Name LLaMA-3-8B-Instruct Mistral-7B-Instruct-v0.2 & 0.3 Qwen2-7B-Instruct (Number of layers) (Number of attention heads) (Dimension of each head) 32 32 32 32 128 28 28 Table 18: Models Configuration Parameters 33 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference F. Statistics of Datasets LongBench is meticulously designed benchmark suite that evaluates the capabilities of language models in handling extended documents and complex information sequences. This benchmark was created for multi-task evaluation of longcontext inputs and includes 17 datasets covering tasks such as single-document QA (Koˇciský et al., 2018; Dasigi et al., 2021), multi-document QA (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022; He et al., 2018), summarization (Huang et al., 2021; Zhong et al., 2021; Fabbri et al., 2019; Wu et al., 2023), few-shot learning (Li & Roth, 2002; Gliwa et al., 2019; Joshi et al., 2017), synthetic tasks and code generation (Guo et al., 2023; Liu et al., 2024d). The datasets feature an average input length ranging from 1K to 18K tokens, requiring substantial memory for KV cache management. Table 19 shows the statistics of the datasets that we used in our experiments. DATASET # TRAIN # TEST GSM8K (COBBE ET AL., 2021) LONGBENCH (BAI ET AL., 2024) NIAH* (KAMRADT, 2023) 7,473 - - 1,319 4,750 Table 19: Dataset Statistics. # TRAIN and # TEST represent the number of training and test samples, respectively. *: The size of the NIAH test set varies based on the context length and step size, typically around 800 samples per evaluation. G. Prompt Table 20 shows the prompt for the Figure 1 The prompt for demonstration . . . . . . . . . . . . The purple-crested turaco (Gallirex porphyreolophus) or, in South Africa, the purple-crested loerie, (Khurukhuru in the Luvend. (Vend. a) language) is species of bird in the clade Turaco with an unresolved phylogenetic placement. Initial analyses placed the purple-crested turaco in the family Musophagidae, but studies have indicated that these birds do not belong to this family and have been placed in the clade of Turacos with an unresolved phylogeny. It is the National Bird of the Kingdom of Eswatini, and the crimson flight feathers of this and related turaco species are important in the ceremonial regalia of the Swazi royal family. This bird has purple-coloured crest above green head, red ring around their eyes, and black bill. The neck and chest are green and brown. The rest of the body is purple, with red flight feathers. Purple-crested turacos are often seen near water sources, where they can be observed drinking and bathing, which helps them maintain their vibrant plumage. Purple-crested turacos are considered to be large frugivores that are known to carry cycad seeds from various plant species long distances from feeding to nesting sites. After fruit consumption, they regurgitate the seeds intact where they can germinate nearby. G. porphyreolophus primarily consumes fruits whole like many other large frugivores which are suggested to be necessary for effective ecosystem functioning. Among similar turacos, the purple-crested turaco have faster minimum transit times when consuming smaller seed diets than larger seed diets, and G. porphyreolophus has been shown to have significantly faster pulp (seedless fruit masses) transit time than another closely related Turaco when fed only the pulp of larger-seeding fruits than smaller-seeding fruits. In addition to their frugivorous diet, these birds are occasionally seen foraging for other food items such as nuts and leaves, which provide essential nutrients. They are also known to coexist with various other animals, including those that might enjoy strawberries and other similar fruits. The purple-crested turacos role in seed dispersal is crucial, and their interaction with different elements of their habitat, including water and diverse plant materials, highlights their importance in maintaining ecological balance. . . . . . . . . . . . . Table 20: The prompt for demonstration Here we provide the CoT prompt exemplars for GSM8K which is used in section 4.1. 34 ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference GSM8K experiemnt CoT Prompt Exemplars Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6. Question: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5. Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39. Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8. Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9. Question: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29. Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday? Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33. Question: Olivia has $23. She bought five bagels for $3 each. How much money does she have left? Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8. Table 21: GSM8K CoT Prompt Exemplars H. Limitations The major limitation of the ChunkKV is that it uses fixed-size token groups for chunking. While adaptive chunking methods could potentially improve performance, they would introduce significant inference latency. Therefore, finding balance between the chunking method and inference latency is key to improving KV cache compression. I. Licenses For the evaluation dataset, all the datasets, including, GSM8K (Cobbe et al., 2021), LongBench (Bai et al., 2024) are released under MIT license. NIAH (Kamradt, 2023) is released under GPL-3.0 license."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology(Guangzhou), Guangzhou, China",
        "The Hong Kong University of Science and Technology, Hong Kong, China"
    ]
}