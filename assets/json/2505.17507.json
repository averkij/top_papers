{
    "paper_title": "Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph",
    "authors": [
        "Qiaosheng Chen",
        "Kaijia Huang",
        "Xiao Zhou",
        "Weiqing Luo",
        "Yuanning Cui",
        "Gong Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 0 5 7 1 . 5 0 5 2 : r Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph Qiaosheng Chen Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China qschen@smail.nju.edu.cn Weiqing Luo Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China wqluo@smail.nju.edu.cn Kaijia Huang Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China atyou813@gmail.com Yuanning Cui Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China yncui.nju@gmail.com Xiao Zhou Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China xzhou@smail.nju.edu.cn Gong Cheng Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China gcheng@nju.edu.cn Abstract The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first largescale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management."
        },
        {
            "title": "1 Introduction\nThe proliferation of open source models and datasets has empow-\nered researchers and developers to build on existing AI tools, driving\ninnovation across diverse domains. It also highlights the critical role\nof resource sharing platforms in the advancement of AI research,\nwhich requires efficient frameworks to share, search, and manage\nsoftware resources to ensure equitable access and foster collabo-\nration [6, 8, 23, 24, 28]. In this context, Hugging Face has emerged\nas a popular platform, democratizing access to cutting-edge ML\nresources and allowing users to find open models and datasets.",
            "content": "Motivation. Existing platforms for open source resource sharing, such as GitHub and Hugging Face, rely primarily on keywordbased search and simplistic metadata tagging [21]. Although these platforms provide access to vast collections of models and datasets, they do not leverage semantic relations (e.g., model evolution, task dependencies, and user collaboration patterns) between resources during searches. As result, they are unable to integrate structural information to recommend and manage resources effectively. This unstructured paradigm severely limits support for advanced Corresponding author queries and analyses, such as tracing models evolution history, identifying relevant datasets for specific task, or recommending underutilized resources based on structural dependencies. Knowledge graphs (KGs) offer proven solution to this limitation. By explicitly representing entities as nodes and connecting them with edges representing typed relations, KGs enable sophisticated querying and analysis, as demonstrated in domains such as encyclopedias [39], academia [50] and e-commerce [26]. In the context of resource management, structured KG can unify separated metadata, resource dependencies, and user interactions into single heterogeneous graph. This representation supports complex tasks such as recommendation [6, 23, 32], link prediction [1], and node classification [51]. Therefore, our work aims to transform resource sharing platforms into KGs that enhance discovery, reproducibility, and management of open source resources. Our Resources. We construct two foundational resources as illustrated in Figure 1: HuggingKG, large-scale ML resource KG built on the Hugging Face community, and HuggingBench, multitask benchmark designed to evaluate practical challenges in open source resource management. With more than 2.6 million nodes and 6.2 million triples, HuggingKG represents the largest publicly available KG in this domain (to our best knowledge). It integrates MLrelated entities (e.g., Model, Dataset, User, Task) with ML-specific relations such as model evolution (e.g., Adapter, Finetune) and user interactions (e.g., Like, Follow), while incorporating rich textual attributes (e.g., extended descriptions). HuggingBench includes the first cross-type resource recommendation test collection in the ML resource domain and the first model tracing test collection, novel domain-specific tasks that existing benchmarks cannot support. We analyze the unique properties of HuggingKG and perform extensive evaluations with HuggingBench to reveal their special characteristics, which differ from traditional KGs or tasks in domains such as encyclopedias, academia, or e-commerce. The main contributions of this work are as follows. HuggingKG: The first KG for ML resources on Hugging Face, featuring the largest scale among comparable works, and uniquely SIGIR 25, July 1318, 2025, Padua, Italy Qiaosheng Chen et al. Figure 1: Illustration of HuggingKG and HuggingBench. capturing domain-specific relations related to model evolution and user interaction as well as rich textual attributes. HuggingBench: new multi-task benchmark including novel test collections for cross-type resource recommendation, task classification, and model lineage tracing, addressing unmet needs in open source resource management. Availability. HuggingKG and HuggingBench are available on Hugging Face.1 The code for constructing HuggingKG and reproducing the experiments is available on GitHub.2 Both resources are licensed under Apache License 2.0."
        },
        {
            "title": "2 Related Work\nKGs for Resource Management. KGs have been extensively used\nto represent and analyze complex relationships in various domains,\nincluding open source resource management. Previous works such\nas DEKR [6] and MLTaskKG [23] have constructed KGs to support\nrecommendation tasks by capturing relationships among ML re-\nsources. DEKR [6] primarily relies on description enhancement for\nML method recommendation. MLTaskKG [23] constructs an AI task-\nmodel KG by integrating static data to support task-oriented ML/DL\nlibrary recommendation. However, both approaches focus on static\nattributes and a narrow set of relations, failing to capture dynamic\nuser interactions and inter-Model relations. In contrast, as shown in\nTable 1, our proposed HuggingKG is built on the rich metadata pro-\nvided by Hugging Face, offering a large-scale KG with a more exten-\nsive set of relations. In addition to generic relations (e.g., Defined\nFor, Cite), HuggingKG incorporates multiple inter-Model relations\n(i.e., Adapter, Finetune, Merge, and Quantize) and captures user\ninteraction signals (i.e., Publish, Like, and Follow). This enriched\nstructure facilitates a deeper analysis of ML resources and supports\nmore effective recommendation strategies.",
            "content": "KG-based Benchmarks. Various benchmark datasets have been proposed to evaluate KG-based tasks. For example, OAG-Bench [51] provides human-curated benchmark for academic graph mining, focusing on citation and collaboration networks. In the domain of open source resource management, our HuggingBench benchmark distinguishes itself by providing datasets for three IR tasks: resource recommendation, task classification, and model tracing. For resource recommendation, paper2repo [32] introduces distant-supervised recommender system that matches papers with related code repositories. However, it incorporates limited range of entity types that are insufficient to build fine-grained interdependencies. Xu et al. [44] leverages multi-modal features from developers sequential behaviors and repository text to generate relevant and tailored suggestions for developers, yet it does not explicitly construct or exploit structured KG. In contrast, as shown in Table 1, HuggingBench benefits from the inherent structure of HuggingKG that captures rich relational data for recommendation. Furthermore, GRETA [5] and recent efforts in automated categorization [25, 34] address specific tagging/classification tasks. GRETA [5] constructs an Entity Tag Graph (ETG) using the crosscommunity knowledge from GitHub and Stack Overflow, and uses an iterative random walk with restart algorithm to automatically assign tags to repositories. HuggingKG integrates richer textual descriptions and metadata to construct graph that encapsulates finegrained relationships among models and datasets, thereby facilitating multi-label task classification for ML resources. Recent work by Bai et al. [1] uses knowledge-aware heterogeneous graph learning approach to predict links between issues and pull requests on GitHub, effectively capturing complex relational information through metapath aggregation. However, it remains confined to linking IssuePR pairs and does not address the broader challenge of tracking model evolution across ML resources. The novel model tracing task in HuggingBench not only pioneers the exploration of inter-Model relations, but also provides practical insights into the evolution, reuse, and optimization of ML models, thereby supporting more informed decision-making in real-world open source resource management."
        },
        {
            "title": "3 HuggingKG Knowledge Graph\n3.1 KG Construction\nThe construction of HuggingKG follows a principled process that\nincludes defining nodes and edges, crawling and converting data\nfrom the Hugging Face community website, and performing data\nverification and cleaning.",
            "content": "Schema Definition. The nodes and edges in HuggingKG are defined based on our meticulous analysis of the Hugging Face website and general IR needs in real-world scenarios. Figure 2 shows an example model page of Qwen/Qwen2.5-7B-Instruct3 on the Hugging Face website. We can intuitively see that the key 1https://huggingface.co/collections/cqsss/huggingbench-67b2ee02ca45b15e351009a2 2https://github.com/nju-websoft/HuggingBench 3https://huggingface.co/Qwen/Qwen2.5-7B-Instruct Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph SIGIR 25, July 1318, 2025, Padua, Italy Table 1: Comparison between KGs and benchmarks on open source resource management. Source #Nodes #Types #Relations #Edges Key Entities & (Attributes) Model Evolution User Interaction Tasks DEKR [6] MLTaskKG [23] paper2repo [32] GRETA [5] Open academic platforms (e.g., PapersWithCode, GitHub) PapersWithCode, ML/DL Papers, ML/DL Framework Docs GitHub, Microsoft Academic GitHub, Stack Overflow AIPL(Facebook/React) [1] AIPL(vuejs/vue) [1] GitHub GitHub 17,483 159,310 39,600 707,891 97, 49,200 HuggingKG & HuggingBench Hugging Face 2,614,270 5 2 4 4 4 8 117,245 Dataset, Method (Description) 39 628,045 Task, Model, Model Implementation - - 9 9 - - 196, 95,160 30 6,246,353 Paper, Repository Repository, Tag Issue, PR, Repository, User Issue, PR, Repository, User Model, Dataset, User, Task (Description) No No No No No No No No Yes (Star) Yes (Search, Raise, Answer) Recommendation Recommendation Recommendation Tag Assignment Yes Yes Issue-PR Link Prediction Issue-PR Link Prediction Yes (Finetune, Adapter, Merge, Quantize) Yes (Publish, Like, Follow) Recommendation, Classification, Tracing Data Crawling and Conversion. The data crawling process is performed utilizing the huggingface_hub library6 and issuing requests to the relevant API endpoints.7 For Model, Dataset, and Space, we retrieve their complete lists, as well as their key attributes (e.g., tags, download count) and edges through functions in the huggingface_hub library. Specifically, to capture the complete README file for each Model or Dataset node, we send request to download and parse the README.md file, storing its content as the description field in text format. The list of Collection and additional metadata for these four node types are obtained by batch API requests. The lists of User and Organization are extracted from the publisher fields of various nodes, including Model, Dataset, Space, Collection, and Paper. User and Organization nodes are distinguished by their profiles from the API endpoints, and edges such as Follow and Affiliated With are also captured. Paper nodes are identified through arxiv: tags present in Model, Dataset, and Collection nodes. Detailed metadata, such as title, abstract, publication date, and authorship, is retrieved through API calls. For authors registered as User, we established edges between User nodes and their associated Paper nodes. Task nodes are identified from three primary sources: tasks fields in Dataset, pipeline tags fields in Model, and direct definitions from the API. Edges between Task nodes and Model or Dataset nodes, are established by parsing metadata fields, allowing us to align models and datasets with the tasks they serve for. Data Verification and Cleaning. After data crawling is completed, we verify and clean all nodes and edges to ensure the accuracy and completeness of HuggingKG. The verification process involves scanning all collected edges and checking whether the nodes involved in each edge exist within the set of collected nodes. Some models may have been deleted by the publisher, but edges involving those models (e.g., model finetuned from the original) may still appear on Web pages. If any invalid edges are detected, they are removed. The data cleaning process primarily focuses on eliminating outliers in node attributes and removing invalid characters from text fields. Time and Space Cost. To reduce time overhead, the construction process employs multi-threaded parallel processing at each Figure 2: An example model page on Hugging Face. attributes of Model include its name, publisher, tags, and text description on the model card, etc. The key relations that can be observed on the page include Finetune between Models and Like between User and Model, etc. Through an analysis of pages such as models, datasets, and spaces, we identify 8 types of nodes and 30 types of edges between them, as illustrated in Figure 3. In addition, we determine the key attributes associated with each node type. For Model, Dataset, Space, and Collection, we adopt the publisher/name format used by Hugging Face serving as the primary identifier. To address potential name duplication across different node types (e.g., model4 and dataset5 sharing the same publisher/name), we introduce type prefix for each node. For example, Model might be represented as model : Qwen/Qwen2.5-7B-Instruct, ensuring its uniqueness. This string is used as unique identifier to detect and prevent duplication during subsequent data crawling and processing. 4https://huggingface.co/aai540-group3/diabetes-readmission 5https://huggingface.co/datasets/aai540-group3/diabetes-readmission 6https://huggingface.co/docs/hub/index 7https://huggingface.co/docs/hub/api SIGIR 25, July 1318, 2025, Padua, Italy Qiaosheng Chen et al. Figure 3: The schema graph of HuggingKG, along with the quantity and proportion of each node and edge type. (a) 𝑃 (𝐴𝐵) for within-type nodes. Figure 5: Description length of Model and Dataset. (b) 𝑃 (𝐴𝐵) for cross-type nodes. Figure 4: Conditional probability 𝑃 (𝐴𝐵) of user co-likes. step. Consequently, the entire process takes approximately 20 hours and the storage of node attributes and edges in the graph amounts to around 5.8 GB. This indicates that the graph can be updated daily. The current version of HuggingKG is constructed with data collected on December 15, 2024."
        },
        {
            "title": "3.2 Statistics and Analysis\nDistribution of Node and Relation Types. Figure 3 shows the dis-\ntribution of node and relation types in HuggingKG. Of the 2,614,270\nnodes, Model makes up 46.0%, followed by User (27.9%) and Space\n(11.8%). Smaller node types such as Task (0.0%) and Paper (0.6%)\nare important for identifying the characteristics of the resources.\nAmong the 6,246,353 edges, user interactions such as Like (31.5%)\nand Publish (28.8%) dominate. Edges about model evolution, e.g.,",
            "content": "Adapter (2.5%), Finetune (1.7%), highlight the technical connections in the community. These patterns reflect community-driven environment focused on user activity and model interoperability. Resource Contextualization. To analyze patterns of user interest, we compute the conditional probabilities 𝑃 (𝐴𝐵) of user co-likes for Model and Dataset across categories, as shown in Figure 4. NLP is the most popular research area, with users from other fields showing strong interest in NLP resources. RL, in particular, is highly involved in NLP, likely due to advances in reinforcement learning with human feedback (RLHF) with large language models (LLMs). The multimodal community is also interested in NLP, CV, and audio resources. comparison of Figures 4a and 4b shows clear differences in interest distribution, both within and between resource types. For example, strong interest in NLP models is from users who like robotics datasets instead of NLP or RL, reflecting the intersection of embodied intelligence and LLMs. These patterns suggest that HuggingKG captures valuable information on user interests, useful for tasks like resource recommendation and trend prediction. Textual Attributes. In HuggingKG, the average length of the description is 270.2 words for Model and 134.1 words for Dataset, much longer than 8.1 words in [6]. The longest Model description exceeds 2.5M words, and the longest Dataset description exceeds 400K words. As shown in Figure 5, the description length distribution exhibits long tail, with 57.2% of Model and 33.4% of Dataset lacking descriptions. Peaks at 700800 words for Model and 1500 1600 words for Dataset are mostly due to template usage. These patterns highlight the challenges of incomplete documentation and information overload, underlining the need for methods that improve metadata quality by combining textual and graph-based data. Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph SIGIR 25, July 1318, 2025, Padua, Italy Table 2: Test collection for resource recommendation. Avg. means the average number of interactions per user. Table 3: External KGs for KG-based recommendation. #Users #Items #Interactions Avg. Model Dataset Space 29,720 5,072 14,171 16,200 3,634 5, 667,365 100,561 297,294 22.46 19.83 20.98 Total 38,624 25,080 1,065, 27."
        },
        {
            "title": "4.1 Resource Recommendation\nApplication Scenario. For ML practitioners, selecting an appropri-\nate pre-trained model from thousands of options on platforms like\nHugging Face is a significant challenge. For example, an NLP prac-\ntitioner working on sentiment analysis requires a model tailored to\ntheir dataset and task. The resource recommendation task addresses\nthis by recommending models based on user interaction history.\nIt enables practitioners to efficiently identify and deploy the most\nsuitable model, reducing time and effort in resource discovery.",
            "content": "Task Definition. The resource recommendation task can be framed as hybrid problem that integrates general collaborative filtering, social recommendation, and KG-based recommendation. For general collaborative filtering, let = {𝑢1, 𝑢2, ..., 𝑢𝑀 } represent the set of users and = {𝑖1, 𝑖2, ..., 𝑖𝑁 } represent the set of items (e.g., Model, Dataset, and Space). The user-item interaction matrix = {𝑦𝑢𝑖 𝑢 U, 𝑖 I} is captured from Like edges, where 𝑦𝑢𝑖 = 1 if User 𝑢 liked item 𝑖, and 𝑦𝑢𝑖 = 0 otherwise. This matrix serves as the foundation for general recommendation systems. Social recommendation is facilitated by using social relationships between users represented by graph = {𝑠𝑢𝑣 𝑢, 𝑣 U} where 𝑠𝑢𝑣 = 1 indicates Follow edge between User nodes 𝑢 and 𝑣. KG-based recommendation draws on structured external knowledge from the HuggingKG knowledge graph, denoted = (V, E), which encodes external entities (e.g., Paper, Collection) and their interrelations as edges (a.k.a. triples in this context). The objective of the recommendation task is to learn prediction function ˆ𝑦𝑢𝑖 = (𝑢, 𝑖), where ˆ𝑦𝑢𝑖 is the predicted probability that user 𝑢 interacts with item 𝑖. The goal is to rank the items for each user based on these predicted probabilities, prioritizing those with the highest likelihood of interaction to ensure that the most relevant items appear at the top of the recommendation list. Dataset Construction. Based on HuggingKG, we extract bipartite graph representing the Like edges between User nodes and three types of item nodes (Model, Dataset, and Space). Following standard practices in recommendation tasks, we derive 5-core subgraph (where each node has at least 5 edges to other nodes) from this bipartite graph. Subsequently, we partition the liked-item list for each user into training, validation, and test sets with split 2hop 1hop Homo Publish #Nodes #Edges (Triples) #Relations 1,242,578 3,063,081 27 462,213 1,180,911 25,080 28,346 7 31,754 25,078 6 Table 4: Test collection for task classification. #Nodes #Edges #Labels #Train #Valid #Test"
        },
        {
            "title": "Model\nDataset",
            "content": "145,466 6,969 131,274 -"
        },
        {
            "title": "Total",
            "content": "152,435 166,199 50 48 52 103,276 6,792 19,167 1, 27,974 708 110,068 20,668 28,682 of 60%, 20%, and 20%, respectively. Table 2 shows statistics of the resource recommendation test collection. To support social recommendation, we introduce 84,913 Follow edges between User nodes in the test collection as external Social information. To support KG-based recommendation, we consider three types of subgraphs of HuggingKG as external KGs: 1hop/2hop are 1-step/2-step neighborhood subgraphs from itemaligned KG nodes, Homo is the subgraph induced from all item nodes, and Publish is the relation-specific subgraph induced from the Publish edges. Table 3 shows statistics of these external KGs."
        },
        {
            "title": "4.2 Task Classification\nApplication Scenario. Platform curators face the challenge of or-\nganizing and tagging models and datasets to improve searchability\nand usability. For example, a newly uploaded model without clear\ntask annotations becomes difficult for users to discover. The task\nclassification task automates this process by analyzing the metadata\nand structure information from the platforms to classify the mod-\nels into relevant tasks (e.g., “text classification” or “named entity\nrecognition”). This ensures proper categorization and improves\naccessibility for users seeking specific features.",
            "content": "Task Definition. The task of classifying Model and Dataset nodes by their associated Task (according to Defined For edges) is framed as multi-label attributed node classification problem, where each instance can be assigned multiple labels from set = {𝑙1, 𝑙2, . . . , 𝑙𝐾 }. The input consists of graph = (V, E), where the nodes represent Model and Dataset nodes, and the edges captured include Finetune edges between Model nodes, Trained Or Finetuned On edges between Model nodes and Dataset nodes, etc. Additionally, feature matrix 𝑑 encodes node features such as textual descriptions or metadata. The output for each node 𝑣 is binary vector y𝑣 {0, 1}𝐾 , where 𝑦𝑣,𝑘 = 1 indicates an association with task 𝑡𝑘 . The goal is to learn function ˆy𝑣 = (G, X) that accurately predicts task labels, using both graph structure and textual information, while optimizing precision and recall across all labels. Dataset Construction. We first select the Model and Dataset nodes in the graph that have associated task labels, based on the Defined For edges between the Model/Dataset nodes and the Task nodes in HuggingKG. We then add all edges between these SIGIR 25, July 1318, 2025, Padua, Italy Qiaosheng Chen et al. Table 5: Test collection for model tracing. #Train #Valid #Test Adapter Finetune Merge Quantize 565 15,639 138 178 65 1,944 24 16 80 1,935 17 Total 16,520 2,049 2,055 nodes and remove any isolated nodes. To better align the task with the scenario of helping the community website automatically predict the task types of newly created models or datasets, we divide the data into training, validation, and test sets based on the creation dates of Model and Dataset. Due to the faster release rate of models compared to datasets, we set longer time range for Dataset than for Model. Specifically, we select Model nodes created between 2024-08-15 and 2024-10-15, and Dataset nodes created between 2024-04-15 and 2024-08-15 for the validation set. The test set comprises Model nodes created between 2024-10-15 and 2024-12-15, as well as Dataset nodes created between 2024-08-15 and 2024-12-15. The remaining Model and Dataset nodes, released before these periods, are used as the training set. Table 4 shows statistics of the task classification test collection."
        },
        {
            "title": "4.3 Model Tracing\nApplication Scenario. Researchers often need to investigate the\nlineage and dependencies of model architectures, such as under-\nstanding the evolution of GPT-3. The model tracing task facilitates\nthis by tracing the base model of a model, identifying base mod-\nels (e.g., GPT-2) and related variants, along with their associated\ndatasets and tasks. This capability supports in-depth analysis of\nmodel development histories and their connections within the com-\nmunity, aiding reproducibility and innovation.",
            "content": "Task Definition. The model tracing task can be formally defined as specialized link prediction problem within heterogeneous graph = (V, E), where represents the set of nodes, including Model and other nodes (e.g., Dataset, Space), and represents the set of edges. Each edge is represented as triple (ℎ, 𝑟, 𝑡), where ℎ is the head node, 𝑟 is the relation (i.e., Adapter, Finetune, Merge or Quantize between Model nodes), and 𝑡 is the tail node. Unlike general link prediction tasks, this task specifically predicts the reverse relation between Model nodes: given relation 𝑟 and tail node 𝑡, the goal is to predict the corresponding head node ℎ that completes the triple (ℎ, 𝑟, 𝑡). The output of the task is probability distribution [0, 1] on all candidate head nodes, with 𝑦ℎ indicating the probability that node ℎ is the correct match for the triple (ℎ, 𝑟, 𝑡). The objective is to learn function ˆy(𝑟,𝑡 ) = (G, 𝑟, 𝑡) that maximizes the likelihood of correctly identifying the true head node ℎ for the triple (ℎ, 𝑟, 𝑡). Dataset Construction. Due to the large size of the complete HuggingKG, we extract subgraph to construct the test collection for model tracing. We select the Model nodes associated with the task type text classification. Next, we identify the triples where these nodes appear as the head or tail node, incorporating the other node in the triple into the node set V. We then include all edges between these nodes to form the edge set E. The resulting subgraph consists of 121,404 nodes, 339,429 edges, and 30 relations. We subsequently partition the inter-Model edges into training, validation, and test sets with split of 80%, 10%, and 10%, respectively. Table 5 shows statistics of the model tracing test collection."
        },
        {
            "title": "5.1 Resource Recommendation\nEvaluation Metrics. Following standard practices in recommender\nsystems [17, 30], we use Recall@𝐾 and NDCG@𝐾 as evaluation\nmetrics to evaluate the ranked list of recommended items. The\nvalue of 𝐾 is set to 5, 10, 20, and 40 for the evaluation.",
            "content": "Baselines. Following common practice in recommendation [29, 30], we select six representative general collaborative filtering methods, including four graph-based methods: LightGCN [17], HCCF [43], SimGCL [49], and LightGCL [4] and two representation learning methods: AutoCF [42] and DCCF [31]. These methods rely on the bipartite user-item graph for interaction modeling. We also adopt two state-of-the-art social recommendation methods: MHCN [48] and DSL [40]. These methods introduce the social graph to capture richer user preferences. In addition, we employ three KG-based recommendation methods: KGIN [41], KGCL [47], and KGRec [46]. These methods utilize unified heterogeneous structure that aligns items in the bipartite graph with nodes from external KGs. We use SSLRec8 [30] to implement the above methods. Implementation Details. For all baseline models, the representation dimension is set to 64. Each model is trained for up to 100 epochs, with fixed batch size of 4,096 and early stopping based on MRR@5 on the validation set. Validation is performed every 3 epochs, and the patience of early stopping is set to 5. We perform grid search to select the optimal learning rate from {1e-3, 1e-4, 1e-5} and the number of GNN layers from {2, 3}. Evaluation Results. As shown in Table 6, among the general collaborative filtering methods, LightGCL achieves the best performance, suggesting that interaction graph augmentation through singular value decomposition provides strong baseline. For social recommendation, MHCN outperforms DSL by +4.80% in Recall@5, showing the effectiveness of multi-channel hypergraph convolution and self-supervised learning. However, it surpasses only two of the six general collaborative filtering methods, indicating that adding social data does not secure improvement. For KG-based recommendation, KGCL consistently outperforms others in all subgraph types, highlighting the benefits of KG augmentation and contrastive learning. Homo subgraph yields the best results, suggesting that item-related nodes provide high-quality context. In sparse graphs, KGCL is robust due to the use of contrastive learning with well-defined positive and negative pairs, which improves the quality of learned representations. In contrast, KGINs performance is highly sensitive to its negative sampling strategy and hyperparameter setting, often instable in sparse or cold-start scenarios. KGRec relies heavily on the quality of entity representations and performs moderately when the graph density is low. 8https://github.com/HKUDS/SSLRec Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph SIGIR 25, July 1318, 2025, Padua, Italy Table 6: Evaluation results of resource recommendation. Method KG Recall@5 Recall@10 Recall@20 Recall@40 NDCG@5 NDCG@10 NDCG@20 NDCG@ 0.0856 0.0834 0.0999 0.1033 0.1003 0.0985 0.0979 0.0932 - - - - - - General Collaborative Filtering LightGCN HCCF SimGCL LightGCL AutoCF DCCF Social Recommendation MHCN DSL KG-Based Recommendation KGIN KGCL KGRec Social 1hop 0.0001 0.0993 0.0558 KGIN KGCL KGRec KGIN KGCL KGRec KGIN KGCL KGRec 2hop Homo Publish 0.0002 0.1007 0.0597 0.0061 0.1054 0.0628 0.0002 0.1036 0.0609 0.1301 0.1254 0.1515 0.1558 0.1530 0.1493 0.1490 0. 0.0004 0.1490 0.0897 0.0004 0.1510 0.0941 0.0096 0.1578 0.0985 0.0003 0.1543 0.0941 0.1932 0.1820 0.2186 0.2228 0.2190 0.2167 0.2162 0. 0.0010 0.2135 0.1395 0.0008 0.2165 0.1423 0.0146 0.2237 0.1476 0.0007 0.2205 0.1385 0.2759 0.2504 0.3010 0.3017 0.3039 0.3003 0.3007 0. 0.0017 0.2918 0.2076 0.0016 0.2959 0.2122 0.0219 0.3059 0.2106 0.0016 0.3011 0.2002 0.0868 0.0847 0.0998 0.1035 0.1012 0.0983 0.0998 0. 0.0002 0.1009 0.0575 0.0003 0.1016 0.0625 0.0065 0.1058 0.0638 0.0002 0.1038 0.0636 0.1003 0.0975 0.1158 0.1198 0.1174 0.1142 0.1154 0. 0.0003 0.1160 0.0681 0.0004 0.1170 0.0729 0.0076 0.1220 0.0751 0.0003 0.1195 0.0734 0.1192 0.1143 0.1358 0.1398 0.1371 0.1343 0.1353 0. 0.0005 0.1351 0.0832 0.0005 0.1364 0.0872 0.0091 0.1416 0.0898 0.0004 0.1392 0.0863 0.1413 0.1328 0.1581 0.1611 0.1598 0.1567 0.1579 0. 0.0007 0.1563 0.1014 0.0007 0.1579 0.1057 0.0111 0.1637 0.1067 0.0007 0.1609 0.1027 Moreover, KG-based methods exhibit higher variance, with Recall@5 having standard deviation of 0.0431, compared to 0.0084 and 0.0033 for social and collaborative filtering methods. This suggests that applying KG-based methods requires careful selection and tuning of the specific approach based on data characteristics such as graph density and relation sparsity. Comparison with Other Benchmarks. To investigate the underwhelming performance of social recommendation methods, we compare the social relation statistics of our test collection with those of the LastFM, Douban and Yelp datasets where MHCN has demonstrated strong performance [48]. LastFM includes 1, 892 users with 25, 434 relations, Douban has 2, 848 users with 35, 770 relations, and Yelp contains 19, 535 users with 864, 157 relations, all of which are considerably denser than our test collection, which consists of 38, 624 users and 84, 913 relations. MHCN relies on dense connections to form meaningful hyperedges, facilitate contrastive learning, and effectively propagate signals. Similarly, despite KGINs strong performance on the Amazon-Book, LastFM, and Alibaba datasets [41], it performs the worst on our test collection. possible factor is the KG sparsity: Amazon-Book has 2, 557, 746 triples and 88, 572 nodes, LastFM has 464, 567 triples and 58, 266 nodes, and Alibaba has 279, 155 triples and 59, 156 nodes, while our dataset is significantly sparser  (Table 3)  . KGIN relies on dense multi-hop paths ( 3 hops) for user-item semantics, which are often missing in sparse KGs. Compared with the aforementioned benchmarks, the metrics of various baselines on our test collection are generally lower, reflecting the difficulty of this recommendation task. In summary, because our test collection provides more domain-specific external relations, which are relatively sparse and specialized compared to relations aligned from huge KGs such as Wikidata, it provides new challenges and benchmarks for recommendation methods that use special additional information for domain adaptation."
        },
        {
            "title": "5.2 Task Classification\nEvaluation Metrics. Following standard practices in multi-label\nnode classification [7], we use Micro-F1.",
            "content": "Baselines. We select nine representative GNN-based methods for node classification. GCN [19], GAT [38], and GraphSAGE [16] establish fundamental architectures with different aggregation schemes. Memory and computation optimization approaches are represented by GraphSAINT [16], RevGCN [22], and RevGAT [22]. The remaining methods (APPNP [20], GRAND [14], GCNII [9]) focus on addressing specific challenges like label propagation, robustness, and over-smoothing in deep GNNs. We use CogDL [7]9 to implement the above methods. For node feature initialization, we try the following settings: Binary: Binary one-dimensional vectors for distinguishing between Model (0) and Dataset (1). Pre-trained Text Embeddings: Embeddings of node description attributes generated using base versions of BERT10 and BGE.11 Finetuned Text Embeddings: Node description attribute embeddings derived from the aforementioned models after finetuning 9https://github.com/THUDM/cogdl 10https://huggingface.co/google-bert/bert-base-uncased 11https://huggingface.co/BAAI/bge-base-en-v1. SIGIR 25, July 1318, 2025, Padua, Italy Qiaosheng Chen et al. Table 7: Evaluation results (Micro-F1) of task classification. Table 8: Evaluation results of model tracing. Method Binary BERT BERT (ft) BGE BGE (ft) Method MRR Hit@1 Hit@3 Hit@5 Hit@ GCN GAT GRAND GraphSAGE APPNP GCNII GraphSAINT RevGCN RevGAT 0.0662 0.0390 0.1228 0.1800 0.0448 0.1149 0.0579 0.1071 0.0335 0.7620 0.5105 0.1297 0.5341 0.7297 0.6456 0.2703 0.6763 0.7412 0.8291 0.8125 0.6089 0.8845 0.8304 0.8836 0.8342 0.8851 0.8849 0.7411 0.5444 0.2646 0.8199 0.7571 0.7779 0.0540 0.8039 0.7569 0.8522 0.8261 0.4532 0.8830 0.8419 0.8802 0.8251 0.8770 0. for 1 epoch. We perform grid search to select the optimal learning rate from {1e-4, 5e-5, 1e-5} and batch size from {8, 16}. The resulting models are denoted as BERT (ft) and BGE (ft). Implementation Details. The hidden size dimension is set to 1,024 for all baselines based on GNN, except GAT and RevGAT, for which it is set to 256 to accommodate GPU memory limitations. Each GNN model is trained for up to 500 epochs, with early stopping based on the Micro-F1 score on the validation set. The patience of validation early stopping is set to 100 epochs. For the training process, the batch size is set at 4,096. We perform grid search to select the optimal learning rate from {1e-3, 1e-4, 1e-5}, weight decay from {0, 1e-5, 1e-4}, and the number of GNN layers from {2, 3, 4}. Evaluation Results. As shown in Table 7, all models demonstrate limited performance with binary features, with GraphSAGE achieving the best score of only 0.1800. When using pre-trained embeddings, we observe significant improvements across most models, with GCN achieving 0.7620 with BERT embeddings and GraphSAGE reaching 0.8199 with BGE embeddings. For finetuned embeddings, the performance improves substantially, with RevGCN achieving 0.8851 and GraphSAGE reaching 0.8830 with finetuned BERT and BGE respectively, followed closely by GCNII. The experimental results clearly demonstrate that using pre-trained text embeddings as initial features significantly enhances model performance, with an average improvement of 0.5391 in the Micro-F1 score. Furthermore, finetuning the embeddings for task-specific feature learning consistently brings additional gains across all methods and embeddings, with an average improvement of 0.2543 and 0.1871 over BERT and BGE pre-trained embeddings, respectively, suggesting that incorporating domain knowledge through finetuning is important for improving performance in this task. We notice an interesting pattern where simpler architectures like GraphSAGE outperform sophisticated models such as GCNII and RevGAT, particularly with BGE embeddings. This suggests that complex architectural designs might not always be beneficial when working with high-quality pre-trained embeddings, as they may introduce noise or over-smoothing in the feature space. Comparison with Other Benchmarks. We find that samplingbased models such as GRAND and GraphSAINT exhibit unexpectedly poor performance with pre-trained embeddings (0.1297 and 0.2703 with BERT, respectively), despite their effectiveness [7] in other multi-label node classification datasets such as ogbn-arxiv [18]. This performance degradation might be attributed to their sampling strategies potentially disrupting the semantic relations encoded in Supervised RESCAL TransE DistMult ComplEx ConvE RotatE HittER Unsupervised ULTRA KG-ICL 0.2694 0.5589 0.2050 0.1807 0.4739 0.5317 0. 0.2380 0.4496 0.1421 0.1109 0.3766 0.4195 0.2900 0.2667 0.6321 0.2321 0.2122 0.5119 0.6029 0.4078 0.2929 0.6973 0.2735 0.2599 0.5903 0.6803 0.4657 0.3470 0.7562 0.3324 0.3066 0.6735 0.7392 0.5314 0.3373 0.4008 0.1440 0. 0.4803 0.3792 0.5309 0.4854 0.6672 0.5938 the pre-trained embeddings. Compared with other commonly used benchmarks, the metrics of our test collection of various baselines are at medium level [7]. In summary, for such new domain-specific classification task, it not only poses new challenges to text embedding models, but also generates new issues worthy of consideration for the GNN-based node classification methods."
        },
        {
            "title": "5.3 Model Tracing\nEvaluation Metrics. Following standard practices in link predic-\ntion [11, 35], we use Mean Reciprocal Rank (MRR) and Hit@𝐾 as\nevaluation metrics for the model tracing experiments, with 𝐾 set\nto 1, 3, 5, and 10. As mentioned in Section 4.3, we only evaluate the\nresults considering the given Model node as the tail node 𝑡 and the\ngiven relation 𝑟 to predict the head node ℎ.",
            "content": "Baselines. According to common practice in the field of link prediction [3, 11, 15], we select seven supervised methods, including five embedding-based methods: RESCAL [27], TransE [2], DistMult [45], ComplEx [37], and RotatE [35] and two deep neural network-based methods: CNN-based ConvE [12] and transformerbased HittER [10]. Among these, TransE is one of the most classic methods, which simply represents nodes as continuous vectors in the embedding space and represents the relations between nodes as translation vectors from the head node to the tail node. We also adopt two recent unsupervised KG foundation models, ULTRA [15] and KG-ICL [11]. These models represent relations using relation graphs and prompt graphs, respectively, which encode nodes and their scores. Since they do not rely on specific learnable representation vectors for nodes or relations, they can directly utilize frozen pre-trained models on our test collection. We use LibKGE [3]12 to implement supervised methods and use the official code1314 of the two unsupervised models. Implementation Details. For all the baseline methods, the representation dimension is set to 64. Each baseline is trained for up to 100 epochs, with early stopping based on the MRR score on the validation set. Validation is performed every 3 epochs, and the patience of early stopping is set to 5. For training process, the batch size is fixed at 256. We perform grid search to select the optimal 12https://github.com/uma-pi1/kge 13https://github.com/DeepGraphLearning/ULTRA 14https://github.com/nju-websoft/KG-ICL Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph SIGIR 25, July 1318, 2025, Padua, Italy anticipated research user community spans multiple disciplines, including IR, ML, data mining, and software engineering, with substantial base of current users. This community is expected to grow significantly in the coming years as the demand for structured knowledge and robust evaluation frameworks increases, driven by advances in the communities of LLM and AI tools. Taken together, these contributions position HuggingKG and HuggingBench as enduring assets for the academic and industrial communities. Limitations and Future Work. Our resources have the following limitations to be addressed in future work. First, the data is currently limited to Hugging Face, which restricts the diversity of entities and relations. In future work, we plan to expand HuggingKG to include additional resource platforms such as GitHub and Kaggle to introduce more types of cross-platform entities and relations. This expansion will enable applications such as cross-domain entity alignment. Second, the test collections in HuggingBench are automatically generated from HuggingKG and lack tasks that require manual annotation, such as question answering (QA), limiting the breadth of the benchmark. To address this, we will focus on designing more advanced LLM agents to support annotation for IR tasks including QA and retrieval, or combining graph-based methods and manual annotations for richer benchmarks. Acknowledgments This work was supported by the Postgraduate Research & Practice Innovation Program of Jiangsu Province. learning rate from {1e-2, 1e-3, 1e-4, 1e-5} and the number of GNN layers from {2, 3}. Evaluation Results. The results for the model tracing task are presented in Table 8. Supervised models, particularly TransE, outperform all other methods, achieving the highest scores in all metrics. In contrast, bilinear models like RESCAL and DistMult, and neural models like ConvE, underperform due to their reliance on multiplicative interactions or convolutions, which fail to capture critical dependencies. In conclusion, simpler geometric transformations are better suited for this task. For unsupervised models, KG-ICL excels in MRR and Hit@1, while ULTRA performs better in Hit@3/5/10 but struggles with accuracy in top-ranked predictions. This difference can be attributed to that ULTRA performs message passing across the entire graph, while KG-ICL initiates message passing from the head node, expanding one hop at each layer, which allows it to focus more on nearby nodes. As result, KG-ICL achieves higher Hit@1 score. In conclusion, unsupervised models exhibit good trade-off between precision and recall for this task. Comparison with Other Benchmarks. In contrast to the results of widely used KG link prediction benchmarks such as FB15k237 [36] and WN18RR [13], TransE performs the best among supervised methods. Although TransE is classic and effective method, its linear modeling struggles with one-to-many relations (where single head node and relation can correspond to multiple tail nodes) [35]. Consequently, many subsequent models focus on addressing this issue. We analyze the test sets of FB15k-237, WN18RR and our dataset, counting the number of answers corresponding to each (head node, relation) pair. The results show that FB15k-237 has an average of 399.82 answers, WN18RR has 24.57, while our dataset has only 1.04. It indicates that the answers in our test set are nearly unique, making TransE sufficient for our needs. More complex methods tend to focus on distinguishing similar nodes at the many end of one-to-many relations, resulting in suboptimal performance. Compared to the metrics of these baselines on FB15k-237 and WN18RR, the metrics on our test collection are generally lower, suggesting that our test collection is overall more challenging. In summary, the unique distribution of relations in our test collection makes model tracing difficult for existing methods and raises new requirements for approaches designed for this task."
        },
        {
            "title": "6 Conclusion\nPredicted Impact. HuggingKG and HuggingBench will enable im-\npactful IR research activities, particularly in advancing resource\nrecommendation and automatic management within open source\ncommunities. These resources not only advance well-established re-\nsearch areas (e.g., recommendation, classification, and KG), but also\nfoster emerging domains related to LLM. For example, by introduc-\ning HuggingKG into the model selection step of methods such as\nHuggingGPT [33], it has the potential to improve the integration of\nAI tools for LLMs. Given their foundational nature, these resources\nare designed to remain relevant and useful over an extended period.\nTo ensure long-term sustainability, we plan to release an updated\nversion of HuggingKG periodically and have provided open source\ncode for KG and benchmark construction, allowing the community\nto maintain and customize the resources for various use cases. The",
            "content": "SIGIR 25, July 1318, 2025, Padua, Italy Qiaosheng Chen et al. References [1] Shuotong Bai, Huaxiao Liu, Enyan Dai, and Lei Liu. 2024. Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous Graph Learning. IEEE Trans. Software Eng. 50, 7 (2024), 19011920. doi:10.1109/TSE.2024.3408448 [2] Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Translating Embeddings for Modeling MultiOksana Yakhnenko. 2013. relational Data. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. 27872795. https://proceedings.neurips.cc/paper/2013/hash/ 1cecc7a77928ca8133fa24680a88d2f9-Abstract.html [3] Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, Patrick Betz, and Rainer Gemulla. 2020. LibKGE - knowledge graph embedding library for reproducible research. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020. 165174. doi:10.18653/V1/2020.EMNLP-DEMOS.22 [4] Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. 2023. LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. https://openreview.net/forum?id=FKXVK9dyMM [5] Xuyang Cai, Jiangang Zhu, Beijun Shen, and Yuting Chen. 2016. GRETA: GraphBased Tag Assignment for GitHub Repositories. In 40th IEEE Annual Computer Software and Applications Conference, COMPSAC 2016, Atlanta, GA, USA, June 10-14, 2016. 6372. doi:10.1109/COMPSAC.2016.124 [6] Xianshuai Cao, Yuliang Shi, Han Yu, Jihu Wang, Xinjun Wang, Zhongmin Yan, and Zhiyong Chen. 2021. DEKR: Description Enhanced Knowledge Graph for Machine Learning Method Recommendation. In SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021. 203212. doi:10.1145/3404835.3462900 [7] Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Zhongming Yu, Hengrui Zhang, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Yuxiao Dong, Yang Yang, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, and Jie Tang. 2023. CogDL: Comprehensive Library for Graph Deep Learning. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023. 747758. doi:10.1145/3543507. [8] Adriane Chapman, Elena Simperl, Laura Koesten, George Konstantinidis, LuisDaniel Ibáñez, Emilia Kacprzak, and Paul Groth. 2020. Dataset search: survey. VLDB J. 29, 1 (2020), 251272. doi:10.1007/S00778-019-00564-X [9] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. 2020. Simple and Deep Graph Convolutional Networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). 17251735. http: //proceedings.mlr.press/v119/chen20v.html [10] Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, and Yangfeng Ji. 2021. HittER: Hierarchical Transformers for Knowledge Graph Embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. 1039510407. doi:10.18653/V1/2021.EMNLP-MAIN.812 [11] Yuanning Cui, Zequn Sun, and Wei Hu. 2024. Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. http://papers.nips.cc/paper_files/paper/2024/hash/ 0d70af566e69f1dfb687791ecf955e28-Abstract-Conference.html [12] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2D Knowledge Graph Embeddings. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. 18111818. doi:10.1609/AAAI.V32I1.11573 [13] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2D Knowledge Graph Embeddings. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018. AAAI Press, 18111818. doi:10.1609/AAAI.V32I1.11573 [14] Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov, and Jie Tang. 2020. Graph Random Neural Networks for Semi-Supervised Learning on Graphs. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. https://proceedings.neurips.cc/ paper/2020/hash/fb4c835feb0a65cc39739320d7a51c02-Abstract.html [15] Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. 2024. Towards Foundation Models for Knowledge Graph Reasoning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. https://openreview.net/forum?id=jVEoydFOl9 [16] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 10241034. https://proceedings.neurips. cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html [17] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020. 639648. doi:10.1145/3397271.3401063 [18] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. [19] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. https://openreview.net/forum?id=SJU4ayYgl [20] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. 2019. Predict then Propagate: Graph Neural Networks meet Personalized PageRank. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. https://openreview.net/forum?id=H1gL-2A9Ym [21] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, and Thomas Wolf. 2021. Datasets: Community Library for Natural Language Processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2021, Online and Punta Cana, Dominican Republic, 7-11 November, 2021. 175184. doi:10.18653/V1/2021.EMNLP-DEMO. [22] Guohao Li, Matthias Müller, Bernard Ghanem, and Vladlen Koltun. 2021. Training Graph Neural Networks with 1000 Layers. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139). 64376449. http://proceedings.mlr.press/v139/li21o.html [23] Mingwei Liu, Chengyuan Zhao, Xin Peng, Simin Yu, Haofen Wang, and Chaofeng Sha. 2023. Task-Oriented ML/DL Library Recommendation Based on Knowledge Graph. IEEE Trans. Software Eng. 49, 8 (2023), 40814096. doi:10.1109/TSE.2023. 3285280 [24] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi Wu, Enrico Shippole, Kurt D. Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, and Sara Hooker. 2024. large-scale audit of dataset licensing and attribution in AI. Nat. Mac. Intell. 6, 8 (2024), 975987. doi:10.1038/ S42256-024-00878-8 [25] Phuong T. Nguyen, Juri Di Rocco, Claudio Di Sipio, Mudita Shakya, Davide Di Ruscio, and Massimiliano Di Penta. 2024. Automatic Categorization of GitHub Actions with Transformers and Few-shot Learning. In Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM 2024, Barcelona, Spain, October 24-25, 2024, Xavier Franch, Maya Daneva, Silverio Martínez-Fernández, and Luigi Quaranta (Eds.). ACM, 468474. doi:10.1145/3674805.3690752 [26] Jianmo Ni, Jiacheng Li, and Julian J. McAuley. 2019. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. Association for Computational Linguistics, 188197. doi:10.18653/V1/D19-1018 [27] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. Three-Way Model for Collective Learning on Multi-Relational Data. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011. 809816. https://icml.cc/2011/papers/438_icmlpaper. pdf [28] Norman W. Paton, Jiaoyan Chen, and Zhenyu Wu. 2023. Dataset Discovery and Exploration: Survey. ACM Comput. Surv. 56, 4 (2023), 37 pages. doi:10.1145/ 3626521 [29] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464 3475. [30] Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, and Chao Huang. 2024. SSLRec: Self-Supervised Learning Framework for Recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, WSDM 2024, Merida, Mexico, March 4-8, 2024. 567 575. doi:10.1145/3616855.3635814 Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph SIGIR 25, July 1318, 2025, Padua, Italy [31] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Disentangled Contrastive Collaborative Filtering. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023. 11371146. doi:10.1145/3539618.3591665 [32] Huajie Shao, Dachun Sun, Jiahao Wu, Zecheng Zhang, Aston Zhang, Shuochao Yao, Shengzhong Liu, Tianshi Wang, Chao Zhang, and Tarek F. Abdelzaher. 2020. paper2repo: GitHub Repository Recommendation for Academic Papers. In WWW 20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020. 629639. doi:10.1145/3366423. [33] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT: solving AI tasks with chatgpt and its friends in hugging face. In Proceedings of the 37th International Conference on Neural Information Processing Systems. [34] Claudio Di Sipio, Riccardo Rubei, Juri Di Rocco, Davide Di Ruscio, and Phuong T. Nguyen. 2024. Automated categorization of pre-trained models in software engineering: case study with Hugging Face dataset. In Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, EASE 2024, Salerno, Italy, June 18-21, 2024. ACM, 351356. doi:10.1145/3661167. 3661215 [35] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. https://openreview.net/forum?id=HkgEQnRqYQ [36] Kristina Toutanova and Danqi Chen. 2015. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, CVSC 2015, Beijing, China, July 26-31, 2015. Association for Computational Linguistics, 5766. doi:10. 18653/V1/W15-4007 [37] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48). 20712080. http://proceedings.mlr.press/v48/trouillon16.html [38] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. https://openreview.net/forum? id=rJXMpikCZ [39] Denny Vrandecic and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM 57, 10 (2014), 7885. doi:10.1145/2629489 [40] Tianle Wang, Lianghao Xia, and Chao Huang. 2023. Denoised Self-Augmented Learning for Social Recommendation. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China. 23242331. doi:10.24963/IJCAI.2023/258 [41] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat-Seng Chua. 2021. Learning Intents behind Interactions with Knowledge Graph for Recommendation. In WWW 21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. 878887. doi:10.1145/ 3442381.3450133 [42] Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, and Ben Kao. 2023. Automated Self-Supervised Learning for Recommendation. In Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023. 9921002. doi:10.1145/3543507.3583336 [43] Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin, and Jimmy X. Huang. 2022. Hypergraph Contrastive Collaborative Filtering. In SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. 7079. doi:10.1145/3477495. 3532058 [44] Yueshen Xu, Yuhong Jiang, Xinkui Zhao, Ying Li, and Rui Li. 2023. Personalized Repository Recommendation Service for Developers with Multi-modal Features Learning. In IEEE International Conference on Web Services, ICWS 2023, Chicago, IL, USA, July 2-8, 2023. 455464. doi:10.1109/ICWS60048.2023.00064 [45] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding Entities and Relations for Learning and Inference in Knowledge Bases. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. http://arxiv.org/abs/1412.6575 [46] Yuhao Yang, Chao Huang, Lianghao Xia, and Chunzhen Huang. 2023. Knowledge Graph Self-Supervised Rationalization for Recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023. 30463056. doi:10.1145/3580305. [47] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. 2022. Knowledge Graph Contrastive Learning for Recommendation. In SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. 14341443. doi:10.1145/3477495.3532009 [48] Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung, and Xiangliang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. In WWW 21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. 413424. doi:10.1145/3442381.3449844 [49] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are Graph Augmentations Necessary?: Simple Graph Contrastive Learning for Recommendation. In SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022. 12941303. doi:10.1145/3477495.3531937 [50] Fanjin Zhang, Xiao Liu, Jie Tang, Yuxiao Dong, Peiran Yao, Jie Zhang, Xiaotao Gu, Yan Wang, Bin Shao, Rui Li, and Kuansan Wang. 2019. OAG: Toward Linking Large-scale Heterogeneous Entity Graphs. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019. ACM, 25852595. doi:10.1145/3292500.3330785 [51] Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, and Jie Tang. 2024. OAG-Bench: Human-Curated Benchmark for Academic Graph Mining. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024. ACM, 62146225. doi:10.1145/3637528."
        }
    ],
    "affiliations": [
        "Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China"
    ]
}