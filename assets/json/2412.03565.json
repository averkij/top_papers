{
    "paper_title": "Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning",
    "authors": [
        "Wujian Peng",
        "Lingchen Meng",
        "Yitong Chen",
        "Yiweng Xie",
        "Yang Liu",
        "Tao Gui",
        "Hang Xu",
        "Xipeng Qiu",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 6 5 3 0 . 2 1 4 2 : r INST-IT: Boosting Multimodal Instance Understanding via"
        },
        {
            "title": "Explicit Visual Prompt Instruction Tuning",
            "content": "Wujian Peng1,2 Lingchen Meng1 Yitong Chen1,2 Yiweng Xie1 Yang Liu1 Tao Gui1 Hang Xu3 Xipeng Qiu1,2 Zuxuan Wu1,2 Yu-Gang Jiang1 1School of Computer Science, Fudan University 2Shanghai Innovation Institute 3Huawei Noahs Ark Lab https://inst-it.github.io"
        },
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at holistic level, they still struggle with instance-level understanding that requires more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed INST-IT, solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. INST-IT consists of benchmark to diagnose multimodal instance-level understanding, large-scale instruction-tuning dataset, and continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of INST-IT, our models not only achieve outstanding performance on INST-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension. Equal contributions. Corresponding author. Figure 1. Current LMMs struggle with instance-level understanding, failing to capture the nuanced details about specific instances. To address this, we create large-scale instance-specific instruction tuning dataset and train multimodal model on it. Compared to existing models, our model show better performance in instance-level understanding. 1. Introduction Recently, Large Multimodal Models (LMMs) have seen remarkable advancements. key breakthrough is visual instruction tuning [16, 39], enabling models to follow any type of user instructions. This paves the way to building general-purpose multimodal assistants capable of handling Inspired by this wide range of real-world tasks [30]. initial work, numerous follow-up studies have emerged in both image-language [10, 12, 40, 54, 73, 99] and videolanguage [33, 37, 46, 72, 74, 76, 90] domains. However, although these models can understand the entire images or videos, they still struggle to comprehend instance-specific content, as illustrated in Fig. 1. This hinders them from fine-grained understanding of visual inputs. Instance-level understanding refers to comprehending the attributes of specific instances within an image or video, as well as the relationships and interactions between them. This requires models to exhibit nuanced comprehension and fine-grained alignment. Instance understanding has been long-standing pursuit of the computer vision community with extensive efforts devoted to object detection [52, 63, 65], instance segmentation [53, 66] and object tracking [18], etc. This capability is essential for realworld applications, where users pay more attention to the instances that they are interested in. In the era of LMMs, although there have been some attempts in exploring multimodal instance understanding [5, 22, 94, 95, 98], they are primarily limited in the image domain, leaving the videos under-explored. Compared to images, understanding instances in videos is considerably more challenging, as it requires not only capturing their spatial information but also their temporal dynamics. Driven by this, we aim to advance the multimodal instance-level understanding in both images and videos. To this end, we focus on three key aspects of the effort: the evaluation benchmark, the instructiontuning dataset, and the training recipe. We begin by introducing an automated data annotation pipeline designed to provide detailed annotations for images and videos, with particular emphasis on individual instances. Specifically, this pipeline first highlights the instances of interest in the images and videos using instancelevel visual prompts [81]. Then, GPT-4o [55] is leveraged to generate fine-grained and multi-level annotations that covering several aspects, including: instance-level captions, frame-level captions, temporal changes descriptions, videolevel captions, and open-ended question-answer pairs. With the help of this pipeline, we first carefully design the INST-IT Bench, benchmark for diagnosing multimodal models in instance-level understanding, which includes 1,000 image test samples and 1,000 video test samples. We conduct extensive evaluations on INST-IT Bench, and find that existing open-source models show limited performance in understanding individual instances. To address this gap, we apply the propose data annotation pipeline to perform fine-grained annotations on nearly 21k videos and 51k images, resulting in INST-IT Dataset, which contains total of 207k frame-level annotations, 21k video-level annotations, and 335k open-ended question answer pairs. To the best of our knowledge, this is the first instruction-tuning dataset that provides fine-grained annotations focused on instance of interest in both images and videos. Based on INST-IT Dataset, we propose continuous instructiontuning recipe that effectively combines our instance understanding datasets with other general instruction-tuning data. By incorporating this small amount of data, the enhanced models demonstrate strong performance not only on INSTIT Bench, but also on several widely used image and video benchmarks, such as AI2D [25] and Egoschema [47]. Our contributions are three-fold: 1. An instance-centric understanding benchmark. We introduce INST-IT Bench, benchmark specifically designed to evaluate instance-level understanding in multimodal models, encompassing both image and video understanding, and perform extensive evaluations on it. 2. An instance-grounded instruction-tuning dataset. We develop INST-IT Dataset, the first instruction-tuning dataset that includes both images and videos, featuring explicit instance-level visual prompts. It is built through GPT-4o-assisted automated annotation pipeline. 3. An instance-enhanced Large Multimodal Model. We further integrate INST-IT Dataset into the instruction tuning of LMMs. We propose continuous instruction tuning approach to effectively enhance spatialtemporal instance understanding while consistently improving general comprehension. 2. Related Work Large Multimodal Models. Recently, significant progress has been witnessed in large multimodal models (LMMs). BLIP-2 [32] and Flamingo [2] leverage visual resamplers to integrate image features as language inputs by extracting fixed number of visual tokens. In parallel line of research, LLaVA [39] and its follow-upds [29, 38, 40, 51, 54, 91, 97] have achieved remarkable success by connecting vision and language through simple projection module. This approach has simplified the pipeline, even outperforming previous models with less training effort. Additionally, researchers are extending LMMs capabilities to temporal understanding by incorporating multi-frame inputs [37, 74] or explicit temporal modules [23, 36], bringing these models closer to applications in real-world scenarios. However, we observe that existing LMMs struggle with instance-level understanding and often fail to accurately follow instructions to ground specific instances. We emphasize the importance of instance understanding and enhance it through instruction fine-tuning with explicit visual prompts, which also enhances general understanding. Multimodal Datasets and Benchmarks. With the rapid progress in LMMs, numerous instruction tuning datasets have been developed. LLaVA-Instruct [39] is one of the first to use GPT-4, leveraging object categories, bounding boxes, and image-level captions to generate diverse visual instruction tuning data. Building on this, follow-up studies have focused on enhancing large language and multi-modal models to generate synthetic data [8, 10, 73] or improve the annotation pipeline [11, 97]. Simultaneously, various multimodal benchmarks have been proposed to evaluate LMMs across different aspects, such as comprehensive understanding [28], OCR and text comprehension [4850, 70], temporal understanding [6, 21, 42, 47, 77], hallucination [34], and instruction-following [60]. However, they focus more on image or video-level understanding and lack fine-grained emphasis on specific instances. In this work, we emphasize the importance of instance understanding in both images and videos. Specifically, we propose the INST-IT Bench to evaluate the instance understanding of LMMs and create the INST-IT Dataset, providing detailed instance-level annotations to enhance instance understanding capabilities. Multimodal Instance Understanding. Understanding individual instances in visual inputs is central focus in computer vision, with key tasks such as object detection [63, 65], instance segmentation [27, 66], and object tracking [18, 45, 84]. In the era of large multimodal models, instance-level understanding has also gained increasing attention. SPEC [56], ARO[88] and Winoground [69], evaluates the ability of vision-language models to comprehend fine-grained visual-linguistic concepts, revealing that models like CLIP [61] struggle to grasp attributes of specific instances or the relationships between them. To address this, works like KOSMOS-2 [57] and Shikra [9] encode instance information in textual form. In parallel, another line of research, including SoM-LLaVA [79], RegionGPT [22], GPT4ROI [94], MG-LLaVA [98], OMG-LLaVA [95], and ViP-LLaVA [5], explores the use of visual prompting to guide models in focusing on specific instances. Among these, SoM-LLaVA [79] is most closely related to our work. It uses SoM [81] to prompt images and asks models to list the instances present, finding that instance-level understanding enhances global understanding. However, SoMLLaVA [79] focuses solely on the image domain and does not address the more challenging video domain. In contrast, we focus on both images and videos, aiming to advance multimodal models in understanding the spatiotemporal changes of individual instances. 3. INST-IT High-quality instruction following data are essential for training effective multimodal models. To address the shortage of instance-grounded data, we propose an automated pipeline to generate detailed annotations for both images and videos, with special emphasis on Instances of Interest. We begin by collecting diverse set of images and videos, construct visual prompts and leverage GPT-4o to generate instruction-following data based on these visual prompted Figure 2. The automated data generation pipeline. We process the video frames sequentially. At each timestamp t, GPT-4o is prompted to create frame-level annotation based on the current frame Xt and the previous frame Xt-1. Then, all the framelevel annotations are aggregated to produce video-level description vid and set of open-ended question-answer pairs qa. inputs (Sec. 3.1). Building upon this pipeline, we carefully design benchmark to assess models in instance-specific understanding (Sec. 3.2). Furthermore, we build largescale vision language dataset (Sec. 3.3) and proposed continuous instruction-tuning pipeline (Sec. 3.4) to enhance the instance understanding capabilities of LMMs. 3.1. Instance-centric Annotation Pipeline Overview. We propose an automated annotation pipeline designed to annotate videos with particular focus on individual instances. As illustrated in Fig. 2, the overall process involves annotating each frame sequentially, combining these frame-level annotations into comprehensive video-level description, and generating set of open-ended questions about the video content. Visual Prompting. To generate more fine-grained annotations focused on specific instances, we augment the images and videos with visual prompts to highlight the instances of interest. Specifically, we use the Set-of-Marks (SoM) visual prompting [81], which overlays numerical ID directly on each instance. We find this method highly effective in guiding GPT-4o to provide annotations focused on individual instances. For more details on SoM prompting, please refer to Appendix A. 3 Figure 3. Visualization of an data example from our INST-IT Dataset. For each video, we provide (a) frame-level annotations, (b) video-level description, and (c) open-ended question-answer pairs. Each frame-level annotation consists of three parts: captions for individual instances, caption for the entire scene, and captions describing the temporal changes involving specific instances. key feature of our dataset is its emphasis on instances of interest, including their state in each frame, how they change between frames, and questions and answers focused on their specific details throughout the video. The contours of instances in this example are deliberately highlighted for better visualization. complete data example can be found in Appendix C.2. Frame-level Annotation. We annotate each frame of the video sequentially. At each timestamp t, we provide GPT4o with the current frame Xt, the previous frame Xt-1, and tailored task prompt . GPT-4o then produces frame-level annotation =(yins ) across three aspects, where yins represents the captions for individual instances, yimg describes the temporal differences from the previous frame: is caption for the entire image, and ydif , yimg , ydif Y = GPT(P , Xt, Xt-1) (1) Video-level Summary. After obtaining annotations for each frame, we employ GPT-4o to aggregate them into vid, comprehensive description for the entire video: vid = GPT(P vid, [Y1, Y2, , YN ]), (2) where vid is the task prompt designed for video-level summary and is the total number of frames. qa={(qi, ai)}M i=1 focusing on specific instances: qa = GPT(P qa, [Y1, Y2, , YN ]) (3) Putting It Altogether. Through the steps outlined above, we can annotate the video content at multiple levels of granularity, highlighting the elements related to specific instances. Specifically, as illustrated in Fig. 3, each video is accompanied by the following three types of annotations: frame-level annotations, each encompassing detailed description of individual instances, the entire image, and temporal changes. comprehensive description covering the entire video. open-ended question-answer pairs that focused on specific instances or the inter-instance relationships. information about Additional the design of each task prompts, as well as the organization of the input for GPT-4o is provided in Appendix A. 3.2. INST-IT Bench Open-Ended Question Answer Pairs. Based on the frame-by-frame annotations, we also prompt GPT-4o with the task prompt qa to create open-ended QA pairs Existing multimodal benchmarks primarily focus on global understanding, failing to provide more in-depth insights into the instance-level comprehension capability of models. 4 To fill this gap, we present INST-IT Bench, benchmark specifically designed to diagnose instance-level understanding in both images and videos. Construction Process. INST-IT Bench is built through semi-automated workflow. Initially, we apply the pipeline described in Sec. 3.1 to generate around 20 open-ended QA pairs for each image and video. These generated pairs are then manually reviewed to ensure both accuracy and diversity. Specifically, we remove overly simple questions, ensuring the remaining ones are focused on specific instances. We also refine the questions and answers, making necessary rephrasing to ensure correctness. After this rigorous process, each sample retains 2 to 5 carefully selected questionanswer pairs from the original 20, forming the final test set. In addition, we generate three hard negative answers for each question, forming multiple-choice QA with four options. In total, INST-IT Bench includes 1,000 QA pairs for 338 images and 1,000 QA pairs for 206 videos. Each QA pair is available in both open-ended and multiple-choice formats. More details can be found in Appendix B. Metrics. Each test sample in INST-IT Bench includes both open-ended and multiple-choice formats. For open-ended QAs, we leverage GPT-4o to score the response from model based on its similarity to the ground-truth answer. For multiple-choice QAs, we calculate the average accuracy across all samples. More details on the metric calculations can be found in Appendix B. 3.3. INST-IT Dataset Instruction fine-tuning is crucial for training multimodal models, however, we find there lacks instruction fine-tuning datasets that focus on specific instances. This gap hinders the development of instance-level understanding of the community. Using the data annotation pipeline presented in Sec. 3.1, we create large-scale instruction-tuning dataset INST-IT Dataset. To the best of our knowledge, this is the first instruction fine-tuning dataset that provides finegrained annotations centric on specific instances Data Sources. We utilize five video instance segmentation datasets (BRUST [3, 17], UVO [75], OVIS [59], LVVIS [71] and YoutubeVIS-2021 [82]) and two object tracking datasets (BenSMOT [35], VidOR [67]) as our video sources, as they provide annotations of instance locations, which is useful in SoM visual prompting [81]. For the image source, we select the SA-1B [27] dataset due to its diversity and abundance of instance objects. Altogether, we collect 51k images and 21k video samples as our visual data pool. More details can be found in Appendix C. Statistics. INST-IT Dataset contains 21k videos and 51k images. On average, each video includes one video-level description, 7.3 frame-level descriptions (each covering individual instances, the entire scene, and temporal differences), and 15.6 open-ended question-answer pairs. We treat images as static, single-frame videos without temporal descriptions. In total, INST-IT Dataset includes visual data from 21k videos and 51k images, along with 21k video-level descriptions, 207k frame-level descriptions, and 335k open-ended QA pairs. For more details about INSTIT Dataset, please refer to Appendix C. 3.4. Instruction Tuning with INST-IT Dataset We adopt the widely-used LLaVA-Next [41] architecture to evaluate the effectiveness of our INST-IT Dataset. We train our model under an image-video joint training pipeline, where we mix our INST-IT Dataset with the open-source LLaVA-Next-DATA [43]. For single-image samples, we follow the original AnyRes paradigm [41] to split and encode sub-images according to the aspect ratio. For video and multi-image data, we batch the samples together, encode them, and flatten them into sequence. Additionally, we apply 2 2 spatial pooling to reduce the number of visual tokens in the video inputs. More details are in Sec. 4.1. 4. Experiments 4.1. Implementation Details We use LLaVA-Next [41] as our baseline multi-modal large model due to its widespread adoption. In the default configuration, Vicuna-1.5-7B [15] serves as the language model with CLIP-ViT-336 [62] as the vision encoder. We utilize the AdamW [44] with cosine learning rate schedule for optimization. During the vision-language alignment stage, we use the LCS-558K dataset [40], and for the supervised fine-tuning stage, we leverage the open-source LLaVA-Next-DATA [43]. For single images, we split the original image into up to 4 sub-images based on its aspect ratio following the AnyRes [41] approach, and then concatenate the global image with these sub-images. For multiple images and video inputs, we skip the AnyRes procedure and encode every single image. Additionally, we apply 2 2 spatial pooling to reduce the number of visual tokens for video inputs. We limit the maximum number of frames to 32 and the context length of LLMs to 6K due to GPU and memory constraints. To enhance instance-level understanding with our INST-IT Dataset, we combine INST-IT Dataset with LLaVA-Next-DATA and introduce an additional continuous supervised fine-tuning (SFT) stage. In this stage, we freeze the first 12 layers of the vision encoder to mitigate potential distribution shifts caused by visually prompted images. Furthermore, we use Qwen2-7B [80] with SigLIPSO400M-384 [89] for improved performance in our main experiment, and Qwen2-1.5B with CLIP-ViT-336 for efficiency in our ablation study. 5 Model LLM Random Guess GPT-4o [55] Gemini-1.5-flash [64] - - - LLaVA-1.5 [40] ViP-LLaVA [5] SoM-LLaVA [79] LLaVA-Next [41] Vicuna-7B [15] Vicuna-7B [15] Vicuna-7B [15] Vicuna-7B [15] LLaVA-NeXT-Video [96] ShareGPT4Video [11] MiniCPM-V 2.6 [83] LLaVA-OV (SI) [29] LLaVA-OV [29] LLaVA-Video [97] InternVL2 [12] Qwen2-VL-Instruct [74] Qwen2-VL-Instruct [74] Vicuna-7B [15] Llama3-8B [19] Qwen2-7B [80] Qwen2-7B [80] Qwen2-7B [80] Qwen2-7B [80] InternLM2.5-7B [7] Qwen2-7B [80] Qwen2-72B [80] #IT N/A N/A N/A 665K 1.2M 695K 765K 860K 1.0M 7.0M 7.2M 8.8M 7.4M N/A N/A N/A LLaVA-Next-INST-IT LLaVA-Next-INST-IT Vicuna-7B [15] Qwen2-7B [80] 920K 920K Image Video Open-Ended Q&A Multi-Choice Q&A Open-Ended Q&A Multi-Choice Q&A - 74.1 65.3 Open-source image models 41.6 42.1 45.1 46. Open-source video models 46.5 43.2 57.6 60.3 48.0 45.1 58.6 48.3 55.5 Our models 68.6 67.9 25.0 84.8 79.5 32.1 29.2 40.0 42. 39.5 48.7 66.8 61.8 71.7 67.0 66.5 64.9 74.7 63.0 75.3 - 65.5 57.9 - - - - 25.8 27.8 40.0 31.4 33.2 34.1 39.8 38.2 45.5 49.3 45. 25.0 81.0 75.8 - - - - 24.8 16.1 45.2 36.4 45.6 53.2 45.5 59.4 74.6 42.1 53.3 Table 1. Performance of LMMs on INST-IT Bench. We conduct extensive evaluations on INST-IT Bench, including state-of-the-art open-source image models, video models, and cutting-edge proprietary models. #IT indicates the number of training samples used during the instruction-tuning stage. N/A indicates that the number of training samples is unknown. Method LLM Vision Encoder CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] Vicuna-7B [15] LLaVA-1.5 [40] Vicuna-7B [15] LLaVA-Next [41] Vicuna-7B [15] DeepStack-L [54] Vicuna-7B [15] DeepStack-L-HD [54] Vicuna-7B [15] VILA [38] Vicuna-7B [15] ShareGPT4V [10] MM-LLM-7B [91] MM-CLIP [91] MM-1.5 [91] InternLM-7B [68] InternVL2 [12] Qwen2-7B [80] LLaVA-OV (SI) [29] LLaVA-OV [29] Qwen2-7B [80] Qwen2-VL-Instruct [74] Qwen2-7B [80] Vicuna-7B [15] LLaVA-Next-INST-IT Qwen2-7B [80] LLaVA-Next-INST-IT InternViT-300M [12] SigLIP-SO400M [89] SigLIP-SO400M [89] DFN-CLIP-H [20] CLIP-ViT-Large [61] SigLIP-SO400 [89] (test) 54.8 66.6 - - - AI2D [25] MMMU [87] POPE [34] GQA [24] MM-Vet [85] (test F1) 85.9 86.4 86.7 86.5 85.5 - 88.6 - - - - 87.2 87.6 (test) 30.5 44.1 29.9 37.5 34.9 37.6 42.2 60.0 58.8 57.5 62.0 38.1 44.7 (val) 62.0 64.2 63.1 65.2 62.3 - - - - - - 65.9 65.5 (val) 35.3 35.1 35.7 35.6 - - 41.8 49.3 47.3 48.8 54.1 37.4 42.7 72.0 83.8 81.6 81.4 83.0 71.0 78.7 Table 2. Main results on image benchmarks. 4.2. Main Experiments Results on INST-IT Bench. We conduct extensive evaluate of existing multimodal models on INST-IT Bench. The results in Tab.1, show that with instruction tuning on LLaVANeXT [41] using INST-IT Dataset, the model achieves significant improvement of nearly 20% on average score, validating the effectiveness of our INST-IT Dataset. Moreover, although ViP-LLaVA [5] utilizes visual prompts for instruction tuning, it shows minor improvement over the baseline LLaVA-1.5 [40], possibly due to overfitting to its training data. In contrast, our model demonstrates consistent improvements on other instance understanding benchmarks, such as ViP-Bench [5], as well as on generalpurpose evaluation sets like AI2D and Egoschema (further discussions are in the following sections). This suggests that model trained with INST-IT Dataset can generalizes well to out-of-domain data. Besides, as Qwen2VL-72B does not show substantial improvements over its smaller 7B model, indicating that simply scaling up the model size cannot address the challenges in instance understanding. Similarly, by comparing the amount of instruction tuning data 6 Method LLM Vision Encoder CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] CLIP-ViT-Large [61] Vicuna-7B [15] DeepStack-L [54] Vicuna-7B [15] IG-VLM [26] LLaVA-Next [41] Vicuna-7B [15] Vicuna-7B [15] SF-LLaVA [78] Vicuna-7B [15] Video-ChatGPT [46] Vicuna-7B [15] VideoLLaMA2 [13] LLaVA-Next-Video [96] Vicuna-7B [15] LLaVA-Next-Video-DPO [96] Vicuna-7B [15] LongVA [93] Qwen2-7B [80] MM-LLM-7B [91] MM-CLIP [91] MM-1.5-Video-SFT [91] InternLM-7B [68] InternVL2 [12] LLaVA-OV [29] Qwen2-7B [80] Qwen2-7B [80] LLaVA-Video [97] Qwen2-VL-Instruct [74] Qwen2-7B [80] Vicuna-7B [15] LLaVA-Next-INST-IT LLaVA-Next-INST-IT Qwen2-7B [80] InternViT-300M [12] SigLIP-SO400M [89] SigLIP-SO400M [89] DFN-CLIP-H [20] CLIP-ViT-Large [61] SigLIP-SO400 [89] (3 avg) - - - (subset) 38.4 35.8 - 47.2 47.3 - 43. ANet-QA [86] EgoSchema [47] Next-QA [77] VideoMME [21] TempCompass [42] (open-ended) 49.3 54.3 53.8 55.5 35.2 50.2 53.5 60.2 - 60.9 - 56.6 56.5 - 53.7 55.2 (w/o subs) - - - - - - 46.5 - 52.4 53.5 54.0 58.2 63.3 63.3 44.3 54.0 (val) 61.0 63.1 - 64.2 - 51.7 - - 68.3 76.8 - 79.4 83.2 - 70.2 73.0 - - - 58.3 61.3 - - 69.4 - 72.9 59.8 63.9 57.2 - 60.1 57.3 66.7 57.8 50.4 Table 3. Main results on video benchmarks. We report the average of 3 parts (MCQA, Y/N, Caption Match) of TempCompass for determinism results. used by each models, we observe that large-scale coarsegrained annotations does not lead to essential improvements either. This highlights the importance of instance-specific annotated data, as we achieve impressive performance by training 7B model on 920k data. Results on academic benchmarks. We evaluate our models using the LMMs-Eval [92] toolkit. To ensure fair comparison with other models, we primarily report results from their original papers or from reproduced results in other studies. On widely-used image benchmarks shown in Sec. 4.1, our INST-IT consistently outperforms LLaVANext, our direct baseline model. Furthermore, when utilizing more advanced language model and vision encoder, our method achieves performance comparable to large-scale SFT LMMs, such as LLaVA-OV and Qwen2-VL-Instruct, while requiring significantly less computational and data cost for instruction tuning. For video understanding benchmarks Sec. 4.1, our method significantly outperforms both LLaVA-Next and LLaVA-Next-Video. These consistent improvements demonstrate that enhancing instance-level understanding through explicit visual prompt instruction tuning is an effective strategy for improving generic spatiotemporal understanding. We further evaluate our model on ViP-Bench [5], an instance-level understanding benchmark that aligns closely with the objectives of our INST-IT. However, while ViP-Bench focuses primarily on domain-specific instance-level referring comprehension, our INST-IT encompasses broader scope, involving cross-instance and cross-timestamp interactions, among other aspects. As shown in Tab. 6, our model demonstrates strong generalization capabilities when evaluated on downstream tasks. Specifically, our INST-IT with Vicuna-7B achieves comparable performance to ViP-LLaVA when using rectangular bounding boxes as visual prompts and even surpasses ViPLLaVA when employing human-style visual prompts. Notably, our model operates as generalist under zero-shot evaluation, whereas ViP-LLaVA benefits from in-domain tuning since it is fine-tuned on the in-domain referring comprehension dataset. 4.3. Ablation Study We use Qwen2-1.5B [80] as the language model and CLIPViT-L-336 [61] as the vision encoder for all our ablation experiments, prioritizing efficiency. We first conduct an ablation study on the training recipe to investigate how to effectively integrate INST-IT Dataset with existing academic SFT datasets [43] for balanced improvement. Next, we perform detailed analysis of the impact of each component in our INST-IT Dataset. Effectiveness of our continuous instruction-tuning paradigm. As shown in Sec. 4.2, directly mixing the video INST-IT Dataset with LLaVA-Next-DATA leads to significant improvements on video benchmarks. However, the performance on generic image understanding slightly declines. We believe this is due to two main reasons: (1) the increased ratio of video data may suppress image understanding; (2) visually prompted images may introduce distribution shift from natural images. To address these issues, we propose continuous SFT paradigm based on single-image models and freeze the first 12 layers of the vision encoder to preserve realistic low-level features. As result, our model achieves balanced performance with this training approach. Detailed dataset combination. To further investigate the effectiveness of each component in INST-IT Dataset, as illustrated in Fig. 3, we conduct an extensive abla7 CL Tune Enc Data comb All layers LLaVA-Next All layers LLaVA-Next & INST-IT Dataset video All layers LLaVA-Next & INST-IT Dataset video None LLaVA-Next & INST-IT Dataset video Last 12 LLaVA-Next & INST-IT Dataset video Last 12 LLaVA-Next & INST-IT Dataset (img+vid) AI2D MMMU POPE GQA INST-IT-I Next-QA VideoMME INST-IT-V (test) 61.1 60.7 62.3 63.1 63.2 63. (w/o subt) 45.7 47.1 46.7 47.2 47.2 46.6 (test F1) 86.9 86.1 86.7 86.9 87.0 87.2 (mc) 31.3 43.0 44.4 44.3 44.0 43.7 (mc) 45.3 60.7 61.8 60.2 60.1 59.6 (mc) 56.6 59.7 62.4 63.2 63.3 64.3 (val) 35.9 34.7 35.7 35.0 34.9 36. (val) 61.4 61.2 62.9 62.5 62.5 62.7 Table 4. Ablation on data training recipe. We utilize LLaVA-Qwen2-1.5B-CLIP336 as the baseline model. INST-IT-I and INST-IT-V indicate the multi-choice splits of the image and video part of our INST-IT Bench, separately. # 0 1 2 3 4 Data Combination LLaVA-Next + inst-cap & img-cap + temporal diff + video-description & qa + INST-IT Dataset image AI2D MMMU POPE GQA INST-IT-I Next-QA VideoMME INST-IT-V (test) 61.1 63.0 63.0 63.2 63.0 (w/o subt) 45.7 46.0 45.6 47.2 46.6 (mc) 56.6 62.4 64.2 63.3 64.3 (mc) 31.3 33.8 36.9 44.0 43.7 (mc) 45.3 58.9 59.6 60.1 59. (val) 35.9 35.1 35.6 34.9 36.1 (val) 61.4 62.7 62.7 62.5 62.7 (F1) 86.9 86.1 87.1 87.0 87.2 Table 5. Ablation on detailed data combination. The dataset combination in line #3 corresponds to the video part of INST-IT Dataset, while line #4 represents the complete INST-IT Dataset by incorporating the image part into line #3. Model Synthesized visual prompts (tight bounding box) Visual prompts from human (e.g. arrow, circle) Rec OCR Know Math Rel Lang All Rec OCR Know Math Rel Lang GPT-4V-turbo-detail:high [1] 59.9 56.9 69.7 71.0 61.4 51.9 58.1 69.8 GPT-4V-turbo-detail:low [1] 51.4 51.7 50.3 67.7 57.5 57.5 53.2 50.3 InstructBLIP-7B [16] 33.3 17 38.9 22.3 26.8 36.9 16.3 7.5 Shikra-7B [9] 18.9 20.6 3.5 40.2 10.0 GPT4ROI-7B [94] 32.5 13.8 9.7 35.6 16.7 Kosmos-2 [58] 9.7 29.5 14.2 21.9 7.5 LLaVA-1.5-7B [40] 40.2 51.8 23.8 6.5 50.8 12.4 41.7 9.7 43.0 30.4 25.7 28.7 Qwen-VL-Chat [4] 46.8 53.9 42.5 9.7 54.8 18.8 ViP-LLaVA-7B [5] 48.2 12.9 64.3 46.2 LLaVA-Next-INST-IT-Vicuna-7B 51.3 23.7 49.0 12.9 48.2 46.3 LLaVA-Next-INST-IT-Qwen2-7B 58.9 24. 80.6 61.1 45.6 60.3 55.0 43.8 9.7 29.3 17.5 9.7 50.0 27.5 6.5 48.2 25.0 44.6 33.1 8.1 16.1 57.5 40.6 19.4 53.6 45.0 49.1 13.0 48.7 22.1 55.3 17.6 55.0 21.3 57.7 22.5 All 60.7 52.8 31.7 33.7 35.1 26.9 41.6 39.2 45.5 45.1 50.5 59.5 55.6 34.2 28.0 29.7 18.5 49.2 40.2 52.9 54.2 48.5 63.7 59.3 35.4 42.9 41.2 45.9 52.5 53.2 Table 6. ViP-Bench Evaluation Results. We direct perform evaluation with our INST-IT models. tion study by progressively adding data components. As shown in Sec. 4.2, the instance-level and image-level frame captions are essential for improving instance understanding in images. Meanwhile, temporal differences, along with video-level descriptions and QA, significantly enhance video instance understanding. Finally, incorporating the image component of INST-IT enables our model to achieve the best-balanced performance across generic image and video understanding benchmarks, as well as our proposed instance understanding benchmark. 5. Conclusion Instance-level understanding that detects, segments, and reasons nuanced relationships among objects has long been the goal of computer vision research, yet limited effort has been made to equip Large Multimodal Models with such capabilities. In this paper, we introduced INST-IT Bench, carefully curated benchmark for evaluating multimodal instance understanding abilities. Extensive evaluations for wide range of open-source models demonstrate the limitedness of current models for understanding at the instance level. To mitigate this issue, we collected INST-IT Dataset, the first instruction-tuning dataset with explicit instancelevel visual prompting and annotation. Building upon INSTIT Dataset, we proposed INST-IT, continuously finedtuned framework that excels in instance understanding as well as widely used image and video benchmarks. Limitation and Future Works. Our current experiments are conducted on 7B and 1.5B models due to the computation cost. Moreover, our current data pipeline is automated but constrained by the overhead of GPT-4o. We can further scale the model size and scale the dataset using 8 model-in-the-loop approach and improve the model through multi-round instruction tuning with self-synthesized data. We leave this direction for future work."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 8 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [3] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst: benchmark for unifying object recognition, segmentation and tracking in video. In WACV, 2023. 5, 16 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 8 [5] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Making large multimodal models understand arbitrary visual prompts. In CVPR, 2024. 2, 3, 6, 7, 8 [6] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. [7] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 6 Internlm2 technical report. [8] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model, 2024. 3 [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3, 8 [10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, 2024. 2, 3, [11] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS, 2024. 3, 6 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2, 6, 7 [13] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 7 [14] Cheng-Han Chiang and Hung-yi Lee. Can large language In ACL, models be an alternative to human evaluations? 2023. 13 [15] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 5, 6, [16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. 2023. 1, 8 [17] Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, and Deva Ramanan. Tao: large-scale benchmark for tracking any object. In ECCV, 2020. 5 [18] Patrick Dendorfer, Aljosa Osep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian D. Reid, Stefan Roth, and Laura Leal-Taixe. Motchallenge: benchmark for singlecamera multiple target tracking. IJCV, 129, 2020. 2, 3 [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 6 [20] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. 6, 7 [21] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 3, 7 [22] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In CVPR, 2024. 2, 3 [23] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model In Proceedings of the for long-term video understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514, 2024. [24] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 6 9 [25] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. 2, 6 [26] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zeroshot video question answering using vlm. arXiv preprint arXiv:2403.18406, 2024. 7 [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 3, 5, 16 [28] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [29] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 6, 7 [30] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. FTCGV, 2024. 1 [31] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. In ECCV, 2024. 16 [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 2 [33] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. ArXiv, 2023. 2 [34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 3, 6 [35] Yunhao Li, Qin Li, Hao Wang, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, and Libo Zhang. Beyond mot: Semantic multi-object tracking. In ECCV, 2024. 5, 16 [36] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. 2 [37] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, 2024. 2 [38] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 2, 6 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 2 [40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 2, 5, 6, 8 [41] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5, 6, 7 [42] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 3, 7 [43] lmms lab. Llava-next-data, 2024. 5, 7 [44] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5 [45] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object tracking: literature review. AI, 2021. [46] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, 2024. 2, 7 [47] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 2, 3, 7 [48] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 3 [49] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. [50] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. 3 [51] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In ECCV, 2025. [52] Lingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng Wang, Zuxuan Wu, Lu Yuan, and Yu-Gang Jiang. Detection hub: Unifying object detection datasets via query adaptation on language embedding. In CVPR, 2023. 2 [53] Lingchen Meng, Shiyi Lan, Hengduo Li, Jose M. Alvarez, Zuxuan Wu, and Yu-Gang Jiang. Segic: Unleashing the In emergent correspondence for in-context segmentation. ECCV, 2024. 2 [54] Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, and Yu-Gang Jiang. Deepstack: Deeply stacking visual tokens is surprisingly simple and effective for lmms. In NeurIPS, 2024. 2, 6, 7 [55] OpenAI. GPT-4o system card, 2024. 2, 6 [56] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnose and optimize: Towards finegrained vision-language understanding. In CVPR, 2024. 3 [57] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306.14824, 2023. 3 [58] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 8 [59] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai. Occluded video instance segmentation: benchmark. IJCV, 2022. 5, 16 [60] Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. 3 [61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 6, 7 [62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [63] Redmon. You only look once: Unified, real-time object detection. In CVPR, 2016. 2, 3 [64] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem W. Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, and etc. Nathan Schucher. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, 2024. 6 [65] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. TPAMI, 2015. 2, 3 [66] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. ArXiv, 2015. 2, 3 [67] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in usergenerated videos. In ICMR, 2019. 5, 16 [68] InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. 6, 7 [69] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visiolinguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. 3 [70] Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 2023. 3 [71] Haochen Wang, Cilin Yan, Keyan Chen, Xiaolong Jiang, Xu Tang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Ov-vis: Open-vocabulary video instance segmentation. IJCV, 2024. 5, 16 [72] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: tracklet-centric multimodal and versatile video understanding system. ArXiv, 2023. 2 [73] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. 2, [74] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 6, 7 [75] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. Unidentified video objects: benchmark for dense, openIn Proceedings of the IEEE/CVF inworld segmentation. ternational conference on computer vision, pages 10776 10785, 2021. 5, 16 [76] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2: Scaling video foundation models for multimodal video understanding. ArXiv, 2024. 2 [77] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. 3, 7 [78] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. 7 [79] An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, et al. List items one by one: new data source and learning paradigm for multimodal llms. In COLM, 2024. 3, [80] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Min Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, 11 [95] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Change Loy Chen, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In NeurIPS, 2024. 2, 3 [96] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 6, 7 [97] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 3, 6, 7 [98] Xiangyu Zhao, Xiangtai Li, Haodong Duan, Haian Huang, Yining Li, Kai Chen, and Hua Yang. Mg-llava: Towards multi-granularity visual instruction tuning. ArXiv, 2024. 2, 3 [99] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv, 2023. Yang Fan, Yang Yao, Yichang Zhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, and Zhi-Wei Fan. Qwen2 technical report. ArXiv, abs/2407.10671, 2024. 5, 6, 7 [81] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. 2023. 2, 3, 5, 13, 16 [82] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, 2019. 5, 16 [83] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6 [84] Alper Yilmaz, Omar Javed, and Mubarak Shah. Object tracking: survey. CSUR, 2006. [85] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024. 6, 13 [86] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, 2019. 7 [87] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 6 [88] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Y. Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? In ICLR, 2023. 3 [89] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 5, 6, 7 [90] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In EMNLP, 2023. [91] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 2, 6, 7 [92] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. 7 [93] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 7 [94] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on regionof-interest. arXiv preprint arXiv:2307.03601, 2023. 2, 3, 8 12 In Appendix A, we outline additional implementation details of the GPT-4o-assisted data annotation pipeline. In Appendix B, we present further information about the instance understanding benchmark, INST-IT Bench. In Appendix C, we share more details about the instruction fine-tuning dataset, INST-IT Dataset. A. Data Annotation Pipeline A.1. Set-of-Marks Visual Prompting Performing instance-level annotations is challenging, and we adopt the SoM visual prompting technique [81] to address this. Specifically, as illustrated in Fig. 4, we overlay numeric ID at the center of each instance and maintain the same ID for given instance across all frames. This simple augmentation can explicitly guide GPT-4o to focus more effectively on the instances of interest, enabling finer-grained and more accurate annotations. Furthermore, segmentation masks are necessary to calculate the center coordinates of each instance. Details on how these masks are obtained are provided in Appendix C.1. A.2. Prompting GPT-4o Task prompt templates. Prompt engineering is crucial for enabling GPT-4o to accomplish specific tasks. In this section, we present the task prompts that we designed to prompt GPT-4o for data annotation: The task prompt for frame-level annotation, Fig. 6. The task prompt vid for video-level annotation, Fig. 7. The task prompt qa for open-ended question-answer pairs generating, Fig. 8. GPT-4o API version. During the annotation process, we use the GPT-4o-2024-08-06 API and leverage its structured output functionality to facilitate output parsing, enabling the model to respond in predefined JSON format. Figure 4. Set-of-Marks visual prompting on the original videos. Each instance is assigned unique numeric ID, which remains consistent across all frames. Figure 5. GPT-4o-based open-ended question answering correctness assessment. The underlined parts in the figure are included only when evaluating the video split, while the italicized parts will be replaced by the actual sample for scoring. B. More Details about INST-IT Bench B.1. Negative Options Generation We use the ground-truth from open-ended QA as the positive option and additionally craft three negative options, forming multiple-choice question with four options. To create hard negatives, we first have the model answer the open-ended questions and use GPT-4o to score the correctness of the responses. If the score is lower than 0.4, we consider it difficult negative answer and include it as one of the negative options. Finally, we randomly shuffle the four options to ensure that the correct one appears in each position with equal probability. B.2. LLM-based Evaluator for Open-Ended QA Recent studies [14, 85] suggest that LLMs can serve as effective evaluators. Building on this, we use GPT-4o to assess the accuracy of open-ended question answering. Specifically, GPT-4o assigns score between 0 and 1 based on three key factors: the question, the ground-truth answer, and the model prediction. Given that INST-IT Bench prioritizes instance-level understanding, we pay special attention to the accuracy of instance ID references. Furthermore, for the video split of INST-IT Bench, we emphasize the correctness of timestamps to ensure temporal correctness. The task prompt for GPT-4o is illustrated in Fig. 5. Figure 6. Frame-level annotation task prompt, the italicized part are placeholders for the actual inputs. Figure 7. Video-level annotation task prompt, the italicized part are placeholders for the actual inputs. 14 Figure 8. Open-ended question-answer pairs generation task prompt, the italicized part are placeholders for the actual inputs. Figure 9. data example from INST-IT Bench. Each test sample includes both open-ended QA and multiple-choice QA, focusing on specific instances or the relationships and interactions between instances. B.3. Data Example"
        },
        {
            "title": "Dataset Name",
            "content": "Ann. Type"
        },
        {
            "title": "Split",
            "content": "Sample Num."
        },
        {
            "title": "Video Instance Segmentation",
            "content": "BRUST [3] UVO [75] OVIS [59] LVVIS [71] YoutubeVIS [82] mask mask mask mask mask training training training training training BenSMOT [35] VidOR [67]"
        },
        {
            "title": "Video Object Tracking\ntraining\ntraining",
            "content": "box box 500 5,135 599 3,057 2,897 2,261 6,969 SA-1B [59] none 151,"
        },
        {
            "title": "Image",
            "content": "Table 7. Data sources. We use 7 video datasets and 1 image dataset as our data sources. We demonstrate their annotation formats, splits we used, and the number of samples from each dataset. gray suit and stands with [2] in sleeveless white lace dress, and [3] in dark floral-patterned dress. The three are close together, suggesting an intimate or focused setting. The progression between <2> and <3> involves subtle changes in posture and interaction. [1] moves closer to [2], appearing to hold hands or engage in an exchange, possibly involving ring, as indicated by bouquet of flowers. [3] remains supportive and smiling, while [5], in red, momentarily holds an object above their head, before disappearing from view by <4>.In frames <5> to <7>, [1] and [2] maintain close interaction, suggestive of significant moment such as an exchange of vows or rings. They are closely observed by [3], who stands smiling nearby, while [1] and [2] occasionally adjust their positions, facing each other initially and then turning outward, which may signal transitioning from an intimate moment to posing for photo. By <7>, [4] joins, dressed in darker attire, emphasizing the formal setting as [3] is no longer visible.Through <8> and <9>, the group dynamics change slightly with the absence of [4] and [3] entering the scene again. [1] and [2] appear to engage in warm interaction as [3] supports them, clapping, alongside visible hands of [4] indicating applause, marking cheerful tone.Finally, during <10> to <12>, the focus shifts as [1] and [2] first engage in kiss, underscoring an intimate conclusion to their ceremony. They later stand apart slightly at the center, with [1] smiling or speaking, and [2] leaning towards [1] suggestively content. Throughout, the consistent joyous mood is accentuated by [3]s ongoing clapping and expression of joy, emphasizing shared celebration and approval from the audience captured. To provide clearer understanding of INST-IT Bench, we present data example in Fig. 9. Each question includes both open-ended and multiple-choice formats, focusing on specific instances or exploring the relationships and interactions between multiple instances. This design highlights the significant distinction from other benchmarks, emphasizing fine-grained understanding at the instance level. C. More Details about INST-IT Dataset C.1. Data Collection and Processing Collection. We select five instance segmentation datasets and two multi-object tracking datasets as sources of video data. To prevent data leakage, we only used the training splits of these datasets. Additionally, we use the SA-1B [27] dataset as source of image data, and only utilize the first five data splits provided by officially. In total, we collect 21,418 videos and 51,101 images. Tab. 7 provides detailed statistics on the sources of our data. Processing. When constructing SoM [81] visual prompts, we need to obtain the mask annotations for each instance to determine the location of the numeric IDs. For the video instance segmentation datasets [3, 59, 71, 75, 82], the instance masks are already provided and can be used directly. For multi-object tracking datasets [35, 67], we use their bounding box annotations as prompts and generate instance masks through the SAM [27]. For images in the SA-1B dataset [27], we employ Semantic-SAM [31] to segment the instances and obtain their masks. C.2. Data Example In this section, we provide complete video annotation from INST-IT Dataset to offer clearer understanding of its content and format. In all annotations, we use the format [ID] to refer to instances and <timestamp> to refer to timestamps. We present the frame-level annotations in Tab. 8 to Tab. 10. We can see that each frame-level annotation consists of three parts: instance-level descriptions yins, image-level descriptions yimg, and temporal differences ydif . Additionally, each video is accompanied by series of open-ended question-answer pairs qa, most of which center on specific instances or their relationships, as illustrated in Tab. 11. Furthermore, we generate dense video-level caption vid summarizing the entire video, as shown below: Video-level caption: The video appears to document formal or celebratory event indoors, possibly ceremony such as wedding or official gathering, occurring in room with wooden paneling and art or framed pictures on the wall. At the beginning, during <1>, [1] is wearing light 16 Frame Instance-level Captions Image-level Captions Temporal Differences 1: Wearing light gray suit with white shirt, standing indoors. 2: Wearing sleeveless white lace dress, holding an object in the hand. 3: Wearing dark floral-patterned dress with long wavy hair. 1: person wearing gray suit with white shirt, short hair. 2: person in white, sleeveless dress 3: perwith long dark hair. son wearing dark floral dress with long dark hair. 5: person wearing red, partially visible in the background. 6: small black cellphone held in hand. 1: Wearing grey suit, standing beside [2] and slightly turned towards them. 2: Wearing white, sleeveless dress with floral textures. Holding bouquet of white flowers. 3: Wearing dark patterned dress, standing slightly behind [2]. 4: Partially visible, wearing dark clothing, located at the edge of the left side of the frame. 5: Seated, wearing red outfit. Holding white object above their head, possibly obscuring their face. 1: Wearing light gray suit jacket, white dress shirt, and dark pants. 2: Wearing white dress with lace overlay, fitted at the waist. 3: Wearing patterned dress with floral design, strapless. 4: Visible part of person wearing dark shirt, seated or standing near the table. 1: An adult wearing light gray suit with button details and white shirt. The expression and stance suggest focus and engagement. 2: An adult in white, lacy dress with thin straps. The person has long dark hair and appears to be smiling, holding hands with [1]. 3: An adult wearing multicolored, patterned dress. The person has long, wavy hair and is smiling while observing [1] and [2]. [1] [2] [3] are standing closely together in an indoor setting. [1] is on the left side wearing formal, light gray suit with white shirt. [2], in the middle, is wearing sleeveless white lace dress, holding something in their hand. [3] is on the right side in dark floralpatterned dress with long, wavy hair. They appear to be in room with wooden paneling and some framed art on the wall. null The scene appears to be in an office setting with wooden table at the foreground. [1] is standing to the left, facing [2], and appears to be holding [2]s finger or hand. [2] stands slightly to the right, returning focus with [1]. [3] is to the right of [2], slightly in the background, smiling and looking forward. bouquet of white flowers lies on the table near [2]. [5] is partially visible in the background on the right, seated and wearing [6] is cellphone held by [5]. Backred. ground shows wooden wall and reflection in window. The scene shows [1] [2] [3] near wooden conference table in professional setting, possibly an office. [1] wears grey suit and is standing to the left, engaged with [2] who is wearing white dress and holding flowers. [3], who is in patterned dress, stands closely behind [2]. The newly appeared [4] is seated to the far left, partially visible at the edge of the frame. [5] is seated on the right side, holding an object above their head, possibly obscuring their face. The room has wooden walls and framed picture hanging on the wall. The setting appears to be indoors, with [1] [2] and [3] standing together around table with bouquet of flowers on it. [1] is interacting with [2], who is at the center, and they are possibly holding hands or engaged in some form of exchange. [3] is standing beside [2] and looking on, slightly leaning towards her. The room has wooden walls and large framed picture in the background. The setting suggests formal or ceremonial atmosphere, possibly wedding or an official gathering. The camera angle is focused on this group, highlighting their interaction. The current frame captures moment in an interior setting with [1] wearing light gray suit, [2] in white lace dress, and [3] in patterned dress. [1] and [2] are engaged, with [1] facing [2] and holding their hand, suggesting an exchange, possibly ring. [2] smiles, indicating moment of happiness. [3] stands to the right, smiling and observing the interaction, detached but engaged with the scene. The background shows wooden wall and framed picture, reflecting formal environment possibly used for ceremonies. bouquet of flowers rests on the table in front of the group. [1] has moved closer to [2] and is now in contact with [2]s hand. [2] has turned slightly towards [1] compared to the previous frame. [3] remains in similar position, but the expression suggests more engagement with the scene. [5] and [6] have appeared in the frame; [5] is visible in the background holding [6]. The table with bouquet of flowers is now visible, indicating shift in camera angle slightly to include more of the right side of the room. Object [5] has lifted an object above their head, possibly piece of paper. Object [4] has appeared in the scene, seated on the left side of the frame, which was not visible earlier. The positions of objects [1], [2], and [3] remain unchanged, as does the background and setting of the room. Overall, no significant movement is noticed in terms of camera angle or position for objects [1] [2] [3]. [1] has moved slightly closer to [2], and they appear to be holding hands or exchanging something. [5] is no longer visible in the frame, possibly due to change in camera angle or positioning of the individuals. Between the previous and the current frame, [1] and [2] have shifted slightly closer, with [1] now directly holding [2]s hand, indicating progression in their interaction, possibly the continuation or conclusion of an exchange, such as the placing of ring. [3] remains in similar position but continues to observe [1] and [2], emphasizing their passive role in the interaction. There is no notable change in the background or environment. timestamp<1> timestamp<2> timestamp<3> timestamp<4> timestamp<5> Table 8. INST-IT Dataset Frame-level Annotation, Part (frame 1-5). Please zoom in to view the instance ID labels. 17 Frame Instance-level Captions Image-level Captions Temporal Differences 1: [1] is wearing grey suit with white shirt, looking forward, standing upright and smiling slightly. 2: [2] is wearing white sleeveless dress, with hair tied back, and is standing with calm expression. 3: [3] is wearing floral dress with an energetic expression, standing with arms slightly bent. timestamp<6> 1: [1] is dressed in grey suit with white shirt, looking formal and neat. 2: [2] is wearing white, sleeveless dress with lightly patterned texture. 4: [4] is dressed in dark outfit, including dark scarf or similar accessory. timestamp<7> 1: Person in gray suit with white shirt underneath. 2: Person wearing white dress with long dark hair. 3: Person with long hair wearing patterned dress, standing in the background. timestamp<8> Between the frames, there is noticeable shift in the poses and exIn the pressions of [1] and [2]. current frame, [1] is now standing upright with slight smile, while previously [1] was leaning towards [2], holding [2]s hand, suggesting shift from interaction to posing. [2], who was previously looking at [1], is now facing forward with calm expression, indicating change from an interactive pose to more neutral one. Both [1] and [2] have adjusted their posture to face the camera more directly. [3] remains in similar positioning as before but has moved slightly closer to [2] and is displaying more energetic expression, emphasizing the cheerful atmosphere. The objects on the table in the foreground, visible in the previous frame, are no longer the focal point, showing that the primary focus is now the individuals standing together. [3] is no longer visible in the current frame. [4] has appeared, standing to the left side of [1] and [2]. [1] and [2] remain in similar positions as in the previous frame, but the group now includes [4]. The image depicts formal setting with group of three adults, [1], [2], and [3], standing closely together. The background features wooden paneled wall and framed picture. [1] and [2] are positioned in the center, both facing forward, suggesting they are the focus of the occasion. [1] is on the left, wearing grey suit, and [2] is to the right of [1] in white dress. They appear to be engaged in ceremony or formal event. [3] is to the right of [2], wearing floral dress, and displays cheerful demeanor. The lighting is bright, illuminating their faces and creating formal, celebratory atmosphere. In the current frame, [1] is positioned in the center, wearing grey suit and white shirt. [2] is to the right of [1], dressed in white sleeveless dress. [4] appears on the left side of the image, wearing dark outfit, which includes scarf, giving formal look. The environment is room with wooden walls, and large map or blueprint hangs on the wall in the background. The lighting highlights the three individuals, [1] [2] [4], and the focus is on them standing in formal setting. [1] and [2] appear to be closer together, engaged in the settings activity, with [4] seeming to join or rejoin the group. The current frame shows group of three individuals indoors, with [1] on the left in gray suit and white shirt, facing slightly towards [2], who is dressed in white dress with long dark hair. [2] is looking at [1], suggesting an interaction or communication between them. [3] is slightly behind [2] and smiling, indicating positive mood. The environment appears to be an office or meeting room with large map or artwork on the wall in the background and wooden wall, suggesting formal or semi-formal setting. The lighting is bright, coming from the windows in the background, creating clear but slightly shadowed detail on the individuals. From the previous frame to the current one, [1] and [2] appear to have shifted slightly closer to each other, with [2]s head turned towards [1] indicating interaction. [3] is now visible in the scene, having entered from the right, which suggests new addition to the group. [4] from the previous frame is no longer visible, indicating they may have exited the frame or moved out of view. The overall composition suggests change in group dynamics as [3] enters and [1] and [2] interact more closely. Table 9. INST-IT Dataset Frame-level Annotation, Part II (frame 6-8). Please zoom in to view the instance ID labels. 18 Frame Instance-level Captions Image-level Captions Temporal Differences 1: Wearing light gray suit with white shirt, standing with arms relaxed at the sides. 2: Wearing sleeveless white dress, with black hair visible, standing sideways. 3: Clapping hands, wearing dark, sleeveless floral-patterned dress. 4: Visible hands clapping, appearing on the left side of the frame. 1: [1] is wearing grey suit with white shirt. The persons expression is neutral. 2: [2] is wearing white dress, has long dark hair, and is smiling. 3: [3] is wearing dark patterned dress, has long dark hair, and is smiling. 4: [4] is partially visible, clapping hands, wearing long sleeve. timestamp<9> timestamp<10> 1: Individual in grey suit with light-colored shirt underneath. 2: Individual in white dress with flower in their hair. 3: Individual in dark floral dress with bare shoulders. 4: Visible hand, partially in the frame, with watch on the wrist. timestamp<11> 1: Adult wearing light grey suit Short dark with white shirt. hair, clean-shaven, and standing upright. 2: Adult in white, sleeveless dress. Long dark hair pulled back. Appears to be smiling with eyes partially closed. 3: Adult in dark floral dress with sleeveless design. Long dark hair down and clapping. timestamp<12> In the current frame, [1] is standing next to [2], both are positioned near wooden wall, with large framed picture or window in the background. [2] is wearing white dress and stands slightly leaning towards [1], who is dressed in gray suit. [3] is to the right, wearing patterned dress and clapping her hands. On the left side of the frame, [4]s hands are visible, indicating clapping gesture. The environment appears to be welllit, possibly indicating celebratory or formal gathering. In the current frame, [1] stands on the left wearing grey suit and appears slightly more composed than before. [2], next to [1], in white dress, continues smiling, directed towards [1]. [3] stands behind [2] with continuous smile and hands still positioned as if clapping, indicating joyous or celebratory mood. [4] is partially visible on the edge, with both hands shown as if engaged in clapping. The background remains the same, with wall decor and wooden frame, suggesting an indoor setting. The lighting is consistent, highlighting positive atmosphere. The current frame captures four adults in what appears to be an intimate celebration setting, inside room with wooden backdrop and framed picture on the wall. [1] and [2] are the main focus, engaged in kiss. Both are facing each other, with [1] in grey suit and [2] in white dress. [3] stands to the side, clapping, and appears joyous, indicating approval or celebration. The environment is that of seemingly formal setting with elements suggesting personal or official celebration. [4] is partially visible, with just hand showing, suggesting congratulatory gesture. In the current frame, [1] and [2] stand close together in the center of the image. [1] is wearing grey suit with white shirt and appears to be speaking or smiling. [2], dressed in white dress, is leaning slightly towards [1] with content expression. [3] is on the right, wearing dark floral dress and clapping, seemingly celebrating with [1] and [2]. The environment is indoors with wooden wall and large framed picture in the background. The overall mood is celebratory, suggesting an event or occasion has taken place. [4] has appeared in the current frame, clapping, which was not present in the previous frame. [1] and [2] have slightly shifted positions, indicating minor adjustment in posture. The lighting in the room appears brighter in the current frame. Between the previous and current frames, [1] has shifted from smiling to neutral expression. [2]s expression remains unchanged, still smiling. [3] continues to smile, maintaining the same engagement level. [4] shows hands in clapping motion slightly more forward than before. The physical positions of all individuals are largely the same, with slight adjustments in posture, possibly due to motion between shots. Between the previous and current frames, [1] and [2] have moved from standing side by side to facing each other and kissing, indicating change from neutral to an intimate interaction. [3] continues to display supportive gesture by clapping, suggesting this action started in the previous frame and continued into the current one. The position of [4] indicates movement from neutral position to congratulatory gesture, seen by the positioning of the arm and hand. The overall increase in physical interaction between [1] and [2] and the supportive gestures by [3] and [4] contribute to more emotionally engaging scene in the current frame. Compared to the previous frame, [1] and [2] were previously kissing, but now they are standing apart, with [2] leaning slightly towards [1]. [1] has shifted from facing [2] to facing slightly outward and appears to be speaking or smiling. [3] remains in the same position but continues clapping, indicating ongoing celebration. The celebratory mood persists, reflecting continuation of the event captured in the previous frame. Table 10. INST-IT Dataset Frame-level Annotation, Part III (frame 9-12). Please zoom in to view the instance ID labels. 19 Question Answer What change occurs with [1]s expression between <10> and the previous frame? [1] changes from smiling to neutral expression. What activity are [1] and [2] involved in at <11>? [1] and [2] are engaged in kiss. What is the overall mood during <11> as suggested by [3]s actions? celebratory or joyous event. What interaction occurs between [1] and [2] at <5>? [1] holds [2]s hand, suggesting an intimate gesture or exchange, likely ring. Who joins [1] and [2] in the frame at <7>? [4] appears in the frame, joining [1] and [2]. What changes in the groups composition between <7> and <8>? [3] reappears, and [4] is no longer visible. What common setting element is seen throughout the frames <1> to <12>? The scene is in an indoor setting with wooden paneling and framed art. What type of event is likely taking place based on the atmosphere in <4> and <6>? formal event, possibly wedding or official gathering. What new elements are introduced in the scene at <2>? [5] holds cellphone in the background, partially visible. What is the mood and lighting like at <6>? The mood is formal and celebratory, with bright lighting enhancing this atmosphere. What new background element appears at <7>? There is map or blueprint on the wall. What is notable about [5]s actions at <3>? [5] is lifting an object above their head, possibly piece of paper. What is the setting like in <3>? How are [1] and [2] interacting at <8>? The group is gathered near wooden conference table in formal setting. They are engaged in conversation or communication, indicated by body language and focus. What does [1]s expression suggest at <12>? [1] speaks or smiles, suggesting engagement with [2] or others. What shift occurs in the focus of the camera between <5> and <6>? The camera focuses more on individuals standing together, reducing focus on the foreground objects. What are [3] and [4] doing at <9>? They clapping their hands in celebration. What decorative element is visible at <2>? bouquet of flowers lies on the table near [2]. How has the posture of [1] and [2] changed by <6>? [1] and [2] face slightly outward, suggesting pose for photograph or audience. What overall physical change occurs between [1] and [2] from <10> to <11>? Theres noticeable increase in their physical interaction, enhancing emotional engagement. Table 11. INST-IT Dataset Open-Ended Question-Answer Pairs. The images of each frame are illustrated in Tab. 8 to Tab. 10."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "School of Computer Science, Fudan University",
        "Shanghai Innovation Institute"
    ]
}