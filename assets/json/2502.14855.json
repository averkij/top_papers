{
    "paper_title": "Prompt-to-Leaderboard",
    "authors": [
        "Evan Frick",
        "Connor Chen",
        "Joseph Tennyson",
        "Tianle Li",
        "Wei-Lin Chiang",
        "Anastasios N. Angelopoulos",
        "Ion Stoica"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \\#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 5 5 8 4 1 . 2 0 5 2 : r Prompt-to-Leaderboard Evan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N. Angelopoulos, Ion Stoica {evanfrick, connorchen, josephtennyson, tianleli, weichiang, angelopoulos, istoica}@berkeley.edu University of California, Berkeley February 21, 2025 *equal contribution Abstract Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures userand prompt-specific variations in model performance. To address this, we propose Prompt-toLeaderboard (P2L), method that produces leaderboards specific to prompt. The core idea is to train an LLM taking natural language prompts as input to output vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2Ls ability to produce prompt-specific evaluations follows power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the #1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l."
        },
        {
            "title": "Introduction",
            "content": "Evaluating the real-world performance of large language models is an unresolved challenge. growing suite of benchmarks, including MMLU [14], MMLU-Pro [37], and GPQA [30], seek to address the challenge by reporting task-specific performance metrics, such as multiple-choice question-answering ability. These highly-curated benchmarks focus on domain-specific performance measures but do not capture the general and subjective nature of organic human preferences. Live evaluations, such as Chatbot Arena [7], assess real-world performance by collecting millions of organic human preferences from users who visit the site and vote between pairs of model responses. These pairwise comparisons are aggregated using Bradley-Terry (BT) regression [5] to form leaderboard. This leaderboard averages over many users and prompts, only providing coarse understanding of performance. For example, if we want to identify the best model for SQL queries, the overall Chatbot Arena leaderboard may not be useful since SQL queries make up only 0.6% of organic submissions and thus have little influence in the ranking. natural solution is to stratify the data and run 1 Figure 1: Pipeline of P2L. P2L takes prompt and outputs an -dimensional vector that we call leaderboard. Once we have leaderboard, we can build better data products, like routers and automatic analyses (see right). separate BT regression for SQL queries. However, collecting the 3,000-5,000 SQL votes needed for stable ranking would require around million total votestaking months to collect. Finer-grained categories, for example SQL table joins, would demand even more data, making stratified regression impractical and slow. And the finest-grained analysesfor example, producing leaderboards for specific prompt or use-caseare rendered impossible. This manuscript proposes solution to this problem via method called Prompt-to-Leaderboard (P2L). P2L takes prompt as input and outputs leaderboard quantifying LLM performance on that specific prompt. Thus, P2L can be used to assess which models are best for specific use-case, as opposed to on average. Per-prompt leaderboards can also be aggregated to form personalized leaderboards, showing which model is best for an individual or enterprise based on their prompt history. The system works by training P2L model, which is an LLM trained on human preference feedback to output Bradley-Terry (BT) coefficient for every model in question; see Section 2.1. Because P2L characterizes the prompt-conditional win rate of any two models, it enables several downstream applications. These include optimally routing prompts to LLMs (Section 2.1.2), personalized evaluations based on users prompt history (Section 2.1.1), automated strength and weakness analysis of models (Section 3.4), and more. Thus, we view P2L as general-purpose tool for highly granular evaluations extracted from large corpuses of preference data. As demonstration of P2Ls utility, we tested our prompt routing strategy on Chatbot Arena between the dates 01/19/202501/27/2025, and it achieved the #1 spot with score increase of 25 points over the previous top model, Gemini-exp-1206 (see P2L router performance in Figure 1). More broadly, P2L is subclass of more general methodology we call Prompt-to-Regression 2 (P2R) for training LLMs to output coefficients of parametric statistical regressions (see Section 2.2). canonical example that we will develop throughout this paper is model taking prompts as input and outputting Bradley-Terry coefficients, as mentioned earlier. However, the method also accommodates other feedback models (ties, real values, etc.) via other parametric models. We describe this method and derive the optimal routing strategy in Section 2. We show experiments and other applications in Section 3."
        },
        {
            "title": "2 P2L method",
            "content": "We describe the P2L method formally, beginning with notation. Consider different LLMs which are presented to humans pairwisemodel on the left, and model on the right, where and are sampled randomly without replacement from [M ] = {1, . . . , }. If the human votes for model A, we set = 0, and if they vote for model B, we set = 1. Furthermore, we let represent two-hot encoding of the model pair, i.e., vector of length with zeros everywhere except +1 in the index and 1 in the index A. We model our data-generating process as tuple (X, Y, Z) of two-hot encodings, votes, and prompts sampled from joint distribution , where denotes the space of natural-language prompts. Also, let Θ denote space of functions mapping prompts to leaderboards, i.e., θ Θ is function from RM , and θ(z)i represents the leaderboard score of model [M ] given prompt z. Finally, let ℓ denote the binary cross-entropy loss and σ denote the sigmoid function."
        },
        {
            "title": "2.1 Core method",
            "content": "Conceptually, our method works as follows. We model the vote conditionally on the prompt and model pair as following Bradley-Terry (BT) model: P(Y = 1 = x, = z) = σ(xθ(z)), for some (unknown) θ : RM . The goal is to approximate θ from data. For any prompt Z, θ(z) represents leaderboard. Each model [M ] has coefficient θ(z)m, and the higher this coefficient is, the more likely model beats any other model on the prompt z. For different prompts, the leaderboard will be different, capturing the idea that different models are better on different prompts. Our target, θ, is precisely the function that takes prompts and outputs leaderboardshence the name, prompt-to-leaderboard (P2L). P2L is strict generalization of marginal BT regression. In marginal BT regression, we simply omit the dependence of the leaderboard on the prompt, and give the best leaderboard on average (marginally). That is, choosing Θ to be the class of constant functions θ(z) θ RM exactly recovers marginal BT regression. However, P2L can be substantially more powerful than marginal BT regression due to heterogeneity in the prompt-conditional performance of different language models. That is, we should leverage language models to extract information on model performance from the prompt. In particular, our work takes Θ to be space of reward models mapping prompts to vectors. Given training dataset Dtrain = {(Xi, Yi, Zi)}N i=1, we find the empirical risk minimizer, ˆθ = argmin θΘ 1 (cid:88) i= ℓ(σ(X θ(Zi)), Yi). (1) 3 Then, as before, we can extract the estimated win rate between any two models as (cid:98)P(Y = 1 = x, = z) = σ(x ˆθ(z)). Lastly, we note that this strategy of training LLMs to output coefficients of parametric statistical models will be generalized in Section 2.2. The resulting prompt-dependent models have both high predictive power and useful statistical interpretation, which is critical to the aforementioned routing and personalization techniques."
        },
        {
            "title": "2.1.1 Aggregating leaderboards",
            "content": "Many practical scenarios require leaderboard for distribution over prompts, not just one. For example, user may want to know which model is best for them based on their chat history, or an enterprise may want to know which model is best for their use-case. In other words, given distribution over prompts Q, we want to ensemble all θ(z) for to form leaderboard over Q. In the case of finite chat history, we can consider to be the discrete uniform distribution over the observed historical prompts. By the Tower property, we can decompose the win rate as EZQ.Y Bern(σ(X θ(Z)))[Y = x] = (cid:90) zZ σ (cid:0)xθ(z)(cid:1) dQ(z). (2) The win rate above no longer follows simple logistic model, but we can fit another logistic model to match it: θ(Q) = argmin (cid:2)ℓ (cid:0)σ(X θ), (cid:1)(cid:3) . θΘ XPX ,ZQ, Bern(σ(X θ(Z))) The idea is that, because we know P(Y = 1 = x, = z) = σ(xθ(z)) for all and z, we can simulate the data-generating process. This allows us to construct synthetic dataset and fit Bradley-Terry model to it. If θ exists, this technique is perfect, in that it recovers the exact same BT coefficients that we would have obtained by observing an infinite population of prompts from Q. In Appendix B.1, we explore an alternative leaderboard aggregation strategy by taking weighted average of the leaderboards. Note also that we use θ, with the understanding that in practice we will use the plug-in estimate based on ˆθ, and the resulting rule will be approximate. We can make this strategy more efficient by leveraging the linearity of the binary cross-entropy loss. Namely, EXP,ZQ,Y Bern(σ(X θ(Z)) (cid:2)ℓ (cid:0)σ(X θ), (cid:1)(cid:3) =EXP,ZQ =EXP,ZQ =EXP,ZQ (cid:2)EY Bern(σ(Xθ(Z)) (cid:2)ℓ (cid:0)σ(X θ), (cid:1) X, Z(cid:3)(cid:3) (cid:2)ℓ (cid:0)σ(X θ), EY Bern(σ(X θ(Z)) [Y X, Z](cid:1)(cid:3) (cid:2)ℓ (cid:0)σ(X θ), σ(X θ(Z))(cid:1)(cid:3) . Thus, we can bypass the need for sampling to simulate . In other words, (2) is equivalent to θ(Q) = argmin θΘ EXPX ,ZQ (cid:2)ℓ (cid:0)σ(X θ), σ(X θ(Z))(cid:1)(cid:3) . (3) This last expression is simple to compute for discrete distributions Q, leading to an efficient algorithm."
        },
        {
            "title": "2.1.2 Optimal routing",
            "content": "Next, we will derive the optimal router based on P2L. We will derive the exact optimal router based on θ and approximate it in practice by ˆθ. Let us assume, for the sake of simplicity, that for each model {1, . . . , }, there is known and fixed cost of inference, = (c1, . . . , cM ). We seek to create router that maximizes performance while remaining below constraint on the average cost, C. We express the router as policy, π : , which takes prompt as input and outputs distribution over models; we seek to estimate the optimal policy, π. We will also consider distribution of opponent models, , to act as baseline for comparison. For instance, we can pick to be point-mass on the single best model, or to be uniform over all [M ] models. One possible interpretation of an optimal router is the one that maximizes the win rate against subject to the cost constraint; that is, for almost every z, this interpretation of π(z) solves the following optimization problem: maximize πM PAq,Bπ,Y Bern(σ(θ(z)B θ(z)A))(Y = 1 = z) subject to EBπ[cB] , (4) In other words, the optimal router should maximize the average win rate against the opponent distribution q. An alternative definition of the optimal router is the one that has the highest Bradley-Terry coefficient. This version of the optimal policy has π(z) equal (almost surely) to the solution to the following optimization problem: maximize πM argmin θR Bπ,Aq,Y Bern(σ(θ(z)B θ(z)A)) (cid:104) ℓ(σ(θ θ(z)A), ) = (cid:105) . (5) subject to EBπ[cB] That is, considering the optimal router as separate model, it should achieve the highest possible spot in the leaderboard subject to the cost constraint. Surprisingly, although the optimization problems in (4) and (5) look different, their optimal solution is the same under the Bradley-Terry model. The solution is given in Theorem 1. The resulting problem has linear objective and linear constraint, and can be solved with any standard solver. If the dominant model is below the cost of C, the policy will deterministically select that model (i.e., it will place probability 1 on sampling that model). Otherwise, it will hedge its bets and randomize over multiple models. Theorem 1 (Optimal prompt-dependent routing). Assume that for every prompt z, the BradleyTerry model holds with coefficients θ(z). Then, the optimization problems in (4) and (5) are both equivalent to the following problem: minimize πRM subject to πWq πc C, 0M π 1M π1M = 1, (6) where represents the population win matrix, with entries ba = σ(θ(z)b θ(z)a). 5 The proof is given in Appendix A. It is important to note that deviations from the Bradley-Terry modelfor example, any non-transitivitywill break this relationship. Another benefit of this approach is that we are able to estimate the value of the objective function of (5) via standard root finder, which means we can estimate the routers position on the leaderboard before deploying it. We give this procedure in Algorithm 1. It is justified by (9) in the proof of Theorem 1. Algorithm 1 Optimal routing with BT estimate Input: q; W; θ(z)j; c; 1: Solve the LP: π = argmax πW πM , πcC 2: Compute = πW 3: Solve for θ by finding the root of the following implicit equation: Output: Optimal router π, estimate of routers BT coefficient θ qa σ(cid:0)θ θ(z)a (cid:1) = (cid:88) a"
        },
        {
            "title": "2.2 Prompt-to-Regression",
            "content": "Here, we give extensions of P2L beyond pairwise preference feedback. This is useful because, in Chatbot Arena, the voting options are not just is better and is better; they also include Tie and Tie (both bad). Thus, P2L model that takes into account all this additional data may learn faster and also learn interesting signals about which prompts are hard and cause models to exhibit different behaviors or failures. Fortunately, our toolkit generalizes to the case where is no longer two-hot encoding and is no longer binary. In fact, our strategy encompasses any parametric statistical model relating and conditionally on Z, regardless of the space in which they live. We call this more general class of models prompt-to-regression models. More formally, let us model the distribution of by saying that for all putative values y, pY =yZ=z,X=x(y) = gθ(z)(y; x), (7) for some (unknown) vector of parameters θ(z). Then, we fit ˆθ(z) by running maximum-likelihood estimation, i.e., maximizing gθ(Zi)(Yi; Xi)pX (Xi). As familiar example, we can set gθ(z) to (cid:81) i=1 BT model relating and : gθ(z)(y; x) = (cid:40) σ(xθ(z)) 1 σ(xθ(z)) = 1, = 0. Note that the formulation of (7), and can be arbitrary, so long as we model their conditional relationship via gθ(z). Thus, the framework can admit real-valued feedback via ordinary least squares, count feedback via Poisson regression, and so on. 6 As one example, we will consider incorporating ties via Rao-Kupper [29] model. Let be two-hot encoding, {A, B, tie}, and gθ(z)(y; x) = σ((x, 1)θ(z)) σ((x, 1)θ(z)) 1 σ((x, 1)θ(z)) σ((x, 1)θ(z)) = B, = A, = tie. In this technique, θ(z) is an (M + 1)-dimensional vector, the last entry of which encodes tie coefficient. The larger this prompt-dependent tie coefficient, the more likely the two models are to tie. Meanwhile, the first entries, ˆθ(z)1:M , comprise the leaderboard. Finally, we consider how to handle the Tie (both bad) category. For this, we developed non-standard statistical model which we call the grounded Rao-Kupper model. In this model, if both model coefficients are small, it increases the probability of Tie (both bad). Inspired by the Plackett-Luce model [28, 21], we imagine the existence of fictitious bad model with coefficient of zero, and use this as grounding point for the model coefficients. Let {A, B, tie, bad}, and for the sake of notational convenience, let θ(z) = (cid:0)β(z), λ(z)(cid:1) where β(z) RM and λ(z) R1}. For notational convenience, we define φ(z)i := exp(β(z)i). The grounded Rao-Kupper model is defined as: gθ(z)(y; x) = φ(z)A φ(z)A+λ(z)φ(z)B +1 φ(z)B φ(z)B +λ(z)φ(z)A+1 1 1+φ(z)A+φ(z)B 1 φ(z)A φ(z)A+λ(z)φ(z)B +1 φ(z)B φ(z)B +λ(z)φ(z)A+1 = = = bad (8) 1 1+φ(z)A+φ(z)B = tie. This model allows us to make efficient use of all data collected on Chatbot Arena by incorporating all votes. It also has the additional advantage that models with higher coefficients have lower probability of being labeled Tie (both bad). Thus, the raw coefficient value of model speaks to its absolute quality, as opposed to its comparative quality against other LLMs as in the BT model."
        },
        {
            "title": "3 Experiments",
            "content": "This section contains suite of experiments that validate the P2L method and demonstrate its utility. In Section 3.2, we show that P2L leads to gains in human preference prediction that scale with model size and data. In Section 3.2, we show direct predictive performance on pairwise human preferences, as well as scaling behavior with data size and parameter count. In Section 3.3, we show P2L allows for optimal cost-efficient routing via the algorithm developed previously in Section 2.1.2. In Section 3.4, we use P2L to automatically identify strengths and weaknesses for different models. In Section 3.5, we explore our aggregation technique against ground truth categories leaderboards, and observe data scaling trends. Finally, in Section 3.6, we show that the P2L has reasonable performance on out-of-distribution data."
        },
        {
            "title": "3.1 Training setup",
            "content": "To train P2L model, we follow this three-step procedure: 7 Figure 2: Loss metrics. The line plot shows the validation loss as function of the number of data points seen during training. The P2L models all substantially outperform the baselines, and performance scales with dataset and model size. The bar plots show the validation loss and mean squared error of the models trained on all 1.5M training points. 1. Begin with pre-trained, instruction-tuned LLM. 2. Remove the existing language model head and replace it with randomly initialized coefficient head. In the BT case, the coefficient head is linear layer producing outputs, one per model. 3. Train the model by running stochastic gradient descent to minimize the negative log-likelihood: L(θ) = (cid:88) i=1 log (cid:0)gθ(Zi)(Yi; Xi)(cid:1) . The result of this procedure is the trained model ˆθ = argmin L(θ), θΘ which is direct generalization of (1). We train on up to = 1.5 million crowdsourced human preference pairs from Chatbot Arena, containing = 130 unique models. We always train for In order to study the scaling laws of P2L as function of model size, we used the 1 epoch. following models as the initializations: SmolLM2-{135, 360}M-Instruct and Qwen2.5-{0.5, 1.5, 3, 7}B-Instruct [2, 36]. We refer to our post-trained versions of these models as P2L-{135,360}M and P2L-{0.5,1.5,3,7}B, respectively."
        },
        {
            "title": "3.2 Feedback prediction",
            "content": "We begin by evaluating P2L on its ability to predict human feedback on prompt-by-prompt basis. In other words, given two models and prompt, we ask how effectively P2L can predict which model will win on that prompt. These experiments measure the ability of P2L to accurately assess relative model quality on prompt-by-prompt basis. 8 In this section, we evaluate the ability of P2L to predict human preferences on Chatbot Arena. We construct holdout validation set containing 41,507 annotated pairwise comparisons across 34 well-used models. We then measure the negative log-likelihood (validation loss) on this dataset; lower validation loss indicates better preference prediction performance. Figure 2 shows the results of our procedure against two baselines. First, we include the constant predictor that gives an equal probability of all preference outcomes; this is an extremely weak baseline akin to flipping coin to decide the winner. Second, we include the average (marginal) leaderboard. For P2L, we show ladder of increasing model and dataset sizes. The more data is used to train P2L, the better the preference predictions become. Notably, the gap between the best P2L leaderboard and the marginal model is several times the gap between the marginal leaderboard and the constant predictor. This indicates that by capturing the prompt-dependent differences in model performance, P2L is able to produce much better predictions of human preference."
        },
        {
            "title": "3.3 Optimal routing",
            "content": "Figure 3: P2L router performance on Chatbot Arena. The left barplot shows the overall score of the router after it was deployed prospectively on Chatbot Arena. The right barplot shows the worst-case category score on Chatbot Arena. In both plots, larger models lead to higher Arena scores after better routers. Next, we evaluate the performance of the optimal router based on P2L as derived in Section 2.1.2. Our evaluations are based on prospective deployments of our router to Chatbot Arena. We treat the router as separate model. 3.3.1 Unconstrained routing We deployed the grounded Rao-Kupper versions of P2L-0.5B, P2L-1.5B, P2L-3B, and P2L-7B onto Chatbot Arena, crowdsourcing total of 8,616 pairwise comparisons between P2L models and public models hosted on Chatbot Arena. The P2L models routed between 34 models, including top models such as Gemini-exp-1206, o1-2024-12-17, and ChatGPT-4o-20241120 as well as other models. (See Appendix D.1 for full model list.) 9 Figure 4: Router model choice distribution in each prompt category. The rows are different models, and the columns are different categories. Each cell represents the probability that the model was selected within that category (i.e., columns sum to 1). Models with an average selection rate below 1% are not shown. Because there is no cost-constraint, the P2L router always picks the highest-ranked model conditionally on the prompt, i.e., the highest entry in ˆθ(z). Marginally, the strongest singular candidate model in the P2L router was Gemini-exp-1206, with score of 1364. As shown in the top plot in Figure 3, all P2L routers, regardless of parameter count, outperformed Gemini-exp-1206. The best model, P2L-1.5B, reached #1 on Chatbot Arena during our testing period with score of 1389. This shows the utility of P2L: differences in model performance on prompt-by-prompt basis allow P2L to outperform all individual LLMs. Next, we discuss scaling performance with respect to the Arena score of the router. We see general trend in Figure 3 that bigger models do better overall. The exception is P2L-1.5B, whose performance was unexplainably strong; otherwise, the trend holds. We also tested other metrics, such as worst-case performance (bottom of Figure 3). Encouragingly, the worst-case performance of P2L is much better than that of the marginal leaderboard, and scaling parameter count is helping immensely. We also observe that the gap between the P2L routers and static models is large. The P2L routers are able to avoid per-prompt model weaknesses and route elsewhere. In fact, the gap between the best P2L router and the best non-routed static model in the overall comparison was 25 points, while this gap grew to 51 points in the minimum category performance case. Figure 4 shows P2L-7Bs routing distribution conditioned on each Chatbot Arena category. Notably, we see relatively diverse routing patterns, even within single category. We also observe intuitive behavior patterns, such that heavily routing to o1-2024-12-17 for math prompts and Gemini-exp-1206 for creative prompts. 10 Figure 5: Arena score versus cost. Both plots show routing performance as function of average cost. The left plot shows the averaged performance across all categories, and the right plot shows the performance in the creative writing category. The black open circles give the raw performance and cost of the models used by the router. Each gold dot represents the Arena score of the P2L-7B router as function of the cost constraint in (6). The plots show that the P2L router dominates and substantially improves the cost-performance Pareto frontier. All confidence intervals are 95%. 3.3.2 Cost-optimal routing We show results of the optimal routing procedure detailed in Theorem 1 with P2L-7B model on Chatbot Arena. Here, we use P2L to route between o1-mini, gpt-4o-2025-05-13, claude-35-sonnet-20240620, gemini-1.5-pro-001, mistral-large-2407, claude-3-5-haiku-20241022, and gemini-1.5-flash-001 and with budgets of {0.00218, 0.0044, 0.00675, 0.00945, 0.0123, }. To get reasonable cost estimates, we calculate the expected cost per query with ci = Oi E[Ti] for all models [M ], where Oi is the output cost per token of model i, and Ti is random variable representing the number of tokens in response from model i. We estimate E[Ti] as the response token length mean overall responses from model in Chatbot Arena. Additionally, we estimate in Theorem 1 according to the Chatbot Arena model sampling distribution. We find the P2L router performs well, with Pareto frontier Arena score versus cost. Furthermore, on the right plot in Figure 5 we find the P2L router continues to show dominant performance in Chatbot Arenas creative category despite large shifts in individual model performances."
        },
        {
            "title": "3.4 Testing for regression and strength/weakness analysis",
            "content": "An important question when developing models is to understand their category-level performance, along with strengths and weaknesses. Imagine, for example, business seeking to upgrade their workflow to cheaper or newer (and presumably more advanced) model. In such business, testing for regression of the model to worse performance may be important. For example, they might ask the question: if switch from GPT-4o to GPT-4o-mini, can do so safely, and will my performance get worse on my customers? This is challenging question to answer because it requires knowledge of the enterprises customer distribution which may require lengthy instrumentation and data collection procedures. However, P2L provides partial solution to this problem. Given large unlabeled dataset of prompts (e.g., customer use-cases), we seek to: (1) Categorize these prompts automatically using an LLM. (2) Produce preference leaderboard within each category, and (3) On per-model basis, analyze for 11 Figure 6: Regression test. We show the strengths of different OpenAI models on various topic clusters based on their win rate against GPT-4o-2024-05-13 as predicted by P2L-7B. For each category, we show the probability given model wins against GPT-4o-2024-05-13 under the BT model. The results show strong category-specific variability in performance; for example, o1-mini is substantially better than GPT-4o-2024-05-13 in Arithmetic Operations and Calculations but substantially worse when asked to write Suspenseful Horror Story. which categories it is weak and strong (relative to itself or its competition). For this, we can use hierarchical clustering approach. Assume access to multilevel hierarchical categorization of prompts (this can be obtained from an LLM). That is, we have function categorize that takes in prompt and an integer level and outputs category in {1, . . . , kl}, for some integer kl. Given set of prompts, category, we can compute per-category leaderboard using θ(unif(Z)) as in (3). Note that the finest-grained categories may have very little data, motivating the need for P2L. Figure 6 shows an example analysis of five different OpenAI models. Here, the percentages are calculated as the win rate against GPT-4o-2024-05-13 under the BT model. According to P2L-7B, OpenAI models performance varies across different categories and topic clusters. While o1 might be better model on average, it is essentially the same compared to GPT-4o-mini on certain creativity tasks. In math flavored tasks, the gap widens significantly. See Figures 8 and 9 for similar and more detailed plots on Llama 3 fine-tunes. We also include variant of our regression analysis under the grounded RK model from (8); this provides guidance as to the absolute reliability of the model, not just preference over alternative models; see Figure 10."
        },
        {
            "title": "3.5 Aggregation scaling",
            "content": "Given distribution of prompts, we aim to evaluate how P2L behaves using the aggregation technique described in 2.1.1. Specifically, we analyze how P2Ls aggregated leaderboards compare to ground truth category leaderboards as well as how this relationship scales with data. First, we calculate ground truth leaderboards over large category from the validation set with marginal regression. We then aggregate P2L over increasing subsets of this categorys prompts. Lastly, we plot the L1 12 Figure 7: Aggregation scaling. The L1 distance between the aggregated leaderboard and the marginal BT regression as function of the number of randomly sampled and aggregated datapoints in two categories: Chinese (left) and Math (right). The optimal performance is given by an estimate of the finite-sample fluctuations in the empirical BT coefficients from the finite validation sample of 1M data points. The P2L estimate converges to near-optimal solution with increased data. function distance between the aggregated leaderboards predicted probabilities and the ground truth leaderboards predicted probabilities as subset size increases. Since both the train and validation set are drawn from the same distribution, we denote the optimal value to be the L1 function distance between the ground truth category leaderboard and the category leaderboard derived from marginal regression on the train set. In contrast to marginal regression, which requires thousands of prompts for stable leaderboard, P2L converges near this optimal value within 100-250 prompts (Figure 7). Here, we see P2Ls potential to create accurate aggregated leaderboards efficiently, while also reinforcing the validity of its per prompt outputs. Furthermore, as we scale the amount of training data seen, P2Ls predictions over singular prompts differ more drastically from category leaderboards while still converging with more prompts (Figure 7). clear scaling law ensues, as increased data allows P2L to make more distinguished individual leaderboards while still maintaining its aggregation ability at the category level."
        },
        {
            "title": "3.6 Performance on out-of-distribution prompts",
            "content": "To assess how P2L generalizes to unseen prompts, we evaluate it on LiveBench [38], verifiable, contamination-free benchmark with 1,000 questions covering diverse categories (e.g., math, coding, reasoning). Unlike Chatbot Arena, it utilizes objective success metrics. We restrict our evaluation to smaller pool of models. Among these models, P2L selects its candidate models for each question based on the predicted prompt-specific performance and then uses the output of the chosen model as the final answer. Table 1 shows that P2L-7B surpasses every static baseline among the model subset, achieving an overall LiveBench score of 59.275. Even far smaller versions (e.g., 1.5B) match or exceed top static models, demonstrating that preference-trained routing generalizes well to an out-of-distribution, ground-truth benchmark. Many real-world deployments require balancing model performance against inference costs. To examine this trade-off, we apply Prompt2Leaderboard to LiveBench at various cost thresholds (e.g., 13 $2, $5, $10, $15 per million tokens) using the cost-optimal routing method discussed in Section 3.3.2. Figure 11 (in the appendix) shows that, in all budgets tested, the P2L cost-aware router consistently scores higher or comparable LiveBench scores to the best-performing model within that specific cost threshold. These gains are most pronounced when the budget permits occasional routing to more expensive (and often stronger) model for prompts that particularly benefit from it. Thus, even under strict monetary constraints, P2Ls flexible prompt-level routing remains powerful approach to maximizing performance on challenging out-of-distribution tasks."
        },
        {
            "title": "4 Discussion and related work",
            "content": "This work develops fundamental tools for granular and query-specific evaluations in all evaluation tasks. Although our experiments are largely based on Chatbot Arena, this is not the only evaluation that could benefit from P2L. As discussed in Section 2, any feedback signal can be accommodated. Thus, our techniques would equally work well for other evaluations [14, 40, 9, 33, 41, 6, 19, 18] as well as cost and latency prediction. Modeling human preference. During Reinforcement Learning from Human Feedback (RLHF), reward model is often trained as proxy to human preference. Similar to P2L, reward model training may use contrastive pairwise or K-wise loss, for example using the BT model [8, 4, 27, 42]. However, reward models are agnostic to model identity, requiring prompt and response to return single score for the response. P2L, which is aware of model identities, instead seeks to output expected model response quality, conditioned on input prompt, instantly generating full leaderboard over all models without requiring model responses to be generated. This yields efficient leaderboard creation over arbitrary prompt sets. Meta-learning. P2L is related to meta learning [32, 31, 11] insofar as we are training model to output models. For example, we have discussed training an LLM (the meta-learner) to output coefficients of BT regression (the learner). However, the meta-learning literature primarily focuses on learners that are deep neural networks. Instead, we let the learner be an extremely simple statistical model that is used for inference. Routing. Prior work on routing LLM queries optimizes trade-offs between cost and performance, typically through classifiers or gating mechanisms. RouteLLM [24] and AutoMix [22] train binary classifiers to decide between strong and weak model, while LLM-Blender [17] ranks candidate responses and blends them. Hybrid LLM [10] selects between cloud and edge models based on predicted query difficulty. Unlike these approaches, which operate over small fixed set of models, P2L learns parametric function mapping prompts to full model leaderboards, enabling flexible selection across large model pools. Its statistical structure supports efficient cost-aware routing, outperforming static models in live crowdsourced settings while scaling to personalized and taskspecific selections. Parametric statistical models. Our work builds on classic log-linear models and GLMs, like those of Bradley and Terry [5], Rao and Kupper [29]; see [23] for review. The closest piece of work to ours is Hastie and Tibshirani [13], which proposes varying-coefficient models. P2L can be seen as subclass of varying-coefficient models. To our knowledge, ours is the first work to parameterize such model via foundation model and backpropagate it end-to-end, while the techniques in Hastie and Tibshirani [13] use bespoke fitting procedures and simpler statistical models than LLMs."
        },
        {
            "title": "References",
            "content": "[1] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md. [2] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. SmolLM2 - with great data, comes great performance, 2024. [3] Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku, 2024. (Accessed on 06/05/2024). [4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [5] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew Carr, Jan Leike, Josh Achiam, Vedant Mishra, Evan Morikawa, Catherine Olsson, Jakub Pachocki, Jack Hewitt, Bowen DasSarma, Sam McCandlish, Dario Amodei, and Tom Brown. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot Arena: An open platform for evaluating LLMs by human preference. arXiv preprint arXiv:2403.04132, 2024. [8] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. 2023. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. Hybrid LLM: Cost-efficient and qualityaware query routing. arXiv preprint arXiv:2404.14618, 2024. [11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 11261135. PMLR, 2017. 15 [12] Evan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, and Banghua Zhu. Athene-70b: Redefining the boundaries of post-training for open models. https:// huggingface.co/Nexusflow/Athene-70B, 2024. Accessed: 2025-02-12. [13] Trevor Hastie and Robert Tibshirani. Varying-coefficient models. Journal of the Royal Statistical Society: Series B, 55(4), 1993. [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Michael Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. OpenAI o1 system card. arXiv preprint arXiv:2412.16720, 2024. [16] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024. [17] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. [18] Percy Liang et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. [19] Zhixing Lin et al. Toxicchat: Analyzing the patterns of toxic behaviors in open-source LLM chat logs. arXiv preprint arXiv:2308.01968, 2023. [20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [21] Duncan Luce. Individual choice behavior, volume 4. Wiley New York, 1959. [22] Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, et al. Automix: Automatically mixing language models. arXiv preprint arXiv:2310.12963, 2023. [23] Peter McCullagh. Generalized linear models. Routledge, 2019. [24] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. RouteLLM: Learning to route LLMs with preference data. arXiv preprint arXiv:2406.18665, 2024. [25] OpenAI. New models and developer products announced at DevDay, 2023. (Accessed on 06/05/2024). [26] OpenAI. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/, 2024. (Accessed on 06/05/2024). 16 [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [28] Robin Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2):193202, 1975. [29] PV Rao and Lawrence Kupper. Ties in paired-comparison experiments: generalization of the bradley-terry model. Journal of the American Statistical Association, 62(317):194204, 1967. [30] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: graduate-level Google-proof Q&A benchmark. arXiv preprint arXiv:2311.12022, 2023. [31] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning, pages 18421850. PMLR, 2016. [32] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987. [33] Aarohi Srivastava et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2023. [34] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [35] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [36] Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. [37] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [38] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. [39] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open Foundation Models by 01.AI, 2024. 17 [40] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [41] Wanjun Zhong et al. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [42] Banghua Zhu, Jiantao Jiao, and Michael Jordan. Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270, 2023."
        },
        {
            "title": "A Proofs",
            "content": "Proof of Theorem 1. The equivalence of (4) and (6) is immediate. Proving the equivalence of (5) and (6) is more challenging, and we focus there. We begin by simplifying the expressions in (5). The cost constraint can be succinctly written as πc C. Regarding the objective, because the binary cross-entropy loss is linear in the response, EBπ,Aq,Y Bern(σ(θ(z)B θ(z)A)) [ℓ(σ(θ θ(z)A), ) = z] = EBπ,Aq [ℓ(σ(θ θ(z)A), σ(θ(z)B θ(z)A)) = z] (cid:34) = EAq ℓ (cid:0)σ(θ θ(z)A), (cid:0)πW(cid:1) (cid:35) = , (cid:1) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) where again represents the population win matrix, with entries the optimization problem in (5) can be equivalently rewritten as ba = σ(θ(z)b θ(z)a). Thus, maximize πM θ(π) subject to πc C, where θ(π) = argmin θR EAq (cid:104) (cid:16) ℓ σ(θ θ(z)A), (πW)A (cid:17)(cid:105) . Examining the first-order conditions of the inner optimization problem for θ(π) shows that the solution satisfies (cid:88) qA σ(cid:0)θ(π) θ(z)A (cid:1) = πWq. (9) Define R(π) = πWq, G(θ) = qA σ(θ θ(z)A). (cid:88) Then θ(π) = G1(R(π)). Since G1 is strictly increasing, maximize π θ(π) maximize π R(π). Thus, the problem reduces to: which is exactly the problem in (6)."
        },
        {
            "title": "B Additional theory",
            "content": "maximize πM , πcC πWq, B.1 Aggregating leaderboards via averaging The BT model tells us that for all Z, log (cid:18) P(Y = 1 = x, = z) (cid:19) 1 P(Y = 1 = x, = z) = xθ(z). 19 Thus, EZQ (cid:20) log (cid:18) P(Y = 1 = x, Z) (cid:19)(cid:21) 1 P(Y = 1 = x, Z) = (cid:90) (cid:125) θ(z)dQ(z) . zZ (cid:124) (cid:123)(cid:122) θ(Q) That is, taking (weighted) average of the values of θ(z) leads to predictor of the expected log-odds. This method has two downsides: firstly, increasing the mth coordinate of θ(Q) does not mean that model is more likely to win against other models on average. Secondly, the function θ(Q) does not have simple relationship with the win rate. This motivates the need for the aggregation metric from Section 2.1.1."
        },
        {
            "title": "C Additional regression tests",
            "content": "Figure 8: Regression test on Llama models with creative writing and math prompts. The percentages shown signify win rates against Llama-3-70B under the BT coefficients predicted from P2L-7B. 20 Figure 9: Regression test on Llama models with instruction following and coding prompts. The percentages shown signify win rates against Llama-3-70B under the BT coefficients predicted from P2L-7B. 21 Figure 10: Regression test using grounded Rao-Kupper. We show the strengths of different OpenAI models on various topic clusters based on P2L-7B with grounded RK regression head (see Section 2.2) and dataset of unlabeled prompts. The percentage represents the sigmoid of the model coefficient. Because the RK model is grounded, this corresponds roughly to signal of the models reliability, i.e., its tendency to produce an answer that exceeds the voters minimum bar of quality. The results show strong category-specific variability in performance; for example, GPT-4o-mini and o1 have roughly the same reliability in the category Suspenseful Horror Story, but not Arithmetic Operations and Calculations. We can also see that some categories are more difficult in general for LLMs to answer reliably, and thus we see larger performance improvements from test-time compute models like o1 and o1-mini. Figure 11: LiveBench cost routing. Comparison of the P2L cost-aware router and static models on LiveBench under various inference-cost constraints. The left plots show each models overall LiveBench performance at different maximum cost thresholds, while the right plots display models relative rankings across multiple categories at the specific cost limit. By adaptively allocating prompts to cheaper or more expensive models when advantageous, the P2L router consistently matches or surpasses the best single model within each budget. 23 Model LiveBench Math Coding Reasoning Language P2L-7B claude-3-5-sonnet-20240620 claude-3-5-sonnet-20241022 P2L-1.5B P2L-3B P2L-0.5B P2L-135M P2L-360M athene-v2-chat gpt-4o-2024-05-13 qwen2.5-72b-instruct gpt-4-turbo-2024-04-09 mistral-large-2407 chatgpt-4o-latest-20241120 gemini-1.5-pro-001 llama-3.1-70b-instruct llama-3-70b-instruct mixtral-8x22b-instruct-v0.1 llama-3.1-8b-instruct mixtral-8x7b-instruct-v0. Score 59.3 59.2 59.0 58.4 57.8 57.0 56.2 54.9 53.4 52.8 52.6 51.2 50.4 49.4 44.2 42.4 41.7 37.5 26.3 22.1 51.9 51.3 51.3 55.3 49.6 51.9 48.9 52.4 53.4 42.7 52.3 40.3 48.4 37.7 36.2 34.4 26.3 28.0 19.5 12.4 65.2 63.5 66.8 67.5 66.8 59.6 63.5 58.1 56.9 50.4 55.6 45.8 45.8 44.4 33.7 32.9 28.7 32.3 14.5 10.6 50.0 54.7 50.0 48.0 50.7 50.7 50.0 44.0 48.0 47.3 47.3 52.7 44.0 44.7 34.0 34.7 40.0 36.0 18.7 23.3 56.5 56.8 57.0 51.4 53.3 51.7 47.1 44.1 37.5 49.3 36.0 45.3 40.5 43.7 37.6 36.4 36.3 27.9 17.8 12. Instruction Following Data Analysis 75.8 72.3 74.1 71.9 70.4 73.4 74.1 74.4 74.6 72.4 73.3 68.4 73.1 74.1 68.9 68.9 68.5 65.5 53.9 46.1 56.3 56.7 54.9 56.7 56.2 54.8 54.0 56.7 50.2 54.4 51.1 54.5 50.4 51.7 54.8 47.3 50.7 35.5 33.3 27.4 Table 1: LiveBench performance comparison. Comprehensive evaluation of language models across seven capability categories: overall LiveBench score, mathematics, coding, reasoning, language understanding, instruction following, and data analysis. Results show performance comparison between p2l models at different parameter scales (135M to 7B), Claude-3.5 Sonnet versions, and other leading language models including GPT-4, Gemini, and LLaMA variants. All models were evaluated using identical inference settings as those employed in Chatbot Arena to ensure fair comparison. Scores are presented as percentages, with the highest score in each category shown in bold and second-highest underlined. P2L-7B achieves top performance in LiveBench Score (59.3) and Instruction Following (75.8), while maintaining competitive performance across other categories."
        },
        {
            "title": "D Additional information",
            "content": "D.1 Model list The full list of models is: athene-v2-chat [12], chatgpt-4o-latest-20241120, claude-3-5haiku-20241022, claude-3-5-sonnet-20240620, claude-3-5-sonnet-20241022 [3], deepseekv3 [20], gemini-1.5-flash-001, gemini-1.5-flash-002, gemini-1.5-pro-001, gemini-1.5-pro002 [34], gemini-2.0-flash-exp, gemini-2.0-flash-thinking-exp-1219, gemini-exp-1206, gemma2-27b-it, gemma-2-9b-it [35], glm-4-plus, gpt-4-1106-preview, gpt-4-turbo-2024-04-09 [25], gpt-4o-2024-05-13, gpt-4o-2024-08-06, gpt-4o-mini-2024-07-18 [26], llama-3-70b-instruct, llama-3.1-405b-instruct-fp8, llama-3.1-70b-instruct, llama-3.1-8b-instruct, llama-3.370b-instruct [1], mistral-large-2407, mixtral-8x22b-instruct-v0.1, mixtral-8x7b-instructv0.1 [16], o1-2024-12-17, o1-mini, o1-preview [15], qwen2.5-72b-instruct [36], and yi-lightning [39]."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}