{
    "paper_title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics",
    "authors": [
        "Lennart Wachowiak",
        "Andrew Coles",
        "Gerard Canal",
        "Oya Celiktutan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the growing use of large language models and conversational interfaces in human-robot interaction, robots' ability to answer user questions is more important than ever. We therefore introduce a dataset of 1,893 user questions for household robots, collected from 100 participants and organized into 12 categories and 70 subcategories. Most work in explainable robotics focuses on why-questions. In contrast, our dataset provides a wide variety of questions, from questions about simple execution details to questions about how the robot would act in hypothetical scenarios -- thus giving roboticists valuable insights into what questions their robot needs to be able to answer. To collect the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots performing varied household tasks. We then asked participants on Prolific what questions they would want to ask the robot in each portrayed situation. In the final dataset, the most frequent categories are questions about task execution details (22.5%), the robot's capabilities (12.7%), and performance assessments (11.3%). Although questions about how robots would handle potentially difficult scenarios and ensure correct behavior are less frequent, users rank them as the most important for robots to be able to answer. Moreover, we find that users who identify as novices in robotics ask different questions than more experienced users. Novices are more likely to inquire about simple facts, such as what the robot did or the current state of the environment. As robots enter environments shared with humans and language becomes central to giving instructions and interaction, this dataset provides a valuable foundation for (i) identifying the information robots need to log and expose to conversational interfaces, (ii) benchmarking question-answering modules, and (iii) designing explanation strategies that align with user expectations."
        },
        {
            "title": "Start",
            "content": "What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics LENNART WACHOWIAK, Kings College London, CDT in Safe and Trusted AI, UK ANDREW COLES, Kings College London, UK GERARD CANAL, Kings College London, UK OYA CELIKTUTAN, Kings College London, UK With the growing use of large language models and conversational interfaces in humanrobot interaction, robots ability to answer user questions is more important than ever. We therefore introduce dataset of 1,893 user questions for household robots, collected from 100 participants and organized into 12 categories and 70 subcategories. Most work in explainable robotics focuses on why-questions, such as Why did you clean the bathroom but not the kitchen?. In contrast, our dataset provides wide variety of questions, from questions about simple execution details to questions about how the robot would act in hypothetical scenarios thus giving roboticists valuable insights into what questions their robot needs to be able to answer. To collect the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots performing varied household tasks. We then asked participants on Prolific what questions they would want to ask the robot in each portrayed situation. In the final dataset, the most frequent categories are questions about task execution details (22.5%), the robots capabilities (12.7%), and performance assessments (11.3%). Although questions about how robots would handle potentially difficult scenarios and ensure correct behavior are less frequent, users rank them as the most important for robots to be able to answer. Moreover, we find that users who identify as novices in robotics ask different questions than more experienced users. Novices are more likely to inquire about simple facts, such as what the robot did or the current state of the environment. As robots enter environments shared with humans and language becomes central to giving instructions and interaction, this dataset provides valuable foundation for (i) identifying the information robots need to log and expose to conversational interfaces, (ii) benchmarking question-answering modules, and (iii) designing explanation strategies that align with user expectations. CCS Concepts: Human-centered computing Empirical studies in interaction design; Interaction design process and methods; Computing methodologies Natural language processing; Information systems Information retrieval; Computer systems organization Robotics. Additional Key Words and Phrases: explainable AI, XAI, robotics, user-centered AI, humanrobot interaction, conversational AI, transparency, trust, communication 1 Introduction In recent years, robots have moved beyond executing narrow tasks in defined spaces. They are now starting to be deployed in complex and dynamic environments shared with humans. Examples include robots in healthcare and assisted living [47], or in the service industry, for instance, in restaurants [18]. When inhabiting the same space as people and executing tasks for or with them, natural language communication becomes essential. Language now also plays central role in instructing robots, as in the case of visionlanguageaction models (VLAs). VLAs are large visionlanguage models fine-tuned to output robot actions given language instruction [46]. Widely publicized VLA demos often feature household tasks, such as cleaning or tidying up further areas with ample human contact. When interacting with such robot, users might naturally have many questions for the robot. For example, users may want to Authors Contact Information: Lennart Wachowiak, lennart.wachowiak@kcl.ac.uk, London, Kings College London, CDT in Safe and Trusted AI, UK; Andrew Coles, Kings College London, London, UK; Gerard Canal, Kings College London, London, UK; Oya Celiktutan, Kings College London, London, UK. 1 5 2 0 2 8 1 ] . [ 1 5 3 4 6 1 . 0 1 5 2 : r 2 Wachowiak et al. understand how the robot works and what its decision-making process looks like. robots ability to explain itself in such situations fosters safer interactions and is known to help users calibrate their trust levels appropriately [55]. Due to the rise in text generation capabilities [1, 8], giving natural language answers to user questions is easier than ever. However, for the answers to be truthful, they must be based on the robots true capabilities, underlying decision-making, and execution traces. If the robots question-answering module lacks this data, large language model (LLM) might simply confabulate the answers [21]. Without having relevant logs and access to run explainability methods, the LLMs answers can devolve into statistically likely language patterns unrelated to reality. Therefore, roboticists need to know what data should be accessible to such question-answering module. With this paper, we provide novel large-scale dataset of questions users have for household robots. Our dataset enables roboticists to see what questions their robot might need to answer. Second, these questions will help roboticists understand what type of data needs to be collected by the robot during task execution and made accessible to the natural language interface. Third, the dataset and its question categories can serve as strong foundation, grounded in real user needs, for benchmarking XAI and question-answering systems. Beyond providing the dataset, our paper addresses the following research questions in an exploratory manner: RQ1: What type of user questions should service robots be able to answer? RQ2: What type of questions do users consider especially important? RQ3: Do the types of questions and the importance the user places on the robots question-answering capabilities vary with the users robot experience and attitudes towards robots? Our main findings and contributions can be summarized as follows: We provide dataset of 1,893 user questions. We have hierarchically categorized the questions into 12 main categories and 70 subcategories (RQ1). The resulting categories show wide variety of question types, many of which are underexplored in the current explainable robotics literature. Beyond the causal why-questions often explored in XAI, the categories include questions about execution details, the robots capabilities, how and if it can handle potential difficulties, technical details of the robot, the robots source of knowledge, and many more. We find that users assign different levels of importance to different question categories (RQ2). However, unifying theme is that questions about safety, failure, and potential issues rank highly. Questions about how the robot would deal with hypothetical scenarios and how it ensures its actions are correct rank highest. In contrast, the why-questions popular in XAI literature have the second-lowest average importance score. We find that users with more positive attitudes towards robots also think it is more important for robots to be able to answer their questions (RQ3). Additionally, we find that the types of questions asked differ based on persons level of experience with robots. Novices used larger allocation of their questions to ask about execution details and the state of the environment. In contrast, more experienced users were more likely to ask questions about how the robot would deal with potentially difficult situations or how it makes decisions. 2 Related Work 2.1 Explainable AI/Robotics Explainability has become core topic in the robotics [3, 42], AI planning [14], and machine learning [2] literature. By providing the reasons for the robots actions and decisions, explanations are supposed to: enable users to understand the robots decision-making and be better at predicting future behavior [9, 50], allow users to calibrate their trust levels appropriately [3, 13, 32, 54], What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 3 improve humanagent team performance due to enhanced knowledge transfer [54], and improve how an agent is perceived [7]. myriad of XAI modules for robots and agents have been developed in the past, typically focusing on specific functionalities of the robot, such as explainable vision [5], classical planning [6, 14], or motion modules [4]. As in our work, previous research has also emphasized the importance of considering user needs and requirements when developing XAI modules, for example, through participatory design [10, 11, 37, 50]. For instance, Gebell√≠ et al. ran co-design study in geriatric unit to understand how to meet the needs of nurses in making robot legible and explainable [17]. With the rise of LLMs (e.g., [1, 19]), these explainability modules can interface with users through natural language. LLMs are now used to answer various user questions by retrieving relevant information from logs or specific XAI modules and synthesizing answers. For example, Sobr√≠n-Hidalgo et al. [43] propose to collect the execution logs of robot module in vector database. Once the robot is asked to answer an explanation-seeking question, an LLM provides the answer using Retrieval Augmented Generation (RAG), thus querying the relevant bits from the database and summarizing the logs for the user. Moreover, Gebell√≠ et al. [16] use LLMs to build user profile based on past interactions so that the generated explanations align more closely with that users mental model. Lastly, LeMasurier et al. [29] compare LLM-generated with templated explanations, for robot controlled via behavior tree [45]. However, the user questions that can be answered depend on the information available to the LLM. Being aware of all possible user questions and their respective importance and frequency is crucial to developing robots that log the right data (so that the LLM is less likely to confabulate it) and implement relevant XAI methods. 2.2 Question Categorizations In our previous work, we categorized the robot modules (e.g., navigation, manipulation, vision, etc.) for which explainability modules have been developed in the explainable robotics literature [51]. Furthermore, we provided categorization of situations, often caused by an interaction rupture or unexpected event, in which users demand an explanation from the robot (e.g., robot errors, uncertainty, suboptimal behavior) [53]. While these taxonomies provide us with insights into the HRI contexts in which questions are asked, in this work, we present dataset of concrete natural language questions users ask in variety of household-related robotics tasks. McGuinness et al. [34] present categorization of user questions in the context of software agent supporting the purchase of laptop. However, the resulting taxonomy is less detailed than what we present in the following; they provide no dataset of natural language questions, and the topic is not concerned with robotics. Raza et al. [40] explore what questions users ask robots in the context of users encountering robot at universitys open day. The 179 questions asked by users were either standard conversation openers (e.g., How are you?), or were concerned with facts and opinions about the university, general facts, or the robot itself (personality, beliefs, goals, abilities). In contrast, our study focuses on users encountering robot executing specific task, rather than social small talk setting. We prompt the users to ask questions related to the robot in the task context, as we are not interested in creating dataset of questions in the context of social chit-chat, such as What is your favorite movie? or How is the weather today?. Lastly, our study provides dataset that is more than 10 times larger and is coded into categories relevant to researchers building XAI and question-answering modules, enabling users to better understand the robot. Additionally, there exists small set of natural language corpora of humanrobot dialogues. For instance, the SCOUT corpus consists of users instructing remote robot to explore its environment [31], or the Vernissage Corpus consists 4 Wachowiak et al. of conversations in which robot talks about paintings and subsequently quizzes people about them [23]. However, these corpora and interactions are not designed to analyze users questions for the robot. 3 Method How the Dataset was Collected 3.1 Study Procedure Participants were recruited on Prolific, from where they accessed our questionnaire created in Qualtrics. The questionnaire form is publicly available (see Appendix A). After reading an information sheet and consent form, each participant was required to watch four or five video scenarios and read two text scenarios. The stimuli order was randomized. Videos and texts were randomly selected out of larger pool of 22 stimuli described in Section 3.2. After encountering video or text scenario, users had to provide natural language questions they would want to ask the robot. Specifically, users answered the following prompt: What questions would you want the robot to be able to answer about its behavior related to the task? Please provide questions directly related to what you saw in the video. Importantly, we are looking for questions in response to which the robot has to explain itself and its behavior. We do not look for general questions about the world, such as How is the weather? Based on this prompt, each user had to provide two questions. We collected two further questions with the following prompt: Beyond questions related directly to the video, what type of questions about its task and behavior do you think robot like in the video should be able to answer? In addition, users also provided importance scores for their questions, answering the following question on Likert scale from 1 (Not at all important) to 5 (extremely important): How important is it to you that robot you interact with can answer such question? In the end, users provided some demographic data and filled out 10-item questionnaire on peoples general attitudes towards robots. The attitude scores are measured via the recent, validated GAToRS questionnaire [27], using the two positive subscales (7-point Likert scales) for personal-level and societal-level attitudes toward robots. An example of statement probing societal-level attitudes towards robots is Robots are good thing for society, because they help people. An example of statement probing personal-level attitudes towards robots is would feel relaxed talking with robot. 3.2 Video and Text Stimuli To determine the types of questions users have for robots assisting them with everyday tasks, we present users with video or text scenarios. Videos. The videos depict robots accomplishing multistep tasks in home, for example, making the bed and then removing trash from the floor. Table 1 describes each videos content and provides an example frame. The videos were What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 5 sourced from publicly available robot demos, either teleoperated or featuring state-of-the-art policies. The original videos are cut to show the execution of one self-contained task, such as making specific meal. Specifically, we included videos from Googles Gemini Robotics [46], Physical Intelligence [22], the Mobile ALOHA project [15], Figure1, Astribot2, Hello Robot3, and 1X4. The average video length was 35.4 seconds (SD=13.8). Links to the original, uncut videos can be found in Appendix B. Texts. In addition to the videos, we wrote six text reports of robots actions while its user was away. As with the videos, users were asked to write down the questions they might have for the robot in this scenario. An example text scenario is the following: Imagine you have robot at home that takes care of various household tasks. After returning from work, you check what your robot did today. It presents you with the following list. 1. moved from the living room to the kitchen. 2. In the kitchen, cleaned the dishes. 3. moved from the kitchen to the living room. 4. In the living room, picked up various items and put them back where they belong. 5. In the living room, cleaned the windows. 6. returned to my charging station. list of all text scenarios used can be found online (see Appendix A). 3.3 Participants The final dataset consists of 1,893 questions from 100 participants. This excludes participants who failed any attention check or gave nonsense answers in the free-text input fields. On average, participants were paid an hourly wage of 9. GBP, which falls in Prolifics recommended range at the time of the study. Filters were set to allow participants who are fluent in English and who have Prolific approval rate of 100%, having participated in at least 5 studies. Of the accepted participants, 50 were female and 50 male. The age range was 18 to 73, with an average age of 32 years (SD=13). The respondents lived in the UK (30 participants), the USA (26), South Africa (17), Poland (8), Spain (4), Canada (3), and 12 other countries with two or fewer participants. Of these, 67 were white, 21 Black/African/Caribbean, 8 Asian (Indian, Pakistani, Bangladeshi, Chinese, any other Asian background), 2 mixed (two or more ethnic groups), 1 other (Arab or other), and 1 preferred not to say. Out of the 100 participants, 4 had programmed robot before, 21 studied computer science, robotics, or engineering, and 33 reported having interacted with robot. The participant statistics are visualized in Figure 1. Ethical clearance was obtained by the Kings College ethics committee (MRSP-24/25-50343). 3.4 Methods for Data Analysis Category Coding (RQ1). We coded all questions into categories in order to give structure to the dataset and make it into an actionable resource for future robotic system design. The creation of new coding scheme was necessary 1https://www.figure.ai/ 2https://www.astribot.com/en 3https://hello-robot.com/ 4https://www.1x.tech/ 6 Wachowiak et al. Example Frame Video Description Robot Length The robot finishes salad bowl by adding nuts and peas using spoon and pliers [V1]. ALOHA-2 (Gemini-Robotics) 35s The robot packs lunchbox with nut mix and an orange [V2]. Apollo (Gemini-Robotics) Two robots collaborate on putting groceries away into the fridge and wooden box [V3]. Figure 02 The robot closes cupboard, cleans spill, and puts away dishes into the sink [V4]. Mobile Manipulator from Physical Intelligence The robot tidies up bedroom by putting away clothes, removing trash, and making the bed [V5]. Mobile Manipulator from Physical Intelligence The robot sorts loose items into different shelves [V6]. Astribot S1 The robot unpacks clothes from dryer into basket [V7]. The robot folds and stacks three items of clothes [V8]. Mobile Manipulator from Physical Intelligence Mobile Manipulator from Physical Intelligence The robot folds t-shirt and hands it over to person [V9]. 1X Eve Two robots tidy up kitchen. They clean spill, put away loose items, and fill the dishwasher [V10]. Hello Robot Stretch 3 robot adds cat food to bowl [V11]. Astribot The robot adds dough to waffle iron and puts the finished waffle on plate [V12]. Astribot S1 The robot adds tea leaves and hot water to pot. It then pours the tea and serves it to person [V13]. Astribot S1 The robot prepares two eggs and shrimp. It then mixes the two ingredients in stir-fry [V14]. Mobile ALOHA The robot prepares some chicken meat in the pan, adding sauces and seasoning [V15]. Mobile ALOHA Table 1. Overview of the video stimuli used. The links to the original videos can be found in Appendix B. 19s 43s 67s 25s 33s 41s 59s 27s 33s 10s 36s 27s 35s 41s What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 7 Fig. 1. Participant Statistics: their experience with robots (Likert scale 17), personal/societal-level attitudes towards robots (measured via GAToRS [27], an average of multiple items using 17 Likert scale), age, gender, ethnicity, and binary statistics on whether they programmed robot before, interacted with one, or studied computer science or engineering. as previous categorizations [34, 40] did not reflect the richness of questions we encountered in the context of users communicating with robots conducting household tasks. Moreover, manual annotation was necessary to ensure high quality, as automated methods, such as clustering, are difficult to interpret and struggle to capture fine-grained differences in user intent. Instead, the questions were coded by the first author of this paper in an inductive, bottom-up manner. No pre-defined (top-down) codes were used. We iteratively annotated the dataset of questions, initially with descriptive terms close to the content. We then iteratively merged related codes into higher-level themes. The final codes are structured in two-level hierarchy. After annotating all questions and defining all categories, the main annotator did second pass over all questions to guarantee that the annotations agree with the final category definitions. To ensure understandability and reproducibility of the codes, we provide codebook with definitions and examples of each code in Tables 24. Moreover, second person re-annotated 100 sentences, which were randomly sampled and balanced across categories, i.e., contained 8 samples per category. Comparing the two annotations, we find that the two annotators agreed on 79% of the questions. Computing Cohens Kappa, chance-corrected coefficient of agreement, we obtain value of .77 (CI = [.68, .86], ùëù < .0001). This value corresponds to substantial agreement according to Landis et al. [28]. With three occurrences, the most common disagreement was between self/task-assessment and execution-details. Resultingly, we improved the definitions in the codebook to make the differences clearer. Other disagreements can be traced back to differences in the interpretation of the question. For example, Are you able to tell me if were low on forks and If it can remember where it placed each item? can be interpreted as the user asking for whether the robot has the ability. However, it can also be interpreted as way of asking for the availability of forks and the item locations. Such disagreements can only be resolved by asking the participant who provided the question. Future extensions of this dataset could benefit from participants providing example answers they would deem satisfying in order for the annotators to better understand their expectations. 8 Wachowiak et al. Statistical Analysis Question Importance Ranking (RQ2). To analyze the differences in assigned importance of the different question categories, we use linear mixed-effects model [30]. It is suited to our data set, where individual data points are non-independent, as multiple questions (of the same or different question categories) are rated by the same participant (within-participant). Simply averaging each participants ratings would mean loss of valuable information, which is why we use linear mixed-effects models. Therefore, we model the participant identifier as well as the stimulus identifier as random effects in our mixed-effects model. We use random intercepts model, meaning the observations per participant/stimulus are moved by dedicated intercept. The fixed effect is the question category. The outcome variable is the importance score. linear mixed-effects model, as described, can be written down as: ùëñùëöùëùùëúùëüùë°ùëéùëõùëêùëí ùëêùëéùë°ùëíùëîùëúùëüùë¶ + (1ùëùùëéùëüùë°ùëñùëêùëñùëùùëéùëõùë°) + (1ùë†ùë°ùëñùëöùë¢ùëôùë¢ùë†) The model thus accounts for within-participant and within-stimulus correlation, which in turn provides us with valid standard errors. The model was fit by restricted maximum likelihood. We correct for multiple comparisons via the HolmBonferroni correction [20]. This ensures the family-wise error rate, i.e., the probability of making one or more false discoveries, stays below ùõº < .05. Statistical Analysis Influence of Attitudes towards Robots and Robotics Experience on Question Importance (RQ3). Different types of users (e.g., based on cognitive styles or socio-cultural backgrounds [25, 26]) might have varying explanation needs and preferences; changing task circumstances can further complicate explanation needs [52, 53]. For instance, explanations have been found to be more effective at repairing trust when persons attitude towards robots is more positive [12]. The researchers explain this phenomenon by stating that explanations can offer rational or justification for users positive attitudes, thus reducing the users potential for experiencing cognitive dissonance after being confronted with robot error. In our dataset, we measure the importance score people assign to the questions they pose. Given the findings in previous research, we decided to explore whether person with more positive attitudes towards robot, as measured by the GAToRS questionnaire [27], also thinks it is more important for robot to be able to answer their questions. Generally, people who see robots more positively might also expect more communicative capabilities from robots (to not fall into state of dissonance with their positive pre-conceptions), as well as simply show greater interest in their communicative capabilities. On the other hand, users with more negative attitudes might especially be interested in explanation capabilities as tool to build trust. Moreover, we wanted to see if the question importance scores are influenced by users experience with robots. For this analysis, we compute correlations between different user-related variables. Spearmans rank correlation coefficient ùúå [44] measures the monotonic relationship between variables. It ranges from -1 to +1, with positive/negative value indicating that the variables tend to increase/decrease together. Again, we correct for multiple testing via the HolmBonferroni correction [20]. Lastly, we run multiple linear regression [36] to gain further insight into how user characteristics are related to how much importance they place on the robots ability to answer their questions. To do so, we use attitudes (societal/personal-level), robotics experience as the predictor variable, and question importance as the outcome variable. Given that the attitude scores are averages of multiple Likert scale items, we can treat them as continuous variables. While the robot experience variable represents scores on 15 scale, it is accepted practice to use such scores as numeric variable in statistical models like linear regression [39]. Statistical models and tests are run via the Python libraries SciPy [49] and statsmodels [41]. What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 4 Results Dataset of Questions for Robot The results of the study, including participant answers and analysis code, are made available as public dataset5. Overall, we collected 2,052 questions. From these, we excluded 143. Most of the excluded questions were commands in the form of question, for example, Could you clean the fridge?. The remaining excluded questions are hard-tointerpret phrases with missing words or unclear meaning, as well as very rare cases of users writing non-serious joke questions. On average, each participant provided 18.93 (SD=2.82) questions to the dataset. The average question length is 9.73 words (SD=4.16). The questions asked in the dataset show linguistic diversity. Most users speak to the robot directly (67%), for example, What kind of ingredients did you put in the salad?. In contrast, other users refer to the robot (or sometimes the environment) in third-person (30%), for example, Why did it flip the waffle at the end?. In rare cases (2%), the questions are asked from 1st person perspective, for example, What items do we have to make meal?. Some of the questions can only be made sense of in the concrete context of the text or video scenario. For example, What about the bathroom floors? only makes sense in the context of the robot having reported mopping the kitchen floor. In terms of extracted tense, most questions are written in the past (50%) or present (48%), and small subset in future tense (2%). However, the (auxiliary) verb tense does not necessarily need to indicate the future for the question to be about potential future actions, as in Are you considering adding any other ingredients other than the ones on the table?. 4.1 Question Categories Our dataset shows high variety of questions and themes. Figure 2 gives an overview of which categories and subcategories are the most prevalent. Tables 2, 3, and 4 provide the definitions of the main categories as well as each of their three most common subcategories. While some question (sub)categories are very prevalent, for example, questions of the form can you do <x>, there is long tail of rare question categories. This variety of questions and the long tail of rare question types show how difficult it is to develop question-answering components in robotics that can reliably work in all situations. Execution Details. The most common in the dataset are questions in the category execution-details, which deal with the specifics of what happened during the robots task execution. These are often what-questions, querying, for instance, what the robot did, what objects it used, or what goal it was trying to achieve. Environment State. similar category in the sense that it mostly deals with simple facts is the environment-state category. In contrast to the execution-details category, these questions are not about what the robot did but are about the state of the external environment, e.g., Are there any foods that have expired?. Abilities. The second largest category is questions that ask the robot about its capabilities. These questions ask about specific additional abilities, modified versions of what the robot already did, or all the abilities the robot has in specific area. Less commonly, users also asked about the limits of robots abilities, for instance, what would be too heavy or too far away. Time. Beyond the question of whether robot can achieve task, people were also interested in how long the robot needs to execute task. In addition, this category includes questions about when something happened or will happen. 5https://github.com/lwachowiak/xai-questions-dataset 10 Wachowiak et al. Code/Category Definition Example from the Dataset environment-state object-properties availability people/pets other execution-details what-actions what-objects how-was-it-done other future-actions what-needs-to-be-done what-next assistance-need other mental-state check-understanding/beliefs emotions opinion/likes other how-did-you-know goal-properties goal-achieved object-properties other Questions (Qs) about the state/properties of the external environment Qs about what is available, if something is available, or the quantity that is available Qs about people/pets the robot interacted with Qs about how the user can/should interact with the environment Did the dishes look rinsed when you put them in the dishwasher? What items do we have to make meal? Was the cat afraid of you? Qs about whether the robot executed specific action Qs about environment objects that were relevant during the task Qs about how exactly the robot did the task Did you power off the washing machine? What kind of ingredients did you put in the salad? What temperature did you cook the food at? Qs about further details of the task execution, e.g., where something was put, what goal was pursued, or what amount order actions were executed in Qs about which tasks in the house need to be done now or in the future Qs about what actions/tasks the robot will do next Will you dry the dishes after washing Does the rug need to be vacuumed? Qs about whether the robot requires help from the user Qs about what external requirements need to be fulfilled for the robot to be able to do the task; and about how it can improve in the future them? Do you have anything that can move out of the way so you can work more efficiently? Qs about the robots understanding of the world, tasks, and instructions Qs about the robots emotions (joy, boredom, etc.) Qs about the robots opinions and likes Qs about other mentalistic concepts, e.g., the robots ideas, inner thoughts, or desires What tasks do you believe is part of cleaning the room? Did you enjoy the task? Whats your perfect lunch box? Qs about how the robot knows specific desiderata of its goal/task Qs about how the robot knows it achieved goal or subgoal Qs about how the robot knows an object has specific property Qs about how the robot knows various other things, such as the right steps to take, which task to prioritize, or which objects are relevant to goal How did the robot know the right time to feed the cat? How does the robot know when the food is ready to be served? How do you know the difference between clean and dirty dishes? Table 2. Description of the different question categories. For each main category, the three most frequent subcategories are defined. The less frequent subcategories are summarized under other. What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 11 Code/Category Definition Example from the Dataset how-did-you-decide goal-properties goal-objects task-priority other potential-issues what-if how-to-ensure-correctness does-it-happen-that self/task-assessment difficulty did-you-ensure correctness other technical-details how-does-it-work who-has-control battery-status other time how-long at-what-time frequency Qs about how the decision about goal details, like location, arrangement, or amount were made Qs about which items were part of the goal(- configuration) Qs about how the priority and order of tasks were determined Qs about how decisions were made, including regarding the actions taken, when to stop, or object properties determined How did you decide where each toy should go when you clean up? How did you pick the lunchbox items? How do you decide which household tasks should be done first? Qs about hypothetical scenarios involving challenges, errors, and changes in external circumstances Qs about how the robot ensures that task is executed correctly Qs about whether specific potential issues can arise What happens if you drop piece of clothing? How do you detect and correct mistakes during folding? Do you ever mistakenly misplace things? Qs about whether the task was difficult or easy for the robot Qs about whether the robot ensured it is doing things correctly/safely/etc. Qs about whether the robot did everything correctly or made any errors the robots general performance, Qs about confidence-levels, task success, and whether any problems arose How easy did you find closing the cupboard? Did you check the smoke alarm was functional before you started cooking? Is it cooked enough to be safe to eat? Qs about technical details of how the robot functions Qs about autonomy and who can give commands Qs about battery levels Qs about hardware and software specifics How do you see the objects around you? Are other humans able to give you tasks? Do you need to be charged? Qs about time lengths (task, actions, expiry dates, etc.) Qs about when something happened/is going to happen Qs about how frequently something occurs How long did it take you to tidy the room? At what time did you receive the mail? How many times day do you feed the cat? How many clothing items can you fold in an hour? task-count-within-time other Qs about the number of tasks the robot can accomplish in set time frame Qs about frequencies of actions/tasks, and about comparing tasks lengths Table 3. Description of the different question categories. For each main category, the three most frequent subcategories are defined. The less frequent subcategories are summarized under other. 12 Wachowiak et al. Code/Category Definition Example from the Dataset what-abilities extra-ability ability-modification ability-list other why why-did-you why-not-alternative why-issues other Qs about whether the robot can execute an additional skill mentioned in the question Qs about whether the same skill can be done in different way Qs that prompt the robot to list its abilities either generally or of specific subcategory Qs about Can you wash the dishes for me too? Can you do that faster? What other kind of chores can you help me with? Qs about why the robot did something (without the question explicitly proposing contrast/alternative) Qs about why the robot did not take an alternative course of action, explicitly mentioned in the question Qs about why the robot failed or was unable to do task Qs about why the robot did not do an additional task mentioned in the question or why it followed specific action order Why did you put the eggs in the fridge? Why did you throw away the milk immediately instead of asking me first? Why did you bump into the bed? Table 4. Description of the different question categories. For each main category, the three most frequent subcategories are defined. The less frequent subcategories are summarized under other. There is also small set of questions about the frequency of robot actions, as well as how many tasks the robot can achieve in fixed amount of time. Self/Task-Assessment. In this category, people ask the robot to assess how the task went, for example, if there were any problems, if the robot was successful, or if the task was difficult. Why. Participants also provided many why-questions, category of questions studied in detail throughout the XAI literature. Questions in this category ask the robot to give reasons for its actions. These questions are usually action-focused. Sometimes, participants provide an explicit contrast, so the robot has to answer why it chose one course of action over another. An example of contrastive why-question is Why did you decide to pick an orange and not any other fruit?. Such an explicit contrast is given in 28% of the why-questions. However, even in the case of questions without explicit contrast, people would expect contrastive explanations according to XAI literature [35]. For instance, when asked Why did you choose the orange?, the robot could automatically infer the contrast by identifying what other fruits were on the table as alternatives. Furthermore, why-questions are asked for varying reasons. For instance, in the question Why did you put the tools away in the places that you did so next time can find them?, the participant explicitly mentions that causal explanation would help them to better predict the robots actions in the future. How Did You Decide. With questions of this category, users ask the robot about how it made decision. Similar to the questions in the why category, questions here may be answered by referring to causes and alternatives. However, depending on the user intent, answers not referring to causes are also possible. Specifically, how did you decide... can be interpreted as placing more emphasis on the underlying decision-making procedure (corresponding to Marrs What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics algorithmic level [33]) rather than on the causes. For instance, the question How did you decide how much water to use on the plants? could be answered by saying measured the soil moisture and compared it to the recommended target, thus emphasizing the algorithm employed. Common subcategories concern the decision-making process behind the details of the goal, such as which objects to involve or goal-properties like where to place something. How Did You Know. While similar to the previous two categories of whyand how-did-you-decide-questions, this category once again slightly shifts the focus. Focusing less on the decision-making process, this category emphasizes the robots knowledge and where this knowledge comes from. Examples of knowledge sources can be the robots perception, learning, or prior information. Common subcategories are concerned with properties of the goal and the environment, with questions such as How did the robot know the right time to feed the cat? (goal property) or How do you know if an item is dishwasher friendly? (object property). Future Actions. This category of questions deals with what the robot plans to do next. The most common subcategories include questions on the robots next actions, whether it needs assistance, and what needs to be done. Mental State. This category of questions targets the robots mentalistic concepts. The largest subcategory contains questions that probe the robots understanding and beliefs, e.g. What tasks do you believe are part of cleaning the room?. The second largest subcategory contains questions that ask about the robots emotions, such as whether it found the task enjoyable, boring, or frustrating. Questions from other subcategories include asking the robot about its opinions, ideas, preferences, or desires. However, whether robot should ascribe some of these mental concepts to itself in answers is controversial topic. For instance, in survey by the UK AI Security institute, 61% of participants thought that it is wrong for chatbot to express emotions, while only 19% thought it was acceptable [48]. With mental concepts such as preferences and beliefs, the same participants were more undecided; however, they still leaned towards it being unacceptable. Technical State. Questions in this category are concerned with mechanistic explanations of how the robot functions, e.g., what algorithms or models it uses or how its visual perception works. Questions in this category also touch upon the robots settings, how to change them, and who can control the robot. Lastly, the category contains questions about the robots hardware and battery functionality. 4.2 Question Importance Scores As explained in Section 3.4, we modeled the question importance via linear mixed-effects model. The question category (e.g., why-questions) is the fixed effect, while the participant ID and the stimulus ID were modeled as random intercepts. Resultingly, we find the participant-level standard deviation (0.58) to exceed the stimulus-level standard deviation (0.26), indicating greater heterogeneity across participants than across stimuli. This means that participants random intercept, which models their baseline importance, typically varies across individuals by 0.58 points on the 15 scale. Figure 3 illustrates the resulting estimated marginal means (EMMs) per category, as well as the respective 95% confidence intervals (CIs). After accounting for participantand stimulus-level variability, we find the category of questions about potential issues to be rated most important (EMM = 4.01, 95% CI = [3.82, 4.20], = 118). These are questions about what the robot would do when facing (difficult) variation of the task, and how it can ensure it acts correctly and avoids issues. Questions rated the least important are in the why-questions (3.58, [3.42, 3.75], = 190) and mental-state questions (3.53, [3.35, 3.72], = 120) categories. We find multiple statistically significant differences between the importance ratings of the categories. After correcting for multiple comparisons, we find ùëù < .001 with: Wachowiak et al. Fig. 2. Hierarchical categorization of user questions for the robot (see Tables 2, 3, and 4 for category definitions). What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 15 Fig. 3. Estimated marginal means (95% CIs) of the importance scores per question category obtained via linear mixed-effects model. Number of samples per category (n) shown on the right, and statistical significance indicated by: *** = ùëù < .001, ** = ùëù < .01, * = ùëù < .05 () potentials-issues vs. () why (difference in EMMs = 0.43, ùëù = .0007) potential-issues vs. mental-state (0.48, ùëù = .0009) We find ùëù < .01 with: how-did-you-know vs. mental-state (0.42, ùëù = .0051) potential-issues vs. what-abilities (0.37, ùëù = .0058) how-did-you-know vs. why (0.37, ùëù = .0066) Lastly, we find ùëù < .05 with: potential-issues vs. time (0.38, ùëù = .0239) self/task-assessment vs. mental-state (0.33, ùëù = .0388) how-did-you-know vs. what-abilities (0.31, ùëù = .0423) 4.3 Differences between Participant Groups To aid future research on adapting explanation capabilities to individual users, we examined the impact of users robotics experience level and attitudes towards robots on what questions the user asks and how important they think it is for robot to be able to answer their question. Correlations with Question Importance Scores. We find that users question importance rating has statistically significant correlations with users personal-level (ùúå = .33, ùëù = .0037) and societal-level (ùúå = .32, ùëù = 0.0047) attitudes towards robots. Users robotics experience is less strongly correlated (ùúå = .19, ùëù = .154) and not statistically significant. full correlation heatmap is visualized in Figure 4a. Linear Regression for Predicting the Importance Score. We examined whether users averaged question importance ratings could be predicted from their attitudes towards robots (personal/societal-level), and robot experience. The Wachowiak et al. (a) Correlations between users robot experience, avg. question importance, and personal/societal-level attitudes towards robots. *** = ùëù < 0.001, ** = ùëù < 0.01, * = ùëù < 0.05 (b) Linear regression coefficients (95% CIs) with robot experience and personal/societal-level attitudes towards robots as predictor of users average question importance rating. Fig. 4. Users rating of how important they think it is that robot can answer their proposed questions is related to their general attitudes towards robots as measured by the GAToRS questionnaire. Fig. 5. Difference in question type distribution based on robot experience. Robot experience is based on self-report on Likert scale 17. There are 76 participants with score between 1 and 3, and 24 participants with score between 4 and 7. multiple linear regression model was statistically significant overall, ùêπ (3, 96) = 5.26, ùëù = .002, explaining 14.1% of the variance in the importance score. The resulting linear regression equation is: ùê¥ùë£ùëî. ùêºùëöùëùùëúùëüùë°ùëéùëõùëêùëí ùëÖùëéùë°ùëñùëõùëî = 2.62 + 0.14(ùëÉùëíùëüùë†ùëúùëõùëéùëô-ùêøùëíùë£ùëíùëô ùê¥ùë°ùë°ùëñùë°ùë¢ùëëùëíùë†) + 0.06(ùëÜùëúùëêùëñùëíùë°ùëéùëô-ùêøùëíùë£ùëíùëô ùê¥ùë°ùë°ùëñùë°ùë¢ùëëùëíùë†) + 0.05(ùëÖùëúùëèùëúùë° ùê∏ùë•ùëù.) The linear regression shows personal-level attitudes towards robots to be the strongest predictor with coefficient of 0.14 (ùê∂ùêº = [0.014, 0.272]), indicating that the predicted importance rating increased by 0.14 points with every point What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 17 increase in the personal-level attitudes scale. Societal attitudes are less important than in the correlation analysis, likely due to the high correlation between the two attitude scales (ùúå = 0.59, ùëù < .001). Coefficients and CIs are visualized in Figure 4b. Differences in Question Categories. Figure 5 shows how users with different levels of robot experience ask different types of questions. To create the graph, participants contributions to each category were normalized, so participants contributions across all categories add up to one. This normalization was necessary as some participants contributed slightly more questions than others. Participants with low robot experience score (13, = 76) asked execution-details questions 23.3% of the time. In contrast, participants with high robot experience score (47, = 24) asked executiondetails questions only 15.7% of the time (difference = -7.6%). similar contrast exists with environment-state questions, which were asked 9.8% of the time by participants with low robot experience and 3.1% by participants with high robot experience (-6.7%). On the other hand, participants with high robot experience asked more questions, for example, in the categories potential-issues (+4.6%), how-did-youdecide (+3.5%), mental-state (+3.1%), and what-abilities (+2.9%). 5 Discussion and Conclusion In this paper, we presented dataset of 1,893 user questions that can help robot practitioners understand user needs when designing natural language interfaces. Below, we discuss the three main findings of (i) what question categories arise, (ii) what their relative importance is, and (iii) how the question types and importance scores of different user groups vary. 5.1 Question Categories We classified our data into 12 categories and 70 subcategories. This helps us understand what type of questions to expect from users. Knowing these categories also helps with understanding what type of data the robot needs to be able to access and collect during task execution. For instance, to answer questions about the environmental state, such data needs to be recorded during task execution. Beyond scanning the environment, this could even include reserving additional actions for exploration, such as checking on the location of objects. To answer questions about its capabilities, the robot needs access to hard-coded capability list or predictive model that allows it to predict what tasks it can achieve. To answer questions about its task performance, the robot needs to understand the users expectations and have reference points for its usual performance. In such manner, each category requires its own considerations of what data the robot needs to collect and have access to in order to answer the questions successfully. Regarding the question categories, we noticed that the types of questions elicited by the text and video scenarios differ. Execution details questions were much more prevalent in the text-based scenarios (36% of all questions) than in the videos (16% of all questions). This is because the text descriptions provide only high-level overview of the robots activities, whereas video scenarios already show the details of an execution. These two different modalities through which users can be informed about robot behavior offer another interesting and novel aspect to our dataset. The two modalities can serve as starting point for future investigations and inform XAI-component developers with respect to their envisioned use case (e.g., asking robots questions on-the-fly during collaborations or supervisor returning to robot having completed set of tasks). 18 Wachowiak et al. 5.2 Question Importance Scores For each question, we also asked the participants how important it is for robot to be able to answer that question. Through pairwise contrasts between the estimated marginal means from linear mixed-effects model, we found various statistical differences in importance scores between question categories. No category scores below 3.5 points on the 15 importance score Likert scale. However, some categories were ranked clearly higher than others. For example, the most important categories dealt with potential issues the robot might run into and how it would handle them. Another important category was questions through which users tried to understand how robot knows things, such as what to do or what an objects properties are. In contrast, why-questions and questions about the robots mental state (e.g., beliefs, desires) ranked lowly. When designing XAI components and natural language interfaces for robots, these findings can help with what question-answering capabilities to prioritize. 5.3 Differences between Participant Groups In the last part of our exploratory data analysis, we found that participants personal and societal-level attitudes towards robots correlated with the average importance score they gave to their questions. This correlation suggests that individuals with higher attitudes towards robots also consider it more important for robot to be able to answer their questions. possible explanation for this is that people with higher attitudes towards robots also have higher expectations regarding what robot is capable of. When exploring whether robot novices ask different questions than participants more experienced with robots, we find that novices tend to ask more questions in the categories execution-details and environment-state, which are both categories concerned with simple facts about what happened and the current state of things. This finding highlights the importance of considering ones users when designing question-answering module, as different users will ask different questions. 5.4 Limitations and Future Work Ambiguous User Intent. As mentioned in Section 3.4, interpreting some questions was occasionally challenging, and assigning them to single category was not always straightforward. Sometimes, the annotator had to infer what the participant intended with their question. For example, the question Are you not afraid the plate will fall from the kitchen counter if you place it on the edge like that? could be an inquiry into the robots assessment of the situation or be intended for the robot to correct its behavior. To remove this uncertainty, future extensions of this dataset could also ask the user to provide an example answer for their question, which they would deem as satisfying. Ecological Validity. Another limitation of our paper is the lower ecological validity of using video and text stimuli compared to putting users into real-world humanrobot interactions. However, video and text stimuli also offer crucial advantages. They allow for highly controlled conditions and reproducible setup. Moreover, they allow us to test user reactions to very diverse set of scenarios, showcasing different types of robots, environments, and tasks. Lastly, data collection is much more scalable than what would be possible in the lab. Therefore, real-world studies would not replace our approach but represent an exciting avenue for complementary future research. Novelty Effect. While participants see multiple stimuli depicting robots executing tasks, they only see the execution of specific task by specific robot once. However, it may be the case that users have different questions during the initial interactions (e.g., asking the robot about its abilities) compared to later, repeated interactions (e.g., asking the What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 19 robot about deviations from its normal behavior). In future work, we would therefore like to study the change in user questions over time when users repeatedly interact with the same robot. Stimuli. In future research, we would like to extend the already rich set of stimuli used to elicit user questions. First, this could include longer videos (e.g., 35 minutes). Second, the dataset would benefit from further videos involving humans who are involved collaboratively in the tasks, as right now, humans are largely relegated to simply observing the robot or receiving an object. However, the video in which the robot feeds cat shows that users have many questions about the interaction between pet and robot indicating that more interactive scenarios involving humans are likely to elicit an even broader and more diverse set of questions. Participant Diversity. While our participant sample was diverse in gender, age, and background, participants mostly came from Western and Anglophone countries due to Prolifics participant pool and our filter for fluency in English. Future research has the opportunity to investigate cultural differences and extend the corpora to include multilingual questions. An example of national differences in XAI research is given by Kopecka [24], who found differences in the type of people who preferred goal vs. belief-based explanations between the UK and South Korea. Acknowledgments Lennart Wachowiak was supported by the Center for Doctoral Training (CDT) in Safe and Trusted AI (STAI) and UKRI (EP/S023356/1), as well as the Kings Institute for Artificial Intelligence. We thank Michelle Nwachukwu for helping with the inter-annotator agreement experiment. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023). [2] Amina Adadi and Mohammed Berrada. 2018. Peeking Inside the Black-Box: Survey on Explainable Artificial Intelligence (XAI). IEEE Access 6 (2018), 5213852160. [3] Sule Anjomshoae, Amro Najjar, Davide Calvaresi, and Kary Fr√§mling. 2019. Explainable Agents and Robots: Results from Systematic Literature Review. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS). IFAAMAS, 10781088. [4] Martim Brand√£o, Gerard Canal, Senka Kriviƒá, and Daniele Magazzeni. 2021. Towards Providing Explanations for Robot Motion Planning. In International Conference on Robotics and Automation (ICRA). IEEE, 39273933. [5] Vanessa Buhrmester, David M√ºnch, and Michael Arens. 2021. Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: Survey. Machine Learning and Knowledge Extraction 3 (2021), 966989. [6] Michael Cashmore, Anna Collins, Benjamin Krarup, Senka Krivic, Daniele Magazzeni, and David Smith. 2019. Towards Explainable AI Planning as Service. In ICAPS Workshop XAIP. [7] Sungwoo Choi, Anna S. Mattila, and Lisa E. Bolton. 2021. To Err Is Human(-oid): How Do Consumers React to Robot Service Failure and Recovery? Journal of Service Research 24, 3 (2021), 354371. [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv preprint arXiv:2507.06261 (2025). [9] Finale Doshi-Velez and Been Kim. 2017. Towards Rigorous Science of Interpretable Machine Learning. arXiv preprint arXiv:1702.08608 (2017). [10] Upol Ehsan and Mark O. Riedl. 2020. Human-Centered Explainable AI: Towards Reflective Sociotechnical Approach. In HCI International - Late Breaking Papers. Springer. [11] Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, and Heinrich Hussmann. 2018. Bringing transparency design into practice. In Conference on Intelligent User Interfaces. ACM. [12] Connor Esterwood and Lionel Robert. 2022. Having the Right Attitude: How Attitude Impacts Trust Repair in HumanRobot Interaction. In ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 332341. [13] Kerstin Fischer, Hanna Mareike Weigelin, and Leon Bodenhagen. 2018. Increasing trust in humanrobot medical interactions: effects of transparency and adaptability. Paladyn, Journal of Behavioral Robotics 9, 1 (2018), 95109. 20 Wachowiak et al. [14] Maria Fox, Derek Long, and Daniele Magazzeni. 2017. Explainable Planning. In Workshop on Explainable Planning at the International Joint Conference on Artificial Intelligence (IJCAI). [15] Zipeng Fu, Tony Zhao, and Chelsea Finn. 2024. Mobile ALOHA: Learning Bimanual Mobile Manipulation Using Low-Cost Whole-Body Teleoperation. In 8th Annual Conference on Robot Learning (CoRL). [16] Ferran Gebell√≠, Lavinia Hriscu, Raquel Ros, S√©verin Lemaignan, Alberto Sanfeliu, and Ana√≠s Garrell. 2025. Personalised Explainable Robots Using LLMs. In ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 13041308. [17] Ferran Gebell√≠, Raquel Ros, S√©verin Lemaignan, and Ana√≠s Garrell. 2024. Co-Designing Explainable Robots: Participatory Design Approach for [18] HRI. In IEEE International Conference on Robot and Human Interactive Communication (ROMAN). IEEE, 15641570. Juan Angel Gonzalez-Aguirre, Ricardo Osorio-Oliveros, Karen Rodr√≠guez-Hern√°ndez, Javier Liz√°rraga-Iturralde, Ruben Morales Menendez, Ricardo Ramirez-Mendoza, Mauricio Adolfo Ramirez-Moreno, and Jorge de Jesus Lozoya-Santos. 2021. Service Robots: Trends and Technology. Applied Sciences 11, 22 (2021), 10702. [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948 (2025). [20] Sture Holm. 1979. Simple Sequentially Rejective Multiple Test Procedure. Scandinavian Journal of Statistics 6, 2 (1979), 6570. [21] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. Transactions on Information Systems 43, 2 (2025), 155. [22] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. 2025. Vision-Language-Action Model with Open-World Generalization. arXiv preprint arXiv:2504.16054 (2025). [23] Dinesh Babu Jayagopi, Samira Sheiki, David Klotz, Johannes Wienke, Jean-Marc Odobez, Sebastien Wrede, Vasil Khalidov, Laurent Nyugen, Britta Wrede, and Daniel Gatica-Perez. 2013. The Vernissage Corpus: Conversational Human-Robot-Interaction Dataset. In ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 149150. [24] Hana Kopecka. 2024. Preferences for AI Explanations: Considering the Role of User and Robot Characteristics. Ph. D. Dissertation. Kings College London. [25] Hana Kopecka and Jose Such. 2020. Explainable AI for Cultural Minds. In Workshop on Dialogue, Explanation and Argumentation for HumanAgent Interaction. [26] Hana Kopecka, Jose Such, and Michael Luck. 2024. Preferences for AI Explanations Based on Cognitive Style and Socio-Cultural Factors. Proceedings of the ACM on Human-Computer Interaction 8, CSCW1 (2024), 132. [27] Mika Koverola, Anton Kunnari, Jukka Sundvall, and Michael Laakasuo. 2022. General Attitudes Towards Robots Scale (GAToRS): New Instrument for Social Surveys. International Journal of Social Robotics 14, 7 (2022), 15591581. J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics 33, 1 (1977). [28] [29] Gregory LeMasurier, Christian Tagliamonte, Jacob Breen, Daniel Maccaline, and Holly Yanco. 2024. Templated vs. Generative: Explaining Robot Failures. In 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN). IEEE, 13461353. [30] Mary Lindstrom and Douglas Bates. 1988. NewtonRaphson and EM algorithms for Linear Mixed-Effects Models for Repeated-Measures Data. J. Amer. Statist. Assoc. 83, 404 (1988), 10141022. [31] Stephanie M. Lukin, Claire Bonial, Matthew Marge, Taylor A. Hudson, Cory J. Hayes, Kimberly Pollard, Anthony Baker, Ashley N. Foots, Ron Artstein, Felix Gervits, Mitchell Abrams, Cassidy Henry, Lucia Donatelli, Anton Leuski, Susan G. Hill, David Traum, and Clare Voss. 2024. SCOUT: Situated and Multi-Modal Human-Robot Dialogue Corpus. In Proceedings of the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, 1444514458. Joseph Lyons, Izz aldin Hamdan, and Thy Vo. 2023. Explanations and Trust: What Happens to Trust When Robot Partner Does Something Unexpected? Computers in Human Behavior 138 (2023), 107473. [32] [33] David Marr. 1982. Vision: computational investigation into the human representation and processing of visual information. [34] Deborah McGuinness, Alyssa Glass, Michael Wolverton, and Paulo Pinheiro Da Silva. 2007. Categorization of Explanation Questions for Task Processing Systems. In ExaCt. 4248. [35] Tim Miller. 2019. Explanation in Artificial Intelligence: Insights from the Social Sciences. Artificial intelligence 267 (2019). [36] DC Montgomery and EA Peck. 1982. Introduction to linear regression analysis. John Wiley & Sons, New York. [37] Henrik Mucha, Sebastian Robert, R√ºdiger Breitschwerdt, and Michael Fellmann. 2020. Towards Participatory Design Spaces for Explainable AI Interfaces in Expert Domains. In German Conference on AI. [38] Sara Nielsen, Mikael Skov, Karl Damkj√¶r Hansen, and Aleksandra Kaszowska. 2023. Using User-Generated YouTube Videos to Understand Unguided Interactions with Robots in Public Places. ACM Transactions on Human-Robot Interaction 12, 1 (2023), 140. [39] Geoff Norman. 2010. Likert Scales, Levels of Measurement and the Laws of Statistics. Advances in health sciences education 15, 5 (2010), 625632. [40] Syed Ali Raza, Jonathan Vitale, Meg Tonkin, Benjamin Johnston, Richard Billingsley, Sarita Herse, and Mary-Anne Williams. 2022. An In-the-Wild Study to Find Type of Questions People Ask to Social Robot Providing Question-Answering Service. Intelligent Service Robotics 15, 3 (2022), 411426. [41] Skipper Seabold and Josef Perktold. 2010. statsmodels: Econometric and Statistical Modeling with Python. In Python in Science Conference. What Questions Should Robots Be Able to Answer? Dataset of User Questions for Explainable Robotics 21 [42] Rossitza Setchi, Maryam Banitalebi Dehkordi, and Juwairiya Siraj Khan. 2020. Explainable robotics in human-robot interactions. Procedia Computer Science 176 (2020), 30573066. [43] David Sobr√≠n-Hidalgo, Miguel Gonz√°lez-Santamarta, √Ångel Guerrero-Higueras, Francisco Rodr√≠guez-Lera, and Vicente Matell√°n-Olivera. 2024. Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models. arXiv preprint arXiv:2402.04206 (2024). [44] Spearman. 1904. The Proof and Measurement of Association between Two Things. The American Journal of Psychology 15, 1 (1904), 72101. [45] Christian Tagliamonte, Daniel Maccaline, Gregory LeMasurier, and Holly Yanco. 2024. Generalizable Architecture for Explaining Robot Failures Using Behavior Trees and Large Language Models. In Companion of the ACM/IEEE International Conference on Human-Robot Interaction (HRI). 10381042. [46] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. 2025. Gemini Robotics: Bringing AI into the Physical World. arXiv preprint arXiv:2503.20020 (2025). [47] Katie Trainum, Rachel Tunis, Bo Xie, and Elliott Hauser. 2023. Robots in Assisted Living Facilities: Scoping Review. JMIR Aging 6, 1 (2023), e42652. [48] UK AI Security Institute. 2024. Should AI Systems Behave Like People? https://www.aisi.gov.uk/work/should-ai-systems-behave-like-people Accessed 2025-09-16. [49] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St√©fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, Carey, ƒ∞lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant√¥nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261272. [50] Lennart Wachowiak, Oya Celiktutan, Andrew Coles, and Gerard Canal. 2023. Survey of Evaluation Methods and Metrics for Explanations in HumanRobot Interaction (HRI). In Explainable Robotics Workshop at IEEE International Conference on Robotics and Automation (ICRA). [51] Lennart Wachowiak, Andrew Coles, Gerard Canal, and Oya Celiktutan. 2024. Taxonomy of Explanation Types and Need Indicators in Human Agent Collaborations. International Journal of Social Robotics 16, 7 (2024), 16811692. [52] Lennart Wachowiak, Andrew Coles, Oya Celiktutan, and Gerard Canal. 2024. Are Large Language Models Aligned with Peoples Social Intuitions for HumanRobot Interactions?. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 25202527. [53] Lennart Wachowiak, Andrew Fenn, Haris Kamran, Andrew Coles, Oya Celiktutan, and Gerard Canal. 2024. When do People Want an Explanation from Robot?. In ACM/IEEE International Conference on Human-Robot Interaction (HRI). 752761. [54] Ning Wang, David V. Pynadath, and Susan G. Hill. 2016. The Impact of POMDP-Generated Explanations on Trust and Performance in Human-Robot Teams. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS). IFAAMAS, 9971005. [55] Ning Wang, David V. Pynadath, and Susan G. Hill. 2016. Trust Calibration within HumanRobot Team: Comparing Automatically Generated Explanations. In International Conference on Human-Robot Interaction (HRI). 109116. Online Resources We made the data and code available on GitHub: https://github.com/lwachowiak/xai-questions-dataset. The repository contains: the dataset of all natural language questions, their categories, and their importance scores our analysis code the annotations from second annotator for computing the inter-annotator agreement the questionnaire form to ensure the reproducibility of our work Additionally, the dataset is hosted on HuggingFace. Video Stimuli References The frames presented in Table 1 and the corresponding video stimuli are snippets from the YouTube videos linked below. If video contains multiple tasks, we indicate the task used for our video stimulus by providing the relevant timestamp. For our stimuli, we sometimes changed the playback speed and may have removed elements such as teleoperators and text boxes through cropping. To see the exact stimuli used, contact the corresponding author of this paper. [V1] https://www.youtube.com/watch?v=1r2GqaGyyIA, Gemini Robotics Salad Making 22 Wachowiak et al. [V2] https://www.youtube.com/watch?v=ikZeU3wKVjM, Gemini Robotics Lunchbox (task starting at 0:50) [V3] https://www.youtube.com/watch?v=Z3yQHYNXPws, Figure Fridge Sorting [V4] https://www.youtube.com/watch?v=Zn8yMaepzVk, Physical Intelligence Kitchen (task starting at 0:34) [V5] https://www.youtube.com/watch?v=Zn8yMaepzVk, Physical Intelligence Bedroom (task starting at 1:23) [V6] https://www.youtube.com/watch?v=L3rLT84qqLk, Astribot Drawer Sorting (task starting at 1:23) [V7] https://www.youtube.com/watch?v=2Fx5hBvT0Ac, Physical Intelligence Dryer Unpacking [V8] https://www.youtube.com/watch?v=Oa19cq_MxE0, Physical Intelligence Laundry Folding [V9] https://www.youtube.com/watch?v=XpBWxLg-3bI, 1X Shirt-Handover (task starting at 0:38) [V10] https://www.youtube.com/watch?v=Ni4p8axgqHM, Hello Robot Dishwasher (task starting at 0:34) [V11] https://www.youtube.com/watch?v=6X-s4Qsn1z4, Astribot Cat Feeding (task starting at 0:18) [V12] https://www.youtube.com/watch?v=6X-s4Qsn1z4, Astribot Waffle Making (task starting at 0:47) [V13] https://www.youtube.com/watch?v=6X-s4Qsn1z4, Astribot Tea Preparation (task starting at 1:53) [V14] https://www.youtube.com/watch?v=mnLVbwxSdNM, Mobile ALOHA Omelette (task starting at 0:11) [V15] https://www.youtube.com/watch?v=mnLVbwxSdNM, Mobile ALOHA Cooking (task starting at 0:34) Our use of the videos falls under YouTubes fair use policy6 that specifically mentions the use in research without the copyright owners permission. The frames in Table 1 do not show any identifiable people. Similar use of YouTube videos can be found, for example, in [38]. 6Fair use on YouTube (Retrieved September 10th, 2025:https://support.google.com/youtube/answer/9783148?hl=en"
        }
    ],
    "affiliations": [
        "Kings College London, CDT in Safe and Trusted AI, UK",
        "Kings College London, UK"
    ]
}