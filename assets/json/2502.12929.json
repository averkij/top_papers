{
    "paper_title": "Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options",
    "authors": [
        "Lakshmi Nair",
        "Ian Trase",
        "Mark Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore a diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% - 69.2% on standard data science tasks, and 37.4% - 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 9 2 9 2 1 . 2 0 5 2 : r Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options"
        },
        {
            "title": "Ian Trase Mark Kim",
            "content": "Pioneering Intelligence Flagship Pioneering Cambridge, MA 02142, United States {lnair,itrase,mkim}@flagshippioneering.com"
        },
        {
            "title": "Abstract",
            "content": "We present novel reasoning approach called Flow-of-Options (FoO), designed to address intrinsic biases in Large Language Models (LLMs). FoO enables LLMs to systematically explore diverse range of possibilities in their reasoning, as demonstrated by an FoO-based agentic system for autonomously solving Machine Learning tasks (AutoML). Our framework outperforms state-of-the-art baselines, achieving improvements of 38.2% 69.2% on standard data science tasks, and 37.4% 47.9% on therapeutic chemistry tasks. With an overall operation cost under $1 per task, our framework is well-suited for cost-sensitive applications. Beyond classification and regression, we illustrate the broader applicability of our FoO-based agentic system to tasks such as reinforcement learning and image generation. Our framework presents significant advancements compared to current state-of-the-art agentic systems for AutoML, due to the benefits of FoO in enforcing diversity in LLM solutions through compressed, explainable representations that also support long-term memory when combined with case-based reasoning."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have impacted automation on wide spectrum of tasks, motivating growing body of work in agentic system design, e.g., web-browsing [1], idea generation [2], and data science [3, 4]. Other approaches have focused on improving the inherent reasoning capabilities of the LLMs, through Tree-of-Thoughts (ToT) [5] or Chain-of-Thoughts (CoT) [6]. This paper seeks to bridge the two areas exploring how thought-based approaches can be extended to improve agentic systems. Hence, we propose Flow-of-Options (FoO)1. Given task, FoO enables LLMs to think through the options available for executing each step in the task, prior to the actual execution. Broadly, FoO is network data structure that explicitly enumerates options for each step in the task, as nodes in the network. Thus, FoO forces the LLM to be aware of, and to explore, broader spectrum of possibilities for completing the task. We demonstrate the practical value of FoO by incorporating it within an agentic framework for automating Machine Learning (ML) tasks (Figure 1a). Recent studies have shown that existing agentic systems, such as AutoGPT [8], LangChain [9], and ResearchAgent [10], often struggle with ML tasks [11]. Alternatives that involve fine-tuning LLMs [12, 13], although promising, involve significant computational effort and would not generalize to broader tasks the same way that reasoning strategy like CoT or ToT can. Recent models from OpenAI have focused on improving outputs by increasing test-time compute, fine-tuning the models to think deeper through CoT-style reasoning [14]. However, LLMs show strong biases towards options favored in their pre-training data when proposing solutions to tasks, 1Similar to flow-of-thought, i.e. stream of consciousness [7] Preprint. Under review. (a) (b) Figure 1: [Left] Overview of our Flow-of-Options based agentic framework incorporating CaseBased Reasoning. [Right] Word clouds (size = frequency) of options generated for solving therapeutic chemistry task using Sklearn. FoO boosts the diversity of the LLMs outputs. impacting the agentic systems built on top of them. In Figure 1b, we see that zero-shot GPT-4o and o1 have bias towards random forest when using the Scikit-learn (Sklearn) package [15], limiting the diversity of the solutions and preventing the LLMs from exploring better options for the task. Our experiments further confirm this bias across multiple tasks. Given this pre-training bias, the model will never step out of its comfort zone without coercion. We propose FoO as strategy to do this. recently proposed approach, DS-Agent [3] offers an alternative workaround that uses case-based reasoning (CBR) [16] to incorporate repository of human-generated solutions within an agentic framework. CBR retrieves past solutions to problems, reuses them for the current problem, and evaluates their effectiveness, retaining the successful solutions. Through the explicit retrieval and reuse of human insights in DS-Agent, the LLM is forced to explore alternatives, thus overcoming its intrinsic biases. While DS-Agent demonstrates improved performance, the system requires the use of carefully engineered repository of human insights (expert knowledge obtained from Kaggle) summarizing the steps that worked/failed for collection of problems. This requirement complicates the application of DS-Agent to most tasks, where Kaggle-like, refined, large-scale repository of human insights is usually unavailable. In this work, we find that the intrinsic knowledge of LLMs is sufficient proxy for human insights, provided the knowledge can be effectively extracted. This motivates the value of Flow-of-Options, demonstrated through the design of an agentic framework for automated ML (Figure 1a). We empirically demonstrate the benefits of our FoO-based agentic framework on 16 typical data science tasks and 20 therapeutic chemistry tasks (across ADME-Tox, drug-drug interaction, drug-target interaction and chemical bond type prediction). ML methods designed by our approach achieves the highest overall rank compared to those designed by existing state-of-the-art baselines, with 38.2% - 69.2% rank improvement in typical data science tasks and 37.4% - 47.9% rank improvement in therapeutic chemistry tasks. Per-task deployment and development costs using GPT-4o are each under $1. Unlike the majority of existing work on agentic systems for data science, we further show the broader applicability of our work, by successfully deploying our system in domains beyond classification and regression: reinforcement learning (cartpole balancing), image generation (MNIST), and in Appendix B.3 clustering (unsupervised ML), machine translation, and traveling salesman."
        },
        {
            "title": "2 Flow-of-Options",
            "content": "k Construction of FoO: Given task consisting of execution steps = {s1, s2, ..., sn}, we sample options for each step using an LLM o(i) pθ(T , si, o(1:i1) ). We prompt the LLM to emphasize diversity in its generations. By conditioning the LLM outputs on the task, the current step of the task, and the previously generated options, we ensure that the LLM outputs are consistent with the task and the options generated in previous steps of the plan. Given the LLM outputs, Flow-of-Options can be represented as network = (V, E, r) (a directed-acyclic graph) of depth n. node in this network represents an option. An edge connects two option nodes, with associated values (initialized to small fixed value at the beginning). To formulate the network, we first instantiate dummy root node. Then, beginning at the root, the network is constructed with the options corresponding to each step of the plan, placed at the corresponding depth i. Note that LLMs are only used to generate options, and not to construct the FoO structure. Let (i) = {o(i) }, denote nodes at depth i. The 2 , ..., o(i) 1 , o(i) 2 (a) (b) Figure 2: [Left] Illustrative example of our agentic workflow. The edge from Φ F2 retains the max value of 0.98 observed on that edge. [Right] Example of Flow-of-Options network (k = 2, = 3). Red arrows denote the edges that are inconsistent as identified by the consistency checker LLM. The best walk in this case follows options 0 1 5 6 (R = 0.899). edges are directed as follows: = {(u, v) (i1), (i)} [1, n] Where, denotes options at depth 1, denotes options at depth i, and (u, v) represents directed edge from to v. Visually, our FoO network can be likened to fully connected neural network, where every option at depth 1 is connected to every option at depth i. Importantly, there are no connections between options within the same depth, i.e., (u (i), (i)) / E. We update the values of the edges by traversing the FoO network using walks as follows: Given network F, walk represents sequence of options, one from each depth: ) where o(i) 2 , ..., o(n) (i) = (o(1) 1 , o(2) Our current implementation naively samples options, and some walks may be repeatedly generated from one iteration to another. In our future work, we will explore alternative informed sampling approaches. Given, walk (W = n), we evaluate the sequence of options for the task to obtain an output value or metric = (W ). We propagate the value to the edges connecting the options using max update: r(o(i) , o(i+1) ) = max(r(o(i) , o(i+1) ), R) o(i) Where r(u, v) represents the value associated with the edge (u, v). Hence, each option in will have their edge value to the subsequent option updated by the max operator. An illustrative example is shown in Figure 2a. Beam Traversal: Once the network is constructed and initial values are updated for the edges in F, it would be beneficial to explore alternative combinations of the highest-valued options when generating walks. Prior work has demonstrated the value of beam search in the context of improving the output generations of language models [17, 18], by maintaining several hypotheses, eventually choosing the one with the highest probability. We follow similar approach, and generate walks, by sampling options from the highest valued options at every step of the task plan. We refer to as the beam width. For identifying the top-b options, we compute the highest value associated with each option (i). This is the maximum value of all the edges coming into from options in (i1): value(v) = max(r(u, v)) (i1) Then, the top-b options in (i) (Topb(V (i))), are the options in (i) that have the highest value. We sample from the top-b options for generating walks. Wbeam = (o(1) 1 , ..., o(n) ) where o(i) Topb(V (i)) 3 Figure 3: Overview of Flow-of-Options incorporated into an agentic framework for automating ML tasks. When starting with an empty case bank, task plan and FoO is generated, evaluated and updated in the case bank (development). With non-empty case bank, the closest case is retrieved and adapted to the new task. Further, either new options are explored (development), or the adapted FoO and task plan are directly used (deployment), based on the users preference. When = 1, the walk will sample the highest valued option for each step si, resulting in the best sequence of options for solving the task. Setting = corresponds to uniform sampling of all options at each step (naive). Intermediate settings explore combinations of high-valued options. Consistency checking: Conceptually, the walks explore all combinations of options at different steps in the task. However, some combinations can lead to inconsistencies in the solution. For instance, if RandomForestRegressor is chosen at depth i, subsequent options in the same walk should not reference different model. To ensure this consistency, we pass the walk through an LLM-based consistency checker. If the LLM identifies an inconsistent walk, it is dropped (unevaluated) and new walk is re-sampled. Figure 2b shows an example Flow-of-Options network. The consistency checker LLM identifies paths from 5 4 and 3 6 as inconsistent due to the discrepancy in models chosen at these steps. Next, we discuss the benefits of FoO, followed by the description of our FoO-based agentic system."
        },
        {
            "title": "2.1 Benefits of Flow-of-Options",
            "content": "Flow-of-Options is special case of directed acyclic graph (DAG) that is similar in structure to fully connected neural network. Why is this formulation useful, as opposed to typical tree or directed acyclic graph? We discuss this in the context of two existing methods that are closely related: SELA [19] (using trees) and Data Interpreter [20] (using DAGs). In addition to distinguishing our work from existing methods, these differences also highlight key challenges of building agentic systems for ML [21], that we seek to alleviate with Flow-of-Options. SELA [19] decomposes tasks with pre-specified steps into tree, combined with Monte-Carlo Tree Search (MCTS) to find optimal paths. FoO offers two improvements over this work: First, by definition, child node in tree can only have one parent node. This eliminates the possibility of exploring different combinations, e.g., of features and models. Converting Figure 2b to tree, can cause combinations of nodes, e.g., 2 3, to never be explored if only node 1 is parent of node 3; Second, we argue that MCTS may be an overkill for AutoML-style problems, since it requires significant number of rollouts for convergence, that are computationally intensive (requiring code execution). Although SELA proposes modified upper confidence bound for trees (UCT) to mitigate this impact, SELA takes the longest among the baselines in our experiments. We believe Auto-ML can be treated differently: poor rollout of feature F1 with ML model M1 does not have to impact F1s value (as with UCT), if better combination of F1 with M2 exists. If we discover once, that F1 and M2 performs well, we can stick to that path with max update, regardless of whether F1 is (on average) good state. Alleviating some of the shortcomings of SELA, Data Interpreter (DI) [20] utilizes an LLM to produce directed acyclic graph (DAG) decomposing task into sub-tasks. Data Interpreter then seeks the most optimal graph based on an output performance measure. FoO improves over this approach in three ways: First, the LLM generated graph in Data Interpreter is not guaranteed to be acyclic 4 (as the authors note). In contrast, FoO is constructed without LLMs (only LLM-generated options). Since LLMs are not involved in the construction of the network itself, cycles are explicitly avoided. Secondly, the DAG generated by Data Interpreter, does not enforce the fully connected structure of FoO. As result, Data Interpreter is not guaranteed to explore combinations of nodes at each depth, e.g., exploring possible combinations of feature engineering techniques with ML models, that could lead to accuracy improvements. Lastly, Data Interpreter does not enforce the exploration of different options for executing sub-tasks, often sticking to RandomForest or XGB models. In contrast, FoO enforces diversity in the options explored, resulting in the discovery of superior solutions. In summary, FoO introduces an improved, more specialized, data structure to tackle range of ML problems. Experiments show that our FoO-based system yields benefits over both SELA and DI."
        },
        {
            "title": "3 Agentic Framework Using FoO",
            "content": "An overview of our proposed agentic framework with Flow-of-Options and Case-based Reasoning (CBR) is shown in Figure 3. We first discuss CBR using FoO before discussing the framework."
        },
        {
            "title": "3.1 Case-Based Reasoning with FoO",
            "content": "Prior work has demonstrated the benefits of Case-Based Reasoning (CBR) in enhancing the problemsolving capabilities of LLMs, while also improving efficiency in terms of computational resources [3]. Motivated by these benefits, we further incorporate CBR with Flow-of-Options, by retrieving and reusing previously generated FoO networks. CBR involves case bank consisting of individual cases c. Specifically, we denote case = (T , F, R), where denotes the task, denotes the FoO network generated for the task, and denotes the best reward achieved with the network for this task. Unlike the long-form descriptions of task solutions in DS-Agent [3], represents compressed and relevant information on the case. For the same task, if new with higher reward is discovered, then the case is updated with the new and the corresponding reward. For new task , we retrieve the closest case that maximizes the cosine similarity of its corresponding task with , i.e., = argmaxc sim(E(Tc), E(T )). Here, E() denotes pretrained embedding model. For the new task, we reuse the original task plan Tc, and the corresponding F. This ensures consistency between the retrieved flow-of-options and the task plan, with the implicit assumption that similar task plan can be applied to similar tasks. We threshold the retrieval based on the similarity score, to ensure the validity of this assumption. Cases with similarity scores below threshold will not be reused, and instead completely new FoO will be developed for the task."
        },
        {
            "title": "3.2 Framework",
            "content": "Inspired by prior work [3], we incorporate two phases in our framework: Development, and Deployment. Our framework begins with user input as prompt, followed by development or deployment. Development: Development takes one of two paths depending on whether the case bank is empty or has data. If the case bank is empty, task Planner (an LLM), generates sequence of steps for completing the task. Given the task plan, the Option Generator generates options for each step of the plan. The generated options are then converted into FoO network as described in Section 2. Once the network is generated, we traverse through the network for iterations, where each iteration uses fixed beam width that is reduced at later iterations to encourage exploration over the high-valued states (Appendix Figure 10 demonstrates the value of reducing beam width over subsequent iterations). Each iteration performs set of walks in batch. At the end of walks, the Plan Executor converts each walk into code, reflectively debugs any errors, and executes the code to extract the final metric Rj = (Wj). The metric is propagated to the nodes in the walk Wj to update F. Once iterations are complete, is added into the case bank. If the case bank is non-empty, the closest case to the user input is retrieved from the case bank. If the similarity (as described in Section 3.1) is below pre-specified threshold, the framework reverts to the empty case bank workflow. If the similarity exceeds the threshold, the framework reuses the corresponding and task plan for the current problem. First, an Adapter agent, adapts the and corresponding task plan to the new problem, e.g., modifying regression classification, but reusing the same class of model (GradientBoostingRegressor GradientBoostingClassifier) 5 (Appendix Figures 13 - 15 show additional examples of adaptations). Once adapted, the Option Generator generates new options for in context of the past options in it. This enables the model to continue exploring the space with some memory of its past explorations. With the updated F, the framework proceeds with walk generations and updates values based on the corresponding code executions. The resultant is then updated in the case bank. We can repeat development on the same task, in which case the previously generated plan and for the task will be retrieved, allowing generation and exploration of new options. This can lead to the discovery of better options (improvement in accuracy), or retention of the past best performing options (stability in accuracy). Deployment: The deployment phase is computationally efficient phase with low resource requirements. Given new task, deployment occurs when similar task with high enough similarity is in the case bank. First, the retrieved task plan and are adapted to the new task. Then deployment is analogous to development with the following setting: = 0 (new options are not generated), = 1, = 1, and = 1 (n same as retrieved F). This setting directly samples the best-valued walk, reducing deployment time and cost. The outcomes for the new task are then updated in the case bank. Improving Computational Efficiency: To boost the practical applicability of our framework, particularly in the case of large FoOs, we implement three techniques. First, when the task plan is generated, we use an LLM to filter out the most important subset of steps that can impact accuracy on the task. Identifying this subset allows us to improve computational efficiency by restricting the depth of to few, relevant steps in the plan. Additionally, this helps narrow the exploration to the steps in the plan that matter, e.g., the different ways of importing package is not critical to accuracy, and can be safely ignored. Secondly, we prune low-value options in to prevent an explosion of the FoO size. While this risks removal of some options from memory, we find that pruning two options per depth yields good balance between computational efficiency and the exploration of reasonable number of unique options. Lastly, within each iteration, we parallelize the walks and their corresponding code executions. This is similar to batch processing of inputs. At the end of the iteration, the values in the batch are then used to update F."
        },
        {
            "title": "4 Experiments and Results",
            "content": "Baseline performance comparison on classification and regression: We evaluate our framework on 16 tasks obtained from [3]. Our baselines include DS-Agent [3], AutoGluon [22], SELA [19], Data Interpreter (DI) [20], Autogen [23], and zero-shot with Chain-of-Thought (CoT) [6]. We also evaluate on 17 ADME-Tox tasks using Therapeutic Data Commons (TDC) [24]. We exclude DS-Agent from TDC tasks, since DS-Agent requires repository of human insights (similar to Kaggle) which is currently unavailable for TDC. Without these insights DS-Agent performs poorly [3]. Similarly, we exclude AutoGluon from TDC tasks (requiring use of packages like RDKit), and language model tasks, since AutoGluon cannot handle them. We also exclude SELA from forecasting and TDC tasks and DI from TDC tasks, as they are not flexibly supported in their MetaGPT implementations [25]. For TDC, we compare against DeepMol [26], an AutoML approach specialized for ADME-Tox. Scaling to computationally intensive scenarios: We evaluate our scalability via drug-drug combination, drug-target interaction prediction (TDC), and chemical bond prediction [27] with 100 data than the previous tasks. Generalizing beyond classification and regression: We evaluate our work on: a) reinforcement learning (RL) task of cartpole balancing, and b) image generation using MNIST. In the context of these tasks, we also investigate the capabilities of the Adapter LLM to adapt prior FoOs on the same task to atypical instantiations of the tasks. Similar to DS-Agent, we retain separate Dtrain, with testing on Dtest. In all cases, we use GPT-4o [28] as the foundational LLM. For Autogen, we follow the official documentation to construct system consisting of two LLM agents: one that produces task plan and code, and another that critiques the output to suggest improvements (reflection) [29]. We use the default parameterization for AutoGluon. We run SELA for 5 rollouts and DS-Agent for 5 development iterations (deployment is direct). We run DeepMol for 5 trials2. For development, we use = 5 iterations, = 3 (walks per batch), = 4 (DS) or = 3 (TDC), and filter tasks to use = 2 (DS) or = 3 (TDC). For 2Typically run for 100 trials for leaderboards, but we use the same number of trials for all approaches for fair comparison. 6 WB MC () () 0.30 304 0.28 322 0.29 321 0.30 314 0.30 309 0.26 263 0.18 182 DS-Agent AutoGluon SELA DI Autogen Zero-shot Ours Development EC () 0.27 0.43 0.37 0.35 0.43 AR () 4.47 1.19 1.11 1.67 1.91 1.59 ES () 0.40 0.61 0.71 0.98 0.67 0.80 0. ST () 0.78 0.80 0.51 0.82 0.80 0.78 0.82 ILI () 6.49 1.05 2.86 5.19 1.53 SS MH SD () () 0.99 0.80 0.69 0.89 0.85 0.72 0.82 0.88 0.85 0.90 0.81 0.83 0.98 0.99 () 0.34 0.52 0.38 0.36 () 0.34 0.54 0.32 0.06 0.40 0.50 0.29 Deployment () 0.67 0.81 0.98 0.69 0.79 0. CA () 0.73 1.36 1.39 0.40 1.38 1.16 0.73 CS () 11.7 11.4 11.8 9.88 10.3 10.0 9.18 HB WR () () 0.65 0.68 0.75 0.76 0.75 0.83 0.72 0.83 0.72 0.76 0.80 Avg. Rank () 3.69 4.67 4.17 2.33 3.19 3.19 1.44 Table 1: Performance of our framework compared to baselines on DS tasks. Results are reported over three independent runs (same prompts and fixed seed for all methods. Each run uses different seed). Development and Deployment applies to DS-Agent and our work. Best results are in bold, with second best underlined. Systems that cannot flexibly handle non-tabular tasks are marked as . denotes that Data Interpreter failed to generate working code in any of the three runs. Figure 4: Our approach shows the capacity to improve. [Right] DeepMol failed in the first three iterations (returning -inf) and [Left] Data-Interpreter failed one iteration (Iteration 2). Tables 1 and 3, we start with an empty case bank, and disable CBR for all development tasks. FoO for each development task is then added to the case bank together at the end. For deployment, we enable CBR FoO are retrieved and reused from the case bank (no new options explored). Please refer to Appendix B.1 for task descriptions."
        },
        {
            "title": "4.1 General Data Science (DS) Tasks",
            "content": "We show results for 16 data science tasks in Table 1 (arrows show whether lower () or higher () metric is preferred). For DS-Agent and our framework, we develop on 7/16 tasks, and deploy on the rest. We see that ML approaches designed by our framework outperforms the baselines with an average rank of 1.44 (best possible rank is 1.0, and worst is 7.0), 38.2% to 69.2% improvement in ranks compared to baselines. All approaches, except DI, succeeded in 100% of the cases. Our approach produces high-performing solutions in 15/16 tasks (except AR). We see the accuracy benefits of FoO even in the absence of CBR, in the development phase. We note that Data Interpreter (DI), while competitive, exhibits two key disadvantages compared to our work: a) DI failed to produce code in 1/3 runs for 5/16 tasks; b) DI uses highly specific, hand-crafted prompts for the tasks (Appendix K) compared to the general guidance provided to our FoO-based system (Appendix L). The specificity of the prompts may, at least in part, contribute to the competitiveness of DI. Despite the more general prompts, FoO enables our approach to outperform the baselines. It is also worth noting that in spite of explicitly specifying several models for tabular tasks in the prompt, DI almost always used XGB or RandomForest only (See Appendix Figure 7) indicating the potential value of enumerating options in FoO form. We demonstrate the capacity of our approach to improve on task, by repeating development on the same task (as described in Section 3.2). Here, we run development repeatedly on the same task with = 1 for five separate runs, saving the FoO at each run into the case bank. At each run, the past FoO is retrieved and new options are explored (we prune two options). We also run the remaining approaches for five independent runs. At each iteration t, DS-Agent reflects on past code from iteration 1. In Figure 4, Autogen, SELA, DI, and zero-shot performances fluctuate (since they do not reuse past experiences, leading to randomness). While DS-Agent generally improves in performance, there are cases where the agents reflection results in worse outcomes (e.g., in iteration 2). Our framework reuses the past Flow-of-Options network, which includes the best solution 7 Development Deployment CW HH () () 0.87 0.35 0.77 0.42 0.81 0.44 0.91 0.34 BI () 0.50 0.50 0.56 0.58 PG () 0.82 0.86 0.81 0.89 LI () 0.69 0.77 0.76 0.75 BB () 0.72 0.73 0.77 0.78 PP () 8.34 10.3 11.2 9. VD () 0.33 0.43 0.29 0.52 C2 () 0.20 0.36 0.34 0.57 C3 () 0.64 0.63 0.64 0.80 DeepMol Autogen Zero-shot Ours C2S C3S HO CH () () 0.14 0.38 0.28 0.42 0.32 0.39 0.32 0.62 () 0.63 0.59 0.59 0. () 0.49 0.32 0.28 0.26 () 0.65 0.76 0.71 0.76 hE () 0.76 0.64 0.69 0.68 DI () 0.79 0.72 0.72 0.84 Avg. Rank () 2.35 2.71 2.82 1.47 Table 3: Performance of our FoO-based agentic framework compared to baselines on TDC ADMETox tasks. Development is performed over Absorption and Distribution tasks. Deployment is performed on Metabolism, Excretion, and Toxicity tasks. Results are reported over three independent runs. Best results are in bold, with second best underlined. found thus far (nodes with highest values). Exploring new options in iterations 0 3, the agent finds improved outcomes. In iteration 4, it fails to find better options, achieving stability by retaining the best past options it has found. Average Cost ($) Average Time (mins) Resource costs for our framework on DS tasks are in Table 2. Although costs vary, they are generally under $1. Since the repetitive LLM querying from prior work [3, 23] is replaced with some non-LLM operations on FoO networks, the costs are low. Although our approach takes longer to develop, it is quick once an initial case bank is produced. SELA took the longest time compared to other baselines, possibly due to expensive MCTS rollouts. SELA DS-Agent (Develop) DS-Agent (Deploy) Data-Interpreter Ours (Develop) Ours (Deploy) 0.17 1.53 0.06 0.2 0.62 0.03 21.01 12.03 0.74 3.39 13.29 0. Table 2: Resource comparisons between LLMbased agentic systems. AutoGen and zero-shot have negligible time and costs. Our framework has negligible deployment costs."
        },
        {
            "title": "4.2 TDC Tasks – ADME-Tox",
            "content": "Results are shown in Table 3. Our framework outperforms the baselines, achieving an average rank of 1.47 (37.4% to 47.9% improvement over baselines), consistently producing high-performing solutions (15/17 tasks except HO and hE)."
        },
        {
            "title": "4.2.1 Computationally Intense Tasks",
            "content": "Our FoO-based agentic framework involves the construction and traversal of FoO networks. In the event that training data and models are large, it would be beneficial to scale our approach to more computationally intensive scenarios. We present one potential strategy that uses coreset selection [30] to overcome this problem. Coreset selection involves selecting subset of the data that reasonably reflects performance on the full training data. We demonstrate the performance of this strategy on tasks with about 100 more data than the previous sections. We use simple data selection strategy that applies stratified sampling on binned label values, although more sophisticated approaches could be applied here [30]. We apply development with our framework on the reduced subset of data ( 1/50th of the original dataset size), that took 15.8 mins. Once the best performing code is generated, we apply it on the complete training set and report performances on the test set. Our results are shown in Table 4. ML approaches designed by our framework performs well across the three tasks, outperforming the baselines. Additionally, our approach explores wider range of possibilities, as shown by the word cloud in Figure 5, highlighting key benefit of FoO. ML approaches designed by our framework achieves 80% - 90% of the performance of human-designed approaches in the 12/19 TDC tasks (Appendix Figure 11). Task DC (TDC) () DTI (TDC) () CB () Avg. Rank () Zero-shot Autogen Ours 11.85 0.42 0.998 1.0 16.61 0.18 0.996 2.0 13.88 0.32 0.998 1.3 Figure 5: ML models explored by the different AI agents across tasks from Section 4.2.1. Our approach explores range of ML model choices. Table 4: Performance of our system on computationally intensive therapeutic chemistry tasks (development). Results reported on three runs. 8 (a) (b) Figure 6: [Left] Reward comparisons for the different methods. Internal: Cartpole positions of our method for the left-constrained variant compared to the original problem. [Right] Output generations and SSIM values for the methods, and output of our method for the color-constrained variant."
        },
        {
            "title": "4.3 Beyond Classification and Regression",
            "content": "We further demonstrate our approach on tasks beyond classification and regression. Our baselines are Autogen and Zero-shot with CoT owing to their flexibility in handling wider range of tasks. Reinforcement Learning (Cartpole Balancing): We show results of our approach on the classic cartpole balancing problem from OpenAI Gym [31] in Figure 6a, compared to baselines. We see distinct performance difference between our approach and baselines on this task. Our framework selects the more sophisticated REINFORCE algorithm [32] compared to either using simple Q-table (Autogen) or random policy (zero-shot). We further evaluate the capacity of our framework to adapt (via the Adapter) to novel, uncanonical, variants of the task, inspired by prior work [33]. Specifically, we added the constraint of staying within the left zone of the arena as much as possible. Our prompts for this task is shown in Appendix M. Appendix Figure 15 shows how the FoO developed on the original problem, is successfully adapted and deployed to the new, constrained problem. We compare the poses of the model outputs for the original and the modified problems in Figure 6a. We see that the adapted solution of modifying the REINFORCE reward function performs well, underscoring the capacity of our framework to effectively adapt to novel variants of past tasks. Synthetic Image Generation using MNIST data: We show results of our framework on synthesis of MNIST images in Figure 6b, with SSIM (Structural Similarity Index Metric) comparing our framework to the baselines. We measure SSIM between the original MNIST dataset and the generated images as measure of similarity of the synthetic generations to the original data. Higher SSIM is preferred. We see improvement in the quality of the synthetic generations for the approach developed by our framework, both quantitatively (in terms of SSIM), and qualitatively. Zero-shot generation uses Generative Adversarial Network and Autogen uses convolutional variational-autoencoder (VAE). Our framework evaluates two architectures for each in terms of activation functions. We also evaluated our framework with the constraint that the generated digits should be red. The adapter proposes adding post-processing step to the generated images to modify the channel data to generate red images (Appendix Figure 16). The results are shown to the right of Figure 6b. Alongside image generation and RL, we demonstrate the application of our work to range of additional tasks such as clustering and machine translation in Appendix B.3."
        },
        {
            "title": "5 Related Work",
            "content": "Existing agentic designs, including recent frameworks overfit to narrow spectrum of tasks in data science by assuming an overall workflow [3, 23, 10, 4]. Prior work has looked at Graph of Thoughts [34] to tackle reasoning problems (different from AutoML), but do not enforce the notion of options or option diversity. For the most closely related works, SELA [19] and Data Interpreter [20], we discussed data structure differences in Section 2.1. We highlight some additional framework-level differences here. In contrast to our work, SELA [19] predefines steps in the pipeline within the LLM prompt, e.g., data analysis, feature engineering, and model selection. Data Interpreter predefines set of task types with detailed set of instructions for each task type. This ranges from specific 9 prompts for feature engineering tasks and model training tasks, to highly specific prompts for image-to-web conversion tasks. This limits their applicability to broader problems. Additionally, SELA and Data Interpreter do not leverage past experiences for new tasks. For therapeutic tasks, prior work has introduced DrugAgent [35]. Similar in spirit to our work, DrugAgent explores an idea space of different models but is specifically designed for the task of drug discovery only and does not use structured thinking. In contrast, Flow-of-Options offers improved flexibility and performance."
        },
        {
            "title": "6 Acknowledgments",
            "content": "We would like to thank Mickey Atwal, Armen Mkrtchyan, and the Pioneering Intelligence team at Flagship Pioneering for their valuable feedback."
        },
        {
            "title": "References",
            "content": "[1] Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu. Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. arXiv preprint arXiv:2410.02052, 2024. [2] Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, et al. Chain of ideas: Revolutionizing research via novel idea development with llm agents. arXiv preprint arXiv:2410.13185, 2024. [3] Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Automated data science by empowering large language models with case-based reasoning. arXiv preprint arXiv:2402.17453, 2024. [4] Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath Shahul Hameed Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, et al. Large language models orchestrating structured reasoning achieve kaggle grandmaster level. arXiv preprint arXiv:2411.03562, 2024. [5] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. [6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [7] Wallace Chafe. Language and the flow of thought. In The new psychology of language, pages 93111. Routledge, 2017. [8] Significant Gravitas. AutoGPT. [9] Harrison Chase. LangChain, October 2022. [10] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. [11] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Benchmarking large language models as ai research agents. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. [12] Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and PierreYves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 36763713. PMLR, 2023. 10 [13] Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, et al. Pangu-agent: fine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878, 2023. [14] OpenAI."
        },
        {
            "title": "Learning",
            "content": "to reason with llms. https://openai.com/index/ learning-to-reason-with-llms/. Accessed: 2024-11-26. [15] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. [16] Janet Kolodner. An introduction to case-based reasoning. Artificial intelligence review, 6(1):334, 1992. [17] Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711, 2012. [18] Giorgio Franceschelli and Mirco Musolesi. Creative beam search. arXiv preprint arXiv:2405.00099, 2024. [19] Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, et al. Sela: Tree-search enhanced llm agents for automated machine learning. arXiv preprint arXiv:2410.17238, 2024. [20] Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, et al. Data interpreter: An llm agent for data science. arXiv preprint arXiv:2402.18679, 2024. [21] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. Automated machine learning: methods, systems, challenges. Springer Nature, 2019. [22] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505, 2020. [23] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. [24] Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. Proceedings of Neural Information Processing Systems, NeurIPS Datasets and Benchmarks, 2021. [25] DeepWisdom. Metagpt. https://github.com/geekan/MetaGPT. Accessed: 2024-12-22. [26] Joao Correia, Joao Capela, and Miguel Rocha. Deepmol: An automated machine and deep learning framework for computational chemistry. bioRxiv, pages 202405, 2024. [27] Christoph Loschen. Perception of chemical bonds via machine learning. Chemrxiv, 2018. [28] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [29] Microsoft. Llm reflection. https://microsoft.github.io/autogen/0.2/docs/topics/ prompting-and-reasoning/reflection/. Accessed: 2024-04-12. [30] Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: comprehensive library for coreset In International Conference on Database and Expert Systems selection in deep learning. Applications, pages 181195. Springer, 2022. [31] Brockman. Openai gym. arXiv preprint arXiv:1606.01540, 2016. 11 [32] Junzi Zhang, Jongho Kim, Brendan ODonoghue, and Stephen Boyd. Sample efficient reinforcement learning with reinforce. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 1088710895, 2021. [33] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [34] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. [35] Sizhe Liu, Yizhou Lu, Siyu Chen, Xiyang Hu, Jieyu Zhao, Tianfan Fu, and Yue Zhao. Drugagent: Automating ai-aided drug discovery programming through llm multi-agent collaboration. arXiv preprint arXiv:2411.15692, 2024. [36] Stefanie Warnat-Herresthal, Konstantinos Perrakis, Bernd Taschler, Matthias Becker, Kevin Baßler, Marc Beyer, Patrick Günther, Jonas Schulte-Schrepping, Lea Seep, Kathrin Klee, et al. Scalable prediction of acute myeloid leukemia using high-dimensional machine learning and blood transcriptomics. Iscience, 23(1), 2020. [37] Salvatore Raieli."
        },
        {
            "title": "Clustering",
            "content": "acute clustering-techniques-with-gene-expression-data-4b35a04f87d5. 2024-12-16. for https://medium.com/leukemiaairesearch/ Accessed: expression techniques with leukemia. myeloid gene data [38] Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter Liu, Sharan Narang, Wei Li, and Yanqi Zhou. Exploring the limits of transfer learning with unified text-to-text transformer. Google, Tech. Rep., 2019. [39] Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, et al. Marian: Fast neural machine translation in c++. arXiv preprint arXiv:1804.00344, 2018. [40] Jörg Tiedemann, Mikko Aulamo, Daria Bakshandaeva, Michele Boggia, Stig-Arne Grönroos, Tommi Nieminen, Alessandro Raganato, Yves Scherrer, Raul Vazquez, and Sami Virpioja. Democratizing machine translation with opus-mt. arXiv preprint arXiv:2212.01936, 2022. [41] David Shmoys and David Williamson. Analyzing the held-karp tsp bound: monotonicity property with application. Information Processing Letters, 35(6):281285, 1990. [42] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [43] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language model-augmented code emulator. arXiv preprint arXiv:2312.04474, 2023. [44] Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, et al. Language models as compilers: Simulating pseudocode execution improves algorithmic reasoning in language models. arXiv preprint arXiv:2404.02575, 2024. [45] DeepWisdom. Metagpt. https://github.com/geekan/MetaGPT/blob/main/metagpt/ prompts/task_type.py. Accessed: 2025-01-09. 12 14 14 14 15 15 16 17 18 19 20 20 21 22 22"
        },
        {
            "title": "Table of Contents",
            "content": "A Code and Data Availability Supplementary Materials: Experiments and Results . B.1 Task Descriptions, and Hyperparameter Settings B.2 Diversity of generated solutions for DS tasks . . B.3 Additional tasks beyond classification and regression . . B.4 Case Study with non-code generation problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Why is beam traversal helpful? Comparison of our framework to human generated code baselines Using FoO with GPT-3.5 Relative advantages and disadvantages of the different baselines Limitations of the overall framework G.1 LLM-Module Specific Challenges . . . . . . . . . . . . . . . . . . . . . . . . . Code generated for the math case study Additional Flow-of-Options Network Examples Task Adaptation Using LLM Adapter Data Interpreter Prompts Our Framework: LLM Prompts Our Framework: User Inputs"
        },
        {
            "title": "A Code and Data Availability",
            "content": "The authors intend to make their code available for public access on Github: https://github. com/flagshippioneering/Flow-of-Options. Supplementary Materials: Experiments and Results B.1 Task Descriptions, and Hyperparameter Settings"
        },
        {
            "title": "Dataset",
            "content": "Abbr. Wild-blueberry-yield WB Media-campaign-cost MC ES EC AR ST ILI SS MH SD CA CS HB WR Enzyme-substrate Ethanol-concentration Airline-reviews Spaceship-titanic Ili Smoker-status Mohs-hardness Weather Software-defects Jigsaw Crab-age Concrete-strength Heartbeat WebMD-reviews"
        },
        {
            "title": "Classification Accuracy\nClassification Accuracy",
            "content": "Table 5: Tasks and corresponding abbreviations for DS-Agent tasks along with their metrics. Dataset Caco2-Wang HIA-Hou Pgp-broccatelli Bioavailability Lipophilicity BBB-Martinz PPBR_AZ VDss_Lombardo CYP2D6_Veith CYP3A4_Veith CYP2D6 Substrate CYP3A4 Substrate Half Life Obach Clearance Hepatocyte AMES hERG DILI Drug-drug combination Drug-target interaction Abbr. CW HH PG LI BB PP VD C2 C3 C2S C3S HO CH hE DI DC DTI"
        },
        {
            "title": "Task\nRegression\nClassification\nClassification\nClassification\nRegression\nClassification\nRegression\nRegression\nClassification\nClassification\nClassification\nClassification\nRegression\nRegression\nClassification\nClassification\nClassification\nRegression\nCorrelation",
            "content": "Metric MAE AUROC AUROC AUROC MAE AUROC MAE Spearman AUPRC AUPRC AUPRC AUPRC Spearman Spearman AUROC AUROC AUROC MAE Pearson Corr."
        },
        {
            "title": "Bond Prediction",
            "content": "CB"
        },
        {
            "title": "Classification Accuracy",
            "content": "Table 6: Tasks and corresponding abbreviations for TDC ADME-Tox tasks as well as additional therapeutic chemistry tasks (DC, DTI, and CB)."
        },
        {
            "title": "RL\nImage Generation\nClustering\nMachine Translation\nTraveling Salesman",
            "content": "T (iters) 1 1 3 3 1 (FoO depth) 1 1 3 3 1 (num options) 4 4 3 3 3 (walks/batch) 4 4 3 3 3 Table 7: Hyperparameter settings of our framework. B.2 Diversity of generated solutions for DS tasks For tasks in Table 1, we show the different ML models selected across the different approaches. We see that our framework (due to its use of FoO) and DS-Agent (owing to the use of human insights), evaluate range of solutions to the problems. In contrast, the remaining approaches show propensity for specific methods like RandomForest or XGB models. Figure 7: Diversity of generated solutions across LLM-based agents for tabular tasks from Table 1 (shown for subset of tabular tasks WB, MC, ES, EC, and ST for visual clarity). B.3 Additional tasks beyond classification and regression B.3.1 Unsupervised Learning Clustering We applied our framework to the problem of clustering gene expression data on Acute Myeloid Leukemia (AML) [36]. We pre-specified the number of clusters to be 7 in the input user prompt (See Appendix M). We follow an existing tutorial for pre-processing the data [37] provided to our approach. The planner produces task plan with the following filtered steps (recall that our approach filters the steps in the task plan to the most relevant steps): 1. Pre-process the data if necessary, ensuring that it Figure 8: Visualization of clusters for the AML gene expression data. 15 is in suitable format for clustering; 2. Select an appropriate clustering algorithm from the Sklearn library, such as KMeans, since the number of clusters is predefined; 3. Fit the clustering algorithm to the gene expression data to perform the clustering. The resultant clustering output for the model chosen by our framework is in Figure 8. Our framework explores different options, such as scaling the data before clustering, followed by agglomerative clustering or k-means clustering. The final method chosen consists of min-max scaling of the data and k++ centroid initialization with k-means clustering to yield Silhouette index of 0.118. B.3.2 Machine Translation"
        },
        {
            "title": "Our approach",
            "content": "Rouge-1 Rouge-2 Rouge-L 0.37 0.55 0.52 Table 8: Rouge scores of the ML model generated by our framework for English-to-French translation. We applied our framework to the problem of translating from English to French. We found that our framework ran into issues with the download and use of Opus-100 dataset using HuggingFace API. Hence, we passed template in our prompt to enable to model to correctly load and use the dataset API. Once provided with the API, our framework proposes the fine-tuning of various pre-trained models, such as T5 [38], MarianMT [39], and Opus-mt [40] models, available on HuggingFace. The finalized approach fine-tunes T5-base from HuggingFace on Opus-100 for English to French translation, achieving the Rouge scores reported in Table 8. We note that besides tokenization, there is no feature engineering involved in this task. Our Planner accounts for this specificity and generates an appropriate task plan focused on model selection, while the Option Generator produces appropriate options for the model choice. B.3.3 Traveling Salesman Problem We applied our framework to the non-ML task of solving the classical traveling salesman problem (TSP). For this experiment, we fixed seed and generated random distance matrix for the TSP with 10 cities. We note that unlike typical ML problems, there is no model selection or feature engineering involved here. Our planner produces task plan that accounts for the nature of this task and filters the following step for option generation: Implement method to find the optimal path that visits each city exactly once and returns to the starting city with the minimum total distance. Our framework explored different options for this step, including brute-force method, genetic algorithm, and dynamic programming using Held-Karp [41]. We evaluated the paths in the FoO (the metric R) using time taken to solve the task, preferring lower metrics. Based on this metric, our framework chose the Held-Karp dynamic programming method, which completed in 0.16 seconds, compared to > 5 seconds with the other approaches. This task in particular demonstrates the potential applicability of our FoO-based framework to tasks beyond ML, provided some scoring metric is available. B.4 Case Study with non-code generation problem"
        },
        {
            "title": "Math problem success rate",
            "content": "Zero-shot CoT Autogen Ours 3/3 0/3 0/3 Table 9: Success rate on three runs of the math problem case study. Our approach succeeds in 100% of the runs whereas the baselines consistently fail in all three runs. We performed small case study on applying our FoO-based agentic system on an math problem solving task. The prompt provided is: The equation x2 + 2x = has two complex solutions. Determine the product of their real parts. This case is interesting for two reasons: 1. It doesnt necessarily involve the notion of options (e.g., multiple ML approaches or feature engineering), and 2. the correct evaluation metric to use (metric used to evaluate walks is unclear). Although, existing approaches have used LLMs for scoring and evaluation [42], we circumvent the need for an evaluator for the purposes of this case study, by setting = 1, = 3, = 1 (number of options), and = 1. This setting results in reasoning chain analogous to CoT. We find that our approach is Figure 9: FoO generated for the math case study. The hyperparameter settings produce single chain of reasoning, analogous to CoT. able to produce the correct solution. The filtered task plan includes: 1. Substitute the values of a, b, and into the quadratic formula; 2. Substitute the value of the square root back into the quadratic formula to find the two solutions for x; 3. Calculate the real parts of the two solutions obtained from the quadratic formula. The corresponding FoO is shown in Figure 9. The model correctly produces the solution of -0.207 ((1 (cid:112)(2))/2). We repeated this experiment for three runs by disabling case-based reasoning. Hence, FoOs were not saved nor retrieved. All three runs are therefore independent. We also apply Autogen and zero-shot for this task, across three independent runs. Table 9 shows the success rates of the three methods on the task, across the three runs. We note that both Autogen and zero-shot failed to produce the correct result in any of the three runs, often producing 0.0 or 1.0 as the result. We note that our approach produced code (the generated code is shown in H), whereas the other two approaches produced written out formulae. For instance, our approach used the cmath package, whereas Autogen and zero-shot explicitly described the formulae for some of the functions that cmath provides, leading to errors. It is possible that the Option Generator, having been prompted to generate options (albeit one option), was more likely to think in terms of different packages that could be used. The code generation here loosely simulates prior approaches on thinking through code in LLMs [43, 44], where enumerating code or pseudocode-like steps has resulted in improvements in reasoning tasks. While hinting at the broader potential of our work, our future work seeks to evaluate this hypothesis more exhaustively on problems well beyond ML. Why is beam traversal helpful? Figure 10: Improvement in metrics from iteration 1 to 2 when beam width is reduced from 1.0 (full width) to 0.5 in the second iteration. 17 In our experiments, we reduce the beam width in later iterations of the development phase of our framework. We find that this annealing of beam width, enables our approach to discover new combinations of the high-valued options, leading to performance improvements. We start with the full beam width = k, and reduce it to = k/2 at iteration 3. In Figure 10, we see that better solutions are discovered when beam width is halved across two subsequent iterations. Even though the FoO structure and options are the same, the reduced beam width encourages exploration of combinations of high-valued options (e.g., combination of high-valued feature with high-valued ML model), leading to discovery of improved solutions."
        },
        {
            "title": "D Comparison of our framework to human generated code baselines",
            "content": "Figure 11: Comparing the accuracy of our framework to human baselines on the TDC leaderboard. The chart shows what percentages of the human baseline code metric, our framework achieves. For the TDC tasks, we compare the output of our framework to the results on the TDC leaderboard (Accessed on December 1st, 2024). We note what percentage ML approaches generated by our framework achieves compared to the output metric of human-generated baseline code at #1 position on the TDC leaderboard (for ADME-Tox tasks, drug-drug, and drug-target interaction). Using FoO with GPT-3.5 Model GPT-3.5 (FoO) GPT-3.5 (zero-shot) GPT-3.5 (Autogen) GPT-4o (FoO) GPT-4o (zero-shot) GPT-4o (Autogen) CW () HH () PG () 0.76 0.43 0.62 0.64 0.43 0.91 0.34 0.77 0.42 0.81 0.44 0.85 0.82 0.82 0.89 0.86 0. Table 10: Comparing our framework using GPT-3.5 to GPT-4o on three TDC Absorption tasks. The denotes that zero-shot GPT-3.5 failed to produce working code within three attempts. In this section, we demonstrate the results of using GPT-3.5 in our FoO-based agentic framework on three TDC tasks. We see in Table 10, that GPT-3.5 combined with our FoO-based agentic framework performs better than GPT-3.5 zero-shot and Autogen variants. Additionally, GPT-3.5 with FoO performs comparably to zero-shot GPT-4o. However, we note that code generation was key challenge with GPT-3.5, leading to failures in some code executions. Most failures resulted from the use of outdated APIs (such as outdated API of RDKit) or incorrect import statements. Apart from the debugging and code generation components, GPT-3.5 was successful at generating the task plan and 18 options, as well as consistency checking. In addition to models that are good at code generation and debugging, we note that our framework is most successful with models that are good at following instructions to produce structured output (e.g., output formatted as JSON with specific set of keys). However, with the pace of research in open-source LLMs that are competitive with closed-source LLMs like GPT-4o, we believe that these requirements will be satisfied by future open-source LLMs."
        },
        {
            "title": "Metric\nDiversity\nTime",
            "content": "Autogen AutoML SELA 6 6 2 3 5 1 5"
        },
        {
            "title": "Cost\nImprovement\nOverall Rank",
            "content": "2 3 2.8 1 3 3.6 3 3 3.6 DS-Agent 4 2 1 (Deploy) 3 (Develop) 6 2 3.2 DI 2 4 2 4 3 3. Ours 1 1 1 (Deploy) 3 (Develop) 5 1 2.0 Table 11: Ranking relative advantages and disadvantages of the methods. DI refers to Data-Interpreter. Quantitative: We note the key advantages and disadvantages of the different approaches on the following dimensions: a) Metric, i.e., accuracy or error, b) Diversity diversity of generated solutions, c) time time taken for the system to generate solution, d) cost $ cost of application of the framework, and e) Improvement ability to improve over time to derive better solutions as the system gains more experience. Based on our results, we rank the approaches from 1 (best) 6 (worst) on each metric  (Table 11)  for summarized view of the relative advantages and disadvantages of the methods. AutoML refers to DeepMol and AutoGluon. The key benefits of our approach are in terms of metric, diversity, and improvement. Our approach outperforms the baselines in terms of the output solutions (Tables 1, 3, 4), and also exhibits greater diversity in generated solutions compared to other LLM-based agents (Figures 5, 7). In terms of time  (Table 2)  , development takes longer both with DS-Agent and our approach. However, once the system builds an initial case bank, user can rely on deployment alone, which is faster. We found that SELA took 23 mins on average across the tabular tasks the largest amount of time taken compared to the other baselines. This was attributable to the time taken by the Monte-Carlo Tree Search approach. With cost  (Table 2)  , our framework improves on DS-Agent by replacing repeated LLM queries by operations on FoO. Lastly, our framework demonstrates the capacity for improvements over time through the incorporation of case-based reasoning with FoO (Figure 4). While DS-Agent also uses reflections to improve over past iterations, some of the reflections resulted in drops in performance across some iterations. The other LLM-based systems do not exhibit improvements over time. Qualitative: We note some advantages and disadvantages of the different approaches on some qualitative measures: a) flexibility the ability of the approach to adapt beyond specific set of tasks, e.g., only TDC or tabular tasks, b) ease of use ease of using the framework or approach, c) interpretability ability for user to understand the decision making process of the LLM. Existing AutoML approaches overfit to tabular ML tasks by specifying the steps in the pipeline, limiting their flexibility. Data-Interpreter and SELA make underlying assumptions regarding the overall task workflows (either task types or steps in the pipeline), limiting their applicability to broader range of tasks. Neither DS-Agent nor our approach makes similar assumptions. In terms of ease, DS-Agent requires curated repository of human insights, unlike our framework. However, there are certain hyper-parameters to our framework that may complicate its application for some users (e.g., number of options to generate, number of steps in the task plan to filter etc.). SELA, DI, DS-Agent, and our framework offer interpretability in the thinking process of the LLMs. DS-Agent offers interpretability via the reflection logs of the LLMs showing the results of each iteration and the improvements applied in subsequent iterations based on the previous result. SELA, DI, and our framework offers interpretability through the underlying data structures (tree, DAG, and FoO)."
        },
        {
            "title": "G Limitations of the overall framework",
            "content": "In this work, we proposed Flow-of-Options and an FoO-based agentic framework for the automation of ML tasks. Although our approach demonstrates improved performances on range of tasks, we highlight some key limitations of our work here. First, our approach assumes the existence of some metric that can be used to evaluate the options. In the event that the metric is not clearly defined, alternative evaluators, like LLMs, can be used as proxy for evaluating outcomes and generating corresponding reward [42]. We also assume that the user provides some input data for each task (or it is obtained from corresponding dataset or benchmark). If precise data is unavailable, connecting the LLM to data loading tools, can help provide the data necessary for our framework. Second, although our approach successfully generates more diverse options for solving different tasks, we still see some residual bias for Random Forest. In the future, we seek to improve the diversity of the option generations to ensure more uniform distribution over range of methods. We also note that although the intrinsic knowledge of LLMs contain several methods, the LLMs are not capable of identifying more novel approaches (e.g., ChemProp for TDC tasks), that we hope to rectify. Lastly, our approach currently naively samples walks and this results in some walks being re-sampled from one iteration to another, leading to repetition in the runs. In our future work, we seek to explore more sophisticated strategies for generating walks, such as tracking of already explored walks or updating the values to weight unexplored paths higher. We further discuss some challenges associate with each LLM module in our framework. G.1 LLM-Module Specific Challenges While GPT-4o based frameworks were able to successfully execute tasks with 100% success rate, we note some engineering challenges for the modules in our framework: Consistency Checker: There were few cases where the consistency checker identified an otherwise consistent pathway as inconsistent. Although the agent would simply pursue other paths, thus producing some working solution in each case, this mistake could cause some viable and promising paths to remain unexplored. Adding an additional evaluator LLM to verify the response, may help mitigate this issue. Retriever: We use the same retrieval mechanism as DS-Agent (using BAAI/LLM-embedder). There were few cases where the case retrieval module did not retrieve the best case for new task, instead retrieving an alternative case that is not as well suited to the current task. The tasks explored in our work do not seem particularly sensitive to this issue. However, improving the retrieval module can further improve the performance of our framework. Plan Executor: Since code is generated from language descriptions of code implementations, e.g., specifying Use GradientBoostingClassifier, there are slight variations from one code generation to the next, when the plan executor interprets the options. Although we found these variations to be insignificant with GPT-4o, using code snippets (e.g., code snippet of loading and using the model) when generating options, can potentially eliminate these differences. Option Generator: In few cases, we noted that the option generator would produce differently phrased, but similar options in spite of being prompted to encourage diversity (See Figure 12). Introducing an additional strategy to have the LLM options be regenerated, or prompting the LLM to modify similar options may help mitigate this issue."
        },
        {
            "title": "H Code generated for the math case study",
            "content": "Case study: Math Problem import cmath # Step 1: Identify the given equation: x^2 + 2x = i. # Recognize that the equation is quadratic equation in the form of ax^2 + bx + = 0, where = 1, = 2, and = -i. = 1 = 2 = -1j # Step 2: Recall the quadratic formula for solving ax^2 + bx + = 0, which is = (-b +- sqrt(b^2 - 4ac)) / (2a). # Implementation detail 0: Substitute the values of a, b, and into the quadratic formula: = (-2 +- sqrt(4 + 4i)) / 2 # Simplify the expression under the square root by converting 4 + 4i into polar form and using De Moivres Theorem to find the square root. delta = b**2 - 4*a*c # Convert 4 + 4i to polar form = abs(delta) theta = cmath.phase(delta) # Calculate the square root in polar form sqrt_r = cmath.sqrt(r) sqrt_theta = theta / 2 # Convert back to rectangular form sqrt_delta = sqrt_r * (cmath.cos(sqrt_theta) + 1j * cmath.sin(sqrt_theta)) # Step 3: Substitute this value into the quadratic formula to find the two solutions for by evaluating both the positive and negative cases of the square root. x1 = (-b + sqrt_delta) / (2*a) x2 = (-b - sqrt_delta) / (2*a) # Step 4: Use the properties of complex conjugates to determine the real parts of the solutions. # Since the coefficients of the quadratic equation are real, the solutions are complex conjugates. # Calculate the real part of one solution and use it for both solutions. real_part_x1 = x1.real real_part_x2 = x2.real # Step 5: Determine the product of the real parts of the two solutions. product_of_real_parts = real_part_x1 * real_part_x2 # Step 6: Conclude with the final result, which is the product of the real parts of the solutions. print(The product of the real parts of the solutions is:, product_of_real_parts) 21 Additional Flow-of-Options Network Examples Figure 12: Examples of flow-of-options networks with varying depths (shown for the RL and clustering tasks). Paths with value of 1000.0 are unexplored paths (either due to consistency check failures or because the walks were not sampled). The tree on the right demonstrates one of the systems limitations regarding generation of similar options in some cases (e.g., Nodes 1 and 3)."
        },
        {
            "title": "J Task Adaptation Using LLM Adapter",
            "content": "In this section, we show examples of how the LLM Adapter adapts network from one task to another under different scenarios. This is shown in Figures 13, 14, 15, and 16. The figures show the description of the nodes in the flow-of-options network. Figure 13: Example of the adapter LLM adapting the task description from Caco-2 cell permeability to Solubility prediction task in TDC. Each number marks node in the network along with its description. The network structure is unchanged since the network is reused, but the description of the nodes are adapted to the new task. 22 Figure 14: Example of the adapter LLM adapting the model from regression to classification task. Figure 15: Example of the adapter LLM adapting to constraints of the new task of cartpole balancing. From the original problem, the adapter modifies the methods to bias the cart to stay in the left. Figure 16: Example of the adapter LLM adapting to constraints of the new task of MNIST image generation. From the original problem, the adapter modifies the decoder output with post-processing."
        },
        {
            "title": "K Data Interpreter Prompts",
            "content": "We obtained these prompts from the Github repository of MetaGPT implementing the data interpreter [45]. Note the specificity in the task types, with task-specific prompts such as image2web conversion (IMAGE2WEBPAGE). New task types would require similar manually constructed prompts. Also note the hand-crafted details such as model choices in MODEL TRAIN PROMPT. Despite the specific model choice instantiations, Data Interpreter mostly resolved to XGB and RF models, demonstrating the benefit of encoding options in more explicit data structure like FoO. Note the in-depth, specific details provided for each task type, such as how and which columns to process, or which models to use for specific tasks. In contrast, the next section demonstrates our LLM prompts which are more general in comparison. Prompt: Data-Interpreter # Prompt for taking on \"eda\" tasks EDA_PROMPT = \"\"\" The current task is about exploratory data analysis, please note the following: - Distinguish column types with select_dtypes for tailored analysis and visualization, such as correlation. - Remember to import numpy as np before using Numpy functions. \"\"\" # Prompt for taking on \"data_preprocess\" tasks DATA_PREPROCESS_PROMPT = \"\"\" The current task is about data preprocessing, please note the following: - Monitor data types per column, applying appropriate methods. - Ensure operations are on existing dataset columns. - Avoid writing processed data to files. - **ATTENTION** Do NOT make any changes to the label column, such as standardization, etc. - Prefer alternatives to one-hot encoding for categorical data. - Only encode or scale necessary columns to allow for potential feature-specific engineering tasks (like time_extract, binning, extraction, etc.) later. - Each step do data preprocessing to train, must do same for test separately at the same time. - Always copy the DataFrame before processing it and use the copy to process. \"\"\" # Prompt for taking on \"feature_engineering\" tasks FEATURE_ENGINEERING_PROMPT = \"\"\" The current task is about feature engineering. when performing it, please adhere to the following principles: - Generate as diverse features as possible to improve the models performance step-by-step. - Use available feature engineering tools if they are potential impactful. - Avoid creating redundant or excessively numerous features in one step. - Exclude ID columns from feature generation and remove them. - Each feature engineering operation performed on the train set must also applies to the dev/test separately at the same time. - **ATTENTION** Do NOT use the label column to create features, except for cat encoding. - Use the data from previous task result if exist, do not mock or reload data yourself. - Always copy the DataFrame before processing it and use the copy to process. \"\"\" # Prompt for taking on \"model_train\" tasks MODEL_TRAIN_PROMPT = \"\"\" The current task is about training model, please ensure high performance: 24 - For tabular datasets - you have access to XGBoost, CatBoost, random forest, extremely randomized trees, k-nearest neighbors, linear regression, etc. - For image datasets - you have access to Swin Transformer, ViT, ResNet, EfficientNet, etc. - For text datasets - you have access to Electra, DeBERTa, GPT-2, BERT, etc. - Avoid the use of SVM because of its high training time. - Keep in mind that your user prioritizes results and is highly focused on model performance. So, when needed, feel free to use models of any complexity to improve effectiveness, such as XGBoost, CatBoost, etc. - If non-numeric columns exist, perform label encode together with all steps. - Use the data from previous task result directly, do not mock or reload data yourself. - Set suitable hyperparameters for the model, make metrics as high as possible. \"\"\" # Prompt for taking on \"model_evaluate\" tasks MODEL_EVALUATE_PROMPT = \"\"\" The current task is about evaluating model, please note the following: - Ensure that the evaluated data is same processed as the training data. If not, remember use object in Done Tasks to transform the data. - Use trained model from previous task result directly, do not mock or reload model yourself. \"\"\" # Prompt for taking on \"image2webpage\" tasks IMAGE2WEBPAGE_PROMPT = \"\"\" The current task is about converting image into webpage code. please note the following: - Single-Step Code Generation: Execute the entire code generation process in single step, encompassing HTML, CSS, and JavaScript. Avoid fragmenting the code generation into multiple separate steps to maintain consistency and simplify the development workflow. - Save webpages: Be sure to use the save method provided. \"\"\" Our Framework: LLM Prompts We present the prompts passed into the different LLM-based components of our framework. Note that our prompts are more general in comparison to frameworks like the Data Interpreter. Importantly, we do not make any assumptions about task types like the Data Interpreter. Prompt: Planner You are an expert ML scientist provided with the following task: {task} Think through the task step-by-step and produce highly detailed list of steps involved in solving this task. Do not include any code in your response. Prompt: Planner Filter Mode You are an expert ML scientist provided with task and high-level plan for solving the task. 25 Task: {task} High-level plan: {plan} Your goal is to rank the steps in the high-level plan based on their relevance to accuracy on the task. First, you must assess each step in the plan and identify whether it can impact accuracy on the task. Second, for all the steps that can impact accuracy on the task, you must rank the steps from most impactful to least impactful. Prompt: Adapter You are provided with task. You are also provided with high-level plan for solving different, but related task. Your goal is to adapt the steps in the plan for the new task. Task: {task} Plan: {plan} For each step in the plan, you must identify if that step needs to be adapted for the current task. If adaptation is necessary, modify the step to adapt it to the new task. If no adaptation is necessary, leave the step as is. Please do not add or remove any steps. Return the adapted plan. Prompt: Option Generator You are an expert ML scientist provided with task, and one step from high-level plan for solving this task. You are also given implementation details for all of the steps prior to the current step. task: {task} prior steps: {s} current step: {step} Your goal is to generate diverse options for implementing the current step if it impacts final accuracy. Firstly, you must assess whether the implementation of the current step can impact final accuracy on the task. If it is expected to impact final accuracy, you must: First, think as broadly as you can to generate {num_options} diverse and distinct options for implementing the current step, while taking into account the prior steps. Second, you must verify that each of your generated options do not conflict with any of the prior steps. Third, you must regenerate alternatives for any options that conflict with prior steps. Please follow these instructions in generating your response: 1. You must maximize the diversity across all your generated options. 2. Your choices must be very specific so that programmer can implement it. For e.g., instead of specifying \"Use features\", you must specify \"Use features X, Y, Z.\" 3. Your choices should not modify any of the prior steps. 4. Do not repeat any of the prior steps. 5. Do not include code in your response. Prompt: Consistency Checker You are given plan of steps for solving task, and new step. Your goal is to verify if the information contained in the new step contradicts any of previous steps of the plan. plan: {plan} new step: {step} The following are examples of contradiction: 1. If the new step references different model than the previous steps in the plan. 2. If the new step references different feature than the previous steps in the plan. First, assess whether the new step contradicts previous steps in the plan as described above. If the new step contradicts other steps in the plan, return the word: yes If the new step does not contradict other steps in the plan, return the word: no Prompt: Code Generator You are an expert AI programmer provided with task and plan of steps for accomplishing the task: task: {task} plan: {plan} You are also given specific implementation details for some of the steps in the plan: {walk} Your goal is to stitch the plan together to create complete Python code. You must ensure to incorporate the specific implementation details into your final code. You must follow the template provided in the task description as closely as possible and integrate the implementation details into the provided template. You must not make any functional changes to the provided implementation details themselves. Your code must be complete and should not leave any additional steps for the user nor raise any errors. Prompt: Code Generator Reflective Debugging Mode You are provided with code and an error message from running the code. Please debug the code systematically to fix the error. For additional context, you are also provided with the original user prompt indicating the task that the code is trying to achieve. Original prompt: {task}, Error message: {error}, Code: {code} The error may also be syntactical in which case you must fix the syntax appropriately. Our Framework: User Inputs We show examples of user inputs for the different tasks. We provide code template in our prompts (similar to DS-Agent [3]) to: a) standardize evaluations between baselines by removing differences in how the models get evaluated; b) to avoid issues with models not knowing what column names exist in the dataset, how to access them or other API issues. This can be alleviated by connecting model to external tools that would allow it to inspect datasets, information contained in them or to retrieve and reason about APIs. Similarly, we pre-specify packages to use, such as Sklearn or Pytorch to prevent import errors when executing code (and to prevent autonomous installation of packages in our environment)."
        },
        {
            "title": "User Input Example for TDC task",
            "content": "Your task is to write Python code for the following task. The dataset name and task description is provided below. Dataset name: {task_name} Task description: {description} Please follow these instructions: 1. Follow the template in the example code shown below, and do not add any new print statements to it. 2. Your code must be complete and executable without needing additional user intervention. 28 3. Please use Sklearn or Pytorch packages only for your ML implementations. Here is an example code snippet showing how to load and evaluate dataset with the name Caco2_Wang\": from tdc.benchmark_group import admet_group group = admet_group(path = data/) predictions_list = [] # For reproducibility for seed in [1, 2, 3, 4, 5]: benchmark = group.get(Caco2_Wang) # all benchmark names in benchmark group are stored in group.dataset_names predictions = {} name = benchmark[name] train_val, test = benchmark[train_val], benchmark[test] train, valid = group.get_train_valid_split(benchmark = name, split_type = default, seed = seed) # NOTE: For the dataset, column names are Drug (for the input SMILES strings) and (for the output labels) # --------------------------------------------- # # Train your model using train, valid, test # # Save test prediction in y_pred_test variable # # --------------------------------------------- # predictions[name] = y_pred_test predictions_list.append(predictions) results = group.evaluate_many(predictions_list) print(results) User Input Example for RL task optional constraint specification shown Your task is to solve classic reinforcement learning problem where the goal is to balance pole upright on cart that can move left or right. # We add this for the variant of the problem However, an important constraint in this problem is that your cart must try to stay in LEFT half of the area as much as possible. Follow the template below for your code: import gym import numpy as np import torch import random from gym.wrappers.monitoring.video_recorder import VideoRecorder import matplotlib.pyplot as plt # For reproducibility SEED = 42 random.seed(SEED) 29 torch.manual_seed(SEED) np.random.seed(SEED) # Create the environment env = gym.make(CartPole-v1, render_mode=\"rgb_array\") video = VideoRecorder(env, \"Cartpole-video.mp4\") # State space has four values -- [cart position, cart velocity, pole angle, pole angular velocity] # Action space has two values -- {0: \"push cart to the left\", 1: \"push cart to the right\"} for episode in range(500): state, _ = env.reset(seed=SEED) total_reward = 0.0 # FILL OUT: Solution for the cartpole problem # FILL OUT: Predict action using model next_state, reward, done, _, _ = env.step(action) # FILL OUT: Track total reward per episode in variable called total_reward # Plot total reward vs. episode plt.plot(total_reward) plt.xlabel(\"Iteration\") plt.ylabel(\"Total Reward\") plt.savefig(\"total_reward.png\") # Print total_reward from the last episode print(\"Final episode reward: \", total_reward) state, _ = env.reset(seed=SEED) done = False while not done: # FILL OUT: Predict action using model state, _, done, _, _ = env.step(action) env.render() video.capture_frame() video.close() env.close()"
        },
        {
            "title": "User Input Example for Clustering Task",
            "content": "Your task is to perform clustering of gene expression data for given dataset. You provided the gene expression profiles for 2321 patients, on 14208 genes. are You must cluster them into 7 clusters. Please visualize the generated clusters and save the visualizations. Please follow the provided template to complete this task. You must restrict yourself to Pytorch or Sklearn packages for this problem. Please ensure that your code does not require any additional steps from the user. Heres template code to follow: import pandas as pd import numpy as np from sklearn.metrics import silhouette_score 30 def compute_ssi(data, predicted_labels): print(f\"Final Silhouette Score: {silhouette_score(data, predicted_labels)}\") def load_dataset(): data = pd.read_table(\"Leukemia-clean.txt\", sep = \"t\") data[\"disease\"] = np.where(data[\"disease\"] == \"Diabetes_Type_I\" , \"Diabetes\", data[\"disease\"]) data[\"disease\"] = np.where(data[\"disease\"] == \"Diabetes_Type_II\" , \"Diabetes\", data[\"disease\"]) other = [CML,clinically_isolated_syndrome, MDS, DS_transient_myeloproliferative_disorder] data = data[data.disease.isin(other)] df = data.drop(\"disease\", axis=1) df = df.drop(\"GSM\", axis=1) df = df.drop(\"FAB\", axis=1) return df.to_numpy() # There are 14208 columns (features) and 2321 samples data = load_dataset() # TO FILL --- Perform clustering ---- # # TO FILL --- Predict labels for each sample and save into predicted_labels ---- # compute_ssi(data, predicted_labels) # TO FILL --- Save the clustering visualizations into file called \"clusters.png\" User Input Example for English-French Translation Task We would like to build language model that can translate from English to French (en-fr). Please use ROUGE score to evaluate the result. Print the final ROUGE score as \"Final Rouge Score: <print final rouge metric>\". 1. You must load and use the opus-100 dataset (en-fr) as shown below. 2. You must restrict yourself to Pytorch or HuggingFace packages only. 3. Your code must be complete, and should not leave additional steps for the user. 4. Please restrict any training or fine-tuning to 1 epochs. 5. Please run all models and tokenzations on the GPU. Example of loading the dataset: from datasets import load_dataset from torch.utils.data import Dataset, DataLoader dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-fr\") # Can be modified if needed. class CustomDataset(Dataset): def __init__(self, dataset, transform=None): \"\"\" Args: data (array-like): Data samples. targets (array-like): The corresponding labels for the data samples. 31 transform (callable, optional): Optional transform to be applied on sample. \"\"\" self.data = dataset #[sample[translation][en] for sample in dataset] #self.targets = [sample[translation][fr] for sample in dataset] self.transform = transform def __len__(self): \"\"\"Returns the total number of samples.\"\"\" return len(self.data) def __getitem__(self, idx): \"\"\"Generates one sample of data.\"\"\" sample = self.data[translation][idx][en] target = self.data[translation][idx][fr] if self.transform: sample = self.transform(sample) return sample, target train_dataset = CustomDataset(dataset[train]) val_dataset = CustomDataset(dataset[validation]) test_dataset = CustomDataset(dataset[test])"
        },
        {
            "title": "User Input Example for Traveling Salesman Problem",
            "content": "You must solve traveling salesman problem: Given 2d matrix distance[][] of size (i.e., number of cities) where distance[i][j] denotes the distance from city to city j. The task is to complete tour from city (0-based index) to all other cities such that we visit each city exactly once and then come back to starting city at minimum total distance. You should evaluate your approach on time taken to complete. You can follow the template below for guidance: import numpy as np import torch import random # For fixing the TSP instance generated by generate_tsp() function SEED = 42 random.seed(SEED) torch.manual_seed(SEED) np.random.seed(SEED) def get_path_length(path, distances): # Path is list of city numbers indicating the path to follow total_path_length = 0 for in range(len(path) - 1): start = path[i] end = path[i + 1] total_path_length += distances[start][end] 32 return total_path_length def generate_tsp(num_cities): \"\"\"Generates random TSP instance with given number of cities.\"\"\" # Create random coordinates for cities coordinates = np.random.rand(num_cities, 2) * 100 # Calculate distances between cities using Euclidean distance distances = np.zeros((num_cities, num_cities)) for in range(num_cities): for in range(num_cities): if != j: distances[i, j] = np.linalg.norm(coordinates[i] - coordinates[j]) return distances # Example usage: num_cities = 10 distances = generate_tsp(num_cities) optimal_path = None time_taken = None # Track time taken by the proposed approach # TO FILL -- Predict optimal path, where optimal_path is list of city numbers ranging from 0 to num_cities assert len(optimal_path) == num_cities + 1, \"Path does not visit all the cities\" assert len(set(optimal_path)) == num_cities, \"Path does not visit all cities once\" print(\"Optimal Path: \", optimal_path) print(\"FINAL Metric - Time taken: \", time_taken)"
        }
    ],
    "affiliations": [
        "Flagship Pioneering, Cambridge, MA 02142, United States"
    ]
}