{
    "paper_title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding",
    "authors": [
        "Ziqi Pang",
        "Yu-Xiong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose MR. Video, an agentic long video understanding framework that demonstrates the simple yet effective MapReduce principle for processing long videos: (1) Map: independently and densely perceiving short video clips, and (2) Reduce: jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video performs detailed short video perception without being limited by context length. Compared with existing video agents that typically rely on sequential key segment selection, the Map operation enables simpler and more scalable sequence parallel perception of short video segments. Its Reduce step allows for more comprehensive context aggregation and reasoning, surpassing explicit key segment retrieval. This MapReduce principle is applicable to both VLMs and video agents, and we use LLM agents to validate its effectiveness. In practice, MR. Video employs two MapReduce stages: (A) Captioning: generating captions for short video clips (map), then standardizing repeated characters and objects into shared names (reduce); (B) Analysis: for each user question, analyzing relevant information from individual short videos (map), and integrating them into a final answer (reduce). MR. Video achieves over 10% accuracy improvement on the challenging LVBench compared to state-of-the-art VLMs and video agents. Code is available at: https://github.com/ziqipang/MR-Video"
        },
        {
            "title": "Start",
            "content": "MR. Video: MapReduce is the Principle for Long Video Understanding"
        },
        {
            "title": "Ziqi Pang",
            "content": "Yu-Xiong Wang University of Illinois Urbana-Champaign 5 2 0 2 2 2 ] . [ 1 2 8 0 6 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose MR. Video (pronounced mister video), an agentic long video understanding framework to demonstrate the simple yet effective MapReduce principle for long video understanding: (1) Map, independently and densely perceiving short video clips, and then (2) Reduce, jointly aggregating information from all clips. Compared with sequence-to-sequence vision-language models (VLMs), MR. Video strategically performs detailed short video perception without being constrained by context length. Compared with existing video agents, which typically rely on sequential steps of key segment selection, MR. Videos Map step enables simpler and more scalable sequence-parallel perception of short video segments. Meanwhile, its Reduce step facilitates more comprehensive context aggregation for reasoning, surpassing the explicit key segment retrieval. The MapReduce principle applies to both VLMs and video agents, and we utilize the convenience of LLM agents to validate its effectiveness. In practice, MR. Video employs two MapReduce stages: (A) Captioning generating short video captions (map), and then standardizing repeated characters and objects into shared names (reduce); (B) Analysis for each user question, analyzing relevant information from individual short videos (map), and then integrating them into final answer (reduce). MR. Videos performance suggests the effectiveness of the MapReduce principle for long video understanding, with >10% accuracy improvement on the challenging LVBench over state-of-the-art VLMs and video agents. The code is at https://github.com/ziqipang/MR-Video. 1. Introduction The ultimate criterion of long video understanding model is the capability of digesting global contexts while perceiving local details simultaneously: the model should dedicate itself to all the video contents and not make any assumptions about the video. To clarify this, we show Fig. 1 as motivating example: in fast-paced sports highlight video, counting the goals of player requires detailed identification of details for every segment as well as comprehensive aggregation across the whole video duration. Unfortunately, existing sequence-to-sequence visionFigure 1. MapReduce Principle. Long video understanding requires both global comprehension and detailed perception, as in the motivating example. For such needs, (a) sequence-to-sequence VLMs and (b) video agents are sub-optimal in terms of context lengths, sequential parallelization, and global context information. (c) Instead, we explore and develop simple MapReduce principle via MR. Video and (d) overcome such challenges. language models (VLMs) [15, 16, 23, 2527, 59] and video agents both have limitations for such scenarios. For instance, VLMs inherently struggle with the contradiction between enormous amounts of long video tokens and limited context lengths, forcing them to conduct frame sampling or token compression (Fig. 1(a)) that might hurt detailed perception, e.g., missing large portion of goals in the example. Subsequently, video agents [8, 41, 53] emerge and necessarily bypass VLMs context length via iteratively retrieving the key segments and analyzing their belonging short video clips (Fig. 1(b)). However, they also demonstrate disadvantages compared with VLMs: (1) video agents generally rely on multi-round exploration of video contents, which harms sequence-parallel perception of video segments, and one step further, scalability; (2) the explicit key segment retrieval might attend to constrained contexts without global comprehension and does not fit into the example requiring unified information aggregation. To address these contradictions, we explore simple yet distinct principle: portraying long video understanding as big data processing, MapReduce [7] suggests an intriguing (1) Map applies independent and standardalternative. ized processing of small information units, i.e., short video clips; and (2) Reduce conducts unified reasoning and aggregate unit-level perception into final answer. If we con1 sider the motivating example of goal counting  (Fig. 1)  , the MapReduce flow correctly addresses: (1) for VLMs, the limited context lengths only handle short video clip; (2) for video agents, map enables independent and sequenceparallel perception of short video clips and reduce effectively provides global comprehension. Thanks to the convenience of large language models (LLMs), we validate such MapReduce principle into video agent: MR. Video. We build MR. Video with two MapReduce. (A) Captioning stage prepares texts as the efficient medium of video analysis. Its map step splits the video into atomic scenes and generates dense descriptions. Then, the reduce step enhances consistency by merging the repeated characters or (B) Analysis stage utilizes capobjects from each scene. tions and video frames to answer questions. Its map generates potential answers or intermediate analysis for atomic short video scenes, and then the reduce step aggregates the scene-level results into the final answer. Referring back to the example  (Fig. 1)  , MR. Video first densely describes the video (Captioning), then counts the scene-level goals and sums them together (Analysis). In fact, we will revisit this example again to examine MR. Video in Sec. 4.5.1. With the MapReduce principle, our MR. Video significantly pushes the limit of long video understanding: on LVBench [38], one of the most challenging benchmarks featuring hour-long videos and diverse questions, our MR. Video achieves an accuracy of 60.8%, more than 10% better than previous VLMs and video agents, with Gemini-Flash perceiving frames and GPT4o understanding texts. To summarize, our MR. Videos contributions are: 1. We propose the MapReduce principle for long video understanding, which conceptually mitigates the context length issues of VLMs and sequence-parallel & global context limitations of video agents. 2. We design MR. Video, video agent with simple MapReduce operations, i.e., sequence-parallel shortvideo perception and global information aggregation. 3. We highlight the potential of the MapReduce principle with MR. Videos significant improvement on benchmarks represented by the challenging LVBench. We hope the MapReduce principle, which is verified in LLM agents, is applicable for VLMs can inspire both VLMs and video agents in future works. 2. Related Work VLMs for Video Understanding. Existing VLMs [3, 4, 1923, 28, 29, 3134, 37, 39, 40, 42, 45, 4750, 59, 62] commonly follow LLaVA [26] by projecting image tokens to LLMs. As an image typically takes over 100 tokens in standard LLaVA model, context lengths become the major challenge for these models in long video understanding: how to digest the whole video without missing details? LongVILA [49]s solution is increasing the context length, but inherently needs more resources and is still limited by context lengths. Another prevalent solution is decreasing the average tokens per frame via merging or pruning. Such compression can follow certain priors, e.g., similarity of features [4, 21, 31, 33, 34, 39, 45, 47, 48], or Q-formerlike [12, 13, 17, 18] learnable module [22]. Notably, the recent VideoChat-Flash [21] can support up to 10k frames with sufficient hardware. However, aggressive compression might lead to unreliable perception of visual details. Such inherent context length limitations of VLMs necessitate more flexible agentic paradigms as below. Video Understanding Agents. Video agents provide meta-level LLM controller on the top of VLMs, which splits long video into sub-tasks of short video [8, 41, 43, 53, 58]. Therefore, they are not constrained by context lengths. By imitating how humans watch videos, video agents can be treated as increasing the test-time compute of VLMs via multi-round exploration [53], key-frame retrieval [8], and tool-use [41]. However, video agents still demonstrate disadvantages compared with VLMs, as mentioned in the introduction: (1) the sequential multi-round exploration hinders scalability, and (2) reliance on key-frame retrieval constrains the understanding of sufficient contexts. From such aspects, the MapReduce principle in our MR. Video bridges these gaps (as in Fig. 1(c)) and marks distinct and simpler framework for long video agents. LLM Agents. Our MR. Video also contributes long video understanding to broader exploration of addressing complex problems with the advanced reasoning ability of LLM agents, such as software engineering [14, 51] and knowledge retrieval and reasoning [52, 54, 60]. The LLM agents can autonomously tackle challenging problems with proper chain-of-thought prompts [44] and decomposed workflows. This work treats LLM agents as convenient interface to prototype the new MapReduce principle for long video understanding and indeed shows significant improvement. 3. Method 3.1. Overview As discussed in the introduction (Sec. 1), the MapReduce principle is compelling for long video understanding because it conceptually addresses the challenge of digesting global contexts while perceiving local details. To verify its effectiveness in practice, we validate this principle with our MR. Video: LLM agent provides convenient way to control the information flow and enables investigation of incorporating MapReduce into long video understanding. MR. Videos overview1 is in Fig. 2. It consists of (A) The Captioning two major MapReduce stages. stage (Sec. 3.2) generates dense captions, which provide broad comprehension of the video contents and serve as an efficient foundation for answering multiple questions on 1The displayed video is the 1st from LVBench (video link). For readers convenience, we will consistently use it for method demonstrations. 2 Figure 2. Overview. MR. Video validates the effectiveness of MapReduce principle with an LLM agent framework. We demonstrate two distinct types of questions for visual details and reasoning. (a) Captioning (Sec. 3.2) first generates detailed captions of individual (b) Question Intention scenes (Map) and then enhances consistency by merging repeated characters/objects for the scenes (Reduce). Analysis (Sec. 3.3) investigates if video segment contributes useful information (Map) and then forms unified analysis at the video level (Reduce). (c) Goal-Aware Analysis (Sec. 3.4) delves deep into detailed perception and reasoning with available contexts, guided by the intention analysis (Map), then unifies them into final answer (Reduce). For clarity, MR. Videos intermediate texts are simplified. (B) The Analysis stage (Sec. 3.3 and the same video. Sec. 3.4) conducts question-specific perception of the video. It first emphasizes understanding the intention of the question (Sec. 3.3), i.e., what the question is actually asking, then purposefully inspects the visual details or longer temporal spans (Sec. 3.4). The Map steps are independent in both stages for different video segments, and the Reduce stage condenses the segment-level results into unified video-level understanding. 3.2. Captioning Our captioning is shown in Fig. 2(a): (1) The Map step (Sec. 3.2.1) generates dense captions at the scene level, and (2) the Reduce step (Sec. 3.2.2) provides coherent names for repeated characters and objects for consistency. For 1hr - 2hr videos on LVBench [38], our captioning generates 500 - 2k captions for the whole video, similar to an article. 3.2.1. Map: Dense Scene Captioning We apply sequence-parallel Map operations for each video segment to generate dense captions and set of key characters and objects, as in Fig. 2(a). Detailed Description. MR. Video utilizes VLMs to describe short video clips in detail. We empirically discover that existing VLMs might struggle with processing video clips with multiple actions or significant transitions. Therefore, we first (1) prompt VLMs to check the uniformity of every short video clip2 and specify the transitioning frame indexes if the video clip contains multiple scenes; then (2) conduct the captioning task by letting the VLMs describe every scene in detail. Such two-step strategy decreases existing VLMs difficulties and provides scenes as atomic units for downstream video perception. Key Characters and Objects. To enable the objectawareness of captions, we sparsely sample frames from longer video segment3 and instruct the VLM to identify the salient characters/objects, describe their identifiable properties, and specify the frame indexes most saliently showing these characters/objects (as Fig. 3(a)). These frames, along with their appearance properties, are pre-pended to the contexts of VLMs before generating captions (the 2nd step in the previous paragraph). 3.2.2. Reduce: Consistent Characters and Objects Identifying consistent characters is an essential requirement for understanding long videos. However, the clip-level captions from Map steps commonly have inconsistencies due 220 frames in total, 10s segments with frames sampled at 2 fps. 330 frames in total, 2min segments with frames sampled at 0.25 fps. 3 Figure 3. Key Characters/Objects in Captioning. (a) The map step extracts the salient characters/objects along with description, which is useful for frames with multiple characters (the 3rd frame). (b) Then, the reduce step uses VLM to associate the repeated characters, enhancing the consistency of captions. to their sequence-parallel generation, such as one name belonging to two characters in different clips or one character being assigned two names for different video clips, as in the Map step of Fig. 3(a). To enhance consistency at the video level, our Reduce step merges the repeated characters and resolves contradicting names with character association and caption modification. (1) MR. Video instructs the VLM to associate the repeated characters/objects by observing the salient frames of extracted characters/objects, as Fig. 3(b). (2) Then MR. Video assigns new set of names for every character to avoid repeated names and accordingly updates the names in the original clip-level captions to newly created ones. For controllability, we format all the names as < NAME > in captions. Although using external tracking tools [8] might further enhance consistency, we use VLMs because of simplicity and the fact that videos frequently changing scenes could break the assumption of trackers. 3.3. Analysis I: Question Intention Analysis MR. Video starts the analysis by understanding the intention of the question. We emphasize the importance of intention analysis because of the inherent ambiguity of questions the questions might only in long-context understanding: contain partial information, and the model has to recover crucial clues like when, how long, and where in the video to perceive. For example, Fig. 4 demonstrates multiple scenes potentially relevant to the questions, while only one should be correctly selected via reasoning. This stage utilizes the captions from captioning Sec. 3.2 and optionally includes video frames. Compared with key-frame retrieval in previous video agents [8, 41], MR. Video uniquely combines broader contexts for localizing the relevant frames. 3.3.1. Map: Segment Intention Analysis Without losing generality, we divide the video into nonoverlapped segments, each empirically containing 32 scenes. For long video with 1k scenes, this is equivalent to 30 segments. Then, the VLM processes the segments agFigure 4. Question Intention Analysis. Long video questions generally require the model to recover certain information, e.g., the meaning of protagonist, what utility room looks like, and confounding relevant segments, as in (a). This motivates MR. Videos explicitly understanding the questions intentions by reasoning both the video contents and questions. gregated captions and middle frames to infer whether any scene is related to the users question. As each segment is not extremely long with the captions, MR. Video can attend to the details without being limited by the context lengths. Within each segment, we instruct MR. Video to focus on what is the question asking about and generate paragraph of analysis as Fig. 2(b). Concretely, its response contains: (1) Reasoning: paragraph analyzing the key subject/criteria mentioned by the questions and how the contents presented in the captions could align with the question in any perspective, e.g., Fig. 4(c). (2) Candidate Scenes: the LLM then lists the potential scenes that could contribute to answering the question. Please note that this is distinct from directly retrieving key frames since it provides more contexts and allows frames that are helpful in indirect ways. (3) Key Subjects: The local caption segment becomes insufficient if the question mentions characters or criteria requiring global video information. So MR. Video specifies its unsure criteria and their identifiable properties here for the global Reduce step to analyze. 3.3.2. Reduce: Global Intention Analysis MR. Videos reduce step aggregates the segment-level analyses at the video level with an LLM processing the texts generated from the map step. For 1 hr - 2 hr video, the LLM only needs to handle 10 - 30 paragraphs of the analysis notes, similar to short article. The reduce step generates similar contents as the map step but covers the contexts of the whole video and localizes the best scenes and subjects for the questions as in Fig. 2(b). Specifically, it responds with: (1) Reasoning: paragraph detailing what the LLM has learned from reading through segment-level analyses and how they fit the questions. (2) Candidate Scenes: the reduce step poten4 sampled frames for visual details or inter-segment sparsely sampled frames for global reasoning. Goal Proposal. When generating the queries for VLMs, we are inspired by the flexibility of Visual Programming [11] and ViperGPT [35]: let LLM propose its queries for the VLMs and understand the candidate scenes in customized ways. As shown in Fig. 5, MR. Video proposes the VLM query to cover multiple aspects of the question, gathering comprehensive information. Perceiving Local and Global Information. We employ the reduce step as different strategies of sampling frames for VLMs to inspect. (1) Local: We densely sample frames within each short segment for objectives requiring detailed visual information, such as the example in Fig. 5. (2) Global: We sparsely sample frames across different segments, e.g., the middle frames of the relevant segments identified by the intention analysis, and let VLM perceive them for information spanning longer temporal ranges. To better leverage the reasoning capabilities of the LLMs, this step can also include the captions of the selected segments to simplify the perception. 3.4.2. Reduce: Answer Generation The last reduce step attends to the global context information and generates final response. With the previous map steps gradually summarizing the information, this reduce step is no longer limited by context lengths and can fully unleash reasoning capabilities. As notable necessity of this reduce step, it merges the scene analysis results together in unified way, especially when different scenes provide contradictory perception results or further calculation of scene-level information is required. 4. Experiments 4.1. Datasets Evaluation Dataset Selection. To validate the MapReduce principle within our limited budget, we focus on the challenging long video benchmark: LVBench [38]. Compared with others [9, 30, 34, 61], LVBench features more extremely long video durations and challenging questions, which are directly reflected by the significantly lowered accuracies of state-of-the-art models. With limited budget, we expand the breadth of evaluation using the subsets, especially the long video parts of LongVideoBench [46], VideoMME [9], and EgoShema [30]. Dataset Settings. LVbench [38] curates 1,549 questions on 103 videos ranging from 30 min to 2 hrs, covering 6 video categories. We utilize the LVBench data as follows. (a) As of March 1st 2025, 4 out of 103 videos are unavailable from YouTube for downloading. So, our comparison in Sec. 4.3 utilizes all the remaining 1,492 questions. (b) For the ablation study (Sec. 4.4), we form subset to save the budget by selecting the first video of each video category in Figure 5. Customized Queries for Perception. With this question requiring detailed visual perception, MR. Video proposes objective-aware queries for the VLMs, confirming the criteria. tially condenses or merges the candidate scenes listed in the map step according to both video contents and the criteria of questions. (3) Key Subjects: by aggregating the global information, LLM can identify the characters/subjects asked by the question and further specify it with the names in captions, e.g., < person >. An example of the reduce process is in Fig. 4(d). 3.3.3. Key Segment Retrieval v.s. Intention Analysis Explicitly reasoning the intention of questions, i.e., completing the contexts, is significant difference between our MapReduce principle and previous video agents [8, 41, 53]. Our design emphasizes using the models reasoning abilities to inspect short video clips in detail (map) and then comprehend the video as whole (reduce). Although the key-segment selection of previous video agents implicitly reflects the intention analysis objective by choosing few frames with the most similar features to the question, it is an over-simplified model for long contexts: in the example of Fig. 4, it is challenging to extract features reflecting protagonist, utility room, or windows before understanding the video contexts. Our quantitative analysis on key segment retrieval is in Sec. 4.4.2. 3.4. Analysis II: Goal-Aware Analysis Based on the analyzed question intentions, MR. Videos final MapReduce stage purposefully gathers the information related to the questions and converts them into final answer, namely, goal-aware analysis, as in Fig. 2(c). An essential functionality of this stage is that MR. Video should explicitly plan the type of information it needs: attending to captions or sparse frames over longer time horizons provides richer contexts for reasoning, e.g., Q2 in Fig. 2; while focusing on densely sampled frames benefits visual recognition, e.g., Q1 in Fig. 2. With both capabilities, our MR. Video can flexibly handle wide range of questions. 3.4.1. Map: Goal-Aware Scene-centric Analysis Starting from the candidate scenes generated by question intention analysis (Sec. 3.3), MR. Video proposes purposeful queries for VLMs to perceive frames intra-segment densely 5 LVBench. This subset has 6 videos and 98 questions in total. For additional evaluation, we use (1) the longest subset of LongVideoBenchs validation set, (2) the long video subset of VideoMME without subtitles, and (3) the validation set of EgoSchema. More details are in Sec. B.4. 4.2. Implementation Details MR. Video Details. Our MR. Video demonstrates simple framework validating the MapReduce principle, only requiring one LLM for text understanding and one VLM for image understanding. We consistently utilize Gemini-2.0Flash [36] as our VLM to control our budget and GPT4o as the default LLM. On average, generating the dense captions for an hour-long video requires approximately $0.8 of Gemini-2.0-Flash, and answering each question from LVbench costs $0.4 GPT4o on average. We provide further details, especially the prompts, in Sec. B. Baseline Evaluation. For the performance of baselines on evaluated benchmarks, we mainly refer to the numbers on the leaderboards or provided by authors. For Gemini2.0-Flash, which is our VLM and has not been evaluated on these benchmarks yet, we follow the standard VLM setting by uniformly sampling 256 video frames for long videos (LVBench, LongVideoBench, Video-MME) and 128 frames for the 3min EgoSchema. The prompts are in Sec. B. Controlled Context Lengths. We highlight vital implementation detail so that our verifying the MapReduce principle is faithful: we explicitly control the VLM to perceive less than 32 frames per query. This ensures MR. Video does not encounter the context length limits. 4.3. Main Comparison 4.3.1. Nuanced Distinctions of Long Video Benchmarks Before analyzing the experimental results, we first clarify an important aspect of long video understanding benchmarks: each benchmark emphasizes different aspects of video understanding in subtle ways. While they all feature diverse set of questions, they vary in wording and style. As result, none of the existing models show consistent advantage across all the benchmarks, even GPT4o and Gemini-Pro. Comparatively, LVBench and LongVideoBench emphasize the challenges of localizing one or multiple key video clips and exact matching of contents, while Video-MME and EgoSchema contain more interpretative questions similar to how humans gain an intuitive impression of video segment. Even LVBench and LongVideoBench are slightly different: LongVideoBench provides more explicit visioncentric cues, and LVBench specifies more from story or event aspect. More detailed examples and discussion are in Sec. A. Therefore, it is critical to notice the nuances of benchmarks and understand the models comprehensively. 4.3.2. LVBench Comparison Given the conceptual advantages of MapReduce, our experiments verify its practical effectiveness. We compare Model ER EU KIR TG RE SUM Overall Proprietary VLMs Gemini-1.5-Pro [36] GPT4o [1] Gemini-2.0-Flash [36] 32.1 26.5 47.4 30.9 23.7 48. 39.3 28.3 56.8 31.8 21.4 39.3 27.0 28 44.4 Open-sourced VLMs InternVL2-40B [6] TimeMarker [5] Qwen2-VL-72B [37] VideoLaMA3-2B [57] mPLUG-Owl3 [55] InternVL2.5-78B [6] VideoLLaMA3-7B [56] Qwen2.5-VL-72B [2] ReTake [39] VideoChat-Flash [21] GLM-4V-Plus [10] Video Agents VideoAgent [41] VideoTree [43] VCA [53] 37.4 42.8 38.0 41.5 46.0 43.8 45.8 - 49.8 51.1 46.2 39.7 39.1 41.1 39.7 41.6 42.0 42.4 - 46.2 46.0 47.8 43.4 34.9 38.3 44.0 42.4 42.1 47.8 - 52.9 49.0 54.1 31.4 38.7 41.4 32.7 41.1 36.8 35.9 - 45.0 38.9 42.7 42.5 38.2 46.5 45.8 47.5 51.0 45.8 - 45.8 48.5 46. 28.0 30.3 43.7 30.3 25.1 40.7 28.0 26.5 37.8 29.3 27.7 38.0 28.0 31.9 46.2 MR. Video (Ours) 59.8 57.4 71.4 58.8 57.7 32.8 32.8 41. 41.4 48.8 46.6 25.9 40.4 37.9 36.2 - 27.6 34.5 37.9 36.4 25.5 27.3 50.0 33.1 27.0 48.6 39.6 41.3 41.3 41.6 43.5 43.6 45.3 47.7 47.8 48.2 48.7 29.3 28.8 41. 60.8 Table 1. LVBench Comparison. Our MR. Video significantly outperforms previous methods by large >10% margin, suggesting the effectiveness of MapReduce principle. (Gemini-2.0Flash is evaluated by ourselves. The VLM accuracies are from the official leaderboard, and the video agent accuracies are from VCA [53].) Model VLMs LVBench LongVideoBench EgoSchema Val (Long) Overall Val Video-MME Long (w/o Sub) Gemini-1.5-Pro [36] GPT4o [1] Gemini-2.0-Flash [36] Video Agents VideoAgent [8] VideoAgent [41] VideoTree [43] VCA [53] MR. Video (Ours) 33.1 27.0 48. - 29.3 28.8 41.3 60.8 60.9 58.6 45.7 - - - - 61.6 - 70.4 71. 62.8 63.2 67.0 73.6 73.0 67.4 65.3 63.0 - - - - 61.8 Table 2. Breadth Comparison on Long Video Benchmarks. Viewing long video benchmarks comprehensively, as explained in Sec. 4.3.1, MR. Video shows better or on-par performance with the baseline models and other video agents. (Except for Gemini2.0-Flash evaluated by ourselves, LongVideoBench accuracies are from their paper [46], EgoSchema accuracies are from VCA [53], and Video-MME accuracies are from the official leaderboard.) MR. Video with VLMs and video agents in Table 1, on the complete set of LVBench [38]. As shown in Table 1, our MR. Video shows significant advantage over all the other methods with an accuracy improvement of over 10%, indicating considerable enhancement of long video understanding. As explained in Sec. 4.1 and Sec. 4.3.1, MR. Video features the challenge of connecting the questions with video content. So, MR. Videos effectiveness supports the MapReduce principle for long video understanding. 4.3.3. Breadth Comparison As shown in Table 2 (LVBench performance is listed for reference), the nuanced differences in benchmarks (Sec. 4.3.1) make it challenging for single model to dominate all the 6 benchmarks. However, our MR. Video demonstrates advantages on the LongVideoBench, which requires precise localization of key video segments, similar to LVBench. This further validates our improvement for LVBench and supports the effectiveness of the MapReduce principle. On EgoSchema and Video-MME, our MR. Video performs on par with its VLM, Gemini-2.0-Flash, while requiring smaller context length of 32 frames. Although our MR. Video has shown gaps to Gemini-Pro on VideoMME, it is due to common challenge and future direction of video agents: since the texts are lossy representations of visual contents, video agents naturally struggle with the interpretative questions on video segment like Video-MME, whereas VLMs are naturally better by observing frames across long ranges without information loss. Although training LLMs for text-based video understanding could alleviate this problem compared to zero-shot prompting, it falls outside our objective of validating the MapReduce principle with simple video agents especially given that MR. Video already shows significant improvements on LVBench and LongVideoBench. Therefore, we conclude that our MR. Video supports the effectiveness and generalization of the MapReduce principle. 4.4. Ablation Study and Analysis Since the main objective of MR. Video is to validate the MapReduce principle, we provide an analysis of its critical steps. By default, we utilize the LVBench subset explained in Sec. 4.1 for the ablation studies in this section. 4.4.1. Consistent Characters We first analyze the benefits of the reduce step in captioning (Sec. 3.2): providing consistent characters/objects in the captions. Specifically, MR. Video comprehends the captions generated from individual video segments before the reduce step, and this w/o Consistent Characters (Fig. 6(a)) performs worse than the MR. Video with consistent characters (59.2% v.s. 62.2%). Therefore, aggregating global information, i.e., is essential for long video understanding. From qualitative sense, consistent characters enable MR. Video to better analyze question intentions. For example, MR. Video can connect different parts of the video and specifies the appearances of < woman > in Fig. 5 with the help of consistent characters, without only relying on the visual cues of sedan chair. Therefore, the reduce capabilities enhance the reliability of long video understanding models. 4.4.2. Question Intention Analysis Our question intention analysis is critical MapReduce step (Sec. 3.3) and integrates more context information. We first analyze its effectiveness and comparison with key frame retrieval. The experimental details are in Sec. B.5.2 (supplementary). Finding Relevant Segments. With LVBench providing Figure 6. Analytical Experiments of MR. Video. (a) We investigate the benefits of MR. Video components and the influence of LLMs. (b) The comparison between our question intention analysis and key frame retrieval suggests the necessity of combining more video contexts for understanding. the ground-truth time intervals for each question, we can compare the relevant scenes identified by MR. Video with the annotations of LVBench via recall, where having time overlap is defined as correct match. MR. Video correctly localizes the relevant scenes for 68.8% of the questions, where video typically contains 500-2k scenes for MR. Video to identify from. To illustrate the importance of comprehensive contexts in this process, we compare our question intention analysis with key segment retrieval as below. Comparison with Key Segment Retrieval. We employ MM-Embed [24], state-of-the-art multi-modal retrieval model, to encode 256 evenly sampled video frames and the questions for retrieval. Since MR. Video needs to select from 500-2k scenes, the fewer 256 candidates decrease the difficulty for the retriever. Following prior works [41], we encode video frames and the questions, then match them according to the inner product of embeddings. Since MR. Video might identify multiple relevant scenes, we let the retriever select the same number of top-k candidate intervals for fair comparison. Finally, this approach achieves an accuracy of 34.4%, which is significantly worse than our question intention analysis (Fig. 6(b)). Notably, MR. Video only accesses captions without video frames on LVBench, which proves the necessity of reduce, i.e., combining global context information, for localizing the key segments. 4.4.3. Goal-aware Analysis Goal-aware analysis (Sec. 3.4) provides the video agents with opportunities to delve deep into video content after coarse analysis, key distinction between video agents and existing VLMs. To validate its necessity, we directly provide the results of global intention analysis to the answer generator (Sec. 3.4.2) and evaluate the accuracy of the responses. Such model achieves an accuracy of 52.0%, worse than full MR. Video (Fig. 6(a)). Therefore, it is crucial for perception to inspect the details with goal awareness after understanding the global context. 4.4.4. Different LLMs LLM plays crucial role in MR. Video for video reasoning through texts. As shown in Fig. 6(a), the performance significantly drops if GPT4o is replaced with Gemini-2.0Figure 7. Referring back to the motivating example  (Fig. 1)  , MR. Video demonstrates the necessity of MapReduce principle: checking every scene in detail (map) and aggregating information for the whole video (reduce). Although the model misses some goals due to strict criteria (the full action of shooting, goal, and celebration) and frame sampling, MR. Video shows the desired behavior of counting exhaustively and speculates at least 78 goals. Flash or Qwen-2.5-7B. Echoing the discussion in Sec. 4.3.3 and Sec. 4.5.2, it is challenging for an LLM to understand videos via the lossy medium of texts, which is the major bottleneck of video agents. From this perspective, our MR. Video is primarily designed to validate the potential of the MapReduce principle while improving the language models with training like VLMs would benefit video agents in future works. 4.5. Case Analysis 4.5.1. Positive Example Analysis Finally, we refer back to the motivating example presented in the beginning  (Fig. 1)  : Is the MapReduce principle capable of such challenging long video understanding? As shown in Fig. 7, MR. Video exhaustively perceives each video clip and searches for the indications of goal by No. 11; then, the reduce step counts the number of goals by rigorous standard. Although the number of goals is smaller than the ground truth due to frame sampling and rigorous standards, MR. Video cannot exclude the wrong answers with the MapReduce information. Our observations on this challenging case suggest: (1) MR. Video indeed follows the MapReduce behavior in real scenarios; (2) the MapReduce principle is instructive for long videos. 4.5.2. Failure Case Analysis As demonstrated in the comparisons (Sec. 4.3), our MR. Video can localize information accurately. However, major disadvantage and future work of existing video agents is the transitioning between the video and text modalities. (1) The LLMs are trained on language resources, which might not be aligned with text-based video understanding. Such misalignment leads to reasoning incapability as in example 1 of Fig. 8: the LLM responds with the wrong answer, even though it already notices the right video clip. (2) Moreover, the upper bound of MR. Video is limited by the quality of captions. If the VLM overlooks specific visual details, such as the noodle and womans face in example 2 of Fig. 8, such Figure 8. Failure Case Analysis. MR. Videos failure largely comes from (1) the LLMs not aligned with humans way of watching videos, so that it cannot convincingly reason about the scene transitions (example 1); and (2) VLMs failing to capture certain details, where the LLM does not have sufficient information (example 2). (Noodles and the blurry face are pointed with arrows.) information loss determines that the LLM cannot get the correct answer. Therefore, both failure analyses reveal the necessity of VLM-like training frameworks for long video understanding instead of relying entirely on the zero-shot abilities of off-the-shelf and VLMs. 5. Conclusions We introduce simple and new principle for long video understanding: MapReduce. It conceptually addresses the inherent challenge of long video understanding, i.e., simultaneously digesting global contexts while perceiving local details. Compared with the frameworks of previous VLMs and video agents, the MapReduce principle shows the advantage of being free from context length limits, better sequence parallelization, and more comprehensive global contexts. Utilizing the convenience of LLM agents, we validate the effectiveness of MapReduce with MR. Video: workflow composed of captioning, question intention analysis, and goal-aware analysis. MR. Video demonstrates an improvement of over 10% on the challenging LVBench. This firmly pushes the limit of long video understanding and validates the potential of the MapReduce principle. Limitations and Future Work. (1) This paper uses LLM agents as convenient tool to verify the MapReduce principle, leaving VLMs unexplored. Future work could incorporate this principle into VLMs via techniques like block attention so that highly detailed local perception can happen simultaneously with global context comprehension. (2) As discussed in Sec. 4.3.3 and Sec. 4.5.2, LLM agents can be enhanced with training or alignment to overcome the information loss issues and enhance the performance for interpretative questions like Video-MME ones."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [3] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jeng-Neng Hwang, Saining Xie, and Christopher Manning. AuroraCap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. 2 [4] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In ECCV, 2024. 2 [5] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile video-llm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024. 6 [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, InternVL: Scaling up vision foundation Lewei Lu, et al. models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 6 [7] Jeffrey Dean and Sanjay Ghemawat. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1):107113, 2008. [8] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. VideoAgent: memory-augmented multimodal agent for video understanding. In ECCV, 2024. 1, 2, 4, 5, 6 [9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal LLMs in video analysis. arXiv preprint arXiv:2405.21075, 2024. 5, 12, 13 [10] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. ChatGLM: family of large language models from GLM-130B to GLM-4 all tools. arXiv preprint arXiv:2406.12793, 2024. 6 [11] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In CVPR, 2023. 5 [12] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In ICML, 2021. 2 [13] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver IO: general architecture for structured inputs & outputs. In ICML, 2022. 2 [14] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWEBench: Can language models resolve real-world github issues? In ICLR, 2024. [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1 [16] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. LLaVA-NeXTInterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1 [17] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. 2 [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 2 [19] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. VideoChat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2 [20] Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy. Temporal preference optimization for longform video understanding. arXiv preprint arXiv:2501.13919, 2025. [21] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. VideoChat-Flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 2, [22] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-VID: An image is worth 2 tokens in large language models. In ECCV, 2024. 2 [23] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-LLaVA: Learning united visual representation by alignment before projection. In ACL, 2024. 1, 2 [24] Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. MM-Embed: Universal multimodal retrieval with multimodal LLMs. In ICLR, 2025. 7, 14 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024. 1 [28] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. NVILA: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. 2 9 [29] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx MLLM: On-demand spatialIn ICLR, temporal understanding at arbitrary resolution. 2025. 2 [30] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. EgoSchema: diagnostic benchmark for very longform video language understanding. In NeurIPS, 2023. 5, 13 [31] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. TimeChat: time-sensitive multimodal large language model for long video understanding. In CVPR, 2024. 2 [32] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. LLaVA-PruMerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [33] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. LongVU: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. [34] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. MovieChat: From dense token to sparse memory for long video understanding. In CVPR, 2024. 2, 5 [35] Dıdac Surıs, Sachit Menon, and Carl Vondrick. ViperGPT: Visual inference via python execution for reasoning. In ICCV, 2023. 5 [36] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 6 [37] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 6 [38] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. LVBench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2, 3, 5, 6, 12, 13 [39] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. ReTaKe: Reducing temporal and knowledge redundancy for long video understanding. arXiv preprint arXiv:2412.20504, 2024. 2, [40] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. LongLLaVA: Scaling multi-modal LLMs to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 2 [41] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena YeungLevy. VideoAgent: Long-form video understanding with In ECCV, 2024. 1, 2, 4, large language model as agent. 5, 6, 7, 14 [42] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, InternVideo2.5: Empowering video Jianfei Gao, et al. MLLMs with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 2 [43] Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal. VideoTree: Adaptive tree-based video representation for LLM reasoning on long videos. arXiv preprint arXiv:2405.19209, 2024. 2, [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 2 [45] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. LongVLM: Efficient long video understanding via large language models. In ECCV, 2024. 2 [46] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In NeurIPS, 2024. 5, 6, 12, 13 [47] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. In CVPR, 2025. 2 [48] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Slowfast-llava: strong training-free baseDehghan. arXiv preprint line for video large language models. arXiv:2407.15841, 2024. 2 [49] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. LongVILA: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. [50] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2 [51] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWEAgent: Agent-computer interfaces enable automated software engineering. In NeurIPS, 2025. 2 [52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. HotpotQA: dataset for diverse, exarXiv preprint plainable multi-hop question answering. arXiv:1809.09600, 2018. 2 [53] Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, and Chuang Gan. VCA: Video curious agent for long video understanding. arXiv preprint arXiv:2412.10471, 2024. 1, 2, 5, 6, 13 [54] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In ICLR, 2023. 2 [55] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mPLUGOWL3: Towards long image-sequence understanding in multi-modal large language models. In ICLR, 2024. 6 [56] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, and Deli Zhao. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 6 [57] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. VideoLLaMA 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 6 [58] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple LLM framework for long-range video question-answering. In EMNLP, 2024. 2 [59] Yuanhan Zhang, Bo Li, Haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. LLaVA-NeXT: strong zero-shot video understanding model, 2024. 1, 2 [60] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. In ICML, 2024. 2 [61] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [62] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv preprint arXiv:2412.10360, 2024. 2 11 MR. Video: MapReduce is the Principle for Long Video Understanding Supplementary Material A. Delving into Long Video Benchmarks In Sec. 4.3.1, we discussed the critical fact that the long video datasets have nuanced differences in their preferred long video understanding capabilities. In this section, we present several representative examples showing such distinctions. Please note that all of these benchmarks have curated diverse set of questions. We demonstrate examples only to provide an intuition of the complexity of question styles instead of claiming that these benchmarks can be solved with few techniques. As in Fig. A, the LVBench [38], LongVideoBench [46], and Video-MME [9] have different question styles. LVBench. For LVBench, the model has to localize the scene of solo fight correctly and understand the meaning of knock down and wipe face to answer the question. Notably, the model has to integrate the contexts of the video and speculate the protagonist first to execute this task. LongVideoBench. Although both require precise localization, LongVideoBench is different from LVBench. LongVideoBench provides explicit and accurate visual cues for the model to localize the object, but the model has to propagate such information across the temporal axis to answer the question. Compared with LVBench, LongVideoBench emphasizes models visual detail perception and temporal association abilities. Video-MME. Unlike the above two benchmarks, many questions in Video-MME are not about specific event. Instead, they are more interpretative, similar to the impression of human after watching the videos."
        },
        {
            "title": "We hope the above analysis reveals the spectrum of long",
            "content": "video understanding and existing benchmarks. B. Prompts and Implementation Details B.1. Captioning Prompts We describe the detailed steps and prompts for our dense captioning of the video (Sec. 3.2). All the datasets share the same captioning prompts. Map: Dense Scene Captioning. As in Sec. 3.2.1, we let each short video segment produce its dense captioning, involving the following three map steps all the video segments are independent within each step to support parallel inference: 1. We split each 10s video segment into individual scenes and check if the first scene of segment can be merged with the last frame of the previous segment. Scene splitting prompts are in Table A, and the Scene Merging prompts are in Table B. 2. We identify the salient characters and use them to generate the dense captions in each video segment. The prompts are in Table C. 3. With the selected characters, we generate the dense captions of each scene with the prompts in Table D. Reduce: Consistent Characters and Objects. As in Sec. 3.2.1, our additional Reduce step enhances consistency by merging the repeated characters into unified names. It involves the following steps: 1. We iteratively check if the characters from two video segments overlap with the prompts in Table E. 2. After assigning new names to all the characters/objects, we modify the old names in the original dense captions with the prompts of Table F. B.2. Analysis Prompts We describe the prompts for question intention analysis (Sec. 3.3). Map: Segment Intention Analysis. We let standard LLM check the scene-level information and understand the user intentions. Each chunk of captions contains 50 scenes. Its prompts are in Table G. Reduce: Global Intention Analysis. This step utilizes an LLM to process the segment-level analyses from the previous step and unify them into condensed video-level analysis. The prompts are in Table H. The most critical part is explicitly instructing the LLM to conduct video-level reasoning and find the most proper scenes. B.3. Analysis II Prompts This section provides the details and prompts for MR. Videos goal-aware analysis (Sec. 3.4). Map: Goal-aware Scene-centric Analysis. Based on the information required to answer the question, MR. Video first proposes customized queries for each question and as in Table and apply these questions to the VLMs. Reduce: Answer Generation. The final step is to combine the results of goal-aware scene-centric analysis with the global intention analysis to generate final response. The prompts are in Table J. B.4. Datasets LVBench Videos. We clarify the unavailable videos from LVBench, as mentioned in Sec. 4.1. LVBench requires users to download from YouTube with provided links to protect the copyright. As of March 1st, 2025, 4 videos are no longer available on YouTube, so we cannot evaluate them. Their IDs are: 28CIeC8cZks, idZkam9zqAs, QgWRyDV9Ozs, gXnhqF0TqqI. After filtering them out, we 12 Figure A. Examples of Different Long Video Benchmarks. still have 1,492 out of 1,543 questions. LVBench Ablation Subset. We select the first video of each category from LVBench (cartoon, live, self-media, documentary, TV, and sports) and form subset for the ablation study, as mentioned in Sec. 4.1. Th six selected videos are: Cm73ma6Ibcs, TiQBTesZUJQ, t-RtDI2RWQs, hROKtPqktO8, rSE2YPcv89U, and CgvJqGxzRfE. They consist of 98 questions in total. Breadth Benchmarks. As mentioned in Sec. 4.1, we utilize several long video benchmarks in addition to LVBench [38] to provide comprehensive evaluation. However, we evaluate on their subsets due to limited computation resources. (1) LongVideoBench [46]. We evaluate the (900, 3600] duration group of the validation set of LongVideoBench. There are 188 videos and 564 questions in total. In the comparison, the accuracies of the VLMs come from Table 5 of the LongVideoBench paper. (2) EgoSchema [30]. We evaluate on the validation set of EgoSchema, which contains 500 videos and questions. The performance mainly comes from the Table 2 of VCA [53]. (3) Video-MME [9]. Our evaluation follows the long video subset of Video-MME, under the setting of not using subtitles. This set contains 300 videos and 900 questions in total. The performance of models comes from the official leaderboard as of March 1st 2025. B.5. Analytical Experiments Details B.5.1. Baseline Evaluation As mentioned in Sec. 4.2, we evaluate Gemini-2.0-Flash on the long video benchmarks. For the 30min to hour ones, including LVBench, LongVideoBench, and Video-MME, we follow the standard setting of uniformly sampling 256 frames from each video. For EgoSchema, whose videos are 3min, we uniformly sample 128 frames for evaluation. 13 ## Context You will be given few continuous screenshots of the video corresponding to approximately 10 seconds of video duration, and provide detailed, faithful, and accurate analysis of this video segment. analysis is to group the video into short segments based on the contents for the sake of captioning and user question answering. The objective of this You should cover each action and event in the clip. ## Instructions To perform the analysis of decomposing video into shorter parts, lets do it step by step. 1. Based on the provided frames of this video segment, please describe the contents of the video segment briefly and accurately. accurate. 2. Based on your description, please answer the following question: combination of multiple scenes?\" The definition of scene is single, self-contained, and continuous event that could be easily summarized into one sentence by human. Single:]\" 3. If the answer to the previous question is \"no\", please provide the index of frame(s) separating the scenes from the given frame. Frames]:\" and in the format of list of integers. The description should be detailed, faithful, and \"Is this video segment single scene or Your answer should come with header: Your answer should come with header: It should come with header: Description]:\". \"[2. \"[1. \"[3. ## Example Your response should be in the following format: [1. Description]: [2. Single: [3. Frames]: This video shows ... yes/no]: [5, 9] No. Please pay special attention to: - The precise localization of the frames is very important for downstream tasks. - The summarization at the scene level should be consistent with the frames you provided. of scenes should be one more than the number of frames in the list. consistent scene, you will give 1 summary; If you provide 1 frame, there should be 2 summaries; if you provide 2 frames, there should be 3 summaries, etc. Now you will be presented the video frames, please perform the analysis carefully. If you provide 0 frames since the images display For instance, the number Table A. Scene Splitting prompts for the Captioning stage (Sec. B.1) You are going to help with determining if short video segment is consistent scene. continuous screenshots of the video clip, and provide detailed, faithful, and accurate analysis of this video segment."
        },
        {
            "title": "You will be given a few",
            "content": "Your objective is simple: frame*. Answer with \"yes\" or \"no\". if *the video clip starting from the second frame* is consistent scene with *the first Now you will be presented the video frames, please perform the analysis carefully. Table B. Scene Merging prompts for the Captioning stage (Sec. B.1) pose multiple candidate scenes, we let the retriever select the same number of top-k candidates for fair comparison. Finally, every question searches its relevant frames via maximum inner-product between the question and video frame embeddings. With LVBench frequently asking about events of specific timestamps, we further provide each frames seconds as interleaved images and texts. Since LongVideoBenchs questions are commonly related to the subtitles, we provide the subtitles of sampled frames as the contexts to the VLM. The prompts used for evaluation are in Table K. B.5.2. Ablation Study on Finding Relevant Segments This section describes more details about the analytical experiments conducted in Sec. 4.4.2, where we compare the question intention analysis of MR. Video with multimodal retriever, MM-Embed [24]. Types of Questions. Since LVBenchs annotations for summarization and reasoning questions might specify long ranges, our evaluation mainly focuses on the question types with precise intervals: key information retrieval, event understanding, entity recognition, and temporal grounding. On our subset for analysis, this results in 64 questions. MM-Embeds Retrieval. Following the practice of VideoAgent [41], every video frame is encoded by concatenating its image contents with timestamp since some questions are related to specific seconds. In addition, every question is encoded along with its options as some questions do not contain specific contexts. Since MR. Video might pro14 # 1. Motivation You are paritipating in video captioning task, but you can only watch few frames of the video and lack broader context. characters and objects in the video. video, that could be influential, and organize them into visual memory for downstream tasks. Therefore, you will use using character-centric and object-centric visual memory that stores the key Your objective is to identify the potential key characters and objects from the Strictly follows the format: You will return the details of the characters that appeared in the current scene. You will return list of the names of the characters and objects that appeared in the # 2. Input and Output You will be given the following inputs: ## 2.1 Input You will have sveral sparsely sampled frames of the current video clip. ## 2.2 Output Your output will have the following format: [1. Appeared Characters]: current scene, from the visual memory. [2. Character Details]: item should contain the name of the character, representative frame of the character, and description about how to identify the character in the frame. [Visual Memory 1:] character]] [Visual Memory Ends] [Visual Memory 2:] character]] [Visual Memory Ends] ... Guidelines: 1. NAME should be general name, such as person a, person b, person c, object a, etc. faithful to the video without making assumptions. 2. how to uniquely identify the character or object from the representative frame. 3. most salient frame showing the front face of the character. FRAME should be the index of the frame that best represents the character or object in the scene, favorably the DESCRIPTION should be short description of the characters and objects appearance and properties, especially [[NAME: name], [DESCRIPTION: description], [FRAME: index of the selected frame to display this [[NAME: name], [DESCRIPTION: description], [FRAME: index of the selected frame to display this It should start from 0."
        },
        {
            "title": "Try to be rigorous and",
            "content": "[NAME1, NAME2, ...] Format is:"
        },
        {
            "title": "Each",
            "content": "Appeared Characters]: Character Details]: ## 2.3 Example Output: [1. [2. [Visual Memory 1:] [Visual Memory Ends] [Visual Memory 2:] 20]] [Visual Memory Ends] [Visual Memory 3:] [Visual Memory Ends] [\"person a\", \"person b\", \"dog a\"] [[NAME: person b], [DESCRIPTION: man with short hair and glasses in the frame], [FRAME: 10]] [[NAME: person c], [DESCRIPTION: woman with long hair and blue dress in the frame], [FRAME: [[NAME: dog a], [DESCRIPTION: dog with standing beside the man with short hair], [FRAME: 10]] When you are selecting the characters for the visual memory, please be picky: Please imagine yourself as human watching the video, trying to perceive the salient things from the video and # 3. Guidelines This is not an easy task, please make sure to use your advanced reasoning ability and check every item and step carefully. The following guidelines are very important for you to finish this task: 1. understanding the deeper plots of the video. 2. (a) Only select the characters and objects that you believe are salient and could significantly influence the plot. It could be person in the movie, an animal in the documentary or cartoon, etc. (b) Only include character if it is displayed saliently with great emphasis. the character clearly. Better to be safe than sorry. 3. 4. Format is very important. Please keep the strings in identical formattings to ensure smooth post-processing. Please make sure the [1. Appeared Characters] and [2. Character Details] are consistent. Make your best judgements. Be conservative if you cannot identify # 4. Your Job Now your job begins. Table C. Character Selection prompts for the Captioning stage (Sec. B.1) 15 Your analysis will be faithful and accurate to the video. # 1. Instructions You will be given few continuous screenshots of video clip, some potential key characters and objects of the video in your memory, and the caption of the previous scene. scene. Input: 1. Visual Memory: objects in the visual memory. 2. Previous Caption: 3. Video Frames: the names, representative video frames, and the identifiable properties of the characters and Your objective is to generate caption for current displayed the few continuous screenshots of the current video clip. the caption of the previous scene. If so, please list their name out. The goal is that human should read your captions and feel like watching continuous video. # 2. Guidelines When generating the caption, please follow the guidelines below and solve this problem step by step: 1. First, describe the main content of the current scene briefly. 2. Second, use the visual memory to identify if any characters or objects from the visual memory appear in the current scene. 3. Third, describe the scene in detail, including the characters, their actions, the objects, the properties of the characters and objects, the environment, and other types of contents, etc. Some more detailed tips: 1. When generating the captions, please take the previous scene as contexts and pretend that you are watching video continuously. 2. When generating the captions, please be faithful to the video and make logical connections between the scenes. 3. When you encounter characters, please utilize the information and name from the visual memory if what you see matches the visual memory. <person a> to refer to the character in your captions. Important Rules: 1. 2. 2.1 What are the characters, what are their appearances, what are there clothes, what are their actions, what are their emotions? 2.2 What are the objects, what are their properties, what are their relationships with the characters? 2.3 What are the environments, what are the background, what are the weather, what are the time of the day? 2.4 Are there any text on the screen? 2.5 If there is anything salient or anything weird, please describe it. The quality of this step is very very very important. want you to be very detailed and faithful to the video. For instance, if the visual memory contains character named \"person a\", you should use At least, you should go over the following aspects: What are they? # 3. Format Your response should be in the following format: [1. [2. [3. Brief Description]: Appeared Characters]: Detailed Description]: ... # captions, string ... ... # the format of [NAME1, NAME2, ...], list of character or object names # the detailed description of the scene, string # 4. Your Job Now your job begins. Table D. Dense Captioning prompts for the Captioning stage (Sec. B.1) 16 # 1. Instructions You will be given two sets of frames captured from video, describing several characters or objects from the video. Your objective is to find if any character or object appears in both sets. or object and find the better frame representing the characters and objects. Input: 1. Set 1: 2. Set 2: the names, representative video frames, and the identifiable properties of the characters and objects. the names, representative video frames, and the identifiable properties of the characters and objects. If so, please help me locate the character The following guidelines are very important for you to finish this task: # 2. Guidelines and Tips This is not an easy task, please make sure to use your advanced reasoning ability and check every item and step carefully. 1. Please work on this problem via two steps: set; (b) if so, find the better frame representing the character or object. 2. Please rely on both the video frame information and the identifiable properties to carefully understand the characters and objects. 3. When you are selecting the better frame for an object, please consider the following factors: should be the most salient frame showing the front face of the character; (b) the frame should be the most representative frame showing the character or object. 4. Sometimes the characters or objects are captured from different angles or distances, please make your best judgement to check if they are the same character or object. (a) the frame (a) check if any items from the first set is repeated with the second (Character name1 in Set 1, Character name1 in Set 2, Better character name1), # 3. Output Format Please strictly follow the format below to ensure smooth post-processing: [Repeated Characters and Objects]: (Character name2 in Set 1, Character name2 in Set 2, Better character name2) ... The answer lists all the repeated characters and objects in the two sets of frames, each tuple contains three items describing the repeated character or object: 1. Character name in Set 1: 2. Character name in Set 2: 3. Better character name: object, must be consistent with the name in Character name in Set 1 or Character name in Set 2. An example output should be: [Repeated Characters and Objects]: the name of the character or object in the first set of frames. the name of the character or object in the second set of frames. the name of the better character or object that represents the repeated character or (person a, person b, person a), (dog a, dog b, dog b) # 4. Your Job Now you will receive two sets of frames and their character descriptions. information provided."
        },
        {
            "title": "Please start your responses with the",
            "content": "Table E. Character Merging prompts for the Captioning stage (Sec. B.1) # 1. Instructions You will be given description of video clip, which potentially contains some characters. have decided to change the name of the characters or objects, and your job is to help me modify the descriptions to the new names. Input: 1. Characters, and Detailed Description. 2. Output: Your output should be the modified description of the video clip strictly following the original format and contents, only with names changed. the old description of the video clip, containing the fields of Brief Description, Appeared list of characters to be modified in the format of OLD NAME -> NEW Name. After some analysis, Old Description: Modified List: # 2. Guidelines 1. 2. 3. 4. Only change the names, do not change the format or any contents. Please remember to update all the Brief Description, Appeared Characters, and Detailed Description. Keep the names consistent. The format of the characters in Brief and Detailed Description is <NAME>, please follow the same format. # 3. Your Job Now your job begins. Table F. Caption Modification prompts for the Captioning stage (Sec. B.1) # 1. Motivation You will conduct the first step of long video understanding: relevance to the users question**. context length for long videos. You will have access to the following information for the current video segment: 1. 2. 3. different video actions. <NAME>. However, it is not entirely reliable (e.g., missing characters or inconsistent tracking across frames), please use it with reasoning. question. The frames sampled from the video, each corresponding to scene in the captions. The captions of the video generated by video captioning model, decomposed into short scenes representing Notably, we have marked the potentially key characters or objects using the format of By using short-segment analysis, you can avoid the limitation of the models **perceiving short video segments** and **analyze their # 2. Output Formats ... [(t start, end), ...]... Reasoning]: Relevant Segments]: (Your reasoning process. Please be precise, concise, and clear. Please strictly following the output format below, which is important for post-processing. [1. [2. to the question. is necessary for the question, merge them into single segment. relevant.) [3. [4. key characters that are mentioned in the question and how to identify them. not related to any characters.) Please strictly follow the time information from the captions. Confidence Level]: Key Characters]: (Your confidence level.) ... (List the time range of the video segments that are relevant if you think continuous period Return an empty list if none of the segments are Mentioning evidence is any.) [(character symnonym in question, identifiable properties or NAME in captions), ...]... (The"
        },
        {
            "title": "Keep the list empty if the question is",
            "content": "# 3. Instructions and Guidelines ## Information Reliability To principle is to **combine the information from the captions, video frames, and the question (including the options, if any)** to analyze the users intention. 1. The question: 2. Video frames: 3. Captions: You should combine the information from the question and video frames when using the captions. raised by the user, the most important and reliable. reliable, but only covers small portion of the video. less reliable, but covering more details, especially the \"NAME\" representing character/object names. The reliability of the information is: Some examples are: Finding the key video segment is critical. ## Analysis Tips 1. Think carefully about how short video segment could contribute to long video understanding by paying attention to the question and video segment contents. - For question on visual details, you should check if the video segment **contains the scene that the user wants**. - For question on information over period of time, such as the order or the number of actions, you should reason **whether this segment can contribute part of the analysis**. - For question on the reason or implication of the story/actions in the video, you should check if the video segment **contains the key information** that can help you understand the story/actions. 2. object, try to use it **precisely** and **rigorously** in your analysis. 3. captions, or clearly specify its appearance properties. 4. 5. 6. guessing, no clear evidence of being relevant to the question) to 5 (almost certain, clear evidence of being relevant to the question). 7. before and after the most possible scene to increase robustness. 20), and its previous and next scenes are (5, 10) and (20, 25), then you should make it (5, 25) so that the contents between two scenes wont be missed. Pay attention to the information reliability mentioned above. Imagine yourself watching video using the sampled frames and the captions. When discussing your analysis, please provide the reasoning process and your confidence level between 1 (almost If the question asks for certain characters in the plot/story, you should potentially localize its NAME in the If the question should be answered with contexts, for \"Relevant Segments\", you should include one more scene If the user mentions clear criteria, such as specific character of For example, if the most possible segment is (10, ## Your Input 1. 2. 3. is the format of \"(t start, end): The question: The frames: The captions: question coming with options. list of frames sampled from the video. list of captions decomposed into short scenes representing different video actions. Each caption caption\". Time is represented in seconds. # 4. Your Job Starts Table G. Segment Intention Analysis prompts for the Question Intention Analysis stage (Sec. B.2) 18 question. Your analysis of short video segments: # 1. Motivation You will conduct **user intention analysis** as step of long video understanding: about. The questions from the users might be vague or not self-contained. finding the relevant video segments, characters/objects, or how the short video segments contribute to the long video understanding. You will have access to the following information: 1. 2. Your analysis is the most important information in this step. containing the following parts: 1. 2. Time is represented in seconds.) 3. 4. characters that are mentioned in the question and how to identify them. [(character symnonym in question, identifiable properties or NAME in captions), ...]... (The periods that are potentially relevant from your analysis. **is the video segment relevant to the question?** You will go through the analysis of each segment Reasoning: Relevant Segments: Confidence Level: Key Characters: You will complete the question by what is the question asking [(t start, end), ...]... (Your confidence level.) Could be unreliable.) (your explanation) ... ... (The key Segment-level analysis might guess some relevant segments or characters for the question. # 2. Instructions and Guidelines ## Objectives Your goal is to merge the information from separate short video segments into complete understanding at the video level. Your most critical output for the downstream parts are the \"relevant segments\" and \"key characters\". you will carefully use your reasoning skills to handle the following issues: 1. most relevant segments and characters based on video-level understanding, and ignore the less relevant ones. Segment-level analysis might contain contradicting information since they come from separate analyses. 2. carefully merge the information from different segments, and provide reliable information for the downstream analyses steps. 3. example, do we want to \"sum\", \"merge\", or \"select\" the information from individual segments. You should clarify how the results from segment-level can contribute to the long video understanding. For"
        },
        {
            "title": "You should",
            "content": "Notably, ... Key Characters]: (Your reasoning process. [(t start, end), ...]... Please be precise, concise, and clear. Reasoning]: Relevant Segments]: ## Output Formats [1. [2. the question. Please strictly follow the time information from the analysis provided to you. think continuous period is necessary for the question.) [3. key characters that are mentioned in the question and how to identify them. not related to any characters.) [4. \"yes\", then this is global question. It is very important to follow the format for the relevant segments section. (t start, end), especially the brackets should be \"()\" and matched. If \"no\", then this is local question.) Local or Global]: ... [(character symnonym in question, identifiable properties or NAME in captions), ...]... (The"
        },
        {
            "title": "Keep the list empty if the question is",
            "content": "Mentioning evidence is any.) (List the time range of the video segments that are relevant to"
        },
        {
            "title": "Merge the scenes if you",
            "content": "(Whether the question requires combining contexts from different segments to answer. If"
        },
        {
            "title": "Every segment should be a format of",
            "content": "Some examples are:"
        },
        {
            "title": "Think carefully about how a short video segment could contribute to long video understanding by paying attention",
            "content": "## Principles and Tips 1. to the question and video segment contents. - For question on visual details, you should check if the video segment **contains the scene that the user wants**. - For question on information over period of time, such as the order or the number of actions, you should reason **whether this segment can contribute part of the analysis**. - For question on the reason or implication of the story/actions in the video, you should check if the video segment **contains the key information** that can help you understand the story/actions. 2. object, try to use it **precisely** and **rigorously** in your analysis. 3. captions, or clearly specify its appearance properties. 4. 5. 6. likely segment and say \"I have low confidence on the relevance of the segments\". Imagine yourself watching video using the sampled analysis. If the question is not really about the **whole video**, do not specify more than 10 relevant segments. You should propose **at least 1 relevant segment**. If the question asks for certain characters in the plot/story, you should potentially localize its <NAME> in the If the user mentions clear criteria, such as specific character of If you dont think any segment is relevant, return most Figuring out the flow of the plots is critical. Finding the key video segment is critical. # 3. Your Job Starts Table H. Global Intention Analysis prompts for the Question Intention Analysis stage (Sec. B.2) 19 # 1. Motivation In this step of long video understanding, you are making preparations for calling vision-language models to analyze Specifically, you will be given the users question and video-level analysis from yourself. sampled video frames. Based on such information, you will **propose question to prompt the vision-language models** to analyze the video frames. You will access the following information: 1. 2. 1. 2. Time is represented in seconds.) 3. characters that are mentioned in the question and how to identify them. related to any characters.) 4. \"yes\", then this is global question. question. video-level analysis from yourself. Reasoning: Relevant Segments: [(character symnonym in question, identifiable properties or NAME in captions), ...]... (Whether the question requires combining contexts from different segments to answer. (Your explanation about which parts of the video are relevant to the question.) (The periods that are potentially relevant from your analysis. Keep the list empty if the question is not If \"no\", then this is local question.) It contains the following information: [(t start, end), ...]... Local or Global: Key Characters: ... ... If (The key In this way, the vision-language models ... Reasoning]: local question: global question: (Your reasoning process. In practice, we will use two ways: In this way, the vision-language models can use Global: Sample 1 video frame for each segment, sequentially. Local: Sample video frames for each relevant segment, e.g., 32 frames. what kind of reasoning should the vision-language models conduct on longer time span? what kind of detailed information or evidence should the vision-language models find in each # 2. Instructions and Guidelines ## Objectives When thinking about the questions to ask, please pay attention to how the next step will sample the video frames for your questions. 1. can use your question to check the details of each segment. 2. your question to check the flow of the plots or conduct reasoning over longer period of time. Therefore, you should propose two questions: 1. segment? 2. ## Output Formats Please strictly follow the output formats below to propose your questions, so that the downstream parts can easily extract the information: [1. what kind of information is missing or important for the question.) Local Question]: (Your question for the local analysis.) [2. [3. Global Question]: ## Principles and Tips 1. to the question and video segment contents. - For question on visual details, you should check if the video segment **contains the scene that the user wants**. - For question on information over period of time, such as the order or the number of actions, you should reason **whether this segment can contribute part of the analysis**. - For question on the reason or implication of the story/actions in the video, you should check if the video segment **contains the key information** that can help you understand the story/actions. 2. time information. 3. best way to distinguish the correct one. vision-language models. 4. 5. reasoning into the question. ..., (D) ..., (E) ...\", \"What is ... Use your knowledge of prompting large language models or vision-language models to improve your question. Your output questions should only contain question and options."
        },
        {
            "title": "Think carefully about how a short video segment could contribute to long video understanding by paying attention",
            "content": "Remember to use the options from the original questions, expressed with (A), (B), (C), (D), to think about the (A) ..., (B) ..., (C) ..., (D) ..., (E) ...\" or similar formats. For example, the question should directly start as \"Describe ... Keep your question concise, clear, and within few sentences."
        },
        {
            "title": "Do not enumerate or explicitly depending on any",
            "content": "Do not include any analyses, speculations, or (Your question for the global analysis.) Please be precise, concise, and clear. It is also important to include the original options as the context for the"
        },
        {
            "title": "Explicitly thinking about",
            "content": "(A) ..., (B) ..., (C) Some examples are: ... ... # 3. Your Job Starts Table I. Customized Queries for Perception prompts for the Goal-aware Analysis stage (Sec. B.3). 20 You will have the users question and series of your analysis # 1. Motivation You are at the last step of long video understanding. to finally answer the users question. Before conducting actual analysis, it is important to understand the steps of the previoous analysis that will be presented to you: 1. Video-level User Intention Analysis: are relevant to the users question. understanding. 2. Goal Proposal: the VLMs to use. second question is called \"global question\", used for joint analysis and reasoning across multiple segments. 3. Goal-aware Analysis: segment using the local question and across multiple segments using the global question. By understanding the previous steps, you will have good understanding of the meaning of the information provided to you, especially which parts are reliable and informative for answering the users question. To call vision-language models to analyze the video segments, you have proposed two questions for The first question is called \"local question\", used for detailed analysis for each segment, and the You first analyze which parts of the video and what kind of characters You also think about how each video segment could contribute to the long video You will receive the results of the vision-language models perception for each video Some examples are: With the information provided to you, imagine youself as human watching the video."
        },
        {
            "title": "Carefully consider whether the analysis at local segments or across multiple segments is more important for",
            "content": "# 2. Instructions and Guidelines 1. Think carefully about how short video segment could contribute to long video understanding by paying attention to the question and video segment contents. - For question on visual details, you should check if the video segment **contains the scene that the user wants**. - For question on information over period of time, such as the order or the number of actions, you should reason **whether this segment can contribute part of the analysis**. - For question on the reason or implication of the story/actions in the video, you should check if the video segment **contains the key information** that can help you understand the story/actions. 2. answering the users question. 3. plots is critical. 4. skills to resolve the contradictions. - If the user has mentioned specific criteria, try to use it **precisely** and **rigorously** in your analysis. - Try to utilize the confidence levels provided in the answers. - Always thinking about your strategy: contribute to the long video understanding. some numbers, or picking the best segment to answer the question? - Humans have limited memory. Always prioritize the most salient information. 5. segments and analyses. It is possible that some information is vague or contradicting each other. For example, do you combine the pieces of information together, summing how the analysis at local segments or across multiple segments could"
        },
        {
            "title": "They might provide additional correspondence information across different",
            "content": "Pay attention to the time information."
        },
        {
            "title": "You should utilize advanced reasoning",
            "content": "Some very useful principles are:"
        },
        {
            "title": "Figuring out the flow of the",
            "content": "Reasoning]: Answer]: captal letter from to (If you cannot find correct answer, please make guess from to based (Your advanced reasoning based on the information above.) ... To ensure correct post-processing, please strictly use this format."
        },
        {
            "title": "Do not add any",
            "content": "# 3. Output Format Please provide your answer in the following format: [1. [2. on the information you have. characters or spaces.) # 4. Your Job Starts -------------------------------- Table J. Answer Generation prompts for the Goal-aware Analysis stage (Sec. B.3). You are helpful assistant with the ability of watching videos and answering the questions raised by human users. You will process few continuous screenshots of the video, and answer the questions raised by human users. encounter any issues that you cannot answer the question, please pick the most possible answer from the options. When you answer, please follow the format of: Answer]: (The answer you choose, from A, B, C, D) Important: If you cannot answer the question, please pick the most possible answer from A, B, C, D, E. Do not leave it blank or select other options. (Why you choose this answer) [2. [1. Reasoning]: If you ... ... Table K. The prompts used for evaluating Gemini-2.0-Flash (Sec. B.5.1)."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}