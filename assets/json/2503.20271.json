{
    "paper_title": "ViLBench: A Suite for Vision-Language Process Reward Modeling",
    "authors": [
        "Haoqin Tu",
        "Weitao Feng",
        "Hardy Chen",
        "Hui Liu",
        "Xianfeng Tang",
        "Cihang Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data."
        },
        {
            "title": "Start",
            "content": "VILBENCH: Suite for Vision-Language Process Reward Modeling Haoqin Tu1 Weitao Feng1* Hardy Chen2 Hui Liu3 Xianfeng Tang3 Cihang Xie1 1UC Santa Cruz 2UT Dallas 3Amazon Research 5 2 0 M 6 2 ] . [ 1 1 7 2 0 2 . 3 0 5 2 : r Figure 1. We present suite of vision-language process reward modeling. We first benchmark current vision-language models as different reward models, and present VILBENCH that requires intensive step-wise reward. Then we collect 73K+ preference reward data to train vision-language process reward model ViLPRM that performs better than other baselines on VILBENCH."
        },
        {
            "title": "Abstract",
            "content": "Process-supervised reward models serve as fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple visionlanguage benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce VILBENCH, vision-language benchmark designed to require intensive process reward signals. Notably, OpenAIs GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% *Work done during Weitaos remote internship at UCSC. accuracy, indicating the benchmarks challenge for current VLLMs. Lastly, we preliminarily showcase promising pathway towards bridging the gap between general VLLMs and reward models by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on VILBENCH by selecting OpenAI o1s generations. We release the implementations at https://ucscvlaa.github.io/ViLBench with our code, model, and data. 1. Introduction Reward models (RMs) play crucial role in aligning model outputs with human preferences, benefiting Large Language Models (LLMs) in both training and inference stages [2, 3537]. The most popular RMs include output reward models (ORMs) and process-supervised reward models (PRMs). While ORMs assess responses at the final output level [41, 58], PRMs provide detailed, step-wise feedback, making them particularly useful for complex reasoning tasks [23, 45, 57]. Despite their advantages in the language domain, the application of PRMs in multimodal contexts remains underexplored, with most vision-language RMs following the ORM paradigm [19, 32, 49, 52]. To advance the study of vision-language process reward modeling, this paper presents comprehensive suite of contributions encompassing (1) benchmarking study of stateof-the-art VLLMs as reward models, (2) newly curated dataset designed for fine-grained step-wise reward evaluation, and (3) an advanced vision-language PRM trained on large-scale vision-language step reward data. Our goal is to provide deeper understanding of the effectiveness of current vision-language reward models and to pave the way for future improvements in multimodal step-wise evaluation techniques. As our first contribution, we evaluate seven VLLMs (six open-weight and one private) following MLLM-as-ajudge [4, 11] across five challenging vision-language tasks. This benchmarking effort systematically analyzes the models rewarding capabilities in various domains, revealing several key insights. For example, we observe that neither ORM nor PRM consistently outperforms the other across all tasks, indicating that different reasoning structures benefit from different rewarding approaches [57]. Additionally, we find that better VLLMs do not always translate to superior reward capabilities, suggesting that rewarding and generation abilities are not inherently correlated. Our results also highlight that in specific domains such as textdominant tasks, PRMs is able to provide greater advantage, suggesting their strong potential in tasks requiring intricate, step-wise reasoning. Next, we introduce VILBENCH, vision-language benchmark that demands step-wise reward feedback. Current vision-language benchmarks primarily focus on evaluating final outputs, which limits their ability to distinguish between improvements driven by ORMs and PRMs. To address this limitation, we curate dataset of 600 examples that emphasize the necessity of step-wise feedback. Our filtering protocol assembles judges from six open-weight VLLMs to select examples that require fine-grained rewards beyond simple correctness assessments. Notably, advanced models like GPT-4o achieve only 27.3% accuracy on VILBENCH, benefiting 3.0% more from PRM-driven step-wise rewards than from ORMs, underscoring our benchmarks difficulty and its emphasis on fine-grained reward assessment. Lastly, as preliminary but promising step towards bridging the gap between general VLLMs and visionlanguage PRMs, we employ an enhanced multimodal Monte Carlo Tree Search (MCTS) algorithm [53] to genModel Name InternLM-X2.5 [54] LLaVA-OneVisoin [21] Qwen2-VL [46] InternVL-2.5 [7] Qwen2.5-VL [1] GPT-4o [33] LLM InternLM2 Qwen2 Qwen2 Qwen2.5 Qwen2.5 Unknown Model Size 7B 7B 7B 8B 3B, 7B Unknown Date 07/2024 08/2024 08/2024 12/2024 02/2025 05/2024 Table 1. VLLMs used as different RMs for VILBENCH. erate ViLReward-73K, dataset of 73.6K stepwise visionlanguage reward samples drawn from five training datasets. With this dataset, we train 3B vision-language PRM that significantly improves the evaluation accuracy of step-wise rewards. Specifically, this model substantially surpasses existing PRMs, achieving an average improvement of 3.3% over standard CoT approaches and up to 2.5% compared to its untrained counterpart on VILBENCH. We also discuss potential challenges and future directions for the development of vision-language PRMs to conclude this paper. 2. Part I: Benchmarking VLLMs as Reward"
        },
        {
            "title": "Models",
            "content": "VLLMs are demonstrating increasing strength across variety of tasks. One effective way to further enhance their performance is by evaluating their test-time scaling ability. To assess the step-wise critique capabilities of VLLMs, we benchmark seven different models (see Table 1 for model details) following the paradigm of LLM-as-a-judge [4, 58] on five widely used vision-language tasks: MMStar [6], MathVista [30], MathVerse [55], MMMU Pro [51], and RealWorldQA [12]. To further explore their inference-time scaling potential, we adopt the Best-of-N (BoN) setting, where VLLMs select the best response from pool of candidate responses [23, 48]. In detail, we adopt GPT-4o (timestamp) as the base solution sampler to sample 24 solutions given one question. Then we incorporate different VLLMs as the deterministic scorer to pick the best response among the candidates by assigning scores between 1 to 5 to each reasoning step. More details about prompt and model generation settings can be found in Appendix A. Through this approach, we uncover four key insights: Findings 1: Neither ORM nor PRM excels across all vision-language tasks. Among the five VL benchmarks in Figure 2, VLLMs as ORMs slightly outperform fine-grained PRMs in four cases, with an average margin of 0.3%. However, on RealWorldQA, challenging VL task involving daily life images, knowledge, and reasoning, the PRM surpasses ORM Interestingly, the four datasets by an average of 1.7%. where ORM performs better (MathVista, MathVerse, MMStar, and MMMU Pro) primarily feature formal reasonFigure 2. Benchmark results of 7 different VLLMs as different reward models on 5 vision-language benchmarks. The base solution generator is GPT-4o. We report the average PRM and ORM scores in subtitles. Figure 3. Correlations between the model performance and its reward performance on MMStar and MathVista datasets. The rewarding performance is averaged over 4 different in the Best-of-N selection with GPT-4o as the base generator. ing and mathematical problems, whereas RealWorldQA focuses on real-world scenarios. This contrasts with prior findings in the language domain, where PRMs have been shown to offer better guidance than ORMs for languageonly math and reasoning tasks [23, 44, 45]. One possible explanation is that current VLLMs are predominantly optimized on visual understanding tasks, rather than step-wise rewarding tasks. Reward models consistently enhance performance across all five benchmarks compared to CoT greedy decoding. As the BoN candidate selection expands, RMs become increasingly effective in boosting performance. However, the impact varies across benchmarks. For example, in RealWorldQA, only four RMs at = 24 improve the base model beyond CoT. In contrast, for the remaining benchmarks, most RMs outperform CoT when > 22. Notably, on MathVerse, nearly all VLLMs enable GPT-4o to surpass its CoT decoding at 2, except for LLaVA-OneVision. This observation suggests that vision-language reward models may be less effective for complex visual perception tasks than for formal reasoning challenges. Findings 2: Better vision-language models do not necessarily lead to better reward models. Previous research has demonstrated that stronger VLLMs tend to produce better ORMs [22]. However, this correlation does not necessarily hold for PRMs. To examine this, we plot the correlation between models greedy performance and its rewarding ability on MMStar and MathVista in Figure 3. In both tasks, ORMs exhibit higher Pearson Figure 4. Model performance with the last of step rewards selected under the Best-of-N paradigm. Method Greedy ORM PRM Text-dominant Visual-dominant 58.9 62.2 +3.3 62.0 +3.1 51.0 49.0 -2.0 48.9 -2.1 Table 2. Average performance gain across 7 RMs from text or visual dominant examples on MathVerse using ORM or PRM. correlation scores between general VLLM capability and rewarding ability, reinforcing the relationship between these two attributes. In contrast, under the PRM setting, the correlation is notably weak, averaging just 0.06%. This suggests that superior VLLM performance does not directly translate to stronger rewarding capabilities, particularly in process supervision. Notably, LLaVA-OneVision and Qwen2.5-VL rank highest as PRMs on these tasks. surprising observation is that GPT-4o, the strongest VLLM among the tested models, underperforms as both an ORM and PRM in the deterministic scoring setting. This may be attributed to GPT4os tendency to overrate responses, introducing bias in certain reward tasks [14, 40]. Findings 1 and 2 highlight the need for the development of more robust and generalizable PRMs in the visionlanguage domain. Findings 3: The best practice for vision-language reward model is usually between PRM and ORM. Beyond PRM and ORM, alternative RMs exist that balance between selecting only the final step (ORM) and considering all step rewards (PRM) by incorporating the last step scores. We conduct experiments using the average of the last step rewards (i.e., [1, 2, 22, 23, all]) as the final reward signal on MMStar and MathVista. As shown in Figure 4, the most effective approach consistently falls between ORM and PRM, with the optimal performance achieved by averaging the last 2 or 4 step rewards. Specifically, when using the last 2 step rewards as the final signal, the selected answer achieves the highest average accuracy, improving by 0.41% and 0.57% over the ORM setting on the two benchmarks. This finding suggests an improved strategy for seDataset Souce Size MMStar 150 MathVista 150 MathVerse 100 100 MMMU Pro RealWorldQA 100 600 Sum Split val testmini testmini* test test test Ori. Size 1,500 1,000 1,000 1,592 756 5,848 E I Table 3. An overview of VILBENCH. * means that we only sample 1000 entries from testmini split of MathVerse. Ori. Size represents the original size of the dataset. lecting vision-language reward signals, striking balance between ORM and PRM for enhanced performance. Findings 4: VLLMs as reward models provide more benefits on text-dominant examples. On MathVerse, certain examples require stronger focus on textual reasoning (text-dominant), while others rely more on visual understanding (visual-dominant). We report the average performance of RMs on these two subsets in Table 2. The results indicate that vision-language RMs provide greater benefits to VLLMs on text-dominant examples but may negatively affect performance on visual-dominant ones. Since reward signals are integrated into the language generation process at the textual level, this finding suggests that current VLLMs exhibit stronger textual-level critique capabilities. This again, highlights the need for the development of specialized vision-language reward models to better handle visually intensive tasks. 2.1. VILBENCH: Vision-Language Benchmark Requiring Intensive Reward Feedback As what we found previously, existing vision-language benchmarks do not require intensive feedback from RMs like vision-language PRMs. To address this, we leverage six open-weight VLLMs to filter samples where they perform well as PRMs but worse as ORMs under the BoN setting. To be concrete, we evaluate the average performance of ORMs and PRMs using 16 response candidates, providDataset Source MAVIS-Geo [56] GeoQA170K [5] CLEVR-Math [24] A-OKVQA [38] ScienceQA [29] Sum Class Math Math Math General General Math&General Size 3,093 8,063 957 2,044 2,769 16,926 PR Size 25,829 31,406 1,425 9,241 5,659 73, Table 4. Statistics of ViLReward-73K, vision-language process reward preference dataset. We show the initial size of the data source (Size) as well as the size of the process reward instance (PR Size). ing the RMs with broader selection. We introduce PRMpreference indicator based on the average PRM and ORM scores, denoted as Sprm and Sorm, respectively. This indicator is calculated as = Sprm Sorm, allowing us to rank all samples across the five tested benchmarks according to their scores. In Table 3, we illustrate how we sample varying amounts of data from each task to construct our final dataset, VILBENCH. For the evaluation metric, since each question is paired with ground truth answer, we use accuracy between predicted answers and the ground truth as the final metric, details about the answer extraction and evaluation are in Appendix C. 3. Part II: ViLPRM: Vision-Language Process Reward Model 3.1. Vision-Language Preference Data Preparation Data Selection and Filtering. Process preference data have been proven to be effective in training RMs in specific domains like math and logical reasoning. However, in scenarios that demand challenging visual perception understandings, the potential of PRMs remains underexplored. In order to generalize the vision-language PRM to subjects other than just math, we consider collecting challenging VL data from general visual perception and math datasets. We follow three rules to filter data for the process reward model training: 1. Unique image content for diverse visual features. 2. Challenging questions that elicit model reasoning for better process scoring. 3. Diverse source of the data for generalizing RM abilities in various domains. In detail, we draw samples from 5 vision-language datasets consisting of 3 vision-language math data and 2 challenging visual perception tasks. For math domain: MAVIS-Geometry [56] is dataset consisting of visual geometry questions that use GPT-4 to rewrite or generate geometry visual problems and solutions. There are four different difficulty levels, and we select the hardest two levels to sample 5,000 data as our metadata. Figure 5. An example of partial MCTS tree we constructed for ViLReward-73K. The metadata is from A-OKVQA. We mark value scores from the preference data at each node. GeoQA170K [10] contains over 170K geometric image-caption and question-answer pairs, building on GeoQA+ [3] and GeoQA3K [28]. We sample one question from each unique images from the data, resulting in 8,063 examples in total. CLEVR-Math [24] is synthesized VQA dataset based on CLEVR [16] that includes math word problem solving. We only consider 957 questions with distinct images and need multi-hop reasoning in the dataset. And for the visual perception domain: A-OKVQA [38] contains question-answering problems about natural images, we select the questions that cannot be answered directly (1,544 examples) and sample 500 data that is straightforward to answer. ScienceQA [29] is comprehensive dataset with 21K examples in science, the data is categorized into 12 grades based on the difficulty level. We use data that is harder than grade 7 as our metadata. We present the detailed data volumes in Table 4. MCTS Data Searching Engine. Instead of assigning coarse-grained binary scores (e.g., good or bad) to each process [23, 45], we adopt the reward calculation method of ReST-MCTS* [53] to assign fine-grained value between 0 to 1 to each reasoning step as the reward value. Unlike the previous text-based ReST-MCTS*, we have enabled visual input for the policy model, allowing the model to answer questions based on visual input. Following the standard MCTS tree construction, there Model ORM PRM 3B 30.8 31.1 (+0.3) 8B 27.8 28.7 (+0.9) GPT-4o 28.1 31.1 (+3.0) o1 34.9 34.9 (+0.0) Table 5. The average accuracy of 3 open-weight VLLMs as reward models on the proposed benchmark. PRMs have more advantage in enhancing model performance than ORMs on our VILBENCH. classify good or bad steps in vision-language reasoning trajectories and avoids using other pre-training data for modality alignment. We formalize the model input and output as: given the input question and the model reasoning response y, the score head transforms the logits feature of the last token into scalar r(x, y). This scalar value r(x, y) serves as the predicted reward score for the inputs. are four major phases while tree node expansion during search: root node selection, node expansion, route simulation and value backpropagation. For the final answer evaluation, we use GPT-4o as the judge to evaluate the final output of tree search. In this part, we mainly introduce the calculation for node value and leave other details and examples in Appendix B. We define the quality value vk [0, 1] of partial solution pk = [s1, s2, . . . , sk] to evaluate its progress toward correct answer. vk reflects the correctness and contribution of each step si, higher vk indicates greater likelihood of being correct. The reasoning distance mk is the minimum steps needed to reach the correct answer from pk, estimated via simulations as it cannot be directly computed. By introducing weighted reward wsk for step sk, incorporating mk and the process reward rsk: (cid:18) wsk = 1 (cid:19) vk1 mk + 1 (1 2rsk), = 1, 2, . . . (1) The quality value updates iteratively: (cid:40) vk = 0, max(vk1 + wsk, 0), otherwise. = 0, (2) Here, mk = k, where is the total steps in solution s. Since reasoning for visual problems is often simpler, we modified the prompt to require the models output steps to be more granular, ensuring that the model does not directly output the answer at the very beginning. We present one example of the reward value tree in Figure 5. 3.2. Process Reward Model Training Based on the derived ViLReward-73K preference data, we train 3B vision-language PRM, named ViLPRM. Model Architecture. ViLPRM is built upon 3B VLLM QwenVL-2.5 [1]. We follow the common practice of PRM to use the pre-trained weights of QwenVL-2.5 for most of the parts, such as the visual encoder and the MLP projector, but append linear layer to output scalar score after the language head [9, 43]. We do not consider generative score modeling due to efficiency concerns. Since the base model QwenVL-2.5 has been aligned with massive visuallanguage data, our reward model only requires learning to Training. Unlike previous works that only assign binary scores to the RM input, we have detailed scores for each input that can classify step responses better. We use the vanilla Mean Square Error (MSE) loss between the ground truth reward and the predicted one to update the ViLPRM. We add more details about training in Appendix D. 4. Validating ViLPRM on VILBENCH To validate the effectiveness of ViLPRM, we conduct experiments under various settings on VILBENCH. 4.1. Experimental Setups We choose the test-time scaling strategy to verify the functionality of different RMs. On our filtered VILBENCH, we first use 4 different VLLMs with various model size as the solution sampler, i.e., Qwen2.5-VL-3B [1], InternVL-2.58B [7], GPT-4o [33] and o1 [34]. For all models except o1, we sample 16 candidate responses for BoN selection. While we sample 4 solutions for o1 due to the high cost of this sampling process. For PRMs, we include four VLLMs Qwen2.5-VL3B [1], Qwen2.5-VL-7B [1], LLaVA-OneVision-7B [21] and recent vision-language PRM URSA-RM (URSA for short) [31] as baselines, where URSA is concurrent vision-language PRM developed using base model of 8B parameters and trained on over 1000K carefully designed preference reward data. It has shown great improvements in the multimodal math problems, but lacks the capacity to generalize to more general vision-language tasks. 4.2. Results and Analysis VILBENCH requires more intensive reward feedback than other VL benchmarks. To confirm that our filtered VILBENCH requires more fine-grained step rewards beyond simple output rewards, we present the average accuracy across three different VLLMs (QwenVL2.5 3B, LLaVA-OneVision 7B, and QwenVL2.5 7B) used as ORMs or PRMs with four solution samplers in Table 5. In most cases, PRMs enhance model performance more effectively than ORMs, yielding an average improvement of 1.4%. However, one notable exception occurs when selecting o1s responses, where no significant difference is observed between ORM and PRM. This may be due to o1s final output steps lacking sufficient detail and accuracy, with its internal Figure 6. Accuracy results of different VLLMs using various RMs under the best-of-n strategy on VILBENCH. Model QwenVL 2.5 3B [1] LLaVA OV 7B [21] QwenVL 2.5 7B [1] URSA [31] ViLPRM (Ours) 2 30.7 30.7 29.0 31.0 31.5 4 31.5 31.2 30.8 30.8 32.0 8 29.7 29.7 29.0 30.0 31.0 16 28.8 29.0 26.7 30.3 31.3 Avg. 30.2 30.2 28.9 30.6 31.5 Table 6. The average accuracy over four solution generators using different PRMs under the different BoN setups. reasoning process hidden from users, making it less effective for prompt engineering [34]. ViLPRM performs better than other VL PRMs. Figure 6 presents the performance of RMs across four Bestof-N (BoN) settings with different solution samplers. As the number of response candidates increases, RMs generally enhance model performance. Specifically trained vision-language PRMs consistently improve results over the CoT strategy for 23. However, VLLM-based RMs may negatively impact performance when becomes too large. For instance, among three sets of responses, three VLLMs acting as RMs exhibit varying degrees of degeneration in candidate selection from 23 to 24. In contrast, the two vision-language PRMs demonstrate greater consistency in identifying superior responses. This finding reinforces our previous claim that current VLLMs are not yet robust enough to serve effectively as reward models. Table 6 reports the average performance of different RMs. Compared to the larger PRM URSA, which is trained on over ten times more data, our ViLPRM achieves superior average performance by 0.9% and outperforms its untrained 3B VLLM counterpart by 1.3%. Additionally, RMs consistently enhance the performance of o1, model known for leveraging an internal thinking process as an efficient test-time scaling technique. This further underscores the importance of developing reliable reward models, even for models with built-in reasoning capabilities. Examples for the Reward Task. In Figure 7, we present an example of using our ViLPRM and URSA for selecting the best response from OpenAI o1s responses. In the example, URSA prefers more steps in the reasoning and even Figure 7. An example of process scores provided by URSA [31] and our ViLPRM. We mark different scores with different colors. with wrong trajectories, while our ViLPRM can choose accurate solutions. This is likely because URSA was trained on massive amount of math reasoning data and may develop the preference for complex rather than accurate reasoning steps [25]. We also present another example about medical reasoning in the Appendix E, which verifies that the proposed vision-language PRM has the capacity to also perform well beyond just math or reasoning tasks. 5. Related Works 6. Discussions and Conclusion Reward Benchmark. There are plenty of works put their emphasis in the text-only reward benchmarks [18, 27, 59], and some of them are specifically designed for PRMs [39, 57]. When shifting to vision-language domain, traditional VLLM evaluation mainly focuses on the general abilities of the model, including multiple aspects like knowledge, reasoning, fairness, and safety [6, 20, 30, 42, 51, 55]. VL-RewardBench [22] is the first work that sources reinforcement learning preference data and rewrites knowledgeintensive vision-language samples to form diverse benchmark. To generalize beyond just general VQA tasks, multimodal RewardBench [50] provides comprehensive and high-quality benchmark for VLLM reward models, covering six key areasgeneral correctness, general preference, knowledge, reasoning, safety, and VQAannotated by domain experts. Our proposed VILBENCH fills the gap of benchmarking PRMs in vision-language domain. Reward Modeling. Reward models are important for guiding AI models at both training and inference stages. There are typically three different forms of RMs: (1) discriminative RM treats the rewarding task as token classification. It usually leverages linear head to fit the reward score via regression loss [35, 41]. (2) LLM-as-a-judge leverages the generative ability of language models to output feedback in the form of text, often critique or explanation of why certain output is good or bad [19, 49, 58]. (3) Implicit RMs that are models optimized using DPO that the predicted log probabilities are interpreted as implicit reward signal [15, 17, 60]. In our work, VLLM-as-a-judge belongs to the second category, while our proposed ViLPRM is discriminative RM. Recent studies have also turned their interests from pure language to vision-language and other modalities. The initial development of vision-language RMs always falls into the specific using scenarios or inferior performance [43, 49]. Most recent works explore both the directions of VLLM-as-a-judge [4, 11, 49] and deterministic rewarding [31, 52]. However, most of the current RMs are output reward models. Liu et al. [26] developed the first vision-language PRM in math domain, and leverage such technique for model reinforcement learning. URSA [31] as concurrent work collects over 1 million vision-language reward data for training 8B PRM that enhances math VLLMs greatly. concurrent work VisualPRM [47] collects 400K reward data for PRM training in vision language and shows great potential in both vision language and language-only reward tasks. Our ViLPRM breaks the tradition of employing solely on the math domain and generalizes to more general visual perception cases. Vision-Language PRM is Bounded by Clear Step Segmentation. How to best split the reasoning step for PRMs has always been problem [8, 13, 25]. In structured tasks like math problems, PRMs provide fine-grained feedback, improving step-by-step reasoning. However, when the segmentation of steps is unclear or reasoning is unnecessary, PRMs may harm the performance. For instance, text-heavy tasks saw 3% accuracy boost with PRMs, while visualdominant tasks suffered 2% drop, likely due to PRMs overemphasizing irrelevant steps. PRMs also struggle when all steps are treated equally. Previous works have proposed to use single step to represent all step rewards [25, 44]. We found that rewarding only the last few critical steps improved accuracy more than using all steps, striking balance between PRMs and ORMs. major challenge is identifying which steps truly matter. Future improvements should focus on adaptive step evaluation, where PRMs automatically adjust reward weight based on step importance. Better segmentation strategies, such as enforcing clearer step structures during training or integrating step selection mechanisms can help PRMs generalize better across tasks. Improved Training Paradigm is Required for Multimodal RMs. Current training approaches for multimodal reward models fail to generalize across diverse tasks. Many RMs, including PRMs, are task-sensitive [25, 57], meaning they work well on specific domains but struggle elsewhere. For example, PRMs trained on math tasks such as URSA perform poorly on vision-heavy reasoning, suggesting that current methods do not equip RMs with broad evaluation skills. Besides, our results show that even advanced VLLMs like GPT-4o do not automatically become good reward models, often overrating responses. To improve vision-language PRMs, training must diversify data sources, integrating both textual and visualInstead of relying solely on stepheavy reasoning tasks. wise learning, future RMs should also consider incorporating adaptive reward mechanisms, adjusting considered step scores based on task complexity. Additionally, evaluation benchmarks for reward models should also go beyond accuracy, assessing consistency, bias, and generalization [50]. Conclusion. We introduce VILBENCH, benchmark for vision-language process reward modeling (PRM), and evaluate seven VLLMs as reward models. Our findings show that PRMs enhance stepwise reasoning in structured tasks but struggle in visual-dominant scenarios, emphasizing the need for adaptive step evaluation. To address this, we develop ViLReward-73K, dataset of 73.6K step-wise rewards, enabling the ViLPRM to surpass other PRMs 3.3% in accuracy. However, task sensitivity remains challenge, highlighting the need for better reasoning step segmentation, adaptive rewards, and more diverse training data. Future work should focus on refining evaluation frameworks and improving generalization for more robust multimodal reward models."
        },
        {
            "title": "Acknowledge",
            "content": "We thank the Microsoft Accelerate Foundation Models Research Program for supporting our computing needs."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 6, 7 [2] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. 1 [3] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th international conference on computational linguistics, pages 15111520, 2022. 5, 13 [4] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal In Fortyllm-as-a-judge with vision-language benchmark. first International Conference on Machine Learning, 2024. 2, 8 [5] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 513523, 2021. 5 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 8 [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 6 [8] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [9] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. 6 [10] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. 5, 13 [11] Wentao Ge, Shunian Chen, Guiming Hardy Chen, Junying Chen, Zhihong Chen, Nuo Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, et al. Mllm-bench: evaluating multimodal llms with per-sample criteria. arXiv preprint arXiv:2311.13951, 2023. 2, 8 [12] Grok-1.5 Team. Grok-1.5 vision preview, 2024. 2 [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 8 [14] Eugenio Herrera-Berg, Tomas Vergara Browne, Pablo LeonVillagra, Marc-Lluıs Vives, and Cristian Buc Calderon. Large language models are biased to overestimate profoundness. arXiv preprint arXiv:2310.14422, 2023. [15] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702, 2023. 8 [16] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. 5, 13 [17] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model posttraining. arXiv preprint arXiv:2411.15124, 2024. 8 [18] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 8 [19] Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. Prometheus-vision: Vision-language model In Findings of the as judge for fine-grained evaluation. Association for Computational Linguistics ACL 2024, pages 1128611315, 2024. 2, 8 [20] Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, et al. Vhelm: holistic evaluation of vision language models. NeurIPS, 2024. 8 [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 6, 7 [22] Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vlrewardbench: challenging benchmark for vision-language generative reward models. arXiv preprint arXiv:2411.17451, 2024. 3, 8 [23] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 2, 3, [24] Adam Dahlgren Lindstrom and Savitha Sam Abraham. language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. 5, 13 Clevr-math: dataset for compositional [25] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025. 7, 8 [26] Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, and Junxian He. Diving into self-evolving training for multimodal reasoning. arXiv preprint arXiv:2412.17451, 2024. 8 [27] Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. arXiv preprint arXiv:2410.16184, 2024. 8 [28] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 5, [29] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning In The via thought chains for science question answering. 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 5, 13 [30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 2, 8, 14 [31] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, and Yujiu Yang. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. 6, 7, 8, 14 [32] Dilxat Muhtar, Enzhuo Zhang, Zhenshi Li, Feng Gu, Yanglangxing He, Pengfeng Xiao, and Xueliang Zhang. Quality-driven curation of remote sensing vision-language arXiv preprint data via learned scoring models. arXiv:2503.00743, 2025. 2 [33] OpenAI. Hello GPT-4o, 2024. 2, 6 [34] OpenAI. Introducing openai o1, 2025. 6, 7 [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 1, 8 [36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1 [38] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowlIn European conference on computer vision, pages edge. 146162. Springer, 2022. 5, 13 [39] Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, and Yu Cheng. Prmbench: fine-grained and challenging benchmark for process-level reward models. arXiv preprint arXiv:2501.03124, 2025. 8 [40] Ziang Song, Tianle Cai, Jason Lee, and Weijie Su. RearXiv ward collapse in aligning large language models. preprint arXiv:2305.17608, 2023. 4 [41] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. 2, [42] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv preprint arXiv:2311.16101, 2023. 8 [43] Chenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Murun Yang, Qiaozhi He, Tong Xiao, Chunliang Zhang, Tongran Liu, Quan Du, et al. Rovrm: robust visual reward model optimized via auxiliary textual preference data. arXiv preprint arXiv:2408.12109, 2024. 6, 8 [44] Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. Openr: An open source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671, 2024. 3, 8 [45] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. 2, 3, 5 [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [47] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. 8 [60] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanarXiv preprint guage models via preference fine-tuning. arXiv:2402.11411, 2024. [48] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 2 [49] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. LlavaarXiv critic: Learning to evaluate multimodal models. preprint arXiv:2410.02712, 2024. 2, 8 [50] Michihiro Yasunaga, Luke Zettlemoyer, and Marjan Ghazvininejad. Multimodal rewardbench: Holistic evaluation of reward models for vision language models. arXiv preprint arXiv:2502.14191, 2025. 8 [51] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multiarXiv discipline multimodal understanding benchmark. preprint arXiv:2409.02813, 2024. 2, 8 [52] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. 2, 8 [53] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. NeurIPS, 2025. 2, 5 [54] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. [55] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. 2, 8, 14 [56] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pages arXiv2407, 2024. 5, 12 [57] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process rearXiv preprint ward models in mathematical reasoning. arXiv:2501.07301, 2025. 2, 8 [58] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 2, 8 [59] Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, et al. Rmb: Comprehensively benchmarking reward models in llm alignment. arXiv preprint arXiv:2410.09893, 2024. 8 A. Experimental Settings for VLLM-as-a-"
        },
        {
            "title": "Judge",
            "content": "In this section, we demonstrate the details of how we benchmark VLLMs as reward models on existing visionlanguage (VL) benchmarks. Following (V)LLM-as-a-judge paradigm, we input the pre-defined scoring rule, the question, as well as the solution steps for VLLMs to judge the score. We show our prompt below: You are highly capable multimodal AI assistant tasked with evaluating the quality of intermediate reasoning steps provided for visual questions. The input answer may represent an incomplete step in the larger reasoning process. Assign score from 1 to 5 based on how well the step contributes to addressing the question. Question: question Answer: answer Your score should reflect the overall quality of the answer, focusing on its relevance, coherence, accuracy, and clarity Scoring Scale (1-5): 5 (Excellent): The reasoning step is highly relevant, accurate, detailed, and exceptionally clear, making strong contribution to addressing the question. 4 (Good): The reasoning step is relevant, mostly accurate, and clear, with logical progression and only minor flaws. 3 (Fair): The reasoning step is somewhat relevant and partially accurate, demonstrating basic logic but lacking detail, clarity, or precision. 2 (Poor): The reasoning step is partially relevant but contains major errors, lacks coherence, or is difficult to understand. 1 (Very Poor): The reasoning step is irrelevant or nonsensical, showing no meaningful connection to the question or image. After your evaluation, please: 1. Assign one overall score from 1 to 5 based on the descriptions above. 2. Explain your reasoning in detail, highlighting specific strengths and weaknesses of the answer. Example Response: Reasoning: [Explanation of the evaluation]. Overall Score: [1-5] B. Data Collection Details for ViLReward-73K B.1. Data Selection We detail the five datasets that we leverage for ViLReward73K. MAVIS-Geometry [56] is mathematical geometry problem dataset that includes 4 different difficulty levels, marked as depth0, depth1, depth2, and depth3. We found that depth0 and depth1 are relatively simple, while Figure 8. MCTS tree we have constructed for geometry problem datasets (e.g., MAVIS-Geometry). One path in the tree yields correct result, while the remaining paths result in incorrect answers. It is worth noting that we use ellipses to omit some nodes in the original MCTS tree for better presentation. depth3, compared to depth2, mostly just increases the number of composite bodies without significantly increasing the difficulty. We chose depth2 as the source for our synthetic data, selecting 5000 examples from it as our question data. MAVIS-Geometry categorizes question types into three classes: text-dominant questions, text-lite questions, and vision-dominant questions. We use visiondominant questions to enhance the models visual capabilities. We demonstrate the MCTS tree constructed on MAVIS-Geometry in Fig. 8. A-OKVQA [38] mainly contains question-answering problems about natural images, while the majority of these problems are relatively straightforward, requiring only basic visual recognition and common knowledge. In our study, we focused specifically on the more challenging questions within this dataset. We kept 9% of the more difficult ones which were labeled as difficult direct answer. To generalize to the general visual perception domain, we also sample 500 examples as metadata from the questions that can be directly answered. GeoQA170K [10] contains over 170K geometric image-caption and question-answer pairs, building on GeoQA+ [3] and GeoQA3K [28]. We sample one question from each unique images from the data, resulting in 8,063 examples in total. CLEVR-Math [24] is synthesized VQA dataset based on CLEVR [16] that includes math word problem solving. They incorporate addition/subtraction types of math problems. We only consider 957 questions with distinct images and need multi-hop reasoning in the dataset. ScienceQA [29] is comprehensive datasets with 21K examples in science, the data is categorized into 12 grades based on the difficulty level. We use data that is harder than grade 7 as our metadata. B.2. MCTS Searching Details Our construction of the search tree is primarily based on the Monte Carlo Tree Search (MCTS). The process of building this search tree follows several key steps: avoid inaccurate extraction of the answer, we follow previous works [30, 55] to employ GPT-based extraction. In detail, we prompt GPT-3.5-turbo to compare the prediction with the ground truth, the input instruction shows below: Given the following: ### Generated Answer: model predicted answer ### Ground Truth Answer: ground truth answer Please compare the final answer in the generated Ignore any response to the ground truth answer. reasoning or intermediate steps and focus only on whether the final letter answer in the generated response matches the ground truth. Output True if the final answer aligns with the ground truth answer; otherwise, output False. D. Vision-Language PRM Training Details We employ the value head architecture for PRM training. In detail, we train the model on our ViLReward-73K for 2 epochs with constant learning rate of 2e5. We randomly sample 300 instances as the validation set during training and save the model checkpoint with the lowest validation loss. E. Scoring Examples from RMs In Figure 9, we present another example in the domain of medical reasoning task. As the PRM URSA [31] was not trained on the domain of general knowledge, it gives biased judges in this case. In the meanwhile, our ViLPRM is capable of providing more accurate and consistent step rewards in this domain. Search Tree Initialization. The search process begins with root node that represents the initial state of the problem. This root node serves as the foundation for the entire search tree, with its parent node set to None. Node Expansion During Search. During each iteration of the MCTS process, the tree undergoes expansion through four essential phases: Selection Phase. Starting from the root node, path is selected based on specific strategy (we use the Upper Confidence Bound algorithm) until node that has not been fully expanded is identified. Expansion Phase. For node that has not been fully expanded, potential child nodes are generated. Each child node represents possible subsequent state and is incorporated into the search tree structure with appropriate depth and parent-child relationships. Simulation Phase. From each newly expanded node, simulation is conducted using predetermined strategy (often random approach) until terminal state is reached. Backpropagation Phase. The results obtained from the simulation are propagated backward through all nodes along the path from the root to the expanded node. This process updates key node statistics including the visit count and value estimations. Termination Criteria. The construction of the search tree continues until specific termination conditions are met. In our implementation, we set the iteration limit to 10, meaning the search process concludes after completing 10 iterations. Once this criterion is satisfied, the search process terminates, and the final tree structure is established. Answer Evaluation. We follow the LLM-as-a-judge approach, using LLM to score the final answers and then propagate these scores from leaf nodes to previous nodes. We first prompt the LLM to extract the final answer from the models output, then input the question, the modelgenerated answer, and the correct answer to the LLM to determine whether the final answer is correct. C. VILBENCH Evaluation Details We employ the accuracy between predicted answers and the ground truth as the metric for our VILBENCH. To Figure 9. An example from o1s generation of medical reasoning example from our VILBENCH."
        }
    ],
    "affiliations": [
        "Amazon Research",
        "UC Santa Cruz",
        "UT Dallas"
    ]
}