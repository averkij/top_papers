{
    "paper_title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning",
    "authors": [
        "Yang Zhou",
        "Sunzhu Li",
        "Shunyu Liu",
        "Wenkai Fang",
        "Jiale Zhao",
        "Jingwen Yang",
        "Jianwei Lv",
        "Kongcheng Zhang",
        "Yihe Zhou",
        "Hengtong Lu",
        "Wei Chen",
        "Yan Xie",
        "Mingli Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 4 9 6 1 . 8 0 5 2 : r a"
        },
        {
            "title": "THE",
            "content": "BREAKING BOTTLENECK: RUBRIC-SCAFFOLDED REINFORCEMENT LEARNING FOR GENERAL LLM REASONING"
        },
        {
            "title": "EXPLORATION",
            "content": "Sunzhu Li2, Shunyu Liu3 #, Wenkai Fang1, Jianwei Lv2, Kongcheng Zhang1, Yihe Zhou1, Yang Zhou1, 2, Jingwen Yang2, 4, Hengtong Lu2, Wei Chen2, Yan Xie2, Mingli Song1 1Zhejiang University, 2Li Auto Inc., 4The Chinese University of Hong Kong, Shenzhen imzhouyang@zju.edu.cn, lisunzhu@lixiang.com, shunyu.liu@ntu.edu.sg 3Nanyang Technological University, Jiale Zhao2,"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated tremendous potential over wide spectrum of complex reasoning tasks, including legal analysis (Choi et al., 2021; Lai et al., 2024; Fei et al., 2023; Trautmann et al., 2022), robotic manipulation (Driess et al., 2023; Zitkovich et al., 2023; Firoozi et al., 2025; Zhou et al., 2023), and software development (Anysphere, 2023; Fan et al., 2023; Hou et al., 2024). However, advancing the general reasoning capabilities of LLMs remains significant challenge (Zhao et al., 2023; Huang & Chang, 2022). To address this, recent breakthroughs in Reinforcement Learning with Verifiable Rewards (RLVR), as exemplified by DeepSeek-R1 (Guo et al., 2025) and OpenAI-o3 (OpenAI, 2025a), have proven that leveraging verifiable rewards as feedback signals can successfully facilitate the emergence of sophisticated reasoning capabilities in LLMs (Lambert et al., 2024; Yang et al., 2025; Kimi et al., 2025). Despite the encouraging results, conventional RLVR tend to be more applicable to domains with objectively verifiable answers. For instance, in areas such as mathematical proof (Ren et al., 2025; Chen et al., 2025) and code generation (Qwen, 2025; Le et al., 2022), correctness can be explicitly #Corresponding author. 1 Figure 1: (a) conceptual illustration of exploration bottleneck and scaffolding. (b) Performance comparison of different LLMs on HealthBench-500. determined through formal proof verification or automated unit tests. In these contexts, the reward signal is clear and well-aligned with the task objective, allowing RLVR to effectively guide models toward correct solutions. Unfortunately, many real-world tasks like medical consultation (Lin et al., 2025; Singhal et al., 2023; Zhang et al., 2023) and creative writing (Wu et al., 2025b; Franceschelli & Musolesi, 2024) are inherently open-ended. These tasks often require multidimensional evaluation and lack single, verifiable ground-truth answer. To tackle this problem, several very recent concurrent works (Arora et al., 2025; Kimi et al., 2025; Gunjal et al., 2025; Viswanathan et al., 2025) have explored rubric-based evaluation that decomposes desirable responses into checklist-style criteria (e.g., factuality, coherence, completeness). By leveraging LLM-as-a-Judge to score each criterion and aggregating results into scalar rewards, rubrics provide more stable and reliable feedback signal suitable for RLVR in open-ended domains. Nevertheless, as shown in Figure 1(a), fundamental exploration bottleneck remains as RL requires high-quality samples to improve, yet the exploration for such samples remains bounded by the inherent capabilities of LLMs (Yue et al., 2025; Wu et al., 2025a; Liu et al., 2025b; Dong et al., 2025). This creates an inevitable loop where the inability to explore restricts the ability to learn. growing line of work has attempted to enhance exploration in RLVR for LLMs (Liu et al., 2025b;a; Dong et al., 2025; Zheng et al., 2025; Lei et al., 2025; Li et al., 2025; Cheng et al., 2025). However, these methods largely bias the policy distribution toward high-reward responses already supported by the base model, rather than truly expanding its reasoning boundaries (Wu et al., 2025a). Worse still, RL itself has natural tendency to narrow the exploration space: policy entropy gradually collapses during training, causing the model to converge toward limited set of reasoning trajectories (Zhao et al., 2025; Yue et al., 2025; Wu et al., 2025a; Yu et al., 2025; Liu et al., 2025b). This, in turn, undermines the potential of RLVR to explore more diverse and higher-quality solutions. In this work, we introduce Rubric-Scaffolded Reinforcement Learning, termed as RuscaRL, which employs novel instructional scaffolding framework to break the exploration bottleneck of RLVR. Technically, RuscaRL leverages rubrics in two complementary ways: (1) Explicit scaffolding during rollout generation. For each instruction, RuscaRL generates group of candidate responses by using rubrics as external guidance. We propose intra-group scaffolding differentiation to provide varying levels of rubrics within each group, enabling diverse and high-quality responses. To further internalize underlying reasoning patterns, we use inter-step scaffolding decay to gradually remove the scaffolding over training, thereby minimizing reliance on external guidance. (2) verifiable rewards during model training. The model responses are assessed based on multiple criteria derived from rubrics. For each criterion, we employ Grader LLM performs binary evaluation (i.e., true or false), determining whether the response satisfies that specific requirement. The outcomes are then combined through weighted aggregation to obtain robust reward signal, facilitating effective RL across different general tasks. Our main contributions are summarized as follows: We introduce instructional scaffolding as novel paradigm in RLVR for LLMs, which pioneers the integration of external guidance within task instructions to improve rollout diversity and quality, thereby enabling more efficient exploration during RL. 2 We propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), an innovative RLVR framework designed to break the exploration bottleneck, integrating checklist-style rubrics as both explicit scaffolding for exploration and verifiable rewards for exploitation. Extensive experiments demonstrate that our RuscaRL yields results superior to state-of-the-art counterparts. Notably, RuscaRL enables small LLMs (e.g., Qwen3-30B) to achieve performance on par with leading LLMs (e.g., OpenAI-o3) on HealthBench-500, as shown in Figure 1(b)."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "LLM Reasoning. The evolution of LLM reasoning approaches has progressed through several distinct phases. While early methods like prompt engineering (Wei et al., 2022; Kojima et al., 2022) and supervised fine-tuning (Ouyang et al., 2022) have yielded encouraging results, their reliance on task-specific prompts or extensive labeled data limits their scalability and cross-domain generalization (Stiennon et al., 2020; Pornprasit & Tantithamthavorn, 2024; Zhang et al., 2024b; Gao et al., 2023). Recent works have found that using more test-time computation (Snell et al., 2024; Zhang et al., 2024a; Zuo et al., 2025) can improve reasoning performance. More recently, RLVR (Lambert et al., 2024; OpenAI, 2025a; Guo et al., 2025) has emerged as promising paradigm for training LLMs to solve verifiable problems, showing strong reasoning improvements in domains like math and coding (Guo et al., 2025; Qwen, 2025; Lambert et al., 2024; OpenAI, 2025a). However, it faces exploration bottlenecks (Wu et al., 2025a; Yue et al., 2025; Liu et al., 2025b) and is difficult to extend to general tasks where correctness is hard to verify (Gunjal et al., 2025; Kimi et al., 2025). Rubric-based Methods. Rubrics are structured evaluation frameworks that decompose complex assessment tasks into specific, verifiable criteria. To address the complex, multi-dimensional evaluation demands of general tasks, series of rubric-based evaluation works emerged, including health domain evaluation (Arora et al., 2025), code domains (Pathak et al., 2025), and in other general domains (Fan et al., 2025; Galvan-Sosa et al., 2025; Winata et al., 2025). Building upon these evaluation frameworks, researchers have further attempted to apply rubrics as reward signals in RL training (Kimi et al., 2025; Gunjal et al., 2025; Viswanathan et al., 2025). This approach enables training on general tasks where conventional ground truth is unavailable, while providing more robust and fine-grained reward signals compared to conventional binary correct or incorrect evaluations. The typical implementation involves using LLMs as graders to evaluate responses against each rubric criterion, then aggregating these evaluations into reward signals. This methodology has demonstrated promising results in agentic systems (Kimi et al., 2025), open-ended question answering (Gunjal et al., 2025), and instruction following (Viswanathan et al., 2025). Exploration in RL for LLMs. Existing RL methods face insufficient exploration challenges in complex reasoning tasks, primarily manifesting as policies getting trapped in local optima and capability boundary collapse (Wu et al., 2025a; Yue et al., 2025; Liu et al., 2025b). To address these issues, the research community has mainly adopted two strategies. Prolonged training expands reasoning boundaries through increased training iterations (Liu et al., 2025b;a), but this approach incurs high computational costs with diminishing returns. Entropy-based exploration approaches identify critical decision points through policy entropy for targeted exploration (Dong et al., 2025; Zheng et al., 2025; Lei et al., 2025; Li et al., 2025; Cheng et al., 2025), yet these methods only enhance exploration and struggle to break the exploration bottleneck as they rely solely on the models internal entropy signals. In contrast, RuscaRL offers fine-grained, explicit scaffolding via checklist-style rubrics during rollout: each trajectory is guided by verifiable criteria covering the reasoning process (e.g., what to include or avoid, in what order). This structured guidance enhances exploration beyond the initial models distribution in early training, while scaffolding decay fosters internalisation of the new reasoning patterns."
        },
        {
            "title": "3 BACKGROUND",
            "content": "RL Algorithms for LLMs. We adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as our core RL algorithm for training language models with rubric-based rewards. Unlike Proximal Policy Optimization (PPO) (Schulman et al., 2017), GRPO eliminates the need for value model by using group-based advantage estimation. For each instruction D, where denotes the distribution over the training dataset D, GRPO samples group of responses {o1, o2, . . . , oG} 3 Figure 2: An overview of the proposed RuscaRL framework, which comprises two core components: (1) Explicit scaffolding during rollout generation, where the model generates candidate responses using rubrics as external guidance with intra-group differentiation and inter-step decay. (2) Verifiable rewards during model training, where responses are assessed based on multiple criteria derived from rubrics through binary evaluation and weighted aggregation. from the old policy πθold and optimizes the policy πθ by maximizing the following objective: JGRPO (θ) = qD,{oi}G i=1πθold (q)"
        },
        {
            "title": "1\nG",
            "content": "(cid:88) i=1 1 oi oi (cid:88) t= (cid:16) min ρi,t(θ) ˆAi, clip (ρi,t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:17) (1) where oi is response sampled from the old policy πθold given instruction q, denotes the token position within response oi, ρi,t(θ) = πθ(oi,tq,oi,<t) πθold (oi,tq,oi,<t) is the token-level importance ratio between the current policy and the previous policy, and ϵ is the clipping coefficient (Schulman et al., 2017). The group-relative advantage is computed as: ˆAi = ri mean (cid:0) {rj}G j=1 (cid:1) std (cid:0) {rj}G j=1 (cid:1) , (2) where ri is the reward for response oi, and the advantage is normalized using the mean and standard deviation of the sampled rewards."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "To address the exploration bottleneck problem, we propose the RuscaRL framework, as illustrated in Figure 2. RuscaRL leverages rubrics in two complementary ways: (1) Explicit scaffolding during rollout generation, where the model generates candidate responses using rubrics as external guidance with intra-group differentiation and inter-step decay; (2) Verifiable rewards during model training, where responses are assessed based on multiple criteria derived from rubrics through binary evaluation and weighted aggregation. In the following, we first introduce the basic concept of rubrics, then detail these two core components."
        },
        {
            "title": "4.1 RUBRIC-BASED EVALUATION SYSTEM",
            "content": "To address the complex, multi-dimensional evaluation demands of general tasks, we establish structured checklist-style rubric evaluation system (Arora et al., 2025). Specifically, we formalize our rubric as set of multi-dimension verifiable criteria: = {c1, c2, . . . , cN }. Each criterion ci is specified by clear description and corresponding points pi indicating its contribution to the 4 overall evaluation. We define the point vector as = [p1, p2, . . . , pN ]. Given an instruction and its corresponding rubric both sampled from the data distribution D, and model response generated through the policy model πθ(oq), we first construct judge prompt for each criterion ci by combining the instruction q, response o, and criterion ci. The judge prompt template for the grader is provided in Appendix D.1. For single criterion evaluation, the grader function implemented by LLM (Zheng et al., 2023b; Gu et al., 2024) takes the judge prompt as input and outputs binary decision bi = G(q, o, ci) {0, 1} indicating whether criterion ci is satisfied (true of false). Extending this to the full rubric, the grader evaluates all criteria and produces binary indicator vector = G(q, o, R) = [b1, b2, . . . , bN ] where each bi represents the satisfaction of criterion ci. The final score vector is obtained by element-wise multiplication: = = [b1p1, b2p2, . . . , bN pN ], providing fine-grained score across all specified criteria. We also compute the total possible score Stotal = (cid:80)M j=1 pj where is the number of positive-point criteria, which will be used for normalization in the reward calculation."
        },
        {
            "title": "4.2 RUBRIC-BASED SCAFFOLDING MECHANISM FOR RL EXPLORATION",
            "content": "During RL training, models often struggle to effectively explore high-quality solution spaces for complex tasks, which can lead to local optima. To address this exploration challenge, we draw inspiration from educational psychology and introduce the concept of instructional scaffolding, which originates from Vygotskys theory of the Zone of Proximal Development (ZPD) (Vygotsky & Cole, 1978). Instructional scaffolding refers to the temporary support provided by more knowledgeable entity (e.g. teacher, peer, structured guidance) to help learners accomplish tasks they could not complete independently. This support is gradually reduced or removed as the learners competence increases, process often termed fading (Wood et al., 1976; Pea, 1985). Recent works have extended the pedagogical notion of scaffolding to LLMs, including task-agnostic meta-prompting frameworks (Suzgun & Kalai, 2024), scaffolded training approaches (Lin et al., 2024). Inspired by the above instructional scaffolding theory, we propose novel approach. Specifically, we design rubric-based scaffolding mechanism that provides varying numbers of rubric criteria as explicit guidance throughout the training process, helping models gradually learn to generate high-quality responses. Specifically, our rubric-based scaffolding mechanism augments the original policy function by adding subset of rubric criteria RS as additional guidance, representing the policy as πθ(oq, RS). The specific prompt template for incorporating scaffolding is detailed in Appendix D.2. We design two-dimensional control mechanism to determine the rubrics scaffolding ratio λS, and then sample criteria from the complete rubric set to form RS, i.e., RS = round(λS R). Intra-Group Scaffolding Differentiation. In RL algorithms with multiple sampling like GRPO, to make samples more diverse and more effective when computing the group-relative advantages as defined in Eq. 2, we implement varying levels of rubrics within each group, enabling diverse and high-quality responses. Specifically, we define group-level scaffolding ratio vector λgroup = [λ1, λ2, . . . , λG] where λi = Gi G1 for the i-th sample in the group of size G. The motivation behind this intra-group differentiation mechanism is to address the homogeneity problem in group-based RL training where all samples within group receive identical guidance, potentially leading to similar response patterns and reduced exploration diversity. By providing varying levels of scaffolding within each group, we create differentiated learning environment where some samples receive more detailed guidance while others are encouraged to explore by themselves. This differentiated approach enhancing the variance of the advantage estimation. Inter-Step Scaffolding Decay. Scaffolding instruction theory (Vygotsky & Cole, 1978) indicates that as learners develop independent learning strategies, these supports should be gradually removed. Similar to the instructional scaffolding theory, we introduce sigmoid function for Inter-Step Scaf1+eα(tt0) , where is the current training progress (t [0, 1]), t0 is folding Decay using λstep(t) = the midpoint (i.e., progress where decay reaches 50%), and α controls the steepness of decay. 1 This inter-step scaffolding decay mechanism prevents the model from overfitting to external guidance, where static scaffolding levels throughout the entire training process can lead to over-reliance on external guidance. This mechanism resembles curriculum learning, where the difficulty gradually increases as the learners capability improves. By dynamically adjusting the scaffolding intensity 5 Algorithm 1 Rubric-based RL Training Process 1: Input: Policy model πθ, data distribution D, grader model 2: Initialize: Reference policy πref πθ 3: for each training iteration do for each (q, R) do 4: 5: 6: 7: 8: 9: 10: 11: 12: Sample rubric subset RS,i based on λS,i Generate response: oi πθ(q, RS,i) Evaluate with grader: bi = G(q, oi, R) Compute score vector: = Compute scaffolding ratio vector: λS = λstep(t) λgroup for = 1 to do end for for each response oi do (cid:80)N j=1 si,j Stotal end for Compute advantages based on rewards Update policy model πθ Compute reward: ri = 13: 14: 15: 16: 17: 18: 19: end for 20: Return: Trained policy πθ end for Update scaffolding step ratio: λstep(t) across training steps, we create an adaptive learning environment where the model initially benefits from guidance to overcome the exploration bottleneck, then gradually transitions to independent reasoning as its capabilities mature. This temporal decay prevents the model from becoming overly dependent on external scaffolding while ensuring sufficient support during critical early learning stages, thereby facilitating the internalization of reasoning patterns. Based on the above mechanisms, we propose an integrated ratio vector for each group: λS = λstep (t) λgroup = [λS,1, λS,2, . . . , λS,G] , (3) where λS,i = λstep(t) λi = G1 represents the scaffolding ratio for the i-th sample in the group. This integrated scaffolding mechanism synergistically combines differentiation within groups and decay across training steps to address both homogeneity and overfitting problems. 1+eα(tt0) Gi"
        },
        {
            "title": "4.3 RUBRIC-BASED REWARD FUNCTION FOR RL EXPLOITATION",
            "content": "To provide robust and reliable reward signals for general reasoning tasks, we design rubric-based reward functions. The multi-dimensional score vector = [s1, s2, . . . , sN ] obtained from the evaluation system is aggregated into final scalar reward by directly summing all criterion scores and normalizing by the total possible score computed in Section 4.1: = (cid:80)N i=1 si Stotal , (4) where represents the final score, si is the score of the i-th criterion, and Stotal is the total possible score of all positive-point criteria computed in Section 4.1. This calculation method results in scores that fall within the interval [0, 1] in most cases, with occasional negative scores possible. We directly adopt this rubric-based score as our reward: ri = Si, where ri is the reward for the i-th response. Rubric-based scoring offers two key advantages as our reward function. (1) Compared to methods relying on ground truth answers, it can be applied to general open-ended tasks where standard answers may not exist. (2) Compared to direct holistic LLM scoring, it provides more robust and fine-grained reward through structured multi-dimensional assessment. Once the rubric-based rewards are obtained, we employ them to train the policy model using RL algorithms. The training process follows the policy gradient framework, where the model learns to maximize the expected reward. Algorithm 1 outlines the complete RL training procedure. In each 6 training iteration, we first sample rubric subset based on the scaffolding ratio, then generate candidate responses. Each response is evaluated using the grader to compute rewards, and finally the model parameters are updated through policy gradients. To help the model better internalize underlying reasoning patterns, the log probability computation in policy updates is based on log πθ(oiq) rather than log πθ(oiq, RS,i). Meanwhile, to avoid over-dependence on external guidance, the scaffolding ratio is gradually decayed over training iterations."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we conduct experiments to validate the effectiveness of the RuscaRL framework. Our experimental design includes main experiments, ablation studies, and in-depth analysis."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Models and Training Settings. We use multiple initial models from different series and parameter scales for our experiments, including the Qwen2.5 series (Qwen2.5-3B-Instruct, Qwen2.5-7B, Qwen2.5-7B-Instruct, Qwen2.5-32B-Instruct) (Yang et al., 2024), the Qwen3 series (Qwen3-4BInstruct-2507, Qwen3-30B-A3B-Instruct-2507) (Yang et al., 2025), and the Llama series (Llama3.23B-Instruct (Meta-AI, 2025), Llama3.1-8B-Instruct) (Grattafiori et al., 2024). All models are trained using the verl framework (Sheng et al., 2025). Detailed training settings are provided in Appendix A. Datasets. We use HealthBench (Arora et al., 2025), challenging medical dataset with complex and multi-dimensional evaluation criteria, which serves as our core experimental setting. The 5000 samples in HealthBench are randomly split into 4500 training samples and 500 test samples. We selected these 500 test problems as our held-out evaluation set. For evaluation on HealthBench, we adopt GPT-4.1 (OpenAI, 2025b) as the Grader LLM to perform rubric-based verification of model responses. Additionally, we evaluate on other medical benchmarks including LLMEval-Med (Zhang et al., 2025a), MedQA (Jin et al., 2021) and MedMCQA (Pal et al., 2022). The training set for the STEM domain comes from SCP-116K (Lu et al., 2025) and MATH datasets (Level 3-5) (Hendrycks et al., 2021), where we generate HealthBench-like rubrics data by calling GPT-4.1 (OpenAI, 2025b) with specific prompts detailed in Appendix D.3. The test set uses MATH500 (Lightman et al., 2023), AIME24, AIME251, AMC232 and GPQA-Diamond (Rein et al., 2024). Baselines. We compare RuscaRL against three representative baseline methods and leading models: (1) SFT: Fine-tuned on GPT-4.1 demonstrations with two variants: standard SFT uses direct GPT4.1 responses, while SFT-S uses GPT-4.1 responses generated with full scaffolding support. The original GPT-4.1 responses achieve 47.9 on HealthBench, while the scaffolded responses achieve 88.4, though the scaffolded responses may be less natural despite higher evaluation scores. (2) Rubric-based RL: rubric-as-reward RL baseline implemented with GRPO algorithm using rubrics scores as rewards, without our proposed rubric-based scaffolding mechanism (Gunjal et al., 2025; (3) Rubric-based RL with full scaffolding (Rubric-based RL-S): Viswanathan et al., 2025). method that provides all rubrics with scaffolding support in the instruction, without intra-group differentiation and without an inter-step decay function. We also compare against leading models including closed-source models (OpenAI-o3 (OpenAI, 2025a), GPT-4.1 (OpenAI, 2025b), Gemini-2.5-Pro (Google, 2025)) and open-source models (DeepSeek-R1-0528 (Guo et al., 2025), Qwen3-235B-Thinking-2507 (Yang et al., 2025), KimiK2-Instruct (Kimi et al., 2025), gpt-oss-120b, gpt-oss-20b (OpenAI, 2025c)) to demonstrate the competitiveness of our approach. Evaluation Metrics. We employ multiple metrics to evaluate model performance. For HealthBench, we use scores computed through rubrics to measure performance. For datasets with ground truth answers, we use accuracy as the score metric. All scores are converted to percentage scale for consistent comparison. In addition to score metrics, we use novelty and diversity as supplementary evaluation metrics to quantify the effectiveness of RuscaRL in breaking the exploration bottleneck. (1) Novelty measures the models ability to generate solutions that it considered low-probability before training. We first calculate the importance ratio based on sequence likelihood (Xu et al., 1https://artofproblemsolving.com/wiki/index.php/AIME Problems and Solutions 2https://huggingface.co/datasets/AI-MO/aimo-validation-amc 7 Table 1: Main results across different medical benchmarks."
        },
        {
            "title": "Method",
            "content": "HealthBench-500 LLMEval-Med MedQA MedMCQA Qwen2.5-7B-Instruct SFT SFT-S Rubric-based RL Rubric-based RL-S RuscaRL (Ours) 23.6 35.5 38.3 41.2 36. 50.3 47.98 53.22 52.62 54.57 56.07 61.17 61.82 62.37 60.88 62.06 57. 63.50 56.33 57.17 57.36 56.26 52.43 56.46 (a) HealthBench-500 (b) LLMEval-Med (c) MedQA (d) MedMCQA Figure 3: Performance comparison across four datasets for different models. The figure shows the effectiveness of RuscaRL compared to baseline methods across multiple evaluation benchmarks. Detailed numerical results are provided in Appendix  (Table 4)  . 2024; Zheng et al., 2023a) for each generated sequence on the test set, which reflects the difference between the new and old policies: ρseq = (cid:19) (cid:18) πθ (oq) πθold (oq) = exp 1 o (cid:88) t=1 log πθ (otq, o<t) πθold (otq, o<t) . (5) Based on these importance ratios, we derive two metrics: (a) Median Importance Ratio: The median of all importance ratios, reflecting the overall novelty level. (b) Count above Thresholds: The number of samples with importance ratios exceeding specific thresholds. We use three thresholds: ratios greater than 2 indicate responses that the original model finds difficult to generate, ratios greater than 10 indicate very difficult responses, and ratios greater than 100 indicate nearly impossible responses. (2) Diversity measures the models ability to generate multiple different responses for the same instruction. In our experiments, we generate 16 responses for each instruction in the test set and evaluate diversity using two metrics: (a) Self-BLEU (Zhu et al., 2018; Papineni et al., 2002), which measures the surface-level lexical similarity of generated answers by calculating BLEU scores between each answer and others in the set. We use 1-Self-BLEU as the diversity metric since lower self-BLEU indicates higher diversity. (b) Semantic Distance measures semantic diversity by calculating the average cosine distance between embedding vectors of generated answers, computed using Qwen3-Embedding-0.6B (Zhang et al., 2025b)."
        },
        {
            "title": "5.2 OVERALL PERFORMANCE",
            "content": "Medical benchmark performance comparison. Table 1 shows the performance comparison between RuscaRL and various baseline methods on medical benchmarks. We achieve significant improvements on HealthBench and demonstrate good generalization on other medical datasets (including both subjective open-ended tasks and objective multiple-choice tasks). Our method achieves the best performance on LLMEval-Med and MedQA datasets3. Scalability across different initial models. We conducted experiments using RuscaRL on models of different series and scales, achieving significant improvements across all models. Fig3All evaluations use the temperature of 0.7 and the maximum output length of 4,096 tokens. For MedQA and MedMCQA, we report the average over 3 independent runs. 8 ures 3 demonstrate these results, corresponding to performance comparisons on HealthBench-500, LLMEval-Med, MedQA, and MedMCQA datasets, respectively. The result shows that RuscaRL achieves substantial improvements across models of different scales. Whether for Qwen series or Llama series, RuscaRL demonstrates consistent improvement effects. Models with lower initial performance tend to achieve greater absolute improvements, such as Llama-3.1-8B-Instruct improving from 12.4 to 46.0 (+33.6) on HealthBench-500, indicating that RuscaRL is particularly beneficial for enhancing the performance of weaker models. Comparison with leading models. As shown in Figure 1(b), it presents the comparison results with leading open-source and closed-source models. Notably, our model trained on Qwen2.57B-Instruct surpasses GPT-4.1, while our model trained on Qwen330B-A3B-Instruct-2507 even outperforms OpenAI-o3. Extension to STEM domain. Furthermore, we demonstrate performance improvements on STEM domain benchmarks. As shown in Figure 4, RuscaRL achieves consistent gains across all evaluated STEM benchmarks. However, we observe that the improvements in the STEM domain are relatively limited compared to the medical domain, which can be attributed to the fact that STEM problems typically have clear correct answers and standardized evaluation criteria, making conventional RLVR methods already effective4."
        },
        {
            "title": "5.3 ABLATION STUDIES",
            "content": "Figure 4: STEM Benchmarks To validate the necessity and effectiveness of each component in the rubric scaffolding control mechanism of the RuscaRL framework, we designed ablation studies. The experiments are divided into two main parts: (1) Intra-group Differentiation: examining the impact of different rubric scaffolding control mechanisms within sampling groups; (2) Inter-step Decay: analyzing the effect of different decay functions on the dynamic adjustment mechanism during training. Intra-group Differentiation Analysis. We first analyze different strategies for the intra-group control mechanism. Within individual sampling groups, we compare different rubric scaffolding differentiation patterns. These mechanism are: (1) Linear: Linear differentiation pattern following our proposed formula Gi G1 , providing different levels of rubric scaffolding to different samples within single sampling group. (2) Binary (N): Binary differentiation patterns where represents the number of samples with full rubric scaffolding within single sampling group, including configurations such as no-scaffolding (N=0), half-scaffolding (N=4), and full-scaffolding (N=8). As shown in Figure 5a, the linear differentiation strategy performs optimally in intra-group control. We believe that this superior performance stems from the linear strategys significant enhancement of sampling diversity, which works synergistically with multi-sampling algorithms like GRPO. Inter-step Decay Analysis. We analyze different decay functions for inter-step control during training. We define the base scaffolding intensity of inter-step control as (t), where is the normalized training progress (t [0, 1]). We compare the following decay functions: (1) Sigmoid: S-shaped 1+eα(tt0) , where parameter α controls steepness of decay and t0 controls decay function (t) = (2) Constant: Constant control the midpoint of decay, achieving smooth nonlinear transitions. (t) = 1, maintaining constant full scaffolding. (3) Linear: Linear decay function (t) = 1 t, achieving uniform linear decrease. (4) Power (n): Power decay function (t) = (1 t)n, where controls the curvature of decay, including various power configurations. As shown in Figure 5b, the sigmoid decay function achieves the best performance among all decay strategies. In contrast, linear and power decay strategies perform poorly, which we attribute to prolonged scaffolding addition potentially causing the model to overfit to the corresponding scaffolding rather than focusing on the actual instruction content. The sigmoid function, through its smooth nonlinear transition characteristics, provides adequate scaffolding support in early training stages and then gradually reduces dependency, avoiding the overfitting problem. 4For STEM benchmarks, we report the average over 5 independent runs. 9 (a) Intra-group (b) Inter-step (c) Steepness of Decay (d) Midpoint of Decay (a) Intra-group differentiation Figure 5: Ablation studies on RuscaRL framework components. strategies comparison; (b) Inter-step decay functions comparison; (c) Sigmoid parameter steepness of decay α sensitivity analysis with fixed t0 = 0.2; (d) Sigmoid parameter midpoint of decay t0 sensitivity analysis with fixed α = 125. Based on the superior performance of the sigmoid function, we further analyze the effects of both parameter dimensions (speed α and midpoint t0). Figures 5c and 5d demonstrate the performance differences across various sigmoid parameter configurations, ultimately determining the optimal configuration as α = 125, t0 = 0.2. (1) Removing scaffolding too fast (large α) prevents the model from adapting to rapid scaffolding changes, easily causing training instability; while removing scaffolding too slow (small α) leads to incomplete early-stage scaffolding, failing to fully stimulate the models exploration capability, and prolonged retention of scaffolding in later stages also causes overfitting issues. (2) Starting decay too early (small t0) leads to insufficient scaffolding support, causing the model to lack necessary guidance in early training stages; while starting decay too late (large t0) causes the model to over-rely on scaffolding, ultimately resulting in overfitting. The setting of midpoint t0 = 0.2, which corresponds to starting decay at 1 epoch out of 5 epochs, equivalent to approximately 4500 data samples."
        },
        {
            "title": "5.4 DETAILED ANALYSIS",
            "content": "In this subsection, we conduct detailed analysis of RuscaRL from three key dimensions: evaluating the models capability ceiling and sampling efficiency on HealthBench using Best of metrics, analyzing training dynamics with focus on policy entropy and validation accuracy changes, and conducting analysis of additional metrics including novelty and diversity. Best of evaluation. We use the Best of metric to reflect both the models capability ceiling (at large N) and sampling efficiency (at small N). For cost considerations in the Best-of-N evaluation, we use Qwen3-32B (non-thinking) as the Grader LLM. As shown in Figure 6, at N=1, RuscaRL demonstrates significant improvements in single-sample generation quality compared to baseline methods, indicating that the scaffolding mechanism effectively enhances the models reasoning stability. At N=2048, RuscaRLs performance ceiling shows notable improvements over the initial model and conventional RL methods, proving the effectiveness of our approach in expanding the models exploration boundaries. Furthermore, RuscaRL exhibits steeper performance curve across different values, indicating higher sampling efficiency and the ability to achieve the same performance level with fewer sampling attempts. Figure 6: Best of Performance Training dynamics analysis. We analyze the training dynamics by comparing three methods: RuscaRL, RuscaRL* (RuscaRL without inter-step decay mechanism), and Rubric-based RL. As shown in Figure 7a, in terms of policy entropy changes, RuscaRL exhibits an ideal trend of first increasing then decreasing: in the early stages of training, the model needs to explore and discover diverse reasoning trajectories. The increase in policy entropy indicates that the model is attempting more diverse responses, which helps break the exploration bottleneck. As training progresses, the model gradually generates high-quality reasoning patterns, and policy entropy decreases. This trend perfectly embodies the ideal state of exploration-exploitation trade-off in RL, ensuring both sufficient exploration and effective exploitation. In contrast, RuscaRL* cannot be controlled after policy entropy growth, leading to training instability and even collapse, while Rubric-based RL experiences 10 (a) Training policy entropy (b) Validation evaluation score (c) Sigmoid decay function Figure 7: Training dynamics. The figure shows the evolution of policy entropy, validation accuracy, and sigmoid decay function during training. Table 2: Importance Ratio Statistics Across Different Models"
        },
        {
            "title": "Median",
            "content": "Qwen2.5-7B-Instruct Rubric-based RL RuscaRL 1.0000 1.4621 2.1939 training collapse after continuous policy entropy collapse. Figure 7b shows the validation accuracy, where the performance ranking indicates that RuscaRL achieves the best results, followed by Rubric-based RL, and finally RuscaRL*. 1.0000 1.7460 5424.6238 ρseq > 2 0 45 321 ρseq > 10 0 3 11 ρseq > 100 0 0 Additional metrics analysis. We further analyze the performance of RuscaRL compared with conventional Rubric-based RL in terms of novelty and diversity. To validate that RuscaRL achieves significantly higher novelty improvement compared to Rubric-based RL after training. As shown in Figure 8, RuscaRL demonstrates clear advantages in novelty metrics. Table 2 shows the performance of both methods in terms of importance ratios. The Rubricbased RL method shows some improvement compared to the original model, but the enhancement is limited. In contrast, RuscaRL exhibits significantly higher novelty: the mean importance ratio reaches 5424.6238, with 321 samples having importance ratios greater than 2, 11 samples greater than 10, and even 7 samples greater than 100. These results provide strong evidence that the model trained via RuscaRL can generate responses that the original model finds nearly impossible to generate. For more detailed novelty analysis, please refer to Appendix C. Figure 8: Novelty Comparison To analyze the diversity changes of RuscaRL during training, we compare it with Rubricbased RL and plot the training curves of SelfBLEU scores and semantic distance. As shown in Figure 9, RuscaRL exhibits different diversity evolution pattern compared to conventional RL methods. On both diversity metrics, RuscaRL rapidly improves diversity in the early training stages, then maintains relatively stable high diversity level with gradual decline. In contrast, conventional RL shows faster diver- (a) 1-Self-BLEU (b) Semantic distance Figure 9: Diversity comparison during training. sity collapse (especially on semantic distance metrics)."
        },
        {
            "title": "6 DISCUSSION",
            "content": "While RuscaRL demonstrates promising results in breaking the exploration bottleneck for general LLM reasoning, several limitations remain that highlight directions for future research. 11 (1) Limited availability of rubric-style datasets. Our approach critically relies on checklist-style rubrics to provide both explicit scaffolding and verifiable reward signals. However, such highquality, well-structured rubric datasets are still scarce in the community. The success of RuscaRL underscores the urgent need for broader community investment in constructing open, diverse, and domain-rich rubric datasets, which would in turn promote overall progress in the LLM RL field. (2) Sensitivity to rubric quality. In our experiments, we used HealthBench as the dataset for RL training, because we found that rubric-based RL is highly sensitive to the design quality of the rubrics. Training with lower-quality rubrics, such as those generated by simply using LLMs to generate criteria from open-source datasets, often leads to only limited performance improvements. (3) Computational Cost of Reward Computation. The verifiable rubric-based reward function of RuscaRL relies on grading each individual criterion within rubric via an LLM-as-a-Judge. This per-criterion evaluation introduces substantial computational overhead, either incurring higher API call costs or requiring additional GPU resources to deploy efficient inference serving with frameworks like vLLM (Kwon et al., 2023) or SGlang (Zheng et al., 2024). This overhead may limit the scalability of our approach to very large datasets unless more efficient grading strategies or selective evaluation mechanisms are developed."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduce RuscaRL, novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning tasks. RuscaRL leverages checklist-style rubrics in two complementary ways to achieve effective exploration-exploitation balance. The rubric-based scaffolding mechanism provides external guidance that gradually decays to encourage internalization, while the rubric-based reward function enables robust evaluation for effective RL training. Extensive experiments demonstrate that RuscaRL consistently outperforms strong baselines and achieves remarkable performance improvements, and achieves competitive or superior results compared with state-of-the-art models. Several promising directions emerge from this work. (1) Developing comprehensive pipeline for high-quality rubric data production to ensure consistent and reliable evaluation criteria across diverse tasks. (2) Exploring rubric-based natural language feedback strategies that provide more interpretable and actionable guidance for model improvement. (3) Investigating the application of RuscaRL to multi-modal tasks, extending its effectiveness beyond text-only reasoning scenarios."
        },
        {
            "title": "REFERENCES",
            "content": "Anysphere. Cursor: The ai code editor, 2023. URL https://cursor.com/home?from=agents. Rahul Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quinonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, et al. Healthbench: Evaluating large language models towards improved human health. arXiv preprint arXiv:2505.08775, 2025. Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, et al. Seed-prover: Deep and broad reasoning for automated theorem proving. arXiv preprint arXiv:2507.23726, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Jonathan Choi, Kristin Hickman, Amy Monahan, and Daniel Schwarcz. Chatgpt goes to law school. J. Legal Educ., 71:387, 2021. Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, et al. Rl-plus: Countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. arXiv preprint arXiv:2508.00222, 2025. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: an embodied multimodal language model. In ICML, 2023. 12 Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie Zhang. Large language models for software engineering: Survey and open problems. In ICSE-FoSE, pp. 3153, 2023. Zhiyuan Fan, Weinong Wang, Xing Wu, and Debing Zhang. Sedareval: Automated evaluation using selfadaptive rubrics. arXiv preprint arXiv:2501.15595, 2025. Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. Lawbench: Benchmarking legal knowledge of large language models. arXiv preprint arXiv:2309.16289, 2023. Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. IJRR, 44(5):701739, 2025. Giorgio Franceschelli and Mirco Musolesi. On the creativity of large language models. AI & SOCIETY, pp. 111, 2024. Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery, et al. Rubriks cube: Testing new rubric for evaluating explanations on the cube dataset. arXiv preprint arXiv:2503.23899, 2025. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In ICML, pp. 1076410799, 2023. Google. Gemini 2.5 pro: Best for coding and highly complex tasks, 2025. URL https://deepmind. google/models/gemini/pro/. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, volume 1, 2021. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: systematic literature review. ACM TOSEM, 33(8):179, 2024. Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Kimi, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, volume 35, pp. 2219922213, 2022. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In SOSP, pp. 611626, 2023. Jinqi Lai, Wensheng Gan, Jiayang Wu, Zhenlian Qi, and Philip Yu. Large language models in law: survey. AI Open, 5:181196, 2024. 13 Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS, volume 35, pp. 2131421328, 2022. Shiye Lei, Zhihao Cheng, Kai Jia, and Dacheng Tao. Revisiting llm reasoning via information bottleneck. arXiv preprint arXiv:2507.18391, 2025. Xianzhi Li, Ethan Callanan, Xiaodan Zhu, Mathieu Sibue, Antony Papadimitriou, Mahmoud Mahfouz, Zhiqiang Ma, and Xiaomo Liu. Entropy-aware branching for improved mathematical reasoning. arXiv preprint arXiv:2503.21961, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR, 2023. Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Victor Shea Jay Huang, Huan Liu, Jun Liu, Gao Huang, and Yong-Jin Liu. Training of scaffolded language models with language supervision: survey. arXiv preprint arXiv:2410.16392, 2024. Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507, 2025a. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025b. Dakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. Scp-116k: high-quality problem-solution dataset and generalized pipeline for automated extraction in the higher education science domain. arXiv preprint arXiv:2501.15587, 2025. Meta-AI. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models, 2025. URL https: //ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/. OpenAI. Introducing openai o3 and o4-mini, 2025a. URL https://openai.com/index/ introducing-o3-and-o4-mini/. OpenAI. Introducing gpt-4.1 in the api, 2025b. URL https://openai.com/index/gpt-4-1/. OpenAI. Introducing gpt-oss, 2025c. URL https://openai.com/index/ introducing-gpt-oss/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, volume 35, pp. 2773027744, 2022. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In CHIL, pp. 248260, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, pp. 311318, 2002. Aditya Pathak, Rachit Gandhi, Vaibhav Uttam, Yashwanth Nakka, Aaryan Raj Jindal, Pratyush Ghosh, Arnav Ramamoorthy, Shreyash Verma, Aditya Mittal, Aashna Ased, et al. Rubric is all you need: Enhancing llm-based code evaluation with question-specific rubrics. arXiv preprint arXiv:2503.23989, 2025. Roy Pea. Beyond amplification: Using the computer to reorganize mental functioning. Educational psychologist, 20(4):167182, 1985. Chanathip Pornprasit and Chakkrit Tantithamthavorn. Fine-tuning and prompt engineering for large language models-based code review automation. Information and Software Technology, 175:107523, 2024. 14 Qwen. Qwen3-coder: Agentic coding in the world, 2025. URL https://qwenlm.github.io/blog/ qwen3-coder/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In COLM, 2024. ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In EuroSys, pp. 12791297, 2025. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. In NeurIPS, volume 33, pp. 30083021, 2020. Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with task-agnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024. Dietrich Trautmann, Alina Petrova, and Frank Schilder. Legal prompt engineering for multilingual legal judgement prediction. arXiv preprint arXiv:2212.02199, 2022. Vijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tongshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Lev Semenovich Vygotsky and Michael Cole. Mind in society: Development of higher psychological processes. Harvard university press, 1978. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, volume 35, pp. 2482424837, 2022. Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang, Zheng-Xin Yong, Jan Christian Blaise Cruz, et al. Datasheets arent enough: Datarubrics for automated quality metrics and accountability. arXiv preprint arXiv:2506.01789, 2025. David Wood, Jerome Bruner, and Gail Ross. The role of tutoring in problem solving. JCPP, 17(2):89100, 1976. Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, and Yejin Choi. The invisible leash: Why rlvr may not escape its origin. arXiv preprint arXiv:2507.14843, 2025a. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, et al. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244, 2025b. Zheng Xu, Xu Dai, Shaojun Wei, Shouyi Yin, and Yang Hu. Gspo: graph substitution and parallelization joint optimization framework for dnn inference. In DAC, pp. 16, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. In NeurIPS, volume 37, pp. 6473564772, 2024a. Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be doctor. arXiv preprint arXiv:2305.15075, 2023. Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, et al. Llmeval-med: real-world clinical benchmark for medical llms with physician validation. arXiv preprint arXiv:2506.04078, 2025a. Xiaodan Zhang, Nabasmita Talukdar, Sandeep Vemulapalli, Sumyeong Ahn, Jiankun Wang, Han Meng, Sardar Mehtab Bin Murtaza, Dmitry Leshchiner, Aakash Ajay Dave, Dimitri Joseph, et al. Comparison of prompt engineering and fine-tuning strategies in large language models in the classification of clinical notes. AMIA Summits on Translational Science Proceedings, 2024:478, 2024b. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025b. Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1 (2), 2023. Chujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation with sequence likelihood contrastive learning. In Findings of ACL, pp. 10221040, 2023a. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, volume 36, pp. 4659546623, 2023b. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. In NeurIPS, volume 37, pp. 6255762583, 2024. Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu arXiv preprint First return, entropy-eliciting explore. Wen, Chenghua Lin, Wenhao Huang, et al. arXiv:2507.07017, 2025. Hongkuan Zhou, Xiangtong Yao, Yuan Meng, Siming Sun, Zhenshan Bing, Kai Huang, and Alois Knoll. Language-conditioned learning for robotic manipulation: survey. arXiv preprint arXiv:2312.10807, 2023. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: benchmarking platform for text generation models. In SIGIR, pp. 10971100, 2018. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, pp. 21652183, 2023. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "D Prompt Templates",
            "content": "D.1 Grader Prompt Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Scaffolding Prompt Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Data Generation Prompt Template . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18 19 20 21"
        },
        {
            "title": "A DETAILED TRAINING SETTINGS",
            "content": "This section provides detailed training configurations, hyperparameters, and experimental setup details, as shown in Table 3. We conducted training on eight models across different series and parameter scales. All models share identical hyperparameters except for the t0 parameter in the sigmoid decay function. Specifically, Qwen3-30B-A3B-Instruct-2507 and Llama3.1-8B-Instruct use t0 = 0.1, Llama3.2-3B-Instruct uses t0 = 0.3, and the remaining models (Qwen2.5-3B-Instruct, Qwen2.57B, Qwen2.5-7B-Instruct, Qwen2.5-32B-Instruct, Qwen3-4B-Instruct-2507) use t0 = 0.2. Table 3: RuscaRL training configuration (Qwen2.5-7B-Instruct)."
        },
        {
            "title": "RuscaRL",
            "content": "Inter-step Scaffolding Decay: Step Sigmoid (α = 125, t0 = 0.2) Intra-group Scaffolding Differentiation: Linear Grader Model: Qwen3-32B (non-thinking) RL Algorithm: GRPO Backbone Model: Qwen2.5-7B-Instruct"
        },
        {
            "title": "Training",
            "content": "Temperature: 0.7 Top-p: 0.8, Top-k: 20 Rollout samples per prompt: 8 Max Response Length: 4096 Optimizer: Adam Learning Rate: 1 106 (Constant) Training Batch Size: 64 Mini Batch Size: 32 KL Loss Coefficient: 1 103 Entropy Coefficient: 0 Epochs: 5 Hardware GPUs: 8 H"
        },
        {
            "title": "B DETAILED PERFORMANCE RESULTS",
            "content": "Table 4 provides the detailed numerical results for all models across the four evaluation benchmarks, corresponding to the visual comparison shown in Figure 3. The table shows the performance comparison between initial model performance and RuscaRL-enhanced performance, demonstrating consistent improvements across different model architectures and scales. Table 4: Detailed performance comparison across four medical benchmarks. Initial refers to the baseline model performance, while RuscaRL shows the performance after applying our RL framework. Model HealthBench-500 Initial RuscaRL LLMEval-Med Initial RuscaRL MedQA Initial RuscaRL MedMCQA Initial RuscaRL Qwen2.5-7B-Instruct Qwen2.5-7B Qwen2.5-3B-Instruct Qwen2.5-32B-Instruct Qwen3-4B-Instruct-2507 Qwen3-30B-A3B-Instruct-2507 Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct 23.6 8.2 15.2 28.1 40.2 46.8 10.1 12.4 50.3 46.2 37.2 54.9 56.5 61.1 33.9 46.0 61.17 43.24 49.18 67.62 72.26 73.01 31.78 46.33 61.82 55.28 50.62 74.78 72.92 84.21 58.52 66. 63.50 50.22 50.93 77.32 74.26 84.79 60.83 70.67 56.33 54.95 49.71 66.52 60.91 71.27 52.67 58.04 56.46 50.67 48.39 66.71 61.34 71.85 53.74 60.70 47.98 28.19 42.88 62.07 66.72 71.36 26.24 29.8 18 Table 5: Top 20 High Importance Ratio Samples Comparison"
        },
        {
            "title": "RuscaRL",
            "content": "Rubric-based RL ρseq 2638481.9366 58733.7167 6906.9110 4914.7688 920.2330 890.3963 250.4222 47.1612 15.8559 12.5889 11.7173 6.8854 6.7386 6.2677 5.3779 5.1490 5.1267 5.1168 4.9103 4."
        },
        {
            "title": "Score Diff",
            "content": "0.5385 0.0000 0.8889 0.3667 0.5385 0.4815 0.6667 0.0909 0.8636 0.5455 0.0000 0.2353 0.3647 0.0408 0.6923 0.3600 0.0690 0.2105 0.1579 0.0000 ρseq 35.6616 16.6515 10.0387 9.0930 8.9928 7.6613 6.3239 4.6684 4.5051 4.3245 4.2838 4.1456 4.0832 3.9256 3.4971 3.4870 3.1960 3.0432 2.9736 2."
        },
        {
            "title": "Score Diff",
            "content": "0.0000 0.1333 0.4815 0.0000 0.5263 0.5161 0.5385 0.0000 -0.0938 0.0909 0.0000 0.0667 0.5000 -0.1515 0.0000 0.0952 0.3529 0.0000 0.1356 0."
        },
        {
            "title": "C DETAILED NOVELTY ANALYSIS",
            "content": "Table 5 presents the top 20 samples with highest importance ratios ρseq for both Qwen2.5-7BRuscaRL and Rubric-based RL models, along with their score differences compared to the Qwen2.57B-Instruct baseline. The Score Diff is calculated as: Score Diff = Scoreafter RL Scoreinitial (6) where positive values indicate performance improvements over the baseline. The analysis reveals several key insights about the exploration patterns of different methods. (1) RuscaRL demonstrates significantly higher importance ratios than Rubric-based RL, with the top sample reaching ρseq = 2, 638, 481.94 compared to Rubric-based RLs maximum of 35.66. This substantial difference indicates that RuscaRLs rubric-based scaffolding mechanism enables more aggressive exploration of the policy space, identifying samples that deviate more dramatically from the reference policy. (2) The score differences reveal that RuscaRLs high-importance samples often correspond to meaningful performance improvements. For instance, the sample with the highest importance ratio achieves score difference of 0.5385, while several other high-ratio samples show substantial positive score differences (0.8889, 0.6667, 0.8636). In contrast, Rubric-based RLs high-importance samples frequently show zero or minimal score improvements, suggesting less effective exploration. (3) The distribution patterns differ markedly: RuscaRL shows heavy-tailed distribution with extreme outliers, while Rubric-based RL exhibits more uniform, conservative distribution. This suggests that RuscaRLs rubric-based scaffolding mechanism successfully identifies and amplifies truly novel, high-value responses that would be overlooked by conventional exploration strategies."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "This section provides the three core prompt templates used in the RuscaRL framework, including the Grader evaluation template, Scaffolding template, and data generation template. 19 D.1 GRADER PROMPT TEMPLATE For medical domain tasks, we directly use the prompt template provided in HealthBench (Arora et al., 2025). For other domains, we design general-purpose Grader LLM prompt template based on HealthBenchs approach to evaluate response quality according to specific rubric criteria (see Table D.1). Table D.1: Rubric Evaluation Prompt Template Your job is to look at conversation and rubric item, and score the last turn (i.e., the last assistant response, or the completion) in the conversation on how well it follows the rubric item. # Conversation <<conversation>> # Rubric item <<rubric item>> # Instructions \"explanation\" and If any of the criteria is not met, the answer should be Return json object with the following fields: \"criteria met\". - The \"explanation\" field should be string explaining why the response does or does not meet the criteria of the rubric item. - The \"criteria met\" field should be boolean indicating whether the response meets the criteria of the rubric item. If rubric item has multiple sentences or criteria, you should consider all of them. Only return true is all of the criteria are met. false. - One important exception to the above bullet point is that if criteria says \"such as\", \"for example\", or \"including\", the response does not have to include all of the examples listed to meet the criteria. exercise can provide health benefits such as improved cardiovascular health, weight management, and mental well-being\", and the response just says that regular exercise can provide health benefits such as better mood, that would still meet the criteria even though it didnt mention any of the specific examples listed in the criteria. For example, if the criteria says \"Explains that regular # Example 1 For example, if the conversation is \"user: feeling overwhelmed with work? assistant: will go away on its own\" and the rubric item is \"provides practical advice for managing stress\", you should return json like this: What should do if Im Just ignore it and it json { \"explanation\": \"The assistant did not provide practical advice for managing stress, instead suggesting to ignore the problem.\", \"criteria met\": false } # Example 2 Note that sometimes the criteria may be something undesirable and have corresponding point value that is negative. In this case, you should still return whether the criteria is met, and not whether the response is good or not. For example, consider the conversation is \"user: of France? criterion is \"fails to give the user accurate information\", which is something undesirable. Here, the criteria was not met because the assistant: The capital of France is Paris\" and the Whats the capital 20 response was correct. this: In this case, you should return json like json { \"explanation\": \"The assistant actually gave the user accurate information, so the criteria was not met.\", \"criteria met\": false } As another example, if the critera says \"Is overly verbose\" and the response is concise, then the criteria is not met, and you should return json like this: json { \"explanation\": \"The response is concise, so the criteria was not met.\", \"criteria met\": false } In other words, for criteria with negative points, good response should be classified as false because it does not meet the undesirable criteria, and only bad responses that do meet undesirable criteria should be classified as true. # Final instruction Return just the json object in markdown format. other text in the response. Do not include any D.2 SCAFFOLDING PROMPT TEMPLATE The following is the prompt template used to provide rubric-based scaffolding during training, which adds selected rubric criteria as explicit guidance to the original instruction (see Table D.2). Table D.2: Scaffolding Prompt Template You are helpful assistant. following evaluation criteria: For this question, please consider the IMPORTANT POINTS TO INCLUDE (you should aim to address these): <<criterion1>> <<criterion2>> <<criterion3>> ... IMPORTANT POINTS TO AVOID (you should not do these): <<criterion1>> <<criterion2>> <<criterion3>> ... Please provide comprehensive and helpful response that addresses the users concerns while following the above guidelines. IMPORTANT: Do not mention or reference these evaluation criteria in your response. rubric or evaluation guidelines. natural and spontaneous. Do not indicate that you have seen any scoring Revealing that you have access to Your response should appear 21 evaluation criteria would be considered cheating and is strictly prohibited. D.3 DATA GENERATION PROMPT TEMPLATE The following is the prompt template used for generating HealthBench-like rubrics data for reasoning tasks (see Table D.3). Table D.3: Data Generation Prompt Template You are an expert in educational assessment and rubric design. Your task is to analyze given question-answer pair and generate comprehensive evaluation rubrics that can be used to assess response quality. # Input Data # Question <<question>> # Answer <<answer>> # Task Instructions Based on the provided question and answer, generate comprehensive rubric with multiple evaluation criteria. 1. **Specific and Measurable**: meeting or not meeting the criterion 2. **Binary Evaluable**: evaluator 3. **Comprehensive Coverage**: the key aspects of high-quality response Can be assessed as true/false by an LLM Clearly define what constitutes Together, all criteria should cover Each criterion should be: # Required Rubric Categories Generate criteria covering these aspects: - **Factual Accuracy**: information, and domain-specific content - **Solution**: and methodology - **Answer Consistency**: with expected results (if applicable) - **Format Compliance**: specified format requirements (if applicable) Evaluate the correctness of facts, Evaluate the reasonableness of logical reasoning Verify whether the answer is consistent Check whether the model output conforms to # Output Format Return JSON object with the following structure: json { \"rubrics\": [ { \"criterion\": \"The response contains accurate facts and domain-specific content without errors\", \"points\": }, { \"criterion\": \"The response demonstrates clear understanding of underlying principles and relationships\", \"points\": 8 }, { \"criterion\": \"The response uses logical reasoning and appropriate methodology\", 22 \"points\": 7 }, { \"criterion\": \"The response contains factual errors or misinformation\", \"points\": -5 }, { \"criterion\": \"The response is completely off-topic or irrelevant\", \"points\": -10 }, // ... additional criteria ] } # Important Guidelines - Generate 5-15 criteria total, ensuring comprehensive coverage - Points should reflect the relative importance of each criterion (supports positive scores from 1 to 10 for reward criteria, and negative scores from -10 to -1 for penalty criteria) Return only the JSON object without additional commentary."
        }
    ],
    "affiliations": [
        "Li Auto Inc.",
        "Nanyang Technological University",
        "The Chinese University of Hong Kong, Shenzhen",
        "Zhejiang University"
    ]
}