{
    "paper_title": "RM -RF: Reward Model for Run-Free Unit Test Evaluation",
    "authors": [
        "Elena Bruches",
        "Daniil Grebenkin",
        "Mikhail Klementev",
        "Vadim Alperovich",
        "Roman Derunets",
        "Dari Baturova",
        "Georgy Mkrtchyan",
        "Oleg Sedukhin",
        "Ivan Bondarenko",
        "Nikolay Bushkov",
        "Stanislav Moiseev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization."
        },
        {
            "title": "Start",
            "content": "RM -RF: Reward Model for Run-Free Unit Test Evaluation 6 2 0 2 9 1 ] . [ 1 7 9 0 3 1 . 1 0 6 2 : r Elena Bruches, Daniil Grebenkin, Mikhail Klementev, Vadim Alperovich, Roman Derunets, Dari Baturova Georgy Mkrtchyan, Oleg Sedukhin, Ivan Bondarenko, Nikolay Bushkov, Stanislav Moiseev Siberian Neuronets LLC, Novosibirsk, Russia T-Technologies, Moscow, Russia Novosibirsk State University, Novosibirsk, Russia {bruches, grebenkin, klementev, derunets, baturova, sedukhin}@sibnn.ai {v.alperovich, g.p.mkrtchyan, n.bushkov, s.moiseev}@t-tech.dev i.bondarenko@g.nsu.ru AbstractWe present RM -RF, lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM -RF predicts from source and test code alone three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM -RF we assemble multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM -RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization. Index Termsunit test generation, large language models (LLM), software engineering automation, LLM4SE, reinforcement learning I. INTRODUCTION Automatic code generation is the autonomous creation of software artifacts using predefined algorithms or intelligent systems. Historically, automated code generation heavily relied on rule-based engines and templating mechanisms [1], [2], limiting their application to simple, repetitive tasks requiring precise specification of logic by developers. However, over the past decade there has been an exponential increase in the capability of automatic code generation driven by advancements in large-scale language models (LLMs) integrated with deep learning and natural language processing technologies. State-of-the-art models, such as GPT [3], DeepSeek [4], Qwen [5] and others, demonstrate exceptional proficiency in interpreting natural-language instructions and synthesizing executable code that resolves complex programming challenges, leading to gains in programmer productivity. Despite these breakthroughs, ensuring alignment between generated code and diverse user requirements remains critical issue. Sophisticated methodologies, particularly that employ reinforcement learning (RL), integrate direct human feedback into the training loop, enabling fine-tuned outputs that conform more closely to practical usage contexts [6]. Nonetheless, gathering high-quality feedback incurs substantial costs in terms of effort and resources. Another promising feature of RL-based approaches is their ability to exploit feedback directly from compilation [7] or unit testing [8] stages during runtime. This mitigates dependency on manual oversight, simultaneously improving precision and scalability across various code generation tasks. Finally, traditional validation measures, such as unit tests that compare produced outputs with expected results, provide robust means of evaluating correctness. However, executing build pipelines, running unit tests, and computing relevant metrics impose significant computational overhead, delaying the overall training and inference processes. In light of these challenges, our study investigates an alternative method to reduce latency and scale up the adoption of automatic code generation technologies, contributing valuable insights toward advancing this field further. We propose RM -RF, simple and scalable approach to guide the unit test generation that evaluates the impact of each generated test case. The approach applies the pre-trained model that removes the need to compile and run tests each time during project testing stage. This model is trained to predict whether the particular test case makes the whole test suite correct (1) (i.e. it compiles and runs successfully), improves the code coverage (2) and mutation kill rate (3) or not. To train this model, we prepared the dataset, which includes samples for three programming languages (Java, Python, and Go), both human-written and generated. To obtain the target values, all samples were run through specialized tools for the estimation of code coverage and mutation kill rate. Each sample includes the focal file and test file contents as context and new added test case, which is intended to be evaluated. We conducted series of experiments, using different family and size models such as Qwen and Codestral. Moreover, we experienced with different types of targets. We compared zeroshot, full-parameter fine-tuning, and PEFT (parameter-efficient fine-tuning) with LoRA [9], achieving the 0.69 average F1score on three targets. Providing the necessary feedback in the form of set of metrics, one can assess whether the test case is useful or not while training the model or generating the code snippet. The key contributions of this work are as follows: 1) Curated Real-world Dataset: We curate and prepare comprehensive dataset specifically designed for predicting (1) code correctness, (2) code coverage ratios, and (3) mutation kill rates. This dataset comprises welldefined training and validation splits. Additionally, we create dedicated holdout set containing test cases generated by state-of-the-art LLMs, ensuring robust evaluation under realistic application conditions. 2) RM -RF: We introduce lightweight yet powerful reward model engineered to assess the utility of generated test cases efficiently. Designed to support Java, Python, and Go simultaneously, RM -RF eliminates the need for project builds and runtime executions, thereby reducing latency and resource costs significantly. 3) Empirical Analysis of SOTA LLMs: We evaluate advanced LLMs in both zero-shot and fine-tuning modes to get deeper insight how well they may process such task. The remainder of this paper is organized as follows. Section II reviews relevant works. Section III presents our dataset design. Section IV describes the experiments with RM -RF model. Section reports our experimental results. Lastly, we discuss limitations and highlight directions for future work. II. BACKGROUND A. Automated Test Generation Early automated unit test tools aimed at maximizing code coverage, such as Randoop [10] and EvoSuite [11], laid the foundation for modern techniques. With the advent of large language models (LLMs), researchers began treating unit test generation as generative task. For example, [12] used sequence-to-sequence Transformer-based model (pretrained on code and text) to generate tests for Java. More recent work employs prompting and chain-of-thought strategies: ChatTester [13] leverages ChatGPT iteratively to refine self-generated tests, Libro [14] applies post-processing steps to filter and rank LLM-generated tests. These approaches enable LLMs to generate or refine tests, but they often rely on heuristic prompting and struggle to consistently improve upon initial outputs. B. Evaluation of the Test Quality Recent studies and benchmarking efforts focus on evaluating LLMs ability to produce meaningful and executable tests, moving toward practical test quality metrics. For example, TestBench [15] evaluates LLMs across five key dimensions: syntactic correctness, compilation success, test validity, code coverage, and defect-detection rates. TestEval [16] examines three distinct dimensions: overall code coverage (cov@k), targeted line and branch coverage, and path coverage. These toolkits are usually targeting individual functions, have no support for assessing complete test suites at the file or repository level, and need time-consuming manual annotation or resorting to static ground-truth test oracles. Although simple to compute, code coverage alone does not reliably indicate test suites ability to find real bugs. Mutation testing [17], by contrast, injects artificial mutants (bugs) and measures the percentage caught (killed) by the tests. higher mutation score typically reflects stronger tests. For example, MuTAP [18] shows that prompt augmentation, with surviving mutants improves test effectiveness, and MutGen [19] incorporates mutation feedback into its prompts to generate tests with high fault detection. recent large-scale industrial study, Mutation-Guided LLM-based Test Generation at Meta (ACH) [20], reinforces the practical value of mutation-based feedback. ACH integrates mutant generation, equivalent mutant detection, and LLM-based test synthesis, achieving over 70% engineer acceptance rate across thousands of Android classes. Notably, many accepted tests did not increase line coverage yet substantially improved mutation scores highlighting that coverage alone is an unreliable measure of test adequacy. In addition, several benchmarks emulate real-world scenarios. DevBench [21] simulates the entire software development lifecycle (SDLC), integrating tasks such as software design, environment setup, implementation, acceptance testing, and [22] prioritizes the genunit testing. Similarly, TestGenEval eration of comprehensive test suites, stressing initial test authorship, suite expansion, and coverage improvement. Our solution addresses the complete test maintenance cycle generation, repair, and update of test suites. Unlike prior work focused on individual functions or test cases, it operates at the granularity of entire test files, reflecting realistic developer workflows. C. Reinforcement Learning and Reward Modeling in Code Generation Traditional outcome-based RL gives sparse rewards only when complete program passes tests. To provide denser feedback, recent work on process-supervised reinforcement learning for code generation [23] automatically labels each line of generated code by compiling mutated versions, training process-based reward model (PRM) that scores the thought process of code generation. Integrating this PRM into the RL loop (in PRLCoder) significantly outperforms outcome-only approaches, especially on complex tasks. Similarly, another recent work on process supervisionguided policy optimization [24] trains line-level PRM that imitates human debugging by delivering immediate feedback on partial code, leveraging it as dense reward signal during training. Moreover, StepCoder [25] introduces an RL framework that decomposes long programs into curriculum of code completion subtasks, applying fine-grained optimization by masking unexecuted segments. Using compiler feedback and verified datasets to guide learning, StepCoder achieves stronger exploration and competitive benchmark results. CodePRM [26] leverages execution results to label reasoning traces, collecting step-by-step thoughtcode pairs with pass/fail outcomes and training model to score each reasoning step. Within generateverifyrefine pipeline, it identifies reasoning errors and corrects them, yielding substantial gains over baseline methods. ÂµCode [27], building on the Markov Decision Process (MDP) formulation [28], treats code generation as one-step recoverable MDP: generator proposes code, verifier evaluates it via execution feedback, and the two alternate in closed loop. This single-step reward scheme significantly outperforms prior hierarchical RL approaches. Outcome Refining Process Supervision (ORPS) [29] unifies process and outcome rewards via tree search: it generates alternative code paths, profiles their execution, and performs self-critique, improving correctness by 26.9% and enhancing code generation efficiency. These RL-based techniques employ compiler and execution feedback often through tests to shape code generation, producing more accurate and robust programs. Lastly, Regression Language Models for Code [30] introduces model that predicts quantitative code metrics such as memory footprint, GPU kernel latency, and model accuracy directly from source text without execution. This approach aligns with our goal of estimating execution-derived test signals in run-free manner, demonstrating that text-based inference of code behavior can be both accurate and scalable. D. Learning and Testing via Adversarial RL Reinforcement learning has also been applied directly to unit test generation. For example, UTRL [31], an adversarial framework where two LLMs co-train: test-generator LLM learns to produce tests that catch the code-generator LLMs bugs, and the code-generator learns to satisfy those tests. In experiments, Qwen-3B model trained with UTRL generated tests of higher quality than supervised fine-tuned model, even outperforming GPT-4.1 on test generation. UTGen [32] tackles the debugging scenario: authors train model to create unit tests (inputs and expected outputs) that reveal flaws in faulty code, and UTDebug to iteratively use those tests for debugging. UTGen surpasses other LLM baselines in producing error-revealing tests, and incorporating UTGens feedback improves Qwen-32Bs pass rates on several benchmarks and demonstrates that UTGen is better judge of code correctness. In parallel, researchers have begun co-evolving the coder and tester. CURE [33] framework trains LLM-based code together under RL: generator and LLM-based unit the coder writes solutions, the tester writes unit tests, and each learns from the others feedback without ground-truth code. Their ReasonFlux-Coder models show notable gains over similarly sized models like DeepSeek-Coder and QwenCoder, and the tester naturally transfers to improved test-time scaling. tester. LLMs are increasingly being taught to generate, critique, and improve both programs and tests, using reinforcement signals from execution. Combining iterative critique (e.g. RefineCoders [34] LLM-as-critic strategy and CTRL [35] models trained to judge code outputs) with RL and co-evolution holds promise for more reliable code and test generation in the future. To bridge the gap between informative execution signals and practical scalability, we introduce RM -RF approach, lightweight pre-trained reward model that predicts without compiling or running whether candidate unit test will allow the suite to compile and run, increase code coverage, and improve mutation kill rate. RM -RF is trained on multilingual dataset (Java, Python, Go) whose labels were obtained by executing each sample through our evaluation pipeline; we train and compare multiple model sizes and tuning regimes. III. DATASET CONSTRUCTION This section describes the motivation for choosing the target the features for the Holdout dataset and training dataset, process of unit test sample data collection, and splitting into subsets according to the target values. A. Data evaluation The set of data target features was chosen to show the unit test quality in the terms of its performance, focal file logic coverage and mutation robustness, and further filtration. These features of the tests can be measured by their correctness, coverage delta value, and mutation coverage delta value. Test correctness. The positive label of test correctness considers lack of any type of runtime or syntax errors during unit test performance. It was estimated by the extraction of test running logs and their analysis. Line coverage delta value (TestCov). The line-level coverage is defined as follows: TestCov = # lines executed by tests # total executable lines in the focal file 100, (1) while TestCov is defined as the difference in line-level coverage between the initial coverage ratio of existing test cases and the final coverage ratio after adding the new test to the existing test suite: TestCov = TestCovfinal TestCovinitial (2) The calculation of line coverage values depended on the project programming language: we used coverage.py1 for Python, JaCoCo2 plugin for Java and cover 3 package for Go tests. Mutation coverage delta value (MutCov). The aim of mutation analysis is to evaluate test suite by introducing small, systematic changes (mutations) into the program under test and checking whether the tests detect the injected faults. Therefore, the mutation coverage delta value or mutation kill rate is estimated as proportion of detectable (or failed) mutants that are killed by the test suite: 1https://github.com/nedbat/coveragepy 2https://github.com/jacoco/jacoco 3https://pkg.go.dev/cmd/cover MutCov = # failed mutants # total mutants 100, (3) Consequently, the mutation coverage delta value is estimated in similar way to the line coverage delta value: MutCov = MutCovfinal MutCovinitial (4) There are few set of mutation analysis tools for every language. In our experiments it was performed with mutpy4 library for Python, PIT5 mutation testing system for Java and go-mutesting6 module for Go projects. According to these set of metrics, every correct test that has mutation coverage delta value or line coverage delta value greater than zero is considered as well-written and useful. B. Data collection The data collection pipeline for Holdout dataset and training dataset included several steps: GitHub project selection and downloading, project execution-based filtering and files content-based filtering. 1) Project selection: We sourced open-source GitHub repositories using the GitHub API, applying set of criteria to ensure that: The repository contains test files with at least five focaltest file pairs; Data has not yet been included in LLM training sets (at least for the time being). The repositories for Holdout dataset had to show updates after January 1, 2020, and the specific source files needed their latest commit to be after January 1, 2025 (or January 1, 2024 for Java). The training dataset projects had to be updated at least after January 1, 2023. Data are available for usage according to type of provided license, like permissive MIT and Apache-2.0; There is confirmed usage experience of this projects, so for Holdout dataset we only considered repositories with minimum of 40 stars (for training dataset >5 stars respectively) and feature contributions from at least two developers. 2) Project execution-based filtering: The training convergence and evaluation of reward model depend on the lack of dependencies which are not connected with the project setup. Thus, we excluded projects that necessitate additional dependencies, installation instructions, or the downloading of extra files for execution, ensuring the dataset remains scalable and reproducible. The unit tests were also executed twice to estimate performance stability and measure the runtime. The tests that run more then 30 seconds were excluded from the set to guarantee reasonable inference time during evaluation stage. 4https://github.com/mutpy/mutpy 5https://pitest.org 6https://github.com/avito-tech/go-mutesting 3) Content-based filtering: The last filtering stage included several steps to make samples suitable for training and evaluation: The samples with fewer than two test cases were excluded; Removed pairs without at least one function of five+ executable lines; Discarded pairs with focal/test files <20 lines or above the 99th percentile (ignoring comments/blank lines); Removed automatically generated files to keep only human-authored samples; Excluded samples with >70% comments in the focal file to focus on executable content. Finally, we evaluated suite of large language models (LLMs) on Holdout dataset, which presents the realworld data. The models tested included DeepSeek-3.1 [4], Gemini-2.5-flash [36], Devstral-small [37], GPT-5 [38], GPT-OSS-120B [39], and Qwen3-Coder [5]. The tests generated by these models were then assessed using our evaluation pipeline to derive the relevant target metrics. C. Training dataset structure Fig. 1. Distribution of training dataset samples across programming languages The structure of the dataset used for training and validation is designed to improve the robustness of reward model to different types of unit tests. The final set consists of 22285 samples and includes five training subsets and single validation one, the samples were split into training and validation subsets with no repository overlap. The distribution of dataset samples across programming languages is presented on Figure 1. The samples were divided into several groups, based on test correctness labels and the origin of the unit test: there are human-written tests (with the humanprefix) and LLMwritten tests (with the generatedprefix). The human-written tests include two sets: human correct (3303 samples, 14.82% of total dataset). The samples from collected data which have successfully and syntactically correct existing tests and new tests. human broken (4349 samples, 19.51% of total dataset). The samples in this subset emulate spectrum of potential errors commonly found in software code, showcasing how different LLMs address various types of test-related bugs. These test errors were generated using different LLMs, intentionally designed to disrupt tests based on specific input prompts. This set of samples includes instances of syntax errors, diverse runtime errors (e.g., invalid calls, missing dependencies), tests with redundant logic, tests lacking verification logic, and other scenarios. The generated part of the dataset was prepared with use of model Qwen2.5-Coder-32B-Instruct [40]. The goal of the generation prompt for this model is to produce adtests for the specified focal file, building on ditional unit the existing test code. This involves ensuring coverage of all key functionalities, exercising edge cases, adhering to testing best practices, and using fixtures and parameterization where appropriate. Processing of generation results considers identifying all test cases in each test file. Then, for every generated test case, we construct an isolated code variant by removing all other generated test cases. This procedure yields dataset comprising multiple versions per test file, with each version containing exactly one test case. All of these samples were finally evaluated and divided to groups, based on test correctness metrics. Therefore, the following subsets are created:"
        },
        {
            "title": "Reward Model Unit Test Evaluation Instruction",
            "content": "You are code assistant that accepts {lang} source file, test file and new test case. Your goal is to validate that the new test case is valid, and that it will increase the code coverage of the source file if it will be added to the existing test suite. Source file: {source_file_content} Test file: {test_file_content} New test case: {new_test_code} The output must be YAML object equivalent to type TestValidation, according to the following Pydantic definitions: class TestValidation(BaseModel): is_new_test_correct: bool increase_coverage: bool increase_mutation_kill_rate: bool generated correct total dataset). These samples have successfully passed and syntactically correct generated tests. (11543 samples, 51.79% of generated not correct (1486 samples, 6.66% of total these tests have dataset). Unlike the previous group, various runtime or syntax errors, which occur during evaluation. The last group - no existing test cases (396 samples, 1.77%) - is created from the human samples and generated, which do not have the existing test cases filled. This subset aims to prevent the LLMs hallucination in the case when there is only single new test. The validation subset has got 1192 samples (5.34% of total dataset). IV. EXPERIMENTS A. Training configuration We employ the Swift framework [41] for fine-tuning and the vLLM library [42] for efficient inference. For smaller models (7B parameters), we perform full supervised fine-tuning (SFT), while for larger models (20B parameters) we apply low-rank adaptation (LoRA) [43] to enable parameter-efficient finetuning. We utilized NVIDIA A100 GPUs for all experiments. Hyperparameters were selected empirically based on preliminary runs. SFT was conducted for 2 epochs with learning rate of 1 105. LoRA fine-tuning was performed for 3 epochs with learning rate of 3 105, an adapter rank of 64, and an alpha coefficient of 16. Training experiments were carried out using the Brain Floating Point (BF16) half-precision format [44] and the Adafactor optimizer [45]. Fig. 2. Reward model unit test evaluation instruction (simplified version of prompt for binary targets) B. Model training prompt The input prompt (Figure 2) for models training consists of the focal file code, existing test cases file, and the new test file presented in diff-style format. This version of the unit test adopts format inspired by tools such as aider7 and other LLM-assisted code editing frameworks. C. Training targets The training targets for the reward model are defined at the end of each input prompt. We hypothesize that the choice of target formulation can directly influence the models performance. Accordingly, we examined multiple target formulations and constructed the corresponding versions of the dataset: and Binary targets: correctness (whether executes the test successfully), coverage compiles increase (whether the test changes covers larger the source code logic), and mutation portion of increase (whether the modification improves the mutation score and detects more potential bugs); Float targets: same for the correctness, while coverage and mutation correspond to the TestCov and MutCov targets respectively, as defined in Section III-A. On the initial stage, we also conducted experiments with reversed binary targets, where the model predicts true for coverage and mutation when the corresponding metrics 7https://github.com/Aider-AI/aider decrease. However, in the final experiments, we used only the standard binary targets, as their behavior was similar to the reversed variant. It is important to note that the targets appear to be interrelated (for instance, the values of TestCov and MutCov heavily rely on the correctness of tests). To validate this observation, we computed the correlations among these targets and discovered that they exhibit weak positive correlations (0.21 between Correctness and MutCov, 0.19 between Correctness and TestCov, and 0.35 between TestCov and MutCov). Among all programming languages analyzed, the correlation between TestCov and MutCov proves to be the most substantial. Additionally, an interesting finding is that the strength of target correlations is highest for the Go language, whereas it diminishes across other languages. V. EVALUATION Both fine-tuned models and zero-shot models were evaluated on the validation dataset, whose characteristics are in section: III-C. The evaluation results and quality metrics are reported in Table I. In addition, the testing was conducted on an extended sample; the methodology for this stage is described below. The reward models produced predictions for two types of targets: binary targets, directly predicting correctness and whether coverage or mutability improved (True/False); and float targets, where continuous gain values were binarized for evaluation (True if > 0, False otherwise). The model performance was evaluated using weighted F1score; class weights were computed based on label frequencies; each sample was assigned the weight of its true class; and weighted Precision and Recall were used to compute the final F1-score. The best performing model, selected for highest mean accuracy, served as the reference model. All other models were compared against it using the Wilcoxon signed-rank test: for each example, the prediction differences were encoded as +1 (model correct, reference wrong), 1 (model wrong, reference correct), or 0 (agreement). The test statistic was defined as the sum of ranks for cases where the model underperformed the reference. value of < 0.05 indicated statistically significant underperformance. The models were tested in the validation subset using this method. Codestral-22B-v0.1 and Qwen2.5-Coder-7B-Instruct proved to be the best for binary and float targets. The primary objective of RM -RF is to reduce latency while maintaining comparable level of quality when generating and evaluating tests. To substantiate this claim, we conducted an empirical comparison between the evaluations produced by RM -RF and those derived from executing traditional testing methods and tools designed to measure both coverage and mutation kill rates. The fine-tuned Qwen2.5-Coder-7B-Instruct was evaluated as RM - RF. We computed the ratio of the useful generated test cases (it should be correct and increase either coverage or mutation kill rate). The findings are summarized in Table III. Lower absolute differences indicate that RM -RF predictions closely align with actual execution outcomes, suggesting that RM -RF can effectively substitute full-scale test runs without losing accuracy. Furthermore, since RM -RF eliminates the need for building projects and executing source code, it offers substantial savings in terms of computational resources and processing time. VI. DISCUSSION Across settings, fine-tuning consistently improves predictive quality over zero-shot baselines (Table I). The best average F1 is achieved by Qwen2.5-Coder-7B-Instruct after SFT (0.69) on binary targets, with particularly strong gain on TestCov (0.76). Binary targets provide robust and easy-to-learn supervision signal, yielding the strongest average results on the validation subset. Float targets binarized for evaluation show slightly lower mean scores on validation but generalize better on the Holdout dataset (Table II), achieving equal or higher results for each metric. This suggests that continuous supervision helps the model capture graded relationships between test modifications and execution outcomes, improving transfer to unseen projects. Early experiments with reversed binary targets exhibited behavior similar to the standard binary formulation but added complexity without consistent benefit; we therefore retain binary and float variants as the main targets. On the Holdout dataset, RM -RF also demonstrates consistent cross-language performance, maintaining stable accuracy across Go, Java, and Python. While MutCov prediction improvements remain most pronounced for Java (up to 0.71 on binary targets), TestCov prediction shows smaller variance across languages. These trends likely stem from structural and tooling differencessuch as languagespecific mutation operators, test frameworks, and code base organization. Together with the stronger holdout performance of float targets, these results indicate that the model captures generalizable relationships between code and test dynamics rather than overfitting to single language or dataset distribution. Full SFT on 7B model outperforms PEFT LoRA on larger backbones in our regime (e.g., 7B SFT average F1 of 0.69 vs. 14B LoRA at 0.63 on binary targets). This points to data/compute sweet spot, where full adaptation of moderately sized model yields better alignment to our task than partial adaptation of larger models. LoRA remains competitive for some metrics and is attractive when memory budgets are tight, but in our experiments, SFT provided the most reliable gains. Analyzing the models performance on various subsets indicates that the subset generated not correct (samples intentionally designed to include errors) poses the greatest challenge for determining test correctness. This difficulty might stem from the fact that the generated code has never been seen by any trained models before. closer examination of error types reveals that issues like Missed Dependencies and TABLE RESULTS ON VALIDATION SUBSET"
        },
        {
            "title": "Binary targets",
            "content": "Zero-shot Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Codestral-22B-v0.1 Fine-tuning Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct (LoRa) Codestral-22B-v0.1 (LoRa) Zero-shot Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Codestral-22B-v0.1 Fine-tuning Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Codestral-22B-v0.1 Zero-shot Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Codestral-22B-v0.1 Fine-tuning Qwen2.5-1.5B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-Coder-14B-Instruct Codestral-22B-v0. 0.52 0.62 0.64 0.59 0.54 0.60 0.62 0.49 0.53 0.56 0.61 0.45 0.53 0.59 0.62 0.51 0.68 (+0.16) 0.69 (+0.07) 0.65 (+0.01) 0.68 (+0.09) 0.68 (+0.14) 0.76 (+0.16) 0.68 (+0.06) 0.66 (+0.17) 0.61 (+0.08) 0.63 (+0.07) 0.55 (-0.06) 0.57 (+0.12) 0.65 (+0.12) 0.69 (+0.10) 0.63 (+0.01) 0.63 (+0.12)"
        },
        {
            "title": "Float targets",
            "content": "0.52 0.62 0.62 0.56 0.51 0.53 0.59 0.51 0.52 0.48 0.49 0.51 0.52 0.54 0.56 0.52 0.66 (+0.14) 0.68 (+0.06) 0.64 (+0.02) 0.67 (+0.11) 0.67 (+0.16) 0.74 (+0.21) 0.65 (+0.06) 0.67 (+0.16) 0.58 (+0.06) 0.64 (+0.16) 0.57 (+0.08) 0.57 (+0.06) 0.64 (+0.12) 0.68 (+0.14) 0.62 (+0.06) 0.63 (+0.11)"
        },
        {
            "title": "Reverse binary targets",
            "content": "0.54 0.63 0.65 0.62 0.49 0.54 0.59 0.53 0.46 0.55 0.57 0.47 0.49 0.57 0.60 0.54 0.68 (+0.14) 0.69 (+0.06) 0.64 (-0.01) 0.65 (+0.03) 0.67 (+0.18) 0.75 (+0.21) 0.65 (+0.06) 0.61 (+0.08) 0.58 (+0.12) 0.65 (+0.10) 0.53 (-0.04) 0.54 (+0.07) 0.64 (+0.15) 0.69 (+0.12) 0.60 (+0.0) 0.60 (+0.06) TABLE II RESULTS FOR HOLDOUT DATASET"
        },
        {
            "title": "Model",
            "content": "Go"
        },
        {
            "title": "Python",
            "content": "Cor. TestCov MutCov Cor. TestCov MutCov Cor. TestCov MutCov Qwen2.5-Coder-7B-Instruct (SFT) Codestral-22B-v0.1 (LoRa) 0.57 0.59 Qwen2.5-Coder-7B-Instruct (SFT) Codestral-22B-v0.1 (LoRa) 0.62 0."
        },
        {
            "title": "Binary targets",
            "content": "0.58 0.52 0.55 0."
        },
        {
            "title": "Float targets",
            "content": "0.61 0.58 0.61 0.53 0.54 0.50 0.57 0.54 0.60 0.46 0.61 0. 0.71 0.37 0.68 0.50 0.55 0.61 0.64 0.56 0.56 0.52 0.61 0. 0.59 0.49 0.67 0.55 TABLE III COMPARE RM -RF WITH EXECUTION-BASED METRICS"
        },
        {
            "title": "Model",
            "content": "Go"
        },
        {
            "title": "Overall",
            "content": "Exec. RM -RF Exec. RM -RF Exec. RM -RF Exec. RM -RF GPT-OSS-120B Devstral-small Gemini-2.5-Flash Qwen3-Coder GPT-5 DeepSeek-3.1 0.56 0.21 0.17 0.52 0.67 0.19 0.40 0.29 0.41 0.34 0.41 0.25 0.16 0.08 0.24 0.18 0.26 0. 0.02 0.01 0.01 0.02 0.07 0.01 0.18 0.05 0.14 0.13 0.27 0.06 0.16 0.04 0.13 0.11 0.20 0.05 0.20 0.05 0.04 0.18 0.19 0.05 0.24 0.07 0.22 0.26 0.28 0.19 0.04 0.02 0.18 0.08 0.09 0. 0.30 0.10 0.08 0.30 0.36 0.09 0.29 0.16 0.27 0.27 0.33 0.17 0.01 0.06 0.19 0.03 0.03 0.08 TABLE IV SPEARMANS RANK CORRELATION COEFFICIENT these directions could further enhance both the scalability and precision of reward modeling for automated test generation."
        },
        {
            "title": "Overall",
            "content": "0.79 0.91 0.89 0.86 0.4 0.6 0.6 0."
        },
        {
            "title": "High",
            "content": "Duplicated Entity are particularly hard for the model to detect, whereas errors such as Invalid Constructor, Undefined Entity, Runtime Errors, and Invalid Call are relatively easier for the model to identify. To further quantify alignment with execution-based evaluations, we computed both NDCG and Spearmans rank correlation [46] between RM -RF predictions and actual executionderived metrics (Table IV). The results show moderate to high monotonic correlation across languages, with coefficients of 0.4 for Go and 0.6 for both Java and Python, and an overall correlation of 0.74. This indicates that RM -RF preserves relative ranking among test cases by quality, reinforcing its potential as fast surrogate for execution-based evaluation. Because RM -RF eliminates build and execution in the evaluation loop, it substantially reduces latency and compute cost while maintaining close alignment with execution-derived outcomes. This efficiency enables larger candidate pools for filtering, faster iterative refinement, and more frequent feedback in RL-based code optimization pipelinesparticularly valuable when mutation testing is the primary bottleneck. To demonstrate this point, the 22B model predicts all samples within less than three hours, whereas complete building and executing steps take several days to yield the final results. Building on these results, several extensions appear especially promising for future work: (1) scaling supervised fine-tuning (SFT) to larger models beyond 7B parameters; (2) integrating RM -RF directly into reinforcement learning loops as reward signal for code generation; (3) exploring curriculum-style supervision that combines binary and float targets (e.g., starting with binary and subsequently fine-tuning on float deltas) to further stabilize mutation and coverage predictions; (4) comparing per-language fine-tuning of specialist reward models against single multilingual variant. Together, VII. LIMITATIONS The present work has several limitations. First, the dataset and models are limited by only three programming languages. However, the coverage across languages should be increased. Second, we did not conduct the experiments in which one model is trained only for one programming language. From one point of view, it could increase the performance. From the other point of view, it would be more difficult to maintain such systems in practice. Third, the reward model was not tested in the RL pipeline. It should be incorporated into the RL loop to increase the quality of the generated tests and speed up the training and generation process. Nevertheless, our experiments showed that the assessment from the RM -RF is close to the traditional tools that proves our concept. Fourth, our dataset uses publicly available open-source code, potentially introducing partial data leakage if models encountered this code during pre-training. Nevertheless, we argue that the models had not been specifically trained to address these exact tasks; thus, even if they met similar code, it would not constitute data leakage. VIII. CONCLUSION In this work, we have introduced RM -RF, novel lightweight reward model tailored for efficient evaluation of automatically generated unit tests without requiring compilation or execution. By predicting key metrics such as successful compilation, increased code coverage, and improved mutation kill rates directly from source and test code, RM -RF achieves significant reductions in latency and resource consumption. Our extensive experiments on diverse multilingual dataset demonstrate its effectiveness across different programming languages (Java, Python, Go) and model configurations, yielding strong predictive performance with an average F1 score of 0.69. These results highlight RM -RFs potential to serve as practical alternative to conventional compile-and-run approaches, providing rapid, scalable, and accurate feedback essential for advancing automated test generation and reinforcement learning-driven code optimization efforts. IX. DATA AVAILABILITY"
        },
        {
            "title": "All",
            "content": "code, data, reproducing our for https://github.com/trndcenter/RM-RF-unit-tests. are publicly available and experimental details necessary at results"
        },
        {
            "title": "REFERENCES",
            "content": "[1] G. Little and R. C. Miller, Keyword programming in java, ser. ASE 07. New York, NY, USA: Association for Computing Machinery, 2007, p. 8493. [Online]. Available: https://doi.org/10.1145/1321631.1321646 [2] T. Gvero and V. Kuncak, Interactive synthesis using free-form queries, in Proceedings of the 37th International Conference on Software Engineering - Volume 2, ser. ICSE 15. IEEE Press, 2015, p. 689692. [3] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman et al., Gpt-4 technical report, 2024. [Online]. Available: https://arxiv.org/abs/2303.08774 [4] DeepSeek-AI, technical Available: https://arxiv.org/abs/2412.19437 Deepseek-v report, 2024. [Online]. [5] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, 2025. [Online]. Available: https://arxiv.org/abs/2505.09388 [6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe, Training language models to follow instructions with human feedback, in Proceedings of the 36th International Conference on Neural Information Processing Systems, ser. NIPS 22. Red Hook, NY, USA: Curran Associates Inc., 2022. [7] Z. Bi, Y. Wan, Z. Wang, H. Zhang, B. Guan, F. Lu, Z. Zhang, Y. Sui, H. Jin, and X. Shi, Iterative refinement of project-level code context for precise code generation with compiler feedback, in Findings of the Association for Computational Linguistics: ACL 2024, L.-W. Ku, A. Martins, and V. Srikumar, Eds. Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 23362353. [Online]. Available: https://aclanthology.org/2024.findings-acl.138/ [8] J. Liu, Y. Zhu, K. Xiao, Q. Fu, X. Han, W. Yang, and D. Ye, Rltf: Reinforcement learning from unit test feedback, 2023. [Online]. Available: https://arxiv.org/abs/2307.04349 [9] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, 2021. [Online]. Available: https://arxiv.org/abs/2106.09685 [10] C. Pacheco and M. D. Ernst, Randoop: feedback-directed random java, in Companion to the 22nd ACM SIGPLAN testing for Conference on Object-Oriented Programming Systems and Applications Companion, ser. OOPSLA 07. New York, NY, USA: Association for Computing Machinery, 2007, p. 815816. [Online]. Available: https://doi.org/10.1145/1297846.1297902 [11] G. Fraser and A. Arcuri, Evosuite: automatic test suite generation for object-oriented software, in Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering, ser. ESEC/FSE 11. New York, NY, USA: Association for Computing Machinery, 2011, p. 416419. [Online]. Available: https://doi.org/10.1145/2025113.2025179 [12] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sundaresan, Unit test case generation with transformers and focal context, 2021. [Online]. Available: https://arxiv.org/abs/2009.05617 [13] Z. Yuan, M. Liu, S. Ding, K. Wang, Y. Chen, X. Peng, and Y. Lou, test generation, Proc. Evaluating and improving chatgpt for unit ACM Softw. Eng., vol. 1, no. FSE, Jul. 2024. [Online]. Available: https://doi.org/10.1145/ [14] S. Kang, J. Yoon, and S. Yoo, Large language models are few-shot testers: Exploring llm-based general bug reproduction, in Proceedings the 45th International Conference on Software Engineering, ser. of IEEE Press, 2023, p. 23122323. [Online]. Available: ICSE 23. https://doi.org/10.1109/ICSE48619.2023.00194 [15] Q. Zhang, Y. Shang, C. Fang, S. Gu, J. Zhou, and Z. Chen, Testbench: Evaluating class-level test case generation capability of large language models, arXiv preprint arXiv:2409.17561, 2024. [16] W. Wang, C. Yang, Z. Wang, Y. Huang, Z. Chu, D. Song, L. Zhang, A. R. Chen, and L. Ma, TESTEVAL: Benchmarking large language models for test case generation, in Findings of the Association for Computational Linguistics: NAACL 2025, L. Chiruzzo, A. Ritter, and L. Wang, Eds. Albuquerque, New Mexico: Association for Computational Linguistics, Apr. 2025, pp. 35473562. [Online]. Available: https://aclanthology.org/2025.findings-naacl.197/ [17] P. G. Frankl, S. N. Weiss, and C. Hu, All-uses vs mutation J. Syst. comparison of testing: Softw., vol. 38, no. 3, p. 235253, Sep. 1997. [Online]. Available: https://doi.org/10.1016/S0164-1212(96)00154-9 an experimental effectiveness, [18] A. M. Dakhel, A. Nikanjam, V. Majdinasab, F. Khomh, and M. C. Desmarais, test generation using pre-trained large language models and mutation testing, Information and Software Technology, vol. 171, p. 107468, 2024. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0950584924000739 Effective [19] G. Wang, Q. Xu, L. C. Briand, and K. Liu, Mutation-guided unit test generation with large language model, 2025. [Online]. Available: https://arxiv.org/abs/2506.02954 [20] M. Harman, J. Ritchey, I. Harper, S. Sengupta, K. Mao, A. Gulati, C. Foster, and H. Robert, Mutation-Guided LLM-based Test Generation at Meta. New York, NY, USA: Association for Computing Machinery, 2025, p. 180191. [Online]. Available: https://doi.org/10.1145/3696630. 3728544 [21] B. Li, W. Wu, Z. Tang, L. Shi, J. Yang, J. Li, S. Yao, C. Qian, B. Hui, Q. Zhang, Z. Yu, H. Du, P. Yang, D. Lin, C. Peng, and K. Chen, Prompting large language models to tackle the full software development [Online]. Available: https://arxiv.org/abs/2403. lifecycle: case study, 2024. [22] K. Jain, G. Synnaeve, and B. Rozi`ere, Testgeneval: real world unit test generation and test completion benchmark, 2025. [Online]. Available: https://arxiv.org/abs/2410.00752 [23] Y. Ye, T. Zhang, W. Jiang, and H. Huang, Process-supervised reinforcement learning for code generation, 2025. [Online]. Available: https://arxiv.org/abs/2502.01715 [24] N. Dai, Z. Wu, R. Zheng, Z. Wei, W. Shi, X. Jin, G. Liu, supervision-guided C. Dun, L. Huang, and L. Yan, policy optimization for code generation, 2025. [Online]. Available: https://arxiv.org/abs/2410.17621 Process [25] S. Dou, Y. Liu, H. Jia, E. Zhou, L. Xiong, J. Shan, C. Huang, X. Wang, X. Fan, Z. Xi, Y. Zhou, T. Ji, R. Zheng, Q. Zhang, T. Gui, and X. Huang, StepCoder: Improving code generation with reinforcement learning from compiler feedback, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), L.-W. Ku, A. Martins, and V. Srikumar, Eds. Bangkok, Thailand: Association for Computational Linguistics, Aug. 2024, pp. 45714585. [Online]. Available: https://aclanthology.org/2024.acl-long.251/ [26] Q. Li, X. Dai, X. Li, W. Zhang, Y. Wang, R. Tang, and Y. Yu, CodePRM: Execution feedback-enhanced process reward model for code generation, in Findings of the Association for Computational Linguistics: ACL 2025, W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, Eds. Vienna, Austria: Association for Computational [Online]. Available: https: Linguistics, Jul. 2025, pp. 81698182. //aclanthology.org/2025.findings-acl.428/ [27] A. K. Jain, G. Gonzalez-Pumariega, W. Chen, A. M. Rush, W. Zhao, and S. Choudhury, Multi-turn code generation through single-step rewards, in Forty-second International Conference on Machine Learning, 2025. [Online]. Available: https://openreview.net/forum?id=aJeLhLcsh0 [28] R. BELLMAN, markovian decision process, Journal of Mathematics and Mechanics, vol. 6, no. 5, pp. 679684, 1957. [Online]. Available: http://www.jstor.org/stable/24900506 [29] Z. Yu, W. Gu, Y. Wang, X. Jiang, Z. Zeng, J. Wang, W. Ye, and S. Zhang, Reasoning through execution: Unifying process and outcome rewards for code generation, in Forty-second International Conference on Machine Learning, 2025. [Online]. Available: https: //openreview.net/forum?id=pLQtovjXiw [30] Y. Akhauri, X. Song, A. Wongpanich, B. Lewandowski, and M. S. Abdelfattah, Regression language models for code, 2025. [Online]. Available: https://arxiv.org/abs/2509.26476 [31] D. Lee, C. Hwang, and K. Lee, Learning to generate unit test [Online]. Available: via adversarial reinforcement https://arxiv.org/abs/2508.21107 learning, 2025. [32] A. Prasad, E. Stengel-Eskin, J. Chen, Z. Khan, and M. Bansal, Learning to generate unit for automated debugging, in Second Conference on Language Modeling, 2025. [Online]. Available: https://openreview.net/forum?id=yeVBHPLXxi tests [33] Y. Wang, L. Yang, Y. Tian, K. Shen, and M. Wang, Co-evolving llm coder and unit tester via reinforcement learning, 2025. [Online]. Available: https://arxiv.org/abs/2506.03136 [34] C. Zhou, X. Zhang, D. Song, X. Chen, W. Gu, H. Ma, Y. Tian, M. Zhang, and L. Hu, Refinecoder: Iterative improving of large language models via adaptive critique refinement for code generation, 2025. [Online]. Available: https://arxiv.org/abs/2502. [35] Z. Xie, J. chen, L. Chen, W. Mao, J. Xu, and L. Kong, Teaching learning, in Fortylanguage models to critique via reinforcement second International Conference on Machine Learning, 2025. [Online]. Available: https://openreview.net/forum?id=UVoxPlv5E1 [36] G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, and Blistein, Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, arXiv e-prints, p. arXiv:2507.06261, Jul. 2025. [37] Mistral AI, DevStral: Introducing the best open-source model for coding agents. https://mistral.ai/news/devstral, 2025, accessed: 202505-30. [38] OpenAI, Introducing GPT-5 in the API, https://openai.com/index/ introducing-gpt-5/, 2025, accessed: 2025-10-28. [39] OpenAI, gpt-oss-120b & gpt-oss-20b model card, 2025. [Online]. Available: https://arxiv.org/abs/2508.10925 [40] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang et al., Qwen2.5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [41] Y. Zhao, J. Huang, J. Hu, X. Wang, Y. Mao, D. Zhang, Z. Jiang, Z. Wu, B. Ai, A. Wang, W. Zhou, and Y. Chen, Swift:a scalable lightweight infrastructure for fine-tuning, 2024. [Online]. Available: https://arxiv.org/abs/2408.05517 [42] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, Efficient memory management for large language model serving with pagedattention, in Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [43] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, LoRA: Low-rank adaptation of large language models, in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=nZeVKeeFYf9 [44] N. Burgess, J. Milanovic, N. Stephens, K. Monachopoulos, and D. Mansell, Bfloat16 processing for neural networks, in 2019 IEEE 26th Symposium on Computer Arithmetic (ARITH). IEEE, 2019, pp. 8891. [45] N. Shazeer and M. Stern, Adafactor: Adaptive learning rates with the 35th International sublinear memory cost, in Proceedings of Conference on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. PMLR, 1015 Jul 2018, pp. 45964604. [Online]. Available: https://proceedings.mlr. press/v80/shazeer18a.html [46] C. Spearman, Reprinted: The proof and measurement of association between two things (2010), International Journal of Epidemiology, vol. 39, no. 5, pp. 11371150, 1904."
        }
    ],
    "affiliations": [
        "Novosibirsk State University, Novosibirsk, Russia",
        "Siberian Neuronets LLC, Novosibirsk, Russia",
        "T-Technologies, Moscow, Russia"
    ]
}