{
    "paper_title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
    "authors": [
        "Mingqi Gao",
        "Yunqi Miao",
        "Jungong Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 0 4 8 0 . 2 1 5 2 : r SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos Mingqi Gao 1 Yunqi Miao 2 Jungong Han"
        },
        {
            "title": "Abstract",
            "content": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on inthe-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAMBody4D, training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while paddingbased parallel strategy enables efficient multihuman inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging inthe-wild videos, without any retraining. Our code and demo are available at: https://github. com/gaomingqi/sam-body4d. 1. Introduction Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D visual observations, offering explicit representations of human geometry. These representations are valuable for numerous human-centric applications, including humanrobot interaction, behaviour analysis, sports performance understanding, immersive VR/AR environments, and embodied AI. 1University of Sheffield, Sheffield, UK 2University of Warwick, Coventry, UK 3Department of Automation, Tsinghua University, Beijing, China. Correspondence to: Jungong Han <jungonghan77@gmail.com>. Preprint. December 10, 2025. Recent advances in image-based HMR have significantly improved generalization in in-the-wild conditions. Most notably, SAM 3D Body (Yang et al., 2025) achieves strong robustness through scalable data engine, separate optimization strategy for body and hands, and decoupled skeleton/shape representation via Momentum Human Rig (MHR (Ferguson et al., 2025)), resulting in more reliable performance than SMPL/SMPL-X-based methods on diverse and complex images. However, when extended to videos, most image-based HMR methods operate in frameby-frame manner, relying heavily on independent human detection results for each input image. As per-frame detections lack temporal continuity, the reconstructed human meshes often fluctuate and fail to remain stable in video scenarios (Fig. 1(c)). This becomes particularly problematic in in-the-wild settings where dynamic camera motion, background clutter, and frequent occlusions often cause mixed identities and tracking breakdowns. Although video-based HMR methods attempt to ensure temporal continuity by modeling temporal information (Kocabas et al., 2020) or incorporating tracking mechanisms, (Wang et al., 2024), they are fundamentally optimization based, which demands large annotated video datasets and carefully crafted objectives. Such reliance restricts their scalability and weakens the robustness to diverse and unpredictable in-the-wild human motions and scene dynamics. Building upon this insight, we propose SAM-Body4D, training-free framework for temporally consistent HMR from videos. Given an input video, SAM-Body4D generates identity-preserving mesh trajectories for the target humans. We first track and segment the target pixels using promptable video segmentation model, producing identityconsistent masklets that carry temporal continuity. These masklets then serve as prompts to guide SAM 3D Body, transferring the inherent temporal coherence of videos to the reconstructed 4D human meshes. Furthermore, an Occlusion-Aware Refiner is introduced to recover missing or corrupted regions caused by occlusions, preventing hallucinated predictions. Additionally, padding-based parallel strategy enables efficient multi-person and multi-frame inference without modifying pre-trained models. An overview of our framework is shown in Fig. 1. Our main contributions are summarized as follows: SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos Figure 1. Illustration of temporally consistent Human Mesh Recovery (HMR) from videos. (a) Input video frames. (b) Identity-consistent human masks, where each person is highlighted with unique and consistent colour across frames. (c) Vanilla image-to-video HMR baseline using SAM 3D Body with automatic human detection and per-frame inference. Note that only the meshes corresponding to the masks in (b) are visualised here; if mesh does not appear in certain frame, it indicates that the corresponding person is not detected in that frame. (d) Our spatial-temporal consistent HMR, where the temporal continuity in masklets is directly propagated into the 4D human meshes. (e) Our full SAM-Body4D with occlusion-aware refinement. Across the 2nd5th columns, SAM-Body4D recovers plausible and temporally stable reconstructions under occlusion. As these humans are heavily occluded, their complete meshes are visualised in the bottom-left corner for clearer observation. We present SAM-Body4D, training-free and scalable framework for temporally consistent and robust human mesh recovery from in-the-wild videos. We achieve identity-consistent 4D mesh trajectories across complex and dynamic scenes using temporally aligned masklets as prompts. We propose an occlusion-aware refinement mechanism that improves reconstruction quality under occlusions. 2. Related Work Human Mesh Recovery. Image-based human mesh recovery (HMR) predicts 3D human body meshes directly from RGB image. Existing approaches can be broadly categorized into regression-based methods and token-based methods. The former directly regresses parameters of 3D human models, such as SMPL-X (Pavlakos et al., 2019), from image features (e.g., HMR (Kanazawa et al., 2018), SPIN (Pavlakos et al., 2018), and PromptHMR (Wang et al., 2025)) while the latter represent joints or mesh vertices as learnable tokens and employ transformer reasoning to model their relationships, enabling more flexible and expressive structured prediction (e.g. TokenHMR (Dwivedi et al., 2024) and MEGA (Fiche et al., 2025)). Despite the strong performance of image-based HMR methods, they do not naturally adapt to video settings, where challenges such as temporal consistency and occlusions become critical. To enforce temporal smoothness, featurelevel temporal modeling approaches, such as VIBE (Kocabas et al., 2020) and TRAM (Wang et al., 2024), build upon image-based feature encoders and introduce temporal modules such as GRU and transformers to aggregate frame-wise representations and learn coherent motion cues over time. 4DHumans (Goel et al., 2023), on the other hand, jointly performs mesh reconstruction and identity-consistent tracking, enabling stable human modeling under occlusions and rapid pose variation. In addition, these approaches are fundamentally optimization-based, requiring large amounts of manually annotated video data and carefully designed loss functions, which restrict their generalizability and scalability. In contrast, our method is entirely training-free, enforcing temporal coherence from the source by directly leveraging the pixel-level continuity of humans in video, rather than relying on featureor pose-space temporal modelling where such continuity may already be lost. Video Object Segmentation (VOS) focuses on tracking and segmenting target object across video frames, typically based on few pixel/box annotations (Gao et al., 2023) or text prompts (Zhou et al., 2022). Prior VOS meth2 SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos ods generally follow memory-based paradigm (Oh et al., 2019; Cheng & Schwing, 2022; Cheng et al., 2024), where historical predictions are leveraged to maintain temporal consistency. However, these approaches still struggle to generalize to in-the-wild scenarios due to limited model capacity and training diversity. Benefiting from billion-scale training data and strong transformer architectures, SAM (Kirillov et al., 2023) demonstrates high segmentation accuracy, strong generalisation to in-the-wild images, and rich prompting modalities. By introducing memory mechanism, SAM 2 (Ravi et al., 2024) extends these strengths to the VOS setting. More recently, SAM 3 (Carion et al., 2025) incorporates text prompts and an independent object perceiver, enabling more user-friendly interactions and stronger robustness to common challenges such as disappearance and reappearance in complex videos. Despite strong performance on visible regions, SAM-based VOS methods cannot recover occluded body parts, resulting in incomplete visual cues for downstream HMR and causing hallucinated geometry during occlusion. To address this limitation, we introduce an occlusion-aware refinement module that reconstructs hidden regions and provides fullbody references for robust and consistent HMR in videos. 3. Preliminary SAM 3 (Carion et al., 2025) is promptable object segmentation model supporting both images and videos. Given t=1, It RHW 3, and user-defined video = {It}T prompts P, SAM 3 predicts mask sequence = {Mt}T t=1, Mt {0, 1}HW . For the t-th frame, the mask is obtained by combining two complementary modules: propagate and detect. ˆMt = propagate(Mt1), Ot = detect(It, P), Mt = match and update( ˆMt, Ot). (1) The propagate module leverages spatial-temporal correspondences between historical predictions (stored in memory) and the current frame, enabling reliable transfer of mask labels across time and preserving target continuity throughout the video. In contrast, the detect module focuses on semantic associations between the prompt and objects in the current frame, which is particularly effective for challenging cases such as disappearance and reappearance in complex scenes. By integrating the outputs of the two complementary modules through match and update, SAM 3 achieves substantially higher accuracy than SAM 2 and other VOS methods on complex videos. SAM 3D Body (Yang et al., 2025) is promptable model supporting Human Mesh Recovery (HMR) from in-the-wild images. It accepts prompts at both the encoder and decoder stages. Given an input image RHW 3, SAM 3D Body performs feature encoding as: = ImgEncoder(I, Penc), (2) where Penc denotes optional encoder prompts (e.g., 2D keypoints or segmentation masks) that help the model focus on the target human. With encoded features, the decoder predicts full-body tokens as: = Decoder(F, Pdec), (3) where Pdec includes optional prompts such as keypoint, camera, or MHR tokens. Then the first output token O0 is passed through an MLP to obtain MHR parameters: θ = {P, S, C, Sk} = MLP(O0), (4) where , S, C, and Sk denote pose, shape, camera pose, and skeleton parameters. During inference, body and hand are optimised separately due to different optimisation strategies and later fused. Unless otherwise specified, θ indicates the final full-body mesh parameters containing both body and hand information. 4. Methodology 4.1. Overview The framework of SAM-Body4D is illustrated in Fig. 2. Given an input video = {It}T t=1 and prompts = {P hi}N i=1 indicating target humans, SAM-Body4D estimates temporally consistent human mesh parameters Θ = {θhi t=1,i=1 for all selected persons. }T,N SAM-Body4D consists of three key components. Masklet Generator produces identity-consistent masklets as temporal tracking cues. An Occlusion-Aware Masklet Refiner enhances these masklets by recovering missing regions when occlusion occurs. Mask-Guided HMR module then predicts per-frame mesh parameters θt. Since each mesh is aligned with its corresponding mask over time, the temporal continuity in masklets is naturally propagated to the reconstructed human meshes. 4.2. Masklet Generator For each target human hi specified by prompts P, we apply SAM 3 (Carion et al., 2025) over the video to obtain spatio-temporally aligned masklets = {M hi }. Following the hybrid propagation-detection formulation in Eq. 1, identity consistency is maintained across frames. Note that other video segmentation models capable of producing temporally aligned instance masks can also be adopted here. 3 SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos Figure 2. Overall framework of the proposed SAM-Body4D. Given an input video with human prompts, SAM-Body4D operates on three main modules in training-free manner. The Masklet Generator derives identity-consistent temporal masklets from the video to provide spatio-temporal tracking cues. The Occlusion-Aware Masklet Refiner enriches these masklets by recovering invisible body regions and stabilizing temporal alignment. Finally, the Mask-Guided HMR module uses refined masklets as spatial prompts to predict accurate and temporally coherent human meshes across the entire sequence. 4.3. Occlusion-Aware Masklet Refiner In in-the-wild videos, humans frequently undergo severe occlusions, where even state-of-the-art segmentation methods like SAM 3 can only capture visible body regions. Such incomplete masklets provide insufficient visual evidence for human mesh estimation and may lead to hallucinated predictions with unrealistic pose and shape. To resolve this issue, we introduce an occlusion-aware refinement module to recover missing regions and offer complete visual references for the subsequent HMR stage. We first obtain mask completion results by feeding video frames and masklets into mask completion model (Diffusion-VAS (Chen et al., 2025)), producing completed masklets = { hi }. For each human hi at frame t, occlusions are detected when the completed mask area becomes larger while the overlap remains low: occ(t, hi) = 1 (cid:32) hi > hi IoU( hi , hi ) < 0.7 (cid:33) (5) To obtain accurate visual evidence for the occluded regions, the detected frames are temporally grouped and re-fed to Diffusion-VAS to recover missing pixels (see the yellow module in Fig. 2). We then update the corresponding frames and masks as: , hi (6) yielding refined video and refined masklets with fullt hi hi hi , body visual cues and improved temporal stability, which provide more reliable supervision for the subsequent HMR module and enable more accurate and consistent meshes across challenging frames. 4.4. Training-Free Mask-Guided HMR With refined video and refined masklets M, we perform training-free HMR. For each target human hi, the corresponding mask hi is used as the encoder prompt Penc in Eq. 2 and Eq. 3, enabling the model to focus on the correct identity. This produces per-frame mesh parameters Θ = {θhi t=1,i=1 consistent with the refined masklets over time. Importantly, the entire pipeline operates in trainingfree manner without any task-specific finetuning. }T,N We further improve the efficiency of the SAM 3D Body HMR stage. The original pipeline performs per-frame sequential inference, and the number of visible humans may vary across frames, making naive batching infeasible. We introduce simple padding mechanism to unify the batch shape so that all humans within the same frame batch can be processed jointly in single forward pass. This parallelisation eliminates redundant per-human inference and yields substantial speed-up while preserving the temporal alignment of the predicted meshes. To further enhance motion stability, we apply lightweight test-time temporal smoothing to the Momentum Human Rig pose and hand parameters, reducing jitter and promoting 4 SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos Figure 3. Visualised comparisons between the vanilla image-to-video extension of SAM 3D-Body and our SAM-Body4D. (a) Input video frames. (b) Identity-consistent human masks. (c) Vanilla per-frame HMR results using SAM 3D-Body with automatic human detection, where missed detections lead to missing meshes. (d) Our SAM-Body4D maintains temporally continuous and identity-preserving mesh trajectories throughout the video by leveraging spatial-temporal masklet guidance. Figure 4. Visualised comparisons between SAM-Body4D w/o and w/ Occlusion-Aware Masklet Refiner. (a) Input video frames; (b) Temporally consistent human masks, where each person is highlighted with unique and consistent color across frames; (c) SAM-Body4D without Occlusion-Aware Masklet Refiner; (d) SAM-Body4D with Occlusion-Aware Masklet Refiner. Across the 2nd6th columns, SAMBody4D produces more robust reconstructions under occlusion (e.g., the blue-rendered person in the 2nd column, the purple-rendered people in the 3rd/4th column, and the green-rendered people in the 5th and 6th columns). Since these subjects are heavily occluded, their meshes without occlusion are shown at the bottom-left/bottom-right for clearer observation. smooth transitions. In addition, for each target human, the scale and shape parameters from the first visible frame are reused across the entire sequence to maintain consistent body proportions and avoid identity drift. These operations require no learning and introduce negligible computational overhead, keeping the entire framework fully training-free. The full procedure of SAM-Body4D is summarised in Algorithm 1, which complements the structural illustration in Fig. 2 by explicitly detailing the execution flow, including identity-consistent masklet generation, occlusion-aware refinement, and our parallel mask-guided HMR strategy. 5. Experiments This section presents qualitative results to demonstrate the effectiveness of our training-free framework for video HMR. Our implementation combines SAM 3 (Carion et al., 2025) as the Masklet Generator, Diffusion-VAS (Chen et al., 2025) for occlusion detection and refinement, and SAM 3D Body (Yang et al., 2025) for Mask-Guided HMR. 5 SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos Algorithm 1 Training-Free Mask-Guided Video HMR Input: Video = {It}T Output: Mesh parameters Θ = {θhi t=1, prompts = {P hi }N }T,N t=1,i=1 i=1 // 1. Masklet Generation (SAM 3) Obtain initial masklets = {M hi // 2. Occlusion-Aware Refinement (Diffusion-VAS) Complete masks using Diffusion-VAS to obtain = { hi for each frame and human hi do } using SAM 3 via Eq. 1. }. if occlusion occ(t, hi) detected by Eq. 5 then Recover missing pixels and update video and masks via Eq. 6. end if end for Obtain refined video and refined masklets M. // 3. Parallel Mask-Guided HMR (SAM 3D Body) Construct frame batches and pad missing humans to fixed batch shape. for each frame batch do Use hi Compute θhi as encoder prompts Penc. via Eq. 2 and Eq. 3 in single forward pass. end for // 4. Temporal Smoothing in MHR Space for each human hi do Fix scale and shape to those from the first visible frame of hi. Apply Kalman smoothing to the per-frame pose and hand parameters of hi. end for return Θ The detection and refinement stages operate at spatial resolution of 512 1024, while lower resolutions can further improve efficiency with moderate loss in visual fidelity. The pipeline supports efficient deployment on single GPU. Without the occlusion refiner, our parallel multi-frame inference achieves substantial speed improvements over sequential per-frame HMR. For instance, on an NVIDIA A100-SXM4-80GB (96GB system memory), processing 480 854 video (90 frames, 5 persons) runs approximately 2 faster with parallel batch size of 32. When the refiner is enabled, both memory usage and runtime increase depending on the duration of the occlusion and the number of persons being refined. 5.1. Comparison with Vanilla Image-to-Video Extension We evaluate the vanilla image-to-video extension of SAM 3D Body, where mesh prediction is performed independently on each frame without any temporal enforcement. As shown in Fig. 3, such per-frame strategy cannot ensure continuous mesh trajectories for the same person across the video. When the target becomes small, suffers motion blur, or is heavily occluded, the detector may fail to localise the human, leading to missing meshes. 6 In contrast, SAM-Body4D leverages temporally aligned masklets to provide consistent target localisation throughout the sequence. These masklets are used as element-wise prompts for HMR, effectively transferring pixel-level continuity to 4D human meshes. Owing to the one-to-one correspondence between mask regions and reconstructed meshes, identity association remains stable, even when visibility temporarily degrades. This comparison highlights that enforcing spatial-temporal continuity at the pixel level is essential for reliable and stable video HMR. By preserving consistent localisation cues, SAM-Body4D maintains identity association and smooth mesh evolution across frames, enabling coherent 4D reconstruction throughout the video. 5.2. Effectiveness of Occlusion-Aware Refinement Occlusions occur frequently in real-world videos, where major body areas are temporarily hidden by objects or other people. In such cases, per-frame HMR relies on incomplete visual evidence and easily hallucinates implausible body structures. To demonstrate the benefit of our refinement, we present visual comparisons under challenging occlusion scenarios in Fig. 4. When only small portion of the body is occluded (e.g., first column of Fig. 4), the vanilla SAM 3D Body can still produce reasonable predictions. However, once most of the body becomes invisible, the baseline depends solely on the limited visible pixels and generates distorted and unstable meshes. In contrast, our occlusion-aware refiner restores missing human regions before HMR inference, enabling SAM-Body4D to preserve plausible pose and consistent body structure throughout the occluded frames. These results highlight that recovering occluded body evidence is essential to mitigate hallucinated predictions and achieve reliable 4D human reconstruction in challenging in-the-wild videos. 6. Conclusion We presented SAM-Body4D, training-free framework for temporally consistent Human Mesh Recovery (HMR) from videos. By leveraging identity-consistent masklets and an occlusion-aware refinement module, our approach effectively transfers pixel-level continuity into coherent 4D human mesh reconstruction. Without requiring any additional training or architectural modification to SAM-3D-Body, SAM-Body4D improves temporal stability and robustness in challenging in-the-wild scenarios. Our parallel multi-frame inference strategy further enables efficient and scalable deployment in practical applications. SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos"
        },
        {
            "title": "References",
            "content": "Carion, N., Gustafson, L., Hu, Y.-T., Debnath, S., Hu, R., Suris, D., Ryali, C., Alwala, K. V., Khedr, H., Huang, A., Lei, J., Ma, T., Guo, B., Kalla, A., Marks, M., Greer, J., Wang, M., Sun, P., Radle, R., Afouras, T., Mavroudi, E., Xu, K., Wu, T.-H., Zhou, Y., Momeni, L., Hazra, R., Ding, S., Vaze, S., Porcher, F., Li, F., Li, S., Kamath, A., Cheng, H. K., Dollar, P., Ravi, N., Saenko, K., Zhang, P., and Feichtenhofer, C. Sam 3: Segment anything with concepts, 2025. URL https://arxiv.org/abs/ 2511.16719. Chen, K., Ramanan, D., and Khurana, T. Using diffusion In CVPR, pp. priors for video amodal segmentation. 2289022900, 2025. Cheng, H. K. and Schwing, A. G. Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. In ECCV, pp. 640658. Springer, 2022. Kocabas, M., Athanasiou, N., and Black, M. J. Vibe: Video inference for human body pose and shape estimation. In CVPR, pp. 52535263, 2020. Oh, S. W., Lee, J.-Y., Xu, N., and Kim, S. J. Video object In segmentation using space-time memory networks. ICCV, pp. 92269235, 2019. Pavlakos, G., Zhu, L., Zhou, X., and Daniilidis, K. Learning to estimate 3d human pose and shape from single color image. In CVPR, pp. 459468, 2018. Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A. A. A., Tzionas, D., and Black, M. J. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Radle, R., Rolland, C., Gustafson, L., et al. Sam 2: Segment anything in images and videos. In ICLR, 2024. Cheng, H. K., Oh, S. W., Price, B., Lee, J.-Y., and Schwing, A. Putting the object back into video object segmentation. In CVPR, pp. 31513161, 2024. Wang, Y., Wang, Z., Liu, L., and Daniilidis, K. Tram: Global trajectory and motion of 3d humans from in-thewild videos. In ECCV, pp. 467487. Springer, 2024. Wang, Y., Sun, Y., Patel, P., Daniilidis, K., Black, M. J., and Kocabas, M. Prompthmr: Promptable human mesh recovery. In CVPR, pp. 11481159, 2025. Yang, X., Kukreja, D., Pinkus, D., Sagar, A., Fan, T., Park, J., Shin, S., Cao, J., Liu, J., Ugrinovic, N., Feiszli, M., Malik, J., Dollar, P., and Kitani, K. Sam 3d body: Robust full-body human mesh recovery. arXiv preprint; identifier to be added, 2025. Zhou, T., Porikli, F., Crandall, D. J., Van Gool, L., and Wang, W. survey on deep learning technique for video segmentation. IEEE TPAMI, 45(6):70997122, 2022. Dwivedi, S. K., Sun, Y., Patel, P., Feng, Y., and Black, M. J. Tokenhmr: Advancing human mesh recovery with tokenized pose representation. In CVPR, pp. 13231333, 2024. Ferguson, A., Osman, A. A., Bescos, B., Stoll, C., Twigg, C., Lassner, C., Otte, D., Vignola, E., Bogo, F., Santesteban, I., et al. Mhr: Momentum human rig. arXiv preprint arXiv:2511.15586, 2025. Fiche, G., Leglaive, S., Alameda-Pineda, X., and MorenoNoguer, F. Mega: Masked generative autoencoder for human mesh recovery. In CVPR, pp. 53665378, 2025. Gao, M., Zheng, F., Yu, J. J., Shan, C., Ding, G., and Han, J. Deep learning for video object segmentation: review. Artificial Intelligence Review, 56(1):457531, 2023. Goel, S., Pavlakos, G., Rajasegaran, J., Kanazawa, A., and Malik, J. Humans in 4d: Reconstructing and tracking humans with transformers. In ICCV, pp. 1478314794, 2023. Kanazawa, A., Black, M. J., Jacobs, D. W., and Malik, J. End-to-end recovery of human shape and pose. In CVPR, pp. 71227131, 2018. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In ICCV, pp. 40154026, 2023."
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University",
        "University of Sheffield",
        "University of Warwick"
    ]
}